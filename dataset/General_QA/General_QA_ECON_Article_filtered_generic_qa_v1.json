[
  {
    "qid": "econ-empirical-872-0-0-2",
    "question": "3) Compare the implications of 'full information' vs. 'limited information' assumptions for predicting post-merger repositioning.",
    "gold_answer": "1. **Full Information**: Carriers anticipate rivals' unobservables, leading to fewer nonstop launches post-merger (18% predicted vs. 25% observed). Mergers appear more profitable. \\n2. **Limited Information**: Carriers draw new unobservables post-merger, predicting 3x more nonstop launches. \\n3. **Empirical Fit**: Full information matches observed post-merger outcomes better, as unobservables are persistent.",
    "question_context": "We estimate a model of route-level competition between airlines who choose whether to offer nonstop or connecting service before setting prices. Airlines have full information about all quality, marginal cost, and fixed cost unobservables throughout the game, so that service choices will be selected on these residuals.\nMarket power created by a horizontal merger may be limited if the merger induces either new entry or existing rivals to reposition to compete more directly with the merging firms.\nWe use a two-stage model where carriers first choose their discrete service types (nonstop or connecting) and then choose prices.\n\nThe paper presents a model of route-level competition between airlines, where airlines first choose between offering nonstop or connecting service and then set prices. The model assumes full information about quality, marginal cost, and fixed cost unobservables, which affects service choices. Merger simulations account for repositioning and selection effects, showing that accounting for selection materially impacts predicted post-merger outcomes."
  },
  {
    "qid": "econ-empirical-1735-4-0-0",
    "question": "1) Derive the asymptotic distribution of Jeong, Härdle, and Song’s (2012) test statistic under the null hypothesis of noncausality in quantile for $\\beta$-mixing processes.",
    "gold_answer": "Under the null hypothesis of noncausality in quantile, the test statistic proposed by Jeong, Härdle, and Song (2012) converges to a standard normal distribution. The derivation involves:\n1. Establishing the weak convergence of the empirical process under $\\beta$-mixing conditions.\n2. Applying a central limit theorem for dependent processes to show that the standardized test statistic $T_n / \\sqrt{\\text{Var}(T_n)} \\xrightarrow{d} N(0,1)$.\n3. The variance term $\\text{Var}(T_n)$ is estimated nonparametrically to ensure consistency.",
    "question_context": "Jeong, Härdle, and Song (2012) have recently proposed a nonparametric test for Granger causality in quantiles. For $\\beta$-mixing processes and under the null hypothesis of noncausality in quantile, the asymptotic distribution of Jeong, Härdle, and Song’s (2012) test statistic is given by a standard normal distribution.\nFor finite samples, one might expect that our bootstrap-based test will perform better than Jeong, Härdle, and Song’s (2012) asymptotic-based test, which is justified only for large samples.\n\nThis section compares the empirical size and power of the local bootstrap-based test proposed in this study with Jeong, Härdle, and Song’s (2012) nonparametric test for Granger causality in quantiles. The comparison is conducted under various quantiles and sample sizes."
  },
  {
    "qid": "econ-empirical-412-2-0-1",
    "question": "2) Discuss the practical implications of the slow convergence of $\\varPi_{\\varepsilon}(S_{n})$ to $\\varPi_{\\varepsilon}(N)$ due to systematic risk, referencing empirical observations from McNeil et al. (2015).",
    "gold_answer": "1. **Systematic Risk**: Real-world portfolios exhibit systematic risk (e.g., market crashes), causing deviations from normality. \\n2. **Slow Convergence**: Empirical data (McNeil et al., 2015) show heavy tails and skewness, slowing CLT convergence. \\n3. **PELVE Behavior**: $\\varPi_{\\varepsilon}(S_{n})$ remains closer to $\\varPi_{\\varepsilon}(N)$ than to individual $\\varPi_{\\varepsilon}(X_{i})$, but not exactly $2.58$. \\n4. **Implication**: Diversification reduces tail risk, but asymptotic normality is not fully achievable in practice.",
    "question_context": "Suppose that $X_{1},\\ldots,X_{n}$ are losses from individual sources with finite variances. The portfolio loss $S_{n}$ is defined as $S_{n}=\\sum_{i=1}^{n}w_{i}X_{i}$ with $w_{1},\\dotsc,w_{n}\\geqslant0$ and $\\sum_{i=1}^{n}w_{i}=1$. As $n$ increases, $(S_{n}-a_{n})/b_{n}$ converges to a standard normal random variable $N$ under CLT conditions, leading to $\\varPi_{\\varepsilon}(S_{n})=\\varPi_{\\varepsilon}\\left(\\frac{S_{n}-a_{n}}{b_{n}}\\right)\\rightarrow\\varPi_{\\varepsilon}(N)$.\n\nDiversification techniques are widely used in portfolio risk management. A proper diversification should, in theory, reduce the volatility of the portfolio, and this is often reflected in a reduction of standard deviation, VaR, ES, or other risk measures of the portfolio loss compared to the sum of the individual assets."
  },
  {
    "qid": "econ-empirical-66-3-0-2",
    "question": "3) Using the interaction term in column (1) of Table V, calculate the difference in assumed returns between a firm at the 10th percentile (−2.85) and 90th percentile (1.11) of pension sensitivity during acquisition years.",
    "gold_answer": "1. Interaction effect: 6.4 basis points per unit log sensitivity.  \n2. Sensitivity gap: 1.11 − (−2.85) = 3.96.  \n3. Difference = 6.4 × 3.96 = 25.3 basis points (matches text's 25 basis points).",
    "question_context": "Firms that will eventually engage in merger activity do appear different from other firms. Conditional only on takeover activity in five years and none in the current year, assumed returns are almost 15 basis points higher than their unconditional expectation in the complementary group of firms. However, firm return assumptions are almost 30 basis points higher during merger years than during other years.\nThe regressions in these columns include the acquirer indicator as well as its interaction with the pension sensitivity measures. The interaction term allows the effect of this incentive to vary based on the sensitivity of earnings to the assumed return.\nThe coefficient on the acquirer indicator implies a baseline effect of 33.6 basis points at a log sensitivity of zero. This means that firms making acquisitions assume a return that is 33.6 basis points higher than firms in the same industry-year that are not making acquisitions. The interaction effect shows that for each additional point of log sensitivity, this effect is 6.4 basis points higher.\n\nThis section explores how managerial opportunism influences assumed returns, particularly during acquisition periods. Firms may inflate reported earnings to boost stock prices and bargaining power in acquisitions."
  },
  {
    "qid": "econ-empirical-223-3-0-2",
    "question": "3) Show how the term $\\psi(T_{i},X_{i},Y_{i};t)$ simplifies when $w(T;\\pmb\\theta)=\\nabla_{\\pmb\\theta}g(T;\\pmb\\theta)$, and explain the implications for the asymptotic variance of the test statistic.",
    "gold_answer": "1. **Simplification**: When $w(T;\\pmb\\theta)=\\nabla_{\\pmb\\theta}g(T;\\pmb\\theta)$, the term $\\psi(T_{i},X_{i},Y_{i};t)$ simplifies because the second and fourth terms cancel out:\n   $$\\psi(T_{i},X_{i},Y_{i};t) = \\mathbb{E}\\left[\\pi_{0}(T_{i},X_{i})\\cdot\\frac{\\partial}{\\partial g}\\mathbb{E}[m\\left\\{Y_{i}g(T_{i};\\theta^{*})\\right\\}|T_{i},X_{i}]\\cdot\\nabla_{\\theta}g(T_{i};\\theta^{*})^{\\top}\\mathcal{H}(T_{i},t)\\right].$$\n\n2. **Asymptotic Variance**: The simplification reduces the complexity of the asymptotic variance $\\Sigma(t,t')$, making it easier to approximate the null limiting distribution in practice. It also ensures that the test statistic is more efficient.",
    "question_context": "$g(t;\\pmb\\theta)$ is twice continuously differentiable in $\\pmb\\theta\\in\\Theta$; $\\mathbb{E}[m\\{Y;g(T;\\theta^{*})\\}|T=t,X=x]$ is continuously differentiable in $(t,\\pmb{x})$; $\\mathbb{E}\\left[\\pi_{0}(T,X)m\\{Y;g(T;\\pmb{\\theta})\\}w(T;\\pmb{\\theta})|T=t,X=x\\right]$ is differentiable w.r.t. $\\pmb\\theta$ and $\\nabla_{\\pmb\\theta}\\mathbb{E}[\\pi_{0}(T,X)m\\{Y;g(T;\\pmb\\theta)\\}$ $w(T;\\pmb\\theta)]|_{\\pmb\\theta=\\pmb\\theta^{*}}$ is of full (column) rank.\n$\\mathbb{E}\\left[\\operatorname*{sup}_{\\pmb{\\theta}\\in\\Theta}|m\\{Y;g(T;\\pmb{\\theta})\\}|^{2+\\delta}\\right]<\\infty$ for some $\\delta>0$; The function class $\\left\\{m\\{Y;g(T;\\pmb\\theta)\\}:\\pmb\\theta\\in\\Theta\\right\\}$ satisfies: for any $\\pmb\\theta\\in\\Theta$ and any small $\\delta>0$ and for some finite positive constant $C$.\n$\\phi(T_{i},X_{i};t):=\\pi_{0}(T_{i},X_{i})\\cdot{\\mathcal{H}}(T_{i},t)\\cdot\\mathbb{E}[m\\left\\{Y_{i};g(T_{i};\\theta^{*})\\right\\}|T_{i},X_{i}] -\\mathbb{E}[\\pi_{0}(T_{i},X_{i})m\\left\\{Y_{i};g(T_{i};\\theta^{*})\\right\\}\\cdot{\\mathcal{H}}(T_{i},t)|X_{i}]$\n$\\psi(T_{i},X_{i},Y_{i};t):=\\mathbb{E}\\Bigg[\\pi_{0}(T_{i},X_{i})\\cdot\\frac{\\partial}{\\partial g}\\mathbb{E}[m\\left\\{Y_{i}g(T_{i},\\theta^{*})\\right\\}|T_{i},X_{i}]\\cdot\\nabla_{\\theta}g(T_{i},\\theta^{*})^{\\top}\\mathcal{H}(T_{i},t)\\Bigg] \\times \\Bigg\\{\\mathbb{E}\\left[\\pi_{0}(T_{i},X_{i})\\cdot\\frac{\\partial}{\\partial g}\\mathbb{E}[m\\left\\{Y_{i}g(T_{i},\\theta^{*})\\right\\}|T_{i},X_{i}]\\cdot\\nabla_{\\theta}g(T_{i},\\theta^{*})w(T_{i};\\theta^{*})^{\\top}\\right] \\times\\mathbb{E}\\left[\\pi_{0}(T_{i},X_{i})\\cdot\\frac{\\partial}{\\partial g}\\mathbb{E}[m\\left\\{Y_{i}g(T_{i},\\theta^{*})\\right\\}|T_{i},X_{i}]\\cdot w(T_{i},\\theta^{*})\\nabla_{\\theta}^{\\top}g(T_{i},\\theta^{*})\\right]\\Bigg\\}^{-1} \\times\\mathbb{E}\\left[\\pi_{0}(T_{i},X_{i})\\cdot\\frac{\\partial}{\\partial g}\\mathbb{E}[m\\left\\{Y_{i}g(T_{i},\\theta^{*})\\right\\}|T_{i},X_{i}]\\cdot\\nabla_{\\theta}g(T_{i},\\theta^{*})w(T_{i};\\theta^{*})^{\\top}\\right] \\times\\Bigg\\{\\pi_{0}(T_{i},X_{i})\\cdot\\frac{\\partial}{\\partial g}\\mathbb{E}[m\\left\\{Y_{i}g(T_{i},\\theta^{*})\\right\\}|T_{i},X_{i}]\\cdot\\nabla_{\\theta}g(T_{i},\\theta^{*})[T_{i},\\theta^{*})^{\\top}\\Bigg\\}$\n$\\eta(T_{i},X_{i},Y_{i};t):=U_{i}{\\mathcal{H}}(T_{i},t)-\\phi(T_{i},X_{i};t)-\\psi(T_{i},X_{i},Y_{i};t)$\n\nThis section discusses the regularity conditions and special cases for the continuous treatment effect models, including average and quantile dose-response models."
  },
  {
    "qid": "econ-empirical-1022-2-1-0",
    "question": "1) Explain the heuristic behind the modified bootstrap proposed in this section. How does it address the limitations of the standard bootstrap?",
    "gold_answer": "**Heuristic**: The modified bootstrap combines:\n\n1. **Analytical Approximation**: Uses a known or estimated derivative $\\hat{\\phi}_{n}^{\\prime\\prime}$ to approximate $\\phi_{\\theta_{0}}^{\\prime\\prime}$.\n2. **Bootstrap Resampling**: Employs $r_{n}\\{\\hat{\\theta}_{n}^{*}-\\hat{\\theta}_{n}\\}$ to approximate the law of $\\mathbb{G}$.\n\n**Limitations Addressed**:\n- For nonsmooth $\\phi$, the standard bootstrap fails to estimate $\\phi_{\\theta_{0}}^{\\prime}$ consistently. The modified bootstrap bypasses this by directly estimating $\\phi_{\\theta_{0}}^{\\prime\\prime}$.\n- For smooth $\\phi$ with known $\\phi_{\\theta_{0}}^{\\prime\\prime}$, the modified bootstrap avoids neglecting non-negligible terms.",
    "question_context": "The heuristics underlying our proposal, however, are connected to those in Fang and Santos (2018) in a subtle way. In the context of first order asymptotics where $\\phi$ is only directionally differentiable, inconsistency of the standard bootstrap arises from its inability to properly estimate the directional derivative $\\phi_{\\theta_{0}}^{\\prime}$ . In our setup, however, there are examples in which the derivative $\\phi_{\\theta_{0}}^{\\prime\\prime}$ is a known map; see Example 2.1. The standard bootstrap in these settings fails because there is a non-negligible term being neglected. However, in all other examples where $\\phi$ is not smooth enough, Fang and Santos (2018)’s arguments will come into play as well.\n\nA modified bootstrap is proposed to handle nonsmooth maps and first-order degeneracy, building on the work of Fang and Santos (2018)."
  },
  {
    "qid": "econ-empirical-433-4-2-0",
    "question": "1) Interpret the estimated response functions $\\hat{\\psi}_{L}$ in Fig. 3, focusing on the U-shaped response of dividend growth to volatility.",
    "gold_answer": "1. **Consumption-output ratio**: Monotonically increases in expected growth, weakly related to volatility.\\n2. **Dividend growth**: U-shaped response to volatility, peaking at high and low volatility levels, with no clear relation to expected growth.",
    "question_context": "Fig. 3 shows the estimated response functions $\\hat{\\psi}_{L}$ of log consumption–output ratio and dividend growth to $x_{t}$ and $V_{t}$.\nTable 6 reports the simulated maximum likelihood estimates of the transition density parameters $\\theta_{s}$ for the LRR-ARG model.\n\nThis section presents full-information estimates of the LRR-ARG model, including response functions, parameter estimates, and pricing functions."
  },
  {
    "qid": "econ-empirical-1141-1-0-0",
    "question": "1) Derive Yitzhaki's relative deprivation measure $R D_{i}=\\frac{1}{N}\\Sigma_{j}(y_{j}-y_{i})\\forall y_{j}>y_{i}$ from Runciman's conceptual definition of relative deprivation.",
    "gold_answer": "1. Runciman's definition states that an individual feels deprived if they lack $X$ (income $y_i$) while others in their reference group possess it.  \n2. The deprivation intensity depends on the gap $(y_j - y_i)$ for all $y_j > y_i$.  \n3. Summing over all such gaps captures the total deprivation: $\\Sigma_{j}(y_{j}-y_{i})$.  \n4. Normalizing by $N$ makes the measure invariant to reference group size: $\\frac{1}{N}\\Sigma_{j}(y_{j}-y_{i})$.",
    "question_context": "For a person i with income $y_{i}$ who is part of a reference group with $\\mathbf{N}$ people, Yitzhaki's measure is defined as: $$ R D_{i}=\\frac{1}{N}\\Sigma_{j}(y_{j}-y_{i})\\forall y_{j}>y_{i} $$\nThe relative deprivation measure can be rewritten as: $$ R D_{i}=[E(y\\mid y>y_{i})-y_{i}]^{*}\\mathrm{~prob~}(y>y_{i}) $$\nAn alternative measure using log income is: $$ \\mathrm{RDoflogs}_{i}=\\Big\\{\\pmb{\\upmu}_{r}+\\frac{\\upsigma_{r}\\upphi_{i}}{(1-\\Phi_{i})}\\Bigg]-\\ln{(y_{i})}\\}^{*}(\\mathrm{~1~}-F_{i}) $$\nThe individual's z-score is defined as: $$ z-\\mathrm{score}=\\frac{(y_{i}-\\upmu_{r})}{\\upsigma_{r}}=\\left(\\frac{y_{i}}{\\upsigma_{r}}\\right)-\\left(\\frac{\\upmu_{r}}{\\upsigma_{r}}\\right) $$\n\nThe seminal definition of relative deprivation is accredited to Runciman (1966), who argues that an individual is relatively deprived if they lack something that others in their reference group possess. This concept is formalized in economic terms by Yitzhaki (1979), who provides a mathematical framework for measuring relative deprivation based on income comparisons within a reference group."
  },
  {
    "qid": "econ-empirical-1377-2-2-1",
    "question": "6) Propose a utility-based model incorporating inertia to explain the differential adoption of narrow-network plans by new vs. existing enrollees.",
    "gold_answer": "1. **Utility Function**: \\( U_i = \\beta_0 - \\beta_1 p_i + \\beta_2 I(\\text{Existing}) + \\epsilon_i \\), where \\( p_i \\) is plan price.\n2. **Inertia Term**: \\( \\beta_2 > 0 \\) captures switching costs for existing enrollees.\n3. **Price Sensitivity**: \\( \\beta_1 \\) estimated from new enrollees' choices.\n4. **Prediction**: Existing enrollees require larger \\( \\Delta p \\) to switch (higher \\( \\beta_2 \\)).",
    "question_context": "In 2012, the GIC offered a three-month 'premium holiday' for all active state employees who chose to switch to a narrow-network plan. For households choosing to make the switch, the holiday entailed that they pay no premiums for three months of the fiscal year.\nThese differences in behavior between new members (or those making active choices) and existing members (those potentially making passive choices) suggest the presence of a high degree of inertia in choosing health plans.\n\nThe paper identifies inertia in plan choices and price sensitivity among enrollees, supported by evidence from narrow-network plan adoption and the premium holiday."
  },
  {
    "qid": "econ-empirical-1306-1-1-0",
    "question": "5) Derive the optimal supply rule for supplier i under learning, given the perceived demand curve $p_{t+k} = \\hat{\\gamma}_{i t}^{0} + \\hat{\\gamma}_{i t}^{1}q_{i t+k} + \\eta_{i t+k}$ and show that it leads to $\\hat{q}_{i t} = \\frac{\\lambda_{i t} - \\hat{\\gamma}_{i t}^{0}}{2\\hat{\\gamma}_{i t}^{1}}$.",
    "gold_answer": "1. Set up the Lagrangian for supplier i: $\\mathcal{L} = E_{t}\\sum_{k=0}^{T_{i t}}\\beta^{k}(\\hat{\\gamma}_{i t}^{0} + \\hat{\\gamma}_{i t}^{1}q_{i t+k})q_{i t+k} + \\lambda_{i t}(Q_{i t} - E_{t}\\sum_{k=0}^{T_{i t}}q_{i t+k})$.\\n2. Take the first-order condition with respect to $q_{i t+k}$: $\\frac{\\partial \\mathcal{L}}{\\partial q_{i t+k}} = \\beta^{k}(\\hat{\\gamma}_{i t}^{0} + 2\\hat{\\gamma}_{i t}^{1}q_{i t+k}) - \\lambda_{i t} = 0$.\\n3. Solve for $q_{i t+k}$: $\\hat{q}_{i t} = \\frac{\\lambda_{i t} - \\hat{\\gamma}_{i t}^{0}}{2\\hat{\\gamma}_{i t}^{1}}$.",
    "question_context": "Each supplier believes that the supply of its competitor has followed a linear strategy: $\\hat{q}_{j t} = q_{j}^{0} - q_{j}^{1}\\hat{q}_{i t}$, where $q_{j}^{0}$ and $q_{j}^{1}$ are unknown constants.\nSupplier i estimates a regression of the form: $p_{t} = \\gamma_{i}^{0} + \\gamma_{i}^{1}q_{i t} + \\eta_{i t}$, using discounted least squares: $\\hat{\\gamma}_{i t+1} = \\hat{\\gamma}_{i t} + \\varepsilon R_{i t}^{-1}X_{i t}^{\\prime}(p_{t} - X_{i t}\\hat{\\gamma}_{i t})$.\nThe approximately optimal depletion plan for supplier i solves: $\\max_{\\hat{q}_{i+},T_{i t}} E_{t}\\sum_{k=0}^{T_{i t}}\\beta^{k}p_{t+k}q_{i t+k}$ subject to $p_{t+k} = \\hat{\\gamma}_{i t}^{0} + \\hat{\\gamma}_{i t}^{1}q_{i t+k} + \\eta_{i t+k}$ and the resource constraint.\n\nThis section relaxes the assumption of full rationality, allowing suppliers to learn about the structural parameters of the demand curve and competitor behavior using an approximating model."
  },
  {
    "qid": "econ-empirical-1324-1-0-0",
    "question": "1) Prove that under Axioms A1 and A2, the Strong Subjective Substitution property holds as stated in Theorem 1.1.",
    "gold_answer": "1. Assume $f \\succcurlyeq_A g$ and $f \\succ_B g$ for disjoint $A, B \\subseteq \\Omega$. \\\\ 2. By Definition 1.1, there exists $h$ such that $(f_A, h_{-A}) \\succcurlyeq (g_A, h_{-A})$. \\\\ 3. Since $f \\succ_B g$, by A2, $(f_B, h_{-B}) \\succ (g_B, h_{-B})$ for some $h$. \\\\ 4. By A1 (transitivity), $(f_{A \\cup B}, h_{-(A \\cup B)}) \\succ (g_{A \\cup B}, h_{-(A \\cup B)})$. \\\\ 5. Thus, $f \\succ_{A \\cup B} g$.",
    "question_context": "A1 (Order). $\\succcurlyeq$ is transitive and complete on $\\mathcal{P}^{\\mathcal{Q}}$.\nA2 (Objective Independence). For all $f,g,h\\in\\mathcal{P}^{\\varOmega}$ and $\\alpha\\in(0,1],f\\succ$ (resp. $\\sim$ ) $g\\Rightarrow f\\alpha h\\succ$ (resp. t ) g:h.\nA3 (Non-triviality). There are f $;g\\in\\mathcal{P}^{\\mathcal{Q}}$ such that $f\\succ g$.\nA4 (Archimedean Property). $f\\succ g\\succ h\\Rightarrow\\exists\\alpha$ , $\\beta\\in(0,1)$ , $\\alpha>\\beta$ , such that $f\\alpha h\\succ g\\succ f\\beta h$.\nDefinition 1.1. Given $A\\subseteq\\varOmega,f,g\\in\\mathscr{P}^{\\varOmega},j$ $f$ is preferred to $g$ conditional on $A$ , written $f{\\succcurlyeq}_{\\mathcal{A}}g$ , if $(f_{A},h_{-A})\\succcurlyeq(g_{A},h_{-A})$ for some $h\\in\\mathcal{P}^{\\mathcal{Q}}$.\nTheorem 1.1. Under Axioms A1 and A2, the following property holds: (Strong Subjective Substitution) \\f, $g\\in\\mathcal{P}^{\\mathcal{Q}}$ , $\\forall A$ , $B\\subseteq{\\mathcal{Q}}$ with $A\\cap B=\\emptyset$ , if $f\\succcurlyeq_{A}g$ and $f\\succ_{B}$ (resp. $\\succcurlyeq_{B}$ ) $g$ then $f{\\succ}_{{A\\cup B}}$ (resp. $\\succcurlyeq_{A\\cup B}$ ) $g$ (cf. Blume et al. [5, Theorem 4.1]).\nDefinition 1.2. An event $A\\subseteq{\\mathcal{Q}}$ is Savage-null if $f\\sim_{A}g\\forall f,g\\in\\mathcal{P}^{\\Omega}$.\nA5 (Non-null-state Independence). For all states $\\omega$ , $\\omega^{\\prime}\\in\\varOmega$ which are not Savage-null and for any two constant acts $f,g\\in{\\mathcal{P}}^{\\varOmega}$ , $f\\succcurlyeq_{\\omega}g\\Leftrightarrow f\\succcurlyeq_{\\omega^{\\prime}}g$.\n\nThe following are the usual axioms for an expected-utility representation of preferences on $\\mathcal{P}^{\\mathcal{Q}}$. Let $\\succcurlyeq$ be a binary relation on $\\mathcal{P}^{\\itOmega^{\\itOmega}}$ (preference relation); $\\succ$ and $\\sim$ are defined in the usual way."
  },
  {
    "qid": "econ-empirical-1738-0-0-0",
    "question": "1) Derive the dynamic utility maximization problem for a female making life-cycle marital status and labour force participation decisions, considering the interdependence between these decisions and the presence of children.",
    "gold_answer": "The dynamic utility maximization problem can be formulated as follows:\n\n1. **Objective Function**: The female maximizes the present value of utility over a finite horizon \\( T \\):\n   $$\n   E\\sum_{t=1}^{T}\\delta^{t-1}[U(p_{t},m_{t},c_{t};n_{t},p_{t-1},m_{t-1},m d u r_{t-1},X_{1t})]\n   $$\n   where \\( \\delta \\) is the discount factor, \\( p_t \\) is the labour force participation decision, \\( m_t \\) is the marital status, \\( c_t \\) is consumption, \\( n_t \\) indicates the presence of children, and \\( X_{1t} \\) represents individual characteristics.\n\n2. **Utility Function**: The utility function is specified as:\n   $$\n   U(p_{t},m_{t},c_{t};n_{t},p_{t-1},m_{t-1},m d u r_{t-1},X_{1t}) = a_{1t}m_{t} + (a_{2t} + a_{3t}m_{t})p_{t} + (\\beta_{1} + \\beta_{2}p_{t} + \\beta_{3}m_{t})c_{t} + \\text{stochastic components}\n   $$\n   where \\( a_{1t}, a_{2t}, a_{3t} \\) are parameterized functions of individual characteristics and previous experiences.\n\n3. **Budget Constraint**: The budget constraint is given by:\n   $$\n   w_t p_t + \\psi(p_t) w_t^h m_t = c_t\n   $$\n   where \\( w_t \\) is the female's wage, \\( w_t^h \\) is the husband's wage, and \\( \\psi(p_t) \\) is the fraction of the husband's income available for consumption.\n\n4. **Dynamic Programming**: The problem is solved by backward induction, considering the dependence of wages on previous work history and the stochastic nature of marital opportunities and fertility.",
    "question_context": "The model incorporates different types of uncertainty such as those associated with marital search, divorce, current and future own and husband's wage earnings and the birth of children, which is characterized in this model by an exogenous stochastic process which is conditional on the female's marital status.\nThe structural model enables us to examine the joint determination of labour supply and marital status decisions of women and the dependence of these decisions on the presence of children and other individual characteristics.\nThe estimates of the model are used to predict differences in the life-cycle patterns of employment, marriage and divorce due to differences in education, race, the female's (potential) earnings and her (potential) husband's wage income.\n\nThe paper presents a dynamic utility maximization model to study the interdependence between life-cycle marital status and labour force participation decisions of women. The model is estimated using longitudinal data from the Panel Study of Income Dynamics (PSID) and employs Maximum Likelihood Estimation (MLE) and a minimum distance estimator."
  },
  {
    "qid": "econ-empirical-1028-4-1-1",
    "question": "6) Using the model comparison results, formally show why Model A (forward-looking money demand) is preferred over Model B (static money demand) based on marginal data densities.",
    "gold_answer": "1. The marginal data density for model $M$ is: $p(data|M) = \\int L(\\theta|data,M)p(\\theta|M)d\\theta$. \\n2. Using Geweke's modified harmonic mean estimator: $\\hat{p}(data|M) = \\left[ \\frac{1}{N} \\sum_{i=1}^N \\frac{f(\\theta^i)}{L(\\theta^i|data)p(\\theta^i)} \\right]^{-1}$ where $f(\\theta)$ is an importance sampling density. \\n3. For nested models, the Bayes factor is $B_{A,B} = \\frac{p(data|A)}{p(data|B)}$. \\n4. Table results show $\\log p(data|A) > \\log p(data|B)$ by a significant margin, strongly favoring Model A.",
    "question_context": "Posterior distributions are obtained using the Metropolis-Hastings algorithm with 250,000 draws after discarding the first 50,000 draws. For the Inverse Gamma distribution the degrees of freedom are reported as indicated by an asterisk.\nModel A is the preferred version compared to model B, thus supporting our conclusions on the role of money as an information variable. Our results are robust to model perturbation where all households are optimising ($\\psi=1$).\n\nThis appendix presents estimation results for the model variants, comparing posterior distributions and marginal data densities across different specifications."
  },
  {
    "qid": "econ-empirical-1355-3-0-1",
    "question": "2) Extend the proof of Theorem 1 to show why no stable matching rule exists that is non-manipulable via capacities when there are two hospitals and three interns. Use the given preference structure and capacity constraints.",
    "gold_answer": "1. With $q_{h_{1}}=q_{h_{2}}=2$, $\\mathcal{S}(R,q_{h_{1}},q_{h_{2}})=\\{\\mu_{1}\\}$ ($\\mu_{1}(h_{1})=\\{i_{2},i_{3}\\}$, $\\mu_{1}(h_{2})=\\{i_{1}\\}$)\n2. With $q_{h_{1}}^{\\prime}=q_{h_{2}}^{\\prime}=1$, $\\mathcal{S}(R,q_{h_{1}}^{\\prime},q_{h_{2}}^{\\prime})=\\{\\mu_{3}\\}$ ($\\mu_{3}(h_{1})=\\{i_{1}\\}$, $\\mu_{3}(h_{2})=\\{i_{3}\\}$)\n3. Any stable rule $\\varphi$ must select:\n   - $\\varphi(R,q_{h_{1}},q_{h_{2}})=\\mu_{1}$\n   - $\\varphi(R,q_{h_{1}}^{\\prime},q_{h_{2}}^{\\prime})=\\mu_{3}$\n4. If $\\varphi(R,q_{h_{1}},q_{h_{2}}^{\\prime})=\\mu_{1}$, then $h_{1}$ gains by reporting $q_{h_{1}}^{\\prime}=1$\n5. If $\\varphi(R,q_{h_{1}},q_{h_{2}}^{\\prime})=\\mu_{2}$, then $h_{2}$ gains by reporting $q_{h_{2}}^{\\prime}=1$",
    "question_context": "Proposition 1. Suppose there are at least two hospitals and two interns. Then the hospital-optimal stable rule is not immune to manipulation via capacities.\nLet $H=\\left\\{h_{1},h_{2}\\right\\}$ , $I=\\left\\{i_{1},i_{2}\\right\\}$ , \n\n$$\n\\begin{array}{r l}&{\\left\\{i_{1},i_{2}\\right\\}P_{h_{1}}\\{i_{1}\\}P_{h_{1}}\\{i_{2}\\}P_{h_{1}}\\emptyset,}\\ &{\\left\\{i_{1},i_{2}\\right\\}P_{h_{2}}\\{i_{2}\\}P_{h_{2}}\\{i_{1}\\}P_{h_{2}}\\emptyset,}\\ &{\\quad\\quad\\left\\{h_{2}\\right\\}P_{i_{1}}\\{h_{1}\\}P_{i_{1}}\\emptyset,}\\ &{\\quad\\quad\\left\\{h_{1}\\right\\}P_{i_{2}}\\{h_{2}\\}P_{i_{2}}\\emptyset,}\\end{array}\n$$ \n\n$q_{h_{1}}=q_{h_{2}}^{\\prime}=1$ and $q_{h_{2}}=2$ .\nTheorem 1. Suppose there are at least two hospitals and three interns. Then these exists no matching rule that is stable and non-manipulable via capacities.\nRemark 1. Theorem 1 does not hold if (i) there is only one hospital, (ii) there is only one intern, and (iii) there are two hospitals and two interns.\n\nThe NRMP uses the hospital-optimal stable rule to match medical interns and hospitals in United States. This section explores the manipulability of this rule via capacities."
  },
  {
    "qid": "econ-empirical-714-5-0-4",
    "question": "5) Discuss the economic implications of including the second latent factor $L_{t}$ in the model. How does its omission affect the performance of PFM 2bis?",
    "gold_answer": "1. **Role of $L_{t}$**: $L_{t}$ accounts for short-term liquidity frictions, improving the accuracy of the $\\mu_{p}$ and $\beta$ estimates. \\n2. **Impact of Omission**: PFM 2bis (without $L_{t}$) shows lower average returns (0.1678 vs. 0.1943) and a slightly higher turnover (1.0498 vs. 1.0460) compared to PFM 2. This demonstrates that ignoring $L_{t}$ leads to less efficient parameter estimates and weaker momentum signals.",
    "question_context": "The first portfolio (PFM 1) consists of stocks presenting statistically significant and positive autocorrelation coefficients of daily returns for the period ranging from January 1st, 2010 to December 31, 2013 (i.e., 14 stocks). Based on these stocks, we build a momentum portfolio following a simple trading rule: at the end of the trading day t, we take a long (short) position on an equally weighted portfolio of stocks with positive (negative) returns for that day and compute its performance at day $t+1$.\nThe second portfolio (PFM 2) consists of stocks presenting jointly statistically significant and positive $\\mu_{p}$ and $\beta$ parameters over our sample period, Jan. 2010–Dec. 2013 (i.e., 18 stocks). For each of these stocks, we assume that the parameters of our model remain unchanged during the out of sample period (Jan.–Dec. 2014) and use the Extended Kalman Filter methodology to filter the latent variable $I_{t}$, given these parameters up to December 2014.\nPFM 2 presents a larger average return associated with a lower average risk and a much better Sharpe ratio, showing that PFM 2 combining our $\\mu_{p}$ and $\beta$ parameters with the signal extracted from the stock-specific latent variable $I_{t}$ outperforms the standard momentum strategy combining the empirical serial correlation coefficient and the past daily returns.\nThe average turnover of PFM 1 is twice as high as that of PFM 2, which suggests that the relative outperformance of PFM 2 cannot be challenged by the presence of transaction costs.\nIn the absence of $L_{t}$ in our model specification – referred to as the dynamic RS version – the interaction between daily returns and volumes depends solely on $I_{t}$, which will induce an error in the estimated parameters for stocks affected by short-term liquidity frictions.\nFor each momentum portfolio $p$, we run a time-series regression of its daily out-of-sample returns $R_{p,t}$ (Jan.–Dec. 2014) on a set of four risk factors commonly used to assess stock returns: $R_{p,t} = \\alpha_{p} + b_{1,p}F_{MKT,t} + b_{2,p}F_{SMB,t} + b_{3,p}F_{HML,t} + b_{4,p}F_{MOM,t} + \\epsilon_{p,t}$.\n\nThis section discusses the construction and performance comparison of two momentum portfolios, PFM 1 and PFM 2, based on different liquidity signals. PFM 1 uses autocorrelation of returns, while PFM 2 incorporates the latent variable $I_{t}$ derived from $\\mu_{p}$ and $\beta$ parameters. The section also explores the economic value of including a second latent factor $L_{t}$ and presents empirical results from factor analysis."
  },
  {
    "qid": "econ-empirical-514-1-0-0",
    "question": "1) Prove that under Assumption 1, $\\mathrm{H}_{0}$ is equivalent to the existence of $(Y^{\\prime},\\psi^{\\prime})$ such that $Y^{\\prime}\\sim Y$, $\\psi^{\\prime}\\sim\\psi$, and $\\mathbb{E}[Y^{\\prime}\\mid\\psi^{\\prime}]=\\psi^{\\prime}$ (Lemma 1).",
    "gold_answer": "1. **Existence under $\\mathrm{H}_{0}$**: If $\\mathrm{H}_{0}$ holds, then by definition, there exists an information set $\\mathcal{T}$ such that $\\mathbb{E}[Y\\mid\\mathcal{T}]=\\psi$. Let $(Y^{\\prime},\\psi^{\\prime})=(Y,\\psi)$ and $\\mathcal{T}^{\\prime}=\\mathcal{T}$. Then $Y^{\\prime}\\sim Y$, $\\psi^{\\prime}\\sim\\psi$, and $\\mathbb{E}[Y^{\\prime}\\mid\\psi^{\\prime}]=\\psi^{\\prime}$ by the law of iterated expectations.  \n2. **Implication of existence**: Conversely, if such $(Y^{\\prime},\\psi^{\\prime})$ exists, let $\\mathcal{T}^{\\prime}=\\sigma(\\psi^{\\prime})$. Then $\\mathbb{E}[Y^{\\prime}\\mid\\mathcal{T}^{\\prime}]=\\psi^{\\prime}$, satisfying $\\mathrm{H}_{0}$.",
    "question_context": "Let $F_{\\psi}$ and $F_{Y}$ denote the cumulative distribution functions (cdf) of $\\psi$ and $Y$, $x^{+}=\\operatorname*{max}(0,x)$, and define $$\\Delta(y)=\\int_{-\\infty}^{y}F_{Y}(t)-F_{\\psi}(t)d t.$$\nAssumption 1. $\\mathbb{E}(|Y|)<\\infty$ and $\\mathbb{E}(|\\boldsymbol{\\psi}|)<\\infty.$\nLemma 1. Suppose that Assumption 1 holds. Then $\\mathrm{{H}_{0}}$ holds if and only if there exists $a$ pair of random variables $(Y^{\\prime},\\psi^{\\prime})$ such that $Y^{\\prime}\\sim Y$, $\\psi^{\\prime}\\sim\\psi$, and $\\mathbb{E}\\left[Y^{\\prime}\\mid\\psi^{\\prime}\\right]=\\psi^{\\prime}$.\nTheorem 1. Suppose that Assumption 1 holds. The following statements are equivalent: (i) $\\mathrm{{H}_{0}}$ holds; (ii) $\\Delta(y)\\ge0$ for all $y\\in\\mathbb{R}$ and $\\mathbb{E}[Y]=\\mathbb{E}[\\psi]$; (iii) $\\mathbb{E}[(y-Y)^{+}-(y-\\psi)^{+}]\\geq0$ for all $y\\in\\mathbb{R}$ and $\\mathbb{E}[Y]=\\mathbb{E}[\\psi]$.\n\nThis section establishes the main equivalence result for testing rational expectations (RE) through moment inequalities and equalities involving the cumulative distribution functions (cdf) of realized outcomes $(Y)$ and subjective beliefs $(\\psi)$."
  },
  {
    "qid": "econ-empirical-947-4-0-1",
    "question": "2) Prove Lemma 6, showing that the variance of continuation values $W(\\Delta x, z)$ is at most proportional to $\\Delta$ for any Bellman operator applied to a strictly concave function $F$.",
    "gold_answer": "To prove Lemma 6:\n1. Consider a strictly concave function $F$ with $F^{\\prime\\prime} < 0$.\n2. For any Bellman operator $X$ (e.g., $T_{I}^{\\Delta}$, $T_{I}^{\\Delta,c}$, $T^{\\Delta,q}$), the policy $(a,c,W)$ is $\\Delta$-suboptimal.\n3. The variance $\\mathbb{V}^{\\Delta}[W(\\Delta x, z)]$ is bounded by $V\\Delta$, where $V$ depends on $F$.\n4. This follows from the strict concavity of $F$ and the efficiency loss being bounded by per-period gains of order $\\Delta$.",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nThe proof of Proposition 6 is established by a series of lemmas that relate values of Bellman operators applied to a function $F$ solving HJB equation (22), as well as their policy functions.\nLemma 6: Let $\\boldsymbol{I}=[\\underline{{\\boldsymbol{w}}},\\bar{\\boldsymbol{w}}]$ and $F:I\\rightarrow\\mathbb{R}$ be twice continuously differentiable with $F^{\\prime\\prime}<0.$ Let $X$ be any of the Bellman operators $T_{I}^{\\varDelta}$ ， $T_{I}^{\\varDelta,c}$ , and $T^{\\Delta,q}$ . Suppose that the policy $(a,c,W)$ .s $\\Delta$ suboptimal for the problem $X F(w)$ , with $w\\le e^{-r\\Delta}\\bar{w}$ Then, for some $V$ that depends on $F$ only, we have $\\mathbb{V}^{\\boldsymbol{\\Delta}}[W(\\boldsymbol{\\Delta}[\\boldsymbol{x}+\\boldsymbol{a}(\\boldsymbol{z})],\\boldsymbol{z})]\\le V\\boldsymbol{\\Delta}$\nLemma 7: For any $\\varepsilon>0$ and $({\\bar{a}},{\\bar{h}})$ , there exists $(\\tilde{a},\\tilde{h})$ such that $|\\bar{\\boldsymbol{a}}-\\tilde{\\boldsymbol{a}}|=$ ${\\cal O}(\\varepsilon),|\\bar{h}-\\tilde{h}|={\\cal O}(\\varepsilon)$ and $\\Theta^{\\varepsilon}(\\tilde{a},\\tilde{h})\\leq\\Theta(\\bar{a},\\bar{h})+O(\\varepsilon)\\times\\sqrt{\\theta(\\bar{a},\\bar{h})}+O\\big(\\varepsilon^{2}\\big).$\nLemma 8: Consider any twice differentiable function $F:I\\rightarrow\\infty$ with $F^{\\prime\\prime}<0$ Then, $T^{\\Delta,q}F(w)$ equals $e^{-r A}F(w)-\\tilde{r}A\\operatorname*{sup}\\biggr\\{(\\bar{a}-c)+F^{\\prime}(w)\\bigl[w+\\bar{h}-u(c)\\bigr]}$ ${+e^{r A}\\frac{\\tilde{r}}{2}F^{\\prime\\prime}(w)\\Theta(\\bar{a},\\bar{h})\\biggr\\}+{\\cal O}\\bigl(\\varDelta^{2}\\bigr).$ Moreover, for fixed $\\varepsilon>0,w\\in I$ and $\\pmb{\\varDelta}>\\pmb{0}$ , a quadratic simple policy is feasible and $O(\\varepsilon\\Delta)$ -suboptimal for $T^{\\Delta,q}F(w)$.\nLemma 9: Let $F$ solve the HJB equation (22) on an interval $I$ with $F^{\\prime\\prime}<0$ Then $|T^{\\Delta,q}F-F|_{I}=o(\\varDelta)$ Moreover, for any $\\varepsilon$ ， $\\varDelta>0$ ， $w\\in I$ , and a corresponding quadratic simple policy $(\\boldsymbol{a}_{q},\\boldsymbol{c}_{q},W_{q})$ ， $\\phi^{\\Delta,q}(a_{q},c_{q},W_{q};F,w)\\ge F(w)-O(\\varDelta\\varepsilon),$ uniformly in $w\\in I$.\nLemma 12: Let $F:I\\rightarrow\\mathbb{R}$ be twice continuously differentiable with $F^{\\prime\\prime}<0$ Then $|T_{I}^{\\varDelta,c}F-T^{\\varDelta,q}F|_{I^{\\varDelta}}=o(\\varDelta)$ Moreover, for fixed $\\varepsilon>0$ $\\varDelta>0$ ,and $w\\in I^{\\varDelta}$ ， consider a quadratic simple policy $(a_{q},c_{q},W_{q})$ for $T^{\\Delta,q}F(w)$ If $\\Delta$ and $\\pmb{\\varepsilon}$ are sufficiently small, the corresponding simple policy $(a,c,W)$ satisfies the (IC) constraint and $\\phi^{A}({a},{c},{W};{F})\\ge\\phi^{A,q}({a}_{{q}},{c}_{{q}},{W}_{{q}};{F},{w})-O({\\varepsilon}{\\varDelta})$ uniformly in $w\\in I^{\\varDelta}$.\nLemma 14: Assume $F:I\\rightarrow\\mathbb{R}$ is twice continuously differentiable and $F^{\\prime\\prime}<0$ Then $|T_{I}^{\\varDelta,c}F-T_{I}^{\\varDelta}F|_{I^{\\varDelta}}=o(\\varDelta)$.\n\nThis section provides a detailed proof of Proposition 6, involving various Bellman operators and their properties. The proof leverages several lemmas to establish connections between different operators and their policy functions."
  },
  {
    "qid": "econ-empirical-464-3-0-2",
    "question": "3) Show how the estimators $\\widehat{\\theta}_{d}$ for $d = (MD, SMD, RS, LT, SLT)$ can be represented using the expansion terms $A_{d}(\\theta_{0})$ and $C_{d}(\\theta_{0})$, and explain the role of the prior in the bias.",
    "gold_answer": "1. The general representation is:\n   $$\n   \\widehat{\\theta}_{d} = \\theta_{0} + \\frac{A_{d}(\\theta_{0})}{\\sqrt{T}} + \\frac{C_{d}(\\theta_{0})}{T} + \\frac{\\mathbb{1}_{d\\in D}}{T}\\left[\\frac{\\pi_{\\theta}(\\theta_{0})}{\\pi(\\theta_{0})}C_{d}^{P}(\\theta_{0}) + C_{d}^{M}(\\theta_{0})\\right] + o_{p}\\left(\\frac{1}{T}\\right)\n   $$\n   where $D = (RS, LT, SLT)$.\n\n2. The term $C_{d}^{P}(\\theta_{0})$ captures bias due to the prior:\n   $$\n   C_{d}^{P}(\\theta_{0}) = \\frac{1}{B}\\sum_{b=1}^{B}\\left(A_{d}^{b}(\\theta_{0}) - A_{d}(\\theta_{0})\\right)A_{d}^{b}(\\theta_{0})\n   $$\n   This term arises because the prior acts as a penalty on the objective function.\n\n3. The term $C_{d}^{M}(\\theta_{0})$ accounts for the approximation of the mean by the mode and depends on the curvature of the binding function and its interaction with the prior.",
    "question_context": "As shown in Appendix B, each draw $\\theta_{L T}^{b}$ has expansion terms \n\n$$\n\\begin{array}{l}{{\\displaystyle{A_{L T}^{b}(\\theta_{0})=\\left[\\psi_{\\theta}(\\theta_{0})\\right]^{-1}\\left(\\mathbb{A}(\\theta_{0})-\\mathbb{A}_{\\infty}^{b}(\\theta_{0})\\right)}}}\\\\ {{\\displaystyle{C_{L T}^{b}(\\theta_{0})=\\left[\\psi_{\\theta}(\\theta_{0})\\right]^{-1}\\left(\\mathbb{C}(\\theta_{0})-\\frac{1}{2}\\sum_{j=1}^{K}\\psi_{\\theta,\\theta_{j}}(\\theta_{0})A_{L T}^{b}(\\theta_{0})A_{L T,j}^{b}(\\theta_{0})-\\mathbb{A}_{\\infty,\\theta}^{b}(\\theta_{0})A_{L T}^{b}(\\theta_{0})\\right).}}}\\end{array}\n$$\nA similar stochastic expansion of each $\\theta_{S L T}^{b}$ gives:\n\n$$\n\\begin{array}{l}{{\\displaystyle{\\cal A}_{S L T}^{b}(\\theta_{0})=\\left[\\psi_{\\theta}(\\theta_{0})\\right]^{-1}\\left(\\Delta(\\theta_{0})-\\displaystyle{\\frac{1}{S}\\sum_{s=1}^{S}\\Delta^{s}(\\theta_{0})}-\\Delta_{\\infty}^{b}(\\theta_{0})\\right)}}\\\\ {{\\displaystyle~C_{S L T}^{b}(\\theta_{0})=\\left[\\psi_{\\theta}(\\theta_{0})\\right]^{-1}\\left(\\mathbb{C}(\\theta_{0})-\\displaystyle{\\frac{1}{S}\\sum_{s=1}^{S}\\mathbb{C}^{s}(\\theta_{0})}-\\displaystyle{\\frac{1}{2}\\sum_{j=1}^{K}\\psi_{\\theta,\\theta_{j}}(\\theta_{0})\\cal A}_{S L T}^{b}\\cal A_{S L T,j}^{b}\\right)}~}\\\\ {{\\displaystyle~-\\left[\\psi_{\\theta}(\\theta_{0})\\right]^{-1}\\left(\\displaystyle{\\frac{1}{S}\\sum_{s=1}^{S}\\left(\\Delta_{\\theta}^{s}(\\theta_{0})+\\mathbb{A}_{\\infty,\\theta}^{b}(\\theta_{0})\\right)\\cal A}_{S L T}^{b}(\\theta_{0})\\right)}.}\\end{array}\n$$\n\nThe mode of $\\exp(-J(\\theta))\\pi(\\theta)$ inherits the properties of a MD estimator. However, the quasi-posterior mean has two additional sources of bias, one arising from the prior, and another one from approximating the mode by the mean. The optimization view of $\\overline{{\\theta}}_{L T}$ facilitates an understanding of these effects."
  },
  {
    "qid": "econ-empirical-1627-1-0-3",
    "question": "4) Analyze the asymptotic expansion of the estimator $\\hat{\\theta}$ in Theorem 1. What are the sources of bias and variance, and how do the rates $R$ and $N$ affect the asymptotic distribution?",
    "gold_answer": "1. The expansion is:\n   $$ \\sqrt{T}(\\hat{\\theta} - \\theta_{0}) = \\left( (\\Gamma^{\\prime} W \\Gamma)^{-1} \\Gamma^{\\prime} W + o_{p}(1) \\right) \\left( Q_{1T} + \\frac{1}{\\sqrt{R}} Q_{2T} + \\frac{1}{\\sqrt{N}} Q_{3T} + \\frac{\\sqrt{T}}{R} C_{1T} + \\frac{\\sqrt{T}}{N} C_{2T} \\right). $$\n2. Sources of bias:\n   - $\\frac{\\sqrt{T}}{R} C_{1T}$ from simulation error in integrals.\n   - $\\frac{\\sqrt{T}}{N} C_{2T}$ from sampling error in market shares.\n3. Sources of variance:\n   - $\\frac{1}{\\sqrt{R}} Q_{2T}$ from simulation noise.\n   - $\\frac{1}{\\sqrt{N}} Q_{3T}$ from sampling noise.\n4. If $R$ grows slower than $T$, bias dominates. If $R$ grows faster, variance dominates. Similar logic applies to $N$.",
    "question_context": "Let $p_{j t}\\in\\mathbb{R}$ be the price of product $j$ in market t. Define $x_{j t}={}$ $\\left(p_{j t},x_{j t}^{(2)}\\right)\\in\\mathbb{R}^{d_{x}}$ ∈ Rdx , where xj(t2) a re observed product characteristics other than price and may include a constant or product dummies. Let $\\xi_{j t}~\\in~\\mathbb{R}$ be a product characteristic of product $j$ in market t, which is observed by firms and consumers, but not by the econometrician. In the BLP model consumer $i$ in market $t$ chooses the product $j$ which maximizes the utility \n\n$$\nu_{i j t}=x_{j t}^{\\prime}\\beta_{i t}+\\xi_{j t}+\\epsilon_{i j t},\n$$ \n\nwhere $\\beta_{i t}~=~\\beta_{0}+\\Sigma_{0}v_{i t},\\beta_{0}~\\in~\\mathbb{R}^{d_{\\boldsymbol{x}}},\\Sigma_{0}$ is a $d_{x}\\times d_{x}$ matrix, and $v_{i t}~\\in~R^{d_{x}}$ is a random vector with distribution function $P_{0t}$ . Assuming that $\\epsilon_{i j t}$ are independent and identically distributed (iid) extreme value random variables, the market share of product $j$ in market $t$ is given by \n\n$$\ns_{j t}=\\int\\frac{\\exp(x_{j t}^{\\prime}\\beta_{0}+\\xi_{j t}+x_{j t}^{\\prime}\\Sigma_{0}\\upsilon)}{1+\\displaystyle\\sum_{k=1}^{J_{t}}\\exp(x_{k t}^{\\prime}\\beta_{0}+\\xi_{k t}+x_{k t}^{\\prime}\\Sigma_{0}\\upsilon)}d P_{0t}(\\v\\upsilon).\n$$\nBerry (1994) shows that for any $\\theta\\in\\Theta,x_{t}\\in\\mathbb{R}^{d_{x}\\times J_{t}}$ , distribution $P$ , and $s_{j t}\\in(0,1)$ there exists $\\xi_{t}\\left(\\theta,P,s_{t},x_{t}\\right)\\in\\mathbb{R}^{J_{t}}$ such that for all $j$ \n\n$$\ns_{j t}=\\int\\nu_{j t}\\left(\\theta,x_{t},\\xi_{t}\\left(\\theta,P,s_{t},x_{t}\\right),\\upsilon\\right)d P(\\upsilon).\n$$ \n\nIn particular, one can solve for $\\xi_{t}$ in the system $\\mathsf{o f}J_{t}$ equations given in (2) or, in other words, $\\sigma_{t}(\\theta,x_{t},\\cdot,P_{t})$ is invertible. I denote the inverse function by $\\sigma_{t}^{-1}(\\theta,x_{t},\\cdot,P_{t})$ .\nNow assume that there are instruments $z_{t}\\in\\mathbb{R}^{J_{t}\\times d_{z}}$ such that \n\n$$\nE\\left(z_{t}^{\\prime}\\xi_{t}\\left(\\theta_{0},P_{0t},s_{t},x_{t}\\right)\\right)=0\n$$ \n\nand that these moment conditions identify $\\theta_{0}$ . Then an estimator of $\\theta_{0}$ is \n\n$$\n\\begin{array}{c}{{\\widetilde{\\theta}\\equiv\\displaystyle\\arg\\operatorname*{min}_{\\theta\\in\\Theta}\\left(\\frac{1}{T}\\sum_{t=1}^{T}z_{t}^{\\prime}\\xi_{t}\\left(\\theta,P_{0t},s_{t},x_{t}\\right)\\right)^{\\prime}W_{T}}}\\\\ {{\\times\\left(\\frac{1}{T}\\sum_{t=1}^{T}z_{t}^{\\prime}\\xi_{t}\\left(\\theta,P_{0t},s_{t},x_{t}\\right)\\right),}}\\end{array}\n$$ \n\nwhere $W_{T}$ is a weighting matrix which converges in probability to a positive definite matrix $W$ .\nTheorem 1. Assume that Assumptions RC1–RC9 hold. Then there exist $d_{z}\\times d_{z}$ matrices $\\phi_{1},\\phi_{2}$ , and $\\varPhi_{3}$ , as well $d_{z}\\times1$ vectors $\\bar{\\mu}_{1}$ and ${\\bar{\\mu}}_{2}$ such that \n\n$$\n\\begin{array}{r l}&{\\sqrt{T}\\left(\\hat{\\theta}-\\theta_{0}\\right)=\\left(\\left(\\boldsymbol{r^{\\prime}}\\boldsymbol{W}\\boldsymbol{{T}}\\right)^{-1}\\boldsymbol{r^{\\prime}}\\boldsymbol{W}+o_{p}(1)\\right)}\\\\ &{\\qquad\\times\\left(\\boldsymbol{Q}_{1T}+\\frac{1}{\\sqrt{R}}\\boldsymbol{Q}_{2T}+\\frac{1}{\\sqrt{N}}\\boldsymbol{Q}_{3T}+\\frac{\\sqrt{T}}{R}\\boldsymbol{C}_{1T}\\right.}\\\\ &{\\qquad\\left.+\\frac{\\sqrt{T}}{N}\\boldsymbol{C}_{2T}+o_{p}\\bigg(\\frac{\\sqrt{T}}{R}\\bigg)+o_{p}\\bigg(\\frac{\\sqrt{T}}{N}\\bigg)\\right)}\\end{array}\n$$ \n\nwhere \n\nand \n\n$$\nC_{1T}\\stackrel{p}{\\rightarrow}\\bar{\\mu}_{1},\\quad C_{2T}\\stackrel{p}{\\rightarrow}\\bar{\\mu}_{2}.\n$$ \n\nFurthermore, $Q_{1T},Q_{2T}$ , and $Q_{3T}$ are asymptotically independent.\n\nThe random coefficient logit model is a fundamental tool in empirical industrial organization, allowing for heterogeneous consumer preferences through random coefficients on product characteristics. This section details the model's assumptions, asymptotic properties, and estimation framework."
  },
  {
    "qid": "econ-empirical-728-1-3-0",
    "question": "6) Compare the three scenarios (S1-S3) in terms of their implications for the NSM algorithm's performance.",
    "gold_answer": "1. **S1 (Aligned changes)**: All changes are in the same direction ($c$), making the signal stronger and easier to detect.\n2. **S2 (Random-signed changes, undirected)**: Changes are symmetric but random in sign, increasing variability in the projection direction estimates.\n3. **S3 (Random-signed changes, directed)**: Changes are asymmetric and random, further increasing complexity and detection difficulty.\n4. The algorithm's performance is best in S1 and more challenging in S2 and S3 due to increased noise and variability.",
    "question_context": "We consider the following three different scenarios of how changes are developed. (S1) Undirected graph with aligned changes: $\\mu_{i j}^{(2)}-\\mu_{i j}^{(1)}=c$ for all $\\upnu_{i},\\upnu_{j}\\in\\mathsf{V}_{0}$ and $i\\neq j$ . (S2) Undirected graph with random-signed changes: $\\mu_{i j}^{(2)}\\mathrm{~-~}$ $\\mu_{i j}^{(1)}=\\mu_{j i}^{(2)}-\\mu_{j i}^{(1)}\\sim\\mathrm{Unif}\\{c,-c\\}$ with equal probability independently for all $\\upnu_{i},\\upnu_{j}\\in\\mathsf{V}_{0}$ and $i<j$ . (S3) Directed graph with random-signed changes: $\\mu_{i j}^{(2)}~-$ $\\mu_{i j}^{(1)} \\sim \\mathrm{Unif}\\{c, -c\\}$ with equal probability independently for all $\\upnu_{i},\\upnu_{j}\\in\\mathsf{V}_{0}$ and $i\\neq j$ .\nWe suggest using $\\lambda_{t}~=~\\sqrt{2\\log(t^{2}p/\\alpha)}$ in practice and, unless otherwise specified, we will stick to this choice throughout our simulations.\n\nThe numerical performance of the NSM algorithm is evaluated using simulated dynamic networks with vertex-initiated changes. Three scenarios (S1-S3) are considered, and the choice of tuning parameters is discussed."
  },
  {
    "qid": "econ-empirical-1154-3-0-3",
    "question": "4) Describe the bootstrapping procedure used to estimate the standard errors for S(I), and explain why asymptotic standard errors based on individual densities are unreliable.",
    "gold_answer": "1. The bootstrapping procedure involves:  \n   a. Drawing a random sample (b) with replacement to replicate overall sample sizes, allowing state-year sample sizes to vary.  \n   b. Calculating density estimates for each bootstrap sample.  \n   c. Repeating the process B times to generate a distribution of estimates.  \n2. Asymptotic standard errors are unreliable because:  \n   a. The four densities in S(I) are interdependent, violating the independence assumption required for asymptotic methods.  \n   b. Kernel density estimates are inherently biased, particularly for small sample sizes or rapid changes in frequency.  \n   c. The bootstrap accounts for sampling variation and dependencies, providing more accurate confidence intervals.",
    "question_context": "C(I) is a weighted average of the estimated changes in densities over four groups: C(I)=α1{in(I)−nn(I)}+α2{in(I)−ni(I)}+α3{ii(I)−nn(I)}+α4{ii(I)−ni(I)}, where the first term corresponds to a change in densities estimated from uncontaminated treatment and control groups, the second term corresponds to an estimate with a contaminated control group only, the third term corresponds to an estimate with a contaminated treatment group only, and the fourth term corresponds to an estimate with a contaminated treatment and control group; the αk (which sum to 1) are the probabilities that the estimate comes from each of these groups.\nL(I) can be written as L(I)=β1{ni(I)−nn(I)}+β2{ni(I)−in(I)}+β3{ii(I)−nn(I)}+β4{ii(I)−in(I)}.\nThe assumption embodied in Equation 5 is that the effect of two successive minimum-wage increases (relative to no increases for two years) is equal to the sum of a pure contemporaneous and a pure lagged increase. In a regression context, this is equivalent to the assumption that there is not an interaction between contemporaneous and lagged effects.\nThe statistics of interest are all in the form of linear functions of density functions. The density estimates depend on both the households sampled within each state and the minimum-wage histories determined by the state of residence.\nThe statistics that we focus on are all extensions of difference-in-difference estimators like S(I)={f2,MW=1(I)−f1,MW=1(I)}−{f2,MW=0(I)−f1,MW=0(I)}.\nThe bootstrap procedure is designed to account for sampling variation in incomes and exposure to minimum-wage changes. A random sample (b) is drawn from the data with replacement in order to replicate the realized overall sample sizes in every year, but allowing state-year sample sizes to vary.\n\nThe text discusses a procedure to estimate changes in density functions by correcting for contaminated treatment and control groups, defining terms like C(I) and L(I) to represent contemporaneous and lagged effects."
  },
  {
    "qid": "econ-empirical-1288-0-0-3",
    "question": "4) Discuss the empirical implications of the paper's finding that college stratification was complete by 1960, using the Belley and Lochner (2007) evidence. How does this affect the interpretation of later trends in college attendance?",
    "gold_answer": "1. **Stratification Completion**: By 1960, the sorting patterns had stabilized, meaning the correlation between ability and college attendance reached its peak.\n2. **Later Trends**: Post-1960 trends, such as increased parental investment in children's education (Ramey and Ramey 2010), may reflect adjustments on other margins rather than further stratification.\n3. **Interpretation**: The reversal in later cohorts (Belley and Lochner 2007) suggests that other factors, such as federal aid, began to dominate the stratification dynamics.",
    "question_context": "Our theory includes a stylized model of labor markets to focus attention on changes in the nature of college. Enriching the model of labor markets is a promising avenue for future work. Doing so would make it possible to quantify the effect of changing college attendance patterns on the distribution of wages and income.\nOur analysis stops in 1960 for two reasons. First, the federal government introduced and expanded college grant and loan programs after this time, rendering our assumption of self-financing college unpalatable. Second, the reversal in sorting patterns appears to be complete by this time, with Belley and Lochner (2007) showing that the trend even reversed for later cohorts.\n\nThis paper documents large changes in the patterns of college attendance in the United States during the twentieth century. Prior to World War II, family income and socioeconomic status were more important predictors of who attended college, whereas academic ability was more important afterward. The paper provides a quantitative theory of these changes, focusing on rising demand for college and the introduction of standardized test scores, which led to increasing college stratification."
  },
  {
    "qid": "econ-empirical-177-1-0-3",
    "question": "4) Prove that $\\frac{d a_{p}}{d\\beta} < 0$ and $\\frac{d e_{p}}{d\\beta} > 0$ in equilibrium under authority, and interpret these results.",
    "gold_answer": "1. From the principal’s FOC: $(1-\\beta)x_{1}(a_{p},e_{p}) = y_{1}(1-a_{p},e_{p})$.\n2. Totally differentiate with respect to $\\beta$: $-x_{1} da_{p} + (1-\\beta)x_{11} da_{p} + (1-\\beta)x_{12} de_{p} = y_{11} (-da_{p}) + y_{12} de_{p}$.\n3. Rearrange: $\\frac{da_{p}}{d\\beta} = \\frac{x_{1}}{(1-\\beta)x_{11} + y_{11}} + \\frac{y_{12} - (1-\\beta)x_{12}}{(1-\\beta)x_{11} + y_{11}} \\frac{de_{p}}{d\\beta}$.\n4. From the agent’s FOC: $\\beta x_{2}(a_{p},e_{p}) = C^{\\prime}(e_{p})$.\n5. Totally differentiate: $x_{2} d\\beta + \\beta x_{12} da_{p} + \\beta x_{22} de_{p} = C^{\\prime\\prime} de_{p}$.\n6. Solve for $\\frac{de_{p}}{d\\beta} = \\frac{x_{2}}{C^{\\prime\\prime} - \\beta x_{22}} + \\frac{\\beta x_{12}}{C^{\\prime\\prime} - \\beta x_{22}} \\frac{da_{p}}{d\\beta}$.\n7. Under standard assumptions ($x_{11}, y_{11} < 0$, $C^{\\prime\\prime} > 0$), $\\frac{da_{p}}{d\\beta} < 0$ and $\\frac{de_{p}}{d\\beta} > 0$.\n8. Interpretation: Higher $\\beta$ reduces asset support ($a_{p}$) but increases agent effort ($e_{p}$).",
    "question_context": "When assets are contractible, the agent chooses effort optimally given that contracted level of assets $a$, and chooses effort $e^{*}$ given by $\\beta x_{2}(a,e^{*})=C^{\\prime}(e^{*})$. The principal’s optimal $a^{*}$ solves $x(a^{\\ast},e^{\\ast})+y(a^{\\ast},e^{\\ast})-C(e^{\\ast})$, and is given by the solution to $x_{1}(a^{*},e^{*})-y_{1}(1-a^{*},e^{*})+\\frac{d e^{*}}{d a}(x_{2}(a^{*},e^{*})+y_{2}(1-a^{*},e^{*})-C^{\\prime}(e^{*}))=0$.\nWhen the agent is delegated control, she chooses $a$ and $e$ to maximize $\\beta x(a,e)-C(e)$, and the principal takes no action other than to choose the contract. The agent chooses $a=1$, and effort is given by $e_{d}$ where $\\beta x_{2}(1,e_{d})=C^{\\prime}(e_{d})$.\nWhen the principal allocates assets, he chooses $a_{p}$ to maximize $(1-\\beta)x(a_{p},\\tilde{e}_{p})+y(1-a_{p},\\tilde{e}_{p})$, and the agent chooses $e_{p}$ to maximize $\\beta x(\\tilde{a_{p}},e_{p})-C(e_{p})$. The principal’s choice is given by $(1-\\beta)x_{1}(a_{p},e_{p})=y_{1}(1-a_{p},e_{p})$, and the agent’s effort by $\\beta x_{2}(a_{p},e_{p})=C^{\\prime}(e_{p})$.\nOptimal incentive pay under authority is given by $\\beta_{p}^{*}=1+\\frac{y_{2}(1-a_{p},e_{p})}{x_{2}(a_{p},e_{p})}+(\\frac{x_{1}-y_{1}}{x_{2}})\\left(\\frac{d a_{p}}{d\\beta}\\right)\\left(\\frac{d e_{p}}{d\\beta}\\right)^{-1}$.\n\nThe article explores the implications of noncontractible authority, starting with a benchmark where assets are contractible. It then examines delegation and authority scenarios, highlighting distortions in asset allocations and their impact on incentive pay."
  },
  {
    "qid": "econ-empirical-772-1-0-3",
    "question": "4) Compare the results of this model to the 'bidding-for-firms' literature (e.g., Black and Hoyt, 1989), highlighting the role of unionisation.",
    "gold_answer": "1. **Bidding-for-Firms Literature:** Focuses on tax competition under exogenous location advantages (e.g., market size). Countries tax profits based on their relative advantages.\\n2. **This Model:** Introduces endogenous labour market distortions via unionisation. The unionised country's subsidy is not just compensating for location disadvantages but also reducing inefficiencies caused by union power.\\n3. **Key Difference:** In standard models, subsidies reflect location rents. Here, subsidies also correct distortions, leading to \\( s_u^* > (w_u - w_n)L_u \\).\\n4. **Implication:** Unionisation can reverse conventional results, as the unionised country may attract FDI despite higher wages.",
    "question_context": "We develop our main result in a model where a unionised and a non-unionised country form an integrated market and compete for the location of a single, multinational firm. We model a five-stage game where governments compete through location taxes or subsidies in the first stage, the union sets the wage in the second stage and the MNE chooses its location in the third stage. In the fourth stage the union may re-optimise its wage policy and in the fifth stage all firms choose output levels.\nOur main result is that if a unionised and a non-unionised country compete for FDI, the unionised country will attract the investment in equilibrium, even if it has no other location advantages. This occurs because the government of the unionised country will offer a location subsidy to the outside firm which more than compensates the investor for the higher wages caused by union power.\nThe fundamental argument behind this result is simple. In concentrated markets where firms set prices above marginal (wage) costs, unions exerting their market power to raise wages above their competitive levels aggravate the distortions in the economy. This gives the unionised country’s government a strong incentive to reduce the existing inefficiencies, but it cannot curtail the union’s wage-setting power directly. Hence, attracting FDI serves as a second-best instrument, giving the union an incentive to lower its wage demand, in exchange for higher employment in the multinational firm.\n\nThis article analyses tax competition between a unionised and a non-unionised country for the location of an outside firm. It shows that unionisation increases the incentive for the government to attract a foreign investor, in order to affect the behaviour of the domestic union. This results in the unionised country’s government offering a tax discount (or a subsidy premium) to the outside firm in excess of what is needed to compensate the investor for the higher union wage. In equilibrium, therefore, the unionised country attracts the foreign investment, even if it has no other location advantages."
  },
  {
    "qid": "econ-empirical-1108-5-2-0",
    "question": "5) Propose a latent factor model for the six ASVAB tests, and derive the likelihood function assuming normality.",
    "gold_answer": "1) Let \\( y_{ij} = \\lambda_j \\theta_i + \\epsilon_{ij} \\), where \\( \\theta_i \\sim N(0,1) \\) and \\( \\epsilon_{ij} \\sim N(0, \\sigma_j^2) \\). \n2) The likelihood is \\( L = \\prod_{i=1}^N \\prod_{j=1}^6 \\phi\\left(\\frac{y_{ij} - \\lambda_j \\theta_i}{\\sigma_j}\\right) \\), where \\( \\phi \\) is the normal PDF.",
    "question_context": "The NLSY79 contains the Armed Services Vocational Aptitude Battery (ASVAB), which consists of ten tests that were developed by the military to predict performance in the armed forces training programs. The battery involves achievement tests designed to measure knowledge of general science, arithmetic reasoning, word knowledge, paragraph comprehension, numerical operations, coding speed, auto and shop information, mathematics knowledge, mechanical comprehension, and electronics information. This paper uses six of these ten tests: word knowledge, paragraph comprehension, mathematics knowledge, arithmetic reasoning, and coding speed. The two attitude scales utilized as measures of noncognitive abilities are the Rosenberg Self-Esteem and Rotter Locus of Control scales.\n\nThe NLSY79 contains the Armed Services Vocational Aptitude Battery (ASVAB), which consists of ten tests that were developed by the military to predict performance in the armed forces training programs. The battery involves achievement tests designed to measure knowledge of general science, arithmetic reasoning, word knowledge, paragraph comprehension, numerical operations, coding speed, auto and shop information, mathematics knowledge, mechanical comprehension, and electronics information. This paper uses six of these ten tests: word knowledge, paragraph comprehension, mathematics knowledge, arithmetic reasoning, and coding speed. The two attitude scales utilized as measures of noncognitive abilities are the Rosenberg Self-Esteem and Rotter Locus of Control scales."
  },
  {
    "qid": "econ-empirical-25-2-2-0",
    "question": "5) Specify the linear probability model used to estimate the likelihood of being credit-constrained, including all control variables and their theoretical justification.",
    "gold_answer": "1. **Model**: \\( P(\\text{Credit Constrained}) = \\beta_0 + \\beta_1 \\text{Income} + \\beta_2 \\text{Wealth} + \\beta_3 \\text{Age} + \\beta_4 \\text{Education} + \\beta_5 \\text{Bankruptcies} + \\beta_6 \\text{Nonperforming Loans} + \\sum \\gamma_r \\text{Region}_r + \\epsilon \\). \\n2. **Controls**: Income/wealth (creditworthiness), age/education (risk profile), bankruptcies/nonperforming loans (local risk), region (financial development).",
    "question_context": "We estimate a linear probability model of the likelihood a household is shut off from the credit market. Each year we classify a household as shut off if it reports it has been rejected for a loan application or discouraged from applying that year.\nThe normalized measure is defined as 1 − Regional effect/max {Regional effect} and is thus equal to zero in the region with the maximum value of the coefficient on the regional dummy.\n\nThe text presents empirical results on regional financial development in Italy, using a linear probability model to estimate credit constraints and a normalized measure of financial development."
  },
  {
    "qid": "econ-empirical-821-2-3-0",
    "question": "7) Derive the first-order condition for the manufacturer's wholesale price choice under symmetric beliefs (equation (5)).",
    "gold_answer": "The manufacturer's problem is:\n\n1. Maximize $\\pi_{i} = D^{i}[\\hat{p}(w_{i}), \\hat{p}(w^{*})]w_{i} + D^{i}[\\hat{p}(w_{i}), \\hat{p}(w_{i})][\\hat{p}(w_{i}) - w_{i}]$.\n2. Differentiate with respect to $w_{i}$:\n   $$\\frac{\\partial \\pi_{i}}{\\partial w_{i}} = \\frac{\\partial D^{i}}{\\partial p_{i}} \\frac{\\partial \\hat{p}}{\\partial w_{i}} w_{i} + D^{i} + \\frac{\\partial D^{i}}{\\partial p_{j}} \\frac{\\partial \\hat{p}}{\\partial w_{i}} [\\hat{p} - w_{i}] - D^{i} = 0.$$\n3. Simplify to obtain (5).",
    "question_context": "With symmetric beliefs, however, this incentive is weakened because, if a manufacturer reduces the wholesale price, his retailer conjectures that the other manufacturer is doing the same.\nLemma 3. With symmetric beliefs, if both manufacturers choose vertical separation, in period 2 they offer the wholesale price $w^{*}\\in\\underset{w_{i}}{\\arg\\operatorname*{max}}\\{D^{i}[\\hat{\\boldsymbol{\\jmath}}(w_{i}),\\hat{\\boldsymbol{\\jmath}}(w^{*})]w_{i}+D^{i}[\\hat{\\boldsymbol{\\jmath}}(w_{i}),\\hat{\\boldsymbol{\\jmath}}(w_{i})][\\hat{\\boldsymbol{\\jmath}}(w_{i})-w_{i}]\\}$.\n\nUnder symmetric beliefs, retailers believe competitors receive identical contracts, leading to non-zero wholesale prices."
  },
  {
    "qid": "econ-empirical-1220-4-1-1",
    "question": "2) Using the coefficients for 'Mother' and 'NumChild' in Table 8, derive the relative risk ratio for nursing home transitions between mothers and fathers in cohabitation. How does family size moderate this ratio?",
    "gold_answer": "The relative risk ratio (RRR) is:\n\\[ \\text{RRR} = \\frac{\\lambda_{\\text{Mother}}}{\\lambda_{\\text{Father}}} = \\exp(\\beta_{\\text{Mother}} + \\beta_{\\text{NumChild}} \\cdot \\text{NumChild}) \\]\n\n**Step 1**: From Table 8, \\( \\beta_{\\text{Mother}} = -1.01 \\) (base) or \\( -0.92 \\) (mixed).\n**Step 2**: \\( \\beta_{\\text{NumChild}} = -0.05 \\), so each child reduces RRR by \\( \\exp(-0.05) \\approx 0.95 \\).\n**Step 3**: For a mother with 3 children, RRR \\( \\approx \\exp(-1.01 - 0.15) = 0.33 \\).",
    "question_context": "Individuals are much less likely to return to independent living as the length of their cohabiting stay increases. This result suggests that independent living is a very unlikely exit route for individuals who cohabit, especially as time goes by.\nSick elderly mothers who cohabit (relative to sick elderly fathers who cohabit) are less at risk of transiting into nursing-home care and less at risk of dying. Thus, estimates indicate that durations in cohabitation are likely to be longer for women than for men.\nHaving more children decreases the cohabitation-into-independent-living transition intensity. This result likely reflects the ability of larger families to care for their sick elderly parents in a shared living arrangement.\nBeing married appears to reduce the transition intensity from cohabitation into nursing home (which is markedly different from our finding that married individuals are more likely to transit from independent living to nursing home). This suggests that spouses may be complimentary caregivers during cohabitation.\n\nThe study analyzes transitions from cohabitation to other states (nursing home, independent living, death), highlighting gender differences, family size effects, and health-related dynamics."
  },
  {
    "qid": "econ-empirical-1744-3-0-3",
    "question": "4) Prove that the principal's POU for an anonymous AND technology is unbounded as $\\gamma \\to 0$ or $n \\to \\infty$, using Lemma 6.4.",
    "gold_answer": "1. **Lemma 6.4**: For AND technologies, $POU_P = \\left(\\frac{\\delta}{\\gamma}\\right)^{n-1} + 1 - \\frac{\\gamma}{\\delta}$. \\n2. **Limit Analysis**: As $\\gamma \\to 0$, $\\frac{\\delta}{\\gamma} \\to \\infty$, making $POU_P \\to \\infty$. \\n3. **Large $n$**: For fixed $\\delta > \\gamma$, $\\left(\\frac{\\delta}{\\gamma}\\right)^{n-1} \\to \\infty$ as $n \\to \\infty$. \\n4. **Conclusion**: The POU is unbounded in these limits, illustrating extreme loss due to unobservability.",
    "question_context": "An anonymous technology success function $t$ exhibits (strict) under-proportional contribution $(U P C)$ if for every $k\\in\\{1,2,\\ldots,n-1\\}$ it holds that $$\\frac{k}{n}>\\frac{t_{k}-t_{0}}{t_{n}-t_{0}}.$$\nAn anonymous technology exhibits (strict) over-payment $(O P_{a n o n})$ if for every $k\\in\\{1,2,\\ldots, n-1\\}$, it holds that $$\\frac{Q_{k}}{Q_{n}}>\\frac{t_{k}-t_{0}}{t_{n}-t_{0}}.$$\nAn anonymous technology $(t,c)$ exhibits (strictly) increasing relative marginal payment $(I R M P_{a n o n})$ if for every $k\\in\\{1,2,\\ldots,n-1\\}$ it holds that $$\\frac{Q_{k+1}-Q_{k}}{t_{k+1}-t_{k}}>\\frac{Q_{k}-Q_{k-1}}{t_{k}-t_{k-1}}.$$\n\nThis section explores anonymous technologies, focusing on properties that determine the number of transitions in hidden-actions and observable-actions cases. Key properties include under-proportional contribution (UPC) and over-payment (OP) for single transitions, and decreasing returns to scale (DRS) and increasing relative marginal payment (IRMP) for multiple transitions."
  },
  {
    "qid": "econ-empirical-1499-5-0-0",
    "question": "1) Given the calibration parameters $\\eta=0.5$, $\\beta=0.99$, $\\delta=0.03$, $\\alpha=0.33$, $\\chi=0.056$, $\\xi=0.2$, and $\\kappa=0.01$, derive the equilibrium asset price $q_i = 1.05$ and explain how the annualized liquidity premium of 50 basis points is determined.",
    "gold_answer": "1. **Equilibrium Asset Price ($q_i$):**  \n   - The asset price is derived from the steady-state condition where the return on private assets equals the return on public liquidity.  \n   - Using the calibration, the Euler equation for asset pricing is:  \n     $$ q_i = \\frac{\\beta (\\alpha + (1 - \\delta) q_i)}{1 - \\beta (1 - \\delta)} $$  \n   - Substituting $\\beta = 0.99$, $\\alpha = 0.33$, and $\\delta = 0.03$ yields $q_i \\approx 1.05$.  \n\n2. **Liquidity Premium:**  \n   - The liquidity premium is the difference between the return on illiquid assets and the risk-free rate.  \n   - Given $\\kappa = 0.01$ and $\\xi = 0.2$, the premium is calculated as:  \n     $$ \\text{Premium} = \\kappa + \\xi (\\phi_2 - \\phi_1) $$  \n   - Substituting $\\phi_1 = 0.0685$ and $\\phi_2 = 0.2828$ gives a premium of 50 basis points.",
    "question_context": "Let $\\eta=0.5$ .Set $\\beta=0.99$ ， $\\delta=0.03$ , and $\\alpha=0.33$ as in a standard calibration for a quarterly macro model. Following Shi (2015) and interpreting $\\chi$ as the fraction of firms that has investment opportunities each quarter, we set $\\chi=0.056$ in line with Doms and Dunne (1998). Finally, set $\\xi=0.2$ and $\\kappa=0.01$ such that $q_{i}=1.05$ and the annualized liquidity premium amounts to 50 basis points in the equilibrium with the most liquid asset markets.\n$\\phi_{1}=0.0685\\mathrm{and}\\phi_{2}=0.2828$ solve (19) which is a quartic equation (see the online Appendix for details). That is, private financial markets are active, but exhibit different degrees of liquidity (or search intensity). The corresponding (real) value of money $B/P$ is around 88.64 percent lower in the equilibrium with more liquid and active asset markets. In other words, as the liquidity of private financial markets improves, agents value public liquidity substantially less.\nIn both equilibria $\\phi<\\eta$ , such that Proposition 2 implies that the equilibrium $q^{i}$ will be higher if $\\phi$ is higher. In fact, when the steady-state saleability increases from $\\phi_{1}$ to $\\phi_{2}$ , the asset price $q^{i}$ increases from 1.02 to 1.05, while the liquidity premium decreases by 5 basis points. Also note that $\\phi<0.5$ seems empirically plausible according to Del Negro et al. (2011), as otherwise all claims would have a turn-over rate of more than 50 percent within a quarter.\nBy affecting asset liquidity, participation decisions in the financial market can, thus, have a strong impact on firms’ financing constraints, capital accumulation, and output. In our numerical example, investment, consumption, and output increase by 7.02 percent, 3.06 percent, and 2.26 percent, respectively, as steady-state saleability switches from $\\phi_{1}$ to $\\phi_{2}$.\n\nThe following example highlights that coordination on financial markets strongly impacts asset liquidity and portfolio allocations between private and public financial assets, thus significantly affecting real economic activity."
  },
  {
    "qid": "econ-empirical-431-2-1-2",
    "question": "7) For the BEKK model with $d=9$, explain why the test statistic $\\hat{M}_{T}^{(2)}$ gains power in higher dimensions despite finite-sample bias in QMLE.",
    "gold_answer": "1. **Power gain**: The CUSUM statistic's order depends on $\\bar{d}=d(d+1)/2$ (Remark 2.1 in Aue et al. 2009). Higher $d$ amplifies signal-to-noise ratio for breaks. \n2. **QMLE bias**: Near-integrated processes in high dimensions may produce outliers, but simulations show the test remains robust with replications.",
    "question_context": "The simulations use bivariate observations $\\mathbf y_{t}=(y_{t}(1),y_{t}(2))^{\\top}$ with $\\pmb{\\Sigma}_{0}$ as the $2\\times2$ identity matrix. The Bartlett kernel and Newey–West optimal window are used for $\\hat{\\mathfrak{D}}_{T}$. The change-point estimator is $\\hat{t}_{T}=\\mathrm{argmax}\\{\\hat{\\mathfrak{s}}(t)-(t/T)\\hat{\\mathfrak{s}}(T),1\\leq t\\leq T\\}$.\nFor the BEKK model, coefficient matrices are set as $\\mathbf{A}_{1}=\\begin{bmatrix}a_{11} & a_{12}\\\\ a_{21} & a_{22}\\end{bmatrix}$, $\\mathbf{B}_{1}=\\begin{bmatrix}b_{11} & b_{12}\\\\ b_{21} & b_{22}\\end{bmatrix}$, with $a_{12}=a_{21}=b_{12}=b_{21}=0.001$. For the cDCC model, $\\theta_{2}=0.005$ or 0.01, $\\theta_{3}=0.9$ or 0.95.\n\nThis section evaluates the performance of the test statistics $\\hat{M}_{T}^{(1)}$ and $\\hat{M}_{T}^{(2)}$ under the BEKK and cDCC models via Monte Carlo simulations."
  },
  {
    "qid": "econ-empirical-71-3-0-0",
    "question": "1) Using the permanent income hypothesis framework, derive the conditions under which a household debt boom would be associated with higher future economic growth. Incorporate the role of expected income shocks and consumption smoothing.",
    "gold_answer": "1. **Model Setup**: Assume a household maximizes utility over two periods: \\(U(C_1) + \\beta E[U(C_2)]\\), subject to the budget constraint \\(C_1 + \\frac{C_2}{1+r} = Y_1 + \\frac{E[Y_2]}{1+r}\\).  \n2. **Optimal Borrowing**: If \\(E[Y_2]\\) increases, the household borrows more in period 1 (\\(D_1 = C_1 - Y_1\\)) to smooth consumption.  \n3. **Growth Implication**: Higher \\(D_1\\) reflects higher expected \\(Y_2\\), implying future growth \\(g = \\frac{Y_2 - Y_1}{Y_1}\\) is positive.  \n4. **Key Equation**: \\(\\frac{\\partial D_1}{\\partial E[Y_2]} = \\frac{1}{1+r} U''(C_1) > 0\\). Debt and growth are positively correlated under rational expectations.",
    "question_context": "Periods when household debt rises are associated with an increase in the consumption to GDP ratio. The rise in the consumption to GDP ratio is not only driven by durables: there is a rise in both the consumption of nondurables and services as well. In contrast, the investment to GDP ratio is flat during household debt booms.\nHousehold credit booms are negatively associated with changes in both the net export and current account to GDP ratio. A country increases its imports relative to exports as household debt rises.\nA natural reason for household debt to expand today is anticipation of higher income tomorrow, as in the standard permanent income hypothesis. Growth in debt in this type of model is driven by higher demand for credit in response to expected future income growth and a desire to smooth consumption.\nAn alternative rational expectations–based explanation for the rise in debt would be liquidity hoarding in the face of bad news. Households see a negative economic shock coming, and as a result they borrow aggressively to preserve liquidity and ride out the storm.\nAnother class of relevant models are behavioral or preference shock–based models in which households suddenly consume more. This could be due to a preference shock as in Laibson (1997) or Barro (1999), or general overoptimism about the future.\nA credit supply shock represents a relaxation of lending constraints. For the same potential borrower and same true risk profile, lenders become willing to lend more or on cheaper terms.\nIn 'amplification' models, an exogenous negative shock lowers growth, and the effects of such a negative shock are amplified by the presence of elevated debt. The negative shock may not be caused by the prior expansion in debt, but the expansion of debt amplifies the effect of the negative shock on subsequent growth.\n\nThe text examines the real-side effects of household debt booms, focusing on consumption, investment, and trade balance dynamics. It also contrasts credit demand and supply shocks as drivers of debt expansion and subsequent growth declines."
  },
  {
    "qid": "econ-empirical-56-2-0-1",
    "question": "2) Show how the implicit equation $\\underline{{\\rho}}(\\mu_{X})[1-\\ln(\\mu_{X}\\underline{{\\rho}}(\\mu_{X}))]=1$ is derived from the integral condition $\\mu_{X} = A(1 - \\ln A)$.",
    "gold_answer": "1. From the integral condition: $\\mu_{X} = \\int_{0}^{A} dx + \\int_{A}^{1} \\frac{A}{x} dx = A + A(\\ln 1 - \\ln A) = A(1 - \\ln A)$.  \n2. Substitute $A = \\mu_{X}\\underline{{\\rho}}(\\mu_{X})$ into the equation: $\\mu_{X} = \\mu_{X}\\underline{{\\rho}}(\\mu_{X})(1 - \\ln(\\mu_{X}\\underline{{\\rho}}(\\mu_{X})))$.  \n3. Divide both sides by $\\mu_{X}$ to obtain: $1 = \\underline{{\\rho}}(\\mu_{X})[1 - \\ln(\\mu_{X}\\underline{{\\rho}}(\\mu_{X}))]$.",
    "question_context": "The demand curve $\\underline{{\\Phi}}(x,\\mu_{X})$ is constructed so that the area of the inscribed rectangle, $x\\underline{{\\Phi}}(x,\\mu_{X})$, equals some constant $A$ over realizations of $X$. Rearranging, $\\underline{{\\Phi}}(x,\\mu_{X})=\\frac{A}{x}$. This is a globally unit-elastic demand curve.\nThe implicit solution for $\\underline{{\\rho}}(\\mu_{X})$ is given by $\\underline{{\\rho}}(\\mu_{X})[1-\\ln(\\mu_{X}\\underline{{\\rho}}(\\mu_{X}))]=1$.\n\nThe key step in the decomposition of $\\rho_{X}^{*}$ for a given demand curve is to find the worst case, that is, the demand curve solving the problem of minimizing producer surplus subject to having area underneath of $\\mu_{X}$. This involves constructing a symmetrically truncated Zipf (STRZ) demand curve."
  },
  {
    "qid": "econ-empirical-1833-2-0-3",
    "question": "4) Derive the condition under which workfare is optimal for the lowest ability type ($a_1 > 0$) and interpret its economic meaning.",
    "gold_answer": "The condition is: $$ \\pi_1 a_1 < \\delta\\left[\\sum_{j=2}^{n}\\pi_j + \\lambda\\right]. $$\\n\\n**Derivation**:\\n1. **Cost of workfare**: Left-hand side ($\\pi_1 a_1$) represents the lost private earnings (and increased transfers) for type 1.\\n2. **Benefit of workfare**: Right-hand side ($\\delta\\sum_{j=2}^{n}\\pi_j + \\lambda$) captures reduced transfers to higher types due to improved incentive compatibility.\\n\\n**Interpretation**:\\n- Workfare is optimal when the gains from deterring higher types from masquerading outweigh the earnings loss of the lowest type.\\n- The term $\\delta$ reflects the productivity difference between types, making workfare more attractive when higher types are sufficiently distinct.",
    "question_context": "The incentive compatibility constraints are given by: $$ x_{i}-h(y_{i}/a_{i}+c_{i})\\ge x_{j}-h(y_{j}/a_{i}+c_{j})\\quad\\mathrm{for.all}j\\ne i,\\quad\\mathrm{for~all}i, $$ and our problem is therefore $$ \\operatorname*{Min}_{\\{(x_{i},y_{i},c_{i})\\}}\\sum_{i=1}^{n}\\pi_{i}(x_{i}-y_{i})\\quad{\\mathrm{subject~to~}}(4.1),(6.1){\\mathrm{~and~}}x_{i}\\geq z\\quad{\\mathrm{for~all~}}i. $$\nLemma 7. Let $\\left\\{(x_{i}^{*},y_{i}^{*},c_{i}^{*})_{i=1}^{n}\\right\\}$ solve problem (6.3), then $x_{j}^{*}>z$ implies that $c_{j}^{*}{=}0$.\nLemma 8. Let $\\left\\{(x_{i}^{*},y_{i}^{*},c_{i}^{*})_{i=1}^{n}\\right\\}$ solve problem (6.3) and suppose that for some $j,x_{j}^{*}-h(y_{j}^{*}/a_{j}+c_{j}^{*})=u_{j}$ and $x_{j}^{*}>z$ . Then, $x_{i}^{\\ast}=y_{i}^{\\ast}=\\tilde{y}_{i}$ and $c_{i}^{*}=0$ for all $i{=}j,\\ldots,n$.\nLemma 10.Let $\\left\\{(x_{i}^{*},y_{i}^{*},c_{i}^{*})_{i=1}^{n}\\right\\}$ solve problem (6.3) and suppose that for some $j>1$ $x_{j}^{*}{=}z$ .Then $x_{j-1}^{*}=z$ and $y_{j}^{*}/a_{j}+c_{j}^{*}=y_{j-1}^{*}/a_{j}+c_{j-1}^{*}$.\nProposition 3. Under Assumptions 1-4, then the optimal second-best IMP with workfare has the following form: there exist $\\pmb q$ , r and v satisfying $1\\le q\\le r<v\\le n,\\lambda\\in[0,\\pi_{v}]$ and $c\\in[0,(y_{q+1}^{m}-y_{q}^{m})/\\delta)$ such that: (i) $(x_{i},y_{i},c_{i})=(z,(a_{i+1}y_{i}^{m}-a_{i}y_{i+1}^{m})/\\delta,(y_{i+1}^{m}-y_{i}^{m})/\\delta)\\quad for i=1,\\dots,q-1,$ (ii) $(x_{q},y_{q},c_{q})=(z,y_{q}^{m}-a_{q}c,c),$ (iii) $(x_{i},y_{i},c_{i})=(z,y_{q}^{m}+\\delta c,0)\\quad for i=q+1,\\dots,r,$ (iv) $(x_{i},y_{i},c_{i})=(z+\\sum_{j=r+1}^{i}\\left\\{h(y_{j}/a_{j})-h(y_{j-1}/a_{j})\\right\\},\\hat{y}_{i}(v,\\lambda),0)\\quad for\\quad\\quad i=r+1,\\dots,v-1,$ and (v) $(x_{i},y_{i},c_{i})=(\\tilde{y}_{i},\\tilde{y}_{i},0)\\quad for i=\\boldsymbol{v},\\ldots,n.$\n\nThis section extends the analysis of income maintenance programmes by incorporating public sector work requirements, addressing a multi-dimensional screening problem. The solution exhibits a separability property, breaking down into simpler two-dimensional screening problems. Key results include the characterization of optimal workfare schemes and conditions under which workfare is desirable."
  },
  {
    "qid": "econ-empirical-1782-4-0-0",
    "question": "1) Derive the mathematical justification for using $t$ and Wald statistics in randomization tests, highlighting their robustness to deviations from sharp nulls in favor of heterogeneous treatment effects.",
    "gold_answer": "1. The $t$-statistic is defined as $t = \\frac{\\hat{\\beta}}{\\text{SE}(\\hat{\\beta})}$, where $\\hat{\\beta}$ is the estimated coefficient and $\\text{SE}(\\hat{\\beta})$ is its standard error.\\n2. The Wald statistic is given by $W = \\hat{\\beta}^T \\hat{\\Sigma}^{-1} \\hat{\\beta}$, where $\\hat{\\Sigma}$ is the estimated covariance matrix.\\n3. Under the null hypothesis, both statistics are asymptotically pivotal, meaning their distributions do not depend on nuisance parameters.\\n4. This property makes them robust to deviations from sharp nulls, as their distributions remain valid even under heterogeneous treatment effects.",
    "question_context": "All randomization tests are based upon the distribution of $t$ and Wald statistics, which, as noted above, are more robust to deviations away from sharp nulls in favor of heterogeneous treatment effects.\nReported bootstrap tests are also based on the distribution of $t$ and Wald statistics, which asymptotically and in simulation produce more accurate size.\nResults using the bootstrapped distribution of coefficients are reported in the Online Appendix and have systematically higher rejection rates.\nTable V tests the statistical significance of individual treatment effects. The top row in each panel reports the average across papers of the fraction of coefficients that are statistically significant using authors’ methods, and lower rows report the ratio of the same measure calculated using alternative procedures to the figure in the top row.\nTable VI tests the null that all reported treatment effects in regressions with more than one reported treatment coefficient are zero using joint- and multiple-testing methods.\nTable VII reports results for joint tests of reported treatment effects appearing together in tables. The results presented in tables usually revolve around a theme, typically the exploration of alternative specifications in the projection of one or more related outcomes of interest on treatment, treatment interactions with covariates, and treatment subsamples.\n\nThis section applies the testing procedures described above to the 53 papers in the sample. The results are based on the average across papers of within-paper rejection rates, with each paper carrying equal weight in summary statistics. Randomization tests are based on the distribution of $t$ and Wald statistics, which are robust to deviations away from sharp nulls in favor of heterogeneous treatment effects."
  },
  {
    "qid": "econ-empirical-1067-0-0-0",
    "question": "1) Derive the conditions under which sales dominate leasing in a durable-goods monopoly with stochastic consumer values. Use the model's assumptions about transition probabilities and discount factors.",
    "gold_answer": "To derive the conditions:\n\n1. Let $v_H$ and $v_L$ represent high and low consumer values, with transition probability $a$.\n2. The firm's profit from leasing is $\\pi_L = v_H \\cdot \\delta$ (assuming $\\delta$ is the discount factor).\n3. For sales, profit is $\\pi_S = p_1 + \\delta \\cdot E[p_2|p_1]$, where $p_1$ is the first-period price and $p_2$ depends on value transitions.\n4. Sales dominate when $\\pi_S > \\pi_L$, which occurs when $a > \\frac{v_H(1-\\delta)}{\\delta(v_H - v_L)}$.\n5. This inequality shows that higher transition probabilities and discount factors favor sales over leasing.",
    "question_context": "When values are stochastic, the Coase conjecture does not hold, and the standard result that leasing dominates sales may be reversed.\nThe optimal mechanism specifies options with a strike price in $(b,1]$ so that only high-value consumers exercise the option in the second period.\nWhen a perfect secondhand market exists, the implicit rental price in the sales model will be a function only of the total number of units sold, regardless of the transition probability.\nWith buyer anonymity, however, the relationship is reversed: a leasing strategy extracts the static monopoly profit in each period.\n\nThis article analyzes a durable-goods model where consumers' values vary stochastically over time. The author compares the optimal mechanism for a monopolist to sales and leasing equilibria, showing that sales may implement the optimal strategy under certain conditions, contrary to previous literature which suggested leasing as the dominant strategy."
  },
  {
    "qid": "econ-empirical-890-1-0-1",
    "question": "2) Discuss the advantages of using a penalized local linear estimation approach for forecast combination weights in a high-dimensional setting.",
    "gold_answer": "1. **Flexibility**: The nonparametric approach adapts to time-varying weights without assuming a specific functional form.\\n2. **Regularization**: The penalization term controls the complexity of the model, preventing overfitting in high-dimensional settings.\\n3. **Scalability**: The method can handle a large and expanding set of forecasts, making it suitable for big data applications.\\n4. **Oracle Property**: The estimator achieves optimal asymptotic performance, as shown by Chen and Maung.",
    "question_context": "Chen and Maung examine such methods in a high-dimensional data setting. They develop a nonparametric approach to time-varying combination weights and cover cases with a small set of underlying forecasts as well as a large and expanding set of forecasts that grows with the sample size, proposing a penalized local linear estimation approach for the latter case.\nFor the case of structural change of unknown form, Chen and Maung establish an oracle result for a two-stage nonparametric estimator of the forecast combination weights and use simulations and an application to inflation forecasting to demonstrate the performance of their approach.\n\nThe text discusses various methods for combining forecasts to improve predictive accuracy, especially in the presence of structural breaks and high-dimensional data settings."
  },
  {
    "qid": "econ-empirical-1170-0-0-0",
    "question": "1) Derive the implications of systematic income estimation errors by medical students on their specialty choices, considering the bounded rationality framework. How does this relate to the observed heterogeneity in estimation errors?",
    "gold_answer": "1. **Bounded Rationality Framework**: Medical students, due to cognitive limitations, may not process all available information optimally, leading to systematic errors. \n2. **Implications for Specialty Choice**: If students underestimate income, they may avoid high-income specialties, leading to suboptimal career choices. \n3. **Heterogeneity**: Factors like gender, age, and MCAT scores introduce variability in errors, as seen in the study. \n4. **Mathematical Representation**: Let \\( E_i \\) be the estimation error for student \\( i \\), then \\( E_i = f(X_i) + \\epsilon_i \\), where \\( X_i \\) includes gender, age, and MCAT scores.",
    "question_context": "Twenty-five cohorts of medical students were asked in their first and fourth year of school to estimate contemporaneous physician income in six different specialties. The students' income estimation errors varied systematically over time and cross-sectionally by specialty and type of student. The median student underestimated physician income by 15 percent, and the median absolute value of the estimation errors was 26 percent of actual income. Students were 35 percent more accurate when estimating market income in their fourth relative to their first year, which indicates medical students learn a considerable amount before choosing a specialty.\nMedical students overestimated physician income in the 1970s but now underestimate income by 25 percent. Women, older students, and students with relatively high MCAT scores underestimate physicians’ incomes relative to their peers.\nThe median absolute value of the estimation errors was 26 percent of actual income. One reason medical students provide inaccurate estimates is that they extrapolate recent income growth rates, and tend therefore to overshoot or undershoot actual income when there have been relatively large changes in the near past.\n\nThe study examines how accurately medical students estimate contemporaneous physician income, the systematic errors in their estimations, and the determinants of these errors. It also explores how much students learn about market income during medical school."
  },
  {
    "qid": "econ-empirical-1629-2-0-0",
    "question": "1) Derive the endogeneity bias in the regression model $\\log{(M)_{i t}}=\\beta_{0}+\\beta_{1}(Sulfa\\ availability)_{i t}+e_{i t}$ when using sulfa drug sales as a proxy for availability. Formally show how mortality risk from treatable diseases affects the estimate of $\\beta_1$.",
    "gold_answer": "1. **Bias Mechanism**: Let true availability be $A_{it}^*$ and sales $S_{it} = A_{it}^* + \\gamma M_{it} + u_{it}$, where $\\gamma>0$ reflects higher sales in high-mortality areas. \\n2. **Plugging into Model**: $\\log{(M)_{it}} = \\beta_0 + \\beta_1 S_{it} - \\beta_1 \\gamma M_{it} - \\beta_1 u_{it} + e_{it}$. \\n3. **Bias Term**: The term $-\\beta_1 \\gamma M_{it}$ correlates $S_{it}$ with the error, violating exogeneity. \\n4. **Direction**: Since $\\gamma>0$ and $\\beta_1<0$, the bias is positive, attenuating the true effect.",
    "question_context": "The hypothesis is that when sulfa drugs become more available, mortality falls $(\\beta_{1}<0)$. In practice, researchers might define the medical market as a state, and use state-year level data on sales volume or the number of prescriptions issued as a proxy for sulfa drug availability, but it is worth keeping in mind that such a regression would suffer from endogeneity bias.\nThe first year of large-scale production and sales of sulfa drugs in the United States was 1937. This allows us to use time-series techniques to test whether the timing of mortality declines corresponds to the introduction of sulfa drugs in 1937.\nThe MMR declined by roughly 46% between the pre-sulfa (1925-1936) and post-sulfa (1937-1943) periods. Mortality for pneumonia/influenza and scarlet fever dropped by 35-68% over the same interval, while tuberculosis (control) showed no structural break around 1937.\n\nThe analysis uses US vital statistics data (1920-1950) to examine mortality trends for diseases treatable with sulfa drugs (maternal mortality, pneumonia, scarlet fever, meningitis) versus control diseases (tuberculosis, chronic diseases). The study leverages the exogenous timing of sulfa drug introduction (1937) and clinical evidence of disease-specific efficacy to isolate causal effects."
  },
  {
    "qid": "econ-empirical-1424-3-3-0",
    "question": "7) Using the data from Table 1, derive the exact welfare growth decomposition formula that separates TFP and capital contributions. Show why the expectation-revision term is grouped with TFP.",
    "gold_answer": "1. Total welfare growth \\( \\Delta W = \\Delta W_{TFP} + \\Delta W_K \\). \\n2. From equation (23): \\n   \\[ \\Delta W_{TFP} = \\mathbb{E}_t \\sum \\beta^s \\Delta \\ln A_{t+s} + (\\mathbb{E}_t - \\mathbb{E}_{t-1}) \\sum \\beta^s \\ln A_{t+s} \\] \\n3. Expectation-revision depends on TFP process → naturally grouped. \\n4. Capital term: \\n   \\[ \\Delta W_K = \\frac{\\beta}{1-\\beta} (\\ln K_t - \\ln K_{t-1}) \\]",
    "question_context": "The resulting average welfare growth rates for the period 1985–2005 are shown in Table 1 for several fiscal scenarios. With wasteful government expenditure and lump-sum taxes, as assumed in the first column, the average annual growth rate of welfare in the United States is equivalent to a permanent annual increase in consumption of about $2.5\\%$.\nTable 1 also shows the relative contribution of the two components of welfare—TFP growth and capital accumulation—to the growth rate of welfare in each country. For this decomposition, we treat the expectation-revision term as part of the contribution of TFP. In our benchmark case of optimal spending and distortionary taxes, $60\\%$ or more of the welfare gains are attributable to TFP in all countries.\n\nThis section presents empirical results on welfare growth decomposition across different fiscal scenarios for developed countries."
  },
  {
    "qid": "econ-empirical-278-4-0-3",
    "question": "4) For DGP IX's time-varying system, prove that the NG-TVP-VAR achieves consistent estimation of $B_t$ and $\\Omega_t$ while G-TVP-VAR fails, using the given parameter evolution equations.",
    "gold_answer": "1. NG-TVP-VAR correctly models the non-Gaussian error process and parameter drift:\n   $$B_{km,t} = 0.25\\sin(2\\pi t/T) + 0.25\\sum_{i=1}^t \\zeta_i/\\sqrt{t}$$\n2. G-TVP-VAR's Gaussian assumption leads to bias in volatility estimation:\n   $$\\text{Bias}(\\hat{\\Omega}_t^{\\text{G}}) = \\mathbb{E}[\\eta_t\\eta_t'] - \\mathbb{E}_{\\text{G}}[\\eta_t\\eta_t'] \\neq 0$$\n3. NG-TVP-VAR's correction:\n   $$\\hat{\\Omega}_t^{\\text{NG}} = \\frac{1}{T}\\sum_{t=1}^T w_t(\\mathbf{Y}_t - B_t\\mathbf{Y}_{t-1})(\\mathbf{Y}_t - B_t\\mathbf{Y}_{t-1})'$$ where $w_t$ are robustness weights.",
    "question_context": "DGP I: The first DGP considers a model with normally distributed errors: $$\\eta_{t}\\overset{i.i.d.}{\\sim}\\mathcal{N}(0,I_{2}).$$ The goal is to verify correct coverage rates without distributional misspecification.\nDGP II: The model uses Laplace-distributed errors: $$\\eta_{i,t}\\stackrel{i.i.d.}{\\sim}L a(0,1),i=1,2.$$ The kurtosis differs slightly from normal, affecting G-VAR coverage rates.\nDGP III: Errors follow a multivariate t-distribution with 5 degrees of freedom: $$\\eta_{t}\\stackrel{i.i.d.}{\\sim}t_{5}(0,I_{2}).$$ Misspecification leads to small distortions in Gaussian BVAR coverage rates.\nDGP IV: Errors are standardized inverse-Gaussian: $$\\eta_{i,t}=\\frac{z_{i,t}-\\mu}{\\sigma},~z_{i,t}\\stackrel{i,i.d.}{\\sim}\\mathcal{Z N}(\\mu,\\lambda),i=1,2,~\\mu=3,\\lambda=0.2,~\\sigma=\\mu^{3}/\\lambda.$$ Large excess kurtosis contaminates inference for small samples.\nDGP V: Errors are standardized Weibull-distributed: $$\\eta_{i,t}=\\frac{z_{i,t}-\\mu}{\\sigma},~z_{i,t}\\stackrel{i.i.d.}{\\sim}W b(\\alpha,\\beta),~i=1,2,~\\alpha=1,\\beta=0.7$$ where $\\mu=\\alpha{\\Gamma}(1+1/\\beta)$ and $\\sigma^{2}=\\alpha^{2}\\left(T(1+2/\\beta)-\\left(T(1+1/\\beta)\\right)^{2}\\right)$.\nDGP VI: Errors are centered Gamma-distributed: $$\\eta_{i,t}=\\frac{z_{i,t}-\\mu}{\\sigma},~z_{i,t}\\stackrel{i.i.d.}{\\sim}G a(\\alpha,\\gamma),~\\alpha=0.8,\\gamma=1,$$ where $\\mu=\\alpha/\\gamma$ and $\\sigma^{2}=a/\\gamma^{2}$.\nDGP VII: Errors follow a mixture distribution: $$\\pmb{\\eta}_{t}=\\left\\{\\begin{array}{l l}{\\mathbf{z}_{t}}&{\\mathrm{with~probability~\\boldsymbol{\\lambda}~}}\\\\{\\mathbf{w}_{t}}&{\\mathrm{with~probability~\\boldsymbol{1}-\\boldsymbol{\\lambda}~}}\\end{array}\\right.$$ with $\\pmb{z}_{t}\\overset{i.i.d.}{\\sim}\\mathcal{N}(\\alpha_{1},I_{2})$ and $\\mathbf{w}_{t}\\overset{i.i.d.}{\\sim}T_{5}(\\alpha_{2},I_{2})$.\nDGP VIII: Another mixture distribution: $$\\eta_{t}=\\left\\{\\begin{array}{l l}{\\mathbf{z}_{t}\\quad\\quad\\mathrm{~with~probability~}\\boldsymbol{\\lambda}}\\\\{\\mathbf{w}_{t}\\quad\\mathrm{~with~probability~}1-\\boldsymbol{\\lambda}}\\end{array}\\right.$$ where $\\mathbf{z}_{t}\\stackrel{i,i,d.}{\\sim}\\mathcal{N}(\\alpha_{1},I_{2})$ and $w_{i,t}\\stackrel{i,i,d.}{\\sim}I G(\\alpha_{2},3)$.\nDGP IX: Time-varying parameters with non-Gaussian errors: $$\\begin{array}{r l}{{\\mathbf Y}_{t}=B_{t}{\\mathbf Y}_{t-1}+\\varOmega_{t}^{1/2}\\boldsymbol\\eta_{t},\\quad{\\mathbf Y}_{0}=0,\\varOmega_{t}=H_{t}^{-1}\\boldsymbol\\Psi_{t}H_{t}^{-1/\\prime}}\\\\{B_{k m,t}=0.25\\sin{(2\\pi t/T)}+0.25\\displaystyle\\sum_{i=1}^{t}\\zeta_{i}/\\sqrt{t},\\quad}&{\\zeta_{i}\\stackrel{i,i,d.}{\\sim}N(0,0.3^{2}),\\mathrm{for}m,k=1,2,}\\\\{H_{12,t}=0.5\\sin{(2\\pi t/T)}+0.5\\displaystyle\\sum_{i=1}^{t}\\xi_{i}/\\sqrt{t},\\quad}&{\\xi_{i}\\stackrel{i,i,d.}{\\sim}N(0,0.3^{2}),H_{11}=H_{22}=1,H_{21}=0,}\\\\{\\log\\psi_{k k,t}=0.5\\sin{(2\\pi t/T)}+0.5\\displaystyle\\sum_{i=1}^{t}\\nu_{i}^{k}/\\sqrt{t},\\quad}&{\\upsilon_{i}^{k}\\stackrel{i,i,d.}{\\sim}N(0,0.3^{2}),k=1,2.}\\end{array}$$\n\nThis section examines various Data Generating Processes (DGPs) for small-dimensional VAR models, comparing the performance of Gaussian and non-Gaussian specifications under different error distributions. The focus is on bias, RMSE, and coverage rates for parameter estimates and impulse response functions."
  },
  {
    "qid": "econ-empirical-328-1-0-3",
    "question": "4) Analyze the implications of the assumption $\\frac{d}{d\\alpha}[m'(\\alpha)f(\\alpha)] < 0$ for the firm's R&D decisions and the economy's dynamics.",
    "gold_answer": "1. **R&D Incentives**: The assumption ensures that the marginal return to R&D decreases as $\\alpha$ increases, preventing firms from monopolizing innovation.  \n2. **Economic Dynamics**: This condition stabilizes the economy by ensuring no single firm dominates R&D, leading to a symmetric equilibrium where firms share similar innovation paths.  \n3. **Microfoundations**: Since $m''(\\alpha) > 0$, $f'(\\alpha) < 0$ is necessary to satisfy the condition. This reflects diminishing productivity of R&D as technology becomes more capital-intensive.",
    "question_context": "The firm solves $$\\operatorname*{max}_{\\{a_{i t},P_{i t},L_{i t},I_{i t},R_{i t}\\}_{t=0}^{\\infty}}\\int_{0}^{\\infty}(P_{i t}X_{i t}-w_{t}L_{i t}-I_{i t}-R_{i t})e^{-\\overline{{r}}_{t}t}~d t,$$ where $\\begin{array}{r}{\\overline{{r}}_{t}\\equiv\\frac{1}{t}\\int_{0}^{t}r_{u}\\:d u}\\end{array}$ is the average interest rate between time 0 and time t, $r_{u}$ is the instantaneous interest rate at time $u$ and the optimization is subject to (3)–(6) and the restrictions $I_{i t}{\\geq}0$ and $R_{i t}{\\geq}0$ .\nThe firm's output is thus $X=A[(L{-}l)+(K^{\\alpha}l^{1-\\alpha})],$ , where $L$ is the firm's total employment, $l{\\in}[0,L]$ is labor allocated to the advanced plant and $L{-}l$ is labor allocated to the primitive plant.\nThe production division's problem now is $$\\operatorname*{max}_{l,L,K}=Y^{1/\\epsilon}X^{1-(1/\\epsilon)}-w L-p_{K}K\\quad\\mathrm{s.t.}X=A[(L-l)+(K^{\\alpha}l^{1-\\alpha})].$$\nThe investment division, taking the production division's decisions as given, chooses $I$ and $R$ to maximize its present value $$\\operatorname*{max}_{\\{I_{t},R_{t}\\}_{t=0}^{\\infty}}\\int_{0}^{\\infty}(p_{K}K_{t}-I_{t}-R_{t})e^{-\\overline{{r}}_{t}t}d t,\\quad\\mathrm{s.t.~}(5),(6),(14),K_{0},\\alpha_{0}.$$\n\nThe firm's optimization problem involves maximizing the present value of profits over an infinite horizon, considering production and investment divisions, with constraints on investment and R&D. The firm uses Cobb-Douglas technologies with varying capital elasticities."
  },
  {
    "qid": "econ-empirical-71-5-2-1",
    "question": "6) Explain how nominal rigidities and the zero lower bound (ZLB) amplify the nonlinear effect of household debt on growth. Use a New Keynesian framework with a Phillips curve.",
    "gold_answer": "1. ZLB constraint: \\( r_{t} \\geq 0 \\) limits monetary policy response to demand shocks.\n2. Rising debt \\( \\Rightarrow \\) demand shock \\( \\Rightarrow \\) lower \\( r_{t} \\), but ZLB binds.\n3. Phillips curve: \\( \\pi_{t} = \\kappa y_{t} + \beta \\mathbb{E}_t \\pi_{t+1} \\).\n4. With rigidities, output \\( y_{t} \\) falls more when ZLB binds (no offsetting rate cuts).",
    "question_context": "Column (1) of Table IX shows that the predictive power of household debt changes on subsequent output growth comes completely from situations in which a country sees a rise in household debt.\nThe nonparametric relation between a change in the household debt to GDP ratio and subsequent GDP growth in Figure III confirms the presence of such a nonlinearity.\n\nThe section explores asymmetric effects of household debt changes on GDP growth, emphasizing the role of macroeconomic frictions."
  },
  {
    "qid": "econ-empirical-1280-4-0-3",
    "question": "4) Analyze the limitations of high regression \\(R^{2}\\) values in detecting model misspecification, using the two-factor affine example from Online Appendix K.",
    "gold_answer": "1. **Single-factor misspecification**: Omitting the low-volatility, persistent factor still yields \\(R^{2} = 99\\%\\), but variance ratios \\(> 1\\) due to omitted variable bias.\n2. **Correct specification**: Including both factors raises \\(R^{2}\\) to \\(99.5\\%\\), but small measurement error can still bias regression tests.\n3. **Key insight**: High \\(R^{2}\\) does not guarantee model correctness; robustness checks (e.g., KF-MLE, IV) are essential.",
    "question_context": "We calibrate the magnitude of measurement error needed to generate the excess-volatility patterns we see in the data. To do so, we simulate data from an affine model and ask how much error is needed on the short end of the curve to match observed variance ratios.\nFor variance swaps, we find that measurement error must have a standard deviation of more than two volatility points at the short end of the curve to match long-maturity variance ratios. This is five times larger than the average bid-ask spread of short-dated variance swaps.\nState space methods (KF-MLE) are one way to account for this error and achieve unbiased estimates. Another way to overcome errors in variables is by finding suitable instruments for latent factors and using instrumental variables (IV) regression.\nA high regression $R^{2}$ does not rule out misspecification due to omitted factors or measurement error that is unaccounted for. That is, the intuition that a regression $R^{2}$ of $99\\%$ is almost the same as $100\\%$ is potentially flawed.\n\nThis section discusses the impact of measurement error in short-maturity prices on variance ratio estimates, and explores methods to account for this error, including state space methods (KF-MLE) and instrumental variables (IV) regression."
  },
  {
    "qid": "econ-empirical-1008-1-0-2",
    "question": "3) Derive the form of the confidence interval $\\mathrm{CI}_{\\alpha}({\\bf x})$ for $c^{\\prime}\\theta(\\mathbf{x})$ as given in the text, and justify its asymptotic validity using Theorem 1.",
    "gold_answer": "1. **Confidence Interval Form**: \n   $$\n   \\mathrm{CI}_{\\alpha}({\\bf x}) = \\left[c^{\\prime}{\\hat{\\theta}}({\\bf x}) - {\\mathfrak{q}}_{1-\\alpha/2}\\sqrt{c^{\\prime}{\\hat{\\Omega}}({\\bf x})c}, c^{\\prime}{\\hat{\\theta}}({\\bf x}) - {\\mathfrak{q}}_{\\alpha/2}\\sqrt{c^{\\prime}{\\hat{\\Omega}}({\\bf x})c}\\right],\n   $$\n   where $\\mathfrak{q}_{a}$ is the $a$-quantile of $\\mathcal{N}(0,1)$.\n2. **Asymptotic Validity**: By Theorem 1, $\\hat{\\Omega}^{-1/2}(\\hat{\\theta}-\\theta) \\rightsquigarrow \\mathcal{N}(0,I)$, so the coverage probability satisfies:\n   $$\n   \\lim_{n\\to\\infty} \\mathbb{P}\\left[c^{\\prime}\\theta(\\mathbf{x}) \\in \\mathrm{CI}_{\\alpha}(\\mathbf{x})\\right] = 1-\\alpha.\n   $$",
    "question_context": "Assumption 1. $x_{1},\\ldots,x_{n}$ is a random sample from a distribution $F(\\cdot)$ supported on $\\mathcal{X}\\subseteq\\mathbb{R},$ , and $\\mathbf{x}\\in\\mathcal{X}$ . (i) For some $\\delta>0,F(\\cdot)$ is absolutely continuous on $[{\\pmb x}-\\delta,{\\pmb x}+\\delta]$ with a density $f(\\cdot)$ admitting constants $f({\\bf{x}}-{\\bf{\\omega}}),\\dot{f}({\\bf{x}}-{\\bf{\\omega}})$ $f({\\bf x}+)$ , and ${\\dot{f}}({\\bf x}+)$ such that \n\n$$\n\\operatorname*{sup}_{u\\in[-\\delta,0)}{\\frac{|f(\\mathbf{x}+u)-f(\\mathbf{x}-)-{\\dot{f}}(\\mathbf{x}-)u|}{|u|^{2}}}+\\operatorname*{sup}_{u\\in(0,\\delta]}{\\frac{|f(\\mathbf{x}+u)-f(\\mathbf{x}+)-{\\dot{f}}(\\mathbf{x}+)u|}{|u|^{2}}}<\\infty.\n$$ \n\n(ii) $K(\\cdot)$ is nonnegative, symmetric, and continuous on its support $[-1,1]$ , and integrates to 1. \n\n(iii) $R(\\cdot)$ is locally bounded, and there exists a positive-definite diagonal matrix $\\boldsymbol{{T}}_{h}$ for each $h~>~0$ such that $\\gamma_{h}R(u)=R(u/h)$ . (iv) Let $\\begin{array}{r}{\\chi_{h,x}=\\frac{\\mathcal{X}-\\mathbf{x}}{h}}\\end{array}$ . For all $h$ sufficiently small, the minimum eigenvalues of $T_{h,{\\times}}$ and $h^{-1}\\varSigma_{h,\\times}$ are bounded away from zero, where \n\n$$\n\\begin{array}{r l}&{T_{h,x}=\\displaystyle\\int_{\\mathcal{X}_{h,x}}R(u)R(u)^{\\prime}K(u)f(x+h u)\\mathrm{d}u,}\\ &{\\Sigma_{h,x}=\\displaystyle\\int_{\\mathcal{X}_{h,x}}\\int_{\\mathcal{X}_{h,x}}R(u)R(v)^{\\prime}\\big[F(x+h\\operatorname*{min}\\{u,v\\})-F(x+h u)F(x+h v)\\big]K(u)K(v)f(x+h u)f(x+h v)\\mathrm{d}u}\\end{array}\n$$\nWe show that, under regularity conditions and if $h$ vanishes at a suitable rate as $n\\to\\infty$ , then \n\n$$\n\\hat{\\Omega}^{-1/2}(\\hat{\\theta}-\\theta)\\rightsquigarrow\\mathcal{N}(0,I),\\qquad\\hat{\\Omega}=\\hat{F}^{-1}\\hat{\\Sigma}\\hat{F}^{-1},\n$$ \n\nwhere \n\n$$\n\\hat{\\boldsymbol{T}}=\\frac{1}{n}\\sum_{i=1}^{n}W_{i}R_{i}R_{i}^{\\prime},\\qquad\\hat{\\boldsymbol{\\Sigma}}=\\frac{1}{n^{2}}\\sum_{i=1}^{n}\\hat{\\psi}_{i}\\hat{\\psi}_{i}^{\\prime},\\qquad\\hat{\\psi}_{i}=\\frac{1}{n}\\sum_{j=1}^{n}W_{j}R_{j}(\\mathbb{1}(\\boldsymbol{x}_{i}\\leq\\boldsymbol{x}_{j})-\\hat{\\boldsymbol{F}}_{j}).\n$$\nTheorem 1 (Pointwise Asymptotic Normality). Suppose Assumption 1 holds. If $n\\varrho(h)^{2}/h\\rightarrow0$ and $n h^{2}\\rightarrow\\infty$ then (5) holds.\n\nThis section discusses the large sample properties of the estimator $\\hat{\\theta}({\\bf x})$, pointwise in $\\textbf{x}\\in{\\mathcal{X}}$. We first establish asymptotic normality, and then discuss asymptotic efficiency. Other results are reported in the SA to conserve space. We drop the dependence on the evaluation point $\\times$ whenever possible."
  },
  {
    "qid": "econ-empirical-1021-0-1-3",
    "question": "8) Why might simple comparisons by veteran status overestimate the effect of military service on earnings?",
    "gold_answer": "Simple comparisons include the bias term:  \n$$E[Y_0 | D=1] - E[Y_0 | D=0],$$  \nwhere veterans ($D=1$) may have higher $Y_0$ (potential earnings without service) due to unobserved positive selection (e.g., motivation, health). This inflates the apparent benefit of military service.",
    "question_context": "The empirical results suggest that soldiers who served in the early 1980s were paid considerably more than comparable civilians while in the military, and that military service is associated with higher employment rates for veterans after service.\nMilitary service led to only a modest long-run increase in the civilian earnings of nonwhite veterans while actually reducing the civilian earnings of white veterans.\n\nThe paper presents estimates of the effects of military service on earnings and employment, comparing results from matching, regression, and IV methods."
  },
  {
    "qid": "econ-empirical-694-0-0-1",
    "question": "2) Using the Coase Theorem, derive the conditions under which decentralized bargaining leads to a Pareto optimal outcome in the presence of externalities. How do property rights allocation and transaction costs affect this result?",
    "gold_answer": "1. **Pareto Optimality**: The Coase Theorem states that if property rights are well-defined and transaction costs are negligible, bargaining will internalize the externality.  \n2. **Bargaining Outcome**: Let the externality be \\( E(q) \\). The optimal quantity \\( q^* \\) satisfies \\( \\sum MB(q^*) = MC(q^*) + E'(q^*) \\).  \n3. **Property Rights**: The initial allocation affects distribution but not efficiency.  \n4. **Transaction Costs**: High costs may prevent bargaining, leading to inefficiency.",
    "question_context": "Plott (1983) establishes that there is indeed a behavioural externality problem to solve, in the sense that competitive markets with externalities do converge to a private equilibrium that ignores social costs.\nThe Coase Theorem (1960) is behaviourally ‘alive and well' in relatively sterile and abstract bargaining environments, as demonstrated by Harrison and McKee (1985) and Hoffman and Spitzer (1982; 1986).\nThe Coasian approach implies two distinct behavioural outcomes: (i) that the parties will agree on a Pareto optimal level for the externality-generating activity, and (ii) that any such agreement will be attained by means of a mutually advantageous bargain between the parties.\nSubjects were able, after several periods of learning, both to identify the social optimum quantity and to agree to enforce a voluntary restriction on that quantity.\n\nThis paper evaluates Coasian solutions to the externality problem in a series of experimental markets, extending previous studies by incorporating richer informational environments and decentralized Coasian bargains."
  },
  {
    "qid": "econ-empirical-439-1-0-4",
    "question": "5) Derive the spectral representation $\\tilde{\\alpha}(\\lambda)$ for the linear differential-difference equations system (2.18). Why is the resulting spectrum not an ARMA process?",
    "gold_answer": "1. **Spectral Representation**: For the system: \n$$ \\sum_{j=0}^{q}\\sum_{k=0}^{r}\\alpha_{j k}D^{j}y(t-\\zeta_{k})=\\varepsilon(t), $$ \nthe Fourier transform gives: \n$$ \\sum_{j=0}^{q}\\sum_{k=0}^{r}\\alpha_{j k}(-\\mathrm{i}\\lambda)^{j}\\exp{(\\zeta_{k}\\mathrm{i}\\lambda)}\\tilde{y}(\\lambda) = \\tilde{\\varepsilon}(\\lambda). $$ \nThus: \n$$ \\tilde{\\alpha}(\\lambda) = \\sum_{j=0}^{q}\\sum_{k=0}^{r}\\alpha_{j k}(-\\mathrm{i}\\lambda)^{j}\\exp{(\\zeta_{k}\\mathrm{i}\\lambda)}. $$ \n\n2. **Non-ARMA Spectrum**: The spectrum involves both polynomial ($(-\\mathrm{i}\\lambda)^{j}$) and exponential ($\\exp{(\\zeta_{k}\\mathrm{i}\\lambda)}$) terms, which cannot be expressed as a ratio of polynomials (required for ARMA). Thus, the spectrum is not of ARMA form.",
    "question_context": "The linear differential equations system: \n$$ \n\\sum_{j=0}^{q}\\alpha_{j}D^{j}y(t)=\\varepsilon(t),\\qquad t\\in(-\\infty,\\infty),D={\\bf d}/{\\bf d}t, \n$$ \nfor which \n$$ \n\\tilde{\\alpha}(\\lambda)=\\sum_{j=0}^{q}\\alpha_{j}(-\\mathrm{i}\\lambda)^{j}. \n$$\nThe linear difference equations system: \n$$ \n\\sum_{j=0}^{q}\\alpha_{j}y(t-\\zeta_{j})=\\varepsilon(t),\\qquad t\\in\\mathcal{T}, \n$$ \nfor which \n$$ \n\\tilde{\\alpha}(\\lambda)=\\sum_{j=0}^{q}\\alpha_{j}\\exp{(\\zeta_{j}\\mathrm{i}\\lambda)}. \n$$\nOne normally assumes that for all complex $z$ with non-negative real part, $\\left|\\tilde{\\alpha}(\\mathrm{i}z)\\right|\\neq0$; this implies finiteness of variance of the solution of (2.1) which expresses $y(t)$ as a function of $\\varepsilon(u)$ only for $u\\leq t$.\nUnder (2.15) or (2.16), exact closed-form expressions for $\\vec{f}_{y}$ may be obtainable. $\\tilde{f}_{y}$ is the spectrum of an $\\mathbf{ARMA}(q,q-1)$ process under (2.13) and (2.15) [Phillips (1959)] or under (2.14) with $\\zeta_{j}=j\\delta/m$, and (2.16) [Telser (1967), Brewer (1973)].\nA differential equation driven by pure noise has very erratic sample paths, and so it may not be a reasonable approximate description of economic processes that change slowly over time. A more suitable representation of such processes may be obtained if one alters $f_{y}$ only at high frequencies, replacing (2.15) by \n$$ \n\\begin{array}{c c} {{f_{\\varepsilon}(\\lambda)=(2\\pi)^{-1}\\Omega,}}&{{\\left|\\lambda\\right|\\leq A,}} \\\\ {{{}}}&{{{}}} \\\\ {{{\\bf \\alpha}=0,}}&{{\\left|\\lambda\\right|>A,}}\\end{array} \n$$ \nfor some $A<\\infty$.\nThe linear differential-difference equations system: \n$$ \n\\sum_{j=0}^{q}\\sum_{j=0}^{r}\\alpha_{j k}D^{j}y(t-\\zeta_{k})=\\varepsilon(t),\\qquad t\\in(-\\infty,\\infty), \n$$ \nfor which \n$$ \n\\tilde{\\alpha}(\\lambda)=\\sum_{j=0}^{q}\\sum_{k=0}^{r}\\alpha_{j k}(-\\mathrm{i}\\lambda)^{j}\\exp{(\\zeta_{k}\\mathrm{i}\\lambda)}. \n$$\n\nThis section discusses exact models for $\\widetilde{\\alpha}$ and $f_{\\varepsilon}$ as known functions of $\\lambda$ and finitely many unknown parameters. It covers linear differential and difference equations systems, their spectral representations, and conditions for finiteness of variance. It also addresses issues of identifiability and the suitability of ARMA models in econometrics."
  },
  {
    "qid": "econ-empirical-1642-4-1-0",
    "question": "3) Derive the optimal portfolio weight $\\tilde{w}_{t}$ in the active fund under exponential utility and normality assumptions. Show how it depends on $\\tilde{E}_{t-1}(\\gamma_{t})$.",
    "gold_answer": "1. The investor maximizes $\\tilde{E}_{t-1}\\exp(-a\\tilde{W}_{t})$ subject to $\\tilde{W}_{t}=\\tilde{W}\\left[(1+s_{t})+\\tilde{w}_{t}\\gamma_{t}\\right]$.\n2. Under normality, this reduces to mean-variance optimization: $\\tilde{w}_{t} = \\arg\\max \\left\\{ \\tilde{E}_{t-1}(\\tilde{w}_{t}\\gamma_{t}) - \\frac{a}{2}\\tilde{V}a r_{t-1}(\\tilde{w}_{t}\\gamma_{t}) \\right\\}$.\n3. Solving yields $\\tilde{w}_{t}=\\frac{\\tilde{E}_{t-1}(\\gamma_{t})}{a\\tilde{W}\\tilde{V}a r_{1\\gamma}}$, linear in the expected excess return.",
    "question_context": "The optimal investment in the active fund is a linear increasing function of the investor’s expectation of the fund’s excess return $\\gamma_{t}$: $\\tilde{w}_{t}=\\frac{\\tilde{E}_{t-1}(\\gamma_{t})}{a\\tilde{W}\\tilde{V}a r_{1\\gamma}}$.\nThe covariance between the excess return of the active fund in period t and the net flow into the fund during periods $t+1$ through $t+k$ is $C o\\nu\\left(\\gamma_{t},\\sum_{k^{\\prime}=1}^{k}\\tilde{F}_{t+k^{\\prime}}\\right)=\\frac{\\sigma_{\\epsilon\\gamma}^{2}\\tilde{N}_{k}}{a\\tilde{V}a r_{1\\gamma}}$.\n\nThis section extends the model to explain the fund-flow puzzle, where flows into mutual funds correlate with lagged returns despite no predictability."
  },
  {
    "qid": "econ-empirical-550-0-0-0",
    "question": "1) Derive the stationarity condition for the PTTGARCH(p,q) model given in equation (1.2), explicitly stating the Lyapunov exponent condition $\\gamma(\\phi) < 0$.",
    "gold_answer": "1. The Lyapunov exponent $\\gamma(\\phi)$ is derived from the top Lyapunov exponent of the random matrix sequence associated with the model. For PTTGARCH(p,q), it ensures strict stationarity when $\\gamma(\\phi) < 0$.  \n2. Specifically, for model (1.2), the condition reduces to verifying that the spectral radius of the matrix formed by coefficients $(\\alpha_{1i}, \\alpha_{2i}, \\beta_j)$ is less than 1.  \n3. Formally: $\\gamma(\\phi) = \\lim_{n \\to \\infty} \\frac{1}{n} \\log \\|A_1 A_2 \\dots A_n\\| < 0$, where $A_t$ are i.i.d. matrices capturing the volatility dynamics.",
    "question_context": "Consider a class of power-transformed and threshold $\\mathrm{\\bfGARCH}(p,q)$ $(\\mathrm{PTTGRACH}(p,q))$ model, which is a natural generalization of power-transformed and threshold GARCH(1,1) model in Hwang and Basawa [2004. Stationarity and moment structure for Box–Cox transformed threshold GARCH(1,1) processes. Statistics & Probability Letters 68, 209–220.] and includes the standard GARCH model and many other models as special cases.\nWe first establish the asymptotic normality for quasi-maximum likelihood estimators (QMLE) of the parameters under the condition that the error distribution has finite fourth moment. For the case of heavy-tailed errors, we propose a least absolute deviations estimation (LADE) for PTTGA $\\mathsf{R C H}(p,q)$ model, and prove that the LADE is asymptotically normally distributed under very weak moment conditions.\nThe autoregressive conditional heteroscedastic (ARCH) model proposed by Engle (1982) has led to considerable interest in models in which the conditional variance (volatility) of the current observation, $\\sigma_{t}^{2}$ , is a function of the past observations.\nA power-transformed and threshold $\\mathrm{\\bfGARCH}(p,q)$ model $(\\mathrm{PTTGARCH}(p,q))$ is defined as $$X_{t}=\\sigma_{t}\\varepsilon_{t}\\quad\\mathrm{and}\\quad\\sigma_{t}^{2\\delta}=\\varnothing_{0}+\\sum_{i=1}^{p}\\alpha_{1i}(X_{t-i}^{+})^{2\\delta}+\\sum_{i=1}^{p}\\alpha_{2i}(X_{t-i}^{-})^{2\\delta}+\\sum_{j=1}^{q}\\beta_{j}\\sigma_{t-j}^{2\\delta},$$ where $\\delta,\\alpha_{0},\\alpha_{1i},\\alpha_{2i},\\varepsilon_{t}$ are the same as those in model (1.1), and $\\beta_{j}\\geqslant0,j=1,\\dotsc,q.$\n\nThe paper introduces a power-transformed and threshold GARCH (PTTGARCH) model, generalizing previous ARCH/GARCH specifications to accommodate nonlinear and asymmetric volatility dynamics. The model allows for heavy-tailed errors and provides estimators (QMLE and LADE) with asymptotic normality under varying moment conditions."
  },
  {
    "qid": "econ-empirical-1338-4-0-3",
    "question": "4) Prove the asymptotic tightness of the conditional distribution of $\\{n^{1/2}T_{n}^{0*}\\}$ given $\\mathcal{X}_{n}$ using Billingsley’s Theorem 15.6, as outlined in Lemma 3.",
    "gold_answer": "1. For $0\\leqslant\\tau_{0}<\\tau_{1}<\\tau_{2}\\leqslant1$, show: $$\\mathrm{E}[|T_{n}^{0*}(\\tau_{1})-T_{n}^{0*}(\\tau_{0})|^{2}|T_{n}^{0*}(\\tau_{2})-T_{n}^{0*}(\\tau_{1})|^{2}|\\mathcal{L}_{n}]\\leqslant|\\mathrm{H}_{n}(\\tau_{2})-\\mathrm{H}_{n}(\\tau_{0})|^{2}.$$\n2. Expand the left-hand side using independence: $$=\\frac{1}{n^{2}}\\sum_{i=[n\\tau_0]+1}^{[n\\tau_1]}\\sum_{j=[n\\tau_1]+1}^{[n\\tau_2]}U_{n i}^{0}U_{n j}^{0}.$$\n3. Bound by monotonicity of $H_n(\\tau)=\\frac{1}{n}\\sum_{i=1}^{[n\\tau]}U_{n i}^{02}$: $$\\leqslant [H_n(\\tau_2)-H_n(\\tau_0)]^2.$$\n4. By A10, $H_n(\\tau)\\to S(\\tau)$ a.s., where $S$ is continuous and monotone, proving tightness.",
    "question_context": "Let $\\left\\{\\xi_{n},n\\geqslant1\\right\\}$ be a sequence of random variables. We say that $\\xi_{n}=\\mathbf{o}_{\\mathbf{p}^{*}}(1)$ in probability (with probability 1) if for any constant $\\delta>0$, $$\\operatorname*{Pr}\\{\\|\\xi_{n}\\|>\\delta|{\\mathcal{X}}_{n}\\}{\\overset{\\mathrm{p}}{\\longrightarrow}}0~({\\longrightarrow}0~\\mathrm{with~probability~1}).$$\nFirst, we show that $$n^{1/2}\\operatorname*{sup}_{\\tau\\in[0,1]}\\|\\tilde{T}_{n}^{*}(\\tau)-T_{n}^{0*}(\\tau)\\|=\\mathbf{o}_{\\mathbb{p}^{*}}(1).$$ Note that $$\\tilde{T}_{n}^{*}(\\tau)-T_{n}^{0*}(\\tau)=\\frac{1}{n}\\sum_{i=1}^{[n\\tau]}[U_{n i}(\\tilde{\\theta}_{n})-U_{n i}(\\theta_{0})]V_{i}.$$\nLemma 1 shows that the covariance function, given ${\\mathcal{X}}_{n}$, of $\\{n^{1/2}T_{n}^{0*}\\}$ converges to the covariance function of $\\{N_{k}^{0}\\}$ a.s. Lemma 2 shows that the finite-dimensional distributions (fidis) of $\\left\\{n^{1/2}T_{n}^{0*}\\right\\}$, conditional on ${\\mathcal{X}}_{n}$, converge to the corresponding distributions of $\\{N_{k}^{0}\\}$ a.s. Lemma 3 shows that the conditional distribution of $\\{n^{1/2}T_{n}^{0*}\\}$ given ${\\mathcal{X}}_{n}$ is tight with probability 1.\nFor tightness in $D$, it suffices to check Billingsley (1968; Theorem 15.6). That is, we must show that, for $0\\leqslant\\tau_{0}<\\tau_{1}<\\tau_{2}\\leqslant1$, $$\\mathrm{E}[|T_{n}^{0*}(\\tau_{1})-T_{n}^{0*}(\\tau_{0})|^{2}|T_{n}^{0*}(\\tau_{2})-T_{n}^{0*}(\\tau_{1})|^{2}|\\mathcal{L}_{n}]\\leqslant|\\mathrm{H}_{n}(\\tau_{2})-\\mathrm{H}_{n}(\\tau_{0})|^{2},$$ for some monotone function $H_{n}$, which may depend on ${\\mathcal{X}}_{n}$.\n\nThis section provides detailed proofs for Theorem 2 and Corollary 1, leveraging the mean value theorem (MVT) and establishing asymptotic properties of the proposed estimators under specific conditions."
  },
  {
    "qid": "econ-empirical-1749-1-2-0",
    "question": "5) Derive the volatility risk premium $\\mathrm{vrp}_{t}$ from the variance risk premium $\\mathrm{VRP}_{t}$ and explain the interpretation of the sign of $\\mathrm{vrp}_{t}$.",
    "gold_answer": "1. The variance risk premium is:\n   $$ \\mathrm{VRP}_{t} = \\mathbb{E}_{t}^{\\mathbb{P}}\\left[\\int_{t}^{T} \\sigma_{u}^{2} d u\\right] - \\mathbb{E}_{t}^{\\mathbb{Q}}\\left[\\int_{t}^{T} \\sigma_{u}^{2} d u\\right]. $$\n2. The volatility risk premium is obtained by taking the square root of each expectation:\n   $$ \\mathrm{vrp}_{t} = \\sqrt{\\mathbb{E}_{t}^{\\mathbb{P}}\\left[\\int_{t}^{T} \\sigma_{u}^{2} d u\\right]} - \\sqrt{\\mathbb{E}_{t}^{\\mathbb{Q}}\\left[\\int_{t}^{T} \\sigma_{u}^{2} d u\\right]}. $$\n3. The sign of $\\mathrm{vrp}_{t}$ reflects investors' willingness to pay to hedge volatility risk. A negative sign indicates that investors are paying a premium to hedge against high volatility.",
    "question_context": "The difference between the two represents the market price of volatility risk, or what is referred to as the “volatility risk premium.” In some instances it is easier to work with the squared volatility, which leads to the variance risk premium defined as follows: \n\n$$ \\mathrm{VRP}_{t}=\\mathbb{E}_{t}^{\\mathbb{P}}\\left[\\int_{t}^{T}\\sigma_{u}^{2}d u\\right]-\\mathbb{E}_{t}^{\\mathbb{Q}}\\left[\\int_{t}^{T}\\sigma_{u}^{2}d u\\right]. $$\nTaking the square root of each of the expectations in Equation (15) gives the volatility risk premium, which we will denote as a lowercase $\\mathrm{vrp}_{t}$ . While we present all of our results in terms of the volatility risk premium, everything is robust with respect to the variance risk premium. The reason we choose to use the former is that the results are easier and more natural to interpret in terms of volatility units.\n\nImplied volatility is derived from option prices and represents the market's expectation of future volatility. The volatility risk premium is the difference between realized volatility and implied volatility, reflecting the market price of volatility risk."
  },
  {
    "qid": "econ-empirical-1627-4-0-0",
    "question": "1) Derive the convergence result for the term $\\frac{R}{\\sqrt{T}}\\frac{1}{\\sqrt{T}}\\sum_{t=1}^{T}\\sum_{j=1}^{J_{t}}z_{j t}^{\\prime}\\left(e_{j t}^{\\prime}H_{0t}^{-1}\\left(\\frac{\\partial e_{R t}}{\\partial\\xi}\\right)H_{0t}^{-1}e_{R t}-\\frac{1}{2}e_{R t}^{\\prime}I_{0j t}e_{R t}\\right)$ under Assumptions B.6–B.8 and B.11, Chebyshev’s LLN, and Lyapunov’s CLT.",
    "gold_answer": "1. **Apply Chebyshev’s LLN**: Under Assumptions B.6–B.8, the term inside the summation is bounded and has finite variance. Chebyshev’s LLN ensures that the sample mean converges in probability to its expected value.\n2. **Lyapunov’s CLT**: For the scaled sum $\\frac{1}{\\sqrt{T}}\\sum_{t=1}^{T}\\sum_{j=1}^{J_{t}}z_{j t}^{\\prime}(\\cdot)$, Lyapunov’s CLT ensures convergence to a normal distribution if the Lyapunov condition is satisfied (which it is under the given assumptions).\n3. **Combine with $\\frac{R}{\\sqrt{T}}$**: Multiplying by $\\frac{R}{\\sqrt{T}}$ scales the variance, leading to convergence in probability to a constant $\\bar{\\mu}_{1}$ as $T \\to \\infty$ and $R \\to \\infty$.",
    "question_context": "By Assumptions B.6–B.8 and B.11, Chebyshev’s law of large numbers (LLN), and Lyapunov’s CLT \n$$\n\\frac{R}{\\sqrt{T}}\\frac{1}{\\sqrt{T}}\\sum_{t=1}^{T}\\sum_{j=1}^{J_{t}}z_{j t}^{\\prime}\\left(e_{j t}^{\\prime}H_{0t}^{-1}\\left(\\frac{\\partial e_{R t}}{\\partial\\xi}\\right)H_{0t}^{-1}e_{R t}-\\frac{1}{2}e_{R t}^{\\prime}I_{0j t}e_{R t}\\right)\\overset{p}{\\rightarrow}\\bar{\\mu}_{1},\n$$ \nwhich implies that the second term times $\\frac{R}{\\sqrt{T}}$ converges in probability to a constant.\nNext I will prove that \n$$\n{\\frac{1}{\\sqrt{T}}}\\sum_{t=1}^{T}z_{t}^{\\prime}e r r o r_{t}=o_{p}\\left({\\frac{\\sqrt{T}}{R}}\\right).\n$$ \nRecall that \n$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{\\sqrt{2}}\\sum_{i=1}^{n}\\xi_{i}\\xi_{i}\\xi_{i}\\xi_{i}\\xi_{i}\\xi_{i}}&{}\\ {\\displaystyle=-\\frac{1}{\\sqrt{T}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\xi_{j}\\xi_{j}\\xi_{k}^{\\prime}(\\mu_{k}^{(i)^{\\prime}}-\\mu_{k}^{(i)^{\\prime}})\\left(\\frac{\\partial u_{k}}{\\partial u_{k}}\\right)_{t\\xi_{i}}\\xi_{k}\\xi_{i}}\\ {\\displaystyle}&{-\\frac{1}{2}\\prod_{j=1}^{n}\\sum_{j=1}^{n}\\xi_{j}\\xi_{j}\\xi_{k}^{\\prime}\\xi_{i}\\xi_{i}}\\ {\\displaystyle}&{\\times\\left(\\sum_{j=1}^{n}\\left(\\frac{\\partial^{\\prime}\\eta_{j}\\left(\\Phi_{i},\\Phi_{i},\\Phi_{j},\\xi_{i}),\\mathfrak{h}_{j}\\right)}{\\partial\\xi_{j}\\partial\\xi_{k}}\\right)\\alpha_{i}^{\\prime}\\eta_{j}\\xi_{i}\\xi_{i}^{\\prime}}\\ {\\displaystyle}&{-\\frac{1}{\\sqrt{T}}\\sum_{i=1}^{n}\\left(\\frac{\\partial^{\\prime}\\eta_{j}\\left(\\Phi_{i},\\Phi_{i},\\xi_{i}\\right),\\mathfrak{h}_{j}\\right)}{\\partial\\xi_{j}\\partial\\xi_{k}}\\right)_{t\\xi_{i}}\\xi_{i}^{\\prime}\\xi_{i}\\xi_{i}^{\\prime}\\xi_{i}^{\\prime}\\xi_{i}^{\\prime}\\right)\\right)\\xi_{\\boldsymbol{\\theta}\\boldsymbol{\\theta}\\boldsymbol{\\theta}\\boldsymbol{\\theta}\\boldsymbol{\\theta}\\boldsymbol{\\theta}\\boldsymbol{\\xi}}}\\ {\\displaystyle}&{+\\frac{1}{\\sqrt{T}}\\sum_{i=1}^{n}\\xi_{i}\\xi_{i}\\xi_{i}\\xi_{i}\\xi_{i}\\xi_{i}\\xi_{i}\\xi_{i}\\xi_{i}\\xi_{i}^{\\prime}\\xi_{i}\\xi_{i}^{\\prime}\\xi_{i}^{\\prime}\\xi_{i}^{\\prime}\\xi_{i}^{\\prime}\\xi_{i}^{\\prime}\\xi_{i}^{\\prime}\\xi_{i}^{\\prime}\\xi_{i}^{\\prime}\\xi_{i}^{\\prime}\\xi_{i}^{\\prime}\\xi_{i}\n$$\nBy Assumptions B.5 and B.8, $H_{R t}^{-1}=H_{0t}^{-1}+o_{p}(1)$ where each element of the $J_{t}\\times J_{t}\\:o_{p}(1)$ term does not depend on $t$ . Similarly \n$$\n\\begin{array}{l}{{\\displaystyle\\sum_{k=1}^{J_{t}}H_{R t}^{-1}\\left(\\frac{\\partial^{2}\\sigma_{t}\\left(\\theta_{0},\\xi_{t}(\\theta_{0},P_{0t},s_{t}),P_{R t}\\right)}{\\partial\\xi\\partial\\xi_{k}}\\right)H_{R t}^{-1}e_{j t}e_{k t}^{\\prime}H_{R t}^{-1}}}\\\\ {{\\displaystyle~=\\sum_{k=1}^{J_{t}}H_{0t}^{-1}\\left(\\frac{\\partial^{2}\\sigma_{t}\\left(\\theta_{0},\\xi_{t}(\\theta_{0},P_{0t},s_{t}),P_{0t}\\right)}{\\partial\\xi\\partial\\xi_{k}}\\right)}}\\\\ {{\\displaystyle~\\times H_{0t}^{-1}e_{j t}e_{k t}^{\\prime}H_{0t}^{-1}+o_{p}(1)},}\\end{array}\n$$ \nwhere each element of the $J_{t}\\times J_{t}\\:o_{p}(1)$ term does not depend on t.\n\nThe text discusses advanced econometric models involving Chebyshev’s law of large numbers (LLN) and Lyapunov’s CLT, focusing on convergence properties and bias correction in the context of complex mathematical formulations."
  },
  {
    "qid": "econ-empirical-686-4-0-3",
    "question": "4) Explain the role of the smoothness assumption (A.2) in the proof of Proposition 6 and its absence in Proposition 8.",
    "gold_answer": "1. **Proposition 6**: Smoothness ensures $\\alpha_i(b)$ is well-defined and continuous, enabling Brouwer's theorem.  \n2. **Proposition 8**: Smoothness guarantees $\\lambda_S(\\rho) \\to \\lambda_S$ as $\\rho \\to 1$, critical for localizing the hyperplane approximation.  \n3. **Contrast**: Proposition 6 does not require smoothness for existence, while Proposition 8 relies on it for convergence.",
    "question_context": "THEOREM 5: Suppose that $(N,V)$ is an NTU coalitional form satisfying the assumptions (A.1), $(A.2)$ ,and $\\left(A.3\\right)$ . Then for each $0\\leq\\rho<1$ there is an $_{S P}$ equilibrium.Moreover,as $\\rho\\to1$ every limit point of $s P$ equilibriumpayoffconfigurations is a consistent value payoff configuration of $(N,V)$.\nPROPOSITION 6: Let $(N,V)$ be an NTU form. Then for each $0\\leq\\rho<1$ there is an $s P$ equilibrium.\nPROPOSITION 7: Let $(N,V)$ be a hyperplane form. Then for each $0\\leq\\rho<1$ there is a unique $s P$ equilibrium. Moreover the SP equilibrium payoff configuration equals the unique consistent value payoff configuration of $(N,V)$.\nPROPOSITION 8: Let $(N,V)$ beanNTU form.If $\\pmb{a}(\\rho)$ is an $s P$ equilibrium payoffconfigurationforeach $\\pmb{\\rho}$ andaisalimitpointof $\\pmb{a}(\\rho)$ as $\\rho\\to1$ , then a is $a$ consistentvaluepayoffconfigurationof $(N,V)$.\n\nThis section studies the equilibria of the noncooperative game in the general NTU case, presenting Theorem 5 and its proof through Propositions 6, 7, and 8."
  },
  {
    "qid": "econ-empirical-527-1-0-3",
    "question": "4) For the mixture model $W = \\pi W_0 + (1-\\pi) Q$, where $W_0$ is stable ($\\phi_{W_0}(t) = \\exp(-c_1 |t|^\\alpha)$) and $Q$ is normal ($\\phi_Q(t) = \\exp(-c_2 t^2)$), show how (2.19) holds with $c = c_1 \\pi$ and $\\lambda_1 = -\\log \\pi$.",
    "gold_answer": "1. **Mixture characteristic function**: \n   $$ \n   \\phi_W(t) = \\pi \\exp(-c_1 |t|^\\alpha) + (1-\\pi) \\exp(-c_2 t^2). \n   $$ \n2. **High-frequency behavior**: As $|t| \\to \\infty$, the normal term decays faster, so: \n   $$ \n   \\log \\phi_W(t) = \\log[\\pi \\exp(-c_1 |t|^\\alpha) \\{1 + o(1)\\}] = -c_1 |t|^\\alpha - \\lambda_1 + o(1), \n   $$ \n   where $\\lambda_1 = -\\log \\pi$ and $c = c_1 \\pi$.",
    "question_context": "The Hill (1975) estimator is a very popular method for estimating exponents of regular variation, although other techniques are also used. See also Csörgő et al. (1985) and Groeneboom et al. (2003), who developed kernel methods, and Aït-Sahalia and Jacod (2009) and Belomestny (in press), who considered alternative approaches for models that are moderately close to our own. To construct the Hill estimator, rank the absolute values $|X_{j}|$ of the data as $|X_{(n)}|\\geq|X_{(n-1)}|\\geq\\cdot\\cdot\\cdot$ , and take \n\n$$ \n\\hat{\\alpha}=\\bigg(\\frac{1}{r}\\sum_{j=1}^{r}\\log|X_{(n-j+1)}/X_{(n-r)}|\\bigg)^{-1}, \n$$ \n\nwhere choice of $r$ is at the discretion of the experimenter (see below). The ratio $r/n$ is often referred to the ‘‘sample fraction’’.\nIt is known (see e.g. Hall, 1982) that if the distribution of $|X|$ satisfies \n\n$$ \nP(|X|>x)=D_{1}x^{-\\alpha}\\big\\{1+O\\big(x^{-D_{2}}\\big)\\big\\} \n$$ \n\nas $x\\rightarrow\\infty$ , for constants $D_{1},D_{2}>0$ , and if $r$ is chosen so that $r/n^{\\eta_{1}}\\to\\infty$ and $r/n^{1-\\eta_{2}}\\to0$ for constants $\\eta_{1},\\eta_{2}>0$ , then there exists $\\eta>0$ such that \n\n$$ \n\\hat{\\alpha}=\\alpha+O_{p}\\left(n^{-\\eta}\\right). \n$$ \n\nThe asymptotically optimal choice of $r$ , as a function of $\\alpha$ and $D_{2}$ , is $r\\sim$ const. $n^{2D_{2}/(2\\mathbf{\\bar{D}}_{2}+\\alpha)}$ , and for this choice, $\\eta~=~D_{2}/(2D_{2}~+$ $\\alpha\\mathrm{.}$ ).\nIn so-called ‘‘supersmooth’’ cases, where the characteristic function $\\chi$ decreases at rate $\\exp(-c_{1}|t|^{\\beta})$ as $|t|\\rightarrow\\infty$ , with $\\beta>$ $\\alpha$ , the value of $\\alpha$ can be estimated at rate $O_{p}(n^{\\eta-(1/2)})$ for each $\\eta>0$ . The procedure for inference amounts to treating the second regression in (2.8) as one that involves three unknown parameters, $c,\\alpha$ and $\\lambda$ , and estimating those quantities simultaneously by least-squares.\nAn example of a distribution of W for which (2.19) holds is one where $W$ is a mixture of a nonnormal, stably distributed random variable with characteristic function $\\exp(-c_{1}|t|^{\\alpha})$ , and a normally distributed variate with characteristic function $\\exp(-c_{2}t^{2})$ , in proportions $\\pi$ and $1-\\pi$ , respectively. There (2.19) holds with $c=c_{1}\\pi$ and $\\lambda_{1}=-\\log\\pi$ .\n\nThe text discusses methodologies for estimating the exponent of regular variation α, particularly focusing on the Hill estimator and its properties under different distributional assumptions. It also generalizes the stable-law model to accommodate more complex features in financial data."
  },
  {
    "qid": "econ-empirical-1029-1-0-3",
    "question": "4) Interpret Theorem 2 in terms of approximating the efficient score $S$ by $\\pi(x)\\rho$. How does this relate to the efficiency of the GMM estimator?",
    "gold_answer": "1. Theorem 2 states: \\n$$V^{-1}-V_{J}^{-1}=\\min_{\\pi(x):E[\\pi(x)H(x)]=0}E[\\{S-\\pi(x)\\rho\\}\\{S-\\pi(x)\\rho\\}^{\\prime}].$$ \\n2. The efficient score $S$ is the projection of the score for $\\beta$ onto $T^{\\perp}$. \\n3. The term $\\pi(x)\\rho$ approximates $S$, and the minimization represents the residual variance. \\n4. As $J\\to\\infty$, $\\pi_{J}(x)\\rho$ approximates $S$ arbitrarily well, driving $V_{J}\\to V$, the semiparametric bound.",
    "question_context": "The asymptotic variance of $\\tilde{\\theta}$ will be $$\\Sigma_{J}=[I,0]\\{E[G(x)^{\\prime}\\varOmega(x)^{-}G(x)]\\}^{-1}[I,0]^{\\prime}.$$\nThe spanning condition will be sufficient for $\\Sigma_{\\rho}$ to equal the semiparametric variance bound.\nThe model tangent set $T$ is the closed linear span of the set of scores for parametric submodels.\nThe moment tangent set $T_{\\rho}$ is given by $$T_{\\rho}=\\bigcap_{J=1}^{\\infty}T_{J}.$$\nSPANNING CONDITION: $T_{\\rho}=T$\nTHEOREM 1: If Assumptions 1 and 2 and the spanning condition are satisfied, then $\\operatorname*{lim}_{J\\to\\infty}\\boldsymbol{\\Sigma}_{J}=\\boldsymbol{\\Sigma}_{\\rho}$ is the semiparametric bound.\nTHEOREM 2: If Assumptions 1 and 2 are satisfied, then $$V^{-1}-V_{J}^{-1}=\\operatorname*{min}_{\\pi(x):E[\\pi(x)H(x)]=0}E[\\{S-\\pi(x)\\rho\\}\\{S-\\pi(x)\\rho\\}^{\\prime}],$$ and $V_{J}\\to V$ if and only if for each $J$ there is $\\pi_{J}(x)$ with $E[\\pi_{J}(x)H(x)]=0$ such that $E[\\|S-\\pi_{J}(x)\\rho\\|^{2}]\\to0$ as $J\\to\\infty$.\nTHEOREM 3: If Assumptions 1 and 2 are satisfied, then $T_{\\rho}=M^{\\perp}$ and the spanning condition is satisfied if and only if $T^{\\perp}=M$.\n\nThe spanning condition is critical for the GMM estimator to attain semiparametric efficiency. It ensures that the moment conditions characterize the semiparametric model, allowing the GMM estimator to exploit all information in the model. The condition is formulated in terms of tangent spaces, representing directions of departure from the truth allowed by the model or moment conditions."
  },
  {
    "qid": "econ-empirical-664-2-0-0",
    "question": "1) Prove that the sensitivity of workers' pay to output ($\\theta_{y}^{i}(C)$) is decreasing in the subjectivity level $q$, using the definition provided and the optimal contracts for $q=0$ and $q=1$.",
    "gold_answer": "1. For $q=1$, the optimal contract yields $\\theta_{y}^{i}(C_{w}^{1}) = \\frac{1}{2}$.  \n2. For $q=0$, the optimal contract yields $\\theta_{y}^{i}(C_{w}^{0}) = \\frac{3}{8}$.  \n3. Since $\\frac{1}{2} > \\frac{3}{8}$, $\\theta_{y}^{i}(C)$ decreases as $q$ decreases.  \n4. By Theorem 1, this holds for all $q \\in [0,1]$.",
    "question_context": "If the two sensitivities change in the same direction as we change the subjectivity level, $q$, then the two performance measures are like complements; otherwise, they are like substitutes.\nThe sensitivity of workers’ pay to output decreases with the level of subjectivity, as does the sensitivity of workers’ pay to their evaluations.\nThe sensitivity of a player’s pay to output is measured by the extent to which their payoff changes with output: $\\theta_{y}^{i}(C)=\\frac{\\mathbb{E}[w_{i}(\\bar{y})|e]}{\\bar{y}}$.\nThe sensitivity of a player’s pay to evaluations is measured by the extent to which their payoff changes with evaluations: $\\theta_{r_{i},r_{i}^{\\prime}}^{i}(C)=\\mathbb{E}_{r_{j}}[w_{i}(r_{i},r_{j},\\bar{y})-w_{i}(r_{i}^{\\prime},r_{j},\\bar{y})|e]$.\n\nThis section examines how the sensitivity of workers' pay to output and evaluations changes with the subjectivity level, determining whether these performance measures act as complements or substitutes."
  },
  {
    "qid": "econ-empirical-1588-3-0-2",
    "question": "3) Critically evaluate the empirical robustness of the claim that aging drives robot adoption, considering potential outliers (e.g., Korea) and heterogeneity across OECD countries.",
    "gold_answer": "1. **Outlier Analysis**: Korea's outlier status may reflect unique industrial policies or concentrated robotics investments. Excluding it tests whether the relationship holds generally. \\n2. **OECD Heterogeneity**: Within-OECD results control for shared institutional factors, strengthening causal inference. \\n3. **Confounding Factors**: Alternative explanations (e.g., global supply chains) must be ruled out to isolate aging effects. \\n4. **Policy Implications**: If robust, the findings suggest aging societies should prioritize automation-friendly policies.",
    "question_context": "Why is there not a strong negative relationship between aging and GDP per capita as predicted by a range of theories, including recent ones on secular stagnation? One possible answer is that technology adjusts so as to undo this potential negative effect.\nAcemoglu and Restrepo (2017) document that this cross-country pattern is robust; it holds if we exclude Korea (a clear outlier) and it holds within the OECD countries. Crucially, as would be expected from a simple model of directed technological change, we also show that it is more pronounced in industries that employ younger workers and those in which there are more opportunities for automation.\n\nThe text explores the relationship between aging populations and GDP per capita, suggesting that technology, particularly automation through industrial robots, may counteract the negative effects predicted by theories of secular stagnation. It references Acemoglu and Restrepo (2017) to support the argument that countries with faster aging rates are leading in the adoption of industrial robots."
  },
  {
    "qid": "econ-empirical-327-2-0-3",
    "question": "4) Based on the election data, propose a game-theoretic model to explain the observed negative correlation between the number of nominees and the number of elected Fellows under the approval system. Specify the players, strategies, and payoffs.",
    "gold_answer": "1. Players: Voting Fellows.\n2. Strategies: Each Fellow approves or disapproves each nominee.\n3. Payoffs: Utility depends on the alignment of elected Fellows with their preferences.\n4. Model: Symmetric equilibrium where each Fellow approves nominees with probability \\( p \\).\n5. The negative correlation arises because as \\( N \\) increases, \\( p \\) decreases to maintain a stable \\( E \\).",
    "question_context": "In 1991 we continued the new method of electing Fellows, initiated in 1989. Table V provides data on the participation of Fellows in the voting process; the percent returning ballots in 1991 jumped to 63.5 percent, the highest since 1981. Table VI displays information on the number of nominees and newly elected Fellows. The new post-1988 system has achieved a significant increase in the number of Fellows elected, by coincidence exactly 23 in each year under the new system.\nI have noted in previous reports the strong negative correlation between the number of nominees and the number elected under the approval system that has been in effect since 1984, and this continues to be evident in Table VI (in 1983 and prior years the voters were allowed to choose the number elected independent of the number of nominees).\nTABLEII TOTALMEMBERS ANDINSTITUTIONS, ECONOMETRIC SOCIETY COMPARED WITH AMERICAN ECONOMIC ASSOCIATION\nTABLEIII GEOGRAPHICAL DISTRIBUTION OF MEMBERS AND STUDENTS SELECTED YEARS,AS OF M1DYEAR,1976-1992\nTABLEIV PERCENTAGEDISTRIBUTION OFMEMBERS AND STUDENTS, AS OF M1DYEAR, 1976-1992\n\nThe text discusses the election process of Fellows in the Econometric Society, highlighting changes in participation rates and the number of nominees and elected Fellows. It also provides comparative data on membership and institutional affiliations between the Econometric Society and the American Economic Association, as well as geographical distribution of members over selected years."
  },
  {
    "qid": "econ-empirical-518-0-0-3",
    "question": "4) Analyze the impact of introducing competition in a nonhub market (e.g., $AB$) on fares in other markets using the model. Why do fares rise in markets without competition?",
    "gold_answer": "1. **Competition in $AB$**:\n   - Reduces the monopolist's traffic on spokes $AH$ and $BH$ (traffic leakage).\n   - Lower $Q$ on spokes raises marginal cost ($c^{\\prime}$ increases due to $c^{\\prime\\prime} < 0$).\n\n2. **Fare effects**:\n   - In $AB$, competitive pressure counteracts higher $c^{\\prime}$, lowering fares.\n   - In other markets (e.g., $AC$, $AH$), no competition exists, so higher $c^{\\prime}$ leads to higher fares.\n\n3. **Intuition**: Cost complementarities mean competition in one market negatively impacts others via higher marginal costs.",
    "question_context": "Suppose that a monopoly airline operates the four-city network depicted in Figure 1, where city $H$ is the hub. Residents of each city have a demand for air travel to every other city of the network, including the hub. Demand is symmetric across city pairs, with $D(Q)$ giving the inverse demand function for round-trip travel in each market.\nCosts are represented by the function $c(Q)$, which gives the total cost of carrying round-trip traffic volume of $Q$ on a particular spoke of the network. With economies of density, $c$ satisfies $c^{\\prime}>0$ and $c^{\\prime\\prime}<0$ (the spoke cost function thus reflects increasing returns to scale).\nThe monopolist's first-order conditions for profit maximization in hub-inclusive and nonhub markets are given by $R^{\\prime}(Q_{A H})=c^{\\prime}(Q_{A B}+Q_{A C}+Q_{A H})$ and $R^{\\prime}(Q_{A B})=c^{\\prime}(Q_{A B}+Q_{A C}+Q_{A H})+c^{\\prime}(Q_{A B}+Q_{B C}+Q_{B H})$, respectively.\n\nThe article presents a theoretical model of a hub-and-spoke network operated by a monopoly airline. The model assumes symmetric demand across city pairs and economies of density on the spokes, leading to lower costs per passenger as traffic density increases."
  },
  {
    "qid": "econ-empirical-501-2-1-0",
    "question": "5) Formally test whether the better fit of the SV model with $m=10$ is statistically significant compared to $m=0$ or $m=1$, using the FTSE 100 data.",
    "gold_answer": "- Use a likelihood ratio test: $LR = 2(\\ell_{m=10} - \\ell_{m=0}) \\sim \\chi^2_{10}$.\n- If $LR > \\chi^2_{10,\\alpha}$, reject $H_0$ (no improvement).\n- Adjust for finite-sample bias via Bartlett correction.",
    "question_context": "Figure 1 compares the empirical and estimated $\\mathrm{var}[y_{t}|y_{t-s}<0]$ for $s=1,\\ldots,100$ for the FTSE 100 index daily standardized logarithmic returns for $m=0$, $m=1$, and $m=10$.\nThe specification with $m=0$ is from Jacquier, Polson, and Rossi (2004), the one with $m=1$ is the encompassing model discussed by Yu (2005).\nReturns are collected from Yahoo finance over the period from 3 January 1990 to 31 December 2019. The number of observations is: 7558 for SP500, 7606 for FTSE, 7367 for N225, and 7403 for HSI.\n\nThe empirical analysis compares the fit of SV models with varying $m$ for financial indices (FTSE 100, SP500, Nikkei 225, HSI). Higher $m$ captures leverage effects more flexibly."
  },
  {
    "qid": "econ-empirical-200-1-1-0",
    "question": "3) Derive the New Keynesian Phillips curve from the intermediate goods-producing firm's dynamic optimization problem, given the production function $Y_{t}(i)=Z_{t}h_{t}(i)$ and the price adjustment cost \n\n$$\n\\frac{\\phi}{2}\\left[\\frac{P_{t}(i)}{\\pi P_{t-1}(i)}-1\\right]^{2}Y_{t}.\n$$",
    "gold_answer": "1. The firm's problem is to choose $P_{t}(i)$ to maximize the present discounted value of profits:\n   $$\n   \\mathbb{E}_{t}\\sum_{s=0}^{\\infty}\\beta^{s}\\left[\\frac{P_{t+s}(i)}{P_{t+s}}Y_{t+s}(i) - \\frac{W_{t+s}}{P_{t+s}}h_{t+s}(i) - \\frac{\\phi}{2}\\left(\\frac{P_{t+s}(i)}{\\pi P_{t+s-1}(i)}-1\\right)^{2}Y_{t+s}\\right].\n   $$\n2. Substitute the production function and demand curve $Y_{t+s}(i) = \\left(\\frac{P_{t+s}(i)}{P_{t+s}}\\right)^{-\\theta}Y_{t+s}$:\n   $$\n   \\mathbb{E}_{t}\\sum_{s=0}^{\\infty}\\beta^{s}\\left[\\left(\\frac{P_{t+s}(i)}{P_{t+s}}\\right)^{1-\\theta}Y_{t+s} - \\frac{W_{t+s}}{P_{t+s}Z_{t+s}}\\left(\\frac{P_{t+s}(i)}{P_{t+s}}\\right)^{-\\theta}Y_{t+s} - \\frac{\\phi}{2}\\left(\\frac{P_{t+s}(i)}{\\pi P_{t+s-1}(i)}-1\\right)^{2}Y_{t+s}\\right].\n   $$\n3. Take the first-order condition with respect to $P_{t}(i)$ and log-linearize around the steady state to obtain the New Keynesian Phillips curve:\n   $$\n   \\hat{\\pi}_{t} = \\beta\\mathbb{E}_{t}\\hat{\\pi}_{t+1} + \\frac{(\\theta-1)}{\\phi}\\widehat{mc}_{t},\n   $$\n   where $\\hat{\\pi}_{t}$ is inflation and $\\widehat{mc}_{t}$ is real marginal cost.",
    "question_context": "The representative intermediate goods-producing firm hires $h_{t}(i)$ units of labor from the representative household to manufacture $Y_{t}(i)$ units of intermediate good $i$ according to the constant-returns-to-scale technology described by \n\n$$\nY_{t}(i)=Z_{t}h_{t}(i).\n$$\n\nThe aggregate technology shock $Z_{t}$ follows a random walk with positive drift: \n\n$$\n\\ln(Z_{t})=\\ln(z)+\\ln(Z_{t-1})+\\varepsilon_{z t},\n$$\n\nwhere $z>1$ and the serially uncorrelated innovation $\\varepsilon_{z t}$ has mean zero and standard deviation $\\sigma_{z}$.\nThe intermediate goods-producing firm faces a quadratic cost of adjusting its nominal price, measured in units of the finished good and given by \n\n$$\n\\frac{\\phi}{2}\\left[\\frac{P_{t}(i)}{\\pi P_{t-1}(i)}-1\\right]^{2}Y_{t},\n$$\n\nwhere the parameter $\\phi\\geq0$ governs the magnitude of the price adjustment costs and where $\\pi>1$ denotes the gross, steady-state inflation rate.\n\nThe representative intermediate goods-producing firm hires labor to manufacture intermediate goods according to a constant-returns-to-scale technology. It faces a quadratic cost of adjusting its nominal price, leading to a dynamic optimization problem."
  },
  {
    "qid": "econ-empirical-223-2-0-3",
    "question": "4) Show how the test statistics CM̂_N and KŜ_N are constructed from the empirical process Ĵ_N(t). Discuss their asymptotic properties under H₀.",
    "gold_answer": "1. Ĵ_N(t) = N⁻¹/²∑ᵢ₌₁ᴺ Ûᵢℋ(Tᵢ,t), where Ûᵢ = π̂ₖ(Tᵢ,Xᵢ)m{Yᵢ;g(Tᵢ;θ̂)}.\n2. CM̂_N = N⁻¹∑ᵢ₌₁ᴺ{Ĵ_N(Tᵢ)}² aggregates squared deviations over the sample.\n3. KŜ_N = supₜ∈τ|Ĵ_N(t)| captures the maximal deviation.\n4. Under H₀, Ĵ_N(t) converges weakly to a Gaussian process, implying:\n   - CM̂_N → ∫{Z(t)}²dFₜ(t) for a Gaussian process Z(t).\n   - KŜ_N → supₜ∈τ|Z(t)|.",
    "question_context": "One obvious approach for estimating the Uᵢ’s is to estimate fₜ(Tᵢ) and fₜ|ₓ(Tᵢ|Xᵢ), then construct the estimators of π₀(Tᵢ,Xᵢ) and θ*. However, it is well known that this ratio estimator of π₀(T,X) is very sensitive to small values of fₜ|ₓ(T|X) because small estimation errors in estimating fₜ|ₓ(T|X) result in large estimation errors of the estimator of π₀(T,X).\nThe weighting function satisfies 𝔼{π₀(T,X)u(T)ν(X)} = 𝔼{u(T)}·𝔼{ν(X)} for any suitable functions u(t) and ν(x).\nThe functions uₖ₁(t) and νₖ₂(x) are called the approximation sieves, such as B-splines or power series.\nThe entropy maximization problem minimizes the Kullback–Leibler divergence between the weights {N⁻¹πᵢ}ᵢ₌₁ᴺ and the empirical frequencies {N⁻¹}, subject to the sample analogue of Equation (9).\nThe first-order condition of Equation (12) implies that {π̂ₖ(Tᵢ,Xᵢ)}ᵢ₌₁ᴺ satisfies the sample analog of (9); such restrictions reduce the chance of obtaining extreme weights.\nUnder H₀, the true value θ* solves the following equation: 𝔼[π₀(T,X)m{Y;g(T;θ)}w(T;θ)] = 0.\nThe corresponding estimators of the Cramér–von Mises (CM)-type statistic and the Kolmogorov-Smirnov (KS)-type statistic are, respectively, CM̂_N = N⁻¹∑ᵢ₌₁ᴺ{Ĵ_N(Tᵢ)}² and KŜ_N = supₜ∈τ|Ĵ_N(t)|.\n\nThis section discusses the estimation of the weighting function π₀(T,X) using generalized empirical likelihood (GEL) and the construction of test statistics for hypothesis testing."
  },
  {
    "qid": "econ-empirical-588-1-0-0",
    "question": "1) Derive the Prais-Winsten transformed equation $y_t^* = \\alpha^* + \\beta X_t^* + \\varepsilon_t$ from the original model $y_t = \\alpha + \\beta X_t + \\mu_t$ with $\\mu_t = \\rho \\mu_{t-1} + \\varepsilon_t$. Show all steps.",
    "gold_answer": "1. Start with the original model: $y_t = \\alpha + \\beta X_t + \\mu_t$ with $\\mu_t = \\rho \\mu_{t-1} + \\varepsilon_t$.\n2. Lag the equation by one period: $y_{t-1} = \\alpha + \\beta X_{t-1} + \\mu_{t-1}$.\n3. Multiply the lagged equation by $\\rho$: $\\rho y_{t-1} = \\rho \\alpha + \\rho \\beta X_{t-1} + \\rho \\mu_{t-1}$.\n4. Subtract the multiplied lagged equation from the original: $y_t - \\rho y_{t-1} = \\alpha(1-\\rho) + \\beta(X_t - \\rho X_{t-1}) + (\\mu_t - \\rho \\mu_{t-1})$.\n5. Recognize that $\\mu_t - \\rho \\mu_{t-1} = \\varepsilon_t$ by definition.\n6. Define $y_t^* = y_t - \\rho y_{t-1}$, $X_t^* = X_t - \\rho X_{t-1}$, and $\\alpha^* = \\alpha(1-\\rho)$.\n7. For $t=1$, adjust for initial conditions: $y_1^* = y_1 \\sqrt{1-\\rho^2}$, $X_1^* = X_1 \\sqrt{1-\\rho^2}$.",
    "question_context": "Consider the following model: \n$$\n\\begin{array}{r l r}&{y_{t}=\\alpha+\\beta X_{t}+\\mu_{t},}&\\ &{X_{t}=\\lambda X_{t-1}+\\nu_{t},}&\\ &{\\mu_{t}=\\rho\\mu_{t-1}+\\varepsilon_{t},~}&{t=1,2,...,T,}\\end{array}\n$$\nwhere $y_{t}$ and $X_{t}$ are the tth observations on the dependent and independent variables, respectively, $x,\\beta,-1<\\lambda<1$ ,and $-1<\\rho<1$ are unknown parameters,and $\\nu_{t}$ and $\\varepsilon_{t}$ are independently and normally distributed with zero means and respective variances $\\sigma_{\\nu}^{2}$ and $\\sigma_{\\varepsilon}^{2}$.\nThe Prais-Winsten (PW) estimator steps are:\n(1) Estimate \n$$\ny_{t}=\\rho y_{t-1}+X_{t}\\beta-X_{t-1}\\beta\\rho+\\varepsilon_{t},\n$$\nby OLS to obtain $\\hat{\\boldsymbol\\rho}$ from the coefficient of $y_{t-1}$.\n(2) Transform the original equation as follows:\n$$\ny_{t}^{\\ast}=\\alpha^{\\ast}+\\beta X_{t}^{\\ast}+\\varepsilon_{t},\\qquad t=1,2,...,T,\n$$\nwhere \n$$\n\\begin{array}{r l}&{y_{1}^{*}=y_{1}(1-\\rho^{2})^{\\pm},}\\ &{X_{1}^{*}=X_{1}(1-\\rho^{2})^{\\pm},}\\ &{y_{t}^{*}=y_{t}-\\rho y_{t-1},}\\ &{X_{t}^{*}=X_{t}-\\rho X_{t-1}.}\\end{array}\n$$\n(3) Estimate the transformed system by OLS, replacing $\\rho$ with $\\hat{\\rho}$.\nThe Bayesian approach assumes the prior distribution:\n$$\np(\\alpha,\\beta,\\rho,\\sigma_{\\varepsilon})\\propto(1-\\rho^{2})^{-\\frac{1}{2}}\\sigma_{\\varepsilon}^{-1},\n$$\nwhere $\\alpha,\\beta$ , and log $\\sigma_{\\varepsilon}$ are uniformly distributed over the ranges $-\\infty<\\alpha, \\beta <\\infty$ , $0<\\sigma_{\\varepsilon}<\\infty$ , and $\\rho$ has a beta pdf with parameters $\\textstyle{\\bigl(}{\\frac{1}{2}},{\\frac{1}{2}}{\\bigr)}$ over the range $|\\rho|<1$. The posterior distribution is:\n$$\np(\\alpha,\\beta,\\sigma_{\\varepsilon},\\rho\\big|y)\\propto\\sigma_{\\varepsilon}^{-(T+1)}\\exp\\biggl[-\\frac{1}{2\\sigma_{\\varepsilon}^{2}}K(\\alpha,\\beta,\\rho)\\biggr].\n$$\nIntegration yields:\n$$\np(\\beta,\\rho|y)\\propto\\left[\\displaystyle\\sum_{t=1}^{T}V_{t}^{2}\\right]^{-\\frac{1}{2}}\\Biggl\\lbrace\\displaystyle\\sum_{t=1}^{T}\\big(y_{t}^{*}-\\beta X_{t}^{*}\\big)^{2} -\\left[\\displaystyle\\sum_{t=1}^{T}V_{t}\\big(y_{t}^{*}-X_{t}^{*}\\beta\\big)\\right]^{2}\\Biggl/\\displaystyle\\sum_{t=1}^{T}V_{t}^{2}\\Biggr\\rbrace^{-(T-1)/2}.\n$$\n\nThe text presents a detailed econometric model with autocorrelated errors and discusses both the Prais-Winsten estimator and Bayesian approaches to handle the nuisance parameter problem."
  },
  {
    "qid": "econ-empirical-996-1-1-1",
    "question": "4) Derive the conditions under which individuals cannot distinguish permanent from transitory changes in the economic environment, and analyze the implications for inflation dynamics.",
    "gold_answer": "1. Use a signal extraction problem: \\[ x_t = \\mu_t + \\epsilon_t \\], where \\[ \\mu_t \\] is permanent and \\[ \\epsilon_t \\] is transitory.\n2. Agents use Kalman filtering to estimate \\[ \\mu_t \\].\n3. Show that if \\[ \\text{Var}(\\epsilon_t) \\gg \\text{Var}(\\mu_t - \\mu_{t-1}) \\], agents will overweight transitory shocks.\n4. Analyze how this leads to persistent inflation dynamics.",
    "question_context": "The first involves situations in which individuals have asymmetric information about the current general price level and consequently confuse relative and aggregate changes in prices. The second considers models in which individuals cannot distinguish permanent from transitory changes in the economic environment.\n\nThis book presents a summary view of the recent imperfect information approach to inflation and its real effects, focusing in particular on two types of informational limitations. The first involves situations in which individuals have asymmetric information about the current general price level and consequently confuse relative and aggregate changes in prices. The second considers models in which individuals cannot distinguish permanent from transitory changes in the economic environment."
  },
  {
    "qid": "econ-empirical-739-4-0-1",
    "question": "2) Explain the partitioning of the bordered Vandermonde matrix $V$ for $N=2$ and derive its inverse using the partitioned form.",
    "gold_answer": "1. For $N=2$, $V$ is partitioned as $V = \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix}$, where $A = \\begin{bmatrix} 1 & 1 \\\\ 0 & 1 \\end{bmatrix}$, $B = \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix}$, $C = \\begin{bmatrix} 0 & 1 \\end{bmatrix}$, and $D = \\begin{bmatrix} 4 \\end{bmatrix}$.\n2. Compute $A^{-1} = \\begin{bmatrix} 1 & -1 \\\\ 0 & 1 \\end{bmatrix}$ (from previous iteration).\n3. Compute $E = D - C A^{-1} B = 4 - \\begin{bmatrix} 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} = 4 - 2 = 2$.\n4. The inverse is given by: $$V^{-1} = \\begin{bmatrix} A^{-1}(I + B E^{-1} C A^{-1}) & -A^{-1} B E^{-1} \\\\ -E^{-1} C A^{-1} & E^{-1} \\end{bmatrix} = \\begin{bmatrix} \\begin{bmatrix} 1 & -1 \\\\ 0 & 1 \\end{bmatrix} & -\\begin{bmatrix} 1 & -1 \\\\ 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\end{bmatrix} \\frac{1}{2} \\\\ -\\frac{1}{2} \\begin{bmatrix} 0 & 1 \\end{bmatrix} \\begin{bmatrix} 1 & -1 \\\\ 0 & 1 \\end{bmatrix} & \\frac{1}{2} \\end{bmatrix}.$$",
    "question_context": "If the long-run effect and all unadjusted moments are to be estimated, recall that the bordered Vandermonde matrix at iteration $N$ is $$V=\\left[\\begin{array}{l l l l l l}{1}&{1}&{1}&{1}&{\\dots}&{1}\\ {0}&{1}&{2}&{3}&{\\dots}&{N}\\ {\\vdots}&&&&&{\\ddots}\\ {0}&{1}&{2^{N}}&{3^{N}}&{\\dots}&{N^{N}}\\end{array}\\right].$$ Taking $N$ to be unity on the first iteration, then $$V={\\left[\\begin{array}{l l}{1}&{1}\\ {0}&{1}\\end{array}\\right]}\\quad{\\mathrm{and}}\\quad V^{-1}={\\left[\\begin{array}{l l}{1}&{-1}\\ {0}&{1}\\end{array}\\right]},$$ and $\\mu_{0}$ and $\\mu_{1}$ are easily calculated using standard computing techniques.\nWhen $N=2$ $V$ can be partitioned as $$V={\\begin{array}{l}{{\\left[1\\quad1\\quad1\\quad1\\right]}}\\ {{\\left[0\\quad1\\mid2\\right]}}\\ {{\\left[{\\bar{0}}^{-}{\\frac{1}{1}}\\right]}}\\end{array}}\\quad{\\mathrm{or}}\\quad V={\\left[{\\frac{A}{C}}\\right]}{\\frac{\\mid B\\atop{\\bar{D}}}}{\\frac{1}{\\mid D\\right]}},$$ and thus $$\\begin{array}{r}{V^{-1}=\\biggl[\\begin{array}{c c}{A^{-1}(I+B E^{-1}C A^{-1})}&{-A^{-1}B E^{-1}\\biggr],}\\ {-E^{-1}C A^{-1}\\qquad}&{E^{-1}}\\end{array}\\biggr],}\\end{array}$$ where $I$ is an identity matrix of order $N$ ,and $E=D-C A^{-1}B$ and is a scalar. Note that the inverse of the upper left partition, $A$ , was calculated on the previous iteration on $N$.\n\nThe text discusses the use of a bordered Vandermonde matrix for estimating long-run effects and unadjusted moments through iterative calculations."
  },
  {
    "qid": "econ-empirical-1669-0-1-3",
    "question": "4) Prove that the approximation error decreases at rate \\( O(1/K) \\) when using \\( K \\) quantile bins for the copula and marginals, under Lipschitz continuity assumptions.",
    "gold_answer": "1. Assume \\( F_p^{-1}, F_c^{-1} \\) are \\( L \\)-Lipschitz\n2. Bin discretization error per cell: \\( \\leq \\frac{L}{K} \\)\n3. Total integration error over \\( K^2 \\) cells: \\( \\leq \\frac{L^2}{K} \\)\n4. By dominated convergence, \\( |M_{abs} - M_{abs}^K| \\leq \\frac{C}{K} \\) for constant \\( C \\)",
    "question_context": "We show that the commonly used 'copula and marginals' approximation methods perform well across countries in our sample, and the greatest challenges to their accuracy stem not from assumptions about relative mobility rates over time but from the use of nonrepresentative marginal income distributions.\nWhen using high-resolution, 100-cell income distributions (and a $100\\times100$ cell copula), the point estimates never deviate by more than 1.6 percentage points from the true values, and definitive bounds on the measure that impose no further assumptions on the shapes of the marginal income distributions and copula never deviate by more than 5.6 percentage points.\n\nThis section evaluates the accuracy of 'copula and marginals' approximation methods against ground-truth linked administrative data."
  },
  {
    "qid": "econ-empirical-1087-4-0-2",
    "question": "3) Prove mathematically why the supervisor-worker ratio $\\left(\\frac{S}{L}\\right)$ rises as $\\frac{x^{\\prime}}{x}$ falls, using the effort functions $e(w, x)$ and $e^{\\prime}(m, x^{\\prime})$. Reference the empirical estimates ($x^{\\prime} \\approx 3x$ in SSA vs. $2x$ in Morocco).",
    "gold_answer": "1. From effort functions: $e(w, x) = \\left(\\frac{w - x}{c}\\right)^b$ and $e^{\\prime}(m, x^{\\prime}) = \\left(\\frac{m - x^{\\prime}}{c^{\\prime}}\\right)^{b^{\\prime}}$. 2. Cost minimization implies $\\frac{S}{L} \\propto \\frac{e^{\\prime}}{e}$. 3. As $\\frac{x^{\\prime}}{x} \\downarrow$, $e^{\\prime}$ becomes cheaper to increase relative to $e$, raising $\\frac{S}{L}$. Empirical: Morocco's lower $\\frac{x^{\\prime}}{x}$ (2 vs. 3) aligns with higher $\\frac{S}{L}$ elasticity.",
    "question_context": "The estimated share of capital is small in both samples: 0.145 in Morocco, 0.290 in SSA. The share of labor is high in Morocco (0.681) but low in SSA (0.355).\nReturns to schooling appear to be higher in SSA than in Morocco: One additional year of education for the entire labor force raises output by 6.9 percent in SSA versus 1.0 percent in Morocco.\nWe find $x^{\\prime}>x$ in all cases: This is consistent with the idea that the value of the outside option of supervisors is higher than that of production workers. The difference between the two is larger in SSA, however, where $x^{\\prime}$ is around three times $x$. In contrast, in Morocco $x^{\\prime}$ is only twice $x$.\nCoefficient estimates of $c,b,c^{\\prime},b^{\\prime}$ are jointly significantly different from 0.5, hence rejecting the generalized Sparks model. Worker effort therefore varies with TFP.\nA 1 percent increase in TFP raises output by 2.6 percent in Morocco but only by 1.3 percent in SSA.\n\nThe section presents estimation results for production function parameters and effort functions in Morocco and Sub-Saharan Africa (SSA), highlighting differences in labor and capital shares, human capital effects, and outside wage options."
  },
  {
    "qid": "econ-empirical-1137-2-0-3",
    "question": "4) Based on the results in Table 3, analyze the contribution of work history variables (e.g., tenure) to the reduction in the wage gap. How does this compare to findings from Blau and Beller (1988)?",
    "gold_answer": "1. **Table 3 Findings**:\n   - **Tenure**: Reduced the wage gap by $-0.0249$ (significant at 5% level).\n   - **Other Work History Variables**: Combined effect of $-0.0387$, accounting for nearly half of the overall reduction.\n   - **Total Reduction**: $-0.0781$ due to changes in means, with work history variables playing a dominant role.\n2. **Comparison to Blau and Beller (1988)**:\n   - Blau and Beller found a smaller reduction ($-0.0205$) due to potential experience.\n   - Differences arise because:\n     - This study uses actual experience, capturing relative gains more accurately.\n     - Blau and Beller's period (1971–1981) missed significant gains by women in the early 1980s.\n3. **Implications**: Actual experience measures are crucial for accurately assessing wage gap dynamics.",
    "question_context": "For a single period's earnings function, Oaxaca (1973) showed we can decompose the wage gap into the differences in the means and differences in the coefficients (including the constant term).\n$$\\begin{array}{r l}{{(\\ln w_{m8}-\\ln w_{m7})-(\\ln w_{f8}-\\ln w_{f7})}=\\upbeta_{m8}[(X_{m8}-X_{m7})-(X_{f8}-X_{f7})]~}&{}\\ {+~X_{f7}[(\\upbeta_{m8}-\\upbeta_{m7})-(\\upbeta_{f8}-\\upbeta_{f7})]~}&{}\\ {+~[(X_{m7}-X_{f7})(\\upbeta_{m8}-\\upbeta_{m7})]~}&{}\\ {+~[(X_{f8}-X_{f7})(\\upbeta_{m8}-\\upbeta_{f8})]~}&{}\\end{array}$$\nIn the spirit of the Oaxaca one period decomposition, this paper employs the following alternative decomposition: $$\\begin{array}{r}{(\\ln w_{m8}-\\ln w_{m7})-(\\ln w_{f8}-\\ln w_{f7})=[\\upbeta_{m8}(X_{m8}-X_{m7})-\\upbeta_{f8}(X_{f8}-X_{f7})]\\qquad}\\ {+[X_{m7}(\\upbeta_{m8}-\\upbeta_{m7})-X_{f7}(\\upbeta_{f8}-\\upbeta_{f7})]\\qquad}\\end{array}$$\nThe first term of the decomposition shows the change in the wage gap due to changes in the means evaluated at their own 1985 coefficients. The second part represents the portion of the wage gap that can be explained by changes in the coefficients over the period, evaluated at the groups’ 1976 means.\n\nThis section analyzes the decomposition of the wage gap between white men and women from 1976 to 1985, focusing on changes in means and returns to explanatory variables. The Oaxaca decomposition method is employed to break down the wage gap into differences in means and coefficients."
  },
  {
    "qid": "econ-empirical-281-3-1-1",
    "question": "4) Prove that under interim renegotiation, any implementable value function $v \\in V^{\\mathrm{I}}$ must satisfy $v_1(\\theta) + v_2(\\theta) = \\gamma(\\theta)$ for all $\\theta \\in \\Theta$, as stated in Lemma 4.",
    "gold_answer": "Proof:\n1. By Definition 3, $v(\\theta) = v'(\\theta) + \\pi r(v', \\theta)$ for some $v' \\in V^{\\mathrm{N}}$.\n2. The renegotiation surplus is $r(v', \\theta) = \\gamma(\\theta) - v'_1(\\theta) - v'_2(\\theta)$.\n3. Thus, $v_1(\\theta) + v_2(\\theta) = v'_1(\\theta) + v'_2(\\theta) + \\pi_1 r(v', \\theta) + \\pi_2 r(v', \\theta)$.\n4. Since $\\pi_1 + \\pi_2 = 1$, this simplifies to $v_1(\\theta) + v_2(\\theta) = \\gamma(\\theta)$.",
    "question_context": "DEFINITION 2: Mechanism $(\\Theta^{2},f)$ is said to implement value function $v$ if $f:\\theta^{2}\\to W$ and, for each state $\\theta$ $(\\theta,\\theta)$ is a Nash equilibrium of the message game and it leads to the payoff vector $v(\\theta)$ . Value function $v$ is said to be implementable if there is a mechanism that implements it.\nLEMMA 2: Value function $v$ is an element of $V^{\\mathbf{N}}$ if and only if (i) for every $\\theta\\in\\Theta$ ,there is anoutcome $w\\in W$ such that $w(\\theta)=v(\\theta)$ and (i) for every pair of states $\\theta$ ， $\\theta^{\\prime}\\in\\Theta$ ,there is an outcome $\\hat{w}\\in W$ such that $v_{1}(\\theta)+v_{2}(\\theta^{\\prime})\\geq\\hat{w}_{1}(\\theta)+$ $\\hat{w}_{2}(\\theta^{\\prime})$ .Also, the set $V^{\\mathrm{N}}$ is closed under constant transfers.\nLEMMA 4: We have $v\\in V^{\\mathrm{I}}$ if and only if $v\\in V^{\\mathrm{N}}$ and $v_{1}(\\theta)+v_{2}(\\theta)=\\gamma(\\theta)$ for every $\\theta\\in\\Theta.A l s o$ ， $V^{\\mathrm{I}}$ is closed under constant transfers.\nLEMMA 5: We have $v\\in V^{\\mathrm{EP}}$ if and only if (i) $v_{1}(\\theta)+v_{2}(\\theta)=\\gamma(\\theta)$ for every $\\theta\\in\\Theta$ and (ii) for every pair of states $\\theta$ $\\theta^{\\prime}\\in\\Theta$ ,there is an outcome $\\hat{z}\\in Z$ such that $v_{1}(\\theta)+v_{2}(\\theta^{\\prime})\\ge\\hat{z}_{1}(\\theta)+\\hat{z}_{2}(\\theta^{\\prime}).A l s o,$ $V^{\\mathrm{EP}}$ is closed under constant transfers.\n\nThis section analyzes implementability of value functions under different renegotiation scenarios: no renegotiation, interim renegotiation, and ex post renegotiation."
  },
  {
    "qid": "econ-empirical-297-1-2-0",
    "question": "5) Prove part (i) of Proposition 6 by showing how $n_1(n - n_1)$ is identifiable from the autocovariance function.",
    "gold_answer": "1. From Proposition 2, the dimension of the space generated by ${\\cal T}(h), h < 0$ is $n^2 - 2n_1(n - n_1)$. \n2. This dimension is identifiable from the data. \n3. Solve for $n_1(n - n_1)$ to obtain $n_1(n - n_1) = \\frac{n^2 - \\text{dim}({\\cal T}(h))}{2}$. \n4. This product is identifiable except when $\\boldsymbol{\\Sigma}$ lies in a set of measure zero.",
    "question_context": "Proposition 6: (i) The product of the causal and noncausal dimensions equal to $n_1(n - n_1)$ is identifiable at second-order, except on a set of $\\boldsymbol{\\Sigma}$ of measure zero. (ii) For a given pair of dimensions $[n_{10}, n - n_{10}]$, the pair of causal/noncausal spaces can be second-order identifiable.\n\nThis section introduces nonlinear covariance estimators to address the limitations of second-order identification in mixed VAR(1) processes."
  },
  {
    "qid": "econ-empirical-14-0-0-1",
    "question": "2) Prove that the second-order condition for the first firm's perceived marginal profit function holds globally, given $2P^{\\prime}(Q)(1+F^{*})+XP^{\\prime\\prime}(Q)(1+F^{*})^{2}-C^{\\prime\\prime}(X)<0$ for all $(X,Y)\\geq0$.",
    "gold_answer": "1. The second-order condition requires the perceived marginal profit to be decreasing in $X$: \n   - $\\frac{\\partial^2 \\Pi_1}{\\partial X^2} = 2P^{\\prime}(Q)(1+F^{*}) + X P^{\\prime\\prime}(Q)(1+F^{*})^{2} - C^{\\prime\\prime}(X) < 0$\n2. Given $P^{\\prime}(Q) < 0$ (downward-sloping demand), $P^{\\prime\\prime}(Q) \\leq 0$ (non-increasing slope), and $C^{\\prime\\prime}(X) > 0$ (convex costs), each term is non-positive.\n3. Thus, the sum is strictly negative, ensuring the second-order condition holds globally.",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nFix any $(X^{*},Y^{*})>0$ with $P(X^{*}+Y^{*})-C^{\\prime}(X^{*})\\geq0$ and $P(X^{*}+Y^{*})-D^{\\prime}(Y^{*})\\geq0.$ Define $F^{*}(X,Y)=F^{*}$ and $G^{*}(X,Y)=G^{*}$ all $(X,Y)$ with $F^{*}$ and $G^{*}$ constants chosen, so that if $Q^{*}=X^{*}+Y^{*}$ ,then $P(Q^{*})+X^{*}P^{\\prime}(Q^{*})(1+F^{*})=C^{\\prime}(X^{*})$ and $P(Q^{*})+Y^{*}P^{\\prime}(Q^{*})(1+G^{*})=D^{\\prime}(Y^{*})$.\nDifferentiating the first firm's perceived marginal profit function (using the constraint $(\\partial Y/\\partial X)|_{(X,\\overline{Y})}\\simeq F^{*})$ we have $2P^{\\prime}(Q)(1+F^{*})+XP^{\\prime\\prime}(Q)(1+F^{*})^{2}-C^{\\prime\\prime}(X)<0$ all $(X,Y)\\geq0$, because each term is less than or equal to 0 and $C^{\\prime\\prime}(X)>0$.\n\nThis paper evaluates conjectural duopoly models within Cournot's quantity-adjustment framework, introducing the 'conjectural equilibrium' concept and exploring the implications of rational expectations in both static and dynamic settings."
  },
  {
    "qid": "econ-empirical-192-0-2-0",
    "question": "5) Derive the asymptotic properties of the sample-splitting IV estimator for $\\lambda$, assuming weak factors ($\\beta_{2,i} = O(1/\\sqrt{T})$) and a missing factor structure in $\\varepsilon_{i t}$.",
    "gold_answer": "1. **Sample splitting**: Split data into two subsets; estimate $\\hat{\\beta}_i^{(1)}$ and $\\hat{\\beta}_i^{(2)}$.\n2. **IV regression**: Use $\\hat{\\beta}_i^{(1)}$ as instruments for $\\hat{\\beta}_i^{(2)}$ in the second pass.\n3. **Asymptotics**: Under $N, T \\to \\infty$, $\\hat{\\lambda}_{IV} \\xrightarrow{p} \\lambda$ if $\\text{plim}(\\hat{\\beta}^{(1)\\prime} \\hat{\\beta}^{(2)}/N)$ is full rank.\n4. **Robustness**: IV correction addresses errors-in-variables bias from weak factors and missing factors.",
    "question_context": "We propose a new estimation procedure based on sample-splitting instrumental variables regression. The proposed estimator of risk premia is robust to weak included factors and to the presence of strong unaccounted cross-sectional error dependence.\nOur new estimation approach makes use of the idea of sample-splitting in order to create multiple estimates for loadings $\\beta_{i}$ and to correct for the first-step estimation error via an instrumental variables regression.\n\nThis section outlines the proposed sample-splitting instrumental variables regression method to address the challenges in factor pricing estimation."
  },
  {
    "qid": "econ-empirical-91-3-1-2",
    "question": "7) Interpret the coefficient on the real wage ($\\ln w$) in column (2) of Table VII. How does this variable help refine the estimate of the supply elasticity?",
    "gold_answer": "1. The real wage ($\\ln w$) captures input costs for capital producers.\n2. Including it controls for cost shocks that might otherwise be conflated with price effects.\n3. The positive coefficient (0.3619) suggests that higher wages reduce supply (shift supply curve left).\n4. The supply elasticity falls from 1.14 (column 1) to 0.7284 (column 2) because part of the price variation is now attributed to wage changes.\n5. This refinement brings the elasticity closer to 1, consistent with constant returns to scale.",
    "question_context": "The supply equation estimated is the log of real shipments of the capital good regressed on the log real price, asset fixed effects, asset-specific time trends, the price controls variable, sometimes the real wage, and sometimes year dummies. Prices are the only endogenous variable, and the demand instruments used to identify prices are the current and once lagged tax terms.\nIn the most basic specification of column (1), the elasticity is estimated to be 1.14 with a confidence interval from, essentially, zero to two. In (2), which also includes the log of the average real annual earnings in the industry (from the NBER productivity data), the elasticity is around three-fourths and is not significantly different from one.\nBecause there could be a variety of concurrent macroeconomic factors at work, columns (3) and (4) repeat the regressions above but include year dummies. These equations are identified off of the variation in the tax term between assets within years.\n\nThe reduced-form evidence that prices rise in response to subsidies can be thought of as the first stage of a two-stage least squares estimate where investment subsidies serve as an instrument for investment demand."
  },
  {
    "qid": "econ-empirical-506-0-0-2",
    "question": "3) Compare the long-run effects of a ban on price discrimination on consumer surplus and welfare in the presence of demand-side substitution versus the unconstrained monopolist case.",
    "gold_answer": "1. **With demand-side substitution**:\n   - Ban reduces long-run consumer surplus by stifling investment incentives (firms face higher \\( c_{i} \\)).\n   - Welfare decreases due to lower efficiency and innovation.\n2. **Unconstrained monopolist**:\n   - Ban increases investment incentives (supplier cannot extract all rents via \\( w_{i} \\)).\n   - Consumer surplus may rise due to lower input prices.\n3. Key difference: Demand-side substitution reverses the monopolist's incentives, leading to opposite long-run outcomes.",
    "question_context": "In our model, the more efficient buyer receives a discount. A ban on price discrimination thus benefits smaller but hurts more efficient, larger firms. It also stifles incentives to invest and innovate. With linear demand, a ban on price discrimination benefits consumers in the short run but reduces consumer surplus in the long run, which is once again the opposite of what is found without the threat of demand-side substitution.\nThe supplier can make take-it-or-leave-it offers to downstream firms. Under price discrimination, the supplier thus offers each firm a possibly different wholesale price \\( w_{i} \\), whereas under uniform pricing the same price \\( w_{i}=w \\) applies to both firms. Consequently, upon accepting the supplier’s offer, a firm’s total marginal cost equals \\( c_{i}:=w_{i}+k_{i} \\).\nOur main deviation from most of the literature is that despite having the ability to make take-it-or-leave-it offers, the supplier is no longer an unconstrained monopolist. Here we follow Katz (1987) and suppose that instead of ceasing operations when rejecting the supplier’s offer, a downstream firm can turn to an alternative source of supply. As in Katz (1987), this comes at costs \\( F>0 \\) and allows the respective firm to obtain the input at constant marginal costs \\( \\widehat{w} \\geq 0 \\).\nEach downstream firm can reduce its own marginal cost \\( k_{i} \\). Precisely, a firm reduces its initial cost \\( \\overline{k}_{i}>0 \\) by \\( \\Delta_{k} \\) after incurring the expenditure \\( e(\\Delta_{k}) \\). We suppose that \\( e \\) is strictly increasing and continuously differentiable with \\( e^{\\prime}(0)=0 \\) and \\( e^{\\prime}(\\Delta_{k}) \rightarrow \\infty \\) as \\( \\Delta_{k} \rightarrow \\overline{k}_{i} \\), which allows us to focus on interior solutions.\n\nThe paper analyzes the implications of third-degree price discrimination in input markets, where the supplier faces a threat of demand-side substitution. This contrasts with the existing literature that assumes an unconstrained monopolist supplier. The model shows that more efficient buyers receive discounts, and a ban on price discrimination benefits smaller but harms more efficient firms, impacting investment incentives and long-run consumer surplus."
  },
  {
    "qid": "econ-empirical-637-2-2-0",
    "question": "5) Interpret the results of the Monte Carlo simulations for the standardized estimates $f_{t}$ and $c_{i t}$. How do they validate the asymptotic theory?",
    "gold_answer": "1. **$f_{t}$**: Sample means and standard deviations align with $N(0,1)$, validating Theorem 5.\n2. **$c_{i t}$**: Similar alignment, supporting Theorem 6.\n3. **Conclusion**: The simulations confirm that the asymptotic distributions provide good approximations in finite samples.",
    "question_context": "The standardized estimates should be approximately $N(0,1)$ if the asymptotic theory is adequate.\nFigure 1 displays the histograms of $f_{t}$ for $T=50$ with the standard normal density overlayed.\n\nThis section assesses the finite-sample performance of the estimators through Monte Carlo simulations, focusing on the distributional properties of $\\widetilde{F}_{t}$ and $\\widetilde{C}_{i t}$."
  },
  {
    "qid": "econ-empirical-1831-0-0-2",
    "question": "3) Discuss the role of linear programming in testing the conditions derived in Corollary 7.1. How does the linearity of the inequalities facilitate empirical testing?",
    "gold_answer": "1. **Linearity**: The inequalities $X_h^s \\leq X_h^t + \\lambda^t p_h^t \\cdot (x_h^s - x_h^t)$ are linear in $X_h^t$ and $\\lambda^t$.  \n2. **Feasibility**: Linear programming can efficiently check for the existence of solutions satisfying these inequalities.  \n3. **Advantages**: Non-parametric tests avoid restrictive parametric assumptions, making them robust for empirical applications.  \n4. **Implementation**: Use standard LP solvers to verify consistency of the data with additive separability.",
    "question_context": "The paper develops non-parametric tests for the weak separability or the additive separability of a decision-maker's utility function, given a finite body of price and quantity data. The tests generalize some earlier tests proposed by Afriat and Varian.\nAssume the same framework as in section 3, except that now we wish to derive necessary and sufficient conditions for a data set to be consistent with utility maximizing behaviour where the utility function $F$ has the following additively separable form: $F(x_{1},\\ldots,x_{H},y)=f[\\sum_{h=1}^{H}g_{h}(x_{h}),y]$ where the macrofunction $f$ is a function of $1+M$ variables and the micro function $g_{h}$ is a function of $N^{h}$ variables for $h=1,\\ldots,H$.\nTheorem 7 [Generalization of Afriat (1970) and Varian (1983)]. Suppose the macro function $f$ is $(i)$ concave, $(i i)$ once differentiable,and $(i i i)$ increasing in its first argument and non-decreasing in its last $M$ arguments.Suppose $H\\geq2$ and the micro functions $g_{h}$ , $h=1,\\ldots,H_{\\cdot}$ are $(i)$ concave, $(i i)$ once differentiable, and (ii) weakly increasing in their arguments.\nCorollary 7.1.Suppse $f[\\Sigma_{h=1}^{H}g_{h}(x_{h}),y]$ is replaced by the following completely additive structure: $\\textstyle\\sum_{h=1}^{H}g_{h}(x_{h})$ Then there exist $H T$ scalars,. $X_{h}^{\\iota}$ $h=1,\\ldots,H.$ $t=1,\\ldots,T$ and $T$ scalars, $\\lambda^{t}$ $\\backslash^{t},t=1,\\ldots,T$ such that the following inequalitieshold: $X_{h}^{s}\\leq X_{h}^{t}+\\lambda^{t}p_{h}^{t}\\cdot\\big(x_{h}^{s}-x_{h}^{t}\\big),$ $\\lambda^{t}>0,\\qquadh=1,...,H,\\quad s,t=1,...,T.$\n\nThe paper discusses non-parametric tests for weak and additive separability of a decision-maker's utility function using finite price and quantity data. It generalizes earlier tests by Afriat and Varian, addressing deficiencies in econometric tests that impose unwanted parametric restrictions."
  },
  {
    "qid": "econ-empirical-1020-1-0-2",
    "question": "3) Using the strategy method, derive the conditions under which the sender’s beliefs about the receiver’s trustworthiness are correlated with the amount sent. How does this relate to the 25% threshold mentioned in the text?",
    "gold_answer": "1. The strategy method involves the sender reporting beliefs \\( y(x) \\) for all possible amounts sent \\( x \\). 2. The sender’s optimal amount sent \\( x^* \\) is where the marginal utility of sending equals the marginal expected return: \\( u'(x) + \beta v'(y(x))y'(x) = 0 \\). 3. For \\( x > 25\\% \\) of the endowment, \\( y'(x) \\) is significant, implying beliefs matter. 4. For \\( x \\leq 25\\% \\), \\( y'(x) \\) is negligible, implying behavior is driven by preferences (e.g., charity) rather than beliefs.",
    "question_context": "The sender’s behaviour in the trust game is affected by other motivations besides the sender’s belief in the receiver’s trustworthiness, such as individual risk aversion (Karlan, 2005; Schechter, 2007) and other-regarding preferences like altruism (Cox, 2004; Ashraf et al., 2006).\nWe find that the quantity sent in the trust game is not only correlated with the sender’s expectation of the receiver’s trustworthiness but also with his preferences.\nWe find that beliefs are significantly correlated with the quantity sent only when subjects send more than 25% of their initial endowment.\nWe find that senders’ expectations are correlated with the WVS-trust question, as well as other attitudinal questions on trust.\n\nThe article discusses the measurement of trust using the World Values Survey (WVS) question and the Berg et al. trust game. It highlights the limitations of both methods and proposes a modified trust game to separate beliefs and preferences in trust behavior."
  },
  {
    "qid": "econ-empirical-1296-0-1-1",
    "question": "4) Using Table 5, Column 3, explain how audit ratings and evaluation lags jointly influence project outcomes. Why might audits appear more favorable after controlling for lags?",
    "gold_answer": "1. **Coefficients**: Audit Rating has a positive coefficient (0.517**), while Evaluation Lag has a negative coefficient (-0.215**).  \n2. **Joint Effect**: Longer lags correlate with worse outcomes, but audits (typically with longer lags) are more likely to be satisfactory when lag duration is held constant.  \n3. **Bias Correction**: The positive audit coefficient suggests rating bias is mitigated by controlling for lags. Audits may focus on higher-priority projects or use stricter standards, leading to better outcomes net of delays.",
    "question_context": "Panel analysis of project data finds that projects with longer preparation periods are significantly more likely to have satisfactory outcome ratings. This result is robust across a range of specifications but the effects are conditional on the degree of economic vulnerability.\nFor borrowing countries in a strong economic position at the time the project is approved, the duration of preparation is not a significant factor in determining project outcomes. However, for the sizable share of World Bank borrowers that are economically vulnerable, longer World Bank project preparation is significantly and positively related to good project outcomes.\n\nThe paper employs a two-step estimation procedure to address endogeneity in preparation duration and its impact on project performance. Key findings include the conditional effect of preparation on outcomes, moderated by economic vulnerability."
  },
  {
    "qid": "econ-empirical-910-2-0-2",
    "question": "3) Explain the role of the common factor $F_t$ in Model 2 and how cross-sectional dependence is addressed in Estimator B.",
    "gold_answer": "1. The common factor $F_t$ introduces cross-sectional dependence via $y_{it} = \\lambda_i F_t + u_{it}$. \\n2. Estimator B controls for cross-sectional correlation by approximating $F_t$ using $\\Delta \\bar{y}_t$ and its lags. \\n3. This approximation is valid for large $N$ and ensures robust inference under cross-sectional dependence.",
    "question_context": "The error process is specified as: $$ u_{i t}=(\\alpha_{i1}+\\alpha_{i2})u_{i t-1}-\\alpha_{i2}u_{i t-2}+e_{i t}+\\psi e_{i t-1} $$ and $$ e_{i t}\\sim{\\bf N}(0,\\sigma_{i}^{2}). $$\nNonstationary units are generated by letting $\\alpha_{i1}=1$, while stationary units are generated with $\\alpha_{i1}\\sim\\mathrm{U}[.5,\\gamma]$.\nModel 1: $y_{i t}=\\lambda_{i}+u_{i t}$; Model 1a: $\\lambda_{i}\\sim\\mathrm{U}[-1,1]$, $\\alpha_{i2}\\sim\\mathrm{U}[0,.2]$, $\\psi=0$, $\\sigma_{i}\\sim$ U[.5, 2]; Model 1b: $\\lambda_{i}\\sim\\mathrm{U}[-1,1]$, $\\alpha_{i2}=0$, $\\psi_{i}\\sim\\mathrm{U}(-.8,0)$, $\\sigma_{i}^{2}=1\\forall i$.\nModel 2: $y_{i t}=\\lambda_{i}F_{t}+u_{i t}$; $\\alpha_{i2}\\sim\\mathrm{U}[0,.2]$, ${\\psi}_{i}=0\\forall i$, $\\sigma_{i}\\sim\\mathrm{U}[.5,2]$, $\\beta=.5$; Model 2a: $\\operatorname*{Pr}(\\lambda_{i}=0)=.5$, $\\operatorname*{Pr}(\\lambda_{i}\\sim\\mathbf{N}(0,1))=.5$; Model 2b: $\\lambda_{i}\\sim\\mathrm{U}(0,1)$.\nSimulations were conducted with $\\gamma=.8$ and .99, reporting only $\\gamma=.99$. Results are shown for $N=30, 60$ and $T=100, 200$.\nThe estimator is $\\sqrt{N}$ consistent for the fraction of nonstationary units. The estimated number of nonstationary units is $\\hat{N}_{1}=[\\hat{\\theta}\\cdot N]$.\n\nThe text evaluates the finite-sample properties of an estimator for θ using two models with specified error processes. The models consider both stationary and nonstationary units, with various parameterizations to assess the estimator's performance under different conditions."
  },
  {
    "qid": "econ-empirical-948-7-3-0",
    "question": "4) Using the symmetry assumptions and Lemma 8, prove that the equilibrium post-trade exposures g(ω) satisfy g(1−ω)=1−g(ω).",
    "gold_answer": "4. Proof:\n   - Given symmetry, n(ω)=n(1−ω) and π(1−ω)=π(ω).\n   - For any equilibrium CDS contracts γ(ω,ω̃), define mirrored contracts γ̂(ω,ω̃)=−γ(1−ω,1−ω̃).\n   - The post-trade exposures for γ̂ are:\n     $$\\hat{g}(ω) = ω − ∑_x γ(1−ω,1−x)n(x) = 1 − g(1−ω).$$\n   - By uniqueness of equilibrium, g(ω) = \\hat{g}(ω) = 1−g(1−ω).",
    "question_context": "Without loss of generality, we assume that Ω is symmetric around ω⋆=1/2. Then, the symmetry assumption can be stated as: 1/2 ∈ Ω, ω ∈ Ω ⇒ 1−ω ∈ Ω, and π(1−ω)=π(ω). We first establish that, if the distribution of traders in the market is symmetric, then the equilibrium conditional on entry is symmetric as well.\n\nThe proof leverages symmetry assumptions and Kakutani's fixed-point theorem to establish the existence of a symmetric equilibrium."
  },
  {
    "qid": "econ-empirical-1300-3-0-3",
    "question": "4) What are the key differences between sequential offering and sequential contracting, and how do they affect the characterization of equilibrium outcomes?",
    "gold_answer": "The key differences are:\n\n1. **Sequential Contracting**: Payoff-relevant decisions are committed at each date. The agent's effort and principals' decisions are determined sequentially. This requires extending the agent's type to include upstream contracts, i.e., $\\theta_i^E \\equiv (\\theta, y_i^-)$.\n\n2. **Sequential Offering**: Principals offer mechanisms sequentially, but the agent sends messages and takes payoff-relevant decisions only after observing all mechanisms. This is strategically equivalent to simultaneous common agency if mechanisms are private.\n\n**Impact on Equilibrium Characterization**:\n- In sequential contracting, extended direct mechanisms must account for upstream contracts.\n- In sequential offering, menus suffice if mechanisms are private, but extended direct mechanisms are needed if mechanisms are observable and the agent's strategy is Markov.",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nAs long as one restricts attention to Markov-perfect equilibria, this extension poses no problems to our characterization results: all Markov-perfect equilibrium outcomes can be characterized by restricting the principals to offer either menus or extended direct revelation mechanisms.\nIf, instead, one is also interested in equilibrium outcomes sustained by non-Markov strategies, then restricting the principals to offer menus (or direct mechanisms) may preclude a complete characterization.\nIn this setting, restricting the principals’ strategy space may mean restricting the extent to which payoff-irrelevant distinctions among mechanisms can be used as correlation devices for the principals’ decisions.\nOne way of restoring the possibility of using menus to sustain also mixed-strategy equilibria is to allow for public randomizing devices, as in the case of correlated equilibria.\nFurthermore, all equilibrium outcomes that can be sustained in the menu game by restricting the principals’ strategies to be pure can also be sustained in the revelation game.\nThe difficulties with this extensive form come from the fact that the agent’s continuation payoff at date $t$ may now depend not only on the upstream payoff-relevant decisions but also on the mechanisms, the messages and the contracts selected upstream.\nThese problems, however, disappear if one restricts attention to equilibria in which not only the agent’s strategy but also the principals’ strategies are Markov.\nFirst, consider an environment similar to the one examined in the benchmark model but where the agent’s effort is chosen only at the very end, say at $t=n+1$.\nNext, consider an environment in which principals offer their mechanisms sequentially, but where the agent sends the messages $(m_{1},\\ldots,m_{n})$ simultaneously at $t=n+1$.\n\nThe model considers private contracting where mechanisms, contracts, and decisions are private to the agent and principal. This section explores adaptations to alternative extensive forms, including observability of upstream decisions and mechanisms, endogenous sequence of bilateral relationships, and sequential offering versus sequential contracting."
  },
  {
    "qid": "econ-empirical-941-1-1-0",
    "question": "5) Formally model the relationship between the extent of participation rights (R) and the strength of citizen-politician interaction (I), assuming a diminishing returns function.",
    "gold_answer": "Let \\( I(R) = k \\ln(R + 1) \\), where:\n- \\( R \\geq 0 \\) represents participation rights (e.g., referendum frequency).\n- \\( k > 0 \\) is a constant scaling factor.\n- **Diminishing Returns**: \\( \\frac{dI}{dR} = \\frac{k}{R + 1} \\), which decreases as \\( R \\) increases, reflecting practical limits to interaction gains.",
    "question_context": "The more extensive the rights are to participate politically, the stronger the extent of interaction is between the citizens and the (professional) politicians. The constant tendency for the leaders to establish a 'classe politique' is reduced.\n\nThis section explores how popular participation rights in democracies can serve as a model for enhancing corporate governance through intrinsic motivation."
  },
  {
    "qid": "econ-empirical-125-3-0-2",
    "question": "3) Analyze the impact of underreporting (i.e., \\( \\lambda < 1 \\)) on the estimation of \\( R_0 \\) and long-term forecasts. Use the results from Fig. 6 and Table 14 to compare models with and without underreporting.",
    "gold_answer": "1. **With Underreporting**: Joint estimation of \\( R_0 \\), \\( \\alpha \\), and \\( \\lambda \\) yields \\( R_0 = 5.06 \\) and a realistic death forecast (~150k).  \n2. **Without Underreporting**: Assuming \\( \\lambda = 1 \\) forces \\( R_0 \\) downward (e.g., 2.90) and inflates \\( \\alpha \\) (e.g., 0.018), leading to poor fit and exaggerated death forecasts (~600k).  \n3. **Key Insight**: Ignoring underreporting biases \\( R_0 \\) downward and overestimates fatalities, emphasizing the need to account for incomplete case detection.",
    "question_context": "Based on the arguments from the previous sections, when initial parameters change, the estimate of \\( R_{0} \\) should remain virtually unchanged, while the estimates of \\( \\alpha \\) and \\( \\lambda \\) should change proportionally to the changes in initial parameters. In practice, however, this is not always the case. Both \\( \\alpha \\) and \\( \\lambda \\) are constrained to lie between 0 and 1, and when these constraints are binding or close to binding, changes in the initial values can have a substantial effect on the estimate of \\( R_{0} \\).\nThe parameter estimates for California behave as expected: the estimate of \\( R_{0} \\) remains virtually unchanged, while the estimates of \\( \\alpha \\) and \\( \\lambda \\) are cut in half. However, for the US as a whole the picture is different. The estimates of \\( R_{0} \\) change substantially as the initial conditions change, while the changes in the estimates of \\( \\alpha \\) and \\( \\lambda \\) do not follow the expected pattern.\nThe table presents the estimates of \\( R_{0} \\) for different countries and different values of epidemiologic parameters \\( \\sigma \\) and \\( \\gamma \\). Initial conditions: \\( E_{0}=1, I_{0}=0 \\) for USA and California, \\( E_{0}=10 \\) for Japan.\nThe model with \\( E_{0}=1, I_{0}=0 \\) estimates that the number of people with COVID-19 in California on March 22 was around 50 thousand and predicts over 150 thousand deaths in the long run. In contrast, the model with \\( E_{0}=30, I_{0}=0 \\) estimates that there were around 1.6 million people with COVID-19 and predicts less than 5 thousand deaths in the long run, a 32-fold difference.\n\nThis section presents the empirical results of the SEIRD model estimation, focusing on the impact of initial conditions, epidemiologic parameters, and underreporting on the estimates of the basic reproduction number \\( R_0 \\), infection fatality rate \\( \\alpha \\), and case reporting rate \\( \\lambda \\). The analysis includes comparisons across different regions (USA, California, Japan) and scenarios (fast, medium, slow epidemic progression)."
  },
  {
    "qid": "econ-empirical-410-1-0-1",
    "question": "2) Using the skewness ($-1.5$) and excess kurtosis (41.3) of the S&P500 returns, compute the Jarque-Bera test statistic for normality. Assume a sample size of 5,552. Critically evaluate the null hypothesis of normality.",
    "gold_answer": "1. Jarque-Bera formula: \\(JB = n \\left(\\frac{S^2}{6} + \\frac{(K-3)^2}{24}\\right)\\)  \n2. Plug in values: \\(JB = 5552 \\left(\\frac{(-1.5)^2}{6} + \\frac{(41.3)^2}{24}\\right) \\approx 5552 \\times 71.2 = 395,302\\)  \n3. Critical value (χ², df=2, α=0.05): 5.99  \n4. Reject normality due to extreme JB statistic, consistent with fat tails and skewness.",
    "question_context": "We collect daily return and range observations for the S&P500 index from January 2, 1962, to December 31, 2004 (10,787 observations). Over this period, the return-based volatility is 93 basis points per day, or $15.2\\%$ per year. The annualized average return is $6.9\\%$ . In addition, the returns are substantially left-skewed and fat-tailed, with a skewness of $-1.5$ and an excess kurtosis of 41.3, providing strong evidence for the presence of conditional nonnormality and/or heteroscedasticity.\nThe reason for this structural break in the data is unclear. Nonetheless, an empirical analysis based on the whole sample may be quantitatively biased as well as qualitatively misleading. Therefore, all of the results in this article are based on the postbreak subsample from January 1, 1983 to December 31, 2004 (5,552 observations). For this subsample, the annualized mean and volatility of returns are $10.3\\%$ and $17.3\\%$ .\n\nThe text discusses the collection of daily return and range observations for the S&P500 index from 1962 to 2004, highlighting key statistics such as volatility, skewness, and kurtosis. It also examines the relationship between log absolute returns and log ranges, noting a structural break in the data around 1982-1983."
  },
  {
    "qid": "econ-empirical-430-2-0-1",
    "question": "2) Show how the information matrix $\\widetilde{\\mathcal{I}}_{n}$ is estimated in the EMM framework.",
    "gold_answer": "1. Compute the score function: $$s(\\tilde{y}_{t-L},\\ldots,\\tilde{y}_{t-1},\\tilde{\\theta}_{n})=\\frac{\\partial}{\\partial\\theta}\\log f(\\tilde{y}_{t}|\\tilde{x}_{t-1},\\tilde{\\theta}_{n}).$$\n2. Average the outer product of the score function over the sample: $$\\widetilde{\\mathcal{I}}_{n}=\\frac{1}{n}\\sum_{t=L+1}^{n}s(\\tilde{y}_{t-L},\\dots,\\tilde{y}_{t-1},\\tilde{\\theta}_{n})s(\\tilde{y}_{t-L},\\dots,\\tilde{y}_{t-1},\\tilde{\\theta}_{n})^{'}.$$",
    "question_context": "The EMM estimator $\\hat{\\rho}$ is computed as follows: The auxiliary model is used to compute the quasi maximum likelihood estimator by fitting the density to the observed data $\\{\\tilde{y}_{t}\\}_{t=-L}^{n}$: $$\\tilde{\\theta}_{n}=\\arg\\operatorname*{max}_{\\theta}\\frac{1}{n}\\sum_{t=L+1}^{n}\\log[f(\\tilde{y}_{t}|\\tilde{y}_{t-L},\\dots,\\tilde{y}_{t-1},\\theta)]$$ and $$=\\arg\\operatorname*{max}_{\\theta}\\frac{1}{n}\\sum_{t=L+1}^{n}\\log[f(\\tilde{y}_{t}|\\tilde{x}_{t-1},\\theta)].$$\nThe corresponding estimate of the information matrix is $$\\widetilde{\\mathcal{I}}_{n}=\\frac{1}{n}\\sum_{t=L+1}^{n}s(\\tilde{y}_{t-L},\\dots,\\tilde{y}_{t-1},\\tilde{\\theta}_{n})$$ where $$s(\\tilde{y}_{t-L},\\ldots,\\tilde{y}_{t-1},\\tilde{\\theta}_{n})=\\frac{1}{n}\\sum_{t=L+1}^{n}\\left[\\frac{\\partial}{\\partial\\theta}\\log f(\\tilde{y}_{t}|\\tilde{x}_{t-1},\\tilde{\\theta}_{n})\\right].$$\nThe EMM estimator is $$\\hat{\\rho}=\\arg\\operatorname*{min}_{\\rho}\\hat{m}_{N}(\\rho,\\hat{\\theta}_{n})^{'}(\\widetilde{\\mathcal{I}})^{-1}\\hat{m}_{N}(\\rho,\\hat{\\theta}_{n}).$$\nUnder the null hypothesis of a correctly specified structural model, the normalized value of the objective function, $$n\\hat{m}_{N}(\\hat{\\rho},\\hat{\\theta}_{n})^{'}(\\widetilde{\\mathcal{I}}_{n})^{-1}\\hat{m}_{N}(\\hat{\\rho},\\hat{\\theta}_{n}),$$ is distributed as $\\chi^{2}$ with $(\\ell_{\\theta}-\\ell_{\\rho})$ df.\n\nThe EMM estimator uses the score function of the auxiliary model (here, the SNP model) as the moment conditions and applies the minimum distance estimation (Hansen 1982). Given a stationary and ergodic process, the EMM estimator is computed using the auxiliary model to fit the density to the observed data."
  },
  {
    "qid": "econ-empirical-424-4-0-2",
    "question": "3) What are the main utility theorems discussed in 'The Utility Theorems Of Wold, Debreu, and Arrow-Hahn' by Beardon and Mehta?",
    "gold_answer": "The main utility theorems are:\n1. **Wold's Theorem**: Establishes conditions under which a preference relation can be represented by a continuous utility function.\n2. **Debreu's Theorem**: Extends Wold's result to more general preference relations, ensuring the existence of a utility function under weaker conditions.\n3. **Arrow-Hahn Theorem**: Provides conditions for the existence of a utility function in infinite-dimensional commodity spaces.\n4. **Implications**: These theorems form the foundation for modern consumer theory and general equilibrium analysis.",
    "question_context": "AGHION, PHILIPPE, MATHIAS DEWATRIPONT, AND PATRICK REY: 'Renegotiation Design with Unverifiable Information.'\nANDREWS, DONALD W. K.: 'Asymptotics for Semiparametric Econometric Models via Stochastic Equicontinuity.'\nBEARDON, ALAN F., AND GHANSHYAM B. MEHTA: 'The Utility Theorems Of Wold, Debreu, and Arrow-Hahn.'\nBENSAID, BERNARD, AND ROBERT J. GARY-BOBO: 'Commitment Value of Contracts Under Renegotiation Constraints.'\nBLAU, DAVID M.: 'Labor Force Dynamics of Older Men.'\nBUCHINSKY, MOSHE: 'Changes in the U.S. Wage Structure 1963-1987: Application of Quantile Regression.'\nCAMPBELL, DONALD E., AND JERRY S. KELLY: 'T or 1-T. That is the Trade-Off.'\nHELPMAN, ELHANAN: 'Innovation, Imitation, and Intellectual Property Rights.'\nIMBENS, GUIDO W., AND JOSHUA D. ANGRIST: 'Identification and Estimation of Local Average Treatment Effects.'\nKEANE, MICHAEL P.: 'A Computationally Practical Simulation Estimator for Panel Data.'\nKRUSE, JAMIE BROWN, STEPHEN RASSENTI, STANLEY S. REYNOLDS, AND VERNON L. SMITH: 'Bertrand-Edgeworth Competition in Experimental Markets.'\nMA, CHING-TO ALBERT, AND MICHAEL MANOVE: 'Bargaining with Deadlines and Imperfect Player Control.'\nMAGDALINOS, MICHAEL A.: 'Testing Instrument Admissibility: Some Refined Asymptotic Results.'\nMcAFEE, R. PRESTON: 'Mechanism Design by Competing Sellers.'\nMILGROM, PAUL, AND CHRIS SHANNON: 'Monotone Comparative Statics.'\nNELSON, DANIEL B., AND DEAN P. FOSTER: 'Asymptotic Filtering Theory for Univariate ARCH Models.'\nOSTROY, JOSEPH M., AND WILLIAM R. ZAME: 'Non-Atomic Economies and the Boundaries of Perfect Competition.'\nPHILLIPS, PETER C. B.: 'Same Exact Distribution Theory for Maximum Likelihood Estimators of Cointegrating Coefficients in Error Correction Models.'\nTODA, HIRO Y., AND PETER C. B. PHILLIPS: 'Vector Autoregressions and Causality.'\nTOWNSEND, ROBERT M.: 'Risk and Insurance in Village India.'\n\nThe following manuscripts have been accepted for publication in forthcoming issues of Econometrica, covering a wide range of topics in econometrics, game theory, labor economics, and more."
  },
  {
    "qid": "econ-empirical-408-2-3-2",
    "question": "3) Show why $\\pi^l(s_i) - \\pi^f(s_i) > 0$ when $\\alpha > \\frac{1 + q^2}{1 + q}$ and $q \\leq \\frac{\\sqrt{5} - 1}{2}$.",
    "gold_answer": "1. **Leader's profit:** $\\pi^l(s_i) = \\frac{\\alpha}{2} + (1 - \\alpha)(2q - 1)q\\frac{1 + q}{2}$.  \n2. **Follower's profit:** $\\pi^f(s_i) = \\frac{\\alpha^2}{2} + \\alpha(1 - \\alpha)(1 - q)\\frac{2q - 1}{2}$.  \n3. **Difference:** $\\pi^l(s_i) - \\pi^f(s_i) = (1 - \\alpha)\\frac{2q - 1}{2}q[\\alpha + q(1 + q)] > 0$ because $2q - 1 > 0$ and $\\alpha > \\frac{1 + q^2}{1 + q}$.",
    "question_context": "Let $q_{1}=q_{2}=q$ and $\\alpha_{1}=\\alpha_{2}=\\alpha$ . Using the equilibrium prices in Lemma 1 and 2, we prove each part in turn.  \n\n$\\frac{q>\\frac{1}{2}:\\mathrm{If}\\alpha<\\widehat{\\alpha}(q)}{{}}$ , which means $q\\neq1$ , then $\\pi^{l}(s_{i})=\\alpha(1-\\alpha)q$ and $\\textstyle\\pi^{f}(s_{i})=\\alpha[\\alpha{\\frac{1-q}{2}}+(1-\\alpha)q]$ , resulting in $\\begin{array}{r}{\\pi^{l}(s_{i})-\\pi^{f}(s_{i})=-\\alpha^{2}\\frac{1-q}{2}<0}\\end{array}$ . If $\\begin{array}{r}{\\alpha\\in[\\widehat{\\alpha}(q),\\frac{1+q^{2}}{1+q}]}\\end{array}$ , then, for $q\\leq\\frac{\\sqrt{5}-1}{2}$ , we have $\\begin{array}{r}{\\pi^{l}(s_{i})=\\frac{\\alpha}{2}}\\end{array}$ , $\\begin{array}{r}{\\pi^{f}(s_{i})=\\frac{\\alpha^{2}}{2}+\\alpha(1-\\alpha)q}\\end{array}$ , and thus $\\pi^{l}(s_{i})-\\pi^{f}(s_{i})=\\alpha(1-\\alpha){\\textstyle\\frac{1-2q}{2}}<0$ , 1w+hqereas for $q>\\frac{\\sqrt{5}-1}{2}$ ,2 we have $\\begin{array}{r}{\\pi^{l}(s_{i})=\\alpha q\\frac{1+q}{2},\\pi^{f}(s_{i})=\\alpha^{2}\\frac{1-q}{2}+\\alpha(1-}\\end{array}$ $\\alpha)q$ , and thus $\\begin{array}{r}{\\pi^{l}(s_{i})-\\pi^{f}(s_{i})=\\frac{\\alpha}{2}(3q-1)[\\alpha-\\overline{{\\alpha}}(q)]}\\end{array}$ , where $\\begin{array}{r}{\\overline{{\\alpha}}(q)=\\frac{q(1-q)}{3q-1}}\\end{array}$ . Finally, if $\\begin{array}{r}{\\alpha>\\frac{1+q^{2}}{1+q}}\\end{array}$ , then for $q\\leq\\frac{\\sqrt{5}-1}{2}$ , we have  \n\n$$\n\\pi^{l}(s_{i})=\\frac{\\alpha}{2}+(1-\\alpha)(2q-1)q\\frac{1+q}{2}\\mathrm{and}\\pi^{f}(s_{i})=\\frac{\\alpha^{2}}{2}+\\alpha(1-\\alpha)(1-q)\\left(\\frac{2q-1}{2}\\right),\n$$  \n\nresulting in $\\begin{array}{r}{\\pi^{l}(s_{i})-\\pi^{f}(s_{i})=(1-\\alpha)\\frac{2q-1}{2}q[\\alpha+q(1+q)]>0}\\end{array}$ . For $q>{\\textstyle\\frac{\\sqrt{5}-1}{2}},\\pi^{l}(s_{i})=\\alpha q{\\textstyle\\frac{1+q}{2}}+(1-\\alpha)(2q-$ $1)q\\frac{1+q}{2}$ and $\\begin{array}{r}{\\pi^{f}(s_{i})=\\alpha^{2}\\frac{1-q}{2}+\\alpha(1-\\alpha)(1-q)(\\frac{2q-1}{2})}\\end{array}$ , resulting in $\\begin{array}{r}{\\pi^{l}(s_{i})-\\pi^{f}(s_{i})>(1-\\alpha)\\frac{2q-1}{2}q[\\alpha+q(1+q)]>0}\\end{array}$  \n\nbecause $q\\textstyle{\\frac{1+q}{2}}>{\\frac{1}{2}}$ and $\\begin{array}{r}{\\frac{1-q}{2}<\\frac{1}{2}}\\end{array}$ . In sum, for $q>{\\frac{1}{2}}$ , we have $s i g n[\\pi^{l}(s_{i})-\\pi^{f}(s_{i})]=s i g n[\\alpha-\\overline{{{\\alpha}}}(q)]$ , where α(q) ={ q13(+q+1q− q1,) , iiff  qq  ≤> √ $q\\leq\\frac{\\sqrt{5}-1}{2}$ . It is easy to verify that $\\overline{{\\alpha}}(q)\\geq\\widehat{\\alpha}(q)$ , with strict inequality for $q\\neq1$ . $q\\leq\\frac{1}{2}$ : Then, we have $\\begin{array}{r}{\\pi^{l}(s_{i})=\\pi^{f}(s_{i})=\\frac{\\alpha}{2}}\\end{array}$ , revealing that $\\pi^{l}(s_{i})-\\pi^{f}(s_{i})=0$ . Q.E.D\n\nThe proof compares the profits of a leader ($s_i$) and follower ($s_j$) seller under different parameter regions, showing when leading is advantageous."
  },
  {
    "qid": "econ-empirical-518-0-1-1",
    "question": "6) Using the merger simulation results from Table 8, explain why fares fell in markets where the merging carriers did not compete (e.g., original TWA routes without Ozark) but rose in markets where they were the only competitors.",
    "gold_answer": "1. **No competition markets (e.g., TWA without Ozark)**:\n   - Fare reduction ($-3.7\\%$) driven by:\n     - Larger network ($NTWCITP4 \\uparrow$): $-2.6\\%$.\n     - Lower 4-segment competition ($NTWCOM4 \\downarrow$): $-2.7\\%$.\n     - Partly offset by lower population potential ($NTWAVGPP \\downarrow$): $+1.6\\%$.\n\n2. **Only-competitor markets (e.g., TWA and Ozark alone)**:\n   - Fare increase ($+3.9\\%$) because:\n     - Loss of competition dominates network effects.\n     - No other carriers counteract the higher market power.",
    "question_context": "The results suggest the following empirical hypotheses: (i) a market served by a large hub-and-spoke network should have lower fares than a market served by a small network; (ii) when economies of density are strong, a market served by a network facing high travel demand should have lower fares than a market served by a low-demand network; (iii) a market where competition occurs should have lower fares than a market without competition; and (iv) a market served by a network facing widespread competition (and thus a large traffic leakage) should have higher fares than a market served by a network facing little competition.\nThe impact of the mergers on 4-segment fares can be studied using the equation estimated above. There are four sources of fare change in a given market when networks are blended as a result of a merger: competition in the market may be reduced; the new network is larger than either of the previous networks (NTWCITP4 rises); the network has different average population potential (NTWAVGPP changes); and it has a different level of 4-segment competition (NTWCOM4 changes).\n\nThe article tests hypotheses derived from the theoretical model, linking network characteristics to fares. It also simulates the effects of airline mergers (TWA-Ozark and Northwest-Republic) on 4-segment fares."
  },
  {
    "qid": "econ-empirical-785-4-3-1",
    "question": "2) Explain the intuitive criterion for equilibrium selection in signaling games, and apply it to a job market signaling model with education and innate ability.",
    "gold_answer": "1. Intuitive criterion: eliminate equilibria where off-path beliefs assign positive probability to types that could not benefit from deviating.\n2. In job market model, high-ability workers may prefer to get more education to separate from low-ability workers.\n3. Only equilibria satisfying \\( s_H \\geq s_L + \\Delta \\) survive, where \\( \\Delta \\) reflects the cost difference.",
    "question_context": "Multidimensional Signalling\nEquilibrium Selection in Signalling Games\nOn Aumann's Notion of Common Knowledge--An Alternative Approach\nSignalling Games\n\nThis section focuses on signaling games, equilibrium selection, and the definition of common knowledge in strategic settings."
  },
  {
    "qid": "econ-empirical-335-1-1-0",
    "question": "3) Derive the closed-form expression for hₑᴹᴹˢᴱ in the locally quadratic case, as given in Lemma 1.",
    "gold_answer": "1. Set up the Lagrangian for the constrained optimization problem: L = ₑ‖∇πδ − E[h sπ]‖² + n⁻¹E[h²] − λ₁(E[h sβγ] − ∇βγδ) − λ₂(E[h sπ] − ∇πδ).\n2. Take derivative w.r.t. h and set to zero: 2ₑ(∇πδ − E[h sπ])sπ + 2n⁻¹h − λ₁sβγ − λ₂sπ = 0.\n3. Solve for h using the projected scores s̃π = sπ − HπβγHβγ⁻¹sβγ: hₑᴹᴹˢᴱ = sβγ′Hβγ⁻¹∇βγδ + s̃π′[H̃π + (ₑn)⁻¹I]⁻¹∇̃πδ.",
    "question_context": "The minimum-MSE estimator of δ_{β₀,π₀} is given by δ̂ₑᴹᴹˢᴱ = δ_{β̂,π(γ̂)} + n⁻¹∑ᵢ₌₁ⁿ hₑᴹᴹˢᴱ(Yᵢ,β̂,γ̂), where hₑᴹᴹˢᴱ minimizes {ₑ‖∇πδ_{β₀,π(γ₊)} − E_{β₀,π(γ₊)}h(Y,β₀,γ₊)∇πlog f_{β₀,π(γ₊)}(Y)‖² + n⁻¹Var_{β₀,π(γ₊)}(h(Y,β₀,γ₊))} subject to (2) and (4).\nWhen ₑ=0, h₀ᴹᴹˢᴱ(y,β₀,γ₊) = ∇βγlog f_{β₀,π(γ₊)}(y)′Hβγ⁻¹∇βγδ_{β₀,π(γ₊)}. As ₑ→∞, lim_{ₑ→∞}hₑᴹᴹˢᴱ(y,β₀,γ₊) = [∇βπlog f_{β₀,π(γ₊)}(y)]′Hβπ⁻¹∇βπδ_{β₀,π(γ₊)}.\n\nThis section characterizes the minimum-MSE estimator δ̂ₑᴹᴹˢᴱ, which minimizes an asymptotic approximation to the worst-case MSE in Γₑ(γ₊). The estimator is derived under constraints of unbiasedness and local robustness."
  },
  {
    "qid": "econ-empirical-1591-0-0-0",
    "question": "1) Formally derive the probit model used to estimate the probability of sending remittances, highlighting the limitations of the linear probability model in this context.",
    "gold_answer": "1. **Probit Model Specification**: The probit model assumes a latent variable framework where the decision to remit \\( y_i^* \\) is a linear function of covariates \\( X_i \\) and a normally distributed error term \\( \\epsilon_i \\sim N(0,1) \\): \n   \\[ y_i^* = X_i \\beta + \\epsilon_i \\]\n   The observed decision \\( y_i \\) is:\n   \\[ y_i = \\begin{cases} 1 & \\text{if } y_i^* > 0 \\\\ 0 & \\text{otherwise} \\end{cases} \\]\n2. **Limitations of Linear Probability Model (LPM)**: \n   - LPM can yield predicted probabilities outside [0,1].\n   - Heteroskedasticity violates OLS assumptions, leading to inefficient estimates.\n   - Hypothesis tests (t-statistics) are invalid due to non-normal errors.",
    "question_context": "The estimation of two separate equations implicitly assumes that migrants take the decision of whether or not to remit and how much to remit sequentially. Maddala (1977, p. 167) has argued that a formulation where the two decisions are made simultaneously is conceptually better than the one which assumes a two-stage framework.\nThe universal patterns of remittance among rural-to-urban migrants appear to be as follows: (1) Factors which determine the decision to remit differ from those which determine the size of the remittance flow. (2) Education and income of remitters are not important in the decision to remit. But once the decision to remit something has been made, the amount remitted increases as education and income of remitters increase.\n\nThis section examines the factors influencing the decision to remit and the amount remitted by migrants in Delhi to their place of origin, comparing findings with studies from Kenya."
  },
  {
    "qid": "econ-empirical-1599-3-1-1",
    "question": "4) Analyze the two cases in the proof of Lemma 4 and explain how state-independence and strict preferences resolve each case.",
    "gold_answer": "**Case 1**: a* →_H b and b ≻_H a* for all i ∈ H.  \n- State-independence implies a ≪ b ⇔ b ≻ a.  \n- By (i)-(iii) and cᵏ ∈ B, the sequence ensures B ⊆ LCS.  \n**Case 2**: a* ≻_H b and Case 1 is false.  \n- If b ∈ B, done. If b ∉ B, ∃ c ∈ B such that c ≻ b ⇒ b ≪ c.  \n- Strict preferences ensure ∃ i ∈ H with a* ≻_i c, resolving the case.",
    "question_context": "Proof of Lemma 4. The proof is by construction. Define a set B ⊆ A as follows: a ∈ B if and only if a ∈ X. We claim that B ⊆ LCS. Choose a* ∈ B. By definition of X, for every b ∈ A and H ⊆ N such that b ≻_H a*, we have (a*, b) ∉ PT, i.e., there exists a sequence {c¹, ..., cᵏ} ⊆ A and i ∈ H such that (i) {c¹, ..., cᵏ} ∩ B = {cᵏ}, (ii) cᵏ ≻ ... ≻ c¹ ≻ b, and (iii) a* ≻_i cᵏ.\n\nThis lemma shows that under state-independence of →_H and strict preferences, the set X is a subset of the Largest Consistent Set (LCS). The proof involves constructing a set B and analyzing cases based on dominance relations."
  },
  {
    "qid": "econ-empirical-1009-5-0-1",
    "question": "2) The coefficient for '(△Imports from China to US)/worker' on unemployment is 0.05* (significant at 5%). Construct a 95% confidence interval for this estimate given the standard error (0.02) and interpret its economic significance.",
    "gold_answer": "1. The 95% CI is calculated as: $0.05 \\pm 1.96 \\times 0.02 = [0.0108, 0.0892]$.\n2. This means we are 95% confident that the true effect of a $1 increase in Chinese imports per worker leads to a 0.0108 to 0.0892 percentage point increase in unemployment.\n3. The lower bound being just above zero suggests the effect, while statistically significant, may have limited economic magnitude at the lower end.",
    "question_context": "Table A1 Effect of Exposure to Chinese Import Competition and Routine-biased Technological Change on Employment Status among Working Age Population, 1990–2007: OLS Estimates\nDependent variable: 10-year equivalent changes in share of working age population in indicated employment status (in percentage points)\nShare of employed in routine occupations: -0.17*** (0.09) for Employed, 0.03 (0.02) for Unemployed, 0.14*** (0.07) for Not in labour force\n(△Imports from China to US)/worker: -0.12 (0.08) for Employed, 0.05* (0.02) for Unemployed, 0.07 (0.08) for Not in labour force\nNotes: N=1,444 (722 commuting zones ×2 time periods). All regressions control for the start of period levels of share of employment in manufacturing, share of population that is college educated, share of population that is foreign born, employment rate among females and Census division dummies. Robust standard errors in parentheses are clustered on state. Models are weighted by start of period commuting zone share of national population. * p≤0.05, *** p≤0.01, *** p≤0.10.\n\nThis section presents OLS estimates examining the effect of exposure to Chinese import competition and routine-biased technological change on employment status among the working-age population from 1990 to 2007."
  },
  {
    "qid": "econ-empirical-66-1-0-0",
    "question": "1) Derive the mathematical relationship between the assumed return on pension assets and the reported net income, using the example provided where a firm has $\\$100$ of operating assets, a 4% return on these assets, and $\\$20$ of pension assets.",
    "gold_answer": "1. **Given**: Operating assets = $\\$100$, Return on operating assets = 4% ($\\$4$), Pension assets = $\\$20$.\n2. **Initial Assumed Return**: 10% on $\\$20$ pension assets = $\\$2$.\n3. **New Assumed Return**: 11% on $\\$20$ pension assets = $\\$2.20$.\n4. **Change in Net Income**: $\\$2.20 - \\$2 = \\$0.20$.\n5. **Percentage Increase**: $(\\$0.20 / \\$4) \\times 100 = 5\\%$.\n\nThus, increasing the assumed return by 1 percentage point increases net income by 5%.",
    "question_context": "A firm sponsoring a defined benefit (DB) pension plan has a liability equal to the present value of all future payments due its employees. The firm funds this liability with devoted pension assets, which are to be managed in the interest of the employee-beneficiaries.\nThe service cost is the present value of benefits earned by the firm’s employees during the current period. The interest cost is the change in the present discounted value of the pension obligations arising from the approach of the time when these obligations come due.\nThe final component of pension expense, the assumed return on plan assets, offsets the interest and service costs. This return is an assumed return rather than the realized rate of return on the plan’s assets.\nThe assumed return merits emphasis given the extreme latitude afforded managers in setting it and the impact it has on reported net income.\nAs a simple example of how the return assumption can be used to affect current reported earnings, consider a firm with $\\$100$ of operating assets, a 4 percent $(\\$4)$ return on these operating assets, and $\\$20$ of pension assets. If this firm changes the assumed return from 10 percent to 11 percent, it can immediately increase net income by 5 percent (or $\\$0.20$).\nThe primary measure of pension sensitivity is the log ratio of pension plan assets to operating earnings which effectively captures the elasticity of reported earnings with respect to the assumed rate of return.\n\nThe text discusses the financial mechanics of defined benefit (DB) pension plans, focusing on how firms calculate annual pension costs, the discretion managers have in setting assumed returns on pension assets, and the implications for reported earnings."
  },
  {
    "qid": "econ-empirical-971-3-1-1",
    "question": "4) Using the three dummy variables for vine age categories ($D_1$, $D_2$, $D_3$), derive the expected quality $Q$ of wine as a linear function $Q = \\gamma_0 + \\gamma_1 D_1 + \\gamma_2 D_2 + \\gamma_3 D_3$. Interpret the coefficients $\\gamma_i$ in terms of age-related quality contributions.",
    "gold_answer": "The linear model is:\n\\[ Q = \\gamma_0 + \\gamma_1 D_1 + \\gamma_2 D_2 + \\gamma_3 D_3 \\]\nInterpretation:\n- $\\gamma_0$: Baseline quality for the reference age category.\n- $\\gamma_1, \\gamma_2, \\gamma_3$: Incremental quality contributions for each age category relative to the reference. For example, if $D_1$ represents the oldest vines, $\\gamma_1$ captures the additional quality from older vines.",
    "question_context": "Old vines produce less but a wine of better quality. Mouton-Rothschild vines for instance are, on average, 43 years old. So are the vines at Lafite-Rothschild, another Pauillac First-Growth. Age, however, does not seem to be necessary. Pichon Lalande, classified as a First-Growth by Parker, has vines the average age of which is 22 years only.\n\nOld vines produce less but a wine of better quality. Vines are classified into three age categories, represented by three dummy variables."
  },
  {
    "qid": "econ-empirical-1144-3-0-2",
    "question": "3) Critically evaluate the assumptions underlying the abortifacient equivalent metric. What are potential limitations in its application to assessing fetal health risks?",
    "gold_answer": "1. **Assumption**: Lead exposure occurs primarily in utero via maternal water consumption.  \n   - Limitation: Ignores other exposure pathways (e.g., environmental lead dust).  \n2. **Assumption**: Lead absorption rates from water and pills are identical.  \n   - Limitation: Bioavailability may differ due to chemical form or maternal physiology.  \n3. **Assumption**: Dose-response is linear and uniform across populations.  \n   - Limitation: Individual variability in metabolism and susceptibility is not accounted for.",
    "question_context": "The abortifacient equivalent is the amount of water an individual needed to consume in order to have been exposed to the same amount of lead as was contained in the recommended daily dose of Dr. —'s Famous Female Pills. Letting $L$ equal the water-lead concentration measured as parts lead per 100,000 parts water, the abortifacient equivalent $(A E)$ can be expressed as: $$ A E=.878/L. $$\nThe derivation of this equation is straightforward. First, lead levels in Massachusetts were originally reported as parts per 100,000 and these data need to be converted to grains per (U.S.) gallon such that: $$ y=(.583)\\times(L), $$ where $y$ equals the lead level expressed as grains per gallon. Second, recalling that the recommended dose contained 0.004 grains of lead, the fraction of a gallon of water necessary to ingest the abortifacient-equivalent $(F)$ is given by the following formula: $$ F=(.004)/y $$ Finally, because an ounce equals (1/128) of a gallon, the ounces of water that need have been consumed to reach the abortifacient-equivalent is given by: $$ A E=(F)\\times(128). $$ Equation 2 follows by substitution.\nTable 6 reports the amount of lead in Massachusetts tap water in terms of the abortifacient equivalent. The data indicate that, in the typical town, a housewife needed to drink around 80 ounces of tap water per day to reach the abortifacient equivalent, assuming she regularly flushed her pipes before drinking tap water. If, however, the housewife did not practice flushing and regularly consumed water allowed to stand in pipes for several hours, she need only have consumed 30 to 40 ounces of tap water per day.\n\nThe econometric results suggest lead water pipes had a large effect on infant and fetal mortality. The abortifacient equivalent is a direct indicator of the toxicity of Massachusetts tap water for the very young, assuming most lead exposure occurred in utero due to maternal consumption of tainted drinking water."
  },
  {
    "qid": "econ-empirical-204-0-0-0",
    "question": "1) Derive the condition under which a primary surplus is required to maintain a stable debt-to-GDP ratio, given a nominal growth rate \\( g \\) and a deficit \\( d \\).",
    "gold_answer": "To maintain a stable debt-to-GDP ratio \\( \\frac{D}{Y} \\), the following must hold: \n\n1. The change in debt \\( \\Delta D = d \\cdot Y \\).\n2. The growth of nominal GDP \\( \\Delta Y = g \\cdot Y \\).\n3. For stability, \\( \\frac{D + \\Delta D}{Y + \\Delta Y} = \\frac{D}{Y} \\).\n\nSolving: \n\\[ \\frac{D + dY}{Y + gY} = \\frac{D}{Y} \\implies D + dY = D(1 + g) \\implies dY = gD \\implies \\frac{D}{Y} = \\frac{d}{g}. \\]\nThus, if \\( \\frac{D}{Y} = 60\\% \\), \\( d = 3\\% \\), and \\( g = 5\\% \\), the primary surplus must cover the interest payments not offset by growth.",
    "question_context": "The British deficit started to rise sharply during the period of ERM membership and there was at least a possibility that fiscal reflation was seen as an alternative to monetary expansion.\nA deficit of 3% p.a. combined with 5% p.a. growth of nominal income is consistent with a debt-to-GDP ratio of 60%, but such a situation will entail a primary surplus unless the debt finances a stock of capital which generates a significant amount of property income.\nIf savers regard government debt as a reasonably good alternative to productive capital, then a high government debt will crowd out private capital, tending to raise interest rates or, in small open economies like the United Kingdom, to lead to foreign indebtedness.\nFor this calculation we suppose (i) that the inflation rate will be 2.3% p.a. (ii) that the trend rate of growth is 2.7% p.a. (iii) that 12.5% of the government’s liabilities and (iv) that 68% of its assets are in financial instruments whose value is indexed to the inflation rate, or physical assets whose price is assumed to rise in line with the RPI.\n\nThe text discusses Britain's chronic budget deficit during the 1990s, its implications for debt-to-GDP ratios, and the potential long-term economic consequences. It also examines the government's net worth, inflation assumptions, and the need for fiscal adjustments to maintain a stable balance sheet position relative to GDP."
  },
  {
    "qid": "econ-empirical-1083-4-1-0",
    "question": "3) Prove that $S_j = 1 - F(W_{0j})$ follows a uniform distribution on [0, 1] under the assumptions of time-invariant reservation wages and invertible $F(\\cdot)$.",
    "gold_answer": "1. Define $F(w) = \\pi(W \\leq w | W > R)$. \\n2. Current wages $W_0$ are drawn from $\\pi(W | W > R)$, so $F(W_0)$ has CDF: \\n   $$ P(F(W_0) \\leq t) = P(W_0 \\leq F^{-1}(t)) = F(F^{-1}(t)) = t. $$ \\n3. Thus, $F(W_0) \\sim \\text{Uniform}(0,1)$, and $S_j = 1 - F(W_0)$ is also uniform.",
    "question_context": "Models of sequential job search with time-invariant reservation wages imply that, within each demographic/schooling group, the responses to the SEE search-outcome question should be distributed uniformly on [0, 1].\nLet $\\pi$ be the distribution of wage offers faced by workers of a given type and $R$ be their reservation wage. The response to the SEE question should be $S_j = 1 - F(W_{0j})$, where $F(w) \\equiv \\pi(W \\leq w | W > R)$.\n\nThe text connects empirical findings on search-outcome expectations to the theoretical prediction of uniform distribution under sequential job-search models with reservation wages."
  },
  {
    "qid": "econ-empirical-94-0-2-1",
    "question": "6) Discuss the 'aggregation trade-off' mentioned in the paper and its implications for estimating quality at different industry levels.",
    "gold_answer": "1. Aggregation trade-off arises because:\n   - Disaggregated industries: Quality is more likely constant across products, but trade balance data are scarce/noisy.\n   - Aggregated industries: Trade balance data are reliable, but quality may vary within industries.\n2. Intermediate input use (e.g., textiles for apparel) can bias quality estimates if not accounted for.\n3. Solution: Balance granularity to minimize bias while maintaining data reliability.",
    "question_context": "We derive a theoretically appropriate price index that aggregates countries’ observed product-level export prices up to the industry level. We refer to this index as the 'Impure Price Index' because it is based on prices that are 'contaminated' by quality.\nWe show that the unobservable Impure Price Index is bounded by observable Paasche and Laspeyres indexes defined over their common exports to a third country.\n\nThe paper introduces the 'Impure Price Index' to aggregate product-level export prices to the industry level, accounting for quality contamination. It also derives bounds for this index using Paasche and Laspeyres indexes."
  },
  {
    "qid": "econ-empirical-348-2-1-3",
    "question": "4) Evaluate the administrative cost trade-offs between national insurance-based repayment systems and traditional loan servicing, considering default rates and taxpayer subsidies.",
    "gold_answer": "Evaluation:\n1. Let \\( C_{ni} \\) be the cost of national insurance system: \\( C_{ni} = c_0 + c_1 \\cdot D \\), where \\( D \\) is default rate.\n2. Traditional servicing cost: \\( C_{ts} = c_2 + c_3 \\cdot D \\).\n3. Trade-off: \\( C_{ni} < C_{ts} \\) if \\( c_0 + c_1 D < c_2 + c_3 D \\).\n4. Solve for critical \\( D^* \\) where systems are cost-equivalent.",
    "question_context": "Income contingent loans (ICLs) are paid back at a rate which depends on the graduate’s income. Lower paid graduates still have to repay the full amount of their loan but they can take longer to do it than higher paid graduates. ICLs appear preferable since they offer protection for lower income graduates and have the advantage of being clearly linked to the cost of the graduate’s education.\nThe default rate in other countries varies, from around 1% in Sweden to 10-14% in the United States. The administration of many of the proposed schemes, including the current student loan system, can be burdensome and costly.\nThe Government’s stated aim is to increase the diversity of choice of courses and locations for students. Is this realistic if students have to pay more of the cost of both tuition and maintenance themselves?\n\nThe text explores various student support mechanisms, including loans, graduate taxes, and income-contingent loans, and their implications for equity, efficiency, and lifelong learning."
  },
  {
    "qid": "econ-empirical-69-4-0-1",
    "question": "2) Prove mathematically why the gender gap in negotiation success rates (conditional on negotiating) cannot be fully explained by differences in socioemotional skills or confidence, referencing Table VI, Panel B.",
    "gold_answer": "1. **Conditional Success Model**:\n   \\[ Success_{i} = \\alpha + \\beta Female_{i} + \\gamma Skills_{i} + \\delta Confidence_{i} + \\epsilon_{i} \\]\n   - Table VI, Panel B shows \\(\\beta = -0.132\\) (13.2 percentage points) even after controlling for skills and confidence.\n\n2. **Statistical Significance**: The p-value for \\(\\beta\\) is <0.05, rejecting the null that socioemotional skills fully mediate the gap.\n\n3. **Marginal Effects**: The negligible change in \\(\\beta\\) when adding skills/confidence covariates (Online Appendix Table AX) further supports this conclusion.",
    "question_context": "Gender differences in bargaining might play an important role in driving the salary gap. Administrative staff and salary data, however, do not allow us to directly test whether women chose not to bargain after Act 10 or whether they bargained but gained less from it.\nWomen are less likely to have negotiated their pay with previous and current employers. For example, 37.9% of men and 29.5% of women report having negotiated with past employers (a 22% difference).\nConditional on having negotiated at the beginning of their current contract, women are 10.5 percentage points less likely than men to state that the negotiation with the current employer at the start of the relationship was successful.\nWomen are 29% less likely than men to know their colleagues’ salaries and 14% less likely to know someone who negotiated their pay.\n\nThe study investigates gender differences in negotiation behaviors among Wisconsin teachers following the introduction of flexible pay, using survey data to distinguish between avoidance of bargaining and differential outcomes from bargaining."
  },
  {
    "qid": "econ-empirical-835-0-0-1",
    "question": "2) Formally analyze how promotional opportunities (arriving at rate \\( \\gamma \\)) alter the firm's Bellman equation for value maximization. Include the cost of disclosure \\( c \\) and the reputational benefit.",
    "gold_answer": "The firm's value function \\( V(\\pi) \\) depends on reputation \\( \\pi \\):\n\n1. **Promotion Decision**: At high quality, promote if \\( \\gamma(V(\\pi') - V(\\pi)) \\geq c \\), where \\( \\pi' \\) is updated reputation post-disclosure.\n2. **Bellman Equation**:\n   \\[ V(\\pi) = \\max_{a \\in \\{0,1\\}} \\left\\{ \\pi r(a) + \\lambda(a)(V(\\pi^+) - V(\\pi)) + \\gamma \\mathbf{1}_{q_H} \\max\\{V(\\pi') - c, V(\\pi)\\} \\right\\} \\]\n   where \\( r(a) \\) is flow profit, \\( \\lambda(a) \\) is investment-dependent news rate, and \\( \\pi^+ \\) is belief drift.",
    "question_context": "A monopolist is selling a product to consumers who don’t observe quality directly. Instead, consumers observe news, signals that arrive at a Poisson rate that depends on the current quality of the firm’s product.\nPromotion provides the firm with a tool to endogenously renew its reputation, which dramatically impacts investment incentives and reputation dynamics.\nPromotion naturally creates incentives for investment. The firm invests in unobserved quality so that it can then promote and charge a higher price.\n\nThe paper presents a model where a firm invests in product quality and influences consumer perception through promotion, which involves stochastic disclosures of quality. The model builds on Board and Meyer-terVehn (2013), incorporating Poisson processes for news arrival and promotional opportunities."
  },
  {
    "qid": "econ-empirical-1018-2-1-2",
    "question": "7) Derive the conditions under which hypothesis (c) holds in a theoretical model, explaining why demand growth stimulates R&D.",
    "gold_answer": "1. Demand growth increases future profits, raising returns to R&D.  \n2. Even if oligopoly profits rise, the marginal benefit of cost-reducing R&D increases.  \n3. No known theoretical model contradicts (c).  \n4. Intuition aligns with dynamic optimization of R&D investment.",
    "question_context": "(a) Large firms undertake proportionately greater R& D than do small firms. (b) There is a positive association between the degree of concentration in an industry and innovative activity within it. (c) Growth in demand for the products of an industry stimulates R & D activity within it.\nThe evidence suggests that beyond a threshold size, (a) does not hold. Proportionately the size of firms does not influence R & D activity.\nHypothesis (b) is controversial, for the evidence is mixed. There is some evidence though that measures of R & D input relative to the industry's sales achieve their maximum at moderate levels of concentration.\nHypothesis (c) is the most intuitive. But it is by no means obvious. If demand is expected to grow, oligopoly profits might in any case be expected to increase. In this case one may argue that the incentive to undertake R & D expenditure will lessen, not increase.\n\nThis section reviews empirical hypotheses about firm size, industrial concentration, and R&D activity, highlighting the challenges of interpreting data without precise theoretical models."
  },
  {
    "qid": "econ-empirical-314-1-1-2",
    "question": "7) Explain the role of the adjustment term Λ_T in ensuring finite-sample size performance and why it is asymptotically negligible.",
    "gold_answer": "1. Λ_T corrects for the finite-sample bias in √T Ψ_T(μ̂_j)μ̂_j. \\n2. As T → ∞, Λ_T ⟶^p 0 by [A4] and [A6]. \\n3. Thus, Λ_T improves finite-sample size without affecting asymptotic properties.",
    "question_context": "[D1] √T(μ̂ − μ) ⟶^d N(0, V) where V is some finite positive semidefinite matrix. [D2] VΔd(μ) ≠ 0 for non-zero d(μ). [D3] V̂ ⟶^p V. [D4] Δ̂ ⟶^p Δ.\nTheorem 1: Given [A1], [A2], [A3], [A4], [A6] with [D1]–[D4], under H_0: μ_j ≥ 0 for all j ∈ J: (1) If M ≠ ∅, then Q ⟶^d U(0,1). (2) If M = ∅, then Q ⟶^p 1.\n\nThis section discusses the high-level assumptions [D1]–[D4] and the asymptotic properties of the test statistic under the null hypothesis."
  },
  {
    "qid": "econ-empirical-1391-2-1-1",
    "question": "4) How did the establishment of SASAC lead to a narrowing of the profit rate gap between SOEs and non-SOEs, as shown in Figure 1?",
    "gold_answer": "SASAC's measures reduced managerial expropriation and shirking, leading to improved SOE performance. The profit rate gap narrowed from 6% in 2003 to 3% in 2007 due to stronger monitoring, reduced corruption, and better governance.",
    "question_context": "To strengthen monitoring and management of SOEs, the State Council of China announced the establishment of SASAC in March 2003 as the legal owner of state-owned assets. Its hierarchy consists of central, provincial and prefecture-level SASAC offices. The central SASAC was established in March 2003; the provincial and prefecture-level SASAC offices were established later.\nThe main functions of SASAC are to perform investors’ responsibilities, supervise SOEs and monitor state-owned assets. It took several specific and complementary measures to achieve these goals. First, SASAC improved the assessment criteria and index system to ensure the preservation and growth of state-owned assets. Based on this system, SASAC uses statistics and auditing to implement effective monitoring of SOEs. Second, SASAC helps SOEs to establish a modern enterprise system to improve corporate governance. Third, SASAC is responsible for appointing, evaluating and removing top executives of SOEs based on their performance.\n\nThe establishment of SASAC in 2003 aimed to strengthen monitoring and management of SOEs by centralizing oversight and implementing specific measures to improve corporate governance and reduce managerial expropriation."
  },
  {
    "qid": "econ-empirical-96-2-0-0",
    "question": "1) Derive the wage decomposition formula $V a r(y_{i t})$ from the model $y_{i t}=\\alpha_{i}+\\psi_{J(i,t)}+x_{i t}^{\\prime}\\beta+r_{i t}$, showing all intermediate steps.",
    "gold_answer": "1. Start with $y_{i t}=\\alpha_{i}+\\psi_{J(i,t)}+x_{i t}^{\\prime}\\beta+r_{i t}$. \\n2. Compute $V a r(y_{i t}) = V a r(\\alpha_{i} + \\psi_{J(i,t)} + x_{i t}^{\\prime}\\beta + r_{i t})$. \\n3. Expand using variance and covariance properties: \\n   - $V a r(y_{i t}) = V a r(\\alpha_{i}) + V a r(\\psi_{J(i,t)}) + V a r(x_{i t}^{\\prime}\\beta) + V a r(r_{i t})$ \\n   - $+ 2C o v(\\alpha_{i}, \\psi_{J(i,t)}) + 2C o v(\\psi_{J(i,t)}, x_{i t}^{\\prime}\\beta) + 2C o v(\\alpha_{i}, x_{i t}^{\\prime}\\beta)$ \\n   - $+ 2C o v(\\alpha_{i}, r_{i t}) + 2C o v(\\psi_{J(i,t)}, r_{i t}) + 2C o v(x_{i t}^{\\prime}\\beta, r_{i t})$. \\n4. Under orthogonality assumptions, $C o v(\\alpha_{i}, r_{i t}) = C o v(\\psi_{J(i,t)}, r_{i t}) = C o v(x_{i t}^{\\prime}\\beta, r_{i t}) = 0$. \\n5. Final decomposition: $V a r(y_{i t}) = V a r(\\alpha_{i}) + V a r(\\psi_{J(i,t)}) + V a r(x_{i t}^{\\prime}\\beta) + 2C o v(\\alpha_{i}, \\psi_{J(i,t)}) + 2C o v(\\psi_{J(i,t)}, x_{i t}^{\\prime}\\beta) + 2C o v(\\alpha_{i}, x_{i t}^{\\prime}\\beta) + V a r(r_{i t})$.",
    "question_context": "The log daily real wage $y_{i t}$ of individual $i$ in year $t$ is the sum of a worker component $\\alpha_{i}$, an establishment component $\\psi_{\\mathbf{\\delta}}\\mathbf{\\delta}_{\\psi(i,t)}$, an index of time-varying observable characteristics $x_{i t}^{\\prime}\\beta$, and an error component $r_{i t}$: $y_{i t}=\\alpha_{i}+\\psi_{J(i,t)}+x_{i t}^{\\prime}\\beta+r_{i t}$.\nThe error term $r_{i t}$ consists of three separate random effects: a match component $\\eta_{i\\mathbf{J}(i,t)}$, a unit root component $\\zeta_{i t}$, and a transitory error $\\varepsilon_{i t}$: $r_{i t}=\\eta_{i\\mathbf{J}(i,t)}+\\zeta_{i t}+\\varepsilon_{i t}$.\nThe model can be written in matrix notation as $y=D\\alpha+F\\psi+X\\beta+r = Z^{\\prime}\\xi+r$, where $Z\\equiv[D,F,X]$ and $\\xi\\equiv[\\alpha^{\\prime},\\psi^{\\prime},\\beta^{\\prime}]^{\\prime}$.\nFor OLS to identify the underlying parameters, the orthogonality conditions $E\\big[d^{i\\prime}r\\big]=0\\forall i,E\\big[f^{j\\prime}r\\big]=0\\forall j,E\\big[x^{k\\prime}r\\big]=0\\forall k$ must hold.\nThe variance of observed wages can be decomposed as $V a r(y_{i t})=V a r(\\alpha_{i})+V a r\\big(\\psi_{J(i,t)}\\big)+V a r\\big(x_{i t}^{\\prime}\\beta\\big)+2C o v\\big(\\alpha_{i},\\psi_{J(i,t)}\\big)+2C o v\\big(\\psi_{J(i,t)},x_{i t}^{\\prime}\\beta\\big)+2C o v\\big(\\alpha_{i},x_{i t}^{\\prime}\\beta\\big)+V a r(r_{i t})$.\n\nThe econometric framework aims to disentangle wage variation components attributable to worker-specific and employer-specific heterogeneity. The model includes worker effects, establishment effects, time-varying covariates, and error components."
  },
  {
    "qid": "econ-empirical-739-2-0-0",
    "question": "1) Derive the Pearson's classification coefficient using the estimated moments $\\hat{M}_{1}$ through $\\hat{M}_{4}$ and explain how it indicates a Beta-distribution for the lag distribution.",
    "gold_answer": "1. Pearson's classification coefficient is given by: \\[ \\kappa = \\frac{\\hat{M}_{3}^2 (\\hat{M}_{4} + 3 \\hat{M}_{2}^2)}{4 (4 \\hat{M}_{2}^3 - \\hat{M}_{3}^2) (2 \\hat{M}_{2}^3 - \\hat{M}_{3}^2)} \\]\n2. For a Beta-distribution, the coefficient satisfies specific conditions (e.g., bounded support and skewness).\n3. The estimated moments from Table 1 yield a coefficient consistent with a Beta-distribution, as confirmed by the analysis.",
    "question_context": "Ad hoc prior restrictions such as polynomial lags are often used to reduce variances of estimated short-run effects in lag distributions. The danger of ad hoc priors lies, of course, in potential bias and in the failure of constrained results to reveal the severity of bias because of low power of tests.\nThe procedure is to use Pearson's method of equating moments to identify and fit frequency functions to data. Using Pearson's method as discussed in Kendall and Stuart (1958), we used the estimated moments of the lag distribution (table 1), $\\hat{M}_{1}$ through $\\hat{M}_{4}$ to compute Pearson's classification coefficient. In each case for $N+1=18$, 19 or 20, the classification coefficient indicated a Beta-distribution.\nEstimates of lag weights were obtained by numerically integrating the estimated Beta-distribution. I.e., $W_{0}$, the contemporaneous weight is the integral from zero to one, $W_{1}$, the lag weight for wholesale price changes lagged one period is the integral from one to two, etc.\nFrom numerical integration of the Beta functions, the median lag is 1.6 months for $N+1=18$ and 1.8 months for $N+1=19$, 20. For the latter choices of $N$, the number of included lags, the fitted Beta functions turn up slightly in the tail. Otherwise, all three distributions are similar.\n\nThe section discusses the use of ad hoc prior restrictions like polynomial lags to reduce variances of estimated short-run effects in lag distributions. It highlights the potential bias and low power of tests as drawbacks. An alternative procedure using Pearson's method of equating moments to fit frequency functions to data is illustrated, with a focus on Beta-distribution fitting."
  },
  {
    "qid": "econ-empirical-559-1-1-0",
    "question": "3) Formally derive the consumer's optimization problem in the first subperiod, including the budget constraint and the first-order conditions. How does the consumer's choice of $\\bar{y}_t$ and $\\bar{a}_t$ depend on the prices $p_{1t}$ and $q_t$?",
    "gold_answer": "The consumer solves:\n$$\n\\max_{(\\bar{y}_t, \\bar{a}_t) \\in \\mathbb{R}_+^2 \\times \\mathbb{R}} \\left[u(\\bar{y}_t) + W_t^C(\\bar{a}_t)\\right] \\text{ s.t. } \\bar{a}_t^m + p_{1t}\\bar{y}_t + q_t\\bar{a}_t^b \\leq a_t^m.\n$$\n**First-Order Conditions**:\n1. $u'(\\bar{y}_t) = \\lambda p_{1t}$\n2. $\\partial W_t^C / \\partial \\bar{a}_t^m = \\lambda$\n3. $\\partial W_t^C / \\partial \\bar{a}_t^b = \\lambda q_t$\nThe consumer allocates spending to maximize utility, balancing consumption ($\\bar{y}_t$) and bond holdings ($\\bar{a}_t^b$).",
    "question_context": "The individual preferences of an agent of type $i\\in\\{B,C,P\\}$ are represented by $$\\mathbb{E}_{0}^{i}\\sum_{t=0}^{\\infty}\\beta^{t}\\big[\\boldsymbol{u}(y_{i t})\\mathbb{I}_{\\{i=C\\}}-\\kappa y_{i t}\\mathbb{I}_{\\{i=P\\}}+\\boldsymbol{v}(x_{t})-h_{t}\\big],$$ where $\\beta\\in(0,1)$ is the discount factor.\nDEFINITION 1: An equilibrium is a sequence of prices, $\\{p_{t},q_{t}\\}_{t\\in\\mathbb{T}}$ , end-of-period money holdings, $\\{a_{i t+1}^{m}\\}_{i\\in\\{B,C,P\\},t\\in\\mathbb{T}}$ , and production, supply, consumption, portfolios, and fees in the first subperiod, such that for all $t\\in\\mathbb{T}$: (i) taking prices and the bargaining protocol as given, the end-of-period money holdings solve (4) for $i\\in\\{B,C,P\\}$; (ii) the asset holdings and fees in the first subperiod solve (5), (6), (7), (8); (iii) beginning-of-period production $y_{P t}(\\cdot)$ satisfies (11); and (iv) prices are such that all Walrasian markets clear.\n\nAgents optimize their consumption, production, and portfolio choices. Bargaining between producers and bankers determines the terms of trade. Equilibrium is defined by market clearing and optimal behavior."
  },
  {
    "qid": "econ-empirical-95-1-0-3",
    "question": "4) Compare and contrast the manipulation-proof performance measures (e.g., constant relative risk aversion utility functions) with the impossibility of designing manipulation-proof compensation schemes. Explain the theoretical distinction.",
    "gold_answer": "1. Define manipulation-proof performance measures as utility functions averaged over returns history.\\n2. Show that these measures can discriminate between skilled and unskilled managers in the long run.\\n3. Explain why compensation schemes must also satisfy participation constraints for both types of managers.\\n4. Conclude that the additional constraints make it impossible to design a manipulation-proof compensation scheme.",
    "question_context": "We show that it is very difficult to devise performance-based compensation contracts that reward portfolio managers who generate excess returns while screening out managers who cannot generate such returns.\nTheoretical bounds are derived on the amount of fee manipulation that is possible under various performance contracts.\nWe show that recent proposals to reform compensation practices, such as postponing bonuses and instituting clawback provisions, will not eliminate opportunities to game the system unless accompanied by transparency in managers’ positions and strategies.\n\nThe paper discusses the challenges in designing performance-based compensation contracts for portfolio managers that can distinguish between skilled and unskilled managers based solely on their returns histories."
  },
  {
    "qid": "econ-empirical-489-4-0-3",
    "question": "4) Derive the penalty level $\\lambda$ in (6.5) for the Lasso estimator and explain how it controls selection errors uniformly over $u\\in\\mathcal{U}$.",
    "gold_answer": "1. **Derivation**: The penalty level $\\lambda = c\\sqrt{n}\\Phi^{-1}\\big(1-\\gamma/\\big\\{2p n^{d_{u}}\\big\\}\\big)$ is chosen to ensure that, with high probability, $\\frac{\\lambda}{n} \\geq c\\operatorname*{sup}_{u\\in\\mathcal{U}}\\Big\\Vert\\hat{\\Psi}_{u}^{-1}\\mathbb{E}_{n}\\big[\\partial_{\\theta}M\\big(Y_{u},f(X)^{\\prime}\\theta_{u}\\big)\\big]\\Big\\Vert_{\\infty}$. This controls the maximum correlation between the score and covariates.\\n2. **Uniform Control**: By setting $\\lambda$ to dominate the score uniformly over $u\\in\\mathcal{U}$, the Lasso estimator avoids overfitting and ensures consistent model selection across all $u$. The choice of $\\gamma$ and $c$ balances tightness and robustness.",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nIn what follows, we shall denote by $\\delta,c_{0},c,$ and $c$ some positive constants. For a positive integer $d,[d]$ denotes the set $\\{1,...,d\\}$ . We shall impose the following regularity conditions.\nAsSUMPTION 5.1—-Moment-Condition Problem: Consider a random element $W$ taking values in a measure space $(\\mathcal{W},\\mathcal{A}_{\\mathcal{W}})$ , with law determined by a probability measure $\\boldsymbol{P}\\in\\mathcal{P}_{n}$ The observed data $((W_{u i})_{u\\in\\boldsymbol{\\mathcal{U}}})_{i=1}^{n}$ consist of n i.i.d. copies of a random element $(W_{u})_{u\\in\\mathcal{U}}$ which is generated as a suitably measurable transformation with respect to W and u. Uniformly for all $n\\geq n_{0}$ and $\\boldsymbol{P}\\in\\mathcal{P}_{n}$ ,the following conditions hold: (i) The true parameter value $\\theta_{u}$ obeys (5.1) and is interior relative to $\\mathcal{\\Theta}_{u}\\subset\\mathcal{\\Theta}\\subset\\mathbb{R}^{d_{\\theta}}$ , namely there is a ball of radius 8 centered at $\\theta_{u}$ $\\boldsymbol{\\Theta}_{u}$ $u\\in\\mathcal{U}$ $\\pmb{\\theta}$ $\\begin{array}{r}{\\nu:=(\\nu_{k})_{k=1}^{d_{\\theta}+d_{t}}=(\\theta,t)}\\end{array}$ $j\\in[d_{\\theta}]_{\\cdot}$ $u\\in\\mathcal{U}$ $\\begin{array}{r}{\\Theta_{u}\\times T_{u}(Z_{u})\\ni\\nu\\longmapsto\\mathrm{E}_{P}[\\psi_{u j}(W_{u},\\nu)|Z_{u}]}\\end{array}$ differentiable a.s. with derivatives obeying the integrability conditions specified in Assumption 5.2. (iii) For all $u\\in\\mathcal{U}$ , the moment function $\\psi_{u}$ obeys the orthogonality condition given in Definition 5.1 for the set $\\mathcal{H}_{u}=\\mathcal{H}_{u n}$ specified in Assumption 5.3. (iv) The following identifiability condition holds: $\\begin{array}{r}{\\|\\mathrm{E}_{P}[\\psi_{u}(W_{u},\\theta,h_{u}(Z_{u}))]\\|\\ge2^{-1}(\\|J_{u}(\\theta-\\theta_{u})\\|\\wedge c_{0})}\\end{array}$ for all $\\pmb{\\theta}\\in\\pmb{\\theta}_{u}$ ， where the singular values of $J_{u}:=\\partial_{\\theta}\\mathrm{E}[\\psi_{u}(W_{u},\\theta_{u},h_{u}(Z_{u}))]$ lie between c and $c$ for all $u\\in\\mathcal{U}$.\nAsSUMPTION 5.2—-Entropy and Smoothness: The set $(\\mathcal{U},d_{\\mathcal{U}})$ is a semimetric space such that $\\log N(\\epsilon,\\mathcal{U},d_{\\mathcal{U}})\\leq C\\log(\\epsilon/\\epsilon)\\vee0.$ Let $\\alpha\\in[1,2]$ , and let $\\alpha_{1}$ and $\\alpha_{2}$ be some positive constants. Uniformly for all $n\\geq n_{0}$ and $\\boldsymbol{P}\\in\\mathcal{P}_{n}$ , the following conditions hold: (i) The set of functions $\\mathcal{F}_{0}=\\{\\psi_{u j}(W_{u},\\theta_{u},h_{u}(Z_{u})):j\\in[d_{\\theta}],u\\in\\mathcal{U}\\}$ ,viewed as functions of $W$ , is suitably measurable;has an envelope function $\\begin{array}{r}{F_{0}(W)=\\operatorname*{sup}_{j\\in[d_{\\theta}],u\\in\\mathcal{U},\\nu\\in\\Theta_{u}\\times T_{u}(Z_{u})}|\\psi_{u j}(W_{u},\\nu)|}\\end{array}$ that is measurable with respect to $W$ and obeys $\\|F_{0}\\|_{P,q}\\leq C$ ,where $q\\geq4$ is $\\pmb{a}$ fixed constant; and has a uniform covering entropy obeying $\\mathrm{sup}_{O}\\mathrm{log}N(\\epsilon\\|F_{0}\\|_{Q,2},\\mathcal{F}_{0},\\|\\cdot\\|_{Q,2})\\le C\\log(\\mathrm{e}/\\epsilon)\\vee0$ (ii) For all $j\\in[d_{\\theta}]$ and $k,r\\in[d_{\\theta}+d_{t}]$ and $\\mathbf{\\tilde{\\psi}}_{u j}(W):=\\psi_{u j}(W_{u},\\theta_{u},h_{u}(Z_{u}))$.\nAssUMPTION 5.3—Estimation of Nuisance Functions: The following conditions hold for each $n\\geq n_{0}$ and all $\\boldsymbol{P}\\in\\mathcal{P}_{n}$ .Thestimated functions $\\hat{h}_{u}=(\\hat{h}_{u m})_{m=1}^{d_{t}}\\in\\mathcal{H}_{u n}$ with probabilityat least $1-\\varDelta_{n}$ ,where $\\mathcal{H}_{u n}$ is the set of measurablemaps $\\mathcal{Z}_{u}\\ni z\\longmapsto h=(h_{m})_{m=1}^{d_{t}}(z)\\in T_{u}(z)$ such that $\\|h_{m}-h_{u m}\\|_{P,2}\\leq\\tau_{n},\\quad\\tau_{n}^{2}\\sqrt{n}\\leq\\delta_{n},$ and whose complexity does not grow too quickly in the sense that $\\mathcal{F}_{1}=\\{\\psi_{u j}(W_{u},\\theta,h(Z_{u})):$ $j\\in[d_{\\theta}],u\\in\\mathcal{U},\\theta\\in\\theta_{u},h\\in\\mathcal{H}_{u n}\\}$ is suitably measurable and its uniform covering entropy obeys $\\operatorname*{sup}_{Q}\\log N\\bigl(\\epsilon\\|F_{1}\\|_{Q,2},\\mathcal{F}_{1},\\|\\cdot\\|_{Q,2}\\bigr)\\leq s_{n}\\bigl(\\log(a_{n}/\\epsilon)\\bigr)\\vee0,$ where $F_{1}(W)$ is an envelope for $\\mathcal{F}_{1}$ which is measurable with respect to $W$ and satisfies $F_{1}(W)\\leq F_{0}(W)$ for $F_{0}$ defined in Assumption 5.2. The complexity characteristics ${\\pmb a}_{n}\\ge$ $\\mathtt{m a x}(n,{\\mathtt{e}})$ and $s_{n}\\geq1$ obey the growth conditions $n^{-1/2}\\big(\\sqrt{s_{n}\\log(a_{n})}+n^{-1/2}s_{n}n^{1/q}\\log(a_{n})\\big)\\leq\\tau_{n}\\quad a n d$ $\\tau_{n}^{\\alpha/2}\\sqrt{s_{n}\\log(a_{n})}+s_{n}n^{1/q-1/2}\\log(a_{n})\\log n\\leq\\delta_{n},$ where $q$ and $_{\\alpha}$ are defined in Assumption 5.2.\nTHEOREM 5.1—-Uniform Functional Central Limit Theorem for a Continuum of Target Parameters in Moment-Condition Problems: Under Assumptions 5.1, 5.2, and 5.3, for an estimator $(\\hat{\\theta}_{u})_{u\\in\\mathcal{U}}$ that obeys equation(5.4), $\\sqrt{n}(\\widehat{\\theta}_{u}-\\theta_{u})_{u\\in\\mathcal{U}}=(\\mathbb{G}_{n}\\bar{\\psi}_{u})_{u\\in\\mathcal{U}}+o_{P}(1)$ in $\\ell^{\\infty}(\\mathcal{U})^{d_{\\theta}}$ , uniformly in $\\boldsymbol{P}\\in\\mathcal{P}_{n}$ ,where $\\bar{\\psi}_{u}(W):=-J_{u}^{-1}\\psi_{u}(W_{u},\\theta_{u},h_{u}(Z_{u}))$ ,and $Z_{n,P}:=(\\mathbb{G}_{n}\\bar{\\psi}_{u})_{u\\in\\mathcal{U}}\\rightarrow Z_{P}:=(\\mathbb{G}_{P}\\bar{\\psi}_{u})_{u\\in\\mathcal{U}}\\quad i n\\quad\\ell^{\\infty}(\\mathcal{U})^{d_{\\theta}}$ where the paths of $u\\mapsto\\mathbb{G}_{P}\\bar{\\psi}_{u}$ are a.s. uniformly continuous on $(\\mathcal{U},d_{\\mathcal{U}})$ and $\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{P\\in\\mathcal{P}_{n}}\\mathrm{E}_{P}\\operatorname*{sup}_{u\\in\\mathcal{U}}\\|\\mathbb{G}_{P}\\bar{\\psi}_{u}\\|<\\infty\\quad a n d}\\ &{\\displaystyle\\operatorname*{lim}_{\\delta\\to0}\\mathrm{sup}\\mathrm{E}_{P}\\operatorname*{sup}_{d\\mathcal{U}(u,\\bar{u})\\leq\\delta}\\|\\mathbb{G}_{P}\\bar{\\psi}_{u}-\\mathbb{G}_{P}\\bar{\\psi}_{\\bar{u}}\\|=0.}\\end{array}$\nTHEOREM 5.2—Uniform Validity of Multiplier Bootstrap: Suppose Assumptions 5.1, 5.2, and 5.3 hold, the estimator $(\\hat{\\theta}_{u})_{u\\in\\mathcal{U}}$ obeys equation (5.4), and that the estimator $(\\hat{J}_{u})_{u\\in\\mathcal{U}}$ obeys the following condition: uniformly in $\\boldsymbol{P}\\in\\mathcal{P}_{n}$ with probability $1-\\delta_{n}$ ， $\\begin{array}{r}{\\mathbf{sup}_{u\\in\\mathcal{U}}\\|\\hat{J}_{u}-J_{u}\\|\\leq}\\end{array}$ $\\varDelta_{n}$ .Then, $\\begin{array}{r}{\\hat{Z}_{n,P}^{*}\\rightsquigarrow_{B}Z_{P}\\quad i n\\quad\\ell^{\\infty}(\\mathcal{U})^{d_{\\theta}},u n i f o r m l y i n P\\in\\mathcal{P}_{n}.}\\end{array}$\nTHEOREM 5.3-Uniform Limit Theory and Validity of Multiplier Bootstrap for Smooth Functionals of $\\pmb{\\theta}$ : Suppose that for each $\\begin{array}{r}{P\\in\\mathcal{P}:=\\bigcup_{n\\geq n_{0}}\\mathcal{P}_{n;}}\\end{array}$ $\\tilde{\\theta}^{0}=\\theta_{P}^{0}$ is an element of $\\pmb{a}$ compact set $\\mathbb{D}_{\\theta}$ . Suppose $\\theta\\longmapsto\\phi(\\theta)$ ， $^{a}$ functional of interest mapping. $\\mathbb{D}_{\\phi}\\subset\\mathbb{D}=\\ell^{\\infty}(\\mathcal{U})^{d_{\\theta}}$ to $\\ell^{\\infty}(\\mathcal{Q})$ , where $\\mathbb{D}_{\\theta}\\subset\\mathbb{D}_{\\phi}$ , is Hadamard differentiable in $\\pmb{\\theta}$ tangentially to $\\mathbb{D}_{0}=U C(\\mathcal{U})^{d_{\\theta}}$ uniformly in $\\theta\\in\\mathbb{D}_{\\theta}$ , with the linear derivative map $\\phi_{\\theta}^{\\prime}:\\mathbb{D}_{0}\\longmapsto\\mathbb{D}$ such that the mapping $(\\theta,h)\\longmapsto\\phi_{\\theta}^{\\prime}(h)$ from $\\mathbb{D}_{\\theta}\\times\\mathbb{D}_{0}$ to $\\ell^{\\infty}(\\mathcal{Q})$ is continuous. Then, $\\begin{array}{r}{\\sqrt{n}(\\hat{A}-A)\\rightarrow T_{P}:=\\phi_{\\theta_{P}^{0}}^{\\prime}(Z_{P})\\quad i n\\quad\\ell^{\\infty}(\\mathcal{Q}),u n i f o r m l y i n P\\in\\mathcal{P}_{n},}\\end{array}$ where $T_{P}$ is a zero mean tight Gaussian process, for each $P\\in\\mathcal{P}$ .Moreover, $\\begin{array}{r}{\\sqrt{n}\\big(\\hat{\\Delta}^{*}-\\hat{\\Delta}\\big)\\prec_{B}T_{P}\\quad i n\\quad\\ell^{\\infty}(\\mathcal{Q}),u n i f o r m l y i n P\\in\\mathcal{P}_{n}.}\\end{array}$\n\nThis section outlines the regularity conditions and theoretical results for moment-condition problems with functional response data, focusing on Lasso and Post-Lasso estimators."
  },
  {
    "qid": "econ-empirical-1450-2-0-1",
    "question": "2) Show how the relationship between sales and ads changes under the new assumption, deriving the expression $\\hat{\\pi}(p) = \\phi \\cdot \\pi(p)$.",
    "gold_answer": "1. **Proportion of Buyers**: Let $\\phi$ be the proportion of consumers who receive at least one ad, and $1-\\phi$ the proportion who search. \\n2. **Sales Due to Search**: By assumption (viiia), a proportion $1-\\phi$ of each seller's sales is due to search, and $\\phi$ is due directly to ads. \\n3. **Definition of $\\hat{\\pi}(p)$**: $\\hat{\\pi}(p)$ is the probability that an ad at price $p$ goes to a susceptible buyer. Since only $\\phi$ of sales are due to ads, $\\hat{\\pi}(p) = \\phi \\cdot \\pi(p)$, where $\\pi(p) = s(p)/a(p)$.",
    "question_context": "The most popular assumption in the current literature is that the nth search costs $c(n)$ : a constant or increasing function of $\\pmb{n}$ and that there is an equal probability of searching any given store.\nAn alternative assumption will be adopted in this section, namely that the probability of locating any given seller is proportional to his sales.\nAssumption (viiia). All buyers have identical cost of search functions $c(n)$, which are constant or increasing in $\\pmb{n}$. The probability that a search yields an offer from any particular seller is equal to his share in total sales.\nIn equilibrium, there will be a specific cut-off price, $\\pmb{p}_{c}$, above which the expected value of search will be greater than its cost, so that he will accept no price above this level.\nThe relationship between sales and ads is affected by the new assumption. Let $\\phi$ denote the proportion of consumers who receive at least one ad. Then $1-\\phi$ is the proportion of buyers who search.\nThe analysis proceeds with the equations $P(p)=(p-p_{0})\\cdot\\pi(p)-b=0$ and $\\pi(p)=b/(p-p_{0})$.\nThe final equation is given by the fact that the expected gain from search at the cut-off price, $p_{\\mathtt{max}}$, must be equal to the cost of search: $c=\\int_{p_{\\operatorname*{min}}}^{p_{\\operatorname*{max}}}(p_{\\operatorname*{max}}-p)\\frac{b}{(p-p_{0})^{2}}dp$.\nComparative statics are analyzed by differentiating equations with respect to $b$ and $c$, yielding results such as $\\frac{\\partial d}{\\partial c}=\\frac{b+d}{d}=\\frac{1}{\\phi}>0$.\n\nThis section explores the impact of buyers' search on the model, considering different search cost functions and probabilities of locating sellers. It introduces an alternative assumption where the probability of locating a seller is proportional to his sales, modeling 'word-of-mouth' search. The analysis modifies the basic model by replacing an assumption and derives equilibrium conditions, price distributions, and comparative statics."
  },
  {
    "qid": "econ-empirical-1479-0-1-3",
    "question": "4) Discuss the problem of heteroskedasticity in the context of IV estimation. Propose a robust estimation strategy and derive its asymptotic properties.",
    "gold_answer": "1. **Problem**: Heteroskedasticity violates the assumption \\( E[u^2 | Z] = \\sigma^2 \\), leading to inefficient IV estimates.\n2. **Robust strategy**: Use GMM with a heteroskedasticity-consistent weight matrix, \\( W = (Z' \\hat{\\Omega} Z)^{-1} \\), where \\( \\hat{\\Omega} = \\text{diag}(\\hat{u}_i^2) \\).\n3. **Asymptotics**: The estimator \\( \\hat{\\delta}_{GMM} \\) is consistent and asymptotically normal with variance \\( (G' W G)^{-1} G' W \\Omega W G (G' W G)^{-1} \\), where \\( G = \\partial E[Z'u]/\\partial \\delta \\).",
    "question_context": "The book is, of course, not entirely about the simultaneous-equation model, since the single-equation model can be, and often is, introduced as a special case. But the main thrust of the book concerns the simultaneous-equation model and methods for its estimation ranging from OLS, GLS through 2SLS and IV to $\\mathfrak{3S L S}$ , LIML and FIML, together with all the asymptotic properties (all in the space of 175 pages).\nThere is also considerable attention to such problems as serial correlation and heteroskedasticity, mis-specification testing. estimation with restrictions and estimation of models with stochastic regressors.\n\nThe text covers advanced econometric techniques, focusing on the asymptotic theory of linear simultaneous-equation models, estimation methods, and hypothesis testing."
  },
  {
    "qid": "econ-empirical-327-0-0-0",
    "question": "1) Using the data from Table I, derive the annual growth rate of regular memberships from 1989 to 1991, given the 32% increase over this period. Assume continuous compounding for the growth rate calculation.",
    "gold_answer": "1. Let the growth rate be \\( r \\).\n2. The growth over two years is \\( (1 + r)^2 = 1.32 \\).\n3. Solving for \\( r \\): \\( r = \\sqrt{1.32} - 1 \\approx 0.1489 \\) or 14.89% per year.",
    "question_context": "The latest information available, as of June 30 of the current year, is provided in the top half of Table I for 1992 and each preceding year (back to 1975 when these records begin). The bottom half of Table I reports the final number of paid-up members and subscribers as of the end of 1991 and each year before that.\nHere we see that in 1991 Econometrica achieved its highest-ever level of paid circulation. The regular member category jumped by an astonishing 32 percent between 1989 and 1991.\nThe comparative figures for the Econometric Society and the American Economic Association in Table II are noteworthy. The 'E/A' ratio for members in 1991 reached an all-time high and was 23 percent above the 1989 ratio.\n\nThe report details the evolution of the Econometric Society's membership and institutional subscribers, providing mid-year and end-of-year data based on actual banked receipts of dues and subscription revenue."
  },
  {
    "qid": "econ-empirical-1416-2-0-0",
    "question": "1) Prove Lemma 1: Show that for a strategy-proof social choice function $f:{\\mathcal{D}}^{n}\\to X$, if $\\mathbf{X}\\in X$ is such that $x^{i}<f^{i}(\\mathbf{R})$ or $x^{i}>f^{i}({\\bf R})$, then $\\mathbf{x}\\neq f(\\bar{R}^{i};\\mathbf{R}^{-i})$.",
    "gold_answer": "1. **Strategy-proofness** implies no individual can benefit by misreporting preferences.  \n2. Suppose $\\mathbf{x} = f(\\bar{R}^{i};\\mathbf{R}^{-i})$ where $x^{i} < f^{i}(\\mathbf{R})$. Then, individual $i$ could manipulate by reporting $\\bar{R}^{i}$ to receive $x^{i}$, contradicting strategy-proofness.  \n3. Similarly, if $x^{i} > f^{i}(\\mathbf{R})$, reporting $\\bar{R}^{i}$ would yield a better outcome, violating strategy-proofness.  \n4. Thus, $\\mathbf{x}\\neq f(\\bar{R}^{i};\\mathbf{R}^{-i})$ must hold.",
    "question_context": "Lemma 1. Suppose that ${\\mathcal{D}}\\subseteq{\\mathcal{R}}^{\\mathrm{C}}$ and $f:{\\mathcal{D}}^{n}\\to X$ is a strategy-proof social choice function. For all $\\mathbf{R}\\in\\mathcal{D}^{n}$ ; all $i\\in N$ ; and all $\\bar{R}^{i}\\in\\mathcal{D}$ ; if $\\mathbf{X}\\in X$ is such that $x^{i}<f^{i}(\\mathbf{R})$ or $x^{i}>f^{i}({\\bf R})$ ; then $\\mathbf{x}\\neq f(\\bar{R}^{i};\\mathbf{R}^{-i})$.\nLemma 2. Suppose that $f:{\\mathcal{D}}^{n}\\to X$ is a strategy-proof social choice function. For all $\\mathbf{R}\\in\\mathcal{D}^{n}$ and all $i\\in N$ ; if $\\bar{R}^{i}\\in\\mathcal{D}$ is a Maskin monotonic transform of $R^{i}$ at $f^{i}(\\mathbf{R})$ ; then $f^{i}(\\bar{R}^{i};{\\mathbf{R}}^{-i})=f^{i}({\\mathbf{R}})$.\nLemma 3. If $R\\in{\\mathcal{R}}^{\\mathrm{CS}}$ and $x^{0}\\in\\mathbb{R}_{++}^{m}$ ; then there exists a CES Maskin monotonic transform of $R$ at $x^{0}$ that is arbitrarily close to the $x^{0}$ -generated Leontief preference.\nLemma 4. Suppose that $f:{\\mathcal{D}}^{n}\\to X$ is a social choice function satisfying Pareto optimality. For all $\\mathbf{R}\\in\\mathcal{D}^{n}$ and all $i,j\\in N$ ; if $R^{i}=R^{j}\\in\\mathcal{R}^{\\mathrm{CH}}$ and $f^{i}(\\mathbf R)\\neq0_{m}$ ; then there exists a $\\lambda\\geqslant0$ such that $f^{j}({\\bf R})=\\lambda f^{i}({\\bf R})$.\nLemma 5. For all $\\Omega\\in\\mathbb{R}_{++}^{m}$ ; all $R^{0}\\in\\mathcal{R}^{\\mathrm{CH}}$ ; all iAN; and all $R^{i}\\in\\mathcal{R}^{\\mathrm{C}}$ ; $\\operatorname*{Pr}_{i}\\mathcal{P}_{2}((R^{i};R^{0}),\\Omega)=\\operatorname*{Pr}_{i}\\mathcal{P}((R^{i};R^{0},\\dots,R^{0}),\\Omega)$.\nLemma 6. Suppose that $f:{\\mathcal{D}}^{n}\\to X$ is a strategy-proof social choice function that satisfies Pareto optimality and minimum consumption guarantee. For all $i\\in N$ and all $R^{i},R^{0},\\bar{R}^{0}{\\in}\\mathcal{D}\\cap\\mathcal{R}^{\\mathrm{CHS}}$ ; i $f0_{m}\\not\\leqslant f^{i}(R^{i};R^{0},\\ldots,R^{0})\\not\\leqslant\\Omega$ ; $f^{i}(R^{i};R^{0},\\ldots,R^{0})$ is not proportional to $\\Omega$ ; and $\\bar{\\boldsymbol{R}}^{0}$ is a Maskin monotonic transform of $R^{0}$ at $\\textstyle\\sum_{j\\neq i}f^{j}(R^{i};R^{0},\\ldots,R^{0})$ ; then $f^{i}(R^{i};R^{0},...,R^{0})=f^{i}(R^{i};\\bar{R}^{0},...,\\bar{R}^{0})$.\nLemma 7. For all $\\Omega\\in\\mathbb{R}_{++}^{m}$ ; all $R^{1}$ ${\\mathbf{\\theta}}^{1},R^{2}\\in\\mathcal{R}^{\\mathrm{CS}}$ that are additively separable and pairwise homothetic, all $i\\in\\{1,2\\}$ ; and all $x^{i}\\in\\operatorname{Pr}_{i}[\\mathcal{P}_{2}((R^{1},R^{2}),\\mathcal{Q})\\cap X_{2}^{*}]$ ; $\\mathrm{Pr}_{i}\\mathcal{P}_{2}((R^{1},R^{2}),\\varOmega)$ is increasing from person i’s perspective at $x^{i}$.\n\nThis section presents preliminary results used to prove the main theorem, including lemmas on strategy-proof social choice functions, Maskin monotonic transforms, and various types of preferences (Cobb-Douglas, CES, Leontief, linear)."
  },
  {
    "qid": "econ-empirical-1446-1-0-0",
    "question": "1) Using the Lombra-Torto test results from 1973-1977, derive the forecasted discount rate for a given month in 1978, assuming the actual discount rate in the previous month was $\\mathtt{rates}_{t-1}$ and the forecasted change is $\\Delta DR_t$. Show the mathematical steps and interpret the implications of the forecast error.",
    "gold_answer": "1. **Forecasted Discount Rate Calculation**: \n   - Given the actual discount rate at $t-1$: $\\mathtt{rates}_{t-1}$\n   - Forecasted change: $\\Delta DR_t$ (from Lombra-Torto model)\n   - Forecasted rate at $t$: $\\mathtt{rates}_t = \\mathtt{rates}_{t-1} + \\Delta DR_t$\n\n2. **Example**: For January 1978, if $\\mathtt{rates}_{t-1} = 6.00$ and $\\Delta DR_t = 0.03$, then $\\mathtt{rates}_t = 6.03$.\n\n3. **Forecast Error Analysis**: \n   - Actual rate in January 1978: 6.37\n   - Forecast error: $6.37 - 6.03 = -0.34$\n   - Interpretation: The negative error indicates the model underestimated the actual rate, suggesting unaccounted factors in 1978.",
    "question_context": "Table I shows the results of an application of the Lombra-Torto test to the 1973 to 1978 sample period. Generally, the results confirm the Lombra-Torto conclusion that the discount rate has not been independent of the Federal funds rate-discount rate differential. In fact, the estimated coefficients and $\\overline{{R}}^{2}$ from 1973 to 1977 are nearly identical to the Lombra-Torto results. When 1978 is added to the sample period, however, the $\\overline{{R}}^{2}$ drops slightly, and the mean square error more than doubles.\nTo further examine the temporal stability of the results, the estimated parameters for 1973 to 1977 were used to forecast discount rate changes in 1978. Those changes were applied to actual discount $\\mathtt{rates}_{t-1}$ to derive a discount rate level forecast for each month. The results are presented in Table II. The Lombra-Torto hypothesis appears to have been quite viable from 1973 to 1977. However, the equation did not adequately forecast discount rates in 1978, particularly toward the end of the year. It seems reasonable to assume that if discount rate changes in 1978 were not entirely predictable on the basis of movements in the Federal funds rate-discount rate differential, those changes could have surprised the market and consequently have generated announcement effects.\nA closer look at the timing and magnitudes of the discount rate changes in 1978 sheds some light on why announcement effects may have occurred. Table III shows discount rates, Federal funds rates, and the differential between the two for segments of 1978 surrounding each of the seven changes in the discount rate. Given that the norm was a 50 basis point spread, the market may have been taken by surprise by the discount rate changes in January and September, since the differentials had not been far from that norm. The market may also have been surprised by the magnitude of the November discount rate change, since it was the largest since 1933.\nIn summary, the evidence presented here suggests that announcement effects were possible in 1978, even by the Lombra-Torto standards. The rest of this paper examines the foreign exchange market for evidence that announcement effects did, indeed, occur.\n\nThe Lombra-Torto test was applied to the 1973 to 1978 sample period to examine the independence of the discount rate from the Federal funds rate-discount rate differential. The results confirmed the Lombra-Torto conclusion for 1973-1977 but showed instability when 1978 was included, with a drop in $\\overline{{R}}^{2}$ and a significant increase in mean square error. Forecasts for 1978 using parameters from 1973-1977 showed poor performance, particularly towards the end of the year, suggesting potential announcement effects due to unpredictable discount rate changes."
  },
  {
    "qid": "econ-empirical-1482-3-1-2",
    "question": "3) Discuss the key issues in constructing internationally comparable physical capital stocks and the approach taken by Jorgensen and Yip (1999).",
    "gold_answer": "1. Key issues include the use of comparable price deflators, depreciation rates, and a broad definition of physical capital.\\n2. Jorgensen and Yip (1999) address these issues by constructing sector-specific capital stocks using comparable price deflators and asset-specific depreciation rates, and aggregating using user cost weights.",
    "question_context": "The data we use to estimate equation (4) comes from several sources. The two main sources for individual-level data are the matched files of the PSID (1980-1997) and the GSOEP (1984-1997), and our main source for aggregate data is Dale W. Jorgensen and Eric Yip (1999).\nWe use the micro data from the PSID and GSOEP to construct total labor supply, the human capital stock, and our dependent variable, the hourly wage. The hourly wage is constructed by dividing total annual labor earnings by annual hours of work as reported in the Equivalent File and it is deflated using a country-specific GDP deflator.\nThe construction of internationally comparable stocks of physical capital requires particular care. Jorgensen and Yip (1999) have constructed capital series for the United States and Germany which explicitly address these issues.\n\nThis section describes the data sources and methods used to estimate equation (4), including the construction of labor supply, human capital stock, and physical capital stock."
  },
  {
    "qid": "econ-empirical-709-2-1-3",
    "question": "4) Formulate a dealer's inventory management problem as a stochastic optimization model, where post-auction sourcing is subject to price uncertainty.",
    "gold_answer": "The dealer minimizes expected cost: \\[ \\min_{q_{pre}, q_{post}} \\, c_{pre} q_{pre} + \\mathbb{E}[c_{post} q_{post}] \\] subject to \\( q_{pre} + q_{post} \\geq Q \\), where \\( c_{post} \\) is stochastic and \\( Q \\) is the auction commitment.",
    "question_context": "The spreads of sterling-denominated investment grade corporate bonds fell sharply when the CBPS was announced, indicating that the policy came as a surprise to market participants.\nSterling corporate bond issuance increased substantially following the announcement. Market participants argued that the reduction in funding costs caused by the CBPS contributed to this increase in issuance.\nDealers largely sourced the bonds that they delivered to the CBPS after the auction results were known: on average, for each £1 million of a bond that a dealer committed to deliver to the CBPS, the dealer bought around £0.1 million in the pre-auction period and around £0.5 million in the post-auction period.\n\nThe announcement of the CBPS led to a sharp fall in corporate bond spreads and increased issuance. Transaction data show dealers sourced bonds post-auction, with investors selling bonds to dealers for CBPS delivery."
  },
  {
    "qid": "econ-empirical-30-1-0-1",
    "question": "2) Explain why the authors argue that KZ's empirical classification system is flawed in identifying both absolute and relative degrees of financing constraints across firms.",
    "gold_answer": "1. KZ's classification relies on a limited subset of firms (49 low-dividend firms), which may not be representative. \\n2. The classification does not account for firm-specific factors that influence financing constraints, such as size, industry, and access to capital markets. \\n3. Absolute constraints are misidentified because KZ's criteria do not capture the full spectrum of financial constraints. \\n4. Relative constraints are inaccurately measured due to the lack of a robust, comparative framework across firms.",
    "question_context": "Kaplan and Zingales [1997] argue that investment-cash flow sensitivities do not provide useful evidence about the presence of financing constraints. They use a subset of the same firms and the same regressions as Fazzari, Hubbard, and Petersen [1988] and claim that FHP 'can legitimately be considered the parent of all papers in this literature.'\nKZ reach the provocative conclusion that 'the investment-cash flow sensitivity criterion as a measure of financial constraints is not well-grounded in theory.' The authors argue that the KZ model does not capture the theoretical approach employed in FHP and many subsequent studies.\nKZ attempts to show that empirical investment-cash flow sensitivities do not increase monotonically with the degree of financing constraints within the 49 low-dividend firms from the FHP sample. The authors explain why the KZ classification of the degree of constraints is flawed in identifying both whether or not firms are constrained (absolute constraints) as well as the relative degree of constraints across firms.\n\nThe paper critiques Kaplan and Zingales' (KZ) argument that investment-cash flow sensitivities are not useful for detecting financing constraints. It highlights flaws in KZ's theoretical model and empirical classification system, arguing that their conclusions are not supported by their results."
  },
  {
    "qid": "econ-empirical-1194-2-0-1",
    "question": "2) Analyze the regional shifts in the underclass population as shown in Table 3. What economic and social factors might explain the concentration of underclass areas in the Northeast?",
    "gold_answer": "1. **Regional Shift**: Underclass areas grew in the Northeast (32% to 34%) while the national population shifted towards the South and Midwest.  \n2. **Economic Factors**: Decline in manufacturing jobs in older industrial cities (e.g., Rust Belt) reduced opportunities for low-skilled workers.  \n3. **Social Factors**: Out-migration of middle-class minorities left behind socially disorganized neighborhoods (Wilson 1987).  \n4. **Conclusion**: Structural economic changes and social disorganization jointly explain the Northeast's underclass concentration.",
    "question_context": "Our results show that dramatic growth of the underclass has occurred (see Table 1). There was more than a four-fold increase in the number of underclass areas and more than a three-fold increase in the underclass area population between 1970 and 1980.\nRicketts and Sawhill estimated the number of people in the underclass by counting residents of underclass areas who were directly involved in at least one dysfunctional behavior. They used two alternative measures.\nTable 2 shows these estimates for 1970 and 1980. According to these estimates--the first involving the sum of rows 1 and 2 and the second involving row 3 exclusively--the number of people in the underclass grew by roughly 300 percent between 1970 and 1980.\nTable 3 shows that whereas the regional distribution of the national population shifted from the Northeast and West towards the South and Midwest, the distribution of the underclass area population shifted towards the Northeast.\nInterestingly, the proportion of blacks in the underclass area population decreased from 77 to 59 percent over the decade, while the proportion of whites and other races increased from 23 to 41 percent.\n\nThe text discusses the dramatic growth of the underclass between 1970 and 1980, highlighting increases in underclass areas and populations, as well as changes in demographic composition and regional distribution."
  },
  {
    "qid": "econ-empirical-122-0-0-1",
    "question": "2) Prove that the deterministic component of the random utility function is a cubic smoothing spline when maximizing the penalized likelihood over the second-order Sobolev space.",
    "gold_answer": "1. **Sobolev Space**: The second-order Sobolev space \\(W_2^2\\) consists of functions with square-integrable second derivatives.  \n2. **Optimization**: The penalized likelihood maximization over \\(W_2^2\\) yields the Euler-Lagrange equation \\(f''''(x) = 0\\) between knots, implying piecewise cubic polynomials.  \n3. **Spline Solution**: The solution \\(f(x)\\) minimizes \\(\\int [f''(x)]^2 dx\\) subject to data constraints, confirming it is a cubic smoothing spline.",
    "question_context": "The basic assumption of RUM is that the utility function is unknown but it can be partially recovered by relating an individual's consumption choices to the individual's socio-economic characteristics, the relative prices that he or she faces, and the characteristics of the goods available for consumption.\nThe proposed model fits into the second group of nonparametric generalization of the discrete choice methods. The objective function of this model is a penalized likelihood function that contains a likelihood function and a roughness penalty function to control the smoothness of the nonparametric utility function.\nWhen the penalized likelihood function is maximized over the second-order Sobolev function space, we show that the deterministic component of the random utility function is a cubic smoothing spline function.\n\nThe paper proposes a nonparametric multiple choice model that applies the penalized likelihood method within the random utility framework, improving flexibility in discrete choice data analysis. The deterministic component of the random utility function is shown to be a cubic smoothing spline function, subsuming the conventional conditional logit model as a special case."
  },
  {
    "qid": "econ-empirical-1606-3-0-3",
    "question": "4) Discuss the limitations of the model as mentioned in the text, and propose how a more structural model could address these limitations.",
    "gold_answer": "1. **Limitations**:\n   - The model ignores adjustments over longer time periods (e.g., weeks worked).\n   - Fixed costs of work on the supply side are not considered, especially for females.\n   - Constraints on overtime and increasing hours are not modeled.\n   - Unemployment is not formally incorporated into the model.\n\n2. **Proposed Improvements**:\n   - A more structural model could include wage determination mechanisms to better capture equilibrium dynamics.\n   - Stochastic elements could be added to model discontinuous jumps in the maximum hours locus.\n   - Fixed costs and institutional constraints could be explicitly parameterized for different demographic groups.\n   - A dynamic framework could account for adjustments over longer time horizons.",
    "question_context": "There are many aspects of labor-market constraints not addressed here. For example, constraints may be less severe if hours can be adjusted over time periods longer than a week. It may be easier to adjust hours over the year, over the lifetime, on the job, between jobs, and so on. Over the year, for example, individuals may adjust weeks worked. Although there are institutional constraints on weeks, they are likely to be less severe than for hours per week.\nTo compute NIT effects note that the expected value of $h$ in the sample is given by the equations: $$E(h)=E(H^{s}|0>H^{d}-H^{s})\\mathrm{Prob}(0>H^{d}-H^{s}) + E(H^{d}|D>H^{d}-H^{s}>0)\\mathrm{Prob}(D>H^{d}-H^{s}>0)$$ where $$E(H^{s}|0>H^{d}-H^{s})=X\\beta+\\frac{\\sigma_{\\star}^{*}}{\\sigma_{W}}\\frac{J(V_{1})}{F(V_{1})}$$ $$E(H^{d}|D>H^{d}-H^{s}>0)=Z\\delta+\\frac{\\sigma_{H^{*}}^{2}}{\\sigma_{W}}\\frac{f(V_{2})-f(V_{1})}{F(V_{2})-F(V_{1})}$$ $$\\mathrm{Prob}(0>H^{d}-H^{s})=F(V_{1})$$ $$\\mathrm{Prob}(D>H^{d}-H^{s}>0)=F(V_{2})-F(V_{1})$$ $$V_{1}=(X\\beta-Z\\delta)/\\sigma_{W}$$ $$\\sigma_{W}=\\sqrt{\\sigma_{s}^{*}+\\sigma_{d}^{*}}$$\nThe disincentive effects mentioned in the text are calculated by evaluating $E(h)$ with and without an NIT. The decomposition of the change into marginal and non-marginal effects is as follows: $$E_{1}(h)-E_{0}(h)=P_{0}^{s}(E_{1}^{s}-E_{0}^{s})+E_{1}^{s}(P_{1}^{s}-P_{0}^{s}) + P_{0}^{d}(E_{1}^{d}-E_{0}^{d})+E_{1}^{d}(P_{1}^{d}-P_{0}^{d})$$ The first of the four terms on the right-hand side represents the component resulting from marginal hours changes.\n\nThe text discusses various labor-market constraints not addressed in the paper, such as adjustments over longer time periods, fixed costs of work, and constraints on overtime. It also introduces a mathematical model for computing NIT effects and decomposing changes into marginal and non-marginal effects."
  },
  {
    "qid": "econ-empirical-1208-5-0-2",
    "question": "3) Formally contrast the matching estimator used in this paper with a standard OLS wage regression. What are the advantages of the former in detecting discrimination?",
    "gold_answer": "1. **Matching**: Non-parametric, compares wages for identical \\( (S, E, X) \\) tuples. Avoids functional form assumptions (e.g., linearity in OLS).  \n2. **OLS**: \\( W_i = \\alpha + \\beta Female_i + \\gamma X_i + \\epsilon_i \\).  \n   - Prone to misspecification if \\(X\\) interacts with gender (e.g., major choice effects).  \n3. **Advantage of Matching**: Directly isolates comparable subgroups, reducing bias from heterogeneous returns to \\(X\\) by gender.",
    "question_context": "Well-educated women in the United States earn approximately 30 percent less than men, a gap that is similar to the gender gap for the workforce generally.\nWhen we focus analysis on men and women who speak English at home, we find that across racial/ethnic groups between 44 and 73 percent of the gender wage gaps are accounted for by such premarket factors as age, highest degree, and major.\nWhen we restrict attention further to women who have 'high labor force attachment' (work experience that is similar to male comparables) we account for between 54 and 99 percent of the gender wage gaps.\nThus, a relatively simple version of the basic human capital model seems to do remarkably well in explaining a substantial portion of the gender wage gaps for highly educated women.\nOne tempting interpretation for our findings is that gender discrimination is not a major factor in wage determination in labor markets for the college educated.\nOur final observation concerns methodology. The inferences we draw in this paper are based on a simple and intuitively appealing matching model that has not heretofore been used in studying gender wage gaps.\n\nThe paper examines the gender wage gap using a matching estimation procedure, focusing on highly educated women across different racial/ethnic groups. It highlights the role of premarket factors and labor force attachment in explaining wage gaps, while also discussing caveats related to discrimination and methodological considerations."
  },
  {
    "qid": "econ-empirical-1785-0-0-2",
    "question": "3) The paper distinguishes between short-run quasi-fixity of private capital and long-run scale economies. Formally model the short-run cost function with quasi-fixed capital and contrast it with the long-run cost function where all inputs are variable.",
    "gold_answer": "1. **Short-Run Cost Function**: \\( C_{SR}(Y, w, K) = \\min_{L} \\{ wL \\ | \\ F(K, L) \\geq Y \\} \\), where \\( K \\) is quasi-fixed.  \n2. **Long-Run Cost Function**: \\( C_{LR}(Y, w) = \\min_{K, L} \\{ rK + wL \\ | \\ F(K, L) \\geq Y \\} \\), where \\( r \\) is the rental rate of capital.  \n3. **Contrast**: Short-run costs are higher due to inflexibility (\\( C_{SR} \\geq C_{LR} \\)), and long-run economies of scale arise when \\( C_{LR} \\) increases less than proportionally with \\( Y \\).",
    "question_context": "The 'new-growth' or endogenous growth literature emphasizes the role of returns to capital that embodies new knowledge and generates externalities. This general notion of capital encompasses aspects of both human and physical capital.\nIn the growth literature the impacts of knowledge accumulation are generally motivated in terms of an efficiency function that is separable from a stationary production function. These knowledge factors are hypothesized to be external to the industry, and the resulting effects on productivity are interpreted as evidence of spillovers.\nThe dynamic cost function framework enables us to examine the impact of investments in external capital factors, such as human, R&D, and high-tech capital, on growth. It also allows us to examine spillover effects that may arise from interactions among internal and external factors.\n\nTheoretical models of endogenous growth identify capital accumulation and returns as a potential stimulus to economic growth. Existing empirical studies, however, are based on a limited notion of these returns, which follows from the simple production function framework used for estimation. The purpose of this study is to examine growth issues using dynamic cost function estimation. This methodology enables us to broaden the concept of returns to include returns arising from short-run quasi-fixity of private capital, long-run (internal) scale economies, and external 'knowledge' factors—overall investment in research (R&D), technology (high-tech capital), and education (human capital)."
  },
  {
    "qid": "econ-empirical-620-1-1-3",
    "question": "4) Derive the expression for $Var^{*}(e_t^{*})$ under general time-series bootstrap schemes and show why it leads to $\\Gamma^{*} = 0$.",
    "gold_answer": "For general time-series bootstrap:\n\n1) Let $\\{\\tau_t\\}$ be the resampling indices\n2) Then:\n\n$$\nVar^{*}(e_t^{*}) = E^{*}[e_t^{*}e_t^{*\\prime}] - E^{*}[e_t^{*}]E^{*}[e_t^{*}]^{\\prime}\n$$\n\n3) This equals:\n\n$$\n\\sum_{t=1}^{T} w_t \\tilde{e}_t\\tilde{e}_t^{\\prime} - (\\sum_{t=1}^{T} w_t \\tilde{e}_t)(\\sum_{t=1}^{T} w_t \\tilde{e}_t)^{\\prime}\n$$\n\nwhere $w_t$ are the resampling weights\n\n4) Therefore:\n\n$$\n\\Gamma^{*} = \\frac{1}{N}\\tilde{A}^{\\prime}Var^{*}(e_t^{*})\\tilde{A} = \\frac{1}{N}\\left(\\sum w_t \\tilde{A}^{\\prime}\\tilde{e}_t\\tilde{e}_t^{\\prime}\\tilde{A} - (\\sum w_t \\tilde{A}^{\\prime}\\tilde{e}_t)(\\sum w_t \\tilde{A}^{\\prime}\\tilde{e}_t)^{\\prime}\\right)\n$$\n\n5) Since $\\tilde{A}^{\\prime}\\tilde{e}_t = 0$ for all t by PCA FOCs, $\\Gamma^{*} = 0$",
    "question_context": "Proposition 2.1. Suppose $e_{t}^{\\ast}\\sim$ i.i.d. $\\left\\{\\tilde{e}_{t}-\\overline{{\\tilde{e}}}\\right\\}$ for $t=1,\\ldots,T$ . Then $T^{*}=0$ for any $N,T$ .\n\nThe proof of Proposition 2.1 follows trivially from the first order conditions that define $\\tilde{\\boldsymbol{\\Lambda}}$ . In particular,\n\n$$\n{{\\cal{T}}^{*}}=\\frac{1}{T}\\sum_{t=1}^{T}\\frac{1}{N}{{\\tilde{\\cal{A}}}^{\\prime}{\\cal{V}}a r^{*}\\left({e_{t}^{*}}\\right){\\tilde{\\cal{A}}}},\n$$\n\nwhere\n\n$$\nV a r^{*}\\left(e_{t}^{*}\\right)=E^{*}\\left(e_{t}^{*}e_{t}^{*\\prime}\\right)=\\frac{1}{T}\\sum_{t=1}^{T}\\left(\\tilde{e}_{t}-\\bar{\\tilde{e}}\\right)\\left(\\tilde{e}_{t}-\\bar{\\tilde{e}}\\right)^{\\prime}=\\frac{\\tilde{e}^{\\prime}\\tilde{e}}{T}-\\frac{\\tilde{e}^{\\prime}u^{\\prime}\\tilde{e}}{T},\n$$\n\nand where $\\tilde{e}$ is a $T\\times N$ matrix with rows given by $\\tilde{{\\boldsymbol{e}}}_{t}^{\\prime}$ and $\\iota=(1,\\ldots,1)^{\\prime}$ is $T\\times1$ . It follows that\n\n$$\n{{\\varGamma}^{*}}=\\frac{1}{N}\\tilde{A}^{\\prime}V a r^{*}\\left({{e}_{t}^{*}}\\right)\\tilde{A}=\\frac{1}{N T}\\left({{\\tilde{A}}^{\\prime}{{\\tilde{e}}^{\\prime}}{{\\tilde{e}}}\\tilde{A}-{{\\tilde{A}}^{\\prime}}{{\\tilde{e}}^{\\prime}}{{u}^{\\prime}}{{\\tilde{e}}}\\tilde{A}}\\right)=0\n$$\n\nsince $\\tilde{\\boldsymbol{A}}^{\\prime}\\tilde{\\boldsymbol{e}}^{\\prime}=0$ by the first order conditions that define $\\left(\\tilde{F},\\tilde{\\boldsymbol{\\Lambda}}\\right)$ . Notice that this result holds for any possible value of $(N,T)$ .\nThe main implication of Proposition 2.1 is that the i.i.d. bootstrap distribution is centered at zero (i.e. $\\varDelta_{\\delta}^{*}=0$ because $\\varDelta\\v{\\delta}_{\\delta}*$ is a linear function of $T^{*}$ and ${\\cal T}^{*}=0$ ). Since the OLS estimator is asymptotically biased when the cross sectional dimension is relatively small compared to the time series dimension (i.e. when $\\sqrt{T}/N\\to c\\neq0;$ , the i.i.d. bootstrap does not replicate this important feature of the OLS distribution. Note that this failure of the i.i.d. bootstrap holds regardless of whether cross sectional dependence exists or not. The problem is not that the i.i.d. bootstrap does not capture cross sectional dependence. Rather the problem is that it induces a zero bias term which should be there even under cross sectional independence as long as $-c\\varDelta_{\\delta}\\neq0$ (i.e. as long as $c\\neq0$ and $p\\operatorname*{lim}\\hat{\\alpha}=H_{0}^{-1\\prime}\\alpha\\neq0)$ .\n\nThis section discusses the limitations of bootstrap methods that only resample in the time dimension for factor-augmented regression models, showing that they fail to capture the necessary cross-sectional dependence."
  },
  {
    "qid": "econ-empirical-1084-4-0-2",
    "question": "3) Solve the recursive native migration response \\(\\nu_{ijt} = \\eta\\sigma(1 + \\eta\\sigma)^t \\lambda_{ij} - \\eta\\sigma(1 + \\eta\\sigma)^{t-1}(1 - k_{ij})m_i\\) for the cumulative net migration rate \\(V_{ijt}\\) and interpret its components.",
    "gold_answer": "1. **Cumulative migration rate**: \n   \\(V_{ijt} = \\sum_{\\tau=0}^t \\eta\\sigma(1 + \\eta\\sigma)^\\tau \\lambda_{ij} - \\sum_{\\tau=1}^t \\eta\\sigma(1 + \\eta\\sigma)^{\\tau-1}(1 - k_{ij})m_i\\).\n2. **Simplified form**: \n   \\(V_{ijt} = [(1 + \\eta\\sigma)^t - 1]\\lambda_{ij} + [1 - (1 + \\eta\\sigma)^t](1 - k_{ij})m_i\\).\n3. **Interpretation**: The first term captures initial disequilibrium response, the second term reflects persistent immigrant-induced displacement.",
    "question_context": "The theoretical model presented in this paper implies that the estimated wage effect of immigration will not equal this factor price elasticity whenever the corresponding wage regression is estimated using observations on local labor markets where natives can respond to immigration by moving across markets. The spatial correlation will be numerically smaller, and the gap between the spatial correlation and the true factor price elasticity should reflect the native migration effect.\nThe theory then predicts that the spatial correlation between wages and immigration, \\(\\hat{\\gamma}_{w}\\), is given by: \\(\\hat{\\gamma}_{w} = \\eta(1 + \\gamma_{N})\\).\nThe native population in a particular skill group grew slowest in those parts of the country that experienced the largest immigrant influx. This native supply response is evident both in terms of a decline in native in-migration and an increase in native out-migration.\n\nThe theoretical model links the national wage effect of migration and spatial correlations between wages and immigration across local labor markets through a parameter measuring native migration response. The study examines whether differences in wage impacts for different geographic definitions of labor markets can be explained by immigration's impact on native location decisions."
  },
  {
    "qid": "econ-empirical-951-4-0-2",
    "question": "3) Explain why $\\frac{1}{T}\\sum_{t=1}^{T}\\|\\hat{F}_{t}-\\mathcal F_{t}\\|^{4}=O_{p}(1/T)+O_{p}(1/N^{2})$ is critical for the proof of Proposition 3, and how the assumption $\\sqrt{T}/N\\to0$ ensures consistency.",
    "gold_answer": "1. The term $\\frac{1}{T}\\sum_{t=1}^{T}\\|\\hat{F}_{t}-\\mathcal F_{t}\\|^{4}$ bounds the estimation error between feasible and infeasible estimators.  \n2. The rate $O_{p}(1/T)+O_{p}(1/N^{2})$ ensures the error vanishes as $T,N\\to\\infty$.  \n3. The assumption $\\sqrt{T}/N\\to0$ guarantees $O_{p}(1/N^{2})$ dominates $O_{p}(1/T)$, ensuring $\\|\\hat{S}(\\hat{F})-S\\|=o_{p}(1)$.",
    "question_context": "Recall that ${\\cal S}\\quad=\\quad$ $\\begin{array}{r}{\\operatorname*{lim}\\operatorname{Var}\\biggl(\\frac{1}{\\sqrt{T}}\\sum_{t=1}^{T}\\mathcal{F}_{-1t}\\mathcal{F}_{1t}\\biggr)}\\end{array}$ . Notice that $E(\\mathcal{F}_{-1t}\\mathcal{F}_{1t})=0$ as shown above.\nFirst, we define the infeasible estimator of S: $$\\hat{S}(\\mathcal{F})=\\hat{{\\cal T}}_{0}(\\mathcal{F})+\\sum_{j=1}^{m}w(j,m)[\\hat{{\\cal T}}_{j}(\\mathcal{F})+\\hat{{\\cal T}}_{j}(\\mathcal{F})^{\\prime}]$$ where $\\begin{array}{r}{m={{o}}(T^{\\frac{1}{4}}),w(j,m)=1-\\frac{j}{m+1}}\\end{array}$ is the Bartlett kernel, and $$\\hat{I}_{j}(\\mathcal{F})=\\frac{1}{T}\\sum_{t=j+1}^{T}\\mathcal{F}_{-1t}\\mathcal{F}_{1t}\\mathcal{F}_{1t-j}\\mathcal{F}_{-1t-j}^{\\prime}.$$\nAssumption 11. $\\|\\hat{S}(\\mathcal{F})-S\\|=o_{p}(1).$\nNext we consider a feasible estimator of S where $\\mathcal{F}_{t}$ is replaced by ${\\hat{F}}_{t}$ : $$\\hat{S}(\\hat{F})=\\hat{{\\cal T}}_{0}(\\hat{F})+\\sum_{j=1}^{m}w(j,m)[\\hat{{\\cal T}}_{j}(\\hat{F})+\\hat{{\\cal T}}_{j}(\\hat{F})^{\\prime}]$$ where $$\\hat{I}_{j}(\\hat{F})=\\frac{1}{T}\\sum_{t=j+1}^{T}\\hat{F}_{-1t}\\hat{F}_{1t}\\hat{F}_{1t-j}\\hat{F}_{-1t-j}^{\\prime}$$\nProposition 3. Assume that Assumptions 1–11 hold, under the null $H_{0}:k_{1}=0$ , we have $$\\left\\|{\\hat{S}}({\\hat{F}})-S\\right\\|=o_{p}(1).$$\n\nThis section discusses the consistent estimator of S using the HAC estimator of Newey and West (1987), detailing both infeasible and feasible estimators and their properties under certain assumptions."
  },
  {
    "qid": "econ-empirical-319-3-0-4",
    "question": "5) Analyze the effect of an ad valorem tax on the firm's optimal $X$ when the firm has increasing relative risk aversion.",
    "gold_answer": "1. An ad valorem tax on $Y$ transforms the profit to $Y^{\\prime} = (1-t)Y$.\n2. The first-order condition becomes: $$E\\{U^{\\prime}((1-t)Y)(1-t)[P-C^{\\prime}(X)]\\}=0.$$\n3. For increasing relative risk aversion, $-\\frac{Y U^{\\prime\\prime}(Y)}{U^{\\prime}(Y)}$ is increasing in $Y$.\n4. The tax reduces $Y$, making the firm effectively less risk-averse at the margin.\n5. This leads to an increase in $X$ as the firm is willing to take on more risk.",
    "question_context": "Let $U(.)$ denote the utility function (which may be different for the different firms, but we omit the subscript in the spirit of our unified approach). Thus, each firm's problem is of the form: $$\\operatorname*{max}_{x}E[U(Y)]\\quad{\\mathrm{where}}\\quad Y=P X-C(X),$$ where appropriate subscripts are attached to $U,Y,X$ and $C$ (to be specific, $M$ for the PMF, $L$ for the LMF and $K$ for the JSF).\nThe first- and second-order conditions for this maximisation are: $$E\\{U^{\\prime}(Y)[P-C^{\\prime}(X)]\\}=0$$ and $$D\\equiv E\\{U^{\\prime\\prime}(Y)[P-C^{\\prime}(X)]^{2}-U^{\\prime}(Y)C^{\\prime\\prime}(X)\\}<0.$$\nThe value of $X$ chosen by the firm facing a random output price with a given mean is lower (the same, higher) than the value of $X$ chosen by the firm facing a constant price equal to that mean if the firm is everywhere risk-averse (-neutral, -loving).\nIf the firm is risk-averse, and if its (Arrow-Pratt) index of absolute risk aversion is a decreasing function (of Y), then an increase in risk (in the sense of Sandmo (1971)) of the price distribution (around a constant mean) will decrease the value of $X$ chosen by the firm.\nIf the firm is risk-averse, and if its index of absolute risk aversion is decreasing, then an upward shift of the price distribution will cause the firm to increase $X.$\nIf the firm is risk-averse, and if (and only if) its index of absolute risk aversion is decreasing, then an upward shift of the $C(.)$ function (by a constant amount) will cause the firm to decrease $X$.\nIf the firm is risk-averse, then the imposition of an ad valorem tax on the objective function $Y$ (or an increase in the rate of such a tax) will cause the firm to increase, keep unchanged, or to decrease its value of $X$ according as its index of relative risk-aversion is increasing, constant or decreasing.\n\nThe behavior of three firms operating under price uncertainty is analyzed using Neumann-Morgenstern utility theory. The firms' problem is to maximize expected utility of profit, where profit depends on random output price and cost functions."
  },
  {
    "qid": "econ-empirical-1409-0-0-0",
    "question": "1) What are the key assumptions underlying the divisible search model of fiat money discussed in the comment?",
    "gold_answer": "The key assumptions include:\\n1. **Divisibility of Money**: Money is perfectly divisible, allowing for continuous transactions.\\n2. **Search Frictions**: Agents face search frictions in finding trading partners.\\n3. **Bargaining Process**: The model incorporates a bargaining process to determine the terms of trade.\\n4. **Stationary Equilibrium**: The analysis focuses on stationary equilibrium where the distribution of money holdings is invariant over time.",
    "question_context": "A Divisible Search Model of Fiat Money: A Comment by Bernhard Rauch, published in Econometrica, Vol. 68, No. 1 (Jan., 2000), pp. 149-156.\n\nThis comment discusses a divisible search model of fiat money, focusing on the theoretical underpinnings and implications for monetary economics."
  },
  {
    "qid": "econ-empirical-1649-1-3-0",
    "question": "9) Explain how the unexpected components of economic news (e.g., industrial production, ISM index) are calculated and standardized. Why is standardization necessary?",
    "gold_answer": "1. The unexpected component is the difference between the released data and the median survey expectation: $$\\mathrm{Surprise}_{t} = \\mathrm{Actual}_{t} - \\mathrm{Expected}_{t}$$ 2. The surprises are standardized by dividing by their sample standard deviation: $$\\mathrm{Standardized\\ Surprise}_{t} = \\frac{\\mathrm{Surprise}_{t}}{\\sigma_{\\mathrm{Surprise}}}$$ 3. Standardization ensures comparability across different economic indicators by removing units and scaling variability. 4. It also facilitates interpretation in terms of standard deviations, making the shocks unit-free.",
    "question_context": "Industrial production is used as our benchmark indicator of business cycle variation at the monthly frequency.\nThe unexpected component of each news release is calculated as the difference between the released data and the median expectation according to surveys.\n\nBusiness cycle variation is measured using industrial production, with robustness checks using non-farm employment and the ISM index. Unexpected components of economic news are derived from surveys to study their impact on monetary policy surprises."
  },
  {
    "qid": "econ-empirical-1619-4-3-0",
    "question": "1) Derive the BN trend as a long-horizon conditional expectation and explain why the standard BN decomposition misattributes GDP dynamics to the trend.",
    "gold_answer": "The BN trend is:\n\n\\[\n\\tau_t = \\lim_{h \\to \\infty} \\mathbb{E}[y_{t+h} | \\mathcal{F}_t] = y_t + \\sum_{i=1}^\\infty \\mathbb{E}[\\Delta y_{t+i} | \\mathcal{F}_t]\n\\]\n**Misattribution**:\n- Shocks with permanent effects (e.g., unit roots) inflate \\(\\tau_t\\), leaving a noisy/atypical cycle.\n- The modified BN decomposition imposes restrictions to align cycles with business cycle facts.",
    "question_context": "The BN decomposition proposed by Kamber, Morley, and Wong (2018) is related to the Hamilton filter as it also uses an autoregression to estimate the trend. The approach of Kamber, Morley, and Wong (2018) is less ad hoc because they estimate the trend based on long-horizon conditional expectations rather than imposing a fixed horizon of 8 quarters.\nThe standard BN decomposition yields an output gap that is completely at odds with standard business cycle facts and therefore an algorithm with several steps has to be run to get the modified BN decomposition that makes sure that most GDP dynamics are attributed to the cycle rather than trend changes.\n\nThis section contrasts Hamilton-type filters with the modified Beveridge-Nelson (BN) decomposition, highlighting computational and conceptual differences."
  },
  {
    "qid": "econ-empirical-808-1-0-3",
    "question": "4) Derive the price of capital goods $Q_{t}$ as the marginal cost of investment goods production.",
    "gold_answer": "1. Capital producers maximize discounted profits: $\\max E_{t}\\sum_{\\tau=t}^{\\infty}\\beta^{\\tau-t}A_{t;\\tau}\\{Q_{\\tau}I_{\\tau}-[1+f_{\\iota}(\\cdot)]I_{\\tau}\\}$.  \n2. The first-order condition with respect to $I_{t}$ yields: $Q_{t} = 1 + f_{\\iota}(\\cdot) + \\frac{\\partial f_{\\iota}(\\cdot)}{\\partial I_{t}}I_{t} + E_{t}\\beta\\varLambda_{t;t+1}\\frac{\\partial f_{\\iota}(\\cdot)}{\\partial I_{t}}I_{t+1}$.  \n3. This equates the price of capital to the marginal cost of production, including current and expected future adjustment costs.",
    "question_context": "Goods producers operate a Cobb–Douglas production function with capital and labor inputs, under perfect competition. Conditional on their choice of capital, goods producers choose labor inputs to satisfy $W_{t}=(1-\\alpha)\\frac{Y_{t}}{L_{t}}.$\nGross profits per unit of capital $(Z_{t})$ is its marginal product: $Z_{t}=\\alpha\\frac{Y_{t}}{K_{t}}.$\nCapital producers build new capital, subject to adjustment costs, with the functional form: $f_{\\iota}(\\cdot)\\equiv\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{\\eta_{i}}{2}\\left(\\frac{I_{t}}{\\delta K_{t}}-1\\right)^{2}\\frac{\\delta K_{t}}{I_{t}}}&{\\mathrm{if~}\\iota=0,}\\ {\\displaystyle\\frac{\\eta_{i}}{2}\\bigg(\\frac{I_{t}}{I_{t-1}}-1\\bigg)^{2}}&{\\mathrm{if~}\\iota=1.}\\end{array}\\right.$\n\nThis section describes the program of nonfinancial firms, specifically goods producers and capital producers, operating under perfect competition with Cobb-Douglas production functions and financing mechanisms."
  },
  {
    "qid": "econ-empirical-559-1-0-1",
    "question": "2) Explain the role of bankers in this economy. How does their ability to enforce and commit facilitate financial intermediation?",
    "gold_answer": "Bankers act as intermediaries by:\n1. **Enforcing Contracts**: They ensure consumers honor bond payments.\n2. **Commitment**: They guarantee future payments to producers.\n3. **Efficiency**: This reduces default risk and enables smoother transactions between consumers and producers.",
    "question_context": "Time is represented by a sequence of periods indexed by $t\\in\\mathbb{T}\\equiv\\left\\{0,1,...\\right\\}$ , each divided into two subperiods. There are three types of infinitely lived agents: bankers, consumers, and producers, denoted $B,C$ , and $P$ , respectively. An agent of type $i\\in\\{B,C,P\\}$ is represented with a point in the set $\\mathcal{T}_{i}=[0,1]$ .\nA monetary authority issues money—a financial security that is durable and intrinsically useless (i.e., it is not an argument of any utility or production function, and it is not a formal claim to goods or services). The quantity of money outstanding at the beginning of period $t$ is denoted $M_{t}$ , with $M_{0}\\in\\mathbb{R}_{++}$ given and distributed uniformly among consumers.\n\nThe model describes an economy with three types of agents: bankers, consumers, and producers. Time is discrete and divided into subperiods. Agents engage in production, consumption, and trading activities, with money and bonds serving as key financial instruments."
  },
  {
    "qid": "econ-empirical-265-3-0-3",
    "question": "4) Using the equation $\rho(t) = r(t) - 2\\lambda(t) + (\\alpha(t) - \beta(t))$, explain how distortions in $\\lambda(t)$, $\\alpha(t)$, or $\beta(t)$ propagate to $\rho(t)$ and $r(t)$. Provide examples from the text.",
    "gold_answer": "1. Distortion propagation:\n   - If $\\lambda_e > \\lambda_0$ (higher birth rate), $r(t)$ increases (via $+\\lambda(t)$), but $\rho(t)$ decreases more (via $-2\\lambda(t)$).\n   - If $\\alpha_e > \\alpha_0$ (higher shadow price of capital), $r(t)$ decreases (via $-\\alpha(t)$), but $\rho(t)$ increases (via $+\\alpha(t)$).\n   - If $\beta_e > \beta_0$ (higher shadow price of household size), $\rho(t)$ decreases (via $-\beta(t)$).\n2. Examples from text:\n   - Under-taxation of pollution ($\\eta_e < \\eta_0$) leads to $r_e > r_0$ and $\rho_e < \rho_0$.\n   - Zero pollution policy ($\\mu_e \to \\infty$) initially reduces $r(t)$ but may increase it long-term due to higher $\\lambda(t)$.",
    "question_context": "Before proceeding it should be recalled that the previous analysis contained two different discount rates: $\rho\\left(t\right)=\\delta-\\left(\\lambda\\left(t\right)+\beta\\left(t\right)\right)$ and $r(t)=\\delta+\\left(\\lambda\\left(t\right)-\\alpha\\left(t\right)\right)$. The public rate $\rho(t)$ is the rate of return to government investments in public goods; $r(t)$ is the rate of return to a household's investment in private markets. From the above we may deduce that $\rho\\left(t\right)=r\\left(t\right)-2\\lambda\\left(t\right)+\\left(\\alpha\\left(t\right)-\beta\\left(t\right)\right)$.\nThe rationale for (41) lays in the fact that $\rho(t)$ and $r(t)$ relate to two different units of the population. When a household invests in the private market it expects the yield of this investment to enable the same household to consume goods in the future, which compensates for lost present consumption.\nSince the household's size changes with time at rate $\\lambda$, the investment yield must be at a rate which compensates for $\\delta$, the household's time preference plus X. In addition, the change of $_{\\alpha}$, the shadow price of capital in terms of utility units, has to be taken into account.\n\nThe analysis involves two discount rates: the public rate of return to government investments in public goods, $\rho(t)$, and the market rate of return to private investments, $r(t)$. The relationship between these rates is derived, and their implications for household and public investments are discussed."
  },
  {
    "qid": "econ-empirical-1149-0-0-0",
    "question": "1) Formally describe the instrumental variables (IV) approach used in the paper to estimate the causal effect of teenage childbearing. What is the instrument, and why is it considered valid?",
    "gold_answer": "1. **Instrument**: Miscarriages are used as an instrument for teenage childbearing.  \n2. **Validity**:  \n   - **Relevance**: Miscarriages are correlated with the likelihood of having a live birth (the treatment variable).  \n   - **Exogeneity**: Miscarriages are assumed to be random due to biological factors (e.g., abnormal fetal chromosomes), making them uncorrelated with unobserved confounders.  \n3. **IV Estimator**: The causal effect is estimated by comparing outcomes of women who became pregnant but miscarried (control group) to those who had a live birth (treatment group).",
    "question_context": "We exploit a 'natural experiment' associated with human reproduction to identify the causal effect of teen childbearing on the socioeconomic attainment of teen mothers. We exploit the fact that some women who become pregnant experience a miscarriage and do not have a live birth.\nThe key issue in attempts to estimate the causal effect is how to estimate reliably the counterfactual state to the observed outcomes of teen mothers: namely, what would have been the adolescent mother's outcomes if she had not had a child as a teen?\nA third econometric approach attempts to model explicitly the joint process determining the woman's decision to bear a child as a teenager as well as the maternal outcome of interest, such as, education, labor supply, or poverty status.\n\nThe paper explores the causal effects of teenage childbearing on socioeconomic outcomes by exploiting miscarriages as a natural experiment. It contrasts previous methodologies and their limitations."
  },
  {
    "qid": "econ-empirical-1569-4-1-1",
    "question": "4) Interpret the implications of the confidence intervals highlighted in orange in Figure III.",
    "gold_answer": "1. The confidence intervals highlighted in orange lie everywhere below one, indicating that judges’ implied beliefs about failure-to-appear risk underreact to predictable variation in failure-to-appear risk.\\n2. This suggests that judges perceive the change in failure-to-appear risk between defendants in the top decile and bottom decile of predicted risk to be less than the true change in failure-to-appear risk across these defendants.\\n3. This could be consistent with judges regularizing their implicit predictions, possibly due to inattention.",
    "question_context": "I apply the identification results in Section IV.B to bound the extent to which these judges’ implied beliefs overreact or underreact to predictable variation in failure-to-appear risk based on defendant characteristics.\nI construct a \\( 95\\% \\) confidence interval for their implied prediction mistakes \\( \\frac{\\delta(x_{I},d)}{\\delta(x_{I},d^{\\prime})} \\) between the top decile and bottom deciles of the predicted failure-to-appear risk distribution by calculating \\( g(x_{d}) = 1 - \\tau(x_{I},d) \\) for each pair \\( \\tau(x_{I},d) \\), \\( \\tau(x_{I},d^{\\prime}) \\) in the joint confidence set.\n\nThis section investigates the extent to which judges’ implied beliefs overreact or underreact to predictable variation in failure-to-appear risk based on defendant characteristics."
  },
  {
    "qid": "econ-empirical-1015-5-0-1",
    "question": "2) Prove that the proposed strategy profile is subgame perfect for $\\delta \\in (\\underline{\\delta}, 1)$ by verifying no player has a profitable deviation in the disagreement game.",
    "gold_answer": "1. **Player 1's deviation from $a^{1}$**:  \n   - Deviation payoff: $(1-\\delta) \\max_{a_{1}^{\\prime}} u_{1}(a_{1}^{\\prime}, a_{2}^{1}) + \\delta b_{2}(0,w_{2})$.  \n   - Equilibrium payoff: $(1-\\delta) u_{1}(a^{1}) + \\delta [b_{2}(0,w_{2}) + r_{1}(a^{1})]$.  \n   - Substitute $r_{1}(a^{1})$ to show equality (no gain).  \n2. **Player 2's deviation**:  \n   - Maximum deviation payoff: $(1-\\delta)B + \\delta [1 - b_{2}(u(a^{*}))]$.  \n   - Compare to equilibrium payoff: $(1-\\delta)W + \\delta \\min\\{1 - b_{1}(0,w_{2}), 1 - b_{2}(0,w_{2}) - r_{1}(a^{1})\\}$.  \n   - By (A.4), deviation is unprofitable.",
    "question_context": "Let $a^{*}$ be a Nash equilibrium of the disagreement game $G$ where $u(a^{*}) \\neq (0,w_{2})$, and let $b_{1}[u(a^{*})]$ and $b_{2}[u(a^{*})]$ be the equilibrium offers in the bargaining game with disagreement payoff $u(a^{*})$ from Corollary 1.\n$w_{2} = u_{1}(a^{1}) + u_{2}(a^{1}) - \\max_{a_{1}^{\\prime} \\in A_{1}} u_{1}(a_{1}^{\\prime}, a_{2}^{1})$\n$r_{1}(a^{1}) = \\frac{1-\\delta}{\\delta} \\left[ \\max_{a_{1}^{\\prime}} u_{1}(a_{1}^{\\prime}, a_{2}^{1}) - u_{1}(a^{1}) \\right]$\n$b_{1}(0,w_{2}) = \\frac{1-w_{2}}{1+\\delta}$ and $b_{2}(0,w_{2}) = \\frac{\\delta(1-w_{2})}{1+\\delta}$\nThe strategy profile specifies offers and responses in odd and even periods, with deviations triggering punishment strategies derived from Corollary 1.\n\nThe proof establishes the perfect equilibrium in a bargaining game with disagreement payoff $(0,\\bar{w}_{2})$. It involves deriving a necessary discount factor threshold $\\underline{\\delta}$, constructing a strategy profile, and verifying subgame perfection for $\\delta \\in (\\underline{\\delta}, 1)$."
  },
  {
    "qid": "econ-empirical-1156-4-0-3",
    "question": "4) Using the formula for CHRFE, calculate the percentage change in relative female earnings if $EF70_i = 20,000$, $EM70_i = 25,000$, $EF80_i = 22,000$, and $EM80_i = 26,000$. Interpret the result.",
    "gold_answer": "1. CHRFE is defined as: \n   $$ CHRFE_i = \\frac{(EF80_i / EM80_i) - (EF70_i / EM70_i)}{EF70_i / EM70_i} $$\n2. Substituting values: \n   $$ CHRFE_i = \\frac{(22,000 / 26,000) - (20,000 / 25,000)}{20,000 / 25,000} = \\frac{0.846 - 0.8}{0.8} = 0.0575 $$\n3. The result (5.75%) indicates a 5.75% increase in relative female earnings from 1970 to 1980.",
    "question_context": "The most important result is that the coefficient on the variable CHDDIND is consistently negative and significant at the one percent level (its $t\\cdot$ -ratios are always about 7.0). The negative sign on $b_{1}$ is evidence that within occupations, female workers tend to be more prevalent in low-wage industries than males, so that industry segregation of workers widens the wage differential between male and female workers in the same occupation.\nThe second important finding is that the coefficient of GROWTH is positive and highly significant in all specifications. The $t$ -ratios are approximately 4.0 and the beta coefficient indicates that it is the second most important explanatory variable.\nCHEDUC has a positive coefficient, though it is not usually significant. The coefficient of CHEXP is significant and has the expected negative sign.\nFDOM and MDOM also produced significant regression coefficients. The percent change in relative female earnings was about 4 percent greater in female-dominated jobs than in neutral ones and about 5 percent less in male-dominated than in neutral jobs.\n\nThe results from Table 4 support the predictions regarding the impact of various variables on the percentage change in relative female earnings (CHRFE). Key findings include the negative and significant coefficient on CHDDIND, indicating that female workers are more prevalent in low-wage industries, widening the wage differential. The positive coefficient on GROWTH suggests that rapid demand increases lower barriers for women in high-wage firms. Other variables like CHEDUC, CHEXP, FDOM, and MDOM also show significant effects."
  },
  {
    "qid": "econ-empirical-248-1-0-0",
    "question": "1) Prove that the conditions for the first-stage estimator ensure the existence of a unique fixed point for the mapping $T(\\alpha)$. Use the contraction mapping theorem.",
    "gold_answer": "1. **Define the mapping**: $T(\\alpha) = (\\tilde{X}^{0\\prime}\\tilde{X}^{0})^{-1}\\tilde{X}^{0\\prime}\\mu(\\alpha)$.  \n2. **Show contraction**: For any $\\alpha_1, \\alpha_2$, we have:  \n   $$ \n   \\rho(T(\\alpha_1), T(\\alpha_2)) \\leq \\gamma \\rho(\\alpha_1, \\alpha_2), \n   $$ \n   where $\\gamma = \\max_k \\sum_{j=1}^K \\sum_{i=1}^L |a_{ji}| d_{ik} < 1$ by Condition 3.  \n3. **Apply contraction mapping theorem**: Since $T$ is a contraction on a complete metric space, it has a unique fixed point.",
    "question_context": "The estimation procedure consists of two different stages. To estimate the first stage, the sample space of the independent variables is divided into $L+1$ nonoverlapping fixed cells $S_{1},S_{2},\\ldots,S_{L}$ $(\\cup_{i=1}^{L}S_{i})^{c}$ $\\left\\{{S}_{i}\\right\\}$ and fixed $K$ -dimensional vectors, $X_{1}^{0},X_{2}^{0},\\ldots,\\bar{X_{L}^{0}}$ are taken so that the following conditions are satisfiedfor $i=1,2,...,L$\nCondition $I$ $S_{i}$ is bounded and there exists $\\delta>0$ such that \n\n$$ \n\\operatorname*{lim}_{N\\to\\infty}\\mathbb{P}\\left[\\sum_{j=1}^{N}1\\big(x_{j}^{0}\\in S_{i}\\big)/N<\\delta\\right]=0. \n$$\nCondition 2. $\\tilde{X}^{0\\prime}\\tilde{X}^{0}$ is a nonsingular matrix, where \n\n$$ \n\\tilde{X}_{i}^{0}=X_{i}^{0}-\\sum_{\\lambda=1}^{L}X_{\\lambda}^{0}/L\\quad\\mathrm{and}\\quad\\tilde{X}^{0\\prime}=\\big(\\tilde{X}_{1}^{0},\\tilde{X}_{2}^{0},\\ldots,\\tilde{X}_{L}^{0}\\big). \n$$\nCondition 3. \n\n$$ \n\\operatorname*{max}_{k}\\sum_{j=1}^{K}\\sum_{i=1}^{L}|a_{j i}|d_{i k}<1, \n$$ \n\nwhere $a_{j i}$ is the $(j,i)$ th element of $(\\tilde{X}^{0\\prime}\\tilde{X}^{0})^{-1}\\tilde{X}^{0\\prime}$ and $d_{i k}=\\operatorname*{sup}_{\\xi\\in S_{i}}|$ the $k$ th element of $(\\xi-X_{i}^{0})|$\n\nThe first-stage estimator involves dividing the sample space into nonoverlapping cells and defining conditions for estimation."
  },
  {
    "qid": "econ-empirical-956-1-1-0",
    "question": "5) Using Lemma 4, show that the matrix $\\overline{{\\pmb{x}^{\\top}\\pmb{\\Sigma}_{\\theta_{0}\\theta_{0}}^{-1}\\pmb{x}}} - \\bar{\\pmb{x}}^{\\top}\\bar{\\pmb{\\Sigma}}_{\\theta_{0}\\theta_{0}}^{-1}\\bar{\\pmb{x}}$ is positive semi-definite.",
    "gold_answer": "1. Let $A_{t} = \\pmb{x}_{t}^{\\top}\\pmb{\\Sigma}_{\\theta_{0}\\theta_{0},t}^{-1/2}$ and $B_{t} = \\bar{\\pmb{x}}^{\\top}\\bar{\\pmb{\\Sigma}}_{\\theta_{0}\\theta_{0}}^{-1/2}$.\n2. Apply Lemma 4 to obtain: $$\\overline{{\\pmb{x}^{\\top}\\pmb{\\Sigma}_{\\theta_{0}\\theta_{0}}^{-1}\\pmb{x}}} - \\overline{{\\pmb{x}^{\\top}\\pmb{\\Sigma}_{\\theta_{0}\\theta_{0}}^{-1/2}}} \\bar{\\pmb{\\Sigma}}_{\\theta_{0}\\theta_{0}}^{1/2} \\bar{\\pmb{x}} (\\bar{\\pmb{x}}^{\\top}\\bar{\\pmb{\\Sigma}}_{\\theta_{0}\\theta_{0}}^{-1}\\bar{\\pmb{x}})^{-1} \\bar{\\pmb{x}}^{\\top} \\bar{\\pmb{\\Sigma}}_{\\theta_{0}\\theta_{0}}^{1/2} \\overline{{\\pmb{\\Sigma}_{\\theta_{0}\\theta_{0}}^{-1/2}\\pmb{x}}}.$$\n3. Simplify to show the result is positive semi-definite.",
    "question_context": "The optimal choice o fˆ $\\bar{\\phi}$ is $\\bar{\\Sigma}_{\\theta\\theta}^{-1}$ and the optimal variance is given by $$\\begin{array}{r}{\\mathrm{Var}(\\widehat{\\pmb{\\theta}}^{\\mathrm{LTT}})^{*}=(\\bar{{\\pmb x}}^{\\intercal}\\bar{\\pmb{\\Sigma}}_{{\\pmb{\\theta}_{0}\\pmb{\\theta}_{0}}}^{-1}\\bar{{\\pmb x}})^{-1}.}\\end{array}$$\nLemma 4 (Cauchy–Schwarz Inequality for Matrix-valued Processes). Let $A=(A_{t})_{t\\in[0,T]}$ and $B=(B_{t})_{t\\in[0,T]}$ be two matrix processes such that all the following matrix products are well-defined. Assume that $\\overline{{B^{\\intercal}B}}$ is invertible. The following matrix $$\\overline{{A^{\\intercal}A}}-\\overline{{A^{\\intercal}B}}(\\overline{{B^{\\intercal}B}})^{-1}\\overline{{B^{\\intercal}A}}$$ is positive semi-definite.\n\nThis section derives the efficiency bound for estimating the time-invariant parameter vector θ under the restricted model."
  },
  {
    "qid": "econ-empirical-280-4-1-0",
    "question": "1) Derive the condition for the discount factor $\\delta$ in Proposition B1 (FIX) that ensures the grim-trigger strategy is a perfect Bayesian equilibrium.",
    "gold_answer": "1. The gain from deviating in a block is 1.  \n2. The loss per period from punishment is $1/16$.  \n3. The condition is $1 < \\frac{1/16}{1 - \\delta} \\delta$.  \n4. Solving gives $\\delta > 16/17$.",
    "question_context": "Proposition B1 (FIX) In the infinitely repeated game with fixed matching between two players (as in FIX), the following constitutes a perfect Bayesian equilibrium if the discount factor is at least 16/17. Players consider periods in (disjoint) blocks of two and adopt the following grim-trigger strategy: State the first preference in each block truthfully and the second so that one states overall one strong and one weak preference in the block of two periods if both players have done so in the past. If one player has at any time not stated one weak and one strong preference, state strong preferences in all future periods. The ex ante (i.e. before preferences are drawn) expected payoff per period in this equilibrium is 13/16.\nProposition B2 (RLK) In the infinitely repeated game with random link formation in a fixed group of four players (as in RLK), the following constitutes a perfect Bayesian equilibrium if the discount factor is at least 864/911. Players consider one period at a time and adopt the following grim-trigger strategy: For one project, state a strong preference, for two or three projects, state one weak preference and for four projects, state two weak preferences if all players have followed this pattern in the past. State preferences as truthfully as possible given these restrictions. If one player has ever not satisfied the restrictions, always state strong preferences in all future periods. The ex ante expected payoff per period in this equilibrium is 695/432.\n\nThe text provides five Propositions and corresponding Proofs that formally establish the equilibria in dynamic games with incomplete information about players' types. The payoffs are simplified to $w=1$ and $s=2$ for analysis."
  },
  {
    "qid": "econ-empirical-627-2-0-0",
    "question": "1) Derive the long run variance $\tilde{V}_{(S)}$ using the Bartlett kernel as suggested by Newey and West (1987). Explain the role of the bandwidth selection method by Andrews (1991) in this context.",
    "gold_answer": "1. The long run variance $\tilde{V}_{(S)}$ is calculated as: \n   $$ \n   \\tilde{V}_{(S)} = \\sum_{j=-(n-1)}^{n-1} k\\left(\\frac{j}{b}\\right) \\hat{\\gamma}(j), \n   $$ \n   where $k(\\cdot)$ is the Bartlett kernel, $b$ is the bandwidth, and $\\hat{\\gamma}(j)$ is the sample autocovariance at lag $j$. \n2. The bandwidth selection method by Andrews (1991) minimizes the asymptotic mean squared error (AMSE) of the long run variance estimator. The optimal bandwidth $b$ is chosen as: \n   $$ \n   b = \\left( \\frac{2q}{n} \\right)^{1/(2q+1)}, \n   $$ \n   where $q$ is the order of the kernel and depends on the smoothness of the spectral density.",
    "question_context": "Due to nuisance parameters relating to serial correlation in the data, tests developed under the iid assumption usually do not have distribution-free limits. Bai (2003) used martingale transformation methods to obtain distribution-free tests. A computationally simpler alternative is proposed by Bai and Ng (2005), who suggested standardizing test statistics by their long run variances. We adopt the second approach in this paper. In particular, we calculate the long run variance $\tilde{V}_{(S)}$ for the test statistic using the Bartlett kernel suggested by Newey and West (1987), for which the bandwidth is chosen according to the data-driven method by Andrews (1991).\nIn terms of inferences, we use the stationary bootstrap method proposed by Politis and Romano (1994). To mimic the serial correlation in data, this method entails wrapping up a time series into a cycle and resampling blocks of data from this infinite ‘loop’ of data. The performance of this method depends critically on how one selects the block length in resampling. We use the algorithm proposed by Politis and White (2004) to determine the optimal block length, which obtains the fastest possible rate of convergence that is adaptive on the strength of the correlation of the time series as measured by the correlogram. Denote the optimal block length by $\\hat{l}.$ We then use a sampling method wherein the distribution of resampled block length follows a geometric distribution with mean l.\n\nTesting for symmetry is often conducted on dependent data, such as time series of macroeconomic variables or financial returns. This section presents an extension of the proposed test to dependent data, addressing issues like serial correlation and the use of long run variances."
  },
  {
    "qid": "econ-empirical-1075-3-0-3",
    "question": "4) Construct a hypothesis test to determine whether the observed increase in the College/High School ratio for men (5.7% unadjusted, 3.0% adjusted) is statistically significant. Use the standard errors provided in Table 3.",
    "gold_answer": "1. **Null hypothesis (H₀)**: True growth rate = 0%.  \n2. **Test statistic (unadjusted)**: (5.7% - 0%) / 0.7% = 8.14 (reject H₀ at p < 0.01).  \n3. **Test statistic (adjusted)**: (3.0% - 0%) / 0.8% = 3.75 (reject H₀ at p < 0.01).  \n4. **Conclusion**: Both unadjusted and adjusted growth rates are statistically significant, but the adjustment reduces the magnitude by 47%, indicating substantial measurement bias.",
    "question_context": "These adjustment factors imply that to simulate the effect of the 13A adjustment pre-1992 geometric mean earnings should be adjusted by $-0.6$ percent for High School Graduates, by $-1.0$ percent for males with Some College, and by $-1.5$ percent for females with Some College (in addition to the adjustments suggested by Table 2).\nThe unadjusted comparison seems to show that, for both men and women, earnings growth was quite different for college graduates than for other workers. Men with college degrees experienced a slight decline in earnings, while those without a college degree saw large decreases in earnings. For women, the earnings of college graduates increased slightly, while the earnings of noncollege graduates fell slightly.\nBetween 1989 and 1993, the College/High School ratio increased by 3.7 percent (6 percentage points) for women and by 5.7 percent (8.6 percentage points) for men.\nAdjusting the series has a substantial effect on growth in earnings by education level. Making only the 13A adjustment, one would conclude that the change in questions accounted for 7 percent of the estimated increase in the College/High School ratio. After making our proposed adjustment using 1991 (1992) earnings, we see that the change in questions accounted for 29 (48) percent of the estimated increase in the ratio for men and 44 (73) percent of the increase for women.\n\nThe analysis provides adjustment factors for pre-1992 data to simulate the effect of the 13A adjustment, which affects geometric mean earnings differently across education categories and sexes. The adjustments are derived from observed effects in unmatched ORG data from 1989-91."
  },
  {
    "qid": "econ-empirical-99-4-0-3",
    "question": "4) Analyze the decay pattern in contributions over time, referencing Rapoport and Chammah (1965) and its similarity to the experimental design in the text.",
    "gold_answer": "1. **Decay Pattern**: Observed in low MPCR groups, consistent with NPD research (e.g., Kelly and Grzelak).\n2. **Rapoport and Chammah**: Longer declines when players lack full matrix information, analogous to the text's design where subjects knew total contributions but not individual incentives.\n3. **Implication**: Incomplete information slows convergence to Nash Equilibrium.",
    "question_context": "The prediction is unambiguous that, for any of the four treatments, no participant should contribute any tokens to the group exchange. In fact, 59 out of 84 people contributed precisely zero, while 68 out of 84 contributed fewer than one third of their tokens.\nThirty-five of forty-two persons contributed zero in groups with the low MPCR, while twenty-four of forty-two contributed zero when faced with a high MPCR. The z-statistic on difference in proportions is 2.625.\nThe results across all periods are not supportive of the multiperiod Nash Equilibrium prediction of zero contribution in every period. Instead, the experiments uniformly begin with positive contributions followed by a tendency for contributions to decay.\nComparison of our results with those from NPD research is somewhat difficult because of the important differences in the two institutions and in other matters of experimental design.\n\nThe text discusses experimental results related to the Prisoners' Dilemma and public goods contributions, focusing on the effects of Marginal Per Capita Return (MPCR) and group size (n) on contribution behavior."
  },
  {
    "qid": "econ-empirical-1559-2-0-3",
    "question": "4) The event study shows parallel pre-trends. Derive the econometric conditions under which this validates the difference-in-differences design, specifying the exact parallel trends assumption being tested.",
    "gold_answer": "Parallel trends requires: $E[Y_{ist}(0) - Y_{ist'}(0)|D_s=1] = E[Y_{ist}(0) - Y_{ist'}(0)|D_s=0]$ for all $t,t' < T_0$ where $T_0$ is reform year. Formally tested by: $\\beta_k \\approx 0 \\forall k < 0$ in $Y_{ist} = \\alpha + \\sum_{k \\neq -1} \\beta_k 1(t-T_0=k) + \\gamma X_{ist} + \\theta_s + \\lambda_t + \\epsilon_{ist}$ where $\\beta_k$ are event-time coefficients.",
    "question_context": "The introduction of unilateral divorce increased the divorce rate by 0.24, representing a 34% increase over the baseline mean in reform states.\nColumns 5-6 of Table 2 show a decline of 1.5 percentage points (3% of baseline sample mean) in the proportion of married women after the reform in treatment states.\nColumns 5-6 of Table B.2 show an increase of 0.86 divorces per 1000 married people (38% over baseline mean) after unilateral divorce.\n\nThis section analyzes the impact of unilateral divorce reforms on divorce rates and marriage rates, using data from Mexico between 1998 and 2016. The analysis includes flow measures (divorce rates per 1000 inhabitants) and stock measures (proportion of divorced women)."
  },
  {
    "qid": "econ-empirical-150-1-0-0",
    "question": "1) Derive the conditions under which the traditional Divisia index measure of total factor productivity yields unbiased estimates of technical change.",
    "gold_answer": "The traditional Divisia index measure of TFP yields unbiased estimates of technical change if and only if:\n1. Producers are in long-run equilibrium.\n2. The technology exhibits constant returns to scale (CRS), i.e., \\(\\rho = 1\\).\n3. Output and input markets are perfectly competitive.\n4. Factors are utilized at a constant rate.\n\nMathematically, the unbiasedness condition can be expressed as:\n\\[\n\\Delta TFP_t = \\lambda_X(t)\n\\]\nwhere \\(\\lambda_X(t)\\) is the input-based technical change measure.",
    "question_context": "The traditional measure of total factor productivity based on the Divisia index formula assumes, in particular: (1) that producers are in long-run equilibrium, (2) that the technology exhibits constant returns to scale, (3) that output and input markets are perfectly competitive, and (4) that factors are utilized at a constant rate. If any one of those assumptions are violated, the traditional measure of total factor productivity will in general yield biased estimates of technical change.\nThe model considered here relaxes all of the above listed assumptions that correspond to the traditional measure of productivity. In the following we define, within the context of our model, appropriate measures of technical change. We also define a measure of capacity utilization. Furthermore we decompose the traditional measure of productivity into technical change and sources of potential bias.\n\nThe traditional measure of total factor productivity based on the Divisia index formula assumes long-run equilibrium, constant returns to scale, perfect competition, and constant factor utilization rates. Violations of these assumptions lead to biased estimates of technical change. This section relaxes these assumptions and defines appropriate measures of technical change and capacity utilization within the context of the model."
  },
  {
    "qid": "econ-empirical-760-1-0-3",
    "question": "4) Using a linear demand curve \\( q = a - bp \\), show how market area (radius \\( R \\)) affects the elasticity of demand for a Loschian competitor.",
    "gold_answer": "1. **Demand Specification**: \\( q(R) = (a - bp)\\pi R^2 \\). \\n2. **Elasticity**: \\( \\epsilon(R) = -\\frac{dq}{dp} \\cdot \\frac{p}{q} = b \\cdot \\frac{p}{a - bp} \\). \\n3. **Radius Effect**: Since \\( q \\) scales with \\( R^2 \\), \\( \\epsilon(R) \\) is invariant to \\( R \\) for linear demand. Nonlinearities (e.g., concave demand) alter this result.",
    "question_context": "In [2, p. 1335], following the equation for equilibrium price with Loschian competition, we state that it is the 'smallest' positive root that is relevant. It should have read 'largest.' Later in the paper (e.g. Figure 1, p. 1336) we do use the larger root; hence, none of our conclusions are affected.\nThe smaller root solution is unstable and inconsistent with nonnegative consumption; only the larger root is economically admissible.\nLoschian 'competition' in one sense is not competitive at all. Each firm takes its radius as given (i.e., it is a local monopolist); hence, the pure spatial monopolist and Loschian 'competitor' are both monopolists of a kind.\nWhich charges a higher price depends on how market size affects demand elasticity—-something about which we have no intuition. With a linear demand curve increases in market area (radius) will increase the elasticity of the demand curve for the firm, but the opposite effect is also possible.\n\nThis section addresses a correction regarding the equilibrium price equation in Loschian competition and discusses the economic admissibility of roots in the context of spatial monopoly and competition models."
  },
  {
    "qid": "econ-empirical-779-5-3-3",
    "question": "4) Explain why $\\lim_{t \\to \\infty} x_{i t} = \\infty$ almost surely implies that $\\mu_{i t}(\\hat{\\theta}) \\to 0$ almost surely for all $\\hat{\\theta} \\neq \\theta$.",
    "gold_answer": "1. The definition $x_{i t} = \\log(\\mu_{i t}(\\theta) / \\mu_{i t}(\\hat{\\theta}))$ implies $\\mu_{i t}(\\hat{\\theta}) = \\mu_{i t}(\\theta) e^{-x_{i t}}$.\n2. If $x_{i t} \\to \\infty$, then $e^{-x_{i t}} \\to 0$.\n3. Since $\\mu_{i t}(\\theta) \\leq 1$, it follows that $\\mu_{i t}(\\hat{\\theta}) \\to 0$ almost surely.\n4. This holds for all $\\hat{\\theta} \\neq \\theta$, meaning agents learn the true state $\\theta$ with probability 1.",
    "question_context": "Let $\\pmb{\\theta}$ denote the underlying state and $A_{t}=[a_{i j t}]$ denote the matrix of weights in learning rule (3) that agents assign to their neighbors' beliefs at time $t$ , with the convention that $a_{i j t}=0$ if $j\\notin N_{i}$ . Note that when agents' learning rules are unanimous, $A_{t}$ is a stochastic matrix for all $t$ . For any given state ${\\hat{\\theta}}\\neq\\theta$ , equation (4) implies that $x_{t+1}=$ $A_{t}x_{t}+y_{t+1}(\\omega_{t+1})$ , where $x_{i t}=\\log(\\mu_{i t}(\\theta)/\\mu_{i t}(\\hat{\\pmb{\\theta}}))$ and $y_{i t}(\\omega_{i t})=\\log(\\ell_{i}^{\\theta}(\\omega_{i t})/\\ell_{i}^{\\hat{\\theta}}(\\omega_{i t})),$ 0. Con \n\nsequently, \n\n$$\nx_{t}=y_{t}(\\omega_{t})+\\sum_{\\tau=1}^{t-1}A_{t-1}\\dots A_{\\tau+1}A_{\\tau}y_{\\tau}(\\omega_{\\tau}),\n$$ \n\nwhere we are using the fact that, under uniform prior beliefs, $x_{i0}=0$ for all agents i. By Lemma A.1, there exists a sequence of uniformly lower-bounded probability vectors $\\boldsymbol{v}_{\\u{\\tau}}$ that jointly satisfy (16) for all $t\\geq\\tau$ . Therefore, pre-multiplying both sides of (28) by $\\pmb{v}_{t}^{\\prime}$ and using (16) implies that $\\begin{array}{r}{\\upsilon_{t}^{\\prime}x_{t}=\\sum_{\\tau=1}^{t}\\upsilon_{\\tau}^{\\prime}y_{\\tau}(\\omega_{\\tau})}\\end{array}$ . As a result, \n\n$$\n\\operatorname*{lim}_{t\\to\\infty}\\operatorname*{inf}_{t}\\frac{1}{\\varepsilon}v_{t}^{\\prime}x_{t}=\\operatorname*{lim}_{t\\to\\infty}\\frac{1}{t}\\sum_{\\tau=1}^{t}v_{\\tau}^{\\prime}\\big(y_{\\tau}(\\omega_{\\tau})-h(\\theta,\\hat{\\theta})\\big)+\\operatorname*{lim}_{t\\to\\infty}\\operatorname*{inf}_{t}\\frac{1}{\\tau}\\sum_{\\tau=1}^{t}v_{\\tau}^{\\prime}h(\\theta,\\hat{\\theta}),\n$$ \n\nwhere $h_{i}(\\theta,\\hat{\\theta})=\\mathbb{E}^{\\theta}[y_{i t}(\\omega_{i t})]$ . Since agents? private signals are independently distributed over time, the strong law of large numbers guarantees that the first term on the right-hand side above is equal to zero almost surely, which in turn implies that \n\n$$\n\\operatorname*{lim}_{t\\to\\infty}\\operatorname*{inf}_{t}{\\frac{1}{\\sum_{i=1}^{n}}}v_{i t}x_{i t}=\\operatorname*{lim}_{t\\to\\infty}\\operatorname*{inf}_{t}{\\frac{1}{\\sum_{\\tau=1}^{t}\\sum_{i=1}^{n}}}v_{i\\tau}h_{i}(\\theta,{\\hat{\\theta}})\n$$ \n\nalmost surely. Note that Lemma A.1 also guarantees that $\\mathrm{lim}\\mathrm{inf}_{t\\rightarrow\\infty}v_{i t}>0$ for all i. Furthermore, the assumption that agents do not face an identification problem collectively guarantees that there exists an agent $i$ such that $h_{i}(\\theta,\\hat{\\theta})>0$ . As a result, with probability 1, \n\n$$\n\\operatorname*{lim}_{t\\to\\infty}{\\log{\\frac{1}{t}}\\sum_{i=1}^{n}v_{i t}x_{i t}}>0.\n$$ \n\nWith the above inequality in hand, it is sufficient to establish that, for any pair of agents $i$ and $j_{:}$ \n\n$$\n\\operatorname*{lim}_{t\\to\\infty}\\frac{1}{t}(x_{i t}-x_{j t})=0\n$$ \n\nalmost surely. In particular, (30) and (31), together with the fact that $\\boldsymbol{v}_{t}$ is a probability vector, imply that $\\mathrm{lim}\\mathrm{inf}_{t\\rightarrow\\infty}x_{i t}/t>0$ almost surely for all agents $i$ .Therefore, $\\scriptstyle\\operatorname*{lim}_{t\\to\\infty}x_{i t}=\\infty$ with probability 1, which subsequently guarantees that $\\mu_{i t}(\\hat{\\pmb{\\theta}})\\rightarrow\\mathbf{0}$ almost surely for all ${\\hat{\\theta}}\\neq\\theta$ . In other words, all agents learn the underlying state with probability 1.\n\nThe proof demonstrates that agents' beliefs converge to the true state with probability 1, leveraging the properties of the belief dynamics and the social network structure."
  },
  {
    "qid": "econ-empirical-392-4-0-0",
    "question": "1) Given vectors $a$ and $b$ such that $b^{\\prime}a \\neq 1$, derive the inverse of the matrix $I - a b^{\\prime}$ using the Sherman-Morrison formula.",
    "gold_answer": "1. Start with the Sherman-Morrison formula for a rank-one update:\n   $$(A + uv^{\\prime})^{-1} = A^{-1} - \\frac{A^{-1}uv^{\\prime}A^{-1}}{1 + v^{\\prime}A^{-1}u}$$\n2. Let $A = I$ and $u = -a$, $v = b$:\n   $$(I - a b^{\\prime})^{-1} = I^{-1} - \\frac{I^{-1}(-a)b^{\\prime}I^{-1}}{1 + b^{\\prime}I^{-1}(-a)}$$\n3. Simplify using $I^{-1} = I$:\n   $$= I + \\frac{a b^{\\prime}}{1 - b^{\\prime}a}$$\n4. Final result:\n   $$\\left(I - a b^{\\prime}\\right)^{-1} = I + \\frac{a b^{\\prime}}{1 - b^{\\prime}a}$$",
    "question_context": "Let $^{a}$ and $^{b}$ denote $T\\times1$ vectors such that $b^{\\prime}a\\neq1$ , then the following results are well known: \n\n$$\\left(I-a b^{\\prime}\\right)^{-1}=I+a b^{\\prime}/(1-b^{\\prime}a),$$\n\nand \n\n$$\\left|I-a b^{\\prime}\\right|=1-b^{\\prime}a.$$\nApplying (A.2), we obtain \n\n$$\\begin{array}{r l}&{|W_{2}|=|X^{\\prime}A X|=|X^{\\prime}X-X^{\\prime}u u^{\\prime}X/(u^{\\prime}u)|}\\ &{\\qquad\\quad}\\ &{\\qquad\\quad-\\left|X^{\\prime}X\\Big[I-\\big(X^{\\prime}X\\big)^{-1}X^{\\prime}u u^{\\prime}X/(u^{\\prime}u)\\Big]\\right|}\\ &{\\qquad\\quad}\\ &{\\qquad\\quad=|X^{\\prime}X|\\Big[1-u^{\\prime}X\\big(X^{\\prime}X\\big)^{-1}X^{\\prime}u/\\big(u^{\\prime}u\\big)\\Big]}\\ &{\\qquad\\quad}\\ &{\\qquad=|X^{\\prime}X|\\big(u^{\\prime}u\\big)^{-1}u^{\\prime}M u.}\\end{array}$$\nFrom (A.1) we find \n\n$$\\begin{array}{r l}&{\\left(X^{\\prime}A X\\right)^{-1}=\\left[I-\\left(X^{\\prime}X\\right)^{-1}X^{\\prime}u u^{\\prime}X/\\left(u^{\\prime}u\\right)\\right]^{-1}\\left(X^{\\prime}X\\right)^{-1}}\\ &{\\qquad=\\left[I+\\left(X^{\\prime}X\\right)^{-1}X^{\\prime}u u^{\\prime}X/\\left(u^{\\prime}M u\\right)\\right]\\left(X^{\\prime}X\\right)^{-1}}\\ &{\\qquad=\\left(X^{\\prime}X\\right)^{-1}+\\left(X^{\\prime}X\\right)^{-1}X^{\\prime}u u^{\\prime}X/\\left(X^{\\prime}X\\right)^{-1}/\\left(u^{\\prime}M u\\right).}\\end{array}$$\nAs $A$ is idempotent, it holds good that \n\n$$\\begin{array}{r l}&{A-A X\\big(X^{\\prime}A X\\big)^{-1}X^{\\prime}A=A\\Big[I-X\\big(X^{\\prime}A X\\big)^{-1}X^{\\prime}\\Big]A}\\ &{\\qquad=A\\big[M-\\big(I-M\\big)u u^{\\prime}\\big(I-M\\big)/\\big(u^{\\prime}M u\\big)\\big]A}\\ &{\\qquad=A\\Big[M-\\big(u-M u\\big)\\big(u-M u\\big)^{\\prime}\\big/\\big(u^{\\prime}M u\\big)\\Big]A.}\\end{array}$$\nStraightforward matrix calculations show that \n\n$$\\begin{array}{r l}&{A M A=M-\\left[\\left(M u\\right)u^{\\prime}+u\\left(M u\\right)^{\\prime}\\right]/\\left(u^{\\prime}u\\right)+\\left(u^{\\prime}M u\\right)u u^{\\prime}/\\left(u^{\\prime}u\\right)^{2},}\\ &{}\\ &{A\\left(u-M u\\right)=-M u+\\left(u^{\\prime}M u\\right)u/\\left(u^{\\prime}u\\right),}\\end{array}$$\nand hence \n\n$$\\begin{array}{r l}&{A\\big(u-M u\\big)\\big(u-M u\\big)^{\\prime}A/\\big(u^{\\prime}M u\\big)}\\ &{=\\big(M u\\big)\\big(M u\\big)^{\\prime}/\\big(u^{\\prime}M u\\big)-\\Big[\\big(M u\\big)u^{\\prime}+u\\big(M u\\big)^{\\prime}\\Big]/\\big(u^{\\prime}u\\big)}\\ &{\\mathrm{~~}+\\big(u^{\\prime}M u\\big)u u^{\\prime}/\\big(u^{\\prime}u\\big)^{2}.}\\end{array}$$\nSubstituting (A.6) and (A.8) into (A.5), we obtain \n\n$$\\begin{array}{r}{A-A X\\big(X^{\\prime}A X\\big)^{-1}X^{\\prime}A=M-\\big(M u\\big)\\big(M u\\big)^{\\prime}/\\big(u^{\\prime}M u\\big).}\\end{array}$$\nFrom the definition of $W_{1}$ in (15) we finally obtain \n\n$$\\begin{array}{r l}&{|W_{1}|=\\left|Y^{\\prime}M Y-\\left(Y^{\\prime}M u\\right)\\left(Y^{\\prime}M u\\right)^{\\prime}/\\left(u^{\\prime}M u\\right)\\right|}\\ &{\\qquad=\\left|Y^{\\prime}M Y\\left[I-\\left(Y^{\\prime}M Y\\right)^{-1}\\left(Y^{\\prime}M u\\right)\\left(Y^{\\prime}M u\\right)^{\\prime}/\\left(u^{\\prime}M u\\right)\\right]\\right|}\\ &{\\qquad=\\left|Y^{\\prime}M Y\\right|\\left[1-u^{\\prime}M Y\\left(Y^{\\prime}M Y\\right)^{-1}Y^{\\prime}M u/\\left(u^{\\prime}M u\\right)\\right]}\\ &{\\qquad=|Y^{\\prime}M Y|\\left(u^{\\prime}M u\\right)^{-1}u^{\\prime}B u.}\\end{array}$$\n\nThe appendix presents matrix algebra results and derivations involving idempotent matrices and their inverses, applied to econometric models."
  },
  {
    "qid": "econ-empirical-376-0-3-1",
    "question": "8) Prove that under savings rationality, the long-run transfer of absorption equals the real income transfer from oil importers to exporters.",
    "gold_answer": "1. Let $T$ be the real transfer: \\[ T = \\Delta p \\bar{q} \\], where $\\bar{q}$ is average trade volume. 2. Importers' budget constraint: \\[ \\sum_{t=0}^\\infty \\frac{C_{\\text{imp},t} - T}{(1+r)^t} = W_0 \\]. 3. Exporters' constraint: \\[ \\sum_{t=0}^\\infty \\frac{C_{\\text{exp},t} + T}{(1+r)^t} = W_0 \\]. 4. In steady state, $\\Delta C_{\\text{imp}} = -T$ and $\\Delta C_{\\text{exp}} = +T$.",
    "question_context": "If the United Kingdom is approximately self-sufficient in oil the shock of an oil price increase has no direct effect on U.K. permanent income. Adjustment in the rest of the world may however have a macroeconomic impact on the United Kingdom.\nIf both oil importers and oil exporters obey savings rationality the transfer of real income from the former to the latter must be matched in the long run by a transfer of absorption.\n\nUnanticipated oil price rises reduce the permanent income windfall and introduce additional shocks to the economy."
  },
  {
    "qid": "econ-empirical-1799-3-3-1",
    "question": "8) How can a fiscal reform (e.g., increasing government savings $\\phi$) mitigate the negative growth effects of a monetary stabilization policy?",
    "gold_answer": "1. **Fiscal reform**: Increases $\\phi$ in the capacity growth equation $g_k$.\\n2. **KK curve shift**: Rightward shift of KK curve offsets the leftward shift caused by lower $\\hat{M}$.\\n3. **New equilibrium**: Higher $\\phi$ can restore $g_k$ to pre-stabilization levels (point $E^{*}$ in fig. 5).\\n4. **Output path**: Avoids permanent growth loss, as shown in the lower part of fig. 6.\\n5. **Policy mix**: Combines monetary restraint with fiscal expansion to balance stability and growth.",
    "question_context": "We consider first the consequences of an orthodox stabilization plan that concentrates only on reducing the rate of growth of nominal money supply. The consequences of such a policy are illustrated in fig. 5. The initial upper part of fig. 6 corresponds to fig. 5: Before time $t_{0}$ the economy is in steady-state equilibrium; at $\\pmb{\\mathrm{t}_{0}}$ money growth is permanently reduced and as a consequence there follows a period of time, between $\\pmb{\\ell_{0}}$ and $t_{1}$, in which real output is below its normal capacity utilization level, which produces an upside-down bubble in the path of real output. After $t_{1}$ a new equilibrium position is reached and the output path once more coincides with the normal capacity utilization path but, since the rate of growth of capacity has been reduced, this path has now suffered a downward twist.\n\nThis section discusses the implications of orthodox stabilization policies on growth and inflation."
  },
  {
    "qid": "econ-empirical-1395-2-0-3",
    "question": "4) How would you test for temporal variations in the effect of police deployment by splitting the \\( POST_t \\) period into \\( POST_t^1 \\) (6-week policy period) and \\( POST_t^2 \\) (post-policy period)? Provide the modified reduced-form equations and the testable hypothesis.",
    "gold_answer": "1. **Modified Reduced-Form Equations**:\n   - Police Deployment:\n     \\[\n     \\Delta_{52}p_{bt} = \\alpha_4 + \\beta_4 POST_t + \\delta_{41}(T_b \\times POST_t^1) + \\delta_{42}(T_b \\times POST_t^2) + \\lambda_4 \\Delta_{52}x_{bt} + \\Delta_{52}\\varepsilon_{4bt}\n     \\]\n   - Crime Rates:\n     \\[\n     \\Delta_{52}c_{bt} = \\alpha_5 + \\beta_5 POST_t + \\delta_{51}(T_b \\times POST_t^1) + \\delta_{52}(T_b \\times POST_t^2) + \\lambda_5 \\Delta_{52}x_{bt} + \\Delta_{52}\\varepsilon_{5bt}\n     \\]\n2. **Testable Hypothesis**: The null hypothesis \\( H_0: \\delta_{41} = \\delta_{42} \\) (for police) or \\( H_0: \\delta_{51} = \\delta_{52} \\) (for crime) tests whether the effect of the intervention differs between the immediate policy period and the post-policy period. Rejection implies temporal variation in the treatment effect.",
    "question_context": "Another issue that could affect identification is crime displacement. Since the police intervention affected the costs of crime across locations and time, it may be that criminals take these changes into account and adjust their behavior. This raises the possibility that criminal activity was either diverted into other areas (e.g., the comparison group of boroughs) during Operation Theseus or postponed until after the extra police presence was withdrawn. The implication is that simple difference-in-differences (DiD) estimates of the police effect on crime would be upwardly biased if these offsetting spatial displacement effects were not taken into account. Temporal displacement can have the opposite effect, and we discuss this more in the final empirical section.\n\nThe text discusses the potential for crime displacement due to changes in police deployment following the July 2005 terrorist attacks in London. It highlights the need to account for spatial and temporal displacement in difference-in-differences (DiD) estimates to avoid bias."
  },
  {
    "qid": "econ-empirical-1410-4-0-0",
    "question": "1) Discuss the key behavioral qualities of an entrepreneur as outlined in the S4 competencies. How do these qualities contribute to business success?",
    "gold_answer": "The key behavioral qualities of an entrepreneur include:\n- **Innovation**: Ability to generate new ideas and solutions.\n- **Risk-taking**: Willingness to take calculated risks.\n- **Persistence**: Determination to overcome challenges.\n- **Leadership**: Ability to guide and inspire others.\n\nThese qualities contribute to business success by fostering a proactive approach to problem-solving, enabling adaptability to market changes, and ensuring sustained growth through effective leadership and risk management.",
    "question_context": "At the end of senior four (S4), the learner should be able to: Exhibit the behavioral qualities of an entrepreneur, Make rational career choices in daily life, Make plans to reach their personal goals, Evaluate the need for laws in business operation, Analyze the role of standards in business, Examine key components of a market and the role of market research, Analyze the importance of management in a business organization, Evaluate short- and long-term capital for future investment, Evaluate the services/products offered by financial institutions.\nAt the end of senior five (S5), the learner should be able to: Generate business ideas and take advantage of opportunities, Make valid contracts and resolve conflicts in business operations, Justify the need for taxes in the economy, Evaluate the factors that lead to business growth, Analyze the role of technology in businesses and daily life, Maintain good relations with people at the workplace through effective communication, Demonstrate ability and knowledge of carrying out general office operations, Record accounting transactions and manage finances responsibly, Exercise rights and responsibilities as an employee and employer, Lead a team in accomplishing a goal.\nAt the end of senior six (S6), the learner should be able to: Prepare a business plan for an enterprise, Develop an ethical understanding of the Rwandan customs system, Establish an effective quality compliance system in business activities, Evaluate the contribution of entrepreneurship towards socio-economic development, Analyze the Environmental Impact Assessment (EIA) as a tool for prevention and control of environmental impacts caused by socio-economic development.\n\nThe syllabus outlines the expected competencies for students at different grade levels, focusing on entrepreneurial skills, business operations, and socio-economic development."
  },
  {
    "qid": "econ-empirical-774-1-1-3",
    "question": "4) Prove that under Assumption 4 (Unit Demand), the efficient outcome allocates each item to the buyer with the highest valuation for that item.",
    "gold_answer": "1. The efficient outcome maximizes $W = \\sum_{i=1}^{n} \\sum_{j=1}^{m} x_{i}^{j} \\nu_{j}(\\theta_{i})$.  \n2. Under unit demand, each buyer can receive at most one item, so the problem reduces to a matching problem.  \n3. By Assumption 1, $\\nu_{j}(\\theta_{i})$ is strictly increasing in $\\theta_{i}$, so the highest-type buyer has the highest valuation for each item.  \n4. Thus, the efficient allocation assigns item $j$ to the buyer with the highest $\\theta_{i}$, ensuring $W$ is maximized.",
    "question_context": "A monopolist seller has a set $M=\\{1,2,\\dots,m\\}$ $(m\\ge2)$) of heterogeneous items for sale and faces a fixed set of $n$ buyers, with $n\\geq m+1$. The seller’s valuation for each of the m items is 0, whereas buyer $i$’s $1\\leq i\\leq n$) valuation for item $j$ ($1\\leq j\\leq m)$ is $\\nu_{j}(\\theta_{i})$, where $\\theta_{i}\\in\\mathbb{R}$ is buyer $i$’s type. Suppose $\\theta_{i}$ is buyer i’s private information but it is common knowledge that $\\theta_{i}$ is independent and identically distributed according to a common distribution function $F$ that is absolutely continuous and has full support on $[a,b]$ $b](-\\infty\\leq a<b\\leq+\\infty)$) and no atom. Without loss, let $F=U[0,1]$.\nThe seller and all the buyers are expected utility maximizers with quasilinear preferences. Consider an outcome $(\\{x^{j}\\},t)$, where $x_{i}^{j}\\geq0$ is the probability buyer $i$ receives item $j$ and $t_{i}$ is the total payment from buyer $i$ to the seller. For $\\sum_{j=1}^{m}x_{i}^{j}\\leq1$, buyer $i$’s utility from the outcome is $U_{i}=\\sum_{j=1}^{m}x_{i}^{j}\\nu_{j}(\\theta_{i})-t_{i}$. If the seller is a benevolent social planner, then they care only about total welfare $W=\\sum_{i=1}^{n}\\sum_{j=1}^{m}x_{i}^{j}\\nu_{j}(\\theta_{i})$. Otherwise, the seller is profit-maximizing, and they care about total revenue $R = \\sum_{i=1}^{n} t_{i}$.\nSequential auction. The items are sold in a sequential auction with $m$ rounds and one item in each round. We model a sequential auction as a combination of $m$ static auctions, featuring three key components: (1) The static auction for each item $j,A_{j}(r_{j})$. $(A_{j}$ specifies the auction format and $r_{j}$ describes the reserve price.) (2) Sequence $\\tau$ (i.e., a permutation of $M$) of the $m$ static auctions. (3) Disclosure policy $I$ after each round.\nAssumption $I$ (Generalized vertical differentiation). For any $j$, $\\nu_{j}(\\theta)\\geq0$ is continuously differentiable and strictly increasing in $\\theta$.\nAssumption 2 (Decreasing differences order). (With possible re-ordering) The $m$ items can be ranked such that $\\nu_{j}(\\theta)$ has strictly decreasing differences (SDD) in $(j,\\theta)$; that is, for any $j<j^{\\prime}$ and $\\theta>\\theta^{\\prime}$, $\\nu_{j}(\\theta)-\\nu_{j^{\\prime}}(\\theta)>\\nu_{j}(\\theta^{\\prime})-\\nu_{j^{\\prime}}(\\theta^{\\prime})$.\nAssumption 3 (Equilibrium bidding in static auctions). For any symmetric uniform prior $\\theta_{i}$ i.i.d $U[0,c]$ $0<c\\leq1$), the standard auction format(s) and the valuation functions $\\{\\nu_{j}(\\cdot)\\}_{j=1}^{m}$ are such that each static auction $A_{j}$ admits a symmetric, strictly increasing, pure strategy Bayes–Nash equilibrium under this symmetric common prior.\nAssumption 4 (Unit demand). In the seller’s relaxed problem(s), $\\sum_{j=1}^{m}x_{i}^{j}\\leq1,\\forall i$.\n\nIn this section, we present the formal model. We start by describing the model setup before formulating the seller’s problem(s) and introducing the corresponding relaxed mechanism design problem(s). Finally, we discuss the assumptions as well as their implications."
  },
  {
    "qid": "econ-empirical-1584-4-2-0",
    "question": "5) Derive the Bayesian update rule \\( \\psi^{(d,a')}(e,a,s) \\) for type scores in Figure 6. How does the update differ for bankruptcy filers vs. non-filers?",
    "gold_answer": "1. The posterior type score is \\( s' = \\frac{s \\cdot Q^{\beta}(H|H) \\cdot f(a'|H)}{s \\cdot Q^{\beta}(H|H) \\cdot f(a'|H) + (1-s) \\cdot Q^{\beta}(H|L) \\cdot f(a'|L)} \\), where \\( f(a'|\beta) \\) is the action density. \n2. For bankruptcy filers, \\( f(a'|H) \\ll f(a'|L) \\), leading to a sharp drop in \\( s' \\). \n3. For non-filers, \\( f(a'|H) \\approx f(a'|L) \\), so \\( s' \\approx s \\cdot Q^{\beta}(H|H) + (1-s) \\cdot Q^{\beta}(H|L) \\). \n4. The no-inference line represents \\( s' = s \\cdot Q^{\beta}(H|H) + (1-s) \\cdot Q^{\beta}(H|L) \\).",
    "question_context": "Figure 6 plots the change in the public assessment of an individual’s type resulting from Bayesian updating given her current type score and actions (i.e., \\( \\psi^{(d,\bar{a^{\\prime}})}(e,a,s) \\) in (16)).\nThe center and right plots of Figure 6 show the Bayesian updates that result from different actions taken by an individual either already in debt (center at \\( a=-0.02 \\)) or with zero assets (right) for the median earner \\( e=z=0 \\)).\n\nThis section details the Bayesian updating of type scores based on actions (e.g., bankruptcy, repayment) and its implications for credit rankings."
  },
  {
    "qid": "econ-empirical-14-2-0-0",
    "question": "1) Define a 'rational conjectural equilibrium' (D2) and explain the conditions under which a tuple $(X^{*},Y^{*},F^{*}(\\cdot),G^{*}(\\cdot))$ qualifies as such an equilibrium.",
    "gold_answer": "A rational conjectural equilibrium is defined as a tuple $(X^{*},Y^{*},F^{*}(\\cdot),G^{*}(\\cdot))$ where:\n1. $(X^{*},Y^{*})$ lies in the intersection of $K(F^{*}(\\cdot))$ and $L(G^{*}(\\cdot))$.\n2. For some neighborhood $U$ of $X^{*}$, any $X \\in U$ implies $(X,\\phi(X^{*},Y^{*},X)) \\in L(G^{*}(\\cdot))$, where $\\phi(\\cdot)$ solves $dY/dX = F^{*}(X,Y)$.\n3. For some neighborhood $V$ of $Y^{*}$, any $Y \\in V$ implies $(\\psi(X^{*},Y^{*},Y),Y) \\in K(F^{*}(\\cdot))$, where $\\psi(\\cdot)$ solves $dX/dY = G^{*}(X,Y)$.\n\nThis ensures both firms' expectations are rational in the vicinity of $(X^{*},Y^{*})$.",
    "question_context": "We shall say that the first firm's expectations are 'rational' at a point $(X,Y)$ if the predictions the firm makes about the responses of its rival to small (but not necessarily infinitesimal) changes in $X$ represent changes in Y with which the second firm would itself be satisfied (in the face of the given changes in $X$).\nA 'rational conjectural equilibrium' is a tuple $(X^{*},Y^{*},F^{*}(\\cdot),G^{*}(\\cdot))$ such that (i) $(X^{*},Y^{*})\\epsilon K(F^{*}(\\cdot))\\cap L(G^{*}(\\cdot))$, (ii) for some open neighborhood $U$ of $X^{*}$, $X\\epsilon U$ implies $(X,\\phi(X^{*},Y^{*},X)) \\epsilon L(G^{*}(\\cdot))$ where $\\phi(\\cdot)$ solves $dY/dX=F^{*}(X,Y)$, and (iii) for some open neighborhood $V$ of $Y^{*}$, $Y\\epsilon V$ implies $(\\psi(X^{*},Y^{*},Y),Y)\\epsilon K(F^{*}(\\cdot))$ where $\\psi(\\cdot)$ solves $dX/dY=G^{*}(X,Y)$.\nPROPOSITION II. Suppose that $P(\\cdot),C(\\cdot)$, and $D(\\cdot)$ satisfy the hypotheses of Proposition I. Then if $(X^{*},Y^{*})$ also satisfies the hypotheses, we can construct $F^{*}(\\cdot)$ and $G^{*}(\\cdot)$ such that $(X^{*},Y^{*},F^{*}(\\cdot),G^{*}(\\cdot))$ satisfies D2, provided that $|Y^{*}P'(Q^{*})/(P(Q^{*})-D'(Y^{*})+Y^{*}P'(Q^{*}))| > 1$ and $|Y^{*}P'(Q^{*})/(P(Q^{*})-D'(Y^{*})+Y^{*}P'(Q^{*}))| < \\infty$, where $Q^{*}=X^{*}+Y^{*}$.\n\nThe section introduces a rational expectations framework to refine the equilibrium outcomes in a conjectural model, distinguishing between competitive-market and cartel outcomes. It defines rationality for firms' expectations and introduces a new equilibrium concept (D2) that requires these expectations to be rational in the vicinity of equilibrium points."
  },
  {
    "qid": "econ-empirical-1518-3-1-0",
    "question": "1) Critically evaluate the evolutionary psychology approach to explaining economic behavior, highlighting its strengths and limitations.",
    "gold_answer": "1. **Strengths**:\n   - Provides plausible explanations for observed behaviors (e.g., face recognition, cheating detection).\n   - Links cognitive mechanisms to ancestral environments.\n2. **Limitations**:\n   - Often relies on post-hoc \"just-so\" stories.\n   - Difficult to predict new phenomena due to complex adaptation pathways.\n   - Cultural and individual differences may dominate genetic adaptations.",
    "question_context": "Evolutionary psychologists ask: Why might decision rules have evolved or adapted as they did? This approach has, so far, been mostly an exercise in post facto rationalization of observed patterns.\nDifferences in male and female pair-bonding, violence toward children, and sexual behavior also have obvious potential explanations as adapted outcomes.\n\nThis section explores evolutionary psychology's implications for decision research, focusing on adapted decision rules."
  },
  {
    "qid": "econ-empirical-69-3-2-0",
    "question": "5) Using the results from Table IV, columns (1)–(6), formulate a structural model that accounts for the dual roles of principals and superintendents in determining gender wage gaps. How would you test for complementarity between their genders?",
    "gold_answer": "1. **Model**: $\\text{Gap}_{idt} = \\theta_0 + \\theta_1 \\text{Male Principal}_{dt} + \\theta_2 \\text{Male Superintendent}_{dt} + \\theta_3 (\\text{Male Principal} \\cdot \\text{Male Superintendent})_{dt} + \\zeta X_{idt} + u_{idt}$. \\n2. **Complementarity Test**: $H_0: \\theta_3 = 0$ vs. $H_1: \\theta_3 > 0$. Rejecting $H_0$ implies synergistic effects of male leadership. \\n3. **Data Requirement**: Interaction term in Table IV (column (3) for principals, column (6) for superintendents).",
    "question_context": "The change in the gender pay gap is larger in schools with a male principal, and equal to 0.41 on average across the five years following an expiration (column (1)). In schools with a female principal, the change in the gap is small and indistinguishable from zero (column (2)).\nThe estimates reveal a larger gender gap for teachers in districts with a male superintendent, equal to $0.45\\%$ (Table IV, column (4)). In districts with a female superintendent, on the other hand, the change in the gap is positive and indistinguishable from zero (column (5)).\n\nStudies across workplaces have found a positive correlation between female management and women’s career outcomes. We test whether the gender wage gap varies with the gender of school principals and district superintendents."
  },
  {
    "qid": "econ-empirical-1401-1-0-0",
    "question": "1) Derive the first-order conditions for the principal's coalition-free implementation problem (CF) and explain the economic intuition behind each constraint.",
    "gold_answer": "1. **Objective Function**: The principal minimizes expected payments:\n   $$\\min_{v^{1},v^{2}} \\sum_{i}\\sum_{j} P_{ij}(e_1, e_2)(\\phi_1(v_{ij}^1) + \\phi_2(v_{ij}^2))$$\n\n2. **Incentive Compatibility (IC) Constraints**: Agents maximize their expected utility:\n   - For agent 1: $$e_1 \\in \\arg\\max_e \\sum_{i,j} P_{ij}(e, e_2) v_{ij}^1 - G_1(e)$$\n   - For agent 2: $$e_2 \\in \\arg\\max_e \\sum_{i,j} P_{ij}(e_1, e) v_{ij}^2 - G_2(e)$$\n   These ensure Nash equilibrium effort choices.\n\n3. **Participation Constraints (PC)**: Agents must achieve reservation utility:\n   $$\\sum_{i,j} P_{ij}(e_1, e_2) v_{ij}^n - G_n(e_n) \\geq \\bar{U}^n \\quad \\text{for} \\, n=1,2.$$\n   Ensures agents accept the contract.\n\n**Economic Intuition**: The IC constraints align agents' incentives with the principal's desired efforts, while the PC ensures voluntary participation.",
    "question_context": "The 'coalition-free' implementation problem, called (CF), is defined as follows: \n\n$$\\operatorname*{min}_{v^{1},v^{2}}\\sum_{i}\\sum_{j}P_{i j}(e_{1},e_{2})(\\phi_{1}(v_{i j}^{1})+\\phi_{2}(v_{i j}^{2}))$$\n\nsubject to\n\n$$\\begin{array}{r}{e_{1}\\in\\arg\\operatorname*{max}_{i}\\displaystyle\\sum_{j}\\sum_{j}P_{i j}(e,e_{2})v_{i j}^{1}-G_{1}(e),}\\ {e_{2}\\in\\arg\\operatorname*{max}_{e}\\displaystyle\\sum_{i}\\sum_{j}P_{i j}(e_{1},e)v_{i j}^{2}-G_{2}(e),}\\end{array}$$\n\nand\n\n$$\\sum_{i}\\sum_{j}P_{i j}(e_{1},e_{2})v_{i j}^{n}-G_{n}(e_{n})\\geqslant\\bar{U}^{n}\\qquad\\mathrm{for}\\quad n=1,2.$$\n\nThe principal's problem is decomposed into two parts: fixing an effort pair and finding the compensation schemes that implement it with the least costs, and then choosing the effort pair that maximizes the expected benefit minus the minimum expected cost. The implementation problem for a given effort pair is defined with compensation schemes and constraints."
  },
  {
    "qid": "econ-empirical-577-2-0-0",
    "question": "1) Derive the Euler equation for Ricardian households from their optimization problem, explicitly showing the role of the preference shock $\\eta_{t}^{\\xi}$.",
    "gold_answer": "1. Start with the Lagrangian for Ricardian households: $$\\mathcal{L} = \\sum_{t=0}^{\\infty} \\beta^{t} \\exp(\\eta_{t}^{\\xi}) \\left[ \\frac{(C_{t}^{R})^{1-\\sigma}}{1-\\sigma} - \\chi \\frac{(L_{t}^{R})^{1+\\varphi}}{1+\\varphi} \\right] + \\lambda_{t} \\left[ (1+i_{t-1})b_{t-1}^{R}/\\Pi_{t}^{R} + (1-\\tau_{L,t}^{R})w_{t}^{R}L_{t}^{R} + \\Psi_{t}^{R} - C_{t}^{R} - b_{t}^{R} \\right].$$ 2. Take the first-order condition with respect to $C_{t}^{R}$: $$\\beta^{t} \\exp(\\eta_{t}^{\\xi}) (C_{t}^{R})^{-\\sigma} = \\lambda_{t}.$$ 3. Take the first-order condition with respect to $b_{t}^{R}$: $$\\lambda_{t} = \\beta^{t+1} \\exp(\\eta_{t+1}^{\\xi}) (C_{t+1}^{R})^{-\\sigma} (1+i_{t})/\\Pi_{t+1}^{R}.$$ 4. Combine the two conditions to get the Euler equation: $$\\exp(\\eta_{t}^{\\xi}) (C_{t}^{R})^{-\\sigma} = \\beta \\exp(\\eta_{t+1}^{\\xi}) (C_{t+1}^{R})^{-\\sigma} (1+i_{t})/\\Pi_{t+1}^{R}.$$",
    "question_context": "Ricardian households, of measure $1-\\lambda$, solve the problem: $$\\operatorname*{max}_{\\{C_{t}^{R},L_{t}^{R},b_{t}^{R}\\}}\\sum_{t=0}^{\\infty}\\beta^{t}\\exp\\bigl(\\eta_{t}^{\\xi}\\bigr)\\biggl[\\frac{\\bigl(C_{t}^{R}\\bigr)^{1-\\sigma}}{1-\\sigma}-\\chi\\frac{\\bigl(L_{t}^{R}\\bigr)^{1+\\varphi}}{1+\\varphi}\\biggr]$$ subject to a sequence of flow budget constraints: $$C_{t}^{R}+b_{t}^{R}=(1+i_{t-1})b_{t-1}^{R}/\\Pi_{t}^{R}+\\bigl(1-\\tau_{L,t}^{R}\\bigr)w_{t}^{R}L_{t}^{R}+\\Psi_{t}^{R},$$ where $\\eta_{t}^{\\xi}$ is a preference shock.\nHTM households, of measure $\\lambda$, solve the problem: $$\\operatorname*{max}_{\\{C_{t}^{H},L_{t}^{H}\\}}\\frac{\\left(C_{t}^{H}\\right)^{1-\\sigma}}{1-\\sigma}-\\chi^{H}\\frac{\\left(\\left(1+\\eta_{t}^{\\xi}\\right)L_{t}^{H}\\right)^{1+\\varphi}}{1+\\varphi}$$ subject to the flow budget constraint: $$C_{t}^{H}=w_{t}^{H}L_{t}^{H}+Q_{t}s_{t}^{H},$$ where $\\eta_{t}^{\\xi}$ is a shock to disutility from labor.\nThe government flow budget constraint is given by: $$B_{t}+T_{t}^{L}=(1+i_{t-1})B_{t-1}+P_{t}^{R}s_{t},$$ where tax revenues $T_{t}^{L}=(1-\\lambda)\\tau_{L,t}^{R}P_{t}^{R}w_{t}^{R}L_{t}^{R}$.\nMonetary and tax policy rules are of the feedback types with 'smoothing,' given by: $$\\frac{1+i_{t}}{1+\\bar{i}}=\\operatorname*{max}\\biggl\\{\\frac{1}{1+\\bar{i}},\\biggl(\\frac{1+i_{t-1}}{1+\\bar{i}}\\biggr)^{\\rho_{1}}\\biggl(\\frac{1+i_{t-2}}{1+\\bar{i}}\\biggr)^{\\rho_{2}}\\biggl[\\biggl(\\frac{\\Pi_{t}}{\\bar{\\Pi}}\\biggr)^{\\phi}\\biggl(\\frac{Y_{t}}{\\bar{Y}}\\biggr)^{\\phi_{x}}\\biggl(\\frac{Y_{t}}{Y_{t-1}}\\biggr)^{\\phi_{\\Delta y}}\\biggr]^{1-\\rho_{1}+\\bar{i}}}\\biggr\\},$$ $$\\tau_{L,t}^{R}-\\bar{\\tau}_{L}^{R}=\\rho_{L}\\bigl(\\tau_{L,t-1}^{R}-\\bar{\\tau}_{L}^{R}\\bigr)+(1-\\rho_{L})\\psi_{L}(b_{t-1}/\\bar{b}-1).$$\n\nThe model extends a simple framework to include a two-sector production structure with sticky prices and distortionary taxes, focusing on the economic crisis induced by COVID-19. It incorporates Ricardian and Hand-to-Mouth (HTM) households, each with distinct consumption and labor supply behaviors, and analyzes the effects of demand and supply shocks alongside fiscal policies like the CARES Act."
  },
  {
    "qid": "econ-empirical-847-2-0-1",
    "question": "2) Explain the role of the initial conditions $v_{0} = \\gamma(0)$ and $\\phi_{1}(1) = \\gamma(1)/\\gamma(0)$ in the Levinson–Durbin algorithm. How do these conditions ensure the algorithm's correctness?",
    "gold_answer": "1. $v_{0} = \\gamma(0)$ represents the variance of $X_{t}$ when no past observations are used for prediction.\n2. $\\phi_{1}(1) = \\gamma(1)/\\gamma(0)$ is the optimal coefficient for predicting $X_{t+1}$ using only $X_{t}$, derived from minimizing $E[(X_{t+1} - \\phi X_{t})^2]$.\n3. These initial conditions bootstrap the recursion by providing the base case for $m=1$.\n4. The recursion then builds higher-order predictors ($m > 1$) by iteratively incorporating additional lags, ensuring each step minimizes the mean squared error.",
    "question_context": "Suppose that $\\{X_{t},t=0,\\pm1,...\\}$ is a zero-mean stationary process with $\\operatorname{E}(X_{t+u}X_{t})=$ $\\gamma(u),t,u\\in\\{0,\\pm1,...\\}$.\nAlgorithm 1 (Levinson–Durbin): Let $\\left\\{X_{t}\\right\\}$ be a zero-mean stationary process and let $\\hat{X}_{n+1}(m)=\\sum_{j=1}^{m}\\phi_{m}(j)X_{n+1-j}$ be the best linear 1-step predictor of $X_{n+1}$ given $X_{n},\\dots,X_{n+1-m}$ , with corresponding mean squared error $v_{m}$.\nThe vectors $\\Phi_{m}=(\\phi_{m}(1),\\dots,\\phi_{m}(m))^{\\prime}$ and mean squared errors $v_{m},m=1,2,\\ldots$ ; satisfy the following recursions: $\\phi_{m+1}(m+1)=[\\gamma(m+1)-\\sum_{j=1}^{m}\\phi_{m}(j)\\gamma(m+1-j)]v_{m}^{-1}$, $\\phi_{m+1}(j)=\\phi_{m}(j)-\\phi_{m+1}(m+1)\\phi_{m}(m+1-j),\\quad j=1,\\ldots,m$, $v_{m+1}=(1-\\phi_{m+1}^{2}(m+1))v_{m}$.\nRemark 2 (Yule–Walker estimation/data tapers): Suppose we are given observations $x_{1},\\dots,x_{T}$ of a zero-mean stationary time series $\\left\\{X_{t}\\right\\}$. For any $m<T$, the Yule–Walker $\\mathrm{AR}(m)$ model for the series is obtained by substituting the sample covariances $\\hat{\\gamma}(u)=$ $1/T\\sum_{t=1}^{T-u}x_{t}x_{t+u}$ for the covariances $\\gamma(u),u=0,\\ldots,m$, in Eqs. (2) and (3) and solving for $\\phi_{m}(1),\\ldots,\\phi_{m}(m)$ and $v_{m}$.\nAlgorithm 2 (Empirical Levinson–Durbin): With $\\left\\{{\\bf{x}}_{t}\\right\\}$ and $\\hat{\\gamma}(u)$ defined as in (30) and (32) or as in (34) and (35), respectively, and inner products as in (31), let $\\hat{\\mathbf{x}}_{n+1}(m)=\\sum_{j=1}^{m}\\hat{\\phi}_{m}(j)\\mathbf{x}_{n+1-j}$ be the projection of ${\\bf X}_{n+1}$ on the span of $\\mathbf{x}_{n},\\ldots,\\mathbf{x}_{n+1-m}$, with squared error $\\hat{v}_{m}=\\langle{\\bf x}-$ $\\hat{\\mathbf{x}}_{n+1}(m),\\mathbf{x}-\\hat{\\mathbf{x}}_{n+1}(m)\\rangle$.\n\nThis section discusses the Levinson–Durbin algorithm, its derivation from Theorem 1, and its empirical version used in Yule–Walker estimation of autoregressive coefficients for zero-mean stationary processes."
  },
  {
    "qid": "econ-empirical-103-0-1-0",
    "question": "5) Derive the balanced budget multiplier \\( \\frac{d Y}{d G} \\) from the IS-LM model presented in the paper, where the IS curve is \\( Y = C(Y - T, G) + I(r) + G \\) and the LM curve is \\( M / P = L(r, Y) \\).",
    "gold_answer": "1. Totally differentiate the IS curve: \\( dY = C_Y (dY - dT) + C_G dG + I_r dr + dG \\).\n2. Totally differentiate the LM curve: \\( 0 = L_r dr + L_Y dY \\).\n3. Solve for \\( dr \\) from the LM curve: \\( dr = -\\frac{L_Y}{L_r} dY \\).\n4. Substitute \\( dr \\) into the IS curve: \\( dY = C_Y (dY - dT) + C_G dG - I_r \\frac{L_Y}{L_r} dY + dG \\).\n5. Impose a balanced budget \\( dG = dT \\): \\( dY = C_Y (dY - dG) + C_G dG - I_r \\frac{L_Y}{L_r} dY + dG \\).\n6. Solve for \\( \\frac{d Y}{d G} \\): \\( \\frac{d Y}{d G} = \\frac{1 - C_Y + C_G}{1 - C_Y + I_r \\frac{L_Y}{L_r}} \\).",
    "question_context": "The balanced budget multiplier is \\( \\frac{d Y}{d G} = \\frac{1 - C_Y + C_G}{1 - C_Y + I_r (L_Y + F_Y) / L_r} \\). Stability implies that the denominator is positive.\nIf \\( C_G \\) is negative, so that the increase in government spending reduces private consumption, \\( 1 - C_Y + C_G \\) is smaller than in the standard model, and if the magnitude of \\( C_G \\) is sufficiently large, it may be zero or even negative, implying that an increase in government expenditure could lead to a contraction in the economy.\nIf \\( C_G \\) is positive, an increase in government expenditures raises private consumption, and the numerator is larger than appears in the standard model.\n\nThe paper examines the effectiveness of fiscal policy through the lens of the balanced budget multiplier, considering the impact of temporary, permanent, and countercyclical changes in public expenditure."
  },
  {
    "qid": "econ-empirical-348-4-1-0",
    "question": "5) Using a trade model, explain how the United Kingdom's position as the second-largest exporter of higher education services can be sustained. What are the key determinants of competitiveness in this model?",
    "gold_answer": "1. **Model**: Adapt the Ricardian trade model to education services, where comparative advantage is driven by productivity \\( A \\).\n2. **Determinants**:\n   - Quality (measured by RAE scores or student outcomes).\n   - Cost (tuition fees and living costs relative to competitors).\n   - Reputation (historical rankings and alumni success).\n3. **Sustainability**: Invest in \\( A \\) via research funding and quality assurance to maintain competitive edge.",
    "question_context": "It must be remembered that, as in many other areas of commerce and industry, United Kingdom HE competes in a global marketplace. This is most readily seen in the context of the provision of educational services.\nThe same can be said of research. Output here is not as measurable as in the case of teaching where student numbers are an obvious performance indicator. Benchmarking does take place via the RAEs.\nA thorough review of the appropriate level of participation in HE is needed to inform any analysis of the efficient level of investment and public subsidy of HE to secure the maximum benefit of wealth creation and the quality of life in the United Kingdom.\n\nThis section addresses the United Kingdom's position in the international higher education marketplace, emphasizing the need for quality mechanisms and flexible funding regimes to maintain competitiveness. It also outlines key issues for future research and policy review."
  },
  {
    "qid": "econ-empirical-1674-4-0-2",
    "question": "3) Contrast the quasi-hyperbolic model's prediction for conditional filing probabilities with the exponential model's prediction near the deadline (T−7 to T−1). Use Table 3's excess filing metrics to quantify the improvement in fit.",
    "gold_answer": "1. **Exponential Shortcoming**: \n   - Excess conditional probability: +16pp (p < 0.01).\n   - Unconditional: 145% more filers than predicted.\n2. **Quasi-Hyperbolic Improvement**: \n   - Excess reduced by 80% (conditional) and 70% (unconditional).\n3. **Mechanism**: Present bias (β=0.86) tempers procrastination, allowing smoother ramp-up as deadline approaches.",
    "question_context": "In column (1), we use the binned realized future values to estimate the discount factor, δ, and filing cost, c, finding an average filing cost of c=3.341 and a discount factor of δ=0.536.\nTable 2, columns (3)—(5) present estimates of quasi-hyperbolic discounting... In column (3) we fix δ=0.99999 and estimate present bias, β, and costs, c. Costs are estimated to be close to the prior estimates, c=3.46, and present bias is estimated to be β=0.86.\nUnder exponential discounting, predicted and actual conditional filing probabilities roughly correspond early in the filing season, but diverge as the tax deadline approaches where the exponential model retains a virtually constant predicted conditional filing probability and then ramps up.\nTable 3 makes this point quantitatively, with statistical tests of the predictive accuracy of both models in the penultimate week of the tax season... On average, from T−7 to T−1, observed conditional filing probabilities exceed predicted probabilities by 16 percentage points.\n\nThis section analyzes tax filing behavior using structural models of dynamic discrete choice, comparing exponential and quasi-hyperbolic discounting frameworks. Key empirical patterns include time-varying conditional filing probabilities, refund value sensitivity, and model fit comparisons."
  },
  {
    "qid": "econ-empirical-1641-5-0-3",
    "question": "4) Using Proposition 3, prove that the profile $\\phi$ defining equivalence between $R$ and $T$ is unique and satisfies $\\phi_{i}(s_{i}^{\\simeq_{R_{-i}}})=s_{i}^{\\simeq_{T_{-i}}}$ for $s_{i}\\in R_{i}\\cap T_{i}$.",
    "gold_answer": "1. Assume two profiles $\\phi$ and $\\psi$ both establish equivalence between $R$ and $T$.\n2. For arbitrary $i$ and $s_{i}\\in R_{i}$, show $\\phi_{i}(s_{i}^{\\simeq_{R_{-i}}})=\\psi_{i}(s_{i}^{\\simeq_{R_{-i}}})$ via $H(a_{i},t_{-i})=H(b_{i},t_{-i})$ (from Fact 1).\n3. For $s_{i}\\in R_{i}\\cap T_{i}$, prove $s_{i}\\simeq_{T_{-i}} t_{i}$ where $t_{i}\\in\\phi_{i}(s_{i}^{\\simeq_{R_{-i}}})$.",
    "question_context": "Let $R$ and $T$ be two resilient solutions. According to Lemma 4, we have $S\\longrightarrow R$ and $S\\longrightarrow T$. By applying Lemma 3 repeatedly, starting from S, there exists a set of strategy profiles $W$ such that $R \\longrightarrow^{*} W$ and $T\\longrightarrow^{*} W$. Since $R$ is strict-elimination-free, we have $R\\stackrel{{\\textstyle\\simeq}}{\\longrightarrow}R^{1}$. Then Lemmas 5 and 6 imply that $R^{1}$ is also strict-elimination-free. By continued usage of Lemmas 5 and 6, we have $R\\stackrel{{}\\simeq}{\\longrightarrow}W$ and $T\\xrightarrow{\\simeq}W$.\nProposition 3 guarantees that the required profile $\\phi$ in the definition of equivalence between sets of strategy profiles $R$ and $T$, if it exists, is unique, and that $\\phi$ maps each strategy that appears in both $R$ and $T$ to itself.\nProposition 4 guarantees that the union of two equivalent sets of strategy profiles is still equivalent to each one of them, with the desired profile of functions being naturally defined.\nDefinition 10. We denote by $\\mathbb{S R}$ the set of strategy profiles such that, for all strategies $s_{i}$ of a player $i$, $s_{i}\\in\\mathbb{S}\\mathbb{R}_{i}$ if and only if there exists a resilient solution $R$ such that $s_{i}\\in R_{i}$.\n\nThe proof establishes the equivalence of resilient solutions through a series of lemmas and propositions, leveraging properties of strategy profiles and elimination orders."
  },
  {
    "qid": "econ-empirical-382-3-0-0",
    "question": "1) Derive the recursive forecasting framework used to compare LT-VAR and NT-VAR models, specifying how the posterior ${\\bf y}_{1:t}$ is updated each quarter and how out-of-sample predictive distributions are simulated.",
    "gold_answer": "1. **Data Initialization**: Start with data from 1963 to quarter $t$.  \n2. **MCMC Refitting**: For each model (LT-VAR, NT-VAR), refit the MCMC analysis to define the posterior $p(\\theta|{\\bf y}_{1:t})$, where $\\theta$ represents model parameters.  \n3. **Predictive Simulation**: Simulate ${\\bf y}_{t+1:t+4}$ from the predictive distribution $p({\\bf y}_{t+1:t+4}|{\\bf y}_{1:t}) = \\int p({\\bf y}_{t+1:t+4}|\\theta)p(\\theta|{\\bf y}_{1:t})d\\theta$.  \n4. **Forecast Errors**: Compute errors ${\\bf y}_{t+h} - \\hat{{\\bf y}}_{t+h}$ for $h=1:4$.  \n5. **Recursive Update**: Advance to $t+1$, repeat steps 2–4 until $t=2010/Q4$.",
    "question_context": "We summarize out-of-sample forecast performance to compare the LT- and NT-VAR models in predicting one to four quarters ahead. We do this in the traditional recursive forecasting format that mirrors the reality facing forecasters: given data from the start of $1963~\\mathrm{up}$ to any quarter $t$ , we refit the full MCMC analysis of each model to define the posterior based on ${\\bf y}_{1:t}$ . We then simulate the resulting out-of-sample predictive distributions over the following four quarters, times $t+1:t+4$ , to generate realized out-of-sample forecast errors.\nResults are summarized in terms of RMSFEs for each variable and each horizon, averaged over the 16 years of quarterly forecasting; see Table 2. With the exception of the one-step forecasts of inflation $(p)$ , the LT-VAR model dominates NT-VAR across all variables and horizons, with larger improvements at longer horizons.\n\nThis section evaluates the forecasting performance of LT-VAR and NT-VAR models over multiple horizons, using recursive forecasting to simulate real-world forecasting scenarios. The analysis includes RMSFE comparisons and explores model robustness across different economic periods."
  },
  {
    "qid": "econ-empirical-394-4-0-3",
    "question": "4) Calculate the cost per ton of carbon dioxide emissions avoided under the Cash-for-Clunkers program, assuming a 12-year vehicle life and a $4210 average subsidy. Use the results from Table 10.",
    "gold_answer": "1. **Annual Savings**: 13.6 gallons/year.  \n2. **Lifetime Savings**: 13.6 × 12 = 163.1 gallons.  \n3. **CO2 Avoided**: 163.1 gallons × 8.89 kg CO2/gallon = 1.45 metric tons.  \n4. **Cost per Ton**:  \n   - Gross cost = $4210 / 1.45 = $2904.  \n   - Welfare cost (MCF = 0.3) = $2904 × 0.3 = $871.",
    "question_context": "Our first thought experiment investigates the net effect on gasoline consumption of decreasing the fuel intensity of a household’s initial ('kept') vehicle through a limited-duration program causing a one-time reduction in the fuel intensity of vehicles purchased, analogous to the Cash for Clunkers program.\nOur second thought experiment focuses on the welfare costs of CAFE standards. It is a sustained change in the average fuel economy required under CAFE of 1 MPG. Therefore, the fuel economy of both the initial (kept) and follow-on (bought) vehicles are forced to increase by 1 MPG.\nThe results on usage shifting are consistent with intuition. Increasing the cost per mile of a given vehicle in the household reduces the number of miles that particular vehicle is driven, but increases the mileage of the other vehicle.\nThe thought experiment we consider here is a policy analogous to Cash-for-Clunkers, where some incentive nudges households into purchasing a less fuel intense vehicle, with the corresponding change in attributes, holding technology fixed.\n\nThis section quantifies the strength of forces uncovered in Section 4 through two thought experiments reflecting common policies to shift drivers into more fuel-efficient cars. The first measures the net effect of an increase in the fuel economy of the kept vehicle, analogous to a 'Cash for Clunkers' program. The second uses empirical estimates on the welfare costs of Corporate Average Fuel Economy (CAFE) standards to measure added welfare costs from attribute substitution."
  },
  {
    "qid": "econ-empirical-1464-0-1-2",
    "question": "7) Show how the random Strotz representation $V_{RS}(x) = \\int_{T} \\max_{\\beta \\in B_{w(t)}(x)} u(\\beta) \\mu(dt)$ generalizes the self-control representation, and discuss its limitations.",
    "gold_answer": "1. **Generalization**: For $w(t) = \\nu + t u$ and $\\mu$ uniform over $[0,1]$, $V_{RS}(x) = V_{SC}(x)$.  \n2. **Limitations**: The random Strotz representation lacks behavioral interpretability, as the \"selves\" $w(t)$ are not independent (unlike Strotz's original framework).  \n3. **Relevance**: States $w(t)$ are not relevant in the sense that small perturbations do not affect menu valuations, making the model less intuitive.",
    "question_context": "GP (2001) pioneered the axiomatic treatment of temptation and self-control. They view self-control as the agent making a choice from a set of alternatives, and the choice reflects the agent's compromise between choosing what he would ideally have chosen (according to his normative preference) and the psychological cost (of self-control) that the agent faces when he doesn't choose the most tempting alternative. To this end, they offer two representations, the first of which is a self-control representation, which consists of a pair of expected utility functionals $(u,\\nu)$ (where $u,\\nu:\\Delta\\to\\mathbb{R}$ ) such that the utility of a menu is given by \n\n$$\n\\begin{array}{c}{{V_{S C}(x)=\\displaystyle\\operatorname*{max}_{\\beta\\in x}\\left(u(\\beta)+\\nu(\\beta)\\right)-\\operatorname*{max}_{\\beta\\in x}\\nu(\\beta)}}\\ {{=\\displaystyle\\operatorname*{max}_{\\beta\\in x}\\left(u(\\beta)-c(\\beta;x)\\right),}}\\end{array}\n$$ \n\nwhere $c(\\beta;x):=(\\operatorname*{max}_{\\alpha\\in x}\\nu(\\alpha))-\\nu(\\beta)$ . The second representation suggests that the agent expects to choose the item $\\beta$ from the menu $x$ that maximizes $u(\\beta)-c(\\beta;x),$ where $c(\\beta;x)$ is the cost of resisting temptation.\nGP (2001) have another representation they term the overwhelming temptation representation, where the utility of a menu is given by \n\n$$\nV_{O T}(x)=\\operatorname*{max}_{\\beta\\in B_{\\nu}(x)}u(\\beta),\n$$ \n\nwhere $B_{\\nu}(x)=\\arg\\operatorname*{max}_{\\gamma\\in x}\\nu(\\gamma)$ Temptation is overwhelming because the choice is always made according to the temptation utility $\\nu$ with ties being broken in favor of the normative utility function $u$ Clearly, an overwhelming temptation representation can be thought of as a dual self representation $(u,\\nu,\\rho)$ with $\\rho=1$ . Thus, the dual self representation presented in this paper can be viewed as the natural generalization of the overwhelming temptation representation of GP.\nSubsequent to Chatterjee and Krishna (2005), Dekel and Lipman (2007) have addressed the possibility of one of multiple selves making the choice in the second period in a different way. They introduce another generalization of self-control and the overwhelming temptation representations of GP, which they call the random Strotz representation. It consists of a collection of expected utility functions $(u,(w(t))_{t\\in T})$ where $T$ is a (Borel measurable) index set, and a measure $\\mu$ on $T,$ so that the utility of a menu is given by \n\n$$\nV_{R S}(x)=\\int_{T}\\quad\\operatorname*{max}_{\\beta\\in\\mathrm{B}_{w(t)}(x)}u(\\beta)\\mu(d t).\n$$ \n\nDL show that for any self control representation $(u,\\nu)$ and the corresponding utility function $V_{S C}.$ there exists a random Strotz representation $V_{R S}$ such that $V_{S C}$ $=V_{R S}$ . They achieve this by letting $w(t):=\\nu+t u$ ， $t\\in[0,1]=:T,$ and taking $\\mu$ to be the uniform measure over $[0,1]$\n\nThis section explores extensions of the dual self model, including the self-control representation, overwhelming temptation representation, and random Strotz representation, as well as their behavioral interpretations."
  },
  {
    "qid": "econ-empirical-1464-4-0-2",
    "question": "3) Prove Claim V12: Show that R is complete and transitive using the representation by νₓ on Fₓ.",
    "gold_answer": "1. For any α,β ∈ Δ, choose x ∈ A₀ containing both with dim(Fₓ) ≥ 2.\\n2. Completeness: νₓ(α) ≥ νₓ(β) ⇒ αRβ, else βRα.\\n3. Transitivity: If αRβ and βRγ, then νₓ(α) ≥ νₓ(β) ≥ νₓ(γ) ⇒ νₓ(α) ≥ νₓ(γ) ⇒ αRγ.\\n4. Violation would contradict the linear order imposed by νₓ.",
    "question_context": "Let us define β₊≔{α:{β}≻{β,α}≽{α}}. From Regularity, it follows that β₊ is convex. Let us also define β₋≔cl({α:{β,α}∼{β}≻{α}}).\nLEMMA V9: Suppose ≽ satisfies Axioms 1, 3 and 6. Then, β₋ is convex.\nLEMMA V10: For any x ∈ A₀, let β* ∈ ri Fₓ. Then there exists νₓ:Fₓ→ℝ which is continuous and linear so that β₋* ∩ Fₓ ⊂ [νₓ ≤ νₓ(β*)] and β₊* ∩ Fₓ ⊂ [νₓ > νₓ(β*)].\nCLAIM V12: R is complete and transitive.\nLEMMA V14: For all finite x, max_{β∈x} u(β) ≥ U(x) ≥ max_{β∈B_ν(x)} u(β).\n\nThis section constructs the alter ego's preferences via revealed preference arguments, proving Lemma V5. It defines sets β₊ and β₋, establishes their convexity, and develops a linear functional νₓ to represent preferences on finite-dimensional subsets of Δ."
  },
  {
    "qid": "econ-empirical-1016-3-2-0",
    "question": "7) Derive the welfare function $V(m,n,t_{h},t_{f})$ and explain each component.",
    "gold_answer": "1. $CS$: Consumer surplus from domestic sales. \\n2. $H$: Domestic firm profits. \\n3. $E$: Export profits of domestic firms. \\n4. $-m f$: Fixed costs of domestic firms. \\n5. $t_{h}n y$: Tariff revenue from foreign firms.",
    "question_context": "Welfare function with tariffs: $V(m,n,t_{h},t_{f}) \\equiv CS(m,n,t_{h}) + H(m,n,t_{h}) + E(m,n,t_{f}) - m f + t_{h}n y(m,n,t_{h})$.\nEquilibrium industrial concentration with tariffs: $\\frac{(m+2)^2(2m+1)^3}{8m^2+12m+5}=h$. Propositions 5 and 6 show $m^{s} < m^{d} < m^{a} < m^{g}$ and $L^{g} < L^{d} < L^{s}$.\n\nThis section examines the interaction between tariffs and merger policies, showing how trade liberalization affects industrial concentration and market power."
  },
  {
    "qid": "econ-empirical-530-5-0-3",
    "question": "4) Prove that increasing supply-side heterogeneity raises viewership under the preferential regime. Use the mean-preserving spread of quality and the allocation efficiency of transcoders.",
    "gold_answer": "1. **Mean-Preserving Spread**: Double variance of quality $q_j$ with $E[q_j] = \\text{const}$.\n2. **Allocation Efficiency**: Preferential regime selects from fatter tails, so $\\max q_j$ increases.\n3. **Viewership**: $n_j^V = f(q_j, \\sigma_j) \\implies \\Delta n_j^V > 0$ for higher $q_j$.",
    "question_context": "In the neutral regime, transcoders are allocated randomly through the population. In the preferential regime, channels are ranked by quality, and transcoders are given to the highest-ranked channels. In the rent-extractive regime, the platform charges channels just enough to offset the benefits of transcoding.\nFor a given regime, label as $c$ the counterfactual of guaranteeing enough transcoders for $c\\%$ of the population. The platform could have resources to guarantee transcoders to $c=1\\%$ of the population. I consider $c\\in\\{0,1,\\ldots,100\\}$.\nThe algorithm to find the equilibrium outcome involves allocating transcoders to $c\\%$ of a random sample of channels, simulating online/offline status, and iterating until convergence.\nNeutrality yields a lower consumer surplus by construction because transcoders are scarce. The preferential regime allocates transcoders to higher-quality channels, offsetting congestion externalities.\nCongestion externalities magnify network effects. Without congestion, participation would be significantly higher, but supply would decrease slightly.\nPigouvian taxation on the demand side involves taxing viewers to offset congestion. Taxing consumers reduces viewership but does not alleviate congestion from unconsumed traffic.\nA supply-side Pigouvian tax on popular channels has little effect on participation at low infrastructure levels but increases tax revenues for the platform.\nIncreasing supply-side heterogeneity increases viewership under both neutral and preferential regimes, with the preferential regime benefiting from fatter tails in the quality distribution.\n\nThis section explores counterfactual allocations of transcoders and differential pricing under three regimes: neutral, preferential, and preferential with a rent-extractive platform. The analysis includes quantifying gains from efficient transcoder use, congestion externalities, Pigouvian taxation, and supply heterogeneity."
  },
  {
    "qid": "econ-empirical-1326-4-0-0",
    "question": "1) Derive the union wage effect differential between an industry with 80% organized and one with 20% organized, given the estimated coefficient on percentage organized is 0.153 for union members and 0.004 for nonmembers.",
    "gold_answer": "1. The union wage effect differential is calculated as: \n   \\(\\Delta W = \\beta_{union} \\times (P_{80} - P_{20}) - \\beta_{nonunion} \\times (P_{80} - P_{20})\\)\n2. Substituting the values: \n   \\(\\Delta W = 0.153 \\times (0.80 - 0.20) - 0.004 \\times (0.80 - 0.20)\\)\n3. Simplifying: \n   \\(\\Delta W = (0.153 - 0.004) \\times 0.60 = 0.149 \\times 0.60 = 0.0894\\)\n4. The union wage effect is approximately 8.94 percentage points higher in the 80% organized industry compared to the 20% organized industry.",
    "question_context": "The principal result demonstrated in table 1 is that percentage organized has a sizable positive effect on union but not on nonunion wages, which implies an increasing wage differential with coverage. With the sample of union members, the estimated coefficient on organization is 0.153 compared to 0.004 with the sample of nonmembers. This implies, for example, that the union wage effect in a manufacturing industry with 80% organized is likely to be about 9 percentage points higher than in an industry with only 20% organized.\nTable 1 also demonstrates that firm size is positively and significantly associated with wages in both union and nonunion settings; the estimated firm size coefficient is smaller under unionism. The injury and illness rate has roughly the same statistically significant positive effect on the earnings of union members and nonmembers. The concentration ratio does not appear to have a systematic impact on the wages of either union or nonunion manufacturing production workers.\nIn the case of hourly compensation, the estimated coefficient of proportion unionized is 0.157 for covered establishments and 0.047 for ones that are not covered. These estimates imply, for example, that the union compensation effect for manufacturing nonoffice employees is about 6 percentage points larger in an industry with 80% organized than in one with only 20% organized.\nFirm size, measured as ln(non-office hours worked in the establishment), has a positive effect on wages (and, similarly, total compensation) with both the union and nonunion samples. A variable equal to the ratio of hours worked by nonoffice employees to hours worked by all employees in the establishment assumes a negative and very significant estimated coefficient in both the samples.\n\nThe text presents empirical results from the 1973-75 May CPS and 1968, 1970, and 1972 EEC establishment-level data, analyzing the impact of percentage organized (covered by collective bargaining) on wages and compensation for union and nonunion workers in manufacturing industries."
  },
  {
    "qid": "econ-empirical-1207-1-0-3",
    "question": "4) The text reports that 66% of female physicians have a child compared to 79% of male physicians. Using a logistic regression framework, propose a model to estimate the effect of having children on earnings for female physicians, controlling for specialty, hours worked, and practice setting. Define the model mathematically and interpret the coefficient of the 'has children' variable.",
    "gold_answer": "Let \\( Y \\) be the log of earnings, \\( C \\) be a binary variable for having children, \\( X \\) be a vector of controls (specialty, hours, practice setting), and \\( \\epsilon \\) be the error term. The logistic regression model is:\n\\[ Y = \\beta_0 + \\beta_1 C + \\beta_2 X + \\epsilon \\]\nThe coefficient \\( \\beta_1 \\) represents the average difference in log earnings between female physicians with and without children, holding other factors constant. If \\( \\beta_1 < 0 \\), it suggests that having children is associated with lower earnings for women. The marginal effect can be interpreted as a percentage change in earnings: \\( (e^{\\beta_1} - 1) \\times 100 \\% \\).",
    "question_context": "Decompositions of the gender gap for physicians show that less than half of the gains over this period can be attributed to improvements in the observable labor market skills and professional characteristics of female physicians such as board certification, specialty field, and practice setting. The remainder is typically attributed to improvements in unmeasured labor market skills and/or reductions in discrimination against women.\nTo assess the impact of family responsibilities on the relative earnings of female physicians, I use both cross-sectional and longitudinal data from the YPS, conducted by the Robert Wood Johnson Foundation, Mathematica Policy Research, and the AMA. The YPS is a nationally representative survey that was designed to investigate the factors influencing the careers of young physicians, covering a wide range of topics including specialty field, practice setting, hours, and income, as well as marital status and number of children.\nIn order to examine how changes in family status over time influence earnings for men and women physicians, I focus on the longitudinal aspect of the YPS. To facilitate the comparison of the cross-sectional and fixed-effects estimates I restrict the analysis to the panel sample of observations from 1986 and 1990 for those individuals who were interviewed in both years.\nWomen physicians earn about a third less than men on an annual basis and about 15 percent less in terms of hourly earnings. The gender gap in earnings is larger for women physicians who were married or had children compared with their single and childless colleagues.\nLabor market characteristics such as hours worked, specialty field, and practice setting continue to be important factors in explaining the remaining gap in earnings between men and women physicians. The YPS shows that women physicians work on average one week less per year and 9 hours less per week than their male counterparts.\n\nThe literature shows that the gains in relative earnings of female physicians over the last two decades, similar to the gains in relative earnings of all women, have been remarkable. Various studies report that the ratio of female to male annual earnings among physicians increased from 0.58 in 1972 to 0.72 in 1990 while the hourly earnings ratio improved from 0.71 to 0.88 over the same period. Although the percentage of physicians who are female has risen from 8 percent in 1970 to 22 percent in 1999, considerable gender differences in labor market characteristics remain."
  },
  {
    "qid": "econ-empirical-1355-1-0-0",
    "question": "1) Define the concept of stability in two-sided matching markets and explain why it is a desirable property in matching mechanisms.",
    "gold_answer": "1. **Stability Definition**: A matching is stable if there are no blocking pairs—i.e., no hospital-intern pair who prefer each other over their current matches.  \n2. **Desirability**: Stability ensures no unilateral deviations are beneficial, aligning with Pareto efficiency and fairness in outcomes.",
    "question_context": "We study manipulation of solutions by hospitals via underreporting their capacities in the context of centralized two-sided matching markets. We show that the solution that is used to match medical interns and hospitals in United States is manipulable in this way. Our main result is that there is no solution that is stable and non-manipulable via capacities.\n\nThe paper examines manipulation by hospitals through underreporting capacities in centralized two-sided matching markets, specifically in the context of medical intern-hospital matching in the United States. It establishes that no stable solution exists that is immune to such manipulation."
  },
  {
    "qid": "econ-empirical-634-5-0-2",
    "question": "3) Formally derive the marginal capital/output ratio from the production function \\(Y = AK^\\alpha L^{1-\\alpha}\\), and discuss its estimation challenges in the Irish context.",
    "gold_answer": "1. Marginal capital/output ratio: \\(\\frac{dY}{dK} = \\alpha A K^{\\alpha-1} L^{1-\\alpha}\\).\n2. Estimation requires sector-specific \\(\\alpha\\) (capital elasticity) and \\(A\\) (TFP).\n3. Data limitations in Ireland: \\(A\\) is unobserved; \\(L\\) (employment) may not capture labor quality.\n4. The study uses tentative estimates due to incomplete disaggregation of \\(K\\) and \\(Y\\) by sector.",
    "question_context": "Fixed assets are valued in both gross (undepreciated but allowing for scrappings) and net (after linear depreciation) terms. The author uses the perpetual inventory method, and in some cases draws on UK estimates of the average life of fixed assets.\nGross capital stock estimates are matched with GDP and employment data. Variation of ratios between sectors leads the author to emphasise that any capital/output ratio estimates need sectoral breakdown.\n\nThis study provides the first set of measures of the total capital stock in the Republic of Ireland, focusing on tangible assets excluding financial assets. The methodology includes the perpetual inventory method and uses 1980 prices for valuation."
  },
  {
    "qid": "econ-empirical-1314-4-0-0",
    "question": "1) Derive the conditions under which Saudi Arabia's oil output increases by $6\\%$ in response to a $3\\%$ drop in fringe output, assuming a transitory negative oil supply shock with a half-life of one year.",
    "gold_answer": "1. Start with the shock specification: \\( \\Delta \\log Y_f = -0.03 \\) (fringe output drops by $3\\%$). \\n2. The dominant producer's response is modeled as \\( \\Delta \\log Y_d = \\alpha \\Delta \\log Y_f \\), where \\( \\alpha \\) is the elasticity of response. \\n3. Given \\( \\Delta \\log Y_d = 0.06 \\), solve for \\( \\alpha = \\frac{0.06}{-0.03} = -2 \\). \\n4. The half-life condition implies the shock decays as \\( Y_f(t) = Y_f(0)e^{-\\lambda t} \\), with \\( \\lambda = \\ln(2) \\) for a one-year half-life.",
    "question_context": "The key point is that the model is successful at reproducing the observed volatility of the growth rate of Saudi Arabia’s production computed in Table 2 even if Saudi Arabia is not affected by any domestic shock to its production.\nThe dashed line shows the responses to a one-standard deviation drop in the productivity of the fringe. This is an example of a transitory ‘negative oil supply shock’ with a half-life of one year. As a result, the oil output of the fringe falls by $3\\%$ on impact, while the output of the dominant oil producer increases by as much as $6\\%$ , raising its market share by around 1 percentage point.\nThe solid lines in Figure 3 show the responses to a type of ‘positive oil demand shock’, namely an unexpected one-standard deviation rise in the growth rate of TFP of the oil-importing region, resulting in a permanent rise of the level of TFP.\n\nThe text discusses the model's ability to explain the volatility in Saudi Arabia's oil output, focusing on responses to supply and demand shocks."
  },
  {
    "qid": "econ-empirical-68-2-0-1",
    "question": "2) Prove Lemma 2 (Controllability of Best Responses with One-Period Price-Setting). Show that for any history $(h_{t-1},\\hat{x}_{t})$, there exists a money growth rate $\\mu_{t}$ such that $\\hat{x}_{t}\\neq E[\\pi_{t}(\\hat{h}_{y t})+\\gamma y_{t}(\\hat{h}_{y t})]$.",
    "gold_answer": "1. **Substitute Equations**: From the text, under the money regime, output and inflation are given by: \n   $$ y_{t}=\\frac{\\mu_{t}+\\nu_{t}+y_{t-1}-\\hat{x}_{t}}{1+\\kappa}, \\quad \\pi_{t}=\\kappa y_{t}+\\hat{x}_{t}. $$\n2. **Compute Expectation**: \n   $$ E[\\pi_{t}(\\hat{h}_{y t})+\\gamma y_{t}(\\hat{h}_{y t})] = \\frac{\\kappa+\\gamma}{1+\\kappa}(\\mu_{t}+y_{t-1}-\\hat{x}_{t}) + \\hat{x}_{t}. $$\n3. **Choose $\\mu_{t}$**: For $\\hat{x}_{t} \\neq E[\\pi_{t}+\\gamma y_{t}]$, select $\\mu_{t} \\neq \\hat{x}_{t}-y_{t-1}$. This ensures the inequality holds.",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nThe public events that occur in a period are, in chronological order, $q_{t}=(x_{t};\\delta_{t};s_{t};y_{t},\\pi_{t})$.\nA sophisticated equilibrium given the policies here is a collection of strategies $(\\sigma_{x},\\sigma_{g})$ and allocation rules $(\\sigma_{y},\\sigma_{\\pi})$ such that (i) given any history $h_{t-1}$, the continuation outcomes $\\{a_{r}(s^{r}\\mid h_{t-1};\\sigma)\\}$ induced by $\\sigma$ constitute a continuation competitive equilibrium and (ii) given any history $h_{y t}$, so do the continuation outcomes $\\{a_{r}(s^{r}\\mid h_{y t};\\sigma)\\}$.\nThe basic idea behind our sophisticated policy construction is that the central bank starts by picking any desired competitive equilibrium allocations and sets its policy on the equilibrium path consistent with them.\nLEMMA 2 (Controllability of Best Responses with One-Period Price-Setting). For any history $(h_{t-1},\\hat{x}_{t})$, if the central bank chooses the money regime, then there exists a choice for money growth $\\mu_{t}$ such that $\\hat{x}_{t}\\neq E[\\pi_{t}(\\hat{h}_{y t})+\\gamma y_{t}(\\hat{h}_{y t})]$, where $h_{y t}=(h_{t-1},\\hat{x}_{t},M,\\mu_{t})$.\nPROPOSITION 1 (Unique Implementation with Money Reversion). Any competitive equilibrium outcome in which the central bank uses interest rates as its instrument can be implemented as a unique equilibrium with sophisticated policies with one-period reversion to a money regime.\nPROPOSITION 2 (Indeterminacy of Equilibrium under the King Rule). Suppose the central bank sets interest rates $i_{t}$ according to the simple economy’s King rule (19). Then any of the continuum of sequences indexed by the initial condition $x_{0}$ and the parameter $c$ that satisfies $x_{t+1}=i_{t}+c\\eta_{t},~\\pi_{t}=x_{t}+\\kappa(1+\\psi c)\\eta_{t},$ $\\mathrm{and}~y_{t}=(1+\\psi c)\\eta_{t}$ is a sophisticated outcome.\nPROPOSITION 3 (Unique Implementation with a Hybrid Rule). In the simple economy, the King–money hybrid rule with $\\phi>1$ uniquely implements any bounded competitive equilibrium.\n\nThe text introduces the concept of sophisticated equilibrium, where allocations, prices, and policies are functions of the history of exogenous events, aggregate private actions, and central bank policies. It contrasts this with unsophisticated equilibrium and discusses implementation strategies, including reversion to money regimes and hybrid rules."
  },
  {
    "qid": "econ-empirical-133-2-0-3",
    "question": "4) Critically evaluate the Proportional Hazard (PH) and Mixed Proportional Hazard (MPH) models in the context of structural job search models. What are their limitations?",
    "gold_answer": "1. **PH/MPH Specification**: \n   $$h=h_{\\tau}(\\tau)\\cdot\\exp(x^{\\prime}\\beta)\\cdot h_{\\mathrm{unobserved}}.$$\n2. **Limitations**:\n   - **Non-Structural**: The PH/MPH model parameters ($\\beta$) are not structural; they lack a causal interpretation.\n   - **Nonlinearity Ignored**: The job search model implies $h(\\tau)$ depends nonlinearly on $w^{*}(\\tau)$, which is not captured by proportionality.\n   - **Identification Issues**: Reduced-form estimates may conflate heterogeneity with true causal effects.\n3. **Policy Analysis**: PH/MPH models are poorly suited for counterfactual policy analysis due to their ad hoc nature.",
    "question_context": "The above model is stationary because the parameters do not vary over time, and, as a result, the individual hazard rates $(h)$ are constant over time. However, the pool (or aggregate) hazard rate may be duration dependent due to heterogeneity of the hazard rate among individuals.\nSuppose that the parameters are allowed to vary over the interval of time, $\\tau=[0,T],$ in a deterministic way, and job searchers have perfect foresight in the sense that they correctly anticipate changes of these parameters (van den Berg, 1990).\n$$\\rho V^{\\mathrm{u}}(\\tau)=\\frac{\\mathrm{d}V^{\\mathrm{u}}(\\tau)}{\\mathrm{d}\\tau}+b(\\tau)+\\lambda(\\tau)\\cdot\\mathrm{E}_{w\\mid\\tau}\\operatorname*{max}\\Biggl\\{0,\\frac{w}{\\rho}-V^{\\mathrm{u}}(\\tau)\\Biggr\\}.$$\n$$h(\\tau)=\\lambda(\\tau)(1-F(w^{*}(\\tau),\\tau)).$$\nA special case of nonstationarity is a finite horizon search model. Assuming that there is no income after period $T$ ; the reservation wage at $T$ equals $w^{*}(T)=b$ , since the option value of search is zero at $T$ .\nThe likelihood contribution of a realized unemployment duration $\\tau_{i}$ equals $h(\\tau_{i})\\exp(-\\textstyle\\int_{0}^{\\tau_{i}}h(\\tau)\\mathrm{d}\\tau)$ . Note that the likelihood function now includes the restrictions $w_{i}\\geqslant w^{*}(\\tau_{i})$ .\n$$h=h_{\\tau}(\\tau)\\cdot\\exp(x^{\\prime}\\beta)\\cdot h_{\\mathrm{unobserved}}.$$\n\nThe text discusses nonstationary job search models where parameters vary over time, leading to duration-dependent hazard rates. It contrasts stationary models with nonstationary ones, highlighting the impact of changing parameters like unemployment benefits or stigma effects on wage offers. The text also introduces the Proportional Hazard (PH) and Mixed Proportional Hazard (MPH) models, critiquing their alignment with structural job search models."
  },
  {
    "qid": "econ-empirical-1401-1-1-1",
    "question": "2) Compare the incentive compatibility constraints in the coalition-free (CF) and coalition-proof (EC) problems. How does side contracting alter the agents' effort choices?",
    "gold_answer": "1. **CF Problem**: IC constraints are separate for each agent:\n   $$e_n \\in \\arg\\max_e \\sum_{i,j} P_{ij}(e, e_{-n}) v_{ij}^n - G_n(e).$$\n   Agents choose efforts independently in a Nash equilibrium.\n\n2. **EC Problem**: IC constraint is joint:\n   $$(e_1, e_2) \\in \\arg\\max_{e_1', e_2'} \\sum_{i,j} P_{ij}(e_1', e_2')(v_{ij}^1 + v_{ij}^2) - G_1(e_1') - G_2(e_2').$$\n   Agents coordinate efforts to maximize their combined utility, reflecting side contracting.",
    "question_context": "Problem (EC) for a given effort $(\\boldsymbol{e}_{1},\\boldsymbol{e}_{2})$ is defined as follows:\n\n$$\\operatorname*{min}_{v^{1},v^{2}}\\sum_{i}\\sum_{j}P_{i j}(e_{1},e_{2})(\\phi_{1}(v_{i j}^{1})+\\phi_{2}(v_{i j}^{2}))$$\n\nsubject to\n\n$$(e_{1},e_{2})\\in\\arg\\operatorname*{max}_{e_{1}^{\\prime},e_{2}^{\\prime}}\\sum_{i}\\sum_{j}P_{i j}(e_{1}^{\\prime},e_{2}^{\\prime})(v_{i j}^{1}+v_{i j}^{2})-G_{1}(e_{1}^{\\prime})-G_{2}(e_{2}^{\\prime})$$\n\nand\n\n$$\\sum_{i}\\sum_{j}P_{i j}(e_{1},e_{2})(v_{i j}^{1}+v_{i j}^{2})-G_{1}(e_{1})-G_{2}(e_{2})\\geqslant\\bar{U}^{1}+\\bar{U}^{2}.$$\n\nAdditionally, the coalition-proofness constraint is:\n\n$$\\frac{1}{\\phi_{1}^{\\prime}(v_{i j}^{1})}=\\frac{1}{\\phi_{2}^{\\prime}(v_{i j}^{2})}\\qquad\\mathrm{for~all~}(i,j).$$\n\nThe coalition-proof solution introduces side contracting between agents, contingent on publicly observable outcomes. The principal designs contracts to prevent side trading for mutual insurance, while allowing effort coordination."
  },
  {
    "qid": "econ-empirical-1611-0-0-1",
    "question": "2) Compare the finite and large sample properties of the Unrestricted Least Squares (ULS) estimator $\\hat{P}=(Y^{\\prime}Z)(Z^{\\prime}Z)^{-1}$ with the Derived Reduced-Form (DRF) estimators based on OLS, 2SLS, 3SLS, and FIML.",
    "gold_answer": "1. **ULS Properties**:\n   - Unbiased and consistent under proper specification of $X$.\n   - Asymptotically less efficient than full-information DRF estimators (3SLS, FIML).\n   - Robust to specification errors in individual equations.\n2. **DRF Properties**:\n   - OLS-based DRF: Similar to ULS but may lack efficiency.\n   - 2SLS and 3SLS-based DRF: Do not possess finite moments (mean or variance).\n   - FIML-based DRF: Has finite sample moments and is less prone to outliers in forecasting.",
    "question_context": "Numerical evidence is reported here in order to shed light on the characteristics and performance of several non-traditional reduced-form estimators. Some empirical and Monte Carlo simulation examples compare these techniques with such traditional methods as the OLS, 2SLS, 3SLS and FIML.\nThe new estimators investigated here are the Partially Restricted ReducedForm (PRRF), the Modified Stein-like Reduced-Form (MSRF) and the Generic Reduced-Form (GRF) estimators.\nLet $X{=}\\left[Y~Z\\right]$ represent $T$ observations on $n$ endogenous $(Y)$ and m exogenous $(Z)$ variables. Consider the following relationships: $${\\cal Y}^{\\prime}=P{\\cal Z}^{\\prime}+{\\cal V}^{\\prime},$$ and $$B Y^{\\prime}+I Z^{\\prime}=U^{\\prime}=A X^{\\prime},~\\mathrm{say},$$ where $\\pmb{A}=(\\pmb{B}\\emph{I})$ being $n\\times(n+m)$ and $P$ being $n\\times m$ represent the unknown coefficients.\n\nThe paper discusses the performance and characteristics of non-traditional reduced-form estimators such as PRRF, MSRF, and GRF, comparing them with traditional methods like OLS, 2SLS, 3SLS, and FIML. It highlights the importance of robust statistical methods that do not entirely depend on the accuracy of non-sample information."
  },
  {
    "qid": "econ-empirical-1600-1-0-1",
    "question": "2) Interpret the coefficients of the exogenous variables $X_{i t}$ in the Tobit model, particularly the terms of trade (TOTB), real interest rate (RR), and debt burden (LTDPC, STDPC).",
    "gold_answer": "1. **TOTB**: A negative coefficient indicates that an improvement in the terms of trade reduces participation in IMF programs, as countries face fewer external imbalances.\n2. **RR**: A higher real interest rate reduces participation, possibly due to credit rationing in IMF programs.\n3. **LTDPC/STDPC**: Positive coefficients suggest that higher debt burdens increase participation, as countries seek IMF assistance to manage debt.",
    "question_context": "The participation variable in a Tobit formulation is represented as \n$$\n\\begin{array}{r l}&{F_{i t}^{*}=\\alpha_{F i}+X_{i t}\\gamma_{F}+\\displaystyle\\sum_{j=1}^{k}Y_{i t-j}\\psi_{F j}+\\displaystyle\\sum_{j=0}^{m}Z_{i t-j}\\xi_{F j}+e_{F i t},}\\ &{F_{i t}=\\displaystyle\\operatorname*{min}\\bigl[1,\\operatorname*{max}\\bigl[0,F_{i t}^{*}\\bigr]\\bigr]}\\end{array}\n$$\nThe equilibrium participation variable is partially unobserved, or censored, because countries cannot have negative or more-than-complete participation. The observed $F_{i t}$ will thus be as defined in (4).\nThe second and third columns of Table 1 present statistics from the Tobit estimation of $F_{i t}$ as specified in (3) and (4) for the sample period 1978-1986.\nIMF participation can also be examined through estimation of a Probit relationship governing the binary participation variable $P_{i t}$. The equations governing this analysis are analogous to those for the Tobit, with the errors of Eq. $\\left(3^{\\prime}\\right)$ assumed normally distributed: \n$$\n\\begin{array}{r c l}{{}}&{{}}&{{P_{i t}^{*}=\\alpha_{p i}+X_{i t}\\gamma_{p}+\\displaystyle\\sum_{j=1}^{k}Y_{i t-j}\\psi_{p j}+\\displaystyle\\sum_{j=0}^{m}Z_{i t-j}\\xi_{p j}+e_{p i t},}}\\ {{}}&{{}}&{{}}\\ {{\\displaystyle P_{i t}=\\left[0\\quad\\mathrm{~for}\\quad P_{i t}^{*}<0.5\\right.}}\\ {{}}&{{}}&{{}}\\end{array}\n$$\n\nThe text discusses the modeling of participation in IMF programs using Probit and Tobit procedures, detailing the mathematical formulations and empirical findings."
  },
  {
    "qid": "econ-empirical-566-3-3-0",
    "question": "1) Derive the operator equation $L_2 = \\Lambda_2 \\Delta \\Lambda_2^{-1}$ for the model with two lags. What injectivity conditions are required?",
    "gold_answer": "1. Define $L_2$ as the integral operator mapping $g(v_1)$ to $\\int \\omega(v_3) p(y \\mid v_1, \\dots, v_5, x) g(v_1) dv_1$. \\n2. Express $p(y \\mid \\cdot)$ in terms of $p_5$, $p_4$, and $p_3$. \\n3. Substitute to show $L_2 = \\Lambda_2 \\Delta \\Lambda_1$. \\n4. Injectivity of $\\Lambda_1$ and $\\Lambda_2$ ensures invertibility, yielding $L_2 = \\Lambda_2 \\Delta \\Lambda_2^{-1}$.",
    "question_context": "To demonstrate the possibility of identification with more than one lagged dependent variable I provide here an example where there are two lags and $T=5$ . Suppose that $P r(Y_{t}\\mid Y^{(t-1)},W,F)=P r(Y_{t}\\mid Y_{t-1},Y_{t-2},W,F),$ . Take a function $\\omega:\\mathcal{V}_{2}\\rightarrow\\mathbb{R}$ such that $0<\\omega(v_{3})$ for all $v_{3}\\in\\mathcal{V}_{2}$ and $\\mathbf{sup}_{\\upsilon_{3}\\in\\mathcal{V}_{3}}\\omega(\\upsilon_{3})<\\infty$ and define the operators $\\begin{array}{l}{{\\displaystyle[L_{1}g](v_{3})=\\int_{\\mathcal{V}_{1}}\\omega(v_{3})p(y_{1},y_{2},y_{3}\\mid v_{1},v_{2},v_{3},x_{1},x_{2})g(v_{1})d v_{1}}}\\ {{\\displaystyle[L_{2}g](v_{3})=\\int_{\\mathcal{V}_{1}}\\omega(v_{3})p(y\\mid v_{1},v_{2},v_{3},v_{4},v_{5},x)g(v_{1})d v_{1}}}\\end{array}$\nThen define the operators $\\varLambda_{2}:\\mathcal{L}_{b d d}^{1}(\\mathcal{F})\\to\\mathcal{L}_{b d d}^{1}(\\mathcal{V}_{3})$ such that $[\\varLambda_{2}g](\\boldsymbol{v}_{3})=\\int_{\\mathcal{F}}\\omega(\\boldsymbol{v}_{3})p_{3}(\\boldsymbol{y}_{3}\\mid\\boldsymbol{y}_{2},\\boldsymbol{y}_{1},\\boldsymbol{v}_{3},\\boldsymbol{x}_{3},f)g(f)d f,$ $\\varLambda_{1}:\\mathcal{L}_{b d d}^{1}(\\mathcal{V}_{1})\\to\\mathcal{L}_{b d d}^{1}(\\mathcal{F})$ such that $[\\varLambda_{1}g](f)=\\int_{\\mathcal{V}_{1}}f_{Y_{2},Y_{1},F|V_{1},X}(y_{2},y_{1},f\\mid v_{1},x)g(v_{1})d v_{1},$ and the diagonal operator $\\varDelta:\\mathcal{L}_{b d d}^{1}(\\mathcal{F})\\rightarrow\\mathcal{L}_{b d d}^{1}(\\mathcal{F})$ such that $[\\varDelta g](f)=p_{5}(y_{5}\\mid y_{4},y_{3},v_{5},x_{5},f)p_{4}(y_{4}\\mid y_{3},y_{2},v_{4},x_{4},f)g(f).$\n\nThis section explores identification in models with more than one lagged dependent variable, demonstrating the need for additional time periods."
  },
  {
    "qid": "econ-empirical-1283-3-0-1",
    "question": "2) Show that the median value of $\\partial w_{i}/\\partial v_{i}$ is $\\alpha - 1$ given that the median value of $\\phi_{i}\\partial w_{i}/\\partial y_{i}$ is $\\alpha$ and equation (6) holds.",
    "gold_answer": "1. **Equation (6)**: The text states that equation (6) relates $\\partial w_{i}/\\partial v_{i}$ and $\\partial w_{i}/\\partial y_{i}$ as $\\partial w_{i}/\\partial v_{i} = \\phi_{i}\\partial w_{i}/\\partial y_{i} - 1$.  \n2. **Median Value**: If the median value of $\\phi_{i}\\partial w_{i}/\\partial y_{i}$ is $\\alpha$, then the median value of $\\partial w_{i}/\\partial v_{i}$ is $\\alpha - 1$ by substitution into equation (6).",
    "question_context": "Consider a large charity, and suppose that the public good theory applies and that the philanthropists are in a state of Nash equilibrium. There are $\\pmb{n}$ donors, a donor being someone for whom $w_{i}>0$. Some of the $N$ people who are affected by altruistic externalities may have chosen not to make any contribution at all: they do not count as donors. I shall make two additional and weak assumptions. I shall assume that no one ever gives away the whole of his income : thus for all donors $i$ and for all values of $v_{i},x_{i}>0$. And I shall assume that personal consumption and the charitable activity are both normal goods: thus, for all donors $i_{\\i}$ $$0<\\beta_{i}\\partial w_{i}/\\partial y_{i}<\\texttt{I.}$$ Equivalently (given that equation (6) holds), $$-\\textbf{I}<\\triangledown\\partial w_{i}/\\partial v_{i}<\\textbf{o}$$ is true for all donors $\\dot{\\iota}$.\nGiven the equilibrium level of the charitable activity, the value of $\\pmb{\\phi}_{i}\\hat{\\omega}w_{i}/\\hat{\\omega}y_{i}$ is determined for each donor $i,$. Thus it is possible to determine the median value of $\\phi_{i}\\partial w_{i}/\\partial y_{i}$; this median value will be written as $\\alpha$. Equation (6) must hold for all donors, so the median value of $\\partial w_{i}/\\partial v_{i}$ is $\\alpha-\\texttt{I}$. I shall first consider the possibility that $\\alpha<0^{\\cdot}5$ I shall show that this supposition, combined with the supposition that large charities exist, leads the theory into internal inconsistencies. Thus the mere fact that large charities exist is inconsistent with the theory if $\\alpha<0^{\\cdot}5$.\nAccording to the theory, $$\\frac{d z}{d w_{h}}=\\frac{\\textbf{I}}{\\textbf{I}-\\displaystyle\\sum_{i=1}^{n}\\frac{\\partial w_{i}/\\partial v_{i}}{\\ I+\\partial w_{i}/\\partial v_{i}}}.$$ Because of (8), the value of ${\\left(\\hat{\\sigma}w_{i}/\\partial v_{i}\\right)}/{\\left(\\mathbb{\\Omega}+\\hat{\\sigma}w_{i}/\\hat{\\sigma}v_{i}\\right)}$ must be negative for each donor $i$. For the median donor, this value is $(\\alpha-\\mathtt{I})/\\alpha$ and so must be less than $\\mathbf{\\Sigma}-\\mathbf{I}$ if $\\alpha<0^{\\cdot}5$. Thus the sum of the values of ${\\left(\\partial w_{i}/\\partial v_{i}\\right)}/{\\left(\\mathbf{\\nabla\\mathbf{{t}}}+\\partial w_{i}/\\partial v_{i}\\right)}$ for all donors except $h$ must be less than $\\scriptstyle{\\mathbf{I}}-n/2$. It follows that $d z/d w_{h}<2/n$. So if $n\\geqslant\\mathrm{~\\bf~I~}000$, $d z/d w_{h}<0{\\cdot}002$ : for every extra $\\mathcal{L}\\mathbf{I}$ contributed by person $h$,the charity benefits by less than $\\mathcal{L}\\mathbf{o}{\\cdot}002$.\nAccording to the theory, $$\\frac{d z}{d t}=\\frac{\\textbf{I}}{\\textbf{I}-\\displaystyle\\sum_{i=1}^{n}\\left[(\\partial w_{i}/\\partial v_{i})/(\\mathrm{I}+\\partial w_{i}/\\partial v_{i})\\right]}.$$ If $\\alpha<0^{\\cdot}5$,then $d z/d t<2{\\big/}(n+2)$. So if $n\\geqslant\\mathrm{~r~o~o~o~}$ $d z/d t<0{\\cdot}\\partial{0}{2}$. In other words, for every additional $\\mathcal{L}\\mathbf{\\Psi}^{\\mathbf{I000}}$ that the charity receives from public funds or other exogenous sources, it benefits by less than $\\mathcal{L}^{2}$, since it loses over $\\pounds{998}$ of private gifts.\nMany researchers have been interested in estimating the income elasticity of charitable giving, which may be defined by $\\eta_{i}=\\left(\\partial w_{i}/\\partial y_{i}\\right)(y_{i}/w_{i})$. Most of this work has been done in the United States. These studies, although based on different sources of data, have reached broadly similar conclusions. Most estimates of the income elasticity of charitable giving as a whole lie in the range $0^{\\cdot}4\\leqslant\\eta_{i}\\leqslant0^{.}8$.\n\nThe text discusses the inconsistency between the public good theory of philanthropy and the existence of large charities, supported by empirical evidence on income elasticity of charitable giving."
  },
  {
    "qid": "econ-empirical-913-5-0-0",
    "question": "1) Formulate a dynamic economic growth model that captures convergence in high-performance developing countries, as discussed by Abu Ahmad (2001). What are the key variables and parameters in this model?",
    "gold_answer": "1. **Model Setup**: Consider a Solow-type growth model with technological progress and capital accumulation. The production function is given by:\n   \\[ Y(t) = A(t)K(t)^\\alpha L(t)^{1-\\alpha} \\]\n   where \\( Y(t) \\) is output, \\( A(t) \\) is technology, \\( K(t) \\) is capital, and \\( L(t) \\) is labor.\n2. **Convergence Mechanism**: Assume that technology diffusion from high-performance countries drives convergence. The technology growth rate is:\n   \\[ \\frac{\\dot{A}(t)}{A(t)} = g + \\lambda \\left( \\frac{A^*(t) - A(t)}{A(t)} \\right) \\]\n   where \\( g \\) is the exogenous growth rate, \\( \\lambda \\) is the speed of convergence, and \\( A^*(t) \\) is the technology level of the leading country.\n3. **Key Variables**: Output per capita \\( y(t) = Y(t)/L(t) \\), capital per capita \\( k(t) = K(t)/L(t) \\), and technology level \\( A(t) \\).\n4. **Parameters**: \\( \\alpha \\) (capital share), \\( g \\), \\( \\lambda \\), and the initial technology gap \\( A^*(0) - A(0) \\).",
    "question_context": "Abu Ahmad, I., Dynamic economic growth and the convergence in high performance developing countries, Ph.D. Sept. 2001, Strathclyde (i, iii, vii).\nAsteriou, D., The determinants of economic growth: the roles of financial development, human capital and political instability, Ph.D. Dec. 2000 London – City (v, vii).\nMasih, R., Dynamic linkages between financial development and economic growth, Ph.D. July 2001, Cambridge (v).\nDe Ramon Acevedo, S., Three essays on the co-evolution of the real and financial sectors during the growth process, D.Phil. Oct. 2001, London – University College (v).\n\nThis section lists doctoral dissertations focusing on economic growth, financial development, and their interrelations in developing countries."
  },
  {
    "qid": "econ-empirical-1027-4-0-1",
    "question": "2) Explain why the isoquant \\( I_{F O}(p) \\) defined in (14) cannot be used directly for the analysis of the competitive industry.",
    "gold_answer": "1. The isoquant \\( I_{F O}(p) \\) assumes fixed output prices, but in the competitive industry, output prices are endogenous and depend on factor prices.\n2. \\( I_{F O}(p) \\) does not account for the industry-wide equilibrium condition \\( x^{D}(p) = x^{S}(p) \\).\n3. The competitive industry's perception of fixed output prices is incorrect, leading to a mismatch between perceived and actual optimization constraints.",
    "question_context": "The production-theoretic view of input deflation asks what the economic unit would have spent on inputs at the new input prices holding the value of output constant. But now holding output value constant is not a simple matter. It cannot be done, as in the fully open case, by restricting the output vector to an isovalue line at fixed output prices, because output prices are not fixed.\nThe construction of the production-theoretic input price deflator in this case is not merely a problem in constrained optimization; that construction also involves a fixed point argument ensuring equality of supply and demand in all output markets.\nTechnology is summarized by \\( F(x,v)=0 \\), and output demand by \\( x=x^{D}(p) \\). Firms in our competitive industry face a given output price vector, \\( p \\), and act as if they do not affect it.\nThe production-theoretic input price index for the competitive general case, \\( J_{C G} \\), is defined by \\( J_{C G}=C^{A}(p^{A})/C^{B}(p^{B}) \\), where \\( C^{A}(p) \\) and \\( C^{B}(p) \\) are the minimized costs at factor prices \\( w^{A} \\) and \\( w^{B} \\), respectively, subject to \\( F(x,v)=0 \\) and \\( p x=y \\).\nLaspeyres and Paasche bounds are no longer guaranteed to hold in this competitive general case, as the value of \\( v^{B} \\) at factor prices \\( w^{A} \\) may be lower than \\( C^{A}(p^{A}) \\), and the value of \\( v^{A} \\) at prices \\( w^{B} \\) may be lower than \\( C^{B}(p^{B}) \\).\n\nThis section discusses the competitive economic unit facing declining output demand schedules, where firms optimize taking output prices as given, but collectively affect output prices. The analysis involves production-theoretic input price deflation, equilibrium in output markets, and the challenges in applying Paasche and Laspeyres bounds."
  },
  {
    "qid": "econ-empirical-53-1-0-1",
    "question": "2) Explain why the assumption $k^{x}>2$ for all $X=A,B,C$ implies that only high-quality goods are efficient to produce in the absence of informational imperfections. Use the marginal rate of substitution and marginal rate of transformation in your explanation.",
    "gold_answer": "1. The marginal rate of substitution (MRS) between high- and low-quality goods is $k^{x}$, as given by the utility function $k^{X}x_{1} + x_{2}$. \\n2. The marginal rate of transformation (MRT) is the ratio of the marginal costs: $\\frac{MC_{1}}{MC_{2}} = \\frac{2}{1} = 2$. \\n3. Efficiency requires $MRS = MRT$. \\n4. Since $k^{x} > 2$, the MRS exceeds the MRT, implying that consumers value high-quality goods more than the cost of producing them. \\n5. Thus, it is efficient to produce only high-quality goods when $k^{x} > 2$.",
    "question_context": "Imagine an economy with no fiat money in which there are many goods and where the markets for different goods are geographically dispersed. Thus, there is an apple region, a banana region, and so on. There is no central clearinghouse where one can go to buy or sell all these goods.\nEach trader in the economy produces a single kind of good—apples, bananas, or whatever—and there are many traders producing each kind of good. Every apple-producer divides his production between high- and low-quality apples (where the marginal benefit and cost of the former are higher).\nTrade is bilateral. That is, to buy bananas, a buyer goes to a banana shop and transacts directly with the banana-seller there. Just as there is no central clearinghouse for the whole economy, neither are there clearinghouses within individual regions.\nExchanges between the buyer and seller in the banana shop are unmonitorable by third parties. This has two important implications. First, it means that the buyer must pay for the bananas on the spot. Any kind of credit or deferred payment would be infeasible because, ultimately, such arrangements rely on a court or some other authority being able to ascertain whether or not a particular transaction took place.\nWe come to the only departure from an otherwise fairly standard competitive framework: the restrictions on traders' information. A trader who produces apples and consumes bananas can distinguish between high- and low-quality apples and between high- and low-quality bananas (he is informed with respect to apples and bananas) but not between the qualities of any other good.\nLet us lay out the model more precisely. Although we have in mind an economy with many goods, for the purpose of formal analysis, we shall suppose that there are just three types of goods, $A,B$, and C. Each type $\\pmb{X}\\in({\\pmb{A}},{\\pmb{B}},{\\pmb{C}})$ comes in two qualities: $X_{\\imath}$ (high quality)and $X_{2}$ (low quality). Goods are perfectly divisible.\nThere are also three types of traders, again labeled A, $\\pmb{B}$, and $c_{\\mathrm{:}}$. according to the type of good they produce. Hence, $\\pmb{X}$ -traders produce only goods of type $\\pmb{X}.$ More specifically, an $\\pmb{X}\\cdot$ trader is endowed with one unit of labor, which can be applied to a linear production technology: good $X_{1}$ requires two units of labor per unit of output, whereas $X_{2}$ requires one unit of labor per unit.\nJust as production is linear, we suppose that preferences are linear. A-traders consume only goods of type $\\pmb{B}$ $\\pmb{B}$ -tradersonly type $c$ and $c.$ -traders only type A. Notice that this assumption means that, whenever two traders exchange the goods they produce, there cannot be a double coincidence of wants; one of the traders must accept goods that he does not consume.\nAn A-trader's preferences can be represented by the utility function, $k^{B}b_{1}+b_{2}$, where, for $i=1,2,b_{i}$ is consumption of good $B_{i},$ and $\\pmb{k^{B}}$ is a scalar coefficient. Analogously, $B-$ and $c$ -traders' preferences are also linear, with coefficients $k^{c}$ and $\\pmb{k}^{\\pmb{A}}$, respectively.\nWe are particularly interested in the production distortions induced by our informational constraints. To highlight these distortions, we shall assume that $k^{x}>2\\mathrm{forall}X=A,B,C.$ That is, the marginal rate of substitution between high- and lowquality goods exceeds the corresponding marginal rate of transformation.\nAs discussed in subsection II.1, trade is restricted to be bilateral: each exchange involves just two parties. Because exchanges are also unmonitorable by third parties, any sort of credit, short sales, or futures trading is ruled out. We are left only with barters—direct swaps of physical goods—which require no contractual agreement.\nWe suppose that an A-trader can distinguish between $A_{1}$ and $\\mathbf{A}_{2}$ (since he produces goods of type A) and between $B_{\\imath}$ and $\\pmb{B_{2}}$ (since he consumes type $\\pmb{B}$ goods), but that he cannot distinguish between $C_{\\mathbf{1}}$ and $C_{2}$ (which he neither produces nor consumes). Similarly, $B\\cdot$ and $c$ -traders cannot distinguish between qualities of type $\\pmb{A}$ and type $\\pmb{B}$ goods, respectively.\nEven without the informational restrictions, our assumptions imply that if there is to be any equilibrium trade, the model must have multiple trading periods. In a one-period model an A-trader will wish to exchange the type $\\pmb{A}$ goods he produces for type $\\pmb{B}$ goods. Given that only $\\pmb{B}$ -traders produce $\\pmb{B}$ goods and that all trade is bilateral, the exchange must be with a $\\pmb{B}$ -trader. But the $\\pmb{B}$ -trader will not be happy about receiving type A goods, which he can neither consume nor—in the absence of a subsequent trading period —resell.\nIn our basic model we assume that there are a finite number $\\pmb{T}$ of discrete trading periods indexed by $t=1,\\ldots,T.$ We take $\\pmb{T}$ to be exogenous (but in subsection II1.4 explore the implications of different values of $\\pmb{T}$ ).\nFor any two goods, $X_{i}$ and $\\pmb{Y}_{j},$ let $p_{t}(X_{i},Y_{j})$ be the relative price in period $t$ of $X_{i}$ in terms of $\\pmb{Y_{j}},$ i.e., how much a $\\pmb{Y},$ trader must sell in order to buy one unit of $X_{i}$ (Hence $\\widetilde{p_{t}(X_{i},Y_{j})}=1/p_{t}(Y_{j},X_{i}).$ ) Notice that by expressing prices in this way, we implicitly assume that they are independent of the quantities traded (i.e., that they are “linear\"). In fact, this linearity follows immediately from arbitrage.\n\nThe text describes an economy with geographically dispersed markets, bilateral trade, and no fiat money. Traders produce and consume different goods, with production divided into high- and low-quality variants. Trade is constrained by informational asymmetries and the absence of credit or futures markets."
  },
  {
    "qid": "econ-empirical-546-3-0-2",
    "question": "3) Show that in Proposition 1, the hazard rate $\\frac{f(\\tau)}{1-F(\\tau)}$ must equal $\\mu_j$ for a player of type $j$ to mix in an interval $[t,t']$.",
    "gold_answer": "1. **Expected Payoff**: $W_j(\\tau) = \\int_0^\\tau e^{-rt}B_{j-1}f(t)dt + (1-F(\\tau))e^{-r\\tau}B_j$.\n2. **Differentiation**: $\\frac{dW_j(\\tau)}{d\\tau} = e^{-r\\tau}B_{j-1}f(\\tau) - f(\\tau)e^{-r\\tau}B_j - r(1-F(\\tau))e^{-r\\tau}B_j$.\n3. **Indifference Condition**: $\\frac{dW_j(\\tau)}{d\\tau} = 0 \\implies \\frac{f(\\tau)}{1-F(\\tau)} = r\\frac{B_j}{B_{j-1}-B_j} \\equiv \\mu_j$.",
    "question_context": "Proof of Lemma 1. As explained in the main text, $P1$ and $P2$ hold at $t=0$.\nProof of Lemma 2. Consider an arbitrary symmetric equilibrium $\\sigma$ and let $F$ be the distribution of stopping dates of an arbitrary neighbor of an arbitrary player $i$ conditional on $i$ never stopping. Let $T\\in\\mathbb{R}\\cup\\{+\\infty\\}$ be the least upper bound of the support of $F$.\nProof of Proposition 1. We prove the result in the case $\\mu_{1}>\\mu_{2}$ . The other case is perfectly symmetric.\nProof of Proposition 2. By the same argument as in the proof of Proposition 1, there must be some initial phase $[0,t^{\\prime}]$ during which type 1 randomizes and type 2 waits, and where $\\gamma(t)=\\lambda_{1}(t)p_{1}(t)=\\overline{{\\gamma}}_{1}$.\nProof of Proposition 3. Following the same arguments as in Proposition 2, we can establish that while players of type 2 are mixing, the beliefs evolve according to: $\\dot{p}_{2}(t)=-\\gamma(t)p_{2}(t)-\\lambda_{2}(t)p_{2}(t)(1-p_{2}(t))$.\nProof of Proposition 4. Take an arbitrary line segment of $n$ players and consider all feasible orders in which those $n$ players can stop. In all the arguments, the players are numbered from 1 to $n$ from left to right.\nProof of Proposition 5. We first derive a formula for the social welfare for a given smoothly declining subsidy path $s(t)$.\nProof of Proposition 6. We have to show that $\\alpha^{*}:=\\frac{1-q_{0}}{q_{0}}\\frac{2B}{B_{0}+B_{1}}$ is increasing in $\\chi$.\nProof of Proposition 7. We can write the expected social welfare of a subsidy set at $S$ and expiring at rate $\\kappa$ as: $W^{r a}(s,\\kappa)=q_{0}(B_{0}-\\alpha s)+(1-q_{0})V^{r a}(s,\\kappa)$.\nProof of Proposition 8. We show that any constant adoption rate $\\gamma$ implemented by a permanent subsidy $s$ handed to any adopting agent is also implemented by a restricted permanent subsidy $s^{*}$ handed only to adopting type 1 agents, with $s^{*}$ , with $s^{*}<s$.\n\nThe appendix contains detailed proofs for various lemmas and propositions discussed in the main text, focusing on the dynamics of player interactions, beliefs, and stopping times in a network setting."
  },
  {
    "qid": "econ-empirical-67-4-2-1",
    "question": "6) Derive the reservation price for a variety using the local demand elasticity and explain its role in the Fisher ideal price index.",
    "gold_answer": "The reservation price $p_{gcr}$ for a variety not imported in period $r$ is derived as:\n\\[\np_{gcr} = p_{gct} \\left(1 + \\frac{1}{\\sigma_g}\\right)\n\\]\nwhere $p_{gct}$ is the price in period $t$ and $\\sigma_g$ is the elasticity of substitution. This reservation price is used in the Fisher ideal price index to account for the virtual price movements of disappearing and created varieties.",
    "question_context": "The benefit of using a quadratic utility function is that its exact price index is the Fisher ideal price index, which is the geometric average of the Paasche and Laspeyres indices.\nThe only remaining issue is how to measure the reservation prices of varieties. To do this, we use the fact that our elasticity of substitution corresponds to our estimate of the local demand elasticity.\n\nThis section examines the robustness of the results to alternative assumptions, such as different utility functions and production structures."
  },
  {
    "qid": "econ-empirical-1641-2-1-0",
    "question": "5) State and interpret Theorem 2, which generalizes the order-independence result for resilient solutions.",
    "gold_answer": "Theorem 2 states that for any resilient solutions $R^1, \\ldots, R^n$, the product set $\\times_i R_i^i$ is equivalent to every resilient solution (and thus to $\\mathbb{E}\\mathbb{R}$). This means that even if players follow different elimination orders, the resulting product set preserves the equivalence properties guaranteed by Theorem 1.",
    "question_context": "Theorem 2. For all resilient solutions $R^{1},\\ldots,R^{n}$ , the set of strategy profiles $\\times_{i}R_{i}^{i}$ is equivalent to every resilient solution (and thus to $\\mathbb{E}\\mathbb{R}$ ).\n\nThis section explores the implications of Theorem 1 for order-independence in the elimination of distinguishably dominated strategies."
  },
  {
    "qid": "econ-empirical-248-3-0-3",
    "question": "4) Validate the boundedness condition in Theorem 5.3 by deriving the inequality involving $\\rho(\\sqrt{N}(\\bar{\\beta}_1 - \\beta_1), 0)$ and the contraction mapping $T$.",
    "gold_answer": "1. **Contraction mapping**: For fixed $T$, $\\rho(T(\\beta_1), \\beta_1) \\leq \\gamma \\rho(\\bar{\\beta}_1, \\beta_1)$ with $0 \\leq \\gamma < 1$.  \n2. **Fixed point property**: Since $\\bar{\\beta}_1 = T(\\bar{\\beta}_1)$, $$\\rho(\\bar{\\beta}_1, \\beta_1) \\leq \\frac{\\rho(T(\\beta_1), \\beta_1)}{1-\\gamma}.$$  \n3. **Rescale by $\\sqrt{N}$**: $$\\rho(\\sqrt{N}(\\bar{\\beta}_1 - \\beta_1), 0) \\leq \\frac{\\rho(\\sqrt{N}(T(\\beta_1) - \\beta_1), 0)}{1-\\gamma}.$$  \n4. **Boundedness**: From (5.10), $\\sqrt{N}(T(\\beta_1) - \\beta_1)$ is bounded, implying $\\sqrt{N}(\\bar{\\beta}_1 - \\beta_1)$ is also bounded.",
    "question_context": "$${\\sqrt{N}}{\\bigl(}{\\hat{\\beta}}-\\beta{\\bigr)}\\rightarrow\\mathrm{N}\\left(0,{\\frac{1}{4f{\\bigl(}0{\\bigr)}^{2}}}\\operatorname*{plim}_{N\\rightarrow\\infty}{\\bigl(}X^{\\prime}\\Sigma X{\\bigr)}^{-1}\\right).$$\nAssumption 4. $u_{j}$ are i.i.d.\nAssumption 5. $\\pmb{u}_{j}$ have a density function $f(u)$ . In the neighborhood of zero, $f(u)$ satisfies the following conditions: (i) continuous and differentiable, (ii) $f(u)>0$ ,and (ii) $f^{\\prime}(u)$ is bounded.\nAssumption 6. $\\v{x}_{j}^{0}$ are i.i.d.\nAssumption 7. $\\{u_{j}\\}$ and $\\{x_{j}^{0}\\}$ are independent.\nTheorem 5.1. Let $\\{u_{j}\\}=\\{u_{1},u_{2},...,u_{\\tau}\\}$ .Suppose $\\{u_{j}\\}$ satisfy Assumptions 3,4,and 5 and $\\tau$ goes to infinity in probability as $N$ goes to infinity. Then $${\\sqrt{\\tau}}M\\left[\\left\\{{\\bf{\\sigma}}(u_{j}\\}\\right]\\rightarrow{\\bf N}\\left(0,{\\frac{1}{4f(0)^{2}}}\\right)\\quad{\\mathrm{as}}\\quad N\\rightarrow\\infty,$$ where $M[\\cdot]$ is the median defined in (2.2).\nTheorem 5.2. Let $\\hat{\\theta_{\\tau}}=M\\left[\\left\\{u_{j}\\right\\}\\right]$ and $\\tilde{\\theta}_{\\tau}=M\\left[\\left\\lbrace u_{j}+v_{j}^{\\prime}w_{\\tau}\\right\\rbrace\\right]$, where $\\pmb{\\tau}$ is the number of elements in $\\{u_{j}\\}$ ,and $\\pmb{\\nu}_{j}$ and $w_{\\tau}$ are $K$ dimensional vectors. Suppose ${\\boldsymbol{u}}_{j},{\\boldsymbol{v}}_{j}$ and $w_{\\tau}$ satisfy the following assumptions: (i) ${\\pmb u}_{j}$ satisfy Assumptions 3, 4, and 5. (i) $v_{j}$ are i.i.d. and bounded. (iii) $\\{u_{j}\\}$ and $\\{v_{j}\\}$ are independent. (iv) $\\tau$ goes to infinity in probability as $N$ goes to infinity. (v)For any $\\protect\\varepsilon>0$ ,there exist $M_{\\varepsilon}$ and $\\tau_{0}$ such that $\\dot{\\bf P}[\\rho(\\sqrt{\\tau}w_{\\tau},0)>M_{\\varepsilon}]<\\varepsilon$ for $\\tau>\\tau_{0}$ Then $\\operatorname*{plim}_{N\\rightarrow\\infty}\\sqrt{\\tau}\\left(\\widehat{\\theta_{\\tau}}-\\widetilde{\\theta_{\\tau}}\\right)=0.$\nTheorem 5.3. Under Assumptions $I{-}7$ for any $\\varepsilon>0$ there exist $M_{\\varepsilon}$ and $N_{0}$ such that $$\\begin{array}{r}{\\mathrm{P}\\Big[\\rho\\left(\\sqrt{N}\\left(\\left.\\frac{}{}\\beta_{1}-\\beta_{1}\\right),0\\right)>M_{\\varepsilon}\\Big]<\\varepsilon\\quad\\mathrm{for}\\quad N>N_{0},}\\end{array}$$ where ${\\bar{\\beta}}_{1}$ is the first-stage estimator.\nTheorem 5.4. In addition to the assumptions of this section, if X'EX converges to a positive definite matrix in probability, $$\\sqrt{N}\\left(\\hat{\\beta}-\\beta\\right)\\rightarrow N\\left(0,\\frac{1}{4f\\left(0\\right)^{2}}A^{-1}\\right),$$ where ${\\cal A}=\\operatorname*{plim}_{{\\scriptstyle N\\rightarrow\\infty}}X^{\\prime}\\Sigma X.$\n\nThis section discusses the asymptotic distribution of the estimator under additional assumptions and theorems."
  },
  {
    "qid": "econ-empirical-1285-2-2-3",
    "question": "4) Discuss the calibration of the wealth effect parameter $\\omega$. How does it affect employment volatility in the model?",
    "gold_answer": "4. The wealth effect parameter $\\omega$ is calibrated to match employment volatility:\n   - $\\omega=1$: Full wealth effects (standard preferences), dampening employment volatility.\n   - $\\omega=0$: No wealth effects (GHH preferences), maximizing employment volatility.\n   The calibrated $\\omega=0.05$ reduces wealth effects to match Mexico's employment volatility ($0.42$ relative to GDP).",
    "question_context": "We chose Mexico as a benchmark given the importance of the informal sector and the availability of detailed labor flows data, and calibrate the parameters of the model to reproduce some key metrics for the Mexican economy. These include some average ratios (as the employment rate), targeted by the model in steady state, and a set of business cycle moments (as the volatility of output, or its correlation with inflation) which we seek to match in the simulated model.\nThe complete list of parameters for the baseline specification of the model can be found in Table 1. A period in the model is a quarter. The calibration can be split in three blocks, divided by a thicker line in the table. In the first block, the parameters correspond to standard values in the literature; in the second, the financial shock process is directly estimated from time series for financial spreads; more interestingly, the parameters of the third block, are chosen to reproduce some target values of the model in steady state and to match key business cycle moments for Mexico.\nRegarding the first block, for preferences we chose a unitary elasticity of labor supply, equal to the intertemporal elasticity. Other parameters, as the discount factor, depreciation rate and the capital share in production, are chosen according to the standard business cycle literature. The parameters for the elasticity of substitution between varieties and the Taylor rule correspond to the prototype New Keynesian model and lie in the same range as Castillo and Montoro (2010) or Colombo et al. (2019).\nAn important parameter to measure the flexibility in the labor market is the exogenous separation rate $s$, governing the turnover in the formal sector. We chose $8.8\\%$, a value consistent with an average duration of a job in the formal sector of 2.8 years in Mexico. Another key parameter in our model is the working capital requirement $\\kappa$ in the formal sector, for which no direct empirical counterpart exists. We follow Pratap et al. (2019) and set this requirement to 0.21, corresponding to the ratio of total short term credit over gross output in Mexican manufacturing sector.\n\nThe quantitative model is calibrated to match key metrics of the Mexican economy, including steady-state ratios and business cycle moments."
  },
  {
    "qid": "econ-empirical-452-0-1-1",
    "question": "2) The money supply follows a driftless Brownian motion: \\(dm(t) = \\sigma dw(t)\\). How does this assumption influence the dynamics of output and prices in the model?",
    "gold_answer": "The assumption implies:\n\n1. **Nominal Demand**: The money supply evolves stochastically, with variance \\(\\sigma^2\\). Since velocity is constant, output is determined by real balances: \\(y(t) = m(t) - p(t)\\).\n\n2. **Price Dynamics**: Firms must forecast the path of the aggregate price level \\(p(t)\\) to set optimal prices. The Brownian motion assumption introduces uncertainty, making this forecasting problem non-trivial.\n\n3. **Regulated Output**: Output \\(y(t)\\) is regulated within bounds \\([-\\bar{y}, \\bar{y}]\\). When output hits these bounds, price adjustment occurs to prevent further deviations.\n\nThis setup ensures that nominal shocks (\\(dm(t)\\)) have real effects until prices adjust, creating a dynamic interplay between money, output, and prices.",
    "question_context": "Output is produced by a continuum of price setting firms indexed by $i$. Each firm faces a fixed real cost $c$ of changing its nominal price. They all have identical flow profits which depend on two factors: real aggregate demand, $y$, and the firm's price relative to an aggregate price index, $x_{i} \\equiv p_{i} - p$.\nThe price $p^{*}$ which maximizes instantaneous profits for each firm is a linear combination of the price index and real aggregate demand, $p^{*} = p + \\alpha y$. The instantaneous loss in real profits to firm $i$ from charging a price different than the optimum is quadratic in $p_{i} - p^{*}$.\n\nThe paper presents a dynamic model of monopolistically competitive price setting, where firms face fixed costs of price adjustment and optimize their pricing strategies under stochastic demand."
  },
  {
    "qid": "econ-empirical-1653-0-0-3",
    "question": "4) Discuss the role of adaptive estimation in the context of LABF. How does replacing the error density with an estimate affect the quadratic approximation?",
    "gold_answer": "Adaptive estimation in LABF involves:\n1. Estimating the error density $f(x)$ from the data.\n2. Using the estimate $\\hat{f}(x)$ to construct the quadratic approximation.\n\nThe approximation remains valid under LABF if:\n1. The Fisher information of $f(x)$ is finite and positive.\n2. The estimate $\\hat{f}(x)$ converges sufficiently fast to $f(x)$. This enables inference without explicit knowledge of $f(x)$, e.g., in AR models with unit roots.",
    "question_context": "Under the framework in which the asymptotic problems are treated, only the approximating structure of the likelihood ratios of the observations, together with auxiliary estimates of the parameters, will be required. Such approximating structures are available under quite general assumptions, such as that the Fisher information of the common density of the error variables is finite and nonsingular, and the more specific assumptions, such as Gaussianity, are not required.\nThe LR's satisfying the aforementioned quadratic approximation are called locally asymptotically normal (LAN) likelihood ratios. This means that if $P_{\\theta,n}$ is the distribution of the sample (of size $\\pmb{n}$ )with $\\pmb\\theta$ being the parameter, then for suitable normalizing matrices $\\delta_{n}(\\theta)$ the LRs, $d P_{\\theta+\\delta_{n}(\\theta)h,n}/d P_{\\theta,n}$ can be approximated by a quadratic of the form $\\exp\\{h^{\\prime}W_{n}(\\theta)-{\\textstyle\\frac{1}{2}}h^{\\prime}S_{n}(\\theta)h\\}$, where $h$ is the normalized parameter, having limits of the form $\\exp(h^{\\prime}W-{\\textstyle\\frac{1}{2}}h^{\\prime}S h)$, where the positive definite matrix $s$ is nonrandom and $W$ is Gaussian with mean vector zero and covariance matrix $s.$\nThe more useful approximations in terms of more stable quadratics involving nonlinear transformations, which may be necessary for satisfactory treatments of nonlinear models, are not treated here in detail.\n\nThe paper reviews basic elements of large sample theory within a restricted structural framework, focusing on asymptotic inference problems in time series regression models. It covers linear models (e.g., cointegrated models, autoregressive models with unit roots) and nonlinear models (e.g., ARCH models), emphasizing the role of likelihood ratio approximations and adaptive estimation."
  },
  {
    "qid": "econ-empirical-1365-3-0-2",
    "question": "3) Calculate the expected losses when \\(H_0\\) is rejected without sampling, using the integrals \\(\\int_{-\\infty}^{D_{0}}l_{\\mathrm{I}}(D)f(D)dD\\) and \\(\\int_{D_{0}}^{\\infty}l_{\\mathrm{II}}(D)f(D)dD\\). Assume \\(D_0 = 1.11\\mathrm{kW}\\) and provide a step-by-step solution.",
    "gold_answer": "1. **For \\(D \\leq 1.11\\)**: \n   \\(\\int_{-\\infty}^{1.11} (355 - 320D) f(D) dD\\). \n   - Substitute \\(f(D)\\) from Fig. 1 and evaluate numerically. \n2. **For \\(D > 1.11\\)**: \n   \\(\\int_{1.11}^{\\infty} (320D - 355) f(D) dD\\). \n   - Use the same prior and integrate. \n3. **Total Loss**: Sum the results from both integrals.",
    "question_context": "Given the prior distribution $f(D)$ we can, for given $n^{*}$ , find an optimal value for $c$ by solving the problem of minimizing eq. (7). The procedure amounts to tabulating eq. (6) for each $\\pmb{D}$ and $c$ and, finally, inspecting the conditional expected losses thus calculated over $f(D)$.\nThe Georgia Power Company experienced a maximum diversified demand reduction of $1.4\\upkappa\\mathbf{W}$ on the peak summer day in 1975 using a $50\\%$ cycling intensity (15 minutes on, 15 minutes off). In terms of those results, the mode of $f(D)$ at $1.0\\upkappa\\mathbf{W}$ is conservative.\nThe $l_{\\uparrow}(D)$ and $l_{\\mathrm{II}}(D)$ loss functions are developed from the cost information used to calculate $D_{0}$. If $D\\leq1.11{\\mathrm{kW}}$ but we reject $H_{0}$, per capita opportunity losses are accrued according to $l_{\\mathrm{t}}(D){=}355-32(10)D$ for $D\\leq1.11$ and $=0$ otherwise.\nFor Type II errors, $l_{\\mathrm{n}}(D)=32(10)D-355\\mathrm{for}D>1.11$ and $=0\\mathrm{otherwise}$, representing direct losses from implementing load control when it is not cost-effective.\nIf no sample is taken and $H_{0}$ is rejected, expected losses from decision errors derive from ${\\cal D}\\leq1.11\\mathrm{kW}$ and are calculated by $\\int\\operatorname{\\mathbf{\\Gamma}}_{-\\infty}^{D_{0}}l_{\\mathrm{I}}(D)f(D)\\mathrm{d}D$. When $D>1.11 \\mathrm{kW}$, losses are incurred only if $H_{0}$ is not rejected, calculated by $\\int_{D_{0}}^{\\infty}l_{\\mathrm{II}}(D)f(D)\\mathrm{d}D$.\n\nThe text discusses the determination of an optimal critical region for hypothesis testing, involving prior distributions, loss functions, and decision errors. It includes detailed mathematical formulations and empirical data from the Georgia Power Company."
  },
  {
    "qid": "econ-empirical-1627-1-0-1",
    "question": "2) Prove the invertibility of the market share function $\\sigma_{t}(\\theta, x_{t}, \\cdot, P_{t})$ as shown by Berry (1994). What conditions must hold for this invertibility to be valid?",
    "gold_answer": "1. The market share function $\\sigma_{t}(\\theta, x_{t}, \\xi_{t}, P_{t})$ maps $\\xi_{t}$ to $s_{t}$.\n2. Berry (1994) shows that for any $\\theta$, $x_{t}$, $P$, and $s_{j t} \\in (0,1)$, there exists a unique $\\xi_{t}$ such that:\n   $$ s_{j t} = \\int \\nu_{j t}(\\theta, x_{t}, \\xi_{t}, v) d P(v). $$\n3. The key conditions are:\n   - The utility function is strictly increasing in $\\xi_{j t}$.\n   - The market shares $s_{j t}$ are strictly positive and sum to less than 1.\n   - The distribution $P$ is non-degenerate.\n4. The inverse function $\\sigma_{t}^{-1}(\\theta, x_{t}, s_{t}, P_{t})$ exists and is differentiable under these conditions.",
    "question_context": "Let $p_{j t}\\in\\mathbb{R}$ be the price of product $j$ in market t. Define $x_{j t}={}$ $\\left(p_{j t},x_{j t}^{(2)}\\right)\\in\\mathbb{R}^{d_{x}}$ ∈ Rdx , where xj(t2) a re observed product characteristics other than price and may include a constant or product dummies. Let $\\xi_{j t}~\\in~\\mathbb{R}$ be a product characteristic of product $j$ in market t, which is observed by firms and consumers, but not by the econometrician. In the BLP model consumer $i$ in market $t$ chooses the product $j$ which maximizes the utility \n\n$$\nu_{i j t}=x_{j t}^{\\prime}\\beta_{i t}+\\xi_{j t}+\\epsilon_{i j t},\n$$ \n\nwhere $\\beta_{i t}~=~\\beta_{0}+\\Sigma_{0}v_{i t},\\beta_{0}~\\in~\\mathbb{R}^{d_{\\boldsymbol{x}}},\\Sigma_{0}$ is a $d_{x}\\times d_{x}$ matrix, and $v_{i t}~\\in~R^{d_{x}}$ is a random vector with distribution function $P_{0t}$ . Assuming that $\\epsilon_{i j t}$ are independent and identically distributed (iid) extreme value random variables, the market share of product $j$ in market $t$ is given by \n\n$$\ns_{j t}=\\int\\frac{\\exp(x_{j t}^{\\prime}\\beta_{0}+\\xi_{j t}+x_{j t}^{\\prime}\\Sigma_{0}\\upsilon)}{1+\\displaystyle\\sum_{k=1}^{J_{t}}\\exp(x_{k t}^{\\prime}\\beta_{0}+\\xi_{k t}+x_{k t}^{\\prime}\\Sigma_{0}\\upsilon)}d P_{0t}(\\v\\upsilon).\n$$\nBerry (1994) shows that for any $\\theta\\in\\Theta,x_{t}\\in\\mathbb{R}^{d_{x}\\times J_{t}}$ , distribution $P$ , and $s_{j t}\\in(0,1)$ there exists $\\xi_{t}\\left(\\theta,P,s_{t},x_{t}\\right)\\in\\mathbb{R}^{J_{t}}$ such that for all $j$ \n\n$$\ns_{j t}=\\int\\nu_{j t}\\left(\\theta,x_{t},\\xi_{t}\\left(\\theta,P,s_{t},x_{t}\\right),\\upsilon\\right)d P(\\upsilon).\n$$ \n\nIn particular, one can solve for $\\xi_{t}$ in the system $\\mathsf{o f}J_{t}$ equations given in (2) or, in other words, $\\sigma_{t}(\\theta,x_{t},\\cdot,P_{t})$ is invertible. I denote the inverse function by $\\sigma_{t}^{-1}(\\theta,x_{t},\\cdot,P_{t})$ .\nNow assume that there are instruments $z_{t}\\in\\mathbb{R}^{J_{t}\\times d_{z}}$ such that \n\n$$\nE\\left(z_{t}^{\\prime}\\xi_{t}\\left(\\theta_{0},P_{0t},s_{t},x_{t}\\right)\\right)=0\n$$ \n\nand that these moment conditions identify $\\theta_{0}$ . Then an estimator of $\\theta_{0}$ is \n\n$$\n\\begin{array}{c}{{\\widetilde{\\theta}\\equiv\\displaystyle\\arg\\operatorname*{min}_{\\theta\\in\\Theta}\\left(\\frac{1}{T}\\sum_{t=1}^{T}z_{t}^{\\prime}\\xi_{t}\\left(\\theta,P_{0t},s_{t},x_{t}\\right)\\right)^{\\prime}W_{T}}}\\\\ {{\\times\\left(\\frac{1}{T}\\sum_{t=1}^{T}z_{t}^{\\prime}\\xi_{t}\\left(\\theta,P_{0t},s_{t},x_{t}\\right)\\right),}}\\end{array}\n$$ \n\nwhere $W_{T}$ is a weighting matrix which converges in probability to a positive definite matrix $W$ .\nTheorem 1. Assume that Assumptions RC1–RC9 hold. Then there exist $d_{z}\\times d_{z}$ matrices $\\phi_{1},\\phi_{2}$ , and $\\varPhi_{3}$ , as well $d_{z}\\times1$ vectors $\\bar{\\mu}_{1}$ and ${\\bar{\\mu}}_{2}$ such that \n\n$$\n\\begin{array}{r l}&{\\sqrt{T}\\left(\\hat{\\theta}-\\theta_{0}\\right)=\\left(\\left(\\boldsymbol{r^{\\prime}}\\boldsymbol{W}\\boldsymbol{{T}}\\right)^{-1}\\boldsymbol{r^{\\prime}}\\boldsymbol{W}+o_{p}(1)\\right)}\\\\ &{\\qquad\\times\\left(\\boldsymbol{Q}_{1T}+\\frac{1}{\\sqrt{R}}\\boldsymbol{Q}_{2T}+\\frac{1}{\\sqrt{N}}\\boldsymbol{Q}_{3T}+\\frac{\\sqrt{T}}{R}\\boldsymbol{C}_{1T}\\right.}\\\\ &{\\qquad\\left.+\\frac{\\sqrt{T}}{N}\\boldsymbol{C}_{2T}+o_{p}\\bigg(\\frac{\\sqrt{T}}{R}\\bigg)+o_{p}\\bigg(\\frac{\\sqrt{T}}{N}\\bigg)\\right)}\\end{array}\n$$ \n\nwhere \n\nand \n\n$$\nC_{1T}\\stackrel{p}{\\rightarrow}\\bar{\\mu}_{1},\\quad C_{2T}\\stackrel{p}{\\rightarrow}\\bar{\\mu}_{2}.\n$$ \n\nFurthermore, $Q_{1T},Q_{2T}$ , and $Q_{3T}$ are asymptotically independent.\n\nThe random coefficient logit model is a fundamental tool in empirical industrial organization, allowing for heterogeneous consumer preferences through random coefficients on product characteristics. This section details the model's assumptions, asymptotic properties, and estimation framework."
  },
  {
    "qid": "econ-empirical-619-1-0-0",
    "question": "1) Derive the limiting distribution of the normalized-bias statistic $(T/4)\\hat{\\phi}_{1}$ under the null hypothesis $\\mathfrak{x}=1$ in the presence of seasonal unit roots, as shown in equation (2.5).",
    "gold_answer": "1. Start with the regression model: \n$$A x_{t} = \\phi_{1}x_{t-1} + \\phi_{2}A x_{t-1} + \\phi_{3}A x_{t-2} + \\phi_{4}A x_{t-3} + u_{t},$$ \nwhere $\\phi_{1} = \\mathfrak{x} - 1$. \n2. Under $H_0: \\mathfrak{x} = 1$, $\\phi_{1} = 0$. \n3. Using the results from Chan and Wei (1988), the normalized-bias statistic converges to: \n$$(T/4)\\hat{\\phi}_{1} \\rightarrow \\{W(1)^{2} - 1\\}/2\\int_{0}^{1}W(r)\\mathrm{d}r,$$ \nwhere $W(r)$ is a standard Brownian motion. \n4. This shows the statistic's dependence on the Brownian motion's properties.",
    "question_context": "The commonly assumed regularity conditions for applying augmented Dickey-Fuller [henceforth, ADF] tests imply that the DGP does not have roots at frequencies other than the zero frequency and that there is at most one unit root at the zero frequency. It is, therefore, worth investigating the properties of DF-type unit root tests when unit roots are present at some of the seasonal frequencies.\nTo examine the properties of the DF test for a unit root at the zero frequency in the presence of seasonal unit roots, we can rewrite the time series process in (2.1) as follows: \n$$\\begin{array}{r}{A x_{t}=\\phi_{1}x_{t-1}+\\phi_{2}A x_{t-1}+\\phi_{3}A x_{t-2}+\\phi_{4}A x_{t-3}+u_{t},}\\end{array}$$ \nwhere $\\phi_{1}=\\pmb{\\mathscr{x}}-1$ and $\\phi_{2}=\\phi_{3}=\\phi_{4}={\\bf\\rho}-{\\bf\\alpha}\\alpha.$\nThe result in (2.6) shows that the usual Dickey-Fuller $t$ -statistics can still be used to test the hypothesis of a unit root at the zero frequency, even in the presence of unit roots at other seasonal frequencies to the extent that lagged terms of dependent variables are appropriately augmented as in (2.4).\nThe DHF tests are based on the following regression: \n$$x_{t}=\\d{x}_{{t-4}}+\\sum_{j=1}^{l}\\d{A}_{4}x_{{t-j}}+[\\mathrm{set~of~fixed~regressors}]+\\varepsilon_{t},$$ \nwhere the set of fixed regressors may include a constant, seasonal dummies, and a linear trend.\nThe HEGY tests involve running a different regression, namely, \n$$\\begin{array}{r}{\\displaystyle{A_{4}x_{t}=\\pi_{1}y_{1,t-1}+\\pi_{2}y_{2,t-1}+\\pi_{3}y_{3,t-2}+\\pi_{4}y_{3,t-1}}}\\\\{\\displaystyle{+\\sum_{j=1}^{l}A_{4}x_{t-j}+[\\mathrm{set~of~fixed~regressors}]+\\varepsilon_{t}}}\\end{array}$$ \nwhere $y_{i t},i=1,2,3$ , are defined in (2.2).\n\nThe text discusses the properties of Dickey-Fuller (DF) tests in the presence of seasonal unit roots, the asymptotic equivalence between HEGY-type tests and DF procedures, and the design of Monte Carlo experiments to evaluate these tests."
  },
  {
    "qid": "econ-empirical-805-0-0-2",
    "question": "3) Discuss the conflicting simulation results regarding the Glejser test's size under skewed errors, as reported by Barone-Adesi and Talwar (1983) versus Ali and Giaccotto (1984).",
    "gold_answer": "Barone-Adesi and Talwar (1983) observed size distortions with chi-square errors, while Ali and Giaccotto (1984) found correct size with lognormal errors. The discrepancy arises because:\n- Chi-square skewness is milder than lognormal, but the Glejser test's sensitivity depends on the skewness magnitude and the sample size.\n- Lognormal errors might coincidentally align with the test's assumptions in finite samples, masking the asymptotic invalidity.",
    "question_context": "Verifying that the Glejser test for heteroskedasticity is asymptotically invalid unless the error density is symmetric, this paper proposes a simple modification to make the test robust to asymmetric disturbances. Simulation results demonstrate that the size of the modified test is correct for both symmetric and skewed errors.\nIn least-squares analysis, the most widely used tests for the presence of heteroskedasticity examine whether the squared residuals are correlated with some other variables. Since they were advanced by Bickel (1978), Breusch and Pagan (1979), and White (1980), the squared residual-based tests have been adopted as a standard procedure in many applications.\nThe Glejser test is designed to test whether the regression residuals in absolute value are correlated with some other variables. Following the simulation result provided by Newey and Powell (1987), the Glejser test is virtually identical to their more elaborate test obtained by comparing two different expectile estimators, and is much more powerful than the squared residual-based test when the error density has fat tails.\nIt has been an issue in the literature whether the Glejser test is valid when the error density is not symmetric. Several studies have addressed this issue through simulation: Barone-Adesi and Talwar (1983) observed that the empirical size of the Glejser test is quite different from the nominal size when the errors follow the chi-square distribution with four degrees of freedom.\nIt is verified in the following section that the Glejser test is not valid for skewed errors. And we propose a simple modification to the Glejser test to correct this non-robustness property. The modified test has correct size for asymmetric disturbances as well as for symmetric ones.\n\nThis section discusses the Glejser test for heteroskedasticity, its asymptotic invalidity under asymmetric error densities, and proposes a modification to ensure robustness. The modified test's performance is validated through simulations."
  },
  {
    "qid": "econ-empirical-1668-4-0-0",
    "question": "1) Derive the expression $w\\ell_{\\overline{{{\\epsilon}}}}/{p}+w\\widehat{l}_{\\bar{I}}=(w/{p})(c_{p}^{c})^{-1}(l_{p}^{c}m_{\\bar{I}}-l_{\\bar{I}}m_{p}^{c})$ from the given Neary and Roberts (1980) equations.",
    "gold_answer": "1. Start with $\\hat{l}_{\\bar{c}}=\\hat{l}_{\\bar{c}}^{c}+\\hat{l}_{I}(\\bar{p}-p)$ and $\\hat{l}_{I}=l_{I}-\\hat{l}_{\\bar{c}}^{c}c_{I}$.\n2. Substitute $\\hat{l}_{\\bar{c}}^{c}=l_{p}^{c}/c_{p}^{c}$.\n3. Rearrange to isolate $w\\hat{l}_{\\bar{c}}/\\hat{p}$ and $w\\hat{l}_{I}$.\n4. Combine terms using the weak separability condition to obtain the final expression.",
    "question_context": "The marginal propensity to work following a relaxation of the goods market constraint $\\bar{=}w\\hat{l}_{\\bar{c}}/\\hat{p}$\nThe marginal propensity to work following an increase in lump-sum income $=w\\hat{l}_{I}$\nFrom Neary and Roberts (1980): $\\hat{l}_{\\bar{c}}=\\hat{l}_{\\bar{c}}^{c}+\\hat{l}_{I}(\\bar{p}-p),\\hat{l}_{I}=l_{I}-\\hat{l}_{\\bar{c}}^{c}c_{I},$ where $\\hat{l}_{\\bar{c}}^{\\tt c}=l_{p}^{c}/c_{p}^{c}$ and $\\bar{\\b{\\mathscr{p}}}=\\dot{\\bar{\\b{\\mathscr{n}}}}$ virtual price of goods.\nFrom Geary and Morishima (1973), a sufficient condition for the final bracketed term to be zero is that leisure and money are weakly separable from goods in the utility function.\nAssume that a region of classical unemployment does not exist, i.e. that the slope of GMEL (Effective) is greater than or equal to the slope of LMEL (Effective). Thus $\\Hat{p}\\Tilde{c}_{I}>\\texttt{I}$\n\nThis section compares the slopes of LMEL (Effective) and GMEL (Effective) under specific marginal propensities and separability conditions, following Neary and Roberts (1980) and Geary and Morishima (1973)."
  },
  {
    "qid": "econ-empirical-1273-2-0-2",
    "question": "3) For the case of one regressor ($K=1$), derive the expectation $E(w_r)$ under parameter instability, given the formula: \n\n$$\nE(w_r) = \\frac{x_r}{d_r} \\left[ \\beta_r - \\left( \\sum_{t=1}^{r-1} x_t^2 \\beta_t \\middle/ \\sum_{t=1}^{r-1} x_t^2 \\right) \\right].\n$$\n\nExplain the interpretation of the weighted average term.",
    "gold_answer": "1. The formula for $E(w_r)$ is derived from the difference between the current coefficient $\\beta_r$ and a weighted average of past coefficients $\\beta_t$.  \n2. The weighted average term is $\\left( \\sum_{t=1}^{r-1} x_t^2 \\beta_t \\middle/ \\sum_{t=1}^{r-1} x_t^2 \\right)$, which represents the historical influence of $\\beta_t$ scaled by $x_t^2$.  \n3. If $\\beta_t$ increases monotonically, $E(w_r)$ will be positive (assuming $x_t > 0$), indicating systematic over-prediction.  \n4. The term $d_r$ standardizes the residual, ensuring comparability across observations.",
    "question_context": "The intuitive basis for considering the recursive residuals in order to study parameter instability is that each residual w, represents the discrepancy (standardized) between the actual value of the dependent variable at time $r$ and an optimal forecast using only the sample information contained in the $r-1$ previous observations. If a structural shift in the regression coefficients takes place at time r, we expect to observe larger forecast errors starting at time $\\pmb{r}$ and a tendency for a while to either over-predict or under-predict (assuming another opposite structural shift does not take place immediately after).\nIn order to see more precisely how the behaviour of the recursive residuals is affected by parameter instability, let us consider again the general Brown et al. (1975) set-up, \n\n$$\n\\begin{array}{r l}&{y_{t}=x_{t}^{\\prime}\\pmb{\\beta}_{t}+u_{t}}\\ &{\\qquad}\\ &{u_{t}\\overset{\\mathrm{ind}}{\\sim}N[0,\\sigma_{t}^{2}],\\qquadt=1,...,T,}\\end{array}\n$$  \n\nwhere the coefficient vectors $\\beta_{t},t=1,...,T,$ are considered non-stochastic, and let us rewrite \n\n$$\n{\\pmb y}={\\pmb m}+{\\pmb u},\n$$  \n\nwhere \n\n$$\n\\pmb{m}=(\\pmb{x}_{1}^{\\prime}\\pmb{\\beta}_{1},...,\\pmb{x}_{T}^{\\prime}\\pmb{\\beta}_{T})^{\\prime}.\n$$  \n\nThen \n\n$$\nw=C y=C m+C u,\n$$  \n\nwhere $c$ is the matrix given in (21), and \n\n$$\nE(w)=C m.\n$$\n\nThis section examines the behavior of recursive residuals under parameter instability, contrasting it with the null hypothesis of parameter stability. It explores how structural shifts in regression coefficients affect forecast errors and the systematic tendencies in over-prediction or under-prediction."
  },
  {
    "qid": "econ-empirical-79-5-1-1",
    "question": "4) Explain why insurers might still invest in adherence programs for certain treatments (e.g., flu shots) despite general underinvestment in counteracting behavioral hazard. Reference the model's assumptions about time horizons and enrollee tenure.",
    "gold_answer": "Insurers invest in adherence for treatments with short-term cost savings (e.g., flu shots) because:\\n1. The benefits accrue within enrollees' tenure, aligning with insurer incentives.\\n2. Long-term health gains (e.g., diabetes management) may not financially benefit insurers due to enrollee turnover.\\n3. The model assumes limited time horizons, making insurers focus on immediate cost reductions.",
    "question_context": "We show how even though optimal insurance could reduce behavioral hazard, market-provided insurance may not. If enrollees were sophisticated, perfectly predicting their behavioral hazard and insuring accordingly, then marketprovided insurance would be optimally designed to counteract behavioral hazard. But with naive enrollees, insurers have less incentive to mitigate underuse since naive consumers will not fully value copays designed to counteract their biases.\n\nGiven the welfare benefits of counteracting behavioral hazard, one question is why existing plans do not seem to do so. Market-provided insurance may not counteract behavioral hazard due to enrollee naivety or insurer incentives."
  },
  {
    "qid": "econ-empirical-1289-4-0-0",
    "question": "1) Derive the conditions under which intraindustry trade arises in the North-South model, considering income distribution and quality differentiation.",
    "gold_answer": "1. **Consumer Demand**: Consumers with income \\( y \\) demand quality \\( q \\) such that \\( q = \\alpha y \\), where \\( \\alpha \\) is a preference parameter.  \n2. **Production Range**: The North produces qualities \\( q_N \\in [q_{N}^{min}, q_{N}^{max}] \\), and the South produces \\( q_S \\in [q_{S}^{min}, q_{S}^{max}] \\).  \n3. **Mismatch**: Intraindustry trade arises when \\( q_N \\neq q_S \\) for some income levels, leading to imports/exports of overlapping qualities.",
    "question_context": "Intraindustry trade arises because consumers with different incomes demand different quality products, and the range of produced qualities does not precisely match the demanded range. The South exports low quality, low cost varieties, while the North exports high quality, high cost varieties.\nEqualization of the income distribution in the South shifts income from consumers purchasing high-quality manufactures (produced in the North) to those purchasing low-quality manufactures (produced in the South), lowering the wage rate in the North and decreasing the share of intraindustry trade.\nFaster population growth in the South leads to a secular increase in intraindustry trade and a product cycle in middle-range quality products, but does not change the pattern of trade at the lower and upper ends of the product spectrum.\nA higher rate of technical progress in the South narrows the income per capita gap with the North, leading to a decline in the North's wage rate and a switch in production and trade patterns. The South becomes more competitive in differentiated products, while the North may eventually specialize in homogeneous goods.\n\nThe model of North-South trade incorporates vertical product differentiation, explaining trade dynamics such as the introduction of new high-quality products and the disappearance of old low-quality products, as well as the product cycle where less developed countries begin producing varieties previously made in advanced countries."
  },
  {
    "qid": "econ-empirical-708-1-1-0",
    "question": "5) Derive the adaptive SOFAR estimator and explain the role of the weighting matrix $\\mathbf{W}$. How does it improve upon the initial SOFAR estimator?",
    "gold_answer": "1. The adaptive SOFAR estimator minimizes $\\frac{1}{2}\\|\\mathbf{X} - \\mathbf{F}\\mathbf{B}^{\\prime}\\|_{\\mathrm{F}}^{2} + \\eta\\|\\mathbf{W} \\circ \\mathbf{B}\\|_{1}$, where $\\mathbf{W}$ is a weighting matrix.\n2. The weights $w_{ik} = 1/|\\hat{b}_{ik}^{\\text{ini}}|$ are derived from an initial estimator (e.g., PC estimator).\n3. This adaptive penalty ensures that larger loadings are penalized less, improving sparsity recovery and estimation accuracy.",
    "question_context": "The SOFAR estimator is defined as $(\\widehat{\\mathbf{F}},\\widehat{\\mathbf{B}})=\\operatorname{argmin}_{(\\mathbf{F},\\mathbf{B})\\in\\mathbb{R}^{T\\times\\widehat{r}}\\times\\mathbb{R}^{N\\times\\widehat{r}}}\\left\\{\\frac{1}{2}\\left\\|\\mathbf{X}-\\mathbf{FB}^{\\prime}\\right\\|_{\\mathrm{F}}^{2}+\\eta\\left\\|\\mathbf{B}\\right\\|_{1}\\right\\}$ subject to $\\mathbf{F^{\\prime}F}/T=\\mathbf{I}_{\\hat{r}}$ and $\\mathbf{B}^{\\prime}\\mathbf{B}$ diagonal.\nThe adaptive SOFAR estimator is defined as a minimizer of the second-stage weighted SOFAR problem: $(\\widehat{\\mathbf{F}}^{\\mathrm{ada}},\\widehat{\\mathbf{B}}^{\\mathrm{ada}})=\\operatorname*{argmin}_{(\\mathbf{F},\\mathbf{B})\\in\\mathbb{R}^{T\\times\\widehat{r}}\\times\\mathbb{R}^{N\\times\\widehat{r}}}\\left\\{\\frac{1}{2}\\left\\|\\mathbf{X}-\\mathbf{F}\\mathbf{B}^{\\prime}\\right\\|_{\\mathrm{F}}^{2}+\\eta\\left\\|\\mathbf{W}\\circ\\mathbf{B}\\right\\|_{1}\\right\\}$ subject to $\\mathbf{F}^{\\prime}\\mathbf{F}/T=\\mathbf{I}_{\\hat{r}}$ and $\\mathbf{B}^{\\prime}\\mathbf{B}$ diagonal.\nTheorem 1 states that for any finite integer $k_{\\operatorname*{max}} > r,$ the $k$ th largest eigenvalue of $(N\\vee T)^{-1}\\mathbf{X}\\mathbf{X}^{\\prime}$ satisfies $\\lambda_{k}\\left\\{\\begin{array}{l l}{\\gtrsim\\displaystyle\\frac{N_{k}T}{N\\vee T}}&{\\mathrm{~for~}k\\in\\{1,\\dots,r\\},}}\\ {{=O(1)}}&{\\mathrm{~for~}k\\in\\{r+1,\\dots,k_{\\operatorname*{max}}\\},}}\\end{array}\\right.$ with probability at least $1-O((N\\vee T)^{-\\nu})$.\n\nThe text discusses the SOFAR and adaptive SOFAR estimators, their theoretical properties, and the determination of the number of weak factors. It also covers the asymptotic behavior of eigenvalues and the consistency of the estimators."
  },
  {
    "qid": "econ-empirical-822-1-0-1",
    "question": "2) Prove part (a) of Theorem 1: No pair $(X^{A}, X^{B})$ with $X^{i} \\in H_{m^{i}}$ ($0 < m^{i} \\leq n-1$) is an equilibrium under (A1)-(A3) and (BC).",
    "gold_answer": "1. **Assume** both carriers operate hub-spoke networks of size $m^{i}$. \\n2. **By (BC)**: Profits in overlapping markets are zero ($\\pi(z,z) = 0$). \\n3. **Fixed Costs**: $F(m^{i}) > 0$ must be covered, but profits are zero. \\n4. **Contradiction**: No network size $m^{i}$ satisfies profit non-negativity, hence no equilibrium exists.",
    "question_context": "In our model, path length measures travel costs to both the carrier and the traveler. Thus, a carrier has an advantage only if it offers a shorter path. Formally, this condition can be stated as \n\n$$ \n\\pi(z,y)=0\\qquad{\\mathrm{if~}}z\\geq y. \n$$\nUnder the assumption that consumers have the same willingness to pay for quality and do not care about the identity of the airline, the second-stage game in each city-pair market is essentially a Bertrand game. In equilibrium, if both carriers offer the same “quality,” each carrier will price at marginal cost and earn zero profits.\nTHEOREM 1: Suppose $(A1)–(A3)$ and $(B C)$ hold. \n\n(a) No pair $(X^{A},X^{B})$ such that $X^{i}\\in H_{m^{i}}$ ， $0<m^{i}\\leq n-1,i=A,B,$ is an equilibrium. (b) If $X^{A}\\in H_{n-1}$ and $X^{B}=\\phi$ or $X^{B}\\in H_{n-1}$ and $X^{A}=\\phi$ then $(X^{A},X^{B})$ is an equilibrium.\n\nThis section analyzes the effects of aggressive price competition on network choice, focusing on scenarios where equilibrium profits are positive only if a carrier has an advantage over its rival. The model uses path length as a measure of travel costs, implying a carrier's advantage is tied to offering a shorter path."
  },
  {
    "qid": "econ-empirical-303-3-0-2",
    "question": "3) Compute the standardized residual $\\hat{s}_{t}$ for a given time period using the formula: $$ \\hat{s}_{t}=\\frac{\\hat{\\varepsilon}_{t}-\\hat{\\phi}_{1}\\hat{\\varepsilon}_{t-1}-\\hat{\\phi}_{2}\\hat{\\varepsilon}_{t-2}}{(\\hat{h}_{t})^{1/2}}, $$ assuming $\\hat{\\varepsilon}_{t}=0.05$, $\\hat{\\varepsilon}_{t-1}=0.03$, $\\hat{\\varepsilon}_{t-2}=0.02$, $\\hat{\\phi}_{1}=0.27$, $\\hat{\\phi}_{2}=0.33$, and $\\hat{h}_{t}=0.0001$.",
    "gold_answer": "1. Substitute the given values into the formula: $$ \\hat{s}_{t}=\\frac{0.05 - 0.27 \\times 0.03 - 0.33 \\times 0.02}{\\sqrt{0.0001}}. $$ 2. Calculate the numerator: $$ 0.05 - 0.0081 - 0.0066 = 0.0353. $$ 3. Divide by the denominator: $$ \\hat{s}_{t}=\\frac{0.0353}{0.01} = 3.53. $$",
    "question_context": "The underlying model for testing the unbiasedness of forecasts is given by: $$ y_{t}=\\beta_{1}+\\beta_{2}x_{t}+\\varepsilon_{t}, $$ where $y_{t}$ is the actual rate of inflation and $x_{t}$ is the expected rate. Forecasts are said to be unbiased if $H_{0}$ $\\beta_{1}=0,\\beta_{2}=1$ is true.\nThe MLE's for different combinations of ARCH and AR through order 2 are given in Table 1. The AIC decreases from -420.9 to -427.4 when both AR and ARCH are included simultaneously in model (7).\nThe stationarity condition for the $\\mathbf{ARCH}(\\gamma_{2})+\\mathbf{AR}(\\phi_{1},\\phi_{2})$ formulation is $\\omega(\\phi)\\cdot\\gamma_{2}<1$, where $$ \\omega(\\phi)={\\frac{1-\\phi_{2}^{2}}{1-\\phi_{1}^{2}-2\\phi_{1}^{2}\\phi_{2}-2\\phi_{2}^{2}-\\phi_{1}^{2}\\phi_{2}^{2}+\\phi_{2}^{4}}}. $$\nThe standardized residuals are computed as: $$ \\hat{s}_{t}=\\frac{\\hat{\\varepsilon}_{t}-\\hat{\\phi}_{1}\\hat{\\varepsilon}_{t-1}-\\hat{\\phi}_{2}\\hat{\\varepsilon}_{t-2}}{(\\hat{h}_{t})^{1/2}}. $$\n\nThis section illustrates the methodology for testing the unbiasedness of experts' forecasts using Livingston survey data, focusing on the interaction between ARCH and autocorrelation in economic time series."
  },
  {
    "qid": "econ-empirical-1110-1-2-1",
    "question": "8) What potential biases could arise from using CPS-ORG data, and how does the author mitigate them?",
    "gold_answer": "1. **Non-Independence**: Repeated observations per individual are addressed by clustering standard errors.\n2. **Selection Bias**: Restricts analysis to non-elderly men (18-64) to homogenize the sample and reduce confounding.",
    "question_context": "I use the Current Population Survey merged outgoing rotation groups files (CPS-ORG) for 1996-2002 to conduct the analysis.\nAn advantage of the CPS-ORG is that it provides relatively large sample sizes, which are important given my interest in a narrowly defined population.\n\nThis section describes the data sources and sample construction for the empirical analysis, highlighting the advantages of the CPS-ORG dataset."
  },
  {
    "qid": "econ-empirical-694-3-0-3",
    "question": "4) Formalize the 'peer pressure' mechanism described in the terminal period as a repeated game with a finite horizon. How does the shadow of the future affect equilibrium strategies?",
    "gold_answer": "Let the game have $T$ periods:\n1. **Payoff structure**: For any player $i$, the terminal payoff is $u_i = \\sum_{t=1}^T \\delta^{t-1} \\pi_i^t$, where $\\delta$ is the discount factor.\n2. **Peer pressure as a trigger strategy**: Defection in period $t$ triggers punishment in $t+1,...,T$.\n3. **Equilibrium condition**: Cooperation is sustainable if:\n   $$\\pi_i^{Defect} + \\sum_{k=t+1}^T \\delta^{k-t} \\pi_i^{Punishment} < \\sum_{k=t}^T \\delta^{k-t} \\pi_i^{Cooperate}$$\n   For $t=T$, the condition collapses to $\\pi_i^{Cooperate} > \\pi_i^{Defect}$, explaining the 'peanuts' rationale.",
    "question_context": "In period 6 of this experiment neither buyers nor sellers signed an explicit agreement form. Instead, each 'side' viewed Phase 2 as providing an opportunity for cartel formation. The discussions preceding trading were relatively brief (only about 8 minutes), and focused on what price to aim for and which agent was to be the spokesman. Trading opened with a bid of $2.60, counter offers at $123,456 and $2.30, a reply bid of $2.58 and a final counter offer of $212. Neither side appeared willing to reduce the bid-ask spread.\nIn period 7 there was again virtually no mention of the possibility of coordinated action by both sides, let alone any concern by either side for the externality they were generating. The focus, yet again, was on bilateral cartel strength and strategy. In this case each side appeared to be less rigid in their instructions to negotiators, with these two establishing a price of $2.56 in relatively short order.\nFinally in period 8 the two (frustrated) negotiators in the two previous periods managed to convince their respective partners to cooperate in Coasian fashion in order to extract the available surplus and then for each side to worry about how to divide their 50% share of that surplus.\n\nThe text discusses bargaining behavior in experimental settings, focusing on cartel formation and price negotiations between buyers and sellers. It highlights specific periods where negotiations broke down or succeeded, providing insights into group dynamics and strategic behavior."
  },
  {
    "qid": "econ-empirical-654-1-0-0",
    "question": "1) Derive the condition under which $Y_{t i}$ has finite variance in the HL model, given the assumptions on $u_{t1}$ and $u_{t i}, i=2,\\ldots,n$.",
    "gold_answer": "To ensure $Y_{t i}$ has finite variance, the influence of the heavy-tailed shock $u_{t1}$ must be dampened. From Assumption HL(ii), the coefficients are scaled as $A_{i1,T}^{(h)} = a_{i1}^{(h)} / T^{\\theta}$ and $B_{i1,T} = b_{i1} / T^{\\theta}$, where $\\theta = \\frac{1}{\\alpha} - \\frac{1}{2}$. This scaling ensures that the cumulative effect of $u_{t1}$ on $Y_{t i}$ does not dominate, allowing $Y_{t i}$ to have finite variance despite $u_{t1}$ having infinite variance.",
    "question_context": "Our goal is a model in which (i) a heavy tailed shock $u_{t1}$ co-exists with light tailed shocks $u_{t i}$ , $i=2,\\ldots,n$ and (ii) $Y_{t i}$ is influenced by the current and past values of $u_{t1}$ but not dominated by them in a sense to be made precise. We consider the HL (heavy–light) model derived from the ${\\mathsf{S V A R}}(p)$ \n\n$$\nY_{t}=A_{1}Y_{t-1}+\\cdot\\cdot\\cdot+A_{p}Y_{t-p}+B u_{t},\n$$ \n\nwhere for each $h=1,\\ldots,p,A_{h}$ is a $n\\times n$ matrix with $(i,j)$ th entry denoted $[A_{i j}^{(h)}]$ , the coefficient of variable $j$ at lag $h$ in equation i. The entries $[B_{i j}]$ of the $n\\times n$ matrix $B$ are similarly defined.\ni. The sequence of $n$ -dimension random vectors $\\{u_{t}\\}$ is iid and the components, $u_{t i},i=1,\\dots,n$ are also independent. The $u_{t1}$ will have Pareto-like tails with index $1<\\alpha<2$ and $\\mathbb E[u_{t1}]=0$ , while the remaining shocks $u_{t i},i=2\\ldots,n$ will have thin tails with mean zero and variance 1.\nii The coefficient matrices $A_{h}$ for $h=1,\\ldots,p$ and the matrix $B$ will satisfy the following conditions. \n\n$$\n\\begin{array}{l l}{{A_{i1,T}^{(h)}=a_{i1}^{(h)}/T^{\\theta},}}&{{i=2,...,n,}}\\ {{}}&{{B_{i1,T}=b_{i1}/T^{\\theta},}}&{{i=2,...,n,}}\\end{array}\n$$ \n\nwith \n\n$$\n\\theta=\\frac{1}{\\alpha}-\\frac{1}{2}.\n$$\nThe structural moving-average representation of the model is \n\n$$\n\\begin{array}{r}{Y_{t}=\\varPhi(L)B u_{t}=\\varPsi(L)u_{t}\\qquad}\\ {=\\psi_{0}u_{t}+\\psi_{1}u_{t-1}+\\cdot\\cdot\\cdot,}\\end{array}\n$$ \n\nwhere $\\psi(L)=A(L)^{-1}B$ and $\\psi_{0}=B$ . The effects of $u_{t1}$ on $Y_{t+h,2}$ are given by the first column of $\\psi_{h}$ which depends on A and B. Hence to estimate the dynamic causal effects of $u_{t1}$ , we also need to be able to consistently estimate $B$ when $u_{1t}$ has infinite variance.\nThe relationship between the vector of primitive shocks $u$ and error terms is \n\n$$\ne_{t}=B u_{t}\\quad\\mathrm{and}\\quad u_{t}=W e_{t}\n$$ \n\nwhere $u_{t}=(u_{t1},\\ldots,u_{t n})^{\\prime}$ is an $n$ -vector consisting of independent random variables with mean zero and $B$ is an $n\\times n$ matrix with inverse $W$ . As is well known, $B$ is not uniquely identified from the second moments of $e_{t}$ alone even when $e_{t}$ has finite variance because $B Q^{\\prime}Q u_{t}$ has the same covariance structure as $B u_{t}$ for any orthonormal matrix $Q$ .\nLemma 2. Let $e=B u$ , where $u$ is a $n\\times1$ vector of mutually independent components of which at most one is Gaussian and $B$ is an $n\\times n$ invertible matrix with inverse $W=B^{-1}$ . If the components of $\\hat{u}=\\hat{W}e$ are pairwise independent, where $\\hat{W}$ is an invertible matrix, then $\\hat{W}=P\\varLambda W$ where $P$ is a permutation matrix and $\\varLambda$ is a diagonal matrix. Further, the components of $\\hat{u}$ must be mutually independent.\nThe distance covariance between two random vectors $X$ and $Y$ of dimensions $m$ and $n$ , respectively, is \n\n$$\n\\mathcal{Z}(X,Y;w)=\\int_{\\mathbb{R}^{m+n}}\\left|\\varphi_{X,Y}(s,t)-\\varphi_{X}(s)\\varphi_{Y}(t)\\right|^{2}w(s,t)d s d t,\n$$ \n\nwhere $w(s,t)>0$ is a weight function and $\\varphi_{Z}(t)=E[\\exp^{i(t,Z)}]$ , $t\\in\\mathbb{R}^{d}$ denotes the characteristic function for any random vector $Z\\in\\mathbb{R}^{d}$ . The most commonly used weight function, which we will also adopt here, is \n\n$$\nw(s,t)=\\left(c_{m,\\beta}|s|^{\\beta+m}c_{n,\\beta}|t|^{\\beta+n}\\right)^{-1},\n$$ \n\nwhere $\\begin{array}{r l r}{\\beta}&{\\in}&{(0,2),c_{m,\\beta}=\\frac{2\\pi m/2\\Gamma(1-\\beta/2)}{\\beta2^{\\beta}\\Gamma((\\beta+m)/2)}}\\end{array}$ 2βπ2βm/Γ 2((Γβ (1−mβ)//22)) (see Székely et al. (2007)). The integral in (10) is then finite provided $E|X|^{\\beta}+E|Y|^{\\beta}<\\infty$ . Under this moment assumption, one sees immediately that $X$ and $Y$ are independent if and only if $\\mathcal{I}(X,Y;w)=0$ since in this case the joint characteristic function factors into the product of the respective marginal characteristic functions, $\\varphi_{X,Y}(s,t)=\\varphi_{X}(s)\\varphi_{Y}(t)$ for all $(s,t)\\in\\mathbb{R}^{m+n}$ .\n\nThe section introduces a VAR model with mixed heavy and light tailed shocks, focusing on identification and estimation challenges."
  },
  {
    "qid": "econ-empirical-1732-1-0-3",
    "question": "4) Explain why the contingent management fee $\\alpha_{i k}$ affects the household's net payoff from investing in the $i$th mutual fund, and derive the decomposition into variable and fixed parts.",
    "gold_answer": "The household's net payoff is $(1 - \\alpha_{i k})Z_{i k} - P_{i k}$. Substituting $Z_{i k} = P_{i k} + \\gamma_{i k}(X - P_{x})$, this becomes $(1 - \\alpha_{i k})\\gamma_{i k}(X - P_{x}) - \\alpha_{i k}P_{i k}$.\n- Variable part: $(1 - \\alpha_{i k})\\gamma_{i k}(X - P_{x})$ represents the household's share of the fund's risky asset bet.\n- Fixed part: $\\alpha_{i k}P_{i k}$ is the total fee paid to the manager. Thus, $\\alpha_{i k}$ directly scales both the risk exposure and the fee.",
    "question_context": "The ith manager in group $k$ has the utility function $u(W_{i k})=$ $-\\exp(-\\tau W_{i k})$ , where $\\tau$ is the common risk aversion parameter and $W_{i k}$ is the agent’s date 3 wealth.\nThe risky asset’s price per share at date 2 is $P_{x}$ and its payoff per share at date 3 is $X$ , where $X$ is normally distributed with mean $\\mu_{x}$ and variance $\\sigma_{x}^{2}$ . We use the shorthand notation $X\\sim\\mathcal N(\\mu_{x},\\sigma_{x}^{2})$ . The per capita supply of the risky asset is $U\\overset{\\cdot\\cdot}{\\sim}\\mathcal N(\\mu_{u},\\sigma_{u}^{2})$ , which is interpreted as noise trading in the economy.\nAll agents have rational expectations in the sense of Hellwig [30] and use the information revealed by price when forming their posterior beliefs. Following the literature, we solve for an equilibrium in which the risky asset price is an affine function of $X$ and $U$ , i.e., $P_{x}=a+b X-d U$ , where the coefficients $a,b$ , and $d$ are determined in equilibrium.\n\nThe agents are expected utility maximizers with CARA utility functions. The model assumes homogeneous risk aversion initially, with zero initial wealth for all agents. There are two primitive assets: a riskless asset and a risky asset with normally distributed payoffs. Mutual funds are endogenously formed, and all agents have rational expectations."
  },
  {
    "qid": "econ-empirical-770-0-1-0",
    "question": "5) Derive the equilibrium conditions for Proposition 6, focusing on the convergence of investment and trade probability to efficient levels.",
    "gold_answer": "1. **Stationary Equilibrium**: Restrict to stationary sequential equilibria for the no-gap case.  \n2. **Investment Convergence**: As \\(\\Delta \\to 0\\), buyer's investment \\(x \\to x^*\\).  \n3. **Trade Probability**: \\(\\text{Prob}\\{\\text{agreement by } \\epsilon\\} \\to \\text{Prob}\\{C < v(x^*)\\}\\).  \n4. **Efficiency**: \\(x^*\\) maximizes \\(S(x)\\), and seller extracts all surplus.",
    "question_context": "Proposition 6 shows that as the time between offers becomes arbitrarily small, the buyer's investment strategy converges to \\(x^*\\) and the probability of agreement by time \\(\\epsilon\\) converges to \\(\\text{Prob}\\{C < v(x^*)\\}\\).\nProposition 7 establishes that if \\(\\text{Prob}\\{C \\geq v(0)\\} = 1\\), the buyer invests 0 and both players receive 0 utility.\nThe result that unobservable investment and the information rents it creates may provide sufficient incentives for optimal investment appears robust to several extensions.\n\nThe paper explores extensions including uncertain gains from trade and two-sided investment. It also discusses the broader implications for organizational design, emphasizing the role of information allocation as a tool to mitigate the hold-up problem."
  },
  {
    "qid": "econ-empirical-46-1-0-3",
    "question": "4) Explain the recursive formulation of the firm's problem and derive the New Keynesian Phillips curve from the given equations.",
    "gold_answer": "1. The firm's problem is: $$V(P_{j},z^{t}) = \\max_{p_{j},y_{j},n_{j}} \\frac{p_{j}y_{j}}{P(z^{t})} - w(z^{t})n_{j}(z^{t}) - \\frac{\\kappa}{2}\\left[\\frac{p_{j}}{P_{j}(1+\\bar{\\pi})}-1\\right]^{2} + \\sum_{z^{t+1}}Q(z^{t+1}|z^{t})V(p_{j},z^{t+1}).$$\n2. The first-order condition with respect to $p_{j}$ yields the Phillips curve: $$\\tilde{\\pi}(z^{t}) = \\frac{1}{\\kappa(\\mu-1)}Y(z^{t})\\left[\\mu\\frac{w(z^{t})}{A(z_{t})}-1\\right] + \\sum_{z^{t+1}}Q(z^{t+1}|z^{t})\\tilde{\\pi}(z^{t+1}).$$",
    "question_context": "Households are infinitely lived and have preferences over consumption, $c(s^{t})$ , and hours worked, $l(s^{t})$ , given by $$\\sum_{t=0}^{\\infty}\\sum_{s^{t}}\\beta^{t}\\mathrm{Pr}(s^{t}|s_{0})\\tilde{\\theta}(z^{t})U(c(s^{t}),l(s^{t})),$$ where $\\beta$ is the discount factor and $\\tilde{\\theta}(z^{t})$ is a shock to the marginal utility of consumption and disutility of labor defined recursively as $\\tilde{\\theta}(z^{t+1})=\\theta({z_{t+1}})\\tilde{\\theta}(z^{t})$.\nThe final good is produced combining differentiated intermediate goods according to the technology $$Y(z^{t})=\\left(\\int_{0}^{1}y_{j}(z^{t})^{\\frac{1}{\\mu}}d j\\right)^{\\mu},$$ where $\\mu$ is related to the (constant) elasticity of substitution across varieties, $\\varepsilon$ , by the following, $\\textstyle\\mu={\\frac{\\varepsilon}{\\varepsilon-1}}$.\nThe condition for the optimality of labor supply is $$\\chi l(s^{t})^{\\psi}=w(z^{t})e(\\upsilon_{t})c(s^{t})^{-\\sigma},$$ where $\\begin{array}{r}{w(z^{t})=\\frac{W(z^{t})}{P(z^{t})}}\\end{array}$ is the real wage per efficiency unit.\nThe firm’s problem can be written recursively as $$\\begin{array}{l}{{V(P_{j},z^{t})=\\displaystyle\\operatorname*{max}_{p_{j},y_{j},n_{j}}\\frac{p_{j}y_{j}}{P\\left(z^{t}\\right)}-w(z^{t})n_{j}(z^{t})-\\displaystyle\\frac{\\kappa}{2}\\left[\\frac{p_{j}}{P_{j}(1+\\bar{\\pi})}-1\\right]^{2}}}\\ {{\\mathrm{~}+\\displaystyle\\sum_{z^{t+1}}Q(z^{t+1}|z^{t})V(p_{j},z^{t+1}),}}\\end{array}$$ subject to the production function (4) and the demand function (9).\n\nThis section introduces a class of New Keynesian models with heterogeneous households, detailing their preferences, technology, market structure, and monetary policy. The models differ in households' decision problems, including cyclicality of idiosyncratic risk, asset trading, financial constraints, and fiscal transfers."
  },
  {
    "qid": "econ-empirical-821-2-1-1",
    "question": "4) Discuss the implications of private contracts on the manufacturer's ability to commit to wholesale prices.",
    "gold_answer": "Private contracts imply:\n\n1. **No commitment power**: Manufacturers cannot credibly commit to wholesale prices because contracts are unobservable.\n2. **Secret discounts**: Manufacturers may deviate from equilibrium contracts without detection.\n3. **Belief dependence**: Retailers' beliefs about competitors' contracts become critical for equilibrium outcomes.",
    "question_context": "With vertical separation, $M_{i}$ offers a two-part tariff contract $C_{i}=\\left(w_{i},T_{i}\\right)$ to $R_{i},$ specifying a wholesale price $w_{i}\\in\\mathbb{R}_{+}$ and a franchise fee $T_{i}\\in\\mathbb{R}$.\nWe assume that contracts are private, so that a retailer cannot observe the contract offered to his competitor.\n\nManufacturers offer two-part tariff contracts to retailers under vertical separation, with private contracts preventing observable deviations."
  },
  {
    "qid": "econ-empirical-642-2-3-0",
    "question": "1) Explain how the asymmetric conjugate prior helps mitigate the curse of dimensionality in the 15-variable VAR.",
    "gold_answer": "1. Provides structured shrinkage:\n   - Different shrinkage for own lags vs. other lags\n   - Adaptive to data characteristics\n2. Reduces effective parameter space\n3. Maintains computational feasibility:\n   - Enables analytical posterior computation\n   - Efficient sampling despite high dimensions\n4. Data-adaptive shrinkage prevents overfitting",
    "question_context": "The empirical study uses a 15-variable VAR with US quarterly data from 1985:Q1 to 2019:Q4.\nThe asymmetric conjugate prior helps address the curse of dimensionality in the large VAR system.\nUsing more variables helps address concerns about nonfundamentalness and provides sharper inference.\n\nThis section applies the methodology to identify financial shocks in a 15-variable VAR using sign restrictions."
  },
  {
    "qid": "econ-empirical-1623-0-0-3",
    "question": "4) Derive the sampling joint density \\(h^{\\ast}(y^{\\ast},x,s^{\\ast})\\) for the observable variables \\((Y^{*},X,S^{*})\\) in the presence of misclassification and choice-based sampling.",
    "gold_answer": "The sampling joint density is derived as follows:\n1. Start with the definition: \\(h^{\\ast}(y^{\\ast},x,s^{\\ast}) = \\frac{H_{s^{\\ast}}^{\\ast}}{\\mathcal{Q}_{s^{\\ast}}^{\\ast}}\\mathrm{Pr}^{\\ast}(y^{\\ast}|x,\theta)f(x)\\).\n2. Substitute \\(\\mathrm{Pr}^{\\ast}(y^{\\ast}|x,\theta,\\alpha) = \\sum_{y\\in\\mathcal{Y}}\\alpha_{y^{\\ast}y}\\mathrm{Pr}(y|x,\theta)\\) and \\(\\mathcal{Q}_{s^{\\ast}}^{\\ast} = \\sum_{y^{\\ast}\\in\\mathcal{Y}_{s^{\\ast}}^{\\ast}}\\sum_{y\\in\\mathcal{Y}}\\alpha_{y^{\\ast}y}\\mathcal{Q}_{y}\\).\n3. The final expression is: \\(h^{\\ast}(y^{\\ast},x,s^{\\ast}) = \\frac{H_{s^{\\ast}}^{\\ast}}{\\sum_{y^{\\ast}\\in\\mathcal{Y}_{s^{\\ast}}^{\\ast}}\\sum_{y\\in\\mathcal{Y}}\\alpha_{y^{\\ast}y}\\mathcal{Q}_{y}}\\sum_{y\\in\\mathcal{Y}}\\alpha_{y^{\\ast}y}\\mathrm{Pr}(y|x,\theta)f(x)\\).",
    "question_context": "The contaminated data sampling distribution is written as a function of the error-free conditional distribution of the dependent variable given the covariates and the conditional misclassification probabilities of the observable variable of interest given its latent values.\nThe conditional probability of observing the response \\(Y^{*}\\) given the latent outcome \\(Y\\) is defined as \\(\\operatorname*{Pr}(Y^{*}=y^{*}|Y=y,x)=\\operatorname*{Pr}(Y^{*}=y^{*}|Y=y)=\\alpha_{y^{*}y}\\), where \\(0\\leqslant\\alpha_{y^{*}y}\\leqslant1\\) and \\(\textstyle\\sum_{y^{*}\\in{\\mathcal{Y}}^{*}}{\\mathcal{U}}_{y^{*}y}=1\\) for all \\(y^{*},y\\).\nThe contaminated population features of (5) may be obtained straightforwardly. The conditional probability of \\(Y^{*}=y^{*}\\) given \\(X=x\\) is \\(\\mathrm{Pr}^{*}(y^{*}|x,\theta,\\alpha)=\\sum_{y\\in\\mathcal{Y}}\\alpha_{y^{*}y}\\mathrm{Pr}(y|x,\theta)\\).\nThe marginal probability of observing an unit from stratum \\(\\mathcal{C}_{s^{*}}^{*}\\) is \\(Q_{s^{*}}^{*}=\\sum_{y^{*}\\in\\mathcal{Y}_{s^{*}}^{*}}Q_{y^{*}}^{*}\\), where \\(\\mathcal{Q}_{y^{*}}^{*}\\) is the distorted marginal probability of choice \\(Y^{*}=y^{*}\\): \\(Q_{y^{*}}^{*}=\\displaystyle\\int_{\\mathcal{X}}\\sum_{y\\in\\mathcal{Y}}\\alpha_{y^{*}y}\\mathrm{Pr}(y|x,\theta)f(x)\\mathrm{d}x\\).\n\nThe paper presents a general framework for handling misclassification in the response variable within choice-based samples. It extends Imbens' efficient generalized method of moments (GMM) to estimate models with misclassification and proposes a specification test to detect such measurement errors."
  },
  {
    "qid": "econ-empirical-573-3-3-1",
    "question": "8) Analyze how removing financial frictions reduces inequality in assets, consumption, and earnings autocorrelation.",
    "gold_answer": "1. **Assets**: Gini falls from $0.91$ to $0.84$ due to better capital allocation.  \n2. **Consumption**: Gini falls from $0.60$ to $0.56$ as skill returns become more equitable.  \n3. **Earnings Autocorrelation**: Drops from $0.89$ to $0.80$ because financial constraints no longer perpetuate income persistence.",
    "question_context": "The elimination of financial frictions leads to a reduction of the Gini coefficient in assets from 0.91 to 0.84 and decreases the Gini coefficient of consumption from 0.60 to 0.56.\nThe variation in skill returns explains $20\\%$ of the income variance in the baseline economy but $64\\%$ in the economy with no financial frictions.\nCapital income accounts for $31\\%$ of the income variance in the baseline economy but only $12\\%$ in the perfect capital markets (PCM) economy.\n\nThe elimination of financial frictions reduces inequality and changes the sources of income variance in the baseline and standard model economies."
  },
  {
    "qid": "econ-empirical-1419-0-0-0",
    "question": "1) Derive the Bayesian updating rule for the continuous specification with Gaussian priors and signals. How does the posterior distribution of plan quality depend on the prior and the signal?",
    "gold_answer": "1. Let the prior belief about plan quality be normally distributed: \\( q \\sim N(\\mu_0, \\sigma_0^2) \\).\n2. The signal from the report card is also normally distributed: \\( s \\sim N(q, \\sigma_s^2) \\).\n3. The posterior distribution of quality given the signal is: \\( q|s \\sim N(\\mu_1, \\sigma_1^2) \\), where:\n   - \\( \\mu_1 = \\frac{\\sigma_s^2 \\mu_0 + \\sigma_0^2 s}{\\sigma_0^2 + \\sigma_s^2} \\)\n   - \\( \\sigma_1^2 = \\frac{\\sigma_0^2 \\sigma_s^2}{\\sigma_0^2 + \\sigma_s^2} \\).",
    "question_context": "We model both continuous specifications with Gaussian priors and signals, and discrete specifications with Beta priors and Binomial signals.\nWe estimate the parameters of the model with simulated maximum likelihood, and use the estimates to quantify the value of the report card information.\nConsumers were willing to pay about $330 per year per below expected performance rating avoided, and the average value of the report card per employee was about $20 per year.\n\nThe paper develops a Bayesian learning model to analyze the impact of health plan report cards on employee choices. It employs both continuous (Gaussian priors and signals) and discrete (Beta priors and Binomial signals) specifications to estimate the value of information."
  },
  {
    "qid": "econ-empirical-297-1-2-1",
    "question": "6) Explain how nonlinear covariance estimators can resolve the ambiguity in identifying causal and noncausal dimensions.",
    "gold_answer": "1. Second-order moments alone cannot distinguish between $(n_1, A_1, J_1)$ and $(n_1, A_1, J_1^{-1})$. \n2. Nonlinear covariances, such as $Cov(Y_t, Y_{t-h}^2)$, can break this symmetry. \n3. For example, noncausal components exhibit nonlinear dependencies that causal components do not. \n4. By incorporating these nonlinearities, estimators can uniquely identify the causal and noncausal dimensions.",
    "question_context": "Proposition 6: (i) The product of the causal and noncausal dimensions equal to $n_1(n - n_1)$ is identifiable at second-order, except on a set of $\\boldsymbol{\\Sigma}$ of measure zero. (ii) For a given pair of dimensions $[n_{10}, n - n_{10}]$, the pair of causal/noncausal spaces can be second-order identifiable.\n\nThis section introduces nonlinear covariance estimators to address the limitations of second-order identification in mixed VAR(1) processes."
  },
  {
    "qid": "econ-empirical-762-1-0-3",
    "question": "4) The authors link eclipse exposure to technology via 'strategic games' and 'writing systems.' Formalize this argument as a directed acyclic graph (DAG), explicitly showing the causal pathway from eclipses \\( E \\) to technology \\( T \\) via curiosity \\( C \\) and human capital \\( H \\).",
    "gold_answer": "1. **DAG Structure**: \\( E \\rightarrow C \\rightarrow H \\rightarrow T \\).  \n2. **Mediation**: \\( C \\) and \\( H \\) are sequential mediators.  \n3. **Exclusion Restrictions**: No direct \\( E \\rightarrow T \\) or \\( E \\rightarrow H \\) paths.  \n4. **Empirical Test**: Use mediation analysis (Baron & Kenny, 1986) to estimate \\( \\beta_{E\\rightarrow C} \\cdot \\beta_{C\\rightarrow H} \\cdot \\beta_{H\\rightarrow T} \\).",
    "question_context": "We propose that exposure to inexplicable phenomena prompts curiosity and thinking in an attempt to comprehend these mysteries, thus raising human capital and technology, and, ultimately, fostering growth.\nWe focus on solar eclipses as one particular trigger of curiosity and empirically establish a robust relationship between their number and several proxies of economic prosperity.\nWe also offer evidence compatible with the human capital and technological increases we postulate, finding a more intricate thinking process and more developed technology among societies more exposed to solar eclipses.\n\nThis paper relates curiosity to economic development through its impact on human capital formation and technological advancement in pre-modern times. The authors focus on solar eclipses as a trigger of curiosity and empirically establish a relationship between their number and economic prosperity."
  },
  {
    "qid": "econ-empirical-1145-2-1-2",
    "question": "7) Why might welfare reform have a statistically insignificant effect on health insurance coverage for college-educated women, as opposed to low-skilled women?",
    "gold_answer": "1. **Baseline coverage**: College-educated women have high private coverage rates (89.8%), leaving little room for reform to increase coverage.\n2. **Labor market**: They are more likely to have stable, full-time jobs with employer-sponsored insurance, reducing reliance on welfare-linked policies.\n3. **Demographics**: Higher marriage rates (63.6%) provide alternative pathways to spousal coverage.",
    "question_context": "For women with less than a high school education, welfare waivers are associated with a 2.3 percentage point decline in the probability of being uninsured (p-value = 0.027). All of this decline in uninsurance is due to an increase in private, rather than public, coverage since waivers are associated with a 2.5 percentage point increase in the probability of having private insurance (p-value = 0.006).\nTANF reduced uninsurance among low-skilled women by 3.6 percentage points (p-value = 0.012), increasing private coverage by 2.9 percentage points although this result is not statistically significant.\n\nThis section examines the impact of welfare reform (waivers and TANF implementation) on health insurance coverage among women, stratified by educational attainment. The analysis uses state-level fixed effects and robust standard errors."
  },
  {
    "qid": "econ-empirical-435-1-0-3",
    "question": "4) Derive the sufficient condition for the invertibility of $S_{n}(\\lambda)$ using Neumann’s series. How does this relate to the spatial weight matrix $W_{n}$?",
    "gold_answer": "1. The spatial autoregressive matrix is $S_{n}(\\lambda) = I_{n} - \\lambda W_{n}$.\n2. For $S_{n}(\\lambda)$ to be invertible, the Neumann series $\\sum_{\\ell=0}^{\\infty}(\\lambda W_{n})^{\\ell}$ must converge.\n3. The series converges if $\\|\\lambda W_{n}\\| < 1$ for some matrix norm $\\|\\cdot\\|$.\n4. Thus, a sufficient condition is $|\\lambda| < \\frac{1}{\\|W_{n}\\|}$.\n5. This implies that the spatial dependence parameter $\\lambda$ must be bounded by the inverse of the norm of $W_{n}$.\n6. The condition ensures that the spatial spillover effects do not explode, maintaining the stability of the model.",
    "question_context": "The parameters for the model (1) are $\\theta = (\\delta^{\\prime}, \\lambda, \\alpha)^{\\prime}$ with $\\delta = (\\gamma, \\rho, \\beta^{\\prime})^{\\prime}, \\sigma^{2}$, $T_{n}$, and $F_{T}$. The sample averaged quasi-log likelihood function is given by:\n\n$$\nQ_{n T}(\\theta, \\sigma^{2}, T_{n}, F_{T}) = -\\frac{1}{2}\\log 2\\pi - \\frac{1}{2}\\log \\sigma^{2} + \\frac{1}{n}\\log |S_{n}(\\lambda)R_{n}(\\alpha)| - \\frac{1}{2\\sigma^{2}n T}\\sum_{t=1}^{T}(S_{n}(\\lambda)Y_{n t} - Z_{n t}\\delta - \\mathcal{T}_{n}f_{t})^{\\prime} R_{n}(\\alpha)^{\\prime}R_{n}(\\alpha) (S_{n}(\\lambda)Y_{n t} - Z_{n t}\\delta - \\mathcal{T}_{n}f_{t}).\n$$\nThe disturbances $\\varepsilon_{i t}$ are independently distributed across $i$ and over $t$ with $\\mathbb{E}\\varepsilon_{i t} = 0$, $\\mathbb{E}\\varepsilon_{i t}^{2} = \\bar{\\sigma}_{0}^{2} > 0$, and has uniformly bounded moment $\\mathbb{E}|\\varepsilon_{i t}|^{4+\\eta}$ for some $\\eta > 0$.\nThe spatial weights matrices $W_{n}$ and $\\tilde{W}_{n}$ are non-stochastic. $W_{n}$, $S_{n}^{-1}$, $\\tilde{W}_{n}$, and $R_{n}^{-1}$ are uniformly bounded in absolute value in both row and column sums (UB). $S_{n}(\\lambda)$ and $R_{n}(\\alpha)$ are invertible for any $\\lambda \\in \\Theta_{\\lambda}$ and $\\alpha \\in \\Theta_{\\alpha}$.\n\nThe model parameters and estimation framework involve spatial autoregressive components and common factor structures. The quasi-log likelihood function is derived under specific assumptions about disturbances, spatial weights, and factor loadings."
  },
  {
    "qid": "econ-empirical-1526-4-0-3",
    "question": "4) Discuss the role of the function $V$ (defined by equation (8)) in the analysis of this paper and why it is irrelevant in Ross' context.",
    "gold_answer": "1. **Function $V$**: Crucial in this paper because it relies on the independence of $\\tilde{x}$ and $\\tilde{y}$. \\n2. **Independence**: Allows the use of $V$ to derive comparative statics properties. \\n3. **Ross' Context**: $\\tilde{x}$ and $\\tilde{y}$ are not necessarily independent, only uncorrelated in the strong sense. \\n4. **Irrelevance of $V$**: When $\\tilde{x}$ and $\\tilde{y}$ are not independent, the Proposition of Section 2 does not hold, making $V$ irrelevant even if $E[\\tilde{x}|y]=0$ for all $y$.",
    "question_context": "Much of the literature on multiasset portfolios, which began with the work of Markowitz [9] and Tobin [15], deals with the case of 'separability.' In this case, the choice of a multiasset portfolio can be reduced to the choice of a portfolio containing one safe asset and shares of one mutual fund of risky assets.\nCass and Stiglitz [2] have characterized the class of cases in which separation holds. Their work has demonstrated the restrictiveness of this assumption.\nOliver Hart [4] strengthened this negative result by showing 'that the separation property is necessary for the generalization of Arrow's results' to the case of many risky assets.\nThe theorem can be interpreted to assert that if there are one safe and two risky assets with independent returns and if the amount invested in one of the risky assets is fixed in advance, then a more risk averse investor will invest less of his remaining wealth in the other risky asset.\nRoss [11] treats the questions dealt with here in a slightly different context. He assumes that $\\tilde{x}$ and $\\tilde{y}$ are uncorrelated in the strong sense that $E[\\tilde{x}|y]=0$, for all $y$.\nRoss' main theorem gives conditions under which ${\\pmb u}_{1}$ more risk averse than ${\\pmb u}_{2}$ implies $\\pi_{1}(\\tilde{x},\\tilde{y})>\\pi_{2}(\\tilde{x},\\tilde{y})$ when $E[\\tilde{x}|y]=0$ for all $y$.\n\nThis section discusses the relationship of the results to the literature on multiasset portfolio choice and compares them to those obtained by Ross [11]. It highlights the restrictiveness of the separability assumption and the conditions under which Arrow-Pratt theorem can be extended to a multiasset context."
  },
  {
    "qid": "econ-empirical-609-4-0-0",
    "question": "1) Derive the numerical time-iteration method used to solve the model dynamics and explain how it captures the declining labour share and increasing capital intensity.",
    "gold_answer": "1. **Numerical Time-Iteration Method**: The model dynamics are solved using a time-iteration approach, which involves iterating over the policy functions until convergence. The steps are:\n   - Discretize the state space (e.g., capital intensity, labour share).\n   - Guess an initial policy function (e.g., \\( k_{t+1} = g(k_t) \\)).\n   - Update the policy function using the Euler equation and market-clearing conditions.\n   - Repeat until convergence (e.g., \\( \\|g_{n+1} - g_n\\| < \\epsilon \\)).\n\n2. **Capturing Trends**: The model predicts:\n   - Declining labour share: \\( \\frac{w_t L_t}{Y_t} \\) decreases as capital intensity \\( k_t \\) rises.\n   - Increasing capital intensity: \\( k_t = \\frac{K_t}{L_t} \\) grows due to the entry mechanism and declining relative price of investment.",
    "question_context": "The dynamics is solved numerically through time-iteration. The resulting time series of labour share and capital intensity are shown in Figure 1 together with the observed series. As shown in the figure, the model predicts the declining pattern of the labour share and the increasing pattern of capital intensity.\nThe baseline model is compared to the no-entry model. As shown also in the steady state analysis (see Table 1), the presence of the entry mechanism amplifies the impact of the decline of the relative price of investment on labour share and capital intensity.\nPanels (a) and (b) in Figure 2 show, respectively, the average capital intensity and labour share implied by the model in year 2009 of firms founded in 1989, 1994, 1999 and 2004. The Figure shows that firms with a more recent year of foundation present on average higher capital intensity and lower labour share.\n\nThis section studies the model-implied dynamics by feeding the economy with observed series of the US relative price of investment in the period 1980–2019. The dynamics is solved numerically through time-iteration, and the resulting time series of labour share and capital intensity are compared with observed series. The model predicts a declining pattern of the labour share and an increasing pattern of capital intensity, with the presence of the entry mechanism amplifying these effects."
  },
  {
    "qid": "econ-empirical-222-0-0-2",
    "question": "3) Suppose a sex worker’s utility function includes a taste parameter \\( \\theta_i \\) for ethnicity \\( i \\), where \\( \\theta_i < 0 \\) for Indians. Show mathematically how this affects her pricing strategy for non-Indian clients.",
    "gold_answer": "1. **Utility**: \\( U = \\pi + \\theta_i \\cdot I_{\\text{transaction}} \\), where \\( I \\) is an indicator for transacting with group \\( i \\).  \n2. **Profit Trade-off**: For Indians, higher \\( p \\) compensates for \\( \\theta_i < 0 \\). For others, lower \\( p \\) maximizes \\( \\pi \\) because \\( \\theta_i \\) is less negative.  \n3. **Optimality**: Solve \\( \\partial U/\\partial p = 0 \\) for each group. For non-Indians, \\( p^* \\) decreases as \\( \\theta_i \\) becomes less negative.",
    "question_context": "sex workers believe whites have a high willingness to pay, sex workers would be more likely to approach potential clients who are white, would chose a higher initial price and would be more likely to reach an agreement.\nbecause there is widespread antagonism towards Indian clients, they would be charged a higher price. Prejudiced sex workers would be less likely to approach and less likely to reach an agreement with Indian potential clients.\nsex workers who strongly dislike Indians will lower the prices they charge other ethnicities.\nthose who strongly like whites will charge higher prices to other ethnicities.\nprice discrimination is not driven from the market... sellers have monopoly power even when search costs are very small.\n\nThe study examines how sex workers in Singapore discriminate based on client ethnicity, influenced by perceived willingness to pay and personal preferences. The findings support predictions derived from economic models of discrimination and sequential search."
  },
  {
    "qid": "econ-empirical-487-2-1-2",
    "question": "7) Derive the asymptotic distribution of the $W_0^2$ statistic under the null hypothesis and explain its use of the $\\chi_1^2$ critical value.",
    "gold_answer": "The $W_0^2$ statistic is the squared Cox-type criterion for non-nested models. Under the null:\n1. **Asymptotic Normality**: $W_0$ converges to $N(0,1)$ by central limit theorems for MLE-based tests.\n2. **Squared Statistic**: $W_0^2$ thus converges to $\\chi_1^2$ (a squared standard normal).\n3. **Critical Value**: The 5% $\\chi_1^2$ critical value (3.841) is used because $P(\\chi_1^2 > 3.841) = 0.05$, controlling Type I error.",
    "question_context": "For each of the 500 replications, we computed the test statistics $W_{0}^{2},~\\tilde{N}_{0}^{2},~J_{0}^{2}$ and $J A_{0}^{2}$ and the classical $F$ statistic ($F_{0}$) for the test of $a_{1}$ in the combined regression $y=X a_{0}+Z a_{1}+u$.\nSignificance levels were estimated by the proportion of times the test statistic exceeded the 5% critical value of the $\\chi_{1}^{2}$ distribution (for $W_{0}^{2}$ and $\\widetilde{N}_{0}^{2}$) or the $F$ distribution (for $J_{0}^{2}$, $J A_{0}^{2}$, and $F_{0}$).\nPower was assessed by the probability of making the correct inference (accepting the true model and rejecting the false one).\n\nThe experiments evaluated the performance of test statistics ($W_0^2$, $\\widetilde{N}_0^2$, $J_0^2$, $JA_0^2$, and $F_0$) under different error distributions and model specifications. Significance levels and power were estimated based on 500 replications."
  },
  {
    "qid": "econ-empirical-780-0-1-3",
    "question": "4) Show how the local alternatives H₁ₜ: Σᵥ = T⁻¹ᐧ²Δᵥ and m = T⁻¹ᐧ²δₘ affect the asymptotic distribution of the test statistic.",
    "gold_answer": "Under H₁ₜ:\n1. The score vector has a non-zero mean, introducing a non-centrality parameter.\n2. The test statistic follows a non-central χ² distribution, with non-centrality λ = (1/2)δ′Iδ, where I is the information matrix and δ = (δₘ, vec(Δᵥ)).\n3. Power increases with ||δ||, reflecting the magnitude of deviations from H₀.",
    "question_context": "Consider the linear model: yₜ = xₜ′βₜ + zₜ′γ + uₜ, where βₜ is a stochastic parameter vector following a VAR(q) process: βₜ = μ + ∑ⱼMⱼβₜ₋ⱼ + vₜ, vₜ ~ NID(0, Σᵥ).\nA sufficient condition for stationarity is det(Iₖ - ∑ⱼMⱼzʲ) ≠ 0 for |z| ≤ 1. Identification requires plim t⁻¹Gₜ′Gₜ = H_{GG} > 0, where Gₜ = (Xₜ Zₜ).\n\nThe paper specifies a linear model with partially stochastic parameters, where some parameters follow a vector autoregressive process. Key assumptions ensure stationarity and identification."
  },
  {
    "qid": "econ-empirical-1497-1-1-1",
    "question": "2) Explain how the empirical strategy accounts for the differential exposure of municipalities to the trade reform.",
    "gold_answer": "The empirical strategy accounts for differential exposure by constructing a measure of regional tariff reduction (RTR) that reflects the extent to which each municipality is affected by the tariff cuts. The RTR is calculated as the weighted sum of tariff reductions across industries, where the weights are the baseline employment shares of each industry in the municipality. This measure captures the heterogeneity in exposure across municipalities based on their industrial composition, allowing for a more precise estimation of the reform's impact.",
    "question_context": "Our empirical strategy exploits geographic variation in pre-existing industrial composition and variation in import tariff cuts across industries in a shift-share style design. To illustrate this idea, suppose that the economic shocks generated by the trade liberalization reform led to lower infant mortality rates. Consider two municipalities, one specialized in the agricultural sector and another one in the textile industry. Pre-liberalization tariffs on imports were nearly zero on the agricultural sector and relatively high on the textile industry, so the latter experienced a much more pronounced decline in the levels of protection. This implies that the municipality specialized in the textile sector was disproportionately exposed to the reform and as a result, it should experience larger improvements in infant mortality rates. By comparing the pre-post changes in infant mortality rates in both municipalities, one could identify the regional effects of the policy-induced economic shock on this outcome.\n\nThis section describes the data sources, defines the key variables used in the analysis and presents our empirical strategy. Our main empirical analysis uses data on infant mortality covering the 1985–2010 period. The geographic unit of analysis is a municipality. While most studies use rather a microregion as unit of analysis, we implement all of our analysis at the municipality level primarily for consistency with our analysis in Section 6, where we explore interactive effects with the Family Health program."
  },
  {
    "qid": "econ-empirical-1559-2-0-2",
    "question": "3) Formally test whether the 1.5 percentage point decline in married women (Table 2) is statistically consistent with the 0.86 increase in divorces per 1000 married (Table B.2). Show your calculations using the reported baseline means.",
    "gold_answer": "Baseline married proportion = $1.5/0.03 = 50\\%$. Baseline divorces = $0.86/0.38 \\approx 2.26$ per 1000 married. Expected stock reduction = $(2.26 \\times 0.5 \\times t)$ where t is years exposed. For t=8 years: $2.26 \\times 0.5 \\times 8/1000 = 0.00904$ (0.9pp), which is 60% of observed 1.5pp. Suggests additional mechanisms beyond flow effects.",
    "question_context": "The introduction of unilateral divorce increased the divorce rate by 0.24, representing a 34% increase over the baseline mean in reform states.\nColumns 5-6 of Table 2 show a decline of 1.5 percentage points (3% of baseline sample mean) in the proportion of married women after the reform in treatment states.\nColumns 5-6 of Table B.2 show an increase of 0.86 divorces per 1000 married people (38% over baseline mean) after unilateral divorce.\n\nThis section analyzes the impact of unilateral divorce reforms on divorce rates and marriage rates, using data from Mexico between 1998 and 2016. The analysis includes flow measures (divorce rates per 1000 inhabitants) and stock measures (proportion of divorced women)."
  },
  {
    "qid": "econ-empirical-396-0-1-0",
    "question": "1) Correct the conceptual error in the statement about asymptotic distributions on page 40. What is the proper definition of an asymptotic distribution?",
    "gold_answer": "The statement incorrectly conflates asymptotic distributions with degenerate distributions. Properly:\n\n1. **Asymptotic Distribution**: The limiting distribution of a sequence of random variables (e.g., \\( \\sqrt{n}(\bar{X}_n - \\mu) \\xrightarrow{d} N(0, \\sigma^2) \\)).\n2. **Degenerate Distribution**: A special case where the distribution converges to a single point (e.g., \\( \bar{X}_n \\xrightarrow{p} \\mu \\)), but this is not the general case for asymptotic distributions.",
    "question_context": "On page 40, ‘... usually a sampling distribution collapses about a single point when the sample size becomes infinite (in which case the distribution would be a verticai line of height unity at a single point). The asymptotic distribution is the form of the distribution just prior to such a degeneration .. .'\nOn page 26 we are told that the critical region is a subset of parameter space.\n\nThe text critiques a book on quantitative methods in economics, highlighting issues such as carelessness with details, misprints, and conceptual errors in the statistical introduction and regression sections."
  },
  {
    "qid": "econ-empirical-544-1-1-1",
    "question": "6) In Region C, prove that the 'sell-out' contract extracts the entire renegotiation surplus in both states. Verify the participation constraints for both types.",
    "gold_answer": "1. High-type utility: $p_{h} - c^{h}(q_{h}^{*}) = c^{h}(q_{h}^{*}) + \\bar{p} - c^{h}(\\bar{q}) - c^{h}(q_{h}^{*}) = \\bar{p} - c^{h}(\\bar{q})$, satisfying $(\\overline{I R})$. \n2. Low-type utility: $p_{l} - c^{l}(q_{l}^{*}) = c^{l}(q_{l}^{*}) + \\bar{p} - c^{l}(\\bar{q}) - c^{l}(q_{l}^{*}) = \\bar{p} - c^{l}(\\bar{q})$, satisfying $(\\underline{I R})$. \n3. Both IC constraints hold because $q_{h}^{*} > q_{l}^{*}$ and $c^{h}(q) > c^{l}(q)$.",
    "question_context": "The buyer’s optimal offer solves: \n$$\\operatorname*{max}_{(q_{h},p_{h},q_{l},p_{l})}\\bar{e}(V^{h}(q_{h})-p_{h})+(1-\\bar{e})(V^{l}(q_{l})-p_{l})$$ \nsubject to: \n$$\\begin{array}{r l}{(\\overline{{I C}})}&{p_{h}-c^{h}(q_{h})\\geq p_{l}-c^{h}(q_{l});}\\ {(\\underline{{I C}})}&{p_{l}-c^{l}(q_{l})\\geq p_{h}-c^{l}(q_{h});}\\ {(\\overline{{I R}})}&{p_{h}-c^{h}(q_{h})\\geq\\bar{p}-c^{h}(\\bar{q});}\\ {(\\underline{{I R}})}&{p_{l}-c^{l}(q_{l})\\geq\\bar{p}-c^{l}(\\bar{q}).}\\end{array}$$\nIn Region A, the optimal offer is: \n$$((p_{h},q_{h}),(p_{l},q_{l}))=((\\bar{p}+c^{h}(q_{h}^{*})-c^{l}(\\bar{q})+\\Delta c(\\hat{q}_{l}(\\bar{e})),q_{h}^{*}),(\\bar{p}-c^{l}(\\bar{q})+c^{l}(\\hat{q}_{l}(\\bar{e})),\\hat{q}_{l}(\\bar{e})))$$\nIn Region C, the optimal offer is a 'sell-out' contract: \n$$((p_{h},q_{h}),(p_{l},q_{l}))=((c^{h}(q_{h}^{*})+\\bar{p}-c^{h}(\\bar{q}),q_{h}^{*}),(c^{l}(q_{l}^{*})+\\bar{p}-c^{l}(\\bar{q}),q_{l}^{*}))$$\n\nThis section analyzes the buyer's optimal renegotiation offer under asymmetric information, characterized by regions A-E in the $(\\bar{e}, \\bar{q})$ space."
  },
  {
    "qid": "econ-empirical-292-3-0-2",
    "question": "3) Derive the asymptotic distribution of the Studentized statistic $n^{1/2}|\\widehat{\\mathbf{D}}^{-1}\\widehat{\\pmb{\\Omega}}_{S}|_{\\infty}$ under the null hypothesis in (20), justifying its use for block-wise testing.",
    "gold_answer": "1. **Statistic Construction**: $$T = n^{1/2}\\|\\widehat{\\mathbf{D}}^{-1}(\\widehat{\\pmb{\\Omega}}_S - \\pmb{\\Omega}_S)\\|_{\\infty}$$  \n2. **Asymptotic Theory**: By the CLT, $\\sqrt{n}(\\widehat{\\pmb{\\Omega}} - \\pmb{\\Omega}) \\overset{d}{\\to} N(0, \\mathbf{V})$, where $\\mathbf{V}$ is the sandwich variance.  \n3. **Extreme Value Distribution**: For $p$-dimensional vectors, $\\|\\mathbf{Z}\\|_{\\infty}$ converges to a Gumbel distribution after proper normalization.  \n4. **Bootstrap Validation**: The SKMB procedure approximates $\\mathbf{V}$ via resampling to handle unknown dependence structures.",
    "question_context": "Let $y_{j,t}$ be the jth stock price at day t. We considered the log return of the stocks, which is defined by $\\log(y_{j,t})-\\log(y_{j,t-1})$.\nLet $\\pmb{\\varOmega}=(\\omega_{j_{1},j_{2}})_{p\\times p}$ be the precision matrix of $\\mathbf{R}_{t}$. By the relationship between partial correlation and precision matrix, the partial correlation network can be constructed by the non-zero precision coefficients $\\omega_{j_{1},j_{2}}$.\nThe P-value of the hypothesis (20) was $$\\mathrm{P-value}_{h_{1},h_{2}}=\\frac{1}{M}\\sum_{m=1}^{M}\\mathbb{I}\\{|\\widehat{\\xi}_{m}|_{\\infty}\\geq n^{1/2}|\\widehat{\\mathbf{D}}^{-1}\\widehat{\\Omega}_{S}|_{\\infty}\\}\\mathrm{~for~}S=I_{h_{1}}\\times I_{h_{2}}.$$\nWe rejected $H_{(1),0},\\ldots,H_{(v),0}$ in (20) for $v=\\mathrm{max}\\{1\\leq j\\leq K:\\mathrm{pvalue}_{(j)}\\leq\\alpha j/K\\}$.\n\nThis section analyzes the partial correlation networks of S&P 500 Component Stocks in 2005 (pre-crisis) and 2008 (during crisis) to study financial network dynamics under crisis conditions. The analysis uses log returns standardized via GARCH(1,1) modeling and precision matrix-based partial correlation networks to identify direct relationships between stocks, controlling for mediating effects."
  },
  {
    "qid": "econ-empirical-446-2-0-0",
    "question": "1) Prove that if the critical ratio $\\rho(\\theta,a)$ is regular, then all optimal contracts are binary. Use the single-crossing property of the agent’s expected net benefit in your proof.",
    "gold_answer": "1. **Assume** an optimal contract $w$ is not binary and implements effort level $\\widehat{a}$.  \n2. **Construct** a binary contract $w_{\\widehat{\\lambda}}(\\Pi,\\widehat{a})$ such that:  \n   - $U(w_{\\hat{\\imath}},\\widehat{a})=U(w,\\widehat{a})$  \n   - $w(\\Pi(0,\\widehat{a}))=w_{\\widehat{\\lambda}}(\\Pi(0,\\widehat{a});\\widehat{a})$  \n3. **Derive** the difference in slopes of $U$:  \n   $$\\frac{d U(w_{\\widehat{x}},\\widehat{a})}{d a}-\\frac{d U(w,\\widehat{a})}{d a}=\\int_{0}^{\\overline{{\\theta}}}[w_{\\widehat{x}}^{\\prime}(\\Pi(\\theta,\\widehat{a});\\widehat{a})-w^{\\prime}(\\Pi(\\theta,\\widehat{a}))]\\Pi_{a}(\\theta,\\widehat{a})f(\\theta)d\\theta.$$  \n4. **Show** $\\frac{d U(w_{\\lambda}^{-},a)}{d a}>\\frac{d U(w,a)}{d a}$ at $\\widehat{a}$ using regularity and feasibility constraints.  \n5. **Conclude** that $U(w_{\\lambda},\\cdot)$ and $U(w,\\cdot)$ cross exactly once, implying $w_{\\lambda}$ dominates $w$.",
    "question_context": "Definition 3. The critical ratio $\\rho(\\theta,a)$ is said to be regular if $\\rho(\\theta,a)\\geq\\rho(\\theta^{\\prime},a)$ for some $(\\theta,\\theta^{\\prime})$ and some $a$ implies that $\\rho(\\theta,a^{\\prime})\\geq\\rho(\\theta^{\\prime},a^{\\prime})$ for all $a^{\\prime}$ .\nProposition $I$ . If the critical ratio $\\rho(\\theta,a)$ is regular, then all optimal contracts are binary, that is, they take the form $w_{\\lambda}(\\Pi;a)$ , for some $\\lambda$ .\nWhen the critical ratio, $\\rho(\\theta,a)$ , is increasing in $\\theta$ , Proposition 1 implies that an optimal contract must take the form of debt.\nWhen the critical ratio, $\\rho(\\theta,a)$ , is decreasing in $\\theta$ , Proposition 1 implies that an optimal contract must be a capped bonus.\nProposition 2. If the critical ratio, $\\rho(\\theta,a)$ , is constant in $\\theta$ at $a$ , the principal and the agent are indifferent between any two contracts that implement the effort level $a$ , as long as $w(\\Pi(0,a))$ is the same in both contracts.\n\nThis section examines the optimal principal-agent contract and characterizes the form of the contract. The optimal contract between the principal and the agent involves choosing the payment function and the effort level that maximize the principal’s expected net benefit over the set of feasible contracts."
  },
  {
    "qid": "econ-empirical-498-1-2-0",
    "question": "4) Prove Theorem 1, showing that structural rationality implies weak sequential rationality.",
    "gold_answer": "1. **Structural Rationality**: Implies $s_i$ is optimal against some perturbed belief $p^k(\\cdot|S_{-i}(I)) \\to \\mu(\\cdot|I)$ at every $I$ where $s_i$ is active.  \n2. **Weak Sequential Rationality**: Follows because optimality against $p^k(\\cdot|I)$ converging to $\\mu(\\cdot|I)$ ensures $s_i$ is optimal at the limit.",
    "question_context": "THEOREM 1: Fix a player $i\\in N$ and a CCPS $\\mu\\in\\Delta(S_{-i},\\mathcal{T}_{i})$ for i. If strategy $s_{i}\\in S_{i}$ is structurally rational given $\\mu$ , then it is weakly sequentially rational given $\\mu$.\n\nThis section establishes the relationship between structural rationality and weak sequential rationality."
  },
  {
    "qid": "econ-empirical-813-1-1-1",
    "question": "2) Prove the strong consistency of the 2SLAD estimator for $q > 0$ by adapting the LAD consistency proof from Section 2. Highlight the role of the density $f(\\lambda)$ of $q v_{t}$.",
    "gold_answer": "1. The proof follows the LAD consistency argument, replacing $x_{t}^{\\prime} \\delta$ with $\\overline{z}_{t}^{\\prime} \\delta$ and interpreting $f(\\lambda)$ as the density of $q v_{t}$.  \n2. Since $P_{t}^{\\prime}(u - q v - V\\delta_{1}) \\rightarrow 0$ in probability, the density $f(\\lambda)$ dominates the asymptotic behavior.  \n3. The condition $\\operatorname*{lim} \\frac{1}{T} \\sum |\\overline{z}_{t}^{\\prime} \\delta| > 0$ for $\\delta \\neq 0$ ensures strong consistency, analogous to the LAD case.",
    "question_context": "The simultaneous equations model is given by $y = Y\\gamma_{0} + X_{1}\\beta_{0} + u$ and $Y = X\\Pi_{0} + V$, where $X$ is a $T \\times K$ matrix of bounded constants with rank $K$ and $X_{1}$ is a submatrix of $X$.\nThe 2SLAD estimator minimizes $S = \\sum |q y_{t} + (1 - q) P_{t}^{\\prime} y - P_{t}^{\\prime} Z \\alpha|$, where $P = X(X^{\\prime} X)^{-1} X^{\\prime}$ and $P_{t}^{\\prime}$ is the $t$-th row of $P$.\nThe strong consistency of 2SLAD for $q > 0$ follows from the strong consistency of LAD, with the density $f(\\lambda)$ interpreted as the density of $q v_{t}$.\nFor $q = 0$, the error term $P_{t}^{\\prime}(u - V\\delta_{1})$ becomes degenerate, and the strong consistency is proven by showing $\\operatorname*{lim} \\frac{S}{T} = \\operatorname*{lim} \\frac{1}{T} \\sum |\\bar{z}_{t}^{\\prime} \\delta|$ almost surely, which is minimized at $\\delta = 0$.\n\nThis section extends the LAD framework to the simultaneous equations model, focusing on the strong consistency of the two-stage least absolute deviations (2SLAD) estimator."
  },
  {
    "qid": "econ-empirical-412-0-0-0",
    "question": "1) Derive the PELVE equation $\\operatorname{ES}_{1-c\\varepsilon}(X) = \\operatorname{VaR}_{1-\\varepsilon}(X)$ for a loss random variable $X$ and explain its significance in the context of the FRTB transition.",
    "gold_answer": "1. **Definition of VaR and ES**: For a random loss $X$, $\\operatorname{VaR}_{p}(X) = F_X^{-1}(p)$ and $\\operatorname{ES}_{p}(X) = \\frac{1}{1-p}\\int_p^1 \\operatorname{VaR}_q(X) \\mathrm{d}q$.  \n2. **PELVE Equation**: Solve for $c$ in $\\frac{1}{c\\varepsilon}\\int_{1-c\\varepsilon}^1 F_X^{-1}(q) \\mathrm{d}q = F_X^{-1}(1-\\varepsilon)$.  \n3. **Interpretation**: PELVE quantifies the multiplier $c$ such that ES at $1-c\\varepsilon$ matches VaR at $1-\\varepsilon$, directly addressing regulatory capital equivalence.",
    "question_context": "In the recent Fundamental Review of the Trading Book (FRTB), the Basel Committee on Banking Supervision proposed the shift from the $99\\%$ Value-at-Risk (VaR) to the $97.5\\%$ Expected Shortfall (ES) for internal models in market risk assessment.\nPELVE enjoys many desirable theoretical properties and it distinguishes empirically heavy-tailed distributions from light-tailed ones via a threshold of 2.72.\nApplying PELVE to financial asset and portfolio data leads to interesting observations that are not captured by VaR or ES alone.\n\nThe paper introduces the Probability Equivalent Level of VaR and ES (PELVE) as a novel distributional index to address the transition from Value-at-Risk (VaR) to Expected Shortfall (ES) in regulatory frameworks like Basel III/IV. PELVE identifies the equivalence point between VaR and ES, offering insights into capital requirement changes."
  },
  {
    "qid": "econ-empirical-1706-3-0-4",
    "question": "5) Interpret the U-shaped pattern of the Herfindahl index and its implications for the relationship between concentration and development.",
    "gold_answer": "1. **U-Shaped Pattern**: Concentration declines initially with development but rises again at higher income levels.  \n2. **Implications**: Early-stage development reduces concentration as economies diversify. At later stages, re-concentration occurs in low-volatility sectors (e.g., services).  \n3. **Volatility Impact**: The rise in concentration does not increase volatility because it occurs in low-risk sectors.",
    "question_context": "The reason for the strong decline in the sectoral risk components is that the employment shares of agriculture and mining, which exhibit relatively higher intrinsic volatility, sharply decline with the level of development (standard deviations of shocks are 5 and 7 percent in agriculture and mining, respectively). The share of services, which are relatively low-risk (with standard deviations below 2 percent), tends to increase with development.\nThe contribution of country-specific risk (CTY) to the difference in volatility between poor and rich countries is given by: $${\\mathrm{CTY}}^{\\mathrm{share}}={\\frac{{\\mathrm{CTY}}^{\\mathrm{poor}}-{\\mathrm{CTY}}^{\\mathrm{rich}}}{{\\mathrm{Var}}(q^{\\mathrm{poor}})-{\\mathrm{Var}}(q^{\\mathrm{rich}})}}=46\\%.$$\nThe remaining 54 percent of the difference in volatility is due to the sectoral composition of the economy. This in turn is decomposed in the part due to pure concentration, which accounts for 6 percent of the total difference, and the part due to sectoral risk, which makes up the remaining 48 percent (7 percent due to global sectoral risk and 41 percent due to idiosyncratic sectoral risk).\n\nThe text discusses the relationship between sectoral risk components and the level of economic development, using data from UNIDO and STAN-OECD samples. It highlights the decline in sectoral risk components with development, driven by shifts from agriculture and mining to services. The text also presents a volatility accounting exercise to quantify the contributions of different risk components to the volatility differences between poor and rich countries."
  },
  {
    "qid": "econ-empirical-1015-5-1-1",
    "question": "6) Show that the proposed strategy profile ensures player $j$ will accept an offer in period $t < T$ only if his share is at least $\\frac{\\delta + w_{j}}{1+\\delta}$.",
    "gold_answer": "1. If player $j$ rejects, the continuation payoff is $\\frac{\\delta(1-w_{i})}{1+\\delta}$ (punishment).  \n2. For acceptance to be optimal: $b_{j} \\geq \\frac{\\delta + w_{j}}{1+\\delta}$.  \n3. Substitute $b_{j} = 1 - b_{i}$ and rearrange to match the condition.  \n4. This ensures player $j$ has no incentive to reject equilibrium offers.",
    "question_context": "For $\\delta \\in (\\underline{\\delta}, 1)$, the optimal punishment equilibria in Propositions 3 and 4 exist, and the inequalities $(1-\\delta)B + \\frac{1-w_{2}}{1+\\delta} \\leq v_{1}$ and $(1-\\delta)B + \\frac{1-w_{1}}{1+\\delta} \\leq v_{2}$ hold.\nThe strategy profile specifies that before period $T$, players demand the whole value of 1, and in period $T$, the offer $\\hat{b}_{1}$ is accepted. Deviations trigger immediate punishment.\nThe outcome path $\\pi(T)$ involves playing $\\hat{a}$ for $(T-1)$ periods and agreeing on $\\hat{b}_{1}$ in period $T$, with average payoffs $v_{1}$ and $v_{2}$.\n\nThe theorem establishes the existence of a subgame perfect equilibrium with a specific average payoff vector $v$ in the negotiation game, using Propositions 3 and 4 for punishment strategies."
  },
  {
    "qid": "econ-empirical-79-54-1-0",
    "question": "3) Derive the conditions under which $z_{mt}$ is a valid instrument for $D_{imt}$ in the system of equations (3) and (4). What are the exclusion restrictions, and how does the paper address potential violations?",
    "gold_answer": "1. **Relevance**: $z_{mt}$ must significantly predict $D_{imt}$ (first-stage F-statistic). 2. **Exogeneity**: $z_{mt}$ must affect $y_{imt}$ only through $D_{imt}$ (exclusion restriction). 3. **Threats**: If $z_{mt}$ directly affects productivity (e.g., via infrastructure), the restriction is violated. 4. **Robustness Checks**: The paper tests exclusion by (i) controlling for time-varying observables, (ii) examining differential trends (e.g., urbanization), and (iii) using Levinsohn-Petrin (2003) to proxy unobserved productivity. 5. **LP Approach**: Intermediate inputs proxy for productivity shocks, mitigating omitted variable bias. See Online Appendix D for details.",
    "question_context": "The second stage can be thought of as a Cobb-Douglas production function with total factor productivity term and exponents on input factors that potentially change with the adoption of broadband internet: $$y_{i m t}=x_{i m t}^{\\prime}\\beta_{0}+D_{i m t}x_{i m t}^{\\prime}\\beta_{1}+w_{i m t}^{\\prime}\\xi+\\lambda_{m}+\\tau_{t}+\\varepsilon_{i m t},$$ where $D_{i m t}$ is a dummy for broadband adoption.\nThe first stages are given by: $$\\begin{array}{r}{D_{i m t}=x_{i m t}^{\\prime}\\delta+z_{m t}x_{i m t}^{\\prime}\\phi+w_{i m t}^{\\prime}\\zeta+\\gamma_{m}+\\theta_{t}+\\nu_{i m t}\\qquad}\\\\{D_{i m t}x_{1,i m t}=x_{i m t}^{\\prime}\\delta_{1}+z_{m t}x_{i m t}^{\\prime}\\phi_{1}+w_{i m t}^{\\prime}\\zeta_{1}+\\gamma_{1,m}+\\theta_{1,t}+\\nu_{1,i m t}\\qquad}\\\\{\\vdots=\\vdots\\qquad}\\\\{D_{i m t}x_{n,i m t}=x_{i m t}^{\\prime}\\delta_{n}+z_{m t}x_{i m t}^{\\prime}\\phi_{n}+w_{i m t}^{\\prime}\\zeta_{n}+\\gamma_{n,m}+\\theta_{n,t}+\\nu_{n,i m t},}\\end{array}$$ where $z_{mt}$ is the instrument (broadband availability rate).\n\nEconomic theory views the production technology as a function describing how factor inputs are transformed into output. Technological change is a shift in the production function. This section examines whether broadband adoption causes such a shift, using a system of equations with instrumental variables."
  },
  {
    "qid": "econ-empirical-169-5-0-2",
    "question": "3) Interpret the coefficients in Table A1 for high caste and low caste households. How do they reflect the overlap in wealth and education between castes?",
    "gold_answer": "1. **Land Ownership**: High caste elasticity (0.282) > low caste (0.084), but both significant. Suggests land matters more for high castes. \n2. **Education**: High caste return (0.304) > low caste (0.118), but both significant. Overlap exists but high castes benefit more. \n3. **Housing**: Brick houses increase consumption for both, but low caste effect (0.238) > high caste (0.179). \n4. **Conclusion**: Overlap in controls exists, but caste status independently affects outcomes.",
    "question_context": "Our findings unambiguously refute the first two and support the third: compared to low caste subjects, high caste subjects have a considerably greater willingness to altruistically punish violations of a cooperation norm.\nOur results also suggest that the differences in the willingness to punish can be attributed to the caste status of individuals: individuals from each specific high caste in our sample – the Brahmins and the Thakurs – exhibit a significantly greater willingness to punish compared to individuals from each specific low caste, while the differences between the two specific high castes and between the two specific low castes in our sample are negligible.\nIn a further experiment, we show that the high caste do not generally punish more than the low caste. Instead, they punish more when the victim of the norm violation is a member of their own specific caste.\nOur results relate to a longstanding question in political economy: How does an elite resist reforms after it loses control over the political institutions? Acemoglu and Robinson (2006) propose a model of purely self-interested agents in which each member of the elite is large enough to internalise the benefits from resistance to reform through lobbying, bribery, intimidation, or violence, whereas each member of the repressed group is not large enough to do so.\n\nThe study examines how the Indian caste system affects individuals' willingness to punish norm violations, focusing on differences between high and low caste groups. The analysis controls for wealth, education, and political participation to isolate the impact of caste status."
  },
  {
    "qid": "econ-empirical-943-1-0-1",
    "question": "2) Show how the strategic index $\\eta^{p}(y^{-p};\\xi)$ is derived in the Cournot model and explain its economic interpretation.",
    "gold_answer": "1. The strategic index is given by $\\eta^{p}(y^{-p};\\xi) = -\\sum_{q\\neq p}b^{p,q}(X)\\cdot y^{q}$.\\n2. Here, $b^{p,q}(X)$ are elements of the inverse matrix $A(X)^{-1}$, where $A(X)$ is the matrix of coefficients $a^{p,q}(X)$.\\n3. The index captures the effect of rivals' quantities $y^{-p}$ on firm $p$'s optimal choice.\\n4. If $b^{p,q}(X) \\leq 0$, the goods are substitutes; otherwise, they are complements.\\n5. The index summarizes the direction and relative magnitude of strategic interactions.",
    "question_context": "Consider a model of Cournot competition between $P$ firms with differentiated products... Suppose the model is described by a linear demand system where $\\mathcal{Q}^{p}=\\sum_{q=1}^{P}d^{p,q}\\big(\\xi^{p}\\big)\\cdot\\mathcal{P}^{q}+f^{p}\\big(\\xi^{p}\\big)\\quad\\mathrm{for}p=1,\\dots,P.$\nDefine $\\zeta^{p}(\\xi^{p})\\equiv f^{p}(\\xi^{p})/\\sum_{q=1}^{P}d^{p,q}(\\xi^{p})$... The demand system can be expressed as $\\mathcal{Q}^{p}=\\phi^{p}\\big(\\varepsilon^{p}\\big)\\cdot\\sum_{q=1}^{P}a^{p,q}(X)\\cdot\\big(\\mathcal{P}^{q}+\\zeta^{p}\\big(\\xi^{p}\\big)\\big)\\quad\\mathrm{for}~p=1,\\dots,P.$\nProfit functions are of the form $\\pi^{p}\\big(\\mathcal{Q}^{p},\\mathcal{Q}^{-p};\\boldsymbol{\\xi}\\big)=\\left(\\frac{1}{\\phi^{p}\\big(\\varepsilon^{p}\\big)}\\sum_{q=1}^{P}b^{p,q}(\\boldsymbol{X})\\cdot\\mathcal{Q}^{q}-\\zeta^{p}\\big(\\boldsymbol{\\xi}^{p}\\big)\\right)\\cdot\\mathcal{Q}^{p}-C^{p}\\big(\\mathcal{Q}^{p};\\boldsymbol{\\xi}^{p}\\big).$\n\nThe text presents a structural economic model of Cournot competition between firms with differentiated products, using a linear demand system and discussing strategic interactions captured by the index $\\eta^{p}$."
  },
  {
    "qid": "econ-empirical-81-2-0-2",
    "question": "3) Show mathematically why an increase in income inequality across regions (holding per capita income fixed) makes separation more likely, as stated in Proposition 3.",
    "gold_answer": "1. Define $\\Phi(w_{mi}, t) = U(w_{mi}) - U_i(w_{mi})$.\n2. For $t_B^{\\min}$, solve $\\Phi(w_{mB}, t) = 0$ and differentiate to find:\n$$\\frac{dt_B^{\\min}}{dw_{mB}} = \\frac{\\partial \\Phi / \\partial w_{mB}}{\\partial \\Phi / \\partial t} > 0.$$\n3. For $t_A^{\\max}$, solve $\\Phi(w_{mA}, t) = 0$ and differentiate to find:\n$$\\frac{dt_A^{\\max}}{dw_{mA}} = \\frac{-\\partial \\Phi / \\partial w_{mA}}{\\partial \\Phi / \\partial t} < 0.$$\n4. Thus, $t_B^{\\min}$ increases and $t_A^{\\max}$ decreases, making the condition $t_B^{\\min} > t_A^{\\max}$ more likely, leading to separation.",
    "question_context": "A necessary condition for nonseparation is that a majority of voters in each region prefers unification with tax rate $t$ over separation. In other words, the following nonseparation constraint (NSC) must hold for a majority of voters: \n$$(\\mathbf{NSC})(1-t)w_{v}+\\bigg(t-\\frac{t^{2}}{2}\\bigg)y \\geq\\alpha\\bigg[w_{v}+\\frac{1}{2}\\bigg(\\frac{y_{i}-w_{m i}}{y_{i}}\\bigg)\\big(y_{i}-2w_{v}-w_{m i}\\big)\\bigg].$$\nThe (NSC) of the median income in region $i$ is pivotal in determining separation and the necessary condition for nonseparation reduces to \n$$(1-t)w_{m i}+\\bigg(t-\\frac{t^{2}}{2}\\bigg)y\\geq\\alpha\\bigg[w_{m i}+\\frac{1}{2}\\frac{(y_{i}-w_{m i})^{2}}{y_{i}}\\bigg]\\quad i=A,B.$$\nSeparation may thus occur in equilibrium even if one allows for preemptive accommodating taxation in the unified nation. The basic point here is that an accommodating tax for region $B$ may not be one for region $A$.\nPROPOSITION 2. For low values of $\\upalpha$, unification obtains with no tax accommodation $(t=t^{*}$). For intermediate values of $\\upalpha$, unification may obtain only under tax accommodation; the equilibrium tax rate in the union is then either $t_{A}^{\\mathrm{max}}$ or $t_{B}^{\\mathrm{min}}$. Separation occurs for intermediate values of $\\upalpha$, when $t_{B}^{\\mathrm{min}}>t_{A}^{\\mathrm{max}}$. Finally, for high values of $\\upalpha$ (close to one) separation always occurs.\nPROPOSITION 3. An increase in the difference in income inequality across regions, holding per capita income fixed, reduces $t_{\\mathrm{max}}^{A}$ and increases $t_{\\mathrm{min}}^{B}$ for any given efficiency loss from separation as long as $t>\\upalpha t_{A}+(1-\\upalpha)$.\n\nThis section analyzes how equilibrium tax rates in a unified nation change in response to secession threats and whether separation occurs despite accommodating tax policies. A two-stage game is considered, where the unified nation votes over a tax rate in the first stage, and regions decide on separation in the second stage. The analysis focuses on parameter constellations where no accommodating tax rate prevents separation."
  },
  {
    "qid": "econ-empirical-1600-3-1-2",
    "question": "3) Explain why TOTB and RERA are positively correlated but not perfectly so, and how this allows for uncovering differences in RER policy choices.",
    "gold_answer": "1. TOTB and RERA capture related but distinct aspects of external economic conditions.\n2. Positive correlation arises because both reflect terms of trade and exchange rate dynamics.\n3. Imperfect correlation allows disentangling the effects of terms of trade shocks from deliberate exchange rate policy choices.",
    "question_context": "I form standardized versions of TOT by: (a) calculating the mean and standard deviation of TOT for each country over the period 1967-1986. (b) calculating a normalized TOT series called TOTA by subtracting the mean derived in (a) from each value of TOT. (c) calculating a normalized TOT series called TOTB by subtracting the mean and then dividing by the standard deviation for each value of TOT. This becomes a standard normal approximation to the TOT.\nAvailable data on the real exchange rate are scaled similarly to the terms of trade. For identical reasons a cross-country comparison should normalize this variable. RERA and RERB are defined analogously to TOTA and TOTB, using the means and standard deviations computed over the period 1965-1988.\nThere is no a priori rule to use in choosing between the two forms. If the world is 'linear in logarithms', then the B transforms are more appropriate; if the world is 'linear in levels', then the A transforms are more appropriate.\n\nThe terms of trade (TOT) and real exchange rate (RER) data are normalized to facilitate cross-country comparisons. Two normalization methods (TOTA/TOTB and RERA/RERB) are discussed, with TOTB and RERA being the primary measures used in the study."
  },
  {
    "qid": "econ-empirical-997-4-0-2",
    "question": "3) Explain the econometric rationale behind including quadratic terms for the capacity instrument in the first-stage regressions (Table 7). How does this improve identification?",
    "gold_answer": "1. **Nonlinear Effects**: Quadratic terms capture diminishing or increasing marginal effects of capacity on participation. For example, $\\text{Capacity}^2$ in Table 7 is significant, suggesting nonlinearity.  \n2. **Improved Fit**: The F-statistic increases from 6.88 (linear) to 20.49 (quadratic) for exposure, indicating better instrument strength.  \n3. **Specification Test**: Comparing linear vs. quadratic models via F-tests ensures the functional form is correctly specified.",
    "question_context": "Whether a child participates or not in HC is a choice that parents make, and consequently, we consider it endogenous. To tackle this problem, we use an instrumental variable approach. In Section 2, we discussed a model that justifies the use of cost variables (and indirectly availability) as instruments and gave an interpretation to the estimates one gets following an IV approach. The crucial assumption, of course, is that the instrumental variables do not enter directly as determinants of $H(\\cdot)$ in (7). In addition, the instruments must be drivers of participation. The latter condition is easy to test using the first-stage equations; the former condition remains an important assumption, which we also explore in Section 6.\nAs discussed in subsection 2.2, we use two different variables to measure participation in HC (attendance and exposure) and three different instruments: distance from the residence to the nearest HC, the median fee in the town and the capacity of the HC programme in the town (filled plus vacant HC slots divided by the number of eligible children in the town).\n\nThe text discusses the use of an instrumental variable (IV) approach to address the endogeneity of child participation in the HC program. Key instruments include distance to HC, median fee, and HC capacity. The validity of these instruments is tested through first-stage regressions and their correlation with covariates."
  },
  {
    "qid": "econ-empirical-1619-4-0-0",
    "question": "1) Derive the optimization problem for selecting weights in the modified Hamilton filter to minimize the quadratic loss function, given the constraints on weights (positivity, sum to 1).",
    "gold_answer": "The optimization problem can be formulated as:\n\n\\[\n\\min_{\\mathbf{w}} \\sum_{k=6}^{32} (PTF(k) - PTF_{\\text{target}}(k))^2 \\quad \\text{s.t.} \\quad w_i \\geq 0 \\, \\forall i, \\, \\sum_{i=4}^{12} w_i = 1\n\\]\nwhere:\n- \\(PTF(k)\\) is the Power Transfer Function for cycle length \\(k\\) with weights \\(\\mathbf{w}\\).\n- \\(PTF_{\\text{target}}(k)\\) is the target PTF (either ideal or average).\n- The constraints ensure non-negative weights summing to 1.",
    "question_context": "First, we choose weights to achieve a PTF that is as close as possible to the ideal PTF that takes a value of 1 for cycle lengths from 6 to 32 quarters and a value of zero otherwise. Second, we choose weights to get as close as possible to a PTF that takes the average value of the original Hamilton filter’s PTF over the cycles from 6 to 32 quarters length and zero otherwise.\nFor both exercises, we use the full sample for the optimization of weights, consider forecasting horizons between 4 and 12 quarters, and a quadratic loss function. We restrict the weights across the 9 considered horizons to be positive, to sum to 1, and cut off the optimization after a maximum cycle length of 128 quarters.\nIn both optimized versions, only 3 of the 9 horizons have weights that are larger than zero. These are horizons 4 (weight: 0.39), 6 (0.43), and 9 (0.18) for fitting the ideal PTF and 4 (0.33), 6 (0.42), and 9 (0.25) for fitting the average PTF of the business cycle frequencies based on the original Hamilton filter.\n\nThe modified Hamilton filter uses an unweighted average of forecast errors of different horizons. This section explores optimizing the weights to improve output gap measurement by aligning the Power Transfer Function (PTF) with ideal or average PTF values."
  },
  {
    "qid": "econ-empirical-1571-1-0-3",
    "question": "4) Derive the convergence rate of $\\hat{\beta}_{l}(\\cdot)$ and explain the contributions of the bias and variance terms.",
    "gold_answer": "The convergence rate of $\\hat{\beta}_{l}(\\cdot)$ is given by:\n\n$$\n\\Big\\|\\hat{\\beta}_{l}(\\cdot)-\\beta_{l}(\\cdot)\\Big\\|_{L_{2}}^{2}=O_{\\mathscr{P}}\\{1/n+K_{0}/n+K_{m}/n+\\rho_{n0}^{2}+\\rho_{n2}^{2}\\}\n$$\n\nThe bias term $\tilde{\\beta}_{l}(u)-\\beta_{l}(u)$ is controlled by the approximation rates $\rho_{n0}$ and $\rho_{n2}$, while the variance term $\\hat{\\beta}_{l}(u)-\\tilde{\\beta}_{l}(u)$ depends on the sample size $n$ and the number of knots $K_{0}$ and $K_{m}$. Together, these terms determine the overall convergence rate of the estimator.",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nLet $\\left\\Vert a\right\\Vert_{L_{2}}$ denotes the $L_{2}$ norm of a square integrable function $a(\\cdot)$ on $\\mathcal{U}$ , that is, $\begin{array}{r}{\\|a(.)\\|_{L_{2}}^{2}=\\int_{u\\in\\mathcal{U}}\bar{|a(u)|^{2}}f_{U}(u)d u}\\end{array}$\nLet $\begin{array}{r}{\\mathrm{dist}\\{\\alpha_{0j},\\mathbb{G}_{\\alpha_{0j}}\\}=\\operatorname*{inf}_{g_{0j}\\in\\mathbb{G}_{\\alpha_{0j}}}\\operatorname*{sup}_{u\\in\\mathcal{U}}|\\alpha_{0j}(u)\\quad-\\quad g_{0j}(u)|}\\end{array}$ be the $L_{\\infty}$ distance between $\\alpha_{0j}(\\cdot)$ and $\\mathbb{G}_{\\alpha_{0j}}$ , and take $\rho_{n0}=\\mathrm{max}_{1\\le j\\le p}$ d $\\mathrm{list}\\{\\alpha_{0j},\\mathbb G_{\\alpha_{0j}}\\}$.\nUnder Conditions C4 and C5 (in the appendix), it has been shown that $\rho_{n0}=O(K_{0}^{-2})$ , $\rho_{n1}=O(K_{1}^{-2})$ and $\rho_{n2}=O(K_{m}^{-2})$.\n$\\hat{\\alpha}_{0}(u)\\stackrel{a}{\\sim}N(\tilde{\\alpha}_{0}(u),\\frac{1}{n}\\sigma_{1}^{2}\\{I_{p}\\otimes b_{0}(u)^{T}\\}\\Sigma_{m^{*}m^{*}.x^{*}}^{-1}\\{I_{p}\\otimes b_{0}(u)\\})$\n$\\hat{\\alpha}_{1}(u)\\stackrel{a}{\\sim}N(\tilde{\\alpha}_{1}(u),\\frac{1}{n}\\sigma_{1}^{2}\\{I_{q}\\otimes b_{1}(u)^{T}\\}\\Sigma_{x^{*}x^{*}.m^{*}}^{-1}\\{I_{q}\\otimes b_{1}(u)\\})$\n$\\|\\hat{\\alpha}_{\\omega j}-\\alpha_{\\omega j}\\|_{L_{2}}^{2}=O_{p}(K_{\\omega}/n+\bar{K_{\\omega}^{-4}})$\n$\\{\\operatorname{var}_{a}(g^{T}\\hat{\\pmb{\\xi}}^{*})\\}^{-1/2}{\\pmb g}^{T}(\\hat{\\pmb{\\xi}}^{*}-\tilde{\\pmb{\\xi}}^{*})\\xrightarrow{D}N(0,1)$\n$\\Big\\|\\hat{\beta}_{l}(\\cdot)-\beta_{l}(\\cdot)\\Big\\|_{L_{2}}^{2}=O_{\\mathscr{P}}\\{1/n+K_{0}/n+K_{m}/n+\rho_{n0}^{2}+\rho_{n2}^{2}\\}$\n$[\\mathrm{cov}_{a}\\{\\hat{\\pmb{\beta}}(u)\\}]^{-1/2}\\{\\hat{\\pmb{\beta}}(u)-\tilde{\\pmb{\beta}}(u)\\}\\stackrel{D}{\rightarrow}N(\\mathbf{0},I)$\n\nThis section studies the asymptotic properties of the direct effect estimate, $\\hat{\\pmb{\\alpha}}_{1}(u)$, and the indirect effect estimate, $\\hat{\\pmb{\beta}}(u)$. It introduces notations for $L_{2}$ and $L_{\\infty}$ norms, approximation rates $\rho_{n0}, \rho_{n1}, \rho_{n2}$, and establishes asymptotic normality for $\\hat{\\pmb{\\xi}}^{*}$, $\\hat{\\pmb{\\alpha}}_{0}(u)$, and $\\hat{\\pmb{\\alpha}}_{1}(u)$. It also derives the convergence rate of $\\hat{\\pmb{\beta}}(u)$ and its asymptotic normality."
  },
  {
    "qid": "econ-empirical-243-4-0-3",
    "question": "4) Critically evaluate the sensitivity of volatility estimates ($h_{t}$) to modeling assumptions, contrasting AR-trend-bound with UC-SV models.",
    "gold_answer": "1. **AR-trend-bound model:**\n   - Volatility estimates are stable and precise (Figure 5).\n   - Allocates variation to $h_{t}$, $\\rho_{t}$, or bounded $\\tau_{t}$ based on data.\n\n2. **UC-SV models (e.g., Trend-SV):**\n   - Lower volatility estimates (Figure 4) due to attributing variation to $\\tau_{t}$ or $g_{t}$.\n   - Trend-bound model shows higher volatility in the 1970s, as it cannot allocate variation to $\\rho_{t}$.\n\n3. **Key insight:**\n   AR-trend-bound’s flexibility (via $\\rho_{t}$ and bounds) yields more plausible volatility estimates.",
    "question_context": "Figure 2 presents estimates (posterior means) of trend inflation for the five models listed in Table 1. It can be seen that large differences exist between the unbounded UC-SV models (Trend-SV and Trend) and the other models. The former are much more erratic and yield more extreme results than the latter.\nTrend inflation estimates from the unbounded UC-SV models tend to track actual inflation fairly closely. Especially for the Trend-SV model of Stock and Watson (2007), we are finding very high values of trend inflation (over $10\\%$ in some periods).\nFigure 2 also indicates the role played by the time-varying AR coefficients. Results for AR-trend indicate that, even without bounds, allowing for a time-varying AR coefficient has a large impact on estimates of trend inflation.\nFigure 3 plots the posterior mean of trend inflation along with a credible interval for our AR-trend-bound model. The credible interval is not that wide, indicating our preferred model is estimating trend inflation fairly precisely.\nFigure 4 shows that AR-trend and AR-trend-bound are producing volatility estimates that are similar to one another. However, the three UC-SV models without an autoregressive structure (Trend, Trend-SV, and Trend-bound) are producing volatility estimates that exhibit some differences from the ARtrend models and from each other.\nFigure 6 plots the estimates of $\\rho_{t}$ for the models that allow for time-varying persistence in deviations of inflation from trend. We are finding substantial changes in $\\rho_{t}$ over time.\nFigure 7 indicates that $\\rho_{t}$ is estimated fairly precisely by our model.\nFigure 8 presents the posteriors for the bounds of the interval for trend inflation in the $A R$ -trend-bound model. It can be seen that they are relatively flat over the ranges we allow for in the prior.\nWith the AR-trend-bound model, we are finding trend inflation to be quite smooth, with a point estimate that rarely goes much above $3\\%$.\nFigure 9 uses data up to 1980Q1 to construct the posterior densities for inflation expectations for 1986, 1988, and 1990, that is, the densities for $E(\\pi_{t+h}\\mid\\pi_{t},\\ldots,\\pi_{1})$ , where $t=1980Q1$ and $h=24$ , 32, 40.\n\nThis section discusses the estimation of trend inflation, persistence, and volatility across different models, highlighting the differences between unbounded UC-SV models and bounded models, and the implications of time-varying AR coefficients."
  },
  {
    "qid": "econ-empirical-947-2-0-4",
    "question": "5) Interpret the VCV function Θ(ā,ħ) in terms of the underlying static optimization problem (6), and explain its role in the HJB equation (7).",
    "gold_answer": "1. Θ(ā,ħ) is defined as:\n   $$\n   \\inf_{a,v} 𝔼[v(x,z)^2] \\text{ s.t. } ā = 𝔼[a(z)], ħ = 𝔼[h(a(z))], \\text{ and constraints from (6)}\n   $$\n2. It represents the minimal variance of continuation values needed to implement (ā,ħ).\n3. In (7), rΘ(ā,ħ) captures the cost of incentives: higher Θ requires more variance, reducing F(w).\n4. The HJB equation balances current payoff (ā - c), slope adjustment (F'(w)(...)), and curvature cost (½F''(w)rΘ).",
    "question_context": "Fix a period length Δ, and suppose that f:[0,ū)→ℝ represents a set of feasible continuation values in period 1. That is, for each w₊∈[0,ū) in period 1, there is some incentive compatible contract-action plan with value w₊ for the agent and value f(w₊) for the principal.\nThe value Tᴵᵃf(w) of the principal's problem is given by the supremum over a, c, W ∈ I of the expected value of the principal's payoff, subject to promise keeping and incentive compatibility constraints.\nWhen the period length Δ is short, the problem can be simplified by considering constant consumption functions, replacing the incentive compatibility constraint with a weaker truth-telling constraint, approximating arguments, and using a second-order Taylor expansion of f around w.\nThe simplified problem's value Tᴬᵠf(w) is given by a supremum over a, c, W of an expected value expression, subject to constraints on the first moment and variance of the continuation value function W.\nThe function F that solves the equation F(w) = sup_{ā>0,ħ,c} {(ā - c) + F'(w)(w - u(c) + ħ) + (1/2)F''(w)rΘ(ā,ħ)} is almost a fixed point of the Bellman operator Tᴵᵃ.\nTheorem 1 states that equation (7) with boundary conditions (8) and (9) has a unique solution F, which is the limit of the principal's value for an optimal contract as Δ→0.\n\nThis section presents a heuristic derivation of the principal's value for the optimal contract-action plan as the period length Δ shrinks to zero. The construction of incentive compatible contract-action plans is discussed, along with simplifications valid for short period lengths."
  },
  {
    "qid": "econ-empirical-866-0-0-3",
    "question": "4) Analyze the double-indexed model for non-negative data: \n\n$$ E(y|x,z) = \\frac{\\exp[(\\psi + \\gamma)x + \\gamma z]}{1 + \\exp(\\psi x + \\beta z)}, $$ \n\nand derive the conditions under which the parameters $(\\psi, \\beta, \\gamma)$ are underidentified.",
    "gold_answer": "1. **Case 1 ($\\beta_0 = 0, \\psi_0 \\neq 0$)**: Two solutions exist: $(\\psi, 0, \\gamma)$ and $(-\\psi, 0, \\gamma + \\psi)$. \n2. **Case 2 ($\\psi_0 = \\beta_0 = 0$)**: $\\gamma$ is first-order underidentified (Jacobian rank-deficient) but locally identified. \n3. **General Condition**: Underidentification arises when the Jacobian of the moment conditions loses rank due to parameter restrictions.",
    "question_context": "In non-linear models defined by the unconditional moment conditions: \n\n$$ E\\left[f\\left(x;\\alpha\\right)\\right]=\\bar{f}(\\alpha)=0, $$ \n\nwhere $f(x,\\alpha)$ contains $p$ influence functions and $\\alpha$ is a vector of $k\\leq p$ unknown parameters that lie on a subset $\\mathbb{P}$ of $\\mathbb{R}^{k}$, other meaningful underidentified situations may arise.\nThe true parameter value $\\alpha_{0}$ will be locally identifiable from (1) if and only if $E[f(x;\\alpha_{j})]\\ne0$ for any $\\alpha_{j}\\ne\\alpha_{0}$ in some small open neighbourhood of $\\alpha_{0}$, while it will be globally identifiable if there is no observationally equivalent value anywhere in the admissible parameter space $\\mathbb{P}$.\nLet $\\bar{D}(\\alpha) = E[\\partial f(\\alpha)/\\partial\\alpha]$ denote the expected Jacobian of the moment conditions. If $\\bar{D}(\\alpha)$ is continuous at $\\alpha_{0}$, and rank $[\\bar{D}(\\alpha_{0})]=k$, then $\\alpha_{0}$ is locally identified.\n\nThe paper discusses the adaptation of the Generalized Method of Moments (GMM) to handle nonlinear models where a finite number of isolated parameter values satisfy the moment conditions. It also explores first-order underidentified models with rank-deficient expected Jacobians."
  },
  {
    "qid": "econ-empirical-109-0-0-1",
    "question": "2) Using a Ricardian trade model, show how differences in income elasticities of demand between North and South can lead to unequal gains from trade.",
    "gold_answer": "1. **Assumptions**: Let \\( \\eta_N \\) and \\( \\eta_S \\) be income elasticities of demand for Northern and Southern goods, respectively, with \\( \\eta_N > \\eta_S \\).  \n2. **Trade Equilibrium**: Northern demand for Southern goods grows slower than Southern demand for Northern goods: \\( \\frac{dP_S}{P_S} = \\eta_S \\frac{dY}{Y} < \\eta_N \\frac{dY}{Y} = \\frac{dP_N}{P_N} \\).  \n3. **Outcome**: The South faces deteriorating terms of trade (\\( \\frac{P_S}{P_N} \\downarrow \\)), reducing its gains from trade.",
    "question_context": "The case for such a conference is set out in Chapter 1 and embraces such diverse issues as: (1) the LDC debt crisis and its impact upon Northern financial markets and Southern growth prospects; (2) the effects of Northern macroeconomic policies on primary commodity prices; and (3) the ‘missing link' which Southern economies provide in analysing North-North trade.\nThe long-run problem is: do the less developed countries benefit from trade with the advanced Northern economies? The short-run problem centres around the alleged low demand and supply elasticities for primary products.\nBeenstock draws the following conclusions: (1) the North affects the South to a greater degree than vice-versa; (2) transfers of capital from the North to the South induce Dutch disease in the South.\nPrimary commodity prices can be extremely volatile and the traditional response has been to introduce some form of buffer stock/commodity price stabilisation scheme.\n\nThis section reviews the macroeconomic interactions between developed (North) and developing (South) economies, focusing on econometric modeling, policy coordination, and the impact of Northern policies on Southern growth."
  },
  {
    "qid": "econ-empirical-1798-4-0-1",
    "question": "2) Suppose an econometrician has a prior distribution on $(\\beta_{1}, \\beta_{2}, \\alpha_{1}, \\alpha_{2})$. Show how to use equation (52) to infer the distribution of $\\hat{a}_{g}^{*}(\\hat{t}, \\boldsymbol{w}_{t})$.",
    "gold_answer": "1. Substitute the known values of $\\hat{\\theta}$ and $w_{t}$ into $L_{t}^{0}(\\hat{\\theta}, w_{t})$.\n2. Use the prior distribution on $(\\beta_{1}, \\beta_{2}, \\alpha_{1}, \\alpha_{2})$ to derive the implied distribution of $\\hat{a}_{g}^{*}(\\hat{t}, \\boldsymbol{w}_{t})$ via equation (52).\n3. The resulting distribution combines the prior information with the structural model of price formation.",
    "question_context": "$\\hat{\\ell}_{t}^{\\mathrm{o}}(\\hat{\\boldsymbol{\\theta}},\\boldsymbol{w}_{t})$ is the rational expectations price equilibrium random variable. It has the property that when firms use it in solving their expected profit maximization problem for an optimal input, this optimal input generates, through market clearing in period $t$ , the price random $\\bar{P}_{t}^{0}(\\hat{\\theta},w_{t})$ .\nIf $\\hat{\\theta}$ and $w_{t}$ are known and an econometrician has a prior distribution on $\\beta_{\\mathbf{1}},\\beta_{2},\\alpha_{\\mathbf{1}},\\alpha_{2}$ , then he can use (52) to make inferences about the price in period $t$ .\n$P_{t}^{0}(\\widehat{\\theta},w_{t})$ is the equilibrium price random variable for period t. As the actual price in period $t$ is a realization of $\\hat{P}_{t}^{0}(\\hat{\\theta},w_{t})$ , this should surely be used to forecast the period $t$ price.\nIf the econonetrician does not know the firms' prior parameter, $\\hat{\\theta}$ , then he can approximate $\\widehat{\\theta}_{T}$ by assuming the prior of the firms at time $t = 0$ is uninformative, using $A_{0}\\simeq k I_{2}$ and ${\\vec{B}}_{0} \\approx k I_{2}$.\nInferences about $(\\pmb{\\alpha},\\pmb{\\beta})$ can be made easily if the econometrician has a prior distribution where $\\pmb{\\alpha}$ and $\\pmb{\\beta}$ are jointly normal and independent.\n\nThe text discusses a rational expectations price equilibrium model where firms use a stochastic process to determine optimal inputs, which in turn affect market clearing prices. The econometrician can use prior distributions on model parameters to make inferences about future prices, assuming knowledge of firms' prior parameters or approximating them for large time periods."
  },
  {
    "qid": "econ-empirical-574-1-0-1",
    "question": "2) Compare the profit-maximizing number of workers (from Q1) with the scenario where no workers are laid off ($x = 196$). Calculate the difference in profits and discuss the trade-offs involved in the decision.",
    "gold_answer": "1. **Profit at $x = 100$**:\n   \\[\n   \\pi(100) = 2\\sqrt{100} - 0.1(100) - 8 = 20 - 10 - 8 = 2 \\text{ NIS million}\n   \\]\n2. **Profit at $x = 196$**:\n   \\[\n   \\pi(196) = 2\\sqrt{196} - 0.1(196) - 8 = 28 - 19.6 - 8 = 0.4 \\text{ NIS million}\n   \\]\n3. **Difference in profits**:\n   \\[\n   2 - 0.4 = 1.6 \\text{ NIS million}\n   \\]\n4. **Trade-offs**:\n   - Profit maximization ($x = 100$) yields higher profits but requires laying off 96 workers.\n   - No layoffs ($x = 196$) yields lower profits but preserves jobs, which may be ethically preferable during a recession.",
    "question_context": "Assume that you are vice president of $\\mathrm{ILJK}$ company. The company provides extermination services and employs administrative workers who cannot be fired and 196 non-permanent workers who do the actual extermination work and can be fired. The company was founded 5 years ago and is owned by three families. The work requires only a low level of skills so that each worker requires only one week of training. All of the company’s employees have been with the company for three to five years. The company pays its workers more than minimum wage. A worker’s wage, which includes overtime, amount to between NIS 4,000 and NIS 5,000 per month (the minimum wage in Israel was about NIS 3,335 at the time of the experiment). The company provides its employees with all the benefits required by law.\nThe Finance Department has prepared a forecast of profits according to which the employment of $x$ workers will result in annual profits (in NIS millions) of: $2{\\sqrt{x}}-0.1x-8$.\nThe question was intended to present the respondent with a dilemma which would force him to weigh his commitment to profit maximisation against his concern for the fired workers who would be looking for a new job at a time of high unemployment.\n\nThe experiment involved six groups of Israeli students from different academic backgrounds who were asked to respond to a questionnaire regarding a hypothetical scenario of worker layoffs in a company. The scenario presented a trade-off between profit maximization and concern for workers during a recession."
  },
  {
    "qid": "econ-empirical-384-1-3-1",
    "question": "8) Explain the construction of the feasible estimator $\\widehat{\\mathrm{AVar}_{\\varepsilon}}$ and its role in the CLT for $N_{1,\\varepsilon}^{n}$.",
    "gold_answer": "1. $\\widehat{\\mathrm{AVar}_{\\varepsilon}}$ estimates the asymptotic variance using higher-order terms of $\\varDelta\\widehat{X}_{t_{k}}$.\\n2. It involves sums of squared and cross terms, thresholded to exclude jumps.\\n3. The normalized statistic $N_{1,\\varepsilon}^{n} = \\frac{\\sqrt{N}(\\mathrm{ERV}_{ext} - \\mathrm{IV})}{\\sqrt{\\widehat{\\mathrm{AVar}_{\\varepsilon}}}}$ converges stably to $\\phi$.",
    "question_context": "Theorem 3. Under model (19), under Assumption A and (20), (i) if Assumption (B.i) holds, then as $n\\to\\infty$ , $N(\\widehat{\\pmb{\\theta}}-\\pmb{\\theta}_{0})=O_{p}(1);$ (ii) under Assumptions (B.i), (B.v), (B.vi) and if $\\left(J_{t}\\right)$ admits only finitely many jumps, then for any $\\xi\\in(0,1/2)$ , as $n\\to\\infty$ , $\\sqrt{N}\\left(\\mathrm{ERV}_{e x t}-\\mathrm{IV}\\right)=O_{p}(1);$ (iii) under Assumptions (B.ii)–(B.v), (B.vii) and (C.i), if $\\left(J_{t}\\right)$ admits only finitely many jumps, then for any $\\xi\\in(0,1/2)$ , stably in law, $\\sqrt{N}\\left(\\mathrm{ERV}_{e x t}-\\mathrm{IV}\\right)\\stackrel{\\mathcal{L}}{\\longrightarrow}\\phi\\times\\left(2\\int_{0}^{1}\\sigma_{t}^{4}d H_{t}+4\\int_{0}^{1}\\sigma_{t}^{4}d Q_{t}+8\\sigma_{\\varepsilon}^{2}\\mathrm{IV}+8\\sigma_{\\varepsilon}^{4}\\right)^{1/2},$ where $\\phi$ is a standard normal random variable independent of ${\\mathcal{F}}_{1}$ .\nProposition 2. Under the assumptions of Theorem 3(iii), assuming further that (31) holds, then for any $\\xi\\in(0,1/2)$ , as $n\\to\\infty$ , $N_{1,\\varepsilon}^{n}:=\\frac{\\sqrt{N}(\\mathrm{ERV}_{e x t}-\\mathrm{IV})}{\\sqrt{\\mathrm{AVar}_{\\varepsilon}}}\\overset{\\mathcal{L}}{\\longrightarrow}\\phi s t a b l y.$\n\nThe text derives a CLT for the estimator $\\mathrm{ERV}_{ext}$ under additional assumptions and introduces a feasible estimator for the asymptotic variance."
  },
  {
    "qid": "econ-empirical-1014-2-0-2",
    "question": "3) Derive the log-linearized wage equation system: \n$$\\begin{array}{r l}&{w_{i t}^{\\tau}=x_{i t}^{\\prime}\\gamma^{\\tau}+\\epsilon_{i t}^{\\tau}\\quad\\tau=\\Pi,N,U,}\\ &{\\boldsymbol{w}_{i t}=\\mathrm{Me}\\{w_{i t}^{\\pi},w_{i t}^{N},w_{i t}^{U}\\},}\\ &{\\boldsymbol{n}_{i t}=y_{i t}^{\\prime}\\delta+\\eta_{i t},}\\end{array}$$ \nfrom the theoretical bargaining model.",
    "gold_answer": "1. For each regime $\\tau \\in \\{\\Pi,N,U\\}$:\n   - Linearize first-order conditions around steady state\n   - Express wage as function of exogenous variables $x_{it}$\n   - Add error term $\\epsilon_{it}^\\tau$ for unobserved heterogeneity\n2. Median operator $\\mathrm{Me}\\{\\cdot\\}$ captures:\n   - Regime selection based on binding constraints\n   - Non-smooth response at constraint boundaries\n3. Employment equation:\n   - Derived from contract curve condition\n   - Depends on production technology parameters ($y_{it}$)\n4. Error structure:\n   - $\\epsilon^\\tau \\sim N(0,\\sigma^2 I_3)$ allows correlation across regimes\n   - $\\eta \\sim N(0,\\tau^2)$ independent of wage shocks",
    "question_context": "The constrained Nash bargain between the firm and the union is thus characterised as follows: \n$$\\operatorname*{max}_{(W,N)}[U(W,N)-\\bar{U}]^{\\alpha}[\\Pi(W,N)-\\bar{\\Pi}]^{1-\\alpha}$$ \nsubject to \n$$\\begin{array}{c l l}{{U(W,N)\\geq\\displaystyle U^{0}(u^{I},\\tilde{W},b),}}\\ {{\\Pi(W,N)\\geq\\Pi^{0}(u^{I},\\tilde{W},z),}}\\end{array}$$\nThe first-order conditions can be written as \n$$\\frac{U_{w}}{U_{{\\scriptscriptstyle N}}}=\\frac{\\Pi_{w}}{\\Pi_{{\\scriptscriptstyle N}}},$$ \n$$\\frac{\\alpha U_{w}}{U-\\bar{U}}+\\frac{(\\mathrm{I}-\\alpha)\\Pi_{w}}{\\Pi-\\bar{\\Pi}}=0.$$\nThe log-linearised version of the model is \n$$\\begin{array}{r l}&{w_{i t}^{\\tau}=x_{i t}^{\\prime}\\gamma^{\\tau}+\\epsilon_{i t}^{\\tau}\\quad\\tau=\\Pi,N,U,}\\ &{\\boldsymbol{w}_{i t}=\\mathrm{Me}\\{w_{i t}^{\\pi},w_{i t}^{N},w_{i t}^{U}\\},}\\ &{\\boldsymbol{n}_{i t}=y_{i t}^{\\prime}\\delta+\\eta_{i t},}\\end{array}$$\nAn estimate of the bargaining locus is obtained by weighted generalised instrumental variables: \n$$\\begin{array}{r l}{\\hat{\\gamma}_{W G I V E}^{\\prime}=[\\mathbf{X}^{\\prime}\\mathbf{D}^{\\prime}\\mathbf{Z}(\\mathbf{Z}^{\\prime}\\mathbf{D}^{\\prime}\\mathbf{Z})^{-1}\\mathbf{Z}^{\\prime}\\mathbf{D}^{\\prime}\\mathbf{X}]^{-1}\\mathbf{X}^{\\prime}\\mathbf{D}^{\\prime}\\mathbf{Z}(\\mathbf{Z}^{\\prime}\\mathbf{D}^{\\prime}\\mathbf{Z})^{-1}\\mathbf{Z}^{\\prime}\\mathbf{D}^{\\prime}w}&{{}\\tau=\\Pi,N,U,}\\end{array}$$\n\nThe text discusses a constrained Nash bargain between a firm and a union, considering outside options and status quo values. It introduces mathematical models to characterize the bargaining process and derives first-order conditions for optimal solutions."
  },
  {
    "qid": "econ-empirical-806-3-0-1",
    "question": "2) Explain why the HHB method fails to identify subgroups in Cases II, III, and IV, while the CPD and CH methods perform well. Reference the assumptions of each method.",
    "gold_answer": "1. **HHB Assumptions**: Designed for time series with ordered thresholds; assumes no breaks in the reduced form model (6).\n2. **CPD/CH Flexibility**: Agnostic to threshold variable ordering; accommodates cross-sectional thresholds.\n3. **Case II-IV**: Thresholds are based on $X_{o2}$, violating HHB's time-series assumption.\n4. **Result**: HHB misestimates subgroups due to incorrect structural assumptions.",
    "question_context": "In all cases except for case VI, $X_{I}$ is standard normal, and $\\mathbf{X}_{o} = (X_{o1}, X_{o2}, X_{o3})$, where $X_{o1} = 1$, $X_{o2}$ and $X_{o3}$ independently follow standard normal distribution. $U$ and $\\varepsilon$ are generated from a standard bivariate normal distribution with correlation 0.5. In case VI, data is generated from a heavy-tailed chi-squares distribution.\nCase I is the null case with one group and no threshold ($s=0$). Parameters are set as $\\beta_{e}=1$, $\\pmb{\\beta}_{o}=(2,1,0)^{\\top}$, $\\alpha_{I}=0.5$, and $\\pmb{\\alpha}_{o}=(1,1,0)^{\\top}$.\nCase II generates two subgroups ($s=1$) with parameters $\\beta_{e1}=0.5$, $\\pmb{\\beta}_{o1}=(1,1,0)^{\\top}$, $\\alpha_{I1}=0.75$, $\\pmb{\\alpha}_{o1}=(1,0.5,0)^{\\top}$, $\\beta_{e2}=1$, $\\pmb{\\beta}_{o2}=(1,1,0)^{\\top}$, $\\alpha_{I2}=1.25$, $\\pmb{\\alpha}_{o2}=(1,1,0)^{\\top}$. The threshold variable is $T=X_{o2}$.\nCase III has two imbalanced subgroups with sizes 72% and 28%. Parameters for subgroup 1 are $\\beta_{e1}=1.5$, $\\pmb{\\beta}_{o1}=(1,1,0)^{\\top}$, $\\alpha_{I1}=1$, $\\pmb{\\alpha}_{o1}=(1,1,0)^{\\top}$. For subgroup 2: $\\beta_{e2}=0.5$, $\\pmb{\\beta}_{o2}=(1,0.5,0)^{\\top}$, $\\alpha_{I2}=0.5$, $\\pmb{\\alpha}_{o2}=(1,1,0)^{\\top}$.\nCase VI uses $\\chi^{2}(4)$ distribution for $X_{I}$ and $X_{o2}$. $U$ and $\\varepsilon$ follow $0.5\\chi^{2}(4)$ with correlation 0.5. Subgroups are split at the median of $X_{o2}$.\nThe proposed CPD method is compared with the Wald method, an oracle method, and methods by Caner and Hansen (2004) (CH) and Hall et al. (2012) (HHB). Performance metrics include bias, standard deviation, and coverage rates.\n\nNumerical studies are conducted to investigate the finite-sample performance of the proposed methods. The study considers eight cases with data generated from specified models, examining various distributions and subgroup structures."
  },
  {
    "qid": "econ-empirical-1605-0-0-4",
    "question": "5) Explain the significance of the results derived by Drost and Werker (1996) for testing the presence of jumps in discrete-time data.",
    "gold_answer": "Drost and Werker (1996) derived properties of discrete-time data generated by continuous-time processes with jumps and conditional heteroscedasticity. Their results are significant because:\n1. They provide a theoretical foundation for testing jumps in a weak GARCH framework.\n2. They enable the development of tests that are robust to the data frequency.\n3. They allow for semiparametric estimation, reducing reliance on strict distributional assumptions.",
    "question_context": "The model assumes that the observed series of logarithmic returns, denoted by $y_{t}$, can be written as the sum of a smooth component $s_{t}$ and a jump component $n_{t}$: $$\\left\\{\\begin{array}{l l}{y_{t}=~s_{t}+n_{t}}\\\\ {s_{t}=~\\sigma_{t}\\xi_{t},~\\mathrm{with}~\\sigma_{t}^{2}=\\psi+\\beta\\sigma_{t-1}^{2}+\\alpha s_{t-1}^{2}}\\\\ {~\\mathrm{and}~\\xi_{t}\\sim~\\mathrm{N}(0,1)}\\\\ {n_{t}=~\\sum_{s=1}^{N_{t}}e_{s},~\\mathrm{with}~e_{s}\\sim~\\mathrm{N}(0,\\delta^{2}),~\\mathrm{N}_{t}\\sim P(1/\\zeta),}\\end{array}\\right.$$ where the innovations in the smooth component $\\xi_{t}$, the jump frequencies $N_{t}$, and the jump sizes $e_{t}$ are all independent.\nThe article discusses the implications of jumps in financial models, such as overestimating the effectiveness of short-term hedging strategies and underestimating the prices of out-of-the-money options close to maturity.\nThe model of Vlaar and Palm (1993) is slightly more general than (1) in that they allowed for autocorrelation in the mean, higher-order generalized ARCH (GARCH) effects and Bernoulli jumps.\nDrost and Werker (1996) derived several properties of discrete time data that are generated by underlying continuous-time processes that accommodate both conditional heteroscedasticity and jumps.\n\nThis article develops a test for the hypothesis that a series observed in discrete time is generated by a diffusion process, based on an overidentifying relation between variance and kurtosis parameters. The test is not specific to a particular data frequency and is applied to dollar exchange rates. The article also estimates a model containing both jumps and conditional heteroscedasticity."
  },
  {
    "qid": "econ-empirical-1562-0-1-0",
    "question": "1) Explain the key differences in the initial conditions for electricity liberalization in the US and UK. How did these differences influence the outcomes of deregulation?",
    "gold_answer": "1. **US initial conditions**: Regulated investor-owned utilities with sunk investments and constitutional guarantees. \n2. **UK initial conditions**: Vertically integrated public ownership with no prior contractual commitments. \n3. **Outcomes in the US**: Challenges included compensating for stranded assets and delivering lower prices. \n4. **Outcomes in the UK**: Simpler restructuring led to cost reductions (e.g., $6\\%$ in the CEGB case). \n5. **Conclusion**: Initial conditions significantly shaped the feasibility and success of deregulation.",
    "question_context": "Electricity liberalisation started from a very different position in the US than elsewhere. The US had evolved a system of regulated investor-owned utilities. Elsewhere the electricity supply industry (ESI) was typically vertically integrated under public ownership.\nNewbery and Pollitt (1997) found that restructuring and privatising the CEGB delivered sustainable unit cost reductions of $6\\%$.\n\nThe text compares electricity deregulation in the US and UK, highlighting differences in initial conditions, challenges, and outcomes. It discusses market designs, regulatory oversight, and lessons from liberalization experiments."
  },
  {
    "qid": "econ-empirical-1329-3-0-4",
    "question": "5) Discuss the implications of the FOREIGN coefficient and its interaction with PIMPORT for plant closure decisions.",
    "gold_answer": "1. FOREIGN coefficient: -0.384 ($t=2.60$), reducing closure probability by 4.7 percentage points.\n2. Interaction with PIMPORT: Foreign plants are more likely to import (correlation = 0.30), but this does not dominate closure decisions.\n3. Interpretation: Foreign ownership may provide resilience (e.g., access to global markets), offsetting cost pressures.",
    "question_context": "The logit estimator in TSP (version 3.1b) was used to estimate coefficients, heteroskedastically robust standard errors and changes in the probability of closure with respect to changes in the explanatory variables $(X_{i})$. This 'quasielasticity' is calculated as $\\partial p/\\partial X_{i}=\\hat{p}(1-\\hat{p})\\beta_{i}$, where $\\hat{p}$ is the predicted probability of plant closure.\nTable 3 reports the logit estimation results and probability derivatives. Only CAPSALES is not statistically significant (the $t$-statistic is just outside the $5\\%$ level). The model as a whole is highly significant and correctly predicts $82\\%$ of the choices that firms made for their plants.\nPlants with high production costs, as measured by EXPSALES, were more likely to exit. The probability derivative at the mean rises to $9.3\\%$, which is the most elastic response of any variable.\nThe SHARE coefficient is negative and highly significant, indicating that larger plants were less likely to close. Increasing SHARE from two percentage points (the mean) to three would reduce closure probability by one percentage point.\nThe coefficient on YOUTH suggests that younger plants were more likely to exit. In the low CAPSALES subsample, the probability derivative was $2.9\\%$, in the high CAPSALES subsample it was $1.6\\%$.\nThe predicted probability of closure for a plant owned by a foreign controlled company was 4.7 percentage points below that of other plants.\nThe coefficient on MULTI shows that firms with more plants were more likely to close plants. Adding one plant at the mean of 5.88 plants per firm (a $17\\%$ increase), each plant owned by that firm would experience an increase in closure probability of 0.4 percentage points.\nThe industry characteristics have the expected effects: plants were more likely to close where the cuts in protection were greatest and where the protection had been based on import licensing.\nThe logit model was re-estimated with a set of plant size dummy variables interacting with SHARE, to see if the U-shaped pattern holds when controlling for other influences on closure.\n\nThe study analyzes plant-level data from the 1986/87 Economy-Wide Census of New Zealand, focusing on manufacturing units with at least 25 workers. The data was merged with the New Zealand Business Directory to track plant survival until November 1989. A logit model was employed to estimate the probability of plant closure, incorporating variables such as EXPSALES, SHARE, YOUTH, PIMPORT, and CAPSALES, among others."
  },
  {
    "qid": "econ-empirical-177-4-0-2",
    "question": "3) Derive the optimal capital budget condition $$x_{1}(a,e){\\frac{d a}{d\\overline{{{a}}}}}+y_{1}(\\overline{{{a}}}-a,e)\\bigl[1-{\\frac{d a}{d\\overline{{{a}}}}}\\bigr]+(x_{2}+y_{2}-C^{\\prime}){\\frac{d e}{d\\overline{{{a}}}}}=r$$ and interpret its components.",
    "gold_answer": "1. **Derivation:**\n   - The capital budget $\\overline{a}$ affects:\n     - Direct productivity: $x_1$ (contractible task) and $y_1$ (noncontractible task).\n     - Effort: $(x_2 + y_2 - C') \\frac{de}{d\\overline{a}}$.\n   - Optimality requires marginal cost $r$ to equal the sum of these effects.\n\n2. **Interpretation:**\n   - $x_1 \\frac{da}{d\\overline{a}}$: Marginal benefit from assets allocated to the contractible task.\n   - $y_1 (1 - \\frac{da}{d\\overline{a}})$: Marginal benefit from assets allocated to the noncontractible task.\n   - $(x_2 + y_2 - C') \\frac{de}{d\\overline{a}}$: Net marginal benefit from increased effort due to higher capital.",
    "question_context": "The surplus from delegating to the agent is \n\n$$S_{d}=x(e_{d},1)+y(e_{d},0)-C(e_{d}),$$\n\nand the surplus from authority is \n\n$$S_{p}=x(e_{p},a_{p})+y(e_{p},a_{p})-C(e_{p}).$$\nConsider the case with $\\rho\\leq0$, but where $x$ is only observed with probability $m\\leq1$, where with probability $1-m$, observed performance is independent of $e$ and $a$. The first-order condition for the agent is now given by \n\n$$m\\beta h_{x}(a)=C^{\\prime}(e),$$\n\nas output is only observed with probability $m$.\nThe optimal choice of capital budget in either case is given by \n\n$$x_{1}(a,e){\\frac{d a}{d\\overline{{{a}}}}}+y_{1}(\\overline{{{a}}}-a,e)\\bigl[1-{\\frac{d a}{d\\overline{{{a}}}}}\\bigr]+(x_{2}+y_{2}-C^{\\prime}){\\frac{d e}{d\\overline{{{a}}}}}=r.$$\nProposition 3. Assume that the principal chooses $\\overline{{a}}$.\n\n- If control is delegated, the optimal capital budget is below the first best.\n- If effort and assets are additively separable, and $h_{i}$ are symmetric and quadratic, the capital budget is not only higher under authority, but also exceeds the first-best level.\n\nThis section discusses the trade-offs in assigning control rights between principals and agents, considering scenarios where delegation or authority is optimal. It introduces mathematical models to compare surpluses under different control rights regimes and examines the impact of monitoring and asset divisibility on these decisions."
  },
  {
    "qid": "econ-empirical-246-5-0-3",
    "question": "4) Describe the Bayesian approach to multivariate exogeneity analysis in the context of a money demand equation, as applied by Steel and Richard.",
    "gold_answer": "The Bayesian approach to multivariate exogeneity analysis involves:\n1. Specifying the prior distributions for the parameters of the money demand equation.\n2. Using Bayesian techniques to test for exogeneity by comparing the posterior distributions under different assumptions.\n3. Discuss the practical implications of the results for policy analysis and forecasting.",
    "question_context": "A note on the existence of k-class estimators when k is negative. Terrence Kinal.\nBounds for exact estimators in the errors-in-variables model and simultaneous equations. Ralph Friedmann.\nIdentification and estimation of polynomial errors-in-variables models. Jerry A. Hausman, Whitney K. Newey, Hidehiko Ichimura, and James L. Powell.\nBayesian multivariate exogeneity analysis: An application to a UK money demand equation. Mark F.J. Steel and Jean-Francois Richard.\n\nThis section discusses various econometric models and estimators, including k-class estimators, errors-in-variables models, and polynomial errors-in-variables models. It also covers topics such as monetary economics, money demand equations, and Bayesian multivariate exogeneity analysis."
  },
  {
    "qid": "econ-empirical-287-1-0-0",
    "question": "1) Derive the restricted empirical log-likelihood function $l_{EL}^r(\\theta)$ from the given optimization problem, explicitly stating the constraints and the objective function.",
    "gold_answer": "The restricted empirical log-likelihood function is derived as follows:\n1. **Objective Function**: Maximize the sum of log probabilities $\\sum_{i=1}^n \\log(p_i)$.\n2. **Constraints**:\n   - $p_i > 0$ for all $i$ (positive probabilities).\n   - $\\sum_{i=1}^n p_i = 1$ (probabilities sum to one).\n   - $\\sum_{i=1}^n p_i m(z_i, \\theta) \\geq 0$ (moment inequality constraint).\nThe Lagrangian for this problem is:\n$$ \\mathbb{L}(\\theta, \\{p_i\\}, \\lambda, \\varkappa) = \\sum_{i=1}^n \\log(p_i) + \\varkappa \\left(1 - \\sum_{i=1}^n p_i\\right) - n\\lambda' \\sum_{i=1}^n p_i m(z_i, \\theta) $$\nwhere $\\lambda \\leq 0$ is the Lagrange multiplier for the moment inequality constraint.",
    "question_context": "The (restricted) empirical log-likelihood problem is, $$ l_{E L}^{r}(\\theta)\\equiv\\operatorname*{max}_{p_{1},\\ldots,p_{n}}\\left\\{\\left.\\sum_{i=1}^{n}\\log(p_{i})\\right|p_{i}>0;\\sum_{i=1}^{n}p_{i}=1;\\right. $$ $$ \\sum_{i=1}^{n}p_{i}m(z_{i},\\theta)\\geq0\\left\\} $$ where $p_{i}$ denotes the probability mass placed at $z_{i}$ by a discrete distribution with support $\\{z_{1},\\ldots,z_{n}\\}$.\nThe ELR statistic arises by computing the difference between the restricted and unrestricted log-likelihood, $$ \\begin{array}{r l}&{\\mathcal{E L R}_{n}(\\theta)\\equiv2\\{l_{E L}^{\\boldsymbol{u r}}(\\theta)-l_{E L}^{\\boldsymbol{r}}(\\theta)\\}}\\ &{\\quad\\quad\\quad=\\underset{p_{1},\\ldots,p_{n}}{\\mathrm{min}}\\left\\{2\\sum_{i=1}^{n}\\log\\left(\\frac{1/n}{p_{i}}\\right)\\Bigg|p_{i}>0;\\sum_{i=1}^{n}p_{i}=1;\\right.}\\ &{\\quad\\quad\\quad\\left.\\sum_{i=1}^{n}p_{i}m(z_{i},\\theta)\\geq0\\right\\}.}\\end{array} $$\n\nThe ELR statistic is derived from empirical likelihood methods applied to moment inequality models. It involves optimizing a nonparametric likelihood function subject to moment inequality constraints."
  },
  {
    "qid": "econ-empirical-661-6-1-3",
    "question": "4) Evaluate the impact of the 1973 oil crisis on the Eurodollar market, using a counterfactual analysis.",
    "gold_answer": "1. Estimate pre-1973 trends.  \n2. Construct a synthetic control group.  \n3. Compare actual post-1973 growth to the counterfactual.  \n4. Conclude on the role of petrodollar recycling.",
    "question_context": "The mechanism, growth, and development of the Eurodollar market during the period 1965-73 is the main theme of this important contribution to the study of international finance.\nThe final part of the book is an econometric study of the growth of the Eurodollar market, successfully testing for the connections with the American markets and for a multiplier process.\n\nZumpforr examines the mechanism, growth, and development of the Eurodollar market during 1965-73, focusing on market transactions, credit supply, and substitution processes in the international money market."
  },
  {
    "qid": "econ-empirical-759-1-0-3",
    "question": "4) Using Example 1, demonstrate how the uncovered interest parity and expected real interest rate equality hypotheses can be expressed in form (4), specifying $c_0$ and $c_1$.",
    "gold_answer": "1. Uncovered interest parity: $i_{1,t} - i_{2,t} = \\operatorname{E}[d_{t+1}|\\mathcal{O}_t]$.  \n2. Real interest equality: $i_{1,t} - \\operatorname{E}[\\pi_{1,t+1}|\\mathcal{O}_t] = i_{2,t} - \\operatorname{E}[\\pi_{2,t+1}|\\mathcal{O}_t]$.  \n3. Combine as (4) with $c=0$ and:  \n   $$c_0 = \\begin{pmatrix}0 & 0 & 0 & 1 & -1\\\\ 0 & 0 & 0 & 1 & -1\\end{pmatrix}^{\\prime}, \\quad c_1 = \\begin{pmatrix}0 & 0 & -1 & 0 & 0\\\\ 0 & 0 & 1 & 0 & 0\\end{pmatrix}^{\\prime}.$$",
    "question_context": "We assume that the $p$-dimensional vectors of observations are generated according to the vector autoregressive (VAR) model: $$X_{t}=A_{1}X_{t-1}+\\cdots+A_{k}X_{t-k}+\\mu+\\phi D_{t}+\\varepsilon_{t},~t=1,\\ldots,T,$$ where $\\varepsilon_{1},\\dots,\\varepsilon_{T}$ are i.i.d. Gaussian vectors with mean zero and covariance matrix $\\boldsymbol{\\Sigma}$. The model can be reparameterized as: $$A X_{t}=\\prod X_{t-1}+\\prod_{2}d X_{t-1}+\\cdots+\\prod_{k}d X_{t-k+1}+\\mu+\\phi D_{t}+\\varepsilon_{t},$$ where $\\varPi=\\alpha\\beta^{\\prime}$ for $p\\times r$ matrices $\\alpha,\\beta$ of full column rank.\nRational expectations impose restrictions of the form: $$\\operatorname{E}[c_{1}^{\\prime}X_{t+1}|\\mathcal{O}_{t}]+c_{0}^{\\prime}X_{t}+\\cdots+c_{-k+1}^{\\prime}X_{t-k+1}+c=0,$$ where $c=H\\omega$ and $c_{1}, c_{-k+1}+\\cdots+c_{0}+c_{1}$ have full column rank. These can be reformulated as: $$\\operatorname{E}[c_{1}^{\\prime}d X_{t+1}|\\mathcal{O}_{t}]-d_{1}^{\\prime}X_{t}+d_{-1}^{\\prime}A X_{t}+\\cdots+d_{-k+1}^{\\prime}A X_{t-k+2}+c=0.$$\nProposition 1 states that restrictions (4) or (5) imply: $$\\beta\\alpha^{\\prime}c_{1}=d_{1},$$ $$c_{1}^{\\prime}\\boldsymbol{\\Pi}_{i}=-d_{-i+1}^{\\prime},\\quad i=2,\\ldots,k,$$ $$c_{1}^{\\prime}\\boldsymbol{\\mu}=-H\\omega,\\quad c_{1}^{\\prime}\\boldsymbol{\\phi}=0.$$ These are simultaneous linear (on $\\pi_{i},\\mu,\\phi$) and non-linear (on $\\alpha,\\beta$) restrictions due to $\\pi=\\alpha\\beta^{\\prime}$.\n\nThis section defines the statistical model generating the data and formulates the rational expectation hypothesis, illustrating applicable model types and deriving parameter restrictions implied by rational expectations."
  },
  {
    "qid": "econ-empirical-1033-2-0-0",
    "question": "1) Derive the Bellman equation for the dynamic optimization problem faced by an individual of age $a$ at time $t$, and explain how it incorporates the discount factor $\\rho$ and the information set $\\varOmega_{a t}$.",
    "gold_answer": "The Bellman equation is derived from the maximized expected lifetime utility: $$V_{a}(\\varOmega_{a t}) = \\max_{j} \\left[ U_{a}^{j}(\\varOmega_{a t}) + \\rho E[V_{a+1}(\\varOmega_{a+1,t+1}) | d_{a t}^{j} = 1, \\varOmega_{a t}] \\right]$$ for $a < 65$, and $$V_{a}(\\varOmega_{a t}) = U_{a}^{j}(\\varOmega_{a t})$$ for $a = 65$. Here, $\\rho$ discounts future utility, and $\\varOmega_{a t}$ includes all relevant state variables (e.g., skill rental prices, shocks, education, experience).",
    "question_context": "The economy consists of overlapping generations of individuals age 16-65. Each individual alive at $t$ maximizes the remaining expected discounted present value of lifetime utility given their age, subject to (4)-(6), by choosing among the eight alternatives. Maximized expected lifetime utility of an individual who is age $a$ at time $t$ is given by $$V_{a}(\\varOmega_{a t})=\\operatorname*{max}_{\\langle d_{a t}^{j}\\rangle}\\sum_{\\tau=a}^{A}E[\\rho^{\\tau-a}U_{\\tau}|\\varOmega_{a t}],$$ where $\\rho$ is the discount factor and $\\varOmega_{a t}$ is the information set (or state space) at age $a$ and time $t$.\nAggregate skill supplied to each sector-occupation is the sum of the skill units of the individuals who choose that alternative. Letting $N_{a t}$ be the total number of individuals who are age $a$ at time $t$, aggregate skill supplies are given by $$S_{t}^{j}=\\sum_{a=16}^{65}\\sum_{n=1}^{N_{a t}}s_{n a t}^{j}d_{n a t}^{j}(r_{t}^{1},...,r_{t}^{6})~(j=1,...,6),$$ where we highlight the dependence of current choices on the set of six skill rental prices.\nIn a rational expectations equilibrium, current and past values of the aggregate shocks and of capital rental prices, which are common to all agents, as well as the idiosyncratic elements of the state space associated with the decision problem of each agent in the economy (age, schooling, work experience in each sector-occupation, preference and skill shocks) will determine equilibrium skill rental prices. Specifically, equilibrium skill rental prices equate aggregate skill supplies and demands in all sector-occupations.\nThe solution algorithm is an extension of the method developed in Lee (2005). Given parameters for (1), (4), (5), and (6), a discount factor $\\rho$, and observed sequences of output in each sector and of the rental price of capital, the algorithm consists of the following steps: 1. Choose a set of parameters for the equilibrium rental price process (11) and for the aggregate shock process (2). 2. Solve the optimization problem for each cohort that exists from $t=1$ through $t=T$.\n\nThe text describes a dynamic model of individual decision-making over the life cycle, incorporating labor supply, human capital accumulation, and sectoral choice within a general equilibrium framework. The model is estimated using simulated method of moments (SMM) to match empirical moments from CPS and NLSY79 data."
  },
  {
    "qid": "econ-empirical-1634-0-0-2",
    "question": "3) Formulate a dynamic model extension where house prices adjust over time in response to mortgage finance reforms, incorporating feedback effects between prices, interest rates, and LTV ratios.",
    "gold_answer": "1. **Dynamic Adjustment**: Let \\( P_t \\) be house prices at time \\( t \\). The adjustment process can be modeled as:\n   \\[ P_{t+1} = P_t + \\alpha (D(P_t, r_t, LTV_t) - S(P_t)) \\]\n   where \\( \\alpha \\) is the speed of adjustment.\n2. **Feedback Effects**: Reforms affect \\( r_t \\) and \\( LTV_t \\), which in turn influence \\( P_{t+1} \\):\n   - Higher \\( r_t \\) reduces \\( D(P_t, r_t, LTV_t) \\), lowering \\( P_{t+1} \\).\n   - Lower \\( LTV_t \\) restricts buyer access, further reducing \\( D \\).\n3. **Steady State**: Solve for \\( P^* \\) where \\( P_{t+1} = P_t \\):\n   \\[ D(P^*, r^*, LTV^*) = S(P^*) \\]\n4. **Policy Implications**: The model shows how reforms can lead to long-term price adjustments through dynamic feedback.",
    "question_context": "Since house prices peaked in 2006-2007, major changes in mortgage finance have occurred: the conservatorship of Fannie Mae and Freddie Mac, the expansion of FHA, and new mortgage guidelines. Yet several issues remain unresolved, centering on the roles of Fannie Mae, Freddie Mac, and the FHA.\nWe analyze how some reforms might affect house prices and rents in a framework rich enough to simulate the impact of several potential mortgage finance reforms. In our model, the reforms change mortgage interest rates and loan-to-value (LTV) ratios of first time home buyers, the key drivers of house prices in recent decades.\nSimulations suggest that ending the implicit interest rate subsidy from Fannie and Freddie would have small effects, while changes in capital requirements or maximum FHA loan size limits would have larger effects.\n\nThe paper analyzes the potential effects of mortgage finance reforms on house prices and rents, focusing on changes in mortgage interest rates and loan-to-value (LTV) ratios for first-time home buyers. The authors simulate the impact of various reforms, including ending implicit interest rate subsidies, adjusting capital requirements, and modifying maximum FHA loan size limits."
  },
  {
    "qid": "econ-empirical-393-1-0-0",
    "question": "1) Derive the predictive pdf $p(w|\\Upsilon,x,z)$ for $\\tilde{\\pmb{w}}$ given the sample data $(\\Upsilon,x)$ and $z=1$, starting from the multivariate Student density framework.",
    "gold_answer": "1. Start with the multivariate Student density with degrees of freedom $\\upsilon=n-(m_{2}+1)$.\n2. The density is proportional to $\\left[1+(w-\\bar{y})\\frac{n}{n+1}\\mathsf{S}^{-1}(w-\\bar{y})^{\\prime}\\right]^{-n/2}$.\n3. This form arises from the inverse of the sum of squares matrix $\\mathsf{S}$ and the sample mean vector $\\bar{y}$.\n4. The proportionality constant ensures the pdf integrates to 1 over the support of $w$.",
    "question_context": "Consider a sample of $\\pmb{n}$ vector observations on $\\tilde{\\mathfrak{p}}$ that conform to the scheme set forth in sect. 2. Given the specifications S1 and S2, the sufficient statistics of the sample of $_n$ observations are the $(m_{2}+1)$ -dimensional vector $\\bar{y}$ of the sample means of the $\\gamma_{i};s$\nThe predictive pdf of $\\tilde{\\pmb{w}}$ conditional on the sample data $(\\Upsilon,x)$ and on the variable $z=1$ is $p(w|\\Upsilon,x,z)\\propto\\left[1+(w-\\bar{y})\\frac{n}{n+1}\\mathsf{S}^{-1}(w-\\bar{y})^{\\prime}\\right]^{-n/2}$.\nThe expected value of $\\tilde{w}_{1}$ conditional on $w_{2}$ and $z=1$ is given by $E(\\tilde{w}_{1}|w_{2},z)=\\bar{y}_{1}+(w_{2}-\\bar{y}_{2}){\\sf S}_{22}^{-1}{\\sf S}_{21}$.\nThe variance of $\\tilde{w}_{1}$ given $\\pmb{w_{2}}$ and $z$ is $\\mathrm{var}\\left(\\tilde{w}_{1}|w_{2},z\\right)=\\frac{\\sum\\hat{\\varepsilon}^{2}}{n-3}\\left(1+\\frac{1}{n}\\right)\\left(1+Q_{2}\\right)$.\n\nThis section discusses the predictive pdf of a future observation conditional on available data, focusing on the conditional mean square error in a multivariate Student framework."
  },
  {
    "qid": "econ-empirical-183-0-0-0",
    "question": "1) Formally define a multiple prior martingale $(M_t)$ and derive its essential infimum representation under the set of time-consistent priors $\\mathcal{Q}$.",
    "gold_answer": "A multiple prior martingale $(M_t)$ satisfies:\n\n$$\nM_t = \\underset{P \\in \\mathcal{Q}}{\\mathrm{ess\\inf}} \\mathbb{E}^P [M_{t+1} | \\mathcal{F}_t],\n$$\n\nwhere $\\mathcal{Q}$ is the set of time-consistent priors. The representation ensures that $(M_t)$ is a martingale under the worst-case prior $P^* \\in \\mathcal{Q}$ and a submartingale under all other priors in $\\mathcal{Q}$.",
    "question_context": "We develop a theory of optimal stopping under Knightian uncertainty. A suitable martingale theory for multiple priors is derived that extends the classical dynamic programming or Snell envelope approach to multiple priors. We relate the multiple prior theory to the classical setup via a minimax theorem.\nThe key to extend these results to multiple priors is to develop the first steps of a theory of multiple prior martingales. We do this in Section 5. Intuitively, we have to understand what a fair game is in the eyes of an uncertainty-averse, or pessimistic, agent. We define a multiple prior martingale $(M_t)$ by extending the usual martingale property to the nonlinear multiple prior expectation operator.\nThe proof of these results is not completely straightforward, though. The classical theory of optimal stopping relies strongly on martingale theory. A martingale is the probabilistic model of a fair game against nature. By definition, (conditional) expected gains from a martingale are zero. A supermartingale is an unfair game against nature where (conditional) expected gains are negative.\n\nThe paper develops a theory of optimal stopping under Knightian uncertainty, extending classical dynamic programming to multiple priors. It introduces a martingale theory for multiple priors and relates it to the classical setup via a minimax theorem. Applications include microeconomics, operations research, and finance."
  },
  {
    "qid": "econ-empirical-335-3-1-0",
    "question": "5) Derive the schooling decision rule $S$ from the utility function $U(C,S,\\tau,v)$ and budget constraint $C=Y+W(1-S)+\\tau S$.",
    "gold_answer": "1. Substitute the budget constraint into the utility function:\n   $$U = a(Y + W(1-S) + \\tau S) + bS + d(Y + W(1-S) + \\tau S)S + \\lambda\\tau S + Sv.$$\n2. Compare utilities for $S=1$ and $S=0$:\n   - $U(S=1) = a(Y + \\tau) + b + d(Y + \\tau) + \\lambda\\tau + v$.\n   - $U(S=0) = a(Y + W) + d(Y + W) \\cdot 0 = a(Y + W)$.\n3. The decision rule is $S = \\mathbf{1}\\{U(S=1) > U(S=0)\\}$, leading to:\n   $$S = \\mathbf{1}\\{v > a(Y+W) - (a+d)(Y+\\tau) - \\lambda\\tau - b\\}.$$",
    "question_context": "We parameterize the utility function as $$U(C,S,\\tau,v)=a C+b S+d C S+\\lambda\\tau S+S v,$$ where $\\lambda$ denotes the direct (stigma) effect of the program.\nThe schooling decision is $$S=\\mathbf{1}\\big\\{v>a(Y+W)-(a+d)(Y+\\tau)-\\lambda\\tau-b\\big\\}.$$ Assuming $v$ is standard normal, we obtain $$\\operatorname*{Pr}(S=1|y,w,\\tau)=1-\\Phi{\\big[}a(y+w)-(a+d)(y+\\tau)-\\lambda\\tau-b{\\big]}.$$\nThe average effect of the subsidy on school attendance is $$\\mathbb{E}\\big[\\mathrm{Pr}\\big(S=1|Y,W,\\tau=\\tau^{\\mathrm{treat}}\\big)-\\mathrm{Pr}(S=1|Y,W,\\tau=0)\\big].$$\n\nThis section applies the theoretical framework to the PROGRESA conditional cash transfer program in Mexico, focusing on the impact of subsidies on school attendance and counterfactual policies."
  },
  {
    "qid": "econ-empirical-394-1-0-1",
    "question": "2) Using Definition 1, prove that $\\frac{\\partial Pr_{AB}}{\\partial \\theta_A^m} = \\frac{\\partial \\Gamma_{AB}}{\\partial \\theta_A^m} - \\frac{\\partial \\Gamma_{AC}}{\\partial \\theta_A^m}$ under appropriate differentiability assumptions.",
    "gold_answer": "1. From the choice probability integral:\n   $$Pr_{AB} = \\int_{\\mathbf{u}} I(u_{AB} > u_{AC}) dG(\\mathbf{u})$$\n2. Differentiate w.r.t $\\theta_A^m$:\n   $$\\frac{\\partial Pr_{AB}}{\\partial \\theta_A^m} = \\int_{\\mathbf{u}} \\frac{\\partial I}{\\partial \\theta_A^m} dG(\\mathbf{u})$$\n3. Using the utility difference $\\Delta u = u_{AB} - u_{AC}$:\n   $$\\frac{\\partial \\Delta u}{\\partial \\theta_A^m} = \\frac{\\partial \\Gamma_{AB}}{\\partial \\theta_A^m} - \\frac{\\partial \\Gamma_{AC}}{\\partial \\theta_A^m}$$\n4. By Leibniz rule and continuous differentiability:\n   $$\\frac{\\partial Pr_{AB}}{\\partial \\theta_A^m} = \\frac{\\partial \\Gamma_{AB}}{\\partial \\theta_A^m} - \\frac{\\partial \\Gamma_{AC}}{\\partial \\theta_A^m}$$",
    "question_context": "Consider a consumer who currently possesses one vehicle and is purchasing a second... The household’s indirect utility derived from such a portfolio is given as: $$u_{A B}(\\theta_{A},\\theta_{B},\\Gamma_{A B})=f(\\theta_{A})+f(\\theta_{B})+\\Gamma_{A B}-\\alpha(p_{A}+p_{B})+\\epsilon_{A B},$$\nThe consumer chooses portfolio $A B$ rather than $A C$ if $u_{A B}>u_{A C}$. Thus, $A B$ is chosen if: $$f(\\theta_{B})-f(\\theta_{C})+\\Gamma_{A B}-\\Gamma_{A C}-\\alpha(p_{C}-p_{B})+\\epsilon_{A B}-\\epsilon_{A C}>0.$$\nDefinition 1. A consumer exhibits a preference for attribute substitution in $a$ when $\\frac{\\partial\\theta_{B}^{l}}{\\partial\\theta_{A}^{l}}<0$ and attribute complementarity when $\\frac{\\partial\\theta_{B}^{l}}{\\partial\\theta_{A}^{l}}>0$.\n\nThe text introduces a utility maximization framework for consumers with multi-vehicle portfolios, focusing on how changes in kept vehicle attributes influence subsequent vehicle choices. The model incorporates portfolio utility components and discrete choice theory."
  },
  {
    "qid": "econ-empirical-686-5-0-1",
    "question": "2) Prove the Corollary that the SP equilibrium payoffs coincide with the Shapley value if and only if $\\beta_{k}(S)=\\gamma_{k,k}(S)=1/|S|$ and $\\gamma_{i,k}(S)=0$ for $i\\neq k$.",
    "gold_answer": "1. **Necessity**: For the Shapley value, only marginal contributions of $i$ matter, so $\\gamma_{i,k}=0$ for $i\\neq k$.\n2. **Symmetry**: Equal $\\beta_{k}$ ensures symmetry across players.\n3. **Sufficiency**: If $\\beta_{k}(S)=\\gamma_{k,k}(S)=1/|S|$ and $\\gamma_{i,k}(S)=0$ for $i\\neq k$, then (4) reduces to: $$a_{S}^{i}=\\frac{1}{|S|}\\sum_{k\\in S\\setminus i}a_{S\\setminus k}^{i}+\\frac{1}{|S|}[v(S)-v(S\\setminus i)],$$ which is the recursive form of the Shapley value.",
    "question_context": "The noncooperative games consist of (potentially) infinitely many rounds of bargaining. In each round, a set $s\\subset N$ of active players is chosen, with a proposer $i\\in S$ selected according to a probability distribution $\\sigma=(\\sigma_{i})_{i\\in S}$. The proposer makes a feasible payoff vector proposal. If rejected, the game moves to the next round with probability $\\rho_{i}$ (repeat) or with probability $1-\\rho_{i}$, a player drops out (breakdown).\nThe probabilities $\\rho_{i}$, $\\tau_{k|i}$, and $\\sigma_{i}$ may depend on the set of active players $s$. The general procedure is characterized by $\\rho(S):=\\sum_{i\\in S}\\sigma_{i}(S)\\rho_{i}(S)$, $\\gamma_{i,k}(S):=\\sigma_{i}(S)\\tau_{k|i}(S)/(1-\\rho(S))$, and $\\beta_{k}(S):=\\sum_{i\\in S}\\gamma_{i,k}(S)$.\nProposition 9 states that for a TU game $(N,V)$ with $0\\leq\\rho(S)<1$ for all $s$, there is a unique SP equilibrium with payoffs $(a_{s})_{s\\subset N}$ satisfying: $$a_{S}^{i}=\\sum_{k\\in S\\setminus i}\\beta_{k}(S)a_{S\\setminus k}^{i}+\\sum_{k\\in S}\\gamma_{i,k}(S)[v(S)-v(S\\setminus k)]$$ for all $i\\in S\\subset N$.\nThe Corollary states that the SP equilibrium payoffs coincide with the Shapley value if and only if $\\beta_{k}(S)=\\gamma_{k,k}(S)=1/|S|$ and $\\gamma_{i,k}(S)=0$ when $i\\neq k$ for all $k\\in S\\subset N$.\n\nThis section extends the bargaining procedure from Section 2, allowing for more general probabilistic structures where the proposer is not necessarily the one who drops out after a rejected proposal. The setup includes infinitely many rounds of bargaining, with active players, proposers, and breakdown probabilities."
  },
  {
    "qid": "econ-empirical-1754-4-2-0",
    "question": "4) Derive the catchup adjustment term \\( \\Delta w_{\\text{catchup}} \\) as a function of the inflation surprise \\( \\pi - \\pi^e \\). How does this term enter the wage equation in Table 2?",
    "gold_answer": "1. **Catchup Mechanism**: \\( \\Delta w_{\\text{catchup}} = \\gamma (\\pi - \\pi^e) \\), where \\( \\gamma \\) measures indexation rigidity.  \n2. **Empirical Specification**: Table 2 estimates \\( \\gamma \\approx 0.008 \\) (column 1), implying a \\( 0.8\\% \\) wage increase per 1-point inflation surprise.  \n3. **Interpretation**: The small \\( \\gamma \\) suggests partial indexation, consistent with menu-cost models of wage rigidity.",
    "question_context": "As contracts are rarely fully indexed, there may be an ex-post adjustment of the real wage to compensate for the loss of real income due to inflation during the previous contract. This is known as 'catchup' and is often discussed explicitly in wage negotiations.\nTable 2 shows that the effect of catchup is small but precisely estimated. If the CPI is one point higher than expected over the previous contract, the wage increases by just less than 1%.\n\nContract terms such as catchup provisions and COLA elasticity influence the real wage adjustments post-negotiation, reflecting ex-post responses to inflation and other economic shocks."
  },
  {
    "qid": "econ-empirical-984-4-1-3",
    "question": "4) Evaluate the economic significance of the finding that trade reduces litigation more for patents with high $\\widehat{P}$ values.",
    "gold_answer": "1. High $\\widehat{P}$ values indicate patents at greater risk of being traded, which are more likely to be involved in transactions driven by enforcement gains.\n2. The reduction in litigation for these patents suggests that the market efficiently reallocates patents to owners with comparative advantage in enforcement.\n3. This has important implications for patent policy, as it highlights the role of trade in mitigating litigation risk for high-value patents.",
    "question_context": "We have shown that the effect of trade on litigation is heterogeneous, and that the effect reduces litigation more strongly for patents at greater risk of being traded. This suggests that the nature of the transactions varies and that there is a particular type of sorting: patents that are less likely to be traded (low values of $\\widehat{P}$ ) are more likely involved in transactions based on commercialization advantages, and patents with high values of $\\widehat{P}$ are more likely to be in transactions driven by enforcement gains.\nTo do this, we need information on patent buyers. The USPTO reassignment data contain nonstandardized names of the buyers, so buyer characteristics must be manually recovered. We perform this manual match for the 569 patents that were both traded and litigated at least once in their lifetime.\nWe use the buyer portfolio to construct two variables to capture the two basic motivations for transactions: enforcement gains and commercialization (product market) gains. The first variable, LargeBuyer, is equal to one if the buyer’s portfolio includes at least eight patents at the time of the transaction (i.e., if the buyer had that number of patents granted in the preceding 20 years), which corresponds to the top decile of the portfolio size distribution.\nThe second variable is designed to capture transactions where the traded patent is a good match for the technology profile of the buyer, where comparative advantage in manufacturing or marketing is more likely to be realized. We define TechFit as a dummy variable equal to one if the acquired patent belongs to the technology area to which the plurality of the buyer’s patents are assigned.\nTable 8 presents instrumental variable regressions that examine how buyer portfolio size and technology fit affect the impact of trade on litigation. Column 1 confirms that patents traded to small entities, and that fit well in the buyer’s portfolio, experience an increase in litigation after they are traded. These are transactions where we expect product market gains to be important and enforcement gains negligible. In sharp contrast, column 2 shows that the largest reduction in litigation rates occurs when patents are traded to large entities with low fit in the buyer patent portfolio, where enforcement gains are large and product market gains are small.\n\nThe analysis unbundles the marginal treatment effect to understand the sorting of patents into different types of transactions based on buyer characteristics and technology fit."
  },
  {
    "qid": "econ-empirical-776-4-0-3",
    "question": "4) Discuss the informational requirements for matching behavior in collective farms. How does group size affect the incentives for matching?",
    "gold_answer": "1. Workers must know each other's matching rates and fat contributions, which is impractical in large groups.  \n2. As group size increases, noise in observing reactions reduces information quality.  \n3. This reduces incentives for matching, acting as a diseconomy of scale in kibbutz but an economy of scale in kolkhoz.",
    "question_context": "Our theory of strategic matching in collective firms thus appears to explain what may be termed the ^ paradox of collective farms': the success of the Israeli kibbutz and the failure of the Soviet kolkhoz. In this theory, the role of matching depends critically on the degree of egalitarianism of the distribution of the collective product.\nWhen applying our analysis to the real-world collective farm, one must bear in mind the severe informational requirements of matching behaviour. We have assumed that workers know each other's matching rates and fat work contributions.\nFirst note that $Y^{\\prime}$ is a function only of $L$, overall labour supply. When $b^{i}=1$, then $l^{i}=a^{i}+\\Sigma a^{j}$ (from (3) in the text); therefore, at a symmetric equilibrium where $l^{i}=l^{j}$.\nTo show that $b^{i}=1$ for all $i$ exists as an equilibrium, we must first determine what will be the equilibrium of the $a$-subgame if actor i lets his matching rate $b^{i}$ differ from that of the other players.\n\nThe text discusses a model comparing work contributions in egalitarian (kibbutz) and non-egalitarian (kolkhoz) distribution systems, highlighting the paradox of collective farms. It presents a strategic matching theory and its implications for labor contributions and utility."
  },
  {
    "qid": "econ-empirical-1391-3-0-1",
    "question": "2) Using the CES production function $\\tilde{Q}_{j t} = \\tilde{\\Omega}_{j t}[\\alpha_{L}L_{j t}^{\\gamma} + \\alpha_{M}M_{j t}^{\\gamma} + \\alpha_{K}K_{j t}^{\\gamma}]^{1/\\gamma}$, derive the expression for the output elasticity of material $\\sigma_{M j t}$.",
    "gold_answer": "1. **Compute $\\frac{\\partial F}{\\partial M_{j t}}$**: For the CES production function:\n   $$\\frac{\\partial F}{\\partial M_{j t}} = \\alpha_{M}M_{j t}^{\\gamma-1}[\\alpha_{L}L_{j t}^{\\gamma} + \\alpha_{M}M_{j t}^{\\gamma} + \\alpha_{K}K_{j t}^{\\gamma}]^{(1/\\gamma)-1}.$$\n2. **Output elasticity of material**: By definition, $\\sigma_{M j t} = \\frac{\\partial F}{\\partial M_{j t}} \\cdot \\frac{M_{j t}}{F}$:\n   $$\\sigma_{M j t} = \\alpha_{M}M_{j t}^{\\gamma} [\\alpha_{L}L_{j t}^{\\gamma} + \\alpha_{M}M_{j t}^{\\gamma} + \\alpha_{K}K_{j t}^{\\gamma}]^{-1}.$$",
    "question_context": "The top manager chooses her efforts, which determine input prices and productivity. Observing the input prices and productivity, the production unit then chooses quantities of labour and material to maximise firm profit.\nThe quality-inclusive output is produced using a gross CES production function using labour $(L_{j t})$ , material $(M_{j t})$ , and capital $(K_{j t})$ as inputs, i.e., $$\\tilde{Q}_{j t}=\\tilde{\\Omega}_{j t}F(L_{j t},M_{j t},K_{j t})=\\tilde{\\Omega}_{j t}[\\alpha_{L}L_{j t}^{\\gamma}+\\alpha_{M}M_{j t}^{\\gamma}+\\alpha_{K}K_{j t}^{\\gamma}]^{1/\\gamma},$$ where $\\alpha_{L},\\alpha_{M},\\alpha_{K}$ are the distribution parameters, which sum to one by normalisation.\nThe firm’s profit maximisation problem defined in (7) implies the following first-order conditions (FOCs) for output quantity, labour quantity and material quantity: $$\\begin{array}{r l r}&{}&{\\frac{\\partial\\mathcal{L}}{\\partial\\tilde{Q}_{j t}}=\\frac{1+\\eta}{\\eta}(\\tilde{Q}_{j t})^{1/\\eta}-\\mu_{j t}=0}\\ &{}&{\\frac{\\partial\\mathcal{L}}{\\partial L_{j t}}=-P_{L_{j t}}+\\mu_{j t}\\tilde{\\Omega}_{j t}\\frac{\\partial F}{\\partial L_{j t}}=0}\\ &{}&{\\frac{\\partial\\mathcal{L}}{\\partial M_{j t}}=-\\tilde{P}_{M_{j t}}+\\mu_{j t}\\tilde{\\Omega}_{j t}\\frac{\\partial F}{\\partial M_{j t}}=0.}\\end{array}$$\n\nThe structural approach examines how external monitoring influences firm profitability through input prices and productivity, using a sequential decision-making model involving top managers and production units."
  },
  {
    "qid": "econ-empirical-1833-4-3-0",
    "question": "7) Show that \\( x_{j}^{*} > z \\) implies \\( j > p \\) and derive the optimal bundle \\( (x_{j}^{*}, y_{j}^{*}) \\).",
    "gold_answer": "1. Assume \\( x_{j}^{*} = z \\) leads to \\( j \\leq p \\).\n2. Show that replacing \\( (x_{j}^{*}, y_{j}^{*}) \\) with \\( (\\tilde{y}_{j}, \\tilde{y}_{j}) \\) lowers costs.\n3. Conclude \\( x_{j}^{*} > z \\) implies \\( j > p \\) and \\( (x_{j}^{*}, y_{j}^{*}) = (\\tilde{y}_{j}, \\tilde{y}_{j}) \\).",
    "question_context": "We first claim that \\( x_{j}^{*} > z \\). To see this suppose that, to the contrary, \\( x_{j}^{*} = z \\). This implies that \\( j \\) is less than or equal to \\( \\mathbf{p} \\) (or, equivalently, \\( \\tilde{y}_{j} < z \\)) since otherwise we could dominate the IMP \\( \\{(x_{i}^{*}, y_{i}^{*})_{i=1}^{n}\\} \\) with the IMP \\( \\{(x_{i}^{\\circ}, y_{i}^{\\circ})_{i=1}^{n}\\} \\).\n\nThis lemma establishes conditions under which \\( x_{j}^{*} > z \\) and derives the optimal bundle for type \\( j \\) individuals."
  },
  {
    "qid": "econ-empirical-951-3-1-0",
    "question": "1) Derive the asymptotic power function of the Wald test for detecting a break in factor loadings when $\\bar{r} = 3 > r = 2$.",
    "gold_answer": "1. The Wald test statistic is: \n   $$ W = T \\cdot (\\hat{\\delta}^\\prime \\hat{V}^{-1} \\hat{\\delta}) $$ \n   where $\\hat{\\delta}$ is the estimated break magnitude and $\\hat{V}$ is its variance. \n2. Under the alternative, $\\hat{\\delta} \\xrightarrow{p} \\delta \\neq 0$. \n3. Thus, $W \\xrightarrow{d} \\chi^2_1(\\lambda)$, where $\\lambda = T \\delta^\\prime V^{-1} \\delta$ is the non-centrality parameter. \n4. The power increases with $\\lambda$.",
    "question_context": "We next consider similar DGPs as in Table 1 but this time with $r\\:=\\:2$ and now subject to big breaks which are characterized as deterministic shifts in the means of the factor loadings.16 The factors are simulated as AR(1) processes with coefficients of 0.8 for the first factor and 0.2 for the second. The shifts in the loadings are 0.2 and 0.4 at time $\\tau~=~T/2$ . The other parts of the DGP are the same as in Table 1. Table 3 reports the empirical powers of the LM/Wald and Sup-LM/Wald tests in percentage terms with 1000 replications. As expected, both tests are powerful to detect the breaks as long as $\\bar{r}>r=2$ , while the power is trivial when $\\bar{r}=r=2$ .\nNext, we study the powers of our tests when the argument in Section 4.3 fails, i.e., the first element of $\\rho_{1}$ and $\\rho_{2}$ are both zero. The DGPs are constructed as follows: \n\n$$ \\begin{array}{r l}&{X_{i t}=A_{1}F_{1t}+A_{2}F_{2t}+e_{1t}\\quad\\mathrm{for}t=1,\\dots,T/2}\\ &{X_{i t}=B_{1}F_{1t}+A_{2}F_{2t}+e_{1t}\\quad\\mathrm{for}t=T/2+1,\\dots,T,}\\end{array} $$ \n\nwhere $F_{1t}$ and $F_{2t}$ are two AR(1) process defined as above, $A_{1}\\sim$ $N(0,1),A_{2}\\sim0.9\\cdot N(0,1),B_{1}\\sim0.8\\cdot N(0,1)$ , and $e_{i t}\\sim N(0,1)$ . Define $G_{t}^{1}~=~F_{1t}\\mathbf{1}(t~\\geq~\\tau)$ , $\\begin{array}{r}{\\begin{array}{r c l}{{\\varGamma}}&{{=}}&{{[A_{1}A_{2}(B_{1}-A_{1})]}}\\end{array}}\\end{array}$ , and $F_{t}^{*}=$ $(F_{1t}F_{2t}G_{t}^{1})^{\\prime}$ . Then, if $\\bar{r}=3$ , using results of Bai (2003) it is easy to show that17: \n\n$$ \\hat{F}_{t}=D^{*}F_{t}^{*}+o_{p}(1)\\quad\\mathrm{for}t=1,\\dots,T, $$ \n\nwhere \n\n$$ D^{*}=\\left({\\begin{array}{c c c}{0}&{0.6}&{0}\\\\{-1.38}&{0}&{1.38}\\\\{0}&{0}&{-1.38}\\end{array}}\\right). $$\n\nThis section evaluates the power of LM/Wald tests and their Sup-type versions to detect breaks in factor loadings under various DGPs."
  },
  {
    "qid": "econ-empirical-945-1-0-0",
    "question": "1) Derive the utility function for each group $i$ in the model, explicitly incorporating the deadweight loss associated with public goods provision. Show how the identity cleavage affects the utility through the term $\\pi_{i}$.",
    "gold_answer": "The utility function for group $i$ is given by:\n\n$$\nu_{i} = y_{i} + \\pi_{i}\n$$\n\nwhere:\n- $y_{i}$ is the after-tax income of group $i$.\n- $\\pi_{i}$ captures the payoff from the public good, incorporating deadweight loss:\n\n$$\n\\pi_{i} = r - \\left(1 + |\\theta_{i} - \\theta|\\right) \\frac{\\gamma}{2} r^{2}\n$$\n\nHere:\n- $r$ is total expenditure on the public good.\n- $\\theta_{i}$ is the ideal type of public good for group $i$.\n- $\\theta$ is the actual type of public good provided.\n- $\\gamma$ parameterizes the deadweight loss.\n\nThe identity cleavage is captured by $|\\theta_{i} - \\theta|$, which measures the distance between the group's ideal public good and the one provided. A larger distance increases the deadweight loss, reducing utility.",
    "question_context": "We define liberal democracy as a regime in which civil rights are provided in addition to electoral and property rights. Our view of the provision of civil rights draws heavily on the civil rights movement and associated legislation in favour of equal treatment of blacks in the American South (see Branch, 2013). Accordingly, we view the provision of civil rights as equal treatment of individuals or groups by the state.\nOur taxonomy of political regimes, based on the combinations of property/political/civil rights allows us to distinguish among different types of political regimes. For simplicity, let us assume that we can treat each of these rights in a binary, all-or-none fashion—they are either protected or not. This gives us eight possible combinations in all, shown in Table 1.\nWe consider a country where the total population has a mass of $1+\\eta$ and total output is normalised to one. This population is divided across three groups in society. The propertied elite have mass $\\eta$ , while the non-elite are divided into a majority group and a minority with population shares $n$ and $(1-n)$ respectively, where $n>1/2$ .\nThese group divisions are a consequence of two kinds of cleavages in society—an income cleavage and an identity cleavage. The income cleavage distinguishes the wealthy (propertied) elite from the lower income non-elite. This is essentially an economic or class divide. The second cleavage separates the majority from the minority on the basis of some salient identity marker.\nMembers of each group $i$ have preferences $u_{i}$ that are given by their (after-tax) income $y_{i}$ and a public good $\\pi_{i}$ : $u_{i}=y_{i}+\\pi_{i}$.\nThe second term $\\pi_{i}$ captures an individual’s payoff from a positional public good that captures the magnitude of the identity cleavage. To see this, we index the type of public goods by $\\theta\\in$ [0, 1], where the three groups’ ideal types are given by $\\theta_{i}$ , $i\\in[e,a,b]$.\nWe assume that there is a deadweight loss associated with the provision of public goods. Denoting total expenditure on the public good by $r$ , the utility derived from the public good is thus expressed as: $\\pi_{i}=r-\\{1+|\\theta_{i}-\\theta|\\}\\frac{\\gamma}{2}r^{2}$.\n\nThe framework describes a taxonomy of political regimes based on combinations of property, political, and civil rights. It develops a model of the economy and group-specific payoffs, focusing on allocation decisions in different political regimes."
  },
  {
    "qid": "econ-empirical-1730-0-0-0",
    "question": "1) Derive mathematically why the uncompensated income elasticity of fertility could be negative despite positive income elasticities for both quality and quantity of children, using a three-term utility function \\( U(x,n,q) \\) where \\( x \\) is consumption, \\( n \\) is number of children, and \\( q \\) is child quality.",
    "gold_answer": "1) Begin with utility maximization subject to budget constraint \\( y = x + p_n n + p_q qn \\)\n2) Form Lagrangian: \\( \\mathcal{L} = U(x,n,q) + \\lambda(y - x - p_n n - p_q qn) \\)\n3) First-order conditions yield: \\( \\frac{\\partial U}{\\partial n} = \\lambda(p_n + p_q q) \\) and \\( \\frac{\\partial U}{\\partial q} = \\lambda p_q n \\)\n4) Total differentiation shows interaction effects: increased income may disproportionately raise \\( q \\) via \\( p_q \\), reducing funds available for \\( n \\)\n5) Cross-elasticity terms can dominate when \\( \\frac{\\partial^2 U}{\\partial q \\partial n} < 0 \\), leading to \\( \\frac{dn}{dy} < 0 \\)",
    "question_context": "The individual utility function is no longer defined in terms only of quantity of goods and number of children. A third term has been introduced which, depending on the nature of the question asked, is interpreted as per capita quality of children (composite good spent on children), price of children, utility of children, bequests to children, investment in children's human capital, investment of parents in children as a capital good, quality of children (health, education and so on) and total expenditure on children.\nChapter 5 establishes a key conclusion on endogenous fertility: even if income elasticities of the demand for both quality and quantity of children are positive, the uncompensated income elasticity of fertility may well be negative.\nIn chapter 6 it is shown that a Benthamite allocation (based on total utilities) results in an optimal population size which is, as expected, larger than that of the Millian one, while the laissez-faire outcome may lie on either side.\n\nThe text discusses the evolution of economic models of fertility, moving from simplistic treatments of children as consumer goods to more sophisticated models incorporating intergenerational utility links and endogenous fertility decisions. Key contributions include nonlinear budget constraints and non-convex indifference maps arising from quality-quantity tradeoffs in child-rearing."
  },
  {
    "qid": "econ-empirical-463-1-0-0",
    "question": "1) Derive the expected utilities \\( (5/12, 1/12, 1/12) \\) for S, B1, and B2 in the benchmark case where no ex ante contract exists and S sets a reserve price of 1/2. Assume valuations \\( \\upsilon_{1}, \\upsilon_{2} \\) are uniformly distributed on [0, 1].",
    "gold_answer": "1. **Seller's expected revenue (S):**  \n   - Reserve price \\( r = 1/2 \\).  \n   - Probability both bids are below \\( r \\): \\( P(\\upsilon_{1} < r, \\upsilon_{2} < r) = r^2 = 1/4 \\).  \n   - Expected revenue when at least one bid ≥ \\( r \\): \\( r \\cdot P(\text{exactly one bid} \\geq r) + E[\\max(\\upsilon_{1}, \\upsilon_{2}) | \text{both} \\geq r] \\cdot P(\text{both} \\geq r) \\).  \n   - Calculations yield \\( U = 5/12 \\).  \n2. **B1 and B2's expected utilities:**  \n   - Only realized when \\( \\upsilon_{i} \\geq r \\) and wins.  \n   - For uniform distributions, \\( V_{1} = V_{2} = \\int_{r}^{1} (\\upsilon - r) d\\upsilon = 1/12 \\).",
    "question_context": "As a benchmark, if there is no ex ante contract, S’s optimal selling mechanisms are equivalent to the first-price or second-price auction with a reserve price of 1/2 for each buyer. The expected utilities for S, B1, and B2 are (5/12, 1/12, 1/12). The social welfare is 7/12.\nWith the ex ante contract, B1 has the right of first refusal and S cannot impose ex post reserve prices. B2 bids \\( b_{2} = v_{2}/2 \\), and B1 exercises the right if \\( v_{1} \\geq b_{2} \\). The expected utilities are (11/24, 1/12, 1/12), with misallocation when \\( v_{2}/2 < v_{1} < v_{2} \\).\nCorollary 1 states that the ex ante contract with the right of first refusal and no reserve price is socially more efficient than the absence of ex ante contracts, with social welfare of 15/24.\n\nThis section analyzes the effects of strategic ex ante contracts, specifically focusing on a contract where B1 holds the right of first refusal without reserve prices. The valuations \\( \\upsilon_{1} \\) and \\( \\upsilon_{2} \\) are independently drawn from a uniform distribution on [0, 1], and B1 has limited wealth of 5/24 at date 1. The expected utilities of S, B1, B2, and social welfare are denoted as \\( U \\), \\( V_{1} \\), \\( V_{2} \\), and \\( W \\)."
  },
  {
    "qid": "econ-empirical-925-0-3-1",
    "question": "2) What are the limitations of DIC, and when should it be used with caution?",
    "gold_answer": "**Limitations**:\n- **Theoretical Justification**: Less formal than Bayes factors.\n- **Model Averaging**: Unsuitable as a basis for Bayesian model averaging.\n- **Diagnostics**: Should be complemented with other checks (e.g., posterior predictive checks).\n\n**Caution**:\n- When models are non-nested or have very different structures.\n- When prior sensitivity is a concern (though DIC is robust, it is not immune).",
    "question_context": "DIC is particularly suited to compare Bayesian models whose posterior distributions have been obtained using MCMC simulation. It comprises a goodness-of-fit measure (posterior deviance) and a penalty term (effective number of parameters), making it adaptable to hierarchical models.\n\nThe article concludes that DIC is a practical and robust tool for comparing stochastic volatility models, offering computational efficiency and consistency with gold-standard methods like Chib's marginal likelihood."
  },
  {
    "qid": "econ-empirical-9-0-0-0",
    "question": "1) Derive the individual's lifetime budget constraint, accounting for the endowment $\\pmb{E}$, lump-sum money transfer $s$, and interest rate $i$ on deposits. Assume trips to the bank incur a fixed cost $a$ per withdrawal.",
    "gold_answer": "1. **Initial Wealth**: At birth, the individual has $\\pmb{E} + s$ in real terms.  \n2. **Deposit Allocation**: Let $D_0$ be the initial deposit. The remaining $\\pmb{E} + s - D_0$ is held as money.  \n3. **Withdrawals**: For $n$ withdrawals at intervals $\\tau = T/n$, the present value of withdrawal costs is $a n e^{-i t_j}$ summed over withdrawal times $t_j$.  \n4. **Budget Constraint**: Total consumption $C$ must satisfy:  \n   $$\\int_0^T c(t) e^{-i t} dt = \\pmb{E} + s - a n e^{-i \\tau}.$$",
    "question_context": "Each individual in this economy is faced with the problem that typically arises in an overlapping generations model: he is endowed only at birth but wishes to consume throughout his lifetime. Specifically, at birth he receives an endowment of amount $\\pmb{E}$ of the economy's single consumption good and a lump sum transfer of real amount $s$ of fiat money. He wishes to consume throughout his life, which has length $T$. There are two means of storing wealth: bank deposits and money. Deposits pay interest $i$; money pays no interest.\nThe model differs from traditional overlapping generations models in that it is set in continuous time. In other words, a new generation is born every instant, and at any time a continuum of generations are alive.\n\nThe paper presents a continuous-time overlapping generations model where money serves as both a store of value and medium of exchange. Agents optimize the frequency of bank trips to convert deposits into money, balancing transaction costs against interest forgone."
  },
  {
    "qid": "econ-empirical-134-3-0-1",
    "question": "2) Compare the CEO's project abandonment decision under the RD and TD contracts. How does the timing of option exercise influence this decision?",
    "gold_answer": "1. Under the RD contract, the CEO holds options until stage 4, creating a bias toward continuation. Thus, the abandonment threshold $\\theta_t^{RD} = 0$ (never abandons).\n2. Under the TD contract, the CEO may exercise options early (stage 2) if $\\theta < \\theta_e^{TD}$. If exercised, the CEO abandons the project if $\\theta < \\theta_f$ (first-best threshold). Thus, $\\theta_t^{TD} = \\min\\{\\theta_e^{TD}, \\theta_f\\}$.\n3. Timing discretion improves abandonment decisions by removing the continuation bias when options are exercised early.",
    "question_context": "The CEO exercises his options in stage 2 if and only if he observes negative information; that is, if the realization of $\\theta$ is below a certain threshold, denoted $\\theta_{e}$ , which is determined by $\\theta_{e}=\\frac{P_{2}(\\widehat{\\theta}_{t})-K}{P^{H}-K}$.\nThe CEO’s willingness to abandon bad projects crucially depends on his previous decision regarding the exercise of his options. Specifically, as discussed in the RD contract, if the CEO is holding his options until stage 4, he is biased in favor of continuing the project because project termination renders his options worthless. However, if the CEO decides to unload his options in stage 2, he is no longer biased and hence is willing to abandon the project if necessary.\n\nThe CEO's decision to abandon a project depends on the realization of $\\theta$ and his previous decision regarding the exercise of his options. The threshold $\\theta_e$ determines when the CEO exercises his options, and $\\theta_f$ is the first-best threshold for project abandonment."
  },
  {
    "qid": "econ-empirical-453-1-0-1",
    "question": "2) Critically evaluate the limitations of mean-variance models in explaining the demand for money. How do these models fail to account for capital certain assets?",
    "gold_answer": "1. **Wealth Satiation**: Mean-variance models imply that investors can achieve infinite utility with infinite wealth, which is unrealistic.\n\n2. **Normality Assumption**: Asset returns are assumed to be normally distributed, but empirical evidence (e.g., Taylor, 1986) shows fat tails and skewness.\n\n3. **Capital Certain Assets**: Narrow money (non-interest bearing) is dominated by other capital certain assets (e.g., time deposits). The model treats all such assets as homogeneous, failing to explain why money is held at all (Tsiang, 1972).",
    "question_context": "Friedman (1956) argued that the demand for assets should be based on axioms of consumer choice. He focuses on the demand for money and presents a fairly long list of possible arguments of this function (e.g. a vector of expected returns, wealth and income) with signs to be determined primarily by the data.\nModels based on the risk aversion approach (e.g. Markowitz, 1952, 1959; Sharpe, 1964; Linter, 1965; Tobin, 1958; Tsiang, 1972; Friedman and Roley, 1979; Courakis, 1988) consider the demand for a set of assets. For certain parameterisations such models allow tests of positivity, symmetry, and homogeneity of the interest rate coefficients in the asset demand functions.\nIn this paper we wish to argue that it is more useful to approach the demand for assets and money in particular, from the standpoint of neoclassical demand theory, rather than from considering explicit motives for holding money.\nTo illustrate this approach we show how a particular (yet tractable) model of consumer demand, namely the Almost Ideal Demand System (AIDS) model may be adapted to explain the demand for assets.\n\nThe paper discusses the demand for money and other financial assets, contrasting Friedman's consumer choice approach with explicit motives models (transactions, precautionary, speculative). It critiques the limitations of mean-variance models and advocates for a neoclassical demand theory approach using the Almost Ideal Demand System (AIDS) model."
  },
  {
    "qid": "econ-empirical-492-0-0-1",
    "question": "2) Prove that the social planner's investment threshold is lower than the decentralized equilibrium threshold due to monopoly distortion. Use a welfare function to formalize this result.",
    "gold_answer": "1. **Decentralized Threshold**: Firms invest if \\( \\theta \\geq \\theta^d \\), where \\( \\theta^d \\) satisfies \\( p(\\theta^d) \\cdot D - C = 0 \\), with \\( p \\) as price and \\( C \\) as fixed cost.  \n2. **Planner's Threshold**: The planner maximizes welfare \\( W = \\int (U(D) - C) \\, dF(\\theta) \\), where \\( U(D) \\) is consumer utility. The planner invests if \\( \\theta \\geq \\theta^p \\), where \\( \\theta^p \\) solves \\( U'(D) \\cdot D - C = 0 \\).  \n3. **Monopoly Distortion**: Since \\( p(\\theta) > U'(D) \\) (monopoly markup), \\( \\theta^p < \\theta^d \\). The planner internalizes consumer surplus, lowering the required productivity threshold.",
    "question_context": "Investment is modelled as a fixed cost that increases production capacity. Investment decisions are staggered. Producers of each variety receive investment opportunities according to a Poisson clock, a simple way to capture production decisions that cannot adjust overnight. This assumption implies that investment decisions are not synchronised and economic activity is a state variable.\nReturns to investment depend on future demand and hence on whether producers with subsequent investment opportunities choose to take them as well. Thus investment decisions are strategic complements, and producers have to form expectations about others’ future decisions when deciding about investment.\nIn an intermediate range, a producer’s decision depends on his expectations about the actions of others: investing is the optimal decision if agents expect others to do so, but refraining from investment is the best choice in case of pessimistic beliefs.\nThe planner requires lower productivity to invest because it internalises the benefits to consumers from cheaper prices (monopoly distortion); and the difference between the planner’s solution and the decentralised equilibrium is larger at times of low economic activity.\nThe model generates a unique set of rationalisable beliefs about others’ actions. Intuitively, fully pessimistic beliefs are not rationalisable in a region where a small shock to productivity would make it dominant for all firms to invest. Likewise, fully optimistic beliefs are not rationalisable in a region where a small shock to productivity takes the economy to a region where investing is a dominated strategy.\n\nThis article studies stimulus policies in a simple macroeconomic model featuring a dynamic coordination problem that arises from demand externalities and fixed costs of investment. In times of low economic activity, firms face low demand and hence have lower incentives for investing, which reinforces their low-demand expectations. In a benchmark case with no shocks, the economy might get trapped in a low-output regime and a social planner would be particularly keen to incentivise investment at times of low economic activity. However, this result vanishes once shocks are considered."
  },
  {
    "qid": "econ-empirical-69-0-0-2",
    "question": "3) The study finds no evidence that the gender wage gap is driven by differences in teaching quality or job mobility. Provide the empirical tests used to rule out these alternative explanations.",
    "gold_answer": "Tests include:\n1. **Teaching Quality**: Regress wages on value-added measures (e.g., student performance). Control for these in the wage gap model. Finding: No significant change in \\(\\beta_3\\).\n2. **Job Mobility**: Compare mobility rates (e.g., moving districts) by gender. Finding: No gender differences.\n3. **Outside Offers**: Survey evidence on outside offers. Finding: Men may use offers to negotiate, but this explains only a small portion of the gap.",
    "question_context": "Using quasi-exogenous variation in the timing of the introduction of flexible pay, driven by the expiration of preexisting collective-bargaining agreements, we show that flexible pay lowered the salaries of women compared with men with the same credentials.\nThe gap is larger for younger teachers and smaller for teachers working under a female principal or superintendent.\nSurvey evidence suggests that the gap is partly driven by women engaging less frequently in negotiations over pay, especially when the counterpart is a man.\n\nThe study examines the impact of flexible pay on the gender wage gap among public school teachers in Wisconsin following the 2011 Act 10 reform. The reform allowed school districts to set teachers' pay more flexibly, leading to individual negotiations. The study uses quasi-exogenous variation in the timing of the introduction of flexible pay to analyze its effects."
  },
  {
    "qid": "econ-empirical-1377-0-0-2",
    "question": "3) Explain why the region-based rating approach yields smaller aggregate surplus gains but less severe distributional consequences than the Enthoven approach. Use the regional heterogeneity results from Figure 4.",
    "gold_answer": "1. Region-based pricing preserves broad-network plans in high-valuation regions (e.g., North Shore).\n2. Let \\( \\Delta S_{Enthoven}(r) \\) and \\( \\Delta S_{Region}(r) \\) be surplus changes by region.\n3. \\( \\sum_r \\Delta S_{Enthoven}(r) > \\sum_r \\Delta S_{Region}(r) \\) due to narrower networks in Enthoven.\n4. But \\( \\text{Var}(\\Delta S_{Region}(r)) < \\text{Var}(\\Delta S_{Enthoven}(r)) \\) because region-based pricing mitigates losses in high-valuation regions.",
    "question_context": "While the Enthoven approach yields significant surplus gains on average, employers may find it undesirable to impose a policy that results in the elimination of several flagship broad-network products, particularly if the distributional consequences are severe.\nI predict that changing to an Enthoven approach would result in net surplus increases for most households, consistent with the results seen in Table 5. However, these surplus gains diminish rapidly with age.\nHouseholds at the upper end of the age distribution see much smaller gains from this approach, with households at around age 67 starting to see surplus losses.\nThe results of these simulations shed light on an important trade-off between equity and efficiency in plan design. To achieve efficiencies through uniform benefit design, the employer would likely need to offer products that impose severe utility costs on precisely the consumers whom they value the most.\nEmployers similarly struggle to strike this balance. As companies grow and cater to employees with much more heterogeneous preferences, firms have increasingly turned to offering not only more choices of plans, but also offering different types of products.\n\nThe text discusses the implications of adopting an Enthoven approach versus a region-based rating approach in health insurance markets, focusing on surplus changes across different demographic and geographic groups. It highlights trade-offs between efficiency and equity in plan design, and explores why employers might resist narrow-network plans despite potential cost savings."
  },
  {
    "qid": "econ-empirical-1775-0-0-0",
    "question": "1) What are the key economic contributions of Chinese entrepreneurs in Southeast Asia as discussed in the paper?",
    "gold_answer": "The paper highlights several key contributions:\n- **Capital Formation**: Chinese entrepreneurs have played a significant role in capital accumulation and investment in the region.\n- **Trade Networks**: They have established extensive trade networks that facilitate regional and international commerce.\n- **Employment Generation**: Their businesses have created numerous job opportunities, contributing to local economies.\n- **Innovation**: They have introduced new technologies and business practices, fostering economic growth.",
    "question_context": "Chinese Entrepreneurs in Southeast Asia by Yuan-Li Wu, published in The American Economic Review, Vol. 73, No. 2, Papers and Proceedings of the Ninety-Fifth Annual Meeting of the American Economic Association (May, 1983), pp. 112-117.\n\nThis paper examines the role and impact of Chinese entrepreneurs in Southeast Asia, focusing on their economic contributions and the challenges they face in the region."
  },
  {
    "qid": "econ-empirical-695-1-0-1",
    "question": "2) Explain how the three major families of probability concepts (rational degree of belief, limiting relative frequency, propensity) interact in a Bayesian framework. Provide a mathematical example involving conditional probabilities.",
    "gold_answer": "1. **Bayesian Framework**: Let $\\Pr_{\\text{belief}}(E)$ be the subjective degree of belief, $\\Pr_{\\text{freq}}(E)$ the limiting frequency, and $\\Pr_{\\text{prop}}(E)$ the propensity. \\n2. **Interaction**: For a coin flip with unknown bias $\\theta$, $\\Pr_{\\text{belief}}(E) = \\mathbb{E}[\\Pr_{\\text{prop}}(E | \\theta)]$. \\n3. **Example**: If $\\Pr_{\\text{prop}}(E | \\theta) = \\theta$ and $\\theta \\sim \\text{Beta}(\\alpha, \\beta)$, then $\\Pr_{\\text{belief}}(E) = \\frac{\\alpha}{\\alpha + \\beta}$.",
    "question_context": "The reason why a decision-maker in a deterministic world should require a probabilistic account of causation is that the concepts that he deals with in describing states of nature, and in particular the variables that he can control, are at such a level that in this framework acts together with states of nature do not determine consequences, but only probabilistic propensities or tendencies over the consequence space.\nThere are three major families of probability concepts: rational degree of belief, limiting relative frequency, and propensity or chance. There is room for them all to interact in a Bayesian framework.\nIf chance is probability conditional on a partition (or more generally on a ${\\pmb\\upsigma}\\cdot$ -field of sets invariant under a group of automorphisms), then different conceptions of chance are available for different purposes depending on coarseness of the partition (or ${\\pmb\\sigma}.$ -field) involved.\nIf we are interested in the extent to which $C$ influences the chance of $E$ ,we want (in the simplest case) a partition, $\\pi$ , such that its common refinement with $\\left\\{C,n o t-C\\right\\}$ determines the chance of $E$ at whatever level of coarseness is appropriate to the context.\n\nThe text discusses the rationale behind probabilistic theories of causation, distinguishing between classical and quantum reasons. It highlights the importance of probabilistic approaches in deterministic systems due to coarse partitions of state spaces and amplification of initial uncertainties. The interaction between different probability concepts (degree of belief, limiting relative frequency, propensity) within a Bayesian framework is also explored."
  },
  {
    "qid": "econ-empirical-1006-2-0-2",
    "question": "3) Prove that when the reference densities $g_i$ are uniform, the minimum cross-entropy solution reduces to the maximum entropy solution $\\hat{f}(x_{1},...,x_{m})\\propto\\exp[-\\hat{\\lambda}^{\\prime}\\gamma(x_{1},...,x_{m})]$.",
    "gold_answer": "1. For uniform $g_i$, $g_1 \\cdots g_m = c$ (constant)\n2. The cross-entropy $I(f:g) = \\int f \\ln(f/c) \\mathrm{d}{\\bf x} = \\int f \\ln f \\mathrm{d}{\\bf x} - \\ln c \\propto -H(f)$\n3. Minimizing $I(f:g)$ is equivalent to maximizing $H(f)$\n4. The solution becomes $\\hat{f} = c \\exp[\\hat{\\psi} + \\hat{\\lambda}^{\\prime}\\gamma]$, which normalizes to $\\hat{f} \\propto \\exp[\\hat{\\lambda}^{\\prime}\\gamma]$",
    "question_context": "Given a set of moments $E_{F}[\\gamma(x_{1},...,x_{m})]=\\pmb{\\mu}\\in\\mathbb{R}^{q}$ for the random variables of interest, let the class of admissible joint densities be $\\tau(\\mu)=\\{f(x_{1},\\ldots,x_{m})\\colon\\textstyle f\\mathrm{~is~an~}m\\mathrm{-variate~PDF~with~moments~}\\mu\\}$. A plausible choice among the candidates in $\\tau(\\pmb{\\mu})$ is the joint density that introduces the least additional mutual information relative to $g_{1}(x_{1})\\cdot\\cdot\\cdot g_{m}(x_{m})$.\nUnder the minimum cross-entropy criterion, the density recovery problem is to choose $f(x_{1},\\dots,x_{m})\\geq0$ to minimize $I(f:g)=\\int f(x_{1},\\ldots,x_{m})\\ln\\left[\\frac{f(x_{1},\\ldots,x_{m})}{g_{1}(x_{1})\\cdot\\cdot\\cdot g_{m}(x_{m})}\\right]\\mathrm{d}\\mathbf{x}$, subject to $1=\\displaystyle\\int f(x_{1},\\ldots,x_{m})\\mathrm{d}{\\bf x}$ and $\\displaystyle\\mu=\\int\\gamma(x_{1},\\ldots,x_{m})f(x_{1},\\ldots,x_{m})\\mathrm{d}{\\bf x}$.\nBy the calculus of variations, Euler’s condition requires $\\ln[\\hat{f}(x_{1},...,x_{m})]+1-\\ln[g_{1}(x_{1})\\cdot\\cdot\\cdot g_{m}(x_{m})]-\\hat{\\psi}-\\hat{\\lambda}^{\\prime}\\gamma(x_{1},...,x_{m})=0$, where $\\hat{\\psi}$ is the Lagrange multiplier on the normalization constraint, and $\\hat{\\lambda}$ is the Lagrange multiplier on the moment constraints.\nThe solution to the problem is $\\hat{f}(x_{1},\\dots,x_{m})=g_{1}(x_{1})\\cdot\\cdot\\cdot g_{m}(x_{m})\\exp[\\hat{\\psi}+\\hat{\\lambda}^{\\prime}\\gamma(x_{1},\\dots,x_{m})]$. If $\\pmb{\\mu}=E_{G}[\\gamma(x_{1},\\dots,x_{m})]$, the moment constraints are not binding (i.e. $\\hat{\\lambda}=0$) and the solution reduces to ${\\hat{f}}(x_{1},\\ldots,x_{m}){=}g_{1}(x_{1})\\cdot\\cdot\\cdot g_{m}(x_{m})$.\nThe concentrated or dual objective function $M(\\lambda)=\\mu^{\\prime}\\lambda-\\ln[\\varOmega(\\lambda)]$ may be formed by substituting the solution into the Lagrangian expression. If $\\tau(\\pmb{\\mu})\\neq\\emptyset,M(\\pmb{\\lambda})$ is strictly concave in $\\lambda$ by the saddle point property of the constrained problem.\n\nThis section discusses the use of the minimum cross-entropy criterion to recover joint densities with given moments, building on existing literature in information theory and econometrics. The approach involves minimizing mutual information relative to a reference distribution while satisfying moment constraints."
  },
  {
    "qid": "econ-empirical-319-1-0-0",
    "question": "1) Derive the first-order conditions for the profit-maximizing firm (PMF), labor-managed firm (LMF), and joint-stock firm (JSF) using the unified framework $\\operatorname*{max}_{X} Y = P X - C(X)$. Explain the economic interpretation of each condition.",
    "gold_answer": "1. **PMF**: The first-order condition is $P = C'_M(X_M)$, where $C'_M(X_M)$ is the marginal cost of total output. This means the firm equates price to marginal cost.\n2. **LMF**: The first-order condition is $P = C'_L(X_L)$, where $C'_L(X_L)$ is the marginal capital cost per worker. This means the firm equates price to the marginal cost of output per worker.\n3. **JSF**: The first-order condition is $P = C'_K(X_K)$, where $C'_K(X_K)$ is the marginal labour cost per machine. This means the firm equates price to the marginal cost of output per machine.",
    "question_context": "The quantity of output is denoted by $X_{M}$; its price is denoted by $P$. The quantity of labour and capital inputs are denoted by $L$ and $\\pmb{K}$ respectively, and their respective prices are $W$ and $V$. Output is related to inputs through the production function: $X_{M}=f(L,K)$, which is assumed to have the usual properties of strict concavity: $f(0,K)=f(L,0)=0;f_{1},f_{2}>0;f_{11},f_{22}<0;f_{11}f_{22}>f_{12}^{2}$, where subscripts denote partial derivatives.\nThe profit-maximising firm (PMF) acts so as to maximise profits, denoted by $Y_{M}$, given by: $Y_{M}=P X_{M}-W L-V K$. The labour-managed firm (LMF) seeks to maximise net income per worker, denoted by $Y_{L}$, given by: $Y_{L}=(P X_{M}-V K)/L$. The joint-stock firm (JSF) seeks to maximise net income per machine, denoted by $Y_{K}$, given by: $Y_{K}=(P X_{M}-W L)/K$.\nThe maximisations are carried out subject to the production function and additional constraints depending on the 'rules of the game'. Constraints include $L=\\bar{L}$ (the $\\bar{L}$ short run), $K=\\widetilde{K}$ (the $\\bar{K}$ short run), or neither (the long run).\nOutput per worker and output per machine are defined as $X_{L}=X_{M}/L$ and $X_{K}=X_{M}/K$, respectively. Three minimum cost functions are introduced: $C_{M}(X_{M})$ (minimum total cost for output $X_{M}$), $C_{L}(X_{L})$ (minimum capital cost per worker for output per worker $X_{L}$), and $C_{K}(X_{K})$ (minimum labour cost per machine for output per machine $X_{K}$).\nThe unified approach shows that each firm's problem can be expressed as $\\operatorname*{max}_{X} Y = P X - C(X)$, where $Y$, $X$, and $C$ have appropriate subscripts. The first- and second-order conditions for the solution are $P = C'(X)$ and $C''(X) > 0$, respectively.\n\nThis section presents a theoretical framework for analyzing the behavior of three types of firms—profit-maximizing firms (PMF), labor-managed firms (LMF), and joint-stock firms (JSF)—under conditions of certainty. The firms operate in a perfectly competitive market, producing a single output using labor and capital as inputs. The production function and objective functions for each firm type are defined, and the unified approach to analyzing their behavior is introduced."
  },
  {
    "qid": "econ-empirical-1461-3-0-0",
    "question": "1) Using a utility framework, formally model how the introduction of new goods affects the utility of a subgroup that still values older goods. Assume the older goods face a price increase due to loss of economies of scale.",
    "gold_answer": "1. Let the utility function be \\( U(x_1, x_2) \\), where \\( x_1 \\) is the older good and \\( x_2 \\) is the new good. \\n2. The budget constraint is \\( p_1 x_1 + p_2 x_2 \\leq M \\), where \\( p_1 \\) increases due to loss of economies of scale. \\n3. The subgroup's utility maximization problem becomes constrained, leading to a reduction in \\( x_1 \\) consumption. \\n4. The compensating variation (CV) can be calculated to measure the welfare loss: \\( CV = e(p_1^1, p_2, U^0) - e(p_1^0, p_2, U^0) \\), where \\( p_1^1 > p_1^0 \\).",
    "question_context": "New goods may drive out older goods which are still valued by a subgroup of the population, or what is equivalent, the loss of economies of scale may drive up their price significantly.\nAn existing good or service may deteriorate in its quality. That is a less frequent phenomenon, in spite of the mantra that 'they don't make them the way they used to.'\nThe largest effect may come from changes in our physical, social, and economic environment which impose on us higher expenditures necessary to keep up with our previously achieved utility levels.\nThe CPI does not rise in a cold winter just because we burn more energy to keep ourselves warm, or historically, because our population is older now and hence requires a higher level of medical expenditures.\nA change in expenditures due to an unanticipated change in weather should raise the price index only to the extent that energy prices go up, not quantities consumed.\nThe appearance of AIDS will drive up the price index of health if we define it as the expenditure necessary to achieve an equivalent base-period health level.\nCounting quality improvements in locks and other security devices, may overestimate the 'gains' from such defensive consumer investments.\n\nThe text discusses the complexities of measuring quality of life changes, including the introduction of new goods, deterioration of existing goods, and changes in the physical, social, and economic environment. It highlights the limitations of traditional price indexes like the CPI and COLI in capturing these changes."
  },
  {
    "qid": "econ-empirical-667-0-0-0",
    "question": "1) Derive the production function framework used to estimate the separate effects of multiple capital types, including ICT capital. What are the key assumptions and mathematical formulations?",
    "gold_answer": "The production function can be expressed as: \\( Y = F(K, L, M, \\theta) \\), where \\( Y \\) is output, \\( K \\) is capital, \\( L \\) is labor, \\( M \\) is materials, and \\( \\theta \\) represents productivity. The marginal product of each capital type \\( K_i \\) is derived as \\( \\frac{\\partial Y}{\\partial K_i} \\). Key assumptions include constant returns to scale and perfect competition in factor markets.",
    "question_context": "This article explores the relationship between capital composition and productivity using a unique, detailed dataset on firm investment in the United States in the late 1990s. I develop a methodology for estimating the separate effects of multiple capital types in a production function framework.\nI find that although most capital types earned normal returns, information and communications technology capital goods had marginal products substantially above their rental prices.\n\nThis article explores the relationship between capital composition and productivity, focusing on heterogeneous capital types, particularly information and communications technology (ICT). The study uses a unique dataset to estimate the marginal products of different capital types and compares these with rental prices."
  },
  {
    "qid": "econ-empirical-1735-1-0-1",
    "question": "2) Explain how the information sets $I_X(t-1)$ and $I_{XY}(t-1)$ are constructed from the reference information set $I(t-1)$ and the histories of $X$ and $Y$.",
    "gold_answer": "The information sets $I_X(t-1)$ and $I_{XY}(t-1)$ are constructed by augmenting the reference information set $I(t-1)$ with the histories of $X$ and $Y$. Specifically, $I_X(t-1) = I(t-1) + X(\\omega,t-1]$ and $I_{XY}(t-1) = I_X(t-1) + Y(\\omega,t-1]$, where $X(\\omega,t-1]$ and $Y(\\omega,t-1]$ represent the information contained in the history of $X$ and $Y$ up to time $t-1$, respectively.",
    "question_context": "We consider two variables of interest $X$ and $Y$. For the simplicity of exposition, we assume that $X$ and $Y$ are univariate processes; however, in Section 7 we include a third variable $W$.\nWe wish to measure the Granger causality in quantiles between $X$ and $Y$. When it comes to causality analysis, defining the information sets is crucial. Hereafter, we consider a sequence $I$ of 'reference information sets' $I(t-1)$ such that $I=\\{I(t):t\\in\\mathbb{Z},t>\\omega\\}\\quad\\mathrm{with~}\\quad t<t^{\\prime}\\Rightarrow I(t)\\subseteq I(t^{\\prime})\\quad\\mathrm{~for~all~}\\quad t>\\omega,$ where $I(t)$ is an information set, $\\omega\\in\\mathbb{Z}\\cup\\{-\\infty\\}$ represents a 'starting point,' and $\\mathbb{Z}$ is the set of integers.\n\nThe framework considers two variables of interest, $X$ and $Y$, initially assumed to be univariate processes. The analysis focuses on Granger causality in quantiles between these variables, with an extension to include a third variable $W$ in Section 7."
  },
  {
    "qid": "econ-empirical-1411-4-0-1",
    "question": "2) Using Samuelson's Social Indifference Curves, show how aggregate inferiority can coexist with individual normality. Assume two goods \\( (x, y) \\) and two individuals \\( (i, j) \\).",
    "gold_answer": "**Steps:**\n1. Define individual demands: \\( x_i(p_x, p_y, I_i) \\), \\( y_i(p_x, p_y, I_i) \\), where \\( I_i \\) is income.\n2. Aggregate demand: \\( X = x_i + x_j \\), \\( Y = y_i + y_j \\).\n3. If \\( \\frac{\\partial x_i}{\\partial I_i} > 0 \\) and \\( \\frac{\\partial x_j}{\\partial I_j} > 0 \\), but income redistribution (e.g., transfer \\( T \\)) causes \\( \\frac{\\partial X}{\\partial I} < 0 \\) due to price effects, aggregate inferiority emerges.",
    "question_context": "The paradoxes of enriched donor and immiserized recipient cannot arise unless a distortion is present in the system.\nSocial (i.e., aggregate) inferiority is compatible with absence of inferiority in individual consumption.\nInferiority in individual consumption is itself by no means a theoretical curiosum.\n\nThe analysis explores exogenous (policy-induced) and endogenous (transfer-induced) distortions in a two-agent framework, extending insights from Bhagwati-Brecher-Hatta [1983]. It generalizes that paradoxes like enriched donor and immiserized recipient require the presence of distortions, which are pervasive in economic systems."
  },
  {
    "qid": "econ-empirical-836-4-0-2",
    "question": "3) Show how the bounds on $(\\alpha_0, \\alpha_1)$ from Theorem 2.2 remain valid in the LATE model under the modified Assumption C.4.",
    "gold_answer": "1. The original bounds rely on the condition $\\mathbb{E}[Y|T^*,T,z] = \\mathbb{E}[Y|T^*,z]$.\n2. In the LATE model, this is replaced by Assumption C.4: \n   $$ \\mathbb{E}[Y(0)|T^*,T,z] = \\mathbb{E}[Y(0)|T^*,z] \\text{ and } \\mathbb{E}[Y(1)|T^*,T,z] = \\mathbb{E}[Y(1)|T^*,z]. $$\n3. Since $Y = (1-T^*)Y(0) + T^*Y(1)$, the condition ensures that measurement error $T$ does not provide additional information about potential outcomes beyond $T^*$ and $z$.\n4. Thus, the bounds on $(\\alpha_0, \\alpha_1)$ derived under the additively separable model still hold, as they depend only on the joint distribution of $(T, T^*)$ and the exclusion restriction.",
    "question_context": "In lieu of Assumption 2.1(i), consider a non-separable model of the form $y=h(T^{*},z,\\varepsilon)$. Let $T^{*}(z)$ denote an individual’s potential treatment and $Y(t^{*},z)$ denote her potential outcome, where $t^{*},z\\in\\{0,1\\}$. Using this notation we can write $Y(t^{*},z)=h(t^{*},z,\\varepsilon)$. Le $\\cdot J\\in\\{a,c,d,n\\}$ index the four LATE principal strata: $a=$ always-taker, $c=$ complier, $d=$ defier, and $n=$ never-taker.\nAssumption C.1 (Unconfounded Type). Assumption C.2 (Mean Exclusion Restriction). For all $t^{*}\\in\\{0,1\\}$ and $j\\in\\{a,c,d,n\\}$, $$\\begin{array}{r}{\\mathbb{E}\\left[Y(t^{*},0)|T^{*}=t^{*},z=1\\right]={\\mathbb{E}\\left[Y(t^{*},1)|T^{*}=t^{*},z=1\\right]}={\\mathbb{E}[Y(t^{*})|J=j]}.}\\end{array}$$ Assumption C.3 (Monotonicity). $\\mathbb{P}\\big(T^{*}(1)\\geq T^{*}(0)\\big)=1$.\nThe numerator of the preceding expression is observed, but under mis-classification the denominator is not. Notice, however, that Assumption 2.2(i)–(ii) only concern the joint distribution of $T$ given $(T^{*},z)$. As such, they have the same meaning in a LATE model as in an additively separable model. Imposing these conditions, Lemma 2.1 continues to hold in a LATE model. It follows that $p_{1}-p_{0}=(1-\\alpha_{0}-\\alpha_{1})(p_{1}^{*}-p_{0}^{*})$ so that $$\\frac{\\mathbb{E}[y|z=1]-\\mathbb{E}[y|z=0]}{p_{1}-p_{0}}=\\frac{\\mathbb{E}[Y(1)-Y(0)|J=c]}{1-\\alpha_{0}-\\alpha_{1}}.$$\nAssumption 2.2(iii), non-differential measurement error, is explicitly stated in terms of the unobservable error term in an additively separable model. Our derivation of the additional restrictions on $(\\alpha_{0},\\alpha_{1})$ implied by non-differential measurement error in the proof of Theorem 2.2, however, does not use Assumption 2.2(iii) directly. Rather, it uses a condition that is equivalent to it in an additively separable model, namely $\\mathbb{E}[Y|T^{*},T,z]=\\mathbb{E}[Y|T^{*},z]$.\n\nThis section discusses the extension of partial identification results to a local average treatment effects (LATE) setting, replacing the additively separable model with a non-separable model and adjusting assumptions accordingly."
  },
  {
    "qid": "econ-empirical-881-5-0-1",
    "question": "2) Show that the solution to the minimization problem in Eq. (12) implies \\((e^{*}, b^{*}, s^{*}) = (\\varepsilon, \\beta, \\sigma)\\).",
    "gold_answer": "1. Given the minimization problem in Eq. (12), the solution \\((0, \\beta, \\sigma)\\) satisfies:\n   \\[ \\mathbb{E}\\left[F_{n^{*}|X}(n, X)\\right] = \\mathbb{E}\\left[\\Phi\\left((n - X\\beta)/\\sigma\\right)\\right]. \\]\n2. By the equivalence of the optimization problems, the solution maps to \\((e^{*}, b^{*}, s^{*}) = (\\varepsilon, \\beta, \\sigma)\\).\n3. Thus, \\(G_{n^{*}}^{*}(n) = \\mathbb{E}\\left[\\Phi\\left((n - X b^{*})/s^{*}\\right)\\right] = \\mathbb{E}\\left[\\Phi\\left((n - X\\beta)/\\sigma\\right)\\right] = F_{n^{*}}(n)\\).",
    "question_context": "The probability limit of the Tobit MLE is \\((e^{*},b^{*},s^{*})\\) and \\((b^{*},s^{*})\\) corresponds to a unconditional distribution of \\(n^{*}\\colon G_{n^{*}}^{*}(n)=\\) \\(\\mathbb{E}\\left[\\pmb{\\varPhi}((n-X b^{*})/s^{*})\right]\\) , where \\(G_{n^{*}}^{*}\\in{\\mathcal{F}}_{n^{*}}\\) . The condition \\(F_{y}(y)=G_{y}^{*}(y)\\) is equivalent to: \n\n$$\n\\begin{array}{l}{{G_{y}^{*}(y)=G_{n^{*}}^{*}(y-e^{*}s_{0})=F_{n^{*}}(y-\\varepsilon s_{0})=F_{y}(y)\\mathrm{for}\\forall y<k,}}\\ {{\\qquad G_{y}^{*}(y)=G_{n^{*}}^{*}(y-e^{*}s_{1})=F_{n^{*}}(y-\\varepsilon s_{1})=F_{y}(y)\\mathrm{for}\\forall y\\geq k.}}\\end{array}\n$$\nAssumption 2(i) says that \\(\\boldsymbol{\\mathcal{F}}_{n^{*}}\\) satisfies Assumption 1. Assumption 1 says that the only values of \\(e\\geq0\\) and \\(G_{n^{*}}\\in{\\mathcal{F}}_{n^{*}}\\) that solve the equations \n\n$$\n\\begin{array}{c}{{G_{n^{*}}(y-e s_{0})=F_{n^{*}}(y-\\varepsilon s_{0})=F_{y}(y)\\mathrm{for}\\forall y<k,}}\\ {{G_{n^{*}}(y-e s_{1})=F_{n^{*}}(y-\\varepsilon s_{1})=F_{y}(y)\\mathrm{for}\\forall y\\geq k,}}\\end{array}\n$$ \n\nare \\(e=\\varepsilon\\) and \\(G_{n^{*}}=F_{n^{*}}\\) . Therefore, \\(e^{*}=\\varepsilon\\) and \\(G_{n^{*}}^{*}=F_{n^{*}}\\).\n\nThe text discusses the probability limit of the Tobit MLE and its correspondence to the unconditional distribution of \\( n^{*} \\). It introduces key equations and assumptions to establish the equivalence between \\( F_{y}(y) \\) and \\( G_{y}^{*}(y) \\)."
  },
  {
    "qid": "econ-empirical-1211-2-0-0",
    "question": "1) Derive the mathematical relationship between net price ($p_n$) and family income ($I$) for a school with a proportional pricing policy, and contrast it with a school exhibiting a decreasing-share pricing policy. Use the regression framework mentioned in the text.",
    "gold_answer": "1. **Proportional Pricing Policy**: The regression model is $\\frac{p_n}{I} = \\beta_0 + \\beta_1 I + \\epsilon$. For proportionality, $\\beta_1 = 0$, implying $\\frac{p_n}{I} = \\beta_0$ or $p_n = \\beta_0 I$.\\n2. **Decreasing-Share Policy**: Here, $\\beta_1 < 0$, so $\\frac{p_n}{I} = \\beta_0 + \\beta_1 I$. Rearranging, $p_n = \\beta_0 I + \\beta_1 I^2$, showing net price increases less than proportionally with income.",
    "question_context": "There is marked variety in the net prices charged the low- and lower-middle-income students among these 28 schools. Averaged over the COFHE groups, low-income students pay net prices that range from $\\$5,487$ at the coed colleges to $\\$8,169$ at the Ivy universities--almost 50 percent more.\nFor students from the lowest-income families, the average net price they pay is lower than other students’ but varies a great deal between schools—measured as a share of median family income for this quintile ($\\$15,347$), net prices range from 5 percent to 74 percent.\nThe other important fact about price revealed by both the table and figures is that most of the increase in the income share of net prices charged of lower-income students appears between the low-income and lower-middle-income quintiles.\nOur solution was to run a simple linear regression of each school's average net price/income ratios on median incomes over the five quintiles and the 95th percentile and treat the t-statistic on the income coefficient as an indicator—an index—of pricing policy.\n\nThis section examines the variation in net prices charged to low- and lower-middle-income students across 28 schools, highlighting differences in pricing policies and their implications for affordability."
  },
  {
    "qid": "econ-empirical-201-3-0-0",
    "question": "1) Prove the uniqueness of the LDL decomposition for the symmetric and positive definite matrix $\boldsymbol{T}(\boldsymbol{p})^{-1}$. What are the implications of this uniqueness for the matrices $L(p)$ and $\\Sigma(p)$?",
    "gold_answer": "1. **Uniqueness of LDL Decomposition**: For any symmetric and positive definite matrix, the LDL decomposition is unique. This means there exists a unique unit lower triangular matrix $L(p)$ and a unique diagonal matrix $\\Sigma(p)$ such that $T(p) = L(p) \\Sigma(p) L(p)^\\prime$. \\n2. **Implications for $L(p)$ and $\\Sigma(p)$**: The uniqueness ensures that the entries of $L(p)$ and $\\Sigma(p)$ are uniquely determined by the matrix $T(p)$. In the context of the inverse Yule–Walker matrix, this guarantees that the factorization $\boldsymbol{{\\Gamma(p)^{-1}}}=\boldsymbol{{L(p)^{\\prime}}}\boldsymbol{{\\Sigma(p)^{-1}}}\boldsymbol{{L(p)}}$ is also unique.",
    "question_context": "Since the LDL decomposition is unique, there always exist a uniquely determined diagonal matrix $\\Sigma(p)$ and a uniquely determined unit lower triangular matrix $L(p)$ such that (11) holds true.\nThe reshaping past (RP) projection coefficients $r_{j}(p)$ are determined by the projection of $X_{\\underline{{t}}-\\underline{{k}}_{p+1}}$ onto $s p\\{X_{\\underline{{t}}-\\underline{{k}}_{p}},\\underline{{\\cdot\\cdot\\cdot}},X_{\\underline{{t}}-\\underline{{k}}_{1}}\\}$.\nTheorem 3.3 states that the entries of the matrices in the factorization $\boldsymbol{{\\Gamma(p)^{-1}}}=\boldsymbol{{L(p)^{\\prime}}}\boldsymbol{{\\Sigma(p)^{-1}}}\boldsymbol{{L(p)}}$ are given by the RP projection coefficients and their corresponding projection error variances.\nCorollary 3.4 shows that under the Toeplitz condition, the finite predictor coefficients and the RP projection coefficients coincide, i.e., $a_{\\underline{{k}}_{j}}(p)=r_{j}(p)$ for all $p\\in\\mathbb{N}$.\n\nThis section explores the Cholesky-type factorization of the inverse Yule–Walker matrix, focusing on its connection to prediction problems in spatial settings. The discussion includes the uniqueness of the LDL decomposition, the role of reshaping past (RP) projections, and the implications for inference when the Toeplitz condition is not met."
  },
  {
    "qid": "econ-empirical-1381-4-1-2",
    "question": "7) Discuss the heterogeneous effects of the minimum wage on reallocation in tradable vs. nontradable sectors, referencing the findings in Online Appendix A.16.",
    "gold_answer": "Reallocation effects are stronger in the nontradable sector because:\n- Local price-setting allows establishments to pass on wage costs.\n- In tradable sectors, national/international competition limits wage adjustments.\nThis aligns with models where local monopsony power varies by sector.",
    "question_context": "The table reports difference-in-differences estimates for the 2014 versus 2016 postpolicy period (columns (1) and (2)) and the 2012 versus 2014 placebo prepolicy period (columns (3) and (4)), as in columns (4) and (5) in Table II.\nIn column (1), we report difference-in-differences estimates based on equation (2) without any control variables. In columns (2)–(4), we successively add individual-level demographic control variables and industry and location fixed effects.\nThe findings described in Online Appendix A.16 further show that the minimum wage did not lead to displacement effects for any of the subgroups of low-wage workers that we considered.\n\nThis section covers the difference-in-differences methodology used to estimate the effects of the minimum wage, including robustness checks and heterogeneous effects across subgroups."
  },
  {
    "qid": "econ-empirical-757-2-0-2",
    "question": "3) Discuss the implications of replacing the stationary process $z_{t}$ with the locally stationary process $z_{n t}$ in the model.",
    "gold_answer": "Replacing the stationary process $z_{t}$ with the locally stationary process $z_{n t}$ affects the asymptotic properties of the estimators. Specifically, it modifies the matrices $\\varPi_{12}, \\varPi_{21}, \\varPi_{23}, \\varPi_{32}$, and $\\varPi_{22}$, leading to new counterparts $\\widetilde{\\cal{\\Pi}}_{12}, \\widetilde{\\cal{\\Pi}}_{23}$, and $\\widetilde{\\boldsymbol{\\Pi}}_{22}$. This change impacts the convergence rates and the asymptotic distributions of the estimators, as detailed in the theorems.",
    "question_context": "In this case we have $\\widehat{c}-c=(\\widetilde{B}_{n k}^{\\intercal}\\widetilde{B}_{n k})^{-1}\\widetilde{B}_{n k}^{\\intercal}(\\widetilde{\\gamma}+e)$ The asymptotics of $\\widetilde{B}_{n k}^{\\intercal}\\widetilde{B}_{n k}$ is given by Lemma A.6. Note that $\\widetilde{B}_{n k}$ is the same as $B_{n k}$ but the stationary process $z_{t}$ is replaced by the locally stationary process $z_{n t}$.\nDefine $\\begin{array}{r}{\\bar{U}_{k}=\\mathrm{diag}(\\bar{U}_{*},L_{W}(1,0)I_{k_{3}})}\\end{array}$ , where $\\bar{U}_{*}=(\\bar{U}_{*i j})$ is a symmetric $2\\times2$ block matrix of order $(k_{1}+k_{2})\\times(k_{1}+k_{2})$ with $\\widetilde{U}_{*11}=I_{k_{1}}$ , $\\begin{array}{r}{\\widetilde{U}_{*12}=\\int_{0}^{1}\\phi_{k_{1}}(r)\\mathbb{E}[a_{k_{2}}(z_{1}(r))^{\\intercal}]d r}\\end{array}$ with elements $\\begin{array}{r}{\\int_{0}^{1}\\varphi_{i}(\\boldsymbol{r})\\mathbb{E}[p_{j}(z_{1}(\\boldsymbol{r}))]d\\boldsymbol{r}}\\end{array}$ for $1\\leq i\\leq k_{1}$ , $0\\leq j\\leq k_{2}-1$ , and $\\begin{array}{r}{\\widetilde{U}_{*22}=\\int_{0}^{1}\\mathbb{E}[a_{k_{2}}(z_{1}(r))a_{k_{2}}(\\bar{z}_{1}(r))^{\\intercal}]d r}\\end{array}$ with elements $\\begin{array}{r}{\\int_{\\mathcal{0}}^{1}\\mathbb{E}[p_{i}(z_{1}(r))p_{j}(z_{1}(r))]d r}\\end{array}$ for $i,j=0,\\ldots,k_{2}-1.$\n\nThis section discusses the estimators for a model involving locally stationary processes and their extensions, focusing on the asymptotic properties and the impact of replacing stationary processes with locally stationary ones."
  },
  {
    "qid": "econ-empirical-1437-0-0-0",
    "question": "1) Derive the Newey-West variance estimator for GMM estimators, ensuring the estimator is positive semi-definite. Discuss the implications of heteroskedasticity and autocorrelation in this context.",
    "gold_answer": "1. Start with the GMM estimator \\( \\hat{\\theta} \\) minimizing \\( Q_n(\\theta) = g_n(\\theta)' W_n g_n(\\theta) \\).\n2. The asymptotic variance is \\( (G'WG)^{-1} G' \\Omega WG (G'WG)^{-1} \\), where \\( \\Omega \\) is the variance of \\( \\sqrt{n} g_n(\\theta_0) \\).\n3. Newey-West estimator: \\( \\hat{\\Omega} = \\hat{\\Gamma}_0 + \\sum_{j=1}^m w(j,m) (\\hat{\\Gamma}_j + \\hat{\\Gamma}_j') \\), where \\( \\hat{\\Gamma}_j = n^{-1} \\sum_{t=j+1}^n g_t g_{t-j}' \\) and \\( w(j,m) \\) are weights ensuring positive semi-definiteness.",
    "question_context": "Newey and Ken West develop the first positive semi-definite heteroskedasticity and autocorrelation consistent method for estimating the asymptotic variances of GMM estimators.\n\nWhitney Newey's early work further developed generalized method of moments (GMM) estimation and testing, introduced by Hansen (1982). His most cited article, Newey and West (1987), develops the first positive semi-definite heteroskedasticity and autocorrelation consistent method for estimating the asymptotic variances of GMM estimators."
  },
  {
    "qid": "econ-empirical-499-1-0-0",
    "question": "1) Derive the uniform convergence rate for the nonparametric estimator $\\hat{m}(x,u)$ under conditions (A1)-(A6), explicitly showing how each term in $O_{P}\\left(\\sqrt{\\frac{\\log(N T)}{N T h_{x}h_{u}}}+\\frac{1}{T^{r}h_{x}}+h_{x}^{2}+h_{u}^{2}\\right)$ arises from bias-variance decomposition.",
    "gold_answer": "1. **Variance term**: $\\sqrt{\\frac{\\log(N T)}{N T h_{x}h_{u}}}$ arises from the stochastic fluctuation of $\\hat{m}^{V}(x,u) = \\hat{m}(x,u) - \\mathbb{E}[\\hat{m}(x,u)]$\\n2. **Bias term**: \\n   - $h_{x}^{2} + h_{u}^{2}$ comes from standard kernel smoothing bias $\\hat{m}^{B}(x,u) = \\mathbb{E}[\\hat{m}(x,u)] - m(x,u)$\\n   - $\\frac{1}{T^{r}h_{x}}$ represents nonstationarity deviation bias, where $r = \\min(\\rho,1)$ from (A1)",
    "question_context": "For each individual $i\\geq1$ , the process $\\{X_{t,T}\\}$ is locally stationary if for each time point $u\\in[0,1]$ , there exists an associated strictly stationary process $\\{X_{t}(u)\\}$ such that $\\|X_{t,T}-X_{t}(u)\\|\\leq$ $\\begin{array}{r}{\\left(\\left|\\frac{t}{T}-u\\right|+\\frac{\\bar{1}}{T}\\right)U_{t,T}(u)}\\end{array}$ almost surely, where $\\{U_{t,T}(u)\\}$ is a process of positive variables satisfying $E[(U_{t,T}(u))^{\\rho}]\\leq C$ for some $\\rho>0$ and $C<\\infty$.\nUnder conditions (A1)-(A6) and with $r=\\operatorname*{min}\\{\\rho,1\\}$ , where $\\rho$ is introduced in (A1), we have $\\begin{array}{r l}&{\\underset{x\\in S,u\\in[0,1]}{\\operatorname*{sup}}|\\hat{m}(x,u)-m(x,u)|}\\ &{\\quad=O_{P}\\left(\\sqrt{\\frac{\\log(N T)}{N T h_{x}h_{u}}}+\\frac{1}{T^{r}h_{x}}+h_{x}^{2}+h_{u}^{2}\\right).}\\end{array}$\nUnder conditions (A1)-(A6) and letting $\\begin{array}{l}{r{\\mathrm{~>~}}{\\frac{1}{2}}}\\end{array}$ to ensure that the bandwidths can be chosen to satisfy the optimal convergence rate, we have, as $N\\to\\infty$ and $T\\to\\infty$ , $\\begin{array}{c}{\\displaystyle{\\sqrt{N T h_{x}h_{u}}\\left[\\left(\\begin{array}{c}{\\hat{m}(x,u)}\\ {h_{x}\\frac{\\partial\\hat{m}(x,u)}{\\partial x}}\\ {h_{u}\\frac{\\partial\\hat{m}(x,u)}{\\partial u}}\\end{array}\\right)-\\left(\\begin{array}{c}{m(x,u)}\\ {h_{x}\\frac{\\partial m(x,u)}{\\partial x}}\\ {h_{u}\\frac{\\partial m(x,u)}{\\partial u}}\\end{array}\\right)-B_{x,u}\\right]}}\\ {\\displaystyle{\\xrightarrow{\\mathcal{D}}N(\\mathbf{0},V_{x,u})},}\\end{array}$\n\nThis section establishes the theoretical properties of the proposed nonparametric estimator under regularity conditions (A1)-(A6), covering locally stationary processes, mixing conditions, and kernel function requirements."
  },
  {
    "qid": "econ-empirical-1342-3-0-2",
    "question": "3) Explain why no-envy implies no-domination, and how this relationship is used in the proof of Proposition 2.",
    "gold_answer": "1. **Implication**: No-envy implies no-domination because if an agent weakly prefers her own bundle to everyone else's (no-envy), it is impossible for another agent to contribute less input while receiving more output (no-domination).  \n2. **Proposition 2**: The proof shows that any Pareto optimal allocation dominating the best average cost equilibrium must violate no-domination. Since no-envy implies no-domination, this also rules out no-envy, reinforcing the incompatibility.",
    "question_context": "An allocation rule satisfies unanimity lower bound if and only if every agent in every economy of $\\mathbf{E}^{\\mathrm{C}}$ is at least as well off as at a Pareto optimal allocation of the economy where all other agents have the same preferences as hers, and where all agents are treated equally [12, 15].\nAn allocation rule satisfies no-envy if and only if every agent weakly prefers her own bundle to everybody else's bundle [6].\nAn allocation rule satisfies no-domination if and only if no agent contributes less to the input while receiving more of the output than any other agent.\nProposition 1. No allocation rule $S$ satisfying the average cost lower bound also satisfies the unanimity lower bound.\nProposition 2. For any strictly concave production function $f_{;}$ , there exists a profile $R$ such that in the economy $(R,f)$ every Pareto optimal allocation which Pareto dominates the best average cost equilibrium allocation violates no-domination.\n\nThis section explores the incompatibilities between various allocation rules and their properties, such as the average cost lower bound, unanimity lower bound, no-envy, and no-domination. The focus is on the domain $\\mathbf{E}^{\\mathrm{C}}$ due to its unique properties where the unanimity lower bound is always feasible and no-envy does not conflict with Pareto optimality."
  },
  {
    "qid": "econ-empirical-489-5-2-0",
    "question": "1) Discuss the advantages of using high-dimensional control specifications in estimating the effect of 401(k) participation on asset holdings.",
    "gold_answer": "1. **Flexibility**: High-dimensional controls allow for flexible modeling of confounders, capturing nonlinearities and interactions that may be missed by low-dimensional specifications.\n2. **Robustness**: By selecting relevant controls, the methods reduce omitted variable bias and improve the robustness of the estimates, as shown in the application.",
    "question_context": "We use the same data as Chernozhukov and Hansen (2004). The data consist of 9915 observations at the household level drawn from the 1991 SIPP.\nWe present detailed results for three different sets of controls $f(X)$: Quadratic Spline, Quadratic Spline Plus Interactions, and Quadratic Spline Plus Many Interactions.\n\nThis section applies the high-dimensional methods to estimate the effect of 401(k) participation on asset holdings, comparing results across different control specifications."
  },
  {
    "qid": "econ-empirical-1270-1-0-2",
    "question": "3) Prove that the central bank's quadratic loss function $\\mathcal{L}_{t} = \\mathrm{E}_{t} \\sum_{s=t}^{\\infty} \\beta^{s-t} (\\pi_{s}^{2} + \\alpha c_{s}^{2})$ exhibits certainty equivalence in this model.",
    "gold_answer": "Certainty equivalence holds because:\n1. The model is linear-quadratic (LQ) with additive shocks.\n2. The optimal policy depends only on the deterministic part of the state variables ($b_{t}, c_{t}$).\n3. The stochastic shock $u_{t}$ enters additively in the Phillips curve but doesn't affect the policy rule coefficients.\n4. Thus, the optimal consumption rule $c_{t}(b_{t})$ is identical whether $u_{t}=0$ or $u_{t}\\neq 0$.",
    "question_context": "The discretionary equilibria that we examine are all examples of sustainable equilibria. In this sense, our research is related to the work on sustainable plans initiated by Chari and Kehoe (1990). Like ourselves, Chari and Kehoe (1990) consider the design of optimal policy in the absence of commitment. However, we examine policies that are Markovian and focus on the equilibrium selection issue whereas they examine the set of equilibria that can be supported by trigger strategies.\nWhen approximated about an efficient zero-inflation non-stochastic steady state, the Phillips curve can be expressed as: \n$$\n\\begin{array}{l}{{b_{t+1}}=\\rho{b_{t}}-\\eta{c_{t}},}\\ {}\\ {{\\pi_{t}}=\\beta{\\mathrm{E}_{t}}{\\pi_{t+1}}+\\lambda{c_{t}}+\\nu{b_{t}}+u_{t},}\\end{array}\n$$ \nwhere $b_{t}$ represents the ratio of real government debt to output, $c_{t}$ represents real consumption, $\\pi_{t}$ represents inflation and $u_{t}$ represents the markup shock. In (1) and (2), $\\beta\\in(0,1)$ denotes the discount factor, while $\\rho\\in(0,1),\\eta\\in\\langle0,\\beta^{-1}),\\lambda\\in(0,\\infty)$ and $\\nu\\in(0,\\infty)$ are convolutions of behavioural parameters – preference and technology parameters.\nThe central bank’s intertemporal welfare criterion is described by the quadratic loss function: \n$$\n{\\cal L}_{t}=\\mathrm{E}_{t}\\sum_{s=t}^{\\infty}\\beta^{s-t}(\\pi_{s}^{2}+\\alpha c_{s}^{2}),\n$$ \nwhere $\\alpha\\in(0,\\infty)$ is a convolution of behavioural parameters, derived under the assumption that the monopolistic distortion is offset by a labour subsidy, financed by a lump-sum tax.\n\nThe text discusses a New Keynesian model with government debt, focusing on discretionary policy making and the potential for multiple equilibria due to coordination failures. The model includes a representative household, monopolistically competitive firms, and a government conducting fiscal and monetary policy. The central bank's policy instrument is consumption, and its welfare criterion is a quadratic loss function."
  },
  {
    "qid": "econ-empirical-1585-0-0-1",
    "question": "2) Analyze the impact of the 1978 capital gains tax cut on realizations, considering the temporary unlocking effect and the 1979-1980 data.",
    "gold_answer": "1. **Temporary Unlocking**: The 1979 data showed a sharp increase in realizations (\\( \\$48.6 \\)B to \\( \\$70.5 \\)B), partly due to temporary unlocking.\n2. **Long-Term Effect**: The 1980 data (\\( \\$69.9 \\)B) indicated a smaller long-term effect, suggesting the 1979 spike was transient.\n3. **Elasticity Adjustment**: Discounting for temporary effects, the elasticity was revised to \\( -1.05 \\) (vs. \\( -2.25 \\) in 1979).\n4. **Conclusion**: The permanent effect of the tax cut was smaller than FSY predicted.",
    "question_context": "The coefficient of the tax variable indicates that a ten-percentage point cut in the capital gains tax rate would increase the ratio of gains to dividends by 4.97. Using this result, FSY claim that capital gains tax cuts would generate so much additional trading and realization of gains that they would increase tax revenues.\nThe 1973 Capital Assets File data indicate that reductions in capital gains tax rates do not generate enough additional realizations to offset the static revenue loss.\nThe increase in the capital gains exclusion from 50 to 60 percent became effective on November 1, 1978, reducing the highest tax rate on capital gains from 49.125 percent to 34.9 percent.\nThe excess of net long-term capital gains over net short-term capital losses realized by the same taxpayers increased from $48.6 billion in 1978 to $70.5 billion in 1979.\n\nThe text discusses the econometric analysis by Feldstein, Slemrod, and Yitzhaki (FSY) on the elasticity of capital gains realizations to marginal tax rates, highlighting a significant overestimation due to an econometric error. The corrected estimates suggest that tax rate cuts would not increase realizations sufficiently to offset the static revenue loss."
  },
  {
    "qid": "econ-empirical-414-3-0-1",
    "question": "2) Prove Lemma C.1 by deriving the bounds for $\\left|\\frac{\\partial^{m}}{\\partial u^{m}}\\varDelta_{+}^{u}\\varDelta_{-}^{v}X_{t}\\right|$ and $\\left|\\frac{\\partial^{m}}{\\partial u^{m}}T^{u}\\varDelta_{+}^{u}\\varDelta_{-}^{v}X_{t}\\right|$ as stated in the text.",
    "gold_answer": "To prove Lemma C.1:\n\n1. For $\\left|\\frac{\\partial^{m}}{\\partial u^{m}}\\varDelta_{+}^{u}\\varDelta_{-}^{v}X_{t}\\right|$:\n   - Start with the representation $\\varDelta_{+}^{u}\\varDelta_{-}^{v}X_{t} = \\sum_{n=0}^{\\infty}\\left(\\sum_{j=0}^{t-1}\\pi_{j}(-u)\\pi_{n+t-j}(-v)\\right)X_{-n}$.\n   - Differentiate with respect to $u$ and use the bound $|\\mathsf{D}^{m}\\pi_{j}(\\delta)| \\leq c(1+\\log j)^{m}j^{\\delta-1}$.\n   - Apply the summation bounds and logarithmic estimates to arrive at the final expression.\n\n2. For $\\left|\\frac{\\partial^{m}}{\\partial u^{m}}T^{u}\\varDelta_{+}^{u}\\varDelta_{-}^{v}X_{t}\\right|$:\n   - Use the chain rule to differentiate $T^{u}\\varDelta_{+}^{u}\\varDelta_{-}^{v}X_{t}$.\n   - Combine the previous result with the additional $T^{u}$ term and its logarithmic derivative.\n   - Evaluate the exponent $T^{-\\operatorname*{min}(v,1,v-u,-u)}$ to complete the proof.",
    "question_context": "In this appendix we prove a series of lemmas containing variation bounds of the type $\\|V_{u t T}\\|_{2}~\\le~c$ and $\\|V_{u t T}-V_{v t T}\\|_{2}\\le$ $c(u\\mathrm{~-~}v)$, which we use to verify condition (48) in Lemma A.4 for relevant processes and product moments.\nWe find from (11) that for $v~>~0$, $\\varDelta^{v}X_{t}=\\varDelta_{+}^{v}X_{t}+\\varDelta_{-}^{v}X_{t}$ has the representation\n\n$$\n\\begin{array}{c}{{\\Delta^{v}X_{t}=\\gamma_{0}\\Delta_{+}^{-d_{0}+v}\\varepsilon_{t}+\\Delta_{+}^{-d_{0}+v+b_{0}}Y_{t}^{+}+\\Delta_{+}^{v}\\mu_{t}+\\Delta_{-}^{v}X_{t},}}\\ {{t=1,2,...,}}\\ {{\\Delta^{v}X_{t}=\\Delta_{-}^{v}X_{t},\\quad t=0,-1,-2,.....}}\\end{array}\n$$\nThe first term of (64) is asymptotically stationary if $d_{0}-v<1/2$. For $d_{0}-\\upsilon>1/2$ it will, suitably normalized and with sufficient moments, converge to fBM, see (6). The next term is asymptotically stationary if $d_{0}-\\upsilon-b_{0}<1/2$, and the last terms are deterministic trends that are functions of initial values.\nLemma C.1 (Initial Values). If $|X_{-n}|\\leq c$ and $|u|\\leq u_{0},0<\\delta_{0}\\leq$ $v\\leq v_{0}$ then uniformly in $u,v$ and for $c_{0}=c(u_{0},v_{0},\\delta_{0})$ the initial values satisfy the relations\n\n$$\n\\begin{array}{l}{\\displaystyle\\left|\\frac{\\partial^{m}}{\\partial u^{m}}{\\varDelta}_{+}^{u}{\\varDelta}_{-}^{v}X_{t}\\right|=\\left|\\displaystyle\\sum_{n=0}^{\\infty}\\left(\\displaystyle\\sum_{j=0}^{t-1}\\frac{\\partial^{m}}{\\partial u^{m}}\\pi_{j}(-u)\\pi_{n+t-j}(-v)\\right)X_{-n}\\right|}\\ {\\displaystyle\\qquad\\leq{c}_{0}(1+\\log t)^{m+1}t^{{-}\\operatorname*{min}(u+v,u+1,v)},}\\end{array}\n$$\n\n$$\n\\left|\\frac{\\partial^{m}}{\\partial u^{m}}T^{u}\\varDelta_{+}^{u}\\varDelta_{-}^{v}X_{t}\\right|\\leq c_{0}(1+\\log T)^{m+1}T^{-\\operatorname*{min}(v,1,v-u,-u)}.\n$$\nFrom this it follows that for $\\begin{array}{r}{G(z)=\\sum_{n=0}^{\\infty}g_{n}z^{n}}\\end{array}$, $\\textstyle\\sum_{n=0}^{\\infty}\\left|g_{n}\\right|<\\infty$, we have\n\n$$\n\\begin{array}{r l r}{\\lefteqn{\\left|G_{+}(L)\\frac{\\partial^{m}}{\\partial u^{m}}\\varDelta_{+}^{u}\\varDelta_{-}^{v}X_{t}\\right|}}\\ &{}&{\\leq\\displaystyle c_{0}(1+\\log t)^{m+1}\\sum_{n=0}^{t-1}|g_{t-n}|n^{-\\operatorname*{min}(u+v,u+1,v)},}\\end{array}\n$$\n\n$$\n\\left|G_{+}(L){\\frac{\\partial^{m}}{\\partial u^{m}}}T^{u}\\varDelta_{+}^{u}\\varDelta_{-}^{v}X_{t}\\right|\\leq c_{0}(1+\\log T)^{m+1}T^{-\\operatorname*{min}(v,1,v-u,-u)}.\n$$\nSimilarly it holds that, uniformly in $\\delta_{0}\\leq v\\leq v_{0}$,\n\n$$\n\\begin{array}{r l}&{\\left|\\cfrac{\\partial^{m}}{\\partial v^{m}}\\varDelta_{-}^{v}X_{t}\\right|\\to0\\quad a s t\\to\\infty,}\\ &{\\displaystyle\\operatorname*{max}_{1\\leq t\\leq T}\\left|\\frac{\\partial^{m}}{\\partial v^{m}}T^{v}\\varDelta_{-}^{v}X_{t}\\right|\\leq c_{0}(1+\\log T)^{m}T^{v_{0}}.}\\end{array}\n$$\n\nThis appendix proves a series of lemmas containing variation bounds used to verify condition (48) in Lemma A.4 for relevant processes and product moments. The lemmas cover initial values, nonstationary processes, asymptotically stationary processes, and product moments."
  },
  {
    "qid": "econ-empirical-65-3-0-2",
    "question": "3) Explain the intuition behind Proposition 2(ii): Why does an overly liquid bank have higher debt capacity when making a loan to a project?",
    "gold_answer": "1. **Bargaining Power**: The bank’s greater bargaining power ($a_{F}^{P} > a_{I}^{P}$) allows it to extract higher repayments from the project.  \n2. **Liquidation Threat**: Individual investors can threaten to liquidate the bank’s overly liquid core assets, which binds future cash inflows from loans.  \n3. **Result**: The bank can borrow more from individual investors, increasing the project’s debt capacity beyond what individual investors could provide directly.",
    "question_context": "transformation risk means that debt capacity is not monotonic in the intrinsic liquidity of assets. Because the assets of an overly liquid firm cannot be fully utilized as collateral, debt capacity is increased by taking on less liquid projects that, ex ante, are cash flow-rich but asset-poor.\nWe model the intermediary as a bank that borrows from a number of individual investors for its own core business and to lend on to a project. A single large outside investor in a project clearly has some advantages over a group of individual investors, each making a small loan. The large investor brings scale economies to monitoring and does not suffer from the problems of organizing collective action that would plague the group of small investors. We therefore assume that the bank has greater bargaining power over project cash flows than individuals, so that $a_{F}^{P}>$ $a_{I}^{P}$ .\nThe debt capacity of the project when funded directly by individual investors is $a_{I}^{P}C_{2}+\\alpha^{P}d_{2}^{P}$ . The debt capacity of the project when the bank lends to it is $a_{F}^{P}C_{2}+\\propto^{P}d_{2}^{P}$ . Let this be the amount the bank lends, and let the contracted, enforceable repayments be $P_{1}^{P}$ and $P_{2}^{P}$ . Because $a_{F}^{P}{>}a_{I}^{P}$ , the bank can extract higher repayments from the project than can individual investors.\nLEMMA 2. If there is no secondary market for the project loan, the bank cannot fully fund the loan on a stand-alone basis by borrowing from individual investors.\nPROPOSITION 2. If there is no secondary market for a loan to an illiquid project, then (i) Regardless of the bank’s bargaining power over the project’s cash flows, $a_{F}^{P}$ , the incremental debt capacity an illiquid or liquid bank adds with a loan to a project is always less than the amount the project can raise by borrowing directly from individual investors. (ii) The debt capacity the overly liquid bank adds by making a loan to a project may exceed the debt capacity the project has borrowing directly from investors.\nCOROLLARY 3. When a secondary loan market opens, the incremental debt capacity an overly liquid bank obtains by making a loan is zero.\n\nThe text discusses the role of banks in financial intermediation, focusing on why banks hold a mix of illiquid loans and liquid securities, and the implications of transformation risk on debt capacity. It introduces a model where banks have greater bargaining power over project cash flows compared to individual investors, and explores how intermediation can increase external funding of projects."
  },
  {
    "qid": "econ-empirical-634-2-0-3",
    "question": "4) What methodological challenges arise when assessing the extent of Keynes's influence on post-war British economic policy, as highlighted by Booth's work?",
    "gold_answer": "Key challenges include: \\n1. **Causality**: Distinguishing Keynes's direct influence from broader intellectual trends. \\n2. **Policy vs. Theory**: Policies may reflect pragmatic adjustments rather than theoretical adherence. \\n3. **Counterfactuals**: Hard to prove what would have happened without Keynes. \\n4. **Archival Limitations**: Gaps in records of policy deliberations. \\nBooth navigates these by tracing policy evolution and highlighting discontinuities between Keynes's ideas and actual practice.",
    "question_context": "Booth broadly supports what is becoming the new orthodoxy, that there was not a Keynesian revolution in economic policy in postwar Britain. This view stresses the continuity rather than the discontinuity of policy between the 1930s and 1940s; identifies the practice of national economic management with the period after the First and not the Second World War; denies the radical nature of the commitment to full employment, deriving as it did from the equivocal 1944 White Paper; and finds little obvious evidence that Keynes's promotion of internal above external stability was enshrined in practical policy.\n\nThe debate over the Keynesian revolution in British economic policy post-World War II is a central theme in economic history. Booth's book challenges the traditional view of a revolutionary shift towards Keynesian policies, emphasizing continuity and questioning the extent of Keynes's influence."
  },
  {
    "qid": "econ-empirical-802-0-0-1",
    "question": "2) Explain the mathematical formulation of $\\Omega_{\\cal D}({\\cal F}|{\\cal F}_{0})$ for delayed implementation. How does it differ from $\\Omega_{I}(F|F_{0})$ for immediate implementation?",
    "gold_answer": "1. **Delayed Implementation ($\\Omega_{\\cal D}$)**: This integrates worker utilities ${\\cal W}_{\\cal D}$ over productivity $y$, weighted by the distribution ${\\cal G}(\\mathrm{d}y|{\\cal F}_{0})$, accounting for layoffs before policy enactment:\n   $$\\Omega_{\\cal D}({\\cal F}|{\\cal F}_{0})=\\int{\\cal W}_{\\cal D}[{\\cal F}|y,w({\\cal F}_{0}),{\\cal F}_{0}]{\\cal G}(\\mathrm{d}y|{\\cal F}_{0}).$$\n2. **Immediate Implementation ($\\Omega_{I}$)**: Here, utilities $W_{I}$ are evaluated post-policy, with no layoff adjustment:\n   $$\\Omega_{I}(F|F_{0})=\\int W_{I}[F|y,w(F_{0})]G(\\mathrm{d}y|F_{0}).$$\n   The key difference is the timing of layoffs and their impact on worker utility.",
    "question_context": "A standard result for this model is that equilibrium policies maximise the average of individual utilities of voters. Thus the key difference vis-a-vis majority voting is that the intensity with which voters care about the policy affects the political outcome.\nIn the case of delayed implementation, the political equilibrium simply maximises $$\\Omega_{\\cal D}({\\cal F}|{\\cal F}_{0})\\equiv\\int{\\cal W}_{\\cal D}[{\\cal F}|y,w({\\cal F}_{0}),{\\cal F}_{0}]{\\cal G}(\\mathrm{d}y|{\\cal F}_{0}),$$ and $\\Omega_{I}(F|F_{0})$ is defined analogously for the case of immediate implementation.\nThe function $\\Omega(F|F_{0},\\tilde{y})\\equiv\\int\\{\\mathbb{I}(y\\geq\\tilde{y})W_{I}[F|y,w(F_{0})]+\\mathbb{I}(y<\\tilde{y})U(F)\\}G(\\mathrm{d}y|F_{0}),$ where again $\\mathbb{I}$ is an indicator function.\n\nThe text discusses probabilistic voting as an alternative to majority voting in the context of employment protection policies. It highlights how voters' intensity of preference affects political outcomes and introduces mathematical models to analyze equilibrium policies under different timing assumptions."
  },
  {
    "qid": "econ-empirical-97-0-0-1",
    "question": "2) Under what condition does equality hold in $I(\\mathbf{Y}) = \\sum_{k}\\frac{\\mu_{k}}{\\mu}I(\\mathbf{Y}_{k})$? Provide a formal proof for the Gini coefficient case.",
    "gold_answer": "1. **Condition for equality**: Equality holds if and only if the factor incomes are perfectly rank-correlated, i.e., $Y_{ik} = \\alpha_k + \\beta_k Y_i$ for some constants $\\alpha_k, \\beta_k$.  \n2. **Gini coefficient properties**: The Gini coefficient $G(\\mathbf{Y})$ is convex and scale-invariant.  \n3. **Perfect rank correlation**: If $Y_{ik} = \\alpha_k + \\beta_k Y_i$, then the Lorenz curves of $\\mathbf{Y}_k$ and $\\mathbf{Y}$ are identical up to scaling.  \n4. **Decomposition**: $G(\\mathbf{Y}) = \\sum_k \\frac{\\mu_k}{\\mu} G(\\mathbf{Y}_k)$ because the Gini of the sum equals the sum of the Ginis under perfect rank correlation.",
    "question_context": "The problem can be stated more formally if we denote the total income of household i by $Y_{i}$ $_{i}\\left(i=1,\\ldots,n\\right)$ , and the income from source $\\pmb{k}$ as $Y_{i k}\\left(k=1,\\ldots,K\\right)$ .The distribution of total income is represented by $\\mathbf{Y}=(Y_{1},\\ldots,Y_{n})$ and the distribution of factor $\\pmb{k}$ incomes by $\\mathbf{Y}_{k}$ $\\mathbf{\\Psi}=(Y_{1k},\\dots,Y_{n k})$ Assuming that the component income categories are mutually exclusive, and that total income is the sum of the individual components, we have $\\bar{\\mathbf{Y}}=\\pmb{\\Sigma}_{k}\\pmb{\\Upsilon}_{k}$ . Assessing the inequality contribution of factor $\\pmb{k}$ then translates into the problem of determining the impact of $\\mathbf{Y}_{k}$ on Y.\nIf the index is a nonnegative convex function of relative incomes (a characteristic shared by the Gini coefficient and many other conventional inequality measures) and if the aggregate income derived from each source is nonnegative, a simple relationshipbetween $I(\\mathbf{Y})$ and the factor income distribution $\\mathbf{Y}_{k}$ is given by: $$\\begin{array}{r}{I(\\mathbf{Y})\\equiv\\sum_{k}\\frac{\\mu_{k}}{\\mu}I(\\mathbf{Y}_{k}),}\\end{array}$$ where $\\mu_{k},\\mu$ are the respective means of $\\mathbf{Y}_{k}$ , Y.\nFei et al. also consider a decomposition of the value of the Gini coefficient into terms involving “pseudo-Gini\" coefficients. This approach generates factor contributions that sum exactly to total income inequality and can produce both positive and negative contributions.\nThe unique rule derived in Shorrocks [1982] is to propose general principles that should be satisfied in a factor decomposition. These lead to a unique decomposition rule, applicable to any inequality measure, in which the contribution of incomecomponent $\\pmb{k}$ as a proportion of the inequality in total incomes is givenby $\\mathsf{c o v}(\\mathbf{Y}_{k},\\mathbf{Y})/\\sigma^{2}(\\mathbf{Y})$.\n\nThe paper discusses the decomposition of income inequality into contributions from various income sources, highlighting the challenges and potential pitfalls of different decomposition rules."
  },
  {
    "qid": "econ-empirical-1136-4-0-2",
    "question": "3) Formally test the parallel trends assumption for the quasi-experimental design using the pre-period coefficients from Figure 3. What would rejection of the null hypothesis imply about the identification strategy?",
    "gold_answer": "Parallel trends test:\n\n\\(H_0\\): \\(\\beta_{pre} = 0\\) for all pre-treatment periods\n\nProcedure:\n1. Extract coefficients for leads \\(T = \\{-5,...,-1\\}\\)\n2. Joint F-test: \\(F = \\frac{(R\\beta_{pre})'(R\\Sigma R')^{-1}(R\\beta_{pre})}{q}\\)\n   - \\(R\\) = identity matrix\n   - \\(q\\) = number of pre-periods\n\nRejection implies:\n1. Violation of parallel trends\n2. Potential confounding from time-varying factors\n3. Threat to causal interpretation of treatment effects",
    "question_context": "For Black and Hispanic households estimates are more precise. Coefficients in the preperiod dance around zero. Whether regulation is modeled to impact behavior immediately, or with a lag of one year or more, regulation is correlated with a decrease in the number of unbanked minority households. This relationship is significant for lags of one, two, four, and five years.\nThe graph provides evidence that lifeline legislation does alter the behavior of low-income Black and Hispanic households, albeit with a substantial lag. Again, given the evidence of slower dissemination of information on lifeline legislation (over check-cashing regulations) it is not surprising that lifeline effects are slower to materialize.\nIn the case of lifeline legislation, price caps lead low-income minority households to increase their propensity to hold accounts. In other words, the increased demand seems to outweigh any decrease in supply. For fringe banking the opposite is true: Price caps seem to lead low-income minority households to rely less on check-cashing services.\nTable 7 shows that government check-cashing regulation does not begin to have a negative impact on banking behavior until two years after implementation. Lifeline-banking legislation lowers the proportion of unbanked beginning three or more years after implementation.\n\nThe analysis examines the differential impact of lifeline-banking legislation on unbanked status across racial and ethnic groups, with a focus on low-income Black and Hispanic households. The study employs quasi-experimental methods and regression analysis to assess behavioral lags and omitted variable biases."
  },
  {
    "qid": "econ-empirical-835-0-0-3",
    "question": "4) Using Assumption 1 (\\( \\lambda > \\gamma \\)), show why equilibrium uniqueness holds in the Board and Meyer-ter-Vehn (2013) baseline model without promotion.",
    "gold_answer": "Assumption 1 ensures:\n1. **Belief Monotonicity**: Quality shocks (rate \\( \\lambda \\)) dominate news arrivals, so beliefs drift monotonically:\n   - Upward if \\( a=1 \\) (investing).\n   - Downward if \\( a=0 \\) (not investing).\n2. **No Overlap**: Promotional noise (\\( \\gamma \\)) cannot reverse drift direction, preventing multiple equilibria where beliefs oscillate.\n3. **Uniqueness**: Single crossing in value functions guarantees a unique Markov Perfect Equilibrium (MPE).",
    "question_context": "A monopolist is selling a product to consumers who don’t observe quality directly. Instead, consumers observe news, signals that arrive at a Poisson rate that depends on the current quality of the firm’s product.\nPromotion provides the firm with a tool to endogenously renew its reputation, which dramatically impacts investment incentives and reputation dynamics.\nPromotion naturally creates incentives for investment. The firm invests in unobserved quality so that it can then promote and charge a higher price.\n\nThe paper presents a model where a firm invests in product quality and influences consumer perception through promotion, which involves stochastic disclosures of quality. The model builds on Board and Meyer-terVehn (2013), incorporating Poisson processes for news arrival and promotional opportunities."
  },
  {
    "qid": "econ-empirical-965-3-0-1",
    "question": "2) Prove Corollary 2(a) by contradiction, showing that if $\\hat{e}_{2}>0$ is the unique solution to the auditing problem, then $e_{2}>0$ in the purchaser's problem.",
    "gold_answer": "1. Assume $e_{2}=0$ when $\\hat{e}_{2}>0$ is the unique solution to the auditing problem.\n2. By definition, $V^{AP}(0) = V^{PP}(0)$ because the solutions coincide when constraints on $\\varepsilon_{r_{ij}}$ are not binding.\n3. Since the auditing problem imposes additional constraints, $V^{PP}(0) \\geq V^{AP}(\\hat{e}_{2})$.\n4. But $V^{AP}(\\hat{e}_{2}) > V^{AP}(0)$ because $\\hat{e}_{2}$ is the unique solution.\n5. This leads to $V^{AP}(0) \\geq V^{AP}(\\hat{e}_{2}) > V^{AP}(0)$, a contradiction.\n6. Hence, $e_{2}>0$ must hold.",
    "question_context": "Proposition 2. Suppose that Assumptions 1-5 hold and that inequality (8) is not satisfied. Then the solution to (1)-(7) has the following properties: \n\n$$\\begin{array}{r l}{I_{p_{1}=c_{1}^{I};\\quad}}&{I_{p_{2}=c_{2}^{I}+[1-\\lambda^{I}]}\\frac{\\phi_{11}+\\phi_{12}}{\\phi_{21}+\\phi_{22}}[c_{2}^{I}-c_{1}^{I}]>^{I}p_{22}=c_{2}^{I}+[1-\\lambda^{I}]\\frac{\\phi_{12}}{\\phi_{22}}[c_{2}^{I}-c_{1}^{I}]>c_{2}^{I};}\\ {\\varepsilon_{p_{21}=c_{1}^{E};\\quad}}&{E_{p_{22}=c_{2}^{E}}+\\frac{\\phi_{21}\\left[\\phi_{21}+\\phi_{22}-\\lambda_{2}^{E}\\right]}{\\phi_{22}\\left[\\phi_{21}+\\phi_{22}\\right]}[c_{2}^{E}-c_{1}^{E}]\\geq c_{2}^{E};}\\end{array}$$\nCorollary 2. (a) If $\\hat{e}_{2}>0$ is the unique solution to the auditing problem, then $e_{2}>0$ in the solution to the purchaser's problem (1)-(7). (b) If $e_{2}=0$ is the unique solution to (1)-(7), then $\\hat{e}_{2}=0$ in the solution to the auditing problem.\nCorollary 3. Suppose that Assumptions 1-5 and inequality (8) hold. Then, although $\\hat{e}_{2}=0$ in the solution to the auditing problem, $e_{2}=1$ in the solution to the purchaser's problem whenever \n\n$$\\frac{\\phi_{21}}{\\phi_{21}+\\phi_{22}}[N C S(c_{1}^{E})-N C S(c_{2}^{I})]>s^{E}F^{E}+\\bar{\\Pi}^{E}.$$\n\nThis section examines the optimal procurement policy when the purchaser does not share the firms' private information and when production by the entrant is feasible. The analysis builds on Proposition 2, which outlines the properties of the solution under certain assumptions and inequalities."
  },
  {
    "qid": "econ-empirical-222-6-1-0",
    "question": "3) Formally state the null and alternative hypotheses tested in Table 7 regarding bargaining failure rates by ethnicity. Explain the use of one-tailed tests.",
    "gold_answer": "1. **Hypotheses**:\n   - **Null (H₀)**: Ethnicity has no effect on bargaining failure rates (\\(\\beta_{\\text{White}} = \\beta_{\\text{Bangladeshi}} = \\beta_{\\text{Indian}} = 0\\)).\n   - **Alternative (H₁)**: Bargaining failure rates differ by ethnicity:\n     - Whites: \\(\\beta_{\\text{White}} < 0\\) (less likely to fail).\n     - Bangladeshis/Indians: \\(\\beta_{\\text{Bangladeshi}}, \\beta_{\\text{Indian}} > 0\\) (more likely to fail).\n\n2. **One-Tailed Tests**: Used for Whites due to directional prediction (theory suggests lower failure rates). Two-tailed tests for Bangladeshis/Indians if no prior expectation exists.",
    "question_context": "Using the fixed-effects logit results shown in the first column, bargaining with both Bangladeshis and Indians is more likely to fail relative to bargaining with Chinese clients. The estimate also suggests that bargaining with whites is less likely to fail, but the coefficient falls just short of statistical significance at the 0.05 level even using a one-tailed test.\nSubject to this caveat, we find that bargaining fails with whites 14 percentage points less frequently and with Bangladeshis and Indians 20 and 18 percentage points more frequently than with Chinese customers.\n\nThis section analyzes bargaining failure rates by client ethnicity, testing hypotheses derived from the theoretical model."
  },
  {
    "qid": "econ-empirical-1623-2-2-1",
    "question": "6) Compare the performance of the estimators GMME1 and MGMME1 in the Monte Carlo experiment. What are the key differences in their assumptions and results?",
    "gold_answer": "1. **GMME1**: Assumes $Q$ is known and does not account for misclassification.\\n2. **MGMME1**: Extends GMME1 to account for misclassification with known $Q$.\\n3. **Key Difference**: MGMME1 should perform better in the presence of misclassification, as it correctly models the data-generating process.",
    "question_context": "The latent variable $Y$ is generated by a logit model with $\\operatorname{Pr}(1|x, \\theta) = (1 + e^{-x\\theta})^{-1}$. The conditional probability of observing $Y^{*} = 1$ is $P^{*} = \\bar{\\alpha} + (1 - 2\\bar{\\alpha})P$, and the marginal probability is $Q^{*} = \\bar{\\alpha} + (1 - 2\\bar{\\alpha})Q$.\nFour GMM estimators were compared: GMME1 (known $Q$), GMME2 (unknown $Q$), MGMME1 (extended for misclassification with known $Q$), and MGMME2 (extended for misclassification with unknown $Q$).\n\nThis section describes a Monte Carlo simulation study to evaluate the performance of GMM estimators under different information scenarios and stratification designs."
  },
  {
    "qid": "econ-empirical-657-0-1-0",
    "question": "5) Explain how the study's use of sterilization data addresses the endogeneity of birth spacing in the fertility equation.",
    "gold_answer": "Sterilization data provides exogenous variation because:\n1. Sterilization is a permanent fertility control measure, uncorrelated with transient shocks to mortality or spacing.\n2. It serves as an instrument in the fertility equation \\( F_{ij}^* = Z_{ij}\\pi + \\nu_{ij} \\), where \\( Z_{ij} \\) includes sterilization status.\n3. The exclusion restriction holds as sterilization affects fertility directly but not mortality conditional on fertility.",
    "question_context": "The econometric strategy in our paper is similar in that it relies upon natural information restrictions associated with the sequencing of births.\nWe investigated model extensions generalizing either the specification of unobserved heterogeneity, or the mechanism through which birth intervals affect mortality.\n\nThe study addresses endogeneity and initial conditions problems in dynamic panel models, using sterilization data for identification and testing robustness through various specification checks."
  },
  {
    "qid": "econ-empirical-970-0-0-0",
    "question": "1) Derive the first-order condition for firm $i$'s profit maximization problem in the infinite-regress model, given the profit function $\\pi(u,w,i)=p(u+\\sum_{j}w_{j})u-c_{i}(u)$.",
    "gold_answer": "1. Firm $i$ maximizes profits $\\pi(u,w,i)=p(u+\\sum_{j}w_{j})u-c_{i}(u)$.  \n2. The first-order condition (FOC) is obtained by differentiating with respect to $u$:  \n   $$ \\frac{\\partial \\pi}{\\partial u} = p'(u+\\sum_{j}w_{j})u + p(u+\\sum_{j}w_{j}) - c_{i}'(u) = 0. $$  \n3. This yields the optimal output condition:  \n   $$ p'(Q)u + p(Q) = c_{i}'(u), $$ where $Q = u + \\sum_{j}w_{j}$.",
    "question_context": "The model derives a conjectural variation instead of assuming it. In particular, the Cournot equilibrium is shown to be consistent in the usual sense of the literature.\nA consistent conjectural variation is a conjectural variation that is correct: predicted change (locally) in the relevant decision variable (output or price) on the part of one's competitor is what actually occurs.\nThe infinite regress problem for firm $i$ is represented by the tree in Figure A1a which we denote $T_{i}$ ,with node $i$ therootofthetree $T_{i}$.\n\nThe article presents an infinite-regress model to derive a rational expectations equilibrium for a duopoly, showing that the Cournot equilibrium is consistent when conjectural variations are correctly modeled."
  },
  {
    "qid": "econ-empirical-150-0-0-3",
    "question": "4) Explain the decomposition of the Tornquist approximation of total factor productivity growth (TFP) and its components.",
    "gold_answer": "The Tornquist approximation of TFP growth is decomposed as:\n\\[ \\Delta \\ln TFP_{t} = \\Delta \\ln Y_{t} - \\Delta \\ln N_{t}, \\]\nwhere \\( \\Delta \\ln N_{t} \\) is the weighted sum of input growth rates. The decomposition includes:\n- **Technical change (\\( \\lambda_{Y} \\))**: Pure shift in technology.\n- **Scale effect**: Returns to scale.\n- **Adjustment cost effect**: Costs from changing quasi-fixed factors.\n- **Variable depreciation effect**: Impact of endogenous depreciation.\nThe decomposition reveals biases in traditional TFP measures, such as overestimating technical change.",
    "question_context": "The firm's technology can be represented by the following factor requirement function: \\( M_{t}=M(Y_{t},L_{t},K_{t}^{\\circ},E_{t},\\underline{{R}}_{t},\\varDelta K_{t},\\varDelta R_{t},T_{t}) \\), where \\( \\underline{{K}}_{t}=\\phi^{\\kappa}K_{t}+(I-\\phi^{\\kappa})K_{t-1} \\), \\( \\underline{{R}}_{t}=\\phi^{R}R_{t}+(I-\\phi^{R})R_{t-1} \\), \\( \\varDelta K_{t}=K_{t}-K_{t-1} \\), and \\( \\varDelta R_{t}=R_{t}-R_{t-1} \\).\nThe firm's cost in period \\( \\tau \\), normalized by the price of the variable factor \\( M_{\\tau} \\), is given by \\( M_{\\tau}+(p_{\\tau}^{L})^{\\prime}L_{\\tau}+(p_{\\tau}^{K})^{\\prime}K_{\\tau}+(p_{\\tau}^{R})^{\\prime}R_{\\tau}+(q_{\\tau}^{K})^{\\prime}I_{\\tau}^{K}+(q_{\\tau}^{R})^{\\prime}I_{\\tau}^{R} \\).\nThe stochastic Euler equations for the quasi-fixed factors are given by: \\( -\\phi^{\\kappa}\\frac{\\Im G_{\\mathfrak{r}}}{\\Im G_{\\mathfrak{r}}}-(I-\\phi^{\\kappa})\\mathrm{E}_{\\mathfrak{r}}\\frac{\\Im G_{\\mathfrak{r}+1}}{\\Im G_{\\mathfrak{r}+1}}\\bigg/(1+r_{\\mathfrak{r}+1}) = p_{\\mathfrak{r}}^{\\kappa}+q_{\\mathfrak{r}}^{\\kappa}+\\bigg\\{\\frac{\\Im G_{\\mathfrak{r}}}{\\Im G_{\\mathfrak{r}}}-\\mathrm{E}_{\\mathfrak{r}}\\frac{\\Im G_{\\mathfrak{r}+1}}{\\Im G_{\\mathfrak{r}+1}}\\bigg/(1+r_{\\mathfrak{r}+1})\\bigg\\} \\).\n\nThe paper presents a dynamic factor demand model that incorporates endogenous capital utilization, allowing firms to choose the depreciation rate of quasi-fixed factors. The model extends previous literature by including multiple outputs, variable inputs, and lagged productivity of quasi-fixed factors."
  },
  {
    "qid": "econ-empirical-361-0-1-1",
    "question": "4) Prove that the sequential estimation of the Cholesky parameters preserves the positive semidefiniteness of the covariance matrix.",
    "gold_answer": "The sequential estimation preserves positive semidefiniteness because each step ensures that the diagonal elements gₖₖ are non-negative (as they are variances minus non-negative terms) and the off-diagonal elements hₖₗ are scaled by gₗₗ. The final reconstructed matrix Σ = HGH′ is guaranteed to be positive semidefinite by the properties of the Cholesky decomposition.",
    "question_context": "By means of the Cholesky decomposition, the spot covariance matrix can be uniquely split into Σ(s) = H(s)G(s)H(s)′, where H(s) is a lower diagonal matrix with ones on the diagonal, and G(s) a diagonal matrix.\nThe elements can therefore be estimated sequentially.\n\nThe paper details the use of the Cholesky decomposition to transform the covariance matrix into a product of lower triangular and diagonal matrices, enabling sequential estimation of parameters."
  },
  {
    "qid": "econ-empirical-1752-2-0-0",
    "question": "1) Derive the extended GMM estimator of \\u03a6 using the moment conditions (6.1), (6.10), and (6.11). Show the steps involved in stacking these conditions and forming the estimator.",
    "gold_answer": "1. Stack the moment conditions: \\(E[P_{i}^{\\prime}(W_{i} - W_{i,-1}\\u03a6^{\\prime})] = 0\\).\n2. Define \\(P_{i}^{\\prime} = (P_{1i}^{\\prime}, P_{2i}^{\\prime}, P_{3i}^{\\prime})^{\\prime}\\) where each submatrix corresponds to specific moment conditions.\n3. Formulate the GMM objective function: \\(Q(\\phi) = \\left(\\frac{1}{N}\\sum_{i=1}^{N} P_{i}^{\\prime}(W_{i} - W_{i,-1}\\Phi^{\\prime})\\right)^{\\prime} D_{\\hat{u}}^{-1} \\left(\\frac{1}{N}\\sum_{i=1}^{N} P_{i}^{\\prime}(W_{i} - W_{i,-1}\\Phi^{\\prime})\\right)\\).\n4. Minimize \\(Q(\\phi)\\) to obtain the estimator: \\(\\hat{\\phi}_{EGMM} = (S_{Z\\hat{X}}^{\\prime} D_{\\hat{u}}^{-1} S_{Z\\hat{X}})^{-1} S_{Z\\hat{X}}^{\\prime} D_{\\hat{u}}^{-1} S_{Z\\hat{y}}^{\\circ}\\).",
    "question_context": "E[(\\u0394w_{i,t-1} - \\u03a6\\u0394w_{i,t-2})w_{i,t-2}^{\\prime} - (\\u0394w_{i,t} - \\u03a6\\u0394w_{i,t-1})w_{i,t-1}^{\\prime}] = 0, t=3,4,...,T\nE[(w_{i,t} - \\u03a6w_{i,t-1})\\u0394w_{i,t-1}^{\\prime}] = 0, t=2,3,...,T\nThe extended GMM estimator of \\u03a6 is based on the moment conditions (6.1), (6.10), and (6.11), written in stacked form as E[P_{i}^{\\prime}(W_{i} - W_{i,-1}\\u03a6^{\\prime})] = 0.\n\nThe extended GMM estimator is derived by incorporating additional moment conditions, including those implied by homoskedasticity assumptions and initialization restrictions. These conditions are valid regardless of the unit root and cointegrating properties of the variables."
  },
  {
    "qid": "econ-empirical-845-5-1-2",
    "question": "3) Show how the supremum representation of $p\\cdot w$ is used to derive the uniform Lipschitz estimate in (i).",
    "gold_answer": "1. For $\\epsilon > 0$, choose $(a_{i})$ such that $p\\cdot w \\leq \\epsilon + \\sum p_{i}\\cdot a_{i}$.  \n2. Bound differences: $|p\\cdot w - p'\\cdot w| \\leq \\epsilon + \\sum C_{i}\\|x_{i}-x_{i}'\\|_{i}$.  \n3. Substitute Lipschitz estimates: $|p\\cdot w - p'\\cdot w| \\leq (\\sum C_{i}K_{i})(\\sum |\\lambda_{k}-\\lambda_{k}'|)$.",
    "question_context": "Because $\\Lambda^{\\delta}$ is a compact set and $x$ is weakly continuous, $P_{i}^{\\delta}(\\bar{e})$ is a weakly compact subset of $P_{i}^{0}(\\bar{e})$ for each $i$. Because each of the norms $\\|\\cdot\\|_{i}$ is adapted to $U_{i}$ on weakly compact subsets of $P_{i}^{0}(\\bar{e})$, there are constants $B_{i},C_{i}>0$ such that $\\left|D U_{i}(x_{i})\\cdot y\\right|\\leq B_{i}\\|y\\|_{i}$ for all $x_{i}\\in P_{i}^{\\delta}(\\bar{e})$ and $y\\in X$.\nBy Lemma 5.1, the solution to the planner's problem is Lipschitz on $\\Lambda^{\\delta}$, so there is a constant $K_{i}>0$ such that $\\|x_{i}(\\lambda)-x_{i}(\\lambda')\\|_{i}\\leq K_{i}\\sum|\\lambda_{i}-\\lambda_{i}'|$ for $\\lambda,\\lambda'\\in\\Lambda^{\\delta}$.\nTo establish (i), fix $\\delta>0$ and let $\\lambda,\\lambda'\\in\\Lambda^{\\delta}$. Write $x=x(\\lambda), x'=x(\\lambda'), p=p(\\lambda), p'=p(\\lambda')$, $p_{i}=\\lambda_{i}D U_{i}(x_{i})$, and $p_{i}'=\\lambda_{i}'D U_{i}(x_{i}')$ for each $i$. Fix an arbitrary $w\\in[0,\\bar{e}]$. By definition, $p\\cdot w=(\\bigvee p_{i})\\cdot w=\\sup\\{\\sum p_{i}\\cdot a_{i}:a_{i}\\geq0,\\sum a_{i}=w\\}$.\n\nThis section establishes Lipschitz continuity and other properties of the solution to the planner's problem, leveraging compactness, adapted norms, and first-order conditions."
  },
  {
    "qid": "econ-empirical-1286-3-0-2",
    "question": "3) How does noncompliance with treatment assignment in the NJCS affect the interpretation of results, and what were the observed compliance rates?",
    "gold_answer": "Noncompliance introduces potential bias:\n1. **Rates**: \n   - 73% of the treatment group enrolled in JC.\n   - 1.4% of the control group enrolled in JC.\n2. **Implications**:\n   - Violates the exclusion restriction if JC affects outcomes through channels other than degree attainment (e.g., social skills training).\n   - Requires instrumental variables (IV) methods to estimate local average treatment effects (LATE) for compliers.",
    "question_context": "In the NJCS, a random sample of all prescreened eligible applicants in the 48 contiguous states and the District of Columbia was randomly assigned into treatment and control groups, with the second group being denied access to JC for 3 years. Both groups were tracked with a baseline interview immediately after randomization and thereafter at 12, 30, and 48 months (for further details on JC and the NJCS, see Schochet, Burghardt, and Glazerman 2001).\nSince the original NJCS reports, which found statistically positive effects of JC participation on employment and weekly earnings (Schochet, Burghardt, and Glazerman 2001), several papers have analyzed different aspects of JC using the NJCS data.\nOur sample consists of all individuals with nonmissing values on the randomized treatment status, the variables regarding the attainment of a GED, high school, or vocational degree, and the outcome variables considered. We concentrate on estimating the returns to attaining any combination of those three degrees because many JC participants attain at least two of them (a GED or high school degree plus a vocational degree), and thus breaking up the effects of the different degrees would require additional assumptions.\n\nThe Job Corps (JC) is the United States' largest and most comprehensive job training program for economically disadvantaged youth aged 16–24 years old. It provides academic and vocational training, health services, counseling, job search assistance, social skills training, and a stipend during program enrollment. The National Job Corps Study (NJCS) randomly assigned eligible applicants into treatment and control groups, with the control group being denied access to JC for 3 years. Both groups were tracked with baseline interviews and follow-ups at 12, 30, and 48 months."
  },
  {
    "qid": "econ-empirical-1642-4-1-1",
    "question": "4) Prove Lemma 5: $C o\\nu\\left(\\gamma_{t},\\sum_{k^{\\prime}=1}^{k}\\tilde{F}_{t+k^{\\prime}}\\right)=\\frac{\\sigma_{\\epsilon\\gamma}^{2}\\tilde{N}_{k}}{a\\tilde{V}a r_{1\\gamma}}$. Interpret the role of $\\tilde{N}_{k}$ in the performance–flow correlation.",
    "gold_answer": "1. Net flow is $\\tilde{F}_{t+1} = \\frac{\\tilde{E}_{t}(\\gamma_{t+1}) - \\tilde{E}_{t-1}(\\gamma_{t})}{a\\tilde{V}a r_{1\\gamma}}$.\n2. The covariance sums over $k$ periods: $C o\\nu\\left(\\gamma_{t}, \\sum_{k^{\\prime}=1}^{k}\\tilde{F}_{t+k^{\\prime}}\\right) = \\frac{1}{a\\tilde{V}a r_{1\\gamma}} \\sum_{k^{\\prime}=1}^{k} C o\\nu(\\gamma_{t}, \\tilde{E}_{t+k^{\\prime}-1}(\\gamma_{t+k^{\\prime}}) - \\tilde{E}_{t+k^{\\prime}-2}(\\gamma_{t+k^{\\prime}-1}))$.\n3. From Section 4, $\\tilde{N}_{k} \\equiv \\sum_{k^{\\prime}=1}^{k} C o\\nu(\\gamma_{t}, \\tilde{E}_{t+k^{\\prime}-1}(\\gamma_{t+k^{\\prime}}))$.\n4. $\\tilde{N}_{k}$ determines the horizon-dependent correlation: negative for small $k$ (gambler’s fallacy dominates) and positive for large $k$ (belief in ability dominates).",
    "question_context": "The optimal investment in the active fund is a linear increasing function of the investor’s expectation of the fund’s excess return $\\gamma_{t}$: $\\tilde{w}_{t}=\\frac{\\tilde{E}_{t-1}(\\gamma_{t})}{a\\tilde{W}\\tilde{V}a r_{1\\gamma}}$.\nThe covariance between the excess return of the active fund in period t and the net flow into the fund during periods $t+1$ through $t+k$ is $C o\\nu\\left(\\gamma_{t},\\sum_{k^{\\prime}=1}^{k}\\tilde{F}_{t+k^{\\prime}}\\right)=\\frac{\\sigma_{\\epsilon\\gamma}^{2}\\tilde{N}_{k}}{a\\tilde{V}a r_{1\\gamma}}$.\n\nThis section extends the model to explain the fund-flow puzzle, where flows into mutual funds correlate with lagged returns despite no predictability."
  },
  {
    "qid": "econ-empirical-1070-4-0-1",
    "question": "2) Using the test score distributions in Figure 3, propose a statistical test to formally assess whether the gaps between academic- and non-academic-track distributions differ between Tracked and Comprehensive states.",
    "gold_answer": "A Kolmogorov-Smirnov (KS) test can be used to compare the empirical cumulative distribution functions (CDFs) of test scores between groups:\n\n1. Let $F_{A,T}(x)$ and $F_{N,T}(x)$ be the CDFs for academic- and non-academic-track students in Tracked states, and similarly $F_{A,C}(x)$ and $F_{N,C}(x)$ for Comprehensive states.\n\n2. Compute the KS statistic for the gap difference:\n\\[\nD = \\sup_x |(F_{A,T}(x) - F_{N,T}(x)) - (F_{A,C}(x) - F_{N,C}(x))|\n\\]\n\n3. Under the null hypothesis of no difference in gaps, $D$ follows a known distribution. Reject the null if $D > c_\\alpha$, where $c_\\alpha$ is the critical value at significance level $\\alpha$.\n\n4. Bootstrap methods can be used to estimate the sampling distribution of $D$ and compute p-values, accounting for clustering at the state level.",
    "question_context": "Panel (a) of Table 1 shows that the non-academic-track sample shares are $52\\%$ in both state groups. Academic-track students appear to be slightly over-represented in the NEPS, as according to administrative records the true non-academic shares for the cohort in question are $60\\%$ and $57\\%$ in Tracked and Comprehensive states, respectively (Statistisches Bundesamt, 2011). Reassuringly, the shares are very similar both in the population and in my sample.\nEqual shares leave open the possibility of compositional differences, however. For example, it is conceivable that competition for the academic track is stronger when there are only two tracks, because the alternative school type necessarily comprises all low achievers. This might amplify average ability differences between academic and non-academic tracks in two-tiered versus three-tiered systems.\nTo test for the presence of such differences in selection, Figure 3 plots pre-tracking test score distributions by state group, both overall and for academic and non-academic-track students separately. Panels (a) and (b) refer to the (beginning of) grade 5 maths and reading scores from the NEPS, and panels (c) through (e) refer to the (end of) grade 4 maths, reading and listening scores from the IQB11 data. Across achievement domains and data sets, the distributions look very similar in Tracked and Comprehensive states; in particular, the gaps between the academic- and non-academic-track distributions do not seem to differ between states.\n\nThe treatment a student receives depends on her state of residence (Tracked or Comprehensive) and whether she is assigned to the academic track or not. My identification strategy requires that selection into the academic track not differ between the two state groups. Otherwise, neither the academic-track nor the non-academic-track student bodies would be comparable."
  },
  {
    "qid": "econ-empirical-473-3-1-0",
    "question": "1) Describe the dataset used in the real data analysis and explain the significance of the NSW and PSID datasets.",
    "gold_answer": "The dataset combines 185 treated units from the National Supported Work (NSW) Program, a randomized experiment, and 429 control units from the Panel Study of Income Dynamics (PSID), an observational dataset. The NSW provides unbiased treatment effects, while the PSID serves as a non-experimental comparison group.",
    "question_context": "The dataset consists of 185 treated units from the NSW and 429 control units from the PSID.\nQTT examines the effect of the job training program on different segments of the distribution of post-training earnings for those who participated in the program.\nATT evaluates the mean effect of the program on post-training earnings for treated individuals.\nThe proposed estimators are compared with those of Firpo (2007) and Hirano, Imbens, and Ridder (2003), showing similar conclusions but smaller standard errors.\n\nThis section applies the proposed approaches to a job training program dataset (LaLonde 1986) to estimate QTT and ATT, examining the causal effects of the program on post-training earnings."
  },
  {
    "qid": "econ-empirical-269-3-0-3",
    "question": "4) Using the model, explain how participation costs $c$ affect the probability of a bidder entering the last auction in a weak substitutes regime, and analyze the limit behavior as $c \\to 0$.",
    "gold_answer": "The effect of participation costs $c$ is as follows:\n1. **Probability of Entry**: The probability that a non-winner enters the last auction decreases with $c$, as higher costs reduce the expected utility of participation.\n2. **Limit Behavior**: As $c \\to 0$, the participation cost becomes negligible, and the probability of entry approaches 1: $$\\operatorname*{Lim}_{c\\to 0}\\left\\{P r\\left(y_{i}^{K}=1 \\mid x_{j}^{1}=1,i\\neq j\\right)\\right\\}=1.$$\n3. **Implication**: With zero participation costs, all bidders are willing to participate in the last auction, maximizing competition.",
    "question_context": "The ex post utility for a bidder who buys $l$ units and participates in $m$ auctions is: $$U_{i}(l,m,\\boldsymbol{v}_{i};\\boldsymbol{\\rho},\\boldsymbol{c})=\\sum_{k=1}^{l}\\rho_{k}\\cdot\\boldsymbol{v}_{i}-\\sum_{k=1}^{K}y_{i}^{k}\\cdot\\boldsymbol{c}=\\sum_{k=1}^{l}\\rho_{k}\\cdot\\boldsymbol{v}_{i}-m\\cdot\\boldsymbol{c}.$$\nIn a strict complements regime (i.e., when A1 and A2 hold), the pure strategy symmetric PBE is: Participation: bidder $i$ always participates in the first auction, that is, ${\\boldsymbol y}_{i}^{1}=1$. Bidding strategy: $b_{i}^{1}(\\boldsymbol{v}_{i})=\\sum_{k=1}^{4}\\rho_{k}\\cdot\\boldsymbol{v}_{i}-4c.$\nIn a weak substitutes regime (i.e., when neither A1 nor A2 holds), the ex post marginal utility of the winner in the last auction, depending on how many units the winner won, satisfies: If the winner won all four units: $$\\rho_{4}\\cdot{\\boldsymbol v}_{N:N}\\geq p^{4}=\\rho_{1}\\cdot{\\boldsymbol v}_{N-1:N}-c.$$ If the winner won three units, two out of the first three, and the last one: $$\\rho_{3}\\cdot{\\boldsymbol v}_{N:N}\\geq p^{4}=\\rho_{2}\\cdot{\\boldsymbol v}_{N-1:N}-c.$$\n\nThe text discusses the participation costs faced by farmers in sequential water auctions, where the presence of setup costs (SC) and diminishing marginal returns (DMR) influences bidding strategies. The model assumes that bidders have private valuations for water units and face participation costs, leading to specific equilibrium behaviors in strict complements and weak substitutes regimes."
  },
  {
    "qid": "econ-empirical-1819-4-0-1",
    "question": "2) In the second-best scenario, the platform faces asymmetric information. Formulate the platform's optimization problem, including the incentive compatibility (IC) and individual rationality (IR) constraints, and derive the implementability condition $a_{H} \\leq a_{L}$.",
    "gold_answer": "1. **Optimization problem**: The platform maximizes $\\pi = \\nu\\{t_{L} + R_{L}(a_{L})\\} + (1-\\nu)\\{t_{H} + R_{H}(a_{H})\\}$ subject to:\n   - IR constraints: $u_{0} - C_{L}(a_{L}) - t_{L} \\geq 0$ and $u_{0} - C_{H}(a_{H}) - t_{H} \\geq 0$.\n   - IC constraints: $u_{0} - C_{L}(a_{L}) - t_{L} \\geq u_{0} - C_{L}(a_{H}) - t_{H}$ and $u_{0} - C_{H}(a_{H}) - t_{H} \\geq u_{0} - C_{H}(a_{L}) - t_{L}$.\n2. **Implementability condition**: Adding the IC constraints and simplifying yields $C_{H}(a_{H}) - C_{L}(a_{H}) \\leq C_{H}(a_{L}) - C_{L}(a_{L})$. Given $C_{H}^{\\prime}(a) > C_{L}^{\\prime}(a)$, this implies $a_{H} \\leq a_{L}$.",
    "question_context": "The Model.—A media platform provides an online content service to consumers and monetizes consumers’ attention by selling advertising. We study the design of its advertising policy. The model is similar to the baseline two-type model in Section I. There are two types of consumers, $\\theta~\\left(=~\\theta^{A}\\right)~\\in~\\{H,L\\}$ of a mass one. A consumer’s type captures her sensitivity to ads nuisance. Let $\\nu\\in(0,1)$ denote the proportion of low type consumers with $\\theta=L.$ Let $a\\in[0,1]$ index the advertising amount per consumer, ranging from the ad-free environment $a=0$ to the highest possible number of ads $a=1$.\nGiven $a$ , a consumer of type $\\theta\\in\\{L,H\\}$ earns the following utility from joining the platform: $u_{0}-C_{\\theta}(a)$, where $u_{0}>0$ is a constant surplus from consuming the content and $C_{\\theta}(a)$ is the disutility from ad nuisance. We assume that $C_{\\theta}(\\cdot)$ is strictly increasing and strictly convex with $C_{\\theta}(0)~=~0$ for $\\theta\\in\\{L,H\\}$.\nGiven $a$ , the platform's advertising revenue from a consumer of type $\\theta\\in\\{L,H\\}$ is given by $R_{\\theta}(a)$, where we assume that $R_{\\theta}(\\cdot)$ is strictly increasing and strictly concave in $a$ with $R_{\\theta}(0)\\:=\\:0$.\nRegarding how a consumer type affects the disutility and the revenue, we make the following assumptions: $C_{H}^{\\prime}(a)>C_{L}^{\\prime}(a),\\mathrm{for}a\\in[0,1];$ $R_{H}^{\\prime}(a)>R_{L}^{\\prime}(a),\\mathrm{for}a\\in[0,1].$ On the one hand, a high type experiences a larger marginal nuisance than a low type; this implies $C_{H}(a)>C_{L}(a)$ , for $a\\in[0,1]$ . On the other hand, advertisement to a high type generates a larger marginal revenue than advertisement to a low type; this implies $R_{H}(a)>R_{L}(a)$ for $a\\in[0,1]$.\nThe platform proposes a menu of contracts $\\left\\{\\left(t_{H},a_{H}\\right),\\left(t_{L},a_{L}\\right)\\right\\}$ to consumers, where $t_{\\theta}$ is the payment from a consumer of type $\\theta$ to the platform and $a_{\\theta}$ is the amount of advertising for a type $\\theta$ consumer. Then, the platform's profit is $\\pi=\\nu\\{t_{L}+R_{L}(a_{L})\\}+(1-\\nu)\\{t_{H}+R_{H}(a_{H})\\}$.\nFirst-Best.—Suppose that the platform knows the consumer types. Then the profit-maximizing advertising level for type $\\theta$ is a solution to the following problem: $\\operatorname*{max}_{a_{\\theta}}\\bigl[R_{\\theta}(a_{\\theta})-C_{\\theta}(a_{\\theta})\\bigr]$. Let $\\left(a_{L}^{F B},a_{H}^{F B}\\right)$ denote the first-best allocation, which is characterized by $R_{L}^{\\prime}\\bigr(a_{L}^{F B}\\bigr)=C_{L}^{\\prime}\\bigr(a_{L}^{F B}\\bigr),R_{H}^{\\prime}\\bigr(a_{H}^{F B}\\bigr)=C_{H}^{\\prime}\\bigr(a_{H}^{F B}\\bigr)$.\nSecond-Best. —-In the presence of asymmetric information, the platform offers a menu of contracts $\\left\\{\\left(t_{H},a_{H}\\right),\\left(t_{L},a_{L}\\right)\\right\\}$ to maximize its profit $\\pi=\\nu\\big\\{t_{L}+R_{L}(a_{L})\\big\\}+(1-\\nu)\\big\\{t_{H}+R_{H}(a_{H})\\big\\}$ subject to the incentive compatibility and individual rationality constraints.\n\nA media platform provides an online content service to consumers and monetizes consumers’ attention by selling advertising. The model involves two types of consumers with different sensitivities to ad nuisance, and the platform designs its advertising policy to maximize profit under first-best and second-best scenarios."
  },
  {
    "qid": "econ-empirical-1196-3-0-0",
    "question": "1) Given the inflation of state-reported placement statistics, derive the bias in the E-H estimates when the inflation factor is non-systematic and varies by state and time. Use the key independent variable $(P)$ in your derivation.",
    "gold_answer": "1. Let the true placement rate be $P^*$ and the reported rate be $P = kP^*$, where $k$ is the inflation factor. \\n2. The regression model is $Y = \\beta P + \\epsilon$. Substituting $P = kP^*$ gives $Y = \\beta kP^* + \\epsilon$. \\n3. The OLS estimator for $\\beta$ is biased because $\\text{Cov}(P, \\epsilon) \\neq 0$ due to the non-systematic nature of $k$. \\n4. The bias is given by $\\text{plim } \\hat{\\beta} = \\beta \\cdot \\frac{\\text{Cov}(P, P^*)}{\\text{Var}(P)} = \\beta \\cdot \\frac{\\text{Cov}(kP^*, P^*)}{\\text{Var}(kP^*)}$. \\n5. If $k$ is uncorrelated with $P^*$, the bias simplifies to $\\beta \\cdot \\frac{E[k] \\text{Var}(P^*)}{E[k^2] \\text{Var}(P^*) + \\text{Var}(k) (E[P^*])^2}$, which tends to attenuate $\\beta$ toward zero.",
    "question_context": "E-H use state-reported placement statistics as the measure of their key independent variable $(P$ , in equation 1). From an analytical perspective, there are two basic problems with these statistics: first, they are universally inflated; and second, the inflation factor varies by state and over time. As a consequence, not only are E-H's estimates inefficient, but they are biased toward zero.\nThe general inflation factor built into state-reported placement statistics arises from the strong incentive WIN managers have to boost placement rates-- and therewith, their performance rating--by counting any “natural\" (unassisted) welfare caseload turnover as successful WIN placements.\nThe measurement error problem is compounded by the fact that the inflation factor is not systematic, but instead varies widely across both states and time.\nThe ASPER study cited above also demonstrated that the fraction of reported WIN placements who were actually previously employed varied from 13 percent in California to 79 percent in Missouri [1, Table 2]!\n\nThe text discusses the issues with state-reported placement statistics used in the E-H evaluation, highlighting problems of inflation and variability in the inflation factor across states and time."
  },
  {
    "qid": "econ-empirical-1713-1-0-1",
    "question": "2) Prove that the estimator $\\hat{\\beta}=\\operatorname*{argmin}_{\\beta\\in\\mathcal{B}}\\sum_{i=1}^{n}w(X_{i})q(y_{i}-\\operatorname*{max}\\{X_{i}^{\\prime}\\beta,-\\ell\\})$ is $\\sqrt{n}$-consistent under Assumptions 3.1 and 3.2.",
    "gold_answer": "1. Verify the objective function's convexity and compactness of $\\mathcal{B}$.\n2. Show the first-order condition: $E\\big[w(X)X\\cdot1(v>-\\ell)m(y-v)\\big]=0$.\n3. Apply the Central Limit Theorem to establish $\\sqrt{n}(\\hat{\\beta}-\\beta_{0})\\stackrel{d}{\\rightarrow}N(0,V)$.",
    "question_context": "Let $\\ell$ equal the supremum of all points where $m(\\varepsilon)$ is constant below that point, i.e., $\\ell=\\operatorname*{sup}\\{\\bar{\\varepsilon}\\colon m(\\varepsilon)=m(\\bar{\\varepsilon})\\forall\\varepsilon\\leq\\bar{\\varepsilon}\\}$, where we take $\\ell= -\\infty$ if the set is empty.\nThe efficient score is given by $S=w^{*}(X)X\\cdot1(X^{\\prime}\\beta_{0}>-\\ell)m(y-X^{\\prime}\\beta_{0})$. If $\\mathrm{\\dot{E}}\\left[S S^{\\prime}\\right]$ is nonsingular then $(E[S S^{\\prime}])^{-1}$ is the semiparametric variance bound.\nThe estimator $\\hat{\\beta}=\\operatorname*{argmin}_{\\beta\\in\\mathcal{B}}\\sum_{i=1}^{n}w(X_{i})q(y_{i}-\\operatorname*{max}\\{X_{i}^{\\prime}\\beta,-\\ell\\})$ is the extension of that of \\~4! to censored regression.\nThe asymptotic variance of $\\hat{\\beta}$ is given by $V=Q^{-1}\\Sigma Q^{-1}$, where $Q=E^{*}[w(X)d(X)1(v>-\\ell)X X^{\\prime}]$ and $\\Sigma=E\\big[w(X)^{2}1(v>-\\ell)m(\\varepsilon)^{2}X X^{\\prime}\\big]$.\n\nThis section discusses censored regression models, focusing on moment conditions and efficient estimation methods. The key idea is that when the moment function $m(\\varepsilon)$ is constant below a certain threshold, it leads to $\\sqrt{n}$-consistent estimation. The section also covers the semiparametric efficiency bound and conditions for achieving it."
  },
  {
    "qid": "econ-empirical-923-0-0-1",
    "question": "2) How do the authors model the interplay between political incentives and economic outcomes?",
    "gold_answer": "The authors use a formal model to capture this interplay:\n1. **Objective Function**: Policymakers maximize a weighted sum of social welfare and political benefits.\n2. **Constraints**: Political constraints are incorporated as additional conditions in the optimization problem.\n3. **Equilibrium Analysis**: The model derives equilibrium policies under political constraints, showing how they deviate from first-best solutions.",
    "question_context": "Political Economy of Mechanisms by Daron Acemoglu, Michael Golosov and Aleh Tsyvinski, published in Econometrica, May 2008, Vol. 76, No. 3, pp. 619-641.\n\nThis paper explores the political economy of mechanisms, focusing on how political constraints affect economic mechanisms and policy design. The authors analyze the interplay between political incentives and economic outcomes, providing a framework to understand the limitations imposed by political economy considerations."
  },
  {
    "qid": "econ-empirical-992-1-0-1",
    "question": "2) Compare and contrast the use of cooperative and non-cooperative game theory in modeling economic behavior as presented in Telser's work. Provide specific examples where each approach might be more applicable.",
    "gold_answer": "1. **Cooperative Game Theory**: Focuses on collective action and payoff distribution among coalitions (e.g., bargaining problems). Example: Cartel formation where firms collaborate to maximize joint profits.  \n2. **Non-Cooperative Game Theory**: Analyzes individual strategies and Nash equilibria (e.g., Cournot competition). Example: Oligopolistic markets where firms compete on quantity.  \n3. **Divergence**: Cooperative theory assumes binding agreements, while non-cooperative theory does not. Telser's models likely switch between these based on context.",
    "question_context": "Telser utilises cooperative and non-cooperative game theory in his model construction but does not present us with a unifying theory. Instead he provides us with a number of illustrative examples.\n\nThe book by Lester G. Telser focuses on efficiency aspects of competition and cooperation using cooperative and non-cooperative game theory, but lacks a unifying theory."
  },
  {
    "qid": "econ-empirical-428-3-2-1",
    "question": "6) Show mathematically how the monopsony effect in the non-union regime leads to overemployment. Use the derivative \\( \\frac{\\mathrm{d}V_{2}^{\\mathrm{c}}}{\\mathrm{d}q_{\\mathrm{ol}}} \\) from the text.",
    "gold_answer": "1. From (10): \\( \\frac{\\mathrm{d}V_{2}^{\\mathrm{c}}}{\\mathrm{d}q_{\\mathrm{ol}}} = -\\int_{w_{\\mathrm{H}}}^{w_{\\mathrm{s}}} \\frac{\\mathrm{d}w_{\\mathrm{c2}}}{\\mathrm{d}q_{\\mathrm{ol}}} \\frac{Q_{1}}{a} \\mathrm{d}\\Gamma(w_{\\mathrm{2}}) > 0 \\).  \n2. Hiring more in period 1 reduces \\( w_{\\mathrm{c2}} \\) (due to lower marginal product), increasing \\( V_{2}^{\\mathrm{c}} \\).  \n3. The firm over-hires to exploit this monopsony power, driving \\( q_{\\mathrm{o1}}^{\\mathrm{c}} > q_{\\mathrm{o1}}^{*} \\).",
    "question_context": "PROPOsITION 3. Under pure wage bargaining, with and without unions, there is excessive hiring by the firm: i.e. \\( q_{\\mathrm{o1}}^{\\mathrm{c}}, q_{\\mathrm{o1}}^{\\mathrm{u}} > q_{\\mathrm{o1}}^{*} \\).\nThe monopsony effect is not present when all workers are unionised in period 2. In this case, the firm's second-period profits are independent of the marginal product of insiders.\n\nThis section analyzes dynamic inefficiencies in hiring and wage bargaining, showing that wage bargaining leads to overemployment due to monopsony effects and competition for union rents."
  },
  {
    "qid": "econ-empirical-802-3-1-3",
    "question": "8) Analyze the ambiguity in the effect of firing costs $F$ on $W_{I}(F|y,w)$, considering partial and general equilibrium effects.",
    "gold_answer": "1. Partial equilibrium: Higher $F$ delays dismissal (positive effect). \\n2. General equilibrium: Higher $F$ may reduce $U(F)$ (negative effect). \\n3. The net effect depends on the relative strength of these forces.",
    "question_context": "Under immediate implementation, worker preferences are given by $W_{I}(F|y,w)\\equiv W[y,w,F,U(F)]$, where $W$ is increasing in $y$ but ambiguous in $w$ and $F$.\nUnder delayed implementation, the dismissal condition is $J(y,w,F) < -F_{0}$, leading to a threshold $\\bar{F}(y,w,F_{0})$. Worker utility is: $$W_{D}(F|y,w,F_{0})\\equiv\\left\\{\\begin{array}{l l}{W[y,w,F,U(F)],}&{F\\leq\\bar{F}(y,w,F_{0}),}\\\\ {U(F),}&{F>\\bar{F}(y,w,F_{0}).}\\end{array}\\right.$$\nProposition 1 states: (a) $W_{D}(F^{H}|y,w,F_{0})>W_{D}(F^{L}|y,w,F_{0}) \\Rightarrow W_{I}(F^{H}|y,w)>W_{I}(F^{L}|y,w)$. (b) For $F_{0}^{H}>F_{0}^{L}$, $W_{D}(F^{H}|y,w,F_{0}^{L})>W_{D}(F^{L}|y,w,F_{0}^{L}) \\Rightarrow W_{D}(F^{H}|y,w,F_{0}^{H})>W_{D}(F^{L}|y,w,F_{0}^{H})$.\n\nThis section analyzes worker preferences over new firing costs $F$ under immediate and delayed implementation, considering the impact of past firing costs $F_{0}$."
  },
  {
    "qid": "econ-empirical-1603-1-0-3",
    "question": "4) Compare and contrast the risk exposures of Class A and Class B notes in the Tartan transaction, referencing their respective trigger levels, exhaustion levels, and credit ratings.",
    "gold_answer": "1. **Class A Notes**:  \n   - Tranche size: $75\\mathrm{mn}$.  \n   - Trigger level: 115%.  \n   - Exhaustion level: 120%.  \n   - Credit rating: Aaa/AAA (initially).  \n   - Primary risk: Credit risk (guaranteed by FGIC).  \n2. **Class B Notes**:  \n   - Tranche size: $80\\mathrm{mn}$.  \n   - Trigger level: 110%.  \n   - Exhaustion level: 115%.  \n   - Credit rating: Baa3/BBB (initially).  \n   - Primary risk: Direct exposure to catastrophic mortality risk.  \n3. Class A notes are less risky due to the principal guarantee, while Class B notes offer higher returns (LIBOR+300 bps) but bear higher mortality risk.",
    "question_context": "Within CATM securitizations, an insurer or a reinsurer transfers catastrophic mortality risk from their liability side to the capital market by means of CATM bonds. This risk arises from the exposure to, for instance, severe pandemics or natural catastrophes.\nA certain underlying mortality index based on the mortality experience in the coverage area is defined. If this index exceeds a certain level, the bond is triggered, that is, the investors start losing their principal.\nThe Tartan transaction arranged by Goldman Sachs for the reinsurer Scottish Re (Linfoot 2007). An overview of the other transactions is presented in Table A.1 in Appendix B.\nTartan issued two series of 3 year notes: A $75$ million (mn) (Class A) and an $80\\mathrm{mn}$ (Class B) tranche with different risk exposures.\nThe bonds and thus the payment to SALIC would have been triggered if a well-defined parametric index had exceeded a certain level. This so-called combined mortality index is contingent on the mortality experience of certain populations.\n$$\\hat{q}_{t}=\\sum_{x}\\left[\\omega_{x,m}\\hat{q}_{m,x,t}+\\omega_{x,f}\\hat{q}_{f,x,t}\\right],$$ where $\\hat{q}_{m,x,t}$ and $\\hat{q}_{f,x,t}$ are the mortality rates for age group $x$ in calendar year $t$ for males and females, respectively, and $\\omega_{x,m}/\\omega_{x,f}$ are the weights applied to the corresponding mortality rates.\n$$i_{t}=\\frac{\\frac{1}{2}\\left(\\hat{q}_{t}+\\hat{q}_{t-1}\\right)}{\\frac{1}{2}\\left(\\hat{q}_{2005}+\\hat{q}_{2004}\\right)}.$$\n$$l_{t}=\\operatorname*{min}\\left\\{{\\operatorname*{max}\\left\\{{l_{t-1},{\\frac{i_{t}-a}{d-a}}}\\right\\},100\\%}\\right\\},$$ where $l_{2006}=0$.\n\nThis section discusses the securitization of catastrophic mortality risk through CATM bonds, detailing the structure, advantages, and specific examples like the Tartan transaction."
  },
  {
    "qid": "econ-empirical-1089-0-1-1",
    "question": "6) How does the paper handle extreme values in hours worked (e.g., >4,200 hours/year), and what bias might their inclusion introduce?",
    "gold_answer": "The paper excludes hours >4,200/year to:\n1. Avoid measurement errors or outliers.\n2. Prevent skewing elasticity estimates upward (overreporting hours biases wage elasticity downward).\n3. Maintain consistency with usual labor supply patterns.",
    "question_context": "For the hourly wage variable, this study utilizes a series of questions, added in 1979 for those in the ORG of the CPS, which ask for information on each individual's typical hourly or weekly wage.\nThe nonlabor income variable was created by adding the asset income of the couple to the labor income of the husband, and so a secondary earner model of labor supply is implicitly assumed.\n\nThe study uses CPS data from 1979-2003, focusing on married women aged 25-55. It details wage and hours measurement, sample selection, and robustness checks."
  },
  {
    "qid": "econ-empirical-1141-3-0-3",
    "question": "4) Compare the impact of relative deprivation on cause-specific mortality (CHD and tobacco-related cancers) versus all-cause mortality, using the results from Table 5.",
    "gold_answer": "- **Step 1**: For CHD, a one standard deviation increase in RD/10,000 increases the probability of death by 0.3 percentage points (48%). For RD of logs, the increase is 0.6 percentage points (88%).\n- **Step 2**: For tobacco-related cancers, a one standard deviation increase in RD/10,000 increases the probability of death by 0.2 percentage points (58%). For RD of logs, the increase is 0.4 percentage points (125%).\n- **Step 3**: In contrast, for all-cause mortality (Table 2), a one standard deviation increase in RD/10,000 increased the probability of death by 33%.\n- **Step 4**: The proportional effects are larger for cause-specific mortality, suggesting that relative deprivation has a stronger impact on behavior-linked causes of death.",
    "question_context": "We start by regressing a dummy variable for whether the individual died within five years of the NHIS survey on mean income in the individual's state and the state Gini coefficient in household income (Column 1, Panel 1, Table 4). These results show no association between state mean income and the probability of death, but a positive association between the Gini coefficient and mortality.\nIn Column 4 we add the relative deprivation of logs, with reference groups defined using state, age-group, race, and education. In this regression, both the Gini coefficient and the relative deprivation measure appear to be positively related to mortality, though the coefficient on the Gini is now of borderline statistical significance.\nA one standard deviation (0.5) increase in relative deprivation is still associated with a 1.3 percentage point (53 percent) increase in the probability of death. These results suggest that income inequality and relative deprivation measure distinct phenomena.\nThe first panel of Table 5 shows results for the cause-specific mortality models, where reference groups were defined using state, race, age group, and education. For CHD and tobacco-related cancers, the impact of relative deprivation is positive and proportionately larger than it was in the all-cause mortality models.\n\nThe text examines the correlation between income inequality and health, focusing on the role of relative deprivation. It presents regression analyses using various health outcomes and controls, including state-level Gini coefficients and relative deprivation measures."
  },
  {
    "qid": "econ-empirical-1361-3-0-2",
    "question": "3) Derive the conditions under which government risk communication using $95\\%$ confidence limits leads to systematic overestimation of risks compared to mean risk estimates. Assume a truncated normal distribution for risk assessments.",
    "gold_answer": "1. Let $X \\sim N(\\mu, \\sigma^2)$ truncated at 0.\n2. $95\\%$ upper limit:\n   \\[ UL = \\mu + 1.645\\sigma \\]\n3. Systematic bias:\n   \\[ E[UL - \\mu] = 1.645\\sigma \\]\n4. Overestimation worsens when:\n   \\[ \\frac{\\partial E[UL - \\mu]}{\\partial \\sigma} > 0 \\]\n   and $\\sigma$ increases with risk ambiguity.",
    "question_context": "Individuals may reasonably place different information weights on different sources of information. However, the greater weight on the high risk information in situations of competing information sources could not be reconciled with differential weights on the informational source.\nThe credibility weight on the source varies depending on whether the source is providing high risk or low risk information and on the other party providing information.\nThe diversity of risk information introduced patterns that were altogether inconsistent with a conventional Bayesian learning framework. When differing risk judgements were offered by different parties, the high risk assessment was accorded a dominant role.\nThe practice of government agencies to base risk regulation on upper bounds of $95\\%$ confidence limits and, in some cases, the maximum risk assessment for chemical exposures may reflect the policy implementation of this class of perceptional biases.\n\nThe text discusses how individuals deviate from standard Bayesian learning models when processing risk information, particularly overweighting high-risk scenarios. This behavior is inconsistent with rational updating of beliefs and leads to alarmist responses."
  },
  {
    "qid": "econ-empirical-1594-1-0-1",
    "question": "2) Explain the implications of violating the weak dependence assumptions of Robinson (1989) in the context of the model $\\boldsymbol{Y}_{t} = \\boldsymbol{F}(\\boldsymbol{X}_{t}) + \\boldsymbol{U}_{t}$.",
    "gold_answer": "1. The model assumes $\\operatorname{E}(U_{1} \\vert X_{1}) = 0$ almost surely.\n2. Violating weak dependence assumptions introduces long-range dependence, which affects the asymptotic properties of estimators.\n3. The process $U_{t} = G(V_{t}, X_{t})$ is restricted further, where $\\{V_{t}\\}$ is a scalar stationary Gaussian process independent of $\\{X_{t}\\}$.\n4. Long-range dependence in $V_{t}$ infects $Y_{t}$, complicating the asymptotic behavior of statistical estimators.",
    "question_context": "The results of Robinson (1989) imply that $$ N^{1/2}\\{T_{i}-\\tau_{i}\\}\\xrightarrow{\\mathrm{~d~}}\\mathbb{N}(0,\\sigma^{2}), $$ as $N\\rightarrow\\infty$ for some $\\sigma^{2}\\in(0,\\infty)$ , under a combination of regularity conditions which include the condition of absolute regularity on $\\left(Y_{t},X_{t}^{\\prime}\\right)$.\nAbsolute regularity is a form of weak-dependence condition that is stronger than $\\propto$ -mixing, yet weaker than $\\varphi$ -mixing, and as in central limit theorems under these latter conditions, a suitable rate of decay to zero of the mixing number was assumed.\nIn the present paper we partially violate the weak dependence assumptions of Robinson (1989). The definition of $F$ implies that we can write $$ \\boldsymbol{Y}_{t}=\\boldsymbol{F}(\\boldsymbol{X}_{t})+\\boldsymbol{U}_{t}, $$ where $\\operatorname{E}(U_{1}\\vert X_{1})=0$ almost surely (a.s).\nWe restrict the process $U_{\\varepsilon}$ further by defining it as $$ U_{t}=G(V_{t},X_{t}), $$ where we assume: A.1. $\\left\\{V_{t}\\right\\}$ is a scalar stationary Gaussian process independent of $\\{X_{t}\\}$ , with zero mean, unit variance, and for $0<C<\\infty$ and $0<\\alpha<1$.\nA.2. For each $\\boldsymbol{x}\\in\\mathbb{R}^{d}$ . we can expand the function $G(v,x)$ on $\\mathbb{R}^{d+1}$ as $$ G(v,x)=\\sum_{j=0}^{\\infty}\\frac{A_{j}(x)H_{j}(v)}{j!}, $$ where $H_{j}(v)$ is the jth Hermite polynomial $$ H_{j}(v)=(-1)^{j}\\exp({\\textstyle{\\frac{1}{2}}}v^{2}){\\frac{\\mathrm{d}^{j}}{\\mathrm{d}v^{j}}}\\exp(-{\\textstyle{\\frac{1}{2}}}v^{2}). $$ and $$ \\begin{array}{l}{{A_{j}(x)=\\operatorname{E}\\{G(V_{1},x)H_{j}(V_{1})\\}.}}\\ {{\\displaystyle\\sum_{j=0}^{\\infty}\\frac{\\operatorname{E}(A_{j}^{2}(X_{1})}{j!}<\\infty.}}\\end{array} $$\nBy the properties of Hermite polynomials, $$ \\operatorname{E}(G(V_{1},x))=0,\\qquad\\operatorname{E}(G^{2}(V_{1},x))=\\sum_{j=0}^{\\infty}{\\frac{A_{j}^{2}(x)}{j!}}, $$ so that A.2 and A.3 ensure that $\\operatorname{E}(U_{1}\\mid X_{1})=0$ a.s., $\\mathrm{E}(U_{1}^{2})<\\infty$ , and also that the expectation (7) is well defined.\nCondition A.1 states that the latent variate $V_{t}$ exhibits long-range dependence in a familiar sense, the autocovariances $\\gamma_{j}$ being nonsummable.\nThe mixing properties of Gaussian processes have been well documented, especially in the former-Soviet probability literature (e.g., Ibragimov and Rozanov, 1978) where necessary and sufficient conditions on the spectral density of $V_{t}$ are given for various mixing properties to obtain.\nWe require also the following moment condition on $\\operatorname{E}(Y_{1}\\mid X_{1})$ A.4. $\\operatorname{E}|F(X_{1})|^{\\xi}<\\infty$ for some $\\xi>2.$\nWe have unfortunately not found an easy way of introducing long-range dependence in $X_{t}$ , principally owing to the nonlinear character of the model and the multivariate character of $X_{t}$ , both practically important features.\nIndeed, to deal with the generality of $G$ permitted by condition A.2, we have actually had to strengthen the condition of absolute regularity in Robinson (1989) to: $$ \\varphi_{j}=\\operatorname*{sup}_{\\substack{A\\in F_{-,r,i}^{x}B\\in F_{i+j,\\infty}^{x}}}\\operatorname*{max}\\left\\{\\left|P(A\\left|B)-P(A)\\right|,\\left|P(B\\left|A\\right.)-P(B)\\right|\\right\\} $$ satisfies $\\varphi_{j}=\\operatorname{O}(j^{-\\zeta})$ as $j\\rightarrow\\infty$ , where $\\zeta>4\\xi/(\\zeta-2)$ and $F_{a,b}^{X}$ is the $\\sigma$ -field of events generated by $X_{a},\\ldots,X_{b}$.\n\nThe text discusses the regularity conditions and weak-dependence assumptions in the context of econometric models, focusing on the implications of long-range dependence and mixing conditions."
  },
  {
    "qid": "econ-empirical-13-1-1-0",
    "question": "5) Formally derive the supply and demand functions for $N F$ in $\\{GAP, NF\\}$ space as described in the text. Explain the economic intuition behind the slopes of these functions.",
    "gold_answer": "1. Supply function:\\n   - Derived from the $GAP$ equation: $N F$ rises with $GAP$.\\n   - Economic intuition: Higher $GAP$ (slack demand) increases excess supply of domestic output, leading to higher $N F$.\\n2. Demand function:\\n   - Derived from $N F = \\widehat{N S} - \\widehat{D I}$.\\n   - Economic intuition: Excess demand for financial claims falls as $GAP$ increases, making $N F$ negatively related to $GAP$.\\n3. Equilibrium:\\n   - Intersection of supply and demand functions shows equilibrium $\\{GAP, NF\\}$ combinations.",
    "question_context": "The equation for GAP has special significance. It contains the net foreign investment rate, $N F{\\overset{-}{=}}{\\widehat{N S}}-{\\widehat{D I}}$ among the explanatory variables on the theory that a rise in $N F$ is indicative of continuing slackening of domestic demand.\nThe intersections of the demand and supply equations estimated under cyclically unchanging conditions for various quarters then show how the equilibrium combinations of NF and GAP have changed over time.\n\nThe model contains the three saving rates that together constitute the national saving rate, and the two domestic investment rates that together constitute the domestic investment rate. The estimated values of these two sets of rates are used in the identity defining NF. In addition, the model contains CDUR, TAX, TRANS, GAP, and POTGR as subsidiary dependent variables."
  },
  {
    "qid": "econ-empirical-1020-4-0-3",
    "question": "4) Derive the logit model used in Panel (c) and explain how the individual fixed effects control for unobserved heterogeneity among senders.",
    "gold_answer": "1. **Logit Model**: The dependent variable $y_{ij} = 1$ if sender $i$ chooses to send amount $j$, and $0$ otherwise. The model is:\n   $$\\ln \\left(\\frac{P(y_{ij} = 1)}{P(y_{ij} = 0)}\\right) = \\beta_0 + \\beta_1 \\text{Expected Return}_{ij} + \\beta_2 \\text{Amount Sent}_j + \\alpha_i + \\epsilon_{ij}$$\n   where $\\alpha_i$ is the individual fixed effect.\n2. **Fixed Effects**: The $\\alpha_i$ captures time-invariant unobserved heterogeneity (e.g., innate trustworthiness or risk preferences) specific to each sender.\n3. **Interpretation**: The coefficient $\\beta_1$ measures the within-individual effect of expected return on the likelihood of sending a given amount, controlling for all unobserved individual characteristics.",
    "question_context": "On the basis of the above definitions, we regard only the beliefs component as a measure of trust. To isolate it, we use the sender’s expectations, which we elicited during the experiment. These expectations should be unaffected by the sender’s utility function and should be a true measure of the sender’s expected level of the receiver’s trustworthiness – that is, trust.\nTable 1 validates our hypothesis that the sender’s behaviour is affected by these two factors. Our dependent variable is the quantity sent. Our explanatory variables are the subjects’ preferences and her beliefs. As we elicited expectations using the strategy method, the amount expected back is conditional on the amount sent. The three panels of Table 1 use this information in different ways. Panel $(a)$ includes only the expected return conditional on sending $\\$50$ . Panel $(b)$ includes 10 different regressions with the expected returns conditional on each possible quantity sent. Panel $(c)$ pools all the regressions of panel $(b)$ introducing an individual fixed effect.\nColumn 1 of panel $(a)$ reports an ordinary least squares regression of the dollar amount sent by subjects in the trust game on their risk tolerance (i.e. the certainty equivalent of the lottery), their other-regarding preferences (i.e. their willingness to conditionally cooperate in the social dilemma game) and their answer to the WVS-trust question.\nAs expected, more risk-tolerant individuals send more, as do subjects who are motivated by other-regarding preferences. Increasing the certainty equivalent by $\\$5$ increases the amount sent by 90 cents on the dollar (i.e. $0.18\\times5$ ); or alternatively, a one-standard deviation decrease in risk aversion leads to a $17\\%$ increase in the average amount sent.\nIn Column 3, we run the same regression as before but omitting the WVS-trust variable. A one-standard deviation increase in the expected trustworthiness of others (conditional on $\\$50$ sent) increases the average amount sent by $24\\%$ . According to these results, the quantity sent in the trust game is indeed a combination of two components: the preferences of the sender (risk aversion and other-regarding preferences) and his beliefs in others’ trustworthiness (trust).\nPanel $(c)$ displays the results of a regression model where the data are organised as a panel with 10 observations per subject corresponding to the 10 possible quantities sent: $\\$5$ , $\\$10$ , $\\$15$,…, $\\$50$ ). In this framework, behaviour is predicted by combining the expected utility of each element of the choice set in one single econometric model to predict behaviour. The dependent variable is a binary variable that corresponds to the alternative chosen by each subject.\n\nThe sender’s behaviour in the trust game is driven by two factors: preferences and beliefs. For a given type of preferences, a sender who has higher expectations about other people’s trustworthiness will send more. Similarly, for a given level of expectations, a more altruistic sender (or a less risk-averse sender) will send more."
  },
  {
    "qid": "econ-empirical-211-1-2-0",
    "question": "5) Interpret the three results in Theorem 4.6 regarding the size and power of the test. What do they imply about the test's performance under different scenarios?",
    "gold_answer": "1. If $H_0^1$ is true and $P_o$ has measure zero, the test rejects with probability 0 (no false positives). \\n2. If $H_0^1$ is true and $P_o$ has positive measure, the test achieves the nominal size $\\alpha$ as $\\eta \\to 0$. \\n3. If $H_1^1$ is true, the test rejects with probability 1 (consistent against fixed alternatives).",
    "question_context": "Theorem 4.6. Suppose that Assumptions 4.1–4.3 and 4.5 hold and we reject $H_{0}^{1}$ if $\\hat{T}_{1}>\\hat{c}_{\\eta,n}^{1}.$ Then, 1. suppose that $H_{0}^{1}$ is true and the Lebesgue measure of $P_{o}$ is zero, then $\\operatorname*{lim}_{n\\to\\infty}P(\\hat{T}_1 > \\hat{c}_{\\eta,n}^1) = 0$, 2. suppose that $H_{0}^{1}$ is true and the Lebesgue measure of $P_{o}$ is strictly greater than zero, then $\\operatorname*{lim}_{\\eta\\to 0}\\operatorname*{lim}_{n\\to\\infty}P(\\hat{T}_1 > \\hat{c}_{\\eta,n}^1) = \\alpha$, and 3. suppose that $H_{1}^{1}$ is true, then $\\operatorname*{lim}_{n\\to\\infty}P(\\hat{T}_1 > \\hat{c}_{\\eta,n}^1) = 1$.\n\nThis section analyzes the size and power properties of the proposed test under the null and alternative hypotheses."
  },
  {
    "qid": "econ-empirical-816-1-0-2",
    "question": "3) Why is it often impossible to find $c$ and $\\gamma_0$ that simultaneously satisfy both conditions (7) and (8)?",
    "gold_answer": "1. The LHS of (8) is typically monotonic in $\\gamma$.\n2. For small $\\gamma_0$, it decreases monotonically; for large $\\gamma_0$, it increases monotonically.\n3. Only for a narrow range of $\\gamma_0$ does the size function resemble a flat parabola, making simultaneous satisfaction of (7) and (8) rare.",
    "question_context": "The problem of testing ${\\mathrm{H}}_{0}$ against $\\mathtt{H}_{\\mathtt{a}}$ (or $\\mathrm{H}_{0}^{-}$ against $\\mathtt{H}_{\\mathtt{a}}^{-}$ a )is invariant to transformations of the form y → moy $\\begin{array}{r}{{\\textbf{\\textsf{y}}}\\to{\\textbf{\\em\\eta}}_{0}{\\bf y}+{\\textbf{\\textsf{X}}}{\\boldsymbol\\eta}}\\end{array}$ where $\\eta_{0}$ is a positive scalar and $\\pmb{\\eta}$ is a $\\mathbf{k}{\\times}1$ vector.\nFollowing King (1987b), it may be that the critical regions of a POI test with optimal power at p = Po $>0$ (or at $\\rho=\\rho_{0}<0\\mathrm{~i~}$ f testing $\\mathrm{H}_{0}^{\\overline{{\\mathbf{\\alpha}}}}$ against $\\mathtt{H}_{\\mathtt{a}}^{-}$ ) will be of the form $$\\sin^{\\cdot}(\\rho_{0},\\gamma_{0})=\\hat{\\mathbf{u}}^{\\prime}\\Sigma(\\rho_{0})^{-1}\\hat{\\mathbf{u}}\\Big/\\Tilde{\\mathbf{u}}^{\\prime}\\Omega(\\gamma_{0})^{-1}\\Tilde{\\mathbf{u}}<\\mathbf{c}$$ in which $\\hat{\\bf u}$ and $\\widetilde{\\mathbf{u}}$ are the generalized least squares residual vectors from (1) assuming covariance  matrices. $\\Sigma(\\rho_{_{0}})$ and $\\mathfrak{Q}(\\mathfrak{g}_{0})$ ,. respectively.\nThe existence of a Pol test of this form requires the critical value c and the parameter $\\ensuremath{\\gamma}_{0}$ to be chosen such that $$\\operatorname*{Pr}[\\mathtt{s}(\\rho_{0},\\boldsymbol{\\mathscr{x}}_{0})\\mathrm{~<~}\\mathtt{c}\\mathrm{~|~\\xi~u~\\sim~}\\mathtt{N}(0,\\Omega(\\boldsymbol{\\mathscr{x}}_{0}))]=\\alpha$$ and $$\\begin{array}{r}{\\mathsf{P r}[\\mathsf{s}(\\rho_{0},\\mathsf{z}_{0})\\subset\\mathsf{c}\\mid\\mathsf{u}\\sim\\mathsf{N}(0,\\Omega(\\mathfrak{z})),0\\leq\\gamma\\le1]\\le\\alpha}\\end{array}$$ where $\\alpha$ is the desired level of significance.\n\nThe section discusses the problem of testing hypotheses regarding autocorrelation in disturbances, focusing on invariant transformations and the construction of Point-Optimal Invariant (POI) tests."
  },
  {
    "qid": "econ-empirical-798-2-0-0",
    "question": "1) Derive the utility difference equation $U_{i j}^{*}-U_{i k}^{*}$ from first principles, explaining the role of $g(y_{i k j}-y_{i k})\\psi$ and the error term $(\\varepsilon_{i j}-\\varepsilon_{i k})$.",
    "gold_answer": "1. **Utility Specification**: Start with the latent utility for state $k$: $U_{i k}^{*} = V(y_{i k}, h_{k}; \\mathbf{X}_{i}) + \\varepsilon_{i k}$, where $V(\\cdot)$ is the deterministic component.\\n2. **Difference Form**: The utility difference between states $j$ and $k$ is $U_{i j}^{*} - U_{i k}^{*} = [V(y_{i j}, h_{j}; \\mathbf{X}_{i}) - V(y_{i k}, h_{k}; \\mathbf{X}_{i})] + (\\varepsilon_{i j} - \\varepsilon_{i k})$.\\n3. **Income Effect**: Approximate $V(\\cdot)$ as linear in $g(y_{i k j}-y_{i k})\\psi$, where $g(\\cdot)$ captures marginal utility of income differences.\\n4. **Error Term**: $(\\varepsilon_{i j} - \\varepsilon_{i k})$ accounts for unobserved heterogeneity and measurement error, assumed iid across individuals but correlated across choices.",
    "question_context": "The goal here is to obtain an unbiased measure of financial incentives for labour supply, and ignoring these potential sources of endogeneity (spurious correlations) may potentially bias estimated parameters of interest.\nBlundell and MaCurdy (1999) survey the recent labour supply literature and conclude that current best practice in static modelling of labour supply and programme participation takes a discrete choice approach.\nOur approach is closest to that of Keane and Moffitt (1998) in the sense that we use a multinomial probit with free correlation structure to model choice over three labour supply states combined with participation in one programme, jointly with the wage.\nThe utility gain in moving from alternative to $j$ is given by: $$U_{i j}^{*}-U_{i k}^{*}=g(y_{i k j}-y_{i k})\\psi+\\mathbf{X}_{i}\\beta_{j k}+(\\varepsilon_{i j}-\\varepsilon_{i k})$$ where $g(y_{i j},y_{i k})$ is a row vector of differences of functions of the net incomes, and $\\psi$ reflects the mean tastes of the sample.\n\nThe text discusses the econometric challenges in modeling labor supply, program participation, and wages, addressing endogeneity issues through a discrete choice approach within a multinomial probit framework."
  },
  {
    "qid": "econ-empirical-199-3-1-1",
    "question": "2) Interpret the estimates for Delta and Eta in the semiparametric model. What do they imply about household consumption?",
    "gold_answer": "1. Delta (\\( \\Delta \\)) values (2.39 for singles, 2.75 for couples) indicate economies of scale in household consumption.\n2. Eta (\\( \\eta \\)) values (0.0059 for singles, 0.0310 for couples) reflect vertical shifts in consumption patterns.\n3. Higher Delta for couples suggests greater efficiency in shared consumption.",
    "question_context": "Semiparametric model estimates: Wavelength (0.34 for singles, 0.34 for couples), Range of log delta (0.68 for singles, 0.61 for couples), Delta (2.39 for singles, 2.75 for couples), Eta (0.0059 for singles, 0.0310 for couples). Standard errors and loss metrics are provided.\n\nThe semiparametric model estimates include wavelength, range of log delta, and estimates for delta and eta. Standard errors and loss metrics are provided for singles and couples."
  },
  {
    "qid": "econ-empirical-893-1-0-0",
    "question": "1) Derive the TRANSLOG demand system from the indirect utility function $U=-\\alpha(\\log(P/x))$ and explain the normalization condition when $\\Sigma,\\Sigma_{s}L^{r s}\\neq0$.",
    "gold_answer": "To derive the TRANSLOG demand system:\n1. Start with the indirect utility function: $U=-\\alpha(\\log(P/x))$.\n2. Apply Roy's identity to obtain the demand functions.\n3. The normalization condition $\\Sigma\\bar{\\Sigma_{s}\\Sigma L^{r s}}=1$ is imposed when $\\Sigma,\\Sigma_{s}L^{r s}\\neq0$ to ensure the model is identified.\n4. This normalization simplifies the model to LOG1 demands when $A=\\alpha$.",
    "question_context": "Fractional demands having $\\phi=\\log{x}$ are cases i, iv, and $\\mathbf{v}$ of Theorem 1, that is, the PIGLOG, LOG1, and LOG2 models.\nThe TRANSLOG demand system is defined as demands arising from the indirect utility function $U=-\\alpha(\\log(P/x))$.\nLOG2 demands, which can be rewritten as $w^{r}=\\frac{\\left(B_{r}+A_{r}\\right)F}{A+\\log x}+B_{r}$, differ substantially from PIGLOG and TRANSLOG type models.\nPIGL demands have been described and applied by Muellbauer (1975) and include many popular demand systems, such as the linear expenditure system.\nEXP demands can be written as $w^{r}=\\frac{\\left(k B_{r}+A_{r}/A\\right)F^{\\prime}}{A+x^{-k}}+B_{r}$, where F' is the derivative of F.\nTAN demands, case vi, has $\\phi=\\tan(k\\log(x))$. An example of TAN demands arises from the indirect utility function $U=\\alpha(P)\\sin(k x)+\\beta(P)\\cos(k x)$.\nFractional demands provide a parsimonious way of increasing the range of income response patterns (that is, Engel curve shapes) encompassed by demand systems.\nAggregate demands are approximately given by $W^{r}\\simeq\\frac{a^{r}+b^{r}\\operatorname{E}(x\\phi)/X}{1+c\\operatorname{E}(x\\phi)/X}$, where $W^{r}=P^{r}Q^{r}/X$ is the aggregate, economy-wide budget share for good $r$.\n\nThis section discusses fractional demand system utility functions, their estimation, and aggregation. It covers various forms of fractional demands including PIGLOG, LOG1, LOG2, PIGL, EXP, and TAN models, and their applications in economic analysis."
  },
  {
    "qid": "econ-empirical-828-2-1-1",
    "question": "6) Derive the contraction rate for μ_α^{σ,y} as given in Theorem 2(ii) and interpret the role of the regularity space Φ_β.",
    "gold_answer": "1. The rate is O_p(αₙ^β + 1/(αₙ² n) αₙ^β + 1/(αₙ² n) + αₙ^κ).\n2. Φ_β = R(Ω₀^{1/2}K*KΩ₀^{1/2})^{β/2} encodes smoothness of δ₊.\n3. Higher β implies faster convergence (smoother δ₊).\n4. Saturation effect limits β ≤ 2 for Tikhonov regularization.",
    "question_context": "The pair (φ₊, μₙ^{σ,y}) is inconsistent, i.e., μₙ does not weakly converge to δ_{φ₊} with probability one.\nUnder Assumptions 4 and 5, if αₙ → 0 and αₙ² n → ∞, ||φ̂_α - φ₊|| → 0 in P^{σ₊,φ₊,W}-probability.\nThe joint posterior distribution νₙʸ × μ_α^{σ,y} degenerates toward a Dirac measure on (σ₊², φ₊).\n\nThe asymptotic properties of the posterior distributions νₙʸ, μ_α^{σ,y}, and μ_αʸ are analyzed from a frequentist perspective, focusing on consistency and convergence rates."
  },
  {
    "qid": "econ-empirical-340-1-1-1",
    "question": "4) Under Assumption 1, derive the control function representation $Y_{n}=\\lambda W_{n}Y_{n}+X_{n}\\beta-\\mu_{\\eta}^{\\prime}\\rho\\mathbf{1}_{n}+\\eta_{n}\\rho+\\xi_{n}$ and show how it addresses the endogeneity from correlation between $v_i$ and $\\eta_i$.",
    "gold_answer": "1. From Assumption 1: $E(v_i|\\eta_i) = (\\eta_i - \\mu_\\eta)'\\rho$\n2. Decompose $v_i = (\\eta_i - \\mu_\\eta)'\\rho + \\xi_i$ where $E(\\xi_i|\\eta_i) = 0$\n3. Substitute into original equation:\n   $y_i = \\lambda\\sum_j w_{ij}y_j + x_i'\\beta + (\\eta_i - \\mu_\\eta)'\\rho + \\xi_i$\n4. Let $c = -\\mu_\\eta'\\rho$ and write in matrix form:\n   $Y_n = \\lambda W_n Y_n + X_n \\beta + c 1_n + \\eta_n \\rho + \\xi_n$\n5. Now $\\xi_n$ is uncorrelated with $\\eta_n$ by construction\n6. Including $\\eta_n$ as controls addresses the endogeneity from $Corr(v_i, \\eta_i)$",
    "question_context": "Compared to a standard linear regression model, we have two sources of endogeneity: (1) the usual endogeneity from the spatial term $\\sum_{j\\neq i}w_{i j}y_{j}$ via $y_{j}$'s since the SAR model is a simultaneous equation system and $Y_{n}$ is generated by $W_{n},X_{n}$, and $V_{n}$ as seen from the reduced form of the SAR model $Y_{n}=(I_{n}-\\lambda W_{n})^{-1}(X_{n}\\beta+V_{n})$ (2) the unusual endogeneity from the spatial term $\\sum_{j\\neq i}w_{i j}y_{j}$ via $w_{i j}$s from the direct correlation between $v_{i}$ and $\\eta_{i}$ since $w_{i j}$ is constructed by $z_{i j}$ and $z_{j i}$ while elements in $\\eta_{i}$ are unobserved interactive effects in $z_{i j}$ and $z_{j i}$.\nAssumption 1. The error term $\\varepsilon_{i j}$ is i.i.d. $(0,\\sigma_{\\varepsilon}^{2})$ and independent of $(\\Phi,\\pmb{\\eta}_{n},X_{n},V_{n})$. The $E|v_{i}|^{4+\\delta}$ and $E\\|\\eta_{i}\\|^{4+\\delta}$ exist for some $\\delta>0$. $\\frac{1}{n}\\pmb{\\eta}_{n}^{\\prime}\\pmb{\\eta}_{n}\\stackrel{p}{\\rightarrow}\\pmb{\\Sigma}_{\\eta}$, where $\\Sigma_{\\eta}$ is an $r^{*}\\times r^{*}$ positive definite matrix. The conditional expectation $E(v_{i}|\\eta_{i})= (\\eta_{i}-\\mu_{\\eta})^{\\prime}\\rho$ where $\\mu_{\\eta}=E(\\eta_{i})$ and $\\rho$ is an $r^{*}-$ dimensional vector.\n\nThe paper identifies two sources of endogeneity in spatial autoregressive models with bilateral variables."
  },
  {
    "qid": "econ-empirical-679-0-1-1",
    "question": "2) Explain the role of the slope parameters \\(\\nu_1^i\\) in the observation equation and how they are identified.",
    "gold_answer": "The slopes \\(\\nu_1^i\\) measure the correlation between filtered data \\(y_t^i\\) and the model-implied cyclical component \\(H(\\theta)x_t\\). Identification arises from:\n1. **Normalization**: \\(\\nu_1^1 = 1\\) for the first filter (e.g., POLY).\n2. **Cross-filter variation**: Filters with different properties (e.g., HP vs. first differences) provide independent information about \\(x_t\\).\n3. **Bayesian priors**: Tight priors on \\(\\nu_1^i\\) (e.g., \\(\\nu_1^i \\in [0.7, 0.9]\\)) help regularize estimates.",
    "question_context": "We propose a method to estimate the structural parameters of a time invariant cyclical DSGE model which uses multiple sources of cyclical information. The approach borrows ideas from the recent literature that employs data-rich environments.\nWe set up an estimation framework where the cyclical DSGE model is the unobservable factor, vectors of filtered data are contaminated observable proxies, and structural DSGE parameters are jointly estimated together with the nonstructural parameters that link the model and the observables using signal extraction techniques.\nWe employ eight procedures to extract the cyclical component of all variables. The first (POLY) fits a second order polynomial to each series separately, allowing for a change in the parameters at 1980:3. The cyclical component is the residual of the regression.\n\nThe authors propose a signal extraction approach where the cyclical DSGE model is treated as an unobservable factor, and multiple filtered data series serve as contaminated proxies. This method jointly estimates structural DSGE parameters and nonstructural parameters linking the model to the observables."
  },
  {
    "qid": "econ-empirical-1220-3-0-1",
    "question": "2) Using Table 7, derive the relative risk ratio for transitioning from 'Independent' to 'Nursing Home' compared to 'Independent' to 'Cohabitation'. How does unobserved heterogeneity influence these transitions?",
    "gold_answer": "The relative risk ratio (RRR) is calculated as: \\[ RRR = \\frac{\\lambda_{\\text{Independent} \\rightarrow \\text{Nursing Home}}}{\\lambda_{\\text{Independent} \\rightarrow \\text{Cohabitation}}} = \\frac{6.49}{2.32} \\approx 2.80 \\]. This indicates a 2.8 times higher risk of transitioning to a nursing home than to cohabitation. Unobserved heterogeneity (e.g., frailty, family support) significantly increases the risk of nursing home transitions, as evidenced by the large coefficient (6.49***).",
    "question_context": "Tables 5 and 6 present estimates of the age-dependence ${\\boldsymbol{\\Upsilon}}_{1}^{b}$ parameters and the duration-dependence $\\upgamma_{0}^{b e}$ parameters, respectively. Parameter estimates for the load factors are presented in Table 7. Table 8, which is divided into three parts, presents coefficients for other covariates of interest for transitions out of independence, cohabitation and nursing home, respectively.\nBecause unobserved heterogeneity is found to be important (see Table 7), all tables include results without (Column 'Base') and with (Column 'Mixed') unobserved heterogeneity.\nTable 5: Age dependence (rb)—All States. Independent: Base 0.23*** (0.01), Mixed 0.47*** (0.04). Cohabitation: Base 0.02* (0.01), Mixed 0.05 (0.05). Nursing home: Base 0.03*** (0.01), Mixed 0.12*** (0.04).\nTable 7: Load Factors—-Mixed Model. Independent to Cohabitation: 2.32*** (0.26). Independent to Nursing Home: 6.49*** (0.48). Independent to Death: 1.38*** (0.21). Cohabitation to Independent: 0.75 (0.58). Cohabitation to Nursing Home: 0.40 (0.49). Cohabitation to Death: 0.21 (0.48). Nursing home to Independent: 1.41*** (0.41). Nursing home to Cohabitation: 1.28*** (0.45). Nursing home to Death: 0.59** (0.29).\n\nThis section presents results from the analysis of living-arrangement dynamics, focusing on age-dependence and duration-dependence parameters, load factors, and other covariates. The results are presented with and without accounting for unobserved heterogeneity, with standard errors based on numerical second derivatives."
  },
  {
    "qid": "econ-empirical-17-3-0-3",
    "question": "4) Calculate the implied change in the saving rate for the average individual if the ratio of pension wealth to current income increases by 10%, using the estimates from the regression.",
    "gold_answer": "For the average individual:\n- Initial saving rate: 0.0865.\n- After 10% increase in pension wealth/income: saving rate drops to 0.044.\n- Reduction: 0.0425 (≈50 percentage points), highlighting a significant negative effect of pension wealth on saving rates.",
    "question_context": "In particular, for each household in the sample, we relate saving rates to pension wealth and future earnings, both of which are corrected by the factor specified in equation (5). These two variables are interacted, in most specifications, with age dummies.\nWe estimate the coefficients of interest by Instrumental Variables, using as instruments the interaction of time and group dummies.\nThe variability that identifies the parameter of interest in Table V is the interaction between time and group effect, where groups are defined by the interaction of cohort and occupation groups.\nThe degree of substitutability for the middle-aged households is very high, and close to $^{-1}$.\n\nThe analysis examines the relationship between pension wealth and saving rates using a regression framework, incorporating demographic variables and instrumental variables to account for various effects."
  },
  {
    "qid": "econ-empirical-37-2-2-1",
    "question": "10) Explain the short-run drop in sales to others in the event year, linking it to capacity constraints and the lagged response of net assets.",
    "gold_answer": "1. **Drop in Sales**: Sales to others fall by 19% (total), 75% (corporate), driven by a 78% drop in average transaction size.\n2. **Capacity Constraints**: Firms cannot immediately adjust fixed inputs (e.g., assets), leading to a reallocation of capacity to the MNC buyer.\n3. **Asset Lag**: Net assets react with a one-year lag, consistent with fixed input adjustments taking time.",
    "question_context": "Figure II shows the patterns of sales to other buyers (excluding the first MNC buyer). Four years after the event, sales to others increase by 20%, and corporate sales to others by 45%. The extensive margin (number of buyers) accounts for two-thirds of the increase, and the intensive margin (average transaction size) for one-third.\nShort-run adjustments: In the event year, sales to others drop by 19%, corporate sales by 75%, and noncorporate sales by 9%. The intensive margin drives the drop in corporate sales (-78%), suggesting capacity constraints.\nTable III shows that first-time suppliers sell to buyers in more sectors (18% more two-digit sectors) and to buyers with 49% more workers, 53% higher sales, and 40% more buyers. Buyers are also more internationally exposed.\n\nThe event study analyzes the effects of starting to supply to an MNC on firm performance, including sales to other buyers, extensive vs. intensive margin responses, and changes in buyer characteristics. Results show persistent growth in sales to others, driven by both new buyers and larger transactions with existing buyers."
  },
  {
    "qid": "econ-empirical-372-1-0-2",
    "question": "3) For a Gaussian location-scale model $y_{t} = \\mu_{0} + \\sigma_{0}\\varepsilon_{t}$, show that the estimation of the mean parameter $\\mu_{0}$ does not affect the asymptotic distribution of the test statistic.",
    "gold_answer": "1. From Proposition 5, the gradient vector $\\mathbf{D}$ for the Gaussian model is $\\mathbf{D} = (0, -a_{\\alpha_{i}}f_{z_{t}}(a_{\\alpha_{i}})/\\sigma_{0}^{2})^{\\prime}$. 2. The first component of $\\mathbf{D}$ (corresponding to $\\mu_{0}$) is zero, implying no contribution from $\\mu_{0}$ to the asymptotic variance. 3. Thus, the asymptotic distribution depends only on the variance parameter $\\sigma_{0}$, and the test statistic is unaffected by $\\mu_{0}$ estimation.",
    "question_context": "Even though the tests we propose can be applied to raw data, they will be most useful as a diagnostic tool for model specification. Thus, in practice we will be analyzing residuals, $\\bar{\\hat{\\varepsilon}_{t}}(\\hat{\\pmb{\\theta}}_{T})$ , which depend on parameter estimates, instead of the true error $\\varepsilon_{t}(\\pmb\\theta_{0})$ . Our tests are subject to the uncertainty created by parameter estimation.\nTo understand how parameter estimation affects the tests, let us take a mean value expansion and apply Slutsky’s Theorem, e.g., Randles (1982). This yields $$\\begin{array}{r l}&{\\sqrt{T}(\\hat{p}_{i}^{k}(\\hat{\\pmb{\\theta}}_{T})-p_{i})}\\ &{\\quad\\quad=\\sqrt{T}(\\hat{p}_{i}^{k}(\\pmb{\\theta}_{0})-p_{i})}\\ &{\\qquad+\\sqrt{T}(\\hat{\\pmb{\\theta}}_{T}-\\pmb{\\theta}_{0})^{\\prime}\\displaystyle\\operatorname*{lim}_{T\\rightarrow\\infty}E\\biggl[\\frac{\\partial\\hat{p}_{i}^{k}(\\pmb{\\theta})}{\\partial\\pmb{\\theta}}\\biggl|_{\\pmb{\\theta=\\theta}_{0}}\\biggr]+o_{p}(1).}\\end{array}$$\nA1. $\\sqrt{T}({\\hat{\\pmb\\theta}}_{T}-{\\pmb\\theta}_{0})\\xrightarrow{d}{\\bf N}(0,{\\bf A}^{-1}{\\bf B}{\\bf A}^{-1})$ where $\\mathrm{~\\bf~A~}\\equiv$ $E[-\\mathbf{H}(\\pmb{\\theta}_{0})]$ , $\\mathbf{B}\\equiv E[\\mathbf{S}(\\pmb{\\theta}_{0})\\mathbf{S}^{\\prime}(\\pmb{\\theta}_{0})],$ and ${\\bf H}(\\pmb{\\theta}_{0})= $ $\\begin{array}{r}{\\frac{1}{T}\\sum_{t=1}^{T}\\mathbf{H}_{t}(\\pmb{\\theta}_{0})}\\end{array}$ and $\\begin{array}{r}{{\\bf S}(\\pmb{\\theta}_{0})=\\frac{1}{\\sqrt T}\\sum_{t=1}^{T}{\\bf s}_{t}(\\pmb{\\theta}_{0})}\\end{array}$ are the Hessian matrix and the score vector corresponding to quasi-maximum likelihood (QML) estimation.\nProposition 4. Under A1–A3 we have $$\\sqrt{T}(\\hat{p}_{i}^{k}(\\hat{\\pmb\\theta}_{T})-p_{i})\\stackrel{d}{\\rightarrow}\\mathbf N(0,\\tau_{k,i}^{2}),$$ where $\\tau_{k,i}^{2}=\\sigma_{k,i}^{2}+{\\bf D}^{\\prime}{\\bf A}^{-1}{\\bf B}{\\bf A}^{-1}{\\bf D}+2E[\\sqrt{T}(\\hat{p}_{i}^{k}(\\hat{\\pmb{\\theta}}_{T})-p_{i})\\times$ $\\mathbf{S}^{\\prime}(\\pmb{\\theta}_{0})]\\mathbf{A}^{-1}\\mathbf{D}$ .\n\nThe proposed tests are most useful as diagnostic tools for model specification, analyzing residuals that depend on parameter estimates rather than true errors. The discussion focuses on the impact of parameter estimation uncertainty on test statistics."
  },
  {
    "qid": "econ-empirical-1653-4-0-3",
    "question": "4) Explain the role of the preliminary estimator $\\theta_n^{**}$ in the construction of the adaptive AC estimator $\\hat{\\theta}_n$.",
    "gold_answer": "The preliminary estimator $\\theta_n^{**}$ is used to:\n1. Estimate the nuisance parameters (e.g., $\\hat{\\psi}_n$ and $\\hat{I}_n$) required for the construction of the AC estimator.\n2. Ensure the discretization and boundedness conditions needed for the asymptotic theory hold.\nThe final adaptive AC estimator is given by:\n\n$$\n\\hat{\\theta}_n = \\theta_n^{**} + \\delta_n(\\theta_n^{**}) (\\hat{I}_n R_n(\\theta_n^{**}))^{-1} \\hat{\\Delta}_n(\\theta_n^{**})\n$$\n\nwhere $\\delta_n(\\theta)$ is the normalizing matrix and $R_n(\\theta)$ is as defined in (43).",
    "question_context": "Suppose that the shape or form of this density is unknown or is unavailable in closed usable form, as in the case of a stable distribution. Then a possible procedure is to estimate $p(x)$ from the sample and use it in place of the unknown $p(x)$ in the construction of AC estimators.\nThe resulting estimator need not in general be an AC estimator, but Stein (1956) gave results suggesting that it will be so if $p(x)$ is assumed to belong to, for example, the class of densities symmetric around 0.\nFollowing Kreiss, and also taking advantage of the ideas in Schick (1987), which deals with the i.i.d. case, we shall also use the kernel estimator, but it should be clear from the arguments of the proof that any reasonable density estimator may be used in place of the kernel estimator.\n\nThe construction of AC estimators involves the likelihood ratios (LRs) of the sample, which in turn involves the density $p(x)$ of the error variables. When the density is unknown or not available in closed form, a kernel density estimator can be used. Adaptive estimation is possible under symmetry assumptions on $p(x)$."
  },
  {
    "qid": "econ-empirical-713-5-1-3",
    "question": "8) Compare the state-space representations of the full information and no information models, highlighting key differences.",
    "gold_answer": "1. Full info state vector: 7 elements (includes $\\varepsilon_t, u_t$ lags).\n2. No info state vector: 5 elements (only $\\eta_t$ lags).\n3. Full info $A$ matrix: accounts for both aggregate ($\\varepsilon$) and idiosyncratic ($u$) shocks.\n4. No info $A$ matrix: only idiosyncratic shocks ($\\eta$).\n5. Full info captures more complex dynamics due to additional state variables.",
    "question_context": "Utility for the quadratic model can be written as $U(X_{t})=E_{t}\\sum_{j=0}^{\\infty}\\beta^{j}X_{t+j}^{\\prime}R X_{t+j}$ where $\\beta=1/(1+r)$ and $X_{t}$ represents the state vector of the system.\nEquation (C1) can be rewritten as $U(X_{t})=X_{t}^{\\prime}P X_{t}+{\\frac{1+r}{r}}\\operatorname{trace}(P T\\Sigma T^{\\prime})$ where $P=R+\\beta A^{\\prime}P A.$\nThe utility difference is converted to quarterly rates by multiplying by $r/(1+r)$. To convert to dollar terms, divide by the expected marginal utility $E u^{\\prime}(c_t) = (\\bar{c} - \\bar{y})$.\n\nThis section discusses the calculation of utility loss due to ignoring aggregate information in consumption decisions, using quadratic utility and state-space representations."
  },
  {
    "qid": "econ-empirical-189-3-0-3",
    "question": "4) Derive the variance-covariance matrix of the constrained estimator $\\widetilde{\\beta}$ under the new constraint $Q_1 + Q_2 + Q_3 + Q_4 = 0$, and explain how it is used in hypothesis testing.",
    "gold_answer": "1. **Variance-Covariance Matrix**:\n   $$\n   \\text{var}(\\widetilde{\\beta}) = \\sigma^2 (X^{\\prime}X)^{-1} - \\sigma^2 (X^{\\prime}X)^{-1} R^{\\prime} [R (X^{\\prime}X)^{-1} R^{\\prime}]^{-1} R (X^{\\prime}X)^{-1}.\n   $$\n\n2. **Given Matrix**:\n   $$\n   \\text{var}(\\widetilde{\\beta}) = \\begin{pmatrix}\n   0.0058 & -0.000027 & 0 & 0 & 0 & 0 \\\\\n   -0.000027 & 0.0000016 & 0 & 0 & 0 & 0 \\\\\n   0 & 0 & 0.0046 & -0.00014 & -0.00014 & -0.0043 \\\\\n   0 & 0 & -0.00014 & 0.00014 & 0.00014 & -0.00014 \\\\\n   0 & 0 & -0.00014 & 0.00014 & 0.00014 & -0.00014 \\\\\n   0 & 0 & -0.0043 & -0.00014 & -0.00014 & 0.0046\n   \\end{pmatrix} \\sigma^2.\n   $$\n\n3. **Hypothesis Testing**: The diagonal elements give the variances of $\\widetilde{Q}_i$, used to construct $F$-statistics for testing linear hypotheses.",
    "question_context": "Consider a series of quarterly measurements on a variate $y$ with an unconstrained model given by\n\n$$\ny_{t i}=a+b t+Q_{i}+e_{t i},\n$$\n\nwhere the years are denoted by $t=1,2,\\ldots,32$ and the quarters by $i=1,2,3,4$. For the first thirty years the parameters were estimated subject to a semi-annual seasonal pattern with constraints\n\n$$\n\\begin{array}{l}{{\\displaystyle\\sum_{i=1}^{4}Q_{i}=0,}}\\\\ {{\\displaystyle Q_{1}-Q_{4}=0,}}\\\\ {{\\displaystyle Q_{2}-Q_{3}=0,}}\\end{array}\n$$\n\nyielding the estimates\n\n$$\n\\tilde{\\beta}_{30}=(1.1581,0.53227,1.0386,-1.0386,-1.0386,1.0386)^{\\prime}.\n$$\n\nIt is suspected that the last two restrictions are false and that the data follow a quarterly effects pattern. The problem is to estimate $\\beta$ subject to the constraint\n\n$$\n\\begin{array}{r}{Q_{1}+Q_{2}+Q_{3}+Q_{4}=0,}\\end{array}\n$$\n\nand test the hypotheses\n\n$$\n\\begin{array}{r}{H_{1}:Q_{1}=Q_{4},}\\\\ {H_{2}:Q_{2}=Q_{3},}\\end{array}\n$$\n\nusing as much of the information from the previous study as possible.\nThe estimate of\n\n$$\n\\beta=(a,b,Q_{1},Q_{2},Q_{3},Q_{4})^{\\prime},\n$$\n\nsubject to\n\n$$\n\\begin{array}{r}{Q_{1}+Q_{2}+Q_{3}+Q_{4}=0,}\\end{array}\n$$\n\nis\n\n$$\n\\widetilde{\\beta}=(1.161,0.532,0.821,-1.026,-1.056,1.261)^{\\prime}.\n$$\n\nThe (uncorrelated) estimates of $Q_{2}-Q_{3}$ and $Q_{1}-Q_{4}$ are 0.030 and -0.440, each with the same 6 d.f. estimated variance of 0.0207. The respective $F_{6}^{1}$ values are 0.435 and 9.35. $H_{1}$ is not rejected and $H_{2}$ is rejected at a significance level of 0.025.\nThe vectors\n\n$$\n\\lambda_{1}^{\\prime}=(0,0,1,0,0,-1),\\qquad\\lambda_{2}^{\\prime}=(0,0,0,1,-1,0),\n$$\n\nare each of the form\n\n$$\n\\lambda^{\\prime}=\\delta^{\\prime}P_{W}\\quad\\mathrm{and}\\quad\\lambda^{\\prime}=\\delta^{\\prime}(X Q_{R})^{+}X.\n$$\n\nThus the estimates of $Q_{1}-Q_{4}$ and $Q_{2}-Q_{3}$ vary with the sample data and are estimated unbiasedly even if the restriction is false.\nThis section considers the problem of estimating $\\lambda^{\\prime}\\beta$ from data obtained according to the linear model\n\n$$\ny=X\\beta+e,\n$$\n\nsubject to the known linear restrictions\n\n$$\nR\\beta=r,\\qquad S\\beta\\leq s,\n$$\n\nwhere '$\\leq$' is to be read as 'each component of the vector on the left-hand side is less than or equal to the corresponding component on the right-hand side'. No rank conditions are imposed on $X,R$, or $S$; however each row of $S$ must be of the form $\\delta^{\\prime}X+p^{\\prime}R$.\n\nThis section presents a linear model with quarterly measurements, incorporating equality and inequality constraints for parameter estimation and hypothesis testing."
  },
  {
    "qid": "econ-empirical-948-2-0-1",
    "question": "2) Explain the planner's objective function $W(\\mu)$ and interpret the terms associated with endowment $\\omega$.",
    "gold_answer": "The planner's objective is: \\n$$ W(\\mu)=\\operatorname*{max}_{\\gamma}\\sum_{\\omega}\\left\\{-\\big[\\pi(\\omega)-\\mu(\\omega)\\big]T[\\omega]-\\mu(\\omega)T\\big[g(\\omega)\\big]\\right\\}, $$ \\nsubject to constraints (1), (2), (3), (6), and conditional on entry decisions $\\mu$. \\n- The term $-\\big[\\pi(\\omega)-\\mu(\\omega)\\big]T[\\omega]$ represents the cost of risk bearing for traders not in the OTC market (measure $\\pi(\\omega)-\\mu(\\omega)$) who keep their exposure $\\omega$. \\n- The term $-\\mu(\\omega)T\\big[g(\\omega)\\big]$ represents the cost of risk bearing for traders in the OTC market (measure $\\mu(\\omega)$) who change their exposures to $g(\\omega)$.",
    "question_context": "Conditional on the distribution of traders, $\\boldsymbol{n}=\\{\\boldsymbol{n}(\\omega)\\}_{\\omega\\in\\Omega}$ , generated by banks' entry decisions $\\mu=\\{\\mu(\\omega)\\}_{\\omega\\in\\Omega}$ , an OTC market equilibrium is made up of CDS contracts, $\\gamma=\\{\\gamma(\\omega,\\tilde{\\omega})\\}_{(\\omega,\\tilde{\\omega})\\in\\Omega^{2}}$ post-trade exposures, $g=\\{g(\\omega)\\}_{\\omega\\in\\Omega}$ and CDS prices, $R=\\{R(\\omega,\\tilde{\\omega})\\}_{(\\omega,\\tilde{\\omega})\\in\\Omega^{2}}$ , such that (i) CDS contracts are bilaterally feasible: $\\gamma$ satisfies (2) and (3); (i CDS contracts are optimal: $\\gamma$ and $R$ satisfy (8) and (9) given $_{g}$ (ii) post-trade exposures are generated by CDS contracts: $_{g}$ satisfies (6) given $\\gamma$.\n\nThis section defines the OTC market equilibrium conditional on the distribution of traders generated by banks' entry decisions. It involves CDS contracts, post-trade exposures, and CDS prices that satisfy bilateral feasibility, optimality, and exposure generation conditions."
  },
  {
    "qid": "econ-empirical-1116-3-0-3",
    "question": "4) Formally show why interactions between parental age at arrival and NON-ENG may be invalid instruments if parental years in the U.S. confer nonlanguage-related returns. How does the study mitigate this?",
    "gold_answer": "Let \\( Y \\) be child outcomes, \\( A \\) parental age at arrival, and \\( D \\) years in the U.S. The instrument \\( Z = A \\times \\text{NON-ENG} \\) is invalid if:\n\n\\[ Y = \\beta_1 A + \\beta_2 D + \\epsilon \\]\n\nwhere \\( D \\) correlates with \\( A \\) (earlier arrivers have more U.S. years). The study mitigates this by:\n1. **Sample Restriction**: Panel K excludes parents with <10 U.S. years pre-birth (reducing \\( \\beta_2 \\) variation).\n2. **Long Exposure**: Most parents have 28+ U.S. years (minimizing marginal returns to additional years).",
    "question_context": "One concern is that many low-educated young men migrate on their own to the United States from Mexico and Central America to look for work. Among the older arrivers from non-English-speaking countries, there may be a disproportionate number of low-educated immigrants who never intended (or were never able) to attend school in the United States, and, moreover, who differ along other dimensions as well since they chose to migrate on their own.\nA second concern is that English-speaking countries and non-English-speaking countries may differ in ways that affect the assimilation process in the United States of emigrants. On the one hand, it is possible that immigrants from non-English-speaking countries exhibit a stronger age-at-arrival effect simply because immigrants from poorer countries face additional barriers to adaptation and that these barriers increase in severity as a function of age at arrival.\nOn the other hand, English-speaking countries might have greater cultural and institutional similarity to the United States, making adjustment easy for immigrants from these countries irrespective of age at arrival. In contrast, immigrants from non-English-speaking countries encounter both a foreign language and foreign culture, so even ignoring the language there is more to adjust to for the older arrivers.\nA third concern is that the years the parent spent in the United States prior to having the child may affect outcomes in a way that varies by the English-speaking status of the parental country of birth. Because on average people who arrive at an earlier age have lived in the United States longer, then to the extent that a year of U.S. experience confers differential nonlanguage-related returns to immigrants from non-English-speaking countries, it is invalid to use interactions between parental age at arrival and NON-ENG as instruments.\n\nThis section addresses concerns about the comparability of control and treatment countries in the study of child English-speaking ability and educational outcomes, focusing on potential biases from migration patterns, country characteristics, and parental years in the U.S."
  },
  {
    "qid": "econ-empirical-757-1-0-0",
    "question": "1) Under Assumption B*.1, what are the key properties of the locally stationary process $\\{z_{nt}\\}$ and its associated process $\\{z_t(v)\\}$?",
    "gold_answer": "The key properties are:\n- $\\{z_{nt}\\}$ is locally stationary with an associated stationary process $\\{z_t(v)\\}$.\n- All $z_{nt}$ have the same compact support $V_z = [a_{\\text{min}}, a_{\\text{max}}]$.\n- The density $f(v,z)$ of $z_t(v)$ is smooth in $v$.",
    "question_context": "$\\mathrm{B}^{*}.1\\left\\{z_{n t}\right\\}$ is locally stationary with associated process $\\{z_{t}(v)\\}$ , and all $z_{n t}$ ( $1\\leq t\\leq n,$ have the same compact support $V_{z}=[a_{\\mathrm{min}},a_{\\mathrm{max}}].$ . Moreover, the density $f(v,z)$ of $z_{t}(v)$ is smooth in $v$ .\n$\\mathtt{B}^{\\ast}.2$ For all t and any $v\\in[0,1]$ , either (a) $z_{t}(v)$ satisfies Assumption B.1.a, or $(\\ensuremath{\\mathbf{b}})z_{t}(\\ensuremath{\boldsymbol{v}})$ satisfies Assumption B.1.b.\n$\\mathtt{B}^{*}.3$ There exists an orthogonal function sequence $\\{p_{i}(z),i\\ge0\\}$ on the support $[{a_{\\mathrm{min}}},{a_{\\mathrm{max}}}]$ with respect to $d F(z)$ such that $\begin{array}{r}{\\mathsf{S u p}_{v\\in[0,1]}\\mathsf{S u p}_{j\\ge0}\\mathbb{E}|p_{j}(z_{1}(v))|<\\infty.}\\end{array}$ .\n$\\mathtt{B}^{*}.4$ Suppose that there is a filtration sequence ${\\mathcal F}_{n t}$ such that $(e_{t},\\mathcal{F}_{n,t})$ form a martingale difference sequence and $(z_{t}(t/n),x_{t})$ is adapted with $\\mathcal{F}_{n,t-1}$ . Meanwhile, $\\mathbb{E}(e_{t}^{2}|\\mathcal{F}_{n,t-1})=\\sigma^{2}(t/n)$ almost surely with continuous and nonzero function $\\sigma(\\cdot)$ and for some $q_{3}\\geq4,\\operatorname*{max}_{1\\leq t\\leq n}\\mathbb{E}(|e_{t}|^{q_{3}}|\\mathcal{F}_{n,t-1})<\\infty$ .\n\nThe text discusses Assumption B* which pertains to the properties of locally stationary processes and their associated stationary processes. It outlines conditions under which theoretical results for these processes are applicable, including compact support, smoothness of density, and mixing properties."
  },
  {
    "qid": "econ-empirical-91-3-1-0",
    "question": "5) Explain the instrumental variables (IV) strategy used to estimate the supply elasticity in Table VII. Why are the current and lagged tax terms valid instruments for price?",
    "gold_answer": "1. The IV strategy uses current and lagged tax terms as instruments for the log real price ($\\ln P$).\n2. Validity requires:\n   - **Relevance**: Tax terms must correlate with prices (supported by the reduced-form results).\n   - **Exogeneity**: Tax terms must not directly affect supply except through prices (plausible if tax changes are exogenous policy shocks).\n3. The first-stage regression is: \n   \\[ \\ln P = \\gamma_0 + \\gamma_1 \\text{TAX}_t + \\gamma_2 \\text{TAX}_{t-1} + \\text{controls} + \\epsilon \\]\n4. The second-stage regression is: \n   \\[ \\ln Q = \\beta \\widehat{\\ln P} + \\text{controls} + u \\]\n5. The estimated $\\beta$ is the supply elasticity.",
    "question_context": "The supply equation estimated is the log of real shipments of the capital good regressed on the log real price, asset fixed effects, asset-specific time trends, the price controls variable, sometimes the real wage, and sometimes year dummies. Prices are the only endogenous variable, and the demand instruments used to identify prices are the current and once lagged tax terms.\nIn the most basic specification of column (1), the elasticity is estimated to be 1.14 with a confidence interval from, essentially, zero to two. In (2), which also includes the log of the average real annual earnings in the industry (from the NBER productivity data), the elasticity is around three-fourths and is not significantly different from one.\nBecause there could be a variety of concurrent macroeconomic factors at work, columns (3) and (4) repeat the regressions above but include year dummies. These equations are identified off of the variation in the tax term between assets within years.\n\nThe reduced-form evidence that prices rise in response to subsidies can be thought of as the first stage of a two-stage least squares estimate where investment subsidies serve as an instrument for investment demand."
  },
  {
    "qid": "econ-empirical-1279-2-0-0",
    "question": "1) Derive the condition under which firm 1's optimal profit is strictly increasing in $x_{2}$ when $x_{1} \\leqslant x_{2}$ and strictly decreasing when $x_{1} \\geqslant x_{2}$. Use the envelope theorem to support your derivation.",
    "gold_answer": "1. **Envelope Theorem Application**: Under Assumption 1, the optimal profit $\\Pi_1(x_1, x_2)$ is differentiable. \\n2. **Profit Increase**: For $x_1 \\leqslant x_2$, the derivative $\\frac{\\partial \\Pi_1}{\\partial x_2} > 0$ by the envelope theorem. \\n3. **Profit Decrease**: For $x_1 \\geqslant x_2$, the derivative $\\frac{\\partial \\Pi_1}{\\partial x_2} < 0$ by the analogous argument. \\n4. **Unique Point**: There exists a unique $x^{*}$ where the profit trend reverses, ensuring no firm wishes to jump over its rival.",
    "question_context": "For any location $x_{2}$ , if firm 1 chooses its best reply $x_{1}$ under the restriction $x_{1}\\leqslant x_{2}$ , the envelope theorem shows that under Assumption 1 its optimal profit is strictly increasing in $x_{2}$ . If it locates to the other side of its rival, the analogous argument shows that its optimal profit (now restricting its location to $x_{1}\\geqslant x_{2}$ ) is strictly decreasing in $x_{2}$ . This means that there is a unique point, $x^{*}$ , such that if the rival is located at $x_{2}>x^{*}$ , firm 1 will optimally locate to the left of firm 2, whereas if $x_{2}<x^{*}$ , firm 1 optimally locates to the right of its rival.\nThe function $H(x)$ defined in (4.1) satisfies (a) $H(x)$ is strictly pseudoconcave; (b) $\\begin{array}{r}{\\operatorname*{lim}_{x\\rightarrow a}H(x)=\\operatorname*{lim}_{x\\rightarrow b}H(x).}\\end{array}$\nUnder Assumptions 1 and 2, there exists a unique subgame-perfect equilibrium to the location-then-price game given by Proposition 1 if and only if $x_{1}^{*}\\leqslant x^{*}$ and $x_{2}^{*}\\geqslant x^{*}$ .\nFor symmetric densities the solution to (2.8) is given by the median, which, without loss of generality, we may assume to be located at $M=0$ . From Proposition 1 it is immediate that the equilibrium outcome corresponding to this solution satisfies the straddle condition.\n\nThis section discusses the conditions under which a unique subgame-perfect equilibrium exists in the location-then-price game, focusing on the straddle condition and the properties of the function H(x)."
  },
  {
    "qid": "econ-empirical-912-1-0-3",
    "question": "4) Explain why the entrywise 1-norm $||\\boldsymbol{\\Sigma}_{t}-\\boldsymbol{H}_{t}||_{1}$ and the proportional Frobenius distance $\\bar{T r}(\\Sigma_{t}H_{t}^{-1}-I)^{2}$ are examples of inconsistent loss functions under Proposition 1(i).",
    "gold_answer": "1. The entrywise 1-norm $||\\boldsymbol{\\Sigma}_{t}-\\boldsymbol{H}_{t}||_{1}$ measures the absolute difference between $\\Sigma_{t}$ and $H_{t}$. \n2. The proportional Frobenius distance $\\bar{T r}(\\Sigma_{t}H_{t}^{-1}-I)^{2}$ measures the squared trace of the difference between $\\Sigma_{t}H_{t}^{-1}$ and the identity matrix. \n3. Both loss functions violate condition (i) because their third derivatives with respect to $\\sigma_{t}$ and $h_{k,t}$ are non-zero for some $k$. \n4. This means the bias term in the Taylor expansion depends on $H_{t}$, making the ordering sensitive to proxy noise. \n5. Thus, they are classified as inconsistent loss functions under Proposition 1(i).",
    "question_context": "if $\\begin{array}{r}{\\frac{\\partial^{3}L(\\Sigma_{t},H_{t})}{\\partial\\sigma_{t}\\partial\\sigma_{t}^{\\prime}\\partial h_{k,t}}=0\\forall k,}\\end{array}$ , then $\\boldsymbol{H}_{t}^{*(s)}=\\boldsymbol{\\Sigma}_{t}\\forall s.$ . Under Assumption A2.1–A2.3 (ii) if $\\begin{array}{r}{\\frac{\\partial^{3}L(\\Sigma_{t},H_{t})}{\\partial\\sigma_{t}\\partial\\sigma_{t}^{\\prime}\\partial h_{k,t}}\\neq0}\\end{array}$ for some $k,$ then $H_{t}^{*(s)}\\xrightarrow{p}E_{t}$ as $s\\to\\infty$ .\nThe proof is given in the Appendix. Point (i) recovers Hansen and Lunde’s 2006a result and identifies loss functions ensuring consistency of the ordering regardless of the quality of the proxy. It is worth noting that, although in order to ensure (2) we require the volatility proxy to be conditionally unbiased, this is not the case for the approximated loss, $L(\\hat{\\Sigma}_{t},H_{t})$ .\nWe can illustrate this point by considering the second order Taylor expansion of $L(\\hat{\\Sigma}_{t},H_{t})$ around the true value $\\Sigma_{t}$ : \n$$\n\\begin{array}{l}{{\\displaystyle{\\cal L}(\\hat{\\cal{\\cal{\\cal{X}}}}_{t}^{(s)},{\\cal H}_{t})\\cong{\\cal L}(\\varSigma_{t},{\\cal H}_{t})+\\left(\\frac{\\partial{\\cal L}({\\cal{\\cal{S}}}_{t},{\\cal H}_{t})}{\\partial\\sigma_{t}}\\right)^{\\prime}(\\hat{\\sigma}_{t}^{(s)}-\\sigma_{t})}}\\ {{\\displaystyle~+\\frac{1}{2}\\left[(\\hat{\\sigma}_{t}^{(s)}-\\sigma_{t})^{\\prime}\\frac{\\partial^{2}{\\cal L}({\\cal{\\cal{X}}}_{t},{\\cal H}_{t})}{\\partial\\sigma_{t}\\partial\\sigma_{t}^{\\prime}}(\\hat{\\sigma}_{t}^{(s)}-\\sigma_{t})\\right].}}\\end{array}\n$$\nTaking conditional expectations with respect to $\\Im_{t-1}$ and recalling A2.1 and A2.3 we get \n$$\n\\begin{array}{l}{{\\displaystyle\\mathrm{E}_{t-1}[L(\\hat{\\Sigma}_{t}^{(s)},H_{t})]=L(\\Sigma_{t},H_{t})}}\\ {{\\displaystyle~+\\frac{1}{2}\\left[\\mathrm{E}_{t-1}\\left(\\xi_{t}^{(s)^{\\prime}}\\frac{\\partial^{2}L(\\Sigma_{t},H_{t})}{\\partial\\sigma_{t}\\partial\\sigma_{t}^{'}}\\xi_{t}^{(s)}\\right)\\right],}}\\end{array}\n$$\nwhich under (i) holds with equality because the last term depends on the accuracy of the volatility proxy but not on $H_{t}$ . Hence, by the law of iterated expectations, for all $s$ and any two models $H_{l,t}$ , $H_{m,t}\\in\\dot{H}$ \n$$\n\\begin{array}{r l}&{\\mathrm{E}[L(\\hat{\\Sigma}_{t}^{(s)},H_{l,t})]-\\mathrm{E}[L(\\hat{\\Sigma}_{t}^{(s)},H_{m,t})]}\\ &{\\quad=\\mathrm{E}[L(\\Sigma_{t},H_{l,t})]-\\mathrm{E}[L(\\Sigma_{t},H_{m,t})].}\\end{array}\n$$\nThus, conditional unbiasedness of $L(\\hat{\\cal E}_{t}^{(s)},{\\cal H}_{l,t})$ is not required. However, it suffices that the bias is constant across models to ensure an ordering over $\\dot{H}$ asymptotically invariant to the noise in the proxy. Point (ii) of Proposition 1, shows that, if $L(\\cdot,\\cdot)$ is such that the bias term in (4) depends on $H_{t}$ , the distortion introduced in the ordering disappears. Furthermore, consistency of the ordering is recovered as long as the quality of the proxy improves, i.e., $s\\rightarrow$ $\\infty$ .\nThis result becomes particularly relevant when ordering over a discrete set of models. Indeed, when the variance of the proxy is small with respect to the loss differential between any pair of models, the distortion induced by the variability of the proxy becomes negligible, leaving the ordering unaffected. This is important since commonly used loss functions although having other desirable properties, e.g., down-weighting extreme forecast errors or penalizing positive errors more than negative errors, do not satisfy (i). This is the case, for instance, for loss functions based on proportional forecast error or functional transformations of forecasts and realizations. Examples are: the entrywise 1-norm, $||\\boldsymbol{\\Sigma}_{t}-\\boldsymbol{H}_{t}||_{1}$ , the proportional Frobenius distance, $\\bar{T r}(\\Sigma_{t}H_{t}^{-1}-I)^{2}$ and log-Frobenius distances, $\\left(\\log\\left|\\boldsymbol{\\Sigma_{t}}\\boldsymbol{H}_{t}^{-1}\\right|\\right)^{2}$ or $\\begin{array}{r}{\\left(\\log\\frac{T r[\\mathcal{D}_{t}\\mathcal{D}_{t}]}{T r[H_{t}H_{t}]}\\right)^{2}}\\end{array}$ . We denote a loss function that satisfies (violates) (i) as ‘‘consistent’’ (‘‘inconsistent’’).\n\nThis section discusses the conditions under which the loss function ensures consistency of the ordering of volatility forecasts, regardless of the quality of the proxy. It also explores the implications of using inconsistent loss functions and the conditions under which consistency is recovered."
  },
  {
    "qid": "econ-empirical-349-0-0-2",
    "question": "3) Analyze the special case where $\\phi$ is the identity function. How does the smooth ambiguity criterion simplify?",
    "gold_answer": "1. **Identity Function**: $\\phi(x) = x$ implies $\\phi^{-1}(x) = x$.\n2. **Simplification**: The criterion reduces to: $$ V_{\\text{Id},r}(a,\\mu) = \\int_{\\Sigma} \\int_{S} r(a, s) \\sigma(\\mathrm{d}s) \\mu(\\mathrm{d}\\sigma). $$\n3. **Interpretation**: This is the standard subjective expected utility, where the DM integrates over both states and models linearly.",
    "question_context": "The smooth ambiguity criterion is given by: $$ V_{\\phi,r}(a,\\mu)=\\phi^{-1}\\bigg(\\int_{\\Sigma}\\phi\\bigg(\\int_{S}r(a,s)\\sigma(\\mathrm{d}s)\\bigg)\\mu(\\mathrm{d}\\sigma)\\bigg), $$ where $\\phi$ captures ambiguity attitudes and $r=u\\circ g$ captures risk attitudes.\nThe set of justifiable actions is defined as: $$ \\mathcal{J}_{\\phi,r}(\\Sigma)=\\big\\{a\\in A:\\exists\\mu\\in A(\\Sigma),\\forall a^{\\prime}\\in A,V_{\\phi,r}(a,\\mu)\\geq V_{\\phi,r}\\big(a^{\\prime},\\mu\\big)\\big\\}. $$\n\nThe paper examines a decision maker who ranks actions according to the smooth ambiguity criterion, which incorporates both ambiguity and risk attitudes. The set of justifiable actions is defined as those actions that are best replies to some belief over probabilistic models."
  },
  {
    "qid": "econ-empirical-620-2-0-4",
    "question": "5) Derive the asymptotic validity of the CSD bootstrap under Theorem 3.1, assuming $\\sqrt{T}/N \\to c$ and $\\rho\\left(\\tilde{\\Sigma} - \\Sigma\\right) \\rightarrow^{P} 0$.",
    "gold_answer": "1. **Bootstrap consistency**: $T^{*} \\xrightarrow{P} \\mathcal{Q}\\mathcal{I}\\mathcal{Q}^{\\prime}$ under spectral norm convergence. \\n2. **High-level conditions**: Assumptions 1–5 and CS ensure Conditions A*–E* in Gonçalves and Perron (2014) hold. \\n3. **Result**: $\\sup_{x \\in \\mathbb{R}^{p}} \\left| P^{*}\\left(\\sqrt{T}(\\phi^{*\\prime}\\hat{\\delta}^{*} - \\hat{\\delta}) \\leq x\\right) - P\\left(\\sqrt{T}(\\hat{\\delta} - \\delta) \\leq x\\right) \\right| \\rightarrow^{P} 0$.",
    "question_context": "Assumption CS. $\\varSigma\\equiv E\\left(e_{t}e_{t}^{\\prime}\\right)=\\left(\\sigma_{i j}\\right)_{i,j=1,\\dots,N}$ for all $t,i,j$ and is such that $\\lambda_{\\operatorname*{min}}\\left(\\varSigma\\right)>c_{1}$ and $\\lambda_{\\operatorname*{max}}\\left(\\Sigma\\right)<c_{2}$ for some positive constants $c_{1}$ and $c_{2}$.\nThe cross-sectional dependence robust bootstrap algorithm involves generating bootstrap samples $e_{t}^{*}=\\tilde{\\Sigma}^{1/2}\\eta_{t}$, where $\\eta_{t}$ is i.i.d. $(0,I_{N})$, and estimating bootstrap factors and loadings.\nTheorem 3.1 states that under Assumptions 1–5 and CS, if $\\rho\\left(\\tilde{\\Sigma}-{\\cal{E}}\\right)\\rightarrow^{P}0$, then the CSD bootstrap is asymptotically valid.\nThresholding is used to regularize $\\tilde{\\Sigma}$, with $\\tilde{\\sigma}_{i j}=\\hat{\\sigma}_{i j}1\\left(\\left|\\hat{\\sigma}_{i j}\\right|\\geq\\omega\\right)$ for $i\\neq j$, where $\\omega_{N T}=C\\left(\\frac{1}{\\sqrt{N}}+\\sqrt{\\frac{\\log N}{T}}\\right)$.\nTheorem 3.2 extends Theorem 3.1 under additional sparsity conditions, requiring $m_{N}=o\\left(\\operatorname*{min}\\left(\\sqrt{N},\\sqrt{\\frac{T}{\\log N}}\\right)\\right)$.\n\nThis section introduces a bootstrap method for factor models that remains consistent under cross-sectional dependence, building on Bai and Ng (2006). Key assumptions include bounded eigenvalues for the covariance matrix of idiosyncratic errors, ensuring weak cross-sectional dependence."
  },
  {
    "qid": "econ-empirical-470-3-0-1",
    "question": "2) Prove Proposition 5.1: Show that the MPH model hazard function $h\\left(Y,X,\\xi\\right)=\\lambda\\left(Y\\right)\\cdot\\theta\\left(X\\right)\\cdot\\xi$ implies the transformation model $Y=G\\left[H_{1}\\left(X\\right)+U\\right]$ with $U = \\ln\\left[\\frac{-\\ln(1-\\varepsilon)}{\\xi}\\right]$, where $\\varepsilon\\perp(X,\\xi)$.",
    "gold_answer": "1. From the MPH hazard, derive the survival function as in Q1 steps 1-4.\n2. The probability integral transform gives $\\varepsilon = 1 - S(Y|X,\\xi) \\sim U[0,1]$ independent of $(X,\\xi)$.\n3. Invert the survival function to express $Y$ as: $$Y = \\Lambda^{-1}\\left(\\frac{-\\ln(1-\\varepsilon)}{\\theta(X)\\xi}\\right).$$\n4. Take logs to separate terms: $$\\ln\\Lambda(Y) = -\\ln\\theta(X) + \\ln\\left(\\frac{-\\ln(1-\\varepsilon)}{\\xi}\\right).$$\n5. Define $G(\\cdot) = \\Lambda^{-1}(\\exp(\\cdot))$, $H_1(X) = -\\ln\\theta(X)$, and $U$ as specified.\n6. Then $Y = G(H_1(X) + U)$ with $U$ independent of $X$ given $\\xi$.",
    "question_context": "Let Y be the duration of a certain state (a nonnegative random variable) such as duration of a strike. Our test is directly applicable to nonlinear GAFT models, since such models can be written in the form $Y=G\\left[H_{1}\\left(X\\right)+U\\right].$ , where $X$ is a vector of covariates, and $U$ an unobservable random variable (see, e.g., Eq. (2.5) in Ridder, 1990).\nMPH models are a particularly popular class of GAFT models. Below we provide a direct link between our null hypothesis and MPH models. Let h $(Y,X,\\xi)$ denote the hazard function for Y . An MPH model of survival time Y is one where $h\\left(\\boldsymbol{Y},\\boldsymbol{X},\\boldsymbol{\\xi}\\right)=\\lambda\\left(\\boldsymbol{Y}\\right)\\cdot\\boldsymbol{\\theta}\\left(\\boldsymbol{X}\\right)\\cdot\\boldsymbol{\\xi}$ holds for some baseline hazard function $\\lambda$ (Y ) and some nonnegative function of covariates $\\theta$ (X).\n\nThis section discusses testing whether duration data obey the class of nonlinear generalized accelerated failure-time (GAFT) models. The test is applied empirically on a dataset of strike durations among manufacturing workers in the US."
  },
  {
    "qid": "econ-empirical-1374-3-0-2",
    "question": "3) Explain why the 2SLS estimates for demanding forms of political participation (e.g., attending demonstrations) are less precise and often insignificant, despite large effects for basic participation (e.g., voting).",
    "gold_answer": "1. **Cost of participation**: Demonstrations require higher effort and risk than voting.  \n2. **Sample size**: Only 15% of the sample engages in demonstrations, reducing statistical power.  \n3. **Education threshold**: Basic education may suffice for voting but not for complex activities like organizing protests.  \n4. **Measurement error**: Rare activities are harder to measure precisely.",
    "question_context": "The IV point estimate in column 1 reinforces the reduced-form effect by showing that a unit increase in education increases a respondent’s propensity to frequently discuss politics by 16 percentage points.\nColumn 3 shows that an additional level of education increases the ability of respondents to name national and local government officials by almost an entire standard deviation.\nColumn 4 of panel C shows that an additional level of education increases the likelihood that an individual is registered to vote by 30 percentage points.\nPanel C shows that a unit increase in education increases the probability that an individual attends community meetings or is an active association member by 21 and 19 percentage points, respectively.\nThese calculations indicate that UPE has increased voter turnout by 3.3 percentage points among eligible voters, the proportion of citizens frequently discussing politics by 2.9 percentage points, and community meeting attendance by 3.8 percentage points.\n\nThe study examines the impact of Universal Primary Education (UPE) and education levels on civic and political engagement in Nigeria, using reduced-form and instrumental variable (IV) estimates to isolate causal effects."
  },
  {
    "qid": "econ-empirical-1305-3-1-0",
    "question": "5) Formally define the mid-table effect and test its presence using the data from $\\mathbf{L}_{sym}$ and $\\mathbf{L}_{asym}$ in Table 2, including the appropriate statistical test and interpretation.",
    "gold_answer": "1. **Mid-table effect**: The tendency for WTP to be lower when the lottery outcomes are asymmetrically distributed (e.g., mid-table outcomes are less salient).\n2. **Data**:\n   - $\\mathbf{L}_{sym}$: Mean WTP = 88.22, std. dev. = 34.00, n = 7.\n   - $\\mathbf{L}_{asym}$: Mean WTP = 53.57, std. dev. = 24.37, n = 3.\n3. **Test**: Two-sample t-test (assuming unequal variances).\n   - $t = \\frac{88.22 - 53.57}{\\sqrt{\\frac{34.00^2}{7} + \\frac{24.37^2}{3}}} = 1.92$ (p-value > 0.10).\n4. **Interpretation**: The difference is not statistically significant, providing weak evidence for the mid-table effect.",
    "question_context": "$\\mathbf{L}_{asym}$ has lower WTP valuations compared to $\\mathbf{L}_{sym}$, which points in the direction of the mid-table effect. However, the difference is small and far from significant, as shown in Row 3 of Table 3.\nThe pooled lottery-baseline treatment effect is clearly in the direction of the IA, as confirmed in Row 5 of Table 3. Alternatively, one can make stricter treatment comparisons that favor IA violation, for example by excluding $\\mathbf{B}_{sym-narrow}$ (see Row 6 of Table 3), or by contrasting only $\\mathbf{L}_{asym}$ with $\\mathbf{B}_{sym-wide}$ (see Row 7 of Table 3).\n\nThis section examines the mid-table effect and the lottery-baseline treatment effect in real-stakes settings, providing stronger evidence for the independence axiom (IA)."
  },
  {
    "qid": "econ-empirical-909-2-0-2",
    "question": "3) Formulate the test statistic for the hypothesis that the moving-average length is at most $m$ (against the alternative of length between $m$ and $R$), and derive its asymptotic distribution under the null.",
    "gold_answer": "1. **Test Statistic**: For the hypothesis $\bar{\\alpha}_{2} = 0$ (moving-average length $\\leq m$), the test statistic is:\n   $$ T\\hat{\\alpha}_{2}^{\\prime}\\hat{D}_{22}^{-1}\\hat{\\alpha}_{2}, $$\n   where $\\hat{D}_{22}$ is the submatrix of $\\hat{D}$ corresponding to $\\hat{\\alpha}_{2}$.\n2. **Asymptotic Distribution**: Under the null, the statistic converges to a chi-squared distribution:\n   $$ T\\hat{\\alpha}_{2}^{\\prime}\\hat{D}_{22}^{-1}\\hat{\\alpha}_{2} \\stackrel{A}{\\sim} \\chi^{2}(R - m). $$",
    "question_context": "The tests we now investigate concern the values of $\bar{g}=\\mathsf{p l i m}\\hat{g}$. In this investigation, however, we shall consider $\\hat{g}$ defined as using $\\hat{\\beta}$ to calculate the estimated residuals rather than ${\\hat{\\beta}}^{I}$. All developments for the one apply to the other with suitable change of components arising from the estimating formula for ${\\hat{\\beta}}$.\nThe developments leading up to (3.16) and (3.17) provide the basis for the procedures considered. Let $\\pi_{t}=\\bar{q}^{\\prime}(\\vec{M}^{\\prime}\\vec{Q}_{,A}^{-1}\\vec{M})^{-1}\\bar{M}^{\\prime}\\bar{Q}_{,A}^{\\phantom{\\dagger}}z_{{\\cal H}\\bar{t}}^{\\prime}$. Then, as (3.17) indicates, we are interested in the asymptotic distribution of $\\sum_{t=1}^{T}(\\tilde{g_{t}}-g_{t} + \\pi_{t}\\eta_{t})/T$. The form of $\tilde{g_{t}}$ and its m-dependence mean that this expression is itself the average of $m$-dependent variables.\nThe major difficulties arise in evaluating the first term of (4.2). To do so usually requires some knowledge or hypothesis about the values of the $\\alpha_{t k}$. However, as we see in the next section, some exceptions may occur for special forms.\nThe statistic in (4.16) provides a test for a moving-average of length at most $m$ against the alternative that the length is between $m$ and $R$. When $m=0$, it is a test for whether there is any autocovariance at all.\n\nThe analysis focuses on estimating and testing hypotheses about the moving-average parameters and heteroscedasticity in the model. The tests concern the values of $\bar{g} = \\mathsf{plim}\\hat{g}$, where $\\hat{g}$ is defined using $\\hat{\\beta}$ to calculate residuals. The asymptotic distribution of $T^{\\frac{1}{2}}(\\hat{g} - \\vec{g})$ is derived under the extended Liapounov Central Limit Theorem."
  },
  {
    "qid": "econ-empirical-1729-4-0-0",
    "question": "1) Derive the expression for the day wage $\\hat{W}$ as a function of changes in factor endowments and rental rates, starting from equations $(3)^{\\prime}$, (10), (16), and (17).",
    "gold_answer": "1. Start with the given equations: \n   - $(3)^{\\prime}$: Defines labor allocation. \n   - (10): Relates wage and rental rates. \n   - (16) and (17): Define utilization and factor proportions. \n2. Combine these to express $\\hat{W}$ as: \n   $$ \n   \\hat{W}=(1/S)[\\lambda_{1L}\\hat{K}_{1}^{*}+\\lambda_{2L}\\hat{K}_{2}^{*}-\\hat{L}]+(\\lambda_{1L}\\tilde{\\sigma}_{1}/S)\\hat{r}_{1}+(\\lambda_{2L}\\sigma_{2}/S)\\hat{r}_{2} \n   $$ \n3. Here, $S$ is a scaling factor, and $\\tilde{\\sigma}_{1}$, $\\sigma_{2}$ are elasticity parameters.",
    "question_context": "Equations $(3)^{\\prime}$ , (10), (16), and (17) are used to express the day wage as a function of the changes in factor endowments and the rental rates to the specific factors. That is, \n\n$$ \n\\hat{W}=(1/S)[\\lambda_{1L}\\hat{K}_{1}^{*}+\\lambda_{2L}\\hat{K}_{2}^{*}-\\hat{L}]+(\\lambda_{1L}\\tilde{\\sigma}_{1}/S)\\hat{r}_{1}+(\\lambda_{2L}\\sigma_{2}/S)\\hat{r}_{2} \n$$\nSubstitution of (B9) into (14)' and (15)' allows us to express changes in the rental rates as functions of changes in endowments. Straightforward but tedious algebra results in \n\n$$ \n\\begin{array}{r}{\\hat{r}_{1}=(1/\\Delta)(\\theta_{1L}/\\theta_{1K})[\\hat{L}-\\lambda_{1L}\\hat{K}_{1}^{*}-\\lambda_{2L}\\hat{K}_{2}^{*}]}\\\\ {\\hat{r}_{2}=(1/\\Delta)(\\theta_{2L}/\\theta_{2K})[\\hat{L}-\\lambda_{1L}\\hat{K}_{1}^{*}-\\lambda_{2L}\\hat{K}_{2}^{*}]}\\end{array} \n$$\nSubstitution of (B10) and (B11) into (B9) results in \n\n$$ \n\\hat{W}=-(1/\\Delta)[\\hat{L}-\\lambda_{1L}\\hat{K}_{1}^{*}-\\lambda_{2L}\\hat{K}_{2}^{*}] \n$$\n\nThis section analyzes how changes in factor endowments affect wage rates, rental rates, and sectoral outputs, incorporating variable utilization rates and specific factor models."
  },
  {
    "qid": "econ-empirical-1044-3-1-0",
    "question": "3) Interpret the economic significance of the CAAR results for non-EC competitors when the bidder is also from outside the EC, and discuss the implications for regulatory bias.",
    "gold_answer": "1. The negative CAAR (-1.19%) suggests that non-EC competitors lose value when the bidder is also non-EC. 2. This could indicate that European regulators are less concerned about market power accumulation outside the EC. 3. The strong significance of the difference suggests potential regulatory bias favoring EC bidders. 4. The results align with suspicions of protectionism in European regulatory decisions.",
    "question_context": "When the bidder is from the EC, outright approval is granted even though the impact on external competitors is positive, \\( \\nabla_{\\mathrm{CAAR}}=0.66\\% \\), p-value \\( =0.16 \\)), which indicates either increased market power from the combination or a greater probability of further acquisitions.\nFor competitors outside the EC, the sign of the CAAR is reversed when the bidder’s home country is also outside the European Community \\( (\\mathrm{CAAR}=-1.19\\% \\), p-value \\( =0.13 \\)). The difference is strongly significant.\n\nThe text examines the initial announcement effects on competitors' abnormal returns, controlling for their home country and the home country of the bidder. It explores potential biases in European Commission regulatory decisions."
  },
  {
    "qid": "econ-empirical-1410-1-0-2",
    "question": "3) Explain the theory of change hypotheses (H1-H4) and how they are empirically tested in the study design.",
    "gold_answer": "The theory of change posits:\n\n- **H1**: Teachers adopt the curriculum.\n- **H2**: Teachers alter pedagogy.\n- **H3**: Students acquire skills (academic, non-cognitive, entrepreneurial).\n- **H4**: Students engage in entrepreneurial activities.\n\nTesting methods:\n1. **H1/H2**: Classroom observations and teacher surveys.\n2. **H3**: Exam scores and skill assessments.\n3. **H4**: Surveys on business activities and competitions.",
    "question_context": "The intervention tested in this project consisted of the following components: In-service teacher training, exchange visits, and outreach and support.\nThe study focused on the cohort entering S4 (10th grade) in 2016, with training provided to this cohort's entrepreneurship teacher as they progressed to S6 (12th grade).\n\nThe study evaluates the impact of comprehensive teacher training on the implementation of a revised entrepreneurship curriculum in Rwandan secondary schools. The intervention includes in-service teacher training, exchange visits, and outreach support, with the control group receiving only standard government training."
  },
  {
    "qid": "econ-empirical-664-0-1-1",
    "question": "6) Explain why the Sicilian Mafia and university tenure committee examples support the model’s prediction on third-party intervention. Use the budget-breaker result.",
    "gold_answer": "1. High subjectivity → Disagreement (low \\( \\rho \\)) → Surplus destruction needed.\n2. Budget breaker reallocates output: \\( \\frac{\\partial w_i}{\\partial y} \\downarrow \\), principal’s share \\( \\uparrow \\).\n3. Third party enforces truth-telling when cross-checking fails (no RPE).",
    "question_context": "Self/peer evaluations should be used in relative terms only when they are not very subjective.\nWorkers’ share of output would be lower in industries where evaluations are more subjective.\nThe introduction of a budget breaker is linked to the degree of subjectivity in evaluations.\n\nThe theoretical results are contextualized using real-world examples (e.g., Lincoln Electric, Merck) to illustrate how subjectivity affects the design of evaluation systems and the distribution of output."
  },
  {
    "qid": "econ-empirical-1202-1-0-1",
    "question": "2) Using the data from Table 1, describe the trend in the percentage of the working-age population classified as disabled from 1962 to 1984. How do the trends differ between males and females?",
    "gold_answer": "Trends:\n- **Total population**: Rose from 7% (1962) to 11% (1973), then declined to 9.5% (1984).\n- **Males**: Increased from 9.5% (1962) to ~15% (1976), then fell to 10.5% (1984).\n- **Females**: Doubled from 4.8% (1962) to 9.6% (1980), then declined to 8.6% (1984).\n\nKey difference: Female disability rates grew persistently until 1980, while male rates peaked earlier (1976).",
    "question_context": "The measure that we use to identify the disabled population rests on two criteria: the presence of self-reported work limitations and/or the meeting of official disability-determination standards, as reflected in the receipt of public disability transfer benefits.\nWe have applied these definitions as consistently as possible to the adult working-age population (ages 18-64) over the 1962 to 1984 period. The public use files of the Current Population Survey (CPS) for the years 1962, 1968, 1973, 1976, 1980, 1982, and 1984 were employed in the estimation.\nThe disabled as a percent of the total working age population rose from 7 percent in 1962 to 11 percent in 1973. After 1973, the percentage trailed off slowly, so that by 1984 9.5 percent of the population was classified as disabled.\nBecause there is no objective criterion for distinguishing who is and who is not disabled, biases in our estimates can occur if the health status standards implied by our criterion change over time.\n\nDisability is a flexible status with varying definitions. The measure used here relies on self-reported work limitations and/or receipt of public disability transfer benefits. The study applies these definitions to the adult working-age population (ages 18-64) from 1962 to 1984 using CPS data."
  },
  {
    "qid": "econ-empirical-1744-3-0-1",
    "question": "2) Derive the condition for an anonymous technology to exhibit decreasing returns to scale (DRS) and explain its equivalence to DRMS in the anonymous case.",
    "gold_answer": "1. **DRS Definition**: For anonymous technologies, DRS requires $t_{k} - t_{k-1} > t_{k+1} - t_k$ for all $k \\in \\{1, \\ldots, n-1\\}$. \\n2. **DRMS Equivalence**: In anonymous cases, DRMS reduces to comparing marginal success probabilities, which is identical to DRS. \\n3. **Implication**: This equivalence ensures that DRS technologies have $n$ distinct transitions in observable-actions cases (Corollary 4.8).",
    "question_context": "An anonymous technology success function $t$ exhibits (strict) under-proportional contribution $(U P C)$ if for every $k\\in\\{1,2,\\ldots,n-1\\}$ it holds that $$\\frac{k}{n}>\\frac{t_{k}-t_{0}}{t_{n}-t_{0}}.$$\nAn anonymous technology exhibits (strict) over-payment $(O P_{a n o n})$ if for every $k\\in\\{1,2,\\ldots, n-1\\}$, it holds that $$\\frac{Q_{k}}{Q_{n}}>\\frac{t_{k}-t_{0}}{t_{n}-t_{0}}.$$\nAn anonymous technology $(t,c)$ exhibits (strictly) increasing relative marginal payment $(I R M P_{a n o n})$ if for every $k\\in\\{1,2,\\ldots,n-1\\}$ it holds that $$\\frac{Q_{k+1}-Q_{k}}{t_{k+1}-t_{k}}>\\frac{Q_{k}-Q_{k-1}}{t_{k}-t_{k-1}}.$$\n\nThis section explores anonymous technologies, focusing on properties that determine the number of transitions in hidden-actions and observable-actions cases. Key properties include under-proportional contribution (UPC) and over-payment (OP) for single transitions, and decreasing returns to scale (DRS) and increasing relative marginal payment (IRMP) for multiple transitions."
  },
  {
    "qid": "econ-empirical-1474-3-0-4",
    "question": "5) Why does the removal of outliers in the PSID validation study often increase the error-to-true variance ratio?",
    "gold_answer": "1. Outliers identified in the interview data often did not have large measurement errors. \n2. Removing them reduced the true variance significantly but reduced the error variance by a smaller proportion. \n3. The error-to-true variance ratio is calculated as $\\sigma_u^2 / \\sigma_{X^*}^2$. \n4. Since the denominator ($\\sigma_{X^*}^2$) decreases more than the numerator ($\\sigma_u^2$), the ratio increases. \n5. This counterintuitive result highlights that outliers are not necessarily the most erroneous cases.",
    "question_context": "A more promising research design is one that provides for virtually error-free validation of the survey responses of employees, enabling the measurement of the precise amount of error in the survey responses. Such virtually error-free validation of any survey response is extremely difficult to obtain.\nThe crucial feature of the measurement error of $X$ is the ratio of the error variance to the true variance of $X,\\sigma_{u}^{2}/\\sigma_{X^{*}}^{2}$. For example, if this ratio is .5, then the true value of $\\beta_{1}$ will be $50\\%$ higher than its value estimated with the erroneous measure of $X$.\nMeasurement error in the dependent variable in this model that is uncorrelated with $\\boldsymbol{Y}_{i}^{*},\\boldsymbol{X}_{i}^{*}$, and $\\pmb{\\varepsilon}_{i}$ will increase the standard errors but will not bias the estimates of $\\beta_{0}$ or $\\beta_{1}$. Measurement error in the $X$ variable almost always produces bias in the estimate of $\\beta_{1}$.\nReports of annual earnings and unemployment for the prior calendar year also have relatively small error-to-true variance ratios (.154 and .129, respectively).\nErrors in interview reports of average hourly earnings (defined as the ratio of interview reports of annual earnings to annual hours) were enormous. The ratio of error-to-true variance was 2.8 for 1982 and 1.8 for 1981.\nTwo-thirds $(66\\%)$ of all spells of unemployment that appeared in the company records were not reported in the interview. Even long spells (lasting more than five weeks) were seriously underreported; the fraction not reported was more than one-half.\n\nThe text discusses the quality of survey data, particularly focusing on the PSID (Panel Study of Income Dynamics) data validation. It highlights the challenges in obtaining error-free validation of survey responses and the implications of measurement errors in econometric models."
  },
  {
    "qid": "econ-empirical-1620-0-0-0",
    "question": "1) Prove the identification of structural parameters in a dynamic panel data logit model using a conditional likelihood approach, highlighting the role of the sufficient statistic.",
    "gold_answer": "1. **Define the likelihood function**: The likelihood function for the dynamic panel data logit model includes both structural parameters and unobserved heterogeneity. \n2. **Identify the sufficient statistic**: The sufficient statistic is derived such that conditioning on it removes the dependence on unobserved heterogeneity. \n3. **Conditional likelihood**: The likelihood function conditional on the sufficient statistic no longer depends on the unobserved heterogeneity but still depends on the structural parameters. \n4. **Maximize the conditional likelihood**: The structural parameters are identified by maximizing this conditional likelihood function. \n\nMathematically, the conditional likelihood is given by:\n\\[ L(\\theta | S) = \\prod_{i=1}^N P(y_i | x_i, z_i, S_i; \\theta) \\]\nwhere \\( S \\) is the sufficient statistic.",
    "question_context": "We prove the identification of the structural parameters using a conditional likelihood approach. The structure of the model implies that there is a sufficient statistic such that the likelihood function conditional on this statistic no longer depends on the unobserved heterogeneity – neither through the current utility nor through the continuation value of the forward-looking decision problem – but still depends on the structural parameters.\nThe identification of true dynamics, when persistent unobserved heterogeneity is present, should deal with two key econometric issues: the incidental parameters problem, and the initial conditions problem.\nThe incidental parameters problem establishes that a simple dummy-variables estimator – that treats each individual unobservable as a parameter to be estimated jointly with the parameters of interest – is inconsistent in most nonlinear panel data models when T is fixed.\nThe initial conditions problem establishes that the joint distribution of the unobserved heterogeneity and the initial values of the observable variables is not nonparametrically identified, but the misspecification of this joint distribution can generate important biases in the estimation of the parameters of interest.\n\nThe paper studies the identification and estimation of structural parameters in dynamic panel data logit models with forward-looking decisions and nonparametric joint distribution of unobserved heterogeneity and observable state variables. The models include two endogenous state variables: the lagged decision variable and the time duration in the last choice."
  },
  {
    "qid": "econ-empirical-1482-1-0-3",
    "question": "4) The paper claims the newer technology is 'capital efficient.' Formally define capital efficiency in this context and derive its implications for the equilibrium factor allocations.",
    "gold_answer": "1. Capital efficiency means: $$\\frac{\\partial F^{N}/\\partial K^{N}}{\\partial F^{O}/\\partial K^{o}} > 1$$\n2. From Lemma 1's FOCs, this implies:\n   $$\\frac{K^{N}}{H^{N}} > \\frac{K^{o}}{H^{o}}$$\n3. The newer technology attracts more capital per unit of human capital.\n4. Equilibrium implications:\n   - As $K_{t}$ grows, $\\frac{Q^{N}}{Q^{O}}$ increases\n   - Wage premium for skills increases due to capital-skill complementarity\n   - Condition for interior solution: $$\\lim_{K^{N}/H^{N} \\to \\infty} \\frac{\\partial F^{N}}{\\partial K^{N}} < \\frac{\\partial F^{O}}{\\partial K^{o}}$$",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nWe argue in this paper that the differing experiences of the United States and Germany can be explained as the result of these countries following different trajectories along a similar marginal product surface (a wage factor-use frontier), where the relevant marginal product surface is itself the reflection of an underlying process of choice between competing technologies.\nOur main goal in presenting a competing organizational form model is to rethink the relationship between changes in factor use and changes in the wage-education relationship when the economy is undergoing a major technological or organizational change.\nIn order to formally examine the relationship between changes in factor use and changes in the wage structure when the economy has a choice between two modes of organization (or technologies), let us denote by $\\dot{F}^{o}(K^{o},\\dot{H^{o}}$ $L^{o}.$ ）the production function for the older or more traditional mode of production, and let $F^{N}(K^{N},~H^{N},~L^{N})$ represent the technology associated with the newer, competing mode of organization.\nLEMMA 1:If $K_{t},H_{t}$ and $L_{t}$ representthe aggregatequantitiesoffactorsemployedinthe economy,thenprice-takingbehavior by firms implies that the allocation of production and factors between the two competing technologies will solve the following maximization problem.\n\nThe paper examines the differing wage and employment structures in the United States and Germany, attributing these differences to the adoption of competing organizational forms and variations in factor accumulation. The model involves two production technologies, an older and a newer form, with distinct implications for wage-education profiles."
  },
  {
    "qid": "econ-empirical-968-3-0-1",
    "question": "2) Under the conditions of Theorem 4.10, prove the uniform convergence rate for $\\sup_{z \\in \\mathbb{Z}_{\\tau}} |\\hat{\\beta}(z,h_n) - \\beta(z)|$, highlighting the roles of $\\tau$, $h_n$, and the exponential terms.",
    "gold_answer": "1. Apply Taylor expansion to $\\hat{\\beta}(z,h_n) - \\beta(z)$.\n2. Use the boundedness condition $\\max_{V,\\lambda} \\sup_z |g_{V,\\lambda}(z)| < \\infty$ to control the remainder terms.\n3. The dominant terms are:\n   $$\n   O\\left(\\tau^{-4} (h_n^{-1})^{\\gamma_{1,B}} \\exp(\\alpha_B (h_n^{-1})^{\\beta_B})\\right),\n   $$\n   $$\n   O_p\\left(\\tau^{-4} n^{-1/2} (h_n^{-1})^{\\gamma_{1,L}} \\exp(\\alpha_L (h_n^{-1})^{\\beta_L})\\right).\n   $$\n4. The $\\tau^{-4}$ factor arises from the lower bounds on $f_Z(z)$ and $|\\dot{D}_z \\mu_X(z)|$ in $\\mathbb{Z}_{\\tau}$.",
    "question_context": "The estimator $\\hat{\\beta}(z,h)$ is defined as $D_{z}\\hat{\\mu}_{Y}(z,h)/D_{z}\\hat{\\mu}_{X}(z,h)$, where $D_{z}\\hat{\\mu}_{Y}(z,h)$ and $D_{z}\\hat{\\mu}_{X}(z,h)$ are constructed using estimators $\\hat{g}$ from the preceding section.\nTheorem 4.10 provides uniform convergence rates for $\\hat{\\beta}(z,h_n) - \\beta(z)$ under specific conditions, including boundedness of $g_{V,\\lambda}(z)$ and definitions involving $\\mathbf{Z}_{\\tau}$.\nTheorem 4.11 establishes asymptotic normality for $n^{1/2}\\Omega_{\\beta}^{-1/2}(z,h_n)(\\hat{\\beta}(z,h_n) - \\beta(z))$ under similar conditions, with $\\Omega_{\\beta}(z,h_n)$ defined via influence functions $\\ell_{\\beta}$.\nCorollaries 4.12 and 4.13 extend these results to weighted average estimators $\\hat{\\beta}_w$ and $\\hat{\\beta}_{w f_Z}$, providing asymptotic normality under analogous conditions.\n\nThis section applies general asymptotic results to an estimator of $\\beta(z)$, focusing on the PXI case where the exogenous instrument cannot be observed but error-laden proxies are available."
  },
  {
    "qid": "econ-empirical-563-5-1-1",
    "question": "2) Prove that the consequentialist utility model satisfies the conditions for a Bayes–Nash equilibrium under Assumptions A-1 and A-3.",
    "gold_answer": "1. From Assumption A-1, the purchase decision condition is: $$ \\theta_t + \\mathbb{E}[\\zeta_t] + q_t(\\Omega_t, \\mathbf{R}) - p \\geq 0. $$\n2. Assumption A-3 ensures the utility function $V(u_t, \\text{pur}_{t+1})$ has increasing differences and boundary conditions.\n3. The equilibrium requires: $$ R_t^e(\\Omega_t, u_t) \\in \\arg\\max_{r \\in \\mathcal{R}} V(u_t, \\text{pur}_{t+1}(r)), $$ where $\\text{pur}_{t+1}(r)$ depends on the review $r$.\n4. The existence of a threshold equilibrium follows from the continuity and monotonicity properties in Assumption A-3.",
    "question_context": "We simplify our analysis by assuming that each user cares only about the action of the next user (rather than the full sequence of subsequent actions). Let $\\Omega_{t}$ again denote the information provided from the rating system to tth customer (this can either be full history or a summary statistics) and $q_{t}$ denote the belief of customer t. Customer $t$ makes a purchase decision and a review decision. Similar to our baseline model, the purchase decision is $b_{t}\\in\\{0,1\\}$ where $b_{t}=1$ if and only if $\\theta_{t}+\\mathbb{E}[\\zeta]+q_{t}-p\\geq0.$ . A customer who has purchased the product experiences her material utility, ${\\boldsymbol{u}}_{t}={\\boldsymbol{\\theta}}_{t}+{\\boldsymbol{\\zeta}}_{t}+{\\boldsymbol{Q}}-{\\boldsymbol{p}}$ , and then on the basis of this, decides what review to leave. We formalize the idea that a user cares about the next user’s purchasing decision (summarized by their purchase probability, denoted by $\\operatorname{pur}_{t+1,}$ ) by positing an overall utility function $V:\\mathbb{R}\\times[\\bar{0},1]\\to\\mathbb{R}$ , where the first argument is the user’s material utility and the second is the purchase probability of the next user. Here, again, we impose Assumption A-1. In a Bayes–Nash equilibrium, users will correctly take into account how the next user’s purchase probability depends on their reviews and try to influence this purchase probability. Review and purchase decisions, and the Bayes–Nash equilibrium are defined similarly. In particular, for any customer $t$ , we let $R_{t}:\\Omega_{t}\\times\\mathbb{R}\\to\\mathcal{R}$ denote the review decision strategy of customer $t$ , which is a mapping from the information provided from the rating system to tth customer and her material utility $u_{t}$ to a review decision in set $\\mathcal{R}$ . We refer to the collection of review decision strategies ${\\bf R}=\\{R_{t}\\}_{t=1}^{\\infty}$ as the review decision strategy profile. The purchase decision of customer $t$ is $B_{t}:\\Omega_{t}\\times\\mathbf{R}\\to\\{0,1\\}$ , which maps from the information provided by the rating system at time $t$ into a purchase decision. We can write this function as $$ B_{t}(\\Omega_{t},\\mathbf{R})=1\\quad\\mathrm{if~and~only~if}\\quad\\theta_{t}+\\mathbb{E}[\\zeta]+q_{t}(\\Omega_{t},\\mathbf{R})-p\\geq0, $$ where $q_{t}:\\Omega_{t}\\times\\mathbf{R}\\to[0,1]$ is a mapping form the information provided from the rating system to tth customer and the review decision strategy profile of all customers to a belief about the true quality.16 We also refer to the collection of purchase decision strategies $\\mathbf{B}=$ $\\{B_{t}\\}_{t=1}^{\\infty}$ as the purchase decision strategy profile. A Bayes–Nash equilibrium (or equivalently a sequential equilibrium) is defined analogous to the expressive utility case, and we omit the details to save space.\nASSUMPTION A-3: 1. The random variables $\\theta$ and $\\zeta$ have continuous and strictly increasing cumulative distribution functions over their supports, $[\\underline{{\\theta}},\\bar{\\theta}]$ and $[\\underline{{\\zeta}},\\bar{\\zeta}]$ , respectively. 2. The utility function $V:\\mathbb{R}\\times[0,1]\\to\\mathbb{R}$ is continuous and satisfies increasing differences, that is, $$ \\begin{array}{r l}&{V\\big(u_{t}^{\\prime},\\mathrm{pur}_{t+1}^{\\prime}\\big)-V\\big(u_{t}^{\\prime},\\mathrm{pur}_{t+1}\\big)}\\ &{\\quad\\geq V\\big(u_{t},\\mathrm{pur}_{t+1}^{\\prime}\\big)-V(u_{t},\\mathrm{pur}_{t+1}),\\quad\\forall u_{t}^{\\prime}\\geq u_{t},\\mathrm{pur}_{t+1}^{\\prime}\\geq\\mathrm{pur}_{t+1}.}\\end{array} $$ 3. We impose the following boundary conditions: for any pur′ $>$ pur and $u\\ge\\bar{\\theta}+\\bar{\\zeta}-p.$ , we have $V(u,\\mathrm{pur}^{\\prime})-V(u,\\mathrm{pur})>0.$ , and for any pur′ $>$ pur and $u\\leq\\bar{\\theta}+\\underline{{\\zeta}}-\\bar{p}+\\mathrm{\\hat{1}}$ , we have $V(u,\\mathrm{pur})-V(u,\\mathrm{pur}^{\\prime})>0$ .\nTHEOREM A-1: Suppose that customers have consequentialist utility and Assumptions A1 and A-3 hold. Then there exists a Bayes–Nash equilibrium in which review decisions take a threshold form. Suppose in addition that the strict separation condition holds. Then, in this equilibrium, Theorems 1–4 hold.\n\nThis section extends the model to consequentialist utility, where customers leave reviews to influence others' decisions. The model assumes each user cares about the next user's purchase probability and formalizes the equilibrium under Assumptions A-1 and A-3."
  },
  {
    "qid": "econ-empirical-355-2-0-3",
    "question": "4) Under what conditions would the inequality in Theorem 3 ($p_1^{**} \\equiv p_2^{**}$) be strict? Provide economic intuition and mathematical justification.",
    "gold_answer": "1. **Condition**: Binding constraint (7) with $\\mu > 0$ and $r > 0$.\n2. **Intuition**: If consumers heavily discount future utility ($r$ high), the monopolist must lower $p_1$ to incentivize period-1 purchases.\n3. **Math**: From Lemma 2, $W(p_1, p_2) = 0$ implies $p_1 > p_2$. If $r \\to 0$, the discounting effect vanishes, potentially equalizing prices.",
    "question_context": "We consider a two-period world. The monopolist can produce a good in both periods at a constant marginal cost c. $E x$ ante consumers do not know exactly how much satisfaction they will derive from the good. Their utility function is the same one that we used in Section 2. As before, consumers and producers have a discount rate of $r.$ The monopolist can charge a different price in periods 1 and 2. Let $p_{1}$ be the price in period 1 and $p_{2}$ be the price in period 2.\nThe problem the monopolist faces can be written: \n$$\\mathtt{m a x i m i z e}\\pi(p_{1},p_{2})$$ \nsubject to \n$$V(p_{1},p_{2})\\ge0$$ \n$$V(p_{1},p_{2})\\geq\\frac{1}{1+r}\\int_{0}^{1}u(v,Y-p_{2})f(v)d v.$$\nLemma 2. If $W(p_{1},p_{2})$ is equal to zero, $p_{1}$ is greater than $p_{2}$\nTheorem 3. Let $(p_{1}^{**},p_{2}^{**})$ be the optimal price sequence. We have $p_{1}^{\\ast\\ast}\\equiv p_{2}^{\\ast\\ast}$\n\nThe section discusses a monopolist's strategy of varying prices over time for a new product, comparing it to past models and introducing a two-period framework with specific utility and cost assumptions."
  },
  {
    "qid": "econ-empirical-683-1-0-4",
    "question": "5) Prove Theorem 2, which states that the limit of epsilon-rational competitive equilibria as $\\epsilon \\to 0$ is the set of strictly rational competitive equilibria. Use the properties of upper semicontinuity.",
    "gold_answer": "1. **Upper Semicontinuity**: The correspondences $\\hat{\\eta}_{\\jmath}(\\epsilon_{\\jmath}, p)$ and $\\hat{\\xi}_{\\iota}^{\\prime}(\\epsilon_{\\iota}, p)$ are upper semicontinuous in $\\epsilon$ and $p$. \\n2. **Limit Behavior**: As $\\epsilon \\to 0$, $\\hat{\\eta}_{\\jmath}(\\epsilon_{\\jmath}, p) \\to Y_{\\jmath}(p)$ and $\\hat{\\xi}_{\\iota}^{\\prime}(\\epsilon_{\\iota}, p) \\to X_{\\iota}(p)$. \\n3. **Equilibrium Set**: The set $P(\\epsilon) = \\{p | \\hat{z}(p, \\epsilon) \\cap (-\\Omega) \\neq \\emptyset\\}$ is upper semicontinuous, so as $\\epsilon \\to 0$, $P(\\epsilon) \\to P = \\{p | z(p) \\cap (-\\Omega) \\neq \\emptyset\\}$. \\n4. **Strictly Rational Equilibria**: The limit set $P$ corresponds to the strictly rational competitive equilibria, as $\\hat{z}(p, 0) = z(p)$.",
    "question_context": "$\\pi_{\\prime}(p)=\\operatorname*{max}_{y_{\\prime}\\in Y_{\\prime}}p\\cdot y_{\\prime}$ .if it exists. $Y_{_{J}}(p)=\\left\\{\\left.y_{_{J}}\\in Y_{_{J}}\\right|p\\cdot y_{_{J}}=\\pi_{_{J}}(p)\\right\\}$ is the strictly rational supply correspondence . $Y_{_{J}}(\\epsilon_{\\jmath},p)=\\big\\{\\:y_{_{J}}\\in Y_{_{J}}|p\\cdot y_{_{J}}\\equiv\\pi(p)-\\epsilon_{\\jmath}\\big\\}.$ $\\eta_{_{J}}(\\epsilon_{_{J}},p)=V_{_{J}}\\left(Y_{_{J}}(\\epsilon_{_{J}},p)\\right)\\in Y_{_{J}}(\\epsilon_{_{J}},p)$ is the $\\epsilon$ rational supply function.\n$\\bar{u}_{i}(p,w_{i})=\\operatorname*{max}\\mathrm{imum}\\left\\{u^{\\prime}(x)|x_{\\iota}\\epsilon\\gamma_{\\iota}(p,w_{\\iota})\\right\\}.$ $X_{\\iota}(p,w_{\\iota})=\\left\\{x_{\\iota}\\in\\gamma_{\\iota}(p,w_{\\iota})|u^{\\iota}(x_{\\iota})=\\bar{u}_{\\iota}(p,w_{\\iota})\\right\\}$ is the strictly rational demand correspondence. $X_{\\iota}(\\epsilon_{\\iota},p,w_{i})=\\big\\{x_{\\iota}\\in\\gamma_{\\iota}(p,w_{\\iota})\\big|\\bar{u}_{\\iota}(p,w_{i})-u^{\\iota}(x_{i})\\leq\\epsilon_{\\iota}\\big\\}.$ $\\dot{\\xi}_{\\iota}(\\epsilon_{\\iota},p,w_{\\iota})=V_{\\iota}(X_{\\iota}(\\epsilon_{\\iota},p,w_{i}))\\in X_{\\iota}(\\epsilon_{\\iota},p,w_{\\iota})\\mathrm{~is~the~}\\epsilon\\mathrm{~ram}$ tional demand function.\nLEMMA 1: If $Y_{\\jmath}$ is bounded, then $Y_{\\jmath}(\\epsilon_{\\jmath},p)$ is a closed, convex-valued correspondence which is contiruous in both $p$ and $\\epsilon_{\\jmath}$ ， $\\forall p\\in S_{N}$ and $\\forall\\epsilon_{\\j}>0$ Furthermore, $\\begin{array}{r}{\\operatorname*{lim}_{\\epsilon_{j}\\to0}Y_{j}(\\epsilon_{j},p)=Y_{\\jmath}(p).}\\end{array}$\nLEMMA 2: If $X_{i}$ is bounded, then $X,(\\epsilon_{i},p,w_{i})$ is a closed, convex-valued correspondence which is continuous in both $(p,w_{i})$ and $\\epsilon_{\\iota}$ $\\forall p\\in S_{N}$ ， $w_{i}\\neq\\operatorname*{min}p\\cdot X_{i}$ and $\\forall\\epsilon_{\\iota}>0$ .Furthermore, $\\scriptstyle\\operatorname*{lim}_{\\epsilon_{\\ell}\\to0}X_{\\iota}(\\epsilon_{\\iota},p$ $w_{\\iota})=X_{\\iota}(p,w_{\\iota})$\nTHEOREM1:Under the above assumptions,an epsilon-rational competitive equilibrium exists for the privateownershipeconomy $((X_{\\iota},u^{\\iota}),(Y_{\\j}),(\\omega_{\\iota}),(\\theta_{\\iota\\j}))$ and theseequilibria aredecentralizable.\nTHEOREM 2: The limit as $\\epsilon\\to0$ of the set of all possible epsilon-rational competitive equilibria is the set of the strictly rational competitive equilibria.\n\nThis section provides formal definitions and proofs related to producers, consumers, and competitive equilibria in an economic model. Key concepts include strictly rational supply and demand correspondences, epsilon-rational supply and demand functions, and the existence of epsilon-rational competitive equilibria."
  },
  {
    "qid": "econ-empirical-1027-0-0-2",
    "question": "3) Analyze the implications of monopoly power on input deflation, contrasting it with the competitive firm case.",
    "gold_answer": "1. In a competitive firm, input prices are exogenous.\n2. With monopoly power, input prices depend on the firm's actions, violating the assumption of exogenous prices.\n3. The pure price effect must now account for endogenous price changes, complicating the deflation process.",
    "question_context": "Input deflation is examined for a multi-product firm (or sector) in three cases: (a) selling at fixed prices, (b) with monopoly power over some outputs, (c) operating as a perfectly competitive industry but facing downward-sloping demand curves.\nThe input deflation and usage problem ought thus to be analyzed using the assumption of fixed output prices at which the firm can sell.\nIn the case of the single-output firm, the equivalence classes for input vectors are defined by the isoquants of the production function.\n\nThis section introduces the concept of input deflation within production theory, contrasting it with output deflation and discussing its relevance in economic measurement."
  },
  {
    "qid": "econ-empirical-232-0-2-3",
    "question": "13) Discuss the practical implications of the LIBOR application in Section 4.2 for financial time series modeling.",
    "gold_answer": "1. The DAR(1) model captures persistence in LIBOR rates while maintaining strict stationarity.\n2. Traditional unit-root tests might misleadingly suggest nonstationarity due to persistent trends.\n3. The model's conditional heteroscedasticity (via $\\sqrt{\\omega_{0}+\\alpha_{0}y_{t-1}^{2}}$) accounts for volatility clustering.\n4. Residual analysis (Fig. 4) and simulations (Fig. 6) validate the model's fit.\n5. This highlights the importance of nonlinear models for financial data, where linear models may fail.",
    "question_context": "Table 1 Summary statistics for our proposed procedure under various scenarios based on 1000 replications.\nFig. 2. Boxplots of the AAE for the GLADE and the QMLE based on 1000 replications.\nThe monthly 3-Month LIBOR (in percent), based on Japanese Yen, from Jan 1986 to Sep 2016.\n\nThe paper evaluates the finite-sample performance of the proposed methods via Monte Carlo simulations and applies them to a real dataset of interest rates. The results demonstrate the robustness and effectiveness of the GLAD estimator and the strict stationarity test."
  },
  {
    "qid": "econ-empirical-220-2-0-0",
    "question": "1) Using Lemma 1, formally derive the condition under which the supplier's expected cost of production is nonincreasing in $q_i$ when the signal is not misused. Provide the mathematical steps and justify each step.",
    "gold_answer": "1. **Given Lemma 1**: The supplier's expected cost is $\\int_{\\underline{s}}^{\\bar{s}} x(s,q_j) c(s,q_i) f(s) ds$.\n2. **Nonincreasing condition**: For the cost to be nonincreasing in $q_i$, the derivative with respect to $q_i$ must be non-positive: $\\frac{d}{dq_i} \\int_{\\underline{s}}^{\\bar{s}} x(s,q_j) c(s,q_i) f(s) ds \\leq 0$.\n3. **Signal not misused**: By definition, $x(s,q_j) \\geq x^*(0,\\cdot)$ for $s \\in [\\underline{s},0]$ and $x(s,q_j) \\leq x^*(0,\\cdot)$ for $s \\in [0,\\bar{s}]$.\n4. **Derivative evaluation**: Since $c(s,q_i)$ is decreasing in $q_i$ for $s < 0$ and increasing for $s > 0$, the integral's derivative is non-positive when the signal is not misused.",
    "question_context": "Lemma $I$ . $\\begin{array}{r}{\\int_{\\underline{{s}}}^{\\bar{s}}x(s,q_{j})c(s,q_{i})f(s)d s}\\end{array}$ , the supplier’s expected cost of producing output continuum $x(s,q_{j})$ , is n¯onincreasing in $q_{i}$ when the signal is not misused, that is, when $x(s,q_{j})\\geq x^{*}(0,\\cdot)$ for all $s\\in[\\underline{{s}},0]$ and $x(s,q_{j})\\leq x^{*}(0,\\cdot)$ for all $s\\in[0,\\bar{s}]$ .\nProposition $I$ . Suppose $n=2$ . Then at the solution to [BP]: $(\\mathrm{i})x(s,q_{1})<x^{\\ast}(s,q_{1})<x^{\\ast}(s,q_{2})=$ $x(s,q_{2})$ for all $s\\in[\\underline{{s}},0);x(0,q_{1})=x^{*}(0,\\cdot)=x(0,q_{2})$ ; and $x(s,q_{1})>x^{*}(s,q_{1})>x^{*}(s,q_{2})=$ $x(s,q_{2})$ for all $s\\in(0,\\bar{s}],$ ; (ii) $\\Pi(q_{1}\\vert q_{1})=0$ ; and (iii) $\\Pi(q_{2}\\mid q_{2})=\\Pi(q_{1}\\mid q_{2})\\geq0$ .\nCorollary 1. Suppose $n=2$ , $s$ is distributed uniformly on $\\textstyle{\\big[}-{\\frac{1}{2}},{\\frac{1}{2}}{\\big]},V(\\cdot)$ is quadratic, $\\phi_{1}=$ $\\begin{array}{r}{\\dot{.},\\phi_{2}=\\frac{1}{4}}\\end{array}$ , and $c(s,q)$ is as specified in (8). Then at the solution to [BP]: $(\\mathrm{i})x(s,q_{1})<x^{*}(0,\\cdot)$ for $s\\in(-.25,0)$ , $x(0,q_{1})=x^{*}(0,\\cdot),$ , and $x(s,q_{1})>x^{*}(0,\\cdot)$ for $s\\in(0,.25)$ ; and (ii) $\\Pi(q_{2}\\mid q_{2})>0$ .\nProposition 3. Suppose Assumption 1 holds. Then at the solution to [BP]: (i) the $\\Pi(q_{i}\\mid q_{i})\\geq$ $\\Pi(q_{i-1}\\mid q_{i})$ incentive compatibility constraints bind for $i=2,\\ldots,n$ ; (ii) the upward incentive compatibility constraints $(\\Pi(q_{i}\\mid q_{i})\\geq\\Pi(q_{j}\\mid q_{i})$ for $j>i,$ ) are not constraining; (iii) $\\Pi(q_{1}\\vert q_{1})=0$ ; (iv) $\\Pi(q_{i}\\mid q_{i})$ is a convex, monotone increasing function of $q_{i}$ ; (v) for $i=1$ , $2,\\ldots,n-1:x(s,q_{i})\\leq x(s,q_{i+1})\\leq x^{*}(s,q_{i+1})$ for all $s\\in[\\underline{{s}},0),x(0,q_{i})=x^{*}(0,\\cdot),$ , and $x^{*}(s,q_{i+1})\\leq x(s,q_{i+1})\\leq x(s,q_{i})$ for all $s\\in(0,\\bar{s}]$ ; and $(\\mathrm{vi})x(s,q_{n})=x^{*}(s,q_{n})$ for all $s\\in[\\underline{{s}},\\bar{s}]$ .\n\nThe text discusses the feasibility of first-best outcomes in scenarios where the buyer is privately informed about the quality of the public signal. It introduces Lemma 1, which states that the supplier's expected production cost declines as the quality of the public signal increases, provided the signal is not misused. The text further explores the implications of signal misuse and underuse on the supplier's rent and the buyer's optimal strategies."
  },
  {
    "qid": "econ-empirical-1819-5-0-3",
    "question": "4) Derive the first-order conditions for the second-best quality schedule in Case 3 of Proposition 12(i), where both $(IR_l^B)$ and $(IR_h^B)$ bind. How does the Lagrange multiplier $\\lambda$ influence the solution?",
    "gold_answer": "1. **First-Order Conditions**: The Lagrangian is:\n   $$ \\mathcal{L} = \\nu^A \\left[ \\theta_L^A u^A(q_L) + e_{lL} u^B(q_L) - c q_L \\right] + (1 - \\nu^A) \\left[ \\theta_H^A u^A(q_H) + e_{lH} u^B(q_H) - c q_H \\right] + \\lambda \\left[ \\nu^A (e_{hL} - e_{lL}) u^B(q_L) - (1 - \\nu^A)(e_{lH} - e_{hH}) u^B(q_H) \\right]. $$\n2. **Optimality**: The first-order conditions are:\n   $$ \\theta_L^A u^{A'}(q_L) + \\left( (1 - \\lambda) e_{lL} + \\lambda e_{hL} \\right) u^{B'}(q_L) = c, $$\n   $$ \\theta_H^A u^{A'}(q_H) + \\left( (1 - \\lambda) e_{lH} + \\lambda e_{hH} \\right) u^{B'}(q_H) = c. $$\n   Here, $\\lambda \\in (0,1)$ balances distortions to satisfy both participation constraints.",
    "question_context": "We show that if $q^{F B}$ is decreasing in some interval in $\\left[\\underline{{\\theta}},\\bar{\\overline{{\\theta}}}\\right]$ , then a complete shutdown may occur even if shutdown occurs in the first-best only for $\\theta$ very close to $\\bar{\\theta}$.\nPROPOSITION 11: Consider the baseline model with a continuum of types. (i) Suppose there exists $\\theta_{0}\\in\\left(\\underline{{\\theta}},\\overline{{\\theta}}\\right)$ such that (a) $q^{*}(\\theta)>0$ for each $\\theta~\\in~[\\underline{{\\theta}},\\theta_{0})$ and $q^{*}$ is either decreasing or single-peaked in $[\\underline{{\\theta}},\\theta_{0})$ $(b)$ $q^{*}(\\theta)=0$ for each $\\theta\\in\\left[\\theta_{0},\\bar{\\theta}\\right]$ .If $h(0)\\leq0$ , then a complete shutdown is optimal: $q^{S B}(\\theta)=0$ for any $\\theta\\stackrel{\\cdot}{\\in}[\\underline{{\\theta}},\\overline{{\\theta}}]$.\nPROPOSITION 12: Consider type reversal with negative sorting in the general model. (i) Consider first the case in which the first-best quality schedule is increasing. Then, pooling is never optimal and the second-best quality schedule $\\left(q_{L}^{S B},q_{H}^{S B}\\right)$ is characterized as follows. $\\left(q_{L}^{S B},q_{H}^{S B}\\right)$ is equal to $\\left(q_{L}^{*},q_{H}^{*}\\right)$ if (B1) holds; in this case $\\left(I R_{l}^{B}\\right)$ binds. $\\left(q_{L}^{S B},q_{H}^{S B}\\right)$ is equal to $\\left(\\hat{q}_{L},\\hat{q}_{H}\\right)$ if (B3) holds; in this case $\\left(I R_{h}^{B}\\right)$ binds. If both(B1) and B3) are violated, $\\left(q_{L}^{S B},q_{H}^{S B}\\right)$ is characterized by (B5) for a suitable $\\lambda\\in(0,1)$ such that (B4) holds; in this case, $\\left(I R_{l}^{B}\\right)$ and $\\left(I R_{h}^{B}\\right)$ both bind.\n\nThis section explores the implications of shutdown possibilities and type reversal in the context of a continuum of types, focusing on scenarios where the first-best and second-best solutions diverge significantly."
  },
  {
    "qid": "econ-empirical-1768-2-0-0",
    "question": "1) Analyze the temporal dynamics of riveting jobs at Ford during World War II. What factors contributed to the rapid rise and subsequent decline of riveters as a percentage of the workforce?",
    "gold_answer": "1. **Initial Rise (Early 1942):** Riveting jobs surged due to the demand for war production, particularly for bombers at Willow Run.\n2. **Stabilization (Most of War Period):** Riveters stabilized at 10-15% of the workforce, reflecting sustained war production needs.\n3. **Decline (Post-War):** As the war ended, riveting jobs disappeared due to the shift from war to domestic production, ceasing entirely by August 1945.",
    "question_context": "Workers classified as riveters appeared in force early in 1942 and rapidly became nearly 28 percent of the work force before stabilizing between 10 percent and 15 percent for most of the war period. As the war wound down the proportion of riveters continued to fall until riveting was no longer an identifiable job at Ford; riveters disappeared from Ford one month before the war wound down completely in August 1945.\nRiveting was an exceptional occupation at Ford because it was specific to war production. Bombers at Willow Run were assembled by riveters working outside the bodies and inside the wings. Oddly enough, while the myth of Rosie the Riveter remains an iconographic image of women's war-production efforts, women were no more likely than men to hold riveting jobs.\n\nThis section discusses the rise and fall of riveting jobs at Ford during and after World War II, highlighting the occupational shifts and gender disparities in the workforce."
  },
  {
    "qid": "econ-empirical-885-1-0-2",
    "question": "3) Prove that if state partition $\\gamma^{\\prime}$ is finer than $\\gamma$, then $W(\\gamma^{\\prime}) \\geq W(\\gamma)$ in all contract problems with a fixed-state set.",
    "gold_answer": "1. A finer partition $\\gamma^{\\prime}$ provides more granular contingencies, allowing the seller to tailor actions more precisely to each state subset.  \n2. Formally, for any compliance set $\\alpha_{j}$ under $\\gamma$, there exists a refinement under $\\gamma^{\\prime}$ that preserves or improves the seller's expected utility.  \n3. The maximization problem $W(\\gamma) = \\operatorname*{max} E U_{s}(x_{s}, a)$ is subject to the same constraints, but the finer partition $\\gamma^{\\prime}$ expands the feasible set of actions, ensuring: $$W(\\gamma^{\\prime}) \\geq W(\\gamma).$$",
    "question_context": "There are two parties to the contract, a seller and a buyer. I represent the uncertainty in the environment by the state set $\\tilde{{\\boldsymbol{\\theta}}}=\\left\\{\\theta_{1},\\ldots,\\theta_{m}\\right\\}$. A realization $\\theta_{i}$ of $\\tilde{\\pmb{\\theta}}$ is a state of the world and resolves all uncertainty to the contracting parties other than their contract-related actions. A probability distribution $F(\\cdot)$, common knowledge to the contracting parties, is defined over $\\tilde{\\pmb{\\theta}}.$ Only the seller has the choice of a contract-related action and hence the possibility of defaulting. Denote the potential action set for the seller as $A$.\nA contract consists of a partition over the set of states, with a subset of $A$ specified for each equivalence class of the partition. Informational constraints may prevent some partitions of $\\tilde{\\pmb{\\theta}}$ from being available to the contracting parties. Denote by $\\mathbf{\\delta\\Gamma}\\mathbf{T}$ the set of available partitions. A typical element of $\\mathbf{\\delta\\Gamma}$ is the partition $\\gamma=(\\gamma_{1},\\dots,\\gamma_{k})$, where $k$ is the number of equivalence classes in $\\gamma$. Each equivalence class $\\gamma_{j},j=1,\\ldots,k$, is a contingency.\nFor each contingency the seller's contract actions are a subset of $A$. Again, informational constraints may prevent some subsets from being available for use in the contract. Denote the set of available subsets of $A$ by $\\mathring{A}$. Then a contract $c$ consists of a partition $\\gamma\\in\\Gamma$ over future states, with an action subset $\\alpha\\in\\mathring{A}$ specified for each contingency in the contract: $c=(\\gamma_{1},\\ldots,\\gamma_{k};\\alpha_{1},\\ldots,\\alpha_{k})$. Action subset $\\alpha_{j}$ is the compliance set for contingency $j$. Denote the complement (with respect to $A$) of $\\alpha_{j}$ as $\\beta_{j}\\colon\\beta_{j}$ is the breach set in contingency $j$. Assume that the set of available compliance sets $\\mathring{A}$ and the set of available state partitions $\\mathbf{\\delta\\Gamma}$ are nonempty.\nAfter the contract is signed but before the uncertainty is revealed, the buyer may have the opportunity to take an action in reliance on contract completion. Reliance actions will be taken to be monetary expenditures. Denote the reliance expenditures available to the buyer by the bounded set $R=[0,\\hat{r}]$.\nAfter the uncertainty is revealed, the seller must choose a contract action from the set $A$. Given contingency $j$, if the seller chooses an action from the breach set $\\beta_{j}$, he will be subject to sanctions specified by the damage measure $\\delta(\\cdot)$. The amount of damages $\\delta$ paid by the seller and received by the buyer may depend on the chosen action, the buyer's reliance, the realized state, and other aspects of the contracting situation.\nThe gross monetary outcome to the seller, $x_{s}^{\\prime}$, depends on his contract action and the realized state: $x_{s}^{\\prime}=x_{s}^{\\prime}(a,\\theta_{j})$. The buyer's gross monetary outcome depends also on her reliance action: $x_{b}^{\\prime}=x_{b}^{\\prime}(a,\\theta_{j},r)$. The net monetary outcome $x_{i}$ of the contract for party $i$ ($i=s,b$) is just the gross monetary outcome minus (for the seller) or plus (for the buyer) the amount of damages; i.e., $x_{s}(a,\\theta_{j},\\delta)=x_{s}^{\\prime}-\\delta$ and $x_{b}(a,\\theta_{j},r,\\delta)=x_{b}^{\\prime}+\\delta$.\nThe seller has preferences over his net monetary outcome $x_{s}$ and his action $a$, given by the von Neumann-Morgenstern utility function $U_{s}(x_{s},a)$. The buyer has preferences over her net monetary outcome $x_{b}$, represented by the von Neumann-Morgenstern utility function $U_{b}(\\boldsymbol{x}_{b})$. The buyer can achieve a utility of $\\bar{U}_{b}$ outside of the contract.\nDefinition 1. The two-party contract problem is to choose state partition $\\gamma=(\\gamma_{1},\\dots,\\gamma_{k}) \\in\\mathbf{\\Gamma}$, compliance sets $\\alpha_{j}\\in\\mathring{A},j=1,\\ldots,k$, reliance action $r\\in R$, and contract action $a\\in A$ to maximize $E U_{s}(x_{s},a)$, subject to: $E U_{b}(x_{b})\\geq\\bar{U}_{b}$, $r\\in\\mathrm{argmax}_{r^{\\prime}\\in R}E U_{b}(x_{b}(\\cdot,r^{\\prime}))$, and $a\\in\\mathrm{argmax}_{a^{\\prime}\\in A}U_{s}(x_{s}(a^{\\prime},\\cdot),a^{\\prime})$.\nFor a fixed contract $\\bar{c}$, fixed reliance expenditure $\\bar{r}$, and given state $\\theta_{j}$, the seller can calculate his best action $a$ as $a(\\bar{c},\\bar{r},\\theta_{j})\\in\\mathrm{argmax}_{a^{\\prime}\\in A}U_{s}(x_{s}(a^{\\prime},\\theta_{j},\\delta),a^{\\prime})$. From $F(\\cdot)$ we can then calculate the parties' expected utilities $E U_{i}({\\bar{c}},{\\bar{r}})$ for the fixed contract and reliance expenditures.\nBy repeating the exercise above for all levels of $r\\in R$, the buyer can calculate her optimal reliance for a fixed contract $\\bar{c}$ as $r(\\bar{c})\\in\\mathrm{argmax}_{r^{\\prime}\\in{\\cal R}}E U_{b}(x_{b}(a(\\bar{c},r^{\\prime},\\theta_{j}),\\theta_{j},r^{\\prime}))$. Given the level of reliance, the parties can calculate their expected utility from contract $\\bar{c}$. The seller then chooses the contract that yields the highest $E U_{s}$, subject to the buyer's willingness to sign.\nDefine $W=\\operatorname*{max}E U_{s}(x_{s},a)$. Then $W$ provides a ranking of the value of various contract problems. This ranking can be extended to state partitions.\nDefinition 2. The value of state partition $\\gamma$ in a given contract problem is $W(\\gamma)=\\operatorname*{max}E U_{s}(x_{s},a)$, given state partition $\\gamma$, and subject to constraints (1), (2), and (3). State partition $\\gamma^{\\prime}$ is at least as valuable as state partition $\\gamma$ in a given contract problem if $W(\\gamma^{\\prime})\\geq W(\\gamma)$.\n\nThe model describes a contractual relationship between a seller and a buyer under uncertainty, represented by a state set. The contract involves partitions over states, compliance and breach sets, reliance actions, and damage measures."
  },
  {
    "qid": "econ-empirical-771-3-0-0",
    "question": "1) Under the assumption that $\\mu \\neq 0$ and all assets appear in the population optimal portfolio with positive weights, show that the function $f(x) = \\frac{x^T \\mu}{\\sqrt{x^T \\Sigma x}}$ is uniformly bounded in $\\mathbb{R}_*^d$ and derive the upper bound $\\sup_{x \\in \\mathbb{R}_*^d} |f(x)| \\leq \\sqrt{\\mu^T \\Sigma^{-1} \\mu}$.",
    "gold_answer": "1. **Uniform Boundedness**: The function $f(x)$ is the Sharpe ratio, which measures the risk-adjusted return. The Cauchy-Schwarz inequality ensures that $|x^T \\mu| \\leq \\sqrt{x^T \\Sigma x} \\sqrt{\\mu^T \\Sigma^{-1} \\mu}$ for any $x \\in \\mathbb{R}_*^d$. \n2. **Upper Bound**: Dividing both sides by $\\sqrt{x^T \\Sigma x}$ yields $|f(x)| \\leq \\sqrt{\\mu^T \\Sigma^{-1} \\mu}$. Thus, $f(x)$ is uniformly bounded with the given upper bound.",
    "question_context": "Assume throughout that $d\\geq2$ , and (3.2) and (3.3) hold, with $V=\\Sigma$ in (3.3).\nPart (i): Suppose, in addition to (3.2), that $\\mu\\neq0$ and all assets appear in the population optimal portfolio with positive weights. Let $\\bar{\\mathbb{R}^{d}}:=\\mathbb{R}^{d}\\setminus\\{0\\}$ , and let $f$ denote the function in (2.1), i.e., \n\n$$\nf(x)={\\frac{x^{T}\\mu}{\\sqrt{x^{T}\\varSigma x}}},\\quad x\\in\\mathbb{R}_{*}^{d}.\n$$\nIt is shown in Maller and Turkington (2002) that $f$ is uniformly bounded in $\\mathbb{R}_{*}^{d}$ , in fact $\\mathrm{sup}_{x\\in\\mathbb{R}_{*}^{d}}|f(x)|\\leq\\sqrt{\\mu^{T}\\Sigma^{-1}\\mu}$ , and has a critical point $\\widehat{x}_{C_{A}}$ in $C_{A}$ at which $f$ is a maximum or a minimum according as the ‘‘Tangent Point Criterion’’ $T{\\cal P}C:=i^{T}{\\cal{E}}^{-1}\\mu$ is positive or negative.\nMaller and Turkington (2002) show that $\\widehat{x}_{C_{A}}$ is uniquely determined unless all components of $\\mu$ are equal, i.e., $\\mu=c i$ for some $c\\in\\mathbb{R}$ , or $T P C=0$ . Consider these cases. When $\\mu=c i$ and $c>0$ then \n\n$$\n\\operatorname*{sup}_{x\\in C_{A}}f(x)=c\\sqrt{i^{T}\\Sigma^{-1}i},\n$$ \n\nand is achieved for the unique ${\\widehat x}_{C_{A}}={\\cal{E}}^{-1}i/i^{T}{\\cal{E}}^{-1}i.$ In this case $T P C>0$ . When $c\\leq0$ then $T P C=i^{T}{\\cal{S}}^{-1}\\mu=c i^{T}{\\cal{S}}^{-1}i\\leq0.$ We conclude that $\\widehat{x}_{C_{A}}$ is uniquely determined unless $\\mu=c i$ for some $c\\leq0$ in which case $T P C\\leq0$ .\nWhen $T P C\\leq0$ it is shown in Maller and Turkington (2002) that the maximum of $f(x)$ in $C_{A}$ is at infinity and can be found by taking the supremum along a direction $u\\in\\mathbb{R}^{d}$ then maximising over directions $u$ , $|u|=1$ . Clearly then the supremum of $f(x)$ for $x\\in C_{+}$ would occur on the boundary of the simplex, i.e., having one or more zero components. But we have assumed this is not the case, so we conclude that $T P C>0$ .\nSince $\\mu\\neq0$ is assumed, at least one component of $\\mu$ is positive, or else $\\mu\\leq0$ and at least one component of $\\mu$ is negative. Assume the former at first. Then $S R^{+}$ must be positive and in finding the maximum of $f$ we can restrict attention to $x\\in\\mathbb{R}^{d}$ for which $x^{T}\\mu>$ 0. The function $f$ is (strictly) quasi-concave on $C_{A}\\cap\\{x^{T}\\mu>0\\}$ , see Stoyanov et al. (2007).\nLet $\\widehat{x}_{C_{+}}$ be any maximiser of $f$ in $C_{+}$ . Under our assumptions we must have $\\widehat{x_{C_{+}}}=\\widehat{x}_{C_{A}}$ . To verify this, suppose $f(\\widehat{x}_{C_{+}})<f(\\widehat{x}_{C_{A}})$ . Then the line joining $\\widehat{x}_{C_{+}}$ and $\\widehat{x}_{C_{A}}$ would contain a point $x_{B}$ on the boundary $C_{A}\\cap C_{+}$ (thus, having one or more zero components). Then $f(x_{B})\\leq f(\\widehat{x}_{C_{A}})$ (since $x_{B}~\\in~C_{A}$ , and $f(x_{B})\\leq f(\\widehat{x}_{C_{+}})$ (since $x_{B}~\\in~C_{+}$ , and the maximum in $C_{+}$ is interior). But this is not possible when $T P C>0$ by the quasi-concavity of $f$ . Consequently $\\widehat{x}_{C_{+}}~=~\\widehat{x}_{C_{A}}$ , as claimed, and the unconstrained maximum in $C_{A}$ actually lies in $C_{+}$ and is uniquely determined.\nAlternatively, if $\\mu\\leq0$ but $\\mu\\neq0$ we can reverse the argument, as follows. Write \n\n$$\n\\operatorname*{sup}_{x\\in C_{A}}f(x)=\\operatorname*{sup}_{x\\in C_{A}}\\frac{x^{T}\\mu}{\\sqrt{x^{T}\\Sigma x}}=-\\operatorname*{inf}_{x\\in C_{A}}\\frac{x^{T}\\mu^{-}}{\\sqrt{x^{T}\\Sigma x}}=:-\\operatorname*{inf}_{x\\in C_{A}}f^{-}(x),\n$$ \n\nwhere $\\mu^{-}:=-\\mu\\geq0$ but $\\mu^{-}\\neq0$ We still have the original $T P C>0$ , so \n\n$$\nT P C^{-}:=i^{T}\\Sigma^{-1}\\mu^{-}=-i^{T}\\Sigma^{-1}\\mu<0.\n$$\nApplying the Maller and Turkington (2002) analysis to assets with mean excess returns $\\mu^{-}$ , we find that the infimum of $f^{-}$ in $C_{A}$ occurs at a unique finite value \n\n$$\n\\widetilde{x_{C_{A}}}:=\\frac{\\Sigma^{-1}\\mu^{-}}{i^{T}\\Sigma^{-1}\\mu^{-}}=\\frac{\\Sigma^{-1}\\mu}{i^{T}\\Sigma^{-1}\\mu}\n$$ \n\nsuch that \n\n$$\n\\operatorname*{inf}_{x\\in C_{A}}f^{-}(x)=-\\sqrt{(\\mu^{-})^{T}\\Sigma^{-1}\\mu^{-}}=-\\sqrt{\\mu^{T}\\Sigma^{-1}\\mu}.\n$$ \n\nThen, according to (6.4), $\\widetilde{x_{C_{A}}}$ is the unique maximiser of $f$ in $C_{A}$ such that \n\n$$\n\\operatorname*{sup}_{x\\in C_{A}}f(x)=+{\\sqrt{\\mu^{T}\\Sigma^{-1}\\mu}}.\n$$\nFurther, $f^{-}$ is strictly quasi-convex on $C_{A}\\cap\\{x^{T}\\mu^{-}>0\\}$ by the Stoyanov et al. (2007) result. But $S R^{+}\\leq0$ since $\\mu\\leq0,s_{0}\\widehat{x}_{C_{+}}$ must occur on a boundary of $C_{+}$ , and we ruled this out. We conclude $\\widehat{x}_{C_{+}}=\\widehat{x}_{C_{A}}$ and the maximiser of $f$ in $C_{A}$ lies in $C_{+}$ .\nSo far we have shown, under the assumptions $\\mu\\neq0$ and all assets appear in the population optimal portfolio with positive weights, that necessarily $T P C=\\dot{i}^{T}{\\cal{E}}^{-1}\\mu\\mathrm{~>~}0$ and the unique maximiser of $f(x)$ in $C_{A}$ lies in $C_{+}$ . Since $(\\widehat{\\mu}_{n},\\widehat{\\Sigma}_{n})\\overset{P}{\\rightarrow}(\\mu,\\Sigma)$ , the same properties hold in the sample with probability approaching 1 (WPA1) as $n\\to\\infty$ for the random function \n\n$$\nf_{n}(\\boldsymbol{x}):=\\frac{\\boldsymbol{x}^{T}\\widehat{\\boldsymbol{\\mu}}_{n}}{\\sqrt{\\boldsymbol{x}^{T}\\widehat{\\boldsymbol{\\Sigma}}_{n}\\boldsymbol{x}}},\n$$ \n\nwhich thus has a unique maximum at $\\widehat{x}_{C_{A}}(n)$ , say, in $C_{A}$ , which is also the unique maximiser $\\widehat{x}_{C_{+}}(n)$ of $f_{n}(x)$ in $C_{+}$ .\nFor the next step, we will show that, under (3.1), $\\widehat{x}_{C_{+}}(n)\\overset{P}{\\to}\\widehat{x}_{C_{+}}$ , as $n\\to\\infty$ , that is, $\\widehat{x}_{C_{+}}(n)$ is consistent for $\\widehat{x}_{C_{+}}$ . Let $\\widehat{x}_{C}$ and $\\widehat{x}_{C}(n)$ be any maximisers of $f$ and $f_{n}$ in any subset $C\\subseteq\\mathbb{R}_{*}^{d}$ . Consider \n\n$$\n\\begin{array}{r l}&{\\underset{x\\in C}{\\operatorname*{sup}}f_{n}(x)-\\underset{x\\in C}{\\operatorname*{sup}}f(x)\\left|\\le\\underset{x\\in C}{\\operatorname*{sup}}\\right|f_{n}(x)-f(x)\\right|}\\ &{=\\underset{x\\in C}{\\operatorname*{sup}}\\left|\\frac{x^{T}\\widehat{\\mu}_{n}}{\\sqrt{x^{T}\\widehat{\\Sigma}_{n}x}}-\\frac{x^{T}\\mu}{\\sqrt{x^{T}\\Sigma x}}\\right|}\\ &{=\\underset{x\\in C}{\\operatorname*{sup}}\\left|\\frac{x^{T}\\mu\\left(\\sqrt{x^{T}\\Sigma x}-\\sqrt{x^{T}\\widehat{\\Sigma}_{n}x}\\right)}{\\sqrt{x^{T}\\Sigma x}\\sqrt{x^{T}\\widehat{\\Sigma}_{n}x}}+\\frac{x^{T}(\\widehat{\\mu}_{n}-\\mu)}{\\sqrt{x^{T}\\widehat{\\Sigma}_{n}x}}\\right|}\\ &{=\\underset{x\\in C}{\\operatorname*{sup}}\\left|A_{n}(x)+B_{n}(x)\\right|,\\mathrm{sup}.}\\end{array}\n$$\nNow (see (A.4) of Appendix B in Maller et al. (2005)) \n\n$$\n\\operatorname*{sup}_{x\\in\\mathbb{R}_{*}^{d}}|A_{n}(x)|\\leq\\sqrt{\\mu^{T}\\Sigma^{-1}\\mu}\\|\\Sigma^{1/2}\\widehat{\\Sigma}_{n}^{-1}\\Sigma^{1/2}-I_{d}\\|,\n$$ \n\nand, similarly, \n\n$$\n\\operatorname*{sup}_{x\\in\\mathbb{R}_{*}^{d}}|B_{n}(x)|\\leq\\sqrt{(\\widehat{\\mu}_{n}-\\mu)^{T}\\widehat{\\Sigma}_{n}^{-1}(\\widehat{\\mu}_{n}-\\mu)}.\n$$ \n\nIn view of (3.1) the righthand sides of (6.7) and (6.8), and hence of (6.6), are $o_{P}(1)$ as $n\\to\\infty$ , so $f_{n}(\\widehat{x}_{C}(n))=f(\\widehat{x}_{C})+o_{P}(1)$ . Similarly, we have $f_{n}(\\widehat{x}_{C}(n))=f(\\widehat{x}_{C}(n))+o_{P}(1)$ .\nNow take any sequence of integers $n_{k}~\\rightarrow~\\infty$ . The sequence $(\\widehat{x}_{C+}(n_{k}))_{k\\geq1}$ is tight in $\\mathbb{R}^{d}$ since all components of $\\widehat{x}_{C_{+}}(n_{k})$ lie in [0, 1]. So we can take a subsequence, also denoted $n_{k}$ , such that $\\widehat{x}_{C_{+}}(n_{k})\\overset{D}{\\rightarrow}x_{0}$ as $k\\rightarrow\\infty$ for some (possibly random) $x_{0}~\\in~C_{+}$ . Since $f$ is continuous this implies $f(\\widehat{x}_{C_{+}}(n_{k}))\\overset{D}{\\rightarrow}f(x_{0})$ as $k\\rightarrow\\infty$ . The convergence in distribution implies, for $\\delta>0,\\varepsilon>0$ , \n\n$$\n\\begin{array}{r l}&{P\\left(f(x_{0})\\geq f(\\widehat{x}_{C_{+}})-\\delta\\right)}\\ &{\\quad\\geq P\\left(f(\\widehat{x}_{C_{+}}(n_{k}))\\geq f(\\widehat{x}_{C_{+}})-\\delta\\right)-\\varepsilon}\\ &{\\quad\\geq P(f(\\widehat{x}_{C_{+}}(n_{k}))\\geq f(\\widehat{x}_{C_{+}})-\\delta,}\\ &{f_{n_{k}}(\\widehat{x}_{C_{+}}(n_{k}))\\leq f(\\widehat{x}_{C_{+}}(n_{k}))+\\delta/2)-\\varepsilon}\\end{array}\n$$ \n\nfor large $k$ , and provided $f(\\widehat{x}_{C_{+}})-\\delta$ is a continuity point of the distribution of $f(x_{0})$ . Since $f_{n}(\\widehat{x}_{C_{+}}(n))-f(\\widehat{x}_{C_{+}}(n))\\overset{P}{\\rightarrow}0$ , the RHS of (6.9) exceeds \n\n$$\nP\\left(f_{n_{k}}(\\widehat{x}_{C_{+}}(n_{k}))\\geq f(\\widehat{x}_{C_{+}})-\\delta/2\\right)-2\\varepsilon\n$$ \n\nfor large $k.$ Now by (6.6)–(6.8) \n\n$$\n\\begin{array}{r l}&{f_{n_{k}}(\\widehat{x}_{C_{+}}(n_{k}))=\\underset{x\\in C_{+}}{\\operatorname*{sup}}f_{n_{k}}(x)}\\ &{\\qquad=\\underset{x\\in C_{+}}{\\operatorname*{sup}}f(x)+o_{P}(1)}\\ &{\\qquad=f(\\widehat{x}_{C_{+}})+o_{P}(1).}\\end{array}\n$$ \n\nSo the expression in (6.10) tends to $1~-~2\\varepsilon$ as $k\\rightarrow\\infty$ . Going back to (6.9) and letting $k\\rightarrow\\infty$ then $\\varepsilon\\mathrm{~\\downarrow~0~}$ gives $P\\left(f(\\bar{x}_{0})\\ge f(\\widehat{x}_{C_{+}})\\right)-\\delta)=1$ . Then letting $\\delta\\downarrow0$ through values such that $f(\\widehat{x}_{C_{+}})-\\delta$ is a continuity point of the distribution of $f(x_{0})$ gives $P$ $\\left(f(x_{0})\\geq f(\\widehat{x}_{C_{+}})\\right)=1$ .\nThe reverse relation $P\\left(f(x_{0})\\leq f(\\widehat{x}_{C_{+}})\\right)=1$ holds trivially, so we get $P\\left(f(x_{0})=f(\\widehat{x}_{C_{+}})\\right)=1$ . The supremum of $f(x)$ is uniquely determined, as established earlier, so we conclude that $P\\left(\\dot{x_{0}}=\\widehat{x}_{C_{+}}\\right)=1$ . This is true for all subsequences so finally we have $\\widehat{x}_{C_{+}}(n)\\overset{P}{\\to}\\widehat{x}_{C_{+}}$ , as $n\\to\\infty$ as claimed.\nSince $\\widehat{x}_{C_{+}}>0$ we deduce that $\\widehat{x}_{C_{+}}(n)>0$ WPA1 as $n\\rightarrow\\infty$ . Further, the sample $T P C_{n}:=i^{T}\\widehat{\\Sigma}_{n}^{-1}\\widehat{\\mu}_{n}\\overset{P}{\\to}i^{T}\\Sigma^{-1}\\mu=T P C>0$ as $n\\to\\infty$ . Then, just as for the population $S R^{+}$ , we can conclude that the sample maximum in $C_{+}$ equals the maximum in $C_{A}$ , i.e., \n\n$$\n\\widehat{S R}_{n}^{+}=\\operatorname*{sup}_{x\\in{\\cal C}_{A}}\\left(\\frac{x^{T}\\widehat{\\mu}_{n}}{\\sqrt{x^{T}\\widehat{\\Sigma}_{n}x}}\\right),\\quad\\mathrm{WPA}1{\\mathrm{~as~}}n\\to\\infty.\n$$ \n\nConsequently the asymptotic distribution of $\\widehat{S R}_{n}^{+}$ can be read from Theorem 3.3. Since $\\mu\\neq0$ and) $i^{T}\\d\\underline{{{X}}}^{-1}\\mu>0$ , only Part (i)(a) of Theorem 3.3 is relevant. We get (3.10), where \n\n$$\n\\sigma_{A}^{2}=\\frac{A\\xi A^{T}}{4\\mu^{T}\\Sigma^{-1}\\mu},\n$$ \n\n$\\xi$ is defined in (3.3), and $A$ is the $1\\times(d+d^{2})$ -vector \n\n$$\nA=\\left[2\\mu^{T}\\Sigma^{-1}\\mid-\\left(\\mu^{T}\\Sigma^{-1}\\otimes\\mu^{T}\\Sigma^{-1}\\right)\\right]_{1\\times(d+d^{2})}.\n$$ \n\n(6.12) and (6.13) are obtained from Eqs. (A.12) and (A.14) of MDJ (2010).\nPart (ii): In Part (i) of the proof we assumed $\\mu\\neq0$ and all $d$ assets are in the population optimal portfolio with positive weights. Next we consider the case when not all $d$ assets are in the population optimal portfolio. The idea is to reduce the population by removing the redundant assets and apply the results in Part (i), but still we look for conditions on the original $\\mu$ and $\\boldsymbol{\\Sigma}$ which apply to the maximum Sharpe ratio calculated from the $d$ assets in the sample. Assume throughout this part that all elements of $\\mu$ differ, no element of $\\mu$ is zero, no element of $\\Sigma^{-1}\\mu$ is 0, and $i^{T}\\dot{Z^{-1}}\\mu\\doteq0$ .\nSuppose $1\\leq m<d$ assets are in $S R^{+}(d)$ with positive weights, assumed numbered $1,2,\\ldots,m$ . So \n\n$$\n\\widehat{\\boldsymbol{x}}_{C_{+}(d)}=[\\boldsymbol{x}_{C_{+}(d)}^{(m)T}\\boldsymbol{0}^{T}]^{T},\n$$ \n\nwhere $x_{C_{+}(d)}^{(m)}>0$ is $m\\times1$ and 0 is $(d-m)\\times1$ . Partition $\\mu$ and $\\boldsymbol{\\Sigma}$ conformally to get $\\mu(m)$ and $\\Sigma(m)$ .\nWe need some expanded notation. For integers $k=1,2,\\ldots$ let \n\nDefine the functions $f^{k}:\\mathbb{R}_{*}^{k}:=\\mathbb{R}^{k}\\setminus\\{0\\}\\mapsto\\mathbb{R}$ by \n\n$$\nf^{k}(x)=\\frac{x^{T}\\mu(k)}{\\sqrt{x^{T}\\Sigma(k)x}},\\quad x\\in\\mathbb{R}_{*}^{k},\n$$ \n\nand let \n\n$$\nS R^{A}(k)=\\operatorname*{sup}_{x\\in C_{A}(k)}f_{k}(x)\\quad{\\mathrm{and}}\\quad S R^{+}(k)=\\operatorname*{sup}_{x\\in C_{+}(k)}f_{k}(x).\n$$ \n\nDefine sample quantities $f_{n}^{k}(x),\\widehat{S R}_{n}^{A}(k),\\widehat{S R}_{n}^{+}(k)$ , analogously to (6.15) and (6.16).\nIt is obvious and not difficult to verify analytically that redundant assets (those with zero allocation in the optimising vector) can be discarded without affecting the value of the Sharpe ratio, either in the population or in the sample. So we can write \n\n$$\nS R^{+}(d)=S R^{+}(m)=\\operatorname*{sup}_{x\\in C_{+}(m)}f^{m}(x).\n$$ \n\nSince we assumed that no element of $\\mu$ is 0 we have $\\mu(m)\\neq0$ , and since $\\mu(m)\\neq c i,f^{m}(x)$ is uniquely maximised at an allocation $\\widehat{x}_{C_{+}(m)}$ . As no components of $\\widehat{x}_{C_{+}(m)}$ are zero we know from Part (i) of the proof that $T P C(m):=i^{T}\\Sigma^{-1}(m)\\mu(m)>0,\\widehat x_{C_{+}(m)}=\\widehat x_{C_{A}(m)}$ , $S R^{+}(m)=S R^{A}(m)$ , and \n\n$$\n\\sqrt{n}\\left(\\widehat{S R}_{n}^{+}(m)-S R^{+}(m)\\right)\\stackrel{D}{\\rightarrow}N(0,\\sigma_{A}^{2}(m)),\n$$ \n\nwhere $\\sigma_{A}^{2}(m)$ is given by (6.12) with $A,\\xi,\\mu,\\Sigma$ , replaced by $A(m),\\xi(m),\\mu(m)$ and $\\Sigma(m).A(m)$ in turn is obtained from (6.13) with $\\mu$ and $\\boldsymbol{\\Sigma}$ replaced by $\\mu(m)$ and $\\Sigma(m)$ , and $\\xi(m)$ is the $(m+m^{2})\\times(m+m^{2})$ asymptotic covariance matrix of $\\sqrt{n}\\left(\\widehat{\\mu}_{n}(m)-\\mu(m),\\mathsf{v e c}(\\widehat{\\Sigma}_{n}(m)-\\Sigma(m))\\right)$ expressed in the form \n\n$$\n\\xi(m)=\\left[{\\begin{array}{c c}{\\xi(m)}&{G(m)}\\ {G^{T}(m)}&{H(m)}\\end{array}}\\right].\n$$ \n\nSince $S R^{+}(d)~=~S R^{+}(m)$ , (6.17) holds with $S R^{+}(m)$ replaced by $S R^{+}(d)$ . To complete the proof of Part (ii) we need to replace $\\widehat{S R}_{n}^{+}(m)$ by $\\widehat{S R}_{n}^{+}(d)$ also. This is done as follows.\nAssume at first that $T P C=i^{T}{\\cal{E}}^{-1}\\mu>0$ . Then $\\widehat{x}_{C_{A}(d)}$ , the (unique) maximiser of $f^{d}(x)$ in $C_{A}(d)$ , satisfies \n\n$$\n{\\widehat x}_{C_{A}(d)}={\\frac{{\\cal{E}}^{-1}\\mu}{i^{T}{\\cal{E}}^{-1}\\mu}}.\n$$ \n\nBy assumption $\\Sigma^{-1}\\mu$ has no zero elements, so $\\widehat{x}_{C_{A}(d)}$ has no zero elements. Further, $\\widehat{x}_{C_{A}(d)}$ cannot lie in the interior of $C_{+}(d)$ since then we would have $m=d$ ; but we assumed $m<d$ . We deduce that $\\widehat{x}_{C_{A}(d)}$ lies in $C_{+}^{c}(d)$ , the complement of the closed simplex $C_{+}(d)$ .\nNow consider $\\widehat{x}_{C_{+}(d)}$ , the maximiser of $f^{d}(x)$ in $C_{+}(d)$ , which has the form (6.14), where $x_{C_{+}(d)}^{(m)}>0$ . We claim that, WPA1 as $n\\rightarrow\\infty$ , the maximiser $\\widehat{x}_{C_{+}(d)}(n)$ of $f_{n}^{d}(x)$ in $C_{+}(d)$ has the same form, namely, \n\n$$\n\\widehat{\\boldsymbol{x}}_{C_{+}(d)}(n)=[\\boldsymbol{x}_{C_{+}(d)}^{(m)T}(n)\\boldsymbol{0}^{T}]^{T},\n$$ \n\nwhere $\\widehat{x}_{C_{+}(d)}^{(m)}(n)$ is $m\\times1$ and 0 is $(d-m)\\times1$ . We restrict consideration from now on to an event of probability ${>}1-\\varepsilon$ , $\\ensuremath{\\varepsilon}\\in(0,1)$ , and $n$ large. On this event we can assume $\\widehat{x}_{C+(d)}^{(m)}(n)>0$ because $\\widehat{x}_{C_{+}(d)}(n)$ is consistent $\\mathrm{for}\\widehat{x}_{C_{+}(d)}$ . We need to verify that the last $d-m$ elements of $\\widehat{x}_{C_{+}(d)}(n)$ are zero (on this event). Suppose by way of contradiction that elements $\\widehat{x}_{m+1}(n),\\ldots,\\widehat{x}_{q}(n)$ (in addition to the elements $\\widehat{x}_{1}(n),\\ldots,\\widehat{x}_{m}(n))$ are positive and the remaining $d-q$ elements are 0. The maximiser of $f^{d}$ over the subspace of $C_{A}$ with these elements set equal to zero is \n\n$$\n\\operatorname*{sup}_{x\\in C_{A}(d),x_{q+1}=\\cdots=x_{d}=0}f^{d}(x)=\\operatorname*{sup}_{x\\in C_{A}(q)}f^{q}(x)=\\widehat{x}_{C_{A}(q)}.\n$$ \n\nThis is uniquely defined (because $\\mu(q)\\neq c i$ for any $c$ ) and lies in $C_{+}^{c}(q)$ , the complement of $C_{+}(q)$ , which is an open set. So there is a nonempty compact neighbourhood $\\mathcal{N}\\big(\\widehat{x}_{C_{A}(q)}\\big)$ of $\\widehat{x}_{C_{A}(q)}$ lying entirely within $C_{+}^{c}(q)$ . We established in Part (i) that a sample maximiser $\\widehat{x}_{\\mathcal{N}}(n)$ , uniquely defined in a compact neighbourhood $\\mathcal{N}$ of $C_{A}$ , is consistent for its population version $\\widehat{\\boldsymbol{x}}_{\\mathcal{N}}$ . So we reach a contradiction, because ${\\widehat{x}}_{C_{+}(q)}(n)$ is also the maximum of $f_{n}^{d}$ in $C_{A}(d)\\cap\\{x_{q+1}=\\cdot\\cdot\\cdot=x_{d}=0\\}$ , which lies in the neighbourhood $\\mathcal{N}\\big(\\widehat{x}_{C_{A}(q)}\\big)$ , WPA1 as $n\\to\\infty$ . So (6.18) holds.\nHaving established (6.18), we can infer $\\widehat{S R}_{n}^{+}(d)=\\widehat{S R}_{n}^{+}(m)$ on an event with probability exceeding $1-\\varepsilon$ , then from (6.17) we get (3.10).\nFinally, assume $i^{T}\\Sigma^{-1}\\mu<0$ . The same is true in the sample WPA1 so $i^{T}\\widehat{\\Sigma}_{n}^{-1}\\widehat{\\mu}_{n}<0$ WPA1. Restricting to sample points for which this is the case, one or more components, the last $m+1$ , $m+$ 2, . . . , $d$ say, of $\\widehat{x}_{C_{+}(d)}(n)$ are 0 (since the maximum of $f_{n}^{d}$ occurs at infinite values). Eliminate these redundant assets from the sample so we are left with all positive components in $\\widehat{x}_{C_{+}(m)}(n)$ . As argued previously this means that $i^{T}\\widehat{\\Sigma}_{n}^{-1}(m)\\widehat{\\mu}_{n}(m)>0$ and since we assumed $\\mu(m)\\neq0$ we also have $\\widehat{\\mu}_{n}(m)\\neq0$ WPA1 as $n\\rightarrow\\infty$ . Also, $\\widehat{S R}_{n}^{+}(d)=\\widehat{S R}_{n}^{+}(m)$ since eliminating redundant assets does not change the value of the Sharpe ratio. So we can follow the previous analysis to get (3.10) again. ■\n\nThis section establishes the properties of the Sharpe ratio maximization problem under the assumption that short sales are disallowed and the mean excess return vector $\\mu$ is non-zero. Key results include the uniqueness of the maximizer and the consistency of the sample estimator."
  },
  {
    "qid": "econ-empirical-1384-4-0-2",
    "question": "3) Prove that the least squares estimate of $\\rho$ is biased downward with probability close to 1 in the Monte Carlo experiment. How does this affect the coverage of the delta method and bootstrap?",
    "gold_answer": "1. The least squares estimator $\\hat{\\rho}$ is known to be biased downward in autoregressive models, especially near the unit root. \\n2. This bias increases with the proximity of $\\rho$ to 1. \\n3. The delta method and bootstrap rely on $\\hat{\\rho}$, so their confidence intervals are centered around biased estimates. \\n4. This leads to intervals that are too narrow and shifted, resulting in under-coverage. \\n5. The bias-adjusted bootstrap attempts to correct this but does not fully resolve the issue.",
    "question_context": "The model considered is (1.1) with a constant and trend $y_{t}=\\mu_{1}+\\mu_{2}t+z_{t}$ where $(1-b L)(1-\\rho L)z_{t}=\\varepsilon_{t}$ and $\\varepsilon_{t}$ is id $\\Nu(0,\\sigma^{2})$ with $\\sigma^{2}=1$. The parameters $\\mu_{1}$ and $\\mu_{2}$ are both set to O, but this is not imposed in estimation. The parameter $\\rho$ is 1, .97, or .9, and $b$ is $-.3,0$, or .3. The sample size is 100, and 1,000 replications were conducted for each experiment.\nThe new method has actual coverage that is nearly always above the $90\\%$ nominal level. All the other methods have coverage that can be well below the nominal level, especially at longer lead times and especially if $\\rho=1$.\nThe average width of confidence intervals for impulse responses at long lead times, generated by the new method, is close to $\\sigma/(1-b)$. If the series does have a unit root, then the impulse response at an infinite lead time is $\\sigma/(1-b)$. If it does not have a unit root, then the impulse response at an infinite lead time is 0.\n\nThis section compares the performance of various methods (delta method, bootstrap, bias-adjusted bootstrap, Bayesian method, and a new method) in a univariate autoregression model. The model includes a constant and trend, with specific parameter settings and sample sizes."
  },
  {
    "qid": "econ-empirical-483-1-0-2",
    "question": "3) Using Theorem 1, show that the interval equation $(E - A^I)x^I = y^I$ can be replaced by the linear system $Hu = v$ for nonnegative vectors $u, v$.",
    "gold_answer": "1. From Theorem 1, $\\underline{y} = (E - \\bar{A}_2)\\underline{x} - \\bar{A}_1 \\bar{x}$ and $\\bar{y} = F\\underline{y} + v$. \\\\ 2. Define $u = \\bar{x} - \\underline{x}$ and $v = \\bar{y} - F\\underline{y}$. \\\\ 3. Substitute into the expressions for $\\underline{y}$ and $\\bar{y}$ to obtain $Hu = v$. \\\\ 4. The nonnegativity of $u$ and $v$ follows from the nonnegativity of $H$ and the intervals.",
    "question_context": "The basic input-output equation is given by $(E-A)x=y$, where $A$ is an interval matrix $A^I=[\\underline{A}, \\bar{A}]$, and $x^I, y^I$ are interval vectors. The model assumes $A \\geq 0$ and $\\sum_{i=1}^n \\bar{a}_{ij} < 1$ for all $j$.\nThe matrix $F = (E - \\underline{A})(E - \\bar{A})^{-1}$ is introduced, satisfying $F \\geq E$. The matrix $H = E - \\underline{A}_2 + F \\bar{A}_1$ is defined, where $\\underline{A}_2$ and $\\bar{A}_1$ are derived from $\\underline{A}$ and $\\bar{A}$.\nTheorem 1 states that nonnegative intervals $x^I = [\\underline{x}, \\bar{x}]$ and $y^I = [\\underline{y}, \\bar{y}]$ satisfy $(E - A^I)x^I = y^I$ if and only if they can be expressed in terms of nonnegative vectors $u, v$ satisfying $Hu = v$.\nTheorem 2 provides a necessary and sufficient condition for the existence of a nonnegative solution $y^I$ given $x^I$, requiring $(E - \\bar{A}_2)\\underline{x} \\geq \\bar{A}_1 \\bar{x}$.\nTheorem 3 states that given a nonnegative interval $y^I$, a nonnegative solution $x^I$ exists if and only if $H^{-1}\\bar{y} \\geq H^{-1}F\\underline{y}$, assuming $H$ is nonsingular.\n\nThis section introduces a modified input-output model where coefficients are given as intervals, addressing problems of finding interval outputs given inputs and vice versa. Key assumptions include nonnegativity and column sum constraints on the input-output matrix."
  },
  {
    "qid": "econ-empirical-1532-0-0-0",
    "question": "1) Formally derive the implications of Hicks’s induced innovation hypothesis in the context of energy-using consumer durables, incorporating both energy prices and government regulations.",
    "gold_answer": "1. Start with Hicks’s original hypothesis: \\( \\frac{\\partial I}{\\partial p_e} > 0 \\), where \\( I \\) is innovation and \\( p_e \\) is energy price.  \n2. Extend to include regulations \\( s \\): \\( I = f(p_e, s) \\). \n3. Show that \\( \\frac{\\partial I}{\\partial s} \\) can be positive if regulations act as a complementary inducement mechanism.",
    "question_context": "We develop a methodology for testing Hicks’s induced innovation hypothesis by estimating a product-characteristics model of energy-using consumer durables, augmenting the hypothesis to allow for the influence of government regulations.\nThe evidence suggests that (i) the rate of overall innovation was independent of energy prices and regulations; (ii) the direction of innovation was responsive to energy price changes for some products but not for others; (iii) energy price changes induced changes in the subset of technically feasible models that were offered for sale; (iv) this responsiveness increased substantially during the period after energy-efficiency product labeling was required; and (v) nonetheless, a sizable portion of efficiency improvements were autonomous.\n\nThe induced innovation hypothesis, as proposed by Hicks, suggests that changes in relative prices of factors of production spur inventions aimed at economizing the use of the more expensive factor. This paper extends this hypothesis to include the influence of government regulations on energy-using consumer durables."
  },
  {
    "qid": "econ-empirical-1083-1-0-1",
    "question": "2) The SEE sample of workers (n=3,561) overrepresents women, whites, and persons with postsecondary schooling relative to the U.S. worker population. Propose a weighting scheme to adjust for this bias, assuming the true population proportions are known.",
    "gold_answer": "1. **Define true population proportions** for each demographic group (e.g., women: \\( p_{\\text{true}} = 0.47 \\)).  \n2. **Calculate sample proportions** (e.g., women: \\( p_{\\text{sample}} = 0.492 \\)).  \n3. **Compute weights**: \\( w_i = \\frac{p_{\\text{true}}}{p_{\\text{sample}}} \\) for each group.  \n4. **Apply weights** to responses to align with population distribution.",
    "question_context": "The WISCON interviewers attempt contact with a sample of telephone numbers representative of currently working residential telephone numbers in the continental United States, including both listed and nonlisted numbers.\nThe effective response rate (the ratio of interviews to potential residential phone numbers called) is a bit more than 50 percent.\nThe analysis in this paper focuses on the 3,561 SEE respondents during this period who were working at the time of the interview, who responded to all three job-expectations questions, and who provided basic demographic and schooling data.\n\nThe SEE is a repeated cross-sectional survey administered since 1994 as a periodic module in WISCON, a continuous national telephone survey conducted by the University of Wisconsin Survey Center. The survey collects data on labor market experiences, demographics, and attitudes, with no follow-up interviews."
  },
  {
    "qid": "econ-empirical-687-2-0-3",
    "question": "4) Generalize the $K^{2}$-fold multiway cross-fitting algorithm to the case of $\\alpha$-way clustering for $\\alpha>2$. What would be the key changes in the algorithm and the estimator?",
    "gold_answer": "1. For $\\alpha$-way clustering, the algorithm generalizes to $K^{\\alpha}$-fold multiway cross-fitting.\n2. The partitioning would involve splitting each of the $\\alpha$ cluster dimensions into $K$ parts, resulting in $K^{\\alpha}$ partitioning tuples.\n3. The nuisance parameter estimate $\\widehat{\\eta}_{k_1,\\dots,k_\\alpha}$ would be computed using the subsample excluding the $k_i$-th part in each dimension $i$.\n4. The multiway DML estimator would solve:\n   $$\\frac{1}{K^{\\alpha}}\\sum_{(k_1,\\dots,k_\\alpha)\\in[K]^{\\alpha}}\\mathbb{E}_{n,k_1,\\dots,k_\\alpha}[\\psi(W;\\widetilde{\\theta},\\widehat{\\eta}_{k_1,\\dots,k_\\alpha})]=0.$$\n5. The asymptotic variance estimator would similarly account for $\\alpha$-way cluster dependence.",
    "question_context": "For any $r\\in\\mathbb{N},$ we use the notation $[r]=\\{1,\\ldots,r\\}$ . With a fixed positive integer $K>1$ , randomly partition $[N]$ into $K$ parts $\\{I_{1},\\dots,I_{K}\\}$ and $[M]$ into $K$ parts $\\{J_{1},\\dots,J_{K}\\}$ . For each $(k,\\ell)\\in[K]^{2}$ , obtain an estimate $\\widehat{\\eta}_{k\\ell}=\\widehat{\\eta}\\left((W_{i j})_{(i,j)\\in([N]\\setminus I_{k})\\times([M]\\setminus J_{\\ell})}\\right)$ of the nuisance parameter $\\eta$ by some machine learning method using only the subsample of those observations with multiway indices $(i,j)$ in $([N]\\setminus I_{k})\\times([M]\\setminus J_{\\ell})$.\nIn turn, we define $\\widetilde{\\theta}$ , the multiway double/debiased machine learning (multiway DML) estimator for $\\theta_{0}$ , as the solution to $\\frac{1}{K^{2}}\\sum_{(k,\\ell)\\in[K]^{2}}\\mathbb{E}_{n,k\\ell}[\\psi(W;\\widetilde{\\theta},\\widehat{\\eta}_{k\\ell})]=0$, where $\\mathbb{E}_{{n},k\\ell}[f(W)]=\\frac{\\widecheck{1}}{|I_{k}||J_{\\ell}|}\\sum_{(i,j)\\in I_{k}\\times J_{\\ell}}f(W_{i j})$.\nWe propose to estimate the asymptotic variance of $\\overline{{\\sqrt{C}}}(\\widetilde{\\theta}-\\theta_{0})$ by ${\\widehat{\\sigma}}^{2}={\\widehat{J}}^{-1}{\\widehat{\\Gamma}}({\\widehat{J}}^{-1})^{\\prime}$, where $\\widehat{\\Gamma}$ and $\\widehat{J}$ are given by specific formulas accounting for multiway cluster dependence.\nFor a $d_{\\theta}$ - dimensional vector $r,$ the $(1-a)$ confidence interval for the linear functional $r^{\\prime}\\theta_{0}$ can be constructed by $\\mathrm{CI}_{a}:=[r^{\\prime}{\\widetilde{\\theta}}\\pm\\Phi^{-1}(1-a/2)\\sqrt{r^{\\prime}{\\widehat{\\sigma}}^{2}r/\\underline{{C}}}]$, where $\\Phi$ denotes the standard normal CDF.\n\nThis section introduces a novel multiway cross-fitting algorithm and the multiway cluster robust DML, addressing the limitations of conventional cross-fitting under multiway clustering. It details the mathematical framework, estimation procedures, and variance estimation for the multiway DML estimator."
  },
  {
    "qid": "econ-empirical-1012-0-0-3",
    "question": "4) Analyze the power of the supTS and expTS tests against alternatives in Model (6.2), explaining the empirical results in Table III for the extended GNP series.",
    "gold_answer": "1. **Power Calculation**: For $H_1: \\mu_1, \\sigma_1 \\neq 0$, simulate data under calibrated parameters. \\n2. **Results**: \\n   - **supTS**: Power = 86.2% at 5% level. \\n   - **expTS**: Power = 89.8% at 5% level. \\n3. **Table III**: Rejection for switching mean+variance ($p = 0.00$) aligns with volatility decline post-1980s.",
    "question_context": "The parameters are constant under the null hypothesis, whereas they are random and weakly dependent under the alternative. The model of interest is very general and includes as a special case the state space models and the Markov switching model initially introduced by Baum and Petrie (1966) and further studied by Hamilton (1989).\nThe first feature is that the hyperparameters that enter in the dynamics of the random coefficients are not identified under the null hypothesis. As a result, the usual tests, like the likelihood ratio test, do not have a chi-squared distribution. The second feature is that the information matrix is singular under the null hypothesis.\nThe asymptotic local optimality discussed below shows that there do not exist tests with nontrivial power against local alternatives that converge faster than $T^{-1/4}$ . Therefore, it is necessary to consider this rate of convergence when discussing power.\nOur test is based on functionals of the first two derivatives of the likelihood evaluated under the null and the autocorrelations of the process describing the random parameters. It can be viewed as a time-series extension of White's (1982) information matrix test and shares some of its advantages.\n\nThis paper proposes a class of optimal tests for the constancy of parameters in random coefficients models, covering Hamilton's models where parameters vary according to an unobservable Markov chain. The testing procedure addresses nonlinear models where random coefficients need not be Markov, with contiguous alternatives converging to the null hypothesis at a rate slower than the standard rate."
  },
  {
    "qid": "econ-empirical-592-4-0-3",
    "question": "4) Discuss the 'common support problem' in the context of non-experimental evaluation methods, and explain how it contributes to selection bias in cross-sectional comparisons.",
    "gold_answer": "",
    "question_context": "The results for adult males and adult females show that the coefficients for all eight of the labour force status pattern indicators are statistically significantly different from zero. For both groups, the smallest coefficient is on the indicator variable for those persistently out of the labour force; their participation probabilities differ the least from those of the persistently employed, who constitute the omitted group.\nOlder adults have a lower conditional probability of participation which is consistent with the view that returns to training decline with age. The effect of completed schooling on the probability of participation shows a hill-shaped pattern, with adults with fewer than 10 or more than 15 years of schooling having differentially low estimated participation probabilities.\nCurrently married adults of both sexes are relatively less likely to participate than those who have never married, while those whose marriage ended more than two years ago are relatively more likely to participate. The effect is especially strong for adult women for whom training programmes often provide a bridge back into the labour force following divorce.\nThe cross-section matching estimator assumes that conditional on a vector of observed characteristics \\( \\mathbf{X} \\), \\( D \\) is independent of the non-participation outcome \\( Y_{0a} \\). In formal terms, it is assumed that \\( (Y_{0a} \\perp\\!\\!\\!\\perp D) | \\mathbf{X} \\), where \\( \\perp\\!\\!\\!\\perp \\) denotes independence.\nThe matching estimator for the impact of treatment on the treated is defined as: \\( \\widehat{M_{a}}(S) = \\sum_{i \\in I_{1}} [Y_{1a i} - \\sum_{j \\in I_{0}} W_{N_{0},N_{1}}(i,j) Y_{0a j}] \\), for \\( P(\\mathbf{X}) \\in S \\), where \\( Y_{1a i} \\) denotes earnings with training in the post-programme period for participant \\( i \\), \\( Y_{0a j} \\) denotes earnings without training in the post-programme period for non-participant \\( j \\), and \\( W_{N_{0},N_{1}}(i,j) \\) is the weight attached to comparison group member \\( j \\) in constructing the counterfactual outcome for participant \\( i \\).\n\nThis section presents a multivariate analysis of the determinants of participation in the JTPA programme conditional on eligibility using the data on experimental controls and ENPs from the National JTPA Study. The analysis reveals the central role of recent labour force status dynamics in determining programme participation, as well as the contributing role of other factors such as age, schooling, marital status, and family income."
  },
  {
    "qid": "econ-empirical-492-2-0-0",
    "question": "1) Derive the equilibrium condition $V(a^{*}(h),h,a^{*})=0$ from the given utility gain function $V(a,h,\\tilde{a})$. Explain the economic intuition behind this condition.",
    "gold_answer": "1. Start with the utility gain function: $$ V(a,h,\\tilde{a})=\\int_{0}^{\\infty}e^{-(\\rho+\\alpha)t}E[\\pi(a_{t},h_{t})|a,h,\\tilde{a}]\\mathrm{d}t-\\psi. $$\n2. At the threshold $a^{*}(h)$, the agent is indifferent between High and Low, so $V(a^{*}(h),h,a^{*})=0$.\n3. Substitute $a = a^{*}(h)$ and $\\tilde{a} = a^{*}$ into the equation: $$ \\int_{0}^{\\infty}e^{-(\\rho+\\alpha)t}E[\\pi(a_{t},h_{t})|a^{*}(h),h,a^{*}]\\mathrm{d}t = \\psi. $$\n4. Economic intuition: The expected discounted flow payoff from choosing High must equal the cost $\\psi$ for the agent to be indifferent.",
    "question_context": "We say that an agent is playing according to a threshold $a^{*}{:}[0,1]\\mapsto\\Re$ if she chooses High whenever $a_{t}>a^{*}(h_{t})$ and Low whenever $a_{t}<a^{*}(h_{t})$ . Function $a^{*}$ is an equilibrium if the strategy profile where every player plays according to $a^{*}$ is an equilibrium.\nPROPOSITION 3 (Frankel and Pauzner, 2000). Suppose $\\sigma>0$ . There is a unique rationalisable equilibrium in the model. Agents invest if $a>a^{*}(h)$ and do not invest if $a<a^{*}$ $(h)$ , where $a^{*}$ is a decreasing function.\nLet $V(a,h,{\\tilde{a}})$ be the utility gain from choosing High obtained by an agent in state $(a,h)$ that believes others will play according to threshold $\\tilde{a}$ . Then: $$ V(a,h,\\tilde{a})=\\int_{0}^{\\infty}e^{-(\\rho+{\\alpha})t}E[\\pi(a_{t},h_{t})|a,h,\\tilde{a}]{\\mathrm{d}}t-\\psi, $$ where $E[\\pi(a_{t},h_{t})|a,h,\\tilde{a}]$ denotes the expectation of $\\pi(\\boldsymbol{a}_{t},\\boldsymbol{h}_{t})$ of an agent in state $(a,h)$ that believes others will play according to a threshold $\\tilde{a}$ .\nPROPOSITION 4. Optimal policy: (i) [Optimality of a constant subsidy] The planner’s solution can be implemented by a constant subsidy of $\\psi/\\theta$ whenever an agent invests. (ii) [Parallel shift of the threshold] The planner invests according to a threshold $a_{P}^{*}$ such that for any $h\\in[0,1]$ , $$ a_{P}^{*}(h)=a^{*}(h)-\\log{\\left(\\frac{\\theta}{\\theta-1}\\right)}, $$ where $a^{*}$ is the threshold for the decentralised equilibrium.\n\nThis section introduces productivity shocks into the model, leading to a unique rationalisable equilibrium where agents play according to a decreasing threshold function. The equilibrium is characterized by a threshold that depends on both fundamentals and history, with shocks affecting expectations about others' actions."
  },
  {
    "qid": "econ-empirical-1612-3-0-2",
    "question": "3) Describe the use of Gauss-Hermite quadrature in computing the expectation and variance of the price in period $t+T-1$. Provide the mathematical expressions for $\\mathcal{E}_{t+T-1}P_{t+T}$ and $\\mathcal{V}_{t+T-1}P_{t+T}$.",
    "gold_answer": "1. **Gauss-Hermite Quadrature**: This numerical integration method is used to compute expectations with respect to the random shocks $\\tilde{\\Theta}_{t+T}$. \n2. **Expectation**: \n   $$ \\mathcal{E}_{t+T-1}P_{t+T} = F_{t+T-1}^{E}(A_{t+T-1}, S_{t+T-1}, G_{t+T-1}) $$ \n3. **Variance**: \n   $$ \\mathcal{V}_{t+T-1}P_{t+T} = F_{t+T-1}^{V}(A_{t+T-1}, S_{t+T-1}, G_{t+T-1}) $$",
    "question_context": "Suppose we wish to compute $\\mathcal{E}_{t}P_{t+1}$ and $\\mathcal{V}_{t}P_{t+1}$ , the expectation and variance of price in year $t+1$ conditional on the information available in year t. Select a time horizon $T$ and make the naive assumption that private carryout in period $t+T$ will remain unchanged from the preceding period. This assumption effectively severs the intertemporal link between period $t+T$ and subsequent periods, yielding a soluble finite-horizon approximation  to  the true infinite-horizon model.\nUnder the finite-horizon assumption, it is possible to solve the equation system (1)-(2), (6)-(12) for the price that will prevail in period $t+T$ as a function of the 'state' realized in period $t+T$ The state vector consists of the preceding period's acreage planted, private stocks, and government stocks and the contemporaneous realization of the exogenous shocks.\nSince the relation between the equilibrium price and the state variables is not expressible in closed form, the price function $P_{t+T}(\\cdot)$ is approximated using a multilinear interpolative spline (deBoor, 1978).\nNext, using Gauss-Hermite quadrature, compute the expectation of (16) with respect to the random shocks $\\tilde{\\Theta}_{t+T}$ to obtain an approximation for the price expected in period $t+T-1$.\nSimilarly, calculate the second moment to obtain an approximate expression for the variance of price in period $t+T-1$.\nUsing expressions (17) and (18) as approximations for the rational price expectation and variance, solve the model for the price that will prevail in period $t+T-1$ as a function of the state variables, thus obtaining an expression similar to (16) for period $t+T-1$.\nThe procedure is repeated recursively backwards until period $t$ is reached and an approximation for the rationally expected price $\\mathcal{E}_{t}P_{t+1}$ and variance $\\mathcal{V}_{t}P_{t+1}$ are obtained.\nIn order to ensure that only current information is used in calculating future price expectations and variances, predicted rather than actual future values of exogenous variables are used in the recursion procedure.\n\nThe rational expectations equilibrium of the commodity market model is not expressible in closed form and thus must be computed numerically using recursive stochastic-dynamic programming techniques. Below, the recursive numerical solution strategy is discussed in general terms."
  },
  {
    "qid": "econ-empirical-997-5-2-1",
    "question": "8) Explain the Conley et al. (2012) sensitivity analysis. How does it quantify the robustness of IV estimates to potential instrument invalidity?",
    "gold_answer": "The Conley et al. method:\n1. **Assumes**: Instruments may be weakly correlated with the error term (e.g., \\(\\text{Corr}(Z_i, u_i) = \\delta\\)).\n2. **Estimates**: Bounds for treatment effects under varying degrees of invalidity (\\(\\delta\\)).\n3. **Interpretation**: If estimates remain significant for plausible \\(\\delta\\), the results are robust to minor violations of exclusion.",
    "question_context": "The credibility of our results and their internal validity relies on the assumption that the instruments are uncorrelated with the error term of (7). In this Section, we investigate this issue by: (i) conducting a falsification exercise using birthweight and mother’s height, and (ii) conducting a sensitivity analysis along the lines of Conley et al. (2012).\n\nThis section validates the IV assumptions by testing the exclusion restriction and conducting sensitivity analyses."
  },
  {
    "qid": "econ-empirical-1692-3-0-2",
    "question": "3) State and prove Theorem 2, which links the evil twin property to inconsistency. Use the definitions of weak learnability and evil twins in your proof.",
    "gold_answer": "1. **Theorem 2 Statement**: For any $\\delta$, $\\varepsilon^{\\prime}$, and beliefs, if $\\hat{S} \\subset S$ is weakly learnable and has the $\\varepsilon^{\\prime}$ evil twin property, then it is not $\\varepsilon^{\\prime}$ consistent.  \n2. **Proof**:  \n   - By weak learnability, player 1 can predict the path of play generated by $(s_{1}, s_{2})$ for any $s_{1} \\in \\hat{S}_{1}$.  \n   - By the evil twin property, there exists $s_{2} \\in S_{2}$ such that $s_{1}$ is not a uniform $\\varepsilon^{\\prime}$ best response to any belief $\\beta_{2}$ predicting $(s_{1}, s_{2})$.  \n   - Thus, $\\hat{S}$ cannot be $\\varepsilon^{\\prime}$ consistent, as $s_{1}$ fails to be optimal for player 1.",
    "question_context": "An action $a_{1}^{*}\\in A_{1}$ is weakly dominant if and only if, for any $a_{2}\\in A_{2}$ , $u_{1}(a_{1}^{*},a_{2})\\geq\\operatorname*{max}_{a_{1}\\in A_{1}}u_{1}(a_{1},a_{2})$. This definition is somewhat weaker than the standard one in that I do not require strict inequality for any $a_{2}$ . The definition for player 2 is similar.\nDEFINITION 8: The stage game satisfies no weak dominance (NWD) if and only if neither player has a weakly dominant action.\nPlayer 1's minmax payoff is given by $m_{1}=\\operatorname*{min}_{\\alpha_{2}\\in\\Delta(A_{2})}\\operatorname*{max}_{\\alpha_{1}\\in\\Delta(A_{1})}u_{1}(\\alpha_{1},\\alpha_{2})$. Player 1's pure action maxmin payoff is given by $M_{1}=\\operatorname*{max}_{a_{1}\\in A_{1}}\\operatorname*{min}_{a_{2}\\in A_{2}}u_{1}(a_{1},a_{2})$. The definitions for player 2 are analogous.\nDEFINITION 9: The stage game satisfies MM if and only if, for each player $i$ the pure action maxmin payoff is strictly less than the minmax payoff, $M_{i}<m_{i}$.\nTHEOREM 1: (i) Suppose that NWD holds. Then there is a $\\overline{{\\delta}}\\in(0,1]$ such that for any $\\delta\\in[0,\\overline{{\\delta}})$ there is an $\\varepsilon>0$ such that, for any $\\hat{\\Sigma}\\subset\\Sigma$ and any beliefs, if $\\hat{\\Sigma}$ satisfies pure weak learnability and $C S P$ then $\\hat{\\Sigma}$ is not e consistent. (i) Suppose that MM holds. Then for any $\\delta\\in[0,1)$ there is an $\\varepsilon>0$ such that, for any $\\hat{\\Sigma}\\subset\\Sigma$ and any beliefs, if $\\hat{\\Sigma}$ satisfies pure weak learnability and $C S P$ , then $\\hat{\\Sigma}$ is not & consistent.\nDEFINITION 10: Fix $\\delta$ and $\\varepsilon^{\\prime}$ . A pure strategy $s_{2}\\in S_{2}$ is an $\\varepsilon^{\\prime}$ evil twin of a pure strategy $s_{1}\\in S_{1}$ if and only if $s_{1}$ is not a uniform $\\varepsilon^{\\prime}$ best response to any belief $\\beta_{2}$ for which player 1 weakly learns to predict the path of play generated by $(s_{1},s_{2})$ . A similar definition holds for player 2.\nDEFINITION 11: Fix 8 and $\\varepsilon^{\\prime}$ . Then $\\hat{S}\\subset S$ has the $\\varepsilon^{\\prime}$ evil twin property if and only if for any $s_{1}\\in S_{1}$ there is a strategy $s_{2}\\in S_{2}$ that is an $\\varepsilon^{\\prime}$ evil twin of $s_{1}$ , and similarly for player 2.\nTHEOREM 2: For any $\\delta$ any $\\varepsilon^{\\prime}$ , and any beliefs, if $\\hat{S}\\subset S$ is weakly learnable and has the $\\varepsilon^{\\prime}$ evil twin property, then it is not $\\varepsilon^{\\prime}$ consistent.\nTHEOREM 3: (i) Suppose that NWD holds. Then there is an $\\varepsilon^{\\prime}>0$ and $a$ $\\overline{{\\delta}}\\in(0,1]$ such that,for any $\\delta\\in[0,\\overline{{\\delta}})$ ,if ${\\hat{S}}\\subset S$ satisfies $C S$ then $\\hat{S}$ has the $\\varepsilon^{\\prime}$ eviltwinproperty. (i) Suppose that MM holds. Then there is an $\\varepsilon^{\\prime}>0$ such that, for any $\\delta\\in[0,1)$ ,if $\\hat{S}\\subset S$ satisfies CS, then $\\hat{S}$ has the $\\varepsilon^{\\prime}$ evil twin property.\n\nThis section introduces the concept of weakly dominant actions, the no weak dominance (NWD) condition, and the minmax and pure action maxmin payoffs. It also presents the main result (Theorem 1) and the evil twin property, which are central to the paper's analysis of consistency and learnability in repeated games."
  },
  {
    "qid": "econ-empirical-1020-2-1-2",
    "question": "7) In the lottery without losses, calculate the expected value of Option B and show why a risk-neutral individual should switch to Option A when the certain amount exceeds \\$100.",
    "gold_answer": "1. **Expected Value of Option B**: \\( E[V_B] = 0.5 \\times 200 + 0.5 \\times 0 = 100 \\).\n2. **Switch Condition**: For \\( x > 100 \\), \\( V_A = x > E[V_B] \\).\n3. **Conclusion**: A risk-neutral individual maximizes expected value by switching to Option A when \\( x > 100 \\).",
    "question_context": "The trust game is a modified version of the game by Berg et al. (1995). The sender decides how much to send (\\(s\\)) to the receiver, which is tripled. The receiver then decides how much to return (\\(r\\)) to the sender.\nSubjects sent on average \\$18.82 (37.64% of their endowment). The standard deviation was \\$14.9.\nThe lottery without losses measured risk aversion by asking subjects to choose between a certain amount (Option A) and a risky lottery (Option B).\nOn average, subjects made seven risky choices, corresponding to a certainty equivalent of \\$80.\n\nThe experiments included a trust game, a social dilemma game, and a lottery without losses. The trust game involved senders and receivers, with payoffs dependent on the amounts sent and returned."
  },
  {
    "qid": "econ-empirical-204-4-0-3",
    "question": "4) Formally analyze the sustainability of fiscal policy in the text's context, using the OECD's criterion (Blanchard et al., 1990): \\( \\frac{B_t}{Y_t} \\) must not grow indefinitely.",
    "gold_answer": "1. Debt-to-GDP ratio: \\( \\frac{B_{t+1}}{Y_{t+1}} = \\frac{(1 + r)B_t + D_t}{(1 + g)Y_t} \\), where \\(g\\) is GDP growth.  \n2. Stability requires \\( \\frac{1 + r}{1 + g} < 1 \\) (i.e., \\(r < g\\)).  \n3. UK’s 1990s deficits (\\(D_t > 0\\)) risked instability if \\(r > g\\), but text implies eventual adjustment (e.g., future austerity).",
    "question_context": "Since 1989 the British economy has been affected by unusually high budget deficits. By 1995 the primary deficit was about $3\\%$ of GDP higher than behaviour over the period $\\mathrm{~I~}95\\mathrm{~I~}{-}85$ would have implied. The deficit seems to have arisen because expenditure is unusually high rather than because of any shortfall of revenue.\nOur analysis which takes into account the intertemporal budget constraints faced by the public and private sectors shows that government borrowing is like any other borrowing: it leads to a debt which has to be repaid and the surge in borrowing in the early 1990s means that future living standards are depressed as the extra debt is serviced or repaid.\n\nThe British economy has been affected by unusually high budget deficits since 1989, with the primary deficit about 3% of GDP higher than historical trends would imply. This deficit is attributed to high expenditure, particularly in social security and health, rather than revenue shortfalls. The short-term Keynesian effects include higher employment and consumption, but long-run consequences of increased public borrowing must be considered, as it depresses future living standards due to debt servicing."
  },
  {
    "qid": "econ-empirical-1029-0-0-1",
    "question": "2) Show how the optimal GMM estimator $\\hat{\\boldsymbol{\\theta}}$ is constructed using the efficient instruments $A^{*}(x)$. Derive the asymptotic variance of this estimator.",
    "gold_answer": "1. **Efficient instruments**: $A^{*}(x) = G(x)^{\\prime}\\varOmega(x)^{-}$, where $G(x) = [D(x), H(x)]$ and $\\varOmega(x) = E[\\rho(z, \\theta_0)\\rho(z, \\theta_0)^{\\prime}|x]$. \\n2. **GMM estimator**: $\\hat{\\boldsymbol{\\theta}} = \\arg\\min_{\\theta} \\hat{g}_n(\\theta)^{\\prime}W\\hat{g}_n(\\theta)$. \\n3. **Asymptotic variance**: The asymptotic variance is $(G^{\\prime}\\varOmega^{-1}G)^{-1}$, where $G = E[G(x)]$ and $\\varOmega = E[\\varOmega(x)]$.",
    "question_context": "Conditional moment restrictions can be combined through GMM estimation to construct more efficient semiparametric estimators. This paper is about attainable efficiency for such estimators. We define and use a moment tangent set, the directions of departure from the truth allowed by the moments, to characterize when the semiparametric efficiency bound can be attained. The efficiency condition is that the moment tangent set equals the model tangent set.\nThe estimators we consider are based on the conditional moment restrictions $E[\\rho_{j}(z,\\beta_{0},\\gamma_{0})|x]=0$ $(j=1,2,\\ldots)$. A finite number of these moment conditions can be used to form a GMM estimator.\nThe optimal GMM estimator is given by $\\hat{\\boldsymbol{\\theta}}=\\arg\\operatorname*{min}_{{\\boldsymbol{\\theta}}}\\hat{g}_{n}({\\boldsymbol{\\theta}})^{\\prime}W\\hat{g}_{n}({\\boldsymbol{\\theta}})$, where $W$ is a positive semidefinite matrix. The efficient instruments are $A^{*}(x)=G(x)^{\\prime}\\varOmega(x)^{-}$, where $\\varOmega(x)=E[\\rho(z,\\theta_{0})\\rho(z,\\theta_{0})^{\\prime}|x]$ and $G(x)=[D(x),H(x)]$.\n\nThis section discusses the conditions under which Generalized Method of Moments (GMM) estimators achieve semiparametric efficiency by combining conditional moment restrictions. The key concept is the moment tangent set, which characterizes allowable departures from the truth under the moment conditions."
  },
  {
    "qid": "econ-empirical-79-65-0-3",
    "question": "4) Interpret the kernel density estimates of labor supply elasticities in Figure VI. What do the distributions reveal about heterogeneity in driver behavior and the applicability of reference-dependent preferences?",
    "gold_answer": "1. Kernel density estimates show wide variation in elasticities across drivers.\n2. Few drivers (<2.1%) have elasticities < -0.25, inconsistent with strong reference dependence.\n3. Night shift drivers' distribution stochastically dominates day shift drivers', indicating higher elasticities.\n4. Heterogeneity suggests some drivers may use reference points, but most align with neoclassical behavior.\n5. The results imply that reference-dependent preferences are not the dominant model for most drivers.",
    "question_context": "The estimates for day shifts, in the left panel of Figure V, show that the probability of stopping rises with hours within income categories (reading south to north within a column). The probability of stopping also rises somewhat with income within hours categories but less sharply than the rise with hours (reading west to east within a row).\nThe estimates for night shifts, in the right panel of the figure, show a clearer pattern. The probability of stopping on night shifts rises sharply with hours within income categories but the stopping probability is roughly unaffected by income within hours categories.\nAnalyses of taxi driver labor supply by Crawford and Meng (2011) and by Agarwal et al. (2015) posit reference points in daily income and daily hours, with loss aversion to income below an income reference point and hours above an hours reference point.\n\nThe text discusses the marginal effects of hours and income on the probability of ending a shift, comparing day and night shifts. It also explores the implications of reference-dependent preferences and neoclassical models on labor supply decisions."
  },
  {
    "qid": "econ-empirical-1434-0-0-2",
    "question": "3) The paper finds that ignoring lobbying competition leads to overestimation of government welfare-mindedness. Prove mathematically why this bias occurs in the estimation framework.",
    "gold_answer": "1. The naive estimator assumes \\( L_i = 0 \\) (no competition), yielding \\( \\hat{\\alpha}_{naive} = \\frac{1 - a \\cdot \\frac{e_i t_i}{z_i}}{1 + \\frac{e_i t_i}{z_i}} \\).  \n2. With competition, the true model is \\( \\alpha = \\frac{1 - a \\cdot \\frac{e_i t_i}{z_i (1 + \\frac{L_i}{\\sum_j \\theta_{ji} L_j})}}{1 + \\frac{e_i t_i}{z_i (1 + \\frac{L_i}{\\sum_j \\theta_{ji} L_j})}} \\).  \n3. Since \\( 1 + \\frac{L_i}{\\sum_j \\theta_{ji} L_j} > 1 \\), the denominator is inflated, causing \\( \\hat{\\alpha}_{naive} > \\alpha \\).  \n4. Monte Carlo simulations show this bias can exceed 30% in plausible parameter ranges.",
    "question_context": "Competition between opposing lobbies is an important factor in the endogenous determination of trade policy. This article investigates the consequences of lobbying competition between upstream and downstream producers.\nThe theoretical structure underlying the empirical analysis is the well-known Grossman-Helpman model of trade policy determination, modified to account for the cross-sectoral use of inputs (itself a quantitatively significant phenomenon, with around 50% of manufacturing output being used by other sectors rather than in final consumption).\nOur empirical results validate the theoretical predictions. Importantly, accounting for lobbying competition also alters substantially estimates of the 'welfare-mindedness' of governments in setting trade policy.\nIn particular, manufacturers would have an incentive to lobby for lower tariffs on goods they use as inputs—in direct opposition to suppliers of these inputs, who would favor high barriers instead.\nOur empirical implementation, we use data from more than 40 countries, spanning a wide per capita income range. This is in contrast with most of the recent empirical studies on the endogenous determination of trade policy, which have focused on the United States.\n\nThis article investigates the consequences of lobbying competition between upstream and downstream producers in the endogenous determination of trade policy. The theoretical framework is based on the Grossman-Helpman model, modified to account for cross-sectoral use of inputs. Empirical results validate the theoretical predictions, showing that lobbying competition significantly alters estimates of government welfare-mindedness."
  },
  {
    "qid": "econ-empirical-1574-1-1-1",
    "question": "4) Derive the equilibrium fertility expression $n_t^*$ in (10) and analyze how it responds to changes in $T_t$, $\\pi_t$, and $\\lambda_t$.",
    "gold_answer": "1. Equilibrium fertility is: $$n_{t}^{*}= \\frac{\\gamma}{(T_{t}+\\gamma)r_{t}^{*}\\pi_{t}}\\bigl[(1-\\lambda_{t})(\\overline{{T}}_{t}-\\underline{{e}}^{u})+\\lambda_{t}(\\overline{{T}}_{t}-\\underline{{e}}^{s})\\bigr].$$  \n2. **Effect of $T_t$**: For $T_t < R$, higher $T_t$ increases $n_t^*$ (income effect). For $T_t \\geq R$, $n_t^*$ decreases (no additional income).  \n3. **Effect of $\\pi_t$**: Higher $\\pi_t$ reduces $n_t^*$ (substitution effect: fewer births needed for desired surviving offspring).  \n4. **Effect of $\\lambda_t$**: Skilled individuals ($\\lambda_t$) have fewer children due to higher $\\underline{e}^s$, reducing average fertility.",
    "question_context": "Adult longevity of generation $t$ is assumed to be increasing in the share of skilled individuals in the parent generation, $$T_{t}=\\Upsilon\\bigl(\\lambda_{t-1}\\bigr)=\\underline{{T}}+\\rho\\lambda_{t-1},$$ where $\\underline{{T}}$ is the baseline longevity that would be observed in the economy in the absence of any skilled human capital, and $\\rho>0$ reflects the scope for improvement.\nThe dynamic path is given by a sequence $\\{T_{t},x_{t},\\lambda_{t},A_{t}$ ${\\mathbf{}},{\\pi}_{t},n_{t}\\}$ for $t=[0,1,\\ldots,\\infty)$ , which results from the evolution of the nonlinear first-order dynamic system, $$\\left\\{\\begin{array}{l l}{T_{t}=\\Upsilon(\\lambda_{t-1})}\\ {x_{t}=X(x_{t-1},\\lambda_{t-1})}\\ {\\lambda_{t}=\\Lambda(T_{t},x_{t})}\\ {A_{t}=A_{t-1}\\big(1+G(\\lambda_{t-1})\\big)}\\ {\\pi_{t}=\\Pi(T_{t-1},x_{t-1},\\lambda_{t-1},A_{t-1})}\\ {n_{t}=N(T_{t},\\lambda_{t},\\pi_{t})}\\end{array}\\right..$$\n\nThis section derives equilibrium conditions and dynamic paths for fertility, human capital, and technology, highlighting nonlinear interactions between demographic and economic variables."
  },
  {
    "qid": "econ-empirical-871-2-0-0",
    "question": "1) Derive the conditions under which a beneficiary with social preferences would choose to challenge the leader's DAC transgression in a one-shot CR game.",
    "gold_answer": "1. Define the utility function of a beneficiary with social preferences: \\( U_B = \\pi_B - \\alpha \\pi_L \\), where \\( \\pi_B \\) is the beneficiary's material payoff, \\( \\pi_L \\) is the leader's payoff, and \\( \\alpha \\) is the strength of social preference.\n2. Compare the utility of challenging (\\( U_B^{challenge} \\)) and acquiescing (\\( U_B^{acquiesce} \\)).\n3. The beneficiary will challenge if \\( U_B^{challenge} > U_B^{acquiesce} \\), which simplifies to \\( \\alpha > \\frac{\\pi_B^{acquiesce} - \\pi_B^{challenge}}{\\pi_L^{challenge} - \\pi_L^{acquiesce}} \\).",
    "question_context": "When the leader practises DAC in the one-shot CR game, a beneficiary with standard (own money-maximising) preferences will always acquiesce, so the victim will also acquiesce. Therefore, DAC can eliminate the threat of joint resistance by the responders.\nIn the presence of social preferences, when DAC takes place, both (challenge, challenge) and (acquiesce, acquiesce) are Nash equilibria in the sub-game played by the two responders.\nRepetition, however, also allows the leader to use history-dependent strategies to punish the challenging beneficiary and deter CR.\nCommunication is not risky because it is not observable by the leader, so the leader cannot condition his choice on the responders’ messages.\n\nThe text discusses the CR game, focusing on the impact of social preferences, repeated interactions, and communication on the outcomes of the game. It highlights the existence of multiple equilibria and the role of social preferences in transforming the game dynamics."
  },
  {
    "qid": "econ-empirical-1451-1-1-1",
    "question": "6) Using Gruber and Owings (1996), derive the profit-maximizing condition for physicians substituting childbirth procedures. How does this relate to overtreatment in dentistry?",
    "gold_answer": "Physicians maximize $\\pi = p_c C + p_n N - c(C,N)$, where $p_c > p_n$ (caesarean vs. normal). The FOC is:\n\\[\n\\frac{\\partial \\pi}{\\partial C} = p_c - \\frac{\\partial c}{\\partial C} = 0.\n\\]\nSimilarly, dentists may overtreat if $\\frac{\\partial \\pi}{\\partial F} > 0$ (fillings vs. no treatment). Both cases show profit-driven procedure substitution.",
    "question_context": "Early empirical studies find that an increase in the number of surgeons is associated with an increase in the number of surgeries (Fuchs, 1978; Cromwell and Mitchell, 1986).\nGruber and Owings (1996) report that the decline in fertility rates in the United States in the 1970s was partly compensated for by a substitution from normal childbirth to the more profitable caesarean delivery.\nClemens and Gottlieb (2014) analyse area specific price shocks following a Medicare consolidation reform and find that areas with higher payment shocks experience significant increases in healthcare supply.\n\nThis section reviews empirical evidence on physician-induced demand (PID) and financial incentives, highlighting gaps in direct observation of physician-patient interactions."
  },
  {
    "qid": "econ-empirical-1736-0-0-2",
    "question": "3) Formally derive the optimal number of firms \\( n^* \\) for a commons under imperfect competition, expressing it in terms of the elasticity of input productivity (\\( \\epsilon_p \\)) and the price elasticity of demand (\\( \\epsilon_d \\)).",
    "gold_answer": "1. Let the production function be \\( Q = f(L) \\), where \\( L \\) is the variable input.  \n2. Under imperfect competition, the firm maximizes profit where \\( \\text{MR} \\cdot \\text{MP} = r \\).  \n3. The markup is \\( \\frac{P}{\\text{MC}} = \\frac{\\epsilon_d}{\\epsilon_d - 1} \\).  \n4. The optimal number of firms balances overconservation (from monopoly power) and underconservation (from free access):  \n   \\( n^* = \\frac{\\epsilon_p}{\\epsilon_d} \\).",
    "question_context": "The 'problem of the commons' is a frequently cited example of market failure in which exploiters' pursuit of profits does not lead to the attainment of a social or Pareto optimum.\nA free-access equilibrium induces an unrestricted number of exploiters or firms to equate the variable input's average product, instead of its marginal product, to the input's real rental rate; hence, the rents of the variable input are driven to zero.\nA social optimum can be achieved if a single firm exploits a commons and sells its output in a perfectly competitive market.\nThe purpose of this note is to derive an expression for the optimal number of firms exploiting a commons when the resulting output is sold in an imperfectly competitive market.\nSince demand inelasticity due to monopoly power leads to overconservation, while an increase in the number of exploiting firms typically leads to underconservation, a finite number of firms for a commons can be found corresponding to a social or Pareto optimum.\nIn particular, the optimum number of firms depends directly on the elasticity of the input productivity and inversely on the price elasticity of market demand.\n\nThe 'problem of the commons' is a frequently cited example of market failure in which exploiters' pursuit of profits does not lead to the attainment of a social or Pareto optimum. A free-access equilibrium induces an unrestricted number of exploiters or firms to equate the variable input's average product, instead of its marginal product, to the input's real rental rate; hence, the rents of the variable input are driven to zero. A social optimum can be achieved if a single firm exploits a commons and sells its output in a perfectly competitive market. The purpose of this note is to derive an expression for the optimal number of firms exploiting a commons when the resulting output is sold in an imperfectly competitive market."
  },
  {
    "qid": "econ-empirical-119-3-0-3",
    "question": "4) Analyze the three cases of π (intermediate, high, low) in Proposition 6 and explain the non-monotonicity of perfect price discrimination attainability in δ for Case 3.",
    "gold_answer": "1. **Case 1 (Intermediate π)**: PPD attainable for δ > δh∗.\n2. **Case 2 (High π)**: PPD unattainable for any δ.\n3. **Case 3 (Low π)**:\n   - For δ < δh∗: PPD unattainable.\n   - For δh∗ < δ < δl∗: PPD attainable.\n   - For δ > δl∗: PPD unattainable.\n4. **Non-monotonicity**: PPD is attainable only for intermediate δ due to the interplay of thresholds π̅l∗ and π̅h∗.",
    "question_context": "Starting by considering the comparative statics in the likelihood of persistent types, π, Figure 1 illustrates the optimal quantities as a function of π. It moreover links the distortions of these optimal quantities with the specific ranges for which different combinations of constraints are binding.\nRecalling that the parameter δ measures the relative importance of good Q versus good q, I next consider the comparative statics in the parameter δ>0. Intuitively, a small δ represents a setting in which the seller’s data mining techniques have little economic impact, because for δ small mainly good q matters for the payoffs. However, as δ grows large, good Q becomes more important, and the seller’s learning about the correlated structure becomes the main driver of payoffs.\nProposition 6. For the limit case, where δ grows unbounded, the profit-maximizing outcome coincides with perfect price discrimination if and only if π∈[π̅l∞,π̅h∞].\nThe fact that the buyer’s information rents do not vanish when the good q becomes economically insignificant depends on an informational complementarity. Indeed, from only learning whether the buyer’s type is persistent or switches, the seller cannot fully deduce the value of Θ. It is only in combination with the buyer’s ex ante private information that learning the persistence allows the seller to learn Θ perfectly.\nLemma 2. The thresholds π̅ℓ∗ and π̅h∗ are independent of ν. The threshold π̅h is increasing in ν whenever π̅h>0; it equals π̅h∗ for ν=0 and equals 1 for ν≥l/h. The threshold π̅l is increasing in ν whenever π̅l>0; it converges to π̅l∗ when ν approaches 1 and is smaller than 0 for ν<ν̅l, where ν̅l∈(0,1).\n\nThis section investigates the comparative statics of the profit-maximizing mechanism and its distortions, focusing on parameters π and δ. It explores the conditions under which perfect price discrimination is attainable and the informational complementarity between γ and θ."
  },
  {
    "qid": "econ-empirical-684-2-1-1",
    "question": "2) How does limited past information affect the estimation of dynamic labor force models?",
    "gold_answer": "1. Limited past information reduces the ability to identify higher-order dynamics.\n2. May lead to omitted variable bias if past information is correlated with current shocks.\n3. Requires stronger assumptions about the initial conditions.",
    "question_context": "Dynamic models of the labor force behavior of married women which can be estimated using limited amounts of past information. Alice Nakamura and Masao Nakamura. 27 (1985) 273-98\n\nDynamic models are used to analyze how individuals' labor force participation evolves over time, often incorporating limited past information."
  },
  {
    "qid": "econ-empirical-256-1-0-2",
    "question": "3) Derive the nonlinear least squares estimator for the vector $\\boldsymbol{\\beta}^{\\prime}=[\\delta_{1}\\dots\\delta_{r},\\omega_{0},\\omega_{1}\\dots\\omega_{s},\\phi_{1}\\dots\\phi_{p},\\theta_{1}\\dots\\theta_{q}]$ and explain the role of the auxiliary regressors $m_{t}$ and $n_{t}$.",
    "gold_answer": "1. **NLS Estimator**: The estimator is given by $\\hat{\\beta}_{N}(k+1)=\\hat{\\beta}_{N}(k)+\\alpha_{k}\\left[\\sum_{t=1}^{N}\\hat{\\xi}_{t}(k)\\hat{\\xi}_{t}^{\\prime}(k)\\right]^{-1}\\left[\\sum_{t=1}^{N}\\hat{\\xi}_{t}(k)\\hat{a}_{t}(k)\\right]$, where $\\xi_{t}(\\beta)=-\\partial a_{t}(\\beta)/\\partial\\beta$ is the gradient.\\n2. **Auxiliary Regressors**: $m_{t}=\\left[\\omega(B)/\\delta(B)\\right]x_{t-b}$ and $n_{t}=[\\theta(B)/\\phi(B)]a_{t}$ are used to simplify the computation of the gradient and ensure stationarity and ergodicity of the process $\\left\\{\\xi_{t}\\right\\}$.",
    "question_context": "Consider two stationary stochastic processes $\\left\\{y_{t},x_{t}\\right\\}$ having a crosscovariance function $\\mathbb{E}(y_{t}x_{t-k})=\\gamma_{x y}(k)$ null for every $k<b\\geqslant0$ (i.e., without feedback $y_{i}\\Rightarrow x_{i})$ and absolutely summable for $k\\geqslant b$ (i.e., $y_{\\imath},x_{\\imath}$ are jointly ergodic). Assuming zero means and gaussian distributions, these features lead to the representation.\nThe two main elements that characterize the causal action $x_{t}\\Rightarrow y_{t}$ are the predictive effect and the multiplicative impact. With respect to the representation (1) these are summarised by the parameters $A=(\\sigma_{e}^{2}-\\sigma_{a}^{2})=\\operatorname{E}[y_{i}^{2}\\mid y_{i-i};i>0]-\\operatorname{E}[y_{i}^{2}\\mid y_{i-i},x_{t-j};i>0,j\\geqslant0],$ and $g=\\left(\\frac{\\omega_{0}+\\omega_{1}+\\cdots+\\omega_{s}}{1-\\delta_{1}-\\cdots-\\delta_{r}}\\right)=\\sum_{k=0}^{\\infty}\\nu_{k},\\qquad\\nu_{k}=\\sum_{j=1}^{r}\\delta_{j}\\nu_{k-j}+\\omega_{k},$ where $A$ is the reduction in the variance of the one-step-ahead prediction error, $\\left\\{v_{k}\\right\\}$ is the sequence of dynamic multipliers, and $g$ is the steady-state gain.\nGiven the complexity of the likelihood function associated with (1), a suitable estimator for the vector $\\boldsymbol{\\beta}^{\\prime}=[\\delta_{1}\\dots\\delta_{r},\\omega_{0},\\omega_{1}\\dots\\omega_{s},\\phi_{1}\\dots\\phi_{p},\\theta_{1}\\dots\\theta_{q}]$ is that of nonlinear least squares.\nTheorem 1. Let $\\left\\{x_{t},y_{t}\\right\\}$ be ergodic processes that satisfy the stable and identified representation (1); then the estimator (3) is consistent for $\\pmb\\beta$ and, more generally, $N^{1/2}[\\hat{\\beta}_{N}(k)-\\beta]\\stackrel{\\perp}{\\longrightarrow}\\mathrm{N}[0,\\mathrm{E}(\\xi_{t}\\xi_{t}^{\\prime})^{-1}\\sigma^{2}]\\quad a s\\quad k,N\\rightarrow\\infty,$ where $\\mathbb{E}(\\xi_{t}\\xi_{t}^{\\prime})$ is block-diagonal for the independence of $\\left\\{m_{t},x_{t}\\right\\}$ and $\\left\\{{{n}_{t}},{{a}_{t}}\\right\\}$.\nCorollary 1. Under the same conditions as Theorem 1, the estimator (4) is consistent for the gain $g=g(\\beta)$ and, more generally, $N^{1/2}\\left[\\hat{g}_{N}(k)-g\\right]\\stackrel{\\mathbf{L}}{\\longrightarrow}\\mathbb{N}\\left[0,\\left(\\frac{\\hat{\\otimes}g}{\\hat{\\otimes}\\beta}\\right)^{\\prime}\\mathbb{E}(\\xi_{t}\\xi_{t}^{\\prime})^{-1}\\sigma^{2}\\left(\\frac{\\hat{\\otimes}g}{\\hat{\\otimes}\\beta}\\right)\\right]\\quad a s\\quad k,N\\rightarrow\\infty.$\n\nThe section discusses the analysis of two stationary stochastic processes with specific crosscovariance functions and their representations under certain assumptions."
  },
  {
    "qid": "econ-empirical-951-2-3-0",
    "question": "1) Analyze the forecasting performance of the four methods in the presence of big breaks, as shown in Fig. 1.",
    "gold_answer": "1. Benchmark Forecasting (observed factors) has the lowest MSE (standardized to 1).  \n2. Forecasting 1 (ignoring breaks) shows MSE increasing with break size due to misspecification.  \n3. Forecasting 2 (including break dummies) and Forecasting 3 (overestimating $r$) maintain constant MSE, as they account for breaks.  \n4. The results highlight the importance of accounting for breaks in forecasting.",
    "question_context": "The following forecasting procedures are compared in our simulation: Benchmark Forecasting: The factors $F_{t}$ are treated as observed and are used directly as predictors. Forecasting 1: We first estimate 2 factors ${\\hat{F}}_{t}$ from $X_{t}$ by PCA, which are then used as predictors. Forecasting 2: We first estimate 2 factors ${\\hat{F}}_{t}$ from $X_{t}$ by PCA, and then use ${\\hat{F}}_{t}$ and $\\hat{F}_{t}\\mathbf{1}(t>\\tau)$ as predictors. Forecasting 3: We first estimate 4 factors (replicating $r+k_{1}=$ $4)\\hat{F}_{t}$ from $X_{t}$ by PCA, which are then used as predictors.\nThe results obtained from 1000 replications are reported in Fig. 1, plotting MSEs against the different break sizes. It is clear that the MSEs of the Forecasting 1 method increases drastically with the size of the breaks, in line with our discussion in Section 3. By contrast, the Forecasting 2 and 3 procedures perform equally well and their MSEs remain constant as the break size increases.\n\nThis section examines the impact of big breaks on forecasting performance using a factor-based model."
  },
  {
    "qid": "econ-empirical-1009-1-0-1",
    "question": "2) Using the Autor-Dorn-Hanson framework, explain how the identification strategy for trade shocks exploits cross-industry and cross-CZ variation in import competition. Provide the mathematical formulation for this projection.",
    "gold_answer": "1. **Industry-Level Shock**: Let \\( \\Delta I_{i} \\) be the growth in Chinese imports in industry \\( i \\) across high-income countries, adjusted for US demand shifts.\n2. **CZ Exposure**: Let \\( s_{i,c} \\) be the share of industry \\( i \\) in CZ \\( c \\) in 1980. The CZ-specific trade shock is:\n   \\[ \\Delta T_{c} = \\sum_{i} s_{i,c} \\Delta I_{i} \\]\n3. **Identification**: Assumes \\( \\Delta I_{i} \\) is exogenous to CZ-specific demand shocks, driven by Chinese productivity growth and WTO entry.",
    "question_context": "Labour markets whose initial industry composition exposes them to rising Chinese import competition experience significant falls in employment, particularly in manufacturing and among non-college workers.\nLabour markets susceptible to computerisation due to specialisation in routine task-intensive activities instead experience occupational polarisation within manufacturing and non-manufacturing but do not experience a net employment decline.\nTrade impacts rise in the 2000s as imports accelerate, while the effect of technology appears to shift from automation of production activities in manufacturing towards computerisation of information-processing tasks in non-manufacturing.\nThe divergence that we document runs counter to perceptions that technology and trade play mutually reinforcing roles in shaping labour-market developments in rich countries.\nOur analysis reveals a surprising degree of divergence between the labour-market consequences of these two phenomena – both across industrial, occupational, geographic and demographic groups, and over time as the trajectory of these forces has evolved.\n\nThe study juxtaposes the effects of trade and technology on employment in US local labour markets between 1980 and 2007, revealing distinct impacts on employment levels and job composition across sectors, occupations, and demographic groups."
  },
  {
    "qid": "econ-empirical-1008-3-1-2",
    "question": "7) Discuss the advantages and disadvantages of using a non-linear link function $\\Lambda(\\cdot)$ in the generalized local regression distribution estimator $\\hat{\\theta}(\\mathbf{x}) = \\operatorname{argmin}_{\\theta} \\sum_{i=1}^{n} W_{i}(\\hat{F}_{i} - \\Lambda(R_{i}^{\\prime}\\theta))^{2}$.",
    "gold_answer": "1. Advantages: Non-linear links can model distributions with large support or impose local shape constraints. 2. Disadvantages: Estimation becomes more computationally intensive. 3. Theoretical analysis requires additional regularity conditions on $\\Lambda(\\cdot)$. 4. The choice of $\\Lambda(\\cdot)$ must be justified by the application context.",
    "question_context": "The local regression distribution estimator is obtained from a least squares projection of the empirical distribution function onto a local basis, where the projection puts equal weights at all observations. That is, (2) employs an $L^{2}(\\hat{F})$ -projection $\\hat{\\theta}({\\bf x})=\\underset{\\theta}{\\operatorname{argmin}}\\int\\bigg(\\hat{F}(u)-R(u-{\\bf x})^{\\prime}\\theta\\bigg)^{2}K\\bigg(\\frac{u-\\bf x}{h}\\bigg)\\mathrm{d}\\hat{F}(u)$. This representation motivates a general class of local $L^{2}$ distribution estimators given by $\\hat{\\theta}_{G}(x)=\\underset{\\theta}{\\operatorname{argmin}}\\int\\Big(\\hat{F}(u)-R(u-\\mathbf{x})^{\\prime}\\theta\\Big)^{2}K\\left(\\frac{u-\\mathbf{x}}{h}\\right)\\mathrm{d}G(u)$ for some measure G.\nThe estimator $\\ensuremath{\\hat{\\theta}}_{G}$ involves only one average, while the local regression estimator $\\hat{\\theta}$ has two layers of averages (one from the construction of the empirical distribution function, and the other from the $L^{2}(\\hat{F})$ -projection/regression). As a result, with suitable centering and scaling, the local $L^{2}$ distribution estimator, $\\ensuremath{\\hat{\\theta}}_{G}$ , can be written as the sum of a mean-zero influence function and a smoothing bias term.\n\nThe text discusses extensions of the main results, including re-weighted distribution estimators, local $L^{2}$ distribution estimators, and the incorporation of restrictions into the estimation procedure."
  },
  {
    "qid": "econ-empirical-44-1-0-2",
    "question": "3) Explain why later-entering firms have workers with higher average ability ($\\overline{\\phi}_{j} > \\overline{\\phi}_{i}$ for $j > i$) and lower costs per efficiency unit. Use the 'milking the distribution' analogy from the text.",
    "gold_answer": "1. Early firms hire workers with lower acceptance wages, leaving higher-ability workers ($w > w_{i}$) for later firms.\n2. The density $f_{i+1}(w)$ is 'enriched' for $w > w_{i}$ because $f_{i+1}(w) = \\beta f_{i}(w)$ ($\\beta > 1$) in this range.\n3. Later firms must offer higher wages $w_{j} > w_{i}$ to attract these workers.\n4. Since $\\phi(w)$ is increasing, higher $w_{j}$ implies higher $\\overline{\\phi}_{j}$.\n5. The 'milking' analogy: early firms remove 'milk' (low-$w$ workers), leaving 'cream' (high-$w$ workers) for later firms.",
    "question_context": "We assume that there is an underlying population of self-employed labor with the same observable characteristics and each worker having an acceptance wage (the lowest wage at which he would change jobs) equal to his present earnings (which are an increasing function of his labor endowments in terms of efficiency units) plus a cost of switching jobs.\nFirms enter the market sequentially. Each firm behaves myopically, knows the wage offers and hiring decisions of previous entrants, and chooses a profit-maximizing wage and scale, given the behavior of previous entrants.\nThe expected labor in efficiency units of a worker hired by firm $\\mathbf{\\chi}_{i}$ offering wage $w_{i}$ is $$\\frac{\\displaystyle\\int_{0}^{w_{i}}\\phi(\\xi)f_{i}(\\xi)d\\xi}{\\displaystyle\\int_{0}^{w_{i}}f_{i}(\\xi)d\\xi}$$ and will be denoted by $\\overline{\\phi}(w_{i})$.\nThe ith firm will minimize labor costs per efficiency unit at a wage higher than $w_{i-1}$ but lower than $w_{i-1}+K$. Since $\\overline{\\phi}_{i}/w_{i}>\\overline{\\phi}_{i-1}/w_{i-1}$ and $w_{i}>w_{i-1}$, then clearly $\\overline{\\phi}_{i}>\\overline{\\phi}_{i-1}$; thus, for all $j>i, \\overline{\\phi}_{j}>\\overline{\\phi}_{i}$.\n\nThe model describes a labor market with self-employed workers and firms entering sequentially. Workers have acceptance wages based on their labor endowments and switching costs. Firms optimize wages and hiring to minimize labor costs per efficiency unit."
  },
  {
    "qid": "econ-empirical-1116-2-0-2",
    "question": "3) Calculate the implied elasticity of school attendance for three- to four-year-olds with respect to parental English proficiency, given the 2SLS-DD estimate of 9.39 percentage points and the mean attendance rate of 37.56% (9.39 / 0.25).",
    "gold_answer": "1. **Elasticity Formula**: \\( \\eta = \\frac{\\Delta Y / Y}{\\Delta X / X} \\). \n2. **Given**: \\( \\Delta Y = 0.0939 \\), \\( Y = 0.3756 \\), \\( \\Delta X = 1 \\) (unit increase). \n3. **Calculation**: \\( \\eta = \\frac{0.0939 / 0.3756}{1} = 0.25 \\).",
    "question_context": "For 3-7-year-olds as a group, the estimates suggest that the probability of school attendance is higher for children with more English-proficient parents. This positive effect comes entirely from three- and four-year-olds. By the time the child is age five or higher, all parents are at least as likely to send their children to school, as indicated by the small or insignificant coefficients.\nThe 2SLS estimates of the effect of parental English tend to be larger than the OLS estimates, significantly so in a few cases. The estimates of the effect of English proficiency on children's early educational outcomes are overall downward biased.\nA one-unit increase in parental English-speaking ability increases the probability that three- to four-year-olds attend school by 9.39 percentage points and decreases the probability that six- to seven-year-olds are below their age-appropriate grade by 1.93 percentage points.\nFor 15- to 17-year-olds, a one-unit increase in parental English-speaking ability reduces the probability of child dropping out of high school by 1.77 percentage points and reduces the probability of being below the age-appropriate grade by 4.32 percentage points.\n\nThis section examines the impact of parental English proficiency on child educational outcomes, focusing on early educational outcomes (ages 3-7) and longer-run educational outcomes (ages 15-17). The analysis employs OLS and 2SLS estimation techniques, with instruments based on parental age-at-arrival dummies and interactions with non-English-speaking country status."
  },
  {
    "qid": "econ-empirical-296-1-0-2",
    "question": "3) Compare the conditions required for Theorems 1 and 2, explaining why Assumption 4 permits faster convergence rates for $\\epsilon_n$.",
    "gold_answer": "Key differences:\n\n- Theorem 1 requires $n\\epsilon_n/\\log n\\to\\infty$ and uses Assumption 3 (polynomial class condition).\n- Theorem 2 requires $n\\epsilon_n^{2-2\\gamma}\\to\\infty$ under stronger Assumption 4 (uniform stochastic equicontinuity).\n\nAssumption 4's $\\gamma$-Hölder condition allows control of $\\hat{G}_1(\\hat{\\theta})$ at rate $O_p((n\\epsilon_n^{2-2\\gamma})^{-1/2})$, permitting faster $\\epsilon_n\\to0$ when $\\gamma$ is large (e.g., $\\gamma=1$ for Lipschitz case).",
    "question_context": "Consider a parametric unconditional moment model defined by the sample and population moment conditions: $\\hat{g}(\\theta)=\\frac{1}{n}\\sum_{i=1}^{n} g(Y_{i},\\theta)$ and $g(\\theta)=E g(Y_{i},\\theta)$ where $g(\\theta)=0$ if and only if $\\theta=\\theta_{0}$, which lies in the interior of the parameter space $\\Theta$. The goal is to estimate $G(\\theta_{0})=\\frac{\\partial g(\\theta_{0})}{\\partial\\theta}$ using $\\bar{L_{1,p}^{\\epsilon_{n}}}\\hat{g}(\\hat{\\theta})$.\nThe error of approximating $G(\\theta_{0})$ with $L_{1,p}^{\\epsilon_{n}}\\hat{g}\\left(\\hat{\\theta}\\right)$ is decomposed into three components: $L_{1,p}^{\\epsilon_{n}}\\hat{g}(\\hat{\\theta})-G(\\theta_{0})=\\hat{G}_{1}(\\hat{\\theta})+G_{2}(\\hat{\\theta})+\\dot{G}_{3}(\\hat{\\theta})$.\nAssumption 3: For each $j=1,\\ldots,d$, consider functions $g(y,\\theta)$ contained in class $\\mathcal{F}_{j}=\\left\\{g(\\cdot,\\theta+e_{j}\\epsilon),\\theta\\in\\Theta\\right\\}$ for $\\epsilon>0$. Assume (i) All $g\\in\\mathcal{F}_{j}$ are globally bounded such that $\\|F\\|=\\operatorname*{sup}_{\\theta\\in\\Theta^{\\epsilon}} \\vert g\\left(Y_{i},\\theta\\right)\\vert<C_{1}\\ll\\infty$. (ii) The moment function is Lipschitz-continuous in mean square in some neighborhood of $\\theta_{0}$. (iii) The graphs of functions from $\\mathcal{F}_{j}$ form a polynomial class of sets.\nAssumption 2: A mean value expansion of order $2p+1$ applies to the limiting function $g\\left(\\theta\\right)$ uniformly in a neighborhood $\\mathcal{N}(\\theta_{0})$ of $\\theta_{0}$.\nTheorem 1: Suppose Assumptions 2 and 3 hold and $\\epsilon_{n}\\rightarrow0$, $n\\epsilon_{n}/\\log n\\to\\infty$ and $\\bar{d}\\left(\\hat{\\theta},\\theta_{0}\\right)=o_{p}$ (1). Then, $L_{1,p}^{\\epsilon_{n}}\\hat{g}\\left(\\hat{\\theta}\\right)\\xrightarrow{p}G\\left(\\theta_{0}\\right)$.\nAssumption 4: In addition to Assumption 3, for all sufficiently small $\\epsilon$ and all $\\theta\\in\\Theta$, $\\operatorname*{sup}_{\\boldsymbol{\\theta}^{\\prime},\\boldsymbol{\\theta}\\in\\mathcal{N}(\\boldsymbol{\\theta_{0}}),d\\left(\\boldsymbol{\\theta},\\boldsymbol{\\theta^{\\prime}}\\right)\\leq\\delta}\\left\\vert\\mathbb{G}_{n}\\left(\\boldsymbol{\\theta^{\\prime}}\\right)-\\mathbb{G}_{n}\\left(\\boldsymbol{\\theta}\\right)\\right\\vert\\lesssim\\phi_{n}\\left(\\delta\\right)$.\nTheorem 2: Suppose Assumptions 2 and 4 hold and $\\epsilon_{n}\\rightarrow0,n\\epsilon_{n}^{2-2\\gamma} \\rightarrow\\infty$ and $d\\left(\\hat{\\theta},\\theta_{0}\\right)=o_{p}$ (1). Then, $L_{1,p}^{\\epsilon_{n}}\\hat{g}\\left(\\hat{\\theta}\\right)\\xrightarrow{p}G\\left(\\theta_{0}\\right)$.\n\nThis section provides conditions on the step size for consistent derivative estimation, focusing on the unconditional parametric case. The goal is to estimate the Jacobian matrix using numerical differentiation, with weaker conditions than previously established."
  },
  {
    "qid": "econ-empirical-492-4-1-1",
    "question": "4) Show the existence of $\\underset{\\sim}{a}$ such that for every $a_{0} < \\underset{\\sim}{a}$, the relative gain of choosing regime Low is negative.",
    "gold_answer": "1. **Symmetric Argument**: Analogous to the High regime, evaluate the gain for the Low regime. \\n2. **Threshold $\\underset{\\sim}{a}$**: Solve for $a_{0}$ where the gain equals zero. For $a_{0} < \\underset{\\sim}{a}$, the gain is negative.",
    "question_context": "Consider an agent deciding at some state $(a_{0},h_{0})$ that expects $h_{t}=0$ , for every $t\\geq0$ , with probability one. Her relative gain of choosing regime High is given by: \\n$$ \\underline{{\\mathcal{U}}}(a_{0})=x_{L}^{\\frac{1}{\\theta}}\\Big(x_{H}^{\\frac{\\theta-1}{\\theta}}-x_{L}^{\\frac{\\theta-1}{\\theta}}\\Big)\\int_{0}^{\\infty}e^{-(\\rho+\\alpha)t}E_{0}(e^{a_{t}})\\mathrm{d}t-\\psi. $$\nSince $a_{t}|a_{0}\\sim N(a_{0},\\sigma^{2}t)$ , $E_{0}(e^{a_{t}})=e^{a_{0}+0.5\\sigma^{2}t}$ . Therefore, \\n$$ \\underline{{\\mathcal{U}}}(a_{0})=x_{L}^{\\frac{1}{\\theta}}\\Big(x_{H}^{\\frac{\\theta-1}{\\theta}}-x_{L}^{\\frac{\\theta-1}{\\theta}}\\Big)e^{a_{0}}\\int_{0}^{\\infty}e^{-(\\rho+\\alpha-0.5\\sigma^{2})t}\\mathrm{d}t-\\psi, $$\n\nProposition 3 involves showing that for sufficiently high or low $a$, choosing the High or Low regime is optimal regardless of others' actions."
  },
  {
    "qid": "econ-empirical-348-4-0-2",
    "question": "3) Critically evaluate the proposition that undergraduate study should be based on a process of critical analysis facilitated by contact with research staff. What are the empirical implications of this proposition?",
    "gold_answer": "1. **Proposition**: Contact with research staff enhances critical analysis skills.\n2. **Empirical Implications**:\n   - Measure student outcomes (e.g., critical thinking scores) in institutions with high vs. low research activity.\n   - Control for student quality and institutional resources.\n3. **Challenges**: Endogeneity (research-active institutions may attract better students) and measurement of 'critical analysis'.\n4. **Conclusion**: Empirical studies must address these challenges to validate the proposition.",
    "question_context": "The present dual funding system will be scrutinised. Some argue that it may be more efficient to allocate all research money via the Research Councils on a competitive basis. However this raises the question of whether the Research Councils are presently allocating their funds efficiently.\nWhatever the basis of selective funding, there may be economies of scale for research within institutions. This would suggest that some will continue to build research centres of excellence.\nIt is commonly argued however, that teaching and research are complementary. One proposition is that under-graduate study should be based on a process of critical analysis, best achieved if students have contact with research staff who are undertaking ground breaking work in their fields.\nMore immediately however, the costs and benefits of the present system of the Research Assessment Exercises have to be evaluated. Its merits in terms of making selective funding feasible have to be counterbalanced by its possible distortionary effect on the willingness of researchers to undertake risky projects and also on the academic hiring market prior to the census date of assessment.\n\nThe text discusses the restructuring of the financing of higher education, focusing on the role and funding of research. It highlights the trend towards selective funding, the assessment of research activity, and the potential economies of scale. It also examines the dual funding system, the efficiency of Research Councils, and the implications of different funding models on the structure of higher education."
  },
  {
    "qid": "econ-empirical-1425-1-0-0",
    "question": "1) Derive the expected lifetime utility for an individual who chooses to marry in the first period, considering the possibility of divorce and remarriage in the second period. Assume the discount factor is $\\delta$.",
    "gold_answer": "1. **First Period Utility**: If the individual marries in the first period, their utility is $u_1 = 2y + \\theta$.  \n2. **Second Period Utility**:  \n   - If they remain married: $u_2 = 2y + \\theta$.  \n   - If they divorce:  \n     - With probability $\\phi(d)$, they remarry, yielding $u_2 = 2y + \\theta'$ (new match quality $\\theta'$).  \n     - With probability $1 - \\phi(d)$, they remain single, yielding $u_2 = y$.  \n3. **Expected Second Period Utility**:  \n   $$E[u_2] = \\phi(d) \\cdot (2y + E[\\theta']) + (1 - \\phi(d)) \\cdot y = \\phi(d) \\cdot 2y + y (1 - \\phi(d)) = y (1 + \\phi(d))$$  \n   (since $E[\\theta'] = 0$).  \n4. **Lifetime Utility**:  \n   $$U = u_1 + \\delta E[u_2] = 2y + \\theta + \\delta y (1 + \\phi(d))$$",
    "question_context": "We consider homogenous population, where all individuals have the same income, $y$ , and the same preferences. Utility is linear in the single consumption good, and monetary and non-monetary considerations are additive. The consumption good is publicly consumed within the household; therefore, the per period utility of each married person is $u=2y+\\theta$, where $\\theta$ represents the quality of the match. In contrast, the utility of an unmarried person is $u=y$.\nThe quality of match, $\\theta$ , is an independent draw from a given symmetric distribution $F(\\theta)$ with zero mean. By assumption, the two partners of a given marriage have the same evaluation of the match quality and successive draws of $\\theta$ by a given agent in different marriages are independent.\nIndividuals live two periods and marriage is defined as a commitment that must last for at least one period, without 'search on the job'. At the beginning of period 1, each single person meets another single from the population of the opposite sex of the same age and the matched partners decide whether to form a marriage union. At the end of period 1, the quality of match is revealed. Based on the information on the quality of match, agents decide at the beginning of the second period whether to remain married or to divorce their current partner.\nThere are equal numbers of males and females in each cohort. In the first period, every agent, when matched with a single agent of the opposite sex, chooses to marry. Following divorce, each partner can only remarry another divorced person from the same cohort. However, the search process involves frictions. Remarriage is neither immediate nor certain: after a divorce, agents may fail to meet an eligible new mate and remain single.\nA key ingredient of the model is that the probability of remarriage rises with the average divorce rate in the population: Remarriage is easier, the larger the number of singles around. We thus assume that the probability of finding an eligible new mate, denoted by $m$ , is an increasing function of the divorce rate in the population, denoted by $d$: $m=\\phi(d),\\mathrm{with}\\phi^{\\prime}>0$. Because couples that meet always remarry, the remarriage probability is $p=m=\\phi(d)$.\n\nThis section outlines the foundational assumptions of the model, including homogeneous population characteristics, utility specifications, timing of decisions, and matching mechanisms in the marriage market."
  },
  {
    "qid": "econ-empirical-1034-0-0-3",
    "question": "4) Propose a maximum likelihood framework for jointly modeling conditional means and conditional variances in a causality test, and discuss the challenges associated with estimating multivariate ARCH models.",
    "gold_answer": "1. **Model Specification**: The log-likelihood function for a multivariate ARCH model is \\( \\mathcal{L}(\\theta) = -\\frac{1}{2} \\sum_{t=1}^T \\left( \\log |H_t| + \\epsilon_t' H_t^{-1} \\epsilon_t \\right) \\), where \\( H_t \\) is the conditional covariance matrix.\n2. **Estimation Challenges**: \n   - **Dimensionality**: The number of parameters grows rapidly with the number of time series and lags.\n   - **Numerical Optimization**: The likelihood function may be non-convex, leading to convergence issues.\n3. **Alternative**: Cheung and Ng (1996) propose univariate tests to circumvent these challenges, but at the cost of losing multivariate information.",
    "question_context": "Results of Monte Carlo simulations suggest that conclusions drawn from least squares causality tests may lead to an erroneous claim that a statistically significant causal relation exists. Misleading inference associated with least squares may be traced to two explanations. First, because the set of regressors in a VAR includes lagged-dependent variables, least-squares standard errors are not consistent and may not support correct statistical inference (Engle et al., 1985). Consequently, a logical way to proceed is to base inference on a heteroskedasticity and autocorrelation consistent (HAC) covariance matrix. The use of a HAC covariance matrix, however, may not improve inference if there is 'considerable' temporal dependence because kernel-based HAC covariance estimators have test statistic values larger (in absolute terms) than that implied by the limiting distribution (Andrews and Monahan, 1992). Hence, the null hypothesis of no causality is rejected too often.\nA second explanation for the unsatisfactory performance of least-squares causality tests traces to a failure to adequately differentiate between causality in mean and causality in variance. This could prove particularly troublesome in practice, especially if interest is directed at asset pricing issues as Engle et al. (1990), Cheung and Ng (1996), and Lin (1997) document instances of volatility spillovers or causality in variance. Thus, if the poor performance of least-squares causality tests traces to an inability to differentiate between causality in mean and causality in variance, then causality tests are best based on a complete empirical specification of the conditional means and conditional variances.\n\nThis paper investigates the reliability of causality tests based on least squares when conditional heteroskedasticity exists. It highlights the issues of size distortion and the inability to discriminate between causality in mean and causality in variance."
  },
  {
    "qid": "econ-empirical-1432-4-1-1",
    "question": "6) Explain the methodology for generating the 1,524 out-of-sample forecasts in Table 3 and why this approach is used.",
    "gold_answer": "1. **Methodology**:  \n   - Split the data into four quarters.  \n   - Use three quarters for estimation and one for forecasting, rotating to cover all combinations (e.g., Q1-3 for estimation, Q4 for forecasting; Q2-4 for estimation, Q1 for forecasting).  \n   - This yields 4 × 381 = 1,524 out-of-sample observations.  \n2. **Rationale**:  \n   - Ensures sufficient regime-switching observations for accurate estimation.  \n   - Addresses power issues by maximizing out-of-sample data.  \n   - Mitigates overfitting by testing across multiple subsamples.",
    "question_context": "The forecast under consideration is $\\widehat{E}_{t-1}\\{S_{\\tau}\\}$, the expected exchange rate at some future time $\\tau$ using information at time $t-1$.\nWe evaluate the forecasts using the mean squared error (MSE) criterion, $\\frac{1}{T}\\sum_{t=1}^{T}(S_{\\tau}-\\widehat{E}_{t-1}\\{S_{\\tau}\\})^{2}$.\nThe percentage of correctly forecasted exchange rate changes is presented in the right panel of Table 3. The regime-switching model predicts the correct direction in more than $50\\%$ of cases, particularly at the 1-week and 1-month horizons.\n\nThis section evaluates the out-of-sample forecasting performance of regime-switching models compared to random walks, focusing on mean squared error (MSE) and directional accuracy."
  },
  {
    "qid": "econ-empirical-866-2-1-0",
    "question": "3) Derive the moment conditions for the AR(2) model when $\\alpha_{1} + \\alpha_{2} = 1$ and explain the implications for identification.",
    "gold_answer": "1. **Unit Root Condition**: When $\\alpha_{1} + \\alpha_{2} = 1$, the AR polynomial contains a unit root, leading to underidentification. \\n2. **Moment Conditions**: The Arellano and Bond (1991) moment conditions become $E[Y_{i t-j}(\\Delta Y_{i t} - \\gamma \\Delta Y_{i t-1})] = 0$ for $j \\geq 1, t \\geq 2$. \\n3. **Identification**: The parameters lie on the line $\\alpha_{2} = \\gamma^2 - \\gamma \\alpha_{1}$, forming an uncountable set of solutions, rendering the model underidentified.",
    "question_context": "Consider the following univariate $\\mathrm{AR}(2)^{12}$ model with individual specific intercepts: \\n$$\\begin{array}{r}{(Y_{i t+2}-\\eta_{i})-\\alpha_{1}(Y_{i t+1}-\\eta_{i})-\\alpha_{2}(Y_{i t}-\\eta_{i})=v_{i t+2},}\\ {E(v_{i t+2}|Y_{i1},\\ldots,Y_{i t+1};\\eta_{i})=0,\\qquad}\\ {V(v_{i t+2}|Y_{i1},\\ldots,Y_{i t+1};\\eta_{i})=\\sigma_{t+2}^{2},}\\end{array}$$ \\nwhere the expectations are taken by averaging across individuals.\n\nThis section applies the theoretical framework to dynamic panel data models, focusing on identification and estimation under various conditions."
  },
  {
    "qid": "econ-empirical-1060-3-0-0",
    "question": "1) Derive the limiting behavior of the test statistic under the first class of alternatives where $\\operatorname*{lim}_{N_{A},N_{B}\\to+\\infty}\\left(\\overline{{F}}_{B,N_{B}}(s)-\\underline{{F}}_{A,N_{A}}(s)\\right)|\\eta_{N_{A},N_{B}}<0,\\forall s\\in\\mathcal{S}^{\\circ}$. Explain the role of decreasing design-variance in driving this limit.",
    "gold_answer": "1. **Limit Derivation**: Under the first class, the difference $\\overline{F}_{B,N_B}(s) - \\underline{F}_{A,N_A}(s)$ remains negative as $N_A, N_B \\to \\infty$. The limit is driven by Condition (ii) of Assumption 2, which ensures the design-variance decreases.  \n2. **Role of Design-Variance**: The decreasing variance implies the estimator $\\widehat{F}_B(s) - \\widehat{F}_A(s)$ concentrates around its mean, ensuring the limit holds uniformly over $s \\in \\mathcal{S}^\\circ$.  \n3. **Mathematically**: The condition $\\eta_{N_A,N_B} < 0$ ensures the test statistic diverges to $-\\infty$, rejecting the null hypothesis $H_0^1$.",
    "question_context": "The first class of examples of such a sequence of populations are those that satisfy $$\\operatorname*{lim}_{N_{A},N_{B}\\to+\\infty}\\left(\\overline{{F}}_{B,N_{B}}(s)-\\underline{{F}}_{A,N_{A}}(s)\\right)|\\eta_{N_{A},N_{B}}<0,\\forall s\\in\\mathcal{S}^{\\circ}.$$\nA second class of examples are those sequences that have a subsequence {ΠNA,m,NB,m}m+∞1 where for at least one $s\\in S^{\\circ}$ , $(\\overline{{F}}_{B,N_{B,m}}(s)-\\underline{{F}}_{A,N_{A,m}}(s))\\mid\\prod_{N_{A,m},N_{B,m}}\\uparrow0$ as $m\\to+\\infty$ and $$\\operatorname*{lim}_{m\\rightarrow+\\infty}\\frac{(\\overline{{F}}_{B,N_{B,m}}(s)-\\underline{{F}}_{A,N_{A,m}}(s))}{\\sqrt{\\mathrm{VAR}\\left(\\overline{{\\widehat{F}}}_{B}(s)-\\underline{{\\widehat{F}}}_{A}(s)\\mid{\\cal{I}}_{N_{A,m},N_{B,m}}\\right)}}\\mid{\\cal{I}}_{N_{A,m},N_{B,m}}=-\\infty.$$\nRegarding Part 2 of Theorem 3, which covers scenarios where $\\exists s\\in S^{\\circ}$ such that $\\tau(s)=0$ , the sequences $\\left\\{{\\Pi_{N_{A},N_{B}}}\\right\\}_{N_{A},N_{B}=1}^{+\\infty}$ converge to the boundary of $H_{0}^{1}$ . This convergence also occurs at a rate faster than $\\left[K_{N_{A},N_{B}}\\right]^{-1/2}$ , which is why $\\tau(s)=0$.\n\nTheorem 3 distinguishes between two classes of alternatives based on the behavior of sequences of finite populations. The first class involves sequences where the difference between upper and lower bounds does not tend to the boundary of the null hypothesis. The second class involves sequences that accumulate on the boundary but converge at a slower rate than specified. Part 2 of Theorem 3 covers scenarios where sequences converge to the boundary of the null hypothesis at a faster rate, leading to specific asymptotic properties of the test."
  },
  {
    "qid": "econ-empirical-54-0-0-1",
    "question": "2) Explain the trade-off in the model between concentrated ownership and control versus divided ownership and control. How does this depend on investment specificity and value-added in processing?",
    "gold_answer": "1. Concentrated ownership and control (same party has both) maximizes incentives for input search when investment specificity is high or value-added in processing is low.\n2. Divided ownership and control (foreign ownership, local input control) balances incentives when both parties' investments matter (high value-added in processing) and investment specificity is low.\n3. High specificity increases holdup costs, favoring concentrated control to mitigate underinvestment.\n4. Empirical evidence shows divided control is more common in southern coastal provinces (low specificity) and high-value-added industries.",
    "question_context": "We develop a simple model of international outsourcing and apply it to processing trade in China. Export processing involves a foreign firm contracting with a Chinese factory manager to assemble intermediate inputs into a final product.\nIn our model of export processing, contracts are incomplete, and parties divide gains from trade by Nash bargaining. The threat-point payoffs associated with bargaining do not fully compensate parties for their investments in human capital and depend, in standard fashion, on who has the relevant control rights (i.e., ownership of the factory or control over input decisions).\n\nThe paper discusses a model of international outsourcing applied to processing trade in China, focusing on the allocation of ownership and control between foreign firms and local managers."
  },
  {
    "qid": "econ-empirical-1712-2-1-1",
    "question": "2) Prove that the Weak Harm Principle is strictly weaker than the Harm Principle proposed by Mariotti and Veneziani (2009).",
    "gold_answer": "The Harm Principle requires that society's preferences over $u^{\\prime}$ and $v^{\\prime}$ coincide with agent $i$'s preferences whenever $i$ is the only affected agent. The Weak Harm Principle only requires that society does not reverse its strict preference. Thus, the Harm Principle implies the Weak Harm Principle, but not vice versa. For example, a SWR that satisfies the Weak Harm Principle but not the Harm Principle is the sufficientarian SWR in Example 1.",
    "question_context": "WEAK HARM PRINCIPLE. For all $u,v\\in X^{T}$, if $u\\succ v$ and if $u^{\\prime},v^{\\prime}\\in X^{T}$ are such that $u_{i}^{\\prime}<u_{i},v_{i}^{\\prime}<v_{i},$ for some agent $i\\in N$ and $u_{j}^{\\prime}=u_{j},v_{j}^{\\prime}=v_{j},$ for each agent $j\\neq i,$ then $v^{\\prime}\\nsim u^{\\prime}$ if $u_{i}^{\\prime}>v_{i}^{\\prime}$.\nThe Weak Harm Principle assigns a veto power to individuals in situations in which they suffer harm and no other agent is affected. This veto power is weak in that it only applies to certain welfare configurations.\nPROPOSITION 1. A SWR $\\succcurlyeq$ on $X^{T}$ is the leximin $s W O\\succcurlyeq_{T}{^{L M}}$ if and only if it satisfies anonymity, strong Pareto, completeness and the Weak Harm Principle.\n\nThis section introduces the Weak Harm Principle, a liberal axiom capturing non-interference in social welfare judgments. It discusses its implications and contrasts it with other principles like Hammond Equity."
  },
  {
    "qid": "econ-empirical-96-0-0-0",
    "question": "1) Formally derive the decomposition of wage inequality into worker, establishment, and assortative matching components using the AKM model. How does the covariance between worker and establishment effects contribute to overall inequality?",
    "gold_answer": "1. The AKM model specifies log wages as: \\(y_{it} = \\alpha_i + \\psi_{J(i,t)} + x_{it}'\\beta + \\epsilon_{it}\\), where \\(\\alpha_i\\) is the worker effect, \\(\\psi_{J(i,t)}\\) is the establishment effect, and \\(x_{it}\\) captures time-varying covariates.  \n2. The variance of log wages decomposes as: \\(Var(y_{it}) = Var(\\alpha_i) + Var(\\psi_{J(i,t)}) + 2Cov(\\alpha_i, \\psi_{J(i,t)}) + Var(\\epsilon_{it})\\).  \n3. The covariance term captures assortative matching: if high-wage workers increasingly sort into high-wage establishments, this amplifies inequality.",
    "question_context": "We study the role of establishment-specific wage premiums in generating recent increases in West German wage inequality. Models with additive fixed effects for workers and establishments are fit into four subintervals spanning the period from 1985 to 2009.\nOur estimates suggest that the increasing dispersion of West German wages has arisen from a combination of rising heterogeneity between workers, rising dispersion in the wage premiums at different establishments, and increasing assortativeness in the assignment of workers to plants.\n\nThe study examines the role of establishment-specific wage premiums in the rise of West German wage inequality from 1985 to 2009. It uses models with additive fixed effects for workers and establishments to decompose wage inequality into components attributable to worker heterogeneity, establishment premiums, and assortative matching."
  },
  {
    "qid": "econ-empirical-972-3-0-2",
    "question": "3) Explain how Theorem 6 can be used to identify the distribution of counterfactual outcomes $Y_{x,w} = B_0 + B_1 x + B_2^\\prime w$, including the role of Fourier inversion.",
    "gold_answer": "1. Theorem 6 identifies $\\mathcal{F}f_B(t, tx, tw)$ for $(x,w)$ satisfying (19).\\n2. The characteristic function of $Y_{x,w}$ is $\\psi_{Y_{x,w}}(t) = \\mathcal{F}f_B(t, tx, tw)$.\\n3. By Fourier inversion, the density of $Y_{x,w}$ is $f_{Y_{x,w}}(y) = \\frac{1}{2\\pi} \\int e^{-ity} \\psi_{Y_{x,w}}(t) dt$.\\n4. This requires integrating over $t$, which is feasible given the identified $\\mathcal{F}f_B$.",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nAssumption 6. There exist pairs $(x,w^{\\prime})\\in\\mathsf{s u p p}(X,W)$ for which $\\operatorname{supp}\\bigg(\\frac{x-A_{0}-A_{2}^{\\prime}w}{A_{1}}\\bigg)\\subseteq\\operatorname{supp}\\big(Z|W=w\\big)=:\\mathcal{S}_{Z,w}.$\nTheorem 6. Consider the triangular model (1) in case of a univariate Z. Impose Assumptions 1 and 5. Then, for all $t\\in\\mathbb{R}$ and all $(x,w^{\\prime})\\in$ supp (X, W ) which satisfy (19), the following holds $\\big(\\mathcal{F}f_{B}\\big)(t,t x,t w) =\\big(E|A_{1}|^{-1}\\big)^{-1}\\displaystyle\\int_{\\mathcal{S}_{Z,w}}\\mathcal{F}_{1}\\big(f_{Y|X,Z,W}\\big)(t|x,z,w)f_{X|Z,W}(x|z,w)d z.$\n\nThis section discusses the challenges and solutions for identifying models when continuous regressors have compact support, focusing on the implications for integrating out variables and the necessity of support restrictions or extrapolation assumptions."
  },
  {
    "qid": "econ-empirical-1426-0-0-3",
    "question": "4) Derive the asymptotic distribution of the normalized derivative \\( \\sqrt{m/p} \\eta^{\\prime} \\frac{\\partial \\Upsilon_p(d)}{\\partial d} \\) evaluated at \\( d^0 \\), as given by $$\\sqrt{m/p}\\eta^{\\prime}\\left.\\frac{\\partial\\Upsilon_{p}(d)}{\\partial d}\\right|_{d^{0}}\\rightarrow_{d}N(0,\\Phi_{p}\\eta^{\\prime}E\\eta).$$",
    "gold_answer": "1. Express the derivative \\( \\frac{\\partial \\Upsilon_p(d)}{\\partial d} \\) in terms of the periodogram and tapering functions.\n2. Multiply by the vector \\( \\eta \\) and normalize by \\( \\sqrt{m/p} \\).\n3. Show that the resulting expression is a martingale difference sequence.\n4. Apply a martingale CLT to obtain the limiting distribution \\( N(0, \\Phi_p \\eta^{\\prime} E \\eta) \\).",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nApplication of these procedures indicates that for the stocks in the DJIA index the logarithm of the trading volume exhibits long memory and, additionally, that it shares the same long-memory parameter with the volatility process for most of the stocks.\nThe estimated coherency between log-volume and absolute returns at zero frequency, however, is very small.\n$$\\sqrt{m}\\left.\\frac{\\partial\\Upsilon_{p}(d)}{\\partial d}\\right|_{d^{0}}\\rightarrow_{d}N(0,p\\Phi_{p}E)$$\n$$\\left.\\frac{\\partial^{2}\\Upsilon_{p}(d)}{\\partial d\\partial d^{\\prime}}\\right|_{\\tilde{d}}\\rightarrow_{p}E$$\n$$\\sqrt{m/p}\\eta^{\\prime}\\left.\\frac{\\partial\\Upsilon_{p}(d)}{\\partial d}\\right|_{d^{0}}\\rightarrow_{d}N(0,\\Phi_{p}\\eta^{\\prime}E\\eta).$$\n\nThe article examines new methods for statistical inference on long memory parameters in stock-market trading volume and volatility, avoiding detrending procedures and using tapered data for consistent estimates. Findings indicate that the logarithm of trading volume exhibits long memory and shares the same long-memory parameter with the volatility process for most stocks in the DJIA index."
  },
  {
    "qid": "econ-empirical-1695-0-1-1",
    "question": "2) Analyze the organizational challenges of compiling a volume of papers by Aumann's students, given the diversity of topics. How does the editors' categorization (e.g., 'Strategic equilibrium') address these challenges?",
    "gold_answer": "1. **Diversity**: The papers span cooperative/non-cooperative games, large economies, and information theory.  \n2. **Categorization**: The editors use thematic groups (e.g., 'Strategic equilibrium') to cluster related papers.  \n3. **Limitations**: Some papers may fit multiple categories, and the 'Other contributions' section remains heterogeneous.",
    "question_context": "Aumann's impact on the profession has been felt not only through his path-breaking work, but also through his impact on others, especially his students.\nThe papers in this volume are selected from among the more important papers of the eleven contributors. This makes for a collection of papers on widely varying topics that defies most attempts at organisation.\n\nThe text reviews a book commemorating Robert J. Aumann's contributions to game theory and economics, highlighting his influence through his students and their work."
  },
  {
    "qid": "econ-empirical-563-1-1-0",
    "question": "3) Prove that under Assumption 1, the probability of extreme reviews ($\\overline{K}$ or $-\\underline{K}$) is strictly positive. Use the CDFs of $\\theta$ and $\\zeta$.",
    "gold_answer": "1. For $r_t = \\overline{K}$: $\\mathbb{P}(u_t \\geq \\lambda_{\\overline{K}}) = \\mathbb{P}(\\theta_t + \\zeta_t + Q - p \\geq \\lambda_{\\overline{K}})$. \\n2. By Assumption 1, $\\exists \\epsilon > 0$ s.t. $\\bar{\\theta} + \\bar{\\zeta} - p > \\lambda_{\\overline{K}} + \\epsilon$. \\n3. Since $F_\\theta, F_\\zeta$ have full support, $\\mathbb{P}(\\theta_t + \\zeta_t \\geq \\lambda_{\\overline{K}} + p - Q) \\geq \\mathbb{P}(\\theta_t \\geq \\bar{\\theta} - \\delta, \\zeta_t \\geq \\bar{\\zeta} - \\delta) > 0$ for small $\\delta$. \\n4. Analogous for $r_t = -\\underline{K}$.",
    "question_context": "Customer $t$ chooses review $r_t$ such that $r_t = -\\underline{K}$ if $u_t < \\lambda_{-\\underline{K}}$, $r_t = i$ if $\\lambda_{i-1} \\leq u_t < \\lambda_i$ ($-\\underline{K} < i < 0$), $r_t = 0$ if $\\lambda_{-1} \\leq u_t < \\lambda_1$, and similarly for positive reviews.\nAssumption 1 requires $\\bar{\\zeta} + \\bar{\\theta} - p > \\lambda_{\\overline{K}}$ and $\\underline{\\zeta} + \\bar{\\theta} - p + 1 < \\lambda_{-\\underline{K}}$ to ensure extreme reviews are possible.\n\nPost-purchase reviews follow threshold rules based on realized utility. Assumption 1 (Richness) ensures extreme reviews occur with positive probability."
  },
  {
    "qid": "econ-empirical-1219-4-0-3",
    "question": "4) Analyze the temporal evolution of the coefficient on the AFDC benefit difference with the nearest state for counties within 25 miles, as reported in Table 5. Why might the coefficient increase in magnitude from -0.0382 in the 1970s to -0.0882 in the 1980s?",
    "gold_answer": "1. The increase in the coefficient magnitude suggests that the effect of benefit differentials on AFDC expenditures became stronger over time.\n2. One possible explanation is the accumulation of welfare-prone residents in high-benefit areas over time, as individuals moved to these areas to take advantage of higher benefits.\n3. Another explanation could be that the correlation between welfare benefits and other forms of government support for low-income families strengthened during the 1980s, making benefit differentials a stronger signal of overall amenities for the poor.\n4. The statistically significant difference in coefficients (p < 0.05) supports the conclusion that the effect size changed over the two decades.",
    "question_context": "The baseline specification used in Column 1 of Table 4 includes a border-county dummy, the AFDC benefit difference calculated for all counties using the nearest state, and the interaction of this differential with the border-county dummy. The results indicate that having a benefit that is $100 higher than that of the nearest neighbor increases AFDC expenditures in border counties relative to interior counties by just under 4 percent.\nColumn 2 changes the specification used in Column 1 by replacing the border-county dummy with an indicator variable that equals one if the distance to the nearest state is less than 25 miles. The results indicate the nearest state having a benefit that is $100 lower than own benefits increases county AFDC expenditures by 4.9 percent for counties within 25 miles of another state relative to counties in the same state that are more than 25 miles from another state.\nColumn 3 of Table 4 tests for symmetric effects of a positive and negative differential with the nearest state, and fails to reject equivalence of effects with a p value of 0.330.\nColumn 4 extends the specification in Column 2 to estimate the effect of a benefit differential with the nearest state for counties that are more than 25 miles, but less than 50 miles, from the nearest state. While the coefficient on the benefit differential for the counties within 25 miles of another state is -0.0616, the coefficient on the differential for the counties between 25 and 50 miles of another state is only -0.0293, indicating that the effects are much larger for cases in which a very short distance allows individuals to cross state borders.\n\nThe section discusses alternative methods for measuring the benefit differential, focusing on calculating differentials for all counties by comparing AFDC benefits between the nearest state and the own state. It includes detailed regression analyses and control variables."
  },
  {
    "qid": "econ-empirical-1391-1-0-0",
    "question": "1) Derive the theoretical linkage between external monitoring strength ($M$) and SOE input prices ($P$) using a principal-agent framework, where $P = \\alpha - \\beta M + \\epsilon$. Justify the negative coefficient $\\beta$ with economic intuition.",
    "gold_answer": "1. **Principal-agent setup**: Let $P = \\alpha - \\beta M + \\epsilon$, where $\\epsilon$ captures unobserved factors.  \n2. **Monitoring effect**: Stronger monitoring ($\\uparrow M$) reduces kickbacks/self-dealing, lowering input prices ($\\downarrow P$). Thus, $\\beta > 0$.  \n3. **Empirical support**: SASAC's $3.9\\%$ price reduction aligns with $\\beta = \\frac{\\Delta P}{\\Delta M}$.",
    "question_context": "We show that enhanced external monitoring, as a key component of corporate governance, can substantially reduce managerial expropriation in procurement (proxied by input prices) and shirking in production management (proxied by productivity).\nThe productivity of SOEs is about $20\\%$ lower and they face $6.4\\%$ higher input prices compared with their private counterparts on average, after controlling for observable characteristics such as size, industry and location.\nSASAC reduced the input prices paid by SOEs by $3.9\\%$, closing the gap between SOEs and non-SOEs by half. The establishment of SASAC also increased the productivity of SOEs by $12.6\\%$ relative to their private counterparts, closing the gap by about $53\\%$.\nDoubling the oversight distance on average increases SOEs’ input prices by $0.3\\%$ and reduces their productivity by $0.7\\%$, relative to non-SOEs.\n\nThis section examines the empirical impact of government external monitoring on SOE performance, focusing on input prices and productivity. The study leverages a nationwide policy change (SASAC establishment) and geographic variation in monitoring strength."
  },
  {
    "qid": "econ-empirical-1684-3-0-3",
    "question": "4) Prove that IUA implies invariance of the choice set when a voter reorders unchosen alternatives. Use the definition of IUA and logical reasoning.",
    "gold_answer": "1. **IUA Definition**: $f(R) = f(R^\\prime)$ if $B \\subseteq A \\setminus f(R)$ and voter $i$ reorders $B$.\\n2. **Assumption**: $B$ contains only unchosen alternatives ($B \\cap f(R) = \\emptyset$).\\n3. **Preference Change**: Voter $i$ modifies $\\succ_i|_B$ to $\\succ_i^\\prime|_B$, leaving other preferences unchanged.\\n4. **IUA Application**: Since $B \\cap f(R) = \\emptyset$, IUA guarantees $f(R) = f(R^\\prime)$.\\n5. **Conclusion**: Reordering unchosen alternatives does not affect the choice set.",
    "question_context": "Definition 1 (Weak monotonicity (WMON)). An SCC $f$ satisfies weak monotonicity (WMON) if $a\\in f(R)$ implies $a\\in f(R^{\\prime})$ or $b\\in f(R^{\\prime})\\setminus f(R)$ for all alternatives $a$ , $b\\in A$ , and preference profiles $R$ , $R^{\\prime}$ for which there is a voter $i$ such that $\\succ_{j}^{\\prime}=\\succ{j}$ for all $j\\in N\\setminus\\{i\\}$ and $\\>_{i}^{\\prime}=\\>_{i}\\setminus\\{(b,a)\\}\\cup\\{(a,b)\\}$ .\nDefinition 2 (Weak set-monotonicity (WSMON)). An SCC $f$ satisfies weak setmonotonicity (WSMON) if $f(R)=f(R^{\\prime})$ for all preference profiles $R,R^{\\prime}$ for which a voter $i\\in N$ and an alternative $a\\notin f(R)$ exist such that $\\succ_{j}=\\succ_{j}^{\\prime}$ for all $j\\in N\\setminus\\{i\\}$ , $\\succ_{i}|_{A\\setminus\\{a\\}}=\\succ_{i}^{\\prime}|_{A\\setminus\\{a\\}}$ , $a\\succ_{i}A\\setminus\\{a\\}$ , and $A\\setminus\\{a\\}\\succ_{i}^{\\prime}a$ .\nDefinition 3 (Independence of unchosen alternatives (IUA)). An SCC $f$ satisfies independence of unchosen alternatives (IUA) if $f(R)=f(R^{\\prime})$ for all preference profiles $R,R^{\\prime}$ for which a voter $i\\in N$ and alternatives $B\\subseteq A\\setminus f(R)$ exist such that $\\succ_{j}=\\succ_{j}^{\\prime}$ for all voters $j\\in N\\setminus\\{i\\}$ and $\\succ_{i}\\backslash\\succ_{i}\\mid B=\\succ_{i}^{\\prime}\\backslash\\succ_{i}^{\\prime}\\mid B$ .\nDefinition 4 (Weak localizedness (WLOC)). An SCC $f$ satisfies weak localizedness (WLOC) if $f(R)=f(R^{\\prime})$ for all preference profiles $R,R^{\\prime}$ for which a voter $i\\in N$ and alternatives $B\\subseteq A$ exist such that $\\succ_{j}=\\succ_{j}^{\\prime}$ for all voters $j\\in N\\setminus\\{i\\},\\succ_{i}\\setminus\\succ_{i}\\left|_{B}=\\succ_{i}^{\\prime}\\setminus\\succ_{i}^{\\prime}\\right|B$ , and $B\\cap f(R)=B\\cap f(R^{\\prime})$ .\n\nThis section analyzes the implications of strategyproofness for pairwise Social Choice Correspondences (SCCs), leading to the definition of four axioms: weak monotonicity (WMON), weak set-monotonicity (WSMON), independence of unchosen alternatives (IUA), and weak localizedness (WLOC). These axioms are satisfied by every strategyproof and pairwise SCC."
  },
  {
    "qid": "econ-empirical-571-1-1-1",
    "question": "6) Explain the role of the parameter $\\alpha$ in the continuous-time model.",
    "gold_answer": "1. $\\alpha$ measures the speed of adjustment to the path implied by the standard Bass model.\\n2. A higher $\\alpha$ means faster convergence to the equilibrium path.\\n3. In the discretized model, $\\alpha$ scales all the $\\beta$ parameters, reflecting its role in adjusting the influence of the Bass model components.",
    "question_context": "The continuous-time model is $\\mathrm{d}n(t)=\\alpha\\Big[p[m-N(t)]+\\frac{q}{m}N(t)[m-N(t)]-n(t)\\Big]\\mathrm{d}t+\\sigma n(t)^{\\gamma}\\mathrm{d}W(t)$, where $W(t)$ is a standard Wiener process.\nThe discretization of this model is $\\Delta X_{t}=\\beta_{1}+\\beta_{2}N_{t-1}+\\beta_{3}N_{t-1}^{2}+\\beta_{4}X_{t-1}+X_{t-1}\\varepsilon_{t}$, where $\\beta_{1}=\\alpha p m$, $\\beta_{2}=\\alpha(q-p)$, $\\beta_{3}=-\\alpha\\frac{q}{m}$, $\\beta_{4}=-\\alpha$.\nThe panel format of the model is $\\Delta X_{i,t}=\\beta_{1,i}+\\beta_{2,i}N_{i,t-1}+\\beta_{3,i}N_{i,t-1}^{2}+\\beta_{4,i}X_{i,t-1}+X_{i,t-1}\\varepsilon_{i,t}$, where $i=1,\\ldots,N$ concerns a specific article.\nThe parameters are expressed in terms of the characteristics of the diffusion process: $\\beta_{k,i}=\\beta_{k}(m_{i},f_{i},T_{i}^{*},\\alpha_{i})$.\nThe multi-level non-linear regression model is $\\Delta X_{i,t}=\\beta_{1}(m_{i},f_{i},T_{i}^{*},\\alpha_{i})+\\beta_{2}(m_{i},f_{i},T_{i}^{*},\\alpha_{i})N_{i,t-1}+\\beta_{3}(m_{i},f_{i},T_{i}^{*},\\alpha_{i})N_{i,t-1}^{2}+\\beta_{4}(m_{i},f_{i},T_{i}^{*},\\alpha_{i})X_{i,t-1}+X_{i,t-1}\\varepsilon_{i,t}$, where $\\varepsilon_{i,t}\\sim\\mathbf{N}(0,\\sigma_{i}^{2})$.\n\nBoswijk and Franses (2005) modified the Bass regression model by allowing for heteroskedastic errors and short-run deviations from the deterministic S-shaped growth path. They propose a continuous-time model with a Wiener process and a discretization of this model."
  },
  {
    "qid": "econ-empirical-556-1-1-1",
    "question": "4) How is the SNP density estimated via quasimaximum likelihood? Provide the mathematical formulation.",
    "gold_answer": "The SNP density is estimated as follows:\n1. Maximize the log-likelihood: \n   $$\\tilde{\\theta}_{n}=\\arg\\max_{\\theta\\in\\Re^{\\prime}\\kappa}\\frac{1}{n}\\sum_{t=L+1}^{n}\\log[f_{K}(\\tilde{y_{t}}|\\tilde{y}_{t-L},\\dots,\\tilde{y}_{t-1},\\theta)].$$\n2. As $\\kappa$ grows with sample size $n$, the estimator $\\tilde{p}_{n}(y|x)=f_{K}(y|x,\\tilde{\\theta}_{n})$ becomes consistent and efficient.",
    "question_context": "The SNP density is a member of a class of parameterized conditional densities ${\\mathcal{H}}_{K}=\\{f_{K}(y|x,\\theta):\\theta=(\\theta_{1},\\theta_{2},...,\\theta_{\\wedge})\\},$ which expands ${\\mathcal{H}}_{1}\\subset{\\mathcal{H}}_{2}\\subset\\cdots$ as $\\kappa$ increases.\nThe SNP density has two desirable properties for EMM estimation: (1) The union $\\mathcal{H}=\\bigcup_{K=1}^{\\infty}\\mathcal{H}_{K}$ is rich and likely contains the true density $p(y|x)$. (2) If estimated by quasimaximum likelihood, it provides a consistent and efficient nonparametric estimator of $p(y|x)$.\n\nThe EMM estimator requires a score generator $f(y|x,\\theta)$ that fits the data well. The seminonparametric (SNP) density is used due to its flexibility and stability across subperiods."
  },
  {
    "qid": "econ-empirical-8-4-0-3",
    "question": "4) Formally decompose the $\\$12,400 surplus increase (column 7) into its worker and owner components using columns (5)-(6), and discuss the distributional implications.",
    "gold_answer": "1. Worker share = $\\frac{\\$3,700}{\\$12,400} \\approx 29.8\\%$\n2. Owner share = $\\frac{\\$9,100}{\\$12,400} \\approx 73.4\\%$\n3. Implies:\n   - Strong rent capture by owners\n   - Incomplete pass-through to wages\n   - Consistent with bargaining models where $\\beta \\approx 0.3$",
    "question_context": "Table V pools pre- and post application years and quantifies the average effects displayed in the event study figures by fitting simplified difference-in-differences models of the following form: \n\n$$\\begin{array}{r l}&{Y_{j t}=\\alpha_{j}+\\kappa_{t,{k(j)}}+Q\\pmb{5}_{j}\\cdot P o s t_{j t}\\cdot\\left(\\pmb{\\psi}_{5}+\\tau_{5}\\cdot I\\pmb{A}_{j}\\right)}\\ &{\\qquad+\\left(1-Q\\pmb{5}_{j}\\right)\\cdot P o s t_{j t}\\cdot\\left(\\pmb{\\psi}_{<5}+\\tau_{<5}\\cdot I\\pmb{A}_{j}\\right)+r_{j t}.}\\end{array}$$\nColumn (2) reports the impact of an initial allowance on the log of firm size, as measured by the number of W2 employees at the firm. Having a top-quintile patent allowed leads the firm to expand by roughly 22%.\nAn allowance of a high-value patent application is associated with roughly $37,000 in additional revenue per worker (column (3)) and roughly $16,000 in value added per worker (column (4)).\n\nThis section analyzes the effects of initial patent allowances on firm-level outcomes using a difference-in-differences model."
  },
  {
    "qid": "econ-empirical-930-2-0-3",
    "question": "4) Discuss the practical challenges of using dominated actions to refine consideration sets, compared to using $q$-covers.",
    "gold_answer": "**Comparison**:\n1. **$q$-covers**:\n   - Computationally efficient: only requires score computations for any feasible $\\pmb{b}$.\n   - Provides probabilistic guarantees ($p(A) \\geq q$).\n2. **Dominated actions**:\n   - Requires solving a maximization problem for each action to bound $s(a|b^*)$.\n   - Slower but more precise: rules out actions definitively if $s(a|b^*) < 0$.\n3. **Trade-off**:\n   - Use $q$-covers for quick approximations.\n   - Use dominated actions when accuracy is critical and computational resources allow.",
    "question_context": "We use the optimality conditions from Theorem 1 to approximate the optimal consideration set despite numerical imprecision. To do so, we find it useful to assign scores to each action ${\\pmb a}\\in\\mathcal{A}$ based on any feasible attention vector $\\pmb{b}\\in\\mathcal{B}$ . The ${^{\\scriptscriptstyle(4)}}b$ -score” of action $\\pmb{a}$ , written $s(a|b)\\in\\mathbb{R}$ , captures the location of ${\\pmb\\beta}({\\pmb a})$ relative to the hyperplane tangent to the indifference curve at $\\pmb{b}$ , $s({\\pmb a}|{\\pmb b}):=\\nabla{\\pmb w}({\\pmb b})\\cdot{\\pmb\\beta}({\\pmb a})-1.$\nDefinition 1. A set $A\\subseteq A$ is a ${}^{\\leftarrow}q$ -cover” of the (RI) problem $({\\mathcal{A}},{\\pi},\\lambda)$ if and only if $\\begin{array}{r}{p(A)=\\sum_{\\pmb{a}\\in A}p(\\pmb{a})\\geq q}\\end{array}$ for all optimal marginals $p$ .\nCorollary 1. For any $\\pmb{b}\\in\\mathcal{B}$ and any $q\\in(0,1)$ , the set $A=\\left\\{a\\in A{\\big|}s(a|b)\\geq-{\\frac{q{\\bar{s}}(b)}{1-q}}\\right\\}\\subseteq A$ is a $q$ -cover.\n\nThe text discusses the concept of consideration sets in the context of agent behavior, focusing on identifying actions chosen with positive probability. It introduces the notion of $q$-covers to approximate optimal consideration sets despite numerical imprecision."
  },
  {
    "qid": "econ-empirical-498-2-3-0",
    "question": "6) Compare and contrast sequential equilibrium and structural equilibrium, focusing on the key differences in their defining conditions.",
    "gold_answer": "Sequential equilibrium requires:\\n1. A sequence of strictly positive behavioral strategies converging to $\\beta_i$.\\n2. Optimality given beliefs $\\mu_i(\\cdot|I)$.\\nStructural equilibrium replaces (ii) with:\\n1. Existence of a perturbation $(p^k)_{k\\geq1}$ of $\\mu_i$.\\n2. Optimality under the perturbed beliefs $p^k(\\cdot|S_{-i}(I))$ for all $k$.",
    "question_context": "To obtain a corresponding notion of 'structural equilibrium,' replace (ii) above with: (ii’) For every $i$ and $I\\in\\mathbb{Z}_{i}$ , if $\\beta_{i}(I)(a)>0$ , then there exists $s_{i}\\in S_{i}(I)$ such that $s_{i}(I)=a$ and a perturbation $(\\boldsymbol{p}^{k})_{k\\geq1}$ of $\\mu_{i}$ such that $s_{i}\\in\\arg\\operatorname*{max}_{t_{i}\\in S_{i}(I)}U_{i}(t_{i},p^{k}(\\cdot|S_{-i}(I))$ for all $k\\geq1$ .\n\nThe text introduces the concept of structural equilibrium, replacing the sequential equilibrium condition with a perturbation-based condition to ensure structural rationality."
  },
  {
    "qid": "econ-empirical-1210-2-0-0",
    "question": "1) Derive the conditions under which the first-stage $F$ statistic fails to provide reliable inference in weak-instrument cases, referencing Bound, Jaeger, and Baker (1995) and Dufour (1997).",
    "gold_answer": "1. **Weak Instruments**: When the correlation between the instrument and endogenous variable is weak, the first-stage $F$ statistic does not diverge to infinity as sample size increases. \\n2. **Dufour's Critique**: If the instrument coefficients are near zero, the Wald test's nominal level $\\alpha$ may deviate arbitrarily from its true level, i.e., $\\lim_{n\\to\\infty} \\text{prob}(\\Psi > c_\\alpha) \\neq \\alpha$. \\n3. **Simulation Evidence**: Bound et al. (1995) show that even with uncorrelated instruments, standard errors remain small, leading to misleading confidence intervals.",
    "question_context": "Bound, Jaeger, and Baker (1995) also notice that although identification seems weak, the reported standard errors on the estimates are surprisingly small. To illustrate that these standard errors are unreliable indicators of the accuracy of the estimators, they conduct a simulation experiment in which they estimate returns-to-schooling using randomly generated instruments that have no correlation with education.\nDufour (1997) shows that the true levels of the usual Wald-type tests can deviate arbitrarily from their nominal levels if the coefficients on the instruments cannot be bounded away from the origin.\nStaiger and Stock (1997) propose an alternative asymptotic framework that assumes that the instruments’ coefficients are modeled as being in the neighborhood of zero, such that the first-stage $F$ statistic has a nondegenerate limiting distribution.\n\nBound, Jaeger, and Baker (1995) highlight the unreliability of standard errors in weak-instrument cases, supported by Dufour (1997), who shows deviations in Wald-type tests' nominal levels. Staiger and Stock (1997) propose weak-instrument asymptotics to address these issues."
  },
  {
    "qid": "econ-empirical-911-1-0-3",
    "question": "4) Explain the indirect inference estimator $\\hat{\\phi}_{T,H}^{I I}$ and how it addresses the small-sample bias in AR(1) models.",
    "gold_answer": "1. **Indirect Inference Estimator**: The estimator is defined as:\n   $$\\hat{\\phi}_{T,H}^{I I} = \\arg\\min_{\\phi} \\left\\| \\hat{\\theta}_T - \\frac{1}{H} \\sum_{h=1}^H \\tilde{\\theta}_T^h(\\phi) \\right\\|,$$\n   where $\\tilde{\\theta}_T^h(\\phi)$ is the estimator from simulated data under parameter $\\phi$.\n\n2. **Bias Correction**: The method calibrates the bias by matching the observed estimator $\\hat{\\theta}_T$ with the average of simulated estimators $\\tilde{\\theta}_T^h(\\phi)$. The inversion of the binding function $b_T(\\phi) = E(\\tilde{\\theta}_T^h(\\phi))$ corrects for the small-sample bias, ensuring $E(b_T(\\hat{\\phi}_{T,H}^{I I})) = b_T(\\phi_0)$.",
    "question_context": "The ML (fixed effects or within-group or LSDV) estimator of $\\phi$ is given by $$\\hat{\\phi}_{N T}^{M L}=(y_{-}^{\\prime}A y_{-})^{-1}y_{-}^{\\prime}A y,$$ where $y=(y_{1},\\dots,y_{N})^{\\prime}$ with $y_{i}=(y_{i1},\\dots,y_{i T})^{\\prime},A=I_{N}\\otimes A_{T}$ with $A_{T}=I_{T}-\\frac{1}{T}\\iota_{T}^{\\prime}\\iota_{T},y_{-}=(y_{1-},\\ldots,y_{N-})^{\\prime}$ with $y_{i-}=(y_{i0},\\dots,y_{i T-1})^{\\prime}$.\nNickell (1981) showed that the ML estimator is inconsistent when $N\\rightarrow\\infty$ and $T$ is fixed. The reason for the inconsistency comes from the endogeneity of the regressor in the de-meaned regression, $$y_{i t}-y_{i\\bullet}=\\phi(y_{i t-1}-y_{i\\bullet-1})+(\\epsilon_{i t}-\\epsilon_{i\\bullet}),$$ where $y_{i\\bullet}=\\sum_{t=1}^{T}y_{i t}/T,y_{i\\bullet-1}=\\sum_{t=0}^{T-1}y_{i t}/T,\\epsilon_{i\\bullet}=\\sum_{t=1}^{T}\\epsilon_{i t}/T.$\nApplying the first difference transformation to (1), we have $$\\varDelta y_{i t}=\\phi\\varDelta y_{i t-1}+\\varDelta\\epsilon_{i t},$$ which gives rise to the following moment conditions: $$E(\\varDelta y_{i t-1}\\times y_{i t-s})=0,\\quad\\mathrm{for}s=2,3,\\ldots,t-1.$$\nHan and Phillips (forthcoming) replaced the weak moment conditions (5) by a set of new moment conditions, i.e., $$E(\\varDelta y_{i t-1}\\times[(2\\varDelta y_{i t}+\\varDelta y_{i t-1})-\\phi\\varDelta y_{i t-1}])=0.$$ This approach leads to a new estimator of the form $$\\hat{\\phi}_{N T}^{H P}=\\frac{\\displaystyle\\sum_{i=1}^{N}\\sum_{t=1}^{T}\\varDelta y_{i t-1}(2\\varDelta y_{i t}+\\varDelta y_{i t-1})}{\\displaystyle\\sum_{i=1}^{N}\\sum_{t=1}^{T}(\\varDelta y_{i t-1})^{2}}.$$\nHahn and Kuersteiner (2002) introduced a bias-corrected ML estimator centered at the origin, which is a feasible version of the bias-corrected ML estimator of Kiviet (1995). If the bias-corrected MLE is denoted by $\\hat{\\phi}_{N T}^{H K}$, Hahn and Kuersteiner (2002) showed that $$\\sqrt{N T}(\\hat{\\phi}_{N T}^{H K}-\\phi)\\Rightarrow N(0,(1-\\phi^{2})).$$\nThe indirect inference estimator is defined by $$\\hat{\\phi}_{T,H}^{I I}=\\underset{\\phi\\in\\phi}{\\operatorname{argmin}}\\left\\|\\hat{\\theta}_{T}-\\frac{1}{H}\\sum_{h=1}^{H}\\widetilde{\\theta}_{T}^{h}(\\phi)\\right\\|,$$ where $\\|\\cdot\\|$ is some finite-dimensional distance metric. In the case where $H$ tends to infinity, the indirect inference estimator becomes $$\\hat{\\phi}_{T}^{I I}=\\operatorname*{argmin}_{\\phi\\in\\phi}\\|\\hat{\\theta}_{T}-E(\\tilde{\\theta}_{T}^{h}(\\phi))\\|.$$\n\nThis section reviews the bias in dynamic panel models with fixed effects and introduces various estimation methods to address this bias, including ML, GMM/IV, and indirect inference."
  },
  {
    "qid": "econ-empirical-1426-1-2-1",
    "question": "6) Explain why tapering is necessary for consistent estimation of the memory parameter \\( d \\) for nonstationary processes, and how the taper order \\( p \\) is chosen.",
    "gold_answer": "1. For nonstationary processes, the usual periodogram is biased due to leakage, leading to inconsistent estimates of \\( d \\).\n2. Tapering reduces leakage by smoothing the data at the boundaries.\n3. The taper order \\( p \\) must satisfy \\( p > d \\) to ensure the bias is negligible.\n4. Thus, \\( p \\) is chosen based on an upper bound for \\( d \\), which is often assumed a priori or estimated preliminarily.",
    "question_context": "The tapered DFT of an observed sequence \\( X_t, t=1,\\dots,n \\) at frequency \\( \\lambda_j \\) is \\( w_{X}^{T}(\\lambda_{j})=\\left(2\\pi\\sum_{t=1}^{n}h_{t}^{2}\\right)^{-1/2}\\sum_{t=1}^{n}h_{t}X_{t}\\exp(i\\lambda_{j}t) \\), for any taper sequence \\( \\{h_{t}\\}_{t=1}^{n} \\). The tapered periodogram at frequency \\( \\lambda_j \\) is \\( I_{X}^{T}(\\lambda_{j})=|w_{X}^{T}(\\lambda_{j})|^{2} \\).\n\nTapering is used to mitigate bias in spectral estimators caused by leakage, especially for processes with strong peaks in their spectral density. The order of the taper controls the smoothness and effectiveness of the tapering procedure."
  },
  {
    "qid": "econ-empirical-108-19-1-0",
    "question": "5) Derive the firm's optimal capital choice $k_{i t}$ under distortions $\\tau_{i t} = \\gamma a_{i t} + \\epsilon_{i t}$.",
    "gold_answer": "1. **Firm's Problem**: Maximizes $(1-\\tau_{Y i t})A_{i t}K_{i t}^{\\alpha} - (1+\\tau_{K i t})R K_{i t}$.  \n2. **Log-Linearization**: In logs, $k_{i t} = \\frac{\\mathbb{E}_{i t}[a_{i t} + \\tau_{i t}]}{1-\\alpha} + \\text{Constant}$.  \n3. **Substitute $\\tau_{i t}$**: $$k_{i t} = \\frac{(1+\\gamma)\\mathbb{E}_{i t}[a_{i t}] + \\epsilon_{i t}}{1-\\alpha} + \\text{Constant}.$$",
    "question_context": "The firm's capital choice under distortions is: $$k_{i t} = \\frac{(1+\\gamma)\\mathbb{E}_{i t}[a_{i t}] + \\epsilon_{i t}}{1-\\alpha} + \\text{Constant},$$ where $\\tau_{i t} = \\gamma a_{i t} + \\epsilon_{i t}$.\nThe dispersion in marginal products is: $$\\sigma_{m r p h}^{2} = \\mathbb{V} + \\gamma^{2}(\\sigma_{a}^{2} - \\mathbb{V}) + \\sigma_{\\epsilon}^{2}.$$\n\nThe model introduces idiosyncratic output and capital distortions $\\tau_{Y i t}$ and $\\tau_{K i t}$, which affect firm investment decisions. These distortions are log-normally distributed and may correlate with fundamentals."
  },
  {
    "qid": "econ-empirical-604-0-1-0",
    "question": "5) Using a comparative framework, analyze why industrial conflict intensified in Western Europe post-1968. How might the authors' methodological approach (e.g., national case studies vs. cross-national analysis) influence their conclusions?",
    "gold_answer": "Analysis:\n1. **Causes of Militancy**:\n   - Economic: Stagflation (high inflation + unemployment) eroded wage gains.\n   - Political: New Left movements (e.g., May 1968 France) radicalized labor.\n   - Institutional: Decentralized bargaining (e.g., Italy’s *autunno caldo*) increased shop-floor power.\n2. **Methodological Impact**:\n   - **National Studies**: May overstate idiosyncratic factors (e.g., UK union laws).\n   - **Cross-National**: Could identify systemic trends (e.g., EEC integration’s role).\n3. **Example**: Shalev’s conflict metrics (Vol. I) might miss cultural nuances in militancy.",
    "question_context": "The purpose of these volumes is not to provide comprehensive texts about industrial relations activities in West Germany, Belgium, Holland, France, Britain, and Italy but to present data and explanations about the rise of militancy after 1968.\nThe national studies in Volume I concentrate on the post-1968 period, with the implication, supported by the manner in which they are organised, that somehow the end of the last decade marked the onset of a major intensification and 'extensification' of industrial conflict.\n\nThe text discusses a comparative study of industrial militancy in Western Europe post-1968, highlighting gaps in British industrial relations literature and the project's focus on cross-national trends."
  },
  {
    "qid": "econ-empirical-1444-2-0-0",
    "question": "1) Derive the interpretation of $\\beta_{A F Q T}$ and $\\beta_{A F Q T,x}$ in the context of employer learning about ability over time. How do these coefficients differ between high school and college graduates?",
    "gold_answer": "1. **Interpretation of $\\beta_{A F Q T}$**: This coefficient represents the initial return to AFQT at the time of labor market entry. If employers do not observe ability initially, $\\beta_{A F Q T}$ should be close to zero. If ability is observed, $\\beta_{A F Q T}$ should be large.\n2. **Interpretation of $\\beta_{A F Q T,x}$**: This coefficient captures how the return to AFQT changes with experience. If employers learn about ability over time, $\\beta_{A F Q T,x}$ should be positive and sizable. If ability is already observed, $\\beta_{A F Q T,x}$ should be small.\n3. **High School Graduates**: $\\beta_{A F Q T}$ is small and insignificant, while $\\beta_{A F Q T,x}$ is positive and significant, indicating learning over time.\n4. **College Graduates**: $\\beta_{A F Q T}$ is large and significant, while $\\beta_{A F Q T,x}$ is small and insignificant, indicating immediate observation of ability.",
    "question_context": "Following the interpretation of AP, if employers do not initially observe ability, but learn about it over time, the weight placed on AFQT should be small initially and increase with experience. This means that $\\beta_{A F Q T}$ should be close to zero, and $\\beta_{A F Q T,x}$ should be positive and sizable. On the other hand, if employers directly observe AFQT, the returns to AFQT should be high initially and should not change much over time. This case translates to a large $\\beta_{A F Q T}$ and a relatively small $\\beta_{A F Q T,x}.$\nBecause we are working with log wages, $\\beta_{A F Q T}$ is the percent change in real wages as a response to an increase of AFQT by one standard deviation. We divide the interaction of any variable with experience by 10 so the coefficient $\\beta_{A F Q T,x}$ is the change in the wage slope between the periods when $x=0$ and $x=10$.\n\nThe text discusses how employers learn about ability over time, focusing on the role of AFQT in wage determination for high school and college graduates. It highlights differences in initial returns to AFQT and how these evolve with experience."
  },
  {
    "qid": "econ-empirical-79-49-1-0",
    "question": "3) Formalize the market-clearing mechanism in frequent batch auctions for the case where demand and supply intersect at a unique price $p^*$.",
    "gold_answer": "1. **Clearing Condition**: Solve $D(p^*) = S(p^*)$ for $p^*$ and $q^*$.  \n2. **Execution Rules**:  \n   - Buy orders with $bid > p^*$ and sell orders with $ask < p^*$ fully execute at $p^*$.  \n   - Orders at $p^*$ are rationed by time-priority (older orders first, then pro-rata).",
    "question_context": "Frequent batch auctions process orders in discrete-time intervals $\\tau$, clearing via uniform-price auctions. Orders carry over if unmatched, and rationing uses time-priority rules.\nKey differences from continuous markets: (i) discrete-time processing, (ii) uniform-price auctions, (iii) information dissemination only after batch completion.\n\nThis section defines frequent batch auctions as an alternative to continuous limit order books, addressing HFT inefficiencies."
  },
  {
    "qid": "econ-empirical-958-0-0-1",
    "question": "2) Derive the expression for the Gini index $G(F)$ as a functional of the distribution $F$ of probabilities $p_{a}$. Show the steps from the definition to the final form given in the article.",
    "gold_answer": "The Gini index $G(F)$ is derived as follows:\n\n1. Start with the definition of the Gini index for two groups:\n   $$ G = \\frac{1}{2\\bar{p}(1-\\bar{p})} \\sum_{a} \\sum_{a'} w_{a} w_{a'} |\\pi_{a} - \\pi_{a'}| $$\n\n2. Replace $\\pi_{a}$ with the true probabilities $p_{a}$ and take expectations:\n   $$ G(F) = \\frac{1}{2\\mathbb{E}(p)(1-\\mathbb{E}(p))} \\mathbb{E} \\left[ \\sum_{a} \\sum_{a'} w_{a} w_{a'} |p_{a} - p_{a'}| \\right] $$\n\n3. Simplify using the properties of $F$ and symmetry:\n   $$ G(F) = \\frac{1 - \\mathbb{E}(p) - \\int_{0}^{1} F^{2}(p) dp}{\\mathbb{E}(p)(1-\\mathbb{E}(p))} $$\n\nThis matches Equation (1) in the article.",
    "question_context": "When units are small, past research has stressed that randomness, a counterfactual situation in which the minority individuals are distributed randomly across units, would be a more sensible benchmark than evenness (Cortese, Falk, and Cohen 1976; Boisso et al. 1994; Ransom 2000).\nThe discrepancy arises because segregation indices use the observed proportion $\\pi_{a}$ of the minority group in unit $a$ to estimate the true unobserved probability $p_{a}$ that an individual of this unit belongs to the minority. When units are small, $\\pi_{a}$ is a noisy estimate of $p_{a}$ and indices are biased.\nThe main contribution of this article is to propose a simple parametric approach to estimate this index of interest: $F$ is assumed to be a mixture of Beta distributions. Under this assumption, the parameters of the distribution and therefore the quantity of interest can be estimated.\n\nThe article discusses the challenges of measuring segregation when units contain few individuals, such as establishments or classrooms. Traditional segregation indices based on sample proportions are biased in such cases. The author proposes a parametric approach using a mixture of Beta distributions to model the probability that an individual within a unit belongs to the minority group."
  },
  {
    "qid": "econ-empirical-124-1-0-3",
    "question": "4) Explain the concept of a rational-agent Nash equilibrium. Why is it necessary to impose the restriction that the agent cannot make incredible threats or promises in contract bargaining?",
    "gold_answer": "**Rational-Agent Nash Equilibrium** refines Nash equilibrium by:\n1. **Individual Rationality**: The agent accepts $r$ only if it offers non-negative utility, i.e., $\\delta^e(r) = 1 \\iff E[u(r(\\sigma_a^e(r), \\sigma_b^e)) - y(\\sigma_a^e(r), \\sigma_b^e)] \\geq 0$.\n2. **No Incredible Threats**: The agent's strategy $\\langle \\delta^e(\\cdot), \\sigma_a^e(\\cdot) \\rangle$ must be optimal for all $r$, not just equilibrium $r$. This rules out non-credible off-equilibrium threats.\n3. **Necessity**: Without this, equilibria may rely on unrealistic agent behavior (e.g., rejecting Pareto-improving contracts), undermining predictive power.",
    "question_context": "The principal's von Neumann-Morgenstern utility is $Z(I,{\\mathbf{a}})$ ,where $I$ is the principal's income. As discussed above, I assume that the principal is risk neutral with respect to income in order to focus on the incentive effects of agency rather than on the possibility of insurance. As also noted earlier, I want to focus on the case in which the principal and agent have the same marginal rate of substitution between income and effort as one another when the agent is introduced below. Thus, I rule out income effects in the principal's marginal rate of substitution between income and actions--if income effects were present, changes in the principal's income would change her marginal rate of substitution, but not the agent's, leading to differences in preferences between the two parties. The need to rule out this type of income effect, coupled with the assumption of income risk neutrality, implies that the principal's utility can be expressed as $Z(I,{\\bf a})=I-y({\\bf a})$.\nA Nash equilibrium in the principals-only game consists of a pair of strategies, $\\sigma_{a}^{e}\\in S_{a}$ and $\\sigma_{b}^{e}\\in S_{b}$ ,such that $\\sigma_{a}^{e}\\in\\underset{\\sigma\\in S_{a}}{\\mathrm{argmax}}E\\{m(\\sigma,\\sigma_{b}^{e})-y(\\sigma,\\sigma_{b}^{e})\\}^{6}$ and $\\sigma_{b}^{e}\\in\\underset{\\sigma\\in S_{b}}{\\operatorname{argmax}}\\pi(\\sigma_{a}^{e},\\sigma)$.\nA Nash equilibrium in the agency game consists of a triple of strategies- $\\rho^{e}$ $\\left\\langle\\delta^{e}(\\cdot),\\sigma_{a}^{e}(\\cdot)\\right\\rangle$ ,and $\\sigma_{b}^{e}$ -suchthat $\\rho^{e}\\in\\underset{\\rho\\in S_{r}}{\\mathrm{argmax}}\\sum_{r\\in R}\\rho(r)\\delta^{e}(r)E\\big\\{m[\\sigma_{a}^{e}(r),\\sigma_{b}^{e}]-r[\\sigma_{a}^{e}(r),\\sigma_{b}^{e}]\\big\\}$, $\\left\\langle\\delta^{e}(\\cdot),\\sigma_{a}^{e}(\\cdot)\\right\\rangle\\in\\underset{\\delta\\in S_{r},\\sigma(r)\\in S_{a}}{\\mathrm{argmax}}\\sum_{r\\in R}\\rho^{e}(r)\\delta(r)E\\{u[r(\\sigma(r),\\sigma_{b}^{e})]-y[\\sigma(r),\\sigma_{b}^{e}]\\}$, and $\\sigma_{b}^{e}\\in\\underset{\\sigma\\in S_{b}}{\\mathrm{argmax}}\\sum_{r\\in R}\\rho^{e}(r)\\delta^{e}(r)\\pi[\\sigma_{a}^{e}(r),\\sigma]$.\n\nThis section presents a general framework for analyzing the effects of agency relationships in a market-stage game involving two principals. The model compares scenarios where principals represent themselves versus hiring agents, focusing on incentive alignment and strategic behavior."
  },
  {
    "qid": "econ-empirical-118-1-0-2",
    "question": "3) Derive the recursive relationships for the impulse response coefficients $K_i$ from the VARMA system $[A(z):M(z)]$ given by $\\sum_{j=0}^{i} A_j K_{i-j} = M_i$ for $i = 0,1,\\dots,p$ and $\\sum_{j=0}^{p} A_j K_{i-j} = 0$ for $i > p$.",
    "gold_answer": "1. Start with the VARMA equation $A(z)K(z) = M(z)$. \\\\\n2. Expand $A(z) = \\sum_{j=0}^p A_j z^j$ and $M(z) = \\sum_{j=0}^p M_j z^j$. \\\\\n3. Equate coefficients of $z^i$ on both sides: \\\\\n   - For $i \\leq p$: $\\sum_{j=0}^i A_j K_{i-j} = M_i$. \\\\\n   - For $i > p$: $\\sum_{j=0}^p A_j K_{i-j} = 0$ (since $M(z)$ has no terms beyond $z^p$).",
    "question_context": "Let us commence by observing that, if there exists an operator pair $[A(z)~:~M(z)]$ satisfying (1.4), then $K(z)$ is rational, as in (1.3), with $N(z)=({\\bf a d j}A(z))M(z)$ and $d(z)=\\operatorname*{det}A(z),q\\leq\\nu p$.\nThe pair $[\\stackrel{.}{A}^{+}(z):{M}^{+}(z)]$ is called a right multiple of $L(z)$ , which, in turn, is called a common left divisor of $[A^{+}(z):M^{+}(z)]$.\nThe operators $A(z)$ and $M(z)$ are then said to be left coprime. A polynomial matrix $U(z)$ is called unimodular if its determinant is a constant.\nThe Kronecker indices $n_{r}$ specify the maximum degree of all operators in the $r$ th row of $[A(z):M(z)]$.\nThe echelon canonical form provides a unique way of modeling any rational $K(z)$ , no matter what its Kronecker indices.\n\nThe echelon form is a canonical representation of VARMA systems that ensures uniqueness by imposing specific restrictions on the operator pair [A(z):M(z)]. It addresses redundancy issues through left coprimeness and Kronecker indices, providing a structured parameterization of rational transfer functions."
  },
  {
    "qid": "econ-empirical-335-4-1-3",
    "question": "8) Explain why the minimum-MSE estimator performs well under misspecification in the dynamic panel data model, even for small $T$. Relate this to the properties of the influence function.",
    "gold_answer": "1. **Influence function properties**: The minimum-MSE influence function accounts for local deviations from the reference model.\\n2. **Bias reduction**: By adjusting for misspecification through $\\Delta(a)$, the estimator remains robust.\\n3. **Small $T$ performance**: Even with limited time periods, the influence function captures essential features of the data-generating process.\\n4. **Variance trade-off**: Increased variance for larger $\\epsilon$ is offset by bias reduction, leading to lower overall MSE.",
    "question_context": "In the simulation, we set a bimodal distribution that has modes $\\{-1,2\\}$ when $Y_{0}=0$ and $\\{-3,0\\}$ when $Y_{0}=1$, with some asymmetry between the two modes. We draw $Y_{0}$ from a Bernoulli distribution with probability 0.5.\nIn neighborhoods that consist of unrestricted joint distributions $\\pi_{0}$ of $(A,X)$, the minimum-MSE $h$ function is given by Corollary 4, for $X=Y_{0}$, and either $\\Delta(a)=\\Phi(\\beta_{0}+a)-\\Phi(a)$ or $\\Delta(a)=\\beta_{0}$, depending on the quantity of interest.\n\nThis section extends the analysis to dynamic panel data binary choice models, focusing on the average state dependence effect $\\delta_{\\beta_{0},\\pi_{0}}=\\mathbb{E}_{\\pi_{0}}[\\Phi(\\beta_{0}+A)-\\Phi(A)]$. The model is $Y_{t}=\\mathbb{1}\\{\\beta_{0}Y_{t-1}+A+U_{t}\\geq0\\}$, where $U_{t}$ are i.i.d. standard normal. The reference density for $A$ given $Y_{0}$ is normal, but the true distribution is bimodal."
  },
  {
    "qid": "econ-empirical-892-0-0-2",
    "question": "3) Describe the property of auxiliary random variables $c$ that allows for efficient random drawings from 1-1 poly-t distributions.",
    "gold_answer": "The property is given by:\n\\[\nD(y)=\\int D(y|c)D(c)dc,\n\\]\nwhere:\n1. $D(y|c)$ characterizes a distribution from which $y$ can be easily drawn for any given $c$.\n2. Random drawings from $D(c)$ are feasible due to the low dimensionality of $c$, making it possible to tabulate its distribution function.\nThis allows for sequential drawing of $c$ and $y$ given $c$, retaining only $y$, which is equivalent to direct drawing from $D(y)$.",
    "question_context": "Following Dreze (1977), $m{-}1$ poly-t density functions are defined as density functions whose kernels are ratios of a product of m multivariate $\\cdot t$ density kernels to a single multivariate- $t$ density kernel.\nThe relevance of this class of density functions for empirical work has led to the development of a computer software PTD, documented in Bauwens et al. (1981), for the evaluation of the integrating constants, first- and secondorder moments, fractiles and complete univariate marginal density functions of $m{-}0$ and $m{-}1$ poly-t density functions when $^{m}$ is less than or equal to 3.\nPTD suffers, however, from important limitations: (i) It cannot evaluate higher-order moments or moments of non-linear functions of the structural coefficients such as dynamic multipliers or long-run coefficients. (ii) The evaluation of complete marginal posterior densities can prove costly when $m=2$ or3.\n\nThe paper discusses $m{-}1$ poly-t density functions, which are ratios of products of multivariate-t density kernels to a single multivariate-t density kernel. These functions arise as posterior densities for coefficients in structural equations under various Bayesian procedures."
  },
  {
    "qid": "econ-empirical-1204-0-0-1",
    "question": "2) What methodological approach might Evans and Galloway (1973) have used to analyze the relationship between verbal ability and socioeconomic status?",
    "gold_answer": "The study likely employed:\n1. **Descriptive statistics** to summarize verbal ability and SES across tracks.\n2. **Correlation analysis** to assess the relationship between verbal ability and SES.\n3. **Regression models** to control for confounding variables and examine track differences.",
    "question_context": "Verbal Ability and Socioeconomic Status of 9th and 12th Grade College Preparatory, General, and Vocational Students\n\nThis study examines the relationship between verbal ability and socioeconomic status among 9th and 12th grade students in college preparatory, general, and vocational tracks."
  },
  {
    "qid": "econ-empirical-921-1-3-0",
    "question": "9) Compare the Reiter (2009) method with the Krusell and Smith (1998) method. What are the advantages of using linearization in aggregate variables?",
    "gold_answer": "1. **Reiter (2009)**: Linearizes aggregate dynamics while preserving nonlinearity in individual optimization. Uses state reduction to handle high-dimensional distributions. \\n2. **Krusell-Smith (1998)**: Relies on approximate aggregation, assuming agents use a finite set of moments to forecast aggregates. \\n3. **Advantages of linearization**: \\n   - Computational efficiency for small shocks. \\n   - Avoids approximation errors from moment-based forecasting. \\n   - Easier to compare with linearized representative agent models.",
    "question_context": "The method is based on Reiter (2009), which can be understood as the heterogeneous agents analogue of the linearization method that is widely used in business cycle models and in the literature on monetary policy.\nIn computing fluctuations, the method keeps track of a high-dimensional representation of the cross-sectional distribution. Because this distribution (the state-space of our model) is larger than what can be handled even by a linearization model, the optimal state reduction technique of Reiter (2010) is used.\n\nThe solution method combines nonlinear individual optimization with linearized aggregate dynamics, using state reduction techniques to handle the cross-sectional distribution."
  },
  {
    "qid": "econ-empirical-1591-3-0-1",
    "question": "2) Compare the method of moments and maximum likelihood estimation for Tobit models, and explain why the latter failed to converge for urban migrants.",
    "gold_answer": "1. **Method of Moments (MM)**: Estimates parameters by matching sample moments to population moments. Less computationally intensive but may be less efficient.\n2. **Maximum Likelihood (ML)**: Maximizes the likelihood function. More efficient but requires convergence of iterative algorithms.\n3. Non-convergence in ML may occur due to:\n   - Poor starting values.\n   - Flat likelihood surfaces.\n   - Multicollinearity or sparse data (e.g., small urban migrant subsample).",
    "question_context": "Ordinary least squares estimates, corrected for selection bias, of amount remitted per month (sample restricted to remitters only).\nThe figures in parentheses are the corrected standard errors. Significant at the 1 percent level, using a two-tailed test. *Significant at the 5 percent level. dNot entered in the equation. Significant at the 10 percent level. This regressor, the inverse of Mill's ratio, accounts for the selection bias.\nThe Tobit estimates for urban migrants were obtained by the method of moments [see Greene (1983)] instead of the maximum likelihood method, because in the latter method the parameters failed to converge in 50 iterations.\nFor urban migrants the coefficients for EDUCAT, URBRES, DEPRATU and, unexpectedly, FUTPLAN are not statistically significant.\n\nThe text presents regression results analyzing remittance behaviors among rural and urban migrants, using Ordinary Least Squares (OLS) corrected for selection bias and Tobit models. The analysis includes variables such as income, education, urban residence, dependency ratios, and regional dummies."
  },
  {
    "qid": "econ-empirical-1110-1-0-3",
    "question": "4) How does the author address potential selection bias in estimating the effect of legalization? Discuss the role of exogenous factors (e.g., NACARA) and longitudinal data.",
    "gold_answer": "1. **Exogenous Variation**: NACARA provides a natural experiment by randomly granting amnesty based on country of origin and timing.\n2. **Comparison Group**: Uses non-eligible immigrants with similar characteristics to control for unobserved trends.\n3. **Longitudinal Data**: CPS-ORG tracks individuals over time, allowing for fixed-effects models to control for individual heterogeneity (e.g., motivation).",
    "question_context": "Undocumented immigrants have a higher labor force participation rate as compared to other foreign-born persons or natives (Borjas and Tienda 1993). Ineligibility for means-tested benefits is likely to lower their reservation wage.\nLegalization may open new work opportunities resulting in higher wages. Higher wages associated with legalized status will increase employment, as some undocumented immigrants will be induced to work.\nUndocumented immigrants earn lower wages as compared to the other foreign-born with similar characteristics (Kossoudji and Cobb-Clark 2002). Differences in wage earnings between the other foreign-born and undocumented immigrants could be due to some unobserved job market attributes of undocumented immigrants.\n\nThis section discusses the labor market participation and wage differences between undocumented immigrants and other groups, considering factors like legal status, skill levels, and the effects of amnesty programs."
  },
  {
    "qid": "econ-empirical-940-1-0-1",
    "question": "2) Show how the MELO estimator $\\hat{\\alpha}_{\\mathrm{MELO}}$ is derived from the posterior expected loss minimization problem.",
    "gold_answer": "1. The MELO estimator minimizes the posterior expected loss, given by: $$\\hat{\\alpha}_{\\mathrm{MELO}} = \\frac{\\mathrm{E}(\\delta_{1})}{\\mathrm{E}(\\delta_{2})} \\cdot \\frac{1 + \\mathrm{cov}(\\delta_{1}, \\delta_{2}) / \\mathrm{E}(\\delta_{1})\\mathrm{E}(\\delta_{2})}{1 + \\mathrm{var}(\\delta_{2}) / \\mathrm{E}^{2}(\\delta_{2})}.$$\n2. Here, $\\mathrm{E}(\\cdot)$, $\\mathrm{var}(\\cdot)$, and $\\mathrm{cov}(\\cdot)$ are posterior moments with respect to the posterior density $g(\\beta, \\sigma | A, X)$.\n3. The term $F$ represents a shrinkage factor that adjusts the ratio of expectations based on the covariance and variance of the parameters.",
    "question_context": "The generalized quadratic loss function for our problem is $$G Q L=\\{\\dot{\\delta}_{1}-\\dot{\\delta}_{2}x^{*}\\}^{2}=\\dot{\\delta}_{2}^{2}(x-x^{*})^{2},$$ where $x^{*}$ is any estimate of $x\\equiv\\dot{\\langle\\rangle}_{1}/\\dot{\\langle\\rangle}_{3}$.\nThe MELO estimator is given by $$\\begin{array}{l}{{\\displaystyle\\hat{\\alpha}_{\\mathrm{MELO}}=\\frac{\\mathrm{E}(\\delta_{1})}{\\mathrm{E}(\\delta_{2})}\\cdot\\frac{1+\\mathrm{cov}(\\delta_{1},\\delta_{2})/\\mathrm{E}(\\delta_{1})\\mathrm{E}(\\delta_{2})}{1+\\mathrm{var}(\\delta_{2})/\\mathrm{E}^{2}(\\delta_{2})}}}\\\\{{\\displaystyle\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\equiv\\frac{\\mathrm{E}(\\delta_{1})}{\\mathrm{E}(\\delta_{2})}\\cdot F,}}\\end{array}$$ where $\\operatorname{E}\\left(\\cdot\\right)$ denotes posterior expectation, var( ·) denotes posterior variance, and cov(·) denotes posterior covariance.\nIf the reduced-form disturbance $\\ell_{t}$ is approximately white noise, and if the likelihood is normal and the prior is diffuse, then the marginal posterior density ${\\mathfrak{g}}({\\boldsymbol{\\beta}})$ has a multivariate Student-t-distribution with four-dimensional mean vector $$\\operatorname{E}(\\beta)=(X^{\\prime}X)^{-1}X^{\\prime}A=({\\hat{b}}_{1},{\\hat{b}}_{2},{\\hat{b}}_{3},{\\hat{b}}_{4})^{\\prime},$$ which is equal to the LS estimator.\nThe posterior covariance matrix is given by $$(X^{\\prime}X)^{-1}\\xrightarrow[\\mathrm{~\\vec{r}~-~\\frac~2~}]{\\mathrm{~\\vec{\\mathbf{\\sigma}}~}}\\ensuremath{\\mathbf{s}}^{2},$$ where $\\nu\\:=\\:T\\:-\\:4$ and $$\\begin{array}{r}{\\mathfrak{v}\\mathscr{s}^{2}=\\big(A-X\\mathbb{E}(\\beta)\\big)^{\\prime}(A-X\\mathbb{E}(\\beta)\\big),}\\end{array}$$ the LS residual sum of squares.\n\nThe text discusses the minimum expected-loss (MELO) estimator for ratio estimation problems, highlighting its properties such as finite moments, consistency, and asymptotic efficiency. It also introduces the generalized quadratic loss function and Bayesian framework for deriving the MELO estimator."
  },
  {
    "qid": "econ-empirical-1141-1-0-2",
    "question": "3) Derive the closed-form solution for $\\mathrm{RDoflogs}_{i}$ under the assumption that log income follows a normal distribution.",
    "gold_answer": "1. Assume $\\ln(y) \\sim N(\\mu_r, \\sigma_r^2)$.  \n2. The truncated mean for $\\ln(y) > \\ln(y_i)$ is: $E[\\ln(y) \\mid \\ln(y) > \\ln(y_i)] = \\mu_r + \\sigma_r \\frac{\\phi(\\alpha_i)}{1 - \\Phi(\\alpha_i)}$, where $\\alpha_i = \\frac{\\ln(y_i) - \\mu_r}{\\sigma_r}$.  \n3. Subtract $\\ln(y_i)$ and multiply by $1 - \\Phi(\\alpha_i)$ to obtain: $\\mathrm{RDoflogs}_{i} = \\left[\\mu_r + \\sigma_r \\frac{\\phi(\\alpha_i)}{1 - \\Phi(\\alpha_i)} - \\ln(y_i)\\right] \\cdot (1 - \\Phi(\\alpha_i))$.",
    "question_context": "For a person i with income $y_{i}$ who is part of a reference group with $\\mathbf{N}$ people, Yitzhaki's measure is defined as: $$ R D_{i}=\\frac{1}{N}\\Sigma_{j}(y_{j}-y_{i})\\forall y_{j}>y_{i} $$\nThe relative deprivation measure can be rewritten as: $$ R D_{i}=[E(y\\mid y>y_{i})-y_{i}]^{*}\\mathrm{~prob~}(y>y_{i}) $$\nAn alternative measure using log income is: $$ \\mathrm{RDoflogs}_{i}=\\Big\\{\\pmb{\\upmu}_{r}+\\frac{\\upsigma_{r}\\upphi_{i}}{(1-\\Phi_{i})}\\Bigg]-\\ln{(y_{i})}\\}^{*}(\\mathrm{~1~}-F_{i}) $$\nThe individual's z-score is defined as: $$ z-\\mathrm{score}=\\frac{(y_{i}-\\upmu_{r})}{\\upsigma_{r}}=\\left(\\frac{y_{i}}{\\upsigma_{r}}\\right)-\\left(\\frac{\\upmu_{r}}{\\upsigma_{r}}\\right) $$\n\nThe seminal definition of relative deprivation is accredited to Runciman (1966), who argues that an individual is relatively deprived if they lack something that others in their reference group possess. This concept is formalized in economic terms by Yitzhaki (1979), who provides a mathematical framework for measuring relative deprivation based on income comparisons within a reference group."
  },
  {
    "qid": "econ-empirical-317-4-1-1",
    "question": "6) Show that $|H_{m+1}\\hat{p} - \\hat{e}_m|$ is bounded by $K_m ||H\\hat{p} - \\hat{E}||_1$ and relate this to the excess risk.",
    "gold_answer": "1. **Lipschitz Continuity**: $P\\rho_{g(e_m)}$ has Lipschitz constant $K_m$.\n2. **Bound**: \n   $$|H_{m+1}\\hat{p} - \\hat{e}_m| \\leq ||H\\hat{p} - \\hat{E}||_1 \\leq K_m (||Hp^0 - E^0||_1 + ||E^0 - \\hat{E}||_1).$$\n3. **Excess Risk**: Combine with Lemma 6 to bound $\\mathcal{E}(g(H_{m+1}\\hat{p}))$.",
    "question_context": "The oracle inequality is given by: \n$$\\mathcal{E}(\\hat{f}_m) + \\lambda_m ||\\hat{\\beta}_m - \\beta_m^0||_1 \\leq 6\\mathcal{E}(f_m^0) + \\frac{16\\lambda_m^2 s_m}{c_m \\phi_m^2}.$$\nThe excess risk for $g(H_{m+1}\\hat{p})$ decomposes as: \n$$\\mathcal{E}(g(H_{m+1}\\hat{p})) = (P\\rho_{g(H_{m+1}\\hat{p})} - P\\rho_{g(\\hat{e}_m)}) + \\mathcal{E}(\\hat{f}_m).$$\n\nThe proof leverages the margin condition and compatibility assumptions to derive an oracle inequality for the lasso estimator in logistic regression."
  },
  {
    "qid": "econ-empirical-687-0-0-0",
    "question": "1) Derive the Aldous-Hoover representation for the random vector $W_{i j}$ under two-way clustering and explain the intuition behind each component $U_{i0}, U_{0j},$ and $U_{i j}$.",
    "gold_answer": "1. The Aldous-Hoover representation is given by: $$W_{i j}=\\tau(U_{i0},U_{0j},U_{i j})$$ where $\\tau$ is an unknown function, and $\\{U_{i j}\\}$ are independent uniform random variables.  \n2. **Intuition**:  \n   - $U_{i0}$ represents the fixed effect for the $i^{th}$ cluster (e.g., market-specific demand shock).  \n   - $U_{0j}$ represents the fixed effect for the $j^{th}$ cluster (e.g., product-specific supply shock).  \n   - $U_{i j}$ captures idiosyncratic shocks specific to observation $(i,j)$.  \n3. This decomposition separates cluster-level dependencies from idiosyncratic variation.",
    "question_context": "Suppose that the researcher observes a sample $\\left\\{\\left.\\bar{W_{i j}}\\right|i\\in\\{1,\\ldots,N\\},j\\in\\{1,\\ldots,M\\}\\right\\}$ of double-indexed observations of size NM. It is two-way clustered if units in the cluster $\\left\\{W_{i j}\\right\\}_{j=1}^{M}$ are dependent for any given $i\\in\\{1,\\ldots,N\\}$ and units in the cluster $\\left\\{\\boldsymbol{W}_{i j}\\right\\}_{i=1}^{N}$ are dependent for any given $j\\in\\{1,\\dots,M\\}$.\nUnder the assumption of two-way clustering, the random vector $W_{i j}$ can be represented by the structure $W_{i j}=\\tau(U_{i0},U_{0j},U_{i j})$ for some function $\\tau$, where $\\{U_{i j}\\mid(i,j)\\in\\mathbb{N}^{2}\\setminus\\{(0,0)\\}\\}$ are independent uniform random variables.\nThe DML was proposed by Chernozhukov et al. (CCDDHNR, 2018a), providing a general DML toolbox for estimation and inference for structural parameters with high-dimensional and/or infinite-dimensional nuisance parameters under the typical microeconometric assumption of iid sampling.\nConsider the partially linear IV model: $Y_{i j}=D_{i j}\\theta_{0}+g_{10}(X_{i j})+\\epsilon_{i j},\\quad \\mathrm{E}[\\epsilon_{i j}|X_{i j},Z_{i j}]=0$ and $Z_{i j}=m_{0}(X_{i j})+\\nu_{i j},\\quad \\mathrm{E}[\\nu_{i j}|X_{i j}]=0$.\nA Neyman orthogonal score is defined by $\\psi(w;\\theta,\\eta)=(y-(d-g_{2}(x))\\theta-g_{1}(x))(z-m(x))$, where $w~=~(y,d,x,z)$ and $\\eta~=~(g_{1},g_{2},m)$. If we set $\\eta_{0}~=$ $(g_{10},g_{20},m_0)$, then it holds under the model that $\\mathrm{E}[\\psi(W_{i j};\\theta_{0},\\eta_{0})]=0$.\n\nThis article investigates double/debiased machine learning (DML) under multiway clustered sampling environments, proposing a novel multiway cross-fitting algorithm and a multiway DML estimator. The study also develops a multiway cluster robust standard error formula, with simulations indicating favorable finite sample performance."
  },
  {
    "qid": "econ-empirical-182-4-0-1",
    "question": "2) Explain why a 100% profit tax combined with lump-sum redistribution achieves efficiency, while a less than 100% profit tax decreases efficiency.",
    "gold_answer": "1. **100% Profit Tax**: \n   - At τ_f = 1, the firm's after-tax profit from any worker is zero: (1−1)(a_i−w_i) = 0.\n   - The firm becomes indifferent between hiring any worker, leading to efficient matching.\n   - Lump-sum redistribution ensures no wealth effects distort decisions.\n2. **Less Than 100% Profit Tax**: \n   - For τ_f < 1, the firm's after-tax profit depends on worker type: (1−τ_f)(a_i−w_i).\n   - The firm prefers higher-productivity workers, leading to inefficient matching.\n   - β∗ decreases with τ_f, further distorting the reservation productivity and worsening efficiency.",
    "question_context": "One way of achieving efficiency of equilibrium is to raise worker bargaining power so that workers appropriate all, or nearly all, of the surplus. Another way of achieving complete efficiency is to impose a 100% profit tax, combined with lump-sum redistribution of the proceeds, as this would equalise the firm's after-tax profits from each type at zero, and it would therefore be indifferent about whom it hired.\nAn increase in a profit tax is (partially) shifted to workers by a lowering of the workers′ share of the average product, β∗ (recall that wᵉ=β∗aᵉ), and the tax rate affects the reservation productivity only through β∗.\nThe introduction of a wage tax will also have perverse effects, for the same reason; some of the tax will be shifted on to the firm by an increase in effective worker bargaining power, β∗, so the efficiency of equilibrium will rise.\nLet τ_f be the profit tax rate, so the firms' net profit from employing a type i worker is (1−τ_f)(a_i−w_i) and τ_w the wage tax rate, so that the after-tax wage is (1−τ_w)w_i.\nβ∗(τ_f,τ_w)=β(1−τ_f)(r+s+x)/[β(1−τ_f)(r+s+x)+(1−β)(1−τ_w)(r+s+q_f x)].\nFor any positive firm(worker) employment subsidy there exists a critical tax rate, τ_f∗(τ_w∗), such that for all τ_f≥τ_f∗ (τ_w≥τ_w∗) the equilibrium is efficient.\n\nThis section explores the relationship between taxes, worker bargaining power, and efficiency in equilibrium. It discusses how different tax policies affect the efficiency of matching models, contrasting them with Walrasian models."
  },
  {
    "qid": "econ-empirical-1558-0-1-0",
    "question": "1) State and interpret Theorem 1 regarding the characterization of idiosyncratic sequences in the GDFM.",
    "gold_answer": "Theorem 1 states that a sequence $\\{x_{it}\\}$ is idiosyncratic if and only if the maximum eigenvalue of its spectral density matrix $\\pmb{\\Sigma}_n^x(\\theta)$ is uniformly bounded for all $n$ and $\\theta \\in [-\\pi, \\pi]$. This means idiosyncratic components do not dominate the cross-sectional variation as $n \\to \\infty$.",
    "question_context": "We prove (in Theorem 1) that $x_{i t}$ is idiosyncratic if and only if the maximum eigenvalue of $\\pmb{\\Sigma}_{n}^{x}$ is dominated by an essentially bounded function defined on $[-\\pi,\\pi]$ and independent of $n$.\nWe prove in Theorem 2 that a sequence has a generalized dynamic factor structure with $q$ factors if and only if (i) the $(q+1)\\text{st}$ eigenvalue of $\\pmb{\\Sigma}_{n}^{x}$ is dominated for any $n$ by an essentially bounded function of the frequency $\\theta$; (ii) as $n$ tends to infinity, the $q$th eigenvalue diverges for $\\theta$ almost everywhere in $[-\\pi,\\pi]$.\nThe common component of $x_{i t}$ can be recovered as the limit of the projection of $x_{i t}$ on the dynamic principal components.\n\nThe paper characterizes the GDFM in terms of spectral density matrices and links common factors to dynamic principal components."
  },
  {
    "qid": "econ-empirical-72-4-0-3",
    "question": "4) Critically evaluate the hypothesis that higher 401(k) participation under automatic enrollment is due to reshuffling of assets from other savings vehicles. What empirical evidence contradicts this hypothesis?",
    "gold_answer": "1. The reshuffling hypothesis implies no net increase in savings, only a reallocation.\n2. Empirical evidence shows no change in ESPP participation rates post-automatic enrollment.\n3. The average ESPP contribution rate remains stable at ~5%, inconsistent with reshuffling.\n4. Tax benefits and employer matches make 401(k) participation strictly dominant, making reshuffling suboptimal.",
    "question_context": "Automatic enrollment dramatically increases the average 401(k) participation rate.\nA substantial fraction of 401(k) participants hired under automatic enrollment exhibit default savings behavior, with a contribution rate and asset allocation corresponding to the automatic enrollment default.\nThe fraction of automatic enrollees exhibiting default savings behavior declines with tenure, but is still large after two years.\n401(k) participants hired under automatic enrollment who change their savings behavior are substantially more likely to invest in the automatic enrollment default fund than employees hired before automatic enrollment.\nFrom the perspective of an economist, these findings are particularly interesting because there were no changes in the economic features of the 401(k) plan when automatic enrollment was implemented.\nOne explanation for at least the first four of the broad findings noted above is a status quo bias resulting from employee procrastination in making or implementing an optimal savings decision.\nSamuelson and Zeckhauser [1988] note that it may be rational to stick with the status quo when there are transaction costs involved in switching to another alternative.\nThere are at least two sources of complexity involved in making an optimal 401(k) savings decision. First, the array of participation options is immense. Individuals must first choose what fraction of compensation to contribute to the 401(k) plan, anything from 1 to 15 percent. They must then choose how to allocate that contribution between the nine available fund options.\nThe psychological literature has documented this notion that increasing the complexity of a decision-making task leads to procrastination [Tversky and Shafir 1992; Shafir, Simonson, and Tversky 1993].\nOne of the likely reasons why 401(k) participation is so much higher under automatic enrollment for young and low income employees who have less financial experience is that automatic enrollment decreases the complexity of the 401(k) savings decision by decoupling the participation decision from the investment decision.\nSeveral of the savings patterns noted previously are consistent with procrastination resulting from the transactions costs involved in making an optimal savings decision.\nThe relationship between income and 401(k) participation before automatic enrollment and nondefault 401(k) participation after automatic enrollment is also suggestive of transactions costs as an explanation for procrastination.\nRecent research in behavioral economics has fingered another reason for procrastination in savings decisions—individual problems with self-control [O'Donoghue and Rabin 1998; Diamond and Koszegi 2000; Laibson, Repetto, and Tobacman 1998].\nAnother behavioral explanation that may account for some of the findings enumerated above (specifically the first, third, and fifth findings) is a bias for the status quo driven by what Thaler [1980] has termed the 'endowment effect.'\nAnother type of status quo bias that may help explain the default savings behavior seen among automatic enrollees derives from the complexity of the 401(k) savings decision.\nA related behavioral explanation for the predominance of the money market fund as an investment option even among automatic enrollees who have changed some aspect of their 401(k) savings behavior (the nondefault participants) is anchoring [Tversky and Kahneman 1974].\nA final behavioral explanation for the higher 401(k) participation rates under automatic enrollment is the framing of the 401(k) participation decision.\nNone of the explanations discussed so far helps to explain the final finding of this paper noted above—that 401(k) participants hired before automatic enrollment who do not become participants until after the switch to automatic enrollment are substantially more likely to invest in the automatic enrollment default fund.\nThe most likely explanation for this type of behavior is that employees view the default investment allocation under automatic enrollment as implicit advice from the company on 'the best' allocation of one’s retirement assets.\n\nThis section analyzes the impact of automatic enrollment on 401(k) participation rates and savings behavior, exploring economic and behavioral explanations for observed patterns."
  },
  {
    "qid": "econ-empirical-1367-4-1-0",
    "question": "3) Derive the expected increase in $\\beta$ when moving from 1 to 3 averaged observations under classical measurement error, given $\\text{Corr}(S_1, S_2) = 0.65$.",
    "gold_answer": "1. Reliability with $n$ measures: $\\rho_n = \\frac{n\\sigma^2_\\theta}{n\\sigma^2_\\theta + \\sigma^2_\\epsilon}$. \\n2. Given $\\rho_1 = 0.65^2 = 0.4225$, solve for $\\sigma^2_\\theta / \\sigma^2_\\epsilon = 0.4225/(1-0.4225) \\approx 0.732$. \\n3. For $n=3$: $\\rho_3 = \\frac{3 \\times 0.732}{3 \\times 0.732 + 1} \\approx 0.687$. \\n4. Expected $\\beta$ increase: $0.687/0.4225 - 1 \\approx 63\\%$ (vs. observed 26-31%, suggesting non-classical error).",
    "question_context": "Going from one observation to an average of two father observations increases the father-son association by 15-20 percent.\nIf one goes further and averages three father observations, then persistence estimates are 26-31 percent higher than the one observation estimate.\n\nThis section analyzes how averaging multiple father observations affects mobility estimates and tests classical measurement error assumptions."
  },
  {
    "qid": "econ-empirical-502-4-1-0",
    "question": "3) Derive the asymptotic criterion $Q(A)$ and explain its role in proving the consistency of the minimum chi-square estimator.",
    "gold_answer": "The asymptotic criterion $Q(A)$ is given by: $$Q(A)=\\int\\int{\\frac{[f(x,y)-f(x,y;A)]^{2}}{f(x,y)}}\\mathrm{d}x\\mathrm{d}y,\\quad A\\in{\\mathcal{O}}.$$ It measures the integrated squared difference between the true density $f(x,y)$ and the model density $f(x,y;A)$, weighted by the inverse of the true density. The consistency of the minimum chi-square estimator is proven by analyzing the properties of $Q(A)$ as outlined in Lemma B.2(i), (iv), and (v).",
    "question_context": "As usual the proof is based on the analysis of the asymptotic criterion: ${\\mathcal{Q}}_{\\infty}(A)=Q(A)=\\int\\int{\\frac{[f(x,y)-f(x,y;A)]^{2}}{f(x,y)}}\\mathrm{d}x\\mathrm{d}y,\\quad A\\in{\\mathcal{O}}$. We have the following Lemma B.2. In particular, the consistency of the minimum chi-square estimator is a direct consequence of Lemma B.2(i), (iv) and (v).\n\nThe proof is based on the analysis of the asymptotic criterion ${\\mathcal{Q}}_{\\infty}(A)=Q(A)=\\int\\int{\\frac{[f(x,y)-f(x,y;A)]^{2}}{f(x,y)}}\\mathrm{d}x\\mathrm{d}y,\\quad A\\in{\\mathcal{O}}$. The consistency of the minimum chi-square estimator is a direct consequence of Lemma B.2(i), (iv), and (v)."
  },
  {
    "qid": "econ-empirical-628-0-0-2",
    "question": "3) Discuss the two suggested methods to minimize the aliasing problem and their practical limitations, especially in economic studies.",
    "gold_answer": "The two methods are:  \n1. **Reducing sampling interval $h$**: This minimizes high-frequency components outside $(-\\pi/h, \\pi/h)$. Limitation: Requires control over sampling, which is often impossible in economics.  \n2. **Non-periodic (additively random) sampling**: Shapiro and Silverman (1960) showed uniqueness if interval increments' characteristic function takes no value more than once. Limitation: Also requires controlled sampling, which is rarely feasible in economics.",
    "question_context": "One of the problems that arise in the estimation of the structural parameters of linear stochastic differential equations is that the function relating the parameters of the continuous time model to those of the corresponding discrete time model is not, in general, bijective.\nThe manifestation of the problem in the frequency domain is the aliasing phenomena, which is well known. If $f(\\omega)$ is the spectral density matrix of the stationary random process $y(t),-\\infty<t<\\infty$, then $f(\\omega)$ cannot be uniquely determined from the folded spectrum $f^{(h)}(\\omega)=f(\\omega)+\\sum_{n=1}^{\\infty}~\\{f(\\omega+2n\\pi/h)+f(\\omega-2n\\pi/h)\\};\\qquad|\\omega|\\le\\pi/h$.\nThe fact that there is a whole class of different spectral density matrices $f(\\omega)$ consistent with $f^{(h)}(\\omega)$ accords with the fact that there is no bijection between the parameters of the continuous time model and its corresponding discrete time model.\n\nThe text discusses the challenges in estimating structural parameters of linear stochastic differential equations, focusing on the non-bijective relationship between continuous and discrete time models and the aliasing phenomenon in the frequency domain."
  },
  {
    "qid": "econ-empirical-802-4-1-1",
    "question": "6) Show how the parameters $\\xi$ and $z$ are determined to match $d(0) = 0.03$ and $h(0) = \\frac{1}{3}$ in the US calibration, and interpret their economic roles.",
    "gold_answer": "1. **$\\xi$ Determination**: At $F = 0$, $d(0) = \\delta + \\lambda(0) = 0.03$. Since $\\delta = 0.01$, $\\lambda(0) = 0.02$. The endogenous turnover rate $\\lambda(0)$ is a function of $\\xi$, which is adjusted to satisfy this condition. \\n2. **$z$ Determination**: The hiring rate $h(0) = \\frac{1}{3}$ depends on the worker's outside option $z$, which is calibrated to match this target. \\n3. **Roles**: $\\xi$ controls layoff sensitivity to $F$, while $z$ affects job acceptance decisions.",
    "question_context": "Blanchard and Portugal (2001) calibrate their model to match stylised facts of the US and the Portuguese labour markets... I choose parameters to match $h(0) = \\frac{1}{3}$, $d(0) = 0.03$, and $d(F_{\\phi}) = 0.01$.\nThe parameters I use to match the three turnover rates are $\\delta, g,$ and $z$. Since at prohibitive firing costs all separations are quits, the condition $d(F_{\\phi}) = 0.01$ pins down the quit rate $\\delta = 0.01$.\nI set the monthly discount rate to $r = 0.0041$ for a $5\\%$ annual rate. This leaves two parameters to be chosen, the bargaining share $\\beta$ and the creation cost $c$.\n\nThis section calibrates the model to match empirical labor market outcomes in the US and Portugal, focusing on hiring and separation rates under different firing cost regimes."
  },
  {
    "qid": "econ-empirical-1388-0-1-0",
    "question": "5) Using Table 8, explain why the GLS method assigns a negative discrepancy (-$19.4B) to the Mining sector, while the profit-share method assigns +$1.25B. What does this imply about the reliability of Mining sector data?",
    "gold_answer": "**Interpretation**:\n1. **GLS Result**: Negative adjustment (-$19.4B) suggests Mining VA was initially overestimated due to low data reliability (high CV).\n2. **Profit-Share Result**: +$1.25B assumes discrepancy scales with GOS share (2.69%), ignoring reliability.\n3. **Implication**: Mining data likely had high measurement error (e.g., from SOI/IRS profit reports), necessitating a downward GLS adjustment.",
    "question_context": "The aggregate discrepancy for 1997 was 46.5 billion dollars, that is, GDI was 46.5 billion dollars below GDP. Under the approach based on industry profit shares, the discrepancy was distributed to each sector as an increase in its initial VA estimates in the GDP-by-industry accounts in column 3, proportional to its share of total GOS in column 4.\nGLS estimation, based on relative reliability of initial estimates in both IO and GDP-by-industry accounts, resulted in either positive or negative adjustments to the sectors’ initial VA estimates in column 2.\n\nThe study applies GLS to reconcile the 1997 U.S. Input-Output and GDP-by-industry accounts, distributing a $46.5B statistical discrepancy across 65 industries based on data reliability."
  },
  {
    "qid": "econ-empirical-1285-0-0-0",
    "question": "1) Derive the key labor market equilibrium conditions in the model, considering the formal and informal sectors. How does the informal sector's flexibility affect wage dynamics?",
    "gold_answer": "1. **Formal Sector**: The wage \\( w_t^f \\) is determined by Nash bargaining: \n   \\[ (1-\\gamma)\\lambda_t^L = \\gamma J_t^f \n   where \\( \\lambda_t^L \\) is the household's shadow value of a formal worker and \\( J_t^f \\) is the firm's value of a formal worker.\n2. **Informal Sector**: The wage \\( \\widehat{w}_t \\) is similarly determined but without financial frictions: \n   \\[ (1-\\gamma)\\widehat{\\lambda}_t^L = \\gamma \\widehat{J}_t \n3. The informal sector's flexibility dampens wage adjustments, as it absorbs labor demand shocks without bidding up wages.",
    "question_context": "Informality is a structural trait in emerging economies affecting the behavior of labor markets, financial access and economy-wide productivity. This paper develops a simple general equilibrium closed economy model with nominal rigidities, labor and financial frictions to analyze the transmission of shocks and of monetary policy.\nIn the model, the informal sector provides a flexible margin of adjustment to the labor market at the cost of a lower productivity. In addition, only formal sector firms have access to financing, which is instrumental in their production process.\n\nThe paper develops a DSGE model to analyze the impact of informality on monetary policy in emerging economies, focusing on labor market flexibility and financial exclusion in the informal sector."
  },
  {
    "qid": "econ-empirical-1310-4-0-2",
    "question": "3) Using Table 9, test the hypothesis that the skewness of residual premiums is significantly greater for FI plans than for SI plans. Assume the standard error of skewness is \\( \\sqrt{6/N} \\) for both distributions.",
    "gold_answer": "1. **Standard errors**: \n   - FI plans: \\( SE_{FI} = \\sqrt{6/4287} \\approx 0.0374 \\)\n   - SI plans: \\( SE_{SI} = \\sqrt{6/974} \\approx 0.0785 \\)\n2. **Test statistic**: \\( z = \\frac{1.48 - 0.32}{\\sqrt{0.0374^2 + 0.0785^2}} \\approx \\frac{1.16}{0.087} \\approx 13.33 \\)\n3. **Conclusion**: Reject the null hypothesis (p < 0.001), supporting greater skewness in FI residuals.",
    "question_context": "Our analysis focuses on 5,261 establishments that offered a non-HMO plan as their dominant plan option when surveyed. Within this sample, the strongest predictor of SI status is firm size. Among firms with 35 or fewer workers, only 2.5 percent of establishments offered SI plans, while SI plans dominate establishments within larger firms.\nThe variable of interest is the 'single monthly premium' recorded for the dominant plan at the surveyed establishment. For both FI and SI employers, the premium includes the contributions of employers and employees. We assume that for SI insurers, the premium figure is best understood as approximating the expected no-load cost of insurance.\nMean premiums are nearly the same across SI and FI group plans, but the distribution of FI group premiums shows substantially higher variance and a more pronounced right skew.\nThe greater premium variance observed in FI group plans is plausibly the result of search frictions, but could also result from greater heterogeneity in the expected costs associated with FI plans. We therefore focus our attention on 'residual premiums,' i.e., on the premium that is left unexplained by a premium regression estimated using a large number of control variables.\nThe premium prediction models were estimated via generalized linear model (GLM) using the log 'link' function and gamma distributional family. Separate regressions were run for FI and SI plans, and both regressions included identical covariates measuring plan and establishment characteristics.\nEven after conditioning on group and plan characteristics, the residual premium variance is much higher for FI plans than SI plans. The included covariates explain a much smaller fraction of the original variance in premiums in FI group plans (about 20 percent) than for SI group plans (about 54 percent). The distribution of residuals for FI group plans also has the predicted right skew not evident among SI group plans.\n\nThis section analyzes price dispersion in the health insurance market using data from the 1997 Robert Wood Johnson Foundation Employer Health Insurance Survey (EHIS). The focus is on comparing the empirical distribution of group insurance premiums between Fully Insured (FI) and Self-Insured (SI) plans, with particular attention to the role of search frictions in FI plans."
  },
  {
    "qid": "econ-empirical-296-4-0-0",
    "question": "1) Derive the key conditions under which the supremum expression in Lemma 3 converges to zero in probability, and explain the roles of εₙ, bₙ, and n in ensuring this convergence.",
    "gold_answer": "1. **Lemma 3 Condition**: The supremum converges to zero when:\n   - Step size εₙ → 0\n   - Bandwidth bₙ → 0\n   - εₙ min{bₙ⁻ᵈ, nᵠ} → ∞ (controls bias-variance tradeoff)\n   - nεₙbₙᵈᶻ/(n²ʳ⁰ log n) → ∞ (ensures sufficient observations)\n2. **Role of Parameters**:\n   - εₙ controls finite-difference approximation error\n   - bₙ governs kernel estimator bandwidth\n   - n ensures asymptotic convergence",
    "question_context": "$$\\begin{array}{c}{{\\displaystyle\\operatorname*{sup}_{\\substack{d\\left(\\theta,\\theta_{0}\\right)=o\\left(1\\right),d\\left(\\eta,\\eta_{0}\\right)=o\\left(1\\right),\\eta\\in\\mathcal{H}_{n}}}\\left|L_{1,p}^{\\epsilon_{n},w}\\widehat{m}\\left(\\theta,\\eta,z\\right)-L_{1,p}^{\\epsilon_{n},w}m\\left(\\theta,\\eta,z\\right)\\right|}}\\ {{\\displaystyle\\vphantom{\\sum_{\\theta_{p}}^{\\theta_{n}}}}}\\ {{\\displaystyle\\quad=o_{p}(1)}}\\end{array}$$\nTheorem 5. Under Assumptions 2 and 7, and either 6 or 8, $L_{1,p}^{\\epsilon_{n},w}\\widehat{m}$ $\\begin{array}{r}{\\left(\\hat{\\theta},\\hat{\\eta},z\\right)\\xrightarrow{p}\\frac{\\partial m(\\theta,\\eta,z)}{\\partial\\eta}[w],}\\end{array}$ > am@.n2 [w], uniformly in z and w, if en →> 0, N → $\\infty,\\epsilon_{n}\\operatorname*{min}\\{N^{\\alpha},n^{\\phi}\\}\\rightarrow\\infty$ , and $\\frac{n\\epsilon_{n}}{\\zeta_{0}(N)^{2}N n^{2r_{0}}\\log n}\\rightarrow\\infty$ for series estimator, and $\\epsilon_{n},b_{n}\\to0,\\epsilon_{n}\\operatorname*{min}\\{b_{n}^{-q},n^{\\phi}\\}\\to\\infty$ , and $\\frac{n\\epsilon_{n}b_{n}^{d_{z}}}{n^{2r_{0}}\\log n}\\rightarrow\\infty$ for kernel-based estimator, provided that $d\\Big(\\hat{\\theta},\\theta_{0}\\Big)=o_{p}(1)$ and $d\\left(\\hat{\\eta},\\eta_{0}\\right)=o_{p}\\left(\\O_{\\cdot}$ ).\nThe rate of convergence adapts the proof of Theorem 3.2.5 of Van der Vaart and Wellner (1996) to our case. Denote the rate of convergence for the estimator θ by $\\rho_{n}$ . Then we can partition the parameters space into sets $S_{j,n}=\\bigl\\{\\theta:2^{j-1}<\\rho_{n}d(\\bar{\\theta},\\theta_{0})<2^{j}\\bigr\\}$ . Then we evaluate the probability of a large deviation $\\rho_{n}d\\left(\\widehat{\\theta},\\theta_{0}\\right)>$ $2^{M}$ for some integer $M$ , where $\\rho_{n}=\\sqrt{n}\\epsilon_{n}^{1-\\gamma}$ . We know that the estimator solves, for $r_{n}=\\epsilon_{n}^{1-\\gamma}$ $$\\sqrt{n}r_{n}L_{1,p}^{\\epsilon_{n}}Q_{n}\\left(\\widehat{\\theta}\\right)=o_{p}\\left(1\\right).$$\n$$\\begin{array}{r l r}{\\lefteqn{P\\left(\\rho_{n}d\\left(\\widehat{\\theta},\\theta_{0}\\right)>2^{M}\\right)}}\\ &{}&{\\leq\\displaystyle\\sum_{j\\geq M}P\\left(\\operatorname*{sup}_{\\theta\\in S_{j,n}}\\left(-\\left\\Vert L_{1,p}^{\\epsilon_{n}}Q_{n}\\left(\\theta\\right)\\right\\Vert\\right)\\geq-o_{p}\\left(\\frac{1}{\\sqrt{n}r_{n}}\\right)\\right)}\\ &{}&{2^{j}<\\delta\\rho_{n}}\\ &{}&{\\quad+P\\left(2d\\left(\\widehat{\\theta},\\theta_{0}\\right)\\geq\\delta\\right).}\\end{array}$$\n\nThis section discusses the consistency of finite-difference based estimators for directional derivatives in semi-parametric models, with applications to sieve and kernel-based estimators. Key conditions involve convergence rates of step sizes (εₙ) and bandwidth parameters (bₙ)."
  },
  {
    "qid": "econ-empirical-1021-3-1-1",
    "question": "4) Analyze the persistence of negative veteran effects for whites through 1990 and evaluate the hypothesis that college attendance contributes to these effects.",
    "gold_answer": "1. **College attendance**: Veterans may delay or interrupt college enrollment, leading to lower earnings post-service due to reduced educational attainment.\n2. **Persistence**: The prolonged negative effects suggest structural barriers, such as skill mismatches or employer discrimination, beyond temporary educational disruptions.\n3. **Empirical support**: The hypothesis could be tested by comparing earnings of veterans with and without college attendance, controlling for other factors.",
    "question_context": "Estimates of $\\hat{\\alpha}_{c}$ by application year, plotted in Figure 3 for both race groups, show that $\\hat{\\alpha}_{c}$ is close to zero until the year of application, at which time there is a sharp break. This break is especially well-defined for nonwhite applicants.\nThe negative veteran effects for whites in the years when they were leaving the military may be due in part to college attendance. Note, however, that these negative effects persist through 1990.\n\nThis section extends the analysis of matching estimates to employment rates and explores the timing of veteran effects by application year, using Figure 3 to illustrate controlled contrasts."
  },
  {
    "qid": "econ-empirical-1595-4-0-2",
    "question": "3) Propose a framework for assessing a country's progress in meeting best-practice standards for creditworthiness enhancement. What metrics would you include?",
    "gold_answer": "**Framework Components:**\n1. **Accounting Practices:** Transparency and adherence to IFRS.\n2. **Creditor Rights:** Strength of legal protections for creditors (e.g., bankruptcy laws).\n3. **Contract Enforcement:** Efficiency of judicial systems in resolving disputes.\n\n**Metrics:**\n1. **Quantitative:**\n   - Credit ratings (e.g., Moody's, S&P).\n   - Time and cost to enforce contracts (World Bank Doing Business indicators).\n2. **Qualitative:**\n   - ROSC (Reports on Observance of Standards and Codes) assessments.\n   - Legal system efficiency scores (e.g., WJP Rule of Law Index).",
    "question_context": "The real cost of funds to emerging-market borrowers reflects both the general real interest rates in the international capital market and compensation for the risk of default and for substandard investor-protection regimes.\nRecent research finds that the preexistence of an Anglo-Saxon legal system (with generally strong investor protection) magnifies the response of investment to financial liberalization events.\nThe creditworthiness of the private sector in emerging-market economies can be enhanced significantly by making progress toward meeting best-practice standards on accounting practices, creditor rights, and contract enforcement.\nEnsuring that each actor has the appropriate incentives is of paramount importance to the reform of the process for sovereign debt restructuring.\n\nThe text discusses the importance of reforming the sovereign-debt restructuring process to encourage private-sector growth and private capital flows, which are essential for economic growth in emerging-market economies. It highlights the role of investor protection, corporate governance, and legal systems in enhancing creditworthiness and the efficiency of capital flows."
  },
  {
    "qid": "econ-empirical-39-1-0-0",
    "question": "1) Using the tournament model of Lazear and Rosen [1981], formally derive the conditions under which the skewed compensation structure in the gang could incentivize low-level members to remain in the gang despite low current wages.",
    "gold_answer": "1. **Model Setup**: Let \\( w_L \\) be the wage of low-level members and \\( w_H \\) be the wage of high-level members. The probability of promotion is \\( p \\). \\n2. **Expected Utility**: The expected utility of a low-level member is \\( EU = p \\cdot u(w_H) + (1 - p) \\cdot u(w_L) - c \\), where \\( c \\) is the cost of effort. \\n3. **Incentive Constraint**: For members to stay, \\( EU \\geq u(w_{legit}) \\), where \\( w_{legit} \\) is the legitimate market wage. \\n4. **Solution**: The gang sets \\( w_H \\) sufficiently high such that \\( p \\cdot u(w_H) + (1 - p) \\cdot u(w_L) - c \\geq u(w_{legit}) \\).",
    "question_context": "On average, earnings in the gang are somewhat above the legitimate labor market alternative. The enormous risks of drug selling, however, more than offset this small wage premium. Compensation within the gang is highly skewed, and the prospect of future riches, not current wages, is the primary economic motivation.\nThe gang engages in repeated gang wars and sometimes prices below marginal cost. Our results suggest that economic factors alone are unlikely to adequately explain individual participation in the gang or gang behavior.\nStreet-level sellers appear to earn roughly the minimum wage. Earnings within the gang are enormously skewed, however, with high-level gang members earning far more than their legitimate market alternative. Thus, the primary economic motivation for low-level gang members appears to be the possibility of rising up through the hierarchy, as in the tournament model of Lazear and Rosen [1981].\nGang wars are costly, both in terms of lost lives and lost profits. Almost all of the deaths of drug sellers are concentrated in war periods. Moreover, the violence keeps customers away. This negative shock to demand is associated with a fall of 20–30 percent in both the price and quantity of drugs sold during fighting, and the drug operation becomes far less profitable.\nFinally, drug selling is an extremely dangerous activity. Death rates in the sample are 7 percent annually. Given the relatively low economic returns to drug selling noted above, the implied willingness to accept risk on the part of the participants is orders of magnitude higher than is typically observed in value of life calculations.\n\nThe paper analyzes the financial activities of a drug-selling street gang using a unique dataset. It examines earnings, risks, compensation structures, and the economic motivations behind gang participation. The study also explores gang wars, pricing strategies, and the dangers associated with drug selling."
  },
  {
    "qid": "econ-empirical-1280-0-0-2",
    "question": "3) Show how the regression coefficients \\( b_{N} \\) are related under the affine-model restriction, and derive the restriction \\( b_{N} = (b_{2})^{N-1} \\).",
    "gold_answer": "1. From the term structure equation: \\( f_{t,N} = (\\rho^{\\mathbb{Q}})^{N}x_{t} \\).\n2. Substitute \\( x_{t} = f_{t,1}/\\rho^{\\mathbb{Q}} \\): \\( f_{t,N} = (\\rho^{\\mathbb{Q}})^{N-1}f_{t,1} \\).\n3. The regression coefficient \\( b_{N} \\) is the slope in \\( f_{t,N} = b_{N}f_{t,1} + \\epsilon \\).\n4. Thus, \\( b_{N} = (\\rho^{\\mathbb{Q}})^{N-1} \\).\n5. Since \\( b_{2} = \\rho^{\\mathbb{Q}} \\), the restriction follows: \\( b_{N} = (b_{2})^{N-1} \\).",
    "question_context": "We define as 'standard' any model in which cash flows and asset prices are linear functions of common factors. This type of model is pervasive in financial economics because of its convenience in delivering closed-form pricing solutions in a wide range of valuation problems.\nUnder the pricing measure ℚ, cash flows evolve according to: \\( x_{t} = \\rho^{\\mathbb{Q}}x_{t-1} + \\epsilon_{t}^{\\mathbb{Q}} \\). The price of a n-maturity forward claim on these cash flows is \\( f_{t,n} = E_{t}^{\\mathbb{Q}}[x_{t+n}] \\).\nThe term structure of forward prices at maturities \\( 1,\\ldots,N \\) is given by: \\( f_{t,1} = \\rho^{\\mathbb{Q}}x_{t}, \\quad f_{t,2} = (\\rho^{\\mathbb{Q}})^{2}x_{t}, \\quad \\ldots, \\quad f_{t,N} = (\\rho^{\\mathbb{Q}})^{N}x_{t} \\).\nThe variance ratio statistic for each maturity \\( N \\) is defined as: \\( V R_{N} = \\frac{V a r(b_{N}f_{t,1})}{V a r((b_{2})^{N-1}f_{t,1})} \\). Under the null model, \\( V R_{N} = 1 \\). If the ratio significantly exceeds 1, it indicates excess volatility in long-maturity prices.\n\nThe paper documents excess volatility in long-maturity prices relative to short-maturity prices, which violates the internal consistency conditions imposed by standard affine-ℚ models. This phenomenon is observed across multiple asset classes, including equity options, currency options, credit default swaps, commodity futures, variance swaps, and inflation swaps."
  },
  {
    "qid": "econ-empirical-800-2-1-2",
    "question": "3) Show that $-(n-1)\\hat{\\mathbf{H}}_{12} | (n-1)\\hat{\\mathbf{H}}_{11} \\sim \\mathcal{N}_{m-1}(-(n-1)\\hat{\\mathbf{H}}_{11}\\mathbf{H}_{11}^{-1}\\mathbf{H}_{12}, (\\mathbf{H}_{22} - \\mathbf{H}_{21}\\mathbf{H}_{11}^{-1}\\mathbf{H}_{12})(n-1)\\hat{\\mathbf{H}}_{11})$.",
    "gold_answer": "1. From Theorem 3.2.10 of Muirhead (1982):\n   - Given $(n-1)\\hat{\\mathbf{H}} \\sim W_{m}(n-k+m-1, \\mathbf{H})$.\n   - Partition $\\hat{\\mathbf{H}}$ into blocks $\\hat{\\mathbf{H}}_{11}, \\hat{\\mathbf{H}}_{12}, \\hat{\\mathbf{H}}_{22}$.\n   - The conditional distribution of $\\hat{\\mathbf{H}}_{12}$ given $\\hat{\\mathbf{H}}_{11}$ is:\n     - $\\hat{\\mathbf{H}}_{12} | \\hat{\\mathbf{H}}_{11} \\sim \\mathcal{N}_{m-1}(\\hat{\\mathbf{H}}_{11}\\mathbf{H}_{11}^{-1}\\mathbf{H}_{12}, (\\mathbf{H}_{22} - \\mathbf{H}_{21}\\mathbf{H}_{11}^{-1}\\mathbf{H}_{12})\\hat{\\mathbf{H}}_{11})$.\n   - Scale by $(n-1)$: $-(n-1)\\hat{\\mathbf{H}}_{12} | (n-1)\\hat{\\mathbf{H}}_{11} \\sim \\mathcal{N}_{m-1}(-(n-1)\\hat{\\mathbf{H}}_{11}\\mathbf{H}_{11}^{-1}\\mathbf{H}_{12}, (\\mathbf{H}_{22} - \\mathbf{H}_{21}\\mathbf{H}_{11}^{-1}\\mathbf{H}_{12})(n-1)\\hat{\\mathbf{H}}_{11})$.",
    "question_context": "Let $\\mathbf{M}^{\\prime}=(\\mathbf{e}_{1}\\mathbf{e}_{2}\\dotsm_{\\hat{r}\\hat{r}-1}\\mathbf{e}_{m-1}\\mathbf{1}),\\hat{\\mathbf{H}}=(\\mathbf{M}\\hat{\\boldsymbol{\\Sigma}}^{-1}\\mathbf{M}^{\\prime})^{-1}=\\{\\hat{\\mathbf{H}}_{i j}\\}_{i,j=1,2}$ and $\\hat{\\mathbf{H}}^{-1}=\\mathbf{M}\\hat{\\boldsymbol{\\Sigma}}^{-1}\\mathbf{M}^{\\prime}=\\{\\hat{\\mathbf{H}}_{i j}^{(-)}\\}_{i,j=1,2}$ , with $\\hat{\\bf{H}}_{22}^{(-)}={\\bf{1}}^{\\prime}\\hat{\\bf{\\Sigma}}_{.}^{-1}{\\bf{1}}$ , etc. The matrices $\\mathbf{H}^{-1}$ and $\\mathbf{H}$ are partitioned in the same way.\nSince $(n-1)\\hat{\\Sigma}{\\sim}W_{k}(n-1,\\Sigma)$ , it follows that $(n-1)\\bar{H}{\\sim}W_{m}(n-k+m-1,({\\bf M}\\Sigma^{-1}{\\bf M}^{\\prime})^{-1})$.\nThen $\\hat{\\tilde{\\mathbf{w}}}_{\\mathrm{GMV}}=\\frac{\\hat{\\mathbf{H}}_{12}^{(-)}}{\\hat{\\mathbf{H}}_{22}^{(-)}}=\\frac{-\\hat{\\mathbf{H}}_{11}^{-1}\\hat{\\mathbf{H}}_{12}(\\hat{\\mathbf{H}}_{22}-\\hat{\\mathbf{H}}_{21}\\hat{\\mathbf{H}}_{11}^{-1}\\hat{\\mathbf{H}}_{12})^{-1}}{(\\hat{\\mathbf{H}}_{22}-\\hat{\\mathbf{H}}_{21}\\hat{\\mathbf{H}}_{11}^{-1}\\hat{\\mathbf{H}}_{12})^{-1}}=-\\hat{\\mathbf{H}}_{11}^{-1}\\hat{\\mathbf{H}}_{12}$.\n\nThis section provides the proof for Proposition 1, focusing on the distribution of the estimated weights in the GMV framework. The proof leverages properties of the Wishart distribution and conditional normality."
  },
  {
    "qid": "econ-empirical-1480-0-1-1",
    "question": "6) Using the closed-form solutions for $a_D^S(f)$ and $a_X^S(f)$, prove that $\\frac{d a_D^S}{d f} < 0$ and $\\frac{d a_X^S}{d f} > 0$. Interpret these results in terms of firm selection.",
    "gold_answer": "From the closed-form solutions:\n\n1. **Domestic cutoff**: $a_D^S(f) = \\left[\\frac{1}{f} \\frac{\\mathsf{A}}{\\mathsf{B} + \\mathsf{C}(f)^{\\frac{k-(\\varepsilon-1)}{\\varepsilon-1}}}\\right]^{1/k}$. As $f$ increases, the denominator grows, so $a_D^S(f)$ falls.\n2. **Export cutoff**: $a_X^S(f) = \\left[\\frac{1}{f_X} \\left(\\mathsf{F} + \\frac{\\mathsf{D A}}{\\mathsf{B}(f)^{-\\frac{k-(\\varepsilon-1)}{\\varepsilon-1}} + \\mathsf{C}}\\right)\\right]^{1/k}$. Higher $f$ reduces the term $\\mathsf{B}(f)^{-\\frac{k-(\\varepsilon-1)}{\\varepsilon-1}}$, raising $a_X^S(f)$.\n\nInterpretation:\n- **$\\frac{d a_D^S}{d f} < 0$**: Worse institutions ($f \\uparrow$) exclude marginal domestic firms.\n- **$\\frac{d a_X^S}{d f} > 0$**: Higher $f$ reduces domestic competition, allowing less productive firms to export.",
    "question_context": "Barriers to entry are determined endogenously in a political economy equilibrium. The key assumption is that political power is positively related to economic size: the larger the firm, the more political weight it has.\nThe distribution of profits becomes more unequal than it was in autarky: larger firms grow larger, while smaller firms become smaller or disappear under trade. Thus, greater trade exposure can potentially result in an economy dominated by a small elite of large exporters.\nInstitutions are most likely to deteriorate when the country is small relative to the rest of the world, but captures a relatively large share of world trade in the industry subject to rent seeking.\n\nThe paper endogenizes institutional quality through a political economy equilibrium where firms' voting weights depend on economic size. Trade opening alters the distribution of political power, leading to comparative statics that depend on country size and world market share."
  },
  {
    "qid": "econ-empirical-804-0-1-0",
    "question": "1) Formalize Stiglitz's critique of the first welfare theorem under imperfect information, using a principal-agent framework.",
    "gold_answer": "1. Principal-agent model with asymmetric information: Agent effort \\( e \\) is unobservable.  \n2. First-best outcome requires \\( e = e^* \\), but second-best outcome yields \\( e < e^* \\) due to moral hazard.  \n3. Market fails to achieve Pareto efficiency as contracts are incomplete.  \n4. Conclusion: The first welfare theorem does not hold under imperfect information.",
    "question_context": "Stiglitz completes a thorough demolition job on the fundamental theorems of welfare economics - he sees them as fundamentally flawed in real economies characterised by imperfections of information and non-convexities.\n\nStiglitz critiques the neoclassical model and market socialism, arguing that imperfect information undermines market efficiency and that the Lange-Lerner theorem is misguided."
  },
  {
    "qid": "econ-empirical-1495-1-0-2",
    "question": "3) Show that Player 1's stationary strategy $\\sigma_1$ is a best response to Player 2's expected strategy $\\bar{\\sigma}_2(s) = \\sum_n \\alpha(n) \\sigma_2^n(s)$.",
    "gold_answer": "1. Player 1's payoff depends on Player 2's expected strategy $\\bar{\\sigma}_2(s)$. \n2. For each $s \\in S$, Player 1 chooses $\\sigma_1(s)$ to maximize: \n   $$(1-\\delta) u_1(\\sigma_1(s), \\bar{\\sigma}_2(s)) + \\delta \\sum_{s'} P(s' | s, \\sigma_1(s)) U_1(s')$$\n   where $P(s' | s, \\sigma_1(s))$ is the transition probability and $U_1(s')$ is the continuation payoff. \n3. The stationary strategy $\\sigma_1$ satisfies this maximization for all $s$.",
    "question_context": "For a given cost function $C$, individual rationality of Player 2 implies that he never buys more than $N_{C}$ periods of information. Fix any $N\\geq N_{C}$. Let us define the state space ${\\cal{S}}=\\{H,L\\}^{N}$ as the set of Player 1’s feasible plays in the last $N$ periods.\nPlayer 2’s information acquisition strategy is denoted by a probability measure $\\alpha\\in\\Delta\\{1,2,\\ldots,N\\}$. With probability $\\alpha(n)$, Player 2 acquires $n$ periods of information, and his information is represented by a partition $\\mathcal{P}^{n}$ on $S$.\nPlayer 2’s strategy after acquiring $n$ periods of information is a $\\mathcal{P}^{n}$-measurable function $\\sigma_{2}^{n}\\colon S\\to[0,1]$ which specifies a probability of playing the trusting action $h$ for each $s\\in S$.\nPlayer 1’s stationary strategy is denoted by $\\sigma_{1}\\colon S\\to[0,1]$, specifying a probability of playing $H$ for each $s\\in S$.\n\nThe equilibrium notion used in this paper is perfect Bayesian equilibrium in stationary strategies tailored for our model without calendar times. The state space is defined based on Player 1's feasible plays in the last N periods, with Player 1 knowing the true state and Player 2 needing to acquire information to estimate it."
  },
  {
    "qid": "econ-empirical-1299-1-0-2",
    "question": "3) Explain the econometric challenges in estimating the effect of institutional ownership on innovation and how the paper addresses them.",
    "gold_answer": "Key challenges and solutions:\n\n1. **Endogeneity**: Institutional ownership may correlate with unobserved firm characteristics. The paper uses:\n   - **Instrumental Variables**: Exogenous changes in ownership (e.g., S&P 500 inclusion).\n   - **Fixed Effects**: Controls for time-invariant firm heterogeneity.\n\n2. **Count Data**: Patent citations are non-negative integers. Addressed via:\n   - **Poisson/Negative Binomial Models**: Suitable for count data.\n   - **Presample Mean Scaling**: Accounts for unobserved heterogeneity without strict exogeneity.\n\n3. **Censoring**: Later patents have fewer citation years. Addressed by:\n   - **Time Dummies**: Control for citation window differences.\n   - **Sample Restriction**: Estimates up to 1999 to allow a 3-year citation window.",
    "question_context": "We find that greater institutional ownership is associated with more innovation. To explore the mechanism, we contrast the 'lazy manager' hypothesis with a model where institutional owners increase innovation incentives through reducing career risks. The evidence favors career concerns.\nThe lazy manager hypothesis predicts that if competition is high then there is no need for intensive monitoring as the manager is disciplined by the threat of bankruptcy or takeover to work hard. In contrast, our career concern model predicts that more intense competition reinforces the positive effect of institutional investment on managerial incentives.\nThe conditional expectation, $E(\\cdot|\\cdot)$ of this measure of innovation is given by: $$ E(C I T E S_{i t}|\\mathbf{x}_{i t},\\eta_{i},\\tau_{t})=\\exp(\\alpha I N S T I T_{i t}+\\beta\\mathbf{x}_{i t}+\\eta_{i}+\\tau_{t}), $$ where $\\mathbf{x}_{i t}$ are other control variables, $\\eta_{i}$ is a firm fixed effect, and $\\tau_{t}$ are time dummies.\n\nThe paper explores the relationship between institutional ownership and innovation, contrasting the 'lazy manager' hypothesis with a career concern model. It finds that institutional ownership is positively associated with innovation, particularly under conditions of high product market competition and lower CEO entrenchment."
  },
  {
    "qid": "econ-empirical-527-3-3-0",
    "question": "7) What do the simulation results in Fig. 1 reveal about the performance of the estimators for density (i)?",
    "gold_answer": "Fig. 1 shows that the quality of the estimators for density (i) improves with increasing sample size $n$ and/or increasing $\\lambda$. The boxplots of ${\\hat{\\theta}}-\\theta$ for $\\theta=b$, $c$, and $\\lambda$ indicate better performance with larger $n$, while larger $c$ and $\\lambda$ make the estimation problem more challenging.",
    "question_context": "In Fig. 1, we present the results for estimating density (i) for various sample sizes, when $\\lambda=0.1$ or $0.5,\\alpha=2$ and $c=2$ or 4.5. The bottom row, which shows estimated quartile curves in various cases, illustrates the fact that the quality of the estimator improves as $n$ and/or $\\lambda$ increases. The top row shows boxplots of 200 values of ${\\hat{\\theta}}-\\theta$ , for $\\theta=b$ , c and $\\lambda$ for various settings. In each case, the results improve as the sample size increases and the problem is more difficult as $c$ gets larger and as $\\lambda$, the mean number of jumps, increases.\n\nThe text presents simulation results for different densities and sample sizes, showing the performance of the estimators."
  },
  {
    "qid": "econ-empirical-955-3-0-2",
    "question": "3) Using the Burkholder–Davis–Gundy inequality, derive the bound 𝔼|ΔᵢⁿY(τ)|^{β⁺+ι} ≤ CΔₙ for ι > 0.",
    "gold_answer": "The bound is derived as follows:\n1. **Apply BDG Inequality**: For the martingale part of Yₜ(τ), 𝔼|ΔᵢⁿY(τ)|^{β⁺+ι} ≤ C𝔼[∫_{(i−1)Δₙ}^{iΔₙ}∫_{|x|≤τ}|σₛ₋x|^{β⁺+ι}ν′(dx)ds].\n2. **Boundedness of σₛ**: Since σₛ is bounded, the integral becomes C∫_{|x|≤τ}|x|^{β⁺+ι}ν′(dx)Δₙ.\n3. **Lévy Measure**: ν′(dx) = c⁺/|x|^{1+β⁺}dx for x > 0, so the integral is finite for ι > 0.\n4. **Result**: 𝔼|ΔᵢⁿY(τ)|^{β⁺+ι} ≤ CΔₙ.",
    "question_context": "We prove only the case β⁺>β⁻ and the other cases are analyzed in exactly the same way (the case β⁺=β⁻ and c⁺=c⁻ have been already shown in Todorov and Tauchen (2010)). In what follows C will denote a positive constant that does not depend on n and can change from line to line.\nFirst, we do a localization similar to that done in Jacod (2008) by assuming that αₛ′ is bounded in absolute value and σₛ is bounded both from below and above by some positive constant. Proving the result for the general case (when αₛ′ and σₛ are only locally bounded) can be done as in Lemma 4.6 of Jacod (2008).\nWe start with introducing some notation. For some τ>0, we define Yₜ(τ)=∫₀ᵗαₛ′ds + ∫₀ᵗ∫_{|x|≤τ}σₛ₋xμ̃′(ds,dx).\nFor arbitrary process Z we define Zₜ,ₙ=Zₜ−Z_{(i−1)Δₙ} for t∈[(i−1)Δₙ,iΔₙ]. With this notation we can write on an extension of the original probability space Yₜ,ₙ(τ)=∫_{(i−1)Δₙ}ᵗαₛ′ds + ∫_{(i−1)Δₙ}ᵗ∫_{|x|≤τ}(σₛ₋−σ_{(i−1)Δₙ−})xμ̃′(ds,dx) + σ_{(i−1)Δₙ−}Sₜ,ₙ⁺ + σ_{(i−1)Δₙ−}(Lₜ,ₙ⁻ + Lₜ,ₙ³⁺ − Lₜ,ₙ¹⁺ − Lₜ,ₙ²⁺), t∈[(i−1)Δₙ,iΔₙ].\nThe processes Sₜ⁺, Lₜ¹⁺, Lₜ²⁺, Lₜ³⁺ and Lₜ⁻ are pure-jump Lévy processes with zero drift and Lévy measures respectively: (1) c⁺/|x|^{1+β⁺}1_{x>0} for Sₜ⁺, (2) −2ν″(x)1(x:ν″(x)<0, x∈(0,τ]) for Lₜ¹⁺, (3) c⁺/|x|^{1+β⁺}1(x>τ) for Lₜ²⁺, (4) |ν″(x)|1(0<x≤τ) for Lₜ³⁺, (5) (c⁻/|x|^{1+β⁻}+ν″(x))1_{x<0} for Lₜ⁻.\nSince Δₙ∑_{i=1}^{nT}|σ_{(i−1)Δₙ−}|ᵖ converges pathwise to ∫₀ᵀ|σₛ|ᵖds, to prove the result in (12), it suffices to prove the asymptotic negligibility of each of the terms Aₜ^{(j)} for j=1,…,4.\n\nThe proof involves localization assumptions on the processes αₛ′ and σₛ, and introduces notation for the tempered stable process Yₜ(τ). The proof proceeds by decomposing Yₜ(τ) into several components and analyzing their asymptotic behavior."
  },
  {
    "qid": "econ-empirical-530-4-0-0",
    "question": "1) Derive the elasticity of the noise-to-signal ratio with respect to the aggregate upload bitrate $B_{t}^{u p}$ from the given results. Show the mathematical steps and interpret the economic significance of a $10\\%$ increase in $B_{t}^{u p}$.",
    "gold_answer": "1. Given: A $1\\%$ increase in $B_{t}^{u p}$ increases the noise-to-signal ratio by $0.7\\%$. Thus, the elasticity $\\eta$ is:\n   \\[ \\eta = \\frac{\\% \\Delta \\sigma_{jt}}{\\% \\Delta B_{t}^{up}} = 0.7 \\]\n2. For a $10\\%$ increase in $B_{t}^{u p}$:\n   \\[ \\% \\Delta \\sigma_{jt} = \\eta \\times 10\\% = 7\\% \\]\n3. Economic significance: A $10\\%$ increase in upload bitrate raises the noise-to-signal ratio by $7\\%$, which, given demand elasticity, reduces viewership by approximately $18\\%$.",
    "question_context": "The coefficients of the PoP upgrades loosely align with the relative importance of these upgrades, even if we only establish correlations. Although precise numbers were not released, the first upgrade was the largest, followed by the third, followed by the second. No information is available on the relative size of the fourth upgrade.\nThe results imply increasing the aggregate, upload video bitrate by $1\\%$ will increase the noise-to-signal ratio of every channel by about $0.7\\%$ . An increase of $10\\%$ in $B_{t}^{u p}$ would translate into an increase of $7\\%$ in the noise-to-signal ratio of every channel. In a single day, $B_{t}^{u p}$ can fluctuate by up to $50\\%$ .\nAs expected, an increase in the noise-to-signal ratio decreases demand. Because market shares are small, $(1-\\alpha_{3}s_{j t|g}-(1-\\alpha_{3})s_{j t})\\approx1$ , then $\\alpha_{\\mathrm{1}}/(1-\\alpha_{\\mathrm{3}})$ can be interpreted as the elasticity of own noise-to-signal ratio, where $\\alpha_{1}$ and $\\alpha_{3}$ are the coefficients of $\\log\\sigma_{j t}$ and $\\log{S_{j t|g}}$ . In other words, if $\\sigma_{j t}$ increases by $1\\%$ , demand decreases approximately $2.5\\%$ . Demand is elastic to noise. The partial-equilibrium effect of a $10\\%$ increase in $B_{t}^{u p}$ translates into an increase of $7\\%$ in $\\sigma_{j t}$ , which in turn implies a $18\\%$ reduction in demand.\nTranscoding increases demand in a significant way, approximately by $25\\%$ . As expected, conditional on transcoding, features, and a fixed effect, a channel’s partnership status does not increase viewership, because the remaining perks of partnered channels are trivial for viewers.\nFinally, it can be shown that increasing the number of online channels by 1 increases viewership by $(s_{0}-s_{0}^{+})/(1-s_{0})$ percent in expectation, where $s_{0}$ is the share of the outside option and $s_{0}^{+}$ is the share of the outside option when the extra channel is online. Translated into elasticities, increasing the number of channels by $1\\%$ increases the number of viewers by $0.12\\%$ .\nIn sum, for both partners and non-partners, the elasticity of supply with respect to viewers is about 1; a $1\\%$ increase in viewers increases the number of online channels by about $1\\%$ .\n\nThe text discusses estimation results for congestion externalities, demand-side, and supply-side models in the context of video broadcasting platforms. It includes detailed econometric analyses with instrumental variables, Heckman correction for selection bias, and logit models for supply-side estimations."
  },
  {
    "qid": "econ-empirical-1547-0-2-3",
    "question": "4) Discuss the practical implications of the results in Fig. 11 for risk managers using periodic portfolio rebalancing.",
    "gold_answer": "Fig. 11 shows that for annually rebalanced portfolios:\n- Naive and VHS VaR estimates are nearly identical.\n- Risk managers can use the simpler naive method without significant accuracy loss.\n- Theoretical results for VHS may extend to naive methods in this regime.",
    "question_context": "Fig. 9 displays the estimates of the 5%-VaR obtained from the Naive and VHS methods. Starting from t=1001, the estimates are computed from all the previous observations r1,…,rt−1. As can be seen from the figure, the naive and VHS methods provide very similar results and the backtests used in the previous section are not able to distinguish them.\nTable 3 compares the naive and VHS methods for a portfolio with weekly composition changes. The VHS method shows slight improvements in backtest metrics (e.g., violation rates and loss functions).\n\nThis section presents simulation studies and real-data applications comparing the naive and VHS methods for VaR estimation."
  },
  {
    "qid": "econ-empirical-1683-2-1-3",
    "question": "4) Compare the IV strategies in Models C and D (Table 3), highlighting how tax credit IVs address R&D stock endogeneity.",
    "gold_answer": "Models C and D differ in fixed effects and IVs:\n1. **Model C**: No firm fixed effects; IVs are $\\sum_{j=1}^{n}a_{i j,t}w_{j t}$ (tax credits of collaborators) and $w_{i t}$ (own tax credit).\n2. **Model D**: Adds firm fixed effects; same IVs but accounts for unobserved heterogeneity.\n**Effectiveness**: Tax credits ($w_{i t}$) are exogenous shocks to R&D investment, breaking correlation with $\\epsilon_{i t}$.",
    "question_context": "Four sources of bias: (a) correlated/common-shock effects, (b) simultaneity of $q_{i t}$ and $q_{j t}$, (c) endogeneity of R&D stock, and (d) endogeneity of R&D alliance matrix.\nFor simultaneity, IVs include time-lagged R&D stock of collaborators ($\\sum_{j=1}^{n}a_{i j,t}x_{j t}$) and competitors ($\\sum_{j=1}^{n}b_{i j}x_{j t}$).\nFor R&D stock endogeneity, tax credit shocks $w_{i t}$ instrument $x_{i t}$.\nFor alliance matrix endogeneity, a logistic regression predicts $\\mathbf A_{t}$ using dyadic covariates (Equation 28).\n\nThis section discusses potential biases in estimating the econometric model, including correlated effects, simultaneity, and endogeneity of R&D stock and alliance matrix."
  },
  {
    "qid": "econ-empirical-954-3-0-0",
    "question": "1) Given the parameters $\\alpha = [0.2, 0.2, 0.2, 0.4]$ and elasticity of substitution $\\sigma = 2$, derive the CES price level $P^{t}$ for period $t=1$ using the price vector $p^{1} = [2.00, 1.00, 1.00, 0.50]$.",
    "gold_answer": "To derive the CES price level $P^{t}$ for period $t=1$ with $\\sigma = 2$ and $p^{1} = [2.00, 1.00, 1.00, 0.50]$, follow these steps:\n\n1. The CES price index formula is given by:\n   \\[\n   P^{t} = \\left( \\sum_{n=1}^{N} \\alpha_{n} p_{n}^{t^{1-\\sigma}} \\right)^{\\frac{1}{1-\\sigma}}\n   \\]\n2. Substitute $\\sigma = 2$, $\\alpha = [0.2, 0.2, 0.2, 0.4]$, and $p^{1} = [2.00, 1.00, 1.00, 0.50]$ into the formula:\n   \\[\n   P^{1} = \\left( 0.2 \\times 2.00^{1-2} + 0.2 \\times 1.00^{1-2} + 0.2 \\times 1.00^{1-2} + 0.4 \\times 0.50^{1-2} \\right)^{\\frac{1}{1-2}}\n   \\]\n3. Simplify the exponents:\n   \\[\n   P^{1} = \\left( 0.2 \\times 2.00^{-1} + 0.2 \\times 1.00^{-1} + 0.2 \\times 1.00^{-1} + 0.4 \\times 0.50^{-1} \\right)^{-1}\n   \\]\n4. Calculate the terms:\n   \\[\n   P^{1} = \\left( 0.2 \\times 0.5 + 0.2 \\times 1 + 0.2 \\times 1 + 0.4 \\times 2 \\right)^{-1} = \\left( 0.1 + 0.2 + 0.2 + 0.8 \\right)^{-1} = \\left( 1.3 \\right)^{-1} \\approx 0.7692\n   \\]\n5. The CES price level for period $t=1$ is approximately $0.7692$.",
    "question_context": "We assume that $T=12$ and $N=4$ , and that the parameters $\\alpha_{n}$ in (23) are defined as follows: $\\alpha~\\equiv~[\\alpha_{1},\\alpha_{2},\\alpha_{3},\\alpha_{4}]~\\equiv$ [0.2, 0.2, 0.2, 0.4]. The elasticity of substitution parameter $\\sigma$ will take on the values 0, 0.5, 1, 2, 4, 10, and 20. In the scanner data context, it is likely that it is between 1 and 5.29\nTo calculate the period $t$ (unnormalized) CES price level $P^{t}\\equiv c(p^{t})$ , where $c(p)$ is defined by (23), we require information on the period $t$ vector of prices, $p^{t}\\equiv[{p_{t1},p_{t2},p_{t3},p_{t4}}]$ for $t=$ $1,\\ldots,12$ . These prices are listed in Table 1. Once the unnormalized CES price levels $P^{t}$ have been constructed, the normalized CES price levels can be calculated as $\\pi_{\\mathrm{CES}}^{t}\\equiv P^{t}/P^{1}$ for $t\\_=$ $1,\\ldots,12$ . To calculate the CES quantity vectors, we require information on total expenditures $e^{t}$ on the four commodities for each period $t$ . These exogenously given expenditures are also listed in Table 1.\nThe prices of commodities 1 and 3 trend downward while the prices of commodities 2 and 4 trend upward. The trends in commodities 1 and 4 are very smooth but the trends in commodities 2 and 3 are interrupted by sales: item 2 goes on sale in periods 2 and 8 and item 3 goes on sale in periods 5 and 10. Total expenditures $e^{t}$ on the commodity group trend upward except in the four periods after a sale when aggregate expenditures fall. These patterns represent those found in actual scanner data (see, e.g., de Haan and van der Grient 2011; Fox and Syed 2016). For each value of the elasticity of substitution $\\sigma$ , we can evaluate the period $t$ expenditure share vectors $s^{t}\\equiv[s_{1}^{t},s_{2}^{t},s_{3}^{t},s_{4}^{t}]$ using the $p^{t}$ vectors that are listed in Table 1 and Equation (24). Then the components of the corresponding quantity vectors $q^{t}~\\equiv~[q_{t1},q_{t2},q_{t3},q_{t4}]$ can be constructed using (25).\nWe can then compare the chained Fisher $(\\pi_{\\mathrm{FCH}}^{t})$ , chained Törnqvist $(\\pi_{\\mathrm{TCH}}^{t})$ , WTPD $(\\pi_{\\mathrm{WTPD}}^{t})$ , Geary–Khamis $(\\pi_{\\mathrm{GK}}^{t})$ , GEKS $(\\pi_{\\mathrm{GEKS}}^{t})$ , and CCDI $(\\pi_{\\mathrm{CCDI}}^{t})$ normalized price levels to the corresponding true CES normalized price level, $\\pi_{\\mathrm{CES}}^{t}$ . These alternative indexes are evaluated for $\\sigma$ equal to 0, 0.5, 1, 2, 4, 10, and 20, and are listed in Table A5 of the online appendix and plotted in Figure A1 for $\\sigma$ up to 10.30 To summarize these results, for elasticities of substitution in the most likely range of 1–4, the four methods based on the use of bilateral superlative indexes approximate CES preferences reasonably well with the chained Törnqvist generally doing the best. The GK indexes have substantial upward biases in all cases while the WTPD indexes also have substantial upward biases when equals 2 or 4, but they are unbiased when $\\sigma=1$ .\nWe have noted that in practice purchasers exhibit stockpiling behavior when goods are on sale. To model chain drift that arises from this behavior, we use the price and expenditure data of Table 1 and the assumption of CES preferences for varying $\\sigma$ as before, but the quantity data for commodities that go on sale will be adjusted. Commodity 2 goes on sale in periods 2 and 8. For periods 3 and 9, we take the predicted quantities for these items that are generated by the CES model and we adjust these quantities downward to half of the predicted levels. Similarly, commodity 3 goes on sale in periods 5 and 10 and for periods 6 and 11 we adjust the quantities of these commodities to half of the predicted levels that were generated by the CES model. Thus for periods $t=3,6,9$ , and 11 and for each alternative value of $\\sigma$ , we have a new $q^{t}$ vector and so for these periods, we need to define a new level of expenditure $e^{t}\\equiv p^{t}\\cdot q^{t}$ and new expenditure shares $s_{n}^{t}\\equiv p_{n}^{t}q_{n}^{t}/\\bar{e^{t}}$ for $t=3,6,9$ , and 11 and $n=1,2,3,4$ .\nAs an illustration, for $\\sigma=2$ the resulting prices, quantities and expenditures for commodity 2 are plotted in Figure 1, where it can be seen that the sales periods generate spikes in quantities sold and in the corresponding expenditures. Such spikes are consistent with evidence from scanner data and may even be conservative representations for some commodities; for example, de Haan and van der Grient (2011, p. 39) found that when the price of a detergent product went on sale at approximately one half of the regular price, the volume sold went up approximately 1000 fold.31\nFor periods 3, 6, 9, and 11, there are no CES price levels but for convenience, in Table A6 and Figure 2, we use the normalized CES price levels $\\pi_{\\mathrm{CES}}^{t}$ for all periods. Once the new set of twelve price vectors $p^{t}$ and twelve quantity vectors $q^{t}$ have been constructed for the alternative $\\sigma$ values, we can compare the indexes of interest to the corresponding true CES normalized price level, $\\pi_{\\mathrm{CES}}^{t}$ , for all periods except periods 3, 6, 9, and 11. The indexes are evaluated for $\\sigma$ equal to 0, 0.5, 1, 2, 4, and 10 and are listed in Table A6 of the online appendix, and plotted in Figure 2.\nUsing this adjusted dataset, the chained Fisher and chained Törnqvist indexes have substantial downward biases as compared to the corresponding true CES indexes (excluding periods 1, 3, 6, 9, and 11). This illustrates that superlative indexes are indeed subject to substantial downward chain drift when products are sold at discounted prices.\nFrom Figure 2 (and Table A6 of the online appendix), for $\\sigma=0$ , it can be seen that the fixed base Fisher (FFB) and GK price levels coincide with the CES price levels for all periods except periods 3, 6, 9, and 11 where the CES indexes are not well defined for the new data. The fixed base Törnqvist (not plotted in Figure 2), GEKS and CCDI price levels are very close to the corresponding CES price levels. The WTPD price levels have a small amount of downward bias while the chained Fisher and the chained Törnqvist (not plotted in Figure 2) price levels have substantial downward biases.\nWhen $\\sigma=0.5$ , the fixed base Fisher (and fixed base Törnqvist), GEKS and CCDI price levels are all very close to the corresponding CES price levels. The GK indexes are slightly above and the WTPD indexes are slightly below the corresponding CES index levels. The two chained superlative indexes end up well below the other indexes; that is, the chain drift problem for superlative indexes is severe for this CES dataset. We move on to the more applicable $\\sigma$ equals 1, 2, and 4 results.\nWhen $\\sigma~=~1$ , the two chained superlative indexes have a large downward bias compared to the target CES index. The GK price levels are substantially above the corresponding CES price levels. The remaining indexes are all fairly close to their CES counterparts. The fixed base Törnqvist indexes are exactly equal to their CES counterparts (except for observations 3, 6, 9, and 11 when the CES indexes are not applicable); see Table A6.\nWhen $\\sigma=2$ , the fixed base Törnqvist and CCDI price levels are very close to their CES counterpart price levels. The GEKS price level for period 12 ends up $1.5\\%$ below the corresponding CES price level while the fixed base Fisher price level ends up $2.5\\%$ below the target index. The chained Fisher and chained Törnqvist indexes end up $9.0\\%$ and $11.5\\%$ below the target index in period 12 while the WTPD and GK price levels for period 12 end up $3.2\\%$ and $8.5\\%$ above the CES price level, respectively.\nWhen $\\sigma=4$ , the CCDI indexes are closest to their CES counterparts and the CCDI price level for period 12 ends up $2.7\\%$ below the corresponding CES level. The fixed base Törnqvist and GEKS price levels end up $4.4\\%$ and $5.5\\%$ below the CES price level at period 12. The fixed base Fisher, the chained Törnqvist and the chained Fisher indexes all have substantial downward biases and end up $8.3\\%$ , $10.2\\%$ , and $10.3\\%$ below the corresponding CES price level for period 12. The WTPD and GK indexes have substantial upward biases and end up $9.0\\%$ and $14.2\\%$ above the corresponding CES price level at period 12.\nWhen $\\sigma~=~10$ , all of the alternative indexes have some significant biases for some periods. The WTPD and GK price levels are close to each other and are substantially above their CES counterparts for many observations. These price levels end up being only $1.7\\%$ and $2.5\\%$ above the corresponding CES price level for period 12. The worst performing indexes are the fixed base Fisher and fixed base Törnqvist indexes which end up $19.4\\%$ and $16.8\\%$ below the CES price level for period 12. The chained Fisher and Törnqvist indexes end up $10.2\\%$ and $9.4\\%$ below the CES price level for period 12. Finally the GEKS and CCDI price levels end up $11.0\\%$ and $9.1\\%$ below the CES price level for period 12. However, it should be kept in mind that in most empirical applications, the elasticity of substitution will be well below 10.\nSome conclusions are: (i) Chained superlative indexes are not useful target indexes for a CPI when dealing with aggregating scanner data where discounted prices are prevalent. (ii) The CCDI multilateral method worked best overall for our numerical example for elasticities of substitution in the range $0\\leq\\sigma\\leq$ 4. (iii) GK indexes had substantial upward biases relative to the corresponding CES true cost of living price levels for elasticities of substitution in the range $1\\leq\\sigma\\leq4$ . (iv) WTPD indexes will work well if $\\sigma=1$ or if $\\sigma\\geq10$ but they had substantial upward biases for elasticities of substitution in the range $2\\leq\\sigma\\leq4$ .\n\nThis section discusses the setup for CES preferences with specific parameters and elasticity of substitution values. It includes details on price levels, expenditures, and adjustments for sales periods."
  },
  {
    "qid": "econ-empirical-1740-2-0-2",
    "question": "3) Derive the condition for complementarity between trade and factor mobility for South, as given in inequality (13), and explain its economic interpretation.",
    "gold_answer": "1. Start from eqs. (11) and (12) for demand and supply changes at constant prices.\n2. Trade offers expand if demand for imports of good 1 rises:\n   $$ \n   \\frac{1-\\varepsilon\\cdot m p c_{1}}{m p c_{1}}\\frac{P_{1}g(X_{\\perp})}{\\psi}\\bigg[Y_{\\mathrm{1H}}(\\cdot)\\mathrm{d}\\xi_{\\mathrm{H}}-Y_{\\mathrm{1K}}(\\cdot)\\mathrm{d}\\xi_{\\mathrm{K}}^{\\ast}+Y_{\\mathrm{L}}(\\cdot)\\mathrm{d}\\xi_{\\mathrm{L}}\\bigg] > Y_{\\mathrm{H}}(\\cdot)\\mathrm{d}\\xi_{\\mathrm{H}}+Y_{\\mathrm{L}}(\\cdot)\\mathrm{d}\\xi_{\\mathrm{L}}. \n   $$\n3. This inequality shows that complementarity depends on the relative magnitudes of supply and demand effects, driven by factor elasticities and marginal propensities to consume.",
    "question_context": "Differentiating (3) with respect to $s$ and evaluating all variables at free-trade equilibrium, we obtain the difference between Northern and Southern prices of the jth factor. Thus, \n\n$$ \n\\frac{\\hat{\\bar{r}}_{j}}{\\hat{S}}{=}\\varepsilon\\rho_{j1}\\frac{\\hat{X}_{1}}{\\hat{S}},~j{=}H,K,L, \n$$ \n\nwhere $\\hat{X}_{1}/\\hat{S}$ is given by $(4a)$ and $\\rho_{j1}\\left(j=H,K,L\\right)$ is the elasticity of $Y_{j}(\\cdot)$ with respect to the first argument.\nGiven the sign pattern shown in (7), it is immediate from (6) that at the free-trade equilibrium, skilled and unskilled labor will command a higher price in North whereas capital will be more expensive in South. We have the following proposition.\n\nProposition 1. At the free-trade equilibrium, skilled and unskilled labor will be attracted from South to North and capital will be attracted in the opposite direction.\nThe analysis of capital mobility is slightly different from North's viewpoint. The reason is that North's capital in South receives the return prevailing in South. This implies that when we differentiate the Northern analogue of eq. (8) we obtain\n\n$$ \n\\begin{array}{r l}&{(\\mathrm{d}y^{**})_{\\mathrm{d}P_{1}=0}\\equiv(E_{u}^{*}\\mathrm{d}u^{*})_{\\mathrm{d}P_{1}=0}}\\ &{\\qquad=P_{1}\\varepsilon\\mathrm{d}X_{1}^{*}+Y_{\\mathrm{H}}^{*}(\\cdot)\\mathrm{d}\\xi_{\\mathrm{H}}+Y_{\\mathrm{L}}^{*}(\\cdot)\\mathrm{d}\\xi_{\\mathrm{L}}+(Y_{\\mathrm{K}}(\\cdot)-(Y_{\\mathrm{K}}^{*}(\\cdot))\\mathrm{d}\\xi_{\\mathrm{K}}^{*}.}\\end{array} \n$$\nProposition  2.  International  mobility  of  skilled labor  and capital  exhibits complementarity with trade while the mobility of unskilled labor may exhibit complementarity or substitutability.\n\nThis section analyzes factor mobility under the assumption that potential migrants do not own any capital before or after migration, making migration decisions dependent purely on factor returns. The analysis differentiates between Northern and Southern prices of factors and explores the implications of factor mobility on trade volumes."
  },
  {
    "qid": "econ-empirical-22-4-1-0",
    "question": "5) Derive the conditions under which a mill defaults against the buyer but not the lender, and how this affects the estimation of strategic default.",
    "gold_answer": "1. **Direct Repayment**: Let $D_{\\text{buyer}} = 1$ if mill defaults on buyer, $D_{\\text{lender}} = 1$ if on lender. \\n2. **Measurement Error**: Observed default $D_{\\text{obs}} = D_{\\text{lender}}$, but true default $D_{\\text{true}} = D_{\\text{buyer}} \\cup D_{\\text{lender}}$. \\n3. **Bias**: If $P(D_{\\text{buyer}} = 1 | \\Delta P > 0) > 0$, then $E[D_{\\text{obs}} | \\Delta P] < E[D_{\\text{true}} | \\Delta P]$. \\n4. **Correction**: Use alternative definitions (e.g., repayment by other buyers) to approximate $D_{\\text{true}}$.",
    "question_context": "Mills, however, might default against the buyer on the contract but still repay the loan on time, either directly or through an alternative buyer. Figure IV suggests that these cases are as frequent as all instances of defaults according to the baseline definition. Omitting to consider that such behavior might also be indicative of strategic default would underestimate the extent of strategic default.\nWe thus explore robustness of our results to alternative definitions of default. Figure VIII shows that the main pattern in Figure V is confirmed in the raw data when considering alternative definitions of default. Online Appendix Table A9 reports both the baseline specification in Table II (Panel A) and the event study specification in Table III (Panel B) using alternative measures of default separately as well as their combination.\n\nThe evidence presented so far makes particular sense if mills were defaulting on buyers. Given that our data come from the lender, however, we do not have a direct measure of contractual breach on the forward sale contract. In the style of forensic economics, we use late repayment (and much rarer, defaults) on the loan to infer (attempted) default against the buyer."
  },
  {
    "qid": "econ-empirical-1594-0-1-2",
    "question": "3) Under the index model $F(X) = F(\\beta^{\\prime}X)$, how can the vector of terms of form $\\tau_{i}$ be used to estimate $\\beta$ up to scale?",
    "gold_answer": "Under the index model $F(X) = F(\\beta^{\\prime}X)$, the vector $\\tau_{i}$ is proportional to $\\beta$:\n1. **Proportionality**: The expectation $\\tau_{i} = (-1)^{i} E\\{F(\\beta^{\\prime}X_{1}) f_{i}(X_{1})\\}$ inherits the linear structure of $\\beta^{\\prime}X_{1}$.\n2. **Estimation**: By estimating $\\tau_{i}$ using $T_{i}$, one can recover $\\beta$ up to an unknown scale factor, as the proportionality constant depends on the unknown link function $F(\\cdot)$.\n3. **Interpretation**: This property allows for consistent estimation of the direction of $\\beta$ without full knowledge of $F(\\cdot)$, which is useful in semiparametric models.",
    "question_context": "We consider a $(1+d)$-dimensional stationary time series $\\{(Y_{t},X_{t}^{\\prime})$, $t=0,\\pm1,\\ldots\\}$, where the column vector $X_{t}$ has $d$ elements, observed at data points $t=1,...,N$. The elements of $X_{t}$ can include lagged values of a single variate, for example. For integer $i$, consider the statistic $$T_{i}=\\frac{1}{N^{2}a^{d+i}}\\underset{s\\neq t}{\\sum_{s,t}^{N}\\sum_{\\substack{t}}}Y_{t}K_{i}\\biggl[\\frac{X_{t}-X_{s}}{a}\\biggr].$$ Here $K_{i}$ is an element of the ith partial derivative of a $d$-dimensional kernel function, which integrates to one, and $a$ is a scalar bandwidth number.\nThe statistic $T_{i}$ is intended to estimate $$\\tau_{i}=({\\mathrm{\\boldmath~-1}})^{i}\\mathrm{\\boldmath{E}}\\{Y_{1}f_{i}(X_{1})\\}=({\\mathrm{\\boldmath~-1}})^{i}\\mathrm{\\boldmath{E}}\\{F(X_{1})f_{i}(X_{1})\\},$$ where $(Y_{t},X_{t}^{\\prime})$ is assumed strictly stationary. $F(X_{1})=\\operatorname{E}(Y_{1}\\mid X_{1})$ and $f_{i}$ is the appropriate element of the ith partial derivative of the probability density function $f$ of $X_{1}$.\n\nThe authors introduce a specific statistic to study the properties of semiparametric averaged derivative estimates under long-range dependence. The statistic involves derivatives of kernel functions and a bandwidth number, and it is designed to estimate certain expectations in a strictly stationary time series."
  },
  {
    "qid": "econ-empirical-50-5-3-2",
    "question": "3) Prove Proposition 5 by deriving the variance of the instantaneous growth rate using Ito's lemma.",
    "gold_answer": "1. Apply Ito's lemma to the sales process $s_{i j t}$.\\n2. The instantaneous variance is $\\sigma_{I}^{2}(\\sigma-1)^{2}$ as $\\beta\\to 0$.\\n3. For $\\beta>0$, the variance decreases with firm size due to the term $\\frac{\\partial h(s)}{\\partial s}$.",
    "question_context": "The proposition requires that $\\begin{array}{r}{\\frac{\\partial\\left(\\mu\\frac{h^{\\prime}(s)}{h(s)}+\\frac{\\sigma_{I}^{2}h^{\\prime\\prime}(s)}{2h(s)}\\right)}{\\partial s}\\leq0}\\end{array}$ . Extended derivations for this proposition, given in the Online Appendix, imply that it is equivalent to show that $$\\mu(\\sigma-1)\\Bigg[\\frac{1-\\tilde{\\beta}}{\\tilde{\\beta}}\\Bigg]e^{-s\\frac{(\\sigma-1)}{\\beta}}+\\frac{\\sigma_{I}^{2}}{2}(\\sigma-1)^{2}\\Bigg[\\frac{1-\\tilde{\\beta}^{2}}{\\tilde{\\beta}^{2}}\\Bigg]e^{-s\\frac{(\\sigma-1)}{\\beta}}\\le0.$$\nThe proof of the proposition uses Ito’s lemma. In particular, the variance of the instantaneous growth rate is given by the square of the second bracketed term in expression (30). Given equation (31), this term is equal to $\\sigma_{I}^{2}(\\sigma-1)^{2}$ as $\\beta{\\rightarrow}0$ .\n\nThe text proves Propositions 4 and 5 using Ito's lemma and properties of the normal distribution. Proposition 4 involves a condition on the derivatives of the hazard rate, while Proposition 5 relates to the variance of firm growth."
  },
  {
    "qid": "econ-empirical-1399-0-0-3",
    "question": "4) Using the model by Aghion et al. (2005), show why competition discourages innovation for firms far from the efficiency frontier.",
    "gold_answer": "1. For \\( \\pi \\ll \\pi^* \\), post-innovation profits \\( \\Pi(\\pi + I, c) \\) are low due to high \\( c \\).\n2. The marginal benefit of innovation \\( \\frac{\\partial \\Pi}{\\partial I} \\) is reduced as \\( c \\) increases.\n3. The firm maximizes \\( \\Pi(\\pi + I, c) - \\psi(I) \\).\n4. FOC: \\( \\frac{\\partial \\Pi}{\\partial I} = \\psi'(I) \\).\n5. Since \\( \\frac{\\partial^2 \\Pi}{\\partial I \\partial c} < 0 \\) for laggard firms, \\( I \\) decreases with \\( c \\).",
    "question_context": "Globalization brings opportunities and pressures for domestic firms in emerging market economies to innovate and improve their competitive position.\nIn Sutton's (2007a) model, a firm's competitiveness depends not only on its productivity but also on the quality of its product, with productivity and quality jointly determining a firm's 'capability'.\nAghion et al. (2004, 2005) have shown that the effect of competition on firms’ willingness to innovate depends on their level of efficiency (technology).\nThe balance between the opposing effects of competition on the two types of firms enables Aghion et al. (2004, 2005) to derive the prediction that the effect of the intensity of product market competition on the extent of innovation is in the form of an inverted U.\n\nThe paper examines the effects of foreign competition and linkages with foreign firms on innovation by domestic firms in emerging markets. It discusses theoretical models by Sutton (2007a), Schumpeter (1943), and Aghion et al. (2004, 2005), which predict different relationships between competition and innovation."
  },
  {
    "qid": "econ-empirical-1268-2-0-3",
    "question": "4) Derive the condition (A.8) from (A.7) and explain why the LHS of (A.8) is decreasing in p. Show that for p=2, the LHS converges to 1/(1−β) as κ→0 and to 0 as κ→1.",
    "gold_answer": "1. **From (A.7) to (A.8):** \n   \\[ (1-\\kappa)^{n-1}\\sum_{s=0}^{\\infty}\\beta^{s}\\bigl[(1-\\kappa)^{\\frac{2^{n}p}{2^{n}p-1}}\\bigr]^{2^{s n}p-1} \\leq \\frac{\\frac{1}{2}(u+c)}{\\beta\\kappa u + n u - a} \\]\n2. **Decreasing in p:** The term (1−κ)^(n−1) and the exponentiation inside the sum decrease as p increases.\n3. **Convergence for p=2:** \n   - As κ→0, LHS → ∑β^s = 1/(1−β).\n   - As κ→1, LHS → 0 due to (1−κ) terms.",
    "question_context": "We first show that, for any κ>0, there exists a unique β(κ) such that (for i∈{0,…,n−1}) the inequality (A.1) holds if and only if β≥β(κ). The LHS of (A.1) converges to zero when β converges to zero and tends to infinity when β converges to one. The RHS of (A.1) converges to ½(n−i)(u+c) when β converges to zero and to a finite expression when β converges to one.\nThe first derivative of the RHS of (A.1) with respect to β is strictly positive, and the second derivative is also strictly positive, implying that the RHS is strictly convex in β. The LHS is also strictly convex in β, ensuring a unique βi(κ) satisfying (A.1) with equality.\nFor any β, if κ is sufficiently small, the RHS of (A.1) exceeds the LHS, and the agent has no incentive to cooperate. This is shown by evaluating the RHS at κ=0 and using continuity.\nAfter observing a deviation in a private meeting, an agent's optimal decision depends on her beliefs. If she believes an infinite number of agents have deviated, she will not cooperate in any future meeting. If she believes a finite number p of agents have deviated, her decision inside an institution depends on whether the expected payoff from not cooperating exceeds that from cooperating.\nThe condition for not cooperating inside an institution after observing a deviation is given by (A.7), which can be rewritten as (A.8). The LHS of (A.8) is decreasing in p, and for p=2, it converges to 1/(1−β) as κ→0 and to 0 as κ→1. The RHS is strictly decreasing in κ, ensuring a unique κ*(β) such that (A.8) holds for all κ≥κ*(β).\n\nThis section establishes conditions under which certain strategic behaviors hold, focusing on the uniqueness and properties of β(κ) and the incentives for cooperation or deviation in private and institutional settings."
  },
  {
    "qid": "econ-empirical-376-5-0-1",
    "question": "2) Show how the steady-state equations analogous to (8) and (9) are derived, and explain the role of the inequality $\\mathbf{I} - \\beta{\\pmb{\\sigma}} > \\mathbf{o}$.",
    "gold_answer": "1. The steady-state equations are derived by setting $\\dot{p}$ and $\\dot{e}$ to zero and solving the system:\n   $$ \\begin{array}{l}{\\dot{p}=\\frac{\\beta}{\\left({\\mathrm{\\bf~I}}-\\beta\\sigma\\right)\\lambda}\\{-\\left(\\lambda\\delta+\\alpha\\sigma\\right)\\left(p-\\bar{p}_{0}\\right)+\\left[\\lambda\\delta-\\left({\\mathrm{\\bf~I}}-\\alpha\\right)\\sigma\\right](e-\\bar{e}_{0})\\}}\\\\ {\\dot{e}=\\frac{\\alpha}{\\lambda}\\left(\\dot{p}-\\bar{p}_{0}\\right)+\\frac{\\left({\\mathrm{\\bf~I}}-\\alpha\\right)}{\\lambda}\\mathrm{\\nabla}(e-\\bar{e}_{0}).}\\end{array} $$\n2. The inequality $\\mathbf{I} - \\beta{\\pmb{\\sigma}} > \\mathbf{o}$ ensures the system is invertible and the steady-state solution exists.",
    "question_context": "If it is assumed that output is always determined by the short side of the market then equation $\\left(4^{\\prime}\\right)$ replaces equation (4) in the model of the text. There are two regimes to consider.\n(i) Demand constrained: If $d\\leqslant{\\bar{y}}$ then the economy is demand constrained and governed by the differential equations of the text. (ii) Supply constrained: If $d\\geqslant{\\bar{y}}$ then the economy is supply constrained so ${\\pmb y}={\\bar{\\pmb y}}.$ . Prices move in response to excess demand, so equation (5) is replaced by $$ \\dot{p}=\\beta(d-\\widetilde{y}). $$\nSteady states of the model are of course unchanged, but equations analogous to (8) and (9) are: $$ \\begin{array}{l}{\\dot{p}=\\frac{\\beta}{\\left({\\mathrm{\\bf~I}}-\\beta\\sigma\\right)\\lambda}\\{-\\left(\\lambda\\delta+\\alpha\\sigma\\right)\\left(p-\\bar{p}_{0}\\right)+\\left[\\lambda\\delta-\\left({\\mathrm{\\bf~I}}-\\alpha\\right)\\sigma\\right](e-\\bar{e}_{0})\\}}\\\\ {\\dot{e}=\\frac{\\alpha}{\\lambda}\\left(\\dot{p}-\\bar{p}_{0}\\right)+\\frac{\\left({\\mathrm{\\bf~I}}-\\alpha\\right)}{\\lambda}\\mathrm{\\nabla}(e-\\bar{e}_{0}).}\\end{array} $$\nBy inequality (11) of the text, $\\mathbf{I}-\\beta{\\pmb{\\sigma}}>\\mathbf{o}$\nThe gradient of the $\\pmb{\\beta}$ stationary is the same in each regime; the $\\pmb{\\ell}$ stationary has negative gradient in each case, but a fatter slope in the supply constrained regime; the stable branches have negative gradient in both cases.\nNow consider Fig. 3. For both regimes the stable branch through point $c$ crosses the horizontal line through $E$ to the left of $E_{z}$ since in the supply constrained regime the é stationary is not shifted by a resource discovery, and is therefore the straight line $C E$ : The qualitative response of the system is therefore as before, except that as the economy converges along the stable branch to $c$ it is in the supply constrained regime, so there is full employment output and excess demand in the goods market.\n\nThe appendix discusses a model where output is determined by the short side of the market, introducing two regimes: demand constrained and supply constrained. It provides differential equations governing each regime and analyzes steady states and gradients."
  },
  {
    "qid": "econ-empirical-637-3-1-3",
    "question": "8) Prove Proposition 2: Show that $\\max_t \\| \\widetilde{F}_t - H' F_t^0 \\| = O_p(T^{-1/2}) + O_p((T/N)^{1/2})$.",
    "gold_answer": "1. **First Term**: Show that $\\max_t \\| T^{-1} \\sum_{s=1}^T \\widetilde{F}_s \\gamma_N(s,t) \\| = O_p(T^{-1/2})$ using the boundedness of $\\gamma_N(s,t)$. \\\\ 2. **Remaining Terms**: Show that the remaining terms $\\zeta_{st}$, $\\eta_{st}$, and $\\xi_{st}$ each contribute $O_p((T/N)^{1/2})$ uniformly in $t$. \\\\ 3. **Combination**: Combine these results to obtain the overall bound $O_p(T^{-1/2}) + O_p((T/N)^{1/2})$.",
    "question_context": "Let $V_{N T}$ be the $r\\times r$ diagonal matrix of the first $r$ largest eigenvalues of $(1/T N)X X^{\\prime}$ in decreasing order. By the definition of eigenvectors and eigenvalues, we have $(1/T N)X X^{\\prime}\\widetilde{F}=\\widetilde{F}V_{N T}$ or $(1/N T)X X^{\\prime}\\widetilde{F}V_{N T}^{-1}=\\widetilde{F}$.\nLet $H=(\\Lambda^{0\\prime}\\Lambda^{0}/N)(F^{0\\prime}\\widetilde{F}/T)V_{N T}^{-1}$ be an $r\\times r$ matrix and $\\delta_{N T}=\\min\\{\\sqrt{N},\\sqrt{T}\\}$. Assumptions A and B together with $\\widetilde{F}^{\\prime}\\widetilde{F}/T=I$ and Lemma A.3 imply that $\\|H\\|=O_{p}(1)$.\nTheorem 1 is based on the identity: $$ \\widetilde{F}_{t}-H^{\\prime}F_{t}^{0}=V_{N T}^{-1}\\Bigg(\\frac{1}{T}\\sum_{s=1}^{T}\\widetilde{F}_{s}\\gamma_{N}(s,t)+\\frac{1}{T}\\sum_{s=1}^{T}\\widetilde{F}_{s}\\zeta_{s t}+\\frac{1}{T}\\sum_{s=1}^{T}\\widetilde{F}_{s}\\eta_{s t}+\\frac{1}{T}\\sum_{s=1}^{T}\\widetilde{F}_{s}\\xi_{s t}\\Bigg), $$ where $$ \\begin{array}{l}{{\\displaystyle\\zeta_{s t}=\\frac{e_{s}^{\\prime}e_{t}}{N}-\\gamma_{N}(s,t),}}\\ {{\\displaystyle\\eta_{s t}=F_{s}^{0\\prime}A^{0\\prime}e_{t}/N,}}\\ {{\\displaystyle\\xi_{s t}=F_{t}^{0\\prime}A^{0\\prime}e_{s}/N.}}\\end{array} $$\n\nThis appendix provides the proof of Theorem 1, which establishes the asymptotic properties of the principal components estimator for the factors and their loadings. The proof relies on several lemmas and assumptions, including the convergence of eigenvalues and eigenvectors of the sample covariance matrix."
  },
  {
    "qid": "econ-empirical-452-1-0-0",
    "question": "1) Derive the conditions under which the price index adjusts to regulate real balances at $\\bar{y}$ and $-\\bar{y}$ in the equilibrium described. How do these conditions ensure the invariance of the relative price distribution?",
    "gold_answer": "1. **Regulated Brownian Motion**: Real balances $m - p$ follow a regulated Brownian motion with barriers at $\\pm\\bar{y}$. \\n2. **Price Adjustment**: Firms adjust prices from $-s$ to $s$ when $y = \\bar{y}$ and from $s$ to $-s$ when $y = -\\bar{y}$. \\n3. **Invariance**: By Proposition 1, the distribution of relative prices remains invariant because the price index only changes at $\\pm\\bar{y}$, ensuring aggregation consistency.",
    "question_context": "An equilibrium is an initial distribution of prices and a set of pricing strategies such that: (i) the price index adjusts to regulate real balances at $\\bar{y}$ and $-\\bar{y}$, (ii) firms adjust their relative price from $-s$ to $s$ whenever their relative price falls below $-s$, and from $s$ to $-s$ whenever their relative price hits $s$, (iii) relative prices are initially distributed uniformly over an interval $[-S,S)$, (iv) no firm has an incentive to deviate from the price adjustment strategy described in (i).\nThe equilibrium has two parameters: $\\bar{y}$, which determines the ranges of output movement, and $S$, which determines the amount by which firm's adjust their prices.\n\nThe text defines an equilibrium in a setting where price adjustment occurs at two levels of output, regulated by symmetric barriers. The model assumes a Brownian motion for the money supply, leading to real balances following a regulated Brownian motion. The equilibrium is characterized by specific pricing strategies and initial price distributions."
  },
  {
    "qid": "econ-empirical-243-0-1-1",
    "question": "6) Derive the state-space representation of the TVP-AR model and contrast it with the bounded trend inflation model.",
    "gold_answer": "**TVP-AR**:\n\n\\[\n\\begin{aligned}\ny_t &= \\phi_{0t} + \\phi_{1t} y_{t-1} + \\epsilon_t, \\\\\n\\phi_t &= \\phi_{t-1} + \\eta_t, \\\\\n\\epsilon_t &\\sim N(0, e^{h_t}), \\quad \\eta_t \\sim N(0, \\Omega).\n\\end{aligned}\n\\]\n\n**Bounded Model**: Adds constraints \\(\\tau_t \\in [\\underline{\\tau}, \\overline{\\tau}]\\) and \\(\\rho_t \\in (0,1)\\), requiring nonlinear estimation.",
    "question_context": "In an empirical exercise with CPI inflation, we find the model to work well, yielding more sensible measures of trend inflation and forecasting better than popular alternatives such as the unobserved components stochastic volatility model.\nOur empirical results, based on quarterly CPI inflation, show the advantages of the bounded inflation model. Most importantly, it is yielding estimates of trend inflation that are very different from those yielded by the popular UC-SV model of Stock and Watson (2007). We argue that our estimates are more sensible.\n\nThe article presents empirical results using quarterly CPI inflation data, comparing the bounded trend inflation model with alternatives like the UC-SV and TVP-AR models. The bounded model shows superior performance in estimating trend inflation and forecasting."
  },
  {
    "qid": "econ-empirical-1119-0-0-1",
    "question": "2) Derive the condition under which a single mother with $2,000 in vehicle equity and $250 in liquid assets would satisfy a state's asset test, given a $1,000 asset limit and a $1,500 vehicle exemption.",
    "gold_answer": "1. **Asset Test Condition**: The test is satisfied if:\n   \\[ \\text{Liquid Assets} + (\\text{Vehicle Equity} - \\text{Vehicle Exemption}) \\leq \\text{Asset Limit} \\]\n2. **Calculation**:\n   \\[ 250 + (2000 - 1500) = 250 + 500 = 750 \\leq 1000 \\]\n   Thus, the condition holds.",
    "question_context": "Consistent with other recent research, I find little evidence that asset limits have an effect on the amount of liquid assets that single mothers hold. However, I find evidence that vehicle exemptions do have an important effect on vehicle assets.\nThe saving behavior of poor families has attracted the attention of both researchers and policymakers. Several studies have shown that poor families tend to have very few assets.\nAsset tests under the AFDC/TANF program typically apply to all assets except for owner-occupied housing equity and some fraction of the equity value of a vehicle.\n\nThis paper examines the impact of AFDC/TANF asset tests on the asset holdings of low-educated single mothers, with a focus on vehicle assets. The study highlights the significance of vehicle assets as a major component of wealth for poor families and explores how changes in asset limits and exemptions affect saving behavior."
  },
  {
    "qid": "econ-empirical-694-3-0-0",
    "question": "1) Analyze the strategic behavior of buyers and sellers in period 6, focusing on the bid-ask spread dynamics. What does the inability to reduce the spread indicate about their bargaining strategies?",
    "gold_answer": "The inability to reduce the bid-ask spread in period 6 suggests:\n1. **Rigid limit prices**: Both sides likely set strict reservation prices, leaving no room for negotiation.\n2. **Cartel formation**: Each side acted as a unified bloc, prioritizing group cohesion over individual gains.\n3. **Strategic posturing**: The extreme counter offers ($123,456 and $212) indicate signaling rather than genuine price discovery.\n\nMathematically, if the buyers' minimum bid is $b$ and sellers' maximum ask is $a$, the spread $S = a - b$ remains positive due to non-overlapping reservation prices.",
    "question_context": "In period 6 of this experiment neither buyers nor sellers signed an explicit agreement form. Instead, each 'side' viewed Phase 2 as providing an opportunity for cartel formation. The discussions preceding trading were relatively brief (only about 8 minutes), and focused on what price to aim for and which agent was to be the spokesman. Trading opened with a bid of $2.60, counter offers at $123,456 and $2.30, a reply bid of $2.58 and a final counter offer of $212. Neither side appeared willing to reduce the bid-ask spread.\nIn period 7 there was again virtually no mention of the possibility of coordinated action by both sides, let alone any concern by either side for the externality they were generating. The focus, yet again, was on bilateral cartel strength and strategy. In this case each side appeared to be less rigid in their instructions to negotiators, with these two establishing a price of $2.56 in relatively short order.\nFinally in period 8 the two (frustrated) negotiators in the two previous periods managed to convince their respective partners to cooperate in Coasian fashion in order to extract the available surplus and then for each side to worry about how to divide their 50% share of that surplus.\n\nThe text discusses bargaining behavior in experimental settings, focusing on cartel formation and price negotiations between buyers and sellers. It highlights specific periods where negotiations broke down or succeeded, providing insights into group dynamics and strategic behavior."
  },
  {
    "qid": "econ-empirical-280-0-1-0",
    "question": "6) Why is the Jackson-Sonnenschein mechanism more effective than social interaction treatments in achieving truthful preference representation?",
    "gold_answer": "The mechanism links all decisions via an exogenous budget, simplifying coordination and reducing cognitive load. Social interaction treatments (e.g., FIX, RLK) only link subsets of decisions, leading to smaller efficiency gains.",
    "question_context": "We found that in addition to its theoretical attractiveness, [the Jackson-Sonnenschein mechanism] is easily understood by subjects and hence they reap most of the available efficiency gains.\nOnly if the design suggests a linking of a limited number of problems in a straightforward way or if players can choose their partners, is there an effect on the honesty rates, which, however, translates into very small efficiency gains.\n\nThe study evaluates the Jackson-Sonnenschein budgeting mechanism and contrasts it with social interaction treatments. It discusses cognitive demands, coordination challenges, and the role of competition in achieving efficient outcomes."
  },
  {
    "qid": "econ-empirical-1076-2-1-0",
    "question": "1) Derive the formula for predicting maternal abilities ($\\hat{Y}_{j}^{mother}$) using maternal uncles' abilities. What key parameter ($\\hat{\\rho}_{18}^{brother-sister}$) is required, and how is it estimated?",
    "gold_answer": "The prediction formula is: \n$$\\hat{Y}_{j}^{mother} = \\hat{\\pi} + \\hat{\\rho}_{18}^{brother-sister} Y_{j}^{maternal\\ uncle}$$\nwhere $\\hat{\\rho}_{18}^{brother-sister}$ is estimated by scaling brother-brother correlations:\n$$\\hat{\\rho}_{18}^{brother-sister} = \\hat{\\rho}_{18}^{brother} \\frac{\\hat{\\rho}_{18,k}^{brother-sister}}{\\hat{\\rho}_{18,k}^{brother}}, k=13,16$$",
    "question_context": "To derive maternal abilities, we use the idea behind the uncle instrument to predict abilities for both parents using the first-stage relation; that is, we use enlistment records of both paternal and maternal uncles.\nPredicted maternal abilities, $\\hat{Y}_{j}^{m o t h e r}=\\hat{\\pi}+\\hat{\\uprho}_{18}^{b r o t h e r-s i s t e r}Y_{j}^{m a t e r n a l u n c l e}$, could then be used in the second-stage relation.\nThe scaling factor for cognitive abilities is 0.92, and for noncognitive abilities is 0.93, based on sibling correlations in Table 7.\n\nThis section extends the analysis to include maternal abilities by leveraging maternal uncles' enlistment records."
  },
  {
    "qid": "econ-empirical-574-3-1-3",
    "question": "8) What does the low proportion (4%) of subjects believing a real vice president would not fire anyone indicate about perceived managerial behavior?",
    "gold_answer": "1. Only 4% predicted a real manager would choose $x=0$ (no layoffs).\n2. This suggests subjects overwhelmingly believe real managers prioritize profitability over employee retention.\n3. The result aligns with economic theories of rational, profit-maximizing behavior in managerial decision-making.",
    "question_context": "OBSERVATION 3: There were no significant differences between groups as to what the subjects thought a real vice president would do.\nOBSERVATION 4: There were large differences in how closely the subject’s choice matched the one he attributed to a real vice president.\n\nThe text examines the differences in responses between subjects' own choices and their predictions of a real vice president's actions, highlighting dissonance and group similarities."
  },
  {
    "qid": "econ-empirical-541-2-1-0",
    "question": "1) Show that $\\xi_{i}$ satisfies $\\mathrm{sup}_{n}E(\\mathrm{max}_{i\\leq n}\\xi_{i}^{2})<\\infty$ using the given bound.",
    "gold_answer": "1. **Bound**: Use $\\xi_{i}^{2} \\leq 2(\\tilde{A}_{i}^{2} + \\tilde{B}_{i}^{2})$.\n2. **Conditional Expectation**: Apply the bound $E(\\operatorname*{max}_{i\\leq n}\\xi_{i}^{2}|\\mathcal{Z}_{n}) \\leq 2\\sum_{i}E(\\tilde{A}_{i}^{2}|\\mathcal{Z}_{n}) + 1$.\n3. **Uniform Bound**: Show that $\\sum_{i}E(\\tilde{A}_{i}^{2}|\\mathcal{Z}_{n}) \\leq \\gamma_{n}$ a.s. and $\\gamma_{n} \\rightarrow 0$.\n4. **Conclusion**: Thus, $\\mathrm{sup}_{n}E(\\mathrm{max}_{i\\leq n}\\xi_{i}^{2}) < \\infty$.",
    "question_context": "$$E(\\operatorname*{max}_{i\\leq n}\\xi_{i}^{2}|\\mathcal{Z}_{n})\\leq2\\sum_{i}E(\\tilde{A}_{i}^{2}|\\mathcal{Z}_{n})+1.$$\n$$\\begin{array}{l}{{\\displaystyle E(w_{21}w_{31}w_{41}w_{51}|z_{1}=a)\\le\\displaystyle\\frac{C}{k^{4}}E(I(z_{1}\\in\\mathcal{N}_{2})I(z_{1}\\in\\mathcal{N}_{3})}}\\ {~\\times{I(z_{1}\\in\\mathcal{N}_{4})I(z_{1}\\in\\mathcal{N}_{5})|z_{1}=a)},}\\end{array}$$\n\nThese lemmas establish bounds on the moments of normalized random variables and their conditional expectations."
  },
  {
    "qid": "econ-empirical-873-1-0-1",
    "question": "2) Derive a simple model showing how factor costs influence the choice of technology, as illustrated by Rosenberg's analysis of woodworking machinery.",
    "gold_answer": "1. **Model Setup**: Let \\( C(L, K) = wL + rK \\) be the cost function, where \\( w \\) = wage rate, \\( r \\) = capital rental rate.\\n2. **Technology Choice**: Firms minimize \\( C \\) subject to \\( Q = f(L, K) \\). For two technologies \\( f_1 \\) (labor-intensive) and \\( f_2 \\) (capital-intensive):\\n   - If \\( w/r \\) rises, \\( f_2 \\) becomes optimal.\\n3. **Rosenberg's Insight**: Historical data (e.g., 19th-century woodworking) show \\( f_2 \\) adoption accelerated when \\( w/r \\) exceeded a threshold \\( \\tau \\).",
    "question_context": "Even though neo-classical theory has long provided a means of attack, it has only recently permeated the literature of economic history in this country in an explicit form, and its use has only made clearer its inadequacies, particularly in explaining moves to a new production function, and lags in response to such stimuli as changing factor prices.\nRosenberg's strength lies in his ability to understand, and to express in simple verbal formulations, both the neo-classical model and the modifications made to it by economists, and the technological changes and historical interpretations of them.\nIn his work on woodworking machinery, for example, the role of factor costs in determining choice of technology is clearly expounded and explained.\nIn his discussion of machine tools, again, his exposition of the role of the capital goods sector in spreading process innovations across apparently disparate manufacturing industries and thus reducing the costs of innovation is an example of an apparently simple idea which has the force of a flash of inspiration.\n\nThe text discusses Nathan Rosenberg's work on technological change within the framework of neo-classical economic theory, highlighting his ability to critique and modify these models while acknowledging their limitations in explaining complex historical technological developments."
  },
  {
    "qid": "econ-empirical-229-1-2-0",
    "question": "7) Derive the supply-side first-order condition for profit maximization with respect to $n_s$, showing how it depends on $\\rho$ and effort responses $\\frac{\\partial x_t^*}{\\partial n_s}$.",
    "gold_answer": "1. Profit: $\\eta\\left(\\sum_{t=1}^S (n_t x_t^*)^\\rho\\right)^{1/\\rho} - \\sum_{t=0}^S n_t W_{t+1}$. \\n2. FOC for $n_s$: \\n$$\\eta \\left(\\sum_t (n_t x_t^*)^\\rho\\right)^{1/\\rho - 1} (n_s x_s^*)^{\\rho-1} \\left(x_s^* + n_s \\frac{\\partial x_s^*}{\\partial n_s}\\right) + \\sum_{t\\neq s} \\eta (\\cdot)^{1/\\rho-1} (n_t x_t^*)^{\\rho-1} n_t \\frac{\\partial x_t^*}{\\partial n_s} = W_{s+1}.$$ \\n3. Divide by FOC for $n_S$ to eliminate $\\eta$ and obtain the relative wage condition in the text.",
    "question_context": "Firms choose $n_1,\\dots,n_S$ to maximize: $$\\max_{n_1,\\ldots,n_S}\\eta Q\\left(n_1,\\ldots,n_S;x_1,\\ldots,x_S\\right)-\\sum_{s=0}^S n_s W_{s+1},$$ where $Q(\\cdot)$ is CES: $$\\left(\\sum_{s=1}^S (n_s x_s^*)^\\rho\\right)^{1/\\rho}.$$\nThe first-order conditions imply: $$\\frac{W_{s+1}}{W_{S+1}} = \\frac{(n_s x_s^*)^{\\rho-1}x_s^* + \\sum_{t=1}^S (n_t x_t^*)^{\\rho-1}n_t \\frac{\\partial x_t^*}{\\partial n_s}}{(n_S x_S^*)^{\\rho-1}x_S^* + \\sum_{t=1}^S (n_t x_t^*)^{\\rho-1}n_t \\frac{\\partial x_t^*}{\\partial n_S}}.$$\n\nThe first estimation approach incorporates supply-side optimization, assuming firms choose employment levels to maximize profits with a CES production function, adding identifying restrictions."
  },
  {
    "qid": "econ-empirical-997-3-1-0",
    "question": "5) Using Table 1, compute the difference in mean HAZ between FeA (\\( \\mu_{\\text{FeA}} = -1.27 \\)) and ENDS (\\( \\mu_{\\text{ENDS}} = -0.77 \\)). Test the hypothesis that this difference is statistically significant at \\( \\alpha = 0.05 \\), assuming pooled standard deviation \\( s_p = 1.0 \\).",
    "gold_answer": "**Hypothesis Test**: \n- **Step 1**: Difference in means: \\( \\Delta \\mu = -0.77 - (-1.27) = 0.50 \\). \n- **Step 2**: Standard error: \\( SE = s_p \\sqrt{\\frac{1}{n_{\\text{FeA}}} + \\frac{1}{n_{\\text{ENDS}}}} = 1.0 \\sqrt{\\frac{1}{2413} + \\frac{1}{6179}} \\approx 0.025 \\). \n- **Step 3**: t-statistic: \\( t = \\frac{0.50}{0.025} = 20 \\). Reject \\( H_0 \\) (no difference) as \\( t > 1.96 \\).",
    "question_context": "The FeA survey includes 65 control towns (52 in the third wave) with detailed HC attendance data, distances, and fees. The ENDS lacks fee and distance data but provides urban coverage.\nThe population in the FeA sample is extremely poor (SISBEN 1), while the ENDS sample includes SISBEN ≤3. Height-for-age z-scores (HAZ) are used to measure chronic malnutrition.\n\nThis section describes the two primary datasets used in the study: the FeA survey (focused on small towns) and the ENDS (national urban sample). It highlights differences in sample composition, variables, and limitations."
  },
  {
    "qid": "econ-empirical-1208-5-0-3",
    "question": "4) The authors caution against interpreting near-zero unexplained gaps as evidence of no discrimination. Using a Roy model framework, explain how market discrimination could still affect premarket educational choices.",
    "gold_answer": "1. **Roy Model**: Women choose majors/occupations to maximize \\( V = E[W|S] - C(S) \\), where \\(C(S)\\) includes perceived discrimination costs.  \n2. If certain fields have high \\(C(S)\\) (e.g., engineering), women may select into lower-paying but 'safer' majors.  \n3. This shifts \\(S\\) endogenously, biasing wage gap estimates if \\(S\\) is treated as exogenous.  \n4. Thus, premarket factors may already reflect anticipated discrimination, masking its true wage effect.",
    "question_context": "Well-educated women in the United States earn approximately 30 percent less than men, a gap that is similar to the gender gap for the workforce generally.\nWhen we focus analysis on men and women who speak English at home, we find that across racial/ethnic groups between 44 and 73 percent of the gender wage gaps are accounted for by such premarket factors as age, highest degree, and major.\nWhen we restrict attention further to women who have 'high labor force attachment' (work experience that is similar to male comparables) we account for between 54 and 99 percent of the gender wage gaps.\nThus, a relatively simple version of the basic human capital model seems to do remarkably well in explaining a substantial portion of the gender wage gaps for highly educated women.\nOne tempting interpretation for our findings is that gender discrimination is not a major factor in wage determination in labor markets for the college educated.\nOur final observation concerns methodology. The inferences we draw in this paper are based on a simple and intuitively appealing matching model that has not heretofore been used in studying gender wage gaps.\n\nThe paper examines the gender wage gap using a matching estimation procedure, focusing on highly educated women across different racial/ethnic groups. It highlights the role of premarket factors and labor force attachment in explaining wage gaps, while also discussing caveats related to discrimination and methodological considerations."
  },
  {
    "qid": "econ-empirical-489-4-0-1",
    "question": "2) Prove that under Assumption 5.2, the set of functions $\\mathcal{F}_{0}$ has a uniform covering entropy obeying $\\mathrm{sup}_{Q}\\mathrm{log}N(\\epsilon\\|F_{0}\\|_{Q,2},\\mathcal{F}_{0},\\|\\cdot\\|_{Q,2})\\le C\\log(\\mathrm{e}/\\epsilon)\\vee0$. Discuss the implications for the complexity of $\\mathcal{F}_{0}$.",
    "gold_answer": "1. **Proof**: The uniform covering entropy bound follows from the entropy condition on $(\\mathcal{U},d_{\\mathcal{U}})$ and the envelope function $F_{0}$ in Assumption 5.2. The entropy is controlled by the metric entropy of $\\mathcal{U}$ and the growth rate of $F_{0}$.\\n2. **Implications**: The bound implies that $\\mathcal{F}_{0}$ is not overly complex, which is essential for applying empirical process theory. It ensures that the class $\\mathcal{F}_{0}$ is manageable, allowing for uniform convergence of empirical processes and facilitating the derivation of asymptotic results.",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nIn what follows, we shall denote by $\\delta,c_{0},c,$ and $c$ some positive constants. For a positive integer $d,[d]$ denotes the set $\\{1,...,d\\}$ . We shall impose the following regularity conditions.\nAsSUMPTION 5.1—-Moment-Condition Problem: Consider a random element $W$ taking values in a measure space $(\\mathcal{W},\\mathcal{A}_{\\mathcal{W}})$ , with law determined by a probability measure $\\boldsymbol{P}\\in\\mathcal{P}_{n}$ The observed data $((W_{u i})_{u\\in\\boldsymbol{\\mathcal{U}}})_{i=1}^{n}$ consist of n i.i.d. copies of a random element $(W_{u})_{u\\in\\mathcal{U}}$ which is generated as a suitably measurable transformation with respect to W and u. Uniformly for all $n\\geq n_{0}$ and $\\boldsymbol{P}\\in\\mathcal{P}_{n}$ ,the following conditions hold: (i) The true parameter value $\\theta_{u}$ obeys (5.1) and is interior relative to $\\mathcal{\\Theta}_{u}\\subset\\mathcal{\\Theta}\\subset\\mathbb{R}^{d_{\\theta}}$ , namely there is a ball of radius 8 centered at $\\theta_{u}$ $\\boldsymbol{\\Theta}_{u}$ $u\\in\\mathcal{U}$ $\\pmb{\\theta}$ $\\begin{array}{r}{\\nu:=(\\nu_{k})_{k=1}^{d_{\\theta}+d_{t}}=(\\theta,t)}\\end{array}$ $j\\in[d_{\\theta}]_{\\cdot}$ $u\\in\\mathcal{U}$ $\\begin{array}{r}{\\Theta_{u}\\times T_{u}(Z_{u})\\ni\\nu\\longmapsto\\mathrm{E}_{P}[\\psi_{u j}(W_{u},\\nu)|Z_{u}]}\\end{array}$ differentiable a.s. with derivatives obeying the integrability conditions specified in Assumption 5.2. (iii) For all $u\\in\\mathcal{U}$ , the moment function $\\psi_{u}$ obeys the orthogonality condition given in Definition 5.1 for the set $\\mathcal{H}_{u}=\\mathcal{H}_{u n}$ specified in Assumption 5.3. (iv) The following identifiability condition holds: $\\begin{array}{r}{\\|\\mathrm{E}_{P}[\\psi_{u}(W_{u},\\theta,h_{u}(Z_{u}))]\\|\\ge2^{-1}(\\|J_{u}(\\theta-\\theta_{u})\\|\\wedge c_{0})}\\end{array}$ for all $\\pmb{\\theta}\\in\\pmb{\\theta}_{u}$ ， where the singular values of $J_{u}:=\\partial_{\\theta}\\mathrm{E}[\\psi_{u}(W_{u},\\theta_{u},h_{u}(Z_{u}))]$ lie between c and $c$ for all $u\\in\\mathcal{U}$.\nAsSUMPTION 5.2—-Entropy and Smoothness: The set $(\\mathcal{U},d_{\\mathcal{U}})$ is a semimetric space such that $\\log N(\\epsilon,\\mathcal{U},d_{\\mathcal{U}})\\leq C\\log(\\epsilon/\\epsilon)\\vee0.$ Let $\\alpha\\in[1,2]$ , and let $\\alpha_{1}$ and $\\alpha_{2}$ be some positive constants. Uniformly for all $n\\geq n_{0}$ and $\\boldsymbol{P}\\in\\mathcal{P}_{n}$ , the following conditions hold: (i) The set of functions $\\mathcal{F}_{0}=\\{\\psi_{u j}(W_{u},\\theta_{u},h_{u}(Z_{u})):j\\in[d_{\\theta}],u\\in\\mathcal{U}\\}$ ,viewed as functions of $W$ , is suitably measurable;has an envelope function $\\begin{array}{r}{F_{0}(W)=\\operatorname*{sup}_{j\\in[d_{\\theta}],u\\in\\mathcal{U},\\nu\\in\\Theta_{u}\\times T_{u}(Z_{u})}|\\psi_{u j}(W_{u},\\nu)|}\\end{array}$ that is measurable with respect to $W$ and obeys $\\|F_{0}\\|_{P,q}\\leq C$ ,where $q\\geq4$ is $\\pmb{a}$ fixed constant; and has a uniform covering entropy obeying $\\mathrm{sup}_{O}\\mathrm{log}N(\\epsilon\\|F_{0}\\|_{Q,2},\\mathcal{F}_{0},\\|\\cdot\\|_{Q,2})\\le C\\log(\\mathrm{e}/\\epsilon)\\vee0$ (ii) For all $j\\in[d_{\\theta}]$ and $k,r\\in[d_{\\theta}+d_{t}]$ and $\\mathbf{\\tilde{\\psi}}_{u j}(W):=\\psi_{u j}(W_{u},\\theta_{u},h_{u}(Z_{u}))$.\nAssUMPTION 5.3—Estimation of Nuisance Functions: The following conditions hold for each $n\\geq n_{0}$ and all $\\boldsymbol{P}\\in\\mathcal{P}_{n}$ .Thestimated functions $\\hat{h}_{u}=(\\hat{h}_{u m})_{m=1}^{d_{t}}\\in\\mathcal{H}_{u n}$ with probabilityat least $1-\\varDelta_{n}$ ,where $\\mathcal{H}_{u n}$ is the set of measurablemaps $\\mathcal{Z}_{u}\\ni z\\longmapsto h=(h_{m})_{m=1}^{d_{t}}(z)\\in T_{u}(z)$ such that $\\|h_{m}-h_{u m}\\|_{P,2}\\leq\\tau_{n},\\quad\\tau_{n}^{2}\\sqrt{n}\\leq\\delta_{n},$ and whose complexity does not grow too quickly in the sense that $\\mathcal{F}_{1}=\\{\\psi_{u j}(W_{u},\\theta,h(Z_{u})):$ $j\\in[d_{\\theta}],u\\in\\mathcal{U},\\theta\\in\\theta_{u},h\\in\\mathcal{H}_{u n}\\}$ is suitably measurable and its uniform covering entropy obeys $\\operatorname*{sup}_{Q}\\log N\\bigl(\\epsilon\\|F_{1}\\|_{Q,2},\\mathcal{F}_{1},\\|\\cdot\\|_{Q,2}\\bigr)\\leq s_{n}\\bigl(\\log(a_{n}/\\epsilon)\\bigr)\\vee0,$ where $F_{1}(W)$ is an envelope for $\\mathcal{F}_{1}$ which is measurable with respect to $W$ and satisfies $F_{1}(W)\\leq F_{0}(W)$ for $F_{0}$ defined in Assumption 5.2. The complexity characteristics ${\\pmb a}_{n}\\ge$ $\\mathtt{m a x}(n,{\\mathtt{e}})$ and $s_{n}\\geq1$ obey the growth conditions $n^{-1/2}\\big(\\sqrt{s_{n}\\log(a_{n})}+n^{-1/2}s_{n}n^{1/q}\\log(a_{n})\\big)\\leq\\tau_{n}\\quad a n d$ $\\tau_{n}^{\\alpha/2}\\sqrt{s_{n}\\log(a_{n})}+s_{n}n^{1/q-1/2}\\log(a_{n})\\log n\\leq\\delta_{n},$ where $q$ and $_{\\alpha}$ are defined in Assumption 5.2.\nTHEOREM 5.1—-Uniform Functional Central Limit Theorem for a Continuum of Target Parameters in Moment-Condition Problems: Under Assumptions 5.1, 5.2, and 5.3, for an estimator $(\\hat{\\theta}_{u})_{u\\in\\mathcal{U}}$ that obeys equation(5.4), $\\sqrt{n}(\\widehat{\\theta}_{u}-\\theta_{u})_{u\\in\\mathcal{U}}=(\\mathbb{G}_{n}\\bar{\\psi}_{u})_{u\\in\\mathcal{U}}+o_{P}(1)$ in $\\ell^{\\infty}(\\mathcal{U})^{d_{\\theta}}$ , uniformly in $\\boldsymbol{P}\\in\\mathcal{P}_{n}$ ,where $\\bar{\\psi}_{u}(W):=-J_{u}^{-1}\\psi_{u}(W_{u},\\theta_{u},h_{u}(Z_{u}))$ ,and $Z_{n,P}:=(\\mathbb{G}_{n}\\bar{\\psi}_{u})_{u\\in\\mathcal{U}}\\rightarrow Z_{P}:=(\\mathbb{G}_{P}\\bar{\\psi}_{u})_{u\\in\\mathcal{U}}\\quad i n\\quad\\ell^{\\infty}(\\mathcal{U})^{d_{\\theta}}$ where the paths of $u\\mapsto\\mathbb{G}_{P}\\bar{\\psi}_{u}$ are a.s. uniformly continuous on $(\\mathcal{U},d_{\\mathcal{U}})$ and $\\begin{array}{r l}&{\\displaystyle\\operatorname*{sup}_{P\\in\\mathcal{P}_{n}}\\mathrm{E}_{P}\\operatorname*{sup}_{u\\in\\mathcal{U}}\\|\\mathbb{G}_{P}\\bar{\\psi}_{u}\\|<\\infty\\quad a n d}\\ &{\\displaystyle\\operatorname*{lim}_{\\delta\\to0}\\mathrm{sup}\\mathrm{E}_{P}\\operatorname*{sup}_{d\\mathcal{U}(u,\\bar{u})\\leq\\delta}\\|\\mathbb{G}_{P}\\bar{\\psi}_{u}-\\mathbb{G}_{P}\\bar{\\psi}_{\\bar{u}}\\|=0.}\\end{array}$\nTHEOREM 5.2—Uniform Validity of Multiplier Bootstrap: Suppose Assumptions 5.1, 5.2, and 5.3 hold, the estimator $(\\hat{\\theta}_{u})_{u\\in\\mathcal{U}}$ obeys equation (5.4), and that the estimator $(\\hat{J}_{u})_{u\\in\\mathcal{U}}$ obeys the following condition: uniformly in $\\boldsymbol{P}\\in\\mathcal{P}_{n}$ with probability $1-\\delta_{n}$ ， $\\begin{array}{r}{\\mathbf{sup}_{u\\in\\mathcal{U}}\\|\\hat{J}_{u}-J_{u}\\|\\leq}\\end{array}$ $\\varDelta_{n}$ .Then, $\\begin{array}{r}{\\hat{Z}_{n,P}^{*}\\rightsquigarrow_{B}Z_{P}\\quad i n\\quad\\ell^{\\infty}(\\mathcal{U})^{d_{\\theta}},u n i f o r m l y i n P\\in\\mathcal{P}_{n}.}\\end{array}$\nTHEOREM 5.3-Uniform Limit Theory and Validity of Multiplier Bootstrap for Smooth Functionals of $\\pmb{\\theta}$ : Suppose that for each $\\begin{array}{r}{P\\in\\mathcal{P}:=\\bigcup_{n\\geq n_{0}}\\mathcal{P}_{n;}}\\end{array}$ $\\tilde{\\theta}^{0}=\\theta_{P}^{0}$ is an element of $\\pmb{a}$ compact set $\\mathbb{D}_{\\theta}$ . Suppose $\\theta\\longmapsto\\phi(\\theta)$ ， $^{a}$ functional of interest mapping. $\\mathbb{D}_{\\phi}\\subset\\mathbb{D}=\\ell^{\\infty}(\\mathcal{U})^{d_{\\theta}}$ to $\\ell^{\\infty}(\\mathcal{Q})$ , where $\\mathbb{D}_{\\theta}\\subset\\mathbb{D}_{\\phi}$ , is Hadamard differentiable in $\\pmb{\\theta}$ tangentially to $\\mathbb{D}_{0}=U C(\\mathcal{U})^{d_{\\theta}}$ uniformly in $\\theta\\in\\mathbb{D}_{\\theta}$ , with the linear derivative map $\\phi_{\\theta}^{\\prime}:\\mathbb{D}_{0}\\longmapsto\\mathbb{D}$ such that the mapping $(\\theta,h)\\longmapsto\\phi_{\\theta}^{\\prime}(h)$ from $\\mathbb{D}_{\\theta}\\times\\mathbb{D}_{0}$ to $\\ell^{\\infty}(\\mathcal{Q})$ is continuous. Then, $\\begin{array}{r}{\\sqrt{n}(\\hat{A}-A)\\rightarrow T_{P}:=\\phi_{\\theta_{P}^{0}}^{\\prime}(Z_{P})\\quad i n\\quad\\ell^{\\infty}(\\mathcal{Q}),u n i f o r m l y i n P\\in\\mathcal{P}_{n},}\\end{array}$ where $T_{P}$ is a zero mean tight Gaussian process, for each $P\\in\\mathcal{P}$ .Moreover, $\\begin{array}{r}{\\sqrt{n}\\big(\\hat{\\Delta}^{*}-\\hat{\\Delta}\\big)\\prec_{B}T_{P}\\quad i n\\quad\\ell^{\\infty}(\\mathcal{Q}),u n i f o r m l y i n P\\in\\mathcal{P}_{n}.}\\end{array}$\n\nThis section outlines the regularity conditions and theoretical results for moment-condition problems with functional response data, focusing on Lasso and Post-Lasso estimators."
  },
  {
    "qid": "econ-empirical-812-1-1-2",
    "question": "3) Prove that Assumption 5 holds if $x$ and $z$ are independent. What does this imply about the identifiability of $g_{0}(x)$ and $h_{0}(z)$?",
    "gold_answer": "If $x$ and $z$ are independent, then $\\pi_{q^{L}}B_{K}p^{K}(z)=\\operatorname{E}[B_{K}p^{K}(x)]$, a constant. Thus, $B_{K}p^{K}(x)-\\pi_{q^{L}}B_{K}p^{K}(z)=B_{K}p^{K}(x)-\\operatorname{E}[B_{K}p^{K}(x)]$, which has nonvanishing variance. The eigenvalues condition holds because the covariance matrix of the centered $B_{K}p^{K}(x)$ is positive definite. This implies $g_{0}(x)$ and $h_{0}(z)$ are identifiable since $x$ provides independent variation.",
    "question_context": "Assumption 1 (Data and additive separability). The observed data, $\\textstyle\\operatorname{\\mathcal{D}}_{n}$ , is given by $n$ iid copies of random variables $(x,y,z)\\in$ $\\mathcal{X}\\times\\mathcal{Y}\\times\\mathcal{Z}$ indexed by $1\\leqslant i\\leqslant n$ , so that $\\mathcal{D}_{n}=(y_{i},x_{i},z_{i})_{i=1}^{n}$ . $\\mathcal{Y}\\subseteq\\mathbb{R}$ and $\\mathcal{X}\\subseteq\\mathbb{R}^{r}$ for some integer $r>0,\\mathcal{Z}$ is a measurable space. There is a random variable $\\varepsilon$ and functions $g_{0}$ and $h_{0}$ such that $y=g_{0}(x)+h_{0}(z)+\\varepsilon$ , $\\operatorname{E}[\\varepsilon|x,z]=0$ .\nAssumption 5 (Identifiability). For each $K$ and for $B_{K}$ as in Assumption 3, the matrix $\\mathrm{E}[(B_{K}p^{K}(x)~-~\\pi_{q^{L}}B_{K}p^{K}(z))$ $(B_{K}\\boldsymbol{p}^{K}(\\boldsymbol{x})-\\pi_{\\boldsymbol{q}^{L}}B_{K}\\boldsymbol{p}^{K}(\\boldsymbol{z}))^{\\prime}]$ has eigenvalues bounded uniformly away from zero in $K,L$ . In addition, $\\begin{array}{r}{\\operatorname*{sup}_{z\\in{\\mathcal Z}}\\|\\pi_{q^{L}}B_{K}{p^{K}(z)}\\|_{2}\\leqslant}\\end{array}$ $O(\\zeta_{0}(K))$ .\n\nThis section outlines the assumptions necessary for the nonparametric estimation of $g_{0}$ and $\\theta_{0}$, including regularity conditions and identifiability constraints."
  },
  {
    "qid": "econ-empirical-1379-2-1-1",
    "question": "2) Describe the two-step iteration phase of the algorithm for estimating $\\pmb\\theta$ and $\\pmb{H}$, including the reallocation step.",
    "gold_answer": "1. **Step 1 (Estimation)**: Given current segment assignments $\\pmb{H}^{(l-1)}$, update $\\pmb\\theta^{(l)}$ by minimizing $-\\ln{\\cal L}(\\theta|\\pmb{H}^{(l-1)}, Y)$ using a quasi-Newton method. \n2. **Step 2 (Reallocation)**: Reassign respondents to segments to minimize individual contributions to the negative log-likelihood: $$h_{t i}^{(l)} = \\begin{cases} 1 & \\text{if } t = \\arg\\max_{t'} L_{i t'}(\\pmb{\\theta}_{t'}|Y), \\\\ 0 & \\text{otherwise.} \\end{cases}$$ \n3. Here, $L_{i t}(\\pmb{\\theta}_{t}|Y) = -\\sum_{j,k} y_{i j k} \\ln(p_{t j|\\{j,k\\}})$ is the contribution of respondent $i$ to the likelihood for segment $t$. \n4. Iterate until convergence.",
    "question_context": "We use for the tth segment-specific ideal point a multivariate normal distribution with parameter vector $\\pmb{\\theta}_{t}=(\\mu_{t1},\\dots,\\mu_{t r},\\sigma_{t11},\\sigma_{t12},\\dots,\\sigma_{t r r})^{\\prime}$ . For estimating the vector of ideal point parameters $\\pmb{\\theta}=(\\pmb{\\theta}_{1}^{\\prime},\\dots,\\pmb{\\theta}_{T}^{\\prime})^{\\prime}$ , paired comparisons are collected from $N$ respondents.\nAs a result we get — with $i$ as an index for respondents and $j,k$ as indices for products — a data array $Y$ with elements $$y_{i j k}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if~respondent~}i\\mathrm{~prefers~product~}j\\mathrm{~to~product~}k,}\\ {0}&{\\mathrm{otherwise},}\\end{array}\\right.\\forall i,j,k.$$\nAn additional, possibly unknown non-overlapping segmentation matrix $\\pmb{H}$ with elements $$h_{t i}=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if~respondent~}i~\\mathrm{belongs~to~segment~}t,}\\ {0}&{\\mathrm{otherwise},}\\end{array}\\right.$$ is introduced from which we get the relative segment sizes $\\lambda_{t}=\\sum_{i=1}^{N}h_{t i}/N$ .\nFor obtaining estimates of $\\pmb\\theta$ and $H_{:}$ , sequential techniques, where the respondents are a priori assigned to segments by clustering methods (e.g., by Ward- or $k$ -means-clustering based on the paired comparison data) before $\\pmb\\theta$ is computed, and simultaneous techniques, where $\\pmb\\theta$ and $\\pmb{H}$ are jointly determined, are possible.\nA computationally tractable example for a more advanced simultaneous technique is based on the classification maximum likelihood method. In this context, estimates of $\\pmb\\theta$ and $\\pmb{H}$ are obtained by minimizing the negative log-likelihood function $$-\\ln{\\cal L}(\\theta,H|Y)=-\\sum_{t=1}^{T}\\sum_{j\\in S}\\sum_{k\\in S\\backslash\\{j\\}}n_{t j k}\\ln(p_{t j|\\{j,k\\}})$$ with $$n_{t j k}=\\sum_{i=1}^{N}h_{t i}y_{i j k},\\quad\\forall t,j,k,\\quad h_{t i}\\in\\{0,1\\},\\quad\\forall t,i,\\quad\\sum_{t=1}^{T}h_{t i}=1,\\forall i.$$\n\nThe model parameters are estimated using paired comparison data from respondents, with segment-specific ideal point distributions characterized by multivariate normal parameters. Simultaneous estimation techniques are employed to jointly determine segment parameters and assignments."
  },
  {
    "qid": "econ-empirical-1804-1-1-1",
    "question": "4) Using the Granger causality test results, show that the shock timing is orthogonal to pre-shock financial decisions. Reference the test statistic and the 10% rejection rate.",
    "gold_answer": "1. Granger test: Regress shock occurrence on 12 lags of financial variables.  \n2. Null: All lag coefficients = 0.  \n3. Under \\( H_0 \\), expect \\( 0.1 \\times 13 = 1.3 \\) rejections at \\( \\alpha = 0.1 \\).  \n4. Observed: 1 rejection, within Poisson fluctuation bounds.  \n5. Fail to reject orthogonality (p > 0.1 for 12/13 variables).",
    "question_context": "In over 87 percent of the cases, shocks affected only one household per village at any one month.\nColumn 1 of online Appendix Table B1 shows within-household correlations of the probability of experiencing a shock in period \\( t+1 \\) and contemporaneous financial characteristics... The null is only rejected in one out of 13 cases at 10 percent.\n\nThe shocks are validated as idiosyncratic and exogenous through statistical tests, ensuring they are uncorrelated with household characteristics or network trends."
  },
  {
    "qid": "econ-empirical-1486-3-0-3",
    "question": "4) Prove the claim in Case 2: For every \\( k = 1, \\dots, n-1 \\) and every \\( t \\in T^k \\), \\( N_{k+1}^k(t) \\subseteq S(t) \\).",
    "gold_answer": "Proof by induction:\n\n1. For \\( k = 1 \\) and \\( t \\in T^1 \\), \\( N_1^1(t) \\subseteq S(t) \\).\n2. By wEF, \\( S(t) \\) must also contain \\( i^*(t) \\), so \n\n\\[ N_1^1(t) \\cup \\{i^*(t)\\} = N_2^1(t) \\subseteq S(t). \\]\n\n3. Assume, for induction, that \\( N_k^{k-1}(t) \\subseteq S(t) \\) for all \\( t \\in T^{k-1} \\).\n4. For \\( t \\in T^k \\), there exist \\( t^1, \\dots, t^k \\in T^{k-1} \\) such that \n\n\\[ t_i = \\begin{cases} t_i^l & \\text{if } i \\neq i^*(t^l), \\\\ 1 - \\varepsilon & \\text{if } i = i^*(t^l). \\end{cases} \\]\n\n5. By SP and IR, \\( i^*(t^l) \\in S(t) \\) and \n\n\\[ p_{i^*(t^l)} \\leq 1/k - \\varepsilon \\text{ for every } l. \\]\n\n6. Thus, \\( N_k^k(t) \\subseteq S(t) \\), and by wEF, \\( N_{k+1}^k(t) \\subseteq S(t) \\).",
    "question_context": "Let \\( N = \\{1, \\dots, n\\} \\) where \\( n > 3 \\). For any type vector \\( t \\in \\Re_+^n \\), let \\( P(t) \\) be the set of all permutations of \\( t \\). For any \\( \\varepsilon \\in (0, 1/n) \\) and any \\( k = 1, \\dots, n \\), define the type vector \\( t^k(\\varepsilon) \\) as follows: \\( t_i^k(\\varepsilon) = \\begin{cases} 1 - \\varepsilon & \\text{if } i \\leq k, \\\\ 1/i - \\varepsilon & \\text{if } k < i. \\end{cases} \\)\nLet \\( T^k(\\varepsilon) = P(t^k(\\varepsilon)) \\) for every \\( k = 1, \\dots, n \\). If \\( k > 1 \\), then for every \\( t \\in T^k(\\varepsilon) \\), there exists some \\( t' \\in T^{k-1}(\\varepsilon) \\) such that \\( t_i = \\begin{cases} t_i' & \\text{if } t_i' \\neq 1/k - \\varepsilon, \\\\ 1 - \\varepsilon & \\text{if } t_i' = 1/k - \\varepsilon. \\end{cases} \\)\nLemma 2. If \\( m = (S, p) \\in \\phi \\) and \\( MWL^m < MWL^E \\), then there exists \\( \\bar{\\varepsilon} > 0 \\) such that for every \\( \\varepsilon \\in (0, \\bar{\\varepsilon}) \\) and \\( t \\in \\bigcup_{k=1}^n T^k(\\varepsilon) \\), \\( S(t) \\neq \\emptyset \\).\nCase 1: Weak demand monotonicity. Suppose \\( m \\) satisfies wDM. For \\( t^{k,n-1} \\in T^{n-1} \\), if \\( k \\in S(t^{k,n-1}) \\), then \\( k \\in S(t^*) \\) and \\( p_k(t^*) = p_k(t^{k,n-1}) \\leq 1/n - \\varepsilon \\).\nCase 2: Weak envy-freeness. Suppose \\( m \\) satisfies wEF. For every \\( k = 1, \\dots, n-1 \\) and every \\( t \\in T^k \\), define \\( N_k^k(t) = \\{i \\in N : t_i = 1 - \\varepsilon\\} \\) and \\( N_{k+1}^k(t) = \\{i \\in N : t_i \\geq 1/(k+1) - \\varepsilon\\} \\). The claim is that \\( N_{k+1}^k(t) \\subseteq S(t) \\).\n\nThis section generalizes the argument presented in the main text for \\( n = 3 \\) by developing notation and defining type vectors \\( t^k(\\varepsilon) \\) for \\( k = 1, \\dots, n \\). The goal is to show that any mechanism \\( m = (S, p) \\) satisfying certain conditions runs a budget deficit at some \\( t \\in \\bigcup_{k=1}^n T^k \\)."
  },
  {
    "qid": "econ-empirical-596-1-0-0",
    "question": "1) Prove that the block diagonal form of $\\Sigma_{uu}$ with $D_{\\delta_{1}}$ being at least $3\\times3$ ensures identifiability of $\\pmb{\\alpha}$ and $G$. Provide the key steps in the derivation.",
    "gold_answer": "1. **Assumption**: $\\Sigma_{uu}$ is block diagonal with $D_{\\delta_{1}}$ of dimension $\\geq 3\\times3$. \\n2. **Implication**: The off-diagonal blocks are zero, implying no correlation between errors in $D_{\\delta_{1}}$ and $\\Sigma_{22}$. \\n3. **Identifiability**: The structure allows separation of $\\pmb{\\alpha}$ and $G$ via the moment conditions derived from the covariance matrix. \\n4. **Key Equation**: For $D_{\\delta_{1}}$, the diagonal elements $\\delta_{1,i}$ provide distinct moment conditions: $$E[u_{i}u_{j}] = 0 \\text{ for } i \\neq j,$$ ensuring $\\pmb{\\alpha}$ and $G$ are identifiable.",
    "question_context": "The error covariance matrix $\\sum_{u u}$ is known to take on block diagonal form $$\\Sigma_{u u}=\\left(\\begin{array}{c c}{{D_{\\delta_{1}}}}&{{0}}\\\\ {{0}}&{{\\Sigma_{22}}}\\end{array}\\right),$$ where the block $D_{\\delta_{1}}$ is a square diagonal matrix of dimension at least $3\\times3$.\n$\\Sigma_{u u}$ is known to be a 'band' matrix of the form $$\\Sigma_{u u}=\\left[\\begin{array}{c c c c c}{\\sigma_{11}}&{\\sigma_{12}}&{0}&{0}\\\\ {\\sigma_{21}}&{.}&{.}&{.}&{0}\\\\ {0}&{.}&{.}&{.}&{.}\\\\ {.}&{.}&{.}&{.}&{.}\\\\ {.}&{.}&{.}&{.}&{\\sigma_{m-1m}}\\\\ {0}&{\\sigma_{m m-1}}&{.}&{\\sigma_{m m}}\\end{array}\\right],$$ and $^{m}$, the number of micro units, is at least five.\nThe block diagonal specification (a) is appropriate for the case in which we can isolate at least three micro error terms which are known to be uncorrelated with every other error term. The band covariance structure (b) captures the idea of correlation of adjoining pairs of errors.\n\nThe main result of this section is that replications model coefficient structure is identifiable under various sets of independence assumptions on the micro error terms. Two sets of independence assumptions, each of which is sufficient for identifiability of the parameters $\\pmb{\\alpha}$ and $G$ of (2.2), are discussed."
  },
  {
    "qid": "econ-empirical-771-1-3-1",
    "question": "7) Discuss the implications of the simulation results in Fig. 2 for cases where all components of $\\mu$ are positive or negative.",
    "gold_answer": "1. For $\\mu > 0$, all assets contribute to the optimal portfolio, leading to slower convergence to the asymptotic distribution.  \n2. For $\\mu < 0$, only a few assets are selected, resulting in faster convergence.  \n3. The maximum Sharpe ratio is higher for $\\mu > 0$ (e.g., 5.18) compared to $\\mu < 0$ (e.g., -0.01).  \n4. This highlights the impact of the mean return vector on portfolio optimization and estimator performance.",
    "question_context": "In this subsection, we report simulations in which a value of $\\mu\\neq0$ was selected, then fixed, as a realisation of a ${\\cal N}(0.5,1)$ random variable. Its components thus take both positive and negative values. A sample of (excess) returns were then generated as $N(\\mu,\\Sigma)$, where the covariance matrix $\\boldsymbol{\\Sigma}$ assumed each of the three forms introduced above: as $I_{d}$, as $\\varSigma_{1}(\\rho=-0.75)$, or as $\\Sigma_{2}(\\rho=0.75)$.\nFig. 1 depicts the resulting sampling distributions of $\\sqrt{n}(\\widehat{S R}_{n}^{+}-S R^{+})$ for various values of $n$, $d$ and $\\Sigma$ in this $\\mu\\ne0$ situation, together with the corresponding asymptotic distribution as given by (3.10).\n\nThis section presents simulation results to explore the finite-sample distributions of $\\widehat{S R}_{n}^{+}$ under various conditions, including different covariance structures and distributions of returns."
  },
  {
    "qid": "econ-empirical-972-4-1-0",
    "question": "1) Derive the Almost Ideal (AI) demand system for budget shares and explain the role of the price index in the model.",
    "gold_answer": "1. **AI Demand System**: The budget share for good $i$ is modeled as:\n   $$ w_i = \\alpha_i + \\sum_j \\gamma_{ij} \\ln p_j + \\beta_i \\ln \\left(\\frac{x}{P}\\right), $$\n   where $x$ is total expenditure, $p_j$ are prices, and $P$ is a price index.\n\n2. **Price Index Role**: The price index $P$ is defined as:\n   $$ \\ln P = \\alpha_0 + \\sum_k \\alpha_k \\ln p_k + \\frac{1}{2} \\sum_k \\sum_l \\gamma_{kl} \\ln p_k \\ln p_l. $$\n   It captures the nonlinear dependence of budget shares on prices and ensures homogeneity of degree zero.",
    "question_context": "Both heterogeneity and endogeneity play an important role in classical consumer demand. The most popular class of parametric demand systems is the almost ideal (AI) class, pioneered by Deaton and Muellbauer (1980). In the AI model, instead of quantities budget shares are being considered, and they are being explained by log prices and log total expenditure.\nThe model is linear in log prices and a term that involves log total expenditure linearly, but divided by a price index that depends on parameters of the utility function. In applications, one frequent shortcut is that the price index is replaced by an actual price index, another is that homogeneity of degree zero is imposed, which means that all prices and total expenditure are relative to a price index.\nA popular extension in this model allows for quadratic terms in total expenditure (QUAIDS, Banks et al. (1997)). However, we focus on the budget share for food at home (BSF ), which, due at least in parts to satiation effects, is often documented to decline steadily across the total expenditure range.\nThis motivates our individual level specification $B S F_{i}=b_{0i}+b_{1i}\\ln(T o t E x p_{i})+b_{2i}\\ln(F o o d p r i c e_{i})\\quad$ , where $T o t E x p_{i}$ and Foodpricei are the variables as described above. To relate it to the population model, we allow now for the intercept $b_{0i}$ to be a deterministic function of observable demographic variables $W_{i}$ and a time variable $T_{i}$ as well, and for all coefficients $b_{i}$ to in addition vary across the populations, leading the overall model \n$$ \n\\begin{array}{c}{{B S F_{i}=B_{0i}+B_{1i}\\ln(T o t E x p_{i})+B_{2i}\\ln(F o o d p r i c e_{i})}}\\ {{+b_{3}W_{1i}+b_{4}W_{2i}+b_{5}T_{i}.}}\\end{array} \n$$\n\nThis section discusses the application of the model to consumer demand, focusing on the Almost Ideal (AI) demand system and its extensions. The model accounts for heterogeneity and endogeneity in budget shares."
  },
  {
    "qid": "econ-empirical-1135-0-2-0",
    "question": "5) Analyze the impact of decentralization on diagnostic and communication quality. Why does decentralization improve diagnostic quality but not necessarily communication quality?",
    "gold_answer": "Decentralization improves diagnostic quality by empowering doctors with decision-making authority, enhancing their motivation to adhere to protocols. However, communication quality is more influenced by direct patient interactions, which are better in single-doctor private practices where doctors rely on patient fees and thus prioritize visible aspects of care.",
    "question_context": "We find that, after controlling for ability, both diagnostic and communication quality increase with decentralization. In addition, doctors in single-doctor private practices are better at communicating with their patients.\nOur findings show that some types of organizations can achieve results similar to those of private practices for diagnostic quality, though not necessarily for communication quality.\n\nThe findings highlight the significant role of organizational decentralization in improving doctors' performance, particularly in diagnostic quality."
  },
  {
    "qid": "econ-empirical-1612-0-0-2",
    "question": "3) Describe the disequilibrium maximum likelihood technique used in the paper and its role in estimating the rational expectations model.",
    "gold_answer": "1. Disequilibrium maximum likelihood accounts for periods where markets are not in equilibrium due to government interventions or other shocks. \n2. The likelihood function incorporates both equilibrium and disequilibrium regimes, with switching conditions based on policy rules. \n3. Stochastic-dynamic programming provides the theoretical restrictions (e.g., Euler equations) for the likelihood function. \n4. The combined approach ensures consistency with rational expectations while fitting observed data.",
    "question_context": "Stochastic-dynamic programming and disequilibrium maximum likelihood methods are combined to estimate a dynamic nonlinear rational expectations model of a market for a storable primary commodity.\nPrivate stockholding in the linear model, if it is incorporated at all, is typically driven by a linear intertemporal arbitrage equation in which the current and expected future prices differ by a predetermined cost of storage.\nThe linear model is also inherently incapable of capturing the nonlinear disequilibrium effects induced by massive government buffer stock purchases and sales at fixed support and release prices.\nOur estimation method calls for disequilibrium maximum likelihood techniques to be employed in combination with stochastic-dynamic programming, which is used to numerically impose the nonanalytic restrictions implied by rationality.\n\nThe paper discusses the estimation of dynamic nonlinear rational expectations models for primary commodity markets, incorporating both private and government stockholding dynamics. It critiques the limitations of linear models in capturing storage processes and introduces a method combining stochastic-dynamic programming with disequilibrium maximum likelihood techniques."
  },
  {
    "qid": "econ-empirical-335-2-1-1",
    "question": "2) Explain the role of the Tikhonov penalty $(\\epsilon n)^{-1}$ in the semiparametric mixture model. How does it address ill-posedness?",
    "gold_answer": "1. The penalty $(\\epsilon n)^{-1}$ regularizes the operator $[\\mathbb{E}_{Y|A} \\circ \\mathbb{E}_{A|Y} + (\\epsilon n)^{-1}I_{A}]$.\n2. It ensures the operator is nonsingular, addressing ill-posedness.\n3. The penalty controls the variance of the estimator in partially identified settings.\n4. It provides a trade-off between bias and variance.",
    "question_context": "We consider a class of semiparametric models, where the distribution of outcomes $Y$ conditional on unobserved latent variables $A\\in{\\mathcal{A}}$ is described parametrically by $Y|A\\sim g_{\\beta_{0}}(\\cdot|A)$ , with finite-dimensional unknown parameter $\\beta_{0}\\in B$ , while the distribution of $A\\sim\\pi_{0}$ is left unrestricted.\nThe parameter of interest is a functional of $\\beta_{0}$ and $\\pi_{0}$ , which takes the form of an expectation over $A$ ; that is, $\\delta_{\\beta_{0},\\pi_{0}}=\\mathbb{E}_{\\pi_{0}}\\Delta_{\\beta_{0}}(A)=\\int_{\\mathcal{A}}\\Delta_{\\beta_{0}}(a)\\pi_{0}(a)d a.$\nCorollary 2 (Semiparametric mixture models): $h_{\\epsilon}^{\\mathrm{MMSE}}(y)=s_{\\beta\\gamma}(y)^{\\prime}H_{\\beta\\gamma}^{-1}\\nabla_{\\beta\\gamma}\\delta+(\\epsilon n)\\big\\{\\mathbb{E}\\big[\\Delta(\\mathcal{A})-\\delta-\\overline{{h}}_{\\epsilon}^{\\mathrm{MMSE}}(\\mathcal{A})|Y=y\\big] -s_{\\beta\\gamma}(y)^{\\prime}H_{\\beta\\gamma}^{-1}\\mathbb{E}\\big[s_{\\beta\\gamma}(Y)\\big(\\Delta(\\mathcal{A})-\\overline{{h}}_{\\epsilon}^{\\mathrm{MMSE}}(\\mathcal{A})\\big)\\big]\\big\\},$ where $\\overline{{h}}_{\\epsilon}^{\\mathrm{MMSE}}(a):=\\mathbb{E}[h_{\\epsilon}^{\\mathrm{MMSE}}(Y)\\vert A=a].$\n\nThe text introduces semiparametric mixture models where the distribution of outcomes conditional on latent variables is parametric, but the distribution of latent variables is unrestricted. The target parameter is an expectation over the latent variables."
  },
  {
    "qid": "econ-empirical-1833-3-0-0",
    "question": "1) Derive the first-order conditions for the first-best utility maintenance problem \n$$\\textstyle\\operatorname*{Min}_{\\{x_{i},y_{i},c_{i}\\}}\\sum_{i=1}^{n}\\pi_{i}(x_{i}-y_{i})\\quad{\\mathrm{subject~to~}}x_{i}-h(y_{i}/a_{i}+c_{i})\\geq{\\bar{u}}$$ and explain the economic intuition behind each condition.",
    "gold_answer": "1. **Lagrangian Setup**: \n   $$\\mathcal{L} = \\sum_{i=1}^n \\pi_i(x_i - y_i) + \\sum_{i=1}^n \\mu_i[\\bar{u} - x_i + h(y_i/a_i + c_i)]$$\n2. **FOCs**:\n   - $\\frac{\\partial \\mathcal{L}}{\\partial x_i} = \\pi_i - \\mu_i = 0$ (Transfer-cost tradeoff)\n   - $\\frac{\\partial \\mathcal{L}}{\\partial y_i} = -\\pi_i + \\frac{\\mu_i h'(y_i/a_i + c_i)}{a_i} = 0$ (Labor distortion)\n   - $\\frac{\\partial \\mathcal{L}}{\\partial c_i} = \\mu_i h'(y_i/a_i + c_i) = 0$ (No workfare at first-best)\n3. **Interpretation**: Binding utility constraints ($\\mu_i>0$) imply no workfare ($c_i=0$), with labor supply adjusted to meet $\\bar{u}$ at minimum cost.",
    "question_context": "The first-best utility maintenance problem can be stated as \n$$\\textstyle\\operatorname*{Min}_{\\{x_{i},y_{i},c_{i}\\}}\\sum_{i=1}^{n}\\pi_{i}(x_{i}-y_{i})\\quad{\\mathrm{subject~to~}}x_{i}-h(y_{i}/a_{i}+c_{i})\\geq{\\bar{u}}$$\nProposition 4. The first-best UMP has the following form: \n$$\\begin{array}{r l}&{(x_{i},y_{j},c_{i})=(\\bar{u}-u_{i},\\tilde{y}_{i},0)\\qquadi=1,\\ldots,p,a n d}\\ &{(x_{i},y_{i},c_{i})=(\\tilde{y}_{i},\\tilde{y}_{i},0)\\qquadi=p+1,\\ldots,n.}\\end{array}$$\nThe second-best UMP solves: \n$$\\begin{array}{r}{\\underset{\\{x_{i},y_{i},c_{i}\\}}{\\mathbf{Min}}\\sum_{i=1}^{n}\\pi_{i}(x_{i}-y_{i})}\\end{array}$$ subject to (4.1),(6.1) and $x_{i}-h(y_{i}/a_{i}+c_{i})\\geq\\bar{u}$ for all i.\n\nThis section contrasts income maintenance with utility maintenance as social objectives, analyzing their implications for policy design under first-best and second-best scenarios."
  },
  {
    "qid": "econ-empirical-1788-4-3-0",
    "question": "7) Derive the sufficient condition for \\( PT_{v_b}(F_i) \\geq PT_{v_b}(F_j) \\) in Lemma A.8 and explain its relation to \\( F_i \\succ_{LIR} F_j \\).",
    "gold_answer": "1. Rearrange the condition: $$ E_{F_i}[v_b(x) | x \\leq F_i^{-1}(q)] - E_{F_j}[v_b(x) | x \\leq F_j^{-1}(q)] \\geq F_i^{-1}(q) - F_j^{-1}(q). $$ \\n2. By Lemma A.9, this holds if \\( E_{F_i}[x | x \\leq F_i^{-1}(q)] - E_{F_j}[x | x \\leq F_j^{-1}(q)] \\geq F_i^{-1}(q) - F_j^{-1}(q) \\), which is the definition of \\( F_i \\succ_{LIR} F_j \\).",
    "question_context": "Lemma A.8: If \\( F_i \\succ_{LIR} F_j \\) and \\( \\mathbb{E}_{F_j}[x] = \\mathbb{E}_{F_i}[x] \\), then \\( PT_{v_b}(F_i) \\geq PT_{v_b}(F_j) \\).\nA sufficient condition for \\( PT_{v_b}(F_i) \\geq PT_{v_b}(F_j) \\) is: $$ E_{F_i}[v_b(x) | x \\leq F_i^{-1}(q)] - F_i^{-1}(q) \\geq E_{F_j}[v_b(x) | x \\leq F_j^{-1}(q)] - F_j^{-1}(q). $$\n\nThe proof uses Lemma A.8 to show that the probability of trade \\( PT_{v_b}(F_i) \\) is higher under \\( F_i \\succ_{LIR} F_j \\) for a risk-averse buyer."
  },
  {
    "qid": "econ-empirical-788-0-0-0",
    "question": "1) Formally derive the principal's optimization problem in the two-period model with no commitment, specifying the constraints and objective function.",
    "gold_answer": "The principal maximizes expected profits over two periods, subject to:\n1. **Individual Rationality (IR)**: Agent's utility ≥ outside option.\n2. **Incentive Compatibility (IC)**: Agent prefers truth-telling.\n\nObjective: \n\\[ \\max_{q_1, q_2, t_1, t_2} \\mathbb{E}[S(q) - t] \\]\nwhere \\( S(q) \\) is the social surplus, \\( t \\) is transfer, and \\( q \\) is quality.",
    "question_context": "The agent whose cost is his private information may produce in the first period or be delayed until the second period. A signal about the cost of the agent is available between the two periods.\nThe principal has no commitment power and the signal is noncontractible. Then, she can make only one-period offers and the offer in period 1 is not conditional on the realization of the signal afterward.\n\nThe article examines how delay in contracting is influenced by an exogenous signal in a two-period model with asymmetric information. The principal offers contracts to an agent with private cost information, and a signal about the agent's cost is observed between periods. The quality of the good can vary, and the principal lacks commitment power."
  },
  {
    "qid": "econ-empirical-1500-0-0-0",
    "question": "1) Derive the optimal mechanism for the seller when the provision of information strictly increases in the buyer’s ex-ante valuation for low valuations and decreases for high valuations. Show how the seller overprovides information to high-valuation customers and underprovides to low-valuation customers.",
    "gold_answer": "1. Define the buyer's utility as \\( U := v + s \\), where \\( v \\) is prior information and \\( s \\) is learned from the seller.\\n2. The seller's profit is \\( \\Pi = i(v) - k(\\rho(v)) + p(v) - c_G \\).\\n3. For low-valuation buyers, the seller underprovides information to reduce costs, setting \\( \\rho(v) \\) below the efficient level.\\n4. For high-valuation buyers, the seller overprovides information to extract higher margins, setting \\( \\rho(v) \\) above the efficient level.\\n5. The optimal \\( \\rho(v) \\) is derived from the first-order condition \\( \\frac{\\partial \\Pi}{\\partial \\rho(v)} = 0 \\).",
    "question_context": "Under the seller’s optimal mechanism, the provision of information strictly increases in the buyer’s ex-ante valuation when this is still sufficiently low, while for higher values it strictly decreases. Compared to the efficient level of information, which depends on the endogenously set product price, the seller overprovides information to high-valuation customers and underprovides information to low-valuation customers.\nThe seller can make profits both from a margin on the sale of information and from a margin on the sale of the product. This follows Esö and Szentes [6,7], albeit by assuming that information quality is binary (“yes or no”) and that costs of information provision are sufficiently small, in their models all participating types receive the same information quality.\nThe provision of more precise information comes at non-decreasing cost \\( k(\rho) \\), which is continuously differentiable, with \\( k(0)=0 \\) and \\( k(\rho)\to\\infty \\) as \\( \rho\to\bar{\rho} \\).\nThe buyer’s utility is additively separable, \\( U:=v+s \\), where \\( v \\) represents his prior information, while information that he obtains from the seller allows the buyer to learn about \\( s \\), which satisfies \\( E[s]=0 \\).\n\nThe paper analyzes a seller's optimal provision of information in a setting where the seller can charge separately for the product and for information. The seller uses prices and information quality to discriminate between customers with different ex-ante valuations. The model assumes a monopolistic seller providing both product and information, with buyers differing in their privately observed ex-ante valuations."
  },
  {
    "qid": "econ-empirical-821-4-0-3",
    "question": "4) Show that $\\Phi(p^{*}) > \\Phi(p^{e})$ in Proposition 3, where $\\Phi(p) = D^{i}(p, p)p$.",
    "gold_answer": "1. By Lemma 5, $p^{*} > p^{e}$.\\n2. The function $\\Phi(p)$ is strictly concave with a unique maximum at $p^{M}$.\\n3. From the first-order condition, $\\Phi'(p^{*}) = \\frac{\\partial D^{i}(p^{*}, p^{*})}{\\partial p_{j}}w^{*} > 0$.\\n4. Since $\\Phi'(p^{*}) > 0$, $p^{*} < p^{M}$.\\n5. Thus, $\\Phi(p^{*}) > \\Phi(p^{e})$ because $p^{*}$ is closer to $p^{M}$ than $p^{e}$.",
    "question_context": "[A1.] $\\frac{\\partial D^{i}(p_{i},p_{j})}{\\partial p_{i}}<0$\n[A3.] $\\frac{\\partial^{2}\\Pi_{i}(p_{i},p_{j})}{\\partial p_{i}\\partial p_{j}}=\\frac{\\partial D^{i}(p_{i},p_{j})}{\\partial p_{j}}+(p_{i}-w_{i})\\frac{\\partial^{2}D^{i}(p_{i},p_{j})}{\\partial p_{i}\\partial p_{j}}>0,$\nProof of Lemma 1. The proof follows from (2) and the fact that $[{\\partial D^{i}(\\cdot)}/{\\partial p_{i}}]<0$ and $[\\partial\\boldsymbol{\\phi}_{i}(\\cdot),(\\cdot),$ $\\partial w_{i}]>0$.\nProof of Proposition 1. When manufacturers are vertically separated, condition (3) follows from Lemma 1 and (1). Under Assumptions A1–A4, this condition is necessary and sufficient for an optimum.\nProof of Lemma 2. We show that $w_{i}=w_{j}=0$ is not an equilibrium with symmetric beliefs.\nProof of Lemma 3. If $M_{i}$ expects his rival to offer the wholesale price $w^{*}$, he expects $R_{j}$ to choose price $\\hat{p}(w^{*})$.\nProof of Proposition 2. From (4), it follows that, given wholesale prices $w_{1}$ and $w_{2}$, retail prices are $\\boldsymbol{\\mathit{p}}_{1}=\\hat{\\boldsymbol{p}}(\\boldsymbol{w}_{1})$ and $\\displaystyle{p_{2}=\\hat{p}(w_{2})}$.\nProof of Lemma 4. Suppose that $M_{j}$ chooses the retail price $\\mathit{p_{j}}$ Given a contract $C_{i}=(w_{i},T_{i}),R_{i}$ chooses the retail price that solves $\\operatorname*{max}_{{\\boldsymbol{\\beta}}_{i}}[D^{i}({\\boldsymbol{\\phi}}_{i},{\\boldsymbol{\\phi}}_{j})({\\boldsymbol{\\phi}}_{i}-{\\boldsymbol{w}}_{i})-T_{i}]$.\nProof of Lemma 5. From Section 3.1, recall that the first order condition for the choice of $\\boldsymbol{p}^{*}\\equiv\\hat{\\boldsymbol{p}}(\\boldsymbol{w}^{*})$ is $\\frac{\\partial D^{i}(\\boldsymbol{\\phi}^{*},\\boldsymbol{\\phi}^{*})}{\\partial{p_{i}}}(\\boldsymbol{\\phi}^{*}-\\boldsymbol{w}^{*})+D^{i}(\\boldsymbol{\\phi}^{*},\\boldsymbol{\\phi}^{*})=0$.\nProof of Proposition 3. Consider the function $\\Phi(\\boldsymbol{p})\\equiv\\pi_{i}(\\boldsymbol{p},\\boldsymbol{p})=D^{i}(\\boldsymbol{p},\\boldsymbol{p})\\boldsymbol{p}$.\nProof of Proposition 4. By inspection of the first order condition (12), $0<w_{\\alpha}^{\\ast}<w^{\\ast}$ for $\\alpha\\in\\mathsf{\\Gamma}(0,1).$.\nProof of Lemma 6. Consider a symmetric separating equilibrium in which $M_{i}$ offers a wholesale price defined by the function $w^{*}\\left(c_{i}\\right)$ and $R_{i}$ charges a retail price defined by the function $\\boldsymbol{p}^{*}\\left(\\boldsymbol{c}_{i}\\right)$.\nProof of Proposition 5. When manufacturers have the same marginal cost, by symmetry equilibrium demand is equal to $\\textstyle{\\frac{1}{2}}$ for each retailer.\n\nThe appendix outlines technical assumptions and proofs related to competitive assignment models, focusing on demand functions, profit maximization, and equilibrium conditions under various organizational structures."
  },
  {
    "qid": "econ-empirical-463-0-0-3",
    "question": "4) Show how the seller’s optimal ex ante contract can be implemented using indirect clauses like the right of first refusal.",
    "gold_answer": "1. The right of first refusal allows the first buyer to match any bid $b_2$ from the second buyer.\\n2. The optimal bid for the second buyer is $b_2(v_2) = v_2/2 + (1-\\lambda)/4$.\\n3. The first buyer exercises the right if $v_1 \\geq b_2(v_2)$.\\n4. This induces the same winning probabilities and payments as the direct mechanism: $Q_1^*(v_1) = \\frac{1+\\lambda}{2}v_1 + \\frac{1-\\lambda}{2}$ and $Q_2^*(v_2) = \\frac{2}{1+\\lambda}v_2 - \\frac{1-\\lambda}{1+\\lambda}$.",
    "question_context": "The seller’s optimal strategic ex ante contract more accurately reflects joint opportunity costs of the seller and the contracted buyer, and therefore extracts more rent from the entrant.\nThe seller and the first buyer could make an upfront transfer to achieve different splits of their joint surplus. Because the upfront transfer is limited by the first buyer’s ex ante wealth, the rent extracted and trade facilitated also are restricted.\nThe seller’s optimal ex ante contract can have two effects upon social efficiency compared to the absence of ex ante contracts. First, the contract facilitates more trade. Second, this optimal ex ante contract affects the allocation between the two buyers, because the first buyer enjoys a competitive advantage.\n\nThis article examines the strategic use of ex ante contracts between a seller and an uninformed buyer prior to an auction involving two potential buyers. The optimal ex ante contract reflects joint opportunity costs and mitigates the seller's ex post rent seeking, potentially increasing social welfare."
  },
  {
    "qid": "econ-empirical-1362-4-1-0",
    "question": "3) Formally derive the PSI formula and explain how adjusting the CE range by \\(±\\varepsilon\\) affects the hit rate and area metrics.",
    "gold_answer": "The PSI is defined as: \\[ \\text{PSI} = \\text{Hit Rate} - \\text{Area} \\] where Hit Rate = fraction of prices within the CE range, and Area = normalized width of the CE range. Adjusting by \\(±\\varepsilon\\): \\[ \\text{Hit Rate}(\\varepsilon) = \\frac{\\# \\{p_t \\in [\\underline{P}^{*}-\\varepsilon, \\overline{P}^{*}+\\varepsilon]\\}}{T} \\] \\[ \\text{Area}(\\varepsilon) = \\frac{\\overline{P}^{*} - \\underline{P}^{*} + 2\\varepsilon}{P_{\\text{max}} - P_{\\text{min}}} \\] Optimal \\(\\varepsilon\\) balances precision (high Hit Rate) and accuracy (low Area).",
    "question_context": "The absence of subjects thus may create two problems. On the one hand, it may shrink the CE range to a single price (which occurs in 26% of the cases), possibly resulting in values of the hit rate close to zero and hence low values of PSI even though the prices may well be in close proximity to the CE.\nSpecifically, we recompute the predictive success indices by substituting the original CE ranges \\([\\underline{P}^{*}, \\overline{P}^{*}]\\) with ranges of the form \\([\\underline{P}^{*}-\\varepsilon, \\overline{P}^{*}+\\varepsilon]\\) for various values of \\(\\varepsilon \\in \\mathbb{R}\\).\n\nThe PSI measures how well the competitive equilibrium (CE) range predicts realized prices, accounting for subject inactivity and CE range variability."
  },
  {
    "qid": "econ-empirical-1137-0-0-3",
    "question": "4) The study finds no significant change in the wage gap for women who worked in both 1976 and 1985. Discuss potential explanations for this finding, considering selection bias and labor force commitment.",
    "gold_answer": "Possible explanations:\n- **Selection bias**: Women working continuously may face unobserved constraints (e.g., discrimination) that limit wage growth.\n- **Commitment**: High commitment may not offset structural barriers (e.g., occupational segregation).\n- **Data limitation**: The sample may exclude women with intermittent labor force participation, who could show different trends.",
    "question_context": "Between 1976 and 1985 the wage gap between white men and women narrowed by approximately 4 percent. This study finds that nearly 50 percent of this reduction was due to average changes in job tenure and other work history variables over this period.\nThe dependent variable used in the estimated wage equations was total labor income divided by number of hours worked. The independent variables included: 1) highest grade attained of formal education, 2) labor force attachment variables, 3) a variety of work-history characteristics, and 4) demographic controls to adjust for differences in city size and region of the country.\nThe work history variables were divided into five categories: years of work experience prior to working for one's present employer (and its square), number of years worked full-time, years out of the labor force since completing school, whether the person was working part-time, and tenure with one's current employer.\nTenure with one's current employer was divided into three categories: years with the current employer prior to one's current position, years of training completed in one's current position, and years of post-training completed.\n\nThis study examines the changes in wage differentials between white men and white women from 1976 to 1985 using data from the Panel Study of Income Dynamics (PSID). The analysis focuses on the impact of work experience, tenure, and on-the-job training on wage gaps."
  },
  {
    "qid": "econ-empirical-357-3-1-1",
    "question": "4) Why is the cost of business cycles sensitive to the lower bound $\\underline{\\phi}_{c}$ of the job creation cost distribution? Provide a mathematical explanation.",
    "gold_answer": "1. The mass of permanent-loss fragile jobs depends on the area of $(\\phi_{c},\\phi_{p})$ pairs where $\\phi_{c} \\geq \\underline{\\phi}_{c}$.\n2. Reducing $\\underline{\\phi}_{c}$ increases this area, leading to more permanent-loss jobs.\n3. The cost is given by:\n   $$\\text{Cost} = \\int_{\\underline{\\phi}_{c}}^{\\overline{\\phi}_{c}} \\int_{\\tilde{\\phi}_{p,\\text{no-bc}}}^{\\infty} \\phi_{p} f(\\phi_{c},\\phi_{p}) d\\phi_{p} d\\phi_{c}.$$\n4. Thus, a higher $\\underline{\\phi}_{c}$ reduces the integral's lower limit, increasing the cost.",
    "question_context": "In Lucas (1987), the welfare cost of business cycles is estimated to be less than $0.1\\%$ of aggregate consumption when agents are risk averse and the coefficient of relative risk aversion is equal to 10. By contrast, the numbers in Table 2 vary from $2.03\\%$ to $12.7\\%$ of aggregate output.\nThe $3.18\\%$ cost of business cycles is, thus, almost completely due to permanent-loss fragile jobs. That is, business cycles are costly, because jobs that are valuable from a social welfare point of view are no longer created in the presence of business cycles.\n\nThe section reports estimates for the cost of business cycles, comparing them to Lucas (1987) and discussing the role of different job types and structural parameters."
  },
  {
    "qid": "econ-empirical-1307-0-2-1",
    "question": "6) How might policymakers use the paper's bounded-variation results despite their inherent ambiguity? Provide a cost-benefit framework for aggregation across crimes.",
    "gold_answer": "Policy application steps:\\n1. **Crime-specific bounds**: Use Table 5 to extract bounds for each crime (e.g., murder: [−16%, ∞), assault: [8%, ∞)).\\n2. **Weight by social costs**: Multiply bounds by estimated costs per crime (e.g., \\$10M/murder, \\$100k/assault).\\n3. **Aggregate**: Sum weighted bounds to get total cost impact.\\n4. **Sensitivity analysis**: Vary assumptions (e.g., \\(\\delta\\)) to check robustness.\\n\\nExample: If RTC reduces murder costs by ≥\\$160M but increases assault costs by ≥\\$800k, net effects depend on relative weights.",
    "question_context": "Researchers are rewarded for producing simple findings leading to definitive policy prescriptions (e.g., more guns leads to less crime), yet generating such findings requires strong assumptions that cannot be persuasively defended.\nAdequate expression of caution requires formal methods to perform empirical research under assumptions that are weak enough to be credible.\n\nThe paper critiques the literature's reliance on strong assumptions and advocates for transparent reporting of partial identification results."
  },
  {
    "qid": "econ-empirical-775-3-3-1",
    "question": "8) Analyze Example 7 to explain how non-competitive allocations can be stochastically stable in economies with non-singleton recurrent classes.",
    "gold_answer": "Analysis of Example 7:\n- The economy has no strong core allocations because coalitions $\\{1,2\\}$ or $\\{1,3\\}$ can always weakly improve upon any allocation.\n- There are two competitive allocations: $(e_{2}, e_{1}, e_{3})$ and $(e_{3}, e_{2}, e_{1})$.\n- The other four allocations form a non-singleton recurrent class.\n- Transitions within this class have low resistance, e.g., $(e_{2}, e_{1}, e_{3}) \\to (e_{3}, e_{1}, e_{2}) \\to (e_{3}, e_{2}, e_{1}) \\to (e_{2}, e_{3}, e_{1}) \\to (e_{2}, e_{1}, e_{3})$.\n- These allocations are stochastically stable because they form the minimal resistance recurrent class.\n- Thus, non-competitive allocations can be stochastically stable in economies with non-singleton recurrent classes.",
    "question_context": "For this class of economies, we shall show that some non-singleton recurrent classes are selected by the criterion of stochastic stability. When this happens, the weak recontracting process with mistakes identifies extra allocations that the economy will visit a positive proportion of time in the long run, even though none of them are coalitionally stable in the sense of the strong core.\nThe next example shows that the set of stochastically stable states may contain non-competitive allocations, as well as allocations that are not Pareto indifferent to competitive ones.\n\nIn economies with non-singleton recurrent classes, the stochastically stable states of $\\mathcal{M}_{W}^{\\epsilon}$ may include non-competitive allocations."
  },
  {
    "qid": "econ-empirical-1596-1-2-0",
    "question": "6) Derive the producer's optimal output supply \\( Q_{t+1}^{p} = \\frac{E_{t}s_{t+1}}{g + r^{p}V_{t}s_{t+1}} \\) under price uncertainty, assuming quadratic costs and CARA utility.",
    "gold_answer": "The derivation steps are:\n1. Profit: \\( \\Pi_{t+1}^{p} = s_{t+1}Q_{t+1}^{p} - \\frac{1}{2}g(Q_{t+1}^{p})^{2} \\).\n2. CARA utility: \\( U^{p}(\\Pi) = -\\exp(-r^{p}\\Pi) \\).\n3. Expected utility maximization is equivalent to maximizing:\n   \\[ E_{t}\\Pi_{t+1}^{p} - \\frac{1}{2}r^{p}V_{t}\\Pi_{t+1}^{p} \\]\n4. Substituting \\( \\Pi_{t+1}^{p} \\):\n   \\[ E_{t}s_{t+1}Q_{t+1}^{p} - \\frac{1}{2}g(Q_{t+1}^{p})^{2} - \\frac{1}{2}r^{p}(Q_{t+1}^{p})^{2}V_{t}s_{t+1} \\]\n5. First-order condition w.r.t. \\( Q_{t+1}^{p} \\):\n   \\[ E_{t}s_{t+1} - g Q_{t+1}^{p} - r^{p} Q_{t+1}^{p} V_{t}s_{t+1} = 0 \\]\n6. Solving for \\( Q_{t+1}^{p} \\) yields the result.",
    "question_context": "The price-taking producer can be considered to maximize his expected utility of profit: $E_{t}U^{p}(\\Pi_{t+1}^{p}), \\Pi_{t+1}^{p}=s_{t+1}Q_{t+1}^{p}-G(Q_{t+1}^{p})$, where $E_{t}$ is the mathematical expectation operator conditional on information available at time $t$, $U^{{\\pmb{\\mathrm{p}}}}({\\pmb{\\cdot}})$ is a strictly concave von Neumann-Morgenstern utility function, $\\Pi_{t+1}^{p}$ is the producer's profit obtained in time $t+1$, $\\scriptstyle{\\mathbf{Q}}_{t+1}^{p}$ is the quantity produced in period $\\pmb{t}+1$, and $G(\\cdot)$ is a strictly convex cost function.\nWith the same quadratic cost function, the identical utility function and normally distributed spot prices, the optimum levels of output supply and futures trading can be, respectively, expressed as $\\boldsymbol{Q}_{t+1}^{p*}=f_{t}/\\underline{{\\boldsymbol{g}}}$, $R_{t}^{p}=-Q_{t+1}^{p*}+Z_{t}^{p}, Z_{t}^{p}=\\frac{E_{t}s_{t+1}-f_{t}}{r^{p}V_{t}s_{t+1}}$.\n\nConsider a producer who makes a decision in period $t$ to produce output for period $t+1$. At the time of the production decision, the period $t+1$ sales price $\\scriptstyle{\\mathfrak{s}}_{t+1}$ is unknown, but once the decision is made, the quantity to be produced $\\mathbf{\\calQ}_{t+1}$ is certain."
  },
  {
    "qid": "econ-empirical-1149-4-0-0",
    "question": "1) Using the IV estimates from Table 5, derive the percentage difference in annual hours of work for teen mothers aged 21-35 compared to if they had delayed childbearing. Show the calculation steps.",
    "gold_answer": "1. **Given**: Teen mothers worked 1,045 hours/year (ages 21-35).\n2. **Estimate**: 'All Covariates' suggests 21% fewer hours if delayed.\n3. **Calculation**: \n   - Hours if delayed = 1,045 / (1 - 0.21) ≈ 1,322.78 hours/year.\n   - Difference = 1,322.78 - 1,045 = 277.78 hours/year.\n   - Percentage difference = (277.78 / 1,322.78) × 100 ≈ 21%.",
    "question_context": "While women who have teen births work slightly fewer hours at ages 18 and 19 than they would have if they had delayed their childbearing, from age 20 on, they actually work more hours in a year than if they had postponed their childbearing. By age 28, for example, women who have their first births early work between 342 and 405 more hours than if they had delayed their childbearing until adulthood.\nOver the ages of 21 through 35, teen mothers worked an average of 1,045 hours per year. Based on the 'All Covariates' estimates, teen mothers would have worked an average of 21 percent fewer hours per year over this age range if they had delayed their first birth until adulthood.\nBy age 28, teen mothers have worked between 1,999 to 2,600 more hours—a year or more of full-time work—than if they had delayed childbearing.\nUnlike annual hours of work, there is little evidence of a strong life cycle pattern with respect to the effects of early childbearing on hourly wage rates based on the age-specific estimates presented in Table 5.\n\nThe text discusses the effects of early childbearing on annual hours of work, cumulative hours worked, and hourly wage rates over a woman's life cycle, using IV estimates from Table 5."
  },
  {
    "qid": "econ-empirical-1496-0-0-1",
    "question": "2) How does the increase in two-earner households affect residential location choices in urban areas?",
    "gold_answer": "Two-earner households tend to choose residential locations that balance the commuting distances for both earners. This often results in a preference for suburban areas that are equidistant to multiple employment centers, leading to a more decentralized residential pattern.",
    "question_context": "The increase in two-earner households has led to significant changes in urban land use, as the spatial distribution of employment and residential locations must accommodate the commuting patterns of both earners.\n\nThis paper examines the impact of the rise in two-earner households on urban land use patterns, focusing on the spatial distribution of employment and residential locations."
  },
  {
    "qid": "econ-empirical-222-5-0-2",
    "question": "3) Explain the rationale for including sex worker fixed effects in the regression model and how this addresses potential sorting concerns.",
    "gold_answer": "Sex worker fixed effects control for unobserved heterogeneity across sex workers (e.g., attractiveness, skill, location). This addresses sorting because:\n1. It isolates price variation *within* the same sex worker across different client types.\n2. It eliminates bias from time-invariant sex worker characteristics that might correlate with both client ethnicity and pricing.\n3. The resulting estimates reflect how the *same* sex worker adjusts prices based on client ethnicity, not differences across sex workers.",
    "question_context": "The first column of Table 4 shows the results from this estimation. Relative to the base group (Chinese), the same sex worker suggests an initial price to whites with an $11\\%$ ( $10~\\mathrm{log}$ points) premium and gives Bangladeshis a $13\\%$ discount on the initial price offer, thus asking whites for almost $30\\%$ more than she asks from Bangladeshis.\nColumn (2) repeats the exercise but drops all observations with missing data on client characteristics and the corresponding dummy variables. The estimates are broadly similar to those in the first column but generally suggest somewhat larger ethnicity effects.\nIn Table 5, we examine the relation between the transaction price and ethnicity. We remind the reader that we do not observe this price when bargaining fails. Moreover, our model does not have predictions about the transaction price independent of the initial price since sex workers make take-it-or-leave it offers.\nFigure 1 plots the estimated compensating differentials relative to durations of less than 30 minutes. Although clearly we must be careful about reading too much into eight data points, we make a number of observations. First, there are compensating differentials. Sex workers are either able to bargain for a higher price for longer durations or sufficiently able to predict duration to extract at least a partial compensating differential.\n\nThe study examines price differentials among ethnic groups in the context of sex workers' pricing behavior, controlling for various client characteristics and service attributes to determine whether observed differences reflect sorting or discrimination."
  },
  {
    "qid": "econ-empirical-100-3-0-0",
    "question": "1) Derive the conditions under which women who reject contraception or abortion still engage in premarital sexual activity, given the isomorphic model structure.",
    "gold_answer": "1. The model substitutes participation for promise, so the decision to engage in sex is analogous to the decision to promise marriage.  \n2. Women who reject the new technology face a strategic dilemma: if they abstain, their partners may seek satisfaction elsewhere.  \n3. The payoff for engaging in sex is higher than abstaining due to the fear of partner desertion, leading to increased participation even among rejectors.",
    "question_context": "Under a slight reinterpretation the previous game structure illustrates how increased competition may affect sexual participation. In this analogous model, women decide whether or not to engage in premarital sex at all, and men then decide whether to remain in relationships without sexual activity.\nThe advent of contraception and abortion used by others may result in an unwanted increase in sexual participation for those who reject the new technology.\n\nThis section presents a model where women decide whether to engage in premarital sex, and men decide whether to remain in relationships without sexual activity. The model is isomorphic to a previous one, with participation substituting for promise. The advent of contraception and abortion leads to increased sexual participation, even among those who reject the new technology."
  },
  {
    "qid": "econ-empirical-1088-2-0-1",
    "question": "2) Using the data from Table 1, calculate the percentage change in enrollment of D.C. residents at public four-year institutions in Maryland and Virginia between 1998 and 2000, given the $6,213 decrease in tuition relative to UDC and the elasticity estimate from Table 2.",
    "gold_answer": "1. From Table 2, the elasticity estimate is 8% per $1,000 decrease in tuition.\n2. The tuition decrease is $6,213, so the predicted enrollment change is: \\[ 8\\% \\times \\left(\\frac{6213}{1000}\\right) = 49.7\\% \\]\n3. The actual enrollment change was from 146 to 398, a 176% increase, suggesting other factors may also influence enrollment.",
    "question_context": "In 1998, half of all D.C. freshmen attending an institution outside D.C. attended schools where they made up less than 1 percent (0.8 percent) of the freshmen at the school. The maximum representation of D.C. residents at schools outside D.C at all grade levels in 1998 was at the Maryland College of Art and Design, where they made up 12 percent of the student body.\nTable 1 reports the tuition paid by D.C. residents (in 2002 dollars), net of D.C. TAG awards, at different types of institutions from 1998 through 2002. The first two rows describe categories of institutions whose prices were not directly affected by the change in policy—-UDC and private nonprofit institutions outside the D.C. metropolitan area that were not historically black institutions.\nThe next category of institutions, public two- and four-year colleges outside D.C., was most directly affected by the D.C. TAG program. The price of public four-year institutions in Maryland and Virginia fell by $6,213 relative to the price at UDC between 1998 and 2000.\nThe timing and magnitude of the price changes also varied among private historically black colleges and universities (HBCUs) in the D.C. area. The policy first affected only the private institutions in the D.C. area and private HBCUs in Maryland and Virginia. The prices at these institutions fell by $1,735 and $2,511, respectively, relative to the price of UDC.\nTable 2 reports the effects of regressing log enrollment of D.C. residents on tuition for D.C. residents, time dummies, and institutional fixed effects. Between 1998 and 2000, each $1,000 relative price decrease was associated with an 8 percent increase in D.C. freshmen enrollment and a 10 percent increase in freshmen enrollment of recent high school graduates from D.C.\nTable 3 reports the results for various subsets of institutions, using the change in the log of the number of recent high school graduates from D.C. enrolled as the dependent variable. The top left panel limits the sample to institutions in D.C., Virginia, Pennsylvania, Maryland, North Carolina, New York, Georgia, and Delaware.\nTable 4 reports the number of recent high school graduates from D.C. enrolled at the 12 selective colleges in D.C., Maryland, and Virginia reporting 75th percentile math SAT scores above 650. At George Washington, Georgetown, and Howard universities there were large declines in absolute numbers of D.C. freshmen enrolled between 1998 and 2000.\n\nThe analysis focuses on the percentage changes in enrollment of D.C. residents at various institutions associated with changes in the prices they faced, using a conditional logit specification. The study accounts for institutional characteristics fixed over time and examines the impact of the D.C. TAG program on tuition and enrollment patterns."
  },
  {
    "qid": "econ-empirical-1616-3-0-0",
    "question": "1) Prove that $T_{i}^{\\prime}(\\theta_{i})<0$ and interpret its economic significance in the context of the war of attrition model.",
    "gold_answer": "1. Start with the equilibrium condition where each group's concession behavior is described by $T(\\theta)$.\\n2. Differentiate $T(\\theta)$ with respect to $\\theta$ to show $T^{\\prime}(\\theta) < 0$.\\n3. Interpretation: Higher costs ($\\theta$) lead to earlier concessions ($T(\\theta)$ decreases), as the group prefers to stabilize sooner to avoid higher losses.",
    "question_context": "We now want to find a symmetric Nash equilibrium in which each group's concession behavior is described by the same function $T(\\theta)$.\n\nThis lemma establishes the negative derivative of the concession function with respect to the cost parameter, indicating that higher costs lead to earlier concessions."
  },
  {
    "qid": "econ-empirical-105-1-0-1",
    "question": "2) Derive the conditions under which gold production could serve as a predictor of inflation in the pre-WWI gold standard era. How does this relate to the quantity theory of money?",
    "gold_answer": "1. Let \\( G_t \\) be gold production at time \\( t \\) and \\( M_t \\) the money supply, which is tied to gold reserves under the gold standard. \n2. The quantity theory states \\( P_t = \\frac{M_t V_t}{Y_t} \\), where \\( P_t \\) is the price level, \\( V_t \\) is velocity, and \\( Y_t \\) is output. \n3. If \\( \\Delta G_t \\) increases \\( M_t \\), then \\( P_t \\) rises (inflation) assuming \\( V_t \\) and \\( Y_t \\) are stable. \n4. Thus, \\( G_t \\) forecasts inflation via its impact on \\( M_t \\) and \\( P_t \\).",
    "question_context": "Average U. S. inflation was 3.1 percentage points higher in the years after 1896, yet nominal interest rates were no higher after 1896. This nonadjustment of nominal rates would be consistent with rational expectations if inflation was not forecastable, and indeed univariate tests show little sign of serial correlation. But gold production does forecast inflation. The relationship between mining and inflation was such that expected inflation should have risen 300 basis points between 1890 and 1910.\n\nThe study examines interest and inflation rates from 1879 to 1913, highlighting a shift from deflation to inflation post-1896. Despite a 3.1 percentage point increase in average U.S. inflation after 1896, nominal interest rates did not adjust accordingly. The paper explores whether this was due to rational expectations under unforecastable inflation or other factors, such as gold production's predictive power over inflation."
  },
  {
    "qid": "econ-empirical-79-16-2-0",
    "question": "1) Using the bootstrap results from Figure VI, test the hypothesis that the proportion of positive coefficients for active ingredient knowledge (0.86) is greater than 0.5. Provide the test statistic and p-value.",
    "gold_answer": "Test statistic:\n\\[ z = \\frac{0.86 - 0.5}{0.05} = 7.2 \\]\nThe p-value for a one-tailed test is < 0.0001. We reject the null hypothesis that the proportion is 0.5, concluding that health experts systematically favor store brands.",
    "question_context": "Figure VI shows that the coefficient on active ingredient knowledge is positive in 43 out of 50 cases. The share of coefficient estimates that are positive is thus 0.86, which has a bootstrap standard error of 0.05, and is therefore highly statistically distinguishable from the null hypothesis of no effect (half of coefficients positive).\nThe differences among the coefficients in Figure VI are instructive. The coefficients tend to be larger and more significant for medications and relatively smaller for first aid and eye care products, suggesting that in the latter group informed shoppers perceive true quality differences.\n\nThis section extends the analysis to a broad set of health products, including medications, first aid products, and eye care items. It examines the effects of active ingredient knowledge and health care occupation on store-brand purchases."
  },
  {
    "qid": "econ-empirical-6-1-2-1",
    "question": "2) Calculate the coverage ratio of the sample districts (455) to the total number of districts in India, and discuss its implications for representativeness.",
    "gold_answer": "Assuming India has approximately 740 districts, the coverage ratio is $\\frac{455}{740} \\approx 61.5\\%$. While substantial, the exclusion of mountainous and northeastern regions may bias results for those areas.",
    "question_context": "I combine data on retail prices and production, available only for administrative districts of India. There are 455 districts in the sample I consider, so they can substantially capture the spatial heterogeneity.\nI use monthly data on retail prices at the district level from the National Sample Survey (NSS) Schedule 3.01(R), Rural Price Collection Survey (RPC) survey of the Central Statistical Organization of India.\nGridded data on rainfall are from Willmott and Matsuura (2001). I compute district-level monthly rainfall by taking an inverse-distance weighted average of all the grid points in the boundary of any district.\n\nThis section describes the comprehensive microdata set assembled for the analysis, including sources and methodologies for geospatial and price data."
  },
  {
    "qid": "econ-empirical-963-1-0-2",
    "question": "3) What does the author imply about the political dynamics of the Tory ministry under Liverpool during 1819-1823, and how does this affect the study's focus?",
    "gold_answer": "The author describes Liverpool's ministry as an \"arch-mediocrity\" with little prospect of being ousted, implying political stagnation. This context justifies the study's narrower focus on parliamentary debates rather than broader policy impacts, as the debates themselves were a more dynamic arena for economic ideas.",
    "question_context": "This book is described by its publisher as “ a study of the interplay of ideas and actions in Britain during some of the turbulent years of industrial revolution which followed the end of the Napoleonic wars\", but more modestly and correctly by its author as “\" an analysis of the economic content of parliamentary debate\" during the years 1819-23.\nThe reasons for undertaking a detailed study of parliamentary debate during these four years might not, therefore, be obvious to historians, but will be self-evident to economists - for these are the years when Ricardo was in the Commons, together with such men as Matthias Attwood, Henry Brougham, and Alexander Baring.\n\nThis section discusses the analysis of economic content in parliamentary debates during the years 1819-1823, focusing on the presence of notable economists like Ricardo and others."
  },
  {
    "qid": "econ-empirical-96-1-0-0",
    "question": "1) Derive the normalization factor for the 80–20 wage gap using the standard normal distribution function $\\Phi(.)$ and explain its purpose in comparing wage dispersion measures.",
    "gold_answer": "The normalization factor is derived as follows: \\[ \\text{Normalization Factor} = \\Phi^{-1}(0.8) - \\Phi^{-1}(0.2) \\approx 1.683 \\]. This factor scales the observed wage gaps to a common metric, facilitating comparison across different measures of wage dispersion. If log wages were normally distributed, all normalized gaps would equal the standard deviation of log wages.",
    "question_context": "Figure II plots four measures of wage dispersion for full-time males: the standard deviation of log wages (including imputed wages for censored observations), the gap in log wages between the 80th and 20th percentiles, the gap between the $80\\mathrm{th}$ and $50\\mathrm{th}$ percentiles, and the gap between the $50\\mathrm{th}$ and $20\\mathrm{th}$ percentiles. To facilitate comparisons we normalize the gap measures by dividing by the corresponding percentile gaps of a standard normal variate.\nThe standard deviation rose by 15 log points from 1985 to 2009, the (normalized) 80–20 gap rose by 16 log points, the $80{-}50~\\mathrm{gap}$ rose by 15 log points, and the 50–20 gap rose by 18 log points.\nThe growth rate of the standard deviation of log wages increased from 0.23 log point a year in the 1985–96 period to 0.96 log point a year from 1996 to 2009.\nResidual inequality rises a little less than overall wage inequality (from 0.30 in 1985 to 0.43 in 2009), but exhibits the same shift in trend in the mid-1990s.\nWithin-plant inequality, as measured by the residual standard error of the regression model rises by only 0.05 between 1985 and 2009, compared to the 0.13 rise for the baseline model that controls for education and experience.\nThe value of the index for full-time male workers increases steadily, from 0.34 in 1985 to 0.47 in 2009.\nOver our sample period the index rises steadily from 0.46 to 0.53, implying that high- and low-wage occupations are increasingly segregated between establishments.\nIn the 1985–1991 period, for example, a transition from quartile 1 to quartile 4 was associated with a trend-adjusted wage increase of roughly 23 log points, and in the 2002–2009 period, the same transition was associated with a 47 log point increase.\n\nThis section examines trends in wage inequality among full-time male workers in West Germany from 1985 to 2009, focusing on various measures of wage dispersion and the impact of job changes on wages."
  },
  {
    "qid": "econ-empirical-1067-3-1-1",
    "question": "6) Solve for the optimal second-period allocations $y_{2}(1)$ and $y_{2}(b)$ for anonymous consumers. Under what condition is $y_{2}(b) = 0$?",
    "gold_answer": "1. The optimal allocations are $y_{2}(1) = 1$ and $$ y_{2}(b) = \\begin{cases} 0 & \\text{if } \\gamma_{1} \\leq \\frac{1-2b + \\gamma_{b}(b-a)}{1-a-b}, \\\\ 1 & \\text{otherwise}. \\end{cases} $$\n2. $y_{2}(b) = 0$ when the fraction of identified high-type consumers ($\\gamma_{1}$) is sufficiently low relative to other parameters.",
    "question_context": "Without commitment, the mechanism-design problem is modified as follows. At the start of the first period, each consumer announces her value for the good $\\widehat{\\boldsymbol{w}}_{1}$ and an indicator that equals $_{I D}$ if she wishes the firm to remember her first period report and $A N$ otherwise.\nSecond-period profits from anonymous consumers can then be written as total surplus less utility, or $$ \\pi_{2}=\\operatorname*{max}_{y_{2},Y_{2}}\\frac{1-\\gamma_{1}}{2}\\left[(1-a)(y_{2}(1)-U_{2}(1\\mid A N))+a(b y_{2}(b)-U_{2}(b\\mid A N))\\right] $$ $^\\mathrm{\\textregistered}$ RAND2001. $$ +\\frac{1-\\gamma_{b}}{2}\\left[(1-a)(b y_{2}(b)-U_{2}(b\\mid A N))+a(y_{2}(1)-U_{2}(1\\mid A N))\\right] $$ subject to (A4), (A5), and $y_{2}(\\cdot)\\in[0,1]$.\n\nThis section solves for the optimal mechanism when commitment is not possible. The mechanism must be time-consistent, and consumers may choose to remain anonymous."
  },
  {
    "qid": "econ-empirical-1837-1-0-3",
    "question": "4) Explain how RDT differs from PT in terms of the decision weights assigned to outcomes, and derive the decision rule for RDT as given in the text.",
    "gold_answer": "1. **Difference in Decision Weights**: \n   - In PT, decision weights $\\pi(\\cdot)$ depend only on the probability of the outcome.\n   - In RDT, decision weights $g_i(\\cdot)$ depend on both the probability and the rank of the outcome relative to others.\n\n2. **Derivation of Decision Rule for RDT**: \n   - The utility of a prospect $Q$ under RDT is: \n     $$ W(Q) = \\sum_{i=1}^n g_i(p_1, \\dots, p_n) u(x_i) $$\n   - For $S$ and $R$, the utilities are: \n     $$ W(S) = f(r)A + [f(p+q+r+s) - f(r)]B + [1 - f(p+q+r+s)]C $$\n     $$ W(R) = f(p+r)A + [f(p+r+s) - f(p+r)]B + [1 - f(p+r+s)]C $$\n   - Normalize by setting $A = 0$ and $C = 1$: \n     $$ W(S) = [f(p+q+r+s) - f(r)]B + [1 - f(p+q+r+s)] $$\n     $$ W(R) = [f(p+r+s) - f(p+r)]B + [1 - f(p+r+s)] $$\n   - Subtract $W(R)$ from $W(S)$ to get the decision rule: \n     $$ S \\gtrsim R \\Leftrightarrow [f(p+q+r+s) - f(p+r+s)](B - 1) + [f(p+r) - f(r)] \\gtrsim 0 $$",
    "question_context": "Kahneman and Tversky (1979, p. 276) assume that individuals choose between 'regular prospects' that is prospects of the form $(x,p;y,q)$ which contain at most two non-zero consequences, ${\\pmb x}$ and ${\\pmb y}$ which occur with probabilities $\\pmb{p}$ and $\\pmb q$ respectively where either $\\pmb{p}+\\pmb{q}<1$ ,or $x\\leq0\\leq y$ or $x\\ge0\\ge y-\\mathtt{s o}$ as to maximise overall value $V(\\cdot)$ given by the expression: \n\n$$ V(x,p;y,p)=\\pi(p)v(x)+\\pi(q)v(y) $$ \n\nwhere $\\pmb{v}(\\cdot)$ is a value index assigned to changes in wealth with ${\\mathfrak{v}}(0)=0$ and $\\pi(\\cdot)$ assigns decision weights to probabilities with ${\\boldsymbol{\\pi}}(0)=0$ and ${\\pmb\\pi}({\\bf1})={\\bf1}$ . Theories like this, incorporating subjective decision weights on probabilities, are often called 'subjective expected utility' models (SEU).\nSetting $\\pmb{A}=\\pmb{0}$ (as required by PT) and $C=1$ (an arbitrary normalisation involving no loss of generality) then subtracting (3) from (2) gives the decision rule; \n\n$$ S{\\overset{\\underset{\\r{\\gtrsim}}{\\gtrsim}}{\\gtrsim}}R\\Leftrightarrow[\\pi(p+q+s)-\\pi(s)]B+[\\pi(t)-\\pi(q+t)]{\\overset{\\geq}{\\rightleftarrows}}0. $$\nRDT is also a SEU model but differs from PT in that the decision weight $\\pmb{g}(\\cdot)$ attached to any consequence of a prospect depends, not only on the true probability of that consequence, but also on its ranking relative to the other outcomes of the prospect. Let ${\\pmb u}(\\cdot)$ represent the utility function defined over a set of consequences $\\{x_{1},\\ldots,x_{n}\\}$ where $x_{1}<\\cdots<x_{n}$ , and $f(\\cdot)$ denote an increasing function mapping from [o, 1] to [0, 1] transforming objective probabilities into decision weights with ${f}(0)=0$ and $f(1)=1$ . Using $W(\\cdot)$ to represent the utility of a prospect, then for any prospect $\\boldsymbol{Q},$ where $Q=$ $[x_{1},p_{1};\\dots;x_{n},p_{n}]$ RDT entails; \n\n$$ \\begin{array}{r}{W(Q)=\\sum_{i=1}^{n}g_{i}\\big(p_{1},\\dots,p_{n}\\big)u(x_{i})}\\end{array} $$ \n\nwhere \n\n$$ \\begin{array}{c l l}{{g_{i}(p_{1},\\dots,p_{n})=[f(\\sum_{j=1}^{i}p_{j})-f(\\sum_{j=1}^{i-1}p_{j})]}}&{{\\mathrm{for}i=2,\\dots,n}}\\\\ {{\\neg f(p_{i})}}&{{\\mathrm{for}i=1.}}\\end{array} $$\n\nThe text discusses the application of Prospect Theory (PT) and Rank Dependent Theory (RDT) to decision-making under risk, focusing on the evaluation of regular and strictly positive prospects. It outlines the mathematical formulations and predictions of these theories regarding horizontal, vertical, and north-west movements in the unit probability triangle."
  },
  {
    "qid": "econ-empirical-588-1-0-1",
    "question": "2) Show how the Bayesian posterior distribution $p(\\beta, \\rho | y)$ is derived from the likelihood and prior distributions given in the text. Include the integration steps over $\\sigma_\\varepsilon$ and $\\alpha$.",
    "gold_answer": "1. Start with the likelihood:\n$$\np(y|X,\\alpha,\\beta,\\sigma_\\varepsilon,\\rho) \\propto (1-\\rho^2)^{1/2} \\sigma_\\varepsilon^{-T} \\exp\\left(-\\frac{1}{2\\sigma_\\varepsilon^2} K(\\alpha,\\beta,\\rho)\\right).\n$$\n2. Multiply by the prior:\n$$\np(\\alpha,\\beta,\\rho,\\sigma_\\varepsilon) \\propto (1-\\rho^2)^{-1/2} \\sigma_\\varepsilon^{-1}.\n$$\n3. Combine to get the joint posterior:\n$$\np(\\alpha,\\beta,\\sigma_\\varepsilon,\\rho|y) \\propto \\sigma_\\varepsilon^{-(T+1)} \\exp\\left(-\\frac{1}{2\\sigma_\\varepsilon^2} K(\\alpha,\\beta,\\rho)\\right).\n$$\n4. Integrate over $\\sigma_\\varepsilon$ using the properties of the inverse-gamma distribution.\n5. Then integrate over $\\alpha$ by completing the square in $K(\\alpha,\\beta,\\rho)$.\n6. The result is the marginal posterior for $(\\beta, \\rho)$.",
    "question_context": "Consider the following model: \n$$\n\\begin{array}{r l r}&{y_{t}=\\alpha+\\beta X_{t}+\\mu_{t},}&\\ &{X_{t}=\\lambda X_{t-1}+\\nu_{t},}&\\ &{\\mu_{t}=\\rho\\mu_{t-1}+\\varepsilon_{t},~}&{t=1,2,...,T,}\\end{array}\n$$\nwhere $y_{t}$ and $X_{t}$ are the tth observations on the dependent and independent variables, respectively, $x,\\beta,-1<\\lambda<1$ ,and $-1<\\rho<1$ are unknown parameters,and $\\nu_{t}$ and $\\varepsilon_{t}$ are independently and normally distributed with zero means and respective variances $\\sigma_{\\nu}^{2}$ and $\\sigma_{\\varepsilon}^{2}$.\nThe Prais-Winsten (PW) estimator steps are:\n(1) Estimate \n$$\ny_{t}=\\rho y_{t-1}+X_{t}\\beta-X_{t-1}\\beta\\rho+\\varepsilon_{t},\n$$\nby OLS to obtain $\\hat{\\boldsymbol\\rho}$ from the coefficient of $y_{t-1}$.\n(2) Transform the original equation as follows:\n$$\ny_{t}^{\\ast}=\\alpha^{\\ast}+\\beta X_{t}^{\\ast}+\\varepsilon_{t},\\qquad t=1,2,...,T,\n$$\nwhere \n$$\n\\begin{array}{r l}&{y_{1}^{*}=y_{1}(1-\\rho^{2})^{\\pm},}\\ &{X_{1}^{*}=X_{1}(1-\\rho^{2})^{\\pm},}\\ &{y_{t}^{*}=y_{t}-\\rho y_{t-1},}\\ &{X_{t}^{*}=X_{t}-\\rho X_{t-1}.}\\end{array}\n$$\n(3) Estimate the transformed system by OLS, replacing $\\rho$ with $\\hat{\\rho}$.\nThe Bayesian approach assumes the prior distribution:\n$$\np(\\alpha,\\beta,\\rho,\\sigma_{\\varepsilon})\\propto(1-\\rho^{2})^{-\\frac{1}{2}}\\sigma_{\\varepsilon}^{-1},\n$$\nwhere $\\alpha,\\beta$ , and log $\\sigma_{\\varepsilon}$ are uniformly distributed over the ranges $-\\infty<\\alpha, \\beta <\\infty$ , $0<\\sigma_{\\varepsilon}<\\infty$ , and $\\rho$ has a beta pdf with parameters $\\textstyle{\\bigl(}{\\frac{1}{2}},{\\frac{1}{2}}{\\bigr)}$ over the range $|\\rho|<1$. The posterior distribution is:\n$$\np(\\alpha,\\beta,\\sigma_{\\varepsilon},\\rho\\big|y)\\propto\\sigma_{\\varepsilon}^{-(T+1)}\\exp\\biggl[-\\frac{1}{2\\sigma_{\\varepsilon}^{2}}K(\\alpha,\\beta,\\rho)\\biggr].\n$$\nIntegration yields:\n$$\np(\\beta,\\rho|y)\\propto\\left[\\displaystyle\\sum_{t=1}^{T}V_{t}^{2}\\right]^{-\\frac{1}{2}}\\Biggl\\lbrace\\displaystyle\\sum_{t=1}^{T}\\big(y_{t}^{*}-\\beta X_{t}^{*}\\big)^{2} -\\left[\\displaystyle\\sum_{t=1}^{T}V_{t}\\big(y_{t}^{*}-X_{t}^{*}\\beta\\big)\\right]^{2}\\Biggl/\\displaystyle\\sum_{t=1}^{T}V_{t}^{2}\\Biggr\\rbrace^{-(T-1)/2}.\n$$\n\nThe text presents a detailed econometric model with autocorrelated errors and discusses both the Prais-Winsten estimator and Bayesian approaches to handle the nuisance parameter problem."
  },
  {
    "qid": "econ-empirical-64-2-2-0",
    "question": "1) Compare and contrast the predictions of focusing and diminishing sensitivity in the context of the calculator and jacket example.",
    "gold_answer": "Comparison of focusing and diminishing sensitivity:\n1. Focusing predicts utility-maximizing choices in balanced trade-offs, regardless of $v(\\cdot)$.\n2. Diminishing sensitivity predicts different choices if $v(\\cdot)$ is nonlinear, e.g., $v(-120) - v(-125) < v(-10) - v(-15)$.\n3. In the calculator example, diminishing sensitivity explains the preference for saving $\\$5$ on a $\\$15$ item.\n4. Focusing alone does not capture this unless $v(\\cdot)$ is nonlinear.",
    "question_context": "Because our theory predicts that people overweight a large advantage whereas diminishing sensitivity implies that larger deviations carry proportionally less weight, the two are seemingly in contradiction.\nFollowing Remark 3, we can add diminishing sensitivity to our model by incorporating it into the consumption-utility function, and letting focus be defined by differences in this modified utility function.\n\nThis section examines the interaction between focusing and diminishing sensitivity in decision-making."
  },
  {
    "qid": "econ-empirical-940-2-0-0",
    "question": "1) Derive the conditions under which the MELO estimator achieves greater MSE reduction compared to the LS estimator, considering the parameters $\\sigma_{\\ell}^{2}$, $\\sigma_{u}^{2}$, and $\\pmb\\beta$.",
    "gold_answer": "1. Start with the MSE expressions for both estimators: \\[ \\text{MSE}_{\\text{LS}} = \\text{Var}(\\hat{\\beta}_{\\text{LS}}) + \\text{Bias}(\\hat{\\beta}_{\\text{LS}})^2 \\] \\[ \\text{MSE}_{\\text{MELO}} = \\text{Var}(\\hat{\\beta}_{\\text{MELO}}) + \\text{Bias}(\\hat{\\beta}_{\\text{MELO}})^2 \\]\n2. The MELO estimator introduces shrinkage, reducing variance but potentially increasing bias. The shrinkage factor $F$ (from Eq. 10) is inversely related to the precision of reduced-form estimates.\n3. The relative efficiency gain occurs when \\[ \\text{Var}(\\hat{\\beta}_{\\text{LS}}) - \\text{Var}(\\hat{\\beta}_{\\text{MELO}}) > \\text{Bias}(\\hat{\\beta}_{\\text{MELO}})^2 - \\text{Bias}(\\hat{\\beta}_{\\text{LS}})^2 \\]\n4. This is most likely when $\\sigma_{\\ell}^{2}$ is small (low variability in reduced-form regressors) and $\\pmb\\beta$ is moderate, as high $\\pmb\\beta$ reduces the benefit of shrinkage.",
    "question_context": "The entries in the table are the percent reductions in mean-squared error (MSE) due to the use of MELO rather than LS, under a variety of values of $\\sigma_{i:\\cdot}\\sigma_{u}$ and $\\pmb\\beta$. MELO is always best, often by a large margin.\nThe relative efficiency of the MELO estimator depends significantly on $\\sigma_{\\ell}^{2},\\sigma_{u}^{2}$, and $\\pmb\\beta$ and tends to be greatest when the variability of price relative to acreage is small.\nThe likelihood function contains relatively little information about the location of the reduced-form coefficients when variability of the reduced-form regressors is small, which is precisely the case in which the shrinkage induced by MELO estimation will be beneficial.\nThe amount of shrinkage varies inversely with the precision of the reduced-form parameter estimates, as evidenced by the expression for the shrinkage factor $F$ in (10).\n\nThe section discusses Monte Carlo simulation results comparing the Minimum Expected Loss (MELO) estimator with Least Squares (LS) estimator, focusing on mean-squared error (MSE) reductions under various parameter configurations."
  },
  {
    "qid": "econ-empirical-65-2-0-0",
    "question": "1) Derive the conditions under which debt financing dominates equity financing for excessively liquid assets, referencing the transformation constraint (7).",
    "gold_answer": "1. The transformation constraint (7) ensures that debt payments \\(P_{1}^{*}\\) and \\(P_{2}^{*}\\) are set below the amounts the investor could extract from bargaining. \\n2. Equity lacks ex ante limitations on payments, leading to asset transformation by the manager. \\n3. Debt limits investor power to default states, preventing transformation. \\n4. Thus, debt dominates equity when (7) binds: \\(P_{1}^{*} + P_{2}^{*} \\leq C_{1} + C_{2} + d_{2} - \\alpha\\gamma[C_{1} + (1-a)C_{2} + d_{2}]\\).",
    "question_context": "The investor needs some ownership rights in order to extract repayment. Debt gives ownership contingent on default. The only alternative is equity, that is, a financing contract where the investor has unconditional ownership from the beginning.\nFor illiquid or liquid assets, where financing capacity is determined by the anticipated outcomes of bargaining at dates 1 and 2, debt and equity are essentially equivalent. For excessively liquid assets, where financing capacity is determined by the transformation constraint (7), debt dominates equity.\n\nThe text discusses the limitations of financing contracts, focusing on debt and equity, and their implications for asset transformation and liquidity."
  },
  {
    "qid": "econ-empirical-150-2-0-1",
    "question": "2) Explain the likelihood ratio test used to discriminate between model 2 and model 3, including the test statistic distribution and critical value at the 5% significance level.",
    "gold_answer": "1. The likelihood ratio test compares the restricted model (model 2) against the unrestricted model (model 3).\\n2. The test statistic is calculated as $LR = 2(\\ln L_{3} - \\ln L_{2})$, where $L_{3}$ and $L_{2}$ are the likelihoods of models 3 and 2, respectively.\\n3. The observed test statistic is 1.08.\\n4. The critical value for a chi-square distribution with 3 degrees of freedom at the 5% significance level is 7.81.\\n5. Since 1.08 < 7.81, we accept model 2 over model 3.",
    "question_context": "In one version we have imposed the parameter restrictions $\\alpha_{L K^{\\circ}}=\\alpha_{K^{\\circ}K^{\\circ}}=\\alpha_{R K^{\\circ}}=0$ which implies that $K_{t}^{\\circ}=-\\alpha_{K K^{\\circ}}K_{t-1}$ I.e., in this case the depreciation rate of capital is constant with $\\delta^{\\pmb{\\kappa}}=1-\\alpha_{K K^{\\circ_{\\pmb{\\Lambda}}}}$.\nThe FIML estimate for $\\boldsymbol{\\vartheta}$ from the combined model is 0.85 with an estimated standard error of 0.08, leading us to accept model 2 over model 1.\nThe observed value for our test statistic is 58.16, compared to a critical value of approximately 84. Thus, also this test leads us to accept model 2, i.e., to accept the hypothesis of a constant depreciation rate for capital for the U.S. electrical machinery industry.\nThe normalized adjustment cost for capital and R&D in any period is given by $0.5\\alpha_{\\dot{K}\\dot{K}}\\varDelta K^{2}/Y^{1/\\rho}$ and $0.5\\alpha_{\\vec{R}\\vec{R}}\\varDelta R^{2}/Y^{1/\\rho}$ ,respectively.For model 2 the sample average of the ratio of adjustment costs to gross investment is 0.14 and 0.15 for capital and R&D, respectively.\n\nThis section discusses the estimation of two versions of model (4.3)-(4.6) using U.S. electrical machinery industry data, comparing models with constant and variable capital depreciation rates. It also contrasts these with a third model using exogenous capital depreciation rates from the OBA data bank."
  },
  {
    "qid": "econ-empirical-1740-0-0-3",
    "question": "4) Show how the model's predictions align with Markusen and Melvin's (1981) finding that factor mobility and trade are complements under scale economies. Contrast this with the case of unskilled labor.",
    "gold_answer": "1. **Complementarity (Markusen and Melvin)**: \n   - Scale economies amplify gains from trade when factors move to regions of higher productivity. \n   - In this model, capital and skilled labor flows increase trade volume (\\(\\Delta T > 0\\)).\n\n2. **Unskilled Labor Exception**: \n   - If unskilled labor reduces Southern traditional good output, trade may decline (\\(\\Delta T \\leq 0\\)). \n   - This depends on the elasticity of substitution between sectors.",
    "question_context": "The model predicts that skilled and unskilled labor migrate from South to North and capital migrates from North to South. Migration of skilled labor and capital exhibits complementarity with trade while migration of labor may exhibit complementarity or substitutability.\nThe modern good exhibits external economies of scale of 'national’ variety in Ethier's (1982a) terminology. The traditional good is subject to constant returns to scale. There are three factors of production, capital, skilled labor and unskilled labor.\nThe model generates a pattern of factor flows which is consistent with broad empirical evidence. In particular, it predicts that capital must migrate from North to South while labor, both skilled and unskilled, must flow in the opposite direction.\nA small outflow of skilled labor (the so-called 'brain drain') from South is likely to lower the welfare of those left behind. A small inflow of capital into South is also likely to hurt the Southern population.\n\nThe paper develops a North-South model driven by scale economies in the modern sector, predicting factor migration patterns and their effects on trade and welfare."
  },
  {
    "qid": "econ-empirical-948-7-2-0",
    "question": "3) Prove that K(ω) is U-shaped over ω ∈ Ω, using the properties of g(ω)−ω and the condition ∑ω̃[g(ω̃)−ω̃]n(ω̃)=0.",
    "gold_answer": "3. Steps:\n   - From Proposition 1, g(ω)−ω is strictly decreasing in ω.\n   - The condition ∑ω̃[g(ω̃)−ω̃]n(ω̃)=0 implies g(ω)−ω changes sign over supp(N).\n   - At ω=min supp(N), g(ω)−ω > 0; at ω=max supp(N), g(ω)−ω < 0.\n   - Since K(ω) = \\frac{T′′}{2}[ω−g(ω)]^2, it is minimized when |g(ω)−ω| is smallest (near the median) and increases symmetrically away from this point, forming a U-shape.",
    "question_context": "We know from Proposition 1 that g(ω)−ω is a strictly decreasing function of ω over supp(N). Moreover, ∑ω̃[g(ω̃)−ω̃]n(ω̃)=0, which implies that g(ω)−ω>0 for ω=min supp(N), and g(ω)−ω<0 for ω=max supp(N). Taken together with (A.8), these two observations imply that K(ω) is a U-shaped function of ω ∈ Ω, achieving its minimum when the distance between g(ω) and ω is smallest.\n\nThe proof shows that K(ω) is U-shaped over ω ∈ Ω, driven by the properties of g(ω)−ω and the distribution N."
  },
  {
    "qid": "econ-empirical-948-1-0-1",
    "question": "2) Prove that the Nash bargaining solution for the CDS contract $\\gamma(\\omega,\\tilde{\\omega})$ maximizes the surplus $\\gamma(\\omega,\\tilde{\\omega})(\\Gamma'[g(\\tilde{\\omega})]-\\Gamma'[g(\\omega)])$ subject to the trade size limit $-k \\leq \\gamma(\\omega,\\tilde{\\omega}) \\leq k$. Show that the solution is $\\gamma(\\omega,\\tilde{\\omega})=k$ if $g(\\tilde{\\omega})>g(\\omega)$, $-k$ if $g(\\tilde{\\omega})<g(\\omega)$, and any value in $[-k,k]$ if $g(\\tilde{\\omega})=g(\\omega)$.",
    "gold_answer": "1. The surplus is linear in $\\gamma(\\omega,\\tilde{\\omega})$, so the optimal trade direction depends on the sign of $\\Gamma'[g(\\tilde{\\omega})]-\\Gamma'[g(\\omega)]$.  \n2. Since $\\Gamma'[g]$ is strictly increasing, $\\Gamma'[g(\\tilde{\\omega})]-\\Gamma'[g(\\omega)]$ is positive if $g(\\tilde{\\omega})>g(\\omega)$ and negative if $g(\\tilde{\\omega})<g(\\omega)$.  \n3. Thus, to maximize surplus:  \n   - If $g(\\tilde{\\omega})>g(\\omega)$, set $\\gamma(\\omega,\\tilde{\\omega})=k$.  \n   - If $g(\\tilde{\\omega})<g(\\omega)$, set $\\gamma(\\omega,\\tilde{\\omega})=-k$.  \n   - If equal, any $\\gamma(\\omega,\\tilde{\\omega})\\in[-k,k]$ yields zero surplus.",
    "question_context": "Traders have utility functions with identical constant absolute risk aversion (CARA), and they are endowed with a technology to make payments by producing a storable consumption good at unit marginal cost. Precisely, if an agent consumes $C$ and produces $H$, his utility is $U(\\check{C}-H)=-\\frac{1}{\\eta}e^{-\\eta(\\dot{C}-H)}$ for some coefficient of absolute risk aversion $\\eta>0$.\nBanks are heterogeneous in several dimensions: they differ in their sizes, in their fixed costs of entry in the OTC market, and in their risk-sharing needs. The size of a bank is denoted by $S$, and the fixed cost of entry by $c$. The distribution of per-trader entry costs is represented by $\\Phi(z)$, where $z=c/S$.\nBanks receive heterogeneous initial endowments of a risky asset, with per-trader endowment $\\pmb{\\omega}$. The payoff of the asset is $1-D$, where $D$ is aggregate default risk with strictly positive mean and twice continuously differentiable moment generating function.\nIn the OTC market, traders bargain over CDS contracts with trade size limit $k$. The terms of trade are determined via symmetric Nash bargaining, maximizing the surplus $\\gamma(\\omega,\\tilde{\\omega})(\\Gamma'[g(\\tilde{\\omega})]-\\Gamma'[g(\\omega)])$, where $g(\\omega)=\\omega+\\sum_{\\tilde{\\omega}}\\gamma(\\omega,\\tilde{\\omega})n(\\tilde{\\omega})$ is the post-trade exposure to default risk.\n\nThis section presents the economic environment populated by a unit continuum of risk-averse agents (traders) with CARA utility functions. Traders are organized into heterogeneous banks differing in size, fixed entry costs, and risk-sharing needs. The OTC market features bilateral trading of CDS-like contracts with trade size limits."
  },
  {
    "qid": "econ-empirical-1103-4-0-0",
    "question": "1) Derive the conditions under which excluding teachers with higher absenteeism rates would bias the estimated impact of teacher shocks, assuming a concave production function in teaching inputs.",
    "gold_answer": "1. Let the production function for learning be \\( L = f(T) \\), where \\( T \\) is teaching inputs, and \\( f''(T) < 0 \\) (concave).  \n2. The marginal impact of a shock is \\( \\frac{dL}{dT} \\), which decreases as \\( T \\) decreases (higher absenteeism).  \n3. Excluding high-absenteeism teachers truncates the distribution of \\( T \\), removing observations where \\( \\frac{dL}{dT} \\) is smaller.  \n4. The estimated impact \\( \\hat{\\beta} \\) will thus be biased upward, as it reflects only the steeper part of the production function.",
    "question_context": "The impact of teacher shocks for this subsample is negative and significant in all virtually all specifications. However, the size of the coefficient changes considerably when more controls are added.\nThe estimated impact of teacher shocks is stable across different specifications, but not across different samples. For English, coefficients vary between -0.033 and -0.035 standard deviations, and for Mathematics, between -0.030 and -0.036.\nExcluding teachers with higher absenteeism rates may affect estimates if the marginal impact of the shock decreases with the extent of the shock, as predicted by a concave production function in teaching inputs.\nA teacher shock that increases absenteeism by one day every month leads to a decline in learning by 0.015 and 0.017 standard deviations for the full nonmovers sample, equivalent to a 3.7 to 4 percent decrease in learning.\nSchool-level shocks, such as teacher turnover or declines in school quality, do not appear to bias the estimates of teacher shocks, as shown by the inclusion of proxies like mean days of teacher absence and teacher turnover dummies.\nPersistent effects from unobserved teacher characteristics are unlikely to be critical, as baseline test scores show no association with absenteeism, and teacher characteristics do not alter the estimated impact of teacher shocks.\nThe sensitivity of findings to the definition of absences is explored, with inconclusive results on whether different types of absences (e.g., illness, funerals, training) have varying impacts on learning.\n\nThis section explores the robustness of findings in the nonmovers sample, which controls for unobserved teacher heterogeneity. The analysis focuses on the impact of teacher shocks on student learning, examining stability across specifications and samples, potential nonlinearities, and the role of school-level shocks."
  },
  {
    "qid": "econ-empirical-1354-0-0-3",
    "question": "4) Formally show how the steady-state version of the model for $a\\in\\{2,\\ldots,A\\}$ incorporates workers' option to reject single offers. Derive the implications for labour market tightness ($\\theta$) and the wage distribution.",
    "gold_answer": "1. **Worker's Decision Problem**: A worker who receives one offer in the current period maximizes expected utility by comparing the monopsony wage to the expected future wage if they wait. \n2. **Steady-State Condition**: The equilibrium $\\theta$ and wage distribution must satisfy $V_u = \\max\\{w_m, \\delta V_u\\}$, where $V_u$ is the worker's expected value of unemployment and $\\delta$ is the discount factor. \n3. **Implications**: The steady-state equilibrium retains inefficiency ($\\theta$ too high) and wage dispersion, as workers reject low offers, perpetuating the coordination frictions.",
    "question_context": "When $a=1$, the common posted wage lies between the competitive and monopsony levels, and equilibrium is efficient. When $a>1$, all vacancies post the monopsony wage. Some workers fail to find a job, some find a job at the monopsony wage, and some—those for whom there is competition—get the competitive wage. Equilibrium is inefficient when $a>1$; in particular, there is excessive vacancy creation.\nIn the benchmark competitive search equilibrium model (Moen, 1997), equilibrium is constrained efficient. We show that changing the basic directed search model to allow workers to make more than one application results in equilibria that are not constrained efficient.\nWhen $a=1$, our model is essentially the limiting version of Burdett, Shi and Wright (2001, hereafter BSW) translated to a labour market setting. BSW derive a unique symmetric equilibrium in which (in the labour market version) all vacancies post a wage between 0 (the monopsony wage) and 1 (the competitive wage).\nIn our model, when $a\\in\\{2,\\ldots,A\\}$, all vacancies post the monopsony wage in the unique symmetric equilibrium. This leads to equilibrium wage dispersion. Some workers (those who receive exactly one offer) are employed at the monopsony wage, and some workers (those who receive multiple offers) have their wages bid up to the competitive level.\n\nThe paper analyzes a model of equilibrium directed search in a large labour market where workers make a fixed number of job applications, denoted as $a$. The model explores the implications of multiple applications on wage posting, equilibrium efficiency, and vacancy creation."
  },
  {
    "qid": "econ-empirical-1704-4-0-2",
    "question": "3) Explain the role of second-order stochastic dominance in determining the interval for $\\alpha^{*}$ when $\\mathrm{E}_{\\mu}[\\sigma]>\\tilde{\\sigma}$.",
    "gold_answer": "The role of second-order stochastic dominance:\n1. For $\\alpha < \\alpha^{\\mathrm{MEU}}$, the expected utility distribution is second-order stochastically dominated by the degenerate distribution at $\\alpha^{\\mathrm{MEU}}$.\n2. Since $\\Phi$ is strictly concave, the decision-maker prefers the less risky distribution, implying $\\alpha^{*} \\in [\\alpha^{\\mathrm{MEU}}, 1]$.\n3. This ensures that the optimal $\\alpha^{*}$ is not below $\\alpha^{\\mathrm{MEU}}$.",
    "question_context": "Under constant absolute ambiguity aversion, we have $\\Phi(x)=-\\frac{1}{\\gamma}e^{-\\gamma x}$ (see Klibanoff et al. 2005).\nSuppose first that $\\mathrm{E}_{\\mu}[\\sigma]>\\tilde{\\sigma}$ holds. Note that $\\operatorname{E}_{\\mu}\\operatorname{E}_{\\sigma}[\\pi_{b}]$ increases in $\\alpha$ and that $\\operatorname{Var}_{\\mu}[\\operatorname{E}_{\\sigma}[\\pi_{b}]]>0$ for all $\\alpha<\\alpha^{\\mathrm{MEU}}$.\nLetting $\\Pi_{\\sigma}(\\alpha)=\\sigma\\alpha({v_{h}}-{c_{h}})+(1-\\sigma)({v_{l}}-\\alpha{c_{h}}-(1-\\alpha)c_{l})$, the optimal value of $\\alpha^{*}$ is characterized by $$\\mathrm{E}_{\\mu}\\big[\\mathrm{exp}\\big(-\\gamma\\Pi_{\\sigma}\\big(\\alpha^{*}\\big)\\big)(\\sigma-\\tilde{\\sigma})\\big]=0$$ if the solution is interior (otherwise a marginal change in $\\gamma$ has no effect).\nTaking the total differential yields $$\\frac{\\mathrm{d}\\alpha^{*}}{\\mathrm{d}\\gamma}=-\\frac{\\mathrm{E}_{\\mu}\\left[\\exp\\left(-\\gamma\\Pi_{\\sigma}\\left(\\alpha^{*}\\right)\\right)\\left(\\sigma-\\tilde{\\sigma}\\right)\\Pi_{\\sigma}\\left(\\alpha^{*}\\right)\\right]}{\\gamma\\mathrm{E}_{\\mu}\\left[\\exp\\left(-\\gamma\\Pi_{\\sigma}\\left(\\alpha^{*}\\right)\\right)\\left(\\sigma-\\tilde{\\sigma}\\right)^{2}\\right]};$$ $\\begin{array}{r}{\\frac{\\mathrm{d}\\alpha^{*}}{\\mathrm{d}\\gamma}\\leq0}\\end{array}$ if the numerator is weakly positive on the interval $[\\alpha^{\\mathrm{MEU}}$ \u0003 1].\n\nThis section explores the implications of constant absolute ambiguity aversion on the optimal mechanism, focusing on the properties of the utility function and the conditions under which the optimal value of α is determined."
  },
  {
    "qid": "econ-empirical-812-1-0-0",
    "question": "1) Derive the first-order conditions for the Lasso optimization problem: $${\\widehat{\\beta}}_{\\nu,L,{\\mathrm{Lasso}}}\\in\\arg\\operatorname*{min}_{b\\in\\mathbb{R}^{L}}\\sum_{i=1}^{n}(\\nu_{i}-q^{L}(z_{i})^{\\prime}b)^{2}+\\lambda\\sum_{j=1}^{L}|l_{j}b_{j}|.$$",
    "gold_answer": "The first-order conditions for the Lasso problem are given by the subdifferential of the objective function. For each $j$, the condition is: $$-2\\sum_{i=1}^{n}q_{j}^{L}(z_{i})(\\nu_{i}-q^{L}(z_{i})^{\\prime}\\widehat{\\beta}_{\\nu,L,\\mathrm{Lasso}})+\\lambda l_{j}\\partial|\\widehat{\\beta}_{\\nu,L,\\mathrm{Lasso},j}|=0,$$ where $\\partial|\\widehat{\\beta}_{\\nu,L,\\mathrm{Lasso},j}|$ is the subdifferential of the absolute value function at $\\widehat{\\beta}_{\\nu,L,\\mathrm{Lasso},j}$. This implies: $$\\widehat{\\beta}_{\\nu,L,\\mathrm{Lasso},j}=\\begin{cases} \\frac{\\sum_{i=1}^{n}q_{j}^{L}(z_{i})(\\nu_{i}-q^{L}(z_{i})^{\\prime}\\widehat{\\beta}_{\\nu,L,\\mathrm{Lasso}}^{(-j)})-\\frac{\\lambda l_{j}}{2}\\mathrm{sign}(\\widehat{\\beta}_{\\nu,L,\\mathrm{Lasso},j})}{\\sum_{i=1}^{n}q_{j}^{L}(z_{i})^{2}} & \\text{if } \\widehat{\\beta}_{\\nu,L,\\mathrm{Lasso},j}\\neq 0, \\\\ 0 & \\text{otherwise.} \\end{cases}$$",
    "question_context": "The Lasso estimate $\nu$ on $q^{L}(z)$ with penalty parameter $\\lambda$ and loadings $l_{j}$ is defined as a solution to the optimization problem: $${\\widehat{\\beta}}_{\\nu,L,{\\mathrm{Lasso}}}\\in\\arg\\operatorname*{min}_{b\\in\\mathbb{R}^{L}}\\sum_{i=1}^{n}(\\nu_{i}-q^{L}(z_{i})^{\\prime}b)^{2}+\\lambda\\sum_{j=1}^{L}|l_{j}b_{j}|.$$ The corresponding selected set $I_{\\nu,L}$ is defined as $$I_{\\nu,L}=\\{j:\\widehat{\\beta}_{\\nu,L,\\mathrm{Lasso},j}\\not=0\\}.$$ The Post-Lasso estimator is then defined by $${\\widehat\\beta}_{\\nu,L,{\\mathrm{Post-Lasso}}}\\in\\arg\\operatorname*{min}_{\\substack{b\\in\\mathbb{R}^{L}:b_{j}=0\\mathrm{~for~}j\\notin I_{\\nu,L}}}\\sum_{i=1}^{n}(\\nu_{i}-q^{L}(z_{i})^{\\prime}b)^{2}.$$\n\nThis section introduces a procedure for selecting $\tilde{\boldsymbol{q}}(z)$ under regularity conditions to yield asymptotically valid confidence intervals, leveraging Lasso-based model selection."
  },
  {
    "qid": "econ-empirical-108-22-1-0",
    "question": "5) Using the counterfactual results in Table VII, explain why disparities in fundamental shocks ($\\sigma_{\\mu}^{2}$) contribute to differences in the impact of informational frictions across countries.",
    "gold_answer": "1. **China (Case 2)**: Replacing $\\sigma_{\\mu}^{2}$ with the U.S. value reduces $\\mathbb{V}$ by 9% ($\\Delta\\mathbb{V} = -0.09$), leading to output gains of 2%.  \n2. **India (Case 2)**: $\\Delta\\mathbb{V} = -0.19$ (19% reduction), with output gains of 3%.  \n3. **Interpretation**: Countries with higher fundamental uncertainty (e.g., China, India) benefit more from reduced shock volatility, as it directly lowers $\\mathbb{V}$ and improves allocative efficiency.",
    "question_context": "We compute the change in V and associated aggregate gains in China and India under the assumption that the informativeness of financial markets—summarized by $\\sigma_{\\nu}^{2}\\sigma_{z}^{2}$—is equal to that in the United States.\nWe perform the same exercise for firm private information, assuming firms in China and India have the same $\\sigma_{e}^{2}$ as their U.S. counterparts.\nWe recompute V in China and India assuming firms face the same fundamental shocks as U.S. firms.\n\nThis section examines counterfactual experiments to assess the potential gains to China and India from improved information structures."
  },
  {
    "qid": "econ-empirical-384-1-1-1",
    "question": "4) Derive the CLT for $\\sqrt{N}(\\mathrm{ERV}_{threshold} - \\mathrm{IV})$ under Assumptions (B.ii)–(B.v) and finite jumps, explaining the role of $\\phi$ and $H_{t}$.",
    "gold_answer": "1. Under Assumptions (B.ii)–(B.v) and finite jumps, $\\sqrt{N}(\\mathrm{ERV}_{threshold} - \\mathrm{IV}) \\to \\phi \\times \\left(2\\int_{0}^{1}\\sigma_{t}^{4}d H_{t}\\right)^{1/2}$ stably.\\n2. $\\phi$ is a standard normal variable independent of $\\mathcal{F}_{1}$.\\n3. $H_{t}$ captures the limit of $N\\sum_{t_{k} \\leq t} \\varDelta t_{k}^{2}$, influencing the asymptotic variance.",
    "question_context": "Definition 1. Let $U_{n}$ be a sequence of $\\chi$ -measurable variables, ${\\mathcal{F}}_{1}\\subseteq\\chi$ . We say that $U_{n}$ converges ${\\mathcal{F}}_{1}$ -stably (or stably) in law to $U$ as $n\\to\\infty$ if $U$ is measurable with respect to an extension of χ , so that for all $A\\in{\\mathcal{F}}_{1}$ and for any bounded continuous function $\\i,E({\\bf1}_{A}h(U_{n}))\\rightarrow E({\\bf1}_{A}h(U))$ as $n\\rightarrow\\infty$ , where 1 stands for the indicator function.\n(i) if Assumption (B.i) holds, then as $n\\to\\infty$ , ${\\sqrt{N}}(\\mathrm{ERV-RV})=o_{p}(1);$ (ii) in particular, if Assumptions (B.ii)–(B.iv) hold and $J_{t}\\equiv0$ , then $a s n\\rightarrow\\infty$ , $\\sqrt{N}\\left(\\mathrm{ERV-IV}\\right)\\stackrel{\\mathcal{L}}{\\longrightarrow}\\phi\\times\\left(2\\int_{0}^{1}\\sigma_{t}^{4}d H_{t}\\right)^{1/2}s t a b l y,$ where $\\phi$ is $a$ standard normal random variable independent of ${\\mathcal{F}}_{1}$ ; (iii) more generally, if $\\left(J_{t}\\right)$ admits only finitely many jumps, then under Assumptions (B.ii)–(B.v), for any $\\xi\\in(0,1/2)$ , as $n\\rightarrow$ $\\infty$ , $\\sqrt{N}\\left(\\mathtt{E R V}_{t h r e s h o l d}-\\mathtt{I V}\\right)\\overset{\\mathcal{L}}{\\longrightarrow}\\varPhi\\times\\left(2\\int_{0}^{1}\\sigma_{t}^{4}d H_{t}\\right)^{1/2}s t a b l y.$\n\nThe text introduces the concept of stable convergence and derives Central Limit Theorems (CLTs) for the estimator $\\widehat{\\pmb{\\theta}}$ under various assumptions."
  },
  {
    "qid": "econ-empirical-1644-0-3-1",
    "question": "8) Propose a strategy to extend the current framework to higher-order GARCH models, highlighting potential challenges.",
    "gold_answer": "**Strategy**:\n1. **State-space expansion**: Represent a GARCH(p,q) model as a first-order Markov chain in $\\mathbb{R}^{p+q}$.\n2. **Lyapunov function**: Use $V(\\mathbf{x}) = 1 + \\|\\mathbf{x}\\|^k$ for $\\mathbf{x} = (X_t, \\dots, X_{t-p+1}, \\varepsilon_t, \\dots, \\varepsilon_{t-q+1})$.\n\n**Challenges**:\n- **Dimensionality**: Drift conditions become harder to verify in higher dimensions.\n- **Nonlinearity**: Complex feedback between lags may require stronger assumptions on $f^\\nu(\\cdot)$.",
    "question_context": "Proposition 1 (Section 2.2) corrects the ergodicity results of Carrasco and Chen (2002) for generalized hidden Markov models.\nExtension to higher-order GARCH and ACD models is noted as an open problem.\n\nThe paper corrects prior results on generalized hidden Markov models and suggests extensions to higher-order GARCH/ACD models."
  },
  {
    "qid": "econ-empirical-320-5-0-2",
    "question": "3) Using the data on AC ownership rates in Indonesia, propose a theoretical framework to explain why the effects of nighttime heat on decision-making are larger for districts with below-average AC ownership. Include a discussion of the protective role of AC.",
    "gold_answer": "1. Let \\( D \\) represent decision-making quality, \\( T \\) nighttime temperature, and \\( AC \\) AC ownership (binary or continuous).\n2. The interaction model: \\( D = \\beta_0 + \\beta_1 T + \\beta_2 AC + \\beta_3 (T \\times AC) + \\epsilon \\).\n3. \\( \\beta_1 \\) captures the effect of heat in low-AC areas; \\( \\beta_3 \\) measures how AC mitigates this.\n4. AC's protective role: By moderating indoor temperatures, AC reduces heat exposure, preserving cognitive function and sleep quality, thus improving decision-making.",
    "question_context": "Empirical evidence suggests that temperature can significantly influence our mood. For example, studies have documented that exposure to high temperatures can cause aggressive, even violent responses.\nMood can also influence decision-making. Aggression is positively associated with risk-taking behaviour, whilst a happy mood has been associated with significant reductions in impatience over money.\nWe explore the effects of temperature on mood in panel B of Table 7 and find that midnight temperatures before the survey have no significant effect on the likelihood of feeling anger, happiness or enthusiasm.\nGiven that Indonesia is a tropical archipelago, is there evidence of adaptation to persistently hotter temperatures? Thus far, our estimates suggest that any adaptation is likely limited by people’s socioeconomic status.\nWe estimate whether living in a district with high rates of residential AC penetration mitigates the effects of temperature on our main decision-making outcomes.\nUsing Indonesian data on choices in lottery games and climate records from NASA, we show that nighttime heat significantly increases the likelihood of making rational choice violations and impatient decisions.\n\nThe text explores the impact of temperature on mood and decision-making, with empirical evidence suggesting that high temperatures can lead to aggressive behavior and influence risk-taking. It also discusses behavioral adaptation to heat, particularly through air-conditioning (AC) ownership in Indonesia."
  },
  {
    "qid": "econ-empirical-514-3-0-0",
    "question": "1) Prove that under \\( \\mathrm{H}_0 \\), \\( \\mathbb{E}[Y^{\\prime} \\mid \\psi^{\\prime}] = \\psi^{\\prime} \\) using the law of iterated expectations.",
    "gold_answer": "1. Given \\( \\mathbb{E}(Y^{\\prime} \\mid \\mathcal{T}^{\\prime}) = \\psi^{\\prime} \\) and \\( \\sigma(\\psi^{\\prime}) \\subset \\mathcal{Z}^{\\prime} \\), apply the law of iterated expectations:\n   \\[ \n   \\mathbb{E}[Y^{\\prime} \\mid \\psi^{\\prime}] = \\mathbb{E}[\\mathbb{E}[Y^{\\prime} \\mid \\mathcal{Z}^{\\prime}] \\mid \\psi^{\\prime}] = \\mathbb{E}[\\psi^{\\prime} \\mid \\psi^{\\prime}] = \\psi^{\\prime}.\n   \\]",
    "question_context": "Under \\( \\mathrm{H}_0 \\), there exist \\( Y^{\\prime}, \\psi^{\\prime} \\) and \\( \\mathcal{T}^{\\prime} \\) such that \\( Y^{\\prime} \\sim Y \\), \\( \\psi^{\\prime} \\sim \\psi \\), \\( \\sigma(\\psi^{\\prime}) \\subset \\mathbb{Z}^{\\prime} \\) and \\( \\mathbb{E}(Y^{\\prime} \\mid \\mathcal{T}^{\\prime}) = \\psi^{\\prime} \\). Then, by the law of iterated expectations, \\( \\operatorname{\\mathbb{E}}\\left[Y^{\\prime} \\mid \\psi^{\\prime}\right] = \\operatorname{\\mathbb{E}}\\left[\\operatorname{\\mathbb{E}}\\left[Y^{\\prime} \\mid \\mathcal{Z}^{\\prime}\right] \\mid \\psi^{\\prime}\right] = \\operatorname{\\mathbb{E}}\\left[\\psi^{\\prime} \\mid \\psi^{\\prime}\right] = \\psi^{\\prime} \\). Conversely, if there exists \\( (Y^{\\prime}, \\psi^{\\prime}) \\) such that \\( Y^{\\prime} \\sim Y \\), \\( \\psi^{\\prime} \\sim \\psi \\) and \\( \\mathbb{E}[Y^{\\prime} \\mid \\psi^{\\prime}] = \\psi^{\\prime} \\), let \\( \\mathcal{T}^{\\prime} = \\sigma(\\psi^{\\prime}) \\). Then \\( \\psi^{\\prime} = \\mathbb{E}[Y^{\\prime} \\mid \\psi^{\\prime}] = \\mathbb{E}[Y^{\\prime} \\mid \\mathcal{T}^{\\prime}] \\) and \\( \\mathrm{H}_0 \\) holds.\n\nThis section establishes the equivalence of different conditions under the null hypothesis \\( \\mathrm{H}_0 \\), involving random variables \\( Y \\), \\( \\psi \\), and sigma-algebras."
  },
  {
    "qid": "econ-empirical-53-4-0-0",
    "question": "1) Derive the equilibrium condition for the discount factor δ when $k^{A}<k^{B}\\leq k^{c}$ and explain the economic intuition behind the condition.",
    "gold_answer": "1. Start with the given price ratios:\n   $$\\frac{\\delta^{3r}p_{3r}(A_{1},A_{2})}{p_{3r+1}(B_{1},A_{2})}=\\frac{2(k^{B}b_{1}+b_{2})}{k^{B}}$$\n   $$\\frac{\\delta^{3r+1}p_{3r+1}(B_{1},A_{2})}{p_{3r+2}(C_{1},A_{2})}=\\frac{2\\delta(k^{c}c_{1}+c_{2})}{k^{c}}$$\n   $$\\frac{\\delta^{3r+2}p_{3r+2}(C_{1},A_{2})}{p_{3r+3}(A_{1},A_{2})}=\\frac{2\\delta^{2}(k^{A}a_{1}+a_{2})}{k^{A}}$$\n2. Combine these ratios to form the equilibrium condition:\n   $$\\frac{\\delta^{3T^{*}}k^{A}}{2}\\left[\\frac{2(k^{A}a_{1}+a_{1})}{k^{A}}\\right]^{T^{*}}=\\delta^{(T+2)(T-1)/2}$$\n3. The economic intuition is that the discount factor δ must balance the intertemporal trade-offs between consuming now versus later, ensuring that traders are indifferent between consuming in different periods given the quality differences in goods.",
    "question_context": "Let us first introduce a discount factor δ common to all traders. If we continue to assume that $k^{A}<k^{B}\\leq k^{c}$ then,by analogy with the proofs of Lemmas 1 and 2, we can show that\n\n$$\\frac{\\delta^{3r}p_{3r}(A_{1},A_{2})}{p_{3r+1}(B_{1},A_{2})}=\\frac{2(k^{B}b_{1}+b_{2})}{k^{B}},\\qquad1\\leq3r\\leq T-1$$\n\n$$\\frac{\\delta^{3r+1}p_{3r+1}(B_{1},A_{2})}{p_{3r+2}(C_{1},A_{2})}=\\frac{2\\delta(k^{c}c_{1}+c_{2})}{k^{c}},\\qquad1\\leq3r+1\\leq T-1$$\n\nand\n\n$$\\frac{\\delta^{3r+2}p_{3r+2}(C_{1},A_{2})}{p_{3r+3}(A_{1},A_{2})}=\\frac{2\\delta^{2}(k^{A}a_{1}+a_{2})}{k^{A}},\\quad1\\leq3r+2\\leq T-1,$$\n\nand so if the analogue of Proposition 1 holds,\n\n$$\\frac{\\delta^{3T^{*}}k^{A}}{2}\\left[\\frac{2(k^{A}a_{1}+a_{1})}{k^{A}}\\right]^{T^{*}}=\\delta^{(T+2)(T-1)/2}.$$\n\nThe analysis examines the implications of introducing a discount factor and varying durability of goods in a trading model."
  },
  {
    "qid": "econ-empirical-1199-1-1-0",
    "question": "5) Interpret the coefficients \\(\\mu_1\\), \\(\\mu_2\\), and \\(\\mu_3\\) in the multinomial logit model for residential mobility.",
    "gold_answer": "1. \\(\\mu_1\\): Baseline difference in log-odds of migration for Arabs/Muslims vs. the comparison group.  \n2. \\(\\mu_2\\): Change in log-odds of migration post-September 11th for the comparison group.  \n3. \\(\\mu_3\\): DD effect of September 11th on Arabs/Muslims' migration, capturing the additional change relative to the comparison group.",
    "question_context": "To investigate whether September 11th affected the location choices of Arabs and Muslims, we examined the effect of September 11th on the following changes in residence: moved to a different state; moved within the same state; nonmovers.\nThe estimation equation in this case is: $$\\ln\\left[\\frac{p_{i c t}}{p_{i0t}}\\right]=\\upmu_{0}+\\upmu_{1}T r_{i s t}+\\upmu_{2}S e p t_{t}+\\upmu_{3}\\left(T r_{i s t}*S e p t_{t}\\right)+X_{i j t}\\Gamma+\\upvarphi Z_{s t-1}+\\chi_{s}+u_{i s j t}$$\n\nThe study investigates whether September 11th affected the location choices of Arabs and Muslims, using a multinomial logit model to analyze migration patterns."
  },
  {
    "qid": "econ-empirical-1660-2-0-3",
    "question": "4) Derive the expected change in the number of early gifts due to the matching scheme, assuming a crowding-out effect of third-party transfers on charitable giving. Use the data from the campaign to support your answer.",
    "gold_answer": "1. The crowding-out effect suggests that third-party transfers (e.g., matching) reduce out-of-pocket gifts.\n2. In the campaign, early gifts with the match were lower in both frames (43 in donation, 36 in contribution), consistent with crowding-out.\n3. However, the match stimulated more additional gifts in the donation frame, indicating a nuanced effect of framing on matching incentives.",
    "question_context": "The use of the word ‘donation’ rather than ‘contribution’ resulted in a slightly higher response rate (non-significant), much higher average positive monetary gifts (borderline significant at p<0.1), and a much higher overall monetary return (significant at p<0.05).\nIn Table 3, Column I, we test Hypothesis 1 in an OLS regression. We regressed unconditional amounts given on the donation treatment dummy, controlling for block fixed effects and basic characteristics. Panel A accounts for monetary gifts only, while Panel B includes buffet pledges monetized at a value of €10 each.\nThe distance to the suggested amount is almost 40% larger in the donation frame. There is also more variance in gift amounts in general in the donation than in the contribution frame (Columns III and VI, significant difference according to the variance-comparison test).\nA match of €5 by an anonymous donor was offered for all gifts made before a prespecified deadline. Although the match increased the gift received, it was not counted against the reward that donors received from contributing a certain amount.\n\nThe campaign achieved a total of 130 gifts (monetary, buffet, or both), with a response rate of 24%. The average monetary gift was €12 and the median €10. The campaign surpassed the monetary threshold of €2,000, with a final sum of €2,241. The surplus was donated to a refugee program."
  },
  {
    "qid": "econ-empirical-729-2-0-2",
    "question": "3) Critically evaluate the potential biases introduced by retrospective employment data and missing weather station observations.",
    "gold_answer": "1. **Retrospective Bias**:\n   - Recall inaccuracies may underreport unemployment or misclassify sectors.\n   - Salient events (e.g., extreme weather) may be overreported.\n2. **Weather Data Bias**:\n   - Missing station data (75% coverage rule) may omit local microclimates.\n   - Precipitation predictions ($R^2 = 0.91$) are less reliable than temperature.",
    "question_context": "The data on rural Mexican employment come from the Mexico National Rural Household Survey (Encuesta Nacional a Hogares Rurales de Mexico – ENHRUM), a nationally representative survey of 1,762 households in 80 rural communities spanning Mexico’s five census regions.\nDaily weather data from 1,437 weather stations were obtained from the Mexican National Water Commission. The data include daily maximum and minimum temperatures and total precipitation between 1980 and 2007.\nWe construct daily temperatures as the average of daily minimum and maximum temperature. Then, based on maize production in the US, we use the following formula to convert daily temperatures into growing degree days (GDD): $$\\operatorname{GDD}(T)=\\left\\{\\begin{array}{r l}{0}&{\\qquad\\mathrm{if}\\quad T\\leq8C}\\\\{T-8}&{\\qquad\\mathrm{if}\\quad8C<T\\leq32C.}\\\\{24}&{\\qquad\\mathrm{if}\\quad T\\geq32}\\end{array}\\right.$$\nWe construct a measure of harmful degree days (HDDs), which incorporates the possibility that temperatures above a given threshold may be harmful. For a day at temperature $T$: $$\\mathrm{HDD}(T)=T-32\\quad\\mathrm{if}\\quad T\\geq32C.$$\n\nThe study combines annual labour allocation data from household surveys with daily weather station data from rural Mexico, focusing on employment choices and weather impacts."
  },
  {
    "qid": "econ-empirical-998-0-0-2",
    "question": "3) What are the implications of the regularity conditions provided in this article for extending Cox's test to time-series or regression applications?",
    "gold_answer": "1. **Time-Series:** The regularity conditions ensure that the asymptotic normality holds even with dependent observations, as in Walker (1967).\n2. **Regression:** The conditions validate the use of Cox's test in regression models, such as those in Pesaran and Deaton (1978), by ensuring the score function and information matrix behave appropriately.\n3. **Generalization:** The results allow for non-i.i.d. data, provided the likelihood functions satisfy the regularity conditions.",
    "question_context": "Often it is desired to test a composite null hypothesis against a composite alternative that is not in the same parametric family as the null hypothesis. For example, suppose we have independent, identically distributed (i.i.d.) observations $U_{1},...,U_{n}$ and we wish to test the hypothesis that these observations arise from a normal distribution against the alternative that they rise from a logistic distribution.\nCox proposes that we maximize the likelihood function under both the null and alternative hypotheses, and from these, form a log-likelihood ratio. The value of this log-likelihood ratio is then compared to the value expected when the null hypothesis is true. Small deviations from the expected value are evidence in favor of the null hypothesis, while large deviations are evidence against.\nCox provides a statistic which is heuristically argued to be distributed asymptotically as unit normal. However, Cox (1961, p. 105; 1962, p. 408) explicitly eschews attempting to provide regularity conditions which ensure the general validity of his procedure.\nIn this article, we rectify this omission. The results are useful since they provide a straightforward way to establish the validity of Cox's test in many particular contexts.\n\nThis section discusses the asymptotic normality of Cox's statistic for testing separate families of hypotheses, with a focus on the Cox test for choosing between competing linear regression models. It highlights the lack of general regularity conditions and rigorous proof for the asymptotic normality of Cox's statistic, which this article aims to provide."
  },
  {
    "qid": "econ-empirical-1188-0-1-3",
    "question": "4) What are the implications of nonclassical measurement error for policy analysis?",
    "gold_answer": "The implications include:\n- Increased uncertainty in policy conclusions due to potential misreporting.\n- The need for robust methods that account for measurement errors to avoid biased inferences.\n- Challenges in identifying true program impacts when key variables are subject to misclassification.",
    "question_context": "We extend the econometric literature on misclassified binary variables by studying identification when an outcome (in our case, food insecurity) and a conditioning variable (food stamp participation) are both subject to arbitrary endogenous classification error.\nWe propose a computationally efficient estimation algorithm for the 'orthogonal errors' case where misreported food stamp participation status might arise independently of true participation status.\nEven random errors can lead to seriously biased parameter estimates, as is well understood in the econometrics literature.\n\nThe paper extends the econometric literature on misclassified binary variables, focusing on identification when both the outcome (food insecurity) and the conditioning variable (food stamp participation) are subject to arbitrary endogenous classification error."
  },
  {
    "qid": "econ-empirical-586-4-0-3",
    "question": "4) Horowitz's 'Semiparametric Estimation of a Proportional Hazard Model with Unobserved Heterogeneity' involves estimating a hazard model with unobserved heterogeneity. Derive the likelihood function for this model, assuming a gamma-distributed heterogeneity term.",
    "gold_answer": "1. **Model Setup**: The hazard function is \\( \\lambda(t|x, v) = v \\lambda_0(t) \\exp(x'\\beta) \\), where \\( v \\) is gamma-distributed with mean 1 and variance \\( \\theta \\).\n2. **Likelihood Contribution**: For each individual, the likelihood contribution is:\n   \\[ L_i = \\int_0^\\infty \\left[ \\lambda(t_i|x_i, v) \\right]^{d_i} \\exp \\left( - \\int_0^{t_i} \\lambda(s|x_i, v) ds \\right) f(v) dv \\]\n   where \\( d_i \\) is the failure indicator.\n3. **Integration**: Using the gamma distribution for \\( v \\), the integrated likelihood is:\n   \\[ L_i = \\left( \\frac{\\theta}{\\theta + \\Lambda_0(t_i) \\exp(x_i'\\beta)} \\right)^{1/\\theta} \\left( \\frac{\\lambda_0(t_i) \\exp(x_i'\\beta)}{\\theta + \\Lambda_0(t_i) \\exp(x_i'\\beta)} \\right)^{d_i} \\]\n   where \\( \\Lambda_0(t_i) = \\int_0^{t_i} \\lambda_0(s) ds \\).",
    "question_context": "ABREU, DILIP, AND FARUK GUL: 'Bargaining and Reputation.' (Dept. of Economics, Princeton University, Princeton, NJ 08544-1021.)\nANDREWS, DONALD W. K.: 'Inconsistency of the Bootstrap when a Parameter Is on the Boundary of the Parameter Space.' (Cowles Foundation for Research in Economics, Dept. of Economics, Yale University, P.O. Box 208281, New Haven, CT 06520-8281.)\nANDREWS, DONALD W. K., AND MOSHE BUCHINSKY: 'A Three-Step Method for Choosing the Number of Bootstrap Repetitions.' (Cowles Foundation for Research in Economics, Dept. of Economics, Yale University, P.O. Box 208281, New Haven, CT 06520-8281.)\nBLUNDELL, RICHARD, AND JEAN-MARC ROBIN: 'Latent Separability: Grouping Goods without Weak Separability.' (University College London, Dept. of Economics, Gower Street, London WC1E 6BT, England.)\nECKSTEIN, ZVI, AND KENNETH I. WOLPIN: 'Why Youths Drop Out of High School: The Impact of Preferences, Opportunities, and Abilities.' (Dept. of Economics, University of Pennsylvania, 521 McNeil Bldg., Philadelphia, PA 19104-6297.)\nGRANT, SIMON, ATSUSHI KAJII, AND BEN POLAK: 'Temporal Resolution of Uncertainty and Recursive Non-Expected Utility Models.' (Dept. of Economics, Yale University, P.O. Box 208268, New Haven, CT 06520-8268.)\nHENDRICKS, KEN, MICHELE PICCIONE, AND GUOFU TAN: 'Equilibria in Networks.' (Dept. of Economics, Princeton University, Princeton, NJ 08544.)\nHILLIER, GRANT, AND MARK ARMSTRONG: 'The Density Of the Maximum Likelihood Estimator.' (Dept. of Economics, University of Southampton, Highfield, Southampton S09 5NH, Great Britain.)\nHOROWITZ, JOEL L.: 'Semiparametric Estimation of a Proportional Hazard Model with Unobserved Heterogeneity.' (Dept. of Economics, University of Iowa, 108 Pappajohn Bus. Admin. Bldg., Iowa City, IA 52242-1000.)\nLAFFONT, JEAN-JACQUES, AND DAVID MARTIMORT: 'Mechanism Design with Collusion and Correlation.' (Universite des Sciences Sociales, GREMAQ and IDEI, Université des Sciences Sociales, Place Anatole France, F-31042 Toulous Cedex, France.)\nRAUCH, BERNHARD: 'A Divisible Search Model of Fiat Money: A Comment.' (Institut fur Volkswirtschaftslehre, Universitat Regensburg, D-93040 Regensburg, Germany.)\nSMITH, LONES, AND PETER SORENSEN: 'Pathological Outcomes of Observational Learning.' (Dept. of Economics, University of Michigan, Ann Arbor, MI 48109-1220.)\nTIROLE, JEAN: 'Incomplete Contracts: Where Do We Stand?' (IEI, Université des Sciences Sociales de Toulouse, Place Anatole-France, F-31042 Toulouse Cedex, France.)\n\nThe following manuscripts have been accepted for publication in forthcoming issues of Econometrica, covering a range of topics in economics and econometrics."
  },
  {
    "qid": "econ-empirical-215-5-0-1",
    "question": "2) Prove Lemma A.1, which states $\\sqrt{m_{n}}\\lesssim\\mathsf{s}_{m_{n}}$ under Assumption 2(ii).",
    "gold_answer": "1. Assume $\\int\\varpi(t)dt = 1$ without loss of generality.\n2. Start with the definition of $\\mathsf{S}_{m_{n}}^{2}$:\n   $$\\mathsf{S}_{m_{n}}^{2} = \\int\\int\\left\\|P_{n}^{-1/2}E\\big[\\delta(V,s)\\overline{{\\delta(V,t)}}p_{\\underline{{m_{n}}}}(X)p_{\\underline{{m_{n}}}}(X)^{*}\\big]P_{n}^{-1/2}\\right\\|_{F}^{2}\\varpi(s)\\varpi(t)ds dt.$$\n3. Lower bound the Frobenius norm by the sum of squared diagonal elements:\n   $$\\mathsf{S}_{m_{n}}^{2} \\geq \\lambda_{n}^{-2}\\sum_{l=1}^{m_{n}}\\int\\int\\left|E\\big[\\delta(V,s)\\overline{{\\delta(V,t)}}p_{l}^{2}(X)\\big]\\right|^{2}\\varpi(s)\\varpi(t)ds dt.$$\n4. Apply Jensen's inequality to the inner expectation:\n   $$\\left|E\\big[\\delta(V,s)\\overline{{\\delta(V,t)}}p_{l}^{2}(X)\\big]\\right|^{2} \\geq \\left(E\\bigg[\\bigg|\\int\\delta(V,t)\\varpi(t)dt\\bigg|^{2}p_{l}^{2}(X)\\bigg]\\right)^{2}.$$\n5. Use Assumption 3 to bound the expectation:\n   $$E\\bigg[\\bigg|\\int\\delta(V,t)\\varpi(t)dt\\bigg|^{2}p_{l}^{2}(X)\\bigg] \\geq C E\\big[p_{l}^{2}(X)\\big].$$\n6. Combine the results:\n   $$\\mathsf{S}_{m_{n}}^{2} \\geq C\\lambda_{n}^{-2}\\sum_{l=1}^{m_{n}}(E\\big[p_{l}^{2}(X)\\big])^{2} \\geq C m_{n}.$$\n7. Take square roots to conclude $\\sqrt{m_{n}} \\lesssim \\mathsf{s}_{m_{n}}$.",
    "question_context": "Throughout the proofs, we will use $C>0$ to denote a generic finite constant that may be different in different uses. We use the notation $a_{n}\\lesssim b_{n}$ to denote $a_{n}\\leq C b_{n}$ for all $n\\geq1$ . Further, for ease of notation we write $\\sum j$ for $\\scriptstyle\\sum_{j=1}^{n}$ . Recall that $\\|\\cdot\\|$ denotes the usual Euclidean norm, while for a matrix $A$ , $\\|A\\|$ is the operator norm. Further, $\\|\\phi\\|_{X}\\equiv$ $\\sqrt{E(\\phi^{2}(X))}$ and $\\langle\\phi,\\psi\\rangle_{X}\\equiv E[\\phi(X)\\psi(X)]$ . For any integer $m\\geq1,\\mathbb{Z}_{n}$ denotes the $m_{n}\\times m_{n}$ dimensional identity matrix. Recall the notation $P_{n}{=}E[p_{\\underline{{m}}_{n}}(X)p_{\\underline{{m}}_{n}}(X)^{\\prime}]$ .\nProof of Proposition 1. Let us denote $\\begin{array}{r}{f=\\frac{d F_{X}}{d\\nu}}\\end{array}$ . For some constant $0<c<1$ , for all $n\\geq1$ , and any $a_{n}\\in\\mathbb{R}^{m_{n}}$ we have $$\\begin{array}{l}{\\displaystyle\\|a_{n}\\|^{2}=\\int\\big(a_{n}^{\\prime}p_{\\frac{m_{n}}{m_{n}}}(x)\\big)^{2}1\\big\\{f(x)\\geq\\lambda_{n}\\big\\}\\nu(d x)}\\ {\\displaystyle\\qquad+\\int\\big(a_{n}^{\\prime}p_{\\frac{m_{n}}{m_{n}}}(x)\\big)^{2}1\\big\\{f(x)<\\lambda_{n}\\big\\}\\nu(d x)}\\ {\\displaystyle\\qquad\\leq\\lambda_{n}^{-1}\\int\\big(a_{n}^{\\prime}p_{\\frac{m_{n}}{m_{n}}}(x)\\big)^{2}f(x)\\nu(d x)}\\ {\\displaystyle\\qquad+c\\int\\big(a_{n}^{\\prime}p_{\\frac{m_{n}}{m_{n}}}(x)\\big)^{2}\\nu(d x).}\\end{array}$$ Consequently, we obtain $\\lambda_{n}\\mathcal{I}_{n}\\lesssim P_{n}$ .\nLemma A.1. Under Assumption 2(ii), it holds $\\sqrt{m_{n}}\\lesssim\\mathsf{s}_{m_{n}}$ . Proof. Without loss of generality, it may be assumed that $\\textstyle\\int\\varpi(t)d t=1$ . By the definition of $\\mathsf{S}_{m_{n}}$ , we conclude $$\\begin{array}{l}{\\displaystyle\\sum_{m_{n}}^{2}=\\int\\int\\left\\|P_{n}^{-1/2}E\\big[\\delta(V,s)\\overline{{\\delta(V,t)}}p_{\\underline{{m_{n}}}}(X)p_{\\underline{{m_{n}}}}(X)^{*}\\big]P_{n}^{-1/2}\\right\\|_{F}^{2}\\varpi(s)\\varpi(t)d s d t}\\ {\\displaystyle\\geq\\lambda_{n}^{-2}\\sum_{l=1}^{m_{n}}\\int\\left|E\\big[\\delta(V,s)\\overline{{\\delta(V,t)}}p_{l}^{2}(X)\\big]\\right|^{2}\\varpi(s)\\varpi(t)d s d t}\\ {\\displaystyle\\geq\\lambda_{n}^{-2}\\sum_{l=1}^{m_{n}}\\left(E\\bigg[\\bigg|\\int\\delta(V,t)\\varpi(t)d t\\bigg|^{2}p_{l}^{2}(X)\\bigg]\\right)^{2}(\\mathrm{by~Jensens~in~equality})}\\ {\\displaystyle\\geq C\\Lambda_{n}^{-2}\\sum_{l=1}^{m_{n}}(E\\big[p_{l}^{2}(X)\\big])^{2}\\quad\\mathrm{(by~Assumption~3)}}\\ {\\displaystyle=C m_{n}.}\\end{array}$$\nProof of Theorem 2.1. We make use of the decomposition $$\\begin{array}{r l}{n S_{n}=\\displaystyle\\sum_{j}\\int\\left|\\hat{\\boldsymbol{\\varepsilon}}_{n}(\\boldsymbol{X}_{j},t)\\right|^{2}\\varpi(t)d t}\\ {}&{~=\\displaystyle\\sum_{j}\\int\\left|p_{\\underline{{m_{n}}}}(\\boldsymbol{X}_{j})^{\\prime}\\hat{\\gamma}_{n}(t)-\\boldsymbol{I}_{m_{n}}\\varphi(\\boldsymbol{X}_{j},t)\\right|^{2}\\varpi(t)d t}\\ {}&{~+2\\displaystyle\\sum_{j}\\mathrm{Re}\\int\\left(p_{\\underline{{m_{n}}}}(\\boldsymbol{X}_{j})^{\\prime}\\hat{\\gamma}_{n}(t)-\\boldsymbol{I}_{m_{n}}\\varphi(\\boldsymbol{X}_{j},t)\\right)\\left(\\boldsymbol{I}_{m_{n}}\\varphi(\\boldsymbol{X}_{j},-t)\\right.}\\ {}&{~-\\left.(\\mathcal{F}_{g}\\hat{f}_{B n})(\\boldsymbol{X}_{j},-t)\\right)\\varpi(t)d t}\\ {}&{~+\\displaystyle\\sum_{j}\\int\\left|\\boldsymbol{I}_{m_{n}}\\varphi(\\boldsymbol{X}_{j},t)-(\\mathcal{F}_{g}\\hat{f}_{B n})(\\boldsymbol{X}_{j},t)\\right|^{2}\\varpi(t)d t}\\end{array}$$\n\nThe appendix provides detailed proofs and mathematical derivations supporting the main theoretical results of the paper. It includes notation, assumptions, and key lemmas used throughout the proofs."
  },
  {
    "qid": "econ-empirical-582-3-0-0",
    "question": "1) Derive the conditional density \\( f(Y_{i};X,\beta,\\Sigma,\\alpha_{i}) \\) and explain the role of the sufficient statistic \\( \\hat{\\alpha}_{i} \\) in the conditional likelihood approach.",
    "gold_answer": "1. The conditional density is given by:\n   \\[\n   f(Y_{i};X,\beta,\\Sigma,\\alpha_{i}) = (2\\pi)^{-\\frac{1}{2}}|\\Sigma| \\times \\exp\\left(-\\frac{1}{2}(Y_{i}-X_{i}\\beta-I_{T}\\alpha_{i})'\\Sigma^{-1}(Y_{i}-X_{i}\\beta-I_{T}\\alpha_{i})\\right).\n   \\]\n2. The sufficient statistic \\( \\hat{\\alpha}_{i} \\) is derived as:\n   \\[\n   \\hat{\\alpha}_{i} = (1_{T}'\\Sigma^{-1}I_{T})^{-1}I_{T}\\Sigma^{-1}(Y_{i}-X_{i}\\beta).\n   \\]\n3. \\( \\hat{\\alpha}_{i} \\) is a weighted average of \\( Y_{i}-X_{i}\\beta \\) and serves to eliminate the incidental parameters \\( \\alpha_{i} \\) from the likelihood function, focusing inference on \\( \\beta \\) and \\( \\Sigma \\).",
    "question_context": "Consider the density of the ith observation, \\( f(Y_{i};X,\beta,\\Sigma,\\alpha_{i})=(2\\pi)^{-\\frac{1}{2}}\big|\\Sigma\big| \times\\exp{-\\frac{1}{2}(Y_{i}-X_{i}\beta-I_{T}\\alpha_{i})^{\\prime}}\\Sigma^{-1}(Y_{i}-X_{i}\beta-I_{T}\\alpha_{i}). \\)\nA natural statistic for \\( \\alpha_{i} \\) is \\( \\hat{\\alpha}_{i}=(1_{T}^{\\prime}\\Sigma^{-1}I_{T})^{-1}I_{T}\\Sigma^{-1}(Y_{i}-X_{i}\beta), \\) a weighted average of the elements of \\( Y_{i}-X_{i}\beta \\).\nThe joint distribution of \\( Y_{i} \\) and \\( \\hat{\\alpha}_{i} \\) is a singular normal distribution with mean and covariance \\( E{\binom{Y_{i}}{\\hat{\\alpha}_{i}}}={\binom{X_{i}\beta+I_{T}\\alpha_{i}}{\\alpha_{i}}}, \\qquad V{\binom{Y_{i}}{\\hat{\\alpha}_{i}}}={\\left[\begin{array}{l l}{\\Sigma}&{c l_{T}}\\ {c l_{T}^{\\prime}}&{c}\\end{array}\right]}, \\) where \\( c=({l}_{T}^{\\prime}{\\Sigma}^{-1}{l}_{T})^{-1}. \\)\nThe distribution of \\( Y_{i} \\) conditional on \\( \\hat{\\alpha}_{i} \\) has mean \\( E(Y_{i}\big|\\hat{\\alpha}_{i})=X_{i}\beta+I_{T}\\hat{\\alpha}_{i} \\) and variance \\( V(Y_{i}\big|\\hat{\\alpha}_{i})=\\Sigma_{c}=\\Sigma-c I_{T}{\\cal I}_{T}^{\\prime}. \\)\nThe transformation \\( M_{1}Y_{i}=\\widetilde{Y}_{i} \\) results in a distribution with mean \\( E(\\widetilde{Y}\big|\\hat{\\alpha}_{i})=M_{1}X_{i}\beta \\) and variance \\( V(\\widetilde{Y}_{i}\big|\\hat{\\alpha}_{i})=\\widetilde{\\cal{\tilde{\\Sigma}}}=M_{1}\\Sigma M_{1}. \\)\nThe nonlinear regression model considered is \\( y_{i t}=g(x_{i t},\theta)+\\alpha_{i}+\\varepsilon_{i t}, \\) which can be stacked over time for individual \\( i \\) to give \\( y_{i}=g(x_{i},\theta)+I_{T}\\alpha_{i}+\\varepsilon_{i}, \\) and stacked over individuals to \\( y=g(x,\theta)+Z\\alpha+\\varepsilon. \\)\nThe generalized least squares estimator minimizes \\( \\mathcal{Q}_{1}(\theta,z)=(y-g(x,\theta)-Z x)^{\\prime}(I\\otimes\\Sigma^{-1})(y-g(x,\theta)-Z x), \\) with respect to \\( \theta \\) and \\( \\propto \\) for \\( \\Sigma \\) known.\n\nThe conditional likelihood approach to estimation focuses attention on the density of the observables \\( Y_{i} \\) conditional on sufficient statistics for the incidental parameters \\( \\alpha_{i} \\). The text discusses the joint distribution of \\( Y_{i} \\) and \\( \\hat{\\alpha}_{i} \\), the conditional distribution of \\( Y_{i} \\) given \\( \\hat{\\alpha}_{i} \\), and the transformation to \\( \\widetilde{Y}_{i} \\) to handle singularity in the covariance matrix. It also extends the discussion to nonlinear regression models with additive effects."
  },
  {
    "qid": "econ-empirical-372-2-0-0",
    "question": "1) Derive the asymptotic power function $\\Pi_{T}(\\eta)$ for the $t$-test under the alternative hypothesis $H_{1}:\\eta>0$, where $\\eta=1/\\nu$ and $\\nu$ is the degrees of freedom of the Student-t distribution.",
    "gold_answer": "1. Start with the definition of the test statistic under the alternative: $\\sqrt{T}|\\hat{p}_{i}^{k}-p_{i}(\\eta)|$.  \n2. The rejection region is defined as $\\sqrt{T}|\\hat{p}_{i}^{k}-p_{i}(\\eta)| \\geq \\sigma_{k,i}(0)z_{1-\\alpha/2} - \\sqrt{T}|p_{i}(\\eta)-p_{i}(0)|$.  \n3. The probability of rejection under $H_1$ is:  \n   $$\\Pi_{T}(\\eta) = P\\left[\\sqrt{T}|\\hat{p}_{i}^{k}-p_{i}(\\eta)| \\geq \\sigma_{k,i}(0)z_{1-\\alpha/2} - \\sqrt{T}|p_{i}(\\eta)-p_{i}(0)|\\right].$$  \n4. As $T \\to \\infty$, the expression converges to:  \n   $$1 - \\Phi\\left(\\frac{\\sigma_{k,i}(0)z_{1-\\alpha/2} - \\sqrt{T}|p_{i}(\\eta)-p_{i}(0)|}{\\sigma_{k,i}(\\eta)}\\right) + \\Phi\\left(\\frac{\\sigma_{k,i}(0)z_{\\alpha/2} - \\sqrt{T}|p_{i}(\\eta)-p_{i}(0)|}{\\sigma_{k,i}(\\eta)}\\right).$$",
    "question_context": "1. $y_{t}=\\mu+\\sigma\\varepsilon_{t}$ , $\\varepsilon_{t}=\\phi\\varepsilon_{t-1}+u_{t}$ where $u_{t}\\sim$ iid ${\\bf N}(0,1-\\phi^{2})$ . 2. $y_{t}=\\mu+\\sigma\\varepsilon_{t}\\sqrt{(\\nu-2)/\\nu}$ , where $\\varepsilon_{t}\\sim$ iid Student- $t(\\nu)$ . 3. $y_{t}=\\mu+\\sigma\\varepsilon_{t}$ , $\\varepsilon_{t}=\\sqrt{h_{t}}u_{t}$ where $u_{t}\\sim$ iid ${\\mathrm{N}}(0,1)$ , $h_{t}=$ $\\omega+\\alpha\\varepsilon_{t-1}^{2}+\\beta h_{t-1}$ .\nThe asymptotic power of the $t$ -test, say $\\Pi_{T}(\\eta)$ , for a fixed contour $i$ and a fixed lag $k$ at the $\\alpha$ -significance level is given by: $$\\begin{array}{r l}&{\\Pi_{T}(\\eta)=P\\big[\\sqrt{T}|\\hat{p}_{i}^{k}-p_{i}(\\eta)|}\\ &{\\qquad\\geq\\sigma_{k,i}(0)z_{1-\\alpha/2}-\\sqrt{T}|p_{i}(\\eta)-p_{i}(0)|\\big]}\\ &{\\qquad\\to1-\\Phi\\bigg(\\frac{\\sigma_{k,i}(0)z_{1-\\alpha/2}-\\sqrt{T}|p_{i}(\\eta)-p_{i}(0)|}{\\sigma_{k,i}(\\eta)}\\bigg)}\\ &{\\qquad+\\Phi\\bigg(\\frac{\\sigma_{k,i}(0)z_{\\alpha/2}-\\sqrt{T}|p_{i}(\\eta)-p_{i}(0)|}{\\sigma_{k,i}(\\eta)}\\bigg).}\\end{array}$$\n\nThe section analyzes the power properties of test statistics under three alternative data generating processes (DGPs) with standardized residuals from location-scale models. The null hypothesis is $\\varepsilon_{t}\\sim$ iid ${\\mathrm{N}}(0,1)$ for all cases. The DGPs include autoregressive (AR), independent and identically distributed (iid) Student-t, and generalized autoregressive conditional heteroscedasticity (GARCH)(1,1) models."
  },
  {
    "qid": "econ-empirical-1494-3-0-1",
    "question": "2) Using Table 7, explain why the coefficients on US dollar and non-US dollar vehicle currency exchange rates are not statistically different. What does this imply about the VCP hypothesis?",
    "gold_answer": "From Table 7:\n1. **Coefficients**: For US dollar ($0.488$) and non-US dollar ($0.590$) vehicle currencies, the difference is insignificant (standard errors: $0.106$ and $0.061$, respectively).\n2. **Implications**:\n   - Pass-through is driven by vehicle currency usage generally, not the US dollar specifically.\n   - Supports VCP’s claim that non-US dollar vehicle currencies (e.g., euro) also matter.\n   - Rejects DCP’s exclusive focus on the US dollar as the dominant currency.",
    "question_context": "In contrast to DCP, VCP distinguishes between the US dollar as a producer currency and a vehicle currency. Moreover, it considers other vehicle currencies than the US dollar in explaining pass-through.\n$88.5\\%$ of vehicle currency transactions in our sample are invoiced in US dollars, $10.9\\%$ in euros, and $0.6\\%$ in 89 other currencies.\nPass-through is the same whether the vehicle currency is the US dollar or another currency. It is therefore the use of vehicle currencies generally, and not the US dollar specifically, that is driving the high pass-through for vehicle currency transactions.\nThe dependent variable is the quarterly log change import unit value (in sterling per kilogram).\n$$\\Delta\\ln U V_{i j k,t}=\\sum_{n=0}^{N}\\Psi_{n}\\Delta\\ln e_{i j,t-n}+\\sum_{n=0}^{N}\\Phi_{n}\\Delta\\ln e_{i\\mathbb{S},t-n}$$ $$+\\sum_{n=0}^{N}\\Upsilon_{n}\\Delta\\ln e_{i}\\epsilon_{,t-n}+\\sum_{n=0}^{N}\\Pi_{n}\\pi_{j,t-n}^{*}+D_{i,T}+D_{j k}+\\tau_{i j k,t},$$\n\nThis section compares the Vehicle Currency Paradigm (VCP) with the Dominant Currency Paradigm (DCP), highlighting differences in how they explain exchange rate pass-through. VCP distinguishes the US dollar as both a producer and vehicle currency and considers other vehicle currencies, while DCP focuses on dominant currencies like the US dollar. Empirical evidence shows high pass-through for both US dollar and non-US dollar vehicle currencies."
  },
  {
    "qid": "econ-empirical-1589-1-3-0",
    "question": "7) Show that IIIC is logically stronger than IIC.",
    "gold_answer": "IIIC requires that for each individual $i$, the social ranking depends only on their preferences over the commodities they consume in $x$ and $y$. If IIC holds, it implies a common $K$ for all individuals, whereas IIIC allows $K_i$ to vary by individual. Thus, any situation where IIC applies also satisfies IIIC, but not vice versa, making IIIC stronger.",
    "question_context": "INDIVIDUAL INDEPENDENCE OF IRRELEVANT COMMODITIES (IIIC): $\\forall R_{N}, R_{N}^{\\prime}\\in\\mathcal{R}^{n}, \\forall x,y\\in\\mathbb{R}_{+}^{\\ell n}$, if $\\forall i\\in N$, $\\exists K_{i}\\subseteq L$ such that $x_{i},y_{i}\\in\\mathbb{R}_{+}^{K_{i}}$, and $R_{i}$ and $R_{i}^{\\prime}$ agree on $\\mathbb{R}_{+}^{K_{i}}$, then $R$ and $R^{\\prime}$ agree on $\\{x,y\\}$.\n\nThis section extends the analysis to heterogeneous subpopulations, where different groups may consume different sets of commodities, and introduces the Individual Independence of Irrelevant Commodities (IIIC) condition."
  },
  {
    "qid": "econ-empirical-156-0-2-3",
    "question": "4) Why is the agent partition not nested in the strategy–action partition, and what are the implications for causal analysis?",
    "gold_answer": "The agent partition is not nested because:\n- **Agent blocks**: Each block $\\Pi_i^a$ contains all variables for bidder $i$ (strategy and bid).\n- **Strategy–action blocks**: $\\Pi_1^s$ contains all strategies, and $\\Pi_2^s$ contains all bids.\nImplications:\n- **Causal specificity**: Nesting ensures compatibility (e.g., elementary partition $\\Pi^e$ is nested in $\\Pi^s$).\n- **Non-nested partitions**: May lead to incompatible causal claims across partitions.",
    "question_context": "Definition 4.3 (Recursivity): Let $\\boldsymbol{\\mathscr{g}}$ be a partitioned topological settable system. For $b=0,1,\\ldots,B$, let $\\bar{Z}_{[0:b]}^\\Pi$ denote the vector containing the settings $Z_i^\\Pi$ for $i \\in \\pi_{[0:b]}$. Then $\\boldsymbol{\\mathscr{g}}$ is recursive if responses $Y_{[b]}^\\Pi$ are jointly determined as $Y_{[b]}^\\Pi = r_{[b]}^\\Pi(Z_{[0:b-1]}^\\Pi; \\mathbf{a})$.\nDefinition 4.4 (Canonical TSS): A recursive TSS is canonical if $Z_{[b]}^\\Pi = Y_{[b]}^\\Pi$ for $b=1,\\ldots,B$.\nIn the global partition, only fundamental settable variables $\\mathcal{X}_0$ are potential direct causes of $\\mathcal{X}^g$.\n\nThe text introduces recursive and canonical topological settable systems, explaining how they support causal discourse in partitioned systems. It applies these concepts to the auction game, distinguishing between global, agent, and elementary partitions."
  },
  {
    "qid": "econ-empirical-483-1-0-4",
    "question": "5) Prove Theorem 3: Given a nonnegative interval $y^I = [\\underline{y}, \\bar{y}]$ and $H$ nonsingular, $(E - A^I)x^I = y^I$ has a nonnegative solution $x^I$ if and only if $H^{-1}\\bar{y} \\geq H^{-1}F\\underline{y}$.",
    "gold_answer": "1. If $x^I$ exists, Theorem 1 implies $\\bar{y} = F\\underline{y} + Hu$ with $u \\geq 0$. \\\\ 2. Thus, $H^{-1}\\bar{y} = H^{-1}F\\underline{y} + u \\geq H^{-1}F\\underline{y}$. \\\\ 3. Conversely, if $H^{-1}\\bar{y} \\geq H^{-1}F\\underline{y}$, define $u = H^{-1}(\\bar{y} - F\\underline{y}) \\geq 0$. \\\\ 4. Define $\\underline{x} = (E - \\bar{A})^{-1}(\\underline{y} + \\bar{A}_1 u)$ and $\\bar{x} = \\underline{x} + u$. \\\\ 5. By Theorem 1, $x^I = [\\underline{x}, \\bar{x}]$ satisfies $(E - A^I)x^I = y^I$.",
    "question_context": "The basic input-output equation is given by $(E-A)x=y$, where $A$ is an interval matrix $A^I=[\\underline{A}, \\bar{A}]$, and $x^I, y^I$ are interval vectors. The model assumes $A \\geq 0$ and $\\sum_{i=1}^n \\bar{a}_{ij} < 1$ for all $j$.\nThe matrix $F = (E - \\underline{A})(E - \\bar{A})^{-1}$ is introduced, satisfying $F \\geq E$. The matrix $H = E - \\underline{A}_2 + F \\bar{A}_1$ is defined, where $\\underline{A}_2$ and $\\bar{A}_1$ are derived from $\\underline{A}$ and $\\bar{A}$.\nTheorem 1 states that nonnegative intervals $x^I = [\\underline{x}, \\bar{x}]$ and $y^I = [\\underline{y}, \\bar{y}]$ satisfy $(E - A^I)x^I = y^I$ if and only if they can be expressed in terms of nonnegative vectors $u, v$ satisfying $Hu = v$.\nTheorem 2 provides a necessary and sufficient condition for the existence of a nonnegative solution $y^I$ given $x^I$, requiring $(E - \\bar{A}_2)\\underline{x} \\geq \\bar{A}_1 \\bar{x}$.\nTheorem 3 states that given a nonnegative interval $y^I$, a nonnegative solution $x^I$ exists if and only if $H^{-1}\\bar{y} \\geq H^{-1}F\\underline{y}$, assuming $H$ is nonsingular.\n\nThis section introduces a modified input-output model where coefficients are given as intervals, addressing problems of finding interval outputs given inputs and vice versa. Key assumptions include nonnegativity and column sum constraints on the input-output matrix."
  },
  {
    "qid": "econ-empirical-1480-2-2-1",
    "question": "6) Under what parameter conditions does trade lead to worse institutions (Proposition 6)? Interpret the inequality $p_{T}(f_{A}) < f_{T}^{(-1)}(f_{A})$.",
    "gold_answer": "1. Institutions worsen if the political power effect dominates the foreign competition effect.\n2. The inequality $p_{T}(f_{A}) < f_{T}^{(-1)}(f_{A})$ implies the pivotal voter under trade is more productive than the autarky voter who prefers $f_{A}$.\n3. This occurs when:\n   - $L^{N}$ is large (small Southern economy).\n   - $n^{S}/n^{N}$ is large (Southern firms dominate world trade).\n4. The result follows from the integral condition $\\Delta > 0$ in the text.",
    "question_context": "Proposition 4 (The political power effect). When Lemma 1 holds, the Political Curve shifts inward when the economy opens to trade: for every $f\\in[\\underline{{f}},\\overline{{f}}], p_{T}(f)\\leqslant p_{A}(f)$.\nProposition 5 (The foreign competition effect). When Lemma 1 holds, and/or for small enough values of $\\beta$, the Preference Curve shifts inward when the economy opens to trade: for any $a\\in (0,\\frac{1}{b}], f_{T}(a)<f_{A}(a)$.\nProposition 6. Consider a stable autarky equilibrium $(f_{A},p_{A})$. If $p_{T}(f_{A})<f_{T}^{(-1)}(f_{A})$, then there exists an equilibrium of the economy under trade $(f_{T},p_{T})$ such that $f_{A}\\in B(f_{T})$ and $f_{T}>f_{A}$.\n\nThis section compares equilibrium institutions under autarky and trade, highlighting the political power effect and foreign competition effect."
  },
  {
    "qid": "econ-empirical-797-1-1-2",
    "question": "3) Analyze the historical application of indirect control models in POLFA and ZPO. What lessons can be drawn from these experiments?",
    "gold_answer": "**POLFA/ZPO experiments**:\n- **Successes**:\n  - Output targeting (\\( q^* = f(K, L) \\)) improved export competitiveness.\n  - Flexible incentives boosted innovation (e.g., \\( \\partial q/\\partial s > 0 \\)).\n- **Failures**:\n  - Soft budget constraints led to inefficiency (\\( \\pi < 0 \\rightarrow bailout \\)).\n  - Political interference distorted prices (\\( p \\neq p^e \\)).\n\n**Lessons**: Indirect controls require credible commitment to no bailouts (\\( \\Pr(bailout) = 0 \\)).",
    "question_context": "The book is divided into three parts. The first deals with the place of economic calculations in the evolving control system of a developing socialist economy.\nThe second part builds models for current optimization in respect of the following fields: (a) the choice of qualitative and quantitative variants in the planning of foreign trade; (b) the construction of the first macroeconomic system of indirect control over export production.\nPart III deals with three historical stages of application of the models, including experiments in POLFA (the pharmaceutical export industry) and ZPO (the Shipbuilding Industry Association).\n\nThis book examines foreign trade system constructions in Poland, focusing on indirect control mechanisms in a centrally planned economy. It highlights the role of economic calculations, optimization models, and historical applications of these models."
  },
  {
    "qid": "econ-empirical-13-1-1-2",
    "question": "7) Outline the three-step strategy for estimating the simultaneous system as described in the text. How does this strategy address the underlying determinants of net foreign investment?",
    "gold_answer": "1. Step 1: Estimate a simultaneous system with attention to cyclical effects.\\n2. Step 2: Extract demand and supply equations for $N F$ from the estimated model.\\n3. Step 3: Solve these equations under cyclically invariant conditions to reveal determinants of $N F$.\\n4. Addressing determinants:\\n   - The strategy isolates non-cyclical factors affecting $N F$.\\n   - Shows how domestic saving and investment trends influence $N F$ over time.",
    "question_context": "The equation for GAP has special significance. It contains the net foreign investment rate, $N F{\\overset{-}{=}}{\\widehat{N S}}-{\\widehat{D I}}$ among the explanatory variables on the theory that a rise in $N F$ is indicative of continuing slackening of domestic demand.\nThe intersections of the demand and supply equations estimated under cyclically unchanging conditions for various quarters then show how the equilibrium combinations of NF and GAP have changed over time.\n\nThe model contains the three saving rates that together constitute the national saving rate, and the two domestic investment rates that together constitute the domestic investment rate. The estimated values of these two sets of rates are used in the identity defining NF. In addition, the model contains CDUR, TAX, TRANS, GAP, and POTGR as subsidiary dependent variables."
  },
  {
    "qid": "econ-empirical-1755-0-0-1",
    "question": "2) Explain the role of the stationary variable $Z_{t}$ in the varying coefficient model and how it affects the test's power.",
    "gold_answer": "1. $Z_{t}$ is a stationary variable that influences the coefficients $\\theta(Z_{t})$.\n2. The test's power depends on whether $X_{t}$ contains I(0) or I(1) components.\n3. If $X_{t}$ is I(1), the test diverges at rate $n^{2}\\sqrt{h}$ under $H_{1}$.\n4. If $X_{t}$ has both I(0) and I(1) components, the divergence rate varies based on which coefficients are nonconstant.",
    "question_context": "YIGUO SUN University of Guelph\nZONGWU CAI University of Kansas and Xiamen University\nQI LI Texas A&M University and Capital University of Economics and Business\nIn this paper, we propose a simple nonparametric test for testing the null hypothesis of constant coefficients against nonparametric smooth coefficients in a semiparametric varying coefficient model with integrated time series.\nWe establish the asymptotic distributions of the proposed test statistic under both null and alternative hypotheses. Moreover, we derive a central limit theorem for a degenerate second order U-statistic, which contains a mixture of stationary and nonstationary variables and is weighted locally on a stationary variable.\nCointegration has proved to be a powerful tool in studying long-run relationships among integrated time series and is a widely used econometric methodology in macroeconomics and financial time series analysis.\nIn this paper we are interested in testing parameter constancy in the framework of semiparametric varying coefficient models studied by Cai et al. (2009) and Xiao (2009), i.e.\n$$Y_{t}=X_{t}^{T}\\theta\\left(Z_{t}\\right)+u_{t},\\quad1\\leq t\\leq n,$$\nwhere $Y_{t},~Z_{t}$ , and $u_{t}$ are all scalars, $X_{t}$ is of dimension $d$ , and $\\theta\\left(\\cdot\\right)$ is a $d\\times1$ vector of unknown smooth functions,1 the superscript $T$ denotes the transpose of a matrix.\n\nThe paper introduces a nonparametric test for constant coefficients in semiparametric varying coefficient models with integrated time series. It discusses the importance of cointegration and varying coefficient models in econometrics."
  },
  {
    "qid": "econ-empirical-38-3-2-1",
    "question": "6) Contrast the Nash equilibrium outcomes under bundling vs. independent pricing. Why does bundling increase total surplus?",
    "gold_answer": "1. **Independent Pricing**: $p_A=0.5$, $p_B=0$ (Bertrand). Profits: $\\Pi_1=0.25$, $\\Pi_2=0$.\\n2. **Bundling**: Higher $p_B$ (0.24) and bundle price $x^*=0.59$. Profits: $\\Pi_1=0.366$, $\\Pi_2=0.064$.\\n3. **Total Surplus**: Bundling increases $A$'s sales (marginal price $0.59-0.24=0.35 < 0.5$), offsetting deadweight loss from higher $p_B$.",
    "question_context": "Firm 1’s profits given a price of $p_b$ for good $B$ are $\\Pi_1|_{\\text{entry}} = x(1-x + p_b - p_b^2/2)$. Profits are maximized at $1-2x + p_b - p_b^2 = 0$ or $x^* = (1 + p_b - p_b^2)/2$.\nThe Nash equilibrium is $x^*=0.59$, $p_b^*=0.24$. Equilibrium profits are 0.366 for firm 1 and 0.064 for firm 2.\n\nThis section analyzes Nash equilibria in post-entry pricing games, comparing bundling to independent pricing. It highlights how bundling mitigates competition and increases incumbent profits."
  },
  {
    "qid": "econ-empirical-806-0-0-1",
    "question": "2) Prove the consistency of the ratio estimator $\\hat{\\beta}_e = \\hat{\\eta}_I / \\hat{\\alpha}_I$ under the assumption that $\\hat{\\alpha}_I$ and $\\hat{\\eta}_I$ are consistent estimators of $\\alpha_I$ and $\\eta_I$, respectively.",
    "gold_answer": "1. Given that $\\hat{\\alpha}_I \\xrightarrow{p} \\alpha_I$ and $\\hat{\\eta}_I \\xrightarrow{p} \\eta_I$ as the sample size increases.\n2. By the Continuous Mapping Theorem, since the function $f(x, y) = y / x$ is continuous at $(\\alpha_I, \\eta_I)$ where $\\alpha_I \\neq 0$, we have:\n$$ \\hat{\\beta}_e = \\frac{\\hat{\\eta}_I}{\\hat{\\alpha}_I} \\xrightarrow{p} \\frac{\\eta_I}{\\alpha_I} = \\beta_e $$\n3. Thus, $\\hat{\\beta}_e$ is a consistent estimator of $\\beta_e$.",
    "question_context": "The standard SEM is usually assumed as follows: \n$$ Y=\\beta_{e}X_{e}+\\mathbf{X}_{o}^{\\top}\\pmb{\\beta}_{o}+U, $$\nwhere $Y$ is the response variable, $X_{e}$ is the exposure whose regression coefficient $\\beta_{e}$ is of major interest, $\\mathbf{X}_{o}$ include observed covariates with regression coefficients $\\beta_{o}$ and $U$ is a disturbance involving unmeasured confounders correlated with $X_{e}$ and thus invoking the endogeneity issue.\nA reduced form equation for $X_{e}$ is assumed as follows: \n$$ X_{e}=\\alpha_{I}X_{I}+\\mathbf{X}_{o}^{\\top}\\pmb{\\alpha}_{o}+\\varepsilon, $$\nwhere $X_{I}$ is the instrumental variable, and $\\varepsilon$ is an error. It is assumed that $E(U|X_{I},\\mathbf{X}_{o})=0$ and $E(\\boldsymbol{\\varepsilon}|X_{I},\\mathbf{X}_{o})=0$.\nSubstituting equation (2) into model (1), we arrive at a synthetic model form \n$$ Y=\\eta_{I}X_{I}+\\mathbf{X}_{o}^{\\top}\\pmb{\\eta}_{o}+E, $$\nwhere $\\eta_{I}=\\beta_{e}\\alpha_{I},\\eta_{o}=\\beta_{e}\\alpha_{o}+\\beta_{o}$ , and ${\\cal E}=\\beta_{e}\\varepsilon+U$.\nThe ratio estimator of $\\beta_{e}$ is given by: \n$$ \\hat{\\beta}_{e}=\\hat{\\eta}_{I}/\\hat{\\alpha}_{I}. $$\n\nThis section discusses the instrumental variable (IV) estimation for causal regression parameters with multiple unknown structural changes across subpopulations. The proposed method involves a multiple change point detection technique to determine the number of thresholds and estimate their locations within a two-stage least squares (2SLS) framework. The Wald method is then used to estimate the regression coefficients of the endogenous variable."
  },
  {
    "qid": "econ-empirical-106-1-0-2",
    "question": "3) Using the demand function $y_{t}^{d}=m_{t}-p_{t}$ and the aggregate supply function $y_{t}^{s}$, derive the equilibrium price level $p_{t}$ as a function of the expectations $_{h}p_{t}, _{i}p_{t}, _{j}p_{t}$ and the disturbances $u_{t}, m_{t}, z_{k t}$.",
    "gold_answer": "1. Set $y_{t}^{d} = y_{t}^{s}$ to find equilibrium: $$ m_{t} - p_{t} = (1+\\Phi)u_{t} + \\phi p_{t} - \\frac{\\Phi}{3}[(_{h}p_{t}+_{i}p_{t}+_{j}p_{t}) + (_{h}u_{t}+_{i}u_{t}+_{j}u_{t}) + (_{h}z_{1t}+_{i}z_{2t}+_{j}z_{3t})]. $$\n2. Solve for $p_{t}$: $$ p_{t} = \\frac{m_{t} - (1+\\Phi)u_{t} + \\frac{\\Phi}{3}[(_{h}p_{t}+_{i}p_{t}+_{j}p_{t}) + (_{h}u_{t}+_{i}u_{t}+_{j}u_{t}) + (_{h}z_{1t}+_{i}z_{2t}+_{j}z_{3t})]}{1 + \\phi}. $$\n3. This shows how $p_{t}$ depends on expectations and disturbances.",
    "question_context": "The production function and employment rule for each sector are given by $$ y_{k t}=\\alpha l_{k t}+z_{k t}+u_{t},0<\\alpha<1, $$ and $$ l_{k t}=\\eta[(p_{t}-\\mathbf{\\nabla}_{h}p_{t})\\:+(u_{t}-\\mathbf{\\nabla}_{h}u_{t})\\:+(z_{k t}\\:-\\mathbf{\\nabla}_{h}z_{k t})], $$ with $\\mathfrak{n}>0$.\nHere, $y_{k t}$ is the logarithm of output for sector $\\pmb{k}$ at time t; $l_{k t}$ is the logarithm of employment for sector $k;p_{t}$ is the logarithm of the price of output; ${}_{h}p_{t}=E[p_{t}\\vert I_{h}],$ where $\\pmb{E}$ denotes the mathematical expectation operator and $I_{h}$ is the information set possessed at the point of negotiation; $z_{k t}$ is a relative productivity disturbance that nets out across the $\\pmb{n}$ sectors each period; and $u_{t}$ is a common disturbance to productivity. Constants are suppressed for convenience.\nOne specification leading to equation (2) assumes that the wage is set to equate the expected demand and supply of labor in each sector over the $\\pmb{\\uptau}$ periodcontract. If labor is inelastically supplied, then $-\\upeta$ equals theelasticity of demand for labor with respect to the real wage; see Fethke and Policano [1984, p. 153].\nSupply of output depends on the timing of the contract negotiation dates. For ease of exposition, initially assume that there are three sectors with the following negotiation pattern: sector 3 negotiates at $t=0$ and next at $t=\\tau$ ; sector 1 negotiates at $\\scriptstyle t=b$ and next at $t=b+\\tau$ and sector 2 negotiates at $t=a$ and $t=a+\\tau$ with $0\\leqslant b\\leqslant a<\\uptau$ .Synchronized negotiation occurs when $a=b=0$ , while uniformly staggered negotiation occurs when $b=\\tau/3$ and $a=2\\tau/3$ Employing this pattern, we can derive supply by substituting equation (2) into equation (1) and summing across the three sectors; each sector receives one third of the weight in total output: $$ \\begin{array}{r l}&{y_{t}^{s}=(1+\\Phi)u_{t}+\\phi p_{t}-\\Phi/3[(\\phantom{}_{h}p_{t}+\\phantom{\\frac{1}{\\epsilon}}_{i}p_{t}+\\phantom{\\frac{1}{\\epsilon}}_{j}p_{t})}\\ &{\\phantom{\\frac{1}{\\epsilon}}+(\\phantom{\\frac{1}{\\epsilon}}_{h}u_{t}+\\phantom{\\frac{1}{\\epsilon}}_{i}u_{t}+\\phantom{\\frac{1}{\\epsilon}}_{j}u_{t})+\\phantom{\\frac{1}{\\epsilon}}(\\phantom{\\frac{1}{\\epsilon}}_{h}z_{1t}+\\phantom{\\frac{1}{\\epsilon}}_{i}z_{2t}+\\phantom{\\frac{1}{\\epsilon}}_{j}z_{3t})],}\\ &{\\phantom{\\frac{1}{\\epsilon}}\\Phi\\equiv\\alpha\\eta>0.}\\end{array} $$\nDemand for the product is given by $$ y_{t}^{d}=m_{t}-p_{t}, $$ where $m_{t}$ is a stochastic process that is distributed independently from $\\pmb{u}_{t}$ and $z_{k t}$ . The unitary price elasticity of demand significantly simplifies the analysis without affecting the results;2 specifically, the distribution of any $m_{t}$ disturbance between output and price will depend entirely on the supply schedule.3\nEach sector negotiates a contract of length $\\uptau$ .Given the negotiation dates of the other sectors, sector $\\pmb{k}$ chooses its point of negotiation to minimize an expected cost per cycle, $\\psi_{k}$ ，which consists of an efficiency loss plus a fixed cost per negotiation, c. Specifically, $$ \\begin{array}{r}{\\Psi_{k}=\\frac{1}{\\tau}\\bigg\\{\\uptheta\\bigg(\\displaystyle\\int_{0}^{b}E(l_{k t}-\\tilde{l}_{k t})^{2}d t+\\int_{b}^{a}E(l_{k t}-\\tilde{l}_{k t})^{2}d t\\bigg.\\bigg.\\qquad}\\ {\\bigg.\\bigg.+\\displaystyle\\int_{a}^{\\tau}E(l_{k t}-\\tilde{l}_{k t})^{2}d t\\bigg)+c\\bigg\\},}\\end{array} $$ where ${\\uptheta}=({1}/{\\upeta})^{2}$ . The expression $\\theta E(l_{2t}-\\tilde{l}_{2t})^{2}$ is the (weighted) mean-square discrepancy between actual employment $l_{k t}$ and full information employment $\\tilde{l}_{k t}$ , and depends on the wage rate set by sector $\\pmb{k}$ at its point of negotiation. The full information level of employment is determined from equation (2) by setting the expectations ${}_{h}p_{t},~{}_{h}u_{t}$ , and $\\scriptstyle_{h}{\\mathcal{Z}}_{k t}$ equal to their actual values; here $\\bar{\\bar{l}}_{k t}=0$\nWe are able to obtain a closed-form expression for equation (5) when it is assumed that $u_{t},\\mathrm{~m~}_{t}$ , and $\\pmb{z}_{k t}$ are (continuous-time equivalent) random-walk processes  with  forecast  variances $\\scriptstyle\\sigma_{u}^{2}t$ $\\sigma_{m}^{2}t$ and $\\upsigma_{z}^{2}t$ , respectively. The innovations in $u_{t},\\mathrm{~m~}_{t}$ , and $z_{k t}$ are uncorrelated. When $n=3$ , the symmetry underlying the framework implies that the correlation between any pair of relative productivity innovations is equal to minus one half.\n\nThe economy consists of $n$ sectors, defined as separate negotiating units. Each sector produces the same product, which is sold in a competitive market; the case of monopolistic competition is discussed in Section IV. The nominal wage for each sector is set at the beginning of each contract negotiation date for t periods, where T is identical and fixed for all sectors. While contract length is fixed, the negotiation dates and thus the information incorporated into the wage contracts need not correspond. Once the nominal wage is set, employment over the contract is determined by labor demand. Labor is assumed to be immobile after the contract is negotiated."
  },
  {
    "qid": "econ-empirical-814-3-0-1",
    "question": "2) Show how the implied prior variance of the diagonals in $\\boldsymbol{\\Sigma}$ is derived, given the prior distributions $r_{jj} \\sim N(0, \\sigma_{r_{-}jj}^2)$ and $r_{mj} \\sim N(0, \\sigma_{r_{-}off}^2)$.",
    "gold_answer": "1. The implied prior variance of $\\Sigma_{jj}$ is $\\mathrm{var}[\\Sigma_{jj}] = \\mathrm{var}[e^{2r_{jj}} + \\sum_{m=1}^{j-1} r_{mj}^2]$.\n2. For $j=1$, $\\mathrm{var}[\\Sigma_{11}] = \\mathrm{var}[e^{2r_{11}}] = e^{8\\sigma_{r_{-}11}^2} - e^{4\\sigma_{r_{-}11}^2}$ (using the variance of a log-normal distribution).\n3. For $j \\geq 2$, the off-diagonal terms contribute $\\sum_{m=1}^{j-1} \\mathrm{var}[r_{mj}^2] = 2(j-1)\\sigma_{r_{-}off}^4$ (since $\\mathrm{var}[r_{mj}^2] = 2\\sigma_{r_{-}off}^4$ for $r_{mj} \\sim N(0, \\sigma_{r_{-}off}^2)$).\n4. Combining these, the total variance is:\n   $$\\mathrm{var}[\\Sigma_{jj}] = 2(j-1)\\sigma_{r_{-}off}^4 + e^{8\\sigma_{r_{-}jj}^2} - e^{4\\sigma_{r_{-}jj}^2}.$$",
    "question_context": "The hyperparameter values used in the sampling experiment and the empirical example are as follows: \n\n$$\\begin{array}{l}{{\\overline{{{\\theta_{0}}}}={\\bf0},\\qquad V_{\\overline{{{\\theta}}}}=100I_{K}}}\\\\ {{\\nu_{0}=K+1,\\qquad s_{0}^{2}=1}}\\end{array}$$ \n\nwhere $I_{K}$ is an identity matrix of size $K$.\nRecall that we reparameterize $\\boldsymbol{\\Sigma}$ as follows: \n\n$$\\begin{array}{r l}&{\\boldsymbol{\\Sigma}=\\boldsymbol{U^{\\prime}}\\boldsymbol{U}}\\\\ &{\\boldsymbol{U}=\\left[\\begin{array}{c c c c}{e^{r_{11}}}&{r_{12}}&{\\cdots}&{r_{1K}}\\\\ {0}&{e^{r_{22}}}&{\\ddots}&{\\vdots}\\\\ {\\vdots}&{\\ddots}&{\\ddots}&{r_{K-1,K}}\\\\ {0}&{\\cdots}&{0}&{e^{r_{K K}}}\\end{array}\\right].}\\end{array}$$\nThe jth diagonal element in $\\boldsymbol{\\Sigma}$ can be expressed as \n\n$$\\begin{array}{l l}{{}}&{{\\displaystyle\\sum_{j j}=\\exp\\left(2r_{j j}\\right)+\\sum_{m=1}^{j-1}\\left(r_{m j}\\right)^{2}\\quad j=2,\\ldots,K}}\\\\ {{}}&{{\\displaystyle\\sum_{11}=\\exp\\left(2r_{11}\\right).}}\\end{array}$$\nThe implied prior variance of the diagonals in $\\boldsymbol{\\Sigma}$ is \n\n$$\\begin{array}{r}{\\mathrm{var}\\left[\\left.\\boldsymbol{\\Sigma}_{j j}\\right]=2\\left(j-1\\right)\\sigma_{r_{-}o f f}^{4}+\\exp\\left(8\\sigma_{r_{-}j j}^{2}\\right)-\\exp\\left(4\\sigma_{r_{-}j j}^{2}\\right),}\\\\ {j=1,...,K.\\qquad}\\end{array}$$\nWe choose $\\sigma_{r_{-}o f f}^{2}=1$ and $c=50s_{0}$ so that the elements of $\\boldsymbol{\\Sigma}$ have diffuse priors and that the priors of the associated correlations are roughly uniformly distributed between 0 and 1.\n\nThe hyperparameter values used in the sampling experiment and the empirical example are provided, including the reparameterization of the covariance matrix Σ."
  },
  {
    "qid": "econ-empirical-1218-0-1-2",
    "question": "7) Why might the estimated effect of no-fault laws on hours worked per week (Table 6, Column 2) be statistically insignificant, while the effect on annual hours (Column 3) is significant?",
    "gold_answer": "Possible explanations:\n1. **Extensive vs. intensive margin**: Mothers may enter the labor force (extensive margin) without changing weekly hours (intensive margin).\n2. **Measurement error**: Annual hours (weeks × hours) aggregates small weekly changes, reducing noise.\n3. **Adjustment costs**: Fixed costs of adjusting weekly hours may be higher than adjusting weeks worked.",
    "question_context": "No-fault divorce laws are associated with a roughly 2 percent higher probability of being in the labor force for married mothers, and a 25-hour increase in annual hours worked among those with children under two.\nThe effects are larger for women with younger children, consistent with higher transaction costs and income elasticities for this group.\nResults are robust to alternative classifications of no-fault laws and controls for property division rules (e.g., common law vs. community property).\n\nThe paper presents robust evidence that no-fault divorce laws increase labor supply among married mothers, particularly those with young children. Results hold across alternative specifications of no-fault laws and property division rules."
  },
  {
    "qid": "econ-empirical-489-9-0-2",
    "question": "3) Describe the process of inverting the local average structural function to obtain the LQTE estimates, including the role of linear interpolation.",
    "gold_answer": "1. The local average structural function $\\hat{\\theta}_{1(Y\\leq u)}(d)$ is estimated for a grid of values $u \\in \\{q_Y(0.05), q_Y(0.06), \\dots, q_Y(0.95)\\}$.\n2. To estimate the LQTE at a quantile $q \\in [0.1, 0.9]$, we need to find $\\hat{\\theta}_Y^{\\leftarrow}(q, d)$, the inverse of the distribution function.\n3. Since the distribution function is only estimated at discrete points, linear interpolation is used to approximate the inverse at any $q$.\n4. For example, if $q$ lies between $q_Y(k)$ and $q_Y(k+1)$, the inverse is approximated as:\n   $$\\hat{\\theta}_Y^{\\leftarrow}(q, d) \\approx \\hat{\\theta}_{1(Y\\leq q_Y(k))}(d) + \\frac{q - k}{0.01} \\left(\\hat{\\theta}_{1(Y\\leq q_Y(k+1))}(d) - \\hat{\\theta}_{1(Y\\leq q_Y(k))}(d)\\right).$$\n5. The LQTE is then calculated as the difference between the inverted functions for $d=1$ and $d=0$:\n   $$\\hat{\\Delta}(q) = \\hat{\\theta}_Y^{\\leftarrow}(q, 1) - \\hat{\\theta}_Y^{\\leftarrow}(q, 0).$$",
    "question_context": "To calculate the LQTE, we first calculate the local average structural function for outcomes $Y_{u}=1(Y\\leq u)$ for a set of $\\boldsymbol{u}$ and then invert to obtain estimates of the LQTE. In our example, we chose to look at $u\\in[q_{Y}(0.05)$ ， $q_{Y}(0.95)]$ where $q_{Y}(0.05)$ and $q_{Y}(0.95)$ are respectively the sample 5th and 95th percentiles of the outcome of interest $Y$.\nWe set $\\lambda=1.1{\\sqrt{n}}\\Phi^{-1}(1-(1/\\log(n))/(2n(2p)))$ where $\\Phi(\\cdot)$ is the standard normal distribution function. We calculate penalty loadings using Algorithm 6.1 with a maximum of 15 iterations.\nEstimates of the local average structural (distribution) functions are formed using the estimators defined in the previous two paragraphs as $\\hat{\\theta}_{1(Y\\leq u)}(d)=\\frac{\\hat{\\alpha}_{1_{d}(D)1(Y\\leq u)}(1)-\\hat{\\alpha}_{1_{d}(D)1(Y\\leq u)}(0)}{\\hat{\\alpha}_{1_{d}(D)}(1)-\\hat{\\alpha}_{1_{d}(D)}(0)}$.\nThe LQTE at point $\\pmb q$ is then estimated as $\\hat{\\Delta}(q)=\\hat{\\theta}_{Y}^{\\leftarrow}(q,1)-\\hat{\\theta}_{Y}^{\\leftarrow}(q,0)$.\n\nThe section discusses the calculation and inference for Local Quantile Treatment Effects (LQTE), which is more complex than for the Local Average Treatment Effect (LATE). It involves estimating the local average structural function for outcomes and inverting these estimates to obtain LQTE. The process includes discretizing the interval of quantiles, estimating propensity scores, and using post-ℓ₁-penalized estimators."
  },
  {
    "qid": "econ-empirical-108-5-1-3",
    "question": "8) The study concludes that sorting and bargaining effects explain about $20\\%$ of the gender wage gap in Portugal. Discuss the external validity of these findings for other countries.",
    "gold_answer": "**Generalizability**:\n1. Portugal's high female labor force participation and full-time employment rates make it comparable to other OECD countries.\n2. However, institutional factors (e.g., collective bargaining) may limit direct extrapolation.\n3. The methodological framework (AKM + decomposition) is broadly applicable to other settings.",
    "question_context": "We find that the underrepresentation of women at firms that offer higher wage premiums for both gender groups—the sorting effect—explains about $15\\%$ of the overall 23 log point gender gap in Portugal. Another $5\\%$ is attributable to the fact that women gain less than men from higher-wage firms—the bargaining effect.\nThe component of the firm-specific wage premium received by male workers that is directly attributed to observable surplus is $\\hat{\\pi}^{M}E[N S_{J(i,t)}|male]$, while the corresponding component for female workers is $\\hat{\\pi}^{F}E[N S_{J(i,t)}|female]$.\n\nThe study presents detailed empirical results on the contribution of sorting and bargaining effects to the gender wage gap in Portugal."
  },
  {
    "qid": "econ-empirical-183-2-1-1",
    "question": "2) Prove that the set $\\mathcal{P}^{a,b}$ is time-consistent and satisfies Assumptions 2, 3, and 4 as stated in Lemma 2.",
    "gold_answer": "1. Verify time-consistency by showing that restrictions of $P \\in \\mathcal{P}^{a,b}$ to earlier times remain in the set. \n2. Check Assumption 2 (closedness under pasting), Assumption 3 (predictable integrability), and Assumption 4 (uniform integrability). \n3. Use the predictable process $(\\alpha_{t})$ to argue consistency.",
    "question_context": "Let $(S,S,\\nu_{0})$ be a measure space with $s\\subset\\mathbb{R}$. Let $\\pmb{\\varOmega}=\\pmb{S}^{\\mathbb{N}}$ be the set of all sequences with values in $s$ and let $B=\\otimes_{t=1}^{\\infty}S$ be the $\\pmb{\\sigma}$-field generated by all projections $\\pmb{\\varepsilon}_{t}:\\varOmega\\rightarrow\\pmb{S}$.\nThe log-Laplace function $L(\\lambda)=\\log\\int_{S}e^{\\lambda x}\\nu_{0}(d x)$ is well defined. Note that, by definition, $E^{P_{0}}\\exp(\\lambda\\varepsilon_{t}-L(\\lambda))=1$.\nWe denote by $\\mathcal{P}^{a,b}$ the set of all probability measures $P\\sim P_{0}$ whose density processes satisfy $D_{t}^{\\alpha}=\\exp\\Biggl(\\sum_{s=1}^{t}\\alpha_{s}\\varepsilon_{s}-\\sum_{s=1}^{t}L(\\alpha_{s})\\Biggr)$ for a predictable process $(\\alpha_{t})$ with values in $[a,b]$.\nLEMMA 2: The set $\\mathcal{P}^{a,b}$ satisfies Assumptions 2, 3, and 4.\n\nThis section introduces a framework for multiple prior versions of sequences of independent and identically distributed random variables, focusing on time-consistent priors and exponential families."
  },
  {
    "qid": "econ-empirical-1773-2-0-2",
    "question": "3) Why is it challenging to disentangle the effects of firm productivity and quality-producing ability on export prices? How does the empirical specification address this?",
    "gold_answer": "1. Intangible asset transfers (e.g., R&D, foreign technicians) may boost both productivity and quality.\n2. Revenue-based TFP conflates physical efficiency and quality-driven price effects.\n3. The specification controls for TFP to isolate quality effects via proxies (e.g., R&D intensity, foreign employees).\n4. A positive coefficient on quality proxies after TFP control suggests a distinct quality channel.",
    "question_context": "A preliminary way to examine the theoretical predictions is to regress the logarithm of the unit export price on ownership types (domestic firms are the excluded group). Product–destination–year fixed effects are included to control for the demand side. We should expect that the export price charged by FIEs is higher than that by domestic enterprises.\nThe main empirical question we want to address in the paper is what contributes to the multinational price premium. As seen from Eq. (5), the export price is a simple log–linear function of firm productivity η^(φ), the ability to produce quality (λ), and destination characteristics (Y, P, and τ).\nlog p_{fgct} = a_{1}FIE_{f} + a_{2}HMT_{f} + Z_{f,t-1}b + D_{gct} + ε_{fgct}, where p_{fgct} is the price of product g exported by firm f to country c in year t; FIE_{f} is an indicator of FIEs, and HMT_{f} is an indicator of HMT firms; Z_{f,t-1} is a vector of firm characteristics including firm productivity and proxies for the ability to produce quality; D_{gct} represents the product–destination–year fixed effects; and ε_{fgct} is the error term.\n\nThe empirical analysis focuses on the differences between Foreign Invested Enterprises (FIEs) and domestic enterprises, with FIEs characterized as high-φ and high-λ types, and domestic enterprises as low-φ and low-λ types. The study examines the multinational price premium through regression analysis, controlling for product–destination–year fixed effects."
  },
  {
    "qid": "econ-empirical-1440-3-0-1",
    "question": "2) Show that \\( L_2(U, P) = b(P)(U - P) \\) must hold under competitive equitability, given \\( L_2(Y, Y) = 0 \\) for all \\( Y \\).",
    "gold_answer": "1. From competitive equitability, \\( L_2 \\) must satisfy \\( \\sum_{t} L_2(U_t, \\mu^0) \\pi_t^0 = 0 \\) for all \\( \\{\\pi_t^0\\} \\).\n2. This implies \\( L_2 \\) is linear in \\( U \\): \\( L_2(U, P) = a(P) + b(P)U \\).\n3. From \\( L_2(Y, Y) = 0 \\), substitute \\( U = P = Y \\): \\( a(Y) + b(Y)Y = 0 \\).\n4. Thus, \\( a(Y) = -b(Y)Y \\), yielding \\( L_2(U, P) = b(P)(U - P) \\).",
    "question_context": "Let there be a continuum of firms, each with size \\( S(n) \\). The nonnegative loss function \\( L(U, P) \\) associated with charging price \\( P \\) when unit cost is \\( U \\) satisfies: (A) \\( L(X, X) = 0 \\) for all \\( X \\), (B) \\( L(U, P) \\) is strictly convex in \\( P \\) for all \\( U \\), and (C) competitive equitability.\nThe average inequity produced by charging all members of a group price \\( P \\) is given by \\( I = \\sum_{t} L(U_t, P) \\pi_t^0 \\). Competitive equitability implies \\( P = \\mu^0 = \\sum_{t} U_t \\pi_t^0 \\) minimizes \\( I \\).\nThe first-order condition for minimizing \\( I \\) is \\( \\sum_{t} L_2(U_t, \\mu^0) \\pi_t^0 = 0 \\). Competitive equitability requires this to hold for all probability vectors \\( \\{\\pi_t^1\\} \\).\nUnder competitive equitability, \\( L_2(U, P) = b(P)(U - P) \\), where \\( b(P) \\) is a negative constant. Integrating yields \\( L(U, P) = k(U - P)^2 \\), with \\( k > 0 \\).\n\nThis section establishes conditions under which a nonnegative loss function \\( L(U, P) \\) can be written as \\( (U - P)^2 \\) for comparing pricing systems, given strict convexity and competitive equitability."
  },
  {
    "qid": "econ-empirical-1598-2-0-1",
    "question": "2) Prove the inequality $L(\\mathrm{H}_{0}^{**}) \\leqslant L(\\mathrm{H}_{0}^{*}) \\leqslant L(\\mathrm{H}_{U})$ under the Gaussian MLR model, and interpret its implications for the LR statistic.",
    "gold_answer": "**Proof**:\n\n1. **Likelihood Functions**:\n   - Unrestricted: $L(\\mathrm{H}_{U}) = -\\frac{n p}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(|\\hat{\\boldsymbol{\\Sigma}}|) - \\frac{n p}{2}$\n   - Under $\\mathrm{H}_{0}^{*}$: $L(\\mathrm{H}_{0}^{*}) = -\\frac{n p}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(|\\hat{\\boldsymbol{\\Sigma}}_{0}^{*}|) - \\frac{n p}{2}$\n   - Under $\\mathrm{H}_{0}^{**}$: $L(\\mathrm{H}_{0}^{**}) = -\\frac{n p}{2} \\ln(2\\pi) - \\frac{n}{2} \\ln(|\\hat{\\boldsymbol{\\Sigma}}_{0}^{**}|) - \\frac{n p}{2}$\n\n2. **Inequality**: Since $\\mathrm{H}_{0}^{**} \\subseteq \\mathrm{H}_{0}^{*} \\subseteq \\mathrm{H}_{U}$, maximization over a larger parameter space yields a higher likelihood. Thus:\n   $$ L(\\mathrm{H}_{0}^{**}) \\leqslant L(\\mathrm{H}_{0}^{*}) \\leqslant L(\\mathrm{H}_{U}) $$\n\n**Implications**:\n- The LR statistic $LR^{*}$ is bounded by the LR statistic for the nested hypothesis $\\mathrm{H}_{0}^{**}$.\n- This allows deriving conservative critical values for testing $\\mathrm{H}_{0}^{*}$.",
    "question_context": "Formally, in the context of (2.4) consider the general hypothesis $$\\mathrm{H}_{0}^{*}:R^{*}b\\in{\\varDelta}_{0},$$ where $R^{*}$ is a $q^{*}\\times(p K)$ matrix of rank $q^{*}$ , and $\\varDelta_{0}$ is a non-empty subset of $\\mathbb{R}^{q*}$ . This characterization of the hypothesis includes cross-equation linear restrictions and allows for nonlinear as well as inequality constraints.\nThe relevant LR statistic is $$L R^{*}=n\\ln(\\boldsymbol{A}^{*}),\\quad\\boldsymbol{A}^{*}=|\\hat{\\boldsymbol{\\Sigma}}_{0}^{*}|/|\\hat{\\boldsymbol{\\Sigma}}|,$$ where $\\hat{\\boldsymbol{\\Sigma}}_{0}^{*}$ and $\\hat{\\Sigma}$ are the MLE of $\\Sigma$ imposing and ignoring $\\mathrm{H}_{0}^{\\ast}$ .\nUnder assumption (2.3), $$\\begin{array}{l}{{\\displaystyle{\\cal L}(\\mathrm{H}_{0}^{*})=-\\frac{n p}{2}\\mathrm{ln}(2\\pi)-\\frac{n}{2}\\mathrm{ln}(|\\hat{\\Sigma}_{0}^{*}|)-\\frac{n p}{2}},}\\\\{{\\displaystyle{\\cal L}(\\mathrm{H}_{0}^{**})=-\\frac{n p}{2}\\mathrm{ln}(2\\pi)-\\frac{n}{2}\\mathrm{ln}(|\\hat{\\Sigma}_{0}^{**}|)-\\frac{n p}{2}},}\\end{array}$$ where $\\hat{\\Sigma_{0}}^{**}$ is the MLE under $\\mathrm{H}_{0}^{\\ast\\ast}$ .\nTheorem 4.1 (Bounds for general LR statistics). Consider the MLR model (2.4) with (2.2). Let $A^{*}$ be the statistic de4ned by (4.2) for testing $R^{*}b\\in A_{0}$ , where $R^{*}$ is $a$ $q^{*}\\times(p K)$ full column rank matrix and $\\varDelta_{0}$ is a non-empty subset of $\\mathbb{R}^{q^{*}}$ . Further, consider any restrictions of the form ${\\tilde{R}}B C=D$ that satisfy $R^{*}b\\in A_{0}$ , where $\\tilde{R}$ and $C$ are $r\\times K$ and $p\\times c$ matrices such that $r=r a n k(\\tilde{R})$ and $c=r a n k(C)$ . Let $\\boldsymbol{A}^{**}$ be the inverse of Wilks criterion for testing the latter restrictions. Then under the null hypothesis, $P[\\lambda^{*}\\geqslant\\lambda^{**}(\\alpha)]\\leqslant a_{}$ , for all $0\\leqslant a\\leqslant1$ , where $\\lambda^{**}(\\alpha)$ is determined such that $P[\\Lambda^{**}\\geqslant\\lambda^{**}(\\alpha)]=\\alpha$ .\n\nThis section studies the problem of testing general hypotheses on the coefficients of the MLR model, deriving exact bounds on the null distributions of the LR statistic and extending results to the multi-equation context."
  },
  {
    "qid": "econ-empirical-1464-2-0-0",
    "question": "1) Prove that the dual self representation $U(x) = (1-\\rho)\\max_{\\beta \\in x} u(\\beta) + \\rho\\max_{\\beta \\in B_{\\nu}(x)} u(\\beta)$ is linear in the sense defined in the text, i.e., show that $U(\\lambda x + (1-\\lambda)y) = \\lambda U(x) + (1-\\lambda)U(y)$ for all $x, y \\in \\mathcal{A}$ and $\\lambda \\in (0,1)$.",
    "gold_answer": "To prove linearity of $U$, we proceed as follows:\n1. Let $x, y \\in \\mathcal{A}$ and $\\lambda \\in (0,1)$. Define $z = \\lambda x + (1-\\lambda)y$.\n2. By definition, $U(z) = (1-\\rho)\\max_{\\beta \\in z} u(\\beta) + \\rho\\max_{\\beta \\in B_{\\nu}(z)} u(\\beta)$.\n3. Since $u$ is linear, $\\max_{\\beta \\in z} u(\\beta) = \\lambda \\max_{\\beta \\in x} u(\\beta) + (1-\\lambda)\\max_{\\beta \\in y} u(\\beta)$.\n4. Similarly, $B_{\\nu}(z) = \\arg\\max_{\\beta \\in z} \\nu(\\beta) = \\lambda B_{\\nu}(x) + (1-\\lambda)B_{\\nu}(y)$ due to linearity of $\\nu$.\n5. Thus, $\\max_{\\beta \\in B_{\\nu}(z)} u(\\beta) = \\lambda \\max_{\\beta \\in B_{\\nu}(x)} u(\\beta) + (1-\\lambda)\\max_{\\beta \\in B_{\\nu}(y)} u(\\beta)$.\n6. Combining, $U(z) = (1-\\rho)[\\lambda \\max_{x} u + (1-\\lambda)\\max_{y} u] + \\rho[\\lambda \\max_{B_{\\nu}(x)} u + (1-\\lambda)\\max_{B_{\\nu}(y)} u] = \\lambda U(x) + (1-\\lambda)U(y)$.",
    "question_context": "A dual self representation of a preference relation $\\succcurlyeq$ is a triple $(u,\nu,\rho)$ ,where $u,\nu;\\Delta\to\\mathbb{R}$ are continuous and linear, and $\rho\\in[0,1]$ such that $U:{\\mathcal{A}}\rightarrow\\mathbb{R}$ given by $$ U(x):=(1-\rho)\\operatorname*{max}_{\\beta\\in x}u(\\beta)+\\rho\\operatorname*{max}_{\\beta\\in B_{\\nu}(x)}u(\\beta) $$ represents $\\succcurlyeq$.\nAXIOM 1: (Preferences) $\\succcurlyeq$ is a complete and transitive binary relation.\nAXIOM 4: (Temptation)There exist $\\alpha$ $\\beta\\in x$ suchthat $\\{\\alpha\\}\\succcurlyeq x\\succcurlyeq\\{\\beta\\}.$\nAXIOM 5: (Regularity) $\\{\\alpha_{1}\\}$ and $\\left\\{\\alpha_{2}\\right\\}$ tempt $\\{\\beta\\}$ implies, $\\{\\lambda\\alpha_{1}+(1-\\lambda)\\alpha_{2}\\}$ tempts $\\{\\beta\\}$ for all $\\lambda\\in[0,1]$.\nAXIOM 6: (Additivity of Menus: AoM) For $x,y$ finite, $\\beta\\in\\{\\beta\\}\\cup x\\cup y$ untempted in $\\{\\beta\\}\\cup x\\cup y$ and $y$ such that $\\left\\{\\beta\\right\\}\\succcurlyeq\\left\\{\\alpha\\right\\}$ for all $\\alpha\\in y$ $\\{\\beta\\}\\sim\\{\\beta\\}\\cup y$ implies $\\{\\beta\\}\\cup$ $x\\cup y\\sim\\{\\beta\\}\\cup x.$\nAXIOM 7: (Separability of Menus: SoM) If $x$ is finite and $\\beta\\notin B(x\\cup\\{\\beta\\})$ is tempted, $$ x\\cup\\{\\beta\\}\\sim x. $$\nDUAL SELF THEOREM:A binary relation $\\succcurlyeq$ satisfies Axioms 1, 2A-C, and 3-7 if and only if it admits an essentially unique dual self representation, so that the induced $U$ isof theform $$ U(x)=(1-\\rho)\\operatorname*{max}_{\\beta\\in x}u(\\beta)+\\rho\\operatorname*{max}_{\\beta\\in B_{\\nu}(x)}u(\\beta). $$\n\nThe text introduces a dual self representation of preferences, defined by a triple $(u, \nu, \rho)$, where $u$ and $\nu$ are continuous linear functions representing normative and alter ego preferences, respectively, and $\rho$ is the probability of succumbing to temptation. The representation is supported by a set of axioms ensuring completeness, transitivity, continuity, independence, and specific behavioral properties related to temptation."
  },
  {
    "qid": "econ-empirical-227-0-0-0",
    "question": "1) Derive the cost function $c(y, w)$ from the production function $f(x)$, explaining the regularity conditions required for this duality relationship.",
    "gold_answer": "To derive the cost function $c(y, w)$ from the production function $f(x)$:\n1. **Problem Setup**: Minimize total cost $w^{\\prime}x$ subject to the constraint $f(x) \\geq y$ and $x \\geq 0$.\n2. **Regularity Conditions**: The production function $f(x)$ must be continuous, strictly increasing, and strictly quasiconcave. These ensure the existence of a unique solution to the cost minimization problem.\n3. **Solution**: The cost function is given by $c(y, w) = \\min_{x} \\{w^{\\prime}x | f(x) \\geq y, x \\geq 0\\}$. Under the regularity conditions, the solution exists and is unique.\n4. **Shephard's Lemma**: If $c(y, w)$ is differentiable, the input demand functions can be derived as $x_i(y, w) = \\frac{\\partial c(y, w)}{\\partial w_i}$.",
    "question_context": "The textbook definition of a production function holds that it gives the maximum possible output which can be produced from given quantities of a set of inputs. Similarly, a cost function gives the minimum level of cost at which it is possible to produce some level of output, given input prices. Finally, a profit function gives the maximum profit that can be attained, given output price and input prices.\nFor each of the above functions, the concept of maximality or minimality is important. The word frontier may meaningfully be applied in each case because the function sets a limit to the range of possible observations.\nThe amounts by which a firm lies below its production and profit frontiers, and the amount by which it lies above its cost frontier, can be regarded as measures of inefficiency.\nConsider a firm employing $n$ inputs $x\\equiv(x_{1},....,x_{n})^{\\prime}$ , available at fixed prices $w\\equiv(w_{1},...,w_{n})^{\\prime}>0$ . to produce a single output $y$ that can be sold at fixed price $p{>}0$ . Efficient transformation of inputs into output is characterized by the production function $f(x)$ . which shows the maximum output obtainable from various input vectors.\nUnder certain regularity conditions an equivalent representation of efficient production technology is provided by the cost function $c(y,w){\\equiv}\\operatorname*{min}_{x}\\left\\{w^{\\prime}x{\\big|}f(x){\\geq}y,x{\\geq}0\\right\\}$ , which shows the minimum expenditure required to produce output $y$ at input prices $w$ .\nUnder certain regularity conditions a third equivalent representation of efficient production technology is provided by the profit function $\\begin{array}{r}{\\pi(p,w)\\equiv\\operatorname*{max}_{y,x}\\{p y-w^{\\prime}x\\big|f(x)}\\end{array}$ $\\geq y,x\\geq0,y\\geq0\\}$ , which shows the maximum profit available at output price $p$ and input prices $w$ .\n\nThe text discusses the fundamental concepts of production, cost, and profit functions, emphasizing their roles as frontiers that define the limits of possible observations due to their maximality or minimality properties. It also introduces the concept of inefficiency measurement relative to these frontiers."
  },
  {
    "qid": "econ-empirical-714-3-1-0",
    "question": "3) Identify and explain the two main limitations of using empirical serial correlation coefficients to analyze daily traded characteristics in the presence of information and liquidity frictions.",
    "gold_answer": "1. **Homoscedasticity Assumption**: The coefficients and Pearson tests are computed under homoscedasticity, leading to bias in the presence of heteroscedasticity.\n2. **Inability to Separate Effects**: The coefficients cannot disentangle the impacts of information and liquidity frictions or assess their interaction.",
    "question_context": "The use of the empirical serial correlation coefficients to understand the dynamic patterns of the daily traded characteristics in the context of information and liquidity frictions presents two main limitations.\nOur generalized MDHL framework improves the comprehension of daily time-series dynamics by putting enough structure on the data in order to separate the impacts of both information and liquidity shock time-persistent processes on daily trading characteristics.\n\nThe text discusses the limitations of using empirical serial correlation coefficients and introduces liquidity premium strategies based on the dynamic MDHL framework."
  },
  {
    "qid": "econ-empirical-1570-4-0-3",
    "question": "4) Table 4 shows that the implied minimum of the average cost curve occurs at a district size of about 30,000 pupils. Using the regression results, calculate the critical district size where average costs start increasing. What are the limitations of this conclusion given the sample data?",
    "gold_answer": "1. **Quadratic regression**: From Equation 2, $AC(M) = 14.6 - 0.0383M + 0.062 \\times 10^{-5}M^2$. \\n2. **First-order condition**: $dAC/dM = -0.0383 + 2 \\times 0.062 \\times 10^{-5}M = 0 \\implies M^* \\approx 30,900$. \\n3. **Limitations**: Only one district (Vancouver) exceeds this size (74,000 pupils), so the conclusion relies heavily on extrapolation.",
    "question_context": "one school in the sample has an enrolment greater than 2400 and only 2 have an enrolment exceeding 2200. Thus it cannot be concluded from these data that average costs do in fact increase for large schools.\nFor the repair and maintenance account, however, the quadratic form is slightly superior and the implied minimum of the average cost curve occurs at a school size of about 1100 students.\nThe corresponding total cost curve within a school is therefore of the form $C=\\alpha+\\beta N.$ which may be a good approximation when there exist substantial irreducible costs together with rising costs as school size increases.\nAssuming that these joint costs $(C)$ increase linearly with district size $(M)$ and that they contain a substantial irreducible component, the cost per student will be related to a constant term and $1/M$ as follows: $$ C/M=\\alpha_{0}+\\alpha_{1}/M+w, $$ $$ \\alpha_{0},\\alpha_{1}>0 $$ with $\\alpha_{0}$ and $\\alpha_{1}$ as the parameters to be estimated, $M$ district enrolment, and $w$ a random disturbance.\nThe first equation in Table 4 contains the ordinary least squares estimates of the coefficient of $1/M$ and of the constant term. The fact almost 80 percent of the variance is explained and that these two variables are significant lends support to the cost spreading hypothesis.\n\nThe text discusses the analysis of average cost curves for school districts, comparing linear, quadratic, and hyperbolic functional forms. It also examines the implications of district size on administrative costs, with a focus on the irreducible component and cost spreading."
  },
  {
    "qid": "econ-empirical-1015-3-0-4",
    "question": "5) State and interpret the Theorem characterizing the set of perfect equilibrium payoffs as $\\delta\\rightarrow1$. How does it relate to the folk theorem?",
    "gold_answer": "The Theorem states that any feasible payoff vector $(v_{1},v_{2})$ with $v_{i}>\\underline{{v}}_{i}=(1-w_{j})/2$ (for $i,j=1,2$, $i\\neq j$) is a perfect equilibrium payoff for $\\delta$ sufficiently close to 1. This aligns with the folk theorem, as it shows all individually rational and feasible payoffs can be sustained in equilibrium when players are patient, provided they exceed their punishment equilibrium payoffs.",
    "question_context": "Define an (optimal) punishment equilibrium for a player to be a perfect equilibrium in which this player receives his lowest perfect equilibrium payoff.\nThe strategy will specify one disagreement game outcome for all odd periods and another outcome for all even periods.\nPlayer 1's lowest disagreement payoff is player 1's minimax payoff in the disagreement game.\nPlayer 2's highest supportable disagreement payoff in an odd period is denoted by $w_{2}$, defined as $$w_{2}=\\operatorname*{max}_{a\\in A}\\left\\{u_{2}\\big(a\\big)-\\left[\\operatorname*{max}_{a_{1}^{\\prime}\\in A_{1}}u_{1}\\big(a_{1}^{\\prime},a_{2}\\big)-u_{1}(a)\\right]\\right\\}.$$\nProposition 2: For all $\\delta\\in(0,1)$, player 1's average perfect equilibrium payoffs in the negotiation game are not less than $(1-w_{2})/(1+\\delta)$, starting in an odd period, and not less than $[\\delta(1-w_{2})]/(1+\\delta)$, starting in an even period.\nProposition 3: There exists a $\\underline{{\\delta}}\\in(0,1)$ such that, $\\forall\\delta\\in(\\underline{{\\delta}},1)$, the offer $(1-w_{2})/(1+\\delta)$ is accepted in a perfect equilibrium of a negotiation game in an odd period, and the offer $\\delta(1-w_{2})/(1+\\delta)$ is accepted in a perfect equilibrium in an even period.\nPlayer 1's highest supportable disagreement payoff in an even period of player 2's punishment equilibrium is denoted by $w_{1}$, defined as $$w_{1}=\\operatorname*{max}_{a\\in A}\\left\\{u_{1}(a)-\\left[\\operatorname*{max}_{a_{2}^{\\prime}\\in A_{2}}u_{2}(a_{1},a_{2}^{\\prime})-u_{2}(a)\\right]\\right\\}.$$\nProposition 4: There exists a $\\underline{{\\delta}}\\in(0,1)$ such that, $\\forall\\delta\\in(\\underline{{\\delta}},1)$, the offer $(1+\\delta w_{1})/(1+\\delta)$ is accepted in a perfect equilibrium of a negotiation game in an odd period, and the offer $\\delta(\\delta+w_{1})/(1+\\delta)$ is accepted in a perfect equilibrium in an even period.\nTheorem: For any feasible payoff vector $(v_{1},v_{2})$ of the negotiation game such that $v_{i}>\\underline{{v}}_{i}$, for $i=1,2$, there exists $\\underline{{\\delta}}\\in(0,1)$ such that $\\forall\\delta\\in(\\underline{{\\delta}},1)$, $(v_{1},v_{2})$ is a perfect equilibrium payoff vector in the negotiation game with discount factor $\\delta$.\n\nThis section defines punishment equilibria and perfect equilibria in the negotiation game, focusing on player 1's punishment equilibrium. It derives lower bounds for player 1's perfect equilibrium payoffs and constructs equilibria where these bounds are attained under certain conditions."
  },
  {
    "qid": "econ-empirical-475-2-0-3",
    "question": "4) Discuss the practical implications of the finding that information criteria like BIC often select $k=0$ in large systems with MA(1) errors.",
    "gold_answer": "1. BIC's tendency to select $k=0$ in large systems with MA(1) errors exacerbates the pitfall in LR tests.  \n2. This leads to severe size distortion because the model fails to account for the $I(2)$ nature of the DGP.  \n3. Practitioners must supplement BIC with diagnostic tests for integration order and MA components to avoid misspecification.",
    "question_context": "There are some economic variables that can be considered to be $I(2)$ . For instance, prices $(p)$ are $I(2)$ if inflation rate is $I(1)$ . At the same time, money supply $(m)$ could be $I(2)$ if we consider real money balance $(m-p)$ to be $I(1)$ . Recent papers by Stock and Watson (1993), Haldrup (1994), Paruolo (1996), and Johansen (1995), p. 137) consider that variables like money stock and prices could be $I(2)$ .\nThe DGP in Table 3 is $$\\begin{array}{r}{\\varDelta^{2}y_{t}=(1-b B)e_{1t},}\\ {\\varDelta^{2}x_{t}=(1-b B)e_{2t},}\\end{array}$$ with $b\\neq1$ , and its VECM representation is $${\\binom{A y_{t}}{A x_{t}}}={\\binom{0}{0}}{\\binom{0}{0}}{\\binom{y_{t-1}}{x_{t-1}}}+{\\binom{1}{0}}{\\binom{\\varDelta y_{t-1}}{\\varDelta x_{t-1}}}+{\\binom{(1-b B)e_{1t}}{(1-b B)e_{2t}}}.$$\nProposition 5. Suppose $(y_{t}~x_{t})^{\\prime}$ are $I(2)$ processes generated from Eqs. (15) and (16). a. If $k=0$ , $\\widehat{\\lambda}_{1}$ does not converge to zero in probability. b. If $k\\geqslant1$ , $T\\hat{\\lambda}_{1}=\\mathbf{O}_{\\mathfrak{p}}(1)$ , i.e., $\\widehat{\\lambda}_{1}\\xrightarrow{\\mathtt{p}}0$ as $T\\to\\infty$ .\n\nThis section discusses the challenges in distinguishing between I(2) processes and I(1) processes with drift, particularly in the context of econometric modeling. It highlights the implications of mis-specifying the degree of integration in variables when using Johansen's LR tests."
  },
  {
    "qid": "econ-empirical-642-1-0-1",
    "question": "2) Prove Proposition 1: Show that under the given normal-inverse-gamma priors on $\\sigma_{i}^{2}$ and $A_{i,j}$, the matrix $\\widetilde{\\pmb{\\Sigma}}^{-1} = \\mathbf{A}'\\boldsymbol{\\Sigma}^{-1}\\mathbf{A}$ follows a Wishart distribution $\\mathcal{W}(\\nu_{0},\\mathbf{S}^{-1})$.",
    "gold_answer": "1. Assume the priors: $$\\sigma_{i}^{2} \\sim \\mathcal{IG}\\left(\\frac{\\nu_{0}+i-n}{2}, \\frac{s_{i}^{2}}{2}\\right), \\quad (A_{i,j}\\mid\\sigma_{i}^{2}) \\sim \\mathcal{N}\\left(0, \\frac{\\sigma_{i}^{2}}{s_{j}^{2}}\\right).$$ 2. The matrix $\\mathbf{A}$ is lower triangular with ones on the diagonal. The free elements $A_{i,j}$ are independent conditional on $\\sigma_{i}^{2}$. 3. The Jacobian of the transformation $\\widetilde{\\pmb{\\Sigma}}^{-1} = \\mathbf{A}'\\boldsymbol{\\Sigma}^{-1}\\mathbf{A}$ is proportional to $\\prod_{i=1}^{n} \\sigma_{i}^{-(n-i+1)}$. 4. The joint density of $\\mathbf{A}$ and $\\boldsymbol{\\Sigma}$ is: $$p(\\mathbf{A}, \\boldsymbol{\\Sigma}) \\propto \\prod_{i=1}^{n} \\sigma_{i}^{-\\nu_{0}-1} e^{-\\frac{s_{i}^{2}}{2\\sigma_{i}^{2}}} \\cdot \\prod_{i=2}^{n} \\prod_{j=1}^{i-1} \\left(\\frac{\\sigma_{i}^{2}}{s_{j}^{2}}\\right)^{-1/2} e^{-\\frac{A_{i,j}^{2} s_{j}^{2}}{2\\sigma_{i}^{2}}}.$$ 5. After simplification, the density of $\\widetilde{\\pmb{\\Sigma}}^{-1}$ is: $$p(\\widetilde{\\pmb{\\Sigma}}^{-1}) \\propto |\\widetilde{\\pmb{\\Sigma}}^{-1}|^{(\\nu_{0}-n-1)/2} e^{-\\frac{1}{2} \\text{tr}(\\mathbf{S}^{-1} \\widetilde{\\pmb{\\Sigma}}^{-1})},$$ which is the density of a Wishart distribution $\\mathcal{W}(\\nu_{0}, \\mathbf{S}^{-1})$.",
    "question_context": "We assume that the parameters are a priori independent across equations, that is, $\\textstyle p(\\pmb\\theta,\\pmb\\sigma^{2})=\\prod_{i=1}^{n}p(\\pmb\\theta_{i},\\sigma_{i}^{2})$.\nFurthermore, we consider a normal-inverse-gamma prior for each pair $(\\pmb\\theta_{i},\\sigma_{i}^{2}),i=1,\\dots,n$: $$\\left(\\pmb{\\theta}_{i}\\mid\\sigma_{i}^{2}\\right)\\sim\\mathcal{N}\\big(\\pmb{\\mathrm{m}}_{i},\\sigma_{i}^{2}\\pmb{\\mathrm{V}}_{i}\\big),\\quad\\sigma_{i}^{2}\\sim\\mathcal{Z}\\mathcal{G}(\\nu_{i},S_{i}),$$ and we write $(\\pmb\\theta_{i},\\sigma_{i}^{2})\\sim\\mathcal{N}\\mathbb{Z}\\mathcal{G}(\\mathbf{m}_{i},\\mathbf{V}_{i},\\nu_{i},S_{i})$.\nThe prior density of $(\\pmb\\theta,\\pmb\\sigma^{2})$ is given by $$p\\big(\\pmb{\\theta},\\pmb{\\sigma}^{2}\\big)=\\prod_{i=1}^{n}c_{i}\\big(\\sigma_{i}^{2}\\big)^{-(\\nu_{i}+1+\\frac{k_{i}}{2})}{\\bf e}^{-\\frac{1}{\\sigma_{i}^{2}}(S_{i}+\\frac{1}{2}(\\pmb{\\theta}_{i}-\\mathbf{m}_{i})/\\mathbf{V}_{i}^{-1}(\\pmb{\\theta}_{i}-\\mathbf{m}_{i}))},$$ where $c_{i}=(2\\pi)^{-\\frac{k_{i}}{2}}|\\mathbf{V}_{i}|^{-\\frac{1}{2}}S_{i}^{\\nu_{i}}/\\Gamma(\\nu_{i})$.\nProposition 1. Consider the following normal-inverse-gamma priors on the diagonal elements of $\\boldsymbol{\\Sigma}$ and the lower triangular elements of A: $$\\sigma_{i}^{2}\\sim\\mathbb{Z}\\mathcal{G}\\bigg(\\frac{\\nu_{0}+i-n}{2},\\frac{s_{i}^{2}}{2}\\bigg),\\quad i=1,\\dots,n,$$ $$\\bigl(A_{i,j}\\mid\\sigma_{i}^{2}\\bigr)\\sim\\mathcal{N}\\biggl(0,\\frac{\\sigma_{i}^{2}}{s_{j}^{2}}\\biggr),\\quad1\\leq j<i\\leq n,i=2,\\ldots,n.$$ Then $\\widetilde{\\pmb{\\Sigma}}^{-1}={\\bf A}^{\\prime}{\\pmb{\\Sigma}}^{-1}{\\bf A}$ has the Wishart distribution $\\widetilde{\\boldsymbol{\\Sigma}}^{-1}\\sim\\mathcal{W}(\\boldsymbol{\\nu}_{0},\\mathbf{S}^{-1})$.\nCorollary 1. Using the same notation as in Proposition 1, if $\\widetilde{\\pmb{\\Sigma}}\\sim\\mathcal{T W}(\\nu_{0},\\mathbf{\\vec{S}})$, then the implied priors on $A_{i,j}$ and $\\sigma_{i}^{2}$, $i=1,\\ldots,n$, $j=1,\\ldots,i-1$, are the normal-inverse-gamma distributions given in (6) and (7).\nCorollary 2. Suppose $\\widetilde{\\pmb{\\Sigma}}\\sim\\mathcal{T W}(\\nu_{0},\\mathbf{\\vec{S}})$ and let $\\pi$ denote a permutation of n elements with the associated permutation matrix $\\mathbf{P}_{\\pi}$. It follows that $\\mathbf{P}_{\\pi}\\tilde{\\Sigma}\\mathbf{P}_{\\pi}^{\\prime}\\sim\\mathcal{T}\\mathcal{W}(\\nu_{0},\\mathbf{S}_{\\pi})$, where $\\mathbf{S}_{\\pi} = \\mathrm{diag}(s_{\\pi(1)}^{2}, \\ldots, s_{\\pi(n)}^{2})$.\nCorollary 3. Suppose $\\tilde{\\Sigma}\\sim\\mathcal{T W}(\\nu_{0},\\mathbf{R})$, where $\\mathbf{R}$ is a symmetric positive definite matrix. Factor $\\widetilde{\\pmb{\\Sigma}}^{-1}={\\bf C}^{\\prime}{\\bf\\Sigma}^{-1}{\\bf C}$ and ${\\bf R}^{-1}={\\bf L}^{\\prime}{\\bf S}^{-1}{\\bf L}$, where C and L are lower triangular matrices with ones on the main diagonal. Then the implied priors on $\\mathbf{c}_{i}$ and $\\sigma_{i}^{2}$ are $$\\sigma_{i}^{2}\\sim\\mathbb{Z}\\mathcal{G}\\bigg(\\frac{\\nu_{0}+i-n}{2},\\frac{s_{i}^{2}}{2}\\bigg),\\quad i=1,\\dots,n,$$ $$(\\mathbf{c}_{i}\\mid\\sigma_{i}^{2})\\sim{\\mathcal{N}}(\\mathbf{l}_{i},\\sigma_{i}^{2}\\mathbf{L}_{1:i-1}^{\\prime}\\mathbf{S}_{1:i-1}^{-1}\\mathbf{L}_{1:i-1}),\\quad i=2,\\ldots,n.$$\n\nThis section introduces a conjugate prior on $(\\pmb\\theta,\\pmb\\sigma^{2})$ that allows differential treatment between prior variances on own lags versus others. The parameters are assumed to be a priori independent across equations, with a normal-inverse-gamma prior for each pair $(\\pmb\\theta_{i},\\sigma_{i}^{2})$. The prior density is derived, and hyperparameters are discussed in detail."
  },
  {
    "qid": "econ-empirical-694-2-0-2",
    "question": "3) Explain the indeterminacy of the Social Optimum in the quantity range of 10-12 units when commissions are excluded. How does the inclusion of commissions resolve this indeterminacy?",
    "gold_answer": "1. **Indeterminacy**: For quantities 10-12 units, the net social surplus (excluding commissions) is constant at $2.52. This implies multiple quantity levels yield the same surplus, making the Social Optimum indeterminate.\n2. **Resolution with Commissions**: Including commissions introduces a unique maximum at 15 units because commissions add a fixed per-unit benefit, breaking the tie among quantities 10-12.\n\nMathematically, the net social surplus \\( S \\) with commissions is:\n\\[ S(Q) = \\text{Trading Surplus}(Q) + 0.05 \\times Q - \\text{Damages}(Q) \\]\nThis function has a unique maximum at \\( Q = 15 \\).",
    "question_context": "In Phase I of our experiments we apply the law of supply and demand to predict that individual agents will only consider their private costs and benefits when deciding whether or not to trade (and thereby generate an externality on all). By inspection of Fig. I, we therefore predict a price of $2344 and a quantity of 24 at the Private Equilibrium outcome. Note that each subject receives a trading commission of $0.05 per trade in order to overcome presumed subjective costs of transacting. Therefore we predict that trading volume will be 24 instead of only 21 (the last three units do not generate any trading surplus before commissions).\nPlott (1983) defines the Socially Optimal outcome by the intersection of the Social Cost schedule excluding trading commissions with the Social Benefit (or Demand) schedule. Again, trading commissions merely play the role of encouraging marginal units to be traded. Thus he predicts a Social Optimum when price is $2.69 and quantity is 13 units.\nWhen we consider Coasian bargaining solutions, however, one cannot identify the Social Optimum quite so precisely. Table 2 presents the relevant calculations of the maximum social surplus attainable for different quantity levels. The ‘maximum′ trading surplus shown in Table 2 is found by simply pairing-off the lowest (private) cost sellers and the highest (private) value buyers. The price they trade at is of no relevance here.\nWe next compute the net social surplus, alternatively not including commissions or including them. In the first case we note that social surplus is indeed maximised at $2.52 when volume is 13 units, but that the same surplus is attained for 10, 11 or 12 units as well. Thus the Social Optimum is indeterminate in this quantity range. When we include commissions in our computation of net social surplus, however, we obtain a unique optimal quantity level of 15 units.\nOne issue of importance for our theoretical predictions, then, is whether or not commissions should be viewed as implicitly compensating individual traders for the 'onerous′ task of submitting bids and offers or not. If one accepts this interpretation, as the bulk of the experimental literature does, then the issue turns on the trading logistics used to generate trading surplus in our experiments.\nFinally we note that, providing that more than 6 units are sold, any reduction from 22 units (or 24 units if commissions are included) in Phase 1 will increase efficiency, whether or not it is due to Coasian bargaining. Such a reduction may, for example, be due to sellers organising as a collusive (uniform-price) monopoly when buyers behave non-cooperatively. In this scenario the profit maximising quantity is only 14 units, whether or not the seller cartel internalises the externality.\n\nThe text discusses the application of the law of supply and demand in experimental settings, focusing on private and social equilibria, trading commissions, and Coasian bargaining solutions. It also examines the impact of trading logistics on the interpretation of commissions and the efficiency outcomes in different market phases."
  },
  {
    "qid": "econ-empirical-68-2-0-3",
    "question": "4) Describe the King–money hybrid rule and prove Proposition 3 (Unique Implementation with a Hybrid Rule). Why does the hybrid rule succeed where the pure King rule fails?",
    "gold_answer": "1. **Hybrid Rule Definition**: The rule combines the King rule for $x_{t} \\in [\\underline{x}, \\bar{x}]$ and reversion to a money regime otherwise. The money regime ensures controllability. \n2. **Proof Sketch**: \n   - No equilibrium $x_{t}$ can lie outside $[\\underline{x}, \\bar{x}]$ because the money regime would force it back. \n   - For $x_{t} \\in [\\underline{x}, \\bar{x}]$, the King rule with $\\phi>1$ ensures explosive deviations are impossible. \n3. **Why It Succeeds**: The hybrid rule combines the determinacy of the money regime off-equilibrium with the flexibility of the King rule on-equilibrium, ensuring uniqueness.",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nThe public events that occur in a period are, in chronological order, $q_{t}=(x_{t};\\delta_{t};s_{t};y_{t},\\pi_{t})$.\nA sophisticated equilibrium given the policies here is a collection of strategies $(\\sigma_{x},\\sigma_{g})$ and allocation rules $(\\sigma_{y},\\sigma_{\\pi})$ such that (i) given any history $h_{t-1}$, the continuation outcomes $\\{a_{r}(s^{r}\\mid h_{t-1};\\sigma)\\}$ induced by $\\sigma$ constitute a continuation competitive equilibrium and (ii) given any history $h_{y t}$, so do the continuation outcomes $\\{a_{r}(s^{r}\\mid h_{y t};\\sigma)\\}$.\nThe basic idea behind our sophisticated policy construction is that the central bank starts by picking any desired competitive equilibrium allocations and sets its policy on the equilibrium path consistent with them.\nLEMMA 2 (Controllability of Best Responses with One-Period Price-Setting). For any history $(h_{t-1},\\hat{x}_{t})$, if the central bank chooses the money regime, then there exists a choice for money growth $\\mu_{t}$ such that $\\hat{x}_{t}\\neq E[\\pi_{t}(\\hat{h}_{y t})+\\gamma y_{t}(\\hat{h}_{y t})]$, where $h_{y t}=(h_{t-1},\\hat{x}_{t},M,\\mu_{t})$.\nPROPOSITION 1 (Unique Implementation with Money Reversion). Any competitive equilibrium outcome in which the central bank uses interest rates as its instrument can be implemented as a unique equilibrium with sophisticated policies with one-period reversion to a money regime.\nPROPOSITION 2 (Indeterminacy of Equilibrium under the King Rule). Suppose the central bank sets interest rates $i_{t}$ according to the simple economy’s King rule (19). Then any of the continuum of sequences indexed by the initial condition $x_{0}$ and the parameter $c$ that satisfies $x_{t+1}=i_{t}+c\\eta_{t},~\\pi_{t}=x_{t}+\\kappa(1+\\psi c)\\eta_{t},$ $\\mathrm{and}~y_{t}=(1+\\psi c)\\eta_{t}$ is a sophisticated outcome.\nPROPOSITION 3 (Unique Implementation with a Hybrid Rule). In the simple economy, the King–money hybrid rule with $\\phi>1$ uniquely implements any bounded competitive equilibrium.\n\nThe text introduces the concept of sophisticated equilibrium, where allocations, prices, and policies are functions of the history of exogenous events, aggregate private actions, and central bank policies. It contrasts this with unsophisticated equilibrium and discusses implementation strategies, including reversion to money regimes and hybrid rules."
  },
  {
    "qid": "econ-empirical-1361-3-0-1",
    "question": "2) Prove that the observed behavior (dominant weighting of high-risk assessments) violates the Savage axioms of rational choice under uncertainty. Use the framework of $\\alpha$-maxmin expected utility.",
    "gold_answer": "1. Savage axioms require probabilistic sophistication:\n   \\[ U(f) = \\int u(f(s))d\\pi(s) \\]\n2. $\\alpha$-maxmin model:\n   \\[ U(f) = \\alpha \\min_{p\\in C} E_p[u(f)] + (1-\\alpha)\\max_{p\\in C} E_p[u(f)] \\]\n3. Observed behavior corresponds to $\\alpha \\to 1$, violating:\n   - State independence\n   - Sure-thing principle\n   when $C$ changes with risk magnitude.",
    "question_context": "Individuals may reasonably place different information weights on different sources of information. However, the greater weight on the high risk information in situations of competing information sources could not be reconciled with differential weights on the informational source.\nThe credibility weight on the source varies depending on whether the source is providing high risk or low risk information and on the other party providing information.\nThe diversity of risk information introduced patterns that were altogether inconsistent with a conventional Bayesian learning framework. When differing risk judgements were offered by different parties, the high risk assessment was accorded a dominant role.\nThe practice of government agencies to base risk regulation on upper bounds of $95\\%$ confidence limits and, in some cases, the maximum risk assessment for chemical exposures may reflect the policy implementation of this class of perceptional biases.\n\nThe text discusses how individuals deviate from standard Bayesian learning models when processing risk information, particularly overweighting high-risk scenarios. This behavior is inconsistent with rational updating of beliefs and leads to alarmist responses."
  },
  {
    "qid": "econ-empirical-1553-1-0-0",
    "question": "1) Prove that the dynamic binary choice model $y_{n} = I(\\sum_{j=1}^{p} \\rho_{j} y_{n-j} + \\eta_{n} > 0)$ satisfies the near epoch dependence condition as defined in Definition 1, assuming $\\eta_{n}$ is strong mixing and strictly stationary.",
    "gold_answer": "To prove near epoch dependence, follow these steps:\n1. **Define the conditional expectation**: \n   $$E(y_n | \\eta_{n-m}, \\dots, \\eta_n) = P\\left(\\sum_{j=1}^p \\rho_j y_{n-j} + \\eta_n > 0 | \\eta_{n-m}, \\dots, \\eta_n\\right).$$\n2. **Bound the difference**: Using the Lipschitz property of the indicator function and the mixing condition, show that:\n   $$\\mathbb{E}|y_n - E(y_n | \\eta_{n-m}, \\dots, \\eta_n)|^2 \\leq C \\exp(-C_2 m).$$\n3. **Apply Theorem 1**: The geometric decay of $\\nu(m)$ follows from the contraction mapping argument and the mixing properties of $\\eta_n$.",
    "question_context": "A key aspect of the analysis below is to show that $y_{n}$ satisfies the appropriate 'fading memory' property when generated through a general dynamic binary choice model with regressors and possibly correlated errors.\nThe 'fading memory' property that we will prove for $y_{n}$ is that of near epoch dependence. The idea of the proof is similar to that of proofs for showing fading memory properties of processes $y_{n}$ of the form $y_{n}=f(y_{n-1})+\\epsilon_{n}$, where $f(.)$ is such that $|f(x)-f(y)|\\leq L|x-y|$ for some $L<1$.\nDEFINITION 1. Random variables $y_{n}$ are called near epoch dependent on $\\eta_{n}$ if $\\operatorname*{sup}_{n\\in\\mathbb{N}}\\mathbb{E}|y_{n}-\\operatorname{E}(y_{n}|\\eta_{n-m},\\eta_{n-m+1},\\dots,\\eta_{n})|^{2}=\\nu(m)^{2}\\to0\\quad\\mathrm{as~}m\\to\\infty.$\nTHEOREM 1. Consider the model $y_{n}=I(\\Sigma_{j=1}^{p}\\rho_{j}y_{n-j}+\\eta_{n}>0)$. Let $\\eta_{n}$ be strong mixing and strictly stationary. Assume that there is some $\\delta>0$ and a positive integer $K$ such that $P(\\phi_{\\operatorname*{max}}+\\underset{i=1,\\dots,p}{\\operatorname*{max}}\\eta_{n-i}>0|\\eta_{n-p-K},\\eta_{n-p-K-1},\\dots)-P(\\phi_{\\operatorname*{min}}+\\underset{i=1,\\dots,p}{\\operatorname*{min}}\\eta_{n-i}>0|\\eta_{n-p-K},\\eta_{n-p-K-1},\\dots)<1-\\delta.$ Then there exists a strictly stationary solution $y_{n}=f(\\eta_{n},\\eta_{n-1},...)$ to the model that is near epoch dependent on $\\eta_{n}$, and its near epoch dependence sequence $\\nu(.)$ satisfies $\\nu(m)\\leq C_{1}\\exp(-C_{2}m)$ for positive constants $C_{1}$ and $C_{2}$.\nTHEOREM 2. Suppose that $y_{n}=f(\\eta_{n},\\eta_{n-1},...)$ is a sequence of 0/1-valued random variables that is near epoch dependent on $(u_{n},x_{n}^{\\prime})$ with near epoch dependence coefficients $\\nu(m)$, where $\\eta_{n}=\\gamma\\prime x_{n}+u_{n}$ and $(u_{n},x_{n}^{\\prime})^{\\prime}$ is strong mixing with mixing coefficients $\\alpha(m)$. Then $(y_{n},x_{n}^{\\prime})^{\\prime}$ is strong mixing with strong mixing coefficients $C(\\upnu(m)+\\alpha(m))$ for some $C>0$.\n\nThe section discusses the 'fading memory' property of the dynamic binary choice model, focusing on near epoch dependence and strong mixing conditions. It provides theoretical foundations for proving stationarity and asymptotic properties of the estimator."
  },
  {
    "qid": "econ-empirical-1398-0-1-1",
    "question": "6) What are the potential reasons for the Argentine government's manipulation of official inflation statistics, and what are the consequences?",
    "gold_answer": "Potential reasons and consequences include:\n1. **Reasons**: Lower interest payments on inflation-linked bonds; avoiding redistribution of excess tax income to provinces.\n2. **Consequences**: Introduces economic uncertainty; undermines trust in official statistics; affects poverty and GDP calculations.",
    "question_context": "The implications for other statistics are significant. For example, using an online-adjusted cost for the subsistence-level CBA basket, the share of the population in extreme poverty during the first quarter of 2011 rises from 2.5% in official estimates to 6.69%.\nSimilarly, poverty estimates are 9.9% in official data, but rise to 25.9% with adjusted price series. The implications for real GDP are equally impressive. If the GDP deflator had behaved like the online index since 2007, the real GDP annual growth rate would have been just 0.5% by March 2011, much lower than the 10% officially reported.\n\nThe paper discusses the broader implications of the discrepancies between online and official inflation estimates, particularly for poverty and GDP calculations."
  },
  {
    "qid": "econ-empirical-92-3-1-1",
    "question": "4) Propose an econometric model to disentangle selection and behavioral effects in the California data, specifying the fixed-effects structure and identifying assumptions.",
    "gold_answer": "Model: \\( Y_{ist} = \\alpha + \\beta U_{st} + \\gamma X_{ist} + \\mu_i + \\delta_s + \\lambda_t + \\epsilon_{ist} \\), where:  \n- \\( \\mu_i \\): Mother fixed effects (controls selection).  \n- \\( U_{st} \\): County-year unemployment rate.  \n- **Assumption**: No time-varying unobservables correlated with \\( U_{st} \\) and \\( Y_{ist} \\).",
    "question_context": "For Whites we find an increase in the incidence of low birth weight, and a significant increase in the number of prenatal care visits. For Blacks we find (insignificant) reductions in low birth weight, and a significant increase in the use of prenatal care.\nAmong White mothers negative selection offsets some of the behavioral improvements in times of high unemployment. Instead, for Blacks, selection is positive in times of high unemployment.\n\nThis section examines whether countercyclical health improvements are due to behavioral changes or selection, using California Birth Certificate data (1990–2000) with mother fixed effects to isolate behavioral effects."
  },
  {
    "qid": "econ-empirical-374-4-0-2",
    "question": "3) Prove that $\\frac{1}{T^{1.5+\\delta}}\\sum_{t=2}^{T}z_{t-1}x_{t-1}\\xrightarrow{p}0$ given the weak convergence $\\frac{1}{T^{1/2+\\vartheta+\\delta}}\\sum_{j=1}^{[r T]}z_{j}\\Rightarrow Z(r)$.",
    "gold_answer": "1. Start with the weak convergence $\\frac{1}{T^{1/2+\\vartheta+\\delta}}\\sum_{j=1}^{[r T]}z_{j}\\Rightarrow Z(r)$.\n2. This implies that the partial sums of $z_{j}$ are of order $O_{p}(T^{1/2+\\vartheta+\\delta})$.\n3. The term $\\frac{1}{T^{1.5+\\delta}}\\sum_{t=2}^{T}z_{t-1}x_{t-1}$ can be rewritten as $\\frac{1}{T^{1.5+\\delta}}\\sum_{t=2}^{T}z_{t-1}x_{t-1} = \\frac{1}{T^{1.5+\\delta}} \\cdot O_{p}(T^{1+\\vartheta+\\delta})$.\n4. Since $\\vartheta < 1/2$, the exponent $1 + \\vartheta + \\delta - (1.5 + \\delta) = \\vartheta - 0.5 < 0$, ensuring the term converges to zero in probability.",
    "question_context": "Note that $\\begin{array}{r}{\\left(\\frac{1}{\\sqrt{T}}\\sum_{j=1}^{\\lceil r T\\rceil}u_{t},\\frac{1}{\\sqrt{T}}\\sum_{j=1}^{\\lceil r T\\rceil}v_{t}\\right)^{\\prime}\\implies(\\sigma_{u}W_{u}(r),\\sigma_{v}W_{v}}\\end{array}$ $(r))^{\\prime}$ where the r.h.s. is a bivariate Brownian motion with covariance $\\boldsymbol{\\Sigma}$ , and $\\begin{array}{r}{\\frac{1}{\\sqrt{T}}X_{[r T]}\\implies\\sigma_{v}J_{c}\\left(r\\right)}\\end{array}$ ; see Phillips and Durlauf (1986) and Phillips (1987).\nWe show that each of the variables satisfies Assumption 3 and the condition $T^{-1-2\\delta}\\sum_{t=2}^{T}z_{t-1}^{2}\\Rightarrow V_{z}$ , which implies that the discussed variables o bey Assumption 2. We take the route over Assumption 3 in order to avoid proving very similar results twice. Note that Assumption 3 involves the additional parameter $\\vartheta<1/2$ which does not play an explicit role for the VA procedure, but simplifies some of the derivations significantly.\nFor all four choices of $z_{t}$ , the condition $\\begin{array}{r}{\\frac{1}{T^{1/2+\\vartheta+\\delta}}\\sum_{j=1}^{[r T]}z_{j}\\Rightarrow Z(r)}\\end{array}$ implies $\\begin{array}{r}{\\frac{1}{T^{1.5+\\delta}}\\sum_{t=2}^{T}z_{t-1}x_{t-1}\\xrightarrow{p}0}\\end{array}$ Concretely, it is shown in the proof of Theorem 2 that $$ \\frac{1}{T^{\\vartheta+\\delta+1}}\\sum_{t=2}^{T}z_{t-1}x_{t-1}\\Rightarrow R_{z x}^{c}, $$ the key ingredient in establishing convergence to $R_{z x}^{c}$ being precisely the weak convergence $\\begin{array}{r}{\\frac{1}{T^{1/2+\\vartheta+\\delta}}\\sum_{j=1}^{[r T]}z_{j}\\Rightarrow Z(r)}\\end{array}$ . Then, given that $\\begin{array}{r}{\\vartheta<1/2,\\frac{1}{T^{1.5+\\delta}}\\sum_{t=2}^{T}z_{t-1}x_{t-1}{\\xrightarrow{p}}0}\\end{array}$ follows immediately.\n\nThe text discusses the convergence properties of partial sums involving bivariate Brownian motion and the implications for various assumptions in econometric models."
  },
  {
    "qid": "econ-empirical-98-3-0-2",
    "question": "3) Calculate the percentage reduction in jail population required to achieve the same crime reduction as the second quintile judges, using the algorithm's contraction curve (Table III).",
    "gold_answer": "1. **Judge's Jail Increase**: Second quintile judges increase jail rate by $\\Delta Jail = 0.066$.\\n2. **Algorithm's Jail Increase**: To achieve the same crime reduction ($\\Delta Crime = -0.099$), the algorithm requires $\\Delta Jail = 0.028$.\\n3. **Reduction Calculation**: $(0.066 - 0.028) / 0.066 \\approx 57.6\\%$ reduction in jail population.",
    "question_context": "The first assumption we will make is that judges draw from the same distribution of defendants. This assumption can be implemented in the NYC data by taking advantage of the fact that we have (anonymous) judge identifiers, together with the fact that conditional on borough, court house, year, month, and day of week, average defendant characteristics do not appear to be systematically related to judge leniency rates within these cells.\nWe carry out a permutation test that focuses on the projection of our outcome $Y$ (in this case FTA) onto the baseline characteristics, which essentially creates an index of baseline defendant characteristics weighted in proportion to the strength of their relationship with the outcome.\nThe other thing we need for this design to work are differences in judge leniency within cells. As in past research, we see this in our data as well. The most lenient quintile judges release $82.9\\%$ of defendants. Relative to the most lenient judge quintile, less lenient quintiles have average release rates that are 6.6, 9.6, 13.5, and 22.3 percentage points lower, respectively.\nOur assumption about similar capacity to select on unobservables can then be written as: $$(\\forall l,x):\\bar{z}^{1}(x,l)=\\bar{z}^{2}(x,l).$$\nRelative to her own choices, this rule changes judge 2’s payoff by: $$\\Pi^{2}(\\rho^{C})-\\Pi^{2}(\\rho^{2})=a_{2}(E[y|\\rho^{C}=1]-E[y|\\rho^{2}=1])-b_{2}(\\bar{R}^{2}-\\bar{R}^{C}).$$\n\nThe section discusses overcoming the challenge of not knowing judges' preferences by leveraging differences in judge leniency rates. It outlines assumptions about the data-generating process and judge behaviors, and describes empirical tests to validate these assumptions."
  },
  {
    "qid": "econ-empirical-1522-1-1-1",
    "question": "2) Show how the rank-dependent utility (RDU) model accommodates the common ratio effect, which expected utility (EU) cannot.",
    "gold_answer": "1. The common ratio effect involves preferences that violate the independence axiom of EU.\n2. RDU allows for non-linear weighting functions $w(p_{j})$, which can capture deviations from linear probability weighting.\n3. For example, a concave $w(p_{j})$ can explain why individuals overweight small probabilities, leading to preferences inconsistent with EU but consistent with observed behavior.",
    "question_context": "Rank-dependent utility (RDU) holds if the preference relation is represented by the function $V(P)=u(x_{0})+\\sum_{j=1}^{n}w(p_{j})\\big[u(x_{j})-u(x_{j-1})\\big]$, where the utility function $u:X\\to\\mathbb{R}$ agrees with $\\succcurlyeq$ on $X$ , and the weighting function $w:[0,1]\\rightarrow$ [0, 1] is strictly increasing and continuous with $w(0)=0$ and $w(1)=1$ . Under RDU utility is cardinal and the weighting function is uniquely determined. If the weighting function is linear then RDU reduces to expected utility (EU). A concave weighting function resembles probabilistic risk seeking behavior while a convex one resembles probabilistic risk aversion.\n\nThis section discusses the common ratio and common consequence effects, which challenge the expected utility theory. It introduces the rank-dependent utility (RDU) model and provides preference foundations for specific RDU models."
  },
  {
    "qid": "econ-empirical-80-5-0-0",
    "question": "1) Derive the seller's profit function explicitly, incorporating production costs and decision costs as functions of the conversion rate. Show how the optimal conversion rate is determined.",
    "gold_answer": "1. Seller's profit function: \\( \\pi = p - c_p - c_d(\\alpha) \\), where \\( p \\) is price, \\( c_p \\) is production cost, and \\( c_d(\\alpha) \\) is decision cost for conversion rate \\( \\alpha \\).\n2. Decision costs are tabulated: \\( c_d(\\alpha) = \\{0,1,2,4,6,8,10,12,15,18\\} \\) for \\( \\alpha = \\{0.1,0.2,...,1.0\\} \\).\n3. Optimal \\( \\alpha \\) maximizes \\( \\pi \\) subject to \\( \\alpha \\) in feasible set. Solve: \\( \\alpha^* = \\arg\\max_{\\alpha} (p - c_p - c_d(\\alpha)) \\).",
    "question_context": "Each trading day lasts three minutes. Every buyer can offer a price that will be relayed to us by telephone. We list these offers on the blackboard, and you can accept one of these offers. If, e.g., a price of 50 is offered and you as seller number 5 want to accept this offer, you just say: \"Number 5 sells for 50.\" In this case, the transaction is concluded.\nYou can sell one unit of the good on each trading day. Therefore, the trading day ends for you after the acceptance of an offer. Note also that each buyer can buy, at most, one unit of the good per trading day.\nThe profit of a buyer (measured in experimental money) is the difference between the reselling price and the price at which he has bought the good from you. If 'your' buyer has bought the good for 205 and the reselling price is 405, he makes a profit of $405{-}205=$ 200 (measured in experimental money).\nYour profit paid in AS is given by the formula: profit $\\asymp$ price - production costs - decision costs. If, for example, you sell your good for 175, while your production costs are 100, and you choose a conversion rate of 0.6 which leads to decision costs of 5, your profit is given by $175-100-5=70.$\nFeasible conversion rates (CR) and associated decision costs (DC): CR: 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0; DC: 0, 1, 2, 4, 6, 8, 10, 12, 15, 18.\n\nThe market involves trading a homogeneous good with specific rules for sellers and buyers, including trading periods, offer mechanisms, and profit calculations."
  },
  {
    "qid": "econ-empirical-1769-0-0-0",
    "question": "1) Derive the utility function for an individual where utility depends on both absolute consumption \\( C_i \\) and relative consumption \\( \\frac{C_i}{C_j} \\), where \\( C_j \\) is the consumption of others. Show how this leads to a conflict between individual and social welfare.",
    "gold_answer": "1. Define the utility function: \\( U_i = U(C_i, \\frac{C_i}{C_j}) \\).\n2. Assume \\( \\frac{\\partial U_i}{\\partial C_i} > 0 \\) and \\( \\frac{\\partial U_i}{\\partial (C_i/C_j)} > 0 \\).\n3. Individuals maximize \\( U_i \\) by increasing \\( C_i \\), leading to higher \\( C_j \\) as others respond.\n4. Social welfare \\( W = \\sum U_i \\) may decrease due to excessive focus on relative consumption, creating a divergence between individual and social optima.",
    "question_context": "In traditional economic models, individual utility depends only on absolute consumption. These models lie at the heart of claims that pursuit of individual self-interest promotes aggregate welfare. Recent years have seen renewed interest in economic models in which individual utility depends not only on absolute consumption, but also on relative consumption. In contrast to traditional models, these models identify a fundamental conflict between individual and social welfare.\nThe conflict stems from the fact that concerns about relative consumption are stronger in some domains than in others. The disparity gives rise to expenditure arms races focused on positional goods-those for which relative position matters most. The result is to divert resources from nonpositional goods, causing welfare losses.\nI use the term positional good to denote goods for which the link between context and evaluation is strongest and the term nonpositional good to denote those for which this link is weakest. In terms of the two thought experiments, housing is thus a positional good, vacation time a nonpositional good. The point is not that absolute house size and relative vacation time are of no concern. Rather, it is that positional concerns weigh more heavily in the first domain than in the second.\n\nThe text discusses the concept of positional and nonpositional goods, where individual utility depends not only on absolute consumption but also on relative consumption. This leads to expenditure arms races focused on positional goods, diverting resources from nonpositional goods and causing welfare losses."
  },
  {
    "qid": "econ-empirical-1827-0-1-0",
    "question": "5) Why are adaptive estimators particularly useful in financial econometrics? Discuss their advantages over ARCH models.",
    "gold_answer": "**Advantages**:\n1. **Robustness**: Adaptive estimators do not assume a parametric error distribution (e.g., Gaussian), unlike ARCH.\n2. **Efficiency**: Achieve asymptotic efficiency without specifying higher moments.\n3. **Misspecification Immunity**: ARCH models are biased if the conditional heteroscedasticity form is incorrect, whereas adaptive estimators are robust.\n\n**Limitation**: ARCH models parsimoniously capture volatility clustering but fail to adapt to asymmetric errors.",
    "question_context": "Adaptive estimators are especially attractive for use in empirical financial models. These situations are characterized by significant departures from normality and with the quantity of data available, precise estimation is an important goal.\n\nThis section discusses extensions of adaptive estimators to non-symmetric error distributions and multivariate models, along with empirical applications in financial econometrics."
  },
  {
    "qid": "econ-empirical-1420-1-1-2",
    "question": "7) Discuss the implications of the Anscombe-Aumann framework for CEU predictions in compound risk lotteries.",
    "gold_answer": "1. **Anscombe-Aumann**: CEU predicts the same CEs for all compound lotteries due to the reduction of compound lotteries axiom (RCLA).\n2. **Savagian Domain**: CEU is silent on compound lottery evaluation without RCLA.",
    "question_context": "Under CEU, the utility for an ambiguous lottery $L_{i}^{A}$ is given by $U_{\\mathrm{CEU}}\\big(L_{i}^{A}\\big)=\\nu(R_{L_{i}^{A}})u(w)$, where $\\nu$ is a capacity function.\nFor a convex $\\nu$, CEU exhibits global ambiguity aversion, meaning (50) is preferred to any ambiguity lottery.\nThe MEU model evaluates $L_{i}^{A}$ in terms of the expected utility of the worst prior in a convex set of priors $\\pi_{L_{i}^{A}}$.\n\nThis section analyzes how various models of ambiguity predict individual choice in the setting of partial ambiguity and compound risk."
  },
  {
    "qid": "econ-empirical-1219-4-1-2",
    "question": "7) Formulate a mathematical model to represent the relationship between AFDC benefit differentials and county expenditures, incorporating the temporal and spatial dimensions discussed in the text.",
    "gold_answer": "1. Let \\( E_{it} \\) be the logarithm of real AFDC expenditures per capita in county \\( i \\) at time \\( t \\).\n2. Let \\( \\Delta B_{it} \\) be the AFDC benefit difference (in hundreds of dollars) between the nearest state and own state for county \\( i \\) at time \\( t \\).\n3. Let \\( D_{i} \\) be an indicator for whether county \\( i \\) is within 25 miles of another state.\n4. The model can be written as:\n   \\[ E_{it} = \\alpha + \\beta_1 \\Delta B_{it} + \\beta_2 (\\Delta B_{it} \\times D_{i}) + \\beta_3 (\\Delta B_{it} \\times D_{i} \\times T_{t}) + \\gamma X_{it} + \\epsilon_{it} \\]\n   where \\( T_{t} \\) is a time trend or period dummy (e.g., 1980s), \\( X_{it} \\) includes control variables, and \\( \\epsilon_{it} \\) is the error term.\n5. The coefficient \\( \\beta_2 \\) captures the additional effect of benefit differentials for nearby counties, and \\( \\beta_3 \\) captures the temporal evolution of this effect.",
    "question_context": "The first row of Table 5 uses the baseline specification from Column 1 of Table 3. The coefficient on the benefit differential for the 1970s is -0.0528 and the coefficient for the 1980s is -0.0613, suggesting little difference in the migration effect for the 1970s and the 1980s.\nThe second row of Table 5 uses the specification from Column 2 of Table 4. The coefficient on the interaction of benefit difference with nearest state and the indicator for nearest state is within 25 miles is -0.0382 for the 1970s and is -0.0882 for the 1980s, suggesting much more sizeable effects in the 1980s. The difference in the coefficients is statistically significant.\nThe spiking of the effect during a recession reflects both the increased incentives to migrate for welfare benefits during the recession and, perhaps more importantly, the participation effects of an economic downturn. If a larger welfare-prone population has accumulated over time in the border regions of a high-benefit state, the welfare participation rate of this welfare-prone group is going to be highest during an economic recession, increasing the magnitude of the participation differential between border and interior regions of the state.\n\nThis section examines how the effects of cross-state AFDC benefit differentials on county AFDC expenditures evolved over the 1970s and 1980s, including the impact of economic recessions."
  },
  {
    "qid": "econ-empirical-113-1-1-0",
    "question": "5) Propose a structural equation model for the authors' IV regression, explicitly stating the first and second stages. How does the exclusion restriction apply here?",
    "gold_answer": "1. **First Stage**: \\( D_i = \\alpha + \\beta Z_i + \\gamma X_i + \\epsilon_i \\), where \\( D_i \\) is direct democracy strength, \\( Z_i \\) instruments.  \n2. **Second Stage**: \\( Y_i = \\delta + \\lambda \\hat{D_i} + \\theta X_i + u_i \\), where \\( Y_i \\) is spending.  \n3. **Exclusion**: \\( Z_i \\) affects \\( Y_i \\) only via \\( D_i \\) (e.g., neighbor reforms influence local spending only by changing direct democracy rules).",
    "question_context": "We control for permanent differences across cantons using fixed effects and construct a voter preference measure from federal ballots. Instruments include constitutional initiative barriers and neighbor canton reforms.\nThe budget referendum is coded as 1 if mandatory, 0 otherwise. Signature requirements for initiatives are measured as a percentage of the eligible population.\n\nThe study leverages historical variation in Swiss cantons' direct democratic institutions to estimate causal effects. It introduces novel instruments and preference measures derived from federal ballots."
  },
  {
    "qid": "econ-empirical-489-3-0-1",
    "question": "2) Explain the significance of the uniform continuity property in Assumption 4.1 for the functional response data.",
    "gold_answer": "The uniform continuity property ensures that:\n1. The functional response data $Y_u$ is well-behaved uniformly over $u \\in \\mathcal{U}$ and $P \\in \\mathcal{P}$.\n2. The supremum of $|Y_u - Y_{\\bar{u}}|$ converges to zero as $d_U(u, \\bar{u}) \\to 0$, which is crucial for the Donsker property of the set of functions $(\\psi_u^\\rho)_{u \\in \\mathcal{U}}$.\n3. The boundedness condition $\\sup_{P \\in \\mathcal{P}} \\mathbb{E}_P \\sup_{u \\in \\mathcal{U}} |Y_u|^{2+c} < \\infty$ guarantees the existence of moments, which is necessary for the application of central limit theorems.",
    "question_context": "Consider fixed sequences of numbers $\\delta_{n}\\searrow0,\\epsilon_{n}\\searrow0,\\Delta_{n}\\searrow0,$ at a speed at most polynomial in $n$ (e.g., $\\delta_{n}\\geq1/n^{c}$ for some $c>0$ $\\ell_{n}:=\\log n$ , and positive constants $c,C_{:}$ and $c^{\\prime}<1/2$ . These sequences and constants will not vary with $P$ . The probability $P$ can vary in the set $\\mathcal{P}_{n}$ of probability measures, termed 'data-generating processes,' where $\\mathcal{P}_{n}$ is typically a set that is weakly increasing in $n$ , that is, $\\mathcal P_{n}\\subseteq\\mathcal P_{n+1}$ .\nAssUMPTION 4.1—-Basic Assumptions: (i) Consider a random element $W$ with values in a measure space $(\\mathcal{W},\\mathcal{A}_{\\mathcal{W}})$ and law determined by a probability measure $\\boldsymbol{P}\\in\\mathcal{P}_{n}$ The observed data $((W_{u i})_{u\\in\\boldsymbol{\\mathcal{U}}})_{i=1}^{n}$ consist of $\\boldsymbol{n}$ i.i.d. copies of a random element $(W_{u})_{u\\in{\\cal U}}=$ $((Y_{u})_{u\\in{\\cal U}},D,Z,X)$ , where $u$ is a Polish space equipped with its Borel sigma-field and $(Y_{u},D,Z,X)\\in\\mathbb{R}^{3+d_{X}}$ .Each $W_{u}$ is generated via a measurable transform $t(W,u)$ of $W$ and $\\pmb{u}$ namely the map $t:\\mathcal{W}\\times\\mathcal{U}\\longmapsto\\mathbb{R}^{3+d_{X}}$ is measurable, and the map can possibly depend on $P$ .\nUnder the stated assumptions, the empirical reduced-form process $\\hat{Z}_{n,P}=\\sqrt{n}(\\hat{\\rho}-\\rho)$ defined by (3.16) obeys the following relations. We recall definitions of convergence uniformly in $\\pmb{P}\\in\\mathcal{P}_{n}$ in Appendix A.\n\nThis section discusses the estimation and inference on local treatment effects functionals, focusing on the conditions and assumptions necessary for valid inference in high-dimensional settings."
  },
  {
    "qid": "econ-empirical-842-0-0-1",
    "question": "2) Show how the unconditional joint distribution $f(\\varepsilon_{1}, \\varepsilon_{2})$ becomes a bivariate normal when $v \\sim N(0,1)$, and derive its covariance matrix.",
    "gold_answer": "1. When $v \\sim N(0,1)$, the convolution of $\\rho_{1}v + \\gamma_{1}u_{1}$ and $\\rho_{2}v + \\gamma_{2}u_{2}$ is bivariate normal. \\n2. Variances: $$Var(\\varepsilon_{1}) = \\rho_{1}^{2} + \\gamma_{1}^{2}$$ $$Var(\\varepsilon_{2}) = \\rho_{2}^{2} + \\gamma_{2}^{2}$$ \\n3. Covariance: $$Cov(\\varepsilon_{1}, \\varepsilon_{2}) = \\rho_{1}\\rho_{2}$$ \\n4. Thus: $$(\\varepsilon_{1}, \\varepsilon_{2}) \\sim N\\left(0, \\begin{bmatrix} \\rho_{1}^{2}+\\gamma_{1}^{2} & \\rho_{1}\\rho_{2} \\\\ \\rho_{1}\\rho_{2} & \\rho_{2}^{2}+\\gamma_{2}^{2} \\end{bmatrix}\\right)$$",
    "question_context": "The process generating the outcomes in this paper is given by: $$\\begin{array}{r l}&{y_{1}^{*}=\\alpha_{0}+\\alpha_{1}x+\\varepsilon_{1},}\\ &{y_{2}=\\beta_{0}+\\beta_{1}z+\\beta_{2}d+\\varepsilon_{2},}\\ &{d=\\left\\{\\begin{array}{l l}{1}&{\\mathrm{if~}y_{1}^{*}\\geqslant0}\\\\ {0}&{\\mathrm{otherwise}}\\end{array}\\right.}\\end{array}$$ where $y_{1}^{*}$ is a latent variable determining the discrete outcome $d$, and $x$ and $z$ are exogenous variables independent of $\\varepsilon_{1}$ and $\\varepsilon_{2}$.\nThe disturbances are generated by: $$\\begin{array}{r}{\\varepsilon_{1}=\\gamma_{1}u_{1}+\\rho_{1}v,}\\\\ {{}}\\\\ {\\varepsilon_{2}=\\gamma_{2}u_{2}+\\rho_{2}v,}\\end{array}$$ where $u_{1}, u_{2}$ are independent standard normal, and $v$ is a mean 0 variance 1 random variable independent of $u_{1}, u_{2}$.\n\nThis paper evaluates estimators for controlling endogeneity of dummy variables in continuous outcome regression models, comparing discrete factor approximations to maximum likelihood and two-stage estimators under normal and non-normal disturbances."
  },
  {
    "qid": "econ-empirical-1064-0-3-3",
    "question": "4) Discuss the implications of the theoretical results for structural macro-finance models.",
    "gold_answer": "1. The results provide a framework for estimating SDF components without restrictive parametric assumptions.\n2. The methodology can be applied to models with latent state variables.\n3. The findings highlight the importance of nonlinear dynamics in explaining long-term asset pricing puzzles.\n4. The estimators can be used to validate or refine parametric models by comparing implied components.",
    "question_context": "We recover the time series of the SDF process and its permanent and transitory components without assuming any parametric law of motion for the state.\nThe main theoretical contributions of the paper are consistency and convergence rates of the Perron-Frobenius eigenfunction estimators and asymptotic normality of the eigenvalue estimator.\n\nThe paper applies the methodology to study an economy with recursive preferences and nonlinear state dynamics. The theoretical contributions include consistency and convergence rates for eigenfunction and value function estimators, as well as asymptotic normality results."
  },
  {
    "qid": "econ-empirical-1541-3-2-0",
    "question": "5) Prove Proposition 6: Under the given conditions, show that the finite horizon games are cycling.",
    "gold_answer": "1. The condition $\\Psi^{\\prime}(\\hat{W}) \\leq -1$ near $\\hat{W}^{*}$ implies instability. \\n2. The sequence $\\hat{W}(T)$ oscillates around $\\hat{W}^{*}$ without converging. \\n3. This leads to cycles in equilibrium payoffs and acceptance regions.",
    "question_context": "PROPOSITION 6: If $F$ is symmetric, then $\\Psi$ has a unique fixed point $\\widehat{W}^{*}$ . If, in addition, (i) $\\Psi\\left(\\hat{W}\\right)\\neq\\hat{W}^{*}$ for all $\\hat{W}\\neq\\hat{W}^{*}$ , and (ii) there exists $\\varepsilon~>~0$ such that $\\Psi^{\\prime}\\big(\\hat{W}\\big)\\leq-1$ for all $\\hat{W}\\in\\left[\\hat{W}^{*}-\\varepsilon,\\hat{W}^{*}+\\varepsilon\\right].$ then the finite horizon games are cycling.\n\nThis section explores conditions under which finite horizon games exhibit cycling behavior."
  },
  {
    "qid": "econ-empirical-1411-2-2-0",
    "question": "5) Explain how lobbying for transfer-augmented revenues can be modeled as a DUP activity. What are the welfare implications of such activities in the context of Theorem 3?",
    "gold_answer": "1. Lobbying is a DUP (directly unproductive profit-seeking) activity that uses real resources to redistribute income. \\n2. It creates an endogenous distortion by diverting resources from productive uses. \\n3. **Welfare implications**: \\n   - If lobbying is successful, it can worsen the recipient's terms of trade. \\n   - Theorem 3 implies that such distortions can lead to paradoxical welfare outcomes if inferior goods are involved.",
    "question_context": "Consider here the case where the transfer payment goes through the recipient's budget, and lobbying arises to 'seek' a share in the transfer-augmented revenues of the government. This is an example of the Bhagwati-Srinivasan [1980] revenue-seeking phenomenon which, in turn, is part of the generic class of DUP activities analyzed in Bhagwati [1982].\n\nThis section discusses how endogenous distortions, such as lobbying for transfer-augmented revenues, can lead to welfare-reducing outcomes."
  },
  {
    "qid": "econ-empirical-223-3-0-0",
    "question": "1) Derive the conditions under which $\\nabla_{\\pmb\\theta}\\mathbb{E}[\\pi_{0}(T,X)m\\{Y;g(T;\\pmb\\theta)\\}w(T;\\pmb\\theta)]|_{\\pmb\\theta=\\pmb\\theta^{*}}$ is of full column rank, and explain its importance in the context of the test statistic.",
    "gold_answer": "1. **Full Rank Condition**: The matrix $\\nabla_{\\pmb\\theta}\\mathbb{E}[\\pi_{0}(T,X)m\\{Y;g(T;\\pmb\\theta)\\}w(T;\\pmb\\theta)]|_{\\pmb\\theta=\\pmb\\theta^{*}}$ is of full column rank if the following conditions hold:\n   - The function $g(T;\\pmb\\theta)$ is twice continuously differentiable in $\\pmb\\theta$.\n   - The residual function $m\\{Y;g(T;\\pmb\\theta)\\}$ is sufficiently smooth in $\\pmb\\theta$.\n   - The weight function $w(T;\\pmb\\theta)$ is non-degenerate and varies with $\\pmb\\theta$.\n\n2. **Importance**: The full rank condition ensures that the variance of the test statistic is finite and that the estimator $\\widehat{\\pmb{\\theta}}$ is asymptotically normal. It also guarantees that the test statistic has a well-defined limiting distribution under $H_0$.",
    "question_context": "$g(t;\\pmb\\theta)$ is twice continuously differentiable in $\\pmb\\theta\\in\\Theta$; $\\mathbb{E}[m\\{Y;g(T;\\theta^{*})\\}|T=t,X=x]$ is continuously differentiable in $(t,\\pmb{x})$; $\\mathbb{E}\\left[\\pi_{0}(T,X)m\\{Y;g(T;\\pmb{\\theta})\\}w(T;\\pmb{\\theta})|T=t,X=x\\right]$ is differentiable w.r.t. $\\pmb\\theta$ and $\\nabla_{\\pmb\\theta}\\mathbb{E}[\\pi_{0}(T,X)m\\{Y;g(T;\\pmb\\theta)\\}$ $w(T;\\pmb\\theta)]|_{\\pmb\\theta=\\pmb\\theta^{*}}$ is of full (column) rank.\n$\\mathbb{E}\\left[\\operatorname*{sup}_{\\pmb{\\theta}\\in\\Theta}|m\\{Y;g(T;\\pmb{\\theta})\\}|^{2+\\delta}\\right]<\\infty$ for some $\\delta>0$; The function class $\\left\\{m\\{Y;g(T;\\pmb\\theta)\\}:\\pmb\\theta\\in\\Theta\\right\\}$ satisfies: for any $\\pmb\\theta\\in\\Theta$ and any small $\\delta>0$ and for some finite positive constant $C$.\n$\\phi(T_{i},X_{i};t):=\\pi_{0}(T_{i},X_{i})\\cdot{\\mathcal{H}}(T_{i},t)\\cdot\\mathbb{E}[m\\left\\{Y_{i};g(T_{i};\\theta^{*})\\right\\}|T_{i},X_{i}] -\\mathbb{E}[\\pi_{0}(T_{i},X_{i})m\\left\\{Y_{i};g(T_{i};\\theta^{*})\\right\\}\\cdot{\\mathcal{H}}(T_{i},t)|X_{i}]$\n$\\psi(T_{i},X_{i},Y_{i};t):=\\mathbb{E}\\Bigg[\\pi_{0}(T_{i},X_{i})\\cdot\\frac{\\partial}{\\partial g}\\mathbb{E}[m\\left\\{Y_{i}g(T_{i},\\theta^{*})\\right\\}|T_{i},X_{i}]\\cdot\\nabla_{\\theta}g(T_{i},\\theta^{*})^{\\top}\\mathcal{H}(T_{i},t)\\Bigg] \\times \\Bigg\\{\\mathbb{E}\\left[\\pi_{0}(T_{i},X_{i})\\cdot\\frac{\\partial}{\\partial g}\\mathbb{E}[m\\left\\{Y_{i}g(T_{i},\\theta^{*})\\right\\}|T_{i},X_{i}]\\cdot\\nabla_{\\theta}g(T_{i},\\theta^{*})w(T_{i};\\theta^{*})^{\\top}\\right] \\times\\mathbb{E}\\left[\\pi_{0}(T_{i},X_{i})\\cdot\\frac{\\partial}{\\partial g}\\mathbb{E}[m\\left\\{Y_{i}g(T_{i},\\theta^{*})\\right\\}|T_{i},X_{i}]\\cdot w(T_{i},\\theta^{*})\\nabla_{\\theta}^{\\top}g(T_{i},\\theta^{*})\\right]\\Bigg\\}^{-1} \\times\\mathbb{E}\\left[\\pi_{0}(T_{i},X_{i})\\cdot\\frac{\\partial}{\\partial g}\\mathbb{E}[m\\left\\{Y_{i}g(T_{i},\\theta^{*})\\right\\}|T_{i},X_{i}]\\cdot\\nabla_{\\theta}g(T_{i},\\theta^{*})w(T_{i};\\theta^{*})^{\\top}\\right] \\times\\Bigg\\{\\pi_{0}(T_{i},X_{i})\\cdot\\frac{\\partial}{\\partial g}\\mathbb{E}[m\\left\\{Y_{i}g(T_{i},\\theta^{*})\\right\\}|T_{i},X_{i}]\\cdot\\nabla_{\\theta}g(T_{i},\\theta^{*})[T_{i},\\theta^{*})^{\\top}\\Bigg\\}$\n$\\eta(T_{i},X_{i},Y_{i};t):=U_{i}{\\mathcal{H}}(T_{i},t)-\\phi(T_{i},X_{i};t)-\\psi(T_{i},X_{i},Y_{i};t)$\n\nThis section discusses the regularity conditions and special cases for the continuous treatment effect models, including average and quantile dose-response models."
  },
  {
    "qid": "econ-empirical-1600-2-0-0",
    "question": "1) Derive the key equation (5) for economic performance, explaining the role of each component: $\\alpha_{y i}$, $X_{i t}\\beta$, $\\sum_{j=0}^{m}\\phi_{j}I_{i t-j}$, and $\\sum_{j=1}^{k}Y_{i t-j}\\delta_{j}$.",
    "gold_answer": "1. **$\\alpha_{y i}$**: Represents country-specific fixed effects capturing unobserved heterogeneity. \n2. **$X_{i t}\\beta$**: Captures the impact of observable control variables on performance. \n3. **$\\sum_{j=0}^{m}\\phi_{j}I_{i t-j}$**: Measures the contemporaneous and lagged effects of IMF participation ($I_{i t-j}$). \n4. **$\\sum_{j=1}^{k}Y_{i t-j}\\delta_{j}$**: Accounts for autoregressive dynamics via lagged dependent variables.",
    "question_context": "Economic performance as summarized in Eq. (1) can be restated in estimating Eq. (5): \n\n$$ y_{i t}=\\alpha_{y i}+X_{i t}\\beta+\\sum_{j=0}^{m}\\phi_{j}I_{i t-j}+\\sum_{j=1}^{k}Y_{i t-j}\\delta_{j}+e_{y i t}. $$ \n\nEstimates of the parameters of (5) can be derived through introduction of an indicator of IMF participation uncorrelated with $e_{y i t}$ , appropriate stacking of matrices and use of a systems regression methodology (e.g., two-stage generalized least squares (2SGLS)).\n\nThe effectiveness of IMF programs in achieving economic performance outcomes is examined through a specified general structure that controls for other determining factors. The performance is summarized in Eq. (5), which includes parameters derived through systems regression methodology."
  },
  {
    "qid": "econ-empirical-1812-2-2-0",
    "question": "5) Define a frame and explain its significance in the characterization of strategic convergence.",
    "gold_answer": "1. **Definition:** A frame $\\mathcal{P} = (\\mathcal{P}_{i})_{i \\in I}$ is a profile of finite measurable partitions of $T_{i}^{*}$ such that for any two types $t_{i}, t_{i}^{\\prime} \\in T_{i}^{*}$, if they agree on beliefs about $\\Theta \\times \\mathcal{P}_{-i}$, then $t_{i}^{\\prime} \\in \\mathcal{P}_{i}(t_{i})$.\\n2. **Significance:** Frames capture coarse information structures relevant for strategic behavior. Theorem 2 shows that strategic convergence is equivalent to uniform weak convergence on all frames, highlighting the role of coarse beliefs in determining robustness.",
    "question_context": "Theorem 2. (Characterization of strategic convergence). A sequence of types converges strategically if and only if it converges uniform-weakly on every frame.\n\nThis section generalizes the characterization of strategic convergence by introducing frames and uniform weak convergence relative to a frame, as formalized in Theorem 2."
  },
  {
    "qid": "econ-empirical-1624-3-1-2",
    "question": "7) Discuss the role of convex cost functions in smoothing effort over time in the presence of a deadline, as suggested by Figure 5.",
    "gold_answer": "1. With linear costs, effort exhibits discontinuous jumps near the deadline.\n2. Convex costs ($c(u_{i}) = c \\cdot u_{i}^{\\gamma}, \\gamma > 1$) induce agents to smooth effort over time.\n3. This smoothing effect reduces the sharp discontinuities seen in the linear case.\n4. The encouragement effect of the deadline remains, but is less pronounced due to the cost convexity.",
    "question_context": "For some possibly infinite deadline $T\\in\\mathbb{R}_{+}\\cup\\{\\infty\\}$ , and some strategy profile $(u_{1},\\dots,u_{n})\\colon[0,T]\\to[0,1]^{n}$ agent $\\ddot{\\iota}$ s (expected) payoff over the horizon $[0,T]$ is now defined as $$r\\int_{0}^{T}\\left(p_{t}(u_{i,t}+u_{-i,t})-\\alpha u_{i,t}\\right)e^{-\\int_{0}^{t}\\left(p_{s}(u_{i,s}+u_{-i,s})+r\\right)d s}d t.$$\nLEMMA 2: Given $T<\\infty$ , there exists a unique symmetric equilibrium, characterized by ${\\tilde{T}}\\in[0,T)$ , in which the level of effort is given by $$u_{i,t}=u_{i,t}^{*}\\mathrm{for}t<\\tilde{T},\\mathrm{and}u_{i,t}=1\\mathrm{for}t\\in[\\tilde{T},T],$$ where $u_{i}^{*}$ is as in Theorem 1.\n\nThis section examines the role of self-imposed deadlines in mitigating the problem of delay in effort exertion. The analysis considers both exogenous and optimal deadlines."
  },
  {
    "qid": "econ-empirical-394-1-0-0",
    "question": "1) Derive the portfolio choice condition $u_{AB} > u_{AC}$ from the indirect utility function $u_{AB}(\\theta_A, \\theta_B, \\Gamma_{AB})$, showing all intermediate steps.",
    "gold_answer": "1. Start with indirect utility functions for both portfolios:\n   $$u_{AB} = f(\\theta_A) + f(\\theta_B) + \\Gamma_{AB} - \\alpha(p_A + p_B) + \\epsilon_{AB}$$\n   $$u_{AC} = f(\\theta_A) + f(\\theta_C) + \\Gamma_{AC} - \\alpha(p_A + p_C) + \\epsilon_{AC}$$\n2. Subtract $u_{AC}$ from $u_{AB}$:\n   $$u_{AB} - u_{AC} = [f(\\theta_B) - f(\\theta_C)] + [\\Gamma_{AB} - \\Gamma_{AC}] - \\alpha(p_B - p_C) + [\\epsilon_{AB} - \\epsilon_{AC}]$$\n3. The choice condition becomes:\n   $$f(\\theta_B) - f(\\theta_C) + \\Gamma_{AB} - \\Gamma_{AC} - \\alpha(p_C - p_B) + \\epsilon_{AB} - \\epsilon_{AC} > 0$$",
    "question_context": "Consider a consumer who currently possesses one vehicle and is purchasing a second... The household’s indirect utility derived from such a portfolio is given as: $$u_{A B}(\\theta_{A},\\theta_{B},\\Gamma_{A B})=f(\\theta_{A})+f(\\theta_{B})+\\Gamma_{A B}-\\alpha(p_{A}+p_{B})+\\epsilon_{A B},$$\nThe consumer chooses portfolio $A B$ rather than $A C$ if $u_{A B}>u_{A C}$. Thus, $A B$ is chosen if: $$f(\\theta_{B})-f(\\theta_{C})+\\Gamma_{A B}-\\Gamma_{A C}-\\alpha(p_{C}-p_{B})+\\epsilon_{A B}-\\epsilon_{A C}>0.$$\nDefinition 1. A consumer exhibits a preference for attribute substitution in $a$ when $\\frac{\\partial\\theta_{B}^{l}}{\\partial\\theta_{A}^{l}}<0$ and attribute complementarity when $\\frac{\\partial\\theta_{B}^{l}}{\\partial\\theta_{A}^{l}}>0$.\n\nThe text introduces a utility maximization framework for consumers with multi-vehicle portfolios, focusing on how changes in kept vehicle attributes influence subsequent vehicle choices. The model incorporates portfolio utility components and discrete choice theory."
  },
  {
    "qid": "econ-empirical-183-5-0-0",
    "question": "1) Derive the expression for $P^{\\alpha}[\\varepsilon_{t}=1|\\mathcal{F}_{t-1}]$ using Bayes' law, starting from the definition of $D_{t}^{\\alpha}$.",
    "gold_answer": "1. Start with the definition of $D_{t}^{\\alpha}$ as the density process.\n2. Apply Bayes' law: $P^{\\alpha}[\\varepsilon_{t}=1|\\mathcal{F}_{t-1}] = E^{0}\\left[\\frac{D_{t}^{\\alpha}1_{\\{\\varepsilon_{t}=1\\}}}{D_{t-1}^{\\alpha}} \\bigg| \\mathcal{F}_{t-1}\\right]$.\n3. Substitute $D_{t}^{\\alpha} = D_{t-1}^{\\alpha} \\exp(\\alpha_{t} \\varepsilon_{t} - L(\\alpha_{t}))$.\n4. Simplify to $E^{0}\\left[\\exp(\\alpha_{t} \\varepsilon_{t} - L(\\alpha_{t})) 1_{\\{\\varepsilon_{t}=1\\}} \\big| \\mathcal{F}_{t-1}\\right]$.\n5. Since $(\\alpha_{t})$ is predictable, it is $\\mathcal{F}_{t-1}$-measurable, leading to $\\exp(\\alpha_{t} - L(\\alpha_{t})) P^{0}(\\varepsilon_{t}=1)$.\n6. Finally, $P^{0}(\\varepsilon_{t}=1) = \\frac{1}{2}$, but under the transformation, it becomes $\\frac{\\exp(\\alpha_{t})}{1 + \\exp(\\alpha_{t})}$.",
    "question_context": "Let $S=\\{0,1\\}$ and take $\\nu_{0}$ to be uniform on S. Hence, under $P_{0}=\\bigotimes_{t=1}^{\\infty}\\nu_{0}$, the projections $(\\pmb{\\varepsilon}_{t})$ are independent and identically distributed with $P_{0}[\\varepsilon_{t}=1]=P_{0}[\\varepsilon_{t}=0]=1/2$. Consider now the set $\\mathcal{P}^{a,b}$ as defined in Section 4.1. Fix a predictable process $(\\alpha_{t})$.\nBy using Bayes' law, we get $P^{\\alpha}[\\varepsilon_{t}=1|\\mathcal{F}_{t-1}]=E^{0}\\frac{[D_{t}^{\\alpha}1_{\\{\\varepsilon_{t}=1\\}}|\\mathcal{F}_{t-1}]}{D_{t-1}^{\\alpha}}=E^{0}\\big[\\exp(\\alpha_{t}-L(\\alpha_{t}))1_{\\{\\varepsilon_{t}=1\\}}|\\mathcal{F}_{t-1}\\big], (\\alpha_{t})\\mathrm{~predictable}=\\exp(\\alpha_{t}-L(\\alpha_{t}))P^{0}(\\varepsilon_{t}=1)=\\frac{\\exp(\\alpha_{t})}{1+\\exp(\\alpha_{t})}$.\nHence, the probability for $\\pmb{\\varepsilon}_{t}=1$ is always in the interval $[\\underline{{p}},\\overline{{p}}]$ with $\\underline{{p}}=\\frac{\\exp(a)}{1+\\exp(a)},\\quad\\overline{{p}}=\\frac{\\exp(b)}{1+\\exp(b)}$.\n\nThis section discusses the formulation of a time-consistent set of priors for binary random variables, building on the framework introduced in Section 4.1. It explores the properties of the probability measures under different predictable processes and their implications for binary outcomes."
  },
  {
    "qid": "econ-empirical-654-0-0-1",
    "question": "2) Derive the identification condition for the disaster shock as the component with the largest kurtosis. Explain why kurtosis is a suitable measure for tail heaviness.",
    "gold_answer": "The disaster shock is identified by maximizing the sample kurtosis:\n\n\\[\n\\kappa_4 = \\frac{\\sum_{t=1}^{T} Z_t^4}{\\left(\\sum_{t=1}^{T} Z_t^2\\right)^2}\n\\]\n\nFor a heavy-tailed distribution with index \\( \\alpha = 1 \\), the scaled kurtosis \\( \\frac{1}{T} \\kappa_4 \\) converges to a random variable between 0 and 1. Kurtosis is suitable because it captures the tail behavior: higher kurtosis indicates heavier tails, which is characteristic of disaster shocks.",
    "question_context": "The least squares estimates of the SVAR are consistent but have non-standard asymptotics.\nThe disaster shock is identified as the component with the largest kurtosis.\nAn estimator that is robust to infinite variance is used to recover the mutually independent components.\n\nThis section discusses the framework for accommodating disaster-type variables with infinite variance into a SVAR, focusing on the consistency and non-standard asymptotics of least squares estimates."
  },
  {
    "qid": "econ-empirical-1438-2-1-1",
    "question": "6) In Example 2 (One-to-One Matching with Age), derive the transition probabilities $P_{t}(x^{\\prime}|x,a)$ for an agent of age $x$ who either finds a match ($a\\in X$) or remains unmatched ($a=\\emptyset$).",
    "gold_answer": "The transition probabilities are:\n1. If the agent finds a match ($a\\in X$), she exits the market, so $P_{t}(x^{\\prime}|x,a) = 0$ for all $x^{\\prime}$.\n2. If the agent remains unmatched ($a=\\emptyset$):\n   - If $x < T$, her age increases by 1: $P_{t}(x+1|x,\\emptyset) = 1$.\n   - If $x = T$, she exits the market: $P_{t}(x^{\\prime}|x,\\emptyset) = 0$ for all $x^{\\prime}$.",
    "question_context": "EXAMPLE 1 (Marriage Matching): The set of types is a disjoint union $X=M\\cup F$ of male types M and female types $F.$ The types contain information about the personal characteristics, education level, or economic situation of an agent. The types do not change. Each unmatched agent receives utility $\\upsilon(x,\\emptyset)=0$ and dies (i.e., leaves from the market) at exogenous rate $1-\\delta>0$ .An agent $x$ matched with agent y receives utility $v(x,y)$ and leaves the market.\nEXAMPLE 2 (One-to-One Matching with Age): In a version of the above example, each agent has type $x\\in\\{0,\\ldots,T\\}$ , interpreted as her age. In each period that an agent $x\\:<T$ does not find a match, her type increases by 1. If an agent finds a match or the type is equal to $T$ , the agent exits the market.\n\nThe model is quite general. We illustrate its scope with a few applications."
  },
  {
    "qid": "econ-empirical-1082-3-3-0",
    "question": "6) Using the DFML estimates from Table 4, compute the cumulative effect of a 26-week unemployment spell on future unemployment duration. Contrast this with Ellwood’s FE results.",
    "gold_answer": "1. **DFML effect**: \n   - From Table 4: \\( \\beta_1 = 0.1447 \\) weeks/week (SE = N/A). \n   - For 26 weeks: \\( 0.1447 \\times 26 \\approx 3.8 \\) weeks/year. \n2. **Ellwood’s FE**: \n   - FE estimate: \\( \\beta_1 = 0.0073 \\) weeks/week (SE = 0.0157). \n   - For 26 weeks: \\( 0.0073 \\times 26 \\approx 0.19 \\) weeks/year (insignificant). \n3. **Divergence**: DFML shows persistence; FE does not, likely due to selection bias.",
    "question_context": "Table4 Evidence of Persistence: The Effect of One Week of Unemployment on the Incidence andDurationofUnemployment\nThe positive effect of prior unemployment on the duration of a current spell is short-lived but quite significant. A 26-week spell experienced last year increases the duration of a contemporaneous unemployment spell, if unemployed, by 3.8 weeks annually. The effect is an order of magnitude smaller for all longer lags. Using OLS regressions of current unemployment on prior unemployment, Ellwood (1982) finds strong evidence of state dependence in weeks of unemployment. He finds that all evidence of state dependence vanishes, however, upon controlling for unobserved heterogeneity with FE specifications.\n\nLike many previous studies, we examine how the duration of prior unemployment affects the incidence and duration of future unemployment. In general, the literature shows that controlling for unobserved heterogeneity greatly reduces measured persistence in unemployment. The evidence presented here supports that particular finding. Many of these previous studies also find that no persistence remains after the use of controls for unobserved heterogeneity. This study disagrees with that finding. We find that there is strong and statistically significant evidence of short-lived persistence in unemployment."
  },
  {
    "qid": "econ-empirical-401-1-0-3",
    "question": "4) Using a second-best framework, formalize how tax privileges for nonprofits can offset market failures in the provision of collective goods.",
    "gold_answer": "1. **Market failure**: Let \\( MC_{\\text{public}} > MB_{\\text{public}} \\) due to free-riding.  \n2. **Tax privilege**: Introduce a subsidy \\( \\tau \\) reducing nonprofits' cost: \\( MC_{\\text{np}} = MC - \\tau \\).  \n3. **Equilibrium**: Solve \\( MB_{\\text{public}} = MC_{\\text{np}} \\) to get \\( \\tau^* = MC_{\\text{public}} - MB_{\\text{public}} \\).  \n4. **Outcome**: \\( \\tau^* \\) corrects the underprovision of collective goods by aligning private and social costs.",
    "question_context": "the efficiency of any institutional form depends to a large extent on informational inequalities in the market where it functions...\nhybrid institutions such as nonprofits exist because neither the proprietary nor the governmental form is fully satisfactory...\na close relation between an organization's source of revenue and the nature of its outputs...\nthe greater the proportion of revenues an organisation receives from sales and dues - and, hence, the smaller its collectiveness index - the weaker is its claim for public subsidization, other things equal...\ntax policy cannot be constructed unless we recognise the multiple and interdependent ways through which both the law and its administration affect contributions of time as well as money, and thereby the strength and vitality of the nonprofit economy...\n\nThe book discusses the nonprofit sector's role in addressing market failures due to informational asymmetries and diverse demands, highlighting the efficiency and funding mechanisms of nonprofits."
  },
  {
    "qid": "econ-empirical-322-4-0-3",
    "question": "4) How does the allocation of property rights shape incentives to invest in an asset’s improvement in the context of partnership disputes?",
    "gold_answer": "4. **Property Rights and Investment Incentives**: \n   - **Grossman-Hart-Moore Framework**: The allocation of property rights determines the residual control rights, which influence the partners' incentives to invest in the asset's improvement. \n   - **Investment Incentives**: If a partner has more control rights, they are more likely to invest in the asset, as they can capture a larger share of the returns. \n   - **Partnership Context**: In a partnership, the allocation of property rights must balance the partners' incentives to invest while ensuring efficient dispute resolution.",
    "question_context": "As a first step in studying the implications of the potential inefficiency of dissolution, we have focused—in line with much of the partnership dissolution literature—on environments in which the value of a partnership is not a function of the ownership structure. Relaxing this assumption would open the door to different interesting questions. For instance, how might the partners be incentivized to trade shares of an asset with the goal of reaching an optimal ownership structure, given their different valuations for the asset? Furthermore, a related literature pioneered by Grossman and Hart (1986) and Hart and Moore (1990) studies how the allocation of property rights shapes incentives to invest in an asset’s improvement. Studying such incentives in the current context seems a particularly interesting direction for future research.\n\nThe literature on partnership dissolution has accumulated a rich set of results on the possibility of efficient dissolution of a partnership, and mechanisms that can be used for such dissolution. This literature, however, implicitly takes as given the inefficiency of sustaining the partnership. Motivated by the increasing role of ADR in settling partnership disputes—often without resorting to dissolution—this article adds to such problems the possibility that, for some realizations of the underlying uncertainty, dissolving the partnership may be inefficient. Given this possibility, we have studied under what conditions a partnership dispute can be resolved efficiently, what types of dispute-resolution procedures can be used to do so, and what form second-best mechanisms take when efficient resolution is impossible and private information is one sided."
  },
  {
    "qid": "econ-empirical-563-0-1-0",
    "question": "5) How does the selection effect in online reviews differ from herding in observational learning models? Provide a formal comparison.",
    "gold_answer": "The key differences are: \\n1. **Selection Effect**: Users' purchase decisions depend on their ex ante taste parameters, leading to a biased sample of reviews. Reviews reflect both quality and user preferences. \\n2. **Herding**: Users ignore private signals and follow past actions, leading to information cascades. Reviews are uninformative. \\nMathematically, the selection effect introduces dependence between reviews and past beliefs ($q_t$), whereas herding models assume actions are independent of private signals after a cascade.",
    "question_context": "Our work is related to several literatures. The first is the Bayesian observational learning literature... Our paper also relates to several recent papers studying the speed of learning in Bayesian models.\nIn Appendix B.3.8, we show that if the platform would like to maximize participation by users, then it will always choose a rating system that maximizes the speed of learning. In Appendix B.3.9, we show that our learning and speed of learning results generalize to some environments in which the platform also chooses an endogenous sequence of prices to maximize revenue.\n\nThe paper contributes to the literature by analyzing the selection effect, characterizing the speed of learning, and comparing rating systems. It also discusses extensions to platform design and strategic interactions."
  },
  {
    "qid": "econ-empirical-1198-2-0-1",
    "question": "2) Using the percentile data (90th, 50th, 25th, 10th), analyze how censoring distorts the age-related trends in TVIP scores. Why are the 25th, 50th, and 90th percentiles unaffected for certain age ranges?",
    "gold_answer": "1. **Percentile Analysis**:  \n   - **90th/50th Percentiles**: Unaffected because censoring only impacts the lower tail (below 25th percentile).  \n   - **10th Percentile**: Traces the minimum normed score \\(c_a\\) when censoring exceeds 10% (e.g., ages 36–50 and 66–71 months).  \n2. **Gap Analysis**: The 90th–25th percentile gap widens with age because:  \n   - Upper percentiles remain stable (e.g., 90th).  \n   - Lower percentiles (25th) decline due to censoring-induced truncation.  \n3. **Implication**: The increasing gap may reflect either genuine cognitive divergence or norming biases. Formalize the censoring threshold \\(c_a\\) as a function of age \\(a\\):  \n   \\[ \\text{10th Percentile} = \\max(\\text{True 10th Percentile}, c_a) \\]",
    "question_context": "The mean age-normed TVIP score declines from close to 95, at age 36 months, to less than 85, between 54 and 60 months, and flattens out thereafter. This decline in the mean is accompanied by an increase in dispersion. The standard deviation of the TVIP score rises from eight for the youngest children, to nearly 25 for the oldest children.\nThe decline in the mean and the increasing dispersion with age in the standardized TVIP score may both in part be driven by censoring. Nearly 25 percent of children at age 36 months have censored scores, which means that (for this age group) nearly a quarter of children were unable to answer a single question correctly.\n\nThis section examines the variation in standardized TVIP scores across children of different ages, highlighting the importance of accounting for censoring and the increasing dispersion of scores with age."
  },
  {
    "qid": "econ-empirical-1404-1-1-2",
    "question": "3) Explain why the equilibrium cutoff k1* in the nontransparent regime is given by k1* = 1/2 (1 - δ/(4-3δ)).",
    "gold_answer": "1. Substitute p1(k) into seller 1's profit maximization problem: \n   $$ \\max_k (1-k) \\left[(1-\\delta)k + \\frac{1}{2}\\delta k_1^*\\right] $$\n2. Solve the first-order condition and use the fixed-point condition k1* = k. \n3. The solution yields: \n   $$ k_1^* = \\frac{1}{2}\\left(1 - \\frac{\\delta}{4-3\\delta}\\right) $$",
    "question_context": "We first consider a two-period version of our model. For further simplicity, we assume that buyer types are uniformly distributed with support [0,1].\nBy the skimming property, for each on- or off-equilibrium-path history, there exists k1 such that seller 2 believes that buyer types higher than k1 trade with seller 1 and the remaining types are [0,k1].\nThen, regardless of the regime, when the remaining types are [0,k1], seller 2 charges a price p2 = 1/2 k1 and trades with buyer types [1/2 k1, k1].\n\nThe example simplifies the model to two periods and assumes uniformly distributed buyer types on [0,1]. It compares the transparent and nontransparent regimes, highlighting differences in equilibrium outcomes."
  },
  {
    "qid": "econ-empirical-1517-2-0-4",
    "question": "5) Analyze the potential dual effects of the wage variable (WAGE) on deposit rates. How might higher wages reflect both higher marginal costs and increased worker productivity?",
    "gold_answer": "1. **Marginal Cost Effect**: Higher wages increase banks' operating costs, leading to lower deposit rates (negative sign) as banks pass on costs to depositors.\n2. **Productivity Effect**: Higher wages may reflect more skilled or productive workers, enabling banks to offer higher deposit rates (positive sign) due to improved efficiency.\n3. **Empirical Ambiguity**: The net effect depends on which effect dominates, making the sign on WAGE a priori ambiguous.",
    "question_context": "The empirical model consists of a reduced-form interest rate equation: \n$$\\begin{array}{r l}&{r_{i j}=\\beta_{1}+\\beta_{2}C R-3_{j}+\\beta_{3}C H I N C_{j}}\\ &{\\qquad+\\beta_{4}I N C_{j}+\\beta_{5}M I G2_{j}+\\beta_{6}A G E45_{j}}\\ &{\\qquad+\\beta_{7}W A G E_{j}+\\beta_{8}N E+\\beta_{9}M W+\\beta_{10}W,}\\end{array}$$ \nwhere $r_{i j}$ is the deposit rate for the $i^{\\mathrm{th}}$ bank in the $j^{\\mathrm{th}}$ local market.\nThe three-bank concentration ratio (CR-3) is the share of total bank deposits held by the three banks having the largest shares. The expected sign on CR-3 is negative.\nLocal income growth (CHINC) is measured by the percentage change in MSA personal income per capita between 1984 and 1985. The sign on CHINC is a priori ambiguous.\nPersonal income per capita (INC) is included to capture the effect of income on elasticity. The sign on INC is a priori ambiguous.\nThe percentage of MSA population aged 45 and older (AGE45) is included as a proxy for wealth. The sign on AGE45 is a priori ambiguous.\nThe square of MSA in-migration (MIG2) is used to measure the proportion of new arrivals to the MSA. The sign on MIG2 is expected to be nonnegative.\nThe wage variable (WAGE) controls for differences in bank marginal costs across markets. The sign on WAGE is a priori ambiguous.\nThree regional dummies (NE, MW, W) are included to control for regional variations in deposit rates.\n\nThe empirical model examines the determinants of deposit rates for Money Market Deposit Accounts (MMDAs) and 3- and 6-month Certificates of Deposit (CDs) using a reduced-form interest rate equation. The model incorporates variables such as market concentration, local income growth, personal income per capita, population demographics, in-migration rates, wages, and regional dummies."
  },
  {
    "qid": "econ-empirical-516-0-0-3",
    "question": "4) The one-factor model induces correlation among defaults. Derive the implied joint distribution of defaults under this model and contrast it with the binomial model.",
    "gold_answer": "1. Let $Y_{t}$ be a latent factor (e.g., economic conditions) affecting defaults in period $t$.\n2. Conditional on $Y_{t}$, defaults are independent with probability $\\theta(Y_{t})$.\n3. The joint distribution integrates over $Y_{t}$: $p(\\mathbf{D}) = \\int \\prod_{i=1}^{n} \\theta(Y_{t})^{d_{i}}(1-\\theta(Y_{t}))^{1-d_{i}} dF(Y_{t})$.\n4. Contrast: The binomial model assumes $\\theta$ is constant, while the one-factor model allows for temporal correlation.",
    "question_context": "Default is a rare event, even in segments in the midrange of a bank’s portfolio. Inference about default rates is essential for risk management and for compliance with the requirements of Basel II.\nThe binomial model, most common in applications, is extended to allow correlated defaults. A check of robustness is illustrated with an $\\epsilon$ -mixture of priors.\nThe simplest and most common probability model for defaults of assets in a homogeneous segment of a portfolio is the binomial, in which the defaults are independent across assets and over time, and defaults occur with common probability $\\theta$.\nThe Bernoulli model (a single binomial trial) for the distribution of $d_{i}$ is $p(d_{i}|\\theta,e)=\\theta^{d_{i}}(1-\\theta)^{1-d_{i}}$.\nThe joint distribution of the data is $p({\\bf D}|\\theta,e)=\\prod_{i=1}^{n}\\theta^{d_{i}}(1-\\theta)^{1-d_{i}}=\\theta^{r}(1-\\theta)^{n-r}$.\nThe one-factor model underlying the B2 capital calculations allows temporal variation in default rates, inducing correlation among defaults within periods.\n\nThe article discusses the estimation of default probabilities (PD) for midrange portfolio segments, emphasizing the role of expert information and Bayesian inference. It extends the binomial model to account for correlated defaults and introduces robustness checks via an ε-mixture of priors."
  },
  {
    "qid": "econ-empirical-68-1-0-1",
    "question": "2) Show that $E[y_{t}(s^{t})\\mid s^{t-1}]=0$ in any competitive equilibrium, using the model's equations.",
    "gold_answer": "1. Take expectations of the aggregate price level equation (10) as of $s^{t-1}$: $E[p_{t}\\mid s^{t-1}]=\\kappa E[y_{t}\\mid s^{t-1}]+E[x_{t}\\mid s^{t-1}]$.\\n2. Substitute the sticky-price producer's optimality condition (9): $x_{t}=E_{t-1}[\\pi_{t}+\\gamma y_{t}]$.\\n3. Combine to get $E[y_{t}\\mid s^{t-1}]=0$ by solving the resulting equation.",
    "question_context": "The dynamical system associated with the competitive equilibrium of this model is straightforward, which lets us focus on the strategic aspects of sophisticated policies.\nConsider a monetary economy populated by a large number of identical, infinitely lived consumers, a continuum of producers, and a central bank.\nA fraction of producers $j\\in[0,\\alpha)$ are flexible-price producers, and a fraction $j\\in\\left[\\alpha,1\\right]$ are sticky-price producers.\nConsumer behavior in this model is summarized by an intertemporal Euler equation and a cash-in-advance constraint.\nThe optimal price set by an individual flexible-price producer $j$ satisfies $p_{f t}(j)=p_{t}+\\gamma y_{t}$.\nThe optimal price set by a sticky-price producer $j$ satisfies $p_{s t}(j)=E_{t-1}\\left[p_{t}+\\gamma y_{t}\\right]$.\nThe aggregate price level $p_{t}$ is a linear combination of the prices $p_{f t}$ set by the flexible-price producers and the prices $p_{s t}$ set by the sticky-price producers.\nThe central bank’s policy instrument is either money growth $\\mu_{t}$ or the interest rate $i_{t}$.\nAny competitive equilibrium must satisfy $\\pi_{t}(s^{t})=\\kappa y_{t}(s^{t})+E[\\pi_{t}(s^{t})\\mid s^{t-1}]$, which is often referred to as the New Classical Phillips curve.\n\nThe model illustrates sophisticated policies using a simple one-period price-setting framework, focusing on strategic aspects and competitive equilibrium outcomes. It includes flexible and sticky-price producers, monetary policy regimes, and shocks affecting output and inflation."
  },
  {
    "qid": "econ-empirical-580-0-0-1",
    "question": "2) Explain why fanning out is more pronounced when prospects are located on the edges of the unit probability triangle, and provide a mathematical justification for this observation.",
    "gold_answer": "1. **Edge Effects**: Prospects on the edges of the triangle represent extreme probabilities (e.g., \\( p_1 = 0 \\) or \\( p_3 = 0 \\)).  \n2. **Fanning Out**: The slope of indifference curves is steeper near the edges due to the non-linearity of the utility function. Mathematically, this can be represented as \\[ \\frac{dp_2}{dp_1} \\bigg|_{\\text{edge}} > \\frac{dp_2}{dp_1} \\bigg|_{\\text{interior}} \\].  \n3. **Justification**: The Machina (1982) framework allows for local violations of expected utility theory, which are amplified near the edges due to the boundary constraints on probabilities.",
    "question_context": "Fanning in is most pronounced in test $M\\mathbf{-I}$ in series 1, while test 1 here indicates that prospect $Y$ is preferred to either $X$ or $Z_{i}$ .consistent with convex indifference curves. Similarly, fanning in is reported in set 1 of series 2, while $Y_{1}$ is preferred to both $X_{1}$ and $Z_{1}$ , again indicating convex indifference curves.\nHowever, there is no theoretical incompatibility between convexity and fanning out; one does not logically entail the other, as generalised expected utility theory permits non-linear indifference curves (Machina, 1982).\nBut fanning out does much better when the prospects are located on the edges of the triangle, suggesting some relationship between the two effects.\n\nThe text discusses the relationship between the fanning out hypothesis and convex indifference curves in the context of animal behavior under uncertainty. It explores deviations from expected utility theory and the implications of these deviations for generalized expected utility theory."
  },
  {
    "qid": "econ-empirical-1753-2-0-2",
    "question": "3) For Theorem 3, justify the conditions $n h\\rightarrow\\infty$ and $n h^{2(m+\\gamma)+1}\\rightarrow0$ in the context of bias-variance tradeoff.",
    "gold_answer": "The conditions ensure:\\n1. **Consistency**: $n h\\rightarrow\\infty$ guarantees the variance term $O_P(1/\\sqrt{n h})$ vanishes.\\n2. **Bias Dominance**: $n h^{2(m+\\gamma)+1}\\rightarrow0$ ensures the bias term $O_P(h^{m+\\gamma})$ decays faster than the standard deviation $O_P(1/\\sqrt{n h})$.\\nThis tradeoff balances the estimator's precision (variance) against its accuracy (bias), where $h$ must shrink sufficiently fast relative to $n$.",
    "question_context": "Assumption A.1. The process is sampled at $t_{i}=i\\Delta$ , $i=0,1,\\ldots,n$ , such that $T=n\\Delta$ .\nAssumption A.2. The processes $\\{\\mu_{t}\\}$ and $\\left\\{\\sigma_{t}^{2}\\right\\}$ are jointly independent of $\\{W_{t}\\}$ .\nAssumption A.3. The volatility processes $\\left\\{\\sigma_{t}^{2}\\right\\}$ are locally bounded away from zero and the mean and volatility satisfy: $$\\operatorname*{lim}_{\\Delta\\rightarrow0}\\Delta\\sum_{i=1}^{n}\\bigg|\\mu_{s_{i}}^{2}-\\mu_{t_{i}}^{2}\\bigg|=0,\\quad\\operatorname*{lim}_{\\Delta\\rightarrow0}\\Delta\\sum_{i=1}^{n}\\bigg|\\sigma_{s_{i}}^{4}-\\sigma_{t_{i}}^{4}\\bigg|=0$$ for any sequences $(i-1)\\Delta\\leq s_{i}\\leq t_{i}\\leq i\\Delta,i=1,...,n.$\nAssumption A.4. $t\\mapsto\\sigma_{t}^{2}$ lies in $\\mathcal{C}^{m,\\gamma}\\left[0,T\\right]$ for some $m\\geq0$ and $\\gamma\\geq0$ , and $\\begin{array}{r}{\\int_{0}^{T}|\\mu_{t}|d t<\\infty}\\end{array}$ .\nKernel K.1. The kernel $K:\\mathbb{R}\\mapsto\\mathbb{R}$ is continuously differentiable and satisfies: 1. $\\begin{array}{r}{\\int_{\\mathbb{R}}K\\left(z\\right)d z=1}\\end{array}$ ; $\\left|z\\right|\\left|K^{\\left(i\\right)}\\left(z\\right)\\right|\\rightarrow0$ as $|z|\\to\\infty$ , $i=0,1$ ; there exists $\\Lambda,L<$ $\\infty$ such that $\\left|K^{\\left(i\\right)}\\left(u\\right)\\right|\\le\\Lambda_{1}$ \u000e and, for some $\\nu>1$ , $\\left|K^{\\left(i\\right)}\\left(u\\right)\\right|\\le\\Lambda_{1}\\left\\|u\\right\\|^{-\\nu}$ for $\\|u\\|\\geq L$ , $i=0,1$ .   2. $\\textstyle\\int_{\\mathbb{R}}z^{i}K\\left(z\\right)d z=0,i=1,\\ldots,r-1$ , and $\\begin{array}{r}{\\int_{\\mathbb{R}}|z|^{r}|K\\left(z\\right)|d z<\\infty}\\end{array}$ , for some $r\\geq0$ .\nTHEOREM 2. Assume (A.1)–(A.3) hold and $(s,\\tau)\\mapsto w(s,\\tau)$ is continuously differentiable on $[0,T]\\times[0,T]$ . Then ${\\mathrm{RV}}_{w}\\left(\\tau\\right)$ given in (8) satisfies $$\\sqrt{\\Delta^{-1}}\\left\\{\\mathrm{RV}_{w}\\left(\\cdot\\right)-\\int_{0}^{T}w\\left(s,\\cdot\\right)\\sigma_{s}^{2}d s\\right\\}\\to^{d}Z_{w}\\left(\\cdot\\right),$$ where $Z_{w}\\left(\\cdot\\right)$ is a zero mean Gaussian process with covariance $$\\operatorname{Cov}\\left(Z_{w}\\left(\\tau_{1}\\right),Z_{w}\\left(\\tau_{2}\\right)\\right)=2\\int_{0}^{T}w\\left(s,\\tau_{1}\\right)w\\left(s,\\tau_{2}\\right)\\sigma_{s}^{4}d s.$$\nTHEOREM 3. Assume that (A.1)–(A.4) hold and $K$ satisfies $(K.I)$ with $r\\geq m+\\gamma$ . Then, for any $a\\to0$ with $a/h\\to0$ , $$\\operatorname*{sup}_{\\tau\\in\\left[a,T-a\\right]}\\left|\\hat{\\sigma}_{\\tau}^{2}-\\sigma_{\\tau}^{2}\\right|=O_{P}\\left(h^{m+\\gamma}\\right)+O_{P}\\left(\\log\\left(n\\right)/\\sqrt{n h}\\right).$$ $A s n h\\rightarrow\\infty$ and $n h^{2(m+\\gamma)+1}\\rightarrow0$ , $$\\sqrt{\\Delta^{-1}h}\\left\\{\\hat{\\sigma}_{\\tau}^{2}-\\sigma_{\\tau}^{2}\\right\\}\\rightarrow^{d}N\\left(0,2\\sigma_{\\tau}^{4}\\int_{\\mathbb{R}}K^{2}\\left(z\\right)d z\\right)$$ for any $\\tau\\in(0,T)$ with asymptotic independence across distinct points.\nTHEOREM 4. Assume that (A.1)–(A.4) hold and $K$ satisfies $(K.I)$ with $r\\geq m+\\gamma.L e t g\\left(t,x\\right)$ be continuous in t and twice continuously differentiable in $x.A s n h^{2(m+\\gamma)}\\rightarrow0$ , $a/h\\to0$ , and $a/\\sqrt\\Delta\\rightarrow0$ , $$\\sqrt{\\Delta^{-1}}\\left\\{\\widehat{\\bf I V}_{g}-{\\bf I V}_{g}\\right\\}\\to^{d}N\\left(0,2\\int_{0}^{T}\\left[\\frac{\\hat{\\sigma}g\\left(s,\\sigma_{s}^{2}\\right)}{\\hat{\\sigma}\\sigma_{s}^{2}}\\right]^{2}\\sigma_{s}^{4}d s\\right).$$\n\nThis section derives the asymptotics of the volatility estimators under specific assumptions, including independence of mean and volatility processes from the Brownian motion, local boundedness of volatility, and smoothness conditions on the volatility process."
  },
  {
    "qid": "econ-empirical-1104-1-0-3",
    "question": "4) The text suggests that economic status improvements are the primary reason for declines in child labor in the second quintile. Propose a counterfactual analysis to quantify this effect, specifying the key equations and identifying assumptions.",
    "gold_answer": "Counterfactual analysis steps:\n1. Let \\( \\Delta L_q \\) be the decline in child labor for quintile \\( q \\) from 1993 to 1998.\n2. Let \\( \\Delta E_q \\) be the change in per capita expenditure for quintile \\( q \\).\n3. Estimate the counterfactual decline \\( \\Delta L_q^{CF} = \\beta \\Delta E_q \\), where \\( \\beta \\) is the elasticity from Q1.\n4. Compare \\( \\Delta L_q^{CF} \\) to the observed \\( \\Delta L_q \\).\n5. Key assumption: No other shocks (e.g., schooling policies) affect \\( \\Delta L_q \\).",
    "question_context": "The dramatic improvement in economic status in Vietnam during the 1990s is evident in Figure 1. Despite being deflated to be in the same units, the mass of the entire distribution of per capita expenditure is shifted right in 1998. Large declines in the population living in households that can afford 2,100 calories per day and declines in the overall poverty rate accompany this dramatic improvement in the per capita expenditure distribution.\nThe negative relationship between child labor and household expenditure is evident in Column 1 of Table 2. The probability that a child works declines with each quintile in 1993 from a high of 39 percent of children 6-15 in the poorest quintile to 16 percent of children in the top quintile in 1993.\nIn the decomposition that follows, economic status improvements appear to be the primary reason for declines in child labor in this group. While most households experience improvements in per capita expenditure, the decomposition below suggests that the improvements in economic status for this second quintile group are large enough to significantly reduce the need for children to work.\n\nThe study explores the link between economic status improvements and child labor using data from the Vietnam Living Standards Surveys (VLSS) for 3,347 panel households with children aged 6 to 15. The analysis focuses on the logarithm of per capita expenditure as a measure of economic status and examines child labor participation rates across different expenditure quintiles."
  },
  {
    "qid": "econ-empirical-1346-1-0-3",
    "question": "4) Derive the food import equation, explaining the inverse relationship with domestic agricultural output and the role of relative rice prices. How does the model account for quality differences in rice?",
    "gold_answer": "1. The food import equation is \\( \\frac{F_{if}}{N} = 0.00396 - 0.0618 \\frac{Y_f}{N} + 0.000463 \\frac{p_d}{p_m} + 0.00405 D \\).\n2. Domestic output \\( Y_f \\) inversely affects imports due to substitution effects.\n3. Relative rice prices \\( \\frac{p_d}{p_m} \\) influence imports; higher domestic prices encourage imports.\n4. Quality differences are captured by the coefficient on \\( \\frac{p_d}{p_m} \\), reflecting Japanese preferences for domestic rice.\n5. The dummy variable \\( D \\) accounts for post-war shifts in import patterns.",
    "question_context": "Consumption: We made use of the ordinary aggregate consumption function in our model, but we explicitly took into account the effects of population size, taxes, lags, and income distribution. Since the equation is expressed in real terms, we might say that we also took into account the price level. As can be seen in equation (l), our income and consumption variables are measured per capita; this brings population into the equation. The income variable is disposable income; therefore taxes and transfer payments are introduced. By writing current consumption as a function of last year's consumption we have lag effects, and it is well known that this form is a transformation of a distributed lag of the Koyck type.\nInvestment: The propensity to invest, as expressed in our model, resembles similar relationships used in many mathematical models of the trade cycle. Investment, capital stock, property income, and interest rate are the familiar variables used. We relate the percentage rate of expansion of fixed capital (investment divided by the stock of capital) to the rate of return on capital (property income as a percent of fixed capital). The rate of interest as a cost factor deterring investment has been unsuccessfully explored in many econometric models.\nExports: Japanese sales abroad are made to depend on two main variables, world activity and relative prices. The activity variable could be an index of world production, an index of world income, or an index of world trade. We have chosen world trade for our variable. It is probably more accurately measured over the span of our sample than are the other possible variables. We assume implicitly that Japan enjoys some marginal share of the world market. Exports could also be affected by relative price which we measure by the ratio of an index of world export prices to an index of Japanese export prices.\nImports: Three categories of imports are distinguished. They are food, materials, and all other. Food imports are singled out for separate study because they are, in Japan, related to domestic food production as a supplement. When domestic agricultural output is high, food imports tend to be low and vice versa. This is particularly true for rice, the dominant food crop. Our equation posits an inverse relation between real per capita food imports and real per capita agricultural output. Another relevant variable is the relative rice price, the ratio between the domestic and imported price of rice.\nProduction: Nonagricultural output is, as usual, made to depend on labor and capital as input factors. Technological trends were also considered but later dropped in the statistical estimation of coefficients. In agriculture, we do not have separate capital input data. We related output per agricultural worker to important factors that affect farm production. One of these is natural conditions, expressed in the form of a regularly published series on damaged crop area as a result of adverse growing conditions.\nPrices and wages: In the U.K. and Dutch models, price mark-up equations for the open economy have been used. These equations show the mark-up of final price over prime unit costs. The main prime costs are wages and import prices. The U.S. and Canadian models differ by using equations for labor's share. These might be interpreted as marginal productivity conditions if the production relations can be approximated by equations of the Cobb-Douglas type.\nLiquidity preference: The Keynesian theory of demand for cash is a generalization over the classical quantity theory in that velocity (or its reciprocal) is not assumed to be constant. It is assumed to be variable and dependent on the rate of interest. In its most general form, this theory expresses the nominal amount of cash as a joint function of money income and interest rate. This theory is based on the assumption that assets may be held in two forms, cash or securities (bonds). We have broadened the scope of possible asset forms to include goods, or, in a sense, equity investments.\nPopulation: In most econometric models, population is considered to be exogenous. An important exception is the work of Valavanis-Vail on an American growth model. He starts from the identity \\(N_{t}=N_{t-1}(1+b_{t}-d_{t}-e_{t})\\), which states that population \\((N_{t})\\) of period \\(t\\) is equal to population of period \\((t-1)\\) augmented by births and decreased by deaths and net emigration. The rates of birth, death, and emigration for the basic time unit considered are \\(b, d,\\) and \\(e\\).\nLabor Supply: Two aspects of labor supply are given explicit treatment in the model. Both are significant in the Japanese scene. Following our prior explanation of population development, we want an expression to show how many of the population are available for productive effort. We have a labor force participation rate--the ratio of labor force to population—which is a simple function of time. The rate of participation is slowly growing, apart from a wartime setback.\n\nThe model is a Keynesian-type model of effective demand for an open economy, incorporating consumption, investment, export, import, production functions, and wage-price determination equations. It specifically addresses the Japanese economy's unique features, such as the relationship between the agricultural sector and the rest of the economy."
  },
  {
    "qid": "econ-empirical-375-2-0-0",
    "question": "1) Derive the expressions for $\\alpha_{1}$ and $\\alpha_{2}$ and explain their economic interpretation in the context of the socially optimal policy.",
    "gold_answer": "1. **Derivation of $\\alpha_{1}$**: \n   - $\\alpha_{1} = \\frac{2c_{1}}{(2-\\pi)(c_{1}+c_{2})}$ is derived from the indifference condition between sequences $\\{(1,1)(1,2)^{\\infty}\\}$ and $(1,2)^{\\infty}$.\n   - **Interpretation**: It represents the threshold belief above which the social planner prefers both firms to search $S_{1}$ initially before diversifying.\n\n2. **Derivation of $\\alpha_{2}$**: \n   - $\\alpha_{2} = 1 - \\frac{2c_{2}}{(2-\\pi)(c_{1}+c_{2})}$ is derived symmetrically for $S_{2}$.\n   - **Interpretation**: It represents the threshold belief below which the social planner prefers both firms to search $S_{2}$ initially before diversifying.",
    "question_context": "Recall that our assumptions about the costs $c_{1}$ and $c_{2}$ imply that regardless of the history, neither firm will ever stop searching. This implies that a pair of pure strategies for the two firms and an initial belief $p_{1}(0)$ define an outcome path, i.e., an infinite sequence $\\{a(t),b(t)\\}_{t=1}^{\\infty}$.\nThe social optimum. Now we consider the socially optimal solution for the infinite-horizon model. Let $\\alpha_{1}=\\frac{2c_{1}}{(2-\\pi)(c_{1}+c_{2})}$, and let $\\alpha_{2}=1-\\frac{2c_{2}}{(2-\\pi)(c_{1}+c_{2})}$.\nProposition 1. The socially optimal policy in the infinite-horizon problem is for both firms to search at $S_{2}$ if $p_{1} < \\alpha_{2}$, both to search at $S_{1}$ if $p_{1} \\ge \\alpha_{1}$, and one to search at each site if $\\alpha_{2} \\leq p_{1} < \\alpha_{1}$.\nThe noncooperative game. Consider the following pair $(\\sigma^{A},\\sigma^{B})$ of stationary pure strategies for the noncooperative game. $\\sigma^{A}$: choose $S_{1}$ if $p_{1} \\geq \\beta$ and $S_{2}$ if $p_{1} < \\beta$. $\\sigma^{B}$: choose $S_{1}$ if $p_{1} \\geq \\beta_{1}$ or if $\\beta_{2} \\le p_{1} < \\beta$; choose $S_{2}$ if $p_{1} < \\beta_{2}$ or if $\\beta \\le p_{1} < \\beta_{1}$.\nProposition 2. In stationary pure-strategy equilibrium, both firms choose $S_{1}$ if $p_{1} \\geq \\beta_{1}$, both choose $S_{2}$ if $p_{1} < \\beta_{2}$, and each chooses a different site if $\\beta_{2} \\le p_{1} < \\beta_{1}$.\n\nThis section analyzes perfect Bayesian equilibria in stationary strategies and compares them with the socially optimal policy in an infinite-horizon search model with two firms and two sites."
  },
  {
    "qid": "econ-empirical-1752-0-0-1",
    "question": "2) Show how the PVAR(1) model \\(\\mathbf{w}_{i t}=(\\mathbf{I}_{m}-\\Phi)\\pmb{\\mu}_{i}+\\Phi\\mathbf{w}_{i,t-1}+\\pmb{\\varepsilon}_{i t}\\) can be rewritten in terms of the deviations \\(\\pmb{\\xi}_{i t} = \\mathbf{w}_{i t} - \\pmb{\\mu}_{i}\\) and derive the associated error correction form.",
    "gold_answer": "Substituting \\(\\pmb{\\xi}_{i t} = \\mathbf{w}_{i t} - \\pmb{\\mu}_{i}\\) into the PVAR(1) model yields:\n\n\\[\n\\pmb{\\xi}_{i t} = \\Phi \\pmb{\\xi}_{i,t-1} + \\pmb{\\varepsilon}_{i t}.\n\\]\n\nThe error correction form is obtained by subtracting \\(\\pmb{\\xi}_{i,t-1}\\) from both sides:\n\n\\[\n\\Delta \\pmb{\\xi}_{i t} = (\\Phi - \\mathbf{I}_{m}) \\pmb{\\xi}_{i,t-1} + \\pmb{\\varepsilon}_{i t}.\n\\]\n\nThis form highlights the adjustment dynamics when the process deviates from equilibrium.",
    "question_context": "The asymptotic variances of the GMM estimators that are based on levels in addition to first differences of the model variables depend on the variance of the individual effects, whereas by construction the fixed effects QML estimator is not subject to this problem.\nThe PVAR(1) model is given by: \\(\\mathbf{w}_{i t}=(\\mathbf{I}_{m}-\\Phi)\\pmb{\\mu}_{i}+\\Phi\\mathbf{w}_{i,t-1}+\\pmb{\\varepsilon}_{i t}\\), where \\(\\Phi\\) is an \\(m \\times m\\) matrix of slope coefficients, \\(\\pmb{\\mu}_{i}\\) is an \\(m \\times 1\\) vector of individual-specific effects, and \\(\\pmb{\\varepsilon}_{i t}\\) is an \\(m \\times 1\\) vector of disturbances.\nThe RE-QML estimator of \\(\\pmb{\\theta}\\) is derived by maximizing the log-likelihood function: \\(\\mathfrak{L}(\\pmb{\\theta})=-\\frac{m N(T+1)}{2}\\log(2\\pi)-\\frac{N}{2}\\log|\\pmb{\\Sigma}_{\\pmb{\\eta}}|-\\frac{N}{2}\\operatorname{tr}(\\pmb{\\Sigma}_{\\mathbf{w}}^{-1}\\pmb{S}_{N,\\mathbf{w}})\\), where \\(\\pmb{\\Sigma}_{\\pmb{\\eta}}\\) is the covariance matrix of the transformed variables.\n\nThis section discusses the estimation and inference in panel vector autoregressions (PVARs) where the individual effects can be random or fixed, the time-series properties of the model variables are unknown a priori, and the time dimension of the panel is short while the cross-sectional dimension is large. The focus is on comparing Generalized Method of Moments (GMM) and Quasi Maximum Likelihood (QML) estimators in terms of their asymptotic and finite-sample properties."
  },
  {
    "qid": "econ-empirical-428-1-0-2",
    "question": "3) Show formally why the presence of a union leads to overemployment in equilibrium, using the model's assumption of decreasing returns to scale in production.",
    "gold_answer": "1. Union wage demand $w_{\\mathrm{u}t}$ must satisfy $f'(Q) \\geq w_{\\mathrm{u}t}/a$ to avoid replacement.  \n2. Under decreasing returns ($f''(Q) < 0$), replacing a union of size $q_{\\mathrm{u}}$ requires hiring $a q_{\\mathrm{u}}$ outsiders, raising $f'(Q)$ significantly.  \n3. The firm's marginal cost of hiring outsiders is $w_t$, but the union exploits the convexity of $f'(Q)$, leading to $q_{\\mathrm{u}} > q^*$ (efficient level).  \n4. Overemployment arises because the union's threat increases the marginal product of the last worker retained.",
    "question_context": "The firm's production function $f(Q)$ is strictly increasing and concave in efficiency units of labour $Q\\geq0$ and satisfies the Inada conditions. The firm faces a (constant) price $p=1$ in the product market. There is a continuum of $\\bar{q}$ workers in the labour market who can each supply one unit of labour.\nAll workers can obtain outside employment at wage $w$. However, workers are heterogeneous from the point of view of the firm: 'Outsiders' have no experience with the firm and no firm-specific skills. 'Insiders' have firm-specific skills and are $a>1$ times as productive as outsiders. The employment of $q_{\\mathrm{i}}$ insiders and $q_{\\mathrm{o}}$ outsiders allows the firm to produce with $Q=a q_{\\mathrm{i}}+q_{\\mathrm{o}}$ efficiency units of labour.\nThe firm operates in an uncertain environment because the outside wage $w$ fluctuates randomly over time. In each period $t$, the workers and the firm bargain about a wage contract for the current period after having observed the realisation of the outside wage $w_{t}$, which is drawn each period from the same distribution $\\Gamma(w)$ with strictly positive density $\\gamma(w)$ on $[\\underline{w},\\bar{w}]$ and $\\underline{w}>0$.\n\nThe paper constructs a new basis for insider-outsider theory by modeling differences between inside and outside workers, focusing on contract dynamics. It maintains the standard assumption that inside workers are cheaper per efficiency unit due to firm-specific skills or hiring costs. The model shows that overemployment is a necessary result of short-term contracting in dynamic settings."
  },
  {
    "qid": "econ-empirical-79-9-0-0",
    "question": "1) Derive the value function $\\hat{V}^{G}(s;\\tilde{\\Pi})$ for the fiscal authority during the grace period, including the constraints on debt evolution. Explain how inflation policy $\\tilde{\\Pi}$ affects the optimal consumption path $C^{G}(s,\\bar{t};\\tilde{\\Pi})$.",
    "gold_answer": "1. The value function is given by:\n   $$\n   \\hat{V}^{G}(s;\\tilde{\\Pi})=\\operatorname*{max}_{c(t)}\\int_{0}^{\\delta}e^{-\\rho t}u(c(t))d t+e^{-\\rho\\delta}\\hat{V}(0),\n   $$\n   subject to:\n   $$\n   \\begin{array}{r l}&{\\dot{b}(t)=c(t)-y+[\\tilde{r}-\\tilde{\\Pi}(t)]b(t),}\\ &{\\dot{b}(0)=b,\\quad b(\\delta)=0,\\quad\\mathrm{and}\\quad\\dot{b}(t)\\leq-\\tilde{\\Pi}(t)b(t).}\\end{array}\n   $$\n2. Higher inflation $\\tilde{\\Pi}(t)$ relaxes the debt constraint by reducing the real value of debt, allowing for higher consumption $C^{G}(s,\\bar{t};\\tilde{\\Pi})$ during the grace period.",
    "question_context": "The fiscal authority’s repayment problem depends on the amount of debt outstanding as well as the contracted interest rate, $\\tilde{r}$. Let $s\\equiv(b,\\tilde{r})$ denote these individual states. The repayment problem also depends on the inflation policy of the central bank during the grace period, which we denote by the function $\\tilde{\\Pi}:[0,\\bar{\\delta}]\\to[0,\\bar{\\pi}]$.\nThe value $\\hat{V}^{G}$ is decreasing in the individual fiscal authority’s debt $b$ and interest rate $\\tilde{r}$ because both increase the real amount to be repaid over the grace period given inflation.\nThe fiscal authority will choose to default when $\\hat{V}^{G}<\\hat{V}$ and repay when $\\hat{V}^{G}\\ge\\hat{V}$.\n\nThis section characterizes the equilibrium response to a rollover crisis, focusing on the fiscal authority's decision to repay or default during a grace period. The analysis includes the impact of monetary policy and the interdependence of fiscal authorities."
  },
  {
    "qid": "econ-empirical-1556-5-2-0",
    "question": "1) Why do capital quality shocks produce a larger drop in asset prices compared to TFP shocks?",
    "gold_answer": "1. Capital quality shocks directly reduce the effective capital stock $\\xi_{t}K_{t}$, lowering marginal productivity.  \n2. This amplifies the decline in asset prices ($Q_{t}$) relative to TFP shocks, which only indirectly affect capital productivity.  \n3. The larger drop in $Q_{t}$ exacerbates the fall in bank equity ($N_{t}$), leading to a sharper countercyclical leverage spike.",
    "question_context": "Capital quality shocks generate fluctuations in leverage that are even more countercyclical (with respect to GDP and assets) than those produced by TFP shocks.\nA negative capital quality shock produces a larger drop in asset prices than that produced by a negative TFP shock, a logical consequence of the former's direct effect on the effective capital stock.\nUnlike TFP shocks, capital quality shocks go some way towards explaining the fall in bank assets during the last recession. Like TFP shocks, however, they are not able to explain the large and long-lasting fall in bank leverage that took place from 2008 onwards.\n\nCapital quality shocks are introduced to assess their distinct effects on financial aggregates and their ability to explain observed leverage dynamics."
  },
  {
    "qid": "econ-empirical-331-4-0-0",
    "question": "1) Using the findings from McConnell and Perez-Quiros (2000) and Ahmed et al. (2002), derive a formal model that captures the relationship between the decline in volatility of manufacturing inventories and the role of materials and supplies inventories. Assume a production function where output \\( Y_t \\) depends on inventories \\( I_t \\) and materials \\( M_t \\).",
    "gold_answer": "1. Start with a production function: \\( Y_t = A_t I_t^{\\alpha} M_t^{\\beta} \\), where \\( A_t \\) is technology.\n2. Assume inventories and materials follow AR(1) processes: \\( I_t = \\rho_I I_{t-1} + \\epsilon_t^I \\), \\( M_t = \\rho_M M_{t-1} + \\epsilon_t^M \\).\n3. Compute the variance of output: \\( \\text{Var}(Y_t) = A_t^2 (\\alpha^2 \\text{Var}(I_t) + \\beta^2 \\text{Var}(M_t) + 2\\alpha\\beta \\text{Cov}(I_t, M_t)) \\).\n4. Show that a decline in \\( \\text{Var}(M_t) \\) leads to a decline in \\( \\text{Var}(Y_t) \\), consistent with the findings.",
    "question_context": "We have analyzed an event that has been documented and studied in recent macroeconomic literature: the decline in U.S. output volatility. Although we confirm the finding of McConnell and Perez-Quiros (2000) of a decline in the volatility of manufacturing and durable goods inventories in the 1980s, we find evidence of multiple breaks, which suggests that the structural change in the variance might not be a one-time phenomena.\nIn agreement with Ahmed et al. (2002), we find that the decline in variance is a phenomenon that extends not only to manufacturing inventory series, but also to sales. In addition, we find that the decline in volatility of manufacturing inventories is accounted for mainly by a decline in that of materials and supplies.\nThus two relevant features of our results are that (a) inventories of materials and supplies played an important role in accounting for the decline in inventory and output volatility, and (b) breaks in the variance of inventories and sales are not a one-time phenomena.\n\nThe text discusses the decline in U.S. output volatility, confirming findings from McConnell and Perez-Quiros (2000) and identifying multiple breaks in variance. It highlights the role of materials and supplies inventories in this decline and suggests that better inventory tracking technology and monetary policy may explain some of the moderation in volatility."
  },
  {
    "qid": "econ-empirical-605-0-0-2",
    "question": "3) Analyze the role of 'irrational types' in the bargaining model. How does slight ex ante irrationality lead to significant reputational effects ex post?",
    "gold_answer": "1. **Irrational Types**: These types have fixed demands and acceptance rules.  \n2. **Reputation Building**: Rational players mimic irrational types to exploit their inflexibility.  \n3. **Ex Post Effects**: Observed behavior amplifies the perceived likelihood of irrationality, making reputational effects larger than the underlying uncertainty.  \n4. **Delay and Inefficiency**: The war of attrition structure arises, leading to delay until rationality is revealed.",
    "question_context": "The paper develops a reputation based theory of bargaining. The idea is to investigate and highlight the influence of bargaining 'postures' on bargaining outcomes. A complete information bargaining model a la Rubinstein is amended to accommodate 'irrational types' who are obstinate, and indeed for tractability assumed to be completely inflexible in their offers and demands.\nA strong 'independence of procedures' result is derived: after initial postures have been adopted, the bargaining outcome is independent of the fine details of the bargaining protocol so long as both players have the opportunity to make offers frequently.\nThe equilibrium outcome reflects the combined influence of the rates of time preference of the players and the ex ante probabilities of different irrational types. As the probability of irrationality goes to zero, delay and inefficiency disappear; furthermore, if there is a rich set of types for both agents, the limit equilibrium payoffs are inversely proportional to their rates of time preference.\n\nThe paper develops a reputation-based theory of bargaining by amending Rubinstein's complete information bargaining model to include 'irrational types' who are inflexible in their offers and demands. The model investigates how initial bargaining postures influence outcomes, deriving a unique continuous-time limit with a war of attrition structure."
  },
  {
    "qid": "econ-empirical-324-3-0-2",
    "question": "3) Explain the empirical significance of choosing $\\iota = 2/3$ for the distance metric $d_{\\iota}(p,f)$ in the analysis.",
    "gold_answer": "1. **Robustness**: The choice of $\\iota = 2/3$ is empirically justified as the results are robust across values ($\\iota = 1/3$, $1$). \n2. **Interpretation**: This intermediate value balances the influence of outlier distances, providing a stable measure for heterogeneous patent-firm distances.",
    "question_context": "The distance between two technology classes, $X$ and $Y$, is defined as: $$d(X,Y)\\equiv1-\\frac{\\#(X\\cap Y)}{\\#(X\\cup Y)},$$ where $0\\leq d(X,Y)\\leq1$.\nThe distance of a patent $p$ to a firm $f$ is computed as: $$d_{\\iota}(\\boldsymbol{p},f)\\equiv\\left[\\frac{1}{\\|\\mathcal{P}_{f}\\|}\\sum_{\\boldsymbol{p^{\\prime}}\\in\\mathcal{P}_{f}}d(X_{\\boldsymbol{p}},Y_{\\boldsymbol{p^{\\prime}}})^{\\iota}\\right]^{1/\\iota},$$ where $0<\\iota\\leq1$ and $\\mathcal{P}_{f}$ is the set of all patents ever invented by firm $f$ prior to patent $p$.\n\nThe notion of technological propinquity between a patent and a firm is formalized using a distance metric based on patent citations and technology classes."
  },
  {
    "qid": "econ-empirical-1381-4-1-0",
    "question": "5) Specify the difference-in-differences equation (2) referenced in the text, including the treatment and control groups, and interpret the coefficient of interest.",
    "gold_answer": "The DiD equation is:\n\\[ Y_{it} = \\alpha + \\beta_1 \\text{Post}_t + \\beta_2 \\text{Treated}_i + \\delta (\\text{Post}_t \\times \\text{Treated}_i) + \\epsilon_{it} \\]\nwhere:\n- \\(Y_{it}\\) is the outcome (e.g., wage growth),\n- \\(\\text{Post}_t\\) is a dummy for the post-policy period,\n- \\(\\text{Treated}_i\\) indicates low-wage workers (wage bin [4.5,8.5)),\n- \\(\\delta\\) captures the causal effect of the minimum wage.\nThe coefficient \\(\\delta\\) is interpreted as the differential change in outcomes for treated workers post-policy.",
    "question_context": "The table reports difference-in-differences estimates for the 2014 versus 2016 postpolicy period (columns (1) and (2)) and the 2012 versus 2014 placebo prepolicy period (columns (3) and (4)), as in columns (4) and (5) in Table II.\nIn column (1), we report difference-in-differences estimates based on equation (2) without any control variables. In columns (2)–(4), we successively add individual-level demographic control variables and industry and location fixed effects.\nThe findings described in Online Appendix A.16 further show that the minimum wage did not lead to displacement effects for any of the subgroups of low-wage workers that we considered.\n\nThis section covers the difference-in-differences methodology used to estimate the effects of the minimum wage, including robustness checks and heterogeneous effects across subgroups."
  },
  {
    "qid": "econ-empirical-1631-0-0-2",
    "question": "3) Critically evaluate the author's argument that 'the method of consecutive and partial improvements has every chance of success' given a long-term strategic vision. What are the potential pitfalls of this approach?",
    "gold_answer": "1. **Advantages**:\n   - Pragmatic: Avoids disruption from radical reforms.\n   - Adaptable: Allows for incremental adjustments based on feedback.\n2. **Pitfalls**:\n   - Path dependence: Early suboptimal choices may constrain future options.\n   - Coordination failures: Partial solutions may conflict, leading to systemic inefficiencies.\n   - Political economy: Resistance from vested interests may stall reforms.",
    "question_context": "The second part builds models for current (as distinct from strategic) optimisation in respect of the following fields : (a) the choice of qualitative and quantitative variants in the planning of foreign trade; (b) the construction of the first macroeconomic system of indirect control over export production; (c) construction of economic-financial principles for an experimental association of enterprises operating in the field of foreign trade; (d) the choice of complementary investments related to foreign trade; (e) the construction of an overall system of indirect control over foreign trade.\nPart III deals with three historical stages of application of the models - the first, based on the general guidelines of the Kalecki Commission of 1961-3; the second, of 1966-70, which includes the experiments in POLFA (the pharmaceutical export industry), and in ZPO (the Shipbuilding Industry Association); and the third, proposals for 1971-80.\nHe argues that systematic solutions to dynamic problems, relating short-term plans to long-term strategies, are likely to be worked out only by collaboration between model designers and ‘business practitioners and administrators who prefer partial and current solutions which frequently clash with the logic of the whole system'.\n\nThe book discusses the theoretical and practical experiences of a Polish research team in developing models for indirect control over foreign trade in a centrally planned economy. It covers economic calculations, optimization models, and historical applications of these models."
  },
  {
    "qid": "econ-empirical-332-0-0-2",
    "question": "3) Explain why excluding the Korean War from the sample period leads to consistent results across different identification strategies. Provide empirical or theoretical justification.",
    "gold_answer": "1. **Korean War Context**: Involved atypical fiscal mobilization (e.g., large-scale rationing, price controls), distorting standard economic relationships.\n2. **Exclusion Rationale**: \n   - Ensures homogeneity in the shock structure (post-war periods align with \"pure\" fiscal experiments).\n   - Mitigates outliers that could bias multiplier estimates.",
    "question_context": "Our new measure predicts that, after a delay, output, hours and consumption all rise following an identified positive shock to government spending. Real wages fall during the period of delayed response of the quantity variables and then start to rise. However, there is quite a lot of uncertainty surrounding these responses. The government spending multiplier is estimated, also with considerable uncertainty, to be 1.5.\nWe confirm that the most widely used alternative approaches to identifying government spending shocks lead to similar responses, once the sample period is restricted to exclude the Korean War.\nThe main advantage of focusing on military spending is that it addresses a very specific kind of fiscal experiment that is easy to replicate in a model: an exogenous change in government spending where that spending is a pure drain on production.\n\nThe article introduces a new method to measure shocks to government spending using stock returns of large military contractors, addressing shortcomings of existing methods. It compares this approach with the sign restriction strategy by Mountford and Uhlig (2002) and discusses the economic responses to such shocks."
  },
  {
    "qid": "econ-empirical-1619-3-3-0",
    "question": "6) Let the modified Hamilton filter use horizons \\( h \\in [a, b] \\). Prove that centering around \\( h = 8 \\) (e.g., [4,12]) optimally balances business cycle coverage and inflation predictability, given the spectral density results.",
    "gold_answer": "**Proof Sketch**:  \n1) **Spectral Density**: The baseline [4,12] range covers 2–8 year cycles (Burns & Mitchell 1946) with minimal leakage.  \n2) **Trade-off**:  \n- Wider bands (e.g., [2,14]) introduce low-frequency noise (\\( > 8 \\) years).  \n- Narrower bands (e.g., [6,10]) exclude relevant cycles.  \n3) **Optimality**: The 8-quarter center minimizes MSE in output growth forecasts (Table 3) while maintaining inflation forecast accuracy (Table 4).",
    "question_context": "Including longer forecast horizons increases correlations with NBER recessions but amplifies cycles beyond 8 years. A trade-off arises: longer horizons match expert gaps better, while shorter horizons improve inflation forecasts.\n\nThis section explores robustness by varying the forecast horizons used in the modified Hamilton filter, analyzing trade-offs between cyclical alignment and frequency coverage."
  },
  {
    "qid": "econ-empirical-1346-3-0-2",
    "question": "3) Derive the linear approximation for the relationship between disposable income ($Y_d$) and total income ($Y$) as stated in the text. What assumptions underlie this simplification?",
    "gold_answer": "1. **Derivation**: The linear form $Y_{d} = -3.56 + 0.92Y$ suggests:  \n   - A fixed intercept ($-3.56$) representing autonomous non-income factors.  \n   - A marginal propensity to consume (MPC) of 0.92 from total income.  \n2. **Assumptions**:  \n   - Stability in tax/transfer policies.  \n   - No structural shifts in income distribution during the extrapolation period.",
    "question_context": "Most of the equations fit well in 1959 except the investment function, the industrial production function, the cost mark-up equation, the wage determination equation, the birth rate equation and the death rate equation.\nThe model is nonlinear, and this makes the simultaneous solution for extrapolation algebraically complicated. If we fix some of the variables that are associated with the large residuals noted above, for 1959, we can enormously simplify the system by nearly reducing it to a set of linear equations.\nThe factorial income distribution ratio, $P/W_{\\mathrm{{i}}}$ .is a nonlinear element in the consumption equation, but this term was fixed at its 1959 value. Nonlinear expressions, $w N_{\\mathfrak{s}}$ and $N_{e}/N_{f}$ were approximated by a Taylor's expansion, with linear terms only, about the previous year's values.\n\nThe model was fit to sample data terminating with the year 1958. Subsequently, data became available for 1959, and the model was extrapolated to that year, given approximately current values of predetermined variables at the time of the extrapolation."
  },
  {
    "qid": "econ-empirical-1096-1-0-3",
    "question": "4) Analyze the longitudinal changes in poverty by comparing the pretransfer poverty gap reduction for families poor in both 1984 and 1986 versus those poor only in 1984. What do the differences in reduction rates imply about the effectiveness of transfers for persistent poverty?",
    "gold_answer": "1. For families poor in both years, transfers removed 76% from poverty in 1986 versus 71% in 1984, indicating improved effectiveness for persistent poverty.\n2. For families poor only in 1984, the reduction rate was lower, suggesting transfers are more effective for long-term poor.\n3. The differences imply that persistent poverty may be more responsive to transfers due to deeper income deficits or higher eligibility for targeted programs.",
    "question_context": "Total transfers of $26.3 billion in 1984 dollars eliminated more that three-quarters of the pretransfer poverty gap——the difference between actual pretransfer cash income and the poverty level--of $9.0 billion $108.4 billion at an annual rate) and reduced the pretransfer poverty rate by two-thirds, from 24.2 percent to 7.9 percent.\nWhile 69 percent of all income-conditioned transfers go to the prewelfare poor (compared to 84 percent to the pretransfer poor), only half of these benefits (52 percent) go to alleviate poverty (fill the poverty gap). Further, only one-quarter (25 percent) of the prewelfare poverty gap is filled by these programs’ benefits.\nFood Stamps is the most effectively targeted program, with 85 percent of the total going to the prewelfare poor and 81 percent of Food Stamp benefits going to alleviate prewelfare poverty. Unfortunately, the size of the program is so small ($702 million) when compared to the prewelfare poverty gap ($4.2 billion)that it fills only a small fraction (6 percent) of the prewelfare poverty gap.\nThe trend downward in pretransfer poverty (from 26.0 percent in April 1984 to 24.2 percent in April 1986) tends to make income-conditioned transfers less target-efficient, that is, a given level of transfers, if spread out over the same families, will go to fewer poor.\nTotal Social Security and Medicare transfers were up $3.3 billion from 1984 to 1986 (in 1984 dollars). A substantial portion of this increase was due to the aging of the population under study-an increase from 17.3 million elderly families in 1984 to 19.8 million in 1986.\nSocial Security alone led to a significant increase in the percentage of the pretransfer poverty gap filled, from 38 percent to 43 percent. Individual Social Security payments rose at the rate of inflation, as measured by the Consumer Price Index (CPI), 5.3 percent over the two years, yet total Social Security payments rose an additional 11.3 percent, reflecting the 14.4 percent increase in the number of elderly families.\nTotal Medicare outlays rose 21.5 percent from 1984 to 1986, even faster than the CPI for medical items (which rose 14.2 percent). This explosion in Medicare outlays was primarily responsible for the significant increases in the percent of the total pretransfer poverty gap filled by various combinations of transfers shown in the table.\nThe average pretransfer poverty gap is down $99, or 26 percent (the prewelfare gap is down $154 or 42 percent). All transfers filled 86 percent of their April 1986 pretransfer poverty gap and removed 53 percent of families poor before transfers in 1984 from poverty in 1986.\n\nThe text discusses the effects of transfers on poverty, focusing on pretransfer and prewelfare poverty gaps, the effectiveness of various transfer programs, and longitudinal changes in poverty rates from 1984 to 1986."
  },
  {
    "qid": "econ-empirical-1430-0-0-3",
    "question": "4) Formalize the integrability constraint from Theorem 4.3's proof for the case where agent $i$ has a 2D signal $(s_1^i,s_2^i)$ affecting alternatives $A$ and $B$. Show how it leads to impossibility when private and social MRS differ.",
    "gold_answer": "For alternatives $A,B$:\n1. Let $W(s_1^i,s_2^i)$ be social welfare difference between $A$ and $B$.\n2. Incentive compatibility requires:\n   $$ \\frac{\\partial t_A}{\\partial s_1^i} - \\frac{\\partial t_B}{\\partial s_1^i} = \\frac{\\partial V_A^i}{\\partial s_1^i} $$\n   $$ \\frac{\\partial t_A}{\\partial s_2^i} - \\frac{\\partial t_B}{\\partial s_2^i} = \\frac{\\partial V_A^i}{\\partial s_2^i} $$\n3. Cross-differentiating yields the integrability condition:\n   $$ \\frac{\\partial^2 V_A^i}{\\partial s_1^i \\partial s_2^i} = \\frac{\\partial^2 V_A^i}{\\partial s_2^i \\partial s_1^i} $$\n4. Efficiency requires $W(s_1^i,s_2^i) = 0$ along indifference curves, imposing a different cross-derivative condition via the social MRS. These can only coincide under the nongeneric congruence condition.",
    "question_context": "We show that such mechanisms exist only if a congruence condition relating private and social rates of information substitution is satisfied. If signals are multi-dimensional, the congruence condition is determined by an integrability constraint, and it can hold only in nongeneric cases where values are private or a certain symmetry assumption holds. If signals are one-dimensional, the congruence condition reduces to a monotonicity constraint and it can be generically satisfied.\nAgent i's payoff in alternative $k$ is given by $$ V_{k}^{i}(s^{i},s^{-i})=\\sum_{j=1}^{N}a_{k i}^{j}s^{j} $$ where $s^{j}\\in[\\underline{{s}}^{j},\\overline{{s}}^{j}]$ denotes the one-dimensional signal of agent $j$.\nThe weak congruence condition is given by $$ \\forall i,\\forall k,k^{\\prime},a_{k i}^{i}>a_{k^{\\prime}i}^{i}\\Rightarrow\\sum_{j=1}^{N}a_{k j}^{i}>\\sum_{j=1}^{N}a_{k^{\\prime}j}^{i}. $$\n\nThe paper studies efficient, Bayes-Nash incentive compatible mechanisms in a social choice setting with informational and allocative externalities. It establishes conditions under which such mechanisms exist, focusing on the congruence between private and social rates of information substitution."
  },
  {
    "qid": "econ-empirical-484-4-0-1",
    "question": "2) Using the $R^2 = 0.88$ and the regression coefficient $-1.9$, construct a confidence interval for the coefficient and discuss its statistical significance.",
    "gold_answer": "1. The high $R^2 = 0.88$ suggests the model explains 88% of the variance in the data.  \n2. The coefficient $-1.9$ is statistically significant at $p < 0.05$, as values below $-1.6$ occur in less than 5% of samples.  \n3. A 95% confidence interval can be approximated as $-1.9 \\pm 1.96 \\cdot SE$, where SE is the standard error. Given the tight distribution around $-1.9$, the interval likely excludes zero, confirming significance.",
    "question_context": "The basic pattern in the left panel is that all OECD countries, including the US, are clustered at the top left of the graph, with relatively low internal inequality and contribution to the reduction in global inequality, at least when compared to the ‘outlier’ countries. These outliers are all GCC countries, with the exception of the Bahamas (BHS), which is close to Saudi Arabia: Oman is OMN, Bahrain is BHR, Kuwait is KWT, the UAE is ARE, and Qatar is QAT. The inclusion of these countries creates a strong negative relationship between internal equality and openness to inequality-reducing migration. The relationship is significant at any standard statistical level and the $\\mathbf{R}^{2}$ is 0.88, with the point estimate of the regression coefficient at 1.9.12\nFurthermore, this relationship is highly significant; coefficients smaller in magnitude than $-1.6$ would lead to this finding in less than $5\\%$ of samples. This implies that in exchange for increasing inequality within a country by a single MLD point a country can reduce global inequality by roughly two MLD points (per native).\n\nThe text discusses the trade-off between internal equality and openness to migration that reduces global inequality, comparing OECD and GCC countries."
  },
  {
    "qid": "econ-empirical-859-2-0-2",
    "question": "3) Explain why constraints (19) and (20) bind when the merged firm's bargaining power $\\alpha_{m}$ exceeds $\\overline{{\\alpha}}_{m}$ in the no-bundling case, and how this leads to quantity distortions.",
    "gold_answer": "When $\\alpha_{m} > \\overline{{\\alpha}}_{m}$: 1. The merged firm seeks to extract more surplus by raising $F_{1}$ and $F_{2}$. 2. Constraints (19) and (20) ensure the retailer earns higher profit by carrying both products than by dropping one. 3. If these constraints bind, the merged firm cannot raise $F_{1}$ or $F_{2}$ further without violating them. 4. To relax the constraints, the merged firm distorts quantities downward, reducing $q_{1}$ and $q_{2}$ below the efficient levels. 5. This allows $F_{1}$ and $F_{2}$ to rise, increasing the Nash product, but at the cost of lower joint profit.",
    "question_context": "Suppose manufacturers 1 and 2 merge. This alters negotiations in potentially three ways. First, it affects the retailer's disagreement profit with the merged firm. After the merger, the retailer's disagreement profit is the profit it would earn if it did not sell products 1 and 2. Second, it may affect the retailer's bargaining power. After the merger, the retailer's bargaining weight in the Nash bargaining solution with respect to the newly merged firm, $(1-\\alpha_{m})\\in[0,1]$, may differ from what it was with respect to each firm separately. Third, it affects the contracts the merged firm may be able to negotiate. After the merger, contracts in which the payments for $q_{1}$ and $q_{2}$ are interdependent may be feasible.\nFormally, let $T_{m}(q_{1},q_{2})$ be the merged firm's contract with the retailer. Then $T_{m}(q_{1},q_{2})$ exhibits bundling if and only if there does not exist $T_{1}(q_{1})$ and $T_{2}(q_{2})$ such that $T_{m}(q_{1},q_{2})=T_{1}(q_{1})+T_{2}(q_{2})$ for all $q_{1},q_{2}\\geq0$.\nLet $C_{m}(q_{1},q_{2})$ be the post-merger cost of producing $q_{1}$ units of product 1 and $q_{2}$ units of product 2. For now, we assume the merger does not affect costs so that $C_{m}(q_{1},q_{2})=C_{1}(q_{1})+C_{2}(q_{2})$.\nThe Nash bargaining solution between the merged firm and retailer solves: $$\\operatorname*{max}_{(q_{1},q_{2},T_{m}(\\cdot,\\cdot))\\in\\mathcal{A}_{m}(\\mathbb{T}_{-1,2}^{B})}(\\pi_{m}-d_{m})^{\\alpha_{m}}\\left(\\pi_{r}-d_{r_{m}}\\right)^{(1-\\alpha_{m})},$$ where $d_{m}$ and $d_{r_{m}}$ are the disagreement profits of the merged firm and retailer.\nProposition 2. Suppose two manufacturers merge. Then, if the merged firm can bundle its products, the fully integrated outcome is realized in all $N$ product bargaining equilibria.\nProposition 3. Suppose two manufacturers merge. If the merged firm cannot bundle its products, then whether the fully integrated outcome is realized depends on $\\alpha_{m}$: (i) if $\\alpha_{m}\\leq\\overline{{\\alpha}}_{m}$, the fully integrated outcome is obtained in all $N$-product bargaining equilibria; (ii) if $\\alpha_{m}>\\overline{{\\alpha}}_{m}$, the merged firm's quantities are distorted downward and the fully integrated outcome is not obtained.\n\nThis section analyzes the effects of mergers between manufacturers on bargaining outcomes, welfare, and equilibrium quantities, considering both scenarios where bundling is feasible and infeasible."
  },
  {
    "qid": "econ-empirical-18-1-0-0",
    "question": "1) Derive the capital accumulation equation $\\dot{K}_{t}=I_{t}-\\delta K_{t}$ from the given production function and depreciation assumptions.",
    "gold_answer": "1. Start with the production function $F(K_t, L_t, I_t)$.\n2. Physical depreciation occurs at a constant rate $\\delta$.\n3. The change in capital $\\dot{K}_t$ is the difference between gross investment $I_t$ and depreciation $\\delta K_t$.\n4. Thus, $\\dot{K}_t = I_t - \\delta K_t$.",
    "question_context": "Consider a competitive firm with production function, $\\theta_{t}=$ $F(K_{t}\\mathcal{L}_{t}\\mathcal{I}_{t})$ ,where $\\mathbf{Q}_{t},K_{t},L_{t}$ ,and $I_{t}$ are net output, capital, labor, and gross investment, respectively, at time t.\nFollowing Lucas [1967], we assume that the production function $\\pmb{F}(\\u)$ is concave with $F_{K}>0,F_{L}$ $>0,F_{I}<0$ and $F_{K K},F_{L L},F_{I I}<0.$\nThe accumulation of physical capital is given by $\\dot{K}_{t}=I_{t}-\\delta K_{t}.$\nThe pre-tax cash flow of the firm at time $t$ .s $p_{t}F(K_{t},L_{t},I_{t})-w_{t}L_{t}$ $-p_{K_{t}}I_{t}$ where $p_{t},w_{t}$ and $p_{K_{t}}$ are the exogenously given price of output, the wage rate, and the price of uninstalled capital, respectively.\nThe firm must pay taxes equal to a fraction $\\tau$ of its taxable income where taxable income will be defined as follows: (i) pre-tax cash flow, plus (ii) the valuation of new investment $v_{t}I_{t}$ $\\lfloor v_{t}$ will be explained below), minus (i) depreciation deductions on existing capital $D_{t}K_{t}$ ,where $D_{t}$ is the depreciation deduction per unit of capital at time t.\nThe value of the firm at time $t^{*}$ is $V_{t^{*}}=\\underset{\\{L_{t},I_{t}\\}}{\\operatorname*{max}}\\int_{t^{*}}^{\\infty}\\{(1-\\tau)[p_{t}F(K_{t},L_{t},I_{t})-w_{t}L_{t}-p_{K_{t}}I_{t}]\\qquad}\\ {-\\tau v_{t}I_{t}+\\tau D_{t}K_{t}\\}e^{-(1-\\tau)r(t-t^{*})}d t,$ where the maximization is subject to the capital accumulation equation and initial condition.\nThe current value Hamiltonian is $H=(1-\\tau)[p_{t}F(K_{t},L_{t},I_{t})-w_{t}L_{t}-p_{K_{t}}I_{t}]-\\tau v_{t}I_{t}\\qquad}\\ {+\\tau D_{t}K_{t}+q_{t}(I_{t}-\\delta K_{t}),}$ where $\\pmb q_{t}$ is the shadow price of installed capital at time $t$.\nSolving the necessary conditions for an optimum, we obtain $(1-\\tau)\\left(-p_{t}\\frac{\\partial F}{\\partial I_{t}}+p_{K_{t}}\\right)+\\tau v_{t}=q_{t}$ and $\\dot{q}_{t}=\\big[(1-\\tau)r+\\delta\\big]q_{t}-(1-\\tau)p_{t}\\frac{\\partial F}{\\partial K_{t}}-\\tau D_{t}$.\nThe tax system will be neutral with respect to investment if equations are independent of the tax parameters. Neutrality can be achieved by setting $v_{t}=q_{t}$ and $D_{t}=\\delta q_{t}-\\dot{q}_{t}$.\n\nThe model describes a competitive firm with a production function, capital accumulation, and tax implications. It includes detailed mathematical formulations and assumptions about production, depreciation, and taxation."
  },
  {
    "qid": "econ-empirical-809-1-0-0",
    "question": "1) Derive the aggregated model $y_{t}^{*}=\\rho^{*}y_{t-1}^{*}+u_{t}^{*}$ for both stock and flow variables, starting from the basic model $y_{t}=\\rho y_{t-1}+u_{t}$. Explain the differences in the aggregation process for stock versus flow variables.",
    "gold_answer": "1. **Stock Variables**:\n   - The aggregated variable is observed at intervals of $m$: $y_{t}^{*} = y_{t}$ for $t = m, 2m, \\ldots$.\n   - The model remains $y_{t}^{*} = \\rho^{m} y_{t-m}^{*} + u_{t}^{*}$, where $u_{t}^{*}$ is the aggregated error term.\n\n2. **Flow Variables**:\n   - The aggregated variable is a sum over $m$ periods: $y_{t}^{*} = (1 + L + L^{2} + \\cdots + L^{m-1})y_{t}$.\n   - Substituting the basic model:\n     $$\n     y_{t}^{*} = \\sum_{i=0}^{m-1} \\rho^{i} y_{t-i} + \\sum_{i=0}^{m-1} u_{t-i}.\n     $$\n   - This leads to $y_{t}^{*} = \\rho^{*} y_{t-1}^{*} + u_{t}^{*}$, where $\\rho^{*} = \\rho^{m}$ and $u_{t}^{*}$ is a transformed ARMA process.",
    "question_context": "Let $y_{t}$ be a variable generated by the discrete first-order process $y_{t}=\\rho y_{t-1}+u_{t},\\qquad t=1,2,\\ldots,n_{b}$, where $n_{b}$ is the number of observations in 'basic' time units and ${\\pmb u}_{t}$ is an error which is assumed to follow a finite-order stationary $\\mathbf{ARMA}(p,q)$ process whose largest AR root has modulus less than $\\rho$.\nThe aggregated variable, denoted as $y_{t}^{*}$, is simply $y_{t}$ observed at the points $t=m, 2m, 3m$, and so on for stock variables. For flow variables, $y_{t}^{*}=(1+L+L^{2}+\\cdots+L^{m-1})y_{t}$, where $L$ is the lag operator.\nThe aggregated form of the model is given by $y_{t}^{*}=\\rho^{*}y_{t-1}^{*}+u_{t}^{*},\\qquad t=m,2m,\\ldots,n_{a}m$, where ${\\pmb u}_{t}^{*}$ is a finite-order stationary $\\mathbf{ARMA}(p,q^{*})$ process, $n_{a}=[n_{b}/m]$, and $\\rho^{*}=\\rho^{m}$.\nConsider testing the unit root hypothesis $\\mathrm{H}_{0}\\colon\\rho=1$ against $\\mathrm{H}_{1}\\colon\\rho=\\mathrm{e}^{-c/n_{b}}$, using some appropriate statistic. The statistic calculated using $n_{b}$ observations is denoted by $t_{b}^{n}$, and that using $n_{a}$ observations by $t_{a}^{n}$.\nProposition 1 states that any test of the unit root hypothesis that is asymptotically independent of nuisance parameters under both $H_{0}$ and $H_{1}$ has a limiting distribution independent of the frequency of sampling, $m$.\n\nThis section discusses the asymptotic local power of unit root tests under temporal aggregation, comparing models in basic time units with their aggregated counterparts. The analysis covers both stock and flow variables, detailing the transformation of the model under aggregation and the implications for hypothesis testing."
  },
  {
    "qid": "econ-empirical-1033-0-0-3",
    "question": "4) Critically evaluate the authors' use of a dynamic discrete choice framework to model sectoral transitions. How do mobility costs and skill specificity interact in this framework?",
    "gold_answer": "The dynamic discrete choice model (extending Heckman-Sedlacek and Keane-Wolpin) assumes individuals choose among 8 alternatives (6 sector-occupations, school, home) each period. Mobility costs \\( \\delta_{jk} \\) are incurred when switching between alternatives. Skill accumulation is sector-occupation-specific: \\[ S_{ijt} = g(E_{ijt}, X_{ijt}, \\eta_{ij}) \\] where \\( E_{ijt} \\) is experience and \\( X_{ijt} \\) schooling. The interaction arises because high mobility costs reduce transitions, while skill specificity lowers the value of switching. The authors find this combination is necessary to explain observed wage persistence and employment shares.",
    "question_context": "Between 1950 and 2000, service-sector employment grew from 57 to 75 percent of total employment. However, over this time, the real hourly wage in the service sector grew only slightly faster than in the goods sector.\nOur estimates imply that there are large mobility costs: output in both sectors would have been double their current levels if these mobility costs had been zero.\nWe find that demand-side factors, that is, technological change and movements in product and capital prices, were responsible for the growth of the service sector.\n\nThe paper examines the growth of the service sector in the U.S. economy over the past 50 years, focusing on the constancy of the relative wage between the service and goods sectors despite significant shifts in employment. The authors develop a two-sector labor market equilibrium model to assess mobility costs and the factors driving sectoral growth."
  },
  {
    "qid": "econ-empirical-433-2-0-2",
    "question": "3) Derive the integral equation (16) from the text and explain its role in identifying the pairwise measurement density.",
    "gold_answer": "1. The integral equation is given by:\n$$\nf(m_{t},m_{t-1}\\mid\\Delta y_{t:t+K_{2}})=\\int f(m_{t},m_{t-1}\\mid s_{t},\\Delta y_{t})f(s_{t}\\mid\\Delta y_{t:t+K_{2}};\\theta_{0}^{s})d s_{t}.\n$$\n2. **Role**: \n   - The equation links the observed density $f(m_{t},m_{t-1}\\mid\\Delta y_{t:t+K_{2}})$ to the latent state density $f(s_{t}\\mid\\Delta y_{t:t+K_{2}};\\theta_{0}^{s})$. \n   - By Assumption 2(c), the backward filtering density is complete, ensuring a unique solution for $f(m_{t},m_{t-1}\\mid s_{t},\\Delta y_{t})$.",
    "question_context": "For any $\\theta^{s}\\in\\Theta$ and $s\\in S$ , the Markov probability kernel has a density $f(\\cdot\\mid s;\\theta^{s})$ w.r.t. the Lebesgue measure on Y × S ⊆ RD+1.\nFor some non-negative integer $K_{1}$ and any $\\theta^{s}$ ${^{1}}{^{s}},{\\tilde{\\theta}^{s}}\\in\\Theta^{s}$ with $\\theta^{s}\\neq\\tilde{\\theta}^{s}$ , there exists some $\\varDelta y_{-K_{1}:1}\\in\\mathcal{Y}^{K_{1}+1}$ such that $f(\\varDelta y_{1}\\mid\\varDelta y_{-K_{1}:0};\\theta^{s})\\neq f(\\varDelta y_{1}\\mid\\varDelta y_{-K_{1}:0};\\tilde{\\theta}^{s}).$ .\nThe linear operator $L_{s_{0}|\\bar{y}_{0},\\varDelta y_{1:K_{2}}}^{y}:\\mathcal{L}^{2}(s_{0})\\mapsto\\mathcal{L}^{2}(\\varDelta y_{1:K_{2}})$ defined by \n\n$$\n\\left({\\cal L}_{s_{0}|\\bar{y}_{0},\\Delta y_{1:K_{2}}}^{y}g\\right)(\\bar{y}_{1:K_{2}})=\\int_{{\\cal S}}g(s_{0},\\bar{y}_{0})f(s_{0}\\mid\\varDelta y_{0:K_{2}}=\\bar{y}_{0:K_{2}};\\theta_{0}^{s})d s_{0}\n$$ \n\nis one-to-one for some non-negative integer $K_{2}$ and all $\\bar{y}_{0}~\\in~\\mathcal{V}$ . Moreover, the measurement density $f(m_{t}\\mathrm{~\\textbf~{~}~})$ $m_{t-1},s_{t},\\varDelta y_{t})$ w.r.t the Lebesgue measure exists, and $f(m^{\\prime}\\mid m,\\cdot,\\varDelta y)\\in{\\mathcal{L}}^{2}(s_{0})$ for every $(m^{\\prime},m,\\varDelta y)$ .\n\nThe text discusses the identification of parameters in polynomial models, focusing on the Markov probability kernel and the conditions under which parameters can be uniquely identified from observed data."
  },
  {
    "qid": "econ-empirical-237-2-2-3",
    "question": "4) Analyze the net welfare effect of public signals when $K(c) = \\kappa c$ and $c_L = 0$. Under what conditions is the net effect negative?",
    "gold_answer": "Analysis:\n1. The net effect depends on the trade-off between direct benefits of public signals and reduced private sharing.\n2. When $\\kappa - \\frac{\\eta^{\\prime}(u(2) - u(1)) c_H \\mu_1^1}{r + \\eta^{\\prime}} < 0$, private sharing collapses ($N = 0$).\n3. For large $N$ or small $\\rho$, the loss from reduced sharing dominates, leading to a net welfare loss.",
    "question_context": "The public signals influence optimal search efforts. Given the market conditions $(\\mu,C)$, the indirect utility $V_{n}$ for nonpublic precision $n$ satisfies the Hamilton-Jacobi-Bellman equation for optimal search effort given by $$0=-(r+\\eta^{\\prime})V_{n}+\\eta^{\\prime}u_{M+n}+\\sup_{c\\in[c_{L},c_{H}]}\\left\\{-K(c)+c\\sum_{m=1}^{\\infty}(V_{n+m}-V_{n})\\mu_{m}^{c}\\right\\}.$$\nLEMMA 7.3: Suppose that $K(c)=\\kappa c$ for some $\\kappa>0$. For given market conditions $(\\mu,C)$, the trigger level $N$ in nonpublic precision of an optimal policy $C^{N}$ is decreasing in the precision $M$ of the public signals.\nPROPOSITION 7.4: Suppose that $K(c)=\\kappa c$ for some $\\kappa>0$. If $C^{N}$ is an equilibrium with $M$ public signals, then for any $M^{\\prime}\\leq M$, there exists some $N^{\\prime}\\geq N$ such that $C^{N^{\\prime}}$ is an equilibrium with $M^{\\prime}$ public signals.\n\nThis section investigates the welfare effects of providing additional public information to agents at birth. The analysis reveals that while public information directly benefits agents, it may also reduce incentives for private information sharing, potentially leading to net welfare losses."
  },
  {
    "qid": "econ-empirical-981-1-0-0",
    "question": "1) Derive the implementability condition (13) from the constraints that the seller has a single unit to sell and the reduced form mechanism $(q,p)$. Show how it ensures feasibility of the mechanism.",
    "gold_answer": "1. **Single Unit Constraint**: The seller can allocate at most one unit, so the expected probability of allocation across all buyers must satisfy $\\int_{0}^{1} q(x) \\theta(x) g(x) dx \\leq 1$.  \n2. **Reduced Form**: Using the definition of $z(x) = 1 - \\int_{x}^{1} \\theta(s) g(s) ds$, the condition becomes $\\int_{x}^{1} q(s) z'(s) ds \\leq \\frac{1}{n_t}(1 - z(x)^{n_t})$.  \n3. **Feasibility**: This ensures that the seller cannot promise allocations that exceed the probability of having at least one buyer with a value above $x$.",
    "question_context": "A symmetric CSCE can be identified by equilibria within a period in the following way. Suppose that a given mechanism $m^{*}$ is proposed as a candidate for an equilibrium within a period. Let $^m$ be any other mechanism, and suppose one seller deviates and plays this other mechanism. Given these mechanisms, and the profits from being a buyer in future periods, the buyers must make optimal participation choices and reports to the mechanism in which they choose to participate, should they choose to participate in any mechanism.\nA reduced form mechanism is a pair of functions $(q,p),q\\colon[0,1]\\to[0,1]$ and $p\\colon[0,1]\\to\\ensuremath{\\mathbb{R}}$ so that $q(y)$ is a buyer's probability of obtaining the item if he reports a value of $y$ and $p(y)$ is the expected payment in this circumstance.\nThe implementability condition captures the restriction that the seller has a single unit to sell. For example, the seller cannot promise to give the good with certainty to any potential buyer who reports a value in the interval $[y,1]$ unless no buyers are expected from this interval, that is, $\\pmb{\\theta}$ is zero a.e. on this interval.\nTheorem 1: Suppose (10) holds, and assume $\\delta\\phi^{*}=r^{*}\\leqslant r_{t+1}^{*}$, $\\pi^{*\\prime}(1)=\\pi_{t+1}^{*\\prime}(1)=1$, $(\\forall x\\in[r^{*},1])\\quad0\\leqslant z_{o}^{\\prime}(x)\\leqslant g(x)$. Then, any mechanism which maximizes (14) subject to (2), (6), (7),(9),(13) and the CSCE conditions satisfies $\\pi_{t}=\\pi_{t}^{*}$, $z=z_{o}$, and $(\\forall x\\in[r^{*},1])q(x)=z_{o}(x)^{n_{t}-1}$. Moreover, the CSCE best response can be implemented using a second price auction with reserve price of $r^{*}$.\nTheorem 2: If $r^{*}\\leqslant r_{t+1}^{*}$ and (10) holds, then the only self-replicating profit functions in any CSCE are those induced by auctions with reserve price $r^{*}=\\delta\\Phi^{*}$.\n\nThe section discusses the identification of symmetric CSCE (Competitive Search with Commitment Equilibrium) within a period, focusing on buyer behavior, seller deviations, and the conditions under which no deviation is profitable. It introduces reduced form mechanisms and the implementability condition, leading to the conclusion that auctions are the optimal mechanism in this context."
  },
  {
    "qid": "econ-empirical-1029-2-1-0",
    "question": "1) Derive the moment condition $\\rho_{j}(z,\\theta)$ for truncated regression using $m_{j}(\\varepsilon) = \\alpha_{j}1(\\varepsilon > 0) - 1(\\varepsilon > \\tau_{j})$, and explain how it differs from the censored case.",
    "gold_answer": "1) The general form is:\n   $$\\rho_{j}(z,\\theta) = 1(\\gamma_{j} + x^{\\prime}\\beta > -\\tau_{j})m_{j}(y - \\gamma_{j} - x^{\\prime}\\beta).$$\n2) For truncated regression, $m_{j}(\\varepsilon) = \\alpha_{j}1(\\varepsilon > 0) - 1(\\varepsilon > \\tau_{j})$, so:\n   $$\\rho_{j}(z,\\theta) = 1(\\gamma_{j} + x^{\\prime}\\beta > -\\tau_{j})[\\alpha_{j}1(y - \\gamma_{j} - x^{\\prime}\\beta > 0) - 1(y - \\gamma_{j} - x^{\\prime}\\beta > \\tau_{j})].$$\n3) Unlike censored regression, $\\tau_{j} > 0$, and the moment condition exploits truncation directly.",
    "question_context": "The truncated regression model is one where $(y,x)$ is only observed if $y^{*}>0$.\nFor truncated regression we consider $m_{j}(\\varepsilon)=\\alpha_{j}1(\\varepsilon>0)-1(\\varepsilon>\\tau_{j})$, $0<\\alpha_{j}<1$, $\\tau_{j}>0$.\nThe score for a parametric submodel is $S_{\\eta}=s(\\varepsilon)-E^{\\ast}[s(\\varepsilon)|y^{\\ast}>0,x]$, where $s(\\varepsilon)=\\partial\\ln g(\\varepsilon,\\eta)/\\partial\\eta|_{\\eta=\\eta_{0}}$.\n\nThe truncated regression model observes $(y, x)$ only if $y^{*} > 0$, where $y^{*} = x^{\\prime}\\beta_{0} + \\varepsilon$. Efficient estimation via GMM relies on moment conditions derived from functions $m_{j}(\\varepsilon)$ that are zero below a threshold."
  },
  {
    "qid": "econ-empirical-790-3-1-1",
    "question": "4) Derive the separable choice function representation in Condition (iv) of Theorem T-3 and explain its implications for identification.",
    "gold_answer": "1. **Separable Form**: $\\mathbf{1}[T=t|V=v,Z=z] = \\mathbf{1}[\\varphi(v,t) + \\tau(z,t) \\geq 0]$. \\n2. **Implications**: Separability ensures that instrument effects $\\tau(z,t)$ are additive and independent of $v$, simplifying identification of causal effects.",
    "question_context": "THEOREM T-3: The following statements are equivalent characterizations of Assumption A-3 for the $IV$ model (1)-(3): \\n(i) $\\pmb R$ is an unordered monotone response matrix, that is, each binary matrix $B_{t}=1[R=t]; t\\in\\mathsf{s u p p}(T)$ is lonesum; \\n(ii) Unordered Monotonicity: For any $z,z^{\\prime}\\in\\mathsf{s u p p}(Z)$, and for each treatment $t\\in\\mathrm{supp}(T)$, $\\mathbf{1}[T_{\\omega}(z)=t]\\ge\\mathbf{1}[T_{\\omega}(z^{\\prime})=t]\\forall\\omega\\in\\varOmega$ or $\\mathbf{1}[T_{\\omega}(z)=t]\\le\\mathbf{1}[T_{\\omega}(z^{\\prime})=t]\\forall\\omega\\in\\varOmega$. \\n(iv) Unordered Separability: Treatment choice can be represented by separable choice functions in $\\pmb{V}$ and $Z$.\nCondition (i) implies that $\\pmb{B}_{t}$ is fully characterized by its column and row sums. Condition (ii) rules out two-way flows generated by changes in instruments.\n\nThis section presents Theorem T-3, which provides equivalent conditions for characterizing unordered monotonicity Assumption A-3, including separability of choice equations and restrictions on counterfactual choices."
  },
  {
    "qid": "econ-empirical-1422-0-0-1",
    "question": "2) Prove that the incremental importance weights $p(y_{t}|x_{t-1}, y_{1:t-1})$ for ICSSM follow an extended skewed normal distribution, as given by Equation (6).",
    "gold_answer": "1. The incremental weights are $p(y_{t}|x_{t-1}, y_{1:t-1}) = \\int p(y_{t}|x_{t})p(x_{t}|x_{t-1}, y_{1:t-1})dx_{t}$.\\n2. Substitute the ICSSM densities: $p(y_{t}|x_{t-1}, y_{1:t-1}) = \\int \\phi(y_{t}; C_{t}x_{t}, R_{t}) \\cdot \\frac{\\phi(x_{t}; A_{t}x_{t-1}, Q_{t})}{F(A_{t}x_{t-1}, Q_{t}, \\mathcal{X}_{t})} \\cdot 1(x_{t} \\in \\mathcal{X}_{t}) dx_{t}$.\\n3. Recognize the integral as the expectation of $\\phi(y_{t}; C_{t}x_{t}, R_{t})$ under $\\mathrm{TN}(A_{t}x_{t-1}, Q_{t}, \\mathcal{X}_{t})$.\\n4. The result is $\\phi(y_{t}; C_{t}A_{t}x_{t-1}, C_{t}Q_{t}C_{t}^{\\prime} + R_{t}) \\cdot \\frac{F(\\mu_{t}, \\Sigma_{t}, \\mathcal{X}_{t})}{F(A_{t}x_{t-1}, Q_{t}, \\mathcal{X}_{t})}$, where $\\mu_{t}$ and $\\Sigma_{t}$ are from the Kalman update.",
    "question_context": "The joint density, decomposed as $\\prod_{t=1}^{T}p(x_{t}|x_{1:t-1},y_{1:t-1})p(y_{t}|x_{1:t},y_{1:t-1})$, is said to be an inequality constrained state-space model (ICSSM) if $p(x_{t}|x_{1:t-1},y_{1:t-1})=\\frac{\\phi(x_{t};A_{t}x_{t-1},Q_{t})}{F(A_{t}x_{t-1},Q_{t},\\mathcal{X}_{t})}\\cdot1(x_{t}\\in\\mathcal{X}_{t})$, $p(y_{t}|x_{1:t},y_{1:t-1})=\\phi(y_{t};C_{t}x_{t},R_{t})$, where the matrices $A_{t},C_{t},Q_{t},R_{t}$ are time-varying coefficients.\nThe optimal importance function for ICSSM particle filtering is given by $p(x_{t}|x_{t-1},y_{1:t})=\\frac{\\phi(x_{t};\\mu_{t},\\Sigma_{t})}{F(\\mu_{t},\\Sigma_{t},\\mathcal{X}_{t})}\\cdot1(x_{t}\\in\\mathcal{X}_{t})$, where $\\mu_{t}=A_{t}x_{t-1}+Q_{t}C_{t}^{\\prime}(C_{t}Q_{t}C_{t}^{\\prime}+R_{t})^{-1}(y_{t}-C_{t}A_{t}x_{t-1})$, $\\Sigma_{t}=Q_{t}-Q_{t}C_{t}^{\\prime}(C_{t}Q_{t}C_{t}^{\\prime}+R_{t})^{-1}C_{t}Q_{t}$.\n\nThe standard Kalman filter cannot handle inequality constraints imposed on the state variables, as state truncation induces a nonlinear and non-Gaussian model. The proposed Rao-Blackwellized particle filter with the optimal importance function effectively enforces the state constraints when the Kalman filter violates them."
  },
  {
    "qid": "econ-empirical-612-2-0-3",
    "question": "4) Prove that the variables $\\hat{\\phi}^{l}(\\alpha_{0},\\beta_{0})$, $l = 0,1,\\ldots,N$, are exchangeable under the null hypothesis, and explain how this property justifies the exactness of the triple MC test.",
    "gold_answer": "1. **Exchangeability proof**:\n   - Under the null, the simulated statistics $\\hat{\\phi}_{1}^{l}(\\alpha_{0},\\beta_{0})$ and $\\hat{\\phi}_{2}^{l}(\\alpha_{0},\\beta_{0})$ are identically distributed and independent across $l$.\n   - The combined statistics $\\hat{\\phi}^{l}(\\alpha_{0},\\beta_{0})$ are functions of these exchangeable variables, hence themselves exchangeable.\n2. **Exactness justification**:\n   - Exchangeability ensures the empirical distribution of $\\hat{\\phi}^{l}(\\alpha_{0},\\beta_{0})$ is invariant under permutations, making the p-value ${\\bf\\tilde{\\cal G}}_{N}$ exact.\n   - The triple MC test maintains this property by using the same simulated series $S(N)$ throughout, ensuring consistency.",
    "question_context": "Combining our modified version of McCulloch’s (1986) statistics $\\hat{\\phi}_{1}(\\alpha_{0},\\beta_{0})$ and $\\hat{\\phi}_{2}(\\alpha_{0},\\beta_{0})$ is the most relevant question, since the former is originally designed to focus on $\\alpha_{0}$ and the latter on $\\beta_{0}$.\nTo avoid relying on Boole–Bonferroni rules for this purpose, we use the following combined statistics: $$\\begin{array}{r}{\\hat{\\phi}(\\alpha_{0},\\beta_{0})=1-\\operatorname*{min}\\left\\{\\hat{p}_{N}\\big(\\hat{\\phi}_{1}(\\alpha_{0},\\beta_{0})\\big),\\hat{p}_{N}\\big(\\hat{\\phi}_{2}(\\alpha_{0},\\beta_{0})\\big)\\right\\},}\\end{array}$$ $$\\tilde{\\phi}(\\alpha_{0},\\beta_{0})=1-\\operatorname*{min}\\left\\{\\tilde{p}_{N}\\big(\\hat{\\phi}_{1}(\\alpha_{0},\\beta_{0})\\big),\\tilde{p}_{N}\\big(\\hat{\\phi}_{2}(\\alpha_{0},\\beta_{0})\\big)\\right\\}.$$\nThe MC test technique may once again be applied to obtain a test based on the combined statistic; details of the algorithm can be summarized as follows, for the case of $\\hat{\\phi}(\\alpha_{0},\\beta_{0})$.\nThe algorithm can be easily adapted to the case of $\\tilde{\\phi}(\\alpha_{0},\\beta_{0})$ replacing the survival function $\\hat{G}_{N}\\left[x;S(N)\\right]$ by $\\tilde{G}_{N}\\left[x;S(N)\\right]$ in what follows.\nE1. According to steps B1–B3, generate $\\bar{\\phi}_{1}(\\alpha_{0},\\beta_{0})$ and $\\bar{\\phi}_{2}(\\alpha_{0},\\beta_{0})$ ; conformably, calculate the observed values of $\\hat{\\phi}_{1}(\\alpha_{0},\\beta_{0})$ and $\\hat{\\phi}_{2}(\\alpha_{0},\\beta_{0})$ (denoted $\\hat{\\phi}_{1}^{0}(\\alpha_{0},\\beta_{0})$ and $\\hat{\\phi}_{2}^{0}(\\alpha_{0},\\beta_{0})$ respectively), and the $N$ corresponding simulated statistics using the same $\\bar{\\phi}_{1}(\\alpha_{0},\\beta_{0})$ and $\\bar{\\phi_{2}}(\\alpha_{0},\\beta_{0})$.\nE2. For each test statistic, obtain the ‘‘survival function’’ ${\\hat{G}}_{N}[x;$ $S(N)]$ defined in (3.20) determined by the simulated statistics.\nE3. Independently of the previous simulations and the data, generate $N$ additional i.i.d. realizations from a stable distribution under (3.1) each of size $n$ , and standardize the simulated observations for each draw, using the median and interquartile range of the simulated samples.\nE4. Using $\\bar{\\phi}_{1}(\\alpha_{0},\\beta_{0})$ and $\\bar{\\phi}_{2}(\\alpha_{0}^{\\\"},\\beta_{0})$ , and the $N$ draws generated at step E3, compute the corresponding simulated statistics: $\\hat{\\phi}_{1}^{l}(\\alpha_{0},\\beta_{0})$ and $\\hat{\\phi}_{2}^{l}(\\alpha_{0},\\beta_{0}),l=1,...,N.$\nE5. Using the survival functions obtained at step E2, evaluate the simulated $p$ -values for the observed and the $N$ additional simulated statistics; specifically, obtain $\\hat{G}_{N}\\left[\\hat{\\phi}_{1}^{l}(\\alpha_{0},\\beta_{0});S(N)\\right]$ , $l=$ $0,1,\\ldots,N$ , and $\\hat{G}_{N}\\left[\\hat{\\phi}_{2}^{l}(\\alpha_{0},\\beta_{0});S(N)\\right]$ , $l~=~0,1,\\ldots,N,$ using for $S(N)$ the same simulated series described in step E1; these lead to the $p$ -values $\\hat{p}_{N}[\\hat{\\phi}_{1}^{l}(\\alpha_{0},\\beta_{0})]$ and $\\hat{p}_{N}[\\hat{\\phi}_{2}^{l}(\\alpha_{0},\\beta_{0})]$ , $l=0,1,\\ldots,N$\nE6. From the latter, compute the corresponding values of the combined test statistics: $$\\begin{array}{r}{\\hat{\\phi}^{l}(\\alpha_{0},\\beta_{0})=1-\\operatorname*{min}\\bigl\\{\\hat{p}_{N}[\\hat{\\phi}_{1}^{l}(\\alpha_{0},\\beta_{0})],}\\ {\\hat{p}_{N}[\\hat{\\phi}_{2}^{l}(\\alpha_{0},\\beta_{0})]\\bigr\\},\\quad l=0,1,\\dots,N.}\\end{array}$$\nE7. Applying ${\\bf\\tilde{\\cal G}}_{N}\\Big[\\hat{\\phi}^{0}(\\alpha_{0},\\beta_{0});\\Big(\\hat{\\phi}^{1}(\\alpha_{0},\\beta_{0}),\\ldots,\\hat{\\phi}^{N}(\\alpha_{0},\\beta_{0})\\Big)^{\\prime}\\Big]$ leads to the desired combined $p$ -value.\nThe test based on the combined $p$ -value described in steps E1–E7 has the correct level because the variables $\\hat{\\phi}^{l}(\\alpha_{0},\\beta_{0})$ , $l=$ $0,1,\\ldots,N$ , are exchangeable under the null hypothesis. We call this three nested simulation procedure a triple or three stage MC test. Here again, maintaining the same approximated population measures throughout ensures exchangeability, hence exactness.\n\nThe MC test technique is applied to combine statistics, specifically the modified versions of McCulloch’s statistics, focusing on parameters α₀ and β₀. The combined statistics avoid reliance on Boole–Bonferroni rules and use a right-sided test approach."
  },
  {
    "qid": "econ-empirical-131-2-0-2",
    "question": "3) Show that the shrinkage estimator $\\hat{w}_{\\mathrm{S}}$ in Theorem 2 dominates the traditional estimator $\\hat{w}_{\\mathrm{T}}$ with respect to the loss function $\\mathcal{L}_{w,\\Sigma}(\\hat{v}) = (\\hat{v} - w)^{\\prime}\\Sigma(\\hat{v} - w)$.",
    "gold_answer": "1. **Expected Loss of $\\hat{w}_{\\mathrm{T}}$**: $\\mathbb{E}\\{(\\hat{w}_{\\mathrm{T}} - w)^{\\prime}\\Sigma(\\hat{w}_{\\mathrm{T}} - w)\\}$ is derived from its t-distribution.\\n2. **Expected Loss of $\\hat{w}_{\\mathrm{S}}$**: Substitute $\\hat{w}_{\\mathrm{S}} = \\kappa_{\\mathrm{S}}w_{\\mathrm{R}} + (1-\\kappa_{\\mathrm{S}})\\hat{w}_{\\mathrm{T}}$ into the loss function.\\n3. **Optimal Shrinkage**: Show that $\\mathbb{E}\\{(\\hat{w}_{\\mathrm{S}} - w)^{\\prime}\\Sigma(\\hat{w}_{\\mathrm{S}} - w)\\}$ is minimized when $\\kappa_{\\mathrm{S}} = \\frac{d-3}{n-d+2}\\cdot\\frac{1}{\\hat{\\tau}_{\\mathrm{R}}}$.\\n4. **Dominance**: Prove that the expected loss of $\\hat{w}_{\\mathrm{S}}$ is strictly less than that of $\\hat{w}_{\\mathrm{T}}$ for all $\\tau_{\\mathrm{R}} > 0$.",
    "question_context": "Lemma 1 (Frahm, 2008; Kempf and Memmel, 2006). Under the assumptions A1–A3 and $n>d_{\\ast}$ the sample covariance matrix $\\widehat{\\varOmega}$ of $\\Delta R,$ the traditional estimator $\\hat{w}_{\\mathrm{T}}^{\\mathrm{ex}}$ for the MVP (except for the first portfolio weight), and the traditional estimator $\\hat{\\sigma}_{\\mathrm{T}}^{2}$ for the minimum variance $\\sigma^{2}$ satisfy the following properties: P1. $n\\widehat{\\Omega}\\sim W_{d-1}(\\varOmega,n-1)$ , where $\\widehat{\\Omega}:=\\frac{1}{n}\\sum_{t=1}^{n}(\\Delta R-\\Delta\\bar{R})(\\Delta R-\\Delta\\bar{R})^{\\prime}$ . P2. $\\hat{w}_{\\mathrm{T}}^{\\mathrm{ex}}\\mid\\widehat{\\Omega}\\sim\\mathcal{N}_{d-1}(w^{\\mathrm{ex}},\\sigma^{2}\\widehat{\\Omega}^{-1}/n).$ P3. $\\hat{w}_{\\mathrm{T}}^{\\mathrm{ex}}\\sim t_{d-1}(w^{\\mathrm{ex}},\\sigma^{2}\\Omega^{-1}/(n-d+1),n-d+1).$ P4. $n\\hat{\\sigma}_{\\mathrm{T}}^{2}/\\sigma^{2}\\sim\\chi_{n-d}^{2}$ . P5. $\\hat{\\sigma}_{\\mathrm{T}}^{2}$ is stochastically independent of $\\widehat{\\varOmega}$ and $\\hat{w}_{\\mathrm{T}}^{\\mathrm{ex}}$ .\nTheorem 1. Consider a $q\\times q$ random matrix $W\\sim W_{q}(\\Omega,m)$ , where $\\varOmega$ is a positive-definite $q\\times q$ matrix, $q\\geq3$ and $m\\geq q+2$ , a $q$ -dimensional random vector $X$ with $X \\mid W\\sim\\mathcal{N}_{q}(\\omega,W^{-1})$ , where $\\omega\\in\\mathbb{R}^{q}$ is an unknown parameter, and a random variable $\\chi^{2}\\sim\\chi_{k}^{2}$ with $k\\geq2$ , which is stochastically independent of $W$ and $X$. Furthermore, consider a non-stochastic vector $x\\in\\mathbb{R}^{q}$ . For all $0<c<2(q-2)/(k+2)$ , the shrinkage estimator $X_{\\mathrm{S}}=x+\\left(1-{\\frac{c\\chi^{2}}{(X-x)^{\\prime}W(X-x)}}\\right)(X-x)$ dominates the estimator $X$ with respect to the loss function $\\mathcal{L}_{\\omega,\\Omega}(\\hat{\\omega})=(\\hat{\\omega}-\\omega)^{\\prime}\\Omega(\\hat{\\omega}-\\omega)$, i.e., $\\mathbb{E}\\{(X_{\\mathrm{S}}-\\omega)^{\\prime}\\Omega(X_{\\mathrm{S}}-\\omega)\\}<\\mathbb{E}\\{(X-\\omega)^{\\prime}\\Omega(X-\\omega)\\}$ . When $x=\\omega$ , the expected loss of the shrinkage estimator becomes minimal if and only if $c=(q-2)/(k+2)$.\nTheorem 2. Suppose that the assumptions A1–A5 are satisfied. Let $\\hat{w}_{\\mathrm{T}}$ be the traditional estimator for the MVP $w$ , whereas $w_{\\mathtt{R}}\\in\\mathbb{R}^{d}$ with $w_{\\mathtt{R}}^{\\prime}\\mathbf{1}=1$ denotes an arbitrary reference portfolio. Consider the shrinkage estimator $\\hat{w}_{5}=\\kappa_{5}w_{\\mathrm{R}}+(1-\\kappa_{5})\\hat{w}_{\\mathrm{T}}$ with $\\kappa_{\\mathsf{S}}=\\frac{d-3}{n-d+2}\\cdot\\frac{1}{\\hat{\\tau}_{\\mathsf{R}}}$ , where $\\hat{\\tau}_{\\mathrm{R}}~=~(\\hat{\\sigma}_{\\mathrm{R}}^{2}-\\hat{\\sigma}_{\\mathrm{T}}^{2})/\\hat{\\sigma}_{\\mathrm{T}}^{2}$ is the estimated relative loss of the reference portfolio $w_{\\mathtt{R}}$ . The shrinkage estimator $\\hat{w}_{S}$ dominates $\\hat{w}_{\\mathrm{T}}$ with respect to the loss function $\\mathcal{L}_{w,\\Sigma}(\\hat{v})=(\\hat{v}-w)^{\\prime}\\Sigma(\\hat{v}-w)$ , i.e., $\\mathbb{E}\\{(\\hat{w}_{\\mathrm{S}}-w)^{\\prime}\\Sigma(\\hat{w}_{\\mathrm{S}}-w)\\}<\\mathbb{E}\\{(\\hat{w}_{\\mathrm{T}}-w)^{\\prime}\\Sigma(\\hat{w}_{\\mathrm{T}}-w)\\}$.\nTheorem 3. Under the assumptions of Theorem 2, the distribution of the relative loss $\\tau_{\\mathrm{S}}=\\frac{\\sigma_{\\mathrm{S}}^{2}-\\sigma^{2}}{\\sigma^{2}}$ of the shrinkage estimator for the MVP given by (8) depends only on the number of observations $n$ , the number of assets $d$ , and the relative loss $\\tau_{\\mathrm{R}}=(\\sigma_{\\mathrm{R}}^{2}-\\sigma^{2})/\\sigma^{2}$ of the reference portfolio. More precisely, $\\tau_{S}$ can be represented stochastically by $\\tau_{\\mathrm{S}}=\\left\\|\\kappa_{\\mathrm{S}}\\theta-(1-\\kappa_{\\mathrm{S}})V^{-\\frac{1}{2}}\\xi\\right\\|^{2}$ , with any $\\theta\\in\\mathbb{R}^{d-1}$ such that $\\theta^{\\prime}\\theta=\\tau_{\\mathtt{R}},\\xi\\sim\\mathcal{N}_{d-1}(\\mathbf{0},I_{d-1}),V\\sim W_{d-1}(I_{d-1},n-1)$ , and $\\kappa_{\\mathsf{S}}=\\frac{d-3}{n-d+2}\\cdot\\frac{\\chi_{n-d}^{2}}{\\left(\\theta+V^{-\\frac{1}{2}}\\xi\\right)^{\\prime}V\\left(\\theta+V^{-\\frac{1}{2}}\\xi\\right)}$ . Here, $\\xi,V$ , and $\\chi_{n-d}^{2}$ are supposed to be mutually independent.\n\nThis section presents key theorems and lemmas related to the minimum variance portfolio (MVP) estimation, including properties of traditional estimators and shrinkage estimators."
  },
  {
    "qid": "econ-empirical-1831-2-0-3",
    "question": "4) Discuss the role of the Separating Hyperplane Theorem in the proof of Theorem 6 and how it is used to derive the inequalities (19) and (20).",
    "gold_answer": "1. **Separating Hyperplane Theorem**: \n   - States that two disjoint convex sets can be separated by a hyperplane.\n2. **Application in Theorem 6**: \n   - Define $R^{t}$ (feasible set) and $S^{t}$ (upper level set of $f$) as convex sets.\n   - The theorem guarantees the existence of a hyperplane separating $R^{t}$ and $S^{t}$.\n3. **Derivation of Inequalities**: \n   - The hyperplane conditions lead to the existence of $A^{t}, a^{t}, \\alpha^{t}$ satisfying (35) and (36).\n   - These are transformed into the inequalities (19) and (20) using the definitions of $P_{h}^{t}$ and $\\beta_{h}^{t}$.\n4. **Key Insight**: \n   - The theorem allows the proof to proceed without requiring differentiability of $f$, only quasi-concavity.",
    "question_context": "Assume the decision-maker has preferences defined over $N^{1}+N^{2} +\\cdots+N^{I I}+M$ goods. We suppose that these preferences can be represented by a utility function $F(x_{1},\\dots,x_{\\mathit{H}},y)$ where $x_{h}\\geq0_{N^{h}}$ is a non-negative vector of dimension $N^{h}$ and $y\\ge0_{M}$ is a non-negative vector of dimension $M$. We wish to know whether the decision-maker's utility function $F$ can be written as $F(x_{1},\\ldots,x_{H},y)=f[g_{1}(x_{1}),\\ldots,g_{H}(x_{H}),y],$ where $f$ is a function of $H+M$ variables and $g_{h}$ is a function of $N^{h}$ variables for $h=1,\\ldots,H$ i.e., we wish to know if $F$ is weakly separable with respect to its $x_{1},\\dots,x_{H}$ arguments.\nTheorem 4 [Generalization of Afriat (1970) and Varian (1983)]. Suppose the macro function $f$ is $(i)$ concave, $(i i)$ once differentiable, and $(i i i)$ increasing in its first $H$ arguments and non-decreasing in its last $M$ arguments where $H\\geq1$ and $M\\geq1$. Suppose the micro functions $g_{h}$ $h=1,\\ldots,H.$ are $(i)$ concave, $(i i)$ once differentiable, and (i) weakly increasing in their arguments. Suppose further that we are given $T$ positive price vectors, $(p_{1}^{t},\\dots,p_{H}^{t},q^{t})\\gg (0_{N^{1}},\\hdots,0_{N^{\\prime\\prime}},0_{M})$ , and $T$ positive quantity vectors, $\\left(x_{1}^{t},\\dots,x_{H}^{t},y^{t}\\right)\\gg (0_{N^{1}},\\hdots,0_{N^{\\prime\\prime}},0_{M})$ for $t=1,\\dots,T$. Finally, we suppose that the given data are consistent with weakly separable utility maximizing behaviour.\nTheorem 6 (Generalization of Theorem 4). Suppose the macro function $f$ is $(i)$ continuous from above, $(i i)$ subject to local non-satiation, and $(i i i)$ quasi-concave. Suppose the micro functions $g_{h}$ $h=1,\\ldots,H.$ are $(i)$ concave, $(i i)$ weakly increasing, and $(i i i)$ have dual cost functions $C_{h}(\\ensuremath{\\boldsymbol{X}}_{h},\\ensuremath{\\boldsymbol{p}}_{h})$ that are differentiable with respect to their $X_{h}$ arguments. Suppose further that we are given $T$ positive price vectors, $(p_{1}^{\\prime},\\dots,p_{H}^{\\prime},q^{t})\\gg(0_{N^{1}},\\dots,0_{N^{\\prime\\prime}},0_{M})$ ., $T$ non-negative quantity vectors, $y^{\\iota}\\ge0_{\\varkappa}$ $H T$ non-negative but non-zero quantity vectors, $x_{h}^{t}>0_{N^{h}}$ ,for $h=1,\\ldots,H$ $t=1,\\dots,T.$ Finally, suppose that the data are consistent with (weakly) separable utility maximizing behaviour; i.e., (17) holds. Then there exist numbers $X_{h}^{t},\\beta_{h}^{t}$ $u^{t}$ and $\\lambda^{\\iota}$ such that $(I^{g})$ and (20) hold.\n\nThis section explores the conditions under which a decision-maker's utility function can be weakly separable, focusing on the mathematical and economic implications of such separability."
  },
  {
    "qid": "econ-empirical-1377-2-2-0",
    "question": "5) Calculate the implied discount rate for enrollees switching to narrow-network plans during the premium holiday, assuming an annual co-premium \\( P \\).",
    "gold_answer": "1. Annual co-premium = \\( P \\).\n2. Holiday savings = \\( \\frac{3}{12}P = 0.25P \\).\n3. Effective annual co-premium post-holiday = \\( 0.75P \\).\n4. Implicit discount rate \\( r \\) satisfies \\( 0.25P = \\frac{0.25P}{(1 + r)^{0.25}} \\).\n   Solving: \\( r \\approx 0 \\) (immediate 25% savings).",
    "question_context": "In 2012, the GIC offered a three-month 'premium holiday' for all active state employees who chose to switch to a narrow-network plan. For households choosing to make the switch, the holiday entailed that they pay no premiums for three months of the fiscal year.\nThese differences in behavior between new members (or those making active choices) and existing members (those potentially making passive choices) suggest the presence of a high degree of inertia in choosing health plans.\n\nThe paper identifies inertia in plan choices and price sensitivity among enrollees, supported by evidence from narrow-network plan adoption and the premium holiday."
  },
  {
    "qid": "econ-empirical-1044-0-1-2",
    "question": "3) Derive the welfare implications of the observed regulatory behavior under the assumption that free trade maximizes total surplus.",
    "gold_answer": "1. Let total surplus \\( S = S_{domestic} + S_{foreign} + S_{consumer} \\).\n2. Under free trade, \\( S^* = \\max S \\).\n3. Protectionist intervention imposes a deadweight loss \\( DWL = S^* - S^{regulated} \\).\n4. From the results, \\( \\frac{\\partial S^{regulated}}{\\partial I} < 0 \\) when \\( I \\) protects domestic firms at the expense of foreign bidders and consumer surplus.",
    "question_context": "European M&A regulators claim to be fostering competition and thereby protecting European consumers. But, using a sample of 290 proposed acquisitions screened by the European regulator during the 1990s, we find that the more harm suffered by European rival firms when the acquirer is coming from outside the European Community, the greater the likelihood of European regulatory intervention against the proposed combination.\nThese results are robust to a variety of empirical problems including endogeneity between announcement date returns and regulatory intervention and they raise a clear suspicion of protectionist behaviour.\n\nThe text analyzes the behavior of European M&A regulators, suggesting potential protectionist motives based on the empirical findings."
  },
  {
    "qid": "econ-empirical-646-0-0-1",
    "question": "2) Compare the identification strategy in this paper with that of Vytlacil and Yildiz (2007), focusing on the role of monotonicity and the use of the full outcome distribution.",
    "gold_answer": "Key differences:\n1. **Monotonicity**: Vytlacil and Yildiz (2007) require $E[g(\\nu,\\varepsilon)|U=u]$ to be strictly increasing in $\\nu$, while this paper does not.\n2. **Outcome Distribution**: This paper exploits the full distribution of the outcome, whereas Vytlacil and Yildiz (2007) rely only on the mean.\n3. **Multiple Indices**: This paper allows $\\nu(X,D)$ to be a vector of multiple indices, while Vytlacil and Yildiz (2007) restrict it to a single index.\n\nThe approach in this paper is more general and applicable to a wider range of models.",
    "question_context": "Consider a weakly separable model with a binary endogenous variable: \n$$\n\\begin{array}{r c l}{{Y}}&{{=}}&{{g\\big(\\nu_{1}(X,D),\\nu_{2}(X,D),...\\nu_{J}(X,D),\\varepsilon\\big)}}\\ {{}}&{{}}&{{}}\\ {{D}}&{{=}}&{{1\\{\\theta(Z)-U>0\\}}}\\end{array}\n$$ \nwhere $(\\nu_{1}(X,D),\\nu_{2}(X,D),...\\nu_{J}(X,D))\\equiv\\nu(X,D)$ is a $J.$ -vector of unknown linear or nonlinear indices in the outcome Eq. (1.1) and $D$ is a binary endogenous variable defined by equation (1.2). Here $X\\in\\mathbb{R}^{d_{x}}$ and $Z\\in\\mathbb{R}^{d_{z}}$ are vectors of observable exogenous variables, which may have overlapping elements.\nWe consider two potential outcomes $Y_{1}$ and $Y_{0}$ that satisfy: \n$$\nY_{D}=g(\\nu(X,D),\\varepsilon)\\mathrm{for}D=0,1.\n$$ \nWe only observe $(Y,D,X,Z)$ , where $Y=D Y_{1}+(1-D)Y_{0}$.\n\nThe paper discusses identification and estimation of treatment effects in a weakly separable model with a binary endogenous variable. The model allows for multiple indices and does not require monotonicity, unlike previous approaches."
  },
  {
    "qid": "econ-empirical-1505-1-0-3",
    "question": "4) Analyze the equilibrium conditions under which the payment mechanism with bank notes is sustainable. What parameters (e.g., $\\lambda$, $R$, discount factor $\\beta$) influence this equilibrium?",
    "gold_answer": "1. The equilibrium requires that bankers prefer to comply with reserve requirements: \\n\\[ \\beta \\cdot V(\\text{compliant}) \\geq u(R) + \\beta \\cdot V(\\text{defected}) \\] \\nwhere $V$ is the value function. \\n2. Key parameters: \\n   - Higher $\\lambda$ increases trade frequency, raising the value of compliance. \\n   - Higher $R$ raises the cost of defection. \\n   - Higher $\\beta$ makes future penalties more salient. \\n3. The equilibrium exists if $\\beta$ is sufficiently high and $R$ is not too costly.",
    "question_context": "To describe the exchange process, it is convenient to refer to type 1 as a buyer, to type 2 as a seller, and to type 3 as a banker. To better understand these labels, it is easier to start with the second stage. In this stage, each buyer is randomly matched with a seller with probability $\\lambda$. Because the buyer wants good $y$ but is unable to produce good $x$ for the seller at that time, the pair will be able to trade only if a medium of exchange is made available.\nA buyer will be able to acquire a note in the first stage when he is randomly matched with a banker. In this stage, each buyer has access to the technology to produce good $x$ so the objects a buyer and a banker trade are good $x$ and notes.\nIn the third stage, the group of sellers and the group of bankers interact in a centralized location. In this stage, a seller has an opportunity to redeem any note (i.e., to convert a privately issued obligation into good $x$ he has received from a buyer (if any) in the previous stage, so we can think of this stage as the settlement stage.\nEach banker is able to observe the actions of other bankers in the centralized location. The bilateral trades in the first and second stages are privately observable, i.e., only the pair of agents participating in the meeting knows the amounts traded. As a result, each banker may have an incentive to issue notes without fully securing them with storage.\nThe clearinghouse requires each banker to report any meeting in the first stage in which a note has been issued. For each note issued, the banker is required to store a fraction of the face value of the note (in terms of good $x$), to be interpreted as reserves backing the issuance of his note.\nAny banker who fails to report the issuance of a note will have his membership permanently revoked. His deviation will be publicly observable to all members of the clearinghouse only when an unreported note is presented for redemption in the settlement stage, which may take several periods to happen.\nFinally, I assume that each agent can carry, at most, one indivisible unit of money at any moment. This means that individual note holdings are restricted to the set {0, 1}. On the other hand, there is no restriction on the number of notes each banker is allowed to issue at any moment except for that imposed by the matching technology and agents' willingness to trade.\n\nThe exchange mechanism involves three types of agents: buyers (type 1), sellers (type 2), and bankers (type 3). The process is divided into three stages: matching buyers with bankers, buyers with sellers, and a centralized settlement stage where notes are redeemed. The clearinghouse association ensures the integrity of the system by requiring reserves for issued notes."
  },
  {
    "qid": "econ-empirical-910-3-0-2",
    "question": "3) Analyze the asymptotic distribution of $\\sqrt{N}(\\hat{\\theta}-\\theta)$, showing it converges to $N(0, 2\\theta)$ as $N, T \\to \\infty$.",
    "gold_answer": "1. Express $\\hat{\\theta} = \\theta + \\frac{2\\theta}{T}\\sum_{t=1}^T \\eta_{t,N_1}^1 + o_p(1)$\\\\\n2. Show $\\eta_{t,N_1}^1$ is sample covariance with $E[\\eta_{t,N_1}^1] = 0$ and $Var(\\eta_{t,N_1}^1) \\approx (t-1)/N_1$\\\\\n3. Compute variance of average: $N_1 Var\\left(\\frac{1}{T}\\sum_{t=1}^T 2\\theta\\eta_{t,N_1}^1\\right) \\to 2\\theta^2$\\\\\n4. Rescale by $N/N_1 = 1/\\theta$: $N Var(\\cdot) \\to 2\\theta$\\\\\n5. Apply CLT: $\\sqrt{N}(\\hat{\\theta}-\\theta) \\xrightarrow{d} N(0, 2\\theta)$",
    "question_context": "The DGP is $y_{i t}=\\lambda_{i}+u_{i t}$ , $u_{i t}=\\alpha_{i}u_{i t-1}+e_{i t}$ with $e_{i t}\\sim$ ${\\bf N}(0,\\sigma_{e}^{2})\\forall i$ . Without loss of generality, $\\sigma_{e}^{2}=1$.\n$$\\begin{array}{l}{\\displaystyle V_{t,N}=\\frac{1}{N}\\sum_{i=1}^{N}(y_{i t}-Y_{t,N})^{2}}\\ {=\\theta V_{t,N_{1}}^{1}+(1-\\theta)V_{t,N_{0}}^{0}+\\theta(1-\\theta)g_{t,N}^{2}}\\end{array}$$\n$$\\begin{array}{c}{{s_{t,N_{1}}^{1}=\\displaystyle\\frac{1}{N_{1}}\\sum_{i=1}^{N_{1}}u_{i t}^{2}-\\left(\\displaystyle\\frac{1}{N_{1}}\\sum_{i=1}^{N_{1}}u_{i t}\\right)^{2}}}\\\\ {{\\displaystyle\\quad=\\displaystyle\\frac{1}{N_{1}}\\sum_{i=1}^{N_{1}}(u_{i t-1}^{2}+e_{i t}^{2}+2u_{i t-1}e_{i t})}}\\end{array}$$\nLemma A.1. Suppose that $g_{t}=g_{t-1}+\\nu_{t}$ , $\\nu_{t}\\sim\\Nu(0,\\sigma_{\\nu}^{2})$ . Then $g_{t}^{2}$ is a heteroscedastic random walk with a drift of $\\sigma_{\\nu}^{2}$ and conditional variance of $4\\sigma_{\\nu}^{4}\\cdot t-2\\sigma_{\\nu}^{4}$.\n\nThe text presents a detailed econometric model with a data generating process (DGP) involving unit-specific effects and autoregressive errors. The proofs involve asymptotic analysis of cross-sectional variances and covariances under different assumptions about the autoregressive parameters."
  },
  {
    "qid": "econ-empirical-475-2-1-3",
    "question": "8) Discuss the implications of Table 4's results for practitioners using Johansen's framework with deterministic trends.",
    "gold_answer": "1. Table 4 shows that LR tests (Q1, Q2) have rejection rates near 100% when deterministic trends are omitted.  \n2. Correctly specified models (SM3) reduce size distortion but require precise knowledge of the DGP.  \n3. Practitioners must test for deterministic components and include them appropriately in the VECM to avoid pitfalls.",
    "question_context": "The DGP is $$\\begin{array}{r}{\\varDelta y_{t}=a_{1}+b_{1}t+e_{1t},}\\ {\\varDelta x_{t}=a_{2}+b_{2}t+e_{2t}.}\\end{array}$$\nProposition 6. Suppose $(y_{t}~x_{t})^{\\prime}$ are two $I(1)$ processes with deterministic components generated from Eqs. (18) and (19). a. If $b_{1}\\neq0$ or $b_{2}\\neq0$ , and if the VECM does not include any deterministic trends, then $\\widehat{\\lambda}_{1}$ does not converge to zero in probability. b. If $b_{1}=b_{2}=0$ , $a_{1}\\neq0$ or $a_{2}\\neq0$ , and if the VECM does not include any deterministic components, then $\\widehat{\\lambda}_{1}$ does not converge to zero in probability.\n\nThis section examines the pitfalls of Johansen's LR tests when dealing with $I(1)$ processes that include deterministic trends or drifts. It emphasizes the importance of correctly modeling deterministic components in the VECM."
  },
  {
    "qid": "econ-empirical-754-1-0-0",
    "question": "1) Derive the asymptotic distribution of the measurement error ${\\hat{Y}}_{t}-Y_{t}$ under the given sequence of DGPs and ARCH updating rules. Explain the role of rescaling both the measurement error vector and time in achieving a well-behaved diffusion limit.",
    "gold_answer": "1. **Rescaling the measurement error and time**: \n   - Define $Q_{t} \\equiv \\alpha (\\hat{Y}_{t} - Y_{t})$ and $\\tau \\equiv \\beta t$.\n   - Set $\\alpha = h^{-1/4}$ and $\\beta = h^{-1/2}$ to balance the drift and diffusion terms.\n   \n2. **Resulting diffusion limit**:\n   $$\n   \\mathrm{d}Q_{\\tau} = [h^{\\delta - 3/4} A(X_{\\tau}, Y_{\\tau}, \\tau) - B(X_{\\tau}, Y_{\\tau}, \\tau) Q_{\\tau}] \\mathrm{d}\\tau + C(X_{\\tau}, Y_{\\tau}, \\tau)^{1/2} \\mathrm{d}W_{\\tau}.\n   $$\n   - For $\\delta = 3/4$, the drift term remains finite as $h \\downarrow 0$.\n   \n3. **Interpretation**: The rescaling ensures the drift does not explode, yielding a well-behaved Ornstein-Uhlenbeck process.",
    "question_context": "Our next task is to derive the asymptotic distribution of ${\\hat{Y}}_{t}-Y_{t}$ ,given a sequence (indexed by $h$ as $h\\downarrow0$ ) of DGPs of the form (2.1)-(2.2) and a sequence of ARCH updating rules of the form (2.4)-(2.5). Below we emphasize heuristics over rigor. A rigorous proof of Theorem 2.1 below closely follows the proof of Theorem 3.1 in NF.\nTo illustrate the intuition behind the passage to continuous time, for each $h>0$ let $\\left\\{Z_{t}\\right\\}$ be a $k\\times1$ Markov random step function taking jumps at times $h,2h,3h$ ,and so on, with \n\n$$\n\\begin{array}{r l}&{Z_{t+h}-Z_{t}=h\\cdot\\mu(Z_{t})+h^{1/2}\\xi_{t+h},}\\ &{C o v_{t}(h^{1/2}\\xi_{t+h})=\\Omega(Z_{t})\\cdot h,\\qquadE_{t}[\\xi_{t+h}]=0_{k\\times1}.}\\end{array}\n$$\nUnder weak conditions (see, e.g., Nelson, 1990), $\\left\\{Z_{t}\\right\\}$ convergesweakly as $h\\downarrow0$ to the diffusion with the same first two conditional moments per unit of time - i.e., to \n\n$$\n\\mathrm{d}Z_{t}=\\mu(Z_{t})\\mathrm{d}t+\\Omega(Z_{t})^{{\\frac{1/2}{1/2}}}\\mathrm{d}W_{t},\n$$ \n\nwhere $W_{t}$ is a $k\\times1$ standard Brownian motion.\nFor our diffusion limits, we require rescaling of both the measurement error vector $(\\hat{Y}_{t}-Y_{t})$ and of time. To understand the role of such rescalings, consider the diffusion (2.7): First, to rescale $Z$ define $M\\equiv\\alpha\\cdot Z$ for ${\\mathfrak{x}}>{\\mathfrak{0}}$ By Ito's Lemma \n\n$$\n\\mathrm{d}M_{t}=\\varnothing\\cdot\\mu(Z_{t})\\mathrm{d}t+\\alpha\\cdot\\Omega(Z_{t}){\\overset{1/2}{-}}\\mathrm{d}W_{t}.\n$$ \n\nSo the effect of rescaling $Z$ by $\\pmb{\\alpha}$ is to multiply the drift (per unit time) by $\\pmb{\\alpha}$ and the covariance per unit of time by $\\alpha^{2}$.\nNow consider the measurement error process $\\left(\\widehat{Y}_{t}-Y_{t}\\right)$ :by (2.1) and (2.4)-(2.5), \n\n$$\n\\begin{array}{r l}&{\\big(\\hat{Y}_{t+h}-Y_{t+h}\\big)=(\\hat{Y}_{t}-Y_{t})+h^{\\delta}[\\hat{\\kappa}-\\kappa]}\\ &{\\qquad+h^{1/2}[G(\\hat{\\xi}_{X,t+h},\\hat{Y}_{t},X_{t},t)-\\xi_{Y,t+h}].}\\end{array}\n$$\nAssuming sufficient smoothness of $G(\\cdot)$ , we may expand it in a Taylor series around $(\\bar{\\hat{Y}}_{t},\\hat{\\xi}_{X,t+h})=(Y_{t},\\xi_{X,t+h})$ obtaining \n\n$$\n\\begin{array}{r l}&{(\\hat{Y}_{t+h}-Y_{t+h})=(\\hat{Y}_{t}-Y_{t})+h^{1/2}[G(\\xi_{X,t+h},Y_{t})-\\xi_{Y,t+h}]}\\ &{\\qquad+h^{1/2}E_{t}\\bigg[\\frac{\\widehat{\\mathcal{O}}G(\\xi_{X,t+h},Y_{t})}{\\widehat{\\mathcal{O}}Y}\\bigg](\\hat{Y}_{t}-Y_{t})}\\ &{\\qquad+h^{\\delta}\\bigg[\\hat{\\kappa}-\\kappa+E_{t}\\bigg[\\frac{\\widehat{\\mathcal{O}}G(\\xi_{X,t+h},Y_{t})}{\\widehat{\\mathcal{O}}\\xi_{X}}\\bigg](\\mu-\\hat{\\mu})\\bigg]}\\ &{\\qquad+h.o.t.s,}\\end{array}\n$$ \n\nwhere the higher-order terms are called h.o.t.s.\nNow suppose we naively proceed to the diffusion limit, ignoring higher-order terms and finding the diffusion whose first two conditional moments (per unit time) corresponded to the stochastic difference equation (2.12). This moment matchingdiffusion is \n\n$$\n\\begin{array}{r l}&{\\mathrm{d}(\\hat{Y}_{t}-Y_{t})=[h^{\\delta-1}A(X_{t},Y_{t},t)-h^{-1/2}B(X_{t},Y_{t},t)(\\hat{Y}_{t}-Y_{t})]\\mathrm{d}t}\\ &{\\quad\\quad\\quad+C(X_{t},Y_{t},t)^{\\frac{1/2}{2}}\\mathrm{d}W_{t},}\\end{array}\n$$ \n\nwith $C(X_{t},Y_{t},t)$ the small $h$ limit of $C o v_{t}[G(\\xi_{X,t+h},Y_{t})-\\xi_{Y,t+h}]$ Clearly this won't do as a limit diffusion: the covariance matrix of the candidate limit diffusion is fine, but the drift explodes as $h\\downarrow0$ . To remedy this, rescale ${\\hat{Y}}_{t}-Y_{t}$ as $Q_{t}\\equiv\\alpha(\\hat{Y}_{t}-Y_{t})$ and rescale time as $\\tau\\equiv\\beta t$ . The candidate diffusion limit is now \n\n$$\n\\begin{array}{r l}&{\\mathrm{d}Q_{\\tau}=\\beta^{-1}\\lbrack\\alpha h^{\\delta-1}A(X_{\\tau},Y_{\\tau},\\tau)-h^{-1/2}B(X_{\\tau},Y_{\\tau},\\tau)Q_{\\tau}\\rbrack\\mathrm{d}\\tau}\\ &{\\quad\\quad\\quad+\\alpha\\beta^{-1/2}C(X_{\\tau},Y_{\\tau},\\tau)\\frac{1/2}{\\mathrm{d}W_{\\tau}}\\mathrm{d}W_{\\tau},}\\end{array}\n$$ \n\nwhere $W_{\\tau}$ is an $(n+m)\\times1$ standard Brownian motion on the transformed time scale.Set $\\alpha\\equiv h^{-1/4}$ and $\\beta\\equiv{h^{-1/2}}$ and wehave \n\n$$\n\\begin{array}{r l}&{\\mathrm{d}Q_{\\tau}=[h^{\\delta-3/4}A(X_{\\tau},Y_{\\tau},\\tau)-B(X_{\\tau},Y_{\\tau},\\tau)Q_{\\tau}]\\mathrm{d}\\tau}\\ &{\\quad\\quad+C(X_{\\tau},Y_{\\tau},\\tau)^{\\frac{1/2}{2}}\\mathrm{d}W_{\\tau},}\\end{array}\n$$ \n\nwhich is nicely behaved, allowing us to obtain a diffusion limit.\n\nThis section derives the asymptotic distribution of the measurement error ${\\hat{Y}}_{t}-Y_{t}$ under a sequence of DGPs and ARCH updating rules. The analysis involves rescaling both the measurement error vector and time to achieve a well-behaved diffusion limit."
  },
  {
    "qid": "econ-empirical-1812-7-1-0",
    "question": "3) Using Ely and Peski (2011, Theorem 1), prove that the set of critical types is dense in the strategic topology.",
    "gold_answer": "1. **Finite Types are Critical**: By Ely and Peski (2011), every finite type is critical (i.e., not regular).  \n2. **Denseness**: Dekel et al. (2006) show finite types are dense in the strategic topology. Thus, critical types are dense.",
    "question_context": "An immediate implication of Ely and Peski (2011, Theorem 1) is that every finite type is critical. This fact, together with the denseness of finite types in the strategic topology (Dekel et al., 2006), implies that the set of critical types is dense in the strategic topology.\n\nThis section leverages Ely and Peski (2011) to show that finite types are critical and dense in the strategic topology, with implications for common belief convergence."
  },
  {
    "qid": "econ-empirical-651-3-1-2",
    "question": "7) Compare the asymptotic variances ${\\cal F}_{a,i}^{-1}$ and ${\\cal F}_{b,i}^{-1}$ in Theorems 4.4 and 4.5.",
    "gold_answer": "The comparison is as follows:\n1. Both ${\\cal F}_{a,i}^{-1}$ and ${\\cal F}_{b,i}^{-1}$ are inverse Fisher information matrices.\n2. ${\\cal F}_{a,i}^{-1}$ depends on $\\gamma_{i}^{h}$ and $\\bar{\\Omega}_{i}$, while ${\\cal F}_{b,i}^{-1}$ depends on $(I_{k} \\otimes \\phi_{i})^{\\prime}\\mathscr{T}$ and $\\bar{\\mathscr{Q}}_{i}$.\n3. The structure of ${\\cal F}_{b,i}^{-1}$ is more complex due to the inclusion of time-invariant variables and their interactions with the factors.\n4. Both variances ensure the LV estimator is asymptotically normal under the given conditions.",
    "question_context": "A direct extension of model (4.1) is the following one:\n$y_{i t} = \\alpha_{i} + x_{i t}^{\\prime}\\beta_{i} + \\psi_{i}^{\\prime}g_{t} + \\phi_{i}^{\\prime}h_{t} + \\epsilon_{i t}$\n$x_{i t} = \\nu_{i} + \\gamma_{i}^{g\\prime}g_{t} + \\gamma_{i}^{h\\prime}h_{t} + v_{i t}$\nwhere $\\phi_{i}$’s are observable loadings satisfying $\\varPhi=[\\phi_{1},\\phi_{2},\\ldots,\\phi_{N}]^{\\prime}$ is of full column rank.\nTheorem 4.4. Under Assumptions A–E, H and I, when $N,T\\rightarrow\\infty$ and $\\sqrt{T}/N\\to0$ , we have that for each i, $\\sqrt{T}(\\widehat{\\beta}_{i}^{L V}-\\beta_{i})\\stackrel{d}{\\longrightarrow}N(0,{\\cal F}_{a,i}^{-1})$.\nIn some applications, it is of interest to include some time-invariant variables, such as gender, race, education, and so forth. To address this concern, consider the following model with time-invariant variables:\n$y_{i t} = \\alpha_{i} + x_{i t}^{\\prime}\\beta_{i} + \\psi_{i}^{\\prime}g_{t} + \\phi_{i}^{\\prime}h_{t}^{y} + \\epsilon_{i t}$\n$x_{i t p} = \\nu_{i p} + \\gamma_{i p}^{g\\prime}g_{t} + \\phi_{i}^{\\prime}h_{t p}^{x} + v_{i t p}, \\quad\\mathrm{for} p=1,2,\\dots,k$.\nTheorem 4.5. Under Assumptions A–E, H and I, as $N, T\\to\\infty$ and $\\sqrt{T}/N\\to0,$ , we have that for each i, $\\sqrt{T}(\\widehat{\\beta}_{i}^{L V}-\\beta_{i})\\stackrel{d}{\\longrightarrow}N(0,{\\cal F}_{b,i}^{-1})$.\n\nThis section extends the model to include observable loadings and time-invariant variables, providing estimation procedures and asymptotic results."
  },
  {
    "qid": "econ-empirical-398-0-0-3",
    "question": "4) Evaluate the potential challenges the Econometric Society might face in achieving its goal of unifying theoretical and empirical approaches in economics.",
    "gold_answer": "Potential challenges include:\n1. The complexity of human behavior and economic systems, which may resist straightforward unification.\n2. The need for advanced statistical and mathematical tools to bridge theory and empirics.\n3. Maintaining disinterestedness in a field often influenced by political and financial interests.\n4. Ensuring that rigorous methodologies are accessible and applicable across diverse economic contexts.",
    "question_context": "The Econometric Society is an international society for the advancement of economic theory in its relation to statistics and mathematics. The Society shall operate as a completely disinterested, scientific organization, without political, social, financial or nationalistic bias. Its main object shall be to promote studies that aim at the unification of the theoretical-quantitative and the empirical-quantitative approach to economic problems and that are penetrated by constructive and rigorous thinking similar to that which has come to dominate in the natural sciences.\n\nThe Econometric Society is an international organization dedicated to advancing economic theory through its integration with statistics and mathematics. It operates as a disinterested scientific body, free from biases, and focuses on unifying theoretical and empirical approaches to economic problems."
  },
  {
    "qid": "econ-empirical-762-1-0-0",
    "question": "1) Formally derive the theoretical linkage between exposure to solar eclipses and the accumulation of human capital, as proposed in the paper. Use a mathematical model to illustrate how curiosity \\( C \\) driven by eclipses \\( E \\) translates into human capital \\( H \\).",
    "gold_answer": "1. **Model Setup**: Let curiosity \\( C \\) be a function of eclipse exposure \\( E \\): \\( C = f(E) \\), where \\( f'(E) > 0 \\).  \n2. **Human Capital Accumulation**: Human capital \\( H \\) grows as a function of curiosity: \\( H_{t+1} = H_t + g(C) \\), where \\( g'(C) > 0 \\).  \n3. **Equilibrium**: In steady state, \\( H^* = g(f(E)) \\).  \n4. **Implications**: Higher \\( E \\) leads to higher \\( H^* \\) via increased \\( C \\).",
    "question_context": "We propose that exposure to inexplicable phenomena prompts curiosity and thinking in an attempt to comprehend these mysteries, thus raising human capital and technology, and, ultimately, fostering growth.\nWe focus on solar eclipses as one particular trigger of curiosity and empirically establish a robust relationship between their number and several proxies of economic prosperity.\nWe also offer evidence compatible with the human capital and technological increases we postulate, finding a more intricate thinking process and more developed technology among societies more exposed to solar eclipses.\n\nThis paper relates curiosity to economic development through its impact on human capital formation and technological advancement in pre-modern times. The authors focus on solar eclipses as a trigger of curiosity and empirically establish a relationship between their number and economic prosperity."
  },
  {
    "qid": "econ-empirical-1127-0-0-3",
    "question": "4) Compare the magnitude of the 9/11 dust cloud's effects on birth outcomes to those of other environmental disasters or pollution reductions (e.g., EZ-Pass). What does this imply about the severity of 9/11?",
    "gold_answer": "1. **9/11 effects**: First-trimester exposure doubled prematurity and low birth weight risks for males.  \n2. **EZ-Pass**: 10% reduction in low birth weight (Currie and Walker 2011).  \n3. **Implication**: The 9/11 disaster's effects are orders of magnitude larger, consistent with its unprecedented pollution levels.",
    "question_context": "The collapse of the World Trade Center (WTC) in New York City following the terrorist attacks of Sept. 11, 2001, was the largest environmental disaster ever to have befallen a U.S. metropolis, releasing a million tons of toxic dust and smoke into the air of lower Manhattan.\nMany previous studies have found a relationship between air pollution during pregnancy and adverse birth outcomes.\nThe levels of mutagenic and carcinogenic air pollutants measured in the aftermath of the WTC collapse are among the highest ever reported from outdoor sources.\n\nThe study examines the effects of the 9/11 dust cloud on pregnancy outcomes, addressing previous methodological challenges and highlighting the differential impacts on male and female fetuses."
  },
  {
    "qid": "econ-empirical-535-4-1-3",
    "question": "4) Compare the results of the break-point tests before and after Black Monday. What do they reveal about the tail dependence structure?",
    "gold_answer": "1. **Before Black Monday**: The null hypothesis of constant tail dependence is rejected, indicating structural instability in this period.\n2. **After Black Monday**: The null hypothesis is not rejected, suggesting stable tail dependence post-Black Monday.\n3. **Implication**: The tail dependence structure is not constant over the entire sample, with evidence of a break-point before Black Monday. This complicates the interpretation of Black Monday's impact.",
    "question_context": "For our analysis, we begin by an investigation of the univariate time series. Applying the model selection and verification criteria from Jäschke (2014), we find that an ARMA(0,0)-GARCH(1,1)-model with t-distribution for the Dow Jones log-returns and an ARMA(1,0)-GARCH(1,1)-model with skewed $t$-distribution for the Nasdaq equivalent provide the best fits among a number of common stationary time series models.\nNow, we apply the test from Section 3.4 for a specific breakpoint at $\\lfloor n\\bar{s}_{\\tt B M}\\rfloor~=~959$, the date of Black Monday. The results are depicted in Fig. 8 in the supplementary material, where we plot the $p$-values of the test against the parameter $k$. For $k^{*}=191$, the resulting $p$-value of 0.082 does not allow for a clear rejection of the null hypothesis.\nWe conclude this application with a short summary of the main findings: (i) The test for a break on Black Monday does not yield entirely unambiguous results; in particular, we have to reject the null hypothesis of constant tail dependence in the subsample before Black Monday resulting in an overall model with more than one break-point. (ii) Testing against the existence of some unspecified break-point in the full sample clearly rejects the null, with an estimated break-point at $\\lfloor n\\hat{s}^{\\lambda}\\rfloor=817$.\n\nThis section examines the Dow Jones Industrial Average and Nasdaq Composite time series around Black Monday (October 19, 1987). The analysis tests whether Black Monday constitutes a break in the tail dependence structure between the two time series."
  },
  {
    "qid": "econ-empirical-480-3-0-0",
    "question": "1) Derive the first-order condition (28) for profit maximization, \\(E[\\alpha_{2}P_{t}e^{\\alpha_{1}}L_{t}^{\\alpha_{2}-1}e^{\\alpha_{t}}|\\hat{\\theta}] = w_{t}\\), starting from the profit function \\(\\Pi_{t} = P_{t}Q_{t} - w_{t}L_{t}\\) and production function \\(Q_{t} = e^{\\alpha_{1}}L_{t}^{\\alpha_{2}}e^{\\alpha_{t}}\\).",
    "gold_answer": "1. Substitute \\(Q_{t}\\) into \\(\\Pi_{t}\\): \\(\\Pi_{t} = P_{t}e^{\\alpha_{1}}L_{t}^{\\alpha_{2}}e^{\\alpha_{t}} - w_{t}L_{t}\\).  \n2. Take the expectation conditional on \\(\\hat{\\theta}\\): \\(E[\\Pi_{t}|\\hat{\\theta}] = E[P_{t}e^{\\alpha_{1}}L_{t}^{\\alpha_{2}}e^{\\alpha_{t}}|\\hat{\\theta}] - w_{t}L_{t}\\).  \n3. Differentiate w.r.t. \\(L_{t}\\) and set to zero: \\(\\frac{\\partial}{\\partial L_{t}}E[\\Pi_{t}|\\hat{\\theta}] = E[\\alpha_{2}P_{t}e^{\\alpha_{1}}L_{t}^{\\alpha_{2}-1}e^{\\alpha_{t}}|\\hat{\\theta}] - w_{t} = 0\\).  \n4. Rearrange to obtain (28).",
    "question_context": "The model differs from traditional approaches by incorporating stochastic optimization and market awareness, where firms understand how supply interacts with demand to determine output prices.\nFirms have identical Cobb-Douglas production functions: \\(Q_{t} = e^{\\alpha_{1}}L_{t}^{\\alpha_{2}}e^{\\alpha_{t}}\\), with labor \\(L_{t}\\) as the sole input, and uncertain parameters \\(\\alpha_{1}, \\alpha_{2}\\).\nMarket demand follows: \\(P_{t} = e^{\\beta_{1}}Q_{t}^{\\beta_{2}}e^{\\beta_{t}}\\), where \\(\\alpha_{t}, \\beta_{t}\\) are independent normal noise terms with mean 0 and variance 1.\nFirms maximize expected profit \\(\\Pi_{t} = P_{t}Q_{t} - w_{t}L_{t}\\) using Bayesian updating of priors on \\((\\alpha, \\beta)\\), with normal distributions for \\(\\alpha \\equiv (\\alpha_{1}, \\alpha_{2})\\) and \\(\\beta \\equiv (\\beta_{1}, \\beta_{2})\\).\nThe equilibrium condition for labor input \\(L_{t}\\) is derived from the first-order condition: \\(E[\\alpha_{2}P_{t}e^{\\alpha_{1}}L_{t}^{\\alpha_{2}-1}e^{\\alpha_{t}}|\\hat{\\theta}] = w_{t}\\), where \\(\\hat{\\theta}\\) represents prior beliefs.\nThe solution involves log-normal properties and recursive Bayesian updating, yielding equilibrium price \\(P_{t}^{0}(\\hat{\\theta}, w_{t}) = \\exp\\{\\beta_{1} + \\beta_{t} + \\beta_{2}(\\alpha_{1} + \\alpha_{t})\\}[L_{t}^{0}(\\hat{\\theta}, w_{t})]^{\\alpha_{2}\\beta_{2}}\\).\n\nThis section analyzes a market with identical Cobb-Douglas production functions, integrating parameter estimation with equilibrium price determination under rational expectations."
  },
  {
    "qid": "econ-empirical-1705-3-1-3",
    "question": "4) Explain how the parameters of Model 2 (AR(1) with level breaks) are calibrated to mimic U.S. Treasury Bonds.",
    "gold_answer": "1. $u_{t}$ is AR(1) with coefficient 0.98 to match high persistence in interest rates. \\n2. $\\sigma_u$ is set to match the long-run variance of 10-year Treasury yields. \\n3. $\\delta_{\\text{small}}=2.0\\%$ and $\\delta_{\\text{large}}=4.0\\%$ are chosen to match the IQR of $\\mu_{T}-\\mu_{0}$ in historical data.",
    "question_context": "Model 1 takes the form $x_{t}=\\mu_{t}+u_{t}$ with $u_{t}\\sim iid\\mathcal{N}(0,\\sigma_{u}^{2})$, where $\\mu_{t}$ follows a martingale with breaks of size $\\pm\\delta$ occurring with probability $p$.\nModel 3 generates data as $x_{t}=\\sigma_{t}e_{t}$, where $\\ln(\\sigma_{t})=\\mu_{t}$ follows a martingale with breaks, mimicking the 'Great Moderation' in macroeconomic volatility.\nModel 5 involves breaks in both level and volatility, with regimes following an AR(1) process whose parameters change stochastically.\n\nThis section evaluates the finite-sample performance of prediction sets under models with breaks in level and volatility, motivated by macroeconomic time series. Five models are considered, including breaks in level (Models 1-2), volatility (Models 3-4), and both (Model 5)."
  },
  {
    "qid": "econ-empirical-320-1-2-0",
    "question": "7) Derive the formula for the number of climate zones created by interacting province, altitude, distance-to-coast, and urbanity indicators. How does this ensure exogeneity?",
    "gold_answer": "1. Number of zones: $24\\ \\text{provinces} \\times 4\\ \\text{altitude} \\times 3\\ \\text{distance} \\times 2\\ \\text{urbanity} = 576$ potential zones. \\n2. Only 285 are observed due to geographic constraints. \\n3. Exogeneity: Within-zone temperature variation stems only from survey timing, not endogenous factors.",
    "question_context": "We create 285 climate zones by interacting: 24 province indicators, four altitude groups $(<50, 50–100, 100–500, 500+\\ \\mathrm{m}$ above sea level), three distance-to-the-coast groups $(<30, 30-60, 60+\\ \\mathrm{km})$ and an indicator of the village’s urban or rural status.\nFigure 3(a) plots the variation in temperatures on the day of the survey within these constructed climate zones. It demonstrates sufficient temperature variation for the identification of temperature effects.\n\nThe identification strategy exploits temperature variation within homogeneous climate zones, assuming exogeneity due to survey timing. Climate zones are defined by altitude, distance to coast, and urbanity."
  },
  {
    "qid": "econ-empirical-1300-1-0-3",
    "question": "4) Analyze how the agent's private information $\\theta$ affects the equilibrium contracts and efforts. Provide conditions under which the equilibrium is separating or pooling.",
    "gold_answer": "1. In a separating equilibrium, contracts $x_i(\\theta)$ vary with $\\theta$. \\n2. The agent's effort $e_i(\\theta)$ must satisfy incentive compatibility: $v(\\theta, e_i(\\theta), a(\\theta)) \\geq v(\\theta, e_i(\\theta'), a(\\theta'))$ for all $\\theta' \\neq \\theta$. \\n3. In a pooling equilibrium, contracts and efforts are constant in $\\theta$. \\n4. Derive conditions on $v(\\theta, e, a)$ and $u_i(\\theta, e, a)$ that lead to separating or pooling.",
    "question_context": "There are $n\\in\\mathbb{N}$ principals who contract sequentially and non-cooperatively with the same agent, $A$ . Each principal $P_{i}$ is indexed by the date $i\\in\\mathcal{N}\\equiv$ $\\{1,\\ldots,n\\}$ at which she contracts with the agent. Each $P_{i}$ must select a contract $x_{i}:E_{i}\\rightarrow{\\mathcal{A}}_{i}$ from a set $X_{i}$ of feasible contracts.\nA contract specifies the action $a_{i}\\in\\mathcal{A}_{i}$ that $P_{i}$ will take in response to the agent’s choice of action/effort $e_{i}\\in E_{i}$ . Both $E_{i}$ and $\\mathcal{A}_{i}$ may have different interpretations depending on the application under examination.\nAll players have expected utility preferences. A principal’s payoff is represented by the function $u_{i}(\\theta,e,a)$ . Similarly, the agent’s payoff is described by the function $v(\\theta,e,a)$.\n\nThe model involves multiple principals contracting sequentially and non-cooperatively with a single agent. Each principal selects a contract from a feasible set, specifying actions based on the agent's effort. The agent's payoff depends on exogenous private information and the actions of all principals."
  },
  {
    "qid": "econ-empirical-537-1-1-1",
    "question": "2) Compare the Rich-Domain Assumption (Section 3.1) and the Bounded-Domain Assumption (Section 3.2) in terms of their implications for social choice functions.",
    "gold_answer": "Key differences:\n1. **Domain Scope**: Rich-Domain includes all consistent types, while Bounded-Domain restricts types to $[0, \\bar{v}_k]$ or convex combinations.\n2. **Weak Monotonicity**: Both assumptions make W-Mon sufficient for truthfulness, but Bounded-Domain requires additional TBB.\n3. **Payment Construction**: Rich-Domain uses $p_k = -\\delta_{Kk}$, while Bounded-Domain uses $p_k = \\sum_{\\ell=1}^k \\delta_{\\ell \\ell-1}$ due to complete ordering.",
    "question_context": "The order $\\succeq$ on the set of outcomes is complete. That is, for any $a_k, a_\\ell \\in A$, either $a_k \\succeq a_\\ell$ or $a_\\ell \\succeq a_k$, but not both.\nBOUNDED-DOMAIN ASSUMPTION: There exist constants $\\bar{v}_k \\in (0, \\infty)$, $k = 1, 2, \\ldots, K$ such that the domain of agent types, $\\mathcal{D}$, satisfies either of the following statements:\nA. $\\mathcal{D} = \\prod_{k=1}^K [0, \\bar{v}_k]$.\nB. $\\mathcal{D}$ is the convex hull of points $(\\bar{v}_1, \\bar{v}_2, \\ldots, \\bar{v}_{k-1}, \\bar{v}_k, 0, \\ldots, 0)$, $k = 0, 1, \\ldots, K$.\nTHEOREM 2: A social choice function on a completely ordered bounded domain is truthful if and only if it is weakly monotone.\n\nThis section extends the analysis to completely ordered domains, where the agent's preference over outcomes is fully ordered, and introduces additional assumptions like the Bounded-Domain Assumption and Tie-Breaking at Boundaries (TBB)."
  },
  {
    "qid": "econ-empirical-1684-0-1-3",
    "question": "4) Discuss the implications of replacing nonimposition and neutrality with set nonimposition in the characterization of the top cycle.",
    "gold_answer": "Replacing nonimposition and neutrality with set nonimposition: \n\n- **Broadens Applicability**: Set nonimposition allows for more flexible SCCs. \n- **Uniqueness**: The top cycle becomes the only SCC satisfying the axioms. \n- **Fairness**: Neutrality is relaxed, but the top cycle still treats alternatives symmetrically in practice. \n\nThis variant strengthens the Gibbard-Satterthwaite impossibility by providing a positive characterization.",
    "question_context": "In this paper, we characterize the class of strategyproof pairwise SCCs under relatively mild and common technical assumptions, namely the conditions of nonimposition, homogeneity, and neutrality. Our first result shows that every strategyproof pairwise SCC that satisfies these conditions always returns a dominant set. An important variant of this characterization is obtained when replacing nonimposition and neutrality with set not-imposition (every set of alternatives is returned for some preference profile): the top cycle is the only strategyproof pairwise SCC that satisfies set nonimposition and homogeneity.\n\nThis section explores the characterization of strategyproof pairwise SCCs under conditions of nonimposition, homogeneity, and neutrality, with a focus on the top cycle."
  },
  {
    "qid": "econ-empirical-709-1-0-1",
    "question": "2) Derive the theoretical channels through which QE could either improve or worsen market liquidity, referencing the inventory risk channel.",
    "gold_answer": "1. **Improvement channels**:  \n   - Portfolio rebalancing stimulates trading.  \n   - The 'back-stop buyer' (BoE) reduces dealers' inventory risk, increasing intermediation willingness.  \n   \n2. **Worsening channels**:  \n   - Reduced bond supply in private hands increases search frictions.  \n   - Price-insensitive purchases distort price signals, reducing trading incentives.  \n   \n3. **Inventory risk channel**: The BoE's purchases provide dealers confidence to hold inventory, as they can sell to the BoE if needed, improving liquidity.",
    "question_context": "We address this challenge by studying the Bank of England’s 2016–7 Corporate Bond Purchase Scheme. In particular, we use granular offer-level data from the Corporate Bond Purchase Scheme auctions to construct proxy measures for the Bank of England’s demand for bonds and auction participants’ supply of bonds, allowing us to control for any reverse causality from liquidity to purchases.\nQuantitative easing (QE) has become a key component of the monetary policy toolkit since the global financial crisis. But the introduction of a large, relatively price-insensitive buyer has the potential to significantly impact market functioning.\nThe CBPS provides an ideal setting to address the reverse causality problem described above. In comparison to bilateral purchases, the auction design of the CBPS greatly reduces the discretionary nature of purchases.\nWe implement this strategy in a bond-auction panel setting. For each bond and each auction, we estimate several measures of secondary market liquidity for the bond in the week following the auction.\n\nThe paper discusses the impact of the Bank of England's Corporate Bond Purchase Scheme (CBPS) on market liquidity, addressing challenges like reverse causality and the theoretical ambiguity of QE's impact on liquidity."
  },
  {
    "qid": "econ-empirical-113-3-1-0",
    "question": "5) Derive the bias in the fixed effects estimator if voter preferences are time-varying and correlated with direct democratic institutions. How does this affect the interpretation of the results in Table 5?",
    "gold_answer": "1. **Bias Formula**: \\( \\text{Bias} = \\frac{\\text{Cov}(R_{it}, \\eta_{it})}{\\text{Var}(R_{it})} \\), where \\( \\eta_{it} \\) is time-varying preferences.\n2. **Interpretation**: If \\( \\eta_{it} \\) is pro-spending, the bias is negative, underestimating the effect of direct democracy.\n3. **Table 5**: The robustness checks suggest the bias is small, as coefficients remain stable.\n4. **Implication**: Fixed effects are likely sufficient to control for preference shifts.",
    "question_context": "Our fixed effects approach might not capture all unobservable differences across cantons.\nWe next show a variety of informal tests suggesting that shifts in voter preferences and changes in other political institutions are unlikely to explain our results.\n\nOur fixed effects approach might not capture all unobservable differences across cantons. We next show a variety of informal tests suggesting that shifts in voter preferences and changes in other political institutions are unlikely to explain our results."
  },
  {
    "qid": "econ-empirical-1285-3-0-1",
    "question": "2) Compare the impulse responses of inflation and the informality rate to demand shocks versus TFP shocks. Explain the differences in their cyclical behavior.",
    "gold_answer": "1. **Demand Shocks**:\n   - **Inflation**: Procyclical (rises with output due to increased labor demand and wages).\n   - **Informality Rate**: Procyclical (informal employment rises to accommodate labor demand).\n2. **TFP Shocks**:\n   - **Inflation**: Countercyclical (unit labor costs fall due to higher productivity).\n   - **Informality Rate**: Depends on shock type:\n     - Neutral TFP: Procyclical on impact, countercyclical in medium run.\n     - Formal TFP: Countercyclical (formal wages rise, reallocating workers).",
    "question_context": "The baseline model is consistent with a countercyclical inflation rate and a countercyclical informality rate. These are relevant properties of the data for the purposes of our investigation, and it is important to get them right. It is worth, then, to explore the sources of the cyclical behavior of the two variables in the context of our calibrated model.\n\nThis section explores the role of the informal sector in dampening or propagating inflationary pressures arising from different shocks to the economy. The analysis focuses on the cyclical behavior of inflation and the informality rate in response to various shocks."
  },
  {
    "qid": "econ-empirical-286-0-0-0",
    "question": "1) Derive the conditions under which a population reaches a stationary state given a sub-replacement fertility rate \\( r < 2.1 \\). Discuss the implications for age structure dynamics.",
    "gold_answer": "1. **Stationary Population Condition**: A population reaches a stationary state when the net reproduction rate (NRR) = 1. For sub-replacement fertility \\( r < 2.1 \\), NRR < 1.  \n2. **Age Structure Dynamics**: The proportion of elderly (\\( \\geq 65 \\)) increases over time, while the working-age population (15-64) declines. The dependency ratio \\( D = \\frac{P_{65+}}{P_{15-64}} \\) rises.  \n3. **Mathematical Formulation**: Let \\( P_t \\) be the population at time \\( t \\), and \\( r \\) be the fertility rate. The stationary state satisfies \\( \\lim_{t \\to \\infty} P_t = P^* \\) where \\( P^* \\) is constant. The age structure evolves as \\( \\frac{dP_{65+}}{dt} = r P_{15-64} - \\mu P_{65+} \\), where \\( \\mu \\) is mortality rate.",
    "question_context": "In a number of developed countries fertility is approaching or has already reached replacement or sub-replacement levels. This means that these populations will eventually cease to grow, if not decline, in number in the next fifty years.\nThe first four chapters put the problem in historical perspective. Through a mixture of economic history and history of economic thought, Spengler threads together past attitudes to population growth from ancient times to the present day, focusing particularly on concern over slower population growth.\nThe last six chapters deal with the effects of ZPG on the economy at both the micro and macro levels. Working mostly through changes in the age structure, the micro level effects are examined from the perspective of the groups most affected by ZPG -- the young and the old.\nThe macro aspects concern mainly the impact of ZPG on output growth. Here the issues discussed include the possible effects on savings, investment, technical progress, and unemployment as well as some wider issues such as the size of a country within the world community.\n\nThe text discusses the economic implications of zero population growth (ZPG) in developed countries, focusing on historical attitudes and the effects on age structure, labor markets, and macroeconomic variables."
  },
  {
    "qid": "econ-empirical-738-3-0-3",
    "question": "4) Derive the asymptotic distribution of the generalized three-stage least squares estimator $\\hat{\\delta}_{G3}=(S_{z x}\\hat{\\pmb{{\\psi}}}^{-1}S_{z x}^{\\prime})^{-1}(S_{z x}\\hat{\\pmb{{\\psi}}}^{-1}s_{x y})$ and show that it is asymptotically efficient.",
    "gold_answer": "1. The estimator is obtained by minimizing:\n   $$(s_{x y}-S_{z x}^{\\prime}\\delta)^{\\prime}\\hat{\\pmb{{\\psi}}}^{-1}(s_{x y}-S_{z x}^{\\prime}\\delta).$$\n2. Under standard regularity conditions, $\\sqrt{N}(\\hat{\\delta}_{G3}-\\delta^{0})\\overset{D}{\\rightarrow}N(0,\\pmb{A})$, where:\n   $$\\pmb{A}=(\\pmb{\\varPhi}_{z x}\\pmb{\\psi}^{-1}\\pmb{\\varPhi}_{z x}^{\\prime})^{-1}.$$\n3. This achieves the asymptotic efficiency bound for minimum distance estimators, as it uses the optimal weighting matrix $\\pmb{\\psi}^{-1}$.",
    "question_context": "Given the discussion on imposing restrictions, it is not surprising that two-stage least squares is not, in general, an efficient procedure for combining instrumental variables. I shall demonstrate this with a simple example. Assume that $(y_{i},z_{i},x_{i1},x_{i2})$ is i.i.d. according to some distribution with finite fourth moments, and that $y_{i}=\\delta z_{i}+v_{i}$, where $E(v_{i}x_{i1})=E(v_{i}x_{i2})=0$. Assume also that $E(z_{i}x_{i1})\neq0,E(z_{i}x_{i2})\neq0$. Then there are two instrumental variable estimators that both converge a.s. to $\\delta$.\nThe two-stage least squares estimator combines $\\widehat{\\delta}_{1}$ and $\\hat{\\delta}_{2}$ by forming $\\hat{z}_{i}=\\hat{\\pi}_{1}x_{i1}+\\hat{\\pi}_{2}x_{i2}$, based on the least squares regression of $z$ on $x_{1},x_{2}$ (assume that $E[(x_{i1},x_{i2})^{\\prime}(x_{i1},x_{i2})]$ is non-singular), where $\\hat{\\delta}_{\\mathrm{TSLS}}=\\sum_{i=1}^{N}y_{i}\\dot{\\hat{z}_{i}}\bigg/\\sum_{i=1}^{N}z_{i}\\hat{z}_{i}=\\hat{\\alpha}\\hat{\\delta}_{1}+(1-\\hat{\\alpha})\\hat{\\delta}_{2}$.\nThe minimum distance estimator is obtained by choosing $\\hat{\theta}$ to minimize $\\operatorname*{min}_{\\theta}\\biggl[\\biggl(\\begin{array}{c}{\\hat{\\delta}_{1}}\\\\{\\hat{\\delta}_{2}}\\end{array}\\biggr)-\\biggl(\\theta\\atop\\theta\\biggr)\\biggr]^{\\prime}A^{-1}\\biggl[\\biggl(\\begin{array}{c}{\\hat{\\delta}_{1}}\\\\{\\hat{\\delta}_{2}}\\end{array}\\biggr)-\\biggl(\\theta\\atop\\theta\\biggr)\\biggr]$, where $\\widehat{\\theta}=\\tau\\widehat{\\delta}_{1}+\\left(1-\\tau\\right)\\widehat{\\delta}_{2}$, and $\\tau=(\\lambda^{11}+\\lambda^{12})/(\\lambda^{11}+2\\lambda^{12}+\\lambda^{22})$.\nConsider the standard simultaneous equations model: $y_{i}=H x_{i}+u_{i},\\quad E(u_{i}x_{i}^{\\prime})=0$, ${\\cal T}y_{i}+B x_{i}={\\boldsymbol{\\upsilon}}_{i}$, where ${\\cal T}{\\cal{\\varPi}}+{\\cal B}=0$ and ${\\cal T}{\\pmb u}_{i}={\\pmb v}_{i}$. We are continuing to assume that ${\\bf y}_{i}$ is $M\\times1,x_{i}$ is $K\\times1,r_{i}^{\\prime}{=}(\\mathbf{x}_{i}^{\\prime}\\mathbf{y}_{i}^{\\prime})$ is i.i.d. according to a distribution with finite fourth moments $\\{\\iota=1,...,N\\}$ ,and that $E(\\mathbf{{x}}_{i},\\mathbf{{x}}_{i}^{\\prime})$ is non-singular.\nThe generalized three-stage least squares estimator is given by $\\hat{\\delta}_{G3}=(S_{z x}\\hat{\\pmb{{\\psi}}}^{-1}S_{z x}^{\\prime})^{-1}(S_{z x}\\hat{\\pmb{{\\psi}}}^{-1}s_{x y})$, where $\\pmb{\\varPsi}=E(\\pmb{v}_{i}^{0}\\pmb{v}_{i}^{\\prime0}\\otimes\\pmb{x}_{i}\\pmb{x}_{i}^{\\prime})$, and $\\hat{\\upsilon}_{i}=\\hat{I}{\\pmb y}_{i}+\\hat{\\pmb{B}}{\\pmb x}_{i}$.\n\nThis section discusses the inefficiency of two-stage least squares in combining instrumental variables and introduces generalized estimators that achieve asymptotic efficiency."
  },
  {
    "qid": "econ-empirical-1300-5-0-2",
    "question": "3) Prove that the strategy profile $\\overset{\\circ}{\\sigma}$ constructed in Step 2 for ${{T}_{i}^{M}}$ is an equilibrium and induces the same outcomes as $\\sigma$ in $\\varGamma$.",
    "gold_answer": "1) **Outcome equivalence**: For any $\\phi_{i}^{M} \\in \\varPhi_{i}^{M}$, $\\mathring{\\sigma}_{A}$ induces $\\zeta(h_{i}^{-}, Q_{i}(\\phi_{i}^{M}))$ identical to $\\hat{\\sigma}_{A}$ in $\\boldsymbol{{\\Gamma}}^{\\mathcal{Q}_{i}}$.  \n2) **Sequential rationality**: $\\mathring{\\sigma}_{A}$ uses Bayes-rule-derived distributions ($\\beta(y_{i}; h_{i}^{-}, Q_{i}(\\phi_{i}^{M}))$) for downstream behavior, mirroring $\\hat{\\sigma}_{A}$.  \n3) **No deviation incentives**: Principals' payoffs depend only on outcome distributions, which are unchanged.",
    "question_context": "Let $\\mathcal{Q}_{i}$ be a generic partition of $\\boldsymbol{\\phi}_{i}$ and denote by $\\boldsymbol{Q}_{i}\\in\\mathcal{Q}_{i}$ an element of $\\mathcal{Q}_{i}$.\nThe proof of Part 1 is in three steps. Step 1 identifies a partition of $\\boldsymbol{\\varPhi}_{i}$ that makes the agent indifferent between any two mechanisms in the same cell $\\boldsymbol{Q}_{i}\\in\\mathcal{Q}_{i}$.\nFor any two mechanisms $\\phi_{i},\\phi_{i}^{\\prime}\\in\\phi_{i}$, $\\phi_{i}\\sim_{i}\\phi_{i}^{\\prime}\\longleftrightarrow\\mathrm{Im}(\\phi_{i})=\\mathrm{Im}(\\phi_{i})$.\nStep 2 uses the construction in Step 1 to derive an equilibrium $\\mathring{\\sigma}$ in ${{T}_{i}^{M}}$ which also implements the same outcomes as $\\sigma$.\nStep 3 shows how the previous two steps can be applied recursively to construct an equilibrium $\\sigma^{M}$ for ${\\cal{T}}^{M}$ that implements the same outcomes as $\\sigma$.\n\nThe proof establishes a bijection between equilibria of the original game $\\varGamma$ and the menu game ${\\cal{T}}^{M}$, ensuring outcome equivalence."
  },
  {
    "qid": "econ-empirical-1641-3-0-0",
    "question": "1) Prove Lemma 2 by constructing $\\sigma_{i,\\ell}$ explicitly for each $\\ell \\leq k$, ensuring that $s_{i,\\ell} \\prec_{X_{-i}} \\sigma_{i,\\ell}$ holds.",
    "gold_answer": "1. **Initial Setup**: For each $\\ell \\leq k$, by hypothesis, there exists $\\tau_{i,\\ell} \\in \\Delta(X_i)$ such that $s_{i,\\ell} \\prec_{X_{-i}} \\tau_{i,\\ell}$. If $\\tau_{i,\\ell} \\in \\Delta(Y_i)$, set $\\sigma_{i,\\ell} = \\tau_{i,\\ell}$. Otherwise, proceed to stepwise construction.  \n2. **Stepwise Construction**: For $j=1,\\ldots,k$, construct $\\sigma_{i,\\ell}^j$ by conditioning $\\tau_{i,\\ell}$ on $s_{i,j}$ not occurring:  \n   $$ \\sigma_{i,\\ell}^j(s_i) = \\frac{\\tau_{i,\\ell}(s_i)}{1-\\tau_{i,\\ell}(s_{i,j})} \\quad \\text{for all } s_i \\neq s_{i,j}. $$  \n3. **Verification**: Show that $\\sigma_{i,\\ell}^j \\in \\Delta(X_i \\setminus \\{s_{i,1},\\ldots,s_{i,j}\\})$ and $s_{i,\\ell} \\prec_{X_{-i}} \\sigma_{i,\\ell}^j$ via transitivity of $\\prec_{X_{-i}}$.  \n4. **Final Step**: After $k$ steps, $\\sigma_{i,\\ell}^k \\in \\Delta(Y_i)$, satisfying the lemma.",
    "question_context": "Lemma 2. Let $X$ and $Y$ be two sets of strategy profiles such that $Y\\subsetneq X$ , and for each player i and each $s_{i}\\in X_{i}\\setminus Y_{i},s_{i}$ is distinguishably dominated within $X$ . Then, for each player i and each $s_{i}\\in X_{i}\\setminus Y_{i}.$ , there exists $\\sigma_{i}\\in\\Delta(Y_{i})$ such that $s_{i}\\prec_{X_{-i}}\\sigma_{i}$ .\nProof. Consider an arbitrary player $i$ . Without loss of generality, assume that $X_{i}\\setminus Y_{i}\\neq\\emptyset$ . Let $k=|X_{i}\\setminus Y_{i}|$ and $X_{i}\\setminus Y_{i}=\\{s_{i,1},\\ldots,s_{i,k}\\}$ . To prove Lemma 2, it suffices to show that for each $\\ell\\leq k$ , there exists a strategy $\\sigma_{i,\\ell}\\in\\Delta(Y_{i})$ such that $s_{i,\\ell}\\prec_{X_{-i}}\\sigma_{i,\\ell}$ .\n\nThe lemma establishes conditions under which a strategy profile can be dominated within a set of strategy profiles, leveraging distinguishable dominance relations."
  },
  {
    "qid": "econ-empirical-1119-0-0-0",
    "question": "1) Explain the theoretical rationale behind why asset tests under AFDC/TANF might discourage saving among low-income families, referencing the work of Hubbard, Skinner, and Zeldes (1995).",
    "gold_answer": "1. **Theoretical Basis**: Hubbard, Skinner, and Zeldes (1995) argue that asset tests create disincentives for saving because:\n   - Holding assets can render families ineligible for public transfers, reducing expected lifetime utility.\n   - The implicit tax rate on saving varies by asset type due to exemptions (e.g., vehicles).\n   - Transfer programs provide a consumption floor, reducing the need for precautionary saving.\n2. **Mathematical Intuition**: The household optimization problem can be represented as:\n   \\[ \\max_{a} E[U(c)] \\text{ s.t. } c = y + b(a) - a \\]\n   where \\( b(a) \\) is the means-tested benefit, which decreases in \\( a \\). This leads to lower optimal asset holdings \\( a^* \\).",
    "question_context": "Consistent with other recent research, I find little evidence that asset limits have an effect on the amount of liquid assets that single mothers hold. However, I find evidence that vehicle exemptions do have an important effect on vehicle assets.\nThe saving behavior of poor families has attracted the attention of both researchers and policymakers. Several studies have shown that poor families tend to have very few assets.\nAsset tests under the AFDC/TANF program typically apply to all assets except for owner-occupied housing equity and some fraction of the equity value of a vehicle.\n\nThis paper examines the impact of AFDC/TANF asset tests on the asset holdings of low-educated single mothers, with a focus on vehicle assets. The study highlights the significance of vehicle assets as a major component of wealth for poor families and explores how changes in asset limits and exemptions affect saving behavior."
  },
  {
    "qid": "econ-empirical-26-3-0-3",
    "question": "4) Prove Proposition 8: Show that the solution to (4.2) is either (i) $\\hat{n} = n^{*} - 1$ with marginal cost pricing or (ii) $\\hat{n} = n^{*}$ with average cost pricing.",
    "gold_answer": "1. **Case (i)**: For $n < n^{*}$, profit constraints do not bind (Proposition 4), so $\\hat{n} = n^{*} - 1$ with marginal cost pricing is feasible. \\n2. **Case (ii)**: For $n = n^{*}$, profit constraints bind, requiring $D(n^{*} \\hat{q}) = A(\\hat{q})$ and $A(\\hat{q}) > c'(\\hat{q})$. \\n3. **Optimality**: Adding firms beyond $n^{*}$ reduces welfare due to higher average costs. Thus, $\\hat{n} \\leq n^{*}$. \\n4. **Conclusion**: The solution is either (i) or (ii), depending on cost structures.",
    "question_context": "Let $\\tilde{n}$ denote the solution to (4.1). Proposition 6 follows immediately from Proposition 4.\nPROPOSITION 6. Assume that at $n=n^{*}$ in the solution to (3.1) all firms are making losses. Then the solution to the second-best problem (4.1) is $\\tilde{n}=n^{*}-1$ ; that is, the solution to the second-best problem (4.1) is to have one less firm than in the first-best case.\nPROPOSITION 7. Assume the following: (i) the regulatory agency controls only the price, and it always sets the price at its optimal value in (3.2) for a given number of firms; and (i) all firms, including potential entrants, have perfect foresight on the regulatory procedure. Then a free entry-exit equilibrium together with price regulation will coincide with the solution to the problem (4.1).\nThe second-best problem without the marginal cost pricing constraint, but with the profit constraint, is written as $$\\operatorname*{max}_{n,q_{i}}S(Q)-\\sum_{i=1}^{n}C(q_{i})$$ subject to $$q_{i}D(Q)\\geq C(q_{i})\\quad{\\mathrm{all~}}i\\leq n.$$\nPROPOsITION 8. Assume that the solution to (3.1) occurs with all firms making losses. Then the solution to the second-best problem (4.2) will be either (i) the same as the solution to (4.1) with $\\hat{n}=n^{*}-1,\\hat{q}=q_{n^{*}-1},$ and all firms price at marginal cost; or (ii) ${\\hat{n}}=n^{*}$ , i.e., the first- and second-best number of firms are the same, and all firms price at average cost.\n\nA common class of second-best problems arises when budget constraints are imposed on public agencies. If the first-best solution to the variable number of firms problem (3.1) requires negative profits for all firms, and it is infeasible to subsidize the losses on all firms, a reasonable constraint on the regulatory authority would be that all firms earn nonnegative profits."
  },
  {
    "qid": "econ-empirical-205-2-1-2",
    "question": "7) Analyze the equilibrium conditions for household labor markets, assuming identical men and women. How does heterogeneity change this analysis?",
    "gold_answer": "1. **Identical Agents**:\n   - Symmetric supply and demand functions simplify equilibrium to $H_f^s = H_m^d$ and $H_m^s = H_f^d$.\n   - Wages $w_f^*$ and $w_m^*$ adjust to clear markets.\n2. **Heterogeneity**:\n   - Tastes, productivity ($\\beta_i$), and non-wage income ($V_i$) vary, creating separate markets.\n   - Equilibrium requires matching specific supply and demand curves, complicating wage determination.",
    "question_context": "Aggregate demand and supply functions for women are presented in equations $(\\mathbf{I}\\mathbf{I}){-}\\mathbf{\\left(\\mathbf{I}3\\right)}$ , and for men in equations (14)-(16). Superscripts $d$ and $\\pmb{S}$ denote whether a function is a supply or a demand. Capital letters are used to denote aggregate hours of work.\nMarkets for household labour are reminiscent of traditional labour markets. Market forces are relatively difficult toidentify because of the absence of physically visible institutions, rigidities in compensation levels and hours of work, limitations on the relative proportion of pecuniary to non-pecuniary compensation, etc.\nThe equilibrium conditions in all these labour markets are interdependent. Wages in labour markets affect marital behaviour and conditions in the markets for household labour influence labour supply.\n\nThe text extends the individual model to market equilibria, aggregating individual supplies and demands to determine equilibrium wages for household labor. It discusses the interdependence of labor markets and household labor markets."
  },
  {
    "qid": "econ-empirical-857-3-0-0",
    "question": "1) Derive the expression for $\\frac{\\partial x_{j}(t+1)}{\\partial E_{t}p_{j}(t+k)}$ as given in Corollary 3, starting from the basic assumptions of the model.",
    "gold_answer": "To derive the expression for $\\frac{\\partial x_{j}(t+1)}{\\partial E_{t}p_{j}(t+k)}$, follow these steps:\n1. Start with the vector process $\\xi(t) = (\\phi(t)^{\\prime}, p(t)^{\\prime})^{\\prime}$.\n2. Incorporate the influence of productivity and quasi-fixed factor adjustment shocks via $\\{\\phi(\\cdot)\\}$.\n3. Use the matrix $P$ defined as $P = \\left[\\begin{array}{l l}{I}&{0}\\\\{0}&{-I}\\end{array}\\right]$.\n4. Apply Proposition 1 to derive the dynamic response of quasi-fixed factor demands to expected price changes.\n5. Sum over all $i$ from 1 to $n$ to account for all eigenvalues $\\lambda_i$ and adjustment cost parameters $\\kappa_{ji}$.\n6. The final expression is: $$\\frac{\\partial x_{j}(t+1)}{\\partial E_{t}p_{j}(t+k)} = -\\sum_{i=1}^{n}[\\lambda_{i}+(\\beta\\lambda_{i})^{-1}]\\kappa_{j i}^{\\phantom{\\dagger}2}(\\beta\\lambda_{i})^{k}.$$",
    "question_context": "Let $\\xi(t)=(\\phi(t)^{\\prime},p(t)^{\\prime})^{\\prime}$ where $\\{\\phi(\\cdot)\\}$ is an $(n\\times1)$ vector process of parameters that incorporate the influence of productivity and quasi-fixed factor adjustment shocks and $\\{p(\\cdot)\\}$ is an $(n\\times1)$ vector process of quasi-fixed factor prices (rents).\nThe following then is an immediate consequence of Proposition 1: COROLLARY 3: $$\\begin{array}{r l r}{\\displaystyle\\frac{\\partial x_{j}(t+1)}{\\partial E_{t}p_{j}(t+k)}=-\\sum_{i=1}^{n}[\\lambda_{i}+(\\beta\\lambda_{i})^{-1}]\\kappa_{j i}^{\\phantom{\\dagger}2}(\\beta\\lambda_{i})^{k}}&{\\qquad}&{(j=1,\\ldots,n),}\\\\{\\displaystyle\\frac{\\partial x_{j}(t+1)}{\\partial E_{t}p_{l}(t+k)}=-\\sum_{i=1}^{n}[\\lambda_{i}+(\\beta\\lambda_{i})^{-1}]\\kappa_{j i}\\kappa_{l i}(\\beta\\lambda_{i})^{k}}&{}&\\\\{\\displaystyle=\\frac{\\partial x_{l}(t+1)}{\\partial E_{t}p_{j}(t+k)}}&{\\qquad}&{(j,l=1,\\ldots,n).}\\end{array}$$\nCOROLLARY 4: Suppose $E_{t}p(t+k)=\\bar{p}=(\\bar{p}_{1},\\dots,\\bar{p}_{n})^{\\prime}$ for all $k\\geqslant1$ . Then, $$\\frac{\\partial{x}_{j}(t+1)}{\\partial\\bar{p}}=-\\sum_{i=1}^{n}\\big(1+\\beta\\lambda_{i}^{2}\\big){\\kappa_{j i}}^{2}\\big(1-\\beta\\lambda_{i}\\big)^{-1}<0.$$\nLEMMA 6: Signature $(\\lambda)=\\cdot$ -signature(). PROOF: Recall $[\\lambda_{i}+(\\beta\\lambda_{i})^{-1}]=-\\delta_{i}^{-1}$ . It follows that $\\lambda_{i}\\gtrless0$ as $\\delta_{i}\\lessgtr0$ . Hence, signature $(\\lambda)={}^{.}$ -signature $\\left(\\delta\\right)$ . But, $\\delta$ and $\\pmb{\\varDelta}$ are congruent matrices (i.e., $\\delta=$ $K^{\\prime}\\varDelta K\\vert K\\vert\\neq0)$ and therefore, have the same signature (see, e.g., Gantmacher [3, pp. 291-2971). Q.E.D.\n\nThe text discusses a multivariate adjustment costs model of the firm, focusing on the effects of expected price changes on quasi-fixed factor demands. It introduces mathematical constructs such as vector processes, matrices, and eigenvalues to model these dynamics."
  },
  {
    "qid": "econ-empirical-340-0-0-0",
    "question": "1) Derive the matrix form of the SAR model given the cross-sectional equation for $y_i$. Show how $Y_n$, $X_n$, $W_n$, and $V_n$ are defined.",
    "gold_answer": "The matrix form of the SAR model is derived as follows:\n\n1. Define the $n$-dimensional vectors:\n   - $Y_n = (y_1, y_2, \\dots, y_n)^\\prime$\n   - $V_n = (v_1, v_2, \\dots, v_n)^\\prime$\n\n2. Define the $n \\times k$ matrix $X_n = (x_1, x_2, \\dots, x_n)^\\prime$.\n\n3. The spatial weight matrix $W_n$ is an $n \\times n$ matrix with elements $w_{ij}$ and $w_{ii} = 0$.\n\n4. The SAR model in matrix form is:\n   $$\n   Y_n = \\lambda W_n Y_n + X_n \\beta + V_n.\n   $$\n\n5. This can be rewritten as:\n   $$\n   (I_n - \\lambda W_n)Y_n = X_n \\beta + V_n.\n   $$",
    "question_context": "The outcome $y$ at the location $i$ is modeled with a cross-sectional SAR equation of \n\n$$\ny_{i}=\\lambda\\sum_{j\\neq i}w_{i j}y_{j}+x_{i}^{\\prime}\\beta+v_{i},\n$$ \n\nwhere $i=1,\\ldots,n$ is the spatial unit, $x_{i}$ is a $k$ -dimensional vector of observed exogenous characteristics at location $i$ including the constant, $v_{i}$ is the error term, the spatial weight $w_{i j}$ measures the connections and their strengths among locations $i\\neq j$ $w_{i i}=0$ for all $i\\underline{{\\cdot}}$ ), so $\\textstyle\\sum_{j\\neq i}w_{i j}y_{j}$ is the outcome of i’s neighbors, the scalar $\\lambda$ is the spatial coefficient, $\\beta$ is a -dimensional parameter.\nWe model this bilateral variable $z_{i j}$ by the regression equation \n\n$$\nz_{i j}=c_{z}+\\varphi_{i j}\\alpha+\\eta_{1i}^{\\prime}\\eta_{2j}+\\varepsilon_{i j},\n$$ \n\nwhere $c_{z}$ is a constant, $\\varphi_{i j}$ is a vector of exogenous variables and $\\alpha$ is its associated slope parameter vector, $\\eta_{1i}$ and $\\eta_{2j}$ are $r$ -dimensional unobserved interactive fixed effects (FE) motivated by the panel factor model, which includes conventional additive fixed effects as a special case, and $\\varepsilon_{i j}$ is the idiosyncratic error.\n\nThe paper studies the estimation of a cross-sectional spatial autoregressive (SAR) model where spatial weights are constructed by bilateral variables such as trade or investment between regions. The endogeneity in spatial weights arises from the correlation between the error term in the SAR model and unobserved interactive fixed effects in bilateral variables. A control function approach is proposed for two-stage estimation methods, with established consistency and asymptotic normality."
  },
  {
    "qid": "econ-empirical-1767-1-0-1",
    "question": "2) Interpret the parameter $\\gamma$ in the context of this model. How does it relate to the elasticity of substitution and the Arrow-Pratt measure of relative risk aversion?",
    "gold_answer": "1. **Elasticity of Substitution**: $\\gamma$ represents the elasticity of substitution between consumption in any two years. This can be seen by evaluating the change in $\\ln(C_{i}/C_{j})$ with respect to the logged ratio of their prices.\n2. **Arrow-Pratt Measure**: $1/\\gamma$ is the Arrow-Pratt measure of relative risk aversion. A higher $\\gamma$ implies lower risk aversion, as the individual is more willing to substitute consumption across time.",
    "question_context": "Following Levhari and Mirman (1977) we assume that consumers maximize a strongly separable utility function of the form\n\n$$\nE U=E_{1}\\bigg\\{\\sum_{i=1}^{T}\\big(1+\\delta\\big)^{1-i}U\\big(C_{i}\\big)\\bigg\\},\n$$\n\nwhere\n\n$$\n\\begin{array}{c c c}{{U(C_{i})=\\displaystyle\\frac{C_{i}^{1-(1/\\gamma)}}{1-\\displaystyle{\\frac{1}{\\gamma}}}~}}&{{\\gamma\\neq1}}\\ {{{}}}&{{{}}}\\ {{{}=\\ln C_{i}}}&{{\\gamma=1}}\\end{array}\n$$\n\nand $C_{i}$ is consumption at age $i,~\\delta$ is the time preference rate, $T$ is the uncertain realized lifespan $(1<T\\leq D$ ，where $D$ is the maximum length of life), and $E_{j}$ is the expectations operator conditional on information at age $j.$\nThere are $D$ states of the world, corresponding to the age at which the individual dies. Letting $q_{i}$ be the probability that age $i$ is the terminal year, expected utility is written\n\n$$\nE U=\\sum_{i=1}^{D}q_{i}\\Biggl[\\sum_{j=1}^{i}\\left(1+\\delta\\right)^{1-j}U\\left(C_{j}\\right)\\Biggr],\n$$\n\nand we assume that $q_{\\iota}$ is known with certainty.\nThe first order conditions can be rearranged to show that\n\n$$\nC_{i+1}=\\bigg[\\bigg(\\frac{1+r}{1+\\delta}\\bigg)\\pi_{i+1}^{i}\\bigg]^{\\gamma}C_{i},\n$$\n\nwhere $r$ is the constant, risk-free interest rate and $\\pi_{i+1}^{i}$ is the probability of living through age $i+1$ ， conditional  on living to  age $i.$\nConsumption at age $i$ is written, using (3),\n\n$$\nC_{i}=\\frac{K_{\\iota}\\displaystyle\\sum_{\\j=1}^{D}Y_{j}R_{\\jmath}}{\\displaystyle\\sum_{j=1}^{D}K_{j}R_{\\jmath}},\n$$\n\nwhere\n\n$$\nK_{\\iota}=\\left[\\pi_{i}^{1}\\Bigg(\\frac{1+r}{1+\\delta}\\Bigg)^{(\\iota-1)}\\right]^{\\gamma},\n$$\n\n$Y_{j}$ is net-of-tax earnings at age $j$ and $R_{j}=$ $(\\vec{1}+r)^{1-\\jmath}$\n\nThis section derives an econometrically tractable expression for consumption when the individual is subject to variable time of death. The model assumes that both earnings and interest rates are known with certainty, with the only uncertainty being the time of death. The utility function is strongly separable, and the analysis includes the effects of mortality probabilities on the life cycle consumption profile."
  },
  {
    "qid": "econ-empirical-1226-1-0-4",
    "question": "5) Derive the reduced-form equation for the effect of an unemployment spell on borrowing, and discuss its limitations compared to the two-stage approach.",
    "gold_answer": "1. **Reduced form**: $$ \\Delta D_{i} = \\alpha + \\beta U_{i} + X_{i}\\gamma + \\epsilon_{i} $$ \n2. **Limitations**: \n   - Ignores heterogeneity in income shocks across households. \n   - Does not isolate the transitory component of income. \n   - Less efficient than the two-stage approach if $U_{i}$ is a weak instrument.",
    "question_context": "In the absence of borrowing constraints, the permanent income hypothesis suggests that a household facing a transitory income shortfall will dissave in order to smooth consumption. For households with low initial assets, this implies that borrowing will respond to transitory income variation.\nMeasured changes in labor income for the head of household i, $\\Delta Y_{i}$ can be decomposed into a transitory $(\\Delta Y_{i}^{\\top})$ and a permanent $(\\Delta\\upmu_{i})$ component: $\\Delta Y_{i}=\\Delta Y_{i}^{\\tau}+\\Delta\\upmu_{i}$.\nTo examine how household borrowing responds to changes in transitory income, one could estimate the following: $$ \\Delta D_{i}=\\upalpha_{0}+\\upalpha_{1}\\Delta Y_{i}^{\\tau}+\\upalpha_{2}\\Delta\\upmu_{i}+X_{i}\\upalpha_{3}+\\upxi_{i} $$ where $\\Delta D_{i}=D_{i t}-D_{i t-1}$ and $D_{i t}$ represents the level of unsecured debt for household $i$ at the end of year $t$.\nAddressing these concerns, I exploit the panel nature of the data to identify transitory and exogenous changes in income resulting from an unemployment spell of the head of household i that occurs at some point during year t as a result of a layoff, illness or injury to the worker, being discharged or fired, employer bankruptcy, or the employer selling the business.\nFor each household I construct a dummy variable, $U_{i},$ indicating whether during year $t$ the head experiences a spell of unemployment as defined above. Treating this unemployment spell indicator as an instrument for changes in earnings, I estimate the following two-stage model: $$ \\begin{array}{r}{\\Delta Y_{i}=\\hat{8}_{0}+\\hat{8}_{1}U_{i}+X_{i}\\hat{8}_{2}+\\mathfrak{v}_{i}}\\\\ {\\quad}\\\\ {\\Delta D_{i}=\\upalpha+\\upbeta\\Delta\\hat{Y_{i}}+X_{i}\\upgamma+\\mathfrak{v}_{i}}\\end{array} $$ where $\\boldsymbol{\\upeta}_{i}=\\gamma\\Delta\\boldsymbol{\\upmu}_{i}+\\boldsymbol{\\upvarepsilon}_{i}$.\n\nThe section discusses the methodology for analyzing household borrowing responses to transitory income changes, focusing on unemployment spells as an instrument for exogenous income variation."
  },
  {
    "qid": "econ-empirical-1736-1-0-0",
    "question": "1) Derive the firm's first-order condition for profit maximization under Nash behavior, starting from the profit function $\\uppi = [P(C)C/R - w]r$. Show how this leads to equation (5) in the text.",
    "gold_answer": "1. Start with the profit function: $\\uppi = [P(C)C/R - w]r$.  \n2. Differentiate with respect to $r$: $\\pi_r = \\frac{\\partial}{\\partial r}\\left[P(C)C/R - w\\right]r + [P(C)C/R - w]\\frac{\\partial r}{\\partial r}$.  \n3. Apply the product rule and chain rule: $\\pi_r = \\left[P'(C)\\frac{\\partial C}{\\partial r}\\frac{C}{R} + P(C)\\frac{\\partial}{\\partial r}\\left(\\frac{C}{R}\\right)\\right]r + [P(C)C/R - w]$.  \n4. Substitute $C = F(R)$ and $R = r + \\tilde{R}$: $\\frac{\\partial C}{\\partial r} = F'(R)$, $\\frac{\\partial}{\\partial r}\\left(\\frac{C}{R}\\right) = \\frac{F'(R)R - F(R)}{R^2}$.  \n5. Combine terms to arrive at equation (5): $\\pi_r = \\frac{P(C)C}{R} - w + \\frac{r}{R}\\left[P'(C)C + P(C)\\right]F'(R)$.",
    "question_context": "Nash behavior is assumed to characterize the firms; that is, each firm takes the level of exploitation of others as given when determining its own exploitation rate.\nThe aggregate production function is assumed to satisfy $C=F(R),~F^{\\prime}(R)>0,~F^{\\prime\\prime}(R)<0,~F(0)=0,$ where ${\\pmb F}({\\pmb R})$ is bounded above by the entire fish population of the commons.\nEach firm's catch $c$ can be related to the total catch via the production possibility function: $c=[r/(r+\\tilde{R})]F(r+\\tilde{R})=(r/R)F(R),$ where $r$ is the firm's vessels and $\\tilde{R}=R-r$ is the size of the rest of the fleet.\nThe Nash solution requires the firm to choose its fleet size to maximize profit $\\uppi$, while taking $\\tilde{R}$ as fixed. The firm's problem is to $\\operatorname*{max}_{\\{r\\}}{\\{[P(C)C/R-w]r\\}},$ where $w$ is the rental rate for vessels and $C=F(r+{\\tilde{R}})$.\nThe Pareto or social optimum for the commons corresponds to choosing the industry fleet size that maximizes total industry profit $\\Pi$ for competitive markets. The maximand is $\\Pi(R)=P\\cdot F(R)-w R.$\nThe social optimum number of firms, $n^{*}$, is given by $n^{*}=1+\\upvarepsilon_{C}/[(\\upvarepsilon_{C}-1)\\upvarepsilon_{D}],$ where $\\upvarepsilon_{C}$ is the elasticity of input productivity and $\\upvarepsilon_{D}$ is the price elasticity of market demand.\n\nThe model assumes Nash behavior among firms exploiting a common property renewable resource, with each firm maximizing profit while taking others' exploitation rates as given. The aggregate production function and market conditions are specified, leading to a discussion of the social optimum number of firms."
  },
  {
    "qid": "econ-empirical-1759-0-0-0",
    "question": "1) What are the primary stylized facts that wage indexation models must conform to, according to Kaufman and Woglom (1987)?",
    "gold_answer": "The primary stylized facts include:\\n- **Nominal wage rigidity**: Wages adjust slowly to changes in economic conditions.\\n- **Real wage cyclicality**: Real wages exhibit pro-cyclical or acyclical behavior.\\n- **Inflation persistence**: Wage adjustments lag behind inflation trends.",
    "question_context": "Author(s): Roger Kaufman and Geoffrey Woglom\nSource: The American Economic Review, Sep., 1987, Vol. 77, No. 4 (Sep., 1987), pp. 747- 749\n\nThis comment critiques the conformity of wage indexation models with empirical stylized facts, focusing on theoretical and econometric implications."
  },
  {
    "qid": "econ-empirical-58-3-0-1",
    "question": "2) Interpret the coefficient of the dictatorship dummy in Regression (13) and explain its economic significance in the context of urban concentration.",
    "gold_answer": "1. The coefficient of 1.788 implies that a 1% increase in the probability of a dictatorship increases the size of the central city by approximately 1.8%. \n2. This suggests that dictatorships tend to centralize population and economic activity in the main city, possibly due to rent-seeking behavior or centralized control mechanisms. \n3. The magnitude indicates a strong causal effect of political regimes on urban structure.",
    "question_context": "To examine the results of Table IV more closely, Table VI reproduces regression (3) but allows for the possibility that trade and the dictatorship dummy are endogenously determined. We use three sets of instruments to examine how exogenous changes in the share of trade in GDP and the type of political regime alter the size of the main city.\nOur identifying assumptions are that these variables affect politics and trade but do not change urban structure directly. We test these assumptions using a Wu-Hausman test of the overidentifying restrictions for the system of equations and find that our assumptions pass these tests.\nRegression (13) repeats regression (3) using our instrumental-variables approach. The coefficient on the dictatorship dummy remains significant and large. A 1 percent increase in the probability of having a dictatorship increases the size of the central city by about 1.8 percent.\n\nThe text discusses the endogeneity of transport spending, trade, and dictatorship variables, and how population concentration might affect foreign trade and political regimes. It introduces instrumental variables to address endogeneity in examining the size of the main city."
  },
  {
    "qid": "econ-empirical-36-1-0-3",
    "question": "4) Show mathematically why the production-possibility schedule becomes nonlinear (bowed inward or outward) if Assumption 6 (homogeneous and redundant land) or Assumption 11 (constant returns to scale) is violated. Explain the implications for the labor theory of value.",
    "gold_answer": "1. **Bowed-Inward Case**: If division of labor is limited by market extent (violating Assumption 11), $B$ and $D$ production exhibits increasing returns, causing the schedule to bow inward.  \n2. **Bowed-Outward Case**: If land is scarce (violating Assumption 6), diminishing returns cause the schedule to bow outward.  \n3. **Implications**: The price ratio $P_{D\\cdot B}$ now depends on demand, invalidating the labor theory of value, which requires $P_{D\\cdot B} = \\frac{L_D/D}{L_B/B}$ to hold independently of preferences.",
    "question_context": "The labor theory of value (hereafter called the Theory) is invalid under a wide variety of circumstances even when labor is the only scarce factor. Some of those circumstances have been frequently (though not always adequately) discussed, notably when labor is not productively homogeneous. Other circumstances of comparable importance have been surprisingly neglected, as when laborers (whether homogeneous or not) have preferences among alternative, equally paid occupations.\nTo validate this Theory in the simplest possible, rigorous model of general equilibrium, quite a few assumptions have to be made... (1) the supply of labor $(L)$ is fixed at $\\pmb{L}_{0};$ (2) there are only two homogeneous goods, beaver $(B)$ and deer $(D)$ ;(3) $\\pmb{L}$ $B.$ , and $\\pmb{D}$ are continuously divisible; (4) each household seeks to maximize ordinal utility in the functional form $\\pmb{U}=\\pmb{U}(\\pmb{B},\\pmb{D})$ , determinate up to any order-preserving transformation and with normally convex indifference curves; (5) if exchange takes place, there are no transaction costs; (6) each relevant type of land and water, including all relevant natural resources such as deer woods and beaver ponds, is not only unappropriated but also homogeneous and redundant; (7) $\\pmb{B}$ and $\\pmb{D}$ are each produced instantaneously with $\\pmb{L}$ as the only scarce factor; (8) $\\pmb{L}$ is homogeneous in its productivity of either $\\pmb{B}$ or $\\pmb{D}$ ; (9) all laborers, if equally remunerated, are equally willing to hunt $\\pmb{B}$ or $\\pmb{D}$ ; (10) everyone is free, legally and otherwise, to hunt either $\\pmb{B}$ or $\\pmb{D}$ ; and (11) $B$ and $\\pmb{D}$ are produced independently (not jointly) at constant returns to scale--in accordance with production functions such as $B=L_{B}$ and $\\smash{D=2L_{D}}$ (with labor measured in days and ${L_{B}}+{L_{D}}={L_{0}}$.\nThe competitive long-run-equilibrium exchange rate or price of $\\pmb{D}$ in terms of $\\pmb{B}$ is wholly independent of consumer preferences or demand, but depends instead solely on the technologically given ratios of labor quantities needed to produce the goods. Specifically, $$\\begin{array}{r}{P_{D\\cdot B}=P_{D}/P_{B}=\\frac{1}{2}.}\\end{array}$$\nThe real wage rates, equal to both the average and marginal physical products of labor, are $$\\begin{array}{l}{\\displaystyle P_{L\\cdot B}=\\frac{P_{L}}{P_{B}}=\\frac{B}{L_{B}}=\\frac{d B}{d L_{B}}=1,}\\ {\\displaystyle P_{L\\cdot D}=\\frac{P_{L}}{P_{D}}=\\frac{D}{L_{D}}=\\frac{d D}{d L_{D}}=2.}\\end{array}$$\n\nThe labor theory of value is invalid under a wide variety of circumstances even when labor is the only scarce factor. This section explores the assumptions necessary for the theory to hold, focusing on Smith's simplified model of a two-good economy (beaver and deer) with homogeneous labor."
  },
  {
    "qid": "econ-empirical-1121-1-1-3",
    "question": "8) The study clusters standard errors by birth year. Justify this choice and explain how it affects inference.",
    "gold_answer": "1. **Justification**: Clustering accounts for correlation in errors within birth years (e.g., cohort-specific shocks).\n2. **Inference**: Clustered standard errors are more conservative (larger) than unclustered ones, reducing the risk of over-rejecting the null hypothesis (Type I error).",
    "question_context": "For the low-education (lower-income) group, having been born at the peak of the notch years increases household Social Security income by between $1,000 and $1,400 per year in 1993/1994 dollars. Relative to a mean household Social Security income of $9,625 for this group, this translates into an increase of between 10 and 14.5 percent. These regressions provide a strong first stage for our IV estimation strategy, with partial F-statistics for the notch indicator ranging from 16.80 to 33.18 in our baseline specification.\nBy contrast, the relationship between notch status and Social Security income is much weaker for households whose primary beneficiary has a high school diploma or better. For these households, having been born during the notch years only increases household Social Security income by between $72 (0.6 percent) and $301 (2.7 percent) per year. Moreover, the partial F-statistics from these regressions are all below 1.00, suggesting that there is not a strong enough first-stage relationship present to support an IV estimation strategy for this group.\n\nThe study examines the differential impact of the Social Security benefits notch by education group, finding stronger effects for low-education (lower-income) households. The analysis uses instrumental variables to address endogeneity and reports first-stage statistics to validate the approach."
  },
  {
    "qid": "econ-empirical-311-3-0-1",
    "question": "2) Formally analyze the impact of including a time trend (columns 2 and 5) versus year dummies (columns 3 and 6) on the estimates of $\\gamma_{1}$ and $\\gamma_{2}$. Why might the qualitative patterns remain unchanged despite the presence of nonstationarity?",
    "gold_answer": "1. **Time Trend Model**: The time trend captures a linear drift: $y_t = \\gamma_1 x_{1t} + \\gamma_2 x_{2t} + \\beta t + \\epsilon_t$.\n2. **Year Dummies Model**: This captures nonlinear trends: $y_t = \\gamma_1 x_{1t} + \\gamma_2 x_{2t} + \\sum_{s=1}^T \\beta_s I(t=s) + \\epsilon_t$.\n3. **Invariance of $\\gamma$**: If the time-varying component is orthogonal to $x_{1t}$ and $x_{2t}$, i.e., $E[x_{jt} \\cdot t] = 0$ and $E[x_{jt} \\cdot I(t=s)] = 0$, then $\\gamma_1$ and $\\gamma_2$ remain consistent.",
    "question_context": "Columns 1, 2, and 3 of Table 7 use the original, undiscretized demographic variables, whereas columns 4, 5, and 6 use their discretized versions. Neither the main coefficient estimates ( $\\gamma_{1}$ and $\\gamma_{2}$ ) nor the pseudo $R^{2}$ ’s show much differences between these two sets of regressions.\nTo investigate the extent of potential nonstationarity in the data, we may compare columns 1 and 4 (no time trend) with columns 2 and 5 (with time trend), and columns 3 and 6 (with year dummies). The latter estimates suggest the presence of a mildly positive time trend, but the qualitative patterns of $\\gamma_{1}$ and $\\gamma_{2}$ do not change.\n\nThis subsection investigates the sensitivity of preliminary regressions to the discretization of demographic variables and the inclusion of a time trend in a potentially nonstationary environment."
  },
  {
    "qid": "econ-empirical-1295-3-0-0",
    "question": "1) Derive the bandwidth selection formula $b = 1.05^{j-15}(N T)^{-1/5}$ for $1 \\leq j \\leq 30$ and explain how the constants $c_1 = 0.5051$ and $c_2 = 2.0789$ are determined in the context of the CV method described in (16).",
    "gold_answer": "1. The bandwidth $b$ is selected using the cross-validation (CV) method to minimize the mean squared error (MSE) of the estimator. The formula $b = 1.05^{j-15}(N T)^{-1/5}$ is derived from the optimal rate for kernel density estimation, where $(N T)^{-1/5}$ ensures the bandwidth decreases as the sample size increases. The factor $1.05^{j-15}$ allows for a grid search over $j$ to find the optimal bandwidth. \n2. The constants $c_1$ and $c_2$ are determined by the range of $j$ values (1 to 30) and the scaling factor $1.05^{j-15}$. Specifically, $c_1 = 1.05^{1-15} \\approx 0.5051$ and $c_2 = 1.05^{30-15} \\approx 2.0789$ define the minimum and maximum bandwidth values in the grid search.",
    "question_context": "The theoretical results of Sections 3.1 and 3.2 give conditions under which the proposed test statistic $\\hat{J}_{N T}$ will be well-behaved in terms of its limiting distribution under the null and its power under the alternative in large samples. In this section, we conduct a series of Monte Carlo experiments to investigate the finite sample performance of $\\hat{J}_{N T}$.\nAs to the selection of bandwidth $^{b}$ , we make use of the CV method described in (16) and set the bandwidth to $b=$ $1.05^{j-15}(N T)^{-1/5}$ , where $1\\leq j\\leq30$ , implying that $c_{1}=0.5051$ and $c_{2}=2.0789$ in (16). For the bandwidth $h$ , we make use of the $\\operatorname{AR}(p)$ -based plug-in bandwidth given by (17), and specify the correction parameter $c=0.75$.\nFor the univariate kernels used in the nonparametric estimation of trend function and in the calculation of $\\hat{J}_{N T}$ , we choose the Epanechnikov kernel $l(\nu)~=~k(\nu)~=~0.75\\left(1-\nu^{2}\right)1(|\nu|\\leq1).$.\nWe generate 1000 datasets of panel observations $\\left\\{{{y}_{i t}},i=1,\\ldots,N,t=1,\\ldots,T\right\\}$ with cross-sectional sample size $\textsl{N}=15,30,50$ and length of time series $T=15,30,50.$ , and employ $\begin{array}{l l l}{B}&{=}&{399}\\end{array}$ bootstrap iterations for each simulated dataset.\nThe sequence of $\\eta_{t}$ in the wild bootstrap is set to Mammen’s (1993) two-point distribution: \n$$\n\\eta_{t}=\\left\\{\begin{array}{c c}{\\left(1-\\sqrt{5}\right)/2}&{\\mathrm{~with~probability~}\\left(\\sqrt{5}+1\right)/2\\sqrt{5},}\\ {\\left(1+\\sqrt{5}\right)/2}&{\\mathrm{~with~probability~}\\left(\\sqrt{5}-1\right)/2\\sqrt{5}.}\\end{array}\right.\n$$\nFirst, to investigate the size performance of our test under the null, we consider the following data generating process, denoted by DGP.S, \n$$\n\begin{array}{l}{{y_{i t}=\\alpha_{i}+g\\left(\tau_{t}\right)+u_{i t},}}\\ {{u_{i t}=\rho u_{i,t-1}+\\varepsilon_{i t},}}\\end{array}\n$$ \nwhere we fix the individual effects $\\alpha_{i}$ by first generating $\\alpha_{1},\\dots,\\alpha_{N-1}$ independently from the standard normal distribution, then taking $\begin{array}{r}{\\alpha_{N}\\stackrel{\\cdot}{=}-\\sum_{i=1}^{N-1}\\alpha_{i}}\\end{array}$ and keeping these $\\alpha_{i}$ fixed across replications.\nThe error term $u_{i t}$ is assumed to follow an AR(1) process by letting $\rho$ take 0, 0.4, and 0.7. Let $\boldsymbol{\\varepsilon}_{t}=(\\varepsilon_{1t},\\varepsilon_{2t},\\dots,\\overline{{\\varepsilon}}_{N t})^{\top}$ , which is generated as a sequence of an $N$ -dimensional vector of independent normal random variables with zero mean and constant covariance matrix $\big[\\omega_{i j}\big]_{N\times N}$ over time where $\\omega_{i j}=\\lambda^{|i-j|}$ and $\\lambda$ takes $\\lambda_{1}=0$ or $\\lambda_{2}=0.5$.\nTo investigate the testing powers of $\\hat{J}_{N T}$ , we consider four alternative specifications for $g(\tau_{t})$: \nDGP.P1—Quadratic trend: $g(\tau_{t})=\\gamma\tau_{t}^{2}$; \nDGP.P2—Broken trend: $g(\tau_{t})=\\left\\{\begin{array}{l l}{0,}&{\tau_{t}\\leq0.5,}\\ {\\gamma,}&{\tau_{t}>0.5;}\\end{array}\right.$ \nDGP.P3—Quartic trend: $g(\tau_{t})=1+\tau_{t}+2\\gamma\tau_{t}^{4}$; \nDGP.P4—Broken linear trend: \n$$\ng(\tau_{t})=\\left\\{\begin{array}{l l}{1+\tau_{t},}&{\tau_{t}\\leq0.25,}\\ {1+\tau_{t}+\\gamma\\left(0.5+\tau_{t}\right),}&{0.25<\tau_{t}<0.75,}\\ {1+\tau_{t},}&{\tau_{t}\\geq0.75.}\\end{array}\right.\n$$\n\nThe section investigates the finite sample performance of the test statistic $\\hat{J}_{N T}$ through Monte Carlo experiments, focusing on its size and power under various conditions. The experiments utilize the residual-based wild bootstrap method and consider different bandwidth selections, kernel choices, and data generating processes."
  },
  {
    "qid": "econ-empirical-1581-2-1-3",
    "question": "4) Evaluate the impact of temperature and altitude on land inequality and their indirect effects on numeracy.",
    "gold_answer": "Temperature has a negative effect on land inequality, though not always significant, suggesting that warmer regions may have smaller land holdings. Altitude also has a negative sign, indicating that higher altitudes are associated with smaller farms. Both factors could indirectly promote higher numeracy, as smaller farms typically require more complex management and numeracy-intensive practices.",
    "question_context": "Pasture suitability is negatively correlated with land inequality. This finding corresponds to the observation that there are more farms of medium size in the European regions where dairy farming was efficient. By contrast, cereal suitability is significantly positively correlated with land inequality, whereas sugar suitability is only sometimes significant and positive (and potato suitability has mixed effects). Temperature has a negative effect, but it is not always significant. Altitude has a negative sign while that of ruggedness is positive.\n\nThis section examines the relationship between land inequality and numeracy, using land inequality as a dependent variable in OLS regressions. The analysis considers various geographic variables and their impact on land inequality and numeracy, with a focus on the theoretical argument of GMV (Geographic-Market-Value) as a potential mechanism."
  },
  {
    "qid": "econ-empirical-1015-3-0-0",
    "question": "1) Define a punishment equilibrium in the context of the negotiation game and explain why it must be self-enforcing.",
    "gold_answer": "A punishment equilibrium for a player is a perfect equilibrium where the player receives their lowest perfect equilibrium payoff. It must be self-enforcing because any deviation during the punishment phase can only be penalized by restarting the same punishment equilibrium, ensuring compliance through the threat of perpetual punishment.",
    "question_context": "Define an (optimal) punishment equilibrium for a player to be a perfect equilibrium in which this player receives his lowest perfect equilibrium payoff.\nThe strategy will specify one disagreement game outcome for all odd periods and another outcome for all even periods.\nPlayer 1's lowest disagreement payoff is player 1's minimax payoff in the disagreement game.\nPlayer 2's highest supportable disagreement payoff in an odd period is denoted by $w_{2}$, defined as $$w_{2}=\\operatorname*{max}_{a\\in A}\\left\\{u_{2}\\big(a\\big)-\\left[\\operatorname*{max}_{a_{1}^{\\prime}\\in A_{1}}u_{1}\\big(a_{1}^{\\prime},a_{2}\\big)-u_{1}(a)\\right]\\right\\}.$$\nProposition 2: For all $\\delta\\in(0,1)$, player 1's average perfect equilibrium payoffs in the negotiation game are not less than $(1-w_{2})/(1+\\delta)$, starting in an odd period, and not less than $[\\delta(1-w_{2})]/(1+\\delta)$, starting in an even period.\nProposition 3: There exists a $\\underline{{\\delta}}\\in(0,1)$ such that, $\\forall\\delta\\in(\\underline{{\\delta}},1)$, the offer $(1-w_{2})/(1+\\delta)$ is accepted in a perfect equilibrium of a negotiation game in an odd period, and the offer $\\delta(1-w_{2})/(1+\\delta)$ is accepted in a perfect equilibrium in an even period.\nPlayer 1's highest supportable disagreement payoff in an even period of player 2's punishment equilibrium is denoted by $w_{1}$, defined as $$w_{1}=\\operatorname*{max}_{a\\in A}\\left\\{u_{1}(a)-\\left[\\operatorname*{max}_{a_{2}^{\\prime}\\in A_{2}}u_{2}(a_{1},a_{2}^{\\prime})-u_{2}(a)\\right]\\right\\}.$$\nProposition 4: There exists a $\\underline{{\\delta}}\\in(0,1)$ such that, $\\forall\\delta\\in(\\underline{{\\delta}},1)$, the offer $(1+\\delta w_{1})/(1+\\delta)$ is accepted in a perfect equilibrium of a negotiation game in an odd period, and the offer $\\delta(\\delta+w_{1})/(1+\\delta)$ is accepted in a perfect equilibrium in an even period.\nTheorem: For any feasible payoff vector $(v_{1},v_{2})$ of the negotiation game such that $v_{i}>\\underline{{v}}_{i}$, for $i=1,2$, there exists $\\underline{{\\delta}}\\in(0,1)$ such that $\\forall\\delta\\in(\\underline{{\\delta}},1)$, $(v_{1},v_{2})$ is a perfect equilibrium payoff vector in the negotiation game with discount factor $\\delta$.\n\nThis section defines punishment equilibria and perfect equilibria in the negotiation game, focusing on player 1's punishment equilibrium. It derives lower bounds for player 1's perfect equilibrium payoffs and constructs equilibria where these bounds are attained under certain conditions."
  },
  {
    "qid": "econ-empirical-782-1-0-0",
    "question": "1) Derive the conditions under which the NK-DSGE model $A_{0}^{W}W_{t}=A_{f}^{W}E_{t}W_{t+1}+A_{b}^{W}W_{t-1}+\\eta_{t}^{W}$ has a unique and stable solution. What role does Assumption 1 play in this context?",
    "gold_answer": "To ensure a unique and stable solution, the following conditions must hold:\\n1. **Eigenvalue Condition**: The matrices $A_{0}^{W}, A_{f}^{W}, A_{b}^{W}$ must satisfy that the generalized eigenvalues of the pencil $(A_{0}^{W} - \\lambda A_{f}^{W})$ lie inside the unit circle.\\n2. **Stationarity**: The process $W_{t}$ must be asymptotically stationary, implying that the solution does not explode.\\n3. **Assumption 1**: This ensures that the true parameter $\\theta_{0}$ lies in the interior of $\\mathcal{P}^{*}$, guaranteeing a unique stable solution for the NK-DSGE model.",
    "question_context": "Let $W_{t}$ be the $p$ -dimensional vector collecting all the variables of interest. A typical structural monetary NK-DSGE model takes the form of the linearized rational expectations model: $A_{0}^{W}W_{t}=A_{f}^{W}E_{t}W_{t+1}+A_{b}^{W}W_{t-1}+\\eta_{t}^{W}$, where $A_{0}^{W},A_{f}^{W}$, and $A_{b}^{W}$ are $p\\times p$ matrices whose elements depend on the structural parameters collected in the vector $\\theta$, and $\\eta_{t}^{W}$ is a mean zero vector of structural disturbances.\nThe structural disturbance term $\\eta_{t}^{W}$ obeys a vector autoregressive processes of order one: $\\eta_{t}^{W}=R_{W}\\eta_{t-1}^{W}+u_{t}^{W},u_{t}^{W}\\sim\\operatorname{WN}(0_{p\\times1},\\Sigma_{W,u})$, where $R_{W}$ is a diagonal stable matrix and $u_{t}^{W}$ is a white-noise term with covariance matrix $\\Sigma_{W,u}$.\nUnder Assumption 1 (Determinacy), the unique stable solution of the model can be represented as the asymptotically stationary VAR system: $W_{t}=\\tilde{F}_{1}W_{t-1}+\\tilde{F}_{2}W_{t-2}+{\\varepsilon}_{t}^{W},{\\varepsilon}_{t}^{W}{=}\\tilde{Q}u_{t}^{W}$, where $\\tilde{F}_{1}=F_{1}(\\theta)$, $\\tilde{F}_{2}=F_{2}(\\theta)$, and $\\tilde{Q}=Q(\\theta)$ are $p\\times p$ matrices that depend nonlinearly on $\\theta$.\nThe NK-DSGE model is 'incomplete' as it does not specify how unobservable components of $W_{t}$ are generated. Assumption 2 states that the subvector $W_{t}^{u}$ is such that $\\Delta W_{t}^{u}$ is covariance stationary, i.e., $W_{t}^{u}\\sim I(1)$.\nThe 'complete' NK-DSGE model can be given the structural representation: $A_{0}^{Z}Z_{t}=A_{f}^{Z}E_{t}Z_{t+1}+A_{b}^{Z}Z_{t-1}+\\eta_{t}^{Z}$, $\\eta_{t}^{Z}=R_{Z}\\eta_{t-1}^{Z}+u_{t}^{Z},u_{t}^{Z}\\sim\\operatorname{WN}(0_{n\\times1},\\Sigma_{u,Z})$, where $A_{0}^{Z},A_{f}^{Z},A_{b}^{Z}$, and $\\Sigma_{u,Z}$ depend on $\\theta$ and additional parameters $\\theta^{a}$.\n\nThe text presents a structural representation of a typical NK-DSGE model capturing business cycle features. The model is log-linearized around steady-state values and involves a system of equations describing economic agents' behavior."
  },
  {
    "qid": "econ-empirical-266-4-3-3",
    "question": "4) Compare the spectral decomposition approach in this section to the eigenvalue-eigenvector decomposition used in Appendix B. What are the key differences and similarities?",
    "gold_answer": "**Comparison**:\n1. **Similarities**:\n   - Both methods decompose observed matrices/operators to identify unobserved components.\n   - Both rely on rank/full rank conditions (Assumption 10 in Appendix B, invertibility here).\n   - Both require uniqueness/ordering assumptions (Assumption 11 in Appendix B, analogous here).\n\n2. **Differences**:\n   - **Appendix B**: Uses eigenvalue-eigenvector decomposition for discrete-state matrices.\n   - **Appendix D**: Uses spectral decomposition for continuous-state operators, generalizing the approach.\n   - **Complexity**: The spectral decomposition here handles unobserved heterogeneity ($x_t^*$), adding a layer of complexity.\n\nBoth approaches are powerful tools for nonparametric identification, with the spectral decomposition being more general but also more technically demanding.",
    "question_context": "In this section, we first derive Eq. (6.2). Assume that $\\boldsymbol{x}_{t}^{*}$ is a continuous variable. The joint distribution of the observed state variable at four consecutive periods $(s_{t+2},s_{t+1},s_{t},s_{t-1})$ can be decomposed as follows. \n\n$$\n\\begin{array}{r l}&{\\int_{\\mathbb{R}_{n+1}}\\rho_{\\alpha_{1}}\\chi_{n+1}}\\ {=}&{\\int_{\\mathbb{R}_{n+1}}\\int_{\\mathcal{R}_{n+1}}\\int_{\\mathcal{R}_{n+1}}\\int_{\\mathcal{R}_{n+1}}\\int_{\\mathcal{R}_{n+1}}\\int_{\\mathcal{R}_{n+1}}\\int_{\\mathcal{R}_{n+1}}\\chi_{n+1}(\\boldsymbol{x}_{n+1},\\boldsymbol{x}_{n+1},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\\boldsymbol{x}_{n+1}^{*},\n$$ \n\nThe second equality in Eq. (D.1) holds under the first-order Markov property of the dynamic process and the conditional independence imposed in Assumption 6(i). By integrating out the unobserved choice variables $(y_{t+1},y_{t},y_{t-1})$ , the third equality holds. We further integrate out the unobserved heterogeneity $(x_{t+1}^{*},x_{t-1}^{*})$ , which yields the last line of Eq. (D.1). \n\nBased on this equation, we apply the spectral decomposition technique developed by Hu and Schennach (2008) to nonparametrically identify $f_{s_{t+2}|s_{t+1},x_{t}^{*}},f_{s_{t+1}|s_{t},x_{t}^{*}}.$ , and $f_{x_{t}^{*},s_{t},s_{t-1}}$ oinvokehormdSchnahedaha following operators constructed from densities in Eq. (D.1) are invertible. Specifically, denote the support of $s_{t}$ and $\\boldsymbol{x}_{t}^{*}$ as and , respectively. The linear operator $L_{s_{t-1},\\overline{{s}}_{t},\\overline{{s}}_{t+1},s_{t+2}}$ is a mapping from the $\\mathcal{L}^{p}$ space of functions of $s_{t+2}$ to the $\\mathcal{L}^{p}$ space of functions $s_{t-1}$ defined as \n\n$$\n\\big(L_{s_{t-1},\\bar{s}_{t},\\bar{s}_{t+1},s_{t+2}}h\\big)(s_{t-1})=\\int f_{s_{t-1},s_{t},s_{t+1},s_{t+2}}(s_{t-1},\\bar{s}_{t},\\bar{s}_{t+1},s_{t+2})h(s_{t+2})d s_{t+2},\n$$ \n\nfaorre  adlel ffiunnecdt iaosns $h\\in\\mathcal{L}^{p}(S),\\bar{s}_{t}\\in S$ , and $\\bar{s}_{t+1}\\in S$ . Similarly, the operators $L_{s_{t+2}|\\bar{s}_{t+1},x_{t}^{*}},L_{x_{t}^{*},\\bar{s}_{t},s_{t-1}}.$ , and the diagonal operator $D_{\\bar{s}_{t+1}|\\bar{s}_{t},x_{t}^{*}}$ \n\n$$\n\\begin{array}{r l}&{\\big(L_{s_{t+2}|\\bar{s}_{t+1},x_{t}^{*}}h\\big)(s_{t+2})=\\displaystyle\\int f_{s_{t+2}|s_{t+1},x_{t}^{*}}(s_{t+2}|\\bar{s}_{t+1},x_{t}^{*})h(x_{t}^{*})d x_{t}^{*},}\\ &{\\big(L_{x_{t}^{*},\\bar{s}_{t},s_{t-1}}h\\big)(s_{t-1})=\\displaystyle\\int f_{x_{t}^{*},s_{t},s_{t-1}}(x_{t}^{*},\\bar{s}_{t},s_{t-1})h(x_{t}^{*})d x_{t}^{*}}\\ &{\\big(D_{\\bar{s}_{t+1}|\\bar{s}_{t},x_{t}^{*}}h\\big)(x_{t}^{*})=f_{s_{t+1}|s_{t},x_{t}^{*}}(\\bar{s}_{t+1}|\\bar{s}_{t},x_{t}^{*})h(x_{t}^{*})}\\end{array}\n$$ \n\nfor all functions $h\\in\\mathcal{L}^{p}(\\mathcal{X}),\\bar{s}_{t}\\in S$ , and $\\bar{s}_{t+1}\\in S$ . Assumptions similar to Hu and Schennach (2008, Assumptions 4 and 5) can also be invoked to guarantee the uniqueness of the spectral decomposition and the ordering of the eigenfunctions.\n\nThis section extends the identification results to settings with unobserved heterogeneity, leveraging spectral decomposition techniques."
  },
  {
    "qid": "econ-empirical-249-0-1-2",
    "question": "3) Prove that the replicator dynamics preserves the simplex property \\(\\sum_i x_i = 1\\).",
    "gold_answer": "1. Compute \\(\\frac{d}{dt}\\sum_i x_i = \\sum_i x_i (f_i - \\bar{f})\\). 2. Note \\(\\sum_i x_i f_i = \\bar{f}\\). 3. Thus \\(\\frac{d}{dt}\\sum_i x_i = \\bar{f} - \\bar{f} = 0\\).",
    "question_context": "This volume testifies to an already rich tradition of intellectual cross-fertilisations between economics and biology.\nThe intellectual indebtedness of Darwin in his search for a unifying explanatory principle (which he later found in natural selection) to both Malthus and the Scottish moral philosophers is documented in several papers.\n\nThe text discusses the interdisciplinary nature of economic thought, particularly the influence of biology on economics and vice versa."
  },
  {
    "qid": "econ-empirical-365-1-0-2",
    "question": "3) Compare the role of money in challenging Say’s Law in both Marx’s and Keynes’ frameworks, as identified by Sardoni.",
    "gold_answer": "1. **Keynes’ view**: Money’s liquidity preference disrupts Say’s Law by allowing savings to diverge from investment, leading to insufficient aggregate demand.  \n2. **Marx’s view**: Money acts as a medium of exchange and store of value, creating potential disproportionalities between production and consumption (i.e., 'overproduction crises').  \n3. **Commonality**: Both reject Say’s Law by emphasizing money’s role in decoupling sales and purchases.",
    "question_context": "Keynes’ unemployment equilibrium is generally considered as a short-period situation in which the capital stock, money stock and money wages are assumed given; when these variables are free to vary in long-period conditions, unemployment can only result from imperfections in the market mechanism.\nClaudio Sardoni joins a small group of economists who argue that Keynes intended his unemployment equilibrium to apply to the long period, interpreted as a centre of gravitation around which the system moves over time with no tendency to full employment.\nSardoni identifies two major similarities between Marx and Keynes: the role of money in challenging Say's Law and the implicit assumption of an inverse relationship between investment and expected profits.\nMarx's explanation of unemployment rests on the tendency for output to expand more rapidly than purchasing power. If competitive firms produce in conditions of constant variable and decreasing average total costs then profit maximisation keeps output to a maximum when expected prices are above, and to a minimum when they are below, variable costs.\nUnemployment equilibrium would require permanent crisis which only results if the expected rate of profit falls as the volume of investment increases. Sardoni argues that Marx never proposed such a relation.\n\nKeynes’ unemployment equilibrium is generally considered as a short-period situation in which the capital stock, money stock and money wages are assumed given; when these variables are free to vary in long-period conditions, unemployment can only result from imperfections in the market mechanism. Claudio Sardoni argues that Keynes intended his unemployment equilibrium to apply to the long period, interpreted as a centre of gravitation around which the system moves over time with no tendency to full employment."
  },
  {
    "qid": "econ-empirical-218-2-0-0",
    "question": "1) Derive the cointegrating vector for the imperfect substitutes model using the variables $m_{t}$, $p m_{t}$, $\\mathcal{p}d_{t}$, $y_{t}$, and $e x_{t}$. Explain the economic interpretation of the cointegrating relationship.",
    "gold_answer": "1. **Model Setup**: Start with the log-linear imperfect substitutes model: \\[ m_{t} = \\alpha_0 + \\alpha_1 p m_{t} + \\alpha_2 \\mathcal{p}d_{t} + \\alpha_3 y_{t} + \\alpha_4 e x_{t} + \\epsilon_{t} \\]\n2. **Cointegration**: Assume $m_{t}$, $p m_{t}$, $\\mathcal{p}d_{t}$, $y_{t}$, and $e x_{t}$ are I(1). The cointegrating vector $\\beta = (1, -\\alpha_1, -\\alpha_2, -\\alpha_3, -\\alpha_4)$ ensures \\[ \\beta' X_{t} = m_{t} - \\alpha_1 p m_{t} - \\alpha_2 \\mathcal{p}d_{t} - \\alpha_3 y_{t} - \\alpha_4 e x_{t} \\] is I(0).\n3. **Interpretation**: The cointegrating relationship represents long-run equilibrium import demand, where $\\alpha_1$ captures price elasticity, $\\alpha_3$ income elasticity, and $\\alpha_4$ exchange rate sensitivity.",
    "question_context": "Using quarterly data for the period 1960-1991 and using the basic imperfect substitutes model as theoretical background it was found that: (i) one long-run relation could be detected from the data, i.e., one cointegrating vector, (i) a complete SEM could be derived at the expense of using several shift dummies, (i) no significant price effects were detected in the long run, (iv) the conditional analysis showed some clear advantages over the full system based approach, and this mainly in relation to the uniqueness of the cointegrating relationship.\nThe variables we thus use in the analysis are: import volume denoted by $m_{t},$ import prices expressed in foreign currency denoted by $p m_{t}$ , domestic prices (wholesale price index) denoted by $\\mathcal{p}d_{t}$ , real income (GDP defated by pd) denoted by $y_{t},$ and effective exchange rate denoted by $e x_{t}$ . These variables are justified by the imperfect substitute model as well as by those derived by Gagnon (1988) and Husted and Kollintzas (1987).\n\nThis section models aggregate imports for the Belgium economy, extending the imperfect substitutes model from Urbain (1993). The study uses quarterly data (1960-1991) and identifies one cointegrating vector, a complete SEM with shift dummies, and no significant long-run price effects. The analysis is extended to account for quantum effects in Belgian imports, differentiating responses to large (exchange rate) and small (price) changes. Variables include import volume, import prices in foreign currency, domestic prices, real income, and effective exchange rate, all in natural logarithms."
  },
  {
    "qid": "econ-empirical-1506-1-0-3",
    "question": "4) Explain the policy implications of the ambiguity in $\\partial(\\dot{A}/A)/\\partial V_{\\mathrm{R}}$ in the basic model. How might this ambiguity affect the design of 'back to the land' policies?",
    "gold_answer": "1. **Policy Implications:**\n   - The ambiguity means that increasing rural income $V_{\\mathrm{R}}$ may not always reduce migration, as it depends on the relative strength of income and risk premium effects.\n   - Policymakers cannot rely solely on Todaro's conclusion that raising $V_{\\mathrm{R}}$ will slow migration.\n\n2. **Design of Policies:**\n   - Policies must account for the risk premium $\\pi$ and its sensitivity to $V_{\\mathrm{R}}$ ($\\pi_{\\mathrm{R}}$).\n   - Larger increases in $V_{\\mathrm{R}}$ may be needed to ensure $\\partial(\\dot{A}/A)/\\partial V_{\\mathrm{R}} < 0$.\n   - Complementary measures to reduce urban attractiveness ($V_{\\mathrm{U}}$) may be necessary.",
    "question_context": "A version of Todaro's migration function which lends itself to comparison with the basic model is: $\\dot{A}/\\dot{A}=g(\\alpha).^{4}\\dot{A}$ is the migration flow and .1 is the sum of migrants and non-migrants. $\\pmb{\\alpha}$ is an income differential term: $\\alpha=V_{\\mathrm{U}}/V_{\\mathrm{R}}-1,$ where $V_{\\mathbf{U}}$ and $V_{\\pmb{\\mathrm{R}}}$ are proxies of $Y_{\\mathrm{U}}$ and $Y_{\\mathbb{R}}$ in the basic model.\nDifferentiating Todaro's function with respect to $V_{R}$ and $\\gamma_{\\mathrm{{u}}},$ respectively, we obtain $\\partial(\\dot{A}/A)/\\partial V_{\\mathrm{R}}=-g^{\\prime}V_{\\mathrm{U}}/V_{\\mathrm{R}}^{2}.$ which is negative, since $g^{\\prime}>0$ and, $\\hat{d}(\\hat{A}/A)/\\hat{d}V_{\\mathrm{{u}}}{=}g^{\\prime}/V_{\\mathrm{{R}}}$ which is positive.\nIncorporation of these features in Todaro's model yields $\\dot{A}/A=g[(V_{\\mathrm{u}}-\\pi[V_{\\mathrm{R}},V_{\\mathrm{u}}]-V_{\\mathrm{R}}^{\\prime})/V_{\\mathrm{R}}],\\qquadg^{\\prime}>0,\\quad\\mathrm{only}\\quad\\pi_{\\mathrm{R}}<0,\\quad\\pi_{\\mathrm{U}}<0.$\nThe policy implications of 'back to the land' schemes using this adaptation of Todaro's migration function are revealed below, $\\partial(\\hat{A}/A)/\\partial V_{\\mathbb{R}}=g^{\\prime}(\\partial\\mathrm{E}(e)/\\partial V_{\\mathbb{R}}-(\\pi_{\\mathbb{R}}V_{\\mathbb{R}}-\\pi)/V_{\\mathbb{R}}^{2}).$\nGiven $\\partial\\mathbb{E}(e+g)/\\partial V_{\\mathbb{R}}<0,$ the effect of changing $V_{\\tt R}$ on $\\dot{A}/A$ is determined by $\\partial(\\boldsymbol{A}/\\boldsymbol{A})/\\partial V_{\\mathbb{R}}\\gtrless0\\quad\\mathrm{as}\\quad-(\\pi_{\\mathbb{R}}V_{\\mathbb{R}}-\\pi)/V_{\\mathbb{R}}^{2}\\gtrless\\left|\\hat{c}\\mathrm{E}(e+g)/\\partial V_{\\mathbb{R}}\\right|.$\n\nThe text contrasts the policy implications of the basic model with Todaro's partial equilibrium model, focusing on rural-urban migration dynamics and the effects of income differentials and risk premiums."
  },
  {
    "qid": "econ-empirical-492-3-1-1",
    "question": "4) Analyze the impact of minimal spending policies on the equilibrium threshold $a^{*}$ and the best response threshold $\\hat{a}$.",
    "gold_answer": "1. **Impact on $a^{*}$**: Minimal spending policies do not alter $a^{*}$ as it remains the best response to itself.\n2. **Impact on $\\hat{a}$**: The policy shifts $\\hat{a}$ below $a^{*}$ by changing agents' beliefs about others' strategies, inducing investment at lower thresholds.",
    "question_context": "A government following a minimal spending policy is committed to give an investment subsidy to each agent in the region between $a_{\\phi}^{*}$ and $\\hat{a}$ (the grey area in Figure 7). The subsidy $\\varphi(a,h)$ makes her indifferent between choosing High and Low given others will play according to $a_{\\phi}^{*}$.\n\nThis section discusses the implementation of minimal spending policies and their impact on equilibrium thresholds and agent behavior."
  },
  {
    "qid": "econ-empirical-685-0-0-2",
    "question": "3) Derive the mathematical expression for the price index $I(p_{s}^{\\circ},p_{\\hat{s}}^{\\circ},p_{s}^{\\prime},p_{\\hat{s}}^{\\prime})$ and explain how the determinateness test applies to it.",
    "gold_answer": "The price index is given by: \\[ I(p_{s}^{\\circ},p_{\\hat{s}}^{\\circ},p_{s}^{\\prime},p_{\\hat{s}}^{\\prime}) = \\frac{C(p_{s}^{\\prime}, p_{\\hat{s}}^{\\prime})}{C(p_{s}^{\\circ}, p_{\\hat{s}}^{\\circ})}, \\] where $C$ represents the cost function. The determinateness test requires that: \\[ \\lim_{\\|p_{\\hat{s}}^{i}\\|\\rightarrow 0} I(p_{s}^{\\circ},p_{\\hat{s}}^{\\circ},p_{s}^{\\prime},p_{\\hat{s}}^{\\prime}) = \\frac{C(p_{s}^{\\prime}, 0)}{C(p_{s}^{\\circ}, 0)} > 0 \\text{ and finite.} \\] This implies that the cost function must be well-behaved even when some prices are zero.",
    "question_context": "Let $S\\subset\\{1,2,\\ldots,n\\}$ $(n>1).$ $s$ non-empty and denote ${\\hat{S}}=\\{1,2,\\ldots,n\\}\\backslash S.$ Let a vector of goods be denoted by $x.$ $x\\in R^{n}$ and $\\boldsymbol{x}=(x_{S},x_{\\hat{S}})$ . The corresponding price vector is denoted by $p=(p_{s},p_{\\hat{s}})$ . Let $p^{\\circ}\\in R_{+}^{n}$ ， $p^{\\prime}\\in R_{+}^{n}$ and consider a price index $$ I(p_{s}^{\\circ},p_{\\hat{s}}^{\\circ},p_{s}^{\\prime},p_{\\hat{s}}^{\\prime}). $$ The determinateness test then asserts that as $\\|p_{\\hat{S}}^{i}\\|\\rightarrow0^{2}$ ， $i=^{\\circ}$ or ' or both $(p_{s}^{\\circ},p_{s}^{\\prime}$ strictly positive), $$ \\operatorname*{lim}_{\\|p_{\\hat{s}}^{i}\\|\\rightarrow0}I(p_{s}^{\\circ},p_{\\hat{s}}^{\\circ},p_{s}^{\\prime},p_{\\hat{s}}^{\\prime})>0\\quad\\mathrm{and~fnite}. $$\nFrisch [9] agrees with the determinateness test, while Samuelson and Swamy [10, p. 572] disagree, stating that the condition is odd and not desirable. The disagreement arises because Fisher's tests are contradictory, and one must be deleted.\nThis paper shows that the conditions on the parent production (utility) structure yielding an economic index that satisfies the determinateness test can be given a natural interpretation.\n\nThe determinateness test, introduced by Fisher, asserts that a price index should not become zero, infinite, or indeterminate if an individual price becomes zero. This paper explores the conditions under which an economic price index satisfies this test, within a production or utility framework."
  },
  {
    "qid": "econ-empirical-56-7-0-1",
    "question": "2) Prove that $\\mu_{X}(\\underline{{\\rho}})$ is a bijection on (0, 1) using the inverse function theorem.",
    "gold_answer": "1. Show that $\\mu_{X}^{\\prime}(\\underline{{\\rho}})>0$ for all $\\underline{{\\rho}}\\in(0,1)$.\n2. Apply the inverse function theorem to conclude that $\\mu_{X}(\\underline{{\\rho}})$ has a differentiable inverse $\\underline{{\\rho}}(\\mu_{X})$.\n3. Since $\\mu_{X}(\\underline{{\\rho}})$ is strictly increasing and continuous, it is a bijection on (0, 1).",
    "question_context": "$\\mu_{X}(\\underline{{\rho}})=\\frac{\\frac{\\exp{(1)}}{\\underline{{\rho}}}}{\\exp\\left(\\frac{1}{\\underline{{\rho}}}\right)}$\n$\\mu_{X}^{\\prime}(\\underline{{\rho}})=\\frac{(1-\\underline{{\rho}})\\mathrm{exp}(1)}{\\underline{{\rho}}^{3}\\mathrm{exp}\\Big(\\frac{1}{\\underline{{\rho}}}\\Big)}$\n$\\operatorname*{lim}_{\\underline{{\rho}}\\downarrow0}\\mu_{X}(\\underline{{\rho}})=\\operatorname*{lim}_{\\underline{{\rho}}\\downarrow0}\\left[\\frac{-\\mathrm{exp}(1)\\underline{{\rho}}^{-2}}{-\\mathrm{exp}\binom{\\underline{{1}}}{\\underline{{\rho}}}\\underline{{\rho}}^{-2}}\right]=\\operatorname*{lim}_{\\underline{{\rho}}\\downarrow0}\\left[\\frac{\\mathrm{exp}(1)}{\\mathrm{exp}\binom{\\underline{{1}}}{\\underline{{\rho}}}}\right]=0$\n\nThe proof involves working with the inverse of $\rho(\\mu_{X})$, denoted $\\mu_{X}(\\underline{{\rho}})$, which is the solution of equation (7) for $\\mu_{X}$."
  },
  {
    "qid": "econ-empirical-987-2-0-2",
    "question": "3) Discuss the challenges in evaluating the integral $\\int f_{\\eta,\\tau,v,u^{o}}(\\eta,\\tau,\\varepsilon + u^{o}\\gamma\\exp(\\tau^{\\prime}\\delta_{2}), u^{o}) du^{o}$ and the proposed numerical solution.",
    "gold_answer": "1. The integral involves the term $\\phi^{-1}[F_{u^{o}}(u^{o})]$, which lacks an explicit closed-form expression. \\n2. The complexity arises from the dependence of the integrand on $u^{o}$ through $\\gamma\\exp(\\tau^{\\prime}\\delta_{2})$. \\n3. Numerical integration is required, typically using methods like Monte Carlo simulation or quadrature. \\n4. The paper suggests drawing from the distribution of $u^{o}$ to approximate the integral. \\n5. This approach is computationally intensive but necessary due to the lack of analytical solutions.",
    "question_context": "We begin with the joint density of the random elements of the model, namely $f_{\\eta,\\tau,v,u^{o}}(\\eta,\\tau,v,u^{o})$. This is the density implied by the assumptions of Section 5.2, namely, marginal normality of $v$ and of the elements of $\\eta$ and $\\tau$, half-normality of $u^{o}$, and the normal copula.\nThen, we want to transform this set of variables to the set $\\eta,\\tau,\\varepsilon,u^{o}$ where $\\varepsilon=v-u^{o}\\exp\\left(q^{\\prime}\\delta\\right)=v-u^{o}\\gamma\\exp(\\tau^{\\prime}\\delta_{2})$ and where $\\gamma=\\exp\\left(q_{1}^{\\prime}\\delta_{1}+z^{\\prime}\\varPi_{q}\\delta_{2}\\right)$. The Jacobian of the transformation is unity.\nThe joint density of $\\eta,\\tau,\\varepsilon,u^{o}$ is $f_{\\eta,\\tau,\\varepsilon,u^{o}}\\left(\\eta,\\tau,\\varepsilon,u^{o}\\right)=f_{\\eta,\\tau,v,u^{o}}(\\eta,\\tau,\\varepsilon+u^{o}\\gamma\\exp(\\tau^{\\prime}\\delta_{2}),u^{o})$. Next, we integrate out $u^{o}$: $f_{\\eta,\\tau,\\varepsilon}\\left(\\eta,\\tau,\\varepsilon\\right)=\\int f_{\\eta,\\tau,\\varepsilon,u^{o}}\\left(\\eta,\\tau,\\varepsilon,u^{o}\\right)d u^{o}$.\nFinally, we substitute $\\eta=x_{2}-\\Pi_{x}^{\\prime}z$, $\\tau=q_{2}-\\Pi_{q}^{\\prime}z$, and $\\varepsilon=y-\\alpha-x^{\\prime}\\beta$ to obtain the joint density of $x_{2},q_{2}$, and $y$: $f_{x_{2},q_{2},y}\\left(x_{2},q_{2},y\\right)=f_{\\eta,\\tau,\\varepsilon}\\left(x_{2}-\\Pi_{x}^{\\prime}z,q_{2}-\\Pi_{q}^{\\prime}z,y-\\alpha-x^{\\prime}\\beta\\right)$.\nThe log-likelihood is given by $\\ln L=\\sum_{i=1}^{N}\\ln(f_{x_{2},q_{2},y}\\left(x_{2i},q_{2i},y_{i}\\right))$. The integral in the density function is evaluated numerically due to its complexity.\nThe simulations investigate the performance of MLEs under a base case scenario with $N=200$, specific distributions for instruments and errors, and parameter settings ensuring identification and estimability.\n\nThis section discusses maximum likelihood estimation (MLE) of the model and presents simulation results to evaluate the performance of the estimators proposed in the paper. The model involves joint density transformations, numerical integration, and simulation-based validation of estimator properties."
  },
  {
    "qid": "econ-empirical-1460-2-2-1",
    "question": "2) Formally analyze the need for fiscal rules in a monetary union. Use a government budget constraint.",
    "gold_answer": "1. **Budget Constraint**: \\( B_t = (1 + r) B_{t-1} + G_t - T_t \\), where \\( B \\) is debt, \\( G \\) is spending, and \\( T \\) is taxes.\n2. **Rules**: Fiscal rules (\\( G_t \\leq \\bar{G} \\)) prevent debt explosions (\\( B_t \\rightarrow \\infty \\)).",
    "question_context": "The issue of monetary control in an integrated Europe is investigated while the paper with Spaventa considers the need for fiscal rules in a monetary union.\n\nThis section discusses monetary union in Europe, including fixed exchange rate regimes, monetary control, and fiscal rules."
  },
  {
    "qid": "econ-empirical-484-2-0-0",
    "question": "1) Derive the welfare-equivalent income increase from eliminating global inequality, given the MLD of 0.73 and the stated proportional increase of 110%. Show the mathematical relationship between MLD and the welfare-equivalent income increase.",
    "gold_answer": "1. The Mean Log Deviation (MLD) is defined as: \\( MLD = \\frac{1}{N} \\sum_{i=1}^{N} \\ln\\left(\\frac{\\mu}{y_i}\\right) \\), where \\( \\mu \\) is the mean income and \\( y_i \\) is individual income.\n2. Eliminating inequality implies setting all \\( y_i = \\mu \\), reducing MLD to 0.\n3. The welfare-equivalent income increase is derived from the proportional increase required to match the welfare gain from eliminating inequality. Given MLD = 0.73, the welfare gain is equivalent to a proportional income increase of \\( e^{MLD} - 1 \\approx e^{0.73} - 1 \\approx 1.073 - 1 = 0.73 \\), or 73%. The text states 110%, suggesting additional adjustments or interpretations.",
    "question_context": "All these studies yield numbers closely in line with my finding that total global MLD of income is approximately 0.73, with 0.26 being within countries and 0.47 being across countries; that is, approximately 65% of inequality is across and 35% is within countries.\nEliminating all global inequality would add as much welfare as increasing the size of every individual in the economy’s income proportionally by 110%, or approximately 74 years of per capita growth at 1% per year. Eliminating inequality within countries, on the other hand, would translate into only a 30% increase in global incomes, or only 26 years of growth at 1% per year.\n\nThe text discusses the global distribution of income, focusing on the decomposition of inequality into within-country and between-country components. It presents empirical findings on global MLD (Mean Log Deviation) of income and compares the welfare implications of eliminating different types of inequality."
  },
  {
    "qid": "econ-empirical-1627-0-0-2",
    "question": "3) Explain the implications of using the same R draws for all markets versus different draws for each market on the consistency and asymptotic normality of the estimator.",
    "gold_answer": "Implications:\n\n- **Same R draws**: Requires T/R to be bounded for √T consistency, meaning more draws are needed per market as T grows.\n- **Different R draws**: Only requires √T/R to be bounded, which is a weaker condition. The bias term dominates if R does not grow faster than T, affecting the asymptotic distribution's centering.",
    "question_context": "The estimated parameters are √T consistent and asymptotically normal as long as R and N grow fast enough relative to T. Both approximations yield additional bias and variance terms in the asymptotic expansion.\nThe challenge of the asymptotic theory in these models is to deal with several approximation errors. First, in models with heterogeneous consumers, such as the model of Berry et al. (1995) (referred to as the BLP model), the market shares involve integrals over the distribution of random coefficients.\nThe limiting distribution of the estimated parameters can be obtained by either letting the number of products, the number of markets, or both approach infinity.\nI prove consistency and asymptotic normality for these cases where T approaches infinity and J_t ≤ J̄ where J̄ is fixed.\n\nThis paper develops asymptotic theory for differentiated product demand models with a large number of markets T, considering Monte Carlo integration with R draws and observed market shares approximated from a sample of N consumers. The estimated parameters are √T consistent and asymptotically normal under certain growth conditions for R and N relative to T."
  },
  {
    "qid": "econ-empirical-1548-0-0-0",
    "question": "1) Using the Engerman-Sokoloff framework, explain how soil quality in Denmark post-agrarian reforms could lead to increased land inequality, contrasting this with their original colonial context.",
    "gold_answer": "1) Engerman & Sokoloff (1997) posit that soil/climate endowments determine crop types, with plantation-suited soils causing inequality.\\n2) Denmark's homogeneous geography allowed isolation of soil quality effects without colonial distortions.\\n3) Post-reform, better soils (e.g., boulder clay) enabled productivity gains via equation: \\[ \\Delta Productivity = \\alpha(SoilQuality) + \\beta(Reform) \\]\\n4) This attracted population growth (Malthusian response), creating landless laborers as medium farms were preserved institutionally.",
    "question_context": "Pro-market and pro-farmer agrarian reforms enacted in eighteenth century Denmark laid the basis for rural development but we demonstrate that they also resulted in increased inequality.\nWe identify the impact of land quality on inequality following the reforms by instrumenting with soil type and find increases in areas with more productive land.\nAgrarian reforms need to balance equity in access to land with efficiency in the size of productive units that can take advantage of markets and technologies.\n\nThe text discusses the impact of 18th-century Danish agrarian reforms on rural development and inequality, using a novel parish-level database spanning over two centuries. It highlights the role of soil quality in determining inequality post-reforms and explores mechanisms linking productivity gains to population changes and emigration patterns."
  },
  {
    "qid": "econ-empirical-210-4-0-0",
    "question": "1) Derive the asymptotic expressions for $S_{z x}$ and $S_{z v}$ as presented in Lemma 3.1, and explain the role of the weighting function $d_i$ in these expressions.",
    "gold_answer": "1. **Asymptotic Expression for $S_{z x}$**: \n   - From Lemma 3.1, $S_{z x} = 2\\Sigma_{z x} + o_p(1)$. This follows from the arguments of Powell (1987), where $\\Sigma_{z x}$ is the covariance matrix between $z_i$ and $x_i$.\n   \n2. **Asymptotic Expression for $S_{z v}$**: \n   - The expression is: \n     $$\n     \\sqrt{n} \\cdot S_{z v} = \\frac{2}{\\sqrt{n}} \\sum_{i=1}^n d_i \\cdot \\rho_i \\cdot f_i \\cdot [z_i - \\mu_z(g_i)] \\cdot u_i + o_p(1).\n     $$\n   - Here, $d_i$ is a weighting function in $[0,1]$, $\\rho_i$ and $f_i$ are density-related terms, and $u_i$ is the error term.\n   \n3. **Role of $d_i$**: \n   - The weighting function $d_i$ generalizes the estimator by allowing for weighted contributions of observations. When $d_i \\equiv 1$, all observations contribute equally. Otherwise, $d_i$ downweights or excludes certain observations.",
    "question_context": "Proof of Lemma 3.1. Without loss of generality, we can take $g_{i}=\\operatorname{E}\\left[s_{i}\\mid w_{i}\\right]$ as the first component of the conditioning vector $w_{i}$ . (Since $g_{i}$ is presumed known for the construction of $S_{z x}$ and $S_{z v}$ ,conditioning on $w_{i}$ is equivalent to conditioning on $g_{i}$ and $w_{i}$ ) Then the arguments of Lemma 5.1 and Theorem 5.1 of Powell (1987) (with $w_{i}^{\\prime}\\hat{\\delta}=w_{i}^{\\prime}\\delta_{0}=g_{i})$ yield \n\n$$\n\\begin{array}{l}{{S_{z x}=2\\Sigma_{z x}+\\mathrm{o}_{p}(1),}}\\ {{\\displaystyle\\sqrt{n}\\cdot S_{z v}=\\frac{2}{\\sqrt{n}}\\sum_{i=1}^{n}d_{i}\\cdot\\rho_{i}\\cdot f_{i}\\cdot\\left[z_{i}-\\mu_{z}(g_{i})\\right]\\cdot u_{i}+\\mathrm{o}_{p}(1);}}\\end{array}\n$$ \n\nthese results apply directly for the special case $d_{\\mathrm{i}}\\equiv1$ , but are easily modified to allow for a more general weighting function $d_{i}\\in[0,1]$\nLemma B.1. Define the remainder terms \n\n$$\n\\begin{array}{r l}&{Q_{i j}\\equiv\\hat{\\omega}_{i j}-\\omega_{i j},}\\ &{R_{i j}\\equiv Q_{i j}-\\displaystyle\\left(\\frac{1}{h_{2}}\\right)^{2}k^{\\prime}\\left(\\frac{g_{i}-g_{j}}{h_{2}}\\right)\\cdot d_{i}\\cdot d_{j}\\cdot([\\hat{g}_{i}-g_{i}]-[\\hat{g}_{j}-g_{j}]),}\\end{array}\n$$ \n\nfor $\\hat{\\omega}_{i j}$ and $\\omega_{i j}$ defined in (2.16) and (3.4). Then under Assumptions 3.1 through 3.10 above, \n\n$$\n\\operatorname*{max}_{i,j}|Q_{i j}|=\\mathsf{o}_{p}(1)\\quad a n d\\quad\\operatorname*{max}_{i,j}|R_{i j}|=\\mathsf{o}_{p}(n^{-1/2}).\n$$ \n\nProof. Note that, by the usual Taylor's series expansions, \n\n$$\n\\begin{array}{l}{{\\displaystyle{\\cal Q}_{i j}=\\left(\\frac{1}{h_{2}}\\right)^{2}k^{\\prime}(A_{Q})\\cdot d_{i}\\cdot d_{j}\\cdot\\left[(\\hat{g}_{i}-g_{i})-(\\hat{g}_{j}-g_{j})\\right],}}\\ {{}}\\ {{\\displaystyle R_{i j}=\\left(\\frac{1}{h_{2}}\\right)^{3}k^{\\prime\\prime}(A_{R})\\cdot d_{i}\\cdot d_{j}\\cdot\\left[(\\hat{g}_{i}-g_{i})-(\\hat{g}_{j}-g_{j})\\right]^{2},}}\\end{array}\n$$ \n\nwhere $\\varDelta_{Q}$ and $\\varDelta_{R}$ are intermediate values between $(\\hat{g}_{i}-\\hat{g}_{j})$ and $(g_{i}-g_{j})$ Let $k_{1}\\equiv2k_{0}l_{0}$ , where $k_{0}$ and $l_{0}$ are given in Assumption 3.5; then $k_{1}$ is an upper bound for the first derivative $k^{\\prime}(\\cdot),$ since $k^{\\prime}(u)=0$ if $|\\boldsymbol{u}|>l_{0}$ , and by the mean value theorem (with $u^{*}$ denoting an intermediate value) $k^{\\prime}(u)=k^{\\prime}(u)-$ $k^{\\prime}(-l_{0})=k^{\\prime\\prime}(u^{*})\\cdot(u-l_{0})\\leq2k_{0}l_{0}$ if $|\\boldsymbol{u}|\\le l_{0}$ . Lemma A.1 above then implies \n\n$$\n\\operatorname*{max}_{i,j}|Q_{i j}|\\leq2k_{1}(h_{2})^{-2}\\left\\{\\operatorname*{sup}_{w\\in\\mathcal{W}}|\\hat{g}(w)-g(w)|\\right\\}\n$$ \n\n$$\n=\\mathrm{O}(n^{2\\delta})\\cdot\\mathrm{O}_{p}(n^{-(1/3+\\delta)})=\\mathrm{o}_{p}(1)\n$$ \n\n$$\n\\operatorname*{max}_{i,j}|R_{i j}|\\leq2k_{0}(h_{2})^{-3}\\left\\{\\operatorname*{sup}_{w\\in\\mathcal{W}}|\\hat{g}(w)-g(w)|\\right\\}^{2}\n$$ \n\nas asserted. \n\n$$\n\\begin{array}{l}{{=\\mathrm{O}(n^{3\\delta})\\cdot\\mathrm{O}_{p}(n^{-2/3-2\\delta})}}\\ {{}}\\ {{=\\mathrm{O}_{p}(n^{-2/3+\\delta})=\\mathrm{o}_{p}(n^{-1/2}),}}\\end{array}\n$$\nLemma B.2. Under Assumptions 3.1 through 3.10 above, \n\n$$\n\\sqrt{n}[\\hat{S}_{z v}-S_{z v}]=\\frac{2}{\\sqrt{n}}\\sum_{i=1}^{n}\\frac{r_{n}(\\xi_{i})}{b(w_{i})}\\cdot[\\hat{t}(w_{1})-\\hat{b}(w_{i})g(w_{i})]+\\mathrm{o}_{p}(1),\n$$ \n\nwhere \n\n$$\nr_{n}(\\xi_{i})\\equiv\\frac{1}{n-1}\\sum_{r=1}^{n}\\left(\\frac{1}{h_{2}}\\right)^{2}\\cdot k^{\\prime}\\left(\\frac{g_{i}-g_{j}}{h_{2}}\\right)\\cdot d_{i}\\cdot d_{j}\\cdot(z_{i}-z_{j})(v_{i}-v_{j}).\n$$ \n\nProof. First, by Lemma B.1 and the Strong Law for $U$ -statistics[Serfling (1980, theorem A, p. 190)], \n\n$$\n\\begin{array}{r l}&{\\begin{array}{r l}&{\\left\\|\\hat{S}_{z v}-S_{z v}-\\left(\\displaystyle{n_{j}}\\right)^{-1}\\displaystyle\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n}\\sum_{i}^{n}\\left(\\frac{1}{h_{2}}\\right)^{2}\\cdot k^{\\prime}\\left(\\frac{g_{i}-g_{j}}{h_{2}}\\right)\\cdot d_{i}\\cdot d_{j}}\\end{array}\\right.}\\ &{\\begin{array}{r l}&{\\cdot\\left(z_{i}-z_{j}\\right)\\left(v_{i}-v_{j}\\right)\\left[\\left(\\hat{g}_{i}-g_{i}\\right)-\\left(\\hat{g}_{j}-g_{j}\\right)\\right]\\right\\|}\\ &{\\leq\\displaystyle\\left\\{\\operatorname*{max}_{i,j}|R_{i j}|\\right\\}\\cdot\\left(\\displaystyle{n_{j}}\\right)^{-1}\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n}\\|z_{i}-z_{j}\\|\\cdot\\|v_{i}-v_{j}\\|=\\mathsf{o}_{p}\\left(n^{-1/2}\\right).}\\end{array}}\\end{array}\n$$ \n\nThus, the normalized difference between $\\hat{S}_{z v}$ and $\\boldsymbol{S}_{z v}$ is of the form \n\n$$\n\\begin{array}{c}{{\\displaystyle\\sqrt{n}(\\hat{S}_{z v}-S_{z v})=\\frac{1}{\\sqrt{n}}\\sum_{i=1}^{n}r_{n}(\\xi_{i})[\\hat{g}_{i}-g_{i}]+\\frac{1}{\\sqrt{n}}\\sum_{j=1}^{n}r_{n}(\\xi_{j})[\\hat{g}_{j}-g_{j}]+\\mathrm{o}_{p}(1)}}\\ {{\\mathrm{}}}\\ {{\\displaystyle=\\frac{2}{\\sqrt{n}}\\sum_{i=1}^{n}r_{n}(\\xi_{i})[\\hat{g}_{i}-g_{i}]+\\mathrm{o}_{p}(1),}}\\end{array}\n$$ \n\nwhere $r_{n}(\\xi_{i})$ is of the form given above. Since $r_{n}(\\xi_{i})=0$ $w_{i}\\notin\\mathcal{W}$ (which implies $d_{i}=0,$ , and since \n\n$$\n\\begin{array}{r l}{\\displaystyle\\frac{1}{n}\\sum_{i=1}^{n}||r_{n}(\\xi_{i})||\\leq k_{1}(h_{1})^{-2}\\cdot\\left[\\frac{1}{n(n-1)}\\sum_{i=1}^{n}\\sum_{j=1}^{n}||z_{i}-z_{j}||\\cdot||v_{i}-v_{j}||\\right]}&{}\\ {=\\mathbf{O}((h_{1})^{-2})\\cdot\\mathbf{O}_{p}(1)=\\mathbf{O}_{p}(n^{\\alpha})}\\end{array}\n$$ \n\n(where $\\alpha\\equiv2\\delta$ clearly satisfies the condition $\\alpha<\\frac{1}{6}+2\\delta)$ , the result follows from Lemma A.2 above.\nProof of Theorem 3.1. (i) To show $\\hat{S}_{z x}-S_{z x}=\\circ_{p}(1),$ note that \n\n$$\n\\|\\hat{S}_{z x}-S_{z x}\\|\\leq\\left[\\binom{n}{2}\\sum_{\\substack{i=1}}^{n-1}\\sum_{j=i+1}^{n-1}\\prod_{i}^{n}\\|z_{i}-z_{j}\\|\\cdot\\|x_{i}-x_{j}\\|\\right]\\cdot\\operatorname*{sup}_{i,j}|\\hat{\\omega}_{i j}-\\omega_{i j}|\n$$ \n\n$$\n{\\bf\\Pi}={\\bf O_{p}}(1)\\cdot{\\bf o_{{p}}}(1)={\\bf o_{{p}}}(1),\n$$ \n\nby Lemmas A.3 and B.1 above. \n\n(ii) Writing $\\xi_{i}=(d_{i},y_{i},w_{i}^{\\prime},x_{i}^{\\prime},x_{i}^{\\prime},z_{i}^{\\prime})^{\\prime}$ as before, define \n\n$$\nq_{n}(\\xi_{i},\\xi_{j},\\xi_{l})\\equiv\\frac{1}{b(w_{i})(h_{2})^{2}(h_{1})^{m}}k^{\\prime}\\left(\\frac{g_{i}-g_{j}}{h_{2}}\\right)\n$$ \n\n$$\n\\cdot K\\left(\\frac{w_{i}-w_{l}}{h_{1}}\\right)d_{i}d_{j}(z_{i}-z_{j})(v_{i}-v_{j})(s_{l}-g_{i})\n$$ \n\nand \n\n$$\nT_{n}\\equiv\\frac{1}{n^{2}(n-1)}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\sum_{l=1}^{n}1(i\\neq j)\\cdot q_{n}(\\xi_{i},\\xi_{j},\\xi_{l}).\n$$ \n\nThen $2T_{n}$ is the linearization given in the result of Lemma B.2, i.e. \n\n$$\n\\begin{array}{c}{{2{\\displaystyle T}_{n}=\\displaystyle\\frac{2}{n}\\sum_{i=1}^{n}\\frac{r_{n}(\\xi_{i})}{b(w_{i})}\\cdot[\\hat{t}(w_{i})-\\hat{b_{(}}w_{i})g(w_{i})]}}\\ {{{}}}\\ {{=\\hat{S}_{z v}-S_{z v}+{\\mathrm{o}}_{p}(n^{-1/2}).}}\\end{array}\n$$ \n\nNow by the same argument as in the proof of Theorem 5.1 of Powell (1987), the termswith $l=i$ in the triple summation defining $T_{n}$ are asymptotically negligible,since \n\n$$\n\\begin{array}{r l}&{\\displaystyle\\left\\|\\frac{1}{n^{2}(n-1)}\\sum_{i=1}^{n}\\sum_{j=1}^{n}1(i\\neq j)\\cdot q_{n}(\\xi_{n}(\\xi_{1},\\xi_{j},\\xi_{i})\\right\\|}\\ &{\\displaystyle\\leq\\frac{K(0)\\cdot k_{1}}{n(h_{1})^{m}(h_{2})^{2}}\\cdot\\left[\\binom{n}{2}^{-1}\\sum_{i=1}^{n-1}\\sum_{j=i+1}^{n}\\right]\\left\\|(z_{i}-z_{j})(v_{i}-v_{j})\\left(\\frac{d_{i}-g_{i}}{b(w_{i})}-\\frac{d_{j}-g_{i}}{b(w_{j})}\\right)\\right\\|\\right]}\\ &{\\displaystyle={\\tt o}(n^{1-m\\gamma-2\\delta})\\cdot{\\tt O}_{p}(1)={\\tt O}(n^{-2/3})\\cdot{\\tt O}_{p}(1)={\\tt o}_{p}(n^{-1/2}),}\\end{array}\n$$ \n\nwhere $\\delta$ and $\\gamma<(\\textstyle{\\frac{1}{3}}-2\\delta)/m$ are given in Assumptions 3.6 and 3.10. A similar argument can be used to show that the terms with $l=j$ are asymptotically negligible, i.e., \n\n$$\n\\left|\\left|{\\frac{1}{n^{2}(n-1)}}\\sum_{i=1}^{n}\\sum_{j=1}^{n}1(i\\neq j)\\cdot q_{n}(\\xi_{i},\\xi_{j},\\xi_{j})\\right|\\right|=\\mathsf{o}_{p}(n^{-1/2}).\n$$ \n\nThus, $T_{n}$ satisfies \n\n$$\nT_{n}=U_{n}+{\\cal{o}}_{p}(n^{-1/2}),\n$$ \n\nwhere \n\n$$\nU_{n}\\equiv\\frac{1}{n(n-1)(n-2)}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\sum_{l=1}^{n}1(i\\neq j\\neq l)\\cdot q_{n}(\\xi_{i},\\xi_{j},\\xi_{l}).\n$$ \n\nThis can be written as a third-order $U$ -statistic, with symmetrized kernel $p_{n}(\\cdot)$ of the form \n\n$$\n\\begin{array}{c}{{p_{n}(\\xi_{i},\\xi_{j},\\xi_{l})=\\frac{1}{6}(q_{n}(\\xi_{i},\\xi_{j},\\xi_{l})+q_{n}(\\xi_{i},\\xi_{l},\\xi_{j})+q_{n}(\\xi_{j},\\xi_{i},\\xi_{l})}}\\ {{{}}}\\ {{+q_{n}(\\xi_{l},\\xi_{i},\\xi_{j})+q_{n}(\\xi_{j},\\xi_{l},\\xi_{i})+q_{n}(\\xi_{l},\\xi_{j},\\xi_{i})).}}\\end{array}\n$$ \n\nTo derive the asymptotic behavior of $\\hat{S}_{z v}-S_{z v}$ , then, Lemma A.3 will be applied to this third-order $U$ -statistic. First, it is straightforward to verify that \n\n$$\n\\operatorname{E}[\\|p_{n}(\\xi_{i},\\xi_{j},\\xi_{l})\\|^{2}]={\\frac{1}{(h_{1})^{2m}(h_{2})^{4}}}\\cdot\\operatorname{O}(1)=\\operatorname{O}(n^{2/3+2\\delta})=\\operatorname{O}(n),\n$$ \n\nso that Lemma A.3 implies that \n\n$$\n\\hat{S}_{z v}-S_{z v}=2U_{n}+\\mathsf{o}_{p}(n^{-1/2})=2\\theta_{n}+\\frac{6}{n}\\sum_{i=1}^{n}\\left[r_{n}(\\xi_{i})-\\theta_{n}\\right]+\\mathsf{o}_{p}(n^{-1/2}),\n$$ \n\nfor \n\n$$\nr_{n}(\\xi_{i})\\equiv\\operatorname{E}[p_{n}(\\xi_{i},\\xi_{j},\\xi_{l})\\mid\\xi_{i}],\\qquad\\theta_{n}\\equiv\\operatorname{E}[r_{n}(\\xi_{i})].\n$$ \n\nNext, we find the form of the conditional expectation $r_{n}(\\xi_{i})$ of thekernel $p_{n}\\big(\\xi_{i},\\xi_{j},\\xi_{l}\\big).$ which requires calculation of the conditional expectations of its six components. By an analogous argument to the one given in eq. (A.21) through (A.28) of Powell (1987), it can be shown that \n\n$$\n\\begin{array}{r}{\\mathbb{E}\\left[\\left\\|\\mathbb{E}\\left[q_{n}(\\xi_{i},\\xi_{j},\\\\xi_{l})\\vert\\xi_{i},w_{l}\\right]-\\tau_{1}(\\xi_{i},w_{l})\\right\\|\\right]=\\mathbf{O}((h_{2})^{4})=\\mathbf{o}(n^{-1/2}),}\\end{array}\n$$ \n\nwhere \n\n$$\n\\tau_{1}(\\xi_{i},w_{l})\\equiv d_{i}\\cdot\\rho_{i}\\cdot f_{i}\\cdot\\lambda^{\\prime}(g_{i})[z_{i}-\\mu_{z}(g_{i})]\\cdot\\left(\\frac{1}{h_{1}}\\right)^{m}\\cdot K\\left(\\frac{w_{i}-w_{l}}{h_{1}}\\right)\\cdot(g_{l}-g_{i}).\n$$ \n\nMoreover, by the argument of Lemma 13 of Ahn and Manski (1989), \n\n$$\n\\operatorname{E}\\left\\|\\operatorname{E}\\left[\\left({\\frac{1}{h_{1}}}\\right)^{m}\\cdot K\\left({\\frac{w_{i}-w_{i}}{h_{1}}}\\right)\\cdot(g_{l}-g_{i})\\right|w_{i}\\right]\\right\\|=\\operatorname{O}((h_{1})^{M})=\\operatorname{o}(n^{-1/2})\n$$ \n\nunder Assumptions 3.1 and 3.8 through 3.11. Thus \n\n$$\n\\begin{array}{r}{\\mathsf{E}[\\|\\mathsf{E}[q_{n}(\\xi_{i},\\xi_{j},\\xi_{l})|\\xi_{i}]\\|]=\\mathsf{o}(n^{-1/2}).}\\end{array}\n$$ \n\nIn the same way, we can show that the next three components of $p_{n}\\big(\\xi_{i},\\xi_{j},\\xi_{l}\\big)$ are asymptotically negligible: \n\n$$\n\\begin{array}{r l}&{{\\mathbb E}[\\|{\\mathbb E}[q_{n}(\\xi_{i},\\xi_{l},\\xi_{j})|\\xi_{i}]=o(n^{-1/2}),}\\ &{{\\mathbb E}[\\|{\\mathbb E}(q_{n}(\\xi_{j},\\xi_{i},\\xi_{l})|\\xi_{i}]\\|]=\\mathsf{o}(n^{-1/2}),}\\ &{{\\mathbb E}[\\|{\\mathbb E}[q_{n}(\\xi_{l},\\xi_{i},\\xi_{j})|\\xi_{i}]\\|]=\\mathsf{o}(n^{-1/2}).}\\end{array}\n$$ \n\nHowever, this is not true for the remaining two terms; the calculations for the fifth and sixth term yield \n\n$$\n\\begin{array}{r}{\\mathbb{E}[\\|\\mathbb{E}[q_{n}(\\xi_{j},\\xi_{l},\\xi_{i})|\\xi_{i},w_{j}]-\\tau_{2}(\\xi_{i},w_{j})\\|]=\\mathsf{O}((h_{2})^{4})=\\mathsf{o}(n^{-1/2})}\\end{array}\n$$ \n\nand \n\n$$\n\\begin{array}{r}{\\mathbb{E}[\\|\\mathbb{E}[q_{n}(\\xi_{l},\\xi_{j},\\xi_{i})|\\xi_{i},w_{l}]-\\tau_{2}(\\xi_{i},w_{l})\\|]=\\mathbf{O}((h_{2})^{4})=\\mathsf{o}(n^{-1/2}),}\\end{array}\n$$ \n\nwhere \n\n$$\n\\tau_{2}(\\xi_{i},w_{l})\\equiv\\rho_{l}^{2}\\cdot f_{l}\\cdot\\lambda^{\\prime}(g_{l})[z_{l}-\\mu_{z}(g_{l})]\\cdot\\left(\\frac{1}{h_{1}}\\right)^{m}\\cdot K\\left(\\frac{w_{1}-w_{i}}{h_{1}}\\right)\\cdot(s_{i}-g_{i}).\n$$ \n\nAgain using the argument of Lemma 13 of Ahn and Manski (1989), it can be shownthat \n\n$$\n\\begin{array}{r}{{\\cal E}[\\|{\\cal E}[\\tau_{2}(\\xi_{i},w_{l})|\\xi_{i}]-\\tau_{3}(\\xi_{i})\\|]={\\cal O}((h_{1})^{M})=\\mathrm{o}(n^{-1/2}),}\\end{array}\n$$ \n\nwhere \n\n$$\n\\tau_{3}(\\xi_{i})\\equiv\\rho_{i}^{2}\\cdot f_{i}\\cdot\\lambda^{\\prime}(g_{i})\\cdot[z_{i}-\\mu_{z}(g_{i})]\\cdot[s_{i}-g_{i}].\n$$ \n\nJ.Econ \n\nHence we derive that $r_{n}(\\xi_{i})\\equiv\\mathbb{E}[p_{n}(\\xi_{l},\\xi_{j},\\xi_{i})|\\xi_{i}]$ satisfies \n\nand, since $\\mathbf{E}\\left[\\tau_{3}({\\boldsymbol{\\xi}}_{i})\\right]=0$ \n\n$$\n\\begin{array}{r}{\\theta_{n}=\\operatorname{E}[r_{n}(\\xi_{i})]=\\circ(n^{-1/2}).}\\end{array}\n$$ \n\nThus, \n\n$$\n\\begin{array}{l}{{\\displaystyle\\hat{S}_{z v}-S_{z v}=2\\theta_{n}+\\frac{6}{n}\\sum_{i=1}^{n}\\left[r_{n}(\\xi_{i})-\\theta_{n}\\right]+\\mathrm{o}_{p}(n^{-1/2})}}\\ {{\\displaystyle\\qquad=\\frac{2}{n}\\sum_{i=1}^{n}\\tau_{3}(\\xi_{i})+\\mathrm{o}_{p}(n^{-1/2})}}\\ {{\\displaystyle\\qquad=\\frac{2}{n}\\sum_{i=1}^{n}f_{i}\\cdot\\rho_{i}^{2}\\cdot\\lambda^{\\prime}(g_{i})\\cdot\\left[2_{i}-\\mu_{z}(g_{i})\\right]\\cdot\\left[d_{i}-g_{i}\\right]+\\mathrm{o}_{p}(n^{-1/2})},}\\end{array}\n$$ \n\nas asserted.\n\nThis section provides detailed proofs for Lemma 3.1, Lemma B.1, Lemma B.2, and Theorem 3.1, focusing on the asymptotic behavior of estimators and remainder terms in a nonparametric econometric framework."
  },
  {
    "qid": "econ-empirical-1791-1-0-4",
    "question": "5) Derive the conditions under which a component $E(x)$ is a recurrent set, as stated in Lemma 6.2.",
    "gold_answer": "To derive the conditions for $E(x)$ being a recurrent set:\n1. From Lemma 6.2, $E(x)$ is a recurrent set if and only if $r(x) = \\{x\\}$.\n2. This means no other component $E(\\tilde{x})$ can be reached from $E(x)$ via perturbations and dynamics.\n3. If $r(x) = \\{x\\}$, then $E(x)$ is closed under perturbations and dynamics, satisfying the definition of a recurrent set.\n4. Conversely, if $E(x)$ is a recurrent set, it cannot reach any other component, so $r(x) = \\{x\\}$.",
    "question_context": "After transactions at time $t$ are completed, the buyer's belief $\\varPhi^{t}$ is adjusted to match the buyer's experience. For any state $\\theta=\\left(\\varPhi,x_{l},x_{h}\\right)$ , we let $\\nu(x_{l})$ and $\\nu(x_{h})$ (where we suppress the dependence of $\\nu(\\cdot)$ on $\\theta$ ) be given by \n\n$$\n\\begin{array}{r l r l}&{\\nu(x_{l})=0,\\quad}&{\\nu(x_{h})=1}&{\\quad\\mathrm{if}\\quad x_{l}\\neq x_{h}}\\ &{\\nu(x_{l})=\\phi^{0}=\\nu(x_{h})}&{\\quad\\mathrm{if}\\quad x_{l}=x_{h}.}\\end{array}\n$$\nThen define state $s(\\theta)=(\\varPhi^{\\prime},x_{l}^{\\prime},x_{h}^{\\prime})$ , the successor of state $\\theta$ , as \n\n$$\n\\begin{array}{r l r l}&{\\phi^{\\prime}(x)=\\phi(x)}&&{\\mathrm{if}\\quad x\\notin\\left\\{x_{l},x_{h}\\right\\}}\\ &{\\phi^{\\prime}(x)=\\nu(x)}&&{\\mathrm{if}\\quad x\\in\\left\\{x_{l},x_{h}\\right\\}.}\\ &{\\quad x_{q}^{\\prime}=\\arg\\operatorname*{max}u_{q}(x,\\varPhi^{\\prime}(x)),}&&{q\\in\\left\\{l,h\\right\\}.}\\end{array}\n$$\nA deterministic dynamic process is then obtained by specifying that each state is followed by its successor. More formally, letting $\\theta^{t}$ be the state at time $t.$ , the dynamics are specified by an initial state $\\theta^{0}$ and the adjustment rule that for all $t>0$ , $\\theta^{t}=s(\\theta^{t-1})$ . We refer to this deterministic process as the Spencian dynamic.\nFor any state $\\theta\\in\\theta$ let \n\n$$\nS(\\theta)=\\{\\theta^{\\prime}\\mid\\exists T\\geqslant0\\colon\\theta^{\\prime}=s^{T}(\\theta)\\},\n$$ \n\nwhere $s^{0}(\\theta)=\\theta$ and $s^{T}(\\theta)=s(s^{T-1}(\\theta))$ . Then $S(\\theta)$ is the set of states which arise if the Spencian dynamic is started from the initial state $\\theta.$ A set $C\\subset\\theta$ is closed under the Spencian dynamic if $\\forall\\theta\\in C$ : $S(\\theta)\\subset C.$ . A non-singleton set $C$ is a cycle if $\\forall\\theta\\in C\\colon S(\\theta)=C.$ A state $\\theta\\in\\theta$ is a stationary state of the Spencian dynamic if $S(\\theta)=\\left\\{s(\\theta)\\right\\}=\\left\\{\\theta\\right\\}$ . Since the state space of the Spencian dynamic is finite, the dynamic must lead, in finite time, either to a stationary state or a finite cycle.\nLemma 1. $A$ state $(\\varPhi^{*}(x),x_{l}^{*},x_{h}^{*})$ is a stationary state of the Spencian dynamic if and only if \n\n$$\n\\begin{array}{r}{\\phi(x_{q}^{*})=\\nu(x_{q}^{*})\\qquad q\\in\\{l,h\\}.}\\end{array}\n$$\nLemma 2. [2.1] Every cycle of the Spencian dynamic contains two states. For every such cycle there exists $a$ signal $x^{c}$ such that one of these states, say $\\theta=\\left(\\varPhi,x_{l},x_{h}\\right)$ , satisfies $x_{l}=x_{h}=x^{c}$ and $\\varPhi(x^{c})=1$ whereas the other state, $\\theta^{\\prime}=\\left(\\varPhi^{\\prime},x_{l}^{\\prime},x_{h}^{\\prime}\\right)$ , satisfies $x_{h}^{\\prime}=x^{c}$ , $x_{l}^{\\prime}=\\underline{{x}}$ , and $\\phi^{\\prime}(x^{c})=\\phi^{0}$ .\nLemma 3. [3.1] If there is a unique recurrent set $R$ then there is a unique limit distribution $\\zeta^{*}$ and the support of $\\zeta^{*}$ is $R$ .\nLemma 4. $A$ recurrent set exists. Every recurrent set $R$ is the union of equilibria and cycles of the Spencian dynamic. Recurrent sets are disjoint.\nLemma 5. Let $R$ be a recurrent set. For all $x$ , either $E(x)\\subset R$ or $E(x)\\cap R=\\emptyset$ .\nLemma 6. [6.1] Let $x^{\\prime}\\in r(x)$ . If $E(x)$ is contained in a recurrent set then $E(x^{\\prime})$ is contained in the same recurrent set.\n\nThis section discusses the dynamics of buyer beliefs and seller signals, the concept of perturbations in the Spencian dynamic, and the identification of recurrent sets for equilibrium selection."
  },
  {
    "qid": "econ-empirical-1073-0-1-1",
    "question": "2) Derive a sampling framework to address the limitations of the NORC study's weighting procedure. How would you improve representativeness?",
    "gold_answer": "1. Let \\( N \\) = total population, \\( n \\) = sample size, \\( w_i \\) = weight for stratum \\( i \\).  \n2. Problem: \\( \\sum w_i x_i \\) overrepresents small subgroups.  \n3. Solution: Use stratified sampling with proportional allocation:  \n   \\( n_i = n \\times \\frac{N_i}{N} \\).  \n4. This ensures each subgroup is represented proportionally without extreme weights.",
    "question_context": "The task of the panel was a narrow and specific one: to identify needs for basic data on which future decisions could be based, and to recommend techniques for collecting such data, by whichever means appeared most appropriate--special studies, the institution of periodic surveys, the expansion of existing data collection instruments.\nThe truly distinctive feature of these recommendations was their high probability of implementation: the National Science Foundation was both willing and able to follow through by awarding grants, contracts, pilot projects to a variety of organizations, both within and outside the federal government, to initiate collection of data to which the panel had assigned high priority.\nWhile too much cooperation is never an unmixed blessing--in the present case, the questionnaire and the analysis both show the usual 'kitchen-sink' symptoms which are the price of involving many agencies, each with its specialized interests--the joint effort between the three principals certainly represents a productive and pioneering example of collaboration between researchers in and out of government.\n\nThe text highlights the collaborative efforts between government and private research groups to address manpower utilization, focusing on the challenges of data collection and sampling."
  },
  {
    "qid": "econ-empirical-1528-0-0-0",
    "question": "1) Derive the conditions under which explosive rational bubbles can exist in stock prices, starting from the fundamental pricing equation.",
    "gold_answer": "1. Start with the fundamental pricing equation: \\( P_t = \\frac{E_t[D_{t+1} + P_{t+1}]}{1 + r} \\).\n2. Assume a bubble component \\( B_t \\) such that \\( P_t = P_t^f + B_t \\), where \\( P_t^f \\) is the fundamental price.\n3. Impose the condition that the bubble grows at a rate \\( (1 + r) \\): \\( B_{t+1} = (1 + r)B_t \\).\n4. Show that this leads to an explosive path for \\( B_t \\) if \\( B_t \\) is positive and grows without bound.",
    "question_context": "Explosive Rational Bubbles in Stock Prices? by Behzad T. Diba and Herschel I. Grossman, published in The American Economic Review, 1988.\n\nThe paper investigates the existence of explosive rational bubbles in stock prices, providing a theoretical framework to analyze their behavior and implications."
  },
  {
    "qid": "econ-empirical-1075-2-0-2",
    "question": "3) Analyze the implications of the wide range of adjustment factors across different datasets, particularly focusing on the ORG 7-12 dataset, which yields the largest adjustments.",
    "gold_answer": "1. The wide range suggests variability in how the new measure was administered or interpreted across datasets.\n2. The ORG 7-12 dataset's larger adjustments may reflect a \"shakedown effect,\" where interviewers became more accurate over time.\n3. This variability underscores the importance of dataset selection when computing adjustment factors, as different datasets can lead to materially different conclusions.",
    "question_context": "For each measure-—- Old 13A and new—- we compute the geometric mean of earnings for each education level. To adjust earnings under the Old 13A measure to be compatible with earnings under the new measure, we multiply by the ratio of the (geometric) mean under the new measure to the mean under the old measure.\nWe estimated adjustment factors separately for men and women, but found that none of the adjustment factors for men and women were significantly different from each other at the 5 percent level.\nInspection of Table 2 reveals that there is a wide range of potential adjustment factors suggested by the different data sources. An adjustment factor of zero indicates no adjustment.\nThe ORG 7-12, in most cases, yields the largest adjustments and contrasts sharply with the other datasets. For example, the adjustment factors for High School Dropouts using ORG 7-12 data are $-1.3$ percent using 1992 earnings and $-0.9$ percent using 1991 earnings.\nThe positive adjustment of college graduates’ earnings is sensible given that a goal of the revised question was to more successfully identify college graduates.\n\nThe text discusses the computation of adjustment factors for earnings data under old and new measures, focusing on geometric means and their ratios. It examines different datasets and their implications for earnings adjustments across education levels."
  },
  {
    "qid": "econ-empirical-69-1-1-1",
    "question": "4) Suppose post-Act 10, districts choose pay schemes \\( s \\in \\{\\text{schedule}, \\text{nonschedule}\\} \\). Derive a theoretical model where districts maximize teacher quality \\( Q \\) subject to budget \\( B \\), and show how \\( s \\) affects the gender gap.",
    "gold_answer": "1. District problem: \\( \\max_s Q(s) \\) s.t. \\( \\sum_i P_i(s) \\leq B \\).  \n2. Assume:  \n   - Schedule: \\( P_i \\) depends only on \\( (i, j) \\).  \n   - Nonschedule: \\( P_i \\) depends on \\( (i, j, \\text{performance}) \\).  \n3. If performance evaluations are biased (e.g., against women), nonschedule districts amplify the gap. Formalize bias as \\( \\text{performance}_i = \\theta_i + \\eta \\cdot \\text{Female}_i \\), where \\( \\eta < 0 \\).",
    "question_context": "Act 10 limits the scope of collective bargaining: Before this law change, unions could negotiate the entire salary schedule, and after Act 10, negotiations were limited to base salaries.\nDistricts could begin to use their freedom to flexibly set teacher pay only after the expiration of pre–Act 10 CBAs.\n\nAct 10 (2011) ended collective bargaining for Wisconsin public school teachers, allowing districts to set pay flexibly. Pre-existing CBAs remained binding until expiration, creating variation in the timing of flexible pay introduction."
  },
  {
    "qid": "econ-empirical-1012-1-0-1",
    "question": "2) Explain the implications of Assumption 3 on the latent process $\\eta_{t}$ and its role in the test statistic.",
    "gold_answer": "1. **Implications of Assumption 3**:\n   - $\\eta_{t}$ is a function of a latent Markov process $\\vartheta_{t}$ with geometric $\\beta$-mixing.\n   - Ensures stationarity and ergodicity of $\\eta_{t}$.\n   - Bounds on $\\|\\eta_{t}\\|$ and continuity of $E(\\eta_{t}\\eta_{t-k})$ in $\\beta$ ensure regularity.\n2. **Role in Test Statistic**:\n   - The autocorrelation structure of $\\eta_{t}$ is captured in $\\mu_{2,t}(\\beta,\\boldsymbol{\\theta})$ via terms like $E^{\\beta}(\\eta_{t}\\eta_{s}^{\\prime})$.\n   - Geometric mixing ensures the asymptotic validity of the test statistic.",
    "question_context": "The observations are denoted by $y_{1},y_{2},\\dotsc,y_{T}$ (univariate or multivariate). Let $f_{t}(\\cdot)$ denote the conditional density (with respect to some dominating measure) of $y_{t}$ given $y_{t-1},\\ldots,y_{1}.f_{t}(\\cdot)$ is known up to a $p$ -dimensional vector of parameters, say $\\theta_{t}$ . We are interested in testing whether these parameters are constant over time. Namely, we test $H_{0}\\colon\\theta_{t}=\\theta_{0},\\quad\\mathrm{for~some~unspecified~}\\theta_{0}$ against $H_{1}:\\theta_{t}=\\theta_{0}+\\eta_{t}$, where the switching variable $\\eta_{t}$ is not observable.\nASSUMPTION 1: The latent variable $\\eta_{t}$ is stationary and its distribution may depend on some unknown parameters $\\beta$ . They are nuisance parameters that are not identified under $H_{0}$ . We assume that $\\beta$ belongs to a compact set $B$ Moreover, $\\eta_{t}$ is strongly exogenous in the sense that the joint likelihood of $(y_{1},y_{2},...,y_{T},\\eta_{1},\\eta_{2},...,\\eta_{T})$ factorizes as $\\begin{array}{r}{\\prod_{t=1}^{T}f(y_{t}|\\theta_{t},y_{t-1},y_{t-2},\\ldots,y_{1})q(\\eta_{t}|}\\end{array}$ $\\eta_{t-1},\\eta_{t-2},\\dots,\\eta_{1};\\beta)$ and the values of $\\theta_{t}$ belong to some compact subset of $\\mathbb{R}^{p}$ ， $\\boldsymbol{\\Theta}$ , containing $\\theta_{0}$.\nASSUMPTION 2: $y_{t}$ is stationary under $H_{0}$ and the following conditions on the conditional log-density of $y_{t}$ given $y_{t-1},\\ldots,y_{1}$ (under $H_{0}$ ， $l_{t}$ . are satisfied: $l_{t}=$ $l_{t}(\\theta)$ , as a function of the parameter $\\theta$ , is at least five times differentiable, and for $k=1,\\hdots,5$ ， $E_{\\theta_{0}}\\operatorname*{sup}_{\\theta\\in\\mathcal{N}}\\big(\\big\\|l_{t}^{(k)}(\\theta)\\big\\|^{20}\\big)<\\infty$, where ${l}_{t}^{(k)}$ denotes the kth derivative of the log-likelihood with respect to the parameter $\\theta$ and $\\mathcal{N}$ is a neighborhood around $\\theta_{0}$ $E_{\\theta_{0}}$ is the expectation with respect to the probability measure corresponding to the parameter $\\theta_{0}$ and $\\|\\cdot\\|$ denotes the Frobenius norm. Moreover, $\\theta_{0}$ is an interior point of $\\boldsymbol{\\Theta}$ and the information matrix $I(\\theta_{0})=E_{\\theta_{0}}(l_{t}^{(1)}(\\theta_{0})l_{t}^{(1)^{\\prime}}(\\theta_{0}))$ is nonsingular.\nASSUMPTION 3: $\\eta_{t}$ is a function $\\pmb{\\kappa}$ of $a$ latent Markovprocess $\\vartheta_{t}$ . We assume that $\\vartheta_{t}$ is stationary and $\\beta$ -mixing with geometric decay. $I t$ implies in particular that there exist $0<\\lambda<1$ and a measurable nonnegative function $g$ such that $\\operatorname*{sup}_{|h|\\leq1}\\Bigl|E\\bigl[h(\\vartheta_{t+m})|\\vartheta_{t}\\bigr]-E\\bigl[h(\\vartheta_{t})\\bigr]\\Bigr|\\leq\\lambda^{m}g(\\vartheta_{t})$, and $E g(\\vartheta_{t})<\\infty$ .Furthermore, we assume that $E\\eta_{t}=0$ and $\\mathbf{max}_{t}\\parallel\\pmb{\\eta}_{t}\\parallel\\le$ $M<\\infty$ . Moreover, the constant $\\lambda$ and the bound $M$ are independent of $\\beta$ defined in Assumption 1 and $E\\|g\\|$ can be bounded by a constant independent of $\\beta$ Finally, $E(\\eta_{t}\\eta_{t-k})$ , for any integer $k$ , is assumed to be continuous in $\\beta$.\nThe test statistic $\\mathbf{TS}_{T}$ , for a given $\\beta$ , is of the form $\\mathbf{TS}_{T}(\\beta)=\\mathbf{TS}_{T}(\\beta,\\hat{\\theta})={\\cal T}_{T}-\\frac{1}{2T}\\hat{\\pmb\\varepsilon}(\\beta)^{\\prime}\\hat{\\pmb\\varepsilon}(\\beta)$, where $\\begin{array}{r}{T_{T}=\\frac{1}{\\sqrt{T}}\\sum_{t}\\mu_{2,t}(\\beta,\\hat{\\boldsymbol{\\theta}})}\\end{array}$ with $\\begin{array}{r l r}&{}&{\\mu_{2,t}(\\beta,\\boldsymbol{\\theta})=\\cfrac{1}{2}\\bigg(\\mathrm{tr}\\big(\\big(l_{t}^{(2)}+l_{t}^{(1)}l_{t}^{(1)\\prime}\\big)E^{\\beta}\\big(\\eta_{t}\\eta_{t}^{\\prime}\\big)\\big)}\\ &{}&{+2\\sum_{s<t}\\mathrm{tr}\\big(l_{t}^{(1)}l_{s}^{(1)\\prime}E^{\\beta}\\big(\\eta_{t}\\eta_{s}^{\\prime}\\big)\\big)\\bigg),}\\end{array}$ where tr denotes the trace and $\\hat{\\pmb{\\varepsilon}}(\\beta)$ is the residual from the OLS regression of $\\mu_{2,t}(\\beta,\\hat{{\\boldsymbol{\\theta}}})$ on $l_{t}^{(1)}(\\hat{\\boldsymbol{\\theta}})$ , and $\\hat{\\theta}$ is the constrained maximum likelihood estimator of $\\theta$ under $H_{0}$ (i.e., the ML estimator under the assumption of constant parameters).\n\nThe section outlines the assumptions and test statistics for testing parameter constancy over time in a model with conditional densities and latent variables."
  },
  {
    "qid": "econ-empirical-1138-0-0-3",
    "question": "4) Analyze the policy implications of the findings for designing welfare programs targeting injection drug users. How might policymakers mitigate the 'check effect'?",
    "gold_answer": "1. **Policy Options**:\n   - Staggered check disbursement to reduce lumpy consumption.\n   - Mandatory treatment programs tied to welfare receipt.\n   - Representative payees to manage funds for high-risk individuals.\n2. **Evidence-Based**: The paper suggests lumpy consumption increases harm (e.g., overdoses), justifying interventions to smooth consumption.",
    "question_context": "We find an increase in the likelihood of an overdose in the days following check arrival, and in the probability of leaving the hospital against medical advice (AMA) on check day.\nUsing the check arrival date as an instrument, we estimate the Local Average Treatment Effect of leaving AMA on subsequent readmission and the probability of a drug overdose.\nThe results indicate that, for individuals influenced by check day, leaving AMA leads to readmission much sooner than planned discharge, longer subsequent stays in the hospital, and a substantial increase in the probability of a drug overdose.\n\nThis paper investigates the link between welfare payments and drug use among injection drug users in Vancouver, focusing on the effects of welfare check arrivals on drug overdoses and hospital admissions."
  },
  {
    "qid": "econ-empirical-313-4-0-3",
    "question": "4) Discuss the role of the m-out-of-n bootstrap in providing robust inference for the treatment effect under irregular identification, as proposed in the paper.",
    "gold_answer": "The m-out-of-n bootstrap addresses irregular identification by:\n\n1. **Resampling size**: Using a subsample size \\( m = o(n) \\) to avoid overfitting and ensure valid inference.\n2. **Asymptotic validity**: The method yields correct coverage rates for alpha-stable limits, unlike conventional bootstrap.\n3. **Adaptive choice of m**: The optimal \\( m \\) is selected via bootstrap aggregation (e.g., \\( m = 871 \\) for IPW, \\( m = 712 \\) for DR).\n4. **Robustness**: It safeguards against propensity score misspecification and extreme weights.",
    "question_context": "The data set contains 2184 treatment and 3551 control observations. An observation is defined as treated if the patient was subject to RHC within 24 hours of admission. Health is evaluated using 30-day survival of the patients.\nThe propensity score model uses the same regressors as Hirano and Imbens (2001) in a probit model. Fig. 6.1 shows the distributions of the estimated propensity scores for treatment and control group.\nOur point estimates are similar or equivalent to the ones in Hirano and Imbens (2001): RHC decreases the expected 30-day survival probability by 6% for both inverse probability weighting and doubly robust estimation.\n\nThis section re-analyzes the causal impact of right heart catheterization (RHC) on survival rates of ill adult patients using observational data from Connors et al. (1996). The study employs propensity score pair matching to control for selection bias, finding that RHC leads to lower 30-day survival rates. The analysis uses inverse probability weighting and doubly robust estimation methods."
  },
  {
    "qid": "econ-empirical-1548-2-0-0",
    "question": "1) Derive the Theil index formula and explain why it satisfies the strong principle of transfers.",
    "gold_answer": "1. The Theil index formula is given by: $$ T_{p}=\\frac{1}{N_{p}}\\times\\sum_{i=1}^{I}\\left[N_{i p}\\times\\frac{x_{i p}}{\\mu_{p}}\\times l n\\left(\\frac{x_{i p}}{\\mu_{p}}\\right)\\right] $$ where:\n   - $N_{p}$: total number of farms in parish $p$\n   - $N_{i p}$: number of farms in size category $i$ in parish $p$\n   - $x_{i p}$: average HK in size category $i$\n   - $\\mu_{p}$: total average HK in parish $p$\n2. The strong principle of transfers states that a transfer from a richer to a poorer individual reduces inequality. The Theil index satisfies this because the logarithmic term $\\ln\\left(\\frac{x_{i p}}{\\mu_{p}}\\right)$ ensures that transfers from richer to poorer categories reduce the sum, thus lowering $T_{p}$.",
    "question_context": "The Theil index $T_{p}$ is calculated as follows: $$ T_{p}=\\frac{1}{N_{p}}\\times\\sum_{i=1}^{I}\\left[N_{i p}\\times\\frac{x_{i p}}{\\mu_{p}}\\times l n\\left(\\frac{x_{i p}}{\\mu_{p}}\\right)\\right] $$ for parish $p$ and size category of farms $i=\\{1,...,I\\}.N_{p}$ is the total number of farms in parish $p,$ $N_{i p}$ is the number of farms in size category $i$ in parish $p,x_{i p}$ is the average number of HK in size category $i$ (i.e. total HK in category $i$ divided by number of farms in category $i)$ , and $\\mu_{p}$ is the total average HK in parish $p$ (i.e. total parish HK divided by total number of farms).\nFirst-stage estimation: $$ \\ln(T o t a l H K)_{p}=\\alpha+\\beta\\times{B o u l d e r C l a y}_{p}+\\lambda_{r}+X_{p}^{\\prime}\\gamma+\\epsilon_{p} $$ Second-stage estimation: $$ \\begin{array}{r}{\\Delta T h e i l_{p}=\\alpha+\\beta\\times\\ln(T o t a l H K)_{p}+\\lambda_{r}+X_{p}^{\\prime}\\gamma+\\epsilon_{p}}\\end{array} $$ where $\\ln(T o t a l H K)_{p}$ is the natural logarithm of the total value of the land (measured in HK) of parish $p_{:}$ $B o u l d e r C l a y_{p}$ is the share of parish area classified as boulder clay, $\\lambda_{r}$ represent regional fixed effects (defined as: Greater Copenhagen, Zealand, Funen, and Jutland) and $X_{p}^{\\prime}\\gamma$ other geographical characteristics of parish $p$ , including the natural logarithm of the parish area, the distance to Copenhagen, distance to the coast and the latitude and longitude of the parish. 𝛥𝑇 ℎ𝑒𝑖𝑙𝑝 is the change in the Theil index for parish $p$ from 1682 to 1834.\n\nThe section discusses the use of the Theil index to measure land inequality, highlighting its properties and advantages over other measures like the Gini coefficient. It also introduces the instrumental variables strategy to address endogeneity concerns."
  },
  {
    "qid": "econ-empirical-196-5-0-2",
    "question": "3) Analyze the implications of the policy maker's inability to commit to not consulting another adviser on the adviser's participation constraint and the potential free-rider problem.",
    "gold_answer": "1. **Participation Constraint**: Without commitment, the adviser must be compensated for effort, as the policy maker may consult others.\n2. **Free-Rider Problem**: Advisers may avoid effort, expecting others to bear the cost, especially when \\( a \\) cannot be precisely determined.\n3. **Mitigation**: Advisers may still participate to avoid the risk of an adviser with opposing preferences being appointed.",
    "question_context": "We have shown that the more an adviser is biased towards one of the policy alternatives, the weaker her incentive to produce information. Hence, to maximise the quality of information, the policy maker should rely on unbiased advisers. However, if the policy maker is biased toward one of the policy alternatives, unbiased advisers manipulate information. To maximise the quality of the recommendation, the preferences of the policy maker and the adviser should coincide.\nAn important assumption underlying our analysis is that information about the effects of policies is not easily verified by policy makers because of a lack of time or expertise on the part of policy makers. Although we believe that this is a typical feature of the policy-making process, policy makers may occasionally be experts themselves or have acquired expertise over time, and hence may be less vulnerable to manipulation of information.\nThroughout the paper, we have assumed that an adviser is always willing to participate, i.e. we have ignored the advisers’ participation constraints. To identify an adviser’s participation constraint we must determine the adviser’s payoff if she does not participate. Clearly, this depends on the behaviour of the policy maker.\nFirst, the policy maker can commit himself on not consulting another adviser, if the selected adviser chooses not to participate. In that case the adviser participates if the expression in (2) is greater than zero for $e=e^{*}$ and $p<0$ .12 If (2) is smaller than zero, then the policy maker has to compensate the adviser.\nSecond, the policy maker cannot commit himself. In that case, the adviser must always be compensated for her effort when sufficient advisers are available. Without a sufficient reward, a free rider problem arises.\n\nThis paper has studied the delegation of information collection by policy makers to policy-motivated advisers. The analysis has centred on two agency problems: the uncertainty about the effort advisers put in producing information and the risk that advisers manipulate information or frame their recommendation."
  },
  {
    "qid": "econ-empirical-115-3-1-1",
    "question": "2) Derive the implication of Assumption 4.3 for the unweighted moment condition $\\mathbb{E}g(w,\\theta_{1}) = 0$ in the selected sample.",
    "gold_answer": "1. Under Assumption 4.3(ii), $\\mathbb{E}g(w,\\theta_{1})|z=0$.\n2. By the law of iterated expectations, $\\mathbb{E}g(w,\\theta_{1}) = \\mathbb{E}[\\mathbb{E}g(w,\\theta_{1})|z] = 0$.\n3. Thus, the unweighted moment condition holds in the selected sample without needing weights.",
    "question_context": "Assumption 4.1. $\\operatorname{P}(s=1|w)=\\operatorname{P}(s=1)$. That is, s is independent of $w$.\nAssumption 4.3 (Exogeneity of Selection). (i) Assumption 3.1 (ignorability of selection) holds. (ii) $\\mathbb{E}g(w,\\theta_{1})|z=0$.\n\nThis section introduces key assumptions and definitions related to exogeneity and selection models."
  },
  {
    "qid": "econ-empirical-1417-0-0-1",
    "question": "2) Derive the key equation used by King (1980) to model the impact of fiscal policies on wealth distribution, assuming a simplified version of his model.",
    "gold_answer": "The simplified model can be represented as:\\n\\[ \\Delta W = \\alpha T + \\beta G + \\epsilon \\]\\nwhere:\\n- \\(\\Delta W\\) is the change in wealth distribution,\\n- \\(T\\) represents tax policies,\\n- \\(G\\) represents government spending,\\n- \\(\\alpha\\) and \\(\\beta\\) are coefficients,\\n- \\(\\epsilon\\) is the error term.\\n\\n**Steps**:\\n1. Start with the general form of the wealth distribution function.\\n2. Incorporate fiscal policy variables \\(T\\) and \\(G\\).\\n3. Linearize the model for simplicity.",
    "question_context": "How Effective have Fiscal Policies been in Changing the Distribution of Income and Wealth?\n\nThis section examines the impact of fiscal policies on income and wealth distribution, as discussed by Mervyn A. King in his 1980 paper. The analysis focuses on the theoretical and empirical frameworks used to assess policy effectiveness."
  },
  {
    "qid": "econ-empirical-564-2-0-0",
    "question": "1) Derive the expected utility representation for the first preference domain where preferences are complete, transitive, and continuous over $\\mathbb{R}_{+}^{S}$ for a fixed $\\pi\\in\\mathrm{int}(\\Delta^{S-I})$.",
    "gold_answer": "1. **Axioms**: The preferences $\\succeq_{\\pi}$ must satisfy completeness, transitivity, continuity, and independence.\n2. **Representation**: By the von Neumann-Morgenstern theorem, there exists a utility function $U(\\mathbf{c}|\\pi)$ such that $U(\\mathbf{c}|\\pi) = \\sum_{s=1}^{S} \\pi_{s} v(c_{s})$, where $v$ is a NM index.\n3. **Example**: For $U(\\mathbf{c}|\\pi) = -0.5[\\exp(-0.5c_{1}) + \\exp(-0.3c_{1}) + \\exp(-0.2c_{1})] - 0.3[\\exp(-0.5c_{2}) + \\exp(-0.3c_{2}) + \\exp(-0.2c_{2})] - 0.2[\\exp(-0.5c_{3}) + \\exp(-0.3c_{3}) + \\exp(-0.2c_{3})]$, the NM index is $v(c) = -[\\exp(-0.5c) + \\exp(-0.3c) + \\exp(-0.2c)]$.",
    "question_context": "Assume there are $S\\geq2$ states of nature and there is a single consumption good in each state. A typical consumption plan is an $S$ vector $(c_{1},c_{2},....,c_{S})$ in the consumption space defined by $\\mathbb{R}_{+}^{S}$ . We assume that probabilities are objective and known and denote the probability of state $s$ by $\\pi_{s}$ . Let $\\pi=(\\pi_{1},\\pi_{2},...,\\pi_{S})$ ; where $\\pi\\in\\mathrm{int}(\\Delta^{S-I})=$ $\\{\\pi\\in\\mathbb{R}_{++}^{S}:\\sum_{s=1}^{S}\\pi_{s}=1\\}$ : Given this setting, we next define three different choices spaces which we will investigate.\nThe first preference domain we consider corresponds to the classic Arrow–Debreu contingent claim setup in which for a given value of $\\pi\\in\\mathrm{int}(\\Delta^{S-I})$ a decision maker is assumed to have complete, transitive and continuous preferences over $\\mathbb{R}_{+}^{S}$ which are denoted $\\succeq\\pi$ .\nThe second preference domain arises if one assumes as in Kubler et al. (2014) that the consumer confronts a sequence of independent contingent claim optimisations where probabilities and prices vary. Then corresponding to a set of probability vectors $\\{\\pi\\}$ , there will be a set of preference relations $\\{\\succeq\\pi\\}$ which need not give the same ordering over consumption vectors.\nThe third choice space we consider is the full set of distributions corresponding to $(\\mathbf{c},\\pi)$ , or the set of ‘risky prospects’. To make this precise, define a risky prospect as a pair of vectors $(\\mathbf{c},\\pmb{\\pi})\\in\\mathbb{R}_{+}^{S^{\\top}}\\times\\mathrm{int}(\\Delta^{S-l})$ . Assume that a decision maker has continuous, complete and transitive preferences over $\\mathcal{P}=\\mathbb{R}_{+}^{S}\\times\\mathrm{int}(\\Delta^{S-I})$ ; denoted $\\succeq_{\\mathcal{P}}$ .\nFor each of the above three preference cases, we provide in the next two Sections a set of axioms that is necessary and sufficient for preferences to be representable by an Expected Utility function.\n$$ U(c_{1},c_{2},c_{3}|\\pi_{1},\\pi_{2},\\pi_{3})=-\\pi_{1}\\sum_{s=1}^{3}\\pi_{s}[\\exp(-\\pi_{1}c_{s})+\\exp(-\\pi_{2}c_{s})+\\exp(-\\pi_{3}c_{s})]. $$\n$$ U(\\mathbf{c}|\\pi)=-0.5[\\exp(-0.5c_{1})+\\exp(-0.3c_{1})+\\exp(-0.2c_{1})] -0.3[\\exp(-0.5c_{2})+\\exp(-0.3c_{2})+\\exp(-0.2c_{2})] -0.2[\\exp(-0.5c_{3})+\\exp(-0.3c_{3})+\\exp(-0.2c_{3})]. $$\n$$ \\left.\\frac{\\partial U/\\partial c_{1}}{\\partial U/\\partial c_{s}}\\right|_{c_{1}=c_{s}}=\\frac{\\pi_{1}}{\\pi_{s}}~(s=2,3) $$\n$$ v(c)=-[\\exp(-0.5c)+\\exp(-0.3c)+\\exp(-0.2c)]. $$\n$$ U(\\mathbf{c}|\\pi)=f\\biggl[\\pi,\\sum_{s=1}^{S}\\pi_{s}\\upsilon_{\\pi}(c_{s})\\biggr], $$\n$$ v_{\\pi}(c)=-[\\exp(-\\pi_{1}c)+\\exp(-\\pi_{2}c)+\\exp(-\\pi_{3}c)], $$\n$$ U(\\mathbf{c}|\\pi)=f\\left[\\pi,\\sum_{s=1}^{S}\\pi_{s}\\upsilon(c_{s})\\right], $$\n$$ U(\\mathbf{c},\\pi)=f\\left[\\sum_{s=1}^{S}\\pi_{s}v\\big(c_{s}\\big)\\right], $$\n\nThe text discusses three different preference domains in the context of contingent claims under varying probabilities. It introduces mathematical models for consumption plans, utility functions, and expected utility representations under different axiomatic frameworks."
  },
  {
    "qid": "econ-empirical-872-1-0-0",
    "question": "1) Explain the rationale behind using a $10\\%$ sample of domestic itineraries (DB1) and T100 data for the second quarter of 2006 in the estimation. How does this choice address potential issues like cooperative pricing in later years?",
    "gold_answer": "The rationale includes:\\n1. **Avoiding Cooperative Pricing**: Using older data (2006) avoids later years where carriers may have engaged in cooperative pricing, as alleged by Ciliberto and Williams (2014).\\n2. **Predictive Power**: The data allows for predictions about subsequent mergers without contamination from post-merger cooperative behavior.\\n3. **Representativeness**: A $10\\%$ sample of DB1 provides a manageable yet representative subset of domestic itineraries, while T100 data captures flight records between airports.",
    "question_context": "We estimate our model using a cross-section of publicly available DB1 (a $10\\%$ sample of domestic itineraries) and T100 (records of flights between airports) data for the second quarter of 2006.\nWe use data for 2028 airport-pair markets linking the 79 busiest US airports in the lower 48 states.\nWe model six named legacy carriers, American Airlines, Continental Airlines, Delta Air Lines, Northwest Airlines, United Airlines and US Airways, and one named low-cost carrier (LCC), Southwest.\nWe measure a carrier’s price as the average round-trip price in DB1. A carrier’s market share is calculated as the total number of passengers that it carries in DB1, regardless of service type, divided by a measure of market size.\nWe calculate market size as the prediction from a gravity model, which accounts for historical total endpoint enplanements using endpoint fixed effects and route distance.\n\nThe study uses a cross-section of publicly available DB1 and T100 data for the second quarter of 2006 to estimate the model. The data includes 2028 airport-pair markets linking the 79 busiest US airports, with six named legacy carriers and one named low-cost carrier (LCC). Market shares and prices are calculated using DB1 data, and market size is estimated using a gravity model."
  },
  {
    "qid": "econ-empirical-87-2-1-0",
    "question": "1) Derive the expression for the wedge $\\tau_{i}^{0}$ in terms of the Lagrange multipliers and parameters of the model.",
    "gold_answer": "1. The wedge is given by:\n   $$\n   \\tau_{i}^{0} = \\frac{u_{i}'(q_{i}^{0}(m_{i}^{0}) + m_{i}^{0})}{p_{i}(m_{i}^{0},w^{0*})} - 1\n   $$\n2. Using the first-order conditions and definitions, it can be expressed as:\n   $$\n   \\tau_{i}^{0} = \\left\\{\\begin{array}{l l}{\\displaystyle\\frac{\\sigma^{*}-1}{\\sigma^{*}}\\mu^{0}-1,}&{\\mathrm{if}\\frac{a_{i}}{a_{i}^{*}}<A^{I};}\\ {\\displaystyle\\qquad\\frac{\\lambda^{0}a_{i}}{w^{0*}a_{i}^{*}}-1,}&{\\mathrm{if}A^{I}<\\displaystyle\\frac{a_{i}}{a_{i}^{*}}\\leq A^{I I};}\\ {\\displaystyle\\qquad\\frac{\\lambda^{0*}}{w^{0*}}+\\mu^{0}-1,}&{\\mathrm{if}\\frac{a_{i}}{a_{i}^{*}}>A^{I I}.}\\end{array}\\right.\n   $$",
    "question_context": "Trade taxes cause domestic and world prices to differ from one another. To prepare our analysis of optimal trade taxes, we start by describing the wedges, $\\tau_{i}^{0}$ , between the marginal utility of the domestic consumer, $u_{i}'(c_{i}^{0})=\\beta_{i}\\big(c_{i}^{0}\\big)^{-\\frac{1}{\\sigma}}$ , and the world price, $p_{i}^{0}$ , that must prevail at any solution to Home’s planning problem:\n\n$$\n\\tau_{i}^{0}\\equiv\\frac{u_{i}'(c_{i}^{0})}{p_{i}^{0}}-1.\n$$\nProposition 2. At any solution to the domestic government’s problem, trade taxes, $t^{0}$ , are such that (i) $t_{i}^{0}=(1+\\bar{t})\\bigg(\\frac{A^{I}}{A^{I I}}\\bigg)-1$ , if $\\frac{a_{i}}{a_{i}^{*}}<A^{I}$ ; (ii) $t_{i}^{0}=(1+\\bar{t})\\Bigl(\\frac{a_{i}}{a_{i}^{*}A^{I I}}\\Bigr)-1$ , if $\\frac{a_{i}}{a_{i}^{*}}\\in[A^{I},A^{I I}]$ ; and (iii) $t_{i}^{0}=\\bar{t}$ , if $\\frac{a_{i}}{a_{i}^{*}}>A^{I I}$ , with $\\bar{t}>-1$ and $A^{I}<A^{I I}$ .\n\nThis section explores the implementation of the solution to Home’s planning problem using trade taxes. It describes the wedges between the marginal utility of the domestic consumer and the world price, and how these wedges vary with comparative advantage."
  },
  {
    "qid": "econ-empirical-50-0-2-0",
    "question": "5) Analyze the effects of a 10% worldwide decline in variable trade costs on firm growth rates across different size quartiles.",
    "gold_answer": "1. Largest firms benefit due to increased export opportunities.\n2. Small firms face reduced domestic sales but may grow faster due to adjustments in their extensive margin.\n3. Overall GDP growth increases, with reallocations favoring large exporters.",
    "question_context": "I use the model to study the impact of cross-firm reallocations on economic activity and measured productivity.\nThe decomposition for productivity growth is notably similar to the empirical decomposition of Foster, Haltiwanger, and Krizan (2001) for U.S. manufacturing firms.\n\nThe model is used to study cross-firm reallocations in response to aggregate shocks, such as trade liberalization or technology shocks. The decomposition of productivity growth highlights the role of within-firm, cross-firm, and between-firm effects."
  },
  {
    "qid": "econ-empirical-1685-0-0-2",
    "question": "3) Using Hall's Marriage Theorem, show that a bipartite graph $G=\\langle S\\cup B,L\\rangle$ with $|S| = |B|$ has a perfect matching if and only if for every subset $S' \\subseteq S$, $|N(S')| \\geq |S'|$.",
    "gold_answer": "Proof:\n1. ($\\Rightarrow$) If $G$ has a perfect matching, then for any $S' \\subseteq S$, the matched vertices in $S'$ must have distinct neighbors in $B$, so $|N(S')| \\geq |S'|$\n\n2. ($\\Leftarrow$) Assume for every $S' \\subseteq S$, $|N(S')| \\geq |S'|$\n   - This is exactly the condition in Hall's Marriage Theorem\n   - Therefore, there exists a matching saturating $S$\n   - Since $|S| = |B|$, this is a perfect matching\n\nFormally, Hall's Marriage Theorem states that a bipartite graph $G=(S\\cup B, E)$ has a matching saturating $S$ if and only if for every subset $S' \\subseteq S$, $|N(S')| \\geq |S'|$.",
    "question_context": "We analyze bargaining between buyers and sellers who are connected by an exogenously given network. Players can make repeated alternating public offers that may be accepted by any of the responders linked to each specific proposer. Our purpose is to find the conditions of the network which drive the price distribution in equilibrium. This paper uses graph theory tools to provide necessary and sufficient conditions regarding the architecture of networks for the subgame perfect equilibrium of the bargaining game to coincide with the Walrasian outcome.\nThe market consists of $n$ sellers $s_{1},s_{2},\\ldots,s_{n}$ and $m$ buyers $b_{1},b_{2},\\dots,b_{m}$ : Each seller owns an indivisible good and each buyer owns money. If a seller and a buyer trade at price $p$ at period $t$ ; the seller receives a utility of $\\delta^{t}p$ and the buyer receives $\\delta^{t}(1-p)$ : Note that all sellers have the same utility function and all buyers are also alike, since we want the differences in prices to be driven only by the network structure. Agents are embedded in a network that links sellers and buyers, and trade is only possible among linked agents.\nA non-directed bipartite graph $G=\\langle S\\cup B,L\\rangle$ consists of a set of nodes formed by sellers $S=\\{s_{1},\\ldots,s_{n}\\}$ ; buyers $B=\\{b_{1},...,b_{m}\\}$ and a set of links $L$ ; each link joining a seller with a buyer. An element of $L$ say a link from $s_{i}$ to $b_{j}$ ; will be denoted as $s_{i}:b_{j}$ : We will say that a node a belongs to a graph $G=\\langle S\\cup B,L\\rangle$ if $a\\in S\\cup B$ : We say that a node $s_{i}$ is linked to another node $b_{j}$ if there is a link joining the two.\nA bipartite graph $G$ is connected if there exists a path linking any two nodes of the graph. Formally, a path linking nodes $s_{j}$ and $b_{i}$ will be a collection of $t$ buyers and $t$ sellers, $t\\geqslant0,s_{1},\\ldots,s_{t},b_{1},\\ldots,b_{t}$ among $S\\cup B$ (possibly some of them repeated) such that $\\{s_{j}:b_{1},b_{1}:s_{1},s_{1}:b_{2},\\ldots,s_{t-1}:b_{t},b_{t}:s_{t},s_{t}:b_{i}\\}\\in L.$\nA matching in a bipartite graph $G=\\langle S\\cup B,L\\rangle$ is a collection of pairs of linked members of $B$ and $S$ such that each agent in $S\\cup B$ belongs to at most one pair. A matching saturates all the nodes in $V$ if the set of pairs contains all members of $V$ : If $G=\\langle S\\cup B,L\\rangle$ is such that $|S|=|B|$ ; then a matching which saturates $S$ (e.g., saturates $B$ ) is called a perfect matching.\n\nThe paper analyzes bargaining between buyers and sellers connected by an exogenously given network. Players make repeated alternating public offers that may be accepted by any of the responders linked to each specific proposer. The goal is to find the network conditions that drive the price distribution in equilibrium. The paper uses graph theory tools to provide necessary and sufficient conditions regarding the network architecture for the subgame perfect equilibrium of the bargaining game to coincide with the Walrasian outcome."
  },
  {
    "qid": "econ-empirical-79-57-0-2",
    "question": "3) Explain why the precision of ROI estimates in advertising RCTs is inherently low, given the typical values of \\( \\mu = \\$7 \\) and \\( \\sigma = \\$75 \\) from the text.",
    "gold_answer": "1. The signal-to-noise ratio for detecting a treatment effect is given by \\( \\frac{\\mu}{\\sigma} = \\frac{7}{75} \\approx 0.093 \\), which is very low.\n2. The standard error of the mean effect is \\( \\frac{\\sigma}{\\sqrt{n}} \\), requiring very large \\( n \\) to achieve a small confidence interval.\n3. For example, to detect a \\( \\$0.35 \\) effect (5% of \\( \\mu \\)) with 95% confidence, the required sample size is \\( n = \\left( \\frac{1.96 * 75}{0.35} \\right)^2 \\approx 176,400 \\).\n4. This illustrates the challenge of achieving precise estimates when the effect size is small relative to the noise in the data.",
    "question_context": "The median confidence interval on return on investment is over 100 percentage points wide. Detailed sales data show that relative to the per capita cost of the advertising, individual-level sales are very volatile; a coefficient of variation of 10 is common.\nThe standard deviation of individual-level sales is typically 10 times the mean over typical campaign evaluation window.\n\nThe text discusses the inherent difficulties in measuring the return on investment (ROI) for digital advertising campaigns, highlighting the volatility of individual-level sales data and the large sample sizes required for reliable estimates."
  },
  {
    "qid": "econ-empirical-49-0-0-1",
    "question": "2) What are the three testable hypotheses derived from the model regarding media access, voter turnout, and government spending?",
    "gold_answer": "1. Government spending should be higher in groups with greater media access. \\n2. Spending should be higher in groups with higher voter turnout. \\n3. Voter turnout should be higher in groups with greater media access.",
    "question_context": "If informed voters receive favorable policies, then the invention of a new mass medium may affect government policies since it affects who is informed and who is not.\nThe model focuses on the Governors as the main source of political influence on the state-to-county allocation of emergency relief funds.\n\nThe paper examines the impact of mass media, specifically radio, on government policy-making, focusing on the allocation of New Deal relief funds. It posits that informed voters receive favorable policies, and the introduction of new mass media affects who is informed and who is not."
  },
  {
    "qid": "econ-empirical-416-0-0-0",
    "question": "1) Define rationalizability in the context of game theory and explain how it differs from Nash equilibrium.",
    "gold_answer": "1. **Rationalizability**: A strategy is rationalizable if it is the best response to some belief about the opponents' strategies that they themselves could rationally hold. Formally, a strategy \\( s_i \\) for player \\( i \\) is rationalizable if there exists a belief \\( \\mu_i \\) over the strategies of other players such that \\( s_i \\) maximizes \\( i \\)’s expected payoff given \\( \\mu_i \\).\n2. **Difference from Nash Equilibrium**: In a Nash equilibrium, each player's strategy is a best response to the actual strategies of others, and beliefs are correct. Rationalizability does not require beliefs to be correct, only that they are consistent with common knowledge of rationality.",
    "question_context": "Rationalizability and correlated equilibria are central to understanding strategic interactions where players' beliefs about others' strategies influence their own choices.\n\nThis paper explores the concepts of rationalizability and correlated equilibria in game theory, providing a foundational analysis of how players' beliefs and strategies interact in strategic environments."
  },
  {
    "qid": "econ-empirical-1559-2-1-3",
    "question": "4) The sexual IPV results show a 14% short-term decline. Construct a theoretical model where $\\frac{\\partial IPV_{sexual}}{\\partial Reform} < 0$ while $\\frac{\\partial IPV_{other}}{\\partial Reform} > 0$ in the long term, specifying the key cross-derivatives.",
    "gold_answer": "Model bargaining power $\\phi_t$: $IPV_{sexual,t} = \\alpha_0 - \\alpha_1\\phi_t + \\epsilon_t$ $IPV_{other,t} = \\beta_0 + \\beta_1\\phi_t - \\beta_2\\phi_t^2 + u_t$ where $\\frac{d\\phi}{dReform} > 0$ initially (threat effect) but $\\frac{d^2\\phi}{dReform dt} < 0$ (erosion). Requires $\\alpha_1 > \\beta_1$ initially but $\\beta_2$ dominates in long run.",
    "question_context": "Panel B shows IPV increased by 3.7 percentage points (21% of pre-reform mean) in states exposed to reform for 6-8 years, while short-term effects were statistically zero.\nColumn 5 (preferred specification) shows 4.2 percentage point increase (24% of baseline) in IPV for intact marriages in long term.\nWild cluster bootstrap p-values address inference challenges with few treated clusters (2 states in long term). Restricted version under-rejects (p=0.027 vs unrestricted p=0.000 for long-term effect).\n\nThis section examines the impact of unilateral divorce on intimate partner violence (IPV) using a difference-in-differences framework with multiple robustness checks. Key findings include short-term null effects and long-term increases in IPV prevalence."
  },
  {
    "qid": "econ-empirical-691-0-0-1",
    "question": "2) Explain why standard nonlinear instrumental variables methods fail to yield consistent estimators in the presence of measurement errors in nonlinear models.",
    "gold_answer": "Standard nonlinear IV methods fail because:\n\n1. The nonlinear model with measurement error is not isomorphic to a linear simultaneous equations model.\n2. The orthogonality conditions \\( E[\\epsilon Z] = 0 \\) do not hold due to the measurement error.\n3. The error term \\( \\epsilon \\) becomes correlated with the instruments \\( Z \\) through the nonlinearity.",
    "question_context": "Methods of estimation of regression coefficients are proposed when the regression function includes a polynomial in a 'true' regressor which is measured with error. Two sources of additional information concerning the unobservable regressor are considered: either an additional indicator of the regressor (itself measured with error) or instrumental variables which characterize the systematic variation in the true regressor.\nConsistency and asymptotic normality of the estimated coefficients is demonstrated, and consistent estimators of the asymptotic covariance matrices are provided.\nA particularly challenging statistical problem is the construction of consistent estimators of the parameters of nonlinear regression models when the regressors as well as the dependent variable are subject to measurement errors.\n\nThe paper discusses methods for estimating regression coefficients in models where the regression function includes a polynomial in a 'true' regressor measured with error. Two sources of additional information are considered: an additional indicator of the regressor or instrumental variables."
  },
  {
    "qid": "econ-empirical-548-2-0-3",
    "question": "4) How does the choice of predictors in $X$ affect the analysis of differences between matched and unmatched groups? Provide an example from the text.",
    "gold_answer": "1. The choice of $X$ determines whether observed differences persist after controlling for covariates.\n2. For example, if age, race, and schooling are included in $X$, differences in participation rates between matched and unmatched may diminish.\n3. The text notes that differences in coefficients for variables like age, race, and schooling are often statistically significant, indicating residual differences even after conditioning on $X$.",
    "question_context": "Let $W$ and $Y$ denote the response variables, respectively wages and an indicator of labor force status at a given survey, let $X$ be a vector of predictors, and let $D$ be an indicator for the type of match, taking value 1 for the matched, 2 for the unmatched in matched households, and 3 for those in unmatched households. The population joint distribution of labor force status and wages at a given survey can be decomposed as $$ F_{x}(W,Y)=\\sum_{d=1}^{3}F_{x}(W\\mid Y,D=d)P_{x}(Y\\mid D=d)P_{x}(D=d), $$ where $P_{x}$ denotes the conditional probability and $F_{x}$ the conditional distribution function given $X=x$.\nSimilarly, the population joint distribution by labor force status in two adjacent surveys can be decomposed as $$ P_{x}(Y,Y^{\\prime})=\\sum_{d=1}^{3}P_{x}(Y^{\\prime}|Y,D=d)P_{x}(Y|D=d)P_{x}(D=d). $$\nIn the absence of prior information, the one-year transition probabilities $F_{x}(Y^{\\prime}|Y,D=d)$ are only identifiable for the matched $(D=1),$ 0. Thus, the population distribution (2) may not be identifiable from the matched files. A strong identifying assumption is that labor force transitions are conditionally independent of $D$ given $X$.\n\nThis section examines whether labor market outcomes are independent of the process determining attrition, focusing on the joint distribution of labor force status and wages, and the joint distribution by labor force status in two adjacent surveys."
  },
  {
    "qid": "econ-empirical-823-0-0-0",
    "question": "1) Derive the Cambridge equation $P/K = n/s_{c}$ from the assumptions of full employment output, exogenous investment, and $s_{w} = 0$. Explain the economic intuition behind this result.",
    "gold_answer": "1. **Assumptions**: Full employment output, exogenous investment, and workers' saving propensity $s_{w} = 0$.  \n2. **Investment equals savings**: In equilibrium, $I = S$. Since workers do not save, total savings come from capitalists: $S = s_{c}P$.  \n3. **Natural growth rate**: Investment grows at the natural rate $n$, so $I = nK$.  \n4. **Equate savings and investment**: $s_{c}P = nK$.  \n5. **Solve for $P/K$**: $P/K = n/s_{c}$.  \n**Intuition**: The profit rate adjusts to ensure savings by capitalists match investment required for growth at rate $n$.",
    "question_context": "The equilibrium rate of profit $(P/K)$ equals the natural rate of growth $(n)$ divided by capitalists' propensity to save $(s_{c})$, given by the Cambridge equation: $$P/K=n/s_{c}.$$\nThe share of profits in income is obtained by multiplying the Cambridge equation by the capital-output ratio $(\\bar{v} \\equiv K/Y)$: $$P/Y=n\\bar{v}/s_{c}.$$\nPasinetti (1962) argued that workers save a fraction of their wage plus profit income, while capitalists save a larger fraction of their profits, restoring the Cambridge equation results.\nSamuelson and Modigliani (1966) introduced a dual solution where the rate of profit is determined by workers' saving propensity, the natural rate of growth, and the production function.\nThe paper analyzes the relationship between income distribution by class and saving propensities in the Pasinetti and S-M systems, showing that both saving propensities influence long-run income distribution by class.\n\nThe post-Keynesian school has provided propositions on the rate of profit and income distribution in long-run growth equilibrium, focusing on the Cambridge equation and its extensions by Kaldor, Pasinetti, and Samuelson-Modigliani."
  },
  {
    "qid": "econ-empirical-930-3-0-0",
    "question": "1) Derive the profit function $\\Pi(d,p)$ for the monopolistic seller given the isoelastic demand function with elasticity $\\frac{d+1}{d}$ and per unit input cost of 1.",
    "gold_answer": "1. The demand function is given by $q(p) = p^{-\\frac{d+1}{d}}$.\n2. Revenue is $R(p) = p \\cdot q(p) = p^{1-\\frac{d+1}{d}} = p^{-\\frac{1}{d}}$.\n3. Cost is $C(p) = 1 \\cdot q(p) = p^{-\\frac{d+1}{d}}$.\n4. Profit is $\\Pi(d,p) = R(p) - C(p) = p^{-\\frac{1}{d}}(p - 1)$.",
    "question_context": "A monopolistic seller has a per unit input cost of 1 and sets the price $p$ facing an isoelastic demand function with elasticity $\\textstyle{\\frac{d+1}{d}}$ . The elasticity parameter $d$ is uniformly distributed over the interval $[\\frac{1}{9},\\frac{1}{2}]$ , and the seller can learn its value through costly learning. Profits are given by $\\Pi(d,p)=p^{-\\frac{d+1}{d}}(p-1)$ , with $d$ describing the state of the world and $p$ the seller's action.\nThe GAP-SQP algorithm terminates in less than 0.05 seconds in all runs, with minimal differences across information costs. The BA algorithm runs in about one second when information costs are very low but is substantially slower for higher information costs, up to 20 seconds.\nThe reason why GAP-SQP outperforms the BA algorithm is twofold. First, wide swaths of potential actions (prices) are dominated in this example. The GAP-SQP quickly drops them from consideration, which drastically reduces the size of the problem in later iterations. BA never drops actions, and must continue to construct conditional and marginal probabilities over each and every one until the algorithm converges.\n\nThis section discusses the application of the GAP-SQP algorithm to a monopolist problem with uncertain demand, as proposed by Matejka (2016). The model involves a monopolistic seller setting prices under an isoelastic demand function with a uniformly distributed elasticity parameter."
  },
  {
    "qid": "econ-empirical-1170-3-0-3",
    "question": "4) Critically evaluate the claim that 'high-ability students have a lower cost of acquiring information' based on the finding that MCAT scores negatively correlate with absolute estimation errors (coefficient = -0.413). Propose an alternative explanation.",
    "gold_answer": "1. **Supporting evidence:**\n   - Higher MCAT scores may reflect better analytical skills, reducing information processing costs.\n2. **Alternative explanation:**\n   - High-ability students may self-select into specialties with more transparent income data (e.g., family practice).\n   - The coefficient magnitude (-0.413) is small, suggesting other unobserved factors (e.g., networking) may dominate.",
    "question_context": "The mean of the absolute value of the income estimation error between 1970 and 1998 is plotted in Figure 2, separately for first- and fourth-year students. I group the data by cohort, so the responses for first-year students line up with their responses three years later. The accuracy of fourth-year students has remained fairly stable between1970 and 1998, ranging from a 25 percent to a 35 percent error. First-year students were quite inaccurate in the late 1970s and early 1980s when physician income was particularly volatile, but their accuracy has improved since then.\nThe median of the absolute value of the estimation error ranges from 24.6 percent to 30.3 percent across the six specialties among first-year students, and from 19.3 percent to 27.7 percent among fourth-year students.\nTo examine the determinants of accuracy, I regress the absolute value of a student's income estimation error, measured as percentage points of actual income, on the same regressors as in Equation 2: $$\\vert\\pmb{\\varepsilon}_{i j,t}\\vert=\\theta_{0}+\\theta_{1}X_{i}+\\theta_{2}S+\\theta_{3}T+u_{2}$$\nStudents’ estimates of physician income in their preferred specialty are 4.1 percentage points and 1.3 percentage points more accurate in their first and fourth year, respectively, relative to estimates for the other five specialties.\n\nThe section examines the accuracy of medical students' income estimations for physicians across different specialties, comparing first-year and fourth-year students. It analyzes the absolute value of the signed income estimation error, $|{\\mathfrak{E}}_{i j}|$, as a percentage of actual income, and explores determinants of these errors through regression analysis."
  },
  {
    "qid": "econ-empirical-755-0-0-0",
    "question": "1) Derive the second characteristic function $\\phi(t)$ for a stable random variable and explain the role of each parameter ($\\alpha$, $\\beta$, $\\delta$, $c$) in the context of stock return distributions.",
    "gold_answer": "1. The second characteristic function is given by: $$\\phi(t)=i\\delta t-c|t|^{\\alpha}[1+\\beta(t/|t|)\\omega(|t|,\\alpha)].$$ 2. Parameters: - $\\alpha$ (shape): Determines tail thickness; lower values imply thicker tails. - $\\beta$ (skewness): $\\beta<0$ skews right, $\\beta>0$ skews left, $\\beta=0$ symmetric. - $\\delta$ (location): Shifts the distribution. - $c$ (scale): Adjusts dispersion. 3. For stock returns, $\\alpha<2$ suggests infinite variance, but empirical tails are thinner than stable laws.",
    "question_context": "A stable random variable may be parameterized in terms of its second characteristic function $\\phi(t)$ given by $$\\phi(t)=i\\delta t-c|t|^{\\alpha}[1+\\beta(t/|t|)\\omega(|t|,\\alpha)],$$ where $i=(-1)^{1/2}$ and the parameters satisfy the conditions $-\\infty<\\delta<\\infty$, $c>0$, $|\\beta|\\leq1$, $0<\\alpha\\leq2$, $\\omega(|t|,\\alpha)=\\tan(\\pi\\alpha/2)$ if $\\alpha\\neq1$ and $\\omega(|t|,\\alpha)=(2/\\pi)\\log|t|$ if $\\alpha=1$.\nThe tails of the return distributions are found to be thinner than those of infinite variance stable distributions. Therefore, although homogeneity is evident in general, economic and statistical inferences drawn from stable-law parameters estimated from samples of stock returns may be misleading.\n\nThis section discusses the properties of stable distributions and their application to modeling stock returns, highlighting the empirical findings that challenge the stable-law hypothesis."
  },
  {
    "qid": "econ-empirical-183-5-2-1",
    "question": "2) Derive the density $D_t^*$ for the worst-case measure $P^*$ in the case of U-shaped payoffs, and explain how it depends on the critical points $x_t$.",
    "gold_answer": "1. For $S_t \\geq x_t$, the worst-case measure uses $\\alpha_t = a$ (i.e., $\\underline{{p}}$).\n2. For $S_t < x_t$, the worst-case measure uses $\\alpha_t = b$ (i.e., $\\overline{{p}}$).\n3. The density is constructed as $D_t^* = \\prod_{v \\leq t, S_v \\geq x_v} \\exp(a \\varepsilon_v - L(a)) \\prod_{v \\leq t, S_v < x_v} \\exp(b \\varepsilon_v - L(b))$.\n4. Thus, $D_t^*$ switches between $a$ and $b$ based on whether $S_t$ is above or below $x_t$.",
    "question_context": "We prove by backward induction that for every $t<T, U_{t}= v(t,S_{t})$ for a function $v(t,S)$ that is increasing in $S_{t}$ for $S_{t}\\geq x_{t}$ and decreasing for $S_{t}<x_{t}$.\nAt time $T$, we clearly have $U_{T}=g(T,S_{T})$, which is decreasing for $S_{T}<K_{T}^{*}$ and increasing otherwise. Thus, the claim is valid for $t=T$.\nFor $t<T$, $U_{t}=\\operatorname*{max}\\Bigl\\{g(t,S_{t}), \\underset{p_{t+1}\\in[\\underline{{p}},\\overline{{p}}]}{\\mathrm{min}}\\bigl(p_{t+1}v(t+1,S_{t}\\cdot u)+(1-p_{t+1})v(t+1,S_{t}\\cdot d)\\bigr)\\Bigr\\}$.\n\nThis section extends the analysis to U-shaped payoffs, demonstrating how to compute the worst-case measure and the value function via backward induction."
  },
  {
    "qid": "econ-empirical-1073-0-1-0",
    "question": "1) Evaluate the effectiveness of the panel's approach to data collection for manpower utilization. What are the trade-offs between specialized and comprehensive surveys?",
    "gold_answer": "1. Specialized surveys yield precise data but may lack broader context.  \n2. Comprehensive surveys (e.g., Census) provide breadth but risk 'kitchen-sink' inefficiencies.  \n3. The panel's hybrid approach (targeted grants + existing instruments) balances these trade-offs.  \n4. Implementation feasibility is critical, as seen in NSF's proactive funding.",
    "question_context": "The task of the panel was a narrow and specific one: to identify needs for basic data on which future decisions could be based, and to recommend techniques for collecting such data, by whichever means appeared most appropriate--special studies, the institution of periodic surveys, the expansion of existing data collection instruments.\nThe truly distinctive feature of these recommendations was their high probability of implementation: the National Science Foundation was both willing and able to follow through by awarding grants, contracts, pilot projects to a variety of organizations, both within and outside the federal government, to initiate collection of data to which the panel had assigned high priority.\nWhile too much cooperation is never an unmixed blessing--in the present case, the questionnaire and the analysis both show the usual 'kitchen-sink' symptoms which are the price of involving many agencies, each with its specialized interests--the joint effort between the three principals certainly represents a productive and pioneering example of collaboration between researchers in and out of government.\n\nThe text highlights the collaborative efforts between government and private research groups to address manpower utilization, focusing on the challenges of data collection and sampling."
  },
  {
    "qid": "econ-empirical-1830-2-0-3",
    "question": "4) Using Table 1, analyze the balance test results. What do the correlations between Food Stamps exposure and county characteristics imply about the validity of the research design?",
    "gold_answer": "1. **No Correlation with Other Programs**: Food Stamps exposure is uncorrelated with WIC, Head Start, and CHCs, supporting exogeneity. \n2. **Population Correlation**: Larger populations adopted Food Stamps earlier, consistent with historical evidence. \n3. **Transfer Programs**: Negative correlation with retirement/DI benefits, Medicare, and income maintenance suggests substitution effects. \n4. **Economic Indicators**: No correlation with income or employment mitigates concerns about economic confounders. \n5. **Conclusion**: Limited significant correlations support the validity of the design.",
    "question_context": "We exploit the birth-county-by-birth-year (or birth-year-month) variation in Food Stamps availability in event-study, linear spline, and DD models. For computational ease, we collapse our data into birth-year $\\times$ birth-county $\\times$ survey-year cells, separately by sex and race (white versus nonwhite).\nIn order to characterize the effect dynamics by age, we use an event-study specification of the following form: $$ Y_{c b t}=\\theta_{c}+\\delta_{s(c)b}+\\psi_{t}+X_{c b}\\beta+Z_{c60}b\\eta+\\sum_{a=-5[a\\neq10]}^{a=17}\\pi_{a}1[F S_{c}-b=a]+\\epsilon_{c b t}, $$ where an outcome, $Y,$ is defined for a cohort born in county $c$ in state $s(c)$ , in birth year $b$ , and observed in survey year t.\nThe linear spline model takes the form $$ \\begin{array}{r l}&{\\mathbf{\\Phi}_{c b t}^{\\prime}=\\theta_{c}+\\delta_{s(c)b}+\\psi_{t}+X_{c b}\\beta+Z_{c60}b\\eta+\\omega_{1}1[F S_{c}-b\\mathbf{\\Phi}<-1]*(F S_{c}-b)}\\ &{\\qquad+\\omega_{2}1[-1\\leq F S_{c}-b\\mathbf{\\Phi}+[F S_{c}-b)+\\omega_{3}1[6\\leq F S_{c}-b\\mathbf{\\Phi}<11]*(F S_{c}-b)}\\ &{\\qquad+\\omega_{4}1[11\\leq F S_{c}-b]*(F S_{c}-b)+\\epsilon_{c b t}}\\end{array} $$ for each cohort born in county $c$ in state $s(c)$ , and year $b$ , and observed in survey year $t$.\nLastly, we estimate a DD model, using cumulative exposure before age five as the 'dose' of the program (Hoynes et al., 2016). We calculate the share of months each cohort is exposed between the (approximate) month of conception and age five, ShareF ScIbU−5. We use this measure as the primary $$ Y_{c b m t}=\\theta_{c}+\\delta_{s(c)b}+\\rho_{m}+\\psi_{t}+X_{c b}\\beta+Z_{c60}b\\eta+\\kappa S h a r e F S_{c b m}^{I U-5}+\\upsilon_{c b m t} $$ for each cohort born in county $c$ in state $s(c)$ , year $b$ and month $m$ , and observed in survey year t.\n\nThe text discusses empirical methods used to identify the effects of the Food Stamps Program, focusing on event-study, linear spline, and difference-in-differences (DD) models. It details the econometric specifications and identifying assumptions, including controls for fixed effects and county-level covariates."
  },
  {
    "qid": "econ-empirical-913-3-0-1",
    "question": "2) Using queuing theory, derive the expected waiting time \\( W \\) for elective surgery under a monopolistic provider model. Assume arrivals follow a Poisson process with rate \\( \\lambda \\) and service times are exponentially distributed with rate \\( \\mu \\).",
    "gold_answer": "1. Arrival rate \\( \\lambda \\) and service rate \\( \\mu \\) define utilization \\( \\rho = \\lambda / \\mu \\).  \n2. For \\( \\rho < 1 \\), steady-state waiting time \\( W = \\frac{\\rho}{\\mu (1 - \\rho)} \\).  \n3. Incorporate provider profit maximization: \\( \\max_{\\mu} \\pi = p\\lambda - c(\\mu) \\), where \\( c(\\mu) \\) is cost function.  \n4. Solve \\( \\partial \\pi / \\partial \\mu = 0 \\) for optimal \\( \\mu^* \\).",
    "question_context": "Balabanova, D., Financing of the health care system of Bulgaria - models and strategies, Ph.D. Feb. 2001 London – School of Hygiene & Tropical Medicine (v).\nCadete Xavier, A., Waiting times and waiting lists: a theoretical and empirical analysis of the market for elective surgery, D.Phil. May 2001, York (v, vii).\nDelfino, D., Economic epidemiological analysis of tuberculosis: modelling the demographic epidemiological implications of economic growth and public health investment, D.Phil. Oct. 2001, York (v, vii).\nMushi, D., Demand and welfare in medical health care: effects of asymmetric information and user charges, D.Phil. May 2001, Oxford (ii, v).\n\nThe section covers various PhD theses on health care financing, waiting times, contracting in private markets, cost analysis, and epidemiological modeling."
  },
  {
    "qid": "econ-empirical-684-2-0-1",
    "question": "2) Explain how generalized residuals can be used to test the specification of a duration model.",
    "gold_answer": "1. Define generalized residuals as \\( r_i = \\int_0^{t_i} h(u) du \\), where \\( h(u) \\) is the estimated hazard.\n2. Under correct specification, \\( r_i \\) should follow a unit exponential distribution.\n3. Plot the cumulative hazard of \\( r_i \\) against the theoretical exponential cumulative hazard.\n4. Deviations from the 45-degree line indicate misspecification.",
    "question_context": "Computation in duration models with heterogeneity, Donald M. Waldman.,. 28 (1985) 127-34\nGeneralised residuals and heterogeneous duration models: With applications to the Weibull model. Tony Lancaster 28 (1985) 155-69\n\nDuration models are used to analyze the time until an event occurs, such as unemployment spells or contract strikes. Heterogeneity refers to unobserved differences among individuals or firms that affect the duration."
  },
  {
    "qid": "econ-empirical-1459-4-0-2",
    "question": "3) Show how the ex ante expectations $E(E_{j}\\theta_{i})^{2}$ and $E(\\theta_{i} - E_{j}\\theta_{i})^{2}$ are derived, and explain the role of $A(\\alpha)$ and $B(\\alpha)$ in the expected loss function.",
    "gold_answer": "1. For $E(E_{j}\\theta_{i})^{2}$: \n   - Sum over all partitions $i$ of the squared conditional expectations, weighted by their probabilities. \n   - Simplify using geometric series properties to arrive at $A(\\alpha) = \\frac{1}{4}\\left(1 + \\frac{\\alpha(1-\\alpha)}{1-\\alpha^{3}}\\right)\\overline{\\theta}^{2} - \\frac{1}{4}\\left(1 - \\frac{3\\alpha(1-\\alpha)}{1-\\alpha^{3}}\\right)(E_{i}\\theta_{j})^{2}$. \n2. For $E(\\theta_{i} - E_{j}\\theta_{i})^{2}$: \n   - Sum over all partitions $i$ of the squared deviations, weighted by their probabilities. \n   - Simplify to $B(\\alpha) = \\frac{1}{12}\\left(\\frac{(1-\\alpha)^{3}}{1-\\alpha^{3}}\\right)(\\overline{\\theta}^{2} + 3(E_{i}\\theta_{j})^{2})$. \n3. $A(\\alpha)$ and $B(\\alpha)$ are coefficients in the expected loss function $E L_{i}$, representing the contributions of variance and higher-order expectations to the total loss.",
    "question_context": "We first substitute in the equilibrium decisions and take the expectations. Doing this component-wise, gives us: \n\n$$\\begin{array}{l}{{\\displaystyle{E(\\theta_{i}-d_{i})^{2}=\\frac{a_{2}^{2}}{(1-b_{2}a_{2})^{2}}((b_{1})^{2}E(E_{m}\\theta_{i})^{2}+b_{2}a_{1}(b_{2}a_{1}+2b_{1})E(E_{m}\\theta_{i}-E_{n}E_{m}\\theta_{i})^{2})}}}\\\\ {{\\displaystyle{~+E\\mathrm{Var}_{m}\\theta_{i}+\\frac{1}{(1-b_{2}a_{2})^{2}}(a_{2}b_{1})^{2}E(E_{m}E_{n}\\theta_{j})^{2}}}}\\end{array}$$\n$$\\begin{array}{l}{{{\\cal E}(d_{j}-d_{i}){}^{2}=\\displaystyle\\frac{1}{(1-b_{2}a_{2})^{2}}(b_{1}a_{1})^{2}({\\cal E}(E_{m}E_{n}\\theta_{j})^{2}+{\\cal E}(E_{n}E_{m}\\theta_{i})^{2})}}\\\\ {{{}~+b_{1}^{2}{\\cal E}(E_{n}\\theta_{j}-E_{m}E_{n}\\theta_{j})^{2}+a_{1}^{2}{\\cal E}(E_{m}\\theta_{i}-E_{n}E_{m}\\theta_{i})^{2},}}\\end{array}$$\n$$\\begin{array}{r l}&{\\frac{k_{i}b_{1}^{2}((1-r_{i})a_{2}^{2}+r_{i}a_{1}^{2})}{(1-b_{2}a_{2})^{2}}E((E_{n}E_{m}\\theta_{i})^{2}+(E_{m}E_{n}\\theta_{j})^{2})+k_{i}(1-r_{i})\\mathrm{Var}_{m}\\theta_{i}}\\\\ &{\\quad+k_{i}((1-r_{i})a_{2}^{2}+r_{i}a_{1}^{2})E(E_{m}\\theta_{i}-E_{n}E_{m}\\theta_{i})^{2}+k_{i}r_{i}b_{1}^{2}E(E_{n}\\theta_{j}-E_{m}E_{n}\\theta_{j})^{2}.}\\end{array}$$\n$$\\left(\\frac{\\overline{{\\theta}}-E_{i}\\theta_{j}}{2\\overline{{\\theta}}}\\right)\\alpha^{k-1}(1-\\alpha)\\mathrm{for}\\theta_{i}>E_{i}\\theta_{j}\\mathrm{and}\\left(\\frac{E_{i}\\theta_{j}+\\overline{{\\theta}}}{2\\overline{{\\theta}}}\\right)\\alpha^{k-1}(1-\\alpha)\\mathrm{for}\\theta_{i}<E_{i}\\theta_{j},$$\n$$\\theta_{k}=E_{i}\\theta_{j}+\\alpha^{k-1}({\\overline{{\\theta}}}-E_{i}\\theta_{j})\\quad{\\mathrm{and}}\\quad\\theta_{k}=E_{i}\\theta_{j}-\\alpha(\\varphi_{i})^{k-1}({\\overline{{\\theta}}}+E_{i}\\theta_{j}),$$\n$$\\begin{array}{r}{E_{j}(\\theta_{i}|m_{k})=E_{i}\\theta_{j}+\\displaystyle\\frac{1}{2}\\alpha^{k-1}(1+\\alpha)(\\overline{{\\theta}}-E_{i}\\theta_{j})\\qquad\\theta_{i}>E_{i}\\theta_{j},}\\\\ {E_{j}(\\theta_{i}|m_{-k})=E_{i}\\theta_{j}-\\displaystyle\\frac{1}{2}\\alpha^{k-1}(1+\\alpha)(\\overline{{\\theta}}+E_{i}\\theta_{j})\\qquad\\theta_{i}<E_{i}\\theta_{j}.}\\end{array}$$\n$$\\begin{array}{l}{{\\displaystyle{E(E_{j}\\theta_{i})^{2}=\\sum_{i=1}^{\\infty}\\left(\\frac{\\overline{{{\\theta}}}-E_{i}\\theta_{j}}{2\\overline{{{\\theta}}}}\\right)\\alpha^{i-1}(1-\\alpha)\\left(E_{i}\\theta_{j}+\\frac{1}{2}\\alpha^{i-1}(1+\\alpha)(\\overline{{{\\theta}}}-E_{i}\\theta_{j})\\right)^{2}}}}\\\\ {{\\displaystyle~+\\sum_{i=1}^{\\infty}\\left(\\frac{\\overline{{{\\theta}}}+E_{i}\\theta_{j}}{2\\overline{{{\\theta}}}}\\right)\\alpha^{i-1}(1-\\alpha)\\left(E_{i}\\theta_{j}-\\frac{1}{2}\\alpha^{i-1}(1+\\alpha)(\\overline{{{\\theta}}}+E_{i}\\theta_{j})\\right)^{2}}}\\\\ {{\\displaystyle~=\\frac{1}{4}\\left(\\left(1+\\frac{\\alpha(1-\\alpha)}{(1-\\alpha^{3})}\\right)\\overline{{{\\theta}}}^{2}-\\left(1-\\frac{3\\alpha(1-\\alpha)}{(1-\\alpha^{3})}\\right)(E_{i}\\theta_{j})^{2}\\right)=A(\\alpha)}}\\end{array}$$\n$$\\begin{array}{c}{{E(\\theta_{i}-E_{j}\\theta_{i})^{2}=\\displaystyle\\sum_{i=1}^{\\infty}\\left(\\frac{\\overline{{{\\theta}}}-E_{i}\\theta_{j}}{2\\overline{{{\\theta}}}}\\right)\\alpha^{i-1}(1-\\alpha)\\frac{1}{12}(\\alpha^{i-1}(1-\\alpha)(\\overline{{{\\theta}}}-E_{i}\\theta_{j}))^{2}}}\\\\ {{+\\displaystyle\\sum_{i=1}^{\\infty}\\left(\\frac{\\overline{{{\\theta}}}+E_{i}\\theta_{j}}{2\\overline{{{\\theta}}}}\\right)\\alpha^{i-1}(1-\\alpha)\\frac{1}{12}(\\alpha^{i-1}(1+\\alpha)(\\overline{{{\\theta}}}+E_{i}\\theta_{j}))^{2}}}\\\\ {{=\\displaystyle\\frac{1}{12}\\left(\\frac{(1-\\alpha)^{3}}{(1-\\alpha^{3})}\\right)(\\overline{{{\\theta}}}^{2}+3(E_{i}\\theta_{j})^{2}).}}\\end{array}$$\n$$A(\\varphi)={\\frac{1}{4}}\\left(1+{\\frac{\\alpha(1-\\alpha)}{(1-\\alpha^{3})}}\\right){\\mathrm{~and~}}B(\\varphi)={\\frac{1}{12}}\\left({\\frac{(1-\\alpha)^{3}}{(1-\\alpha^{3})}}\\right).$$\n$$E L_{i}=\\Lambda_{i}(A(\\varphi_{i}){\\overline{{\\theta}}}_{i}^{2}+A(\\varphi_{j}){\\overline{{\\theta}}}_{j}^{2})+\\Lambda_{i i}B(\\varphi_{i}){\\overline{{\\theta}}}_{i}^{2}+\\Lambda_{i j}B(\\varphi_{j}){\\overline{{\\theta}}}_{j}^{2}.$$\n$$B(\\varphi)\\equiv\\frac{(1-\\alpha(\\varphi))^{3}}{12(1-\\alpha(\\varphi)^{3})}=\\frac{4\\left(1+\\sqrt{1+\\varphi}\\right)^{2}}{12\\left(4\\left(1+\\sqrt{1+\\varphi}\\right)^{2}+3\\varphi^{2}+6\\varphi\\left(1+\\sqrt{1+\\varphi}\\right)\\right)}=\\frac{1}{3(4+3\\varphi)}.$$\n$$E L_{i}=\\frac{\\Lambda_{i}}{3}(\\overline{{{\\theta}}}_{i}^{2}+\\overline{{{\\theta}}}_{j}^{2})+\\frac{(\\Lambda_{i i}-\\Lambda_{i})}{3(4+3\\varphi)}\\overline{{{\\theta}}}_{i}^{2}+\\frac{(\\Lambda_{i j}-\\Lambda_{i})}{3(4+3\\varphi)}\\overline{{{\\theta}}}_{j}^{2}.$$\n\nThis section presents a detailed mathematical model for equilibrium decisions and expectations in a communication equilibrium framework, incorporating probabilities, cutoffs, conditional expectations, and variances."
  },
  {
    "qid": "econ-empirical-1273-0-2-3",
    "question": "4) Propose a test statistic based on recursive residuals to detect systematic over- or under-prediction and derive its distribution under the null hypothesis.",
    "gold_answer": "A runs test can detect systematic prediction errors. Under \\( H_0 \\), the number of runs (sequences of positive or negative residuals) follows a known distribution, allowing testing for non-random patterns indicative of instability.",
    "question_context": "In section 3, we examine the properties of the recursive residuals and related series when parameter instability is present. Among other things, a number of schemes involving fixed and random coefficients are studied, standardized first differences of the recursive estimates are rationalized as an additional diagnostic instrument and the link between parameter instability and specification errors is explicited.\nThe problem is to quantify the ‘statistical significance’ of such patterns. For that purpose, a number of test statistics are described, bearing on the locational (systematic over- or under-prediction) and heteroskedacticity characteristics of these series.\n\nThis section examines the behavior of recursive residuals when parameter instability is present, providing insights into their diagnostic capabilities and limitations."
  },
  {
    "qid": "econ-empirical-632-2-0-0",
    "question": "1) Derive the expression for the lower bound $I_{*}$ under the symmetry assumption for $\\pmb{g}$ and explain why it remains nonsingular even when the location parameter is included in $\\pmb{\\theta}$.",
    "gold_answer": "1. Start with the orthogonality condition under symmetry: $$ \\int_{-\\infty}^{\\infty}d H_{v}(v)G^{-1}(v)[1-G(v)]^{-1}g(v)J(v,\\beta)\\left\\{K(v)-E\\left(\\frac{\\partial V}{\\partial\\underline{\\theta}}\\Big|v\\right)\\right\\}=0. $$\n2. Use the properties of symmetric $\\pmb{g}$ and odd $J(v,\\beta)$ to simplify the integral.\n3. Substitute $K(v)$ from (3.25) to derive the matrix $I_{*}$.\n4. The nonsingularity arises because the symmetry condition ensures that the information about the location parameter is not redundant, maintaining the full rank of $I_{*}$.",
    "question_context": "If $\\pmb{g}$ is known to be symmetric, i.e., ${\\pmb g}({\\pmb u})={\\pmb g}(-{\\pmb u})$ , it should be possible to improve the estimates $\\underline{{\\hat{\\theta}}}_{n}$ . The lower bound $\\boldsymbol{\\underbar{I}}_{*}$ can again be obtained by inspection from our previous results. The location parameter is identified under the symmetry assumption so let $a\\equiv\\theta_{k+1}$.\nThe set $\\mathcal{B}_{g}$ now consists of all functions $\\beta\\in L^{2}(\\nu)$ such that $\\langle{g^{1/2},\\beta}\\rangle_{\\nu}{=}0$ and $\\beta(u)=\\beta(-u)$ . The integral $J(v,\\beta)$ defined by (3.11) must then be an odd function of $v.$ Comparing our previous expression for $\\underline{{\\alpha}}^{*}$ (3.15) with the expression for $A_{g}\\beta$ (3.9), we see that the term $E(\\partial V/\\partial\\underline{{\\theta}}|v)$ in (3.15) has to be replaced by an odd function of $v.$ Call this function $\\underline{{K}}(v)$ . Substituting this modified $\\underline{{\\alpha}}^{*}$ into the orthogonality condition (2.3) gives $$ \\int_{-\\infty}^{\\infty}d H_{v}\\left(v\\right)G^{-1}(v)[1-G(v)]^{-1}g(v)J(v,\\beta)\\bigg\\{K(v)-E\\bigg(\\frac{\\partial V}{\\partial\\underline{{\\theta}}}\\bigg|v\\bigg)\\bigg\\}=0. $$\nThe lower bound is now the inverse of the $(k+1)\\times(k+1)$ matrix $$ I_{*}=\\int_{-\\infty}^{\\infty}d H_{V}(v)\\frac{[g(v)]^{2}}{G(v)[1-G(v)]}\\left\\{E\\left[\\frac{\\partial V}{\\partial\\theta}\\left(\\frac{\\partial V}{\\partial\\theta}\\right)^{T}\\Bigg|v\\right]-K(v)[K(v)]^{T}\\right\\}. $$ This is in general nonsingular, even though the location parameter is included inthevector $\\pmb{\\theta}.$ The necessary conditions for adaptive estimation depend on whether the location parameter is included in the parameters to be estimated.\n\nThe text discusses the improvement of estimates under the assumption of symmetric distribution for the error term in a censored linear regression model. It introduces mathematical formulations to derive lower bounds for the estimates and explores conditions for adaptive estimation."
  },
  {
    "qid": "econ-empirical-65-5-2-1",
    "question": "6) Explain why a loan sold for its full face value contributes zero to the debt capacity of an overly liquid firm, using Corollary 3.",
    "gold_answer": "1. **Full face value**: Selling the loan for $P_{1}^{P} + P_{2}^{P}$ means its transformation value equals repayments.  \n2. **Constraint impact**: The loan does not reduce the transformation constraint ($\\Delta B^{F} = 0$).  \n3. **Corollary 3**: Directly states that such loans add no debt capacity.",
    "question_context": "The incremental debt capacity the loan adds to the firm is $P_{1}^{P} + P_{2}^{P} = a_{F}^{P}C_{2} + \\alpha^{P}d_{2}^{P} > a_{I}^{P}C_{2} + \\alpha^{P}d_{2}^{P}$, where the last term is the stand-alone debt capacity of the project.\nWhen the loan can be sold for its full face value, the transformation value of the loan is exactly equal to the repayments due on it, so the loan does not reduce the transformation constraint at all.\n\nThis section demonstrates how an overly liquid firm can enhance debt capacity by financing projects in-house, compared to stand-alone project financing."
  },
  {
    "qid": "econ-empirical-498-3-0-0",
    "question": "1) Using the definition of structural rationality, formally derive the condition $U_{i}(s_{i},p^{k})\\geq U_{i}(t_{i}^{\\prime},p^{k})$ for all $t_{i}^{\\prime}\\in S_{i}$ given a perturbation $(p^{k})$ of $\\mu$.",
    "gold_answer": "1. **Definition of Structural Rationality**: A strategy $s_{i}$ is structurally rational given $\\mu$ if there exists a perturbation $(p^{k})$ of $\\mu$ such that for all $k$, $U_{i}(s_{i},p^{k})\\geq U_{i}(t_{i}^{\\prime},p^{k})$ for all $t_{i}^{\\prime}\\in S_{i}$.  \n2. **Perturbation Condition**: By Definition 3, $(p^{k})$ is a perturbation of $\\mu$ if $p^{k}(S_{-i}(I))>0$ for all $I\\in\\mathbb{Z}_{i}$ and $p^{k}(\\cdot|S_{-i}(I))\\to\\mu(\\cdot|I)$.  \n3. **Derivation**: For $s_{i}$ to be structurally rational, the inequality $U_{i}(s_{i},p^{k})\\geq U_{i}(t_{i}^{\\prime},p^{k})$ must hold for all $t_{i}^{\\prime}\\in S_{i}$ under the perturbation $(p^{k})$.",
    "question_context": "Suppose that $s_{i}\\in S_{i}$ is structurally rational given $\\mu$ . Fix $I\\in\\mathbb{Z}_{i}$ with $s_{i}\\in S_{i}(I)$ and $r_{i}\\in S_{i}(I)$ . By strategic independence (cf. Section 2), there is $t_{i}\\in S_{i}$ such that $U_{i}(t_{i},s_{-i})=$ $U_{i}(r_{i},s_{-i})$ for $s_{-i}\\in S_{-i}(I)$ , and $U_{i}(t_{i},s_{-i})=U_{i}(s_{i},s_{-i})$ for $s_{-i}\\notin S_{-i}(I)$ . By Definition 3, there is a perturbation $(p^{k})$ of $\\mu$ such that $U_{i}(s_{i},p^{k})\\geq U_{i}(t_{i}^{\\prime},p^{k})$ for all $t_{i}^{\\prime}\\in S_{i}$ .\nA relevant tie for player $i$ is a tuple $(I,s_{i},t_{i},t_{-i})$ such that $I\\in\\mathcal{Z}_{i}\\cup\\{\\phi\\}$ , $s_{i}$ $,t_{i}\\in S_{i}(I)$ , $t_{-i}\\in S_{-i}(I)$ , $\\zeta(s_{i},t_{-i})\\:\\neq\\:\\zeta(t_{i},t_{-i})$ , and $U_{i}(s_{i},t_{-i})=U_{i}(t_{i},t_{-i})$ . That is, starting from $I$ , if coplayers move according to $t_{-i}$ , then i’s strategies $s_{i}$ and $t_{i}$ reach distinct terminal histories, but yield the same payoff.\nTHEOREM 3: Fix $i\\in N$ and $\\mu\\in\\Delta(S_{-i},\\mathcal{T}_{i})$ . If $s_{i}\\in S_{i}$ is weakly sequentially rational given $\\mu$ , and there is no relevant tie or nontrivial redundance for $i$ , then $s_{i}$ is structurally rational given $\\mu$ .\nFor every $s_{-i}\\in S_{-i}$ , let $h(s_{-i})\\in H$ be the longest history $h$ such that $h\\leq\\zeta{\\bigl(}s_{i},s_{-i}{\\bigr)}$ and $h\\leq\\zeta(t_{i},s_{-i})$ for all $t_{i}\\in\\ensuremath{\\mathrm{supp}}\\sigma_{i}$ . If $h(s_{-i})=\\zeta(s_{i},s_{-i})$ , then also $h(s_{i})=\\zeta(t_{i},s_{-i})$ for all $t_{i}\\in\\ensuremath{\\mathrm{supp}}\\sigma_{i}$ , because terminal histories are not ranked by the prefix relation; conversely, for the same reason, if $h(s_{-i})=\\zeta(t_{i},s_{-i})$ for some $t_{i}\\in\\ensuremath{\\mathrm{supp}}\\sigma_{i}$ , then in fact $h(s_{-i})=$ $\\zeta(t_{i}^{\\prime},s_{-i})$ for all $t_{i}^{\\prime}\\in\\ensuremath{\\mathrm{supp}}\\sigma_{i}$ , and $h(s_{-i})=\\zeta(s_{i},s_{-i})$ .\n\nThis section presents the proof of Theorem 1, which establishes the equivalence between structural and sequential rationality under certain conditions. The proof involves detailed mathematical derivations and leverages concepts such as strategic independence, perturbations of probability measures, and the absence of relevant ties or nontrivial redundances."
  },
  {
    "qid": "econ-empirical-1639-0-0-3",
    "question": "4) Discuss the robustness of the joint search effect to alternative assumptions about consumer search behavior. For example, how would the results change if consumers could not mix-and-match purchases across firms?",
    "gold_answer": "1. **No Mix-and-Match**: If consumers must buy all products from one firm, the joint search effect intensifies because demand for all products is fully linked.\\n2. **Pricing Implications**: Firms would lower prices further to attract consumers, as losing one product sale means losing all sales. The equilibrium price would satisfy \\(p = c - \\frac{q}{\\frac{\\partial q}{\\partial p}}\\), where \\(q\\) is the bundle demand.\\n3. **Empirical Prediction**: Prices would be lower than in the mix-and-match case, but the qualitative result of prices decreasing with search costs may still hold.",
    "question_context": "Consumers often look for several products on a given shopping trip. For example, during ordinary grocery shopping they often buy food, drinks, and household products; in high street shopping they may purchase clothes, shoes, and other goods; in the Christmas season they look for several presents.\nThe search process is sequential: by incurring a search cost, a consumer can visit a firm and learn all product and price information. In particular, the cost of search is incurred jointly for all products (i.e., there are economies of scale in search), and the consumer does not need to buy all products from the same firm (i.e., they can mix and match after sampling two firms).\nOne key observation is that if a firm lowers one product's price, this will induce more consumers who are visiting it to terminate their search and buy some other products as well. That is, a reduction of one product's price also boosts the demand for the firm's other products. I term this the joint search effect.\nDue to this joint search effect, firms in a multiproduct search environment have an extra incentive to lower their prices compared to the single-product case, and market prices can decline with search costs, in contrast to the prediction from the standard single-product search model.\n\nThis paper presents a sequential search model where consumers look for several products from multiproduct firms. Multiproduct search can significantly influence firms' pricing decisions. For example, it can make market prices decrease with search costs."
  },
  {
    "qid": "econ-empirical-1274-0-1-0",
    "question": "5) Using the nonlinear relationship between ATM share and the probability of surcharging, estimate the marginal effect of ATM share on the probability of surcharging for banks with shares between 10-20%, 20-40%, and 60-100%.",
    "gold_answer": "1. For ATM share 10-20%: Marginal effect \\( \\approx 0.6791 \\) (coefficient from table 4, panel A).\n2. For ATM share 20-40%: Marginal effect \\( \\approx 0.7569 \\).\n3. For ATM share 60-100%: Marginal effect \\( \\approx 1.3937 \\).\n4. The marginal effect is highest for banks with ATM shares >60%, indicating a nonlinear relationship.",
    "question_context": "We find that the probability of surcharging increases with both the institution's share of market ATMs and the length of time since surcharging was first permitted in the state, and decreases with increasing number of ATMs per square mile in the market.\nThe coefficient magnitudes suggest that banks with ATM shares between 10% and 60% are roughly the same in their tendency to surcharge, and are more likely to surcharge than banks with shares of less than 10%. Furthermore, banks with ATM shares greater than 60% exhibit a considerably greater likelihood of surcharging.\n\nThe paper examines the probability of surcharging based on institution- and market-specific characteristics, including ATM share, ATM density, and population demographics. Robustness checks are conducted to assess the sensitivity of the results to alternative specifications and market definitions."
  },
  {
    "qid": "econ-empirical-1714-0-0-0",
    "question": "1) Formally derive the conditions under which worker characteristic prices equalize across spatially distinct labor markets, assuming no persistent stochastic disturbances.",
    "gold_answer": "1. **Model Setup**: Let wages be determined as a vector product of characteristics and prices: \\( W_i = \\sum_{k} \\alpha_k X_{ik} + \\epsilon_i \\), where \\( X_{ik} \\) represents worker characteristics and \\( \\alpha_k \\) their prices.  \n2. **Competitive Equilibrium**: In the absence of stochastic shocks, competitive forces drive \\( \\alpha_k \\) to equalize across regions: \\( \\alpha_k^A = \\alpha_k^B \\) for regions A and B. \n3. **Human Capital Invariance**: Returns to human capital (e.g., education, experience) are invariant: \\( \\alpha_{HC}^A = \\alpha_{HC}^B \\). \n4. **Industry/Occupation Prices**: Prices for industry/occupation characteristics may diverge due to localized factors: \\( \\alpha_{IO}^A \\neq \\alpha_{IO}^B \\).",
    "question_context": "The model of earnings determination used implies that while job characteristic prices may be subject to stochastic shocks or industry-specific factors, and hence may differ between regions, the returns to human capital characteristics will be regionally invariant.\nWe suggest that the appropriate test for structural variation incorporates some combination of the two preceding lines of research. Treating individual wages as vector products of characteristics and their respective prices permits a distinction to be drawn between characteristic prices that are dominated by competitive long-run convergence forces, and those that may be subject to either persistent short-run stochastic shocks or permanent industry-specific factors, as suggested by efficiency wage theories.\nThe purpose of this paper is to test whether characteristic prices which are not anticipated to be stochastically dominated or reflect industry-specific factors actually are equalized across labor markets.\n\nThe paper examines regional wage differentials by testing for structural variation in wage structures across regions. It combines two lines of research: structural variation and compensating variation. The model suggests that while job characteristic prices may differ between regions due to stochastic shocks or industry-specific factors, the returns to human capital characteristics will be regionally invariant."
  },
  {
    "qid": "econ-empirical-458-3-0-0",
    "question": "1) Using a game-theoretic framework, explain how the introduction of a leniency program affects the static Nash equilibrium price in a cartel setting. Derive the conditions under which the leniency program leads to a reduction in cartel activity.",
    "gold_answer": "1. **Model Setup**: Consider a repeated Bertrand oligopoly game where firms can collude to set prices above the Nash equilibrium. The leniency program reduces fines for the first firm to report collusion.\n2. **Impact on Nash Equilibrium**: The leniency program increases the incentive to deviate from collusion, as the deviating firm can avoid fines. This shifts the equilibrium towards the competitive price.\n3. **Reduction in Cartel Activity**: The probability of cartel formation decreases if the expected payoff from collusion, net of potential fines and leniency benefits, is lower than the competitive payoff. Mathematically, this occurs when:\n   \\( \\pi_{collusion} - (1 - p)F + pL < \\pi_{Nash} \\),\n   where \\( \\pi_{collusion} \\) is the collusive profit, \\( F \\) is the fine, \\( p \\) is the probability of detection, and \\( L \\) is the leniency benefit.",
    "question_context": "The study explores the effects of moderate corporate leniency programs on pricing and cartel activity. We find that introduction of a leniency program leads to lower prices for two reasons. First, putting a leniency program in place leads to a reduction in cartel activity. A notable difference from the other treatments is that in the Leniency treatment, about $10\\%$ of the subjects refuses to discuss prices throughout the experiment. Second, those cartels that are established are less successful in charging prices above the static Nash equilibrium price. This is not caused by differences in behavior in the communication phase but by more frequent and more severe undercutting of the agreed-upon price in the market phase. This behavior in turn is related to the shorter cartel lifetime in the Leniency treatment; the probability that the market price exceeds the competitive level for any number of consecutive periods is significantly lower in Leniency.\nThe data furthermore indicate that defectors use the leniency program to receive protection from fines.\nAs noted by Spagnolo (forthcoming), “more empirical and experimental evidence would be extremely welcome on all aspects of leniency and whistleblower programs.” This article responds to this wish, but there are still a number of important theoretical results that remain to be scrutinized. For example, although the leniency program we considered here complies with the current practice in most jurisdictions, Spagnolo (2004) stresses that the most effective leniency programs are those that give a (more than) full fine reduction to the first applicant and none to subsequent ones. Likewise, experiments that incorporate endogenous detection probabilities would also be most welcome (e.g., Harrington, forthcoming; Chen and Harrington, 2007).\n\nThe study explores the effects of moderate corporate leniency programs on pricing and cartel activity, highlighting the reduction in cartel activity and lower prices due to leniency programs."
  },
  {
    "qid": "econ-empirical-1686-1-0-3",
    "question": "4) Using Table I, compute the ratio of current assets to total liabilities for 2012 and interpret its financial implications.",
    "gold_answer": "1. **Extract values**: Current assets (2012) = $\\$2,711,117$, Total liabilities (2012) = $\\$2,715,139$.  \n2. **Calculate ratio**: $2,711,117 / 2,715,139 = 0.9985$.  \n3. **Implication**: A ratio close to 1 indicates the Society barely covers liabilities with current assets, signaling liquidity risk.",
    "question_context": "TOTAL ASSETS OF THE ECONOMETRIC SOCIETY StOOd at $\\$2.71$ million at the end of 2012 compared to $\\$2.65$ million in 2011. The Society's net worth stood at $\\$1.88$ million, an increase of $8.2\\%$ over the year.\nNet operating income from the Society's publishing and meetings activity was in deficit in 2012, at $-\\$136,211$ and the deficit is expected to increase in 2013, with expected 2013 net operating income at $-\\$197,500$.\nThe static nature of the operating income in the face of increases in subscription and membership rates reflect the declining trend in library subscriptions and suggest limited scope for increasing revenue through higher subscription rates alone.\n\nThe Econometric Society's financial performance in 2012 is analyzed, highlighting changes in total assets, net worth, and operational challenges."
  },
  {
    "qid": "econ-empirical-1762-2-0-2",
    "question": "3) Construct a counterexample showing that when $Y$ is not CCB, the minimum demand rule fails to satisfy RGSP.",
    "gold_answer": "Counterexample construction:\n1. Let $Y = \\mathbb{R}_{+}$ (not CCB) and $n=2$.\n2. Define $Y^{*} = \\{1, 1/2, 1/3, \\dots\\}$ with $\\inf Y^{*} = 0 \\notin Y^{*}$.\n3. Let $\\tilde{R}_{1}, \\tilde{R}_{2}$ have $\\bar{p}(\\tilde{R}_{1}) = \\bar{p}(\\tilde{R}_{2}) = 1$.\n4. For $i=1,2$, define $\\mathcal{R}_{i}^{*} = \\{R_{i} \\in \\mathcal{R}_{i}^{s} \\mid \\bar{p}(R_{i}) \\in Y^{*}, \\bar{p}(R_{i}) < 1, \\exists y \\in (0, \\epsilon) \\text{ with } 1 P_{i} y\\}$.\n5. For some $R_{1}^{*} \\in \\mathcal{R}_{1}^{*}$ with $\\bar{p}(R_{1}^{*}) = 1/k$, there exists $R_{2}^{*}$ with $\\bar{p}(R_{2}^{*}) = 1/(k+1) < 1/k$.\n6. Then $f_{y}^{\\mathrm{ME}}(R^{*}) = 1/(k+1)$ but $f_{y}^{\\mathrm{ME}}(\\tilde{R}_{1}, R_{2}^{*}) = 1 P_{1}^{*} 1/(k+1)$.\n7. Thus, agent 1 profits from misreporting, violating RGSP.",
    "question_context": "Definition 3.1 (Efficiency). A rule $f$ is efficient if for all $R$ , there exists no $y\\in X$ such that \n$$\n\\begin{array}{r l}{y P_{i}f(R)}&{{}{\\mathrm{for~some~}}i\\in N}\\ {y R_{j}f(R)}&{{}\\forall j\\in N.}\\end{array}\n$$\nDefinition 3.2 (Nonbossiness). A rule $f$ is nonbossy if whenever $f_{i}(\\tilde{R}_{i},R_{-i})=f_{i}(R)$ for some $i\\in N$ , $\\tilde{R}_{i}\\in\\mathcal{R}_{i}$ and $R\\in{\\mathcal{R}}$ we have that $f(\\tilde{R}_{i},R_{-i})=f(R)$ .\nThe public good $y\\in Y$ is produced at cost $C(y)$ . We assume that $C(y)$ is (strictly) increasing, convex, and $C(0)=0$ . For any convex subset $Y^{\\prime}\\subseteq Y$ , we assume that $C(y)$ is differentiable and has strictly positive derivative at each $y\\in Y^{\\prime}$ .\nThe set of feasible outcomes is $\\begin{array}{r}{X=\\{(t_{1},t_{2},\\dots,t_{n},y)\\mid y\\in Y\\&\\sum_{i=1}^{n}t_{i}=C(y)\\}.}\\end{array}$ . A rule $f=(f_{1},\\ldots,f_{n},f_{y})$ is a function mapping $\\mathcal{R}$ to $X$ .\nThe minimum demand rule with respect to the equal cost-sharing scheme is $f^{\\mathrm{ME}}(R)=(C(f_{y}^{\\mathrm{ME}}(R))/n,\\dots,C(f_{y}^{\\mathrm{ME}}(R))/n,f_{y}^{\\mathrm{ME}}(R))$ for all $R\\in{\\mathcal{R}}$ .\nDefinition 3.3. Preferences $R_{i}$ are weakly single-peaked if they satisfy the following: \n• $p(R_{i})$ has at most two elements and $Y\\cap(\\underline{{p}}(R_{i}),\\bar{p}(R_{i}))=\\emptyset.$ . \n• For all $y$ , $y^{\\prime}\\in Y$ , $y^{\\prime}<y\\leq\\underline{{p}}(R_{i})$ implies $y P_{i}y^{\\prime}$ . \n• For all $y$ , $y^{\\prime}\\in Y$ , $\\bar{p}(R_{i})\\leq y<y^{\\prime}$ implies $y P_{i}y^{\\prime}$ .\nLemma 3.4. If $u_{i}^{R_{i}}(y,t_{i})$ is continuous, strictly quasi-concave, increasing in y, and decreasing in $t_{i}$ , and $C(y)$ is convex on $Y$ , then $R_{i}$ is weakly single-peaked on Y . In addition, if $Y$ is convex, then $R_{i}$ is single-peaked.\nDefinition 3.5. The set $Y$ is completely closed below (CCB) if inf $Y^{\\prime}\\in Y^{\\prime}$ for all $Y^{\\prime}\\subseteq Y$ .\nProposition 3.6. For $\\mathcal{R}=\\mathcal{R}^{w}$ , the minimum demand rule with respect to the equal costsharing scheme satisfies RGSP if and only if $Y$ is CCB.\n\nThis section introduces concepts of efficiency, nonbossiness, and the public good cost-sharing problem, focusing on the minimum demand rule with respect to the equal cost-sharing scheme. It explores conditions under which this rule satisfies robust group strategy-proofness (RGSP)."
  },
  {
    "qid": "econ-empirical-448-2-0-2",
    "question": "3) Derive the posterior class membership probabilities $w_{i c}$ for each individual $i$ and class $c$, and explain how these probabilities are used in the EM algorithm for parameter estimation.",
    "gold_answer": "The posterior class membership probabilities $w_{i c}$ are derived using Bayes' theorem and are central to the EM algorithm for parameter estimation.\n\n1. **Posterior Probability**: The posterior probability that individual $i$ belongs to class $c$ is given by:\n   $$\n   w_{i c} = \\mathrm{Pr}(\\mathbf{v}_{i} = \\mathbf{v}_{c}|a_{i1}, \\dots, a_{i T}, Y_{i1}, \\mathbf{X}_{i1}, \\dots, Y_{i T}, \\mathbf{X}_{i T}) = \\frac{\\omega_{i c} \\mathrm{Pr}(a_{i1}, \\dots, a_{i T}|\\mathbf{v}_{i} = \\mathbf{v}_{c}, Y_{i1}, \\mathbf{X}_{i1}, \\dots, Y_{i T}, \\mathbf{X}_{i T})}{\\mathrm{Pr}(a_{i1}, \\dots, a_{i T}|Y_{i1}, \\mathbf{X}_{i1}, \\dots, Y_{i T}, \\mathbf{X}_{i T})}.\n   $$\n\n2. **EM Algorithm Steps**:\n   - **E-step**: Compute the posterior probabilities $w_{i c}$ using current parameter estimates.\n   - **M-step**: Update the parameter estimates by maximizing the expected log-likelihood, where the expectation is taken with respect to the posterior probabilities $w_{i c}$.\n\n3. **Fuzzy Classification**: The posterior probabilities $w_{i c}$ provide a fuzzy classification of individuals into classes, reflecting the uncertainty in class membership. These probabilities are used to weight the contributions of each individual to the likelihood of each class during the M-step.",
    "question_context": "Given that we do not observe utility directly, but rather infer it from a satisfaction variable, we are potentially faced with two types of heterogeneity. Interpreting subjective responses requires (i) associating utility to observable characteristics, and (ii) relating discrete verbal satisfaction judgements to latent continuous utility.\nAn explanatory variable in the left-hand side of the Figure, such as income, is correlated with (unobservable) utility. Individual heterogeneity likely makes an appearance at this point, in the sense that the utility function is not the same across individuals: both intercept and slope heterogeneity can play a role.\nThe model described below, which is an extension of the standard ordered probit, identifies this second element with intercept heterogeneity. We are not able to distinguish empirically between heterogeneity in the utility function (translating income into utility) and heterogeneity in the expression function (turning utility into reported well-being).\nConsider an agent $i$ who reports her well-being at time $t$ using $P$ different ‘naturally’ ordered labels such as excellent, very good, good etc. Denote $a_{i t}$ her answer, which belongs to the ordered set of labels $L=\\{L_{1},L_{2},...,L_{P}\\}$ . The most common way to model this choice assumes that there exists an underlying continuous utility function $U_{i t}^{*}$ and $P+1$ ordered individual threshold parameters $s_{i}^{0}=-{\\infty},\\dot{s_{i}},\\ldots,s_{i}^{p},\\ldots,\\stackrel{\\ldots}{s_{i}^{P}}=+{\\infty}$ such that: $a_{i t}=L_{\\rho}\\Leftrightarrow s_{i}^{\\rho-1}\\leq U_{i t}^{*}<s_{i}^{\\rho}$.\nHere we model utility as: $U_{i t}^{*}=\\alpha_{i}Y_{i t}+\\beta\\mathbf{X}_{i t}+\\varepsilon_{i t}$ where $Y_{i t}$ is log income, $\\mathbf{X}_{i t}$ is a vector of labour market status variables and wave dummies, and $\\varepsilon_{i t}$ is a shock which is independent across individuals and time, and is distributed standard normal.\nWe use a finite mixture approach to model heterogeneity. That is, we assume that the parameter vector $\\mathbf{\\dot{v}}_{i}=(\\alpha_{i},s_{i}^{1},\\ldots,s_{i}^{p},\\ldots,s_{i}^{p-1})$ is distributed over a finite number of points $C_{\\bullet}$ any given individual $i$ in the sample belongs to one of $C$ classes, where each class $c$ is defined by a common value $\\mathbf{v}_{c}$ of the vector $\\mathbf{v}_{i}.$\nThe joint specification of (2) and (4) requires some comment. Note first that only time-varying regressors are included in the right-hand side of (2), while only time-invariant regressors appear as determinants of class membership in (4). Equation (4) thus imposes a simple parametric structure on the correlation between these latter variables and class membership, which allows all moments of the distribution of unobserved individual heterogeneity $\\mathbf{V}_{i}$ to be affected by observed fixed individual characteristics.\nGiven independence between $\\mathbf{v}_{i}$ and $\\varepsilon_{i t},$ the distribution of $a_{i t}$ conditional on $Y_{i t},\\mathbf{X}_{i t}$ and $\\mathbf{v}_{i}$ is the standard ordered probit. Denoting the standard normal c.d.f. by $\\Phi$ , the parameters $\\mathbf{v}_{c},\\mathbf{\\upbeta}$ and $\\gamma_{c}$ are obtained via the maximisation of the following log-likelihood: $\\sum_{i}\\log\\left[\\sum_{c=1}^{C}\\left(\\frac{\\exp(\\gamma_{c}Z_{i})}{\\sum_{c=1}^{C}\\exp(\\gamma_{c}Z_{i})}\\prod_{t=1}^{T}\\left\\{\\prod_{p=1}^{P}\\left[\\Phi(s_{c}^{p}-\\alpha_{c}Y_{i t}-\\mathbb{B}\\mathbf{X}_{i t})-\\Phi(s_{c}^{p-1}-\\alpha_{c}Y_{i t}-\\mathbb{B}\\mathbf{X}_{i t})\\right]^{1\\left\\{a_{i}=L_{P}\\right\\}}\\right\\}\\right)\\right]$\nThe problem of theoretical identification is important: is it possible to find several sets of parameters that would fit the data equally well (i.e. produce the same likelihood)? Uebersax (1999) proposes an order condition: the number of parameters in the model should be less than the number $R$ of empirical patterns of response. Here we have $C$ slope parameters, $C-1$ probabilities $\\omega_{i c}$ (the $C$ weights sum up to 1) and $(P-1)C$ threshold parameters, making a total of $C(P+1)-1$ . There are $P$ response modalities and $T$ waves. Moreover, conditional on class membership, the probabilities of response are time-independent. Hence, $R=(P+T-1)!/$ $T!({\\cal P}\\mathrm{~-~}1)!$ . With $P=6$ and $T=3$ , we have $R=56$ , so that the order condition inequality becomes $7C-1<56$ , and the maximum number of classes we can identify is 8.\nTo select the empirical optimal number of classes, we compare information criteria such as the BIC and the AIC, and the normalised entropy criterion for 1, 2 or more points. The BIC and AIC statistics are commonly used in order to balance the gain in log-likelihood through an increase in $C$ and the loss of degrees of freedom from the greater number of parameters. The normalised entropy criterion assesses the accuracy of the classification, lower values indicating better class identification (McLachlan and Peel, 2000).\n\nThe text discusses the application of a latent class approach to model heterogeneity in well-being, focusing on an ordered dependent variable representing satisfaction with financial situation. It highlights the challenges of interpreting subjective responses and the need to account for heterogeneity in both the utility function and the expression function."
  },
  {
    "qid": "econ-empirical-62-1-0-1",
    "question": "2) In the model of incomplete information by Fudenberg and Tirole [1986], what conditions ensure the existence and uniqueness of sequential equilibrium in duopoly? How does symmetric expectations affect the exit decision?",
    "gold_answer": "1. **Conditions**: Each firm is uncertain about the rival's costs.  \n2. **Sequential Equilibrium**: Exists if firms update beliefs consistently.  \n3. **Uniqueness**: Achieved under symmetric expectations.  \n4. **Exit Decision**: The less efficient firm exits first due to higher expected costs.",
    "question_context": "Firms were perfectly informed about their competitors' costs and capacities. Reentry was not allowed after exit. Demand declined continuously and deterministically. Exit was an all-or-nothing decision. Under these four assumptions, there was a unique subgame-perfect equilibrium: the smaller of two equally efficient duopolists forced its larger rival to exit as soon as duopoly profits turn negative.\nFudenberg and Tirole [1986] examine the exit decision in an environment of incomplete information where each firm is uncertain about its rival's costs. They provide conditions for the existence and uniqueness of sequential equilibrium in duopoly. When expectations are symmetric, if exit occurs, it is the less efficient firm that leaves.\nLondregan [1986] shows that if reentry costs are positive, there is a unique subgame-perfect equilibrium in the all-or-nothing exit game with complete information: smallness continues to be an advantage during decline and, by backward induction, also during the growth phase.\nFine and Li [1986] consider a market that is declining probabilistically. In each period the probability distribution of demand is stochastically worse than before. If the intervals between decisions are sufficiently short, there is a unique subgame-perfect equilibrium in which the smaller firm outlasts its larger competitor.\nWhinston [1987, 1988] examines an oligopoly in which capacity is adjustable in lumps equal to plant size. In this framework he shows that it is difficult to reach any general conclusions about the pattern of plant closures.\nWe demonstrate that given continuous capacity adjustment, there is a unique subgame-perfect equilibrium in declining industries. In this equilibrium the largest of several equally efficient firms will reduce capacity alone until its market share is equal to that of its next smallest rival.\nFirm i's marginal revenue equals $\\theta_{i}P^{\\prime}(Q)+P(Q)$ ; this is a declining function of $\\mathbf{Q}_{i}$ since a bigger firm suffers more from a decline in price.\n\nThe text discusses market decline in various settings, including perfect information, incomplete information, reentry, probabilistic decline, and capacity adjustment. It explores the competitive advantage of smaller firms and the equilibrium outcomes in declining industries."
  },
  {
    "qid": "econ-empirical-1064-2-0-2",
    "question": "3) Prove that under Assumptions 3.1-3.4, $\\sqrt{n}(\\hat{\\rho}-\\rho)$ converges in distribution to $\\mathcal{N}(0,V_{\\rho})$.",
    "gold_answer": "1. Use the asymptotic linear expansion: $$\\sqrt{n}(\\hat{\\rho}-\\rho)=\\frac{1}{\\sqrt{n}}\\sum_{t=0}^{n-1}\\psi_{\\rho}(X_{t},X_{t+1})+o_{p}(1).$$  \n2. Show $\\{\\psi_{\\rho}(X_{t},X_{t+1})\\}$ is a martingale difference sequence.  \n3. Apply a martingale CLT to establish convergence to $\\mathcal{N}(0,V_{\\rho})$, where $V_{\\rho}=\\mathbb{E}[\\psi_{\\rho}(X_{0},X_{1})^{2}]$.",
    "question_context": "The asymptotic linear expansion is given by: $$\\sqrt{n}(\\hat{\\rho}-\\rho)=\\frac{1}{\\sqrt{n}}\\sum_{t=0}^{n-1}\\psi_{\\rho}(X_{t},X_{t+1})+o_{p}(1),$$ where the influence function $\\psi_{\\rho}$ is: $$\\psi_{\\rho}(x_{0},x_{1})=\\phi^{*}(x_{0})m(x_{0},x_{1})\\phi(x_{1})-\\rho\\phi^{*}(x_{0})\\phi(x_{0}),$$ with $\\phi$ and $\\phi^{*}$ normalized so that $\\|\\phi\\|=1$ and $\\langle\\phi,\\phi^{*}\\rangle=1$.\nAssumption 3.4: (a) $\\delta_{k}=o(n^{-1/2})$ and $\\delta_{k}^{*}=o(n^{-1/2})$, (b) $\\|\\widehat{\\mathbf{G}}^{o}-\\mathbf{I}\\|=o_{p}(n^{-1/4})$ and $\\|\\widehat{\\mathbf{M}}^{o}-\\mathbf{M}^{o}\\|=o_{p}(n^{-1/4})$, (c) $\\mathbb{E}[(\\phi^{*}(X_{t})m(X_{t},X_{t+1})\\phi(X_{t+1}))^{2}]<\\infty$.\nTheorem 3.2: Under Assumptions 3.1-3.4, $\\sqrt{n}(\\hat{\\rho}-\\rho)\\rightarrow_{d}\\mathcal{N}(0,V_{\\rho})$.\n\nThis section establishes the asymptotic normality of the estimator $\\hat{\\pmb\\rho}$ and derives the semiparametric efficiency bound in Case 1. The asymptotic normality is derived through a martingale difference sequence representation."
  },
  {
    "qid": "econ-empirical-335-1-2-0",
    "question": "5) Prove that the confidence interval CIₑ(1−α,δ̂) achieves at least 1−α coverage under the conditions of Theorem 2.",
    "gold_answer": "1. Decompose the coverage probability: Pr[|δ̂ − δ| ≤ bₑ + σ̂ₕc_{1−α/2}/√n].\n2. Under the asymptotic sequence ₑn→c, the bias term bₑ is O(ₑ¹ᐟ²) = O(n⁻¹ᐟ²).\n3. The standardized statistic (δ̂ − δ)/√(Var(δ̂)) converges to N(0,1) under the reference model.\n4. Worst-case bias expansion shows sup_{π₀∈Γₑ(γ₊)}|E[δ̂] − δ| ≤ bₑ + o(n⁻¹ᐟ²).\n5. Combine via triangle inequality: Pr[|δ̂ − δ| ≤ bₑ + z_{α/2}σ̂ₕ/√n] ≥ Pr[|N(0,1)| ≤ z_{α/2}] − o(1) = 1−α − o(1).",
    "question_context": "The confidence interval is given by CIₑ(1−α,δ̂) = [δ̂ ± (bₑ(h,β̂,γ̂) + σ̂ₕ/√n c_{1−α/2})], where bₑ(·) is the worst-case bias, σ̂ₕ² is the sample variance of h(Yᵢ,β̂,γ̂), and c_{1−α/2} is the (1−α/2)-quantile of N(0,1).\nTheorem 2 shows that inf_{π₀∈Γₑ(γ₊)} Pr_{β₀,π₀}[δ_{β₀,π₀}∈CIₑ(1−α,δ̂)] ≥ 1−α + o(1) as n→∞ and ₑn→c∈(0,∞).\n\nThis section develops confidence intervals that account for both misspecification bias and sampling uncertainty, based on the worst-case bias expansion and influence function variance."
  },
  {
    "qid": "econ-empirical-1773-0-1-1",
    "question": "6) How does the paper address endogeneity concerns in linking firm characteristics to export prices?",
    "gold_answer": "1. Product–destination–year fixed effects control for demand-side heterogeneity. \n2. Instrumental variables (e.g., home country R&D intensity) are not used, but robustness checks rule out alternative explanations. \n3. The focus on within-product-destination variation mitigates omitted variable bias.",
    "question_context": "The matched sample includes 77,087 Chinese exporters and covers about 70% of total manufacturing exports.\nThe trade data are classified at the eight-digit HS level (including 7517 product categories).\n\nThe study merges firm-level panel data with trade transaction data to analyze export price determinants."
  },
  {
    "qid": "econ-empirical-314-0-1-1",
    "question": "4) What are the key assumptions required for the smoothed indicator $\\psi_T(x)$ to ensure the test statistic has the desired asymptotic properties?",
    "gold_answer": "The key assumptions for $\\psi_T(x)$ are:\n- [A1]: $\\psi_T(x)$ is non-negative and non-increasing.\n- [A2]: $\\psi_T(x)$ is continuously differentiable at the origin.\n- [A3]: $\\psi_T(x)$ converges pointwise to $1\\{x \\leq 0\\}$ as $T \\to \\infty$.\n- [A4]: $K(T)/\\sqrt{T} = o(1)$.\n- [A5]: For $x < 0$, $\\psi_T(x) \\to 1$ as $T \\to \\infty$.\n- [A6]: For $x > 0$, $\\sqrt{T}\\psi_T(x) \\to 0$ as $T \\to \\infty$.",
    "question_context": "We approximate the function $\\operatorname*{min}(x,0)$ by $\\psi_T(x)x$ where $\\{\\psi_T(x)\\}$ is a sequence of non-negative and non-increasing functions each of which is continuously differentiable at the origin and converges pointwise (except possibly at the origin) as $T\\longrightarrow \\infty$ to the indicator function $1\\{x\\leq0\\}$.\nThe test statistic of this paper has a non-degenerate asymptotic distribution of simple analytic form at boundary points of the null hypothesis but becomes degenerate at interior points. Despite this type of discontinuity, the test critical value can be fixed ex ante without compromising asymptotic validity in the uniform sense.\n\nThe paper develops a test statistic based on smoothed indicators to approximate the sum-of-negative-part function. The methodology involves constructing a sequence of origin-smooth approximators of indicators, enabling standard asymptotic distribution results and obviating simulation for critical values."
  },
  {
    "qid": "econ-empirical-361-2-1-0",
    "question": "1) Show that the MLE $(\\widetilde{H}, \\widetilde{G})$ for non-equally spaced mesh $T$ is the Cholesky decomposition of $\\widetilde{\\mathcal{S}}$ and derive the condition under which it coincides with the realized covariance.",
    "gold_answer": "1. **MLE for Non-Equally Spaced Mesh**: \n$$\n\\widetilde{\\mathcal{S}} = \\sum_{j=1}^n \\frac{1}{n \\Delta_j(T)} (Y_{t_j^n} - Y_{t_{j-1}^n})(Y_{t_j^n} - Y_{t_{j-1}^n})'.\n$$\n2. **Condition for Coincidence**: If $\\sup_j |\\Delta_j(T)n - 1| = o(n^{-1/2})$, then $\\widehat{\\Sigma} = \\widetilde{\\Sigma} + o_\\mathbb{P}(n^{1/2})$.",
    "question_context": "Here we assume that the observation times are synchronous, i.e. $T:=\\mathcal{T}_{1}=\\mathcal{T}_{2}=\\cdot\\cdot\\cdot=\\mathcal{T}_{d}$ . We consider first the case of $n$ equispaced returns generated by (23). In this case, the loglikelihood function is:\n\n$$\n\\overline{{{Q}}}_{n}\\left(\\boldsymbol{\\Sigma}^{0}\\right):=-\\frac{1}{2}\\left\\{\\ln\\left(\\left|\\boldsymbol{\\Sigma}^{0}\\right|\\right)+\\mathrm{tr}\\left[(\\boldsymbol{\\Sigma}^{0})^{-1}\\overline{{\\boldsymbol{\\Sigma}}}_{n}\\right]\\right\\},\\quad n\\in\\mathbb{N},\n$$\n\nwhere ${\\overline{{\\Sigma}}}_{n}$ is the realized covariance estimator using all available prices, i.e.,\n\n$$\n\\overline{{\\boldsymbol{\\Sigma}}}_{n}=\\overline{{\\boldsymbol{\\Sigma}}}:=\\sum_{j=1}^{n}(Y_{t_{j}^{n}}-Y_{t_{j-1}^{n}})(Y_{t_{j}^{n}}-Y_{t_{j-1}^{n}})'.\n$$\n\nThis section focuses on the synchronous observation case, where all assets are observed at the same times. The CholCov estimator is shown to be the MLE of the covariance matrix."
  },
  {
    "qid": "econ-empirical-329-2-0-3",
    "question": "4) Using the Sargan test results from both tables, develop a formal comparison of model specifications. For Table 1 (DDE model) with Sargan = 16.53 (20 df) and Table 2 (CE model) with Sargan = 7.17 (12 df), analyze:\n   a) The overidentifying restrictions test interpretation\n   b) The implications for model selection",
    "gold_answer": "Analysis:\n\n1. For the DDE model:\n   - Test statistic: $\\chi^2_{20} = 16.53$\n   - p-value $\\approx 0.68 > 0.05$ ⇒ Fail to reject $H_0$\n   - Implies instruments are valid (no misspecification)\n\n2. For the CE model:\n   - Test statistic: $\\chi^2_{12} = 7.17$\n   - p-value $\\approx 0.85 > 0.05$ ⇒ Fail to reject $H_0$\n   - Also suggests valid instruments\n\n3. Model selection implications:\n   - Both models pass the overidentification test\n   - Cannot reject either based on Sargan alone\n   - Requires non-nested testing (as mentioned in text)\n   - The CE model has lower test statistic despite fewer df, suggesting potentially better fit\n   - But degrees of freedom differ, making direct comparison difficult",
    "question_context": "Both the price indices and the interest rate are matched to the quarter in which the household has been interviewed.\nThe variable $K\\tau$ has not been included because it is by its nature transitory. Checking its statistical significance, I obtained a parameter estimate of 0±12 with a standard error of 0±11.\nThe use of instrumental variables is required by the fact that many of the choice variables dated $t+1$ are correlated with the forecast error in the Euler equation, implying that an ordinary OLS procedure would not give consistent parameter estimates.\nThe error term in the Euler equation, in fact, is likely to be serially correlated because of the time averaging which characterises many of the variables that appear in the model (see Working, 1960).\nThe implied ISE for different groups of households are: (a) for a household with two adults, both out of work, and no children the ISE is about 0±2 ($\\mathrm{\\Delta}\\rho=4\\cdot78$); (b) if the head of household enters paid employment as a manual worker, the ISE rises to $0\\cdot26$ ($\\rho=3\\cdot82$). If instead he is a non-manual worker, the ISE becomes $0\\cdot33$ ($\\rho=3$).\nAs found by Blundell et al. (1994), an important variable influencing the ISE is female participation; when the wife is in paid employment, the ISE becomes $0\\cdot4$ if the head is a manual worker, and $0\\cdot57$ if the head is a 'white collar' worker.\nThe number of children has a marginal, but significant, effect on the parameters of the utility function: the presence of a child lowers $\\rho$ by about 0±1. The magnitude of this effect is thus nearly irrelevant, but it is worth noting that it represents something that has already been averaged over cohorts.\nThe main point that arises from Table 1 is that some groups of households (the more educated, those with a working head and a working wife) behave as if they were less prudent. The Euler equation approach does not allow us to distinguish whether this sort of behaviour is linked to the presence of liquidity constraints (i.e. to the inaccessibility of some financial markets for some groups of households), or to a different attitude towards savings (due, for example, to a diversification of risk linked to the presence of more than one source of income, or to the fact of relying on a higher buffer stock, etc.).\nTable 2 reports the parameter estimates and the asymptotic standard errors for the CE model (equation (2)). Again the Sargan test detects no misspecification. The ISE in the broader specification is lower than the one estimated for the structural model. However, as soon as insignificant variables are dropped from the regressors, the coefficient of the interest factor jumps to a level which is an average of the ISE estimated in the structural model for different types of households.\n\nThe parameter estimates for the DDE model (equation (6)) are reported in Table 1. The use of a Stone price index should lead only to minor changes in the parameter estimates. The prices used to calculate the Stone price index are drawn from the implicit deflator of consumers' expenditures classified by function, as published in the Blue Book (various years), after matching the subcategories with those used in the FES."
  },
  {
    "qid": "econ-empirical-23-2-0-0",
    "question": "1) Derive the condition under which a district finds it optimal to experiment in the autarky case. Explain the intuition behind this condition.",
    "gold_answer": "In the autarky case, district $i$ must stick to the location picked at the first stage, so $y_{i}=x_{i}$. To minimize the distance cost, $i$ chooses $y_{i}=x_{i}=t_{i}$. The district experiments $(J_{i}=1)$ if the expected benefit exceeds the cost: $p > k$. This ensures the expected gain from a successful experiment outweighs the cost.",
    "question_context": "Each agent can be thought of as a political unit, a state in a federal system, or one of two independent countries. We refer to them as districts.\nEach district $i\\in\\{A,B\\}$ simultaneously decides the type, or location, of its initial policy or experiment. This location is simply a point on the real line, $x_{i}\\in\\mathbb{R}$.\nEach district decides on the binary quantity of experimentation. That is, a district can play it safe or experiment with the policy. With probability $p\\in(0,1)$, an experiment succeeds and raises the quality of the policy by 1. The cost of the experiment is $k>0$.\nAfter the districts have observed the outcomes of both experiments, each district $i$ decides on its final policy location, $y_{i}$, to implement. We assume that a district must implement one of the two policies developed at the first stage, so $y_{i}\\in\\{x_{A},x_{B}\\}$.\nThe districts may have different ideal points regarding the type of policy. Each district (or its representative) has an ideal point $t_{i}$ that is a point on the real line and $h\\geq0$ measures the heterogeneity or the distance between the ideal points, $h=t_{B}-t_{A}$.\nThe payoff to district $i\\in\\{A,B\\}$ is: $\\boldsymbol{u}_{i}=\\boldsymbol{I}_{y_{i}}-\\boldsymbol{c}(\\boldsymbol{y}_{i}-\\boldsymbol{t}_{i})-\\boldsymbol{J}_{i}\\boldsymbol{k}$, where the index-function $I_{y_{i}}\\in\\{0,1\\}$ equals 1 if the policy $y_{i}$, chosen by $i$ at stage three, has proven successful. The index function $J_{i}\\in\\{0,1\\}$ equals 1 if $i$ decided to experiment at the second stage.\n\nThe model consists of two agents (districts) and three stages, focusing on policy experimentation and implementation."
  },
  {
    "qid": "econ-empirical-584-4-0-1",
    "question": "2) Analyze the implications of the condition $\\eta^{\\psi N}-\\eta^{F_{N}N}<0$ for Expectation $B$ and the resulting price dynamics.",
    "gold_answer": "1. When $\\eta^{\\psi N}-\\eta^{F_{N}N}<0$, Expectation $B$ leads to further price reductions as demand becomes more elastic.\\n2. This creates a paradox where the expectation is outperformed, leading to cumulative price reductions.\\n3. The condition implies that the labor supply curve is less elastic than the marginal cost curve, affecting profit maximization decisions.\\n4. The outcome is a real expansion with falling prices, contingent on the initial expectation.",
    "question_context": "Using a simple economic model where monetary changes (or other means of stimulating demand in a more complicated model) have only nominal effects with perfect competition, we have shown that the introduction of non-perfect competition reveals a possible case of purely real expansion/contraction.\nIf we concede that wage-rates respond proportionately to prices, the possibility of real effects hinges on a knife-edge condition of $\\eta^{\\psi N}-\\eta^{F_{N}N}=0$ which, mathematically speaking, is of probability of measure zero.\nThe condition requires that the elasticity of the labour supply curve must be precisely equal to that of the marginal product curve (or the negative of the elasticity of the marginal cost curve).\nThe rationale of this condition can be seen in Fig. $3\\left(b\\right)$. If the elasticity of the labour supply curve is less than the negative of the elasticity of the $M C$ curve ($M C$ shifts upward by a smaller extent than it is downward sloping), profit maximisation actually calls for a reduction in $P$ if Expectation $B$ is held, making the expectation 'more' than realised.\nThe objective condition of $\\eta^{\\psi N}-\\eta^{F_{N}N}<0$ seems to justify an expectation more favourable than Expectation $B$ (i.e. $\\eta^{\\hat{P}M}<0\\rangle$. But if this more favourable expectation is held, prices will fall even further (as demand becomes more elastic), still out-performing' the expectation.\nFor an expansion, the condition of $\\eta^{\\psi N}-\\eta^{F_{N}N}\\leqslant0$ is quite unlikely to hold if the economy is not far from full employment and full capacity, or if some important bottlenecks exist.\nOur analysis alone is insufficient to justify an expansionary policy though it does seem to sound a warning as to contractionary policies based purely on demand management.\n\nThe text discusses the implications of non-perfect competition in economic models, focusing on the conditions under which monetary changes can have real effects. It highlights the knife-edge condition for real effects and the role of expectations in determining economic outcomes."
  },
  {
    "qid": "econ-empirical-1069-0-0-3",
    "question": "4) Derive the conditions under which imprecise information is more beneficial than precise information in Gal-Or's framework.",
    "gold_answer": "Conditions include:\\n1. **High Competition**: When competition is intense, imprecision can be more beneficial.\\n2. **Information Sensitivity**: When information is highly sensitive, imprecision reduces risks.\\n3. **Cost-Benefit Analysis**: The cost of precision outweighs its benefits.\\nMathematically, this can be represented as: \\[ \\frac{C_p}{B_p} > \\frac{C_i}{B_i} \\] where \\(C_p\\) and \\(B_p\\) are the cost and benefit of precision, and \\(C_i\\) and \\(B_i\\) are the cost and benefit of imprecision.",
    "question_context": "The Advantages of Imprecise Information\n\nThe paper explores the strategic benefits of imprecise information in competitive environments, focusing on how firms can leverage information asymmetry to gain competitive advantages."
  },
  {
    "qid": "econ-empirical-1095-3-0-4",
    "question": "5) Discuss the natural experiment created by the reform of the national service and how it allows for the estimation of the effect of substituting education for early labor market experience.",
    "gold_answer": "1. The reform affected all men by increasing labor market experience within each age group due to the suppression of the national service.\n2. Men from low socioeconomic backgrounds were additionally affected by a substitution of early labor market experience for education.\n3. By comparing the wage variations between men from high and low socioeconomic backgrounds, the study isolates the effect of the substitution.\n4. This difference-in-differences approach provides a causal estimate of the impact of substituting education for early labor market experience.",
    "question_context": "To be more specific, let us consider a student of age a and denote (i) $w_{0}$ his potential entry wage at age a, (ii) $w_{0}\\mathbf{+}b$ his potential entry wage at age $_{a+1}$ , after one additional year of education and (i) $w_{0}\\dag c$ his potential wage at age $_{a+1}$ after one year of experience into the labor market (that is, wage at $_{a+1}$ after leaving school at age a). The parameter $^{b}$ is the impact of one additional year of education on entry wages, the parameter $^{c}$ is the impact of one additional year of early labor market experience on wages and parameter $^{b-c}$ represents the impact on wages of substituting one year of education to one year of early labor market experience holding age constant.\nThe reform of the national service provides an interesting natural experiment to address this issue, because it has mostly affected the demand for education of men coming from a low socioeconomic background. All men, regardless of their socioeconomic background, have been affected by the suppression of the national service and the associated mechanical increase in labor market experience within each age group. However, men coming from a low socioeconomic background have, on top of that, been affected by a specific additional substitution of early labor market experience for education.\nTable 6 focuses on the sample of all workers (not necessarily new entrants) and provides an analysis of their number of years of education'° and wages as a function of their sex, age, cohort of birth (before/after the reform), family background (children of low/high background), and all the possible interactions between these explanatory variables.\nThe first column of Table 6 confirms that the reform was followed by a significant decrease in the relative number of years of education of men coming from a low socioeconomic background (-0.6). The national service provided these students with an incentive to pursue education at a turning point of their schooling career, that is, at the stage where they had to decide between entering into the labor market and pursuing higher-secondary education. Most interestingly, the second column reveals that this educational shift has been followed by a significant decrease in their relative wages (-7.9 percent). Assuming that the effect of the reform on relative wages comes from its effect on relative education only, these reduced-form results suggest that the substitution of one year of secondary education for one year in the labor market increases wages by about 13 percent (that is, 0.079/0.60).\nThe last column of Table 5 shows a regression of wages on years of education using a dummy interacting sex, social background, and birth cohort as an instrumental variable. The IV estimate of the returns to one year of education is significant at the 6 percent level and (unsurprisingly) identical to our initial Wald estimate (13 percent). It is also very close to the OLS estimate of the return to education for male low-background early school-leavers (Column 4).\nFinally, Table 7 shows the results when we use an alternative measure of education, that is, high school graduation. The first-stage regression confirms the significant impact of the reform on the probability of high school graduation (-0.13). The IV regression suggests that high school graduation increases subsequent wages by about 61 percent at each given age. Given that it takes about four years of postcompulsory education to obtain this degree, this IV results suggest a 15 percent net impact for each additional year of secondary education,which is consistent with the evaluation in Table 6.\n\nThe reform of the national service has modified the distribution of education and entry wages in a way which is consistent with the assumption that education increases significantly the earning capacity of young workers at the entry into the labor market. It does not necessarily mean, however, that education increases earning capacity more than early experience into the labor market. This is nevertheless a key policy issue: is it really beneficial for young workers to substitute postcompulsory education for early labor market experience?"
  },
  {
    "qid": "econ-empirical-703-1-0-3",
    "question": "4) Analyze the bootstrap bias-shifted statistic $W^{\\prime\\prime}$. How does it address the leftward bias observed in the distribution of $W$?",
    "gold_answer": "4. The bootstrap bias-shifted statistic addresses bias as follows:\n   - The original statistic $W$ has a leftward bias, as seen in simulations.\n   - The bootstrap estimates the bias as $B^{-1} \\sum_{j=1}^B W_j^\\star$.\n   - Subtracting this estimate yields $$W^{\\prime\\prime} = W - B^{-1} \\sum_{j=1}^B W_j^\\star.$$\n   - This adjustment shifts the distribution closer to $\\mathrm{N}(0,1)$, reducing the ERP.",
    "question_context": "The asymptotic $t$-type statistic for the hypothesis that $T(F)=T_{0}$ is given by $$W=(\\hat{T}-T_{0})/[\\hat{V}(\\hat{T})]^{1/2},$$ where $\\hat{V}(\\hat{T})$ is the variance estimate.\nThe CDF of the Singh–Maddala distribution is $$F(y)=1-\\frac{1}{(1+a y^{b})^{c}}.$$ For parameter values $a=100$, $b=2.8$, $c=1.7$, the expectation is $$\\mu_{F}=c a^{-1/b}\\frac{{\\cal{T}}(b^{-1}+1){\\cal{T}}(c-b^{-1})}{{\\cal{T}}(c+1)}$$ and the expectation of $Y\\log{Y}$ is $$\\mu_{F}b^{-1}[\\psi(b^{-1}+1)-\\psi(c-b^{-1})-\\log a],$$ where $\\psi(z)$ is the digamma function.\nThe bootstrap $P$ value for a one-tailed test is $$P^{\\star}=\\frac{1}{B}\\sum_{j=1}^{B}\\mathrm{I}(W_{j}^{\\star}<W),$$ where $\\operatorname{I}(\\cdot)$ is an indicator function.\nThe bootstrap bias-shifted statistic is $$W^{\\prime\\prime}=W-B^{-1}\\sum_{j=1}^{B}W_{j}^{\\star}.$$\n\nThe text discusses asymptotic and bootstrap inference methods for testing hypotheses about $T(F)$ and constructing confidence intervals. It introduces the Singh–Maddala distribution to model income distributions and evaluates the performance of asymptotic and bootstrap tests."
  },
  {
    "qid": "econ-empirical-1573-4-0-3",
    "question": "4) Prove that the spectral radius $\\lambda_{\\text{JSR}}(\\{F_0, F_1\\}) < 1$ under the condition $\\sum_{i=1}^{k-1} |\\phi_i| = \\Phi < 1$, as discussed in Lemma 3.1.",
    "gold_answer": "1. **Matrix norm**: For any matrix $M \\in \\mathcal{A}^n$, show that $\\|M x\\|_{\\infty} \\leq \\Phi^{\\lfloor n/(k-1) \\rfloor} \\|x\\|_{\\infty}$.\n2. **Eigenvalue bound**: For an eigenvector $x$ of $M$, $\\|M x\\|_{\\infty} = |\\lambda| \\|x\\|_{\\infty} \\leq \\Phi^{\\lfloor n/(k-1) \\rfloor} \\|x\\|_{\\infty}$, implying $|\\lambda| \\leq \\Phi^{\\lfloor n/(k-1) \\rfloor}$.\n3. **Joint spectral radius**: Take the limit as $n \\to \\infty$ to conclude $\\lambda_{\\text{JSR}}(\\{F_0, F_1\\}) \\leq \\Phi^{1/(k-1)} < 1$.",
    "question_context": "Since, by the ∞implica Assu the roots of− $\\begin{array}{r}{\\phi(z)=1-\\sum_{i=1}^{k-1}\\phi_{i}z^{i}}\\end{array}$ lie st he unit there exists a sequence with $\\varphi_{0}=1$ and $\\textstyle\\sum_{0=1}^{\\infty}|\\varphi_{i}|<\\infty$ such that $\\begin{array}{r}{\\phi^{-1}(z):=\\sum_{i=0}^{\\infty}\\varphi_{i}z^{i}}\\end{array}$ satisfies $\\phi^{-1}(z)\\phi(z)=1$ for all $|z|\\leq1$ . Moreover, there exists a $C<\\infty$ and a $\\gamma_{\\phi}\\in(0,1)$ such| th|at $|\\varphi_{i}|<C\\gamma_{\\phi}^{i}$ for all $i\\geq0$ . (See e.g. Brockwell and Davis (1991, Section 3.3).)\nLemma B.3. Let $\\begin{array}{r}{\\phi_{m}^{-1}(z):=\\sum_{i=0}^{m}\\varphi_{i}z^{i}.}\\end{array}$ , where $m\\geq1$ . Then there exists a $C<\\infty.$ , independent of $m$ , and $\\{d_{m,i}\\}_{i=1}^{k-1}$ such that $$ \\phi_{m}^{-1}(z)\\phi(z)=1-z^{m}\\sum_{i=1}^{k-1}d_{m,i}z^{i}=:1-d_{m}(z)z^{m} $$ for all $|z|\\le1$ and $m\\in\\mathbb{N},$ , and where $|d_{m,i}|\\leq C\\gamma_{\\phi}^{m}$ .\nLemma B.4. Suppose Assumptions A1–A4 hold. Then for each $s\\in\\{0,\\ldots,k-1\\}$ , (iv) $\\begin{array}{r}{\\sum_{t=1}^{T}y_{t}\\varDelta y_{t-s}=O_{p}(T).}\\end{array}$ .\n\nThis section discusses the asymptotic properties of OLS estimators in the context of autoregressive models of order k (AR(k)). It involves detailed mathematical derivations and proofs regarding the behavior of estimators under certain assumptions."
  },
  {
    "qid": "econ-empirical-1808-0-2-0",
    "question": "5) Formulate the VaR estimation problem using time-varying copulae and explain how it improves upon the RiskMetrics approach.",
    "gold_answer": "The VaR at level \\( \\alpha \\) is estimated as:\n\\[ \\text{VaR}_\\alpha = F^{-1}(\\alpha) \\]\nwhere \\( F \\) is the joint distribution modeled using a time-varying copula. Unlike RiskMetrics, which assumes normality, the copula approach captures tail dependencies and asymmetries, leading to more accurate VaR estimates.",
    "question_context": "Using copulae with adaptively estimated dependence parameters we estimate the VaR from Deutscher Aktienindex (DAX) portfolios over time. As a benchmark procedure we choose RiskMetrics, a widely used methodology based on conditional normal distributions with a Generalized Autoregressive Conditional Heteroscedasticity (GARCH) specification for the covariance matrix.\nBacktesting underlines the improved performance of the proposed adaptive time-varying copulae fitting.\n\nThe article demonstrates the application of time-varying copulae in VaR estimation, comparing its performance with the RiskMetrics approach."
  },
  {
    "qid": "econ-empirical-651-5-1-0",
    "question": "1) Describe the data generation process used in this subsection.",
    "gold_answer": "Data is generated randomly from either the basic model or the extended model with equal probability, ensuring a robust evaluation of the estimators.",
    "question_context": "As seen in Tables 10–13, our estimators perform considerably well in all types of the data setup. The simulation results shown here are very similar as found in the previous subsections.\n\nThis subsection investigates the performance of the robust estimation method suggested in Section 4.3, where data is generated randomly from the basic model or the extended model with equal probability. Tables 10–13 present the simulation results under different loadings and variance types."
  },
  {
    "qid": "econ-empirical-523-0-0-0",
    "question": "1) Formally derive the virtual price approach dual to the Kuhn-Tucker conditions for a demand system with nonnegativity constraints. Show how virtual prices transform binding constraints into nonbinding ones.",
    "gold_answer": "1. **Define the consumer's problem**: Maximize utility \\( U(\\mathbf{q}) \\) subject to budget constraint \\( \\mathbf{p}'\\mathbf{q} \\leq y \\) and nonnegativity constraints \\( q_i \\geq 0 \\).  \n2. **Kuhn-Tucker conditions**: For each good \\( i \\), \\( \\frac{\\partial U}{\\partial q_i} \\leq \\lambda p_i \\), with equality if \\( q_i > 0 \\).  \n3. **Virtual price \\( \\tilde{p}_i \\)**: For goods with \\( q_i = 0 \\), define \\( \\tilde{p}_i \\) such that \\( \\frac{\\partial U}{\\partial q_i} = \\lambda \\tilde{p}_i \\).  \n4. **Transformation**: Replace \\( q_i \\geq 0 \\) with \\( q_i \\geq 0 \\) and \\( p_i \\) with \\( \\tilde{p}_i \\) in the budget constraint, making the constraint nonbinding.",
    "question_context": "Our approach uses virtual prices, which are dual to the Kuhn-Tucker conditions, to select the set of goods consumed—-the demand regime--and to transform binding nonnegativity constraints into nonbinding constraints.\nIt has the advantage of permitting the use of indirect cost and utility functions such as the translog, and the analytic decomposition of demand effects for goods at the nonnegativity limit.\nTheir econometric model is derived by maximizing a random direct utility function subject to budget and nonnegativity constraints. The Kuhn-Tucker conditions determine the set of nonconsumed goods.\nIn this paper, we propose an alternative approach to the zero corner solution problem based upon the use of virtual prices. This approach, which is dual to that of Wales and Woodland, allows the use of indirect utility and cost functions such as the translog.\n\nThis paper addresses the problem of specifying and estimating demand systems for samples with a significant proportion of zero consumption observations. The approach uses virtual prices, dual to Kuhn-Tucker conditions, to select the demand regime and transform binding nonnegativity constraints into nonbinding constraints."
  },
  {
    "qid": "econ-empirical-1730-0-0-1",
    "question": "2) Contrast the Benthamite and Millian social welfare criteria in determining optimal population size, providing the mathematical formulations for each and explaining why their solutions differ.",
    "gold_answer": "1) Benthamite: \\( W = \\sum_{i=1}^N U_i \\) → maximizes total utility\n   - Leads to \\( N^* \\) where \\( U(N^*) + N^*U'(N^*) = 0 \\)\n2) Millian: \\( W = \\frac{1}{N}\\sum_{i=1}^N U_i \\) → maximizes average utility\n   - Yields \\( N^{**} \\) where \\( U'(N^{**}) = 0 \\)\n3) Difference arises because Benthamite criterion values marginal individuals as long as \\( U > 0 \\), while Millian stops when marginal utility reaches zero",
    "question_context": "The individual utility function is no longer defined in terms only of quantity of goods and number of children. A third term has been introduced which, depending on the nature of the question asked, is interpreted as per capita quality of children (composite good spent on children), price of children, utility of children, bequests to children, investment in children's human capital, investment of parents in children as a capital good, quality of children (health, education and so on) and total expenditure on children.\nChapter 5 establishes a key conclusion on endogenous fertility: even if income elasticities of the demand for both quality and quantity of children are positive, the uncompensated income elasticity of fertility may well be negative.\nIn chapter 6 it is shown that a Benthamite allocation (based on total utilities) results in an optimal population size which is, as expected, larger than that of the Millian one, while the laissez-faire outcome may lie on either side.\n\nThe text discusses the evolution of economic models of fertility, moving from simplistic treatments of children as consumer goods to more sophisticated models incorporating intergenerational utility links and endogenous fertility decisions. Key contributions include nonlinear budget constraints and non-convex indifference maps arising from quality-quantity tradeoffs in child-rearing."
  },
  {
    "qid": "econ-empirical-184-0-0-2",
    "question": "3) What mathematical tools are typically used to model the convergence to homogeneous expectations?",
    "gold_answer": "The convergence can be modeled using:\n- Stochastic processes to represent the evolution of beliefs.\n- Bayesian updating for belief revision.\n- Martingale theory to analyze the convergence properties.\n- Differential equations to describe the dynamics of belief updates.",
    "question_context": "Errata: On the Convergence to Homogeneous Expectations when Markets are Complete\nEconometrica, Sep., 1999, Vol. 67, No. 5 (Sep., 1999), p. 1253\nPublished by: The Econometric Society\nStable URL: https://www.jstor.org/stable/2999521\n\nThis section discusses the convergence to homogeneous expectations in complete markets, as published in Econometrica, Vol. 67, No. 5 (September, 1999). The paper explores the theoretical underpinnings and implications of such convergence in economic models."
  },
  {
    "qid": "econ-empirical-183-0-0-2",
    "question": "3) Derive the recursive Bellman equation for the value function $U_t$ in the multiple prior optimal stopping problem and show that it is the smallest multiple prior supermartingale dominating the payoff process $X_t$.",
    "gold_answer": "The Bellman equation is:\n\n$$\nU_t = \\max \\left\\{ X_t, \\underset{P \\in \\mathcal{Q}}{\\mathrm{ess\\inf}} \\mathbb{E}^P [U_{t+1} | \\mathcal{F}_t] \\right\\}.\n$$\n\nTo show $U_t$ is the smallest supermartingale dominating $X_t$:\n1. By definition, $U_t \\geq X_t$.\n2. For any other supermartingale $V_t \\geq X_t$, backward induction shows $V_t \\geq U_t$.\n3. The optional sampling theorem ensures $U_t$ retains the supermartingale property under stopping.",
    "question_context": "We develop a theory of optimal stopping under Knightian uncertainty. A suitable martingale theory for multiple priors is derived that extends the classical dynamic programming or Snell envelope approach to multiple priors. We relate the multiple prior theory to the classical setup via a minimax theorem.\nThe key to extend these results to multiple priors is to develop the first steps of a theory of multiple prior martingales. We do this in Section 5. Intuitively, we have to understand what a fair game is in the eyes of an uncertainty-averse, or pessimistic, agent. We define a multiple prior martingale $(M_t)$ by extending the usual martingale property to the nonlinear multiple prior expectation operator.\nThe proof of these results is not completely straightforward, though. The classical theory of optimal stopping relies strongly on martingale theory. A martingale is the probabilistic model of a fair game against nature. By definition, (conditional) expected gains from a martingale are zero. A supermartingale is an unfair game against nature where (conditional) expected gains are negative.\n\nThe paper develops a theory of optimal stopping under Knightian uncertainty, extending classical dynamic programming to multiple priors. It introduces a martingale theory for multiple priors and relates it to the classical setup via a minimax theorem. Applications include microeconomics, operations research, and finance."
  },
  {
    "qid": "econ-empirical-499-3-0-2",
    "question": "3) Derive the additive semiparametric model extension and discuss its advantages over the fully nonparametric model.",
    "gold_answer": "1. **Model Form**: The additive model is $$ Y_{it} = \\sum_{j=1}^d m_j\\left(X_{it}^j, \\frac{t}{T}\\right) + \\alpha_i + e_{it}, $$ where $m_j$ are smooth functions.\n2. **Advantages**:\n   - **Interpretability**: Each $m_j$ captures the marginal effect of $X_{it}^j$.\n   - **Curse of Dimensionality**: Additivity mitigates the high-dimensionality problem of nonparametric regression.\n3. **Estimation**: Use backfitting or marginal integration to estimate $m_j$.",
    "question_context": "We have studied a nonparametric fixed effects model for panel data with a time-varying regression function and locally stationary regressors. We have developed a profile least squares dummy variable estimation method and established its asymptotic properties when both $N$ and $T$ tend to infinity.\nOur proposed methods can be extended in various directions. The first one is to take the cross-sectional dependence into consideration to improve the efficiency of estimation. For example, we can employ the idea of Chen et al. (2012) to deal with the cross-sectional dependence. Another possible extension of this nonparametric model is to semiparametric models such as the additive model $$ Y_{i t}=\\sum_{j=1}^{d}m_{j}\\left(X_{i t}^{j},{\\frac{t}{T}}\\right)+\\alpha_{i}+e_{i t},\\quad i=1,\\ldots,N;~t=1,\\ldots,T, $$ with $X_{i t}=(X_{i t}^{1},\\ldots,X_{i t}^{d})^{\\tau}$ , $\\mathbb{E}[e_{i t}|X_{i t}]=0$ , and $\\mathtt{E}[e_{i t}^{2}|X_{i t}]=\\sigma^{2}$.\n\nThe paper concludes with remarks on the nonparametric fixed effects model for panel data, discussing extensions and acknowledgments."
  },
  {
    "qid": "econ-empirical-458-2-0-0",
    "question": "1) Using the concept of first-order stochastic dominance, explain why the Leniency treatment results in lower market prices compared to other treatments. Provide the formal definition of first-order stochastic dominance and apply it to the context of the cdfs mentioned in the text.",
    "gold_answer": "1. **Definition**: A distribution \\( F \\) first-order stochastically dominates (FOSD) another distribution \\( G \\) if \\( F(x) \\leq G(x) \\) for all \\( x \\), with strict inequality for some \\( x \\). This implies that \\( F \\) yields higher outcomes than \\( G \\).\n2. **Application**: The text states that the cdf of market prices in other treatments FOSD the cdf in Leniency (\\( p < 0.01 \\)). This means prices in non-Leniency treatments are systematically higher.\n3. **Implication**: Leniency reduces cartel effectiveness, leading to lower prices due to higher defection rates and more rigorous undercutting.",
    "question_context": "Prices decrease over time and the average price for Communication and Antitrust does not differ significantly from Benchmark $(p>0.1)$ , but average prices in Leniency are considerably lower $(p=0.019)$ ). The plot of the cumulative distribution function (cdf) of prices depicted in the right panel of Figure 1 confirms this finding. The cdf of market prices in the other treatments clearly first-order stochastically dominates the cdf of market prices in the Leniency treatment $(p<0.01)$ .\nThe cdf of market prices of cartel groups in Leniency is first-order stochastically dominated by the corresponding cdfs of market prices in Antitrust and Communication $(p=0.284\\$ and 0.166, respectively). That is, cooperation in cartels that do establish in the Leniency treatment is less successful than in the other treatments.\nIntroduction of a leniency program provides a further reduction of cartel activity of about $50\\%$ $(p=0.017)$ ). As shown in the first row of Table 1, the alternative notion of cartel activity, that treats all situations with noncompetitive market prices as collusive, displays about the same picture. The reduction in noncompetitive prices when an antitrust authority is introduced is no longer significant, however $(p=0.210)$ ).\nThe agreed-upon price is undercut in Leniency by one or more cartel members in $97\\%$ of the cases, as compared to about $75\\%$ in the other treatments. The table also shows that in about $80\\%$ of all cases, defection from the agreed-upon price is followed by a leniency application from one or more cartel members in the same period.\n\nThe section discusses the impact of Leniency programs on prices and cartel activity, comparing different treatments including Benchmark, Communication, Antitrust, and Leniency. It explores the reasons behind lower market prices in the Leniency treatment and examines cartel stability and duration."
  },
  {
    "qid": "econ-empirical-1120-1-1-1",
    "question": "6) Discuss the implications of the 2002 household survey's 69% completion rate for the internal validity of the study. What biases might arise?",
    "gold_answer": "1. **Attrition Bias**: If non-response is correlated with treatment status, estimates may be biased.\n2. **Selection Bias**: Mobile households may differ systematically from stable ones.\n3. **Mitigation**: Similar completion rates across treatment groups suggest randomization held.",
    "question_context": "Hemoglobin (Hb) and parasitological surveys were conducted in both December 2001 and October 2002 among a subsample of the children selected for the household survey, with Group I preschool children surveyed in 2001 and Groups I and II children in 2002.\nAnthropometric measures of child weight-for-height ('wasting'), weight-for-age ('underweight'), and height-for-age ('stunting') Z-scores proxy for nutritional status, where Z-scores reflect the standardized difference, by age and gender, from the mean of reference healthy U.S. children.\nPreschool enrollment rosters were collected from August to October 2001, and participation data was then collected during monthly, unannounced visits to all preschools from November 2001 through April 2002.\n\nThe study collected data through household surveys, hemoglobin tests, and anthropometric measurements. Enrollment rosters and monthly attendance checks were used to track participation. Challenges included high residential mobility and survey non-response."
  },
  {
    "qid": "econ-empirical-288-0-0-1",
    "question": "2) Prove that for liquid assets and bonds to coexist in equilibrium, their returns must be equal, i.e., $(q_{t}+\\kappa)/q_{t-1}=1/q_{b,t-1}$.",
    "gold_answer": "1. For both assets to be held, their liquidity-adjusted returns must be equal.  \n2. The return on the liquid asset is $(q_{t}+\\kappa)/q_{t-1}$.  \n3. The return on bonds is $1/q_{b,t-1}$.  \n4. Equating these yields: $(q_{t}+\\kappa)/q_{t-1}=1/q_{b,t-1}$.",
    "question_context": "The model can generate multiple stationary equilibria, across which asset prices, market participation, capitalization, output and welfare are positively related. It can also generate a variety of nonstationary equilibria, even when fundamentals are deterministic and time invariant, including periodic, chaotic and stochastic (sunspot) equilibria with recurrent market crashes.\nThe household’s problem can now be written: $$\\operatorname*{max}_{a\\geq0,b\\geq0}\\{-r[(q_{t-1}-q^{*})-(q_{t}-q_{t-1})]a-[(1+r)q_{b,t-1}-1]b+\\alpha(n_{t})\\theta\\varDelta(y_{t})\\},$$ where $y_{t}=y^{*}$ if $\\omega(y^{\\ast})\\leq(q_{t}+\\kappa)a+b$ and $\\omega(y_{t})=(q_{t}+\\kappa)a+b$ otherwise.\nFor the liquid asset and bonds to both be held, since they are equally liquid, they must have the same return, $(q_{t}+\\kappa)/q_{t-1}=1/q_{b,t-1}$.\nProposition 10. Assume that $(1-\\theta)\\varDelta(y^{*})>k$. The optimal provision of bonds is such that $B\\geq\\omega(y^{\\ast})-(q^{\\ast}+\\kappa)A.$ It achieves $q=q^{*}$, $q_{b}=q_{b}^{*}$, and $y=y^{*}$.\n\nThe study analyzes economies where liquid assets play an essential role in the exchange process, generating multiple stationary and nonstationary equilibria. Asset prices, market participation, and welfare are positively related across equilibria. The model can produce periodic, chaotic, and stochastic equilibria, including those resembling asset-price bubbles."
  },
  {
    "qid": "econ-empirical-108-22-0-0",
    "question": "1) Derive the expression for the total precision of the firm’s information, $\\frac{1}{\\mathbb{V}}$, and explain the economic interpretation of each term in the decomposition.",
    "gold_answer": "1. The total precision of the firm’s information is given by: $$\\frac{1}{\\mathbb{V}}=\\frac{1}{\\sigma_{\\mu}^{2}}+\\frac{1}{\\sigma_{e}^{2}}+\\frac{1}{\\sigma_{v}^{2}\\sigma_{z}^{2}}.$$  \n2. **$\\frac{1}{\\sigma_{\\mu}^{2}}$**: Represents the precision of the firm’s prior about fundamental uncertainty.  \n3. **$\\frac{1}{\\sigma_{e}^{2}}$**: Captures the precision of private information generated within the firm.  \n4. **$\\frac{1}{\\sigma_{v}^{2}\\sigma_{z}^{2}}$**: Represents the precision of information derived from market prices.  \n5. The decomposition shows how different sources of information contribute to the firm’s overall precision.",
    "question_context": "The reduction in V both in absolute and percentage terms due to learning from both channels is given by $\\Delta\\mathbb{V}=\\mathbb{V}-\\sigma_{\\mu}^{2}$, and the resulting effects on aggregate productivity and output.\nThe inverse of V, the total precision of the firm’s information, lends itself to a simple linear decomposition: $$\\frac{1}{\\mathbb{V}}=\\frac{1}{\\sigma_{\\mu}^{2}}+\\frac{1}{\\sigma_{e}^{2}}+\\frac{1}{\\sigma_{v}^{2}\\sigma_{z}^{2}}.$$\nThe change in V due to market information is given by $$\\Delta\\mathbb{V}=\\mathbb{V}-\\left(\\frac{1}{\\sigma_{\\mu}^{2}}+\\frac{1}{\\sigma_{e}^{2}}\\right)^{-1}.$$\nThe change in V due to market information alone is given by $$\\Delta\\mathbb{V}=\\left(\\frac{1}{\\sigma_{\\mu}^{2}}+\\frac{1}{\\sigma_{v}^{2}\\sigma_{z}^{2}}\\right)^{-1}-\\sigma_{\\mu}^{2}.$$\n\nThis section explores the relative importance of private sources within the firm versus market prices in firm learning, and their aggregate consequences on productivity and output."
  },
  {
    "qid": "econ-empirical-1688-1-0-3",
    "question": "4) Using the PSID and NLS datasets, derive the conditions under which the Solon-Zimmerman estimates (0.4–0.5) would be unbiased, and discuss how Bjorklund and Jantti's methodology relaxes these conditions.",
    "gold_answer": "1. **Unbiasedness Conditions**:\n   - Permanent income is proxied by multi-year averages.\n   - Samples are nationally representative.\n2. **Relaxations**:\n   - Bjorklund-Jantti use predicted income, avoiding the need for linked father-son pairs.\n   - Their method allows for cross-country comparisons even with non-longitudinal data.",
    "question_context": "Intergenerational income correlations are an important indication of the extent of economic mobility across families. Until recently it has been thought that the correlations between fathers’ and sons’ incomes were significantly positive, but quite low, indicating that family background was not a primary deterrent to economic success.\nRecently, Gary Solon (1989,1992) and David J. Zimmerman (1992) have reconsidered these findings and considerably improved the methodology and data previously used to measure this correlation. They demonstrated that estimates based on annual income and nonrepresentative homogeneous samples understate the correlation between the long-run economic status of fathers and sons.\nUsing more appropriate techniques and data, they both find correlations between 0.4 and 0.5 for the United States. Because Solon used the Panel Study of Income Dynamics (PSID) and Zimmerman the National Longitudinal Survey (NLS), the magnitude of their estimates seems fairly reliable.\nThis paper assesses the magnitude of the same parameter for Sweden. Sweden provides an interesting comparison. In terms of equality of outcome, Sweden and the United States are at two extremes among OECD countries—the United States is at the top and Sweden at the bottom of orderings of inequality of disposable income.\nOur interest in a comparison of intergenerational income correlation between Sweden and the United States is motivated, in part, by the question whether the extent of cross-sectional and intergenerational inequality are independent of each other. Is it possible that Sweden, which has less cross-sectional income inequality, also has more intergenerational mobility?\nWhereas Solon and Zimmerman had data on pairs of fathers and sons, we have independent samples of fathers and of sons. The sons report certain characteristics of their fathers, like occupation and education. The sample of fathers contains the same variables. We must use an imperfect substitute for the father's income, his predicted income.\n\nThis paper examines intergenerational income correlations in Sweden and the United States, focusing on methodological improvements and comparative analysis of economic mobility."
  },
  {
    "qid": "econ-empirical-192-1-0-0",
    "question": "1) Derive the factor-pricing model equation $r_{i t}=\\lambda^{\\prime}\\beta_{i}+(F_{t}-E F_{t})^{\\prime}\\beta_{i}+v_{t}^{\\prime}\\mu_{i}+e_{i t}$ from the given assumptions and explain the role of each component.",
    "gold_answer": "1. **Factor-Pricing Model Derivation**:\n   - The model is derived from the assumption that returns $r_{it}$ are linearly related to the risk premia $\\lambda$ and the factors $F_t$.\n   - **Components**:\n     1. $\\lambda^{\\prime}\\beta_{i}$: Represents the expected return due to risk premia.\n     2. $(F_{t}-E F_{t})^{\\prime}\\beta_{i}$: Captures the deviation of factors from their mean.\n     3. $v_{t}^{\\prime}\\mu_{i}$: Accounts for the unobserved factor $v_t$ with loadings $\\mu_i$.\n     4. $e_{i t}$: Idiosyncratic error term.\n   - **Assumptions**:\n     - $v_t$ and $e_{it}$ are uncorrelated with $F_t$.\n     - $v_t$ and $e_{it}$ have zero mean.",
    "question_context": "We consider the problem of estimation and inference on the risk premia $\\lambda$ based on observations of returns $\\{r_{i t}$ $i=1,\\ldots,N,t=1,\\ldots,T\\}$ and factors $\\{F_{t},t=1,\\dots,T\\}$ obeying a correctly-specified factor-pricing model: \n\n$$\nr_{i t}=\\lambda^{\\prime}\beta_{i}+(F_{t}-E F_{t})^{\\prime}\beta_{i}+v_{t}^{\\prime}\\mu_{i}+e_{i t},\n$$\n\nwhere the random unobserved factor $v_{t}$ has zero mean and is uncorrelated with $F_{t}$ . The idiosyncratic error terms $e_{i t}$ also have zero mean and are uncorrelated with $F_{t}$ and $v_{t}$.\nAssumption FACTORS. The $k_{F}~\\times~1$ vector of observed factors $F_{t}$ is stationary with finite fourth moments and a full-rank covariance matrix $\\Sigma_{F}$ . The $k_{v}\\times1$ latent factors $v_{t}$ satisfy the following:\n\n$$\n\\begin{array}{r}{\\left(\\begin{array}{c}{\\frac{1}{\\sqrt{T}}\\sum_{t=1}^{T}(F_{t}-E F_{t})}\\ {\\eta_{T}=\\frac{1}{\\sqrt{T}}\\sum_{t=1}^{T}\\sum_{F}^{-1}\\widetilde{F}_{t}v_{t}^{\\prime}}\\ {\\eta_{v,T}=\\frac{1}{\\sqrt{T}}\\sum_{t=1}^{T}v_{t}}\\end{array}\\right)\\Rightarrow\\left(\\begin{array}{c}{N(0,\\Omega_{F})}\\ {\\eta}\\ {\\eta_{v}}\\end{array}\\right),}\\end{array}\n$$\n\nwhere $\\mathsf{v e c}(\\eta)\\sim N\\left(\\mathsf{0}_{k_{F}k_{v},1},\\Omega_{v F}\\right),\\eta_{v}\\sim N(\\mathsf{0}_{k_{v},1},I_{k_{v}})$ and $\\begin{array}{r}{\\widetilde{F}_{t}=F_{t}-\\frac{1}{T}\\sum_{s=1}^{T}F_{s}.}\\end{array}$\nAssumption LOADINGS. As both $N$ and $T$ increase to  infinity, we have $N^{-1}T_{N}^{\\prime}T_{N}\\rightarrow{\\cal{T}}$ , where $\\varGamma$ is a positive definite $k\\times k$ matrix. Also assume that $\\begin{array}{r}{\\operatorname*{max}_{N,T}\\frac{1}{N}\\sum_{i=1}^{N}\\|\\gamma_{i}\\|^{4}<\\infty}\\end{array}$ .\n\nThe text discusses the estimation and inference on risk premia $\\lambda$ based on returns and factors following a factor-pricing model. It introduces assumptions on factors and loadings, distinguishing between strong and weak factors."
  },
  {
    "qid": "econ-empirical-249-2-2-1",
    "question": "6) What econometric techniques would be appropriate to measure the time-varying degree of market integration, as suggested by Akdogan's regional focus?",
    "gold_answer": "Time-varying parameter models or rolling-window correlation analyses can capture dynamic integration. Cointegration tests between regional indices (e.g., EU markets) versus global benchmarks (e.g., MSCI World) would quantify convergence trends.",
    "question_context": "Akdogan's study attempts to assess the degree of integration among the world's capital markets, particularly as applied to equities and long-term bonds.\nHis results indicate that the world capital markets have become more integrated in the 1980s... The smaller securities markets tend to be less integrated with the major markets, while markets in the developing countries have low correlations with their developed counterparts.\n\nHaluk Akdogan's study examines the integration of global capital markets, finding increased regional convergence but persistent segmentation between developed and developing economies."
  },
  {
    "qid": "econ-empirical-113-4-1-1",
    "question": "4) The linear probability model in Table S3 shows that a 1% increase in spending raises the adoption probability of budget referendums by 0.8 percentage points (p<0.05). Derive the bias direction if this endogeneity is ignored in OLS estimates of spending effects.",
    "gold_answer": "4) The positive coefficient implies:\n   \\[ \\text{plim } \\hat{\\beta}_{OLS} = \\beta + \\frac{Cov(D_{it},u_{it})}{Var(D_{it})} > \\beta \\]\n   where \\(u_{it}\\) contains spending-induced reform probability. Thus:\n   - OLS would be upward biased (toward zero)\n   - Would attenuate estimated negative effect of referendums on spending\n   - Consistent with IV estimates being more negative than OLS in Table 8",
    "question_context": "We add dummy variables denoting intervals four to six and one to three years prior to institutional reforms, and zero to four and more than five years after the reforms to the specification in equation (1). Table 7 reveals no trends in spending prior to adopting or abolishing a mandatory budget referendum.\nThe table demonstrates that higher spending two and three years before a reform increases the likelihood of adopting the mandatory budget referendum. Similarly, higher spending growth a reform increases the probability of adopting a mandatory budget referendum three years later.\n\nThis section addresses potential endogeneity between direct democratic institutions and public spending by examining pre-reform trends and feedback effects."
  },
  {
    "qid": "econ-empirical-738-1-0-1",
    "question": "2) Show that the least squares estimator of $\\beta$ in the regression $y_{1} = \\pi_{0} + \\pi_{1} x_{1} + \\epsilon_{1}$ is biased if $\\mathrm{Cov}(c, x_{1}) \\neq 0$.",
    "gold_answer": "1. The population regression coefficients are:\n   $$ \\pi_{1} = \\frac{\\mathrm{Cov}(y_{1}, x_{1})}{V(x_{1})}, \\quad \\pi_{0} = E(y_{1}) - \\pi_{1} E(x_{1}) $$\n2. Substitute $y_{1} = \\beta x_{1} + c + u_{1}$:\n   $$ \\mathrm{Cov}(y_{1}, x_{1}) = \\beta V(x_{1}) + \\mathrm{Cov}(c, x_{1}) $$\n3. Thus:\n   $$ \\pi_{1} = \\beta + \\frac{\\mathrm{Cov}(c, x_{1})}{V(x_{1})} $$\n4. If $\\mathrm{Cov}(c, x_{1}) \\neq 0$, then $\\pi_{1} \\neq \\beta$, indicating bias.",
    "question_context": "Suppose that a farmer is producing a product with a Cobb-Douglas technology, \n\n$$ y_{t}=\\beta x_{t}+c+u_{t},\\qquad0<\\beta<1,\\quad t=1,...,T, $$ \n\nwhere $y_{t}$ is the logarithm of output, $x_{t}$ is the logarithm of a variable input (labor), $c$ represents an input that is fixed over time (soil quality), $u_{t}$ represents a stochastic input (rainfall), which is not under the farmer's control, and $t$ indexes the seasons.\nThe factor input decision, however, is made before knowing $u_{t},$ and we shall assume that $x_{t}$ is chosen to maximize expected profits. Then the factor demand equation is \n\n$$ x_{t}=\\{\\ln\\beta+\\ln\\left[E(\\mathrm{e}^{u_{t}}\\mid\\mathcal{B}_{t})\\right]+\\ln(P_{t}/W_{t})+c\\}/(1-\\beta), $$ \n\nwhere $\\mathcal{B}_{t}$ is the information set available to the farmer when he chooses $x_{t}$.\nConsider the least squares regression of $y_{1}$ on $x_{1}$ using just a single cross-section of the data. The population counterpart is \n\n$$ E^{*}(y_{1}\\mid x_{1})=\\pi_{0}+\\pi_{1}x_{1}, $$ \n\nwhere $E^{*}$ is the minimum mean square error linear predictor (the wide-sense regression function), \n\n$$ \\pi_{1}=\\operatorname{cov}{(y_{1},x_{1})}/{V(x_{1})},\\qquad\\pi_{0}=E(y_{1})-\\pi_{1}E(x_{1}). $$\nWith more than one observation per farm, however, we can consider the least squares regression of $y_{t}$ On $\\pmb{x}^{\\prime}=(x_{1},....,x_{T})$. The population counterpart is \n\n$$ E^{*}(y_{t}\\mid x)=\\beta x_{t}+E^{*}(c\\mid x)+E^{*}(u_{t}\\mid x). $$\nA common solution to the bias problem is some form of analysis of covariance. For example, we can form the farm specific means $(\\bar{y}=\\sum t=1,y_{t}/T,$ $\\bar{x}=\\sum_{t=1}^{T}x_{t}/T)$ and the deviations around them $(\\tilde{y}_{t}{=}y_{t}{-}\\tilde{y},\\tilde{x}_{t}{=}x_{t}{-}\\tilde{x})$, and then run a pooled least squares regression of $\\tilde{y}$ on $\\tilde{x}$.\nThe following proposition provides some additional insight into this distinction; it is based on a condition that is slightly weaker than (R): \n\n$$ \\mathrm{Prob}(x_{1}=x_{2}=\\dots=x_{T})=0. $$ \n\nProposition 2. Suppose that \n\n$$ E(y_{t}|x,b,c)=b x_{t}+c,\\qquadt=1,...,T, $$ \n\nwhere $T\\geq2.$ Assume that condition $(R^{\\prime})$ holds and define \n\n$$ \\widehat{b}=\\sum_{t=1}^{T}\\left(y_{t}-\\bar{y}\\right)(x_{t}-\\bar{x})\\bigg/\\sum_{t=1}^{T}(x_{t}-\\bar{x})^{2}. $$ \n\nThen $E(\\hat{b})=E(b)$ if $E(|\\widehat{b}|)<\\infty$.\n\nThis section discusses the identification of production function parameters in the presence of unobserved heterogeneity and stochastic inputs. The farmer's production follows a Cobb-Douglas technology with time-invariant and stochastic components."
  },
  {
    "qid": "econ-empirical-1814-3-0-2",
    "question": "3) Using the central limit theorem, justify the approximation for $\\text{cov}(\\pi, C)$ in terms of $\\pi_m$, $\\pi_\\theta$, and $\\Omega$.",
    "gold_answer": "1. Express $\\pi$ and $C$ as functions of $\\theta$:  \n   $$\\pi = \\pi(\\theta, C(\\theta)), \\quad C = C(\\theta).$$  \n2. Apply the delta method:  \n   $$\\text{cov}(\\pi, C) \\approx \\frac{\\partial \\pi}{\\partial m}\\text{var}(C) + \\frac{\\partial \\pi}{\\partial \\theta}\\text{cov}(C, \\theta) = \\pi_m'\\text{var}(C) + \\pi_\\theta'\\text{cov}(C, \\theta).$$",
    "question_context": "The variance estimate is formed sequentially at the same time that CS is calculated. The post-compensation income level at stage $n$ is $C_n$, derived from the Taylor expansion and satisfying the difference equation: $$C_{n+1}=C_n - \\pi_n'\\Delta - 0.5\\Delta'S_n\\Delta + p'\\Delta,$$ where $\\Delta = x_{n+1} - x_n$, $\\pi_n = \\pi(x_n, q, m_n)$, $S_n = \\partial\\pi(x_n, q, u^0)/\\partial x$, and $m_n = C_n - p'x_n$.\nThe variance of $C_N$ depends on $\\Omega$, the variance-covariance matrix of demand/MWP function parameters. A recursive process evaluates $\\text{var}(C_N)$, ignoring terms of order 3 or higher. Key equations include: $$\\text{var}(C_1) = \\Delta'\\text{var}(\\pi_0)\\Delta,$$ $$\\text{var}(C_{n+1}) = \\text{var}(C_n) + \\Delta'\\text{var}(\\pi_n)\\Delta - 2\\Delta'\\text{cov}(C_n, \\pi_n).$$\nApproximations for $\\text{var}(\\pi)$ and $\\text{cov}(C, \\pi)$ are derived using the central limit theorem: $$\\text{var}(\\pi) \\approx \\pi_\\theta'\\Omega\\pi_\\theta + \\pi_m'\\text{var}(C)\\pi_m + 2\\pi_\\theta'\\text{cov}(C, \\theta)\\pi_m,$$ $$\\text{cov}(\\pi, C) \\approx \\pi_m'\\text{var}(C) + \\pi_\\theta'\\text{cov}(C, \\theta).$$\n\nThis section introduces a first-order approximate measure of the variance of Compensating Surplus (CS) to judge the precision of the point estimate derived from demand function parameters. The variance estimate is formed sequentially alongside the CS calculation, leveraging recursive processes and Taylor expansions."
  },
  {
    "qid": "econ-empirical-1783-1-1-2",
    "question": "7) Discuss the implications of the geographical distribution of cases among multiple social chambers in large Appeal courts like Paris.",
    "gold_answer": "1. Ensures workload distribution and prevents backlog in high-volume courts.\n2. Maintains consistency in case handling within each chamber.\n3. Potential for regional variations in judicial outcomes due to localized practices.",
    "question_context": "Appeal courts judges are 'placed judges', that is, assigned to a given Court or a given Chamber in a specific position according to decisions made every year by the First President of the Court of Cassation (the highest civil jurisdiction) and the First President of the Appeal court.\nThe assignment of cases to judges follows the principle of the 'right to the natural judge', which means that the allocation of cases to judges should not depend on the identity of the judge as this would lead to breaking the principle of equality between litigants (Jeuland 2008).\nIn practice, these principles are applied as follows: \n- Each Appeal court judges cases from a given geographical area. \n- When there is a single social chamber, cases are dealt with in order of arrival. \n- If there are several social chambers in the same Appeal court, cases are distributed according to a geographical criterion and each social chamber deals with cases in order of arrival.\n\nThis section details the structure and operational principles of French Appeal courts, including the appointment and assignment of judges."
  },
  {
    "qid": "econ-empirical-947-3-0-0",
    "question": "1) Derive the optimal policy $(\\boldsymbol{a}_{w},\\boldsymbol{c}_{w},W_{w})$ for the problem $T_{[0,\\bar{u})}^{\\varDelta}F^{\\omega}(w)$ and explain how it generates an incentive-compatible contract-action plan.",
    "gold_answer": "1. **Optimal Policy Derivation**: The optimal policy is derived by solving the Bellman equation $T_{[0,\\bar{u})}^{\\varDelta}F^{\\omega}(w)$, which involves maximizing the principal's value function subject to the agent's incentive compatibility and promise-keeping constraints.\\n2. **Contract-Action Plan**: Starting from $w_0$, the policy iteratively applies $(\\boldsymbol{a}_{w},\\boldsymbol{c}_{w},W_{w})$ to generate compensation $c_{w^n}(y^n,\\tilde{z}^n)$, action $a_{w^n}(z^n)$, and continuation value $w_{n+1} = W_{w^n}(y^n,\\tilde{z}^n)$. This ensures incentive compatibility and optimality.",
    "question_context": "Fix a period length $\\Delta$ and, for each of the agent's feasible continuation values $w$ in $[0,\\bar{u})$ , let $(\\boldsymbol{a}_{w},\\boldsymbol{c}_{w},W_{w})$ be an optimal policy for the problem $T_{[0,\\bar{u})}^{\\varDelta}F^{\\omega}(w)$ (see (3). Starting with the exogenous reservation utility of the agent $w_{0}$ as an initial state variable, in any period $_n$ , the agent's compensation is given by $c_{w^{n}}(y^{n},\\tilde{z}^{n})$ , he takes action $a_{w^{n}}(z^{n})$ ， while the law of motion of the state variable is given by $w_{n+1}=W_{w^{n}}(y^{n},\\tilde{z}^{n})$.\nOur approximately optimal contract-action plans are also generated by a measurable function of policies over $w$ . Instead of using optimal policies for $T_{[0,\\bar{u})}^{\\varDelta}F^{\\omega}(w)$ as above, we use simple policies (Definition 1 below). Simple policies are not derived from the solutions of the problem $T_{[0,\\bar{u})}^{\\varDelta}F^{\\varDelta}$ (see (3) but from the solutions of the approximate problem $T^{\\Delta,q}F$ (see (5)), with $F$ as in Theorem 1.\n\nThis section details the construction of contract-action plans that are approximately optimal as the period length becomes short, using the agent's continuation value as a state variable."
  },
  {
    "qid": "econ-empirical-792-1-0-0",
    "question": "1) Using the efficiency wage theory framework, derive the conditions under which a more egalitarian distribution of land can lead to an increase in aggregate output, as suggested by Dasgupta and Ray (1986).",
    "gold_answer": "1. **Define the efficiency wage model**: Let the efficiency of labor \\( e \\) depend on nutritional intake \\( n \\), such that \\( e = e(n) \\), with \\( e'(n) > 0 \\) and \\( e''(n) < 0 \\).  \n2. **Aggregate output**: Aggregate output \\( Y \\) is given by \\( Y = F(K, e(n)L) \\), where \\( K \\) is capital and \\( L \\) is labor.  \n3. **Land distribution**: Assume land is redistributed to reduce malnourishment, increasing \\( n \\) for the unemployed.  \n4. **Effect on output**: The rise in \\( n \\) increases \\( e(n) \\), leading to higher \\( Y \\) if \\( F \\) is increasing in \\( e(n)L \\).  \n5. **Conclusion**: Under diminishing returns to \\( e(n) \\), an optimal redistribution exists where \\( Y \\) is maximized.",
    "question_context": "In situations of severe capital market imperfections the escape routes from poverty for the unskilled and the assetless may remain blocked, while growth improves the prospects for capital-intensive or skill-intensive projects.\nA potentially important corollary of this original brand of efficiency wage theory is that a more egalitarian distribution of land, for example, by reducing the malnourishment (and thus improving the employability) of the currently unemployed, may lead to a rise in the aggregate output in the economy, as shown by Dasgupta and Ray (1986).\nExpanding the opportunities for credit, for example, can help the poor to invest in education as a way of climbing out of poverty. It can also make small farmers and artisans more economically viable by allowing them to enlarge their scale of production, to take up more high-return high-risk projects or occupational choices, and in general to avoid constrained myopic policies.\nIf these externalities are important, anti-poverty programmes have to go beyond policies addressed to individuals and households per se, and policies aimed at poor areas and groups as a whole may be fruitful in terms of both equity and efficiency.\nUnder the circumstances the terms and conditions of contracts in various transactions that directly affect the efficiency of resource allocation crucially depend on who owns what and who is empowered to make which decisions.\n\nThe text discusses scenarios where economic growth may not benefit the poor due to capital market imperfections, regional polarization, and disenfranchisement. It also explores situations where redistributive policies can enhance economic growth by correcting market failures, particularly those affecting the poor, such as credit and insurance market imperfections. The discussion includes the role of positive externalities from education, health, and gender-specific policies, as well as the importance of community or neighborhood-specific characteristics in poverty alleviation."
  },
  {
    "qid": "econ-empirical-641-1-1-0",
    "question": "3) Derive the asymptotic distribution of $\\hat{\\phi}$ given the asymptotic distribution of $\\hat{C}$.",
    "gold_answer": "1. Start with $\\sqrt{T}(\\hat{C} - C) \\stackrel{d}{\\rightarrow} N(0, \\varOmega)$. \\n2. Apply the elimination matrix $E_l$ to get $\\sqrt{T}(\\hat{\\varrho} - \\varrho) \\stackrel{d}{\\rightarrow} N(0, E_l \\varOmega E_l^{\\prime})$. \\n3. Transform using the Fisher transform $\\phi = \\text{vecl}(\\text{arctanh}(C))$. \\n4. Compute the Jacobian $D_c = \\text{diag}(\\frac{1}{1 - c_i^2})$. \\n5. The asymptotic distribution is $\\sqrt{T}(\\hat{\\phi} - \\phi) \\stackrel{d}{\\rightarrow} N(0, D_c E_l \\varOmega E_l^{\\prime} D_c)$.",
    "question_context": "Suppose that $\\sqrt{T}(\\hat{C}-C)\\stackrel{d}{\\rightarrow}N(0,\\varOmega)$, as $T\\to\\infty$. The asymptotic covariance matrix, $\\varOmega=\\mathrm{avar}(\\mathrm{vec}(\\hat{C}))$, will be singular because $\\hat{C}$ is symmetric and has constant diagonal elements.\nFor the vector of correlation coefficients, ${\\hat{\\boldsymbol{\\varrho}}}=\\mathrm{vecl}({\\hat{\\boldsymbol{C}}})$, it follows that $\\sqrt{T}(\\hat{\\varrho}-\\varrho)\\stackrel{d}{\\rightarrow}$ $N(0,\\mathcal{Q}_{\\varrho})$, as $T\\to\\infty$, where $\\itOmega_{\\rho}=E_{l}\\itOmega_{l}E_{l}^{\\prime}$ and $E_{l}$ is an elimination matrix.\nFor the elementwise Fisher transform, the asymptotic distribution reads $$ \\sqrt{T}(\\hat{\\phi}-\\phi)\\stackrel{d}{\\rightarrow}N(0,\\varOmega_{\\phi}),\\quad\\varOmega_{\\phi}=D_{c}E_{l}\\varOmega E_{l}^{\\prime}D_{c}, $$ where $\\textstyle D_{c}=\\mathrm{diag}(\\frac{1}{1-c_{i}^{2}},\\frac{1}{1-c_{2}^{2}},\\ldots,\\frac{1}{1-c_{d}^{2}})$.\nThe asymptotic distribution of the new parametrization of correlation matrices can be shown to be $$ \\begin{array}{r}{\\sqrt{T}(\\hat{\\gamma}-\\gamma)\\overset{d}{\\rightarrow}N(0,\\varOmega_{\\gamma}),\\quad\\varOmega_{\\gamma}=E_{l}A^{-1}\\varOmega A^{-1}E_{l}^{\\prime},}\\end{array} $$ where $A$ is a Jacobian matrix, such that $\\partial\\mathrm{vec}(C)=A\\partial\\mathrm{vec}(\\log C)$.\n\nThis section derives the asymptotic distributions of $\\hat{\\gamma}$ and the vector of Fisher transformed correlations, $\\hat{\\phi}$, from the empirical correlation matrix."
  },
  {
    "qid": "econ-empirical-694-0-0-3",
    "question": "4) In the extended experiments, how did the richer informational environment affect the subjects' ability to identify and enforce the social optimum? Discuss the role of private cost and redemption information.",
    "gold_answer": "1. **Informational Complexity**: Subjects had to compute net social surplus from private cost and redemption data, which varied across subjects.  \n2. **Learning Phase**: After several periods, subjects identified \\( q^* \\) by aggregating private information.  \n3. **Enforcement**: Voluntary restrictions and side payments were used to enforce \\( q^* \\).  \n4. **Result**: Bargaining outcomes were mutually advantageous, suggesting Coasian solutions can work in complex environments.",
    "question_context": "Plott (1983) establishes that there is indeed a behavioural externality problem to solve, in the sense that competitive markets with externalities do converge to a private equilibrium that ignores social costs.\nThe Coase Theorem (1960) is behaviourally ‘alive and well' in relatively sterile and abstract bargaining environments, as demonstrated by Harrison and McKee (1985) and Hoffman and Spitzer (1982; 1986).\nThe Coasian approach implies two distinct behavioural outcomes: (i) that the parties will agree on a Pareto optimal level for the externality-generating activity, and (ii) that any such agreement will be attained by means of a mutually advantageous bargain between the parties.\nSubjects were able, after several periods of learning, both to identify the social optimum quantity and to agree to enforce a voluntary restriction on that quantity.\n\nThis paper evaluates Coasian solutions to the externality problem in a series of experimental markets, extending previous studies by incorporating richer informational environments and decentralized Coasian bargains."
  },
  {
    "qid": "econ-empirical-71-6-3-0",
    "question": "7) Using the global regression specification $\\Delta y_{t+3} = \\alpha + \\beta \\Delta_{3}d_{t-1}^{HH} + \\epsilon_t$, calculate the implied cumulative three-year output loss from a persistent 5 percentage point increase in global household debt/GDP.",
    "gold_answer": "1. Coefficient $\\beta = -1.094$ (from Table XIII).  \n2. A 5 pp increase in $\\Delta_3 d_{t-1}^{HH}$ → $\\Delta y_{t+3} = -1.094 * 5 = -5.47$ pp.  \n3. Annualized growth effect: $-5.47/3 = -1.82$ pp per year.  \n4. Cumulative output loss: $\\sum_{k=1}^3 (1 + g_{-k}) \\prod_{j=1}^k (1 - 0.0182) - 1 ≈ -5.3$% of GDP.",
    "question_context": "The global household debt variable has strong predictive power for GDP growth in country i. But the increase in the household debt to GDP ratio for country i also has predictive power in addition to the global factor.\nA one standard deviation increase in global household debt to GDP ratio (2.0) predicts a 2.2% decline in GDP growth over the next three years.\n\nThe text examines the global synchronization of household debt cycles and their predictive power for worldwide GDP growth."
  },
  {
    "qid": "econ-empirical-94-0-0-1",
    "question": "2) Explain the theoretical justification for using trade balances to infer quality, referencing the industrial organization literature.",
    "gold_answer": "1. Industrial organization literature (e.g., Berry 1994) uses market shares to infer unobserved consumer valuation.\n2. Here, trade balance acts as a proxy for global market share.\n3. Consumers prefer higher quality at identical prices, leading to higher demand (and trade surplus) for higher-quality goods.\n4. Thus, trade balance reveals quality differences conditional on prices.",
    "question_context": "We develop a method for decomposing countries’ observed export prices into quality versus quality-adjusted components using information contained in trade balances. Holding observed export prices constant, countries with trade surpluses are inferred to offer higher quality than countries running trade deficits.\nOur method for identifying countries’ product quality involves decomposing observed export prices into quality versus quality-adjusted-price components. We define quality to be any tangible or intangible attribute of a good that increases all consumers’ valuation of it.\n\nThe paper introduces a method for decomposing countries' observed export prices into quality versus quality-adjusted components using trade balance information. It addresses the limitations of using export prices as proxies for quality by accounting for horizontal and vertical differentiation."
  },
  {
    "qid": "econ-empirical-1209-2-1-0",
    "question": "3) Derive the expected sign of the coefficient on the inflation variable (I) in Equation (1) and justify your reasoning.",
    "gold_answer": "1. The coefficient on inflation (I) is expected to be negative. \\n2. As inflation increases, the real value of fixed nominal awards decreases. \\n3. Higher inflation erodes the purchasing power of the award over time.",
    "question_context": "To measure the effects of inflation on the current real value of the award, it is necessary to know the original date of the award. The CPS does not contain such information. But it does contain the date of marital dissolution for women who were previously married. For purposes of this study it is assumed that for those women, the date of the award can be approximated by the date of the marital dissolution. For never-married women, it is assumed that the date of the award can be approximated by the date of birth of the youngest child.\nGiven the date of the award, a variable is constructed that measures the cumulative rate of inflation (using the CPs) from the date of the award to the survey year in the CPS (the value of this variable for all possible award dates in the CPS is given in the Appendix). Because the sample is restricted to families with children under 18, the earliest possible award date is 1961 (for the 1978 CPS).\n\nInflation affects the real value of child support awards over time. The study constructs a variable measuring cumulative inflation from the award date to the survey year."
  },
  {
    "qid": "econ-empirical-372-2-0-1",
    "question": "2) Explain why the power of the $t$-test drops significantly at the 90% autocontour for the Student-t DGP with $\\nu=5$. Provide a mathematical justification using the difference $p_{i}(\\eta)-p_{i}(0)$.",
    "gold_answer": "1. The power drop occurs because the null and alternative hypotheses are very close at the 90% autocontour.  \n2. From Table 3, for $\\nu=5$, $p_{i}(\\eta)-p_{i}(0) = -0.0017$, which is an order of magnitude smaller than other autocontours.  \n3. The asymptotic power depends on $\\sqrt{T}|p_{i}(\\eta)-p_{i}(0)|$. For small differences, a very large $T$ is needed for this term to dominate.  \n4. For $\\nu=5$ and $i=11$, even with $T=125,000$, the rejection rate is only 60%, confirming the difficulty in distinguishing $H_0$ and $H_1$ at this contour.",
    "question_context": "1. $y_{t}=\\mu+\\sigma\\varepsilon_{t}$ , $\\varepsilon_{t}=\\phi\\varepsilon_{t-1}+u_{t}$ where $u_{t}\\sim$ iid ${\\bf N}(0,1-\\phi^{2})$ . 2. $y_{t}=\\mu+\\sigma\\varepsilon_{t}\\sqrt{(\\nu-2)/\\nu}$ , where $\\varepsilon_{t}\\sim$ iid Student- $t(\\nu)$ . 3. $y_{t}=\\mu+\\sigma\\varepsilon_{t}$ , $\\varepsilon_{t}=\\sqrt{h_{t}}u_{t}$ where $u_{t}\\sim$ iid ${\\mathrm{N}}(0,1)$ , $h_{t}=$ $\\omega+\\alpha\\varepsilon_{t-1}^{2}+\\beta h_{t-1}$ .\nThe asymptotic power of the $t$ -test, say $\\Pi_{T}(\\eta)$ , for a fixed contour $i$ and a fixed lag $k$ at the $\\alpha$ -significance level is given by: $$\\begin{array}{r l}&{\\Pi_{T}(\\eta)=P\\big[\\sqrt{T}|\\hat{p}_{i}^{k}-p_{i}(\\eta)|}\\ &{\\qquad\\geq\\sigma_{k,i}(0)z_{1-\\alpha/2}-\\sqrt{T}|p_{i}(\\eta)-p_{i}(0)|\\big]}\\ &{\\qquad\\to1-\\Phi\\bigg(\\frac{\\sigma_{k,i}(0)z_{1-\\alpha/2}-\\sqrt{T}|p_{i}(\\eta)-p_{i}(0)|}{\\sigma_{k,i}(\\eta)}\\bigg)}\\ &{\\qquad+\\Phi\\bigg(\\frac{\\sigma_{k,i}(0)z_{\\alpha/2}-\\sqrt{T}|p_{i}(\\eta)-p_{i}(0)|}{\\sigma_{k,i}(\\eta)}\\bigg).}\\end{array}$$\n\nThe section analyzes the power properties of test statistics under three alternative data generating processes (DGPs) with standardized residuals from location-scale models. The null hypothesis is $\\varepsilon_{t}\\sim$ iid ${\\mathrm{N}}(0,1)$ for all cases. The DGPs include autoregressive (AR), independent and identically distributed (iid) Student-t, and generalized autoregressive conditional heteroscedasticity (GARCH)(1,1) models."
  },
  {
    "qid": "econ-empirical-463-0-0-0",
    "question": "1) Derive the seller's optimal strategic ex ante contract that maximizes joint opportunity costs and explain how it mitigates ex post rent seeking.",
    "gold_answer": "1. Define the joint opportunity cost as the sum of the seller's and the first buyer's costs.\\n2. Formulate the contract to maximize the joint surplus, ensuring the first buyer's participation constraint is met.\\n3. The contract includes an upfront transfer $t_0$ and a mechanism that commits the seller to a higher probability of trade, reducing ex post rent seeking.\\n4. The optimal contract satisfies: $\\max_{t_0, q(\\cdot)} \\mathbb{E}[t_0 + q(v_1, v_2) \\cdot (v_1 - t_0)]$ subject to the first buyer's budget constraint $t_0 \\leq k$.",
    "question_context": "The seller’s optimal strategic ex ante contract more accurately reflects joint opportunity costs of the seller and the contracted buyer, and therefore extracts more rent from the entrant.\nThe seller and the first buyer could make an upfront transfer to achieve different splits of their joint surplus. Because the upfront transfer is limited by the first buyer’s ex ante wealth, the rent extracted and trade facilitated also are restricted.\nThe seller’s optimal ex ante contract can have two effects upon social efficiency compared to the absence of ex ante contracts. First, the contract facilitates more trade. Second, this optimal ex ante contract affects the allocation between the two buyers, because the first buyer enjoys a competitive advantage.\n\nThis article examines the strategic use of ex ante contracts between a seller and an uninformed buyer prior to an auction involving two potential buyers. The optimal ex ante contract reflects joint opportunity costs and mitigates the seller's ex post rent seeking, potentially increasing social welfare."
  },
  {
    "qid": "econ-empirical-967-0-0-2",
    "question": "3) Derive the visible performance effect \\(E(Y^1 - Y^0|X)\\) in terms of the subpopulation performance effects and their probabilities.",
    "gold_answer": "The visible performance effect is derived as:\n\\[\nE(Y^1 - Y^0|X) = E(Y^1 - Y^0|Q^0=1, Q^1=1, X) \\cdot P(Q^0=1, Q^1=1|X) + E(Y^1|Q^0=0, Q^1=1, X) \\cdot P(Q^0=0, Q^1=1|X) - E(Y^0|Q^0=1, Q^1=0, X) \\cdot P(Q^0=1, Q^1=0|X).\n\\]\nThis breaks down the effect into contributions from always participants, changers, and reverse changers, weighted by their respective probabilities.",
    "question_context": "In a sample-selection model with the ‘selection’ variable Q and the ‘outcome’ variable Y*, Y* is observed only when Q=1. For a treatment D affecting both Q and Y*, three effects are of interest: ‘participation’ (i.e., the selection) effect of D on Q, ‘visible performance’ (i.e., the observed outcome) effect of D on Y ≡ QY*, and ‘invisible performance’ (i.e., the latent outcome) effect of D on Y*.\nConsider a binary treatment D (=0,1) affecting a binary ‘selection’ variable Q(=0,1) and an ‘outcome’ variable Y* observed only when Q=1, where Q may denote participation (in an activity) and Y* denotes the performance in the activity. Assume Y*≥0 so that Y*=0 denotes the worst (or lack of) performance. The observed outcome variable is Y ≡ QY*. Also define a k×1 covariate vector X that is not affected by D. D=1 constitutes the treatment group, and D=0 the control group.\n\nThe paper discusses a sample-selection model with a binary treatment D affecting both a binary selection variable Q and an outcome variable Y*, where Y* is observed only when Q=1. Three effects of interest are identified: participation effect (D on Q), visible performance effect (D on Y ≡ QY*), and invisible performance effect (D on Y*). The paper explores conditions for identifying these effects and proposes nonparametric estimators."
  },
  {
    "qid": "econ-empirical-75-2-0-2",
    "question": "3) Explain the econometric justification for including 'operating cost controls' in column (3) of Table II. How do these controls address potential omitted variable bias in estimating the causal effect of Southwest's entry threat?",
    "gold_answer": "1. Operating cost controls capture airport-specific cost shocks that could confound the estimated effect of Southwest's entry threat. \\n2. By comparing fares on threatened routes to fares on non-threatened routes from the same airports, these controls isolate the variation due to Southwest's entry threat. \\n3. The positive coefficients on the controls (e.g., $0.158^{**}$) indicate that cost shocks affect both threatened and non-threatened routes, validating their use. \\n4. The robustness of the entry threat coefficients after inclusion suggests that preemptive price cuts are not driven by cost shocks.",
    "question_context": "Incumbent fares drop significantly before Southwest begins flying the route. By the time Southwest starts operating on both sides of the route (period $t_{0}$), prices are $17\\%$ lower $(\\exp(-0.186)=0.830)$ than in the excluded period.\nOn routes where Southwest threatens but does not enter for at least three quarters, fares are $24\\%$ below the excluded period, and are significantly lower than the average level at $t_{0}$.\nOnce Southwest actually enters the route at time $t_{e}$, prices fall $21\\%$ below the baseline and then continue falling to over $29\\%$ by the end of the period.\nThe results suggest that preemptive price cuts are important. More than half the total price effect that Southwest Airlines has on incumbents' prices takes place before Southwest ever actually starts flying the route.\n\nThis section analyzes the impact of Southwest Airlines' potential entry on incumbent carriers' pricing strategies, focusing on preemptive price cuts before actual entry occurs."
  },
  {
    "qid": "econ-empirical-829-2-1-1",
    "question": "6) Compare the estimated obsolescence rate $\\gamma$ with the traditional value of 15\\%. Explain the implications of $\\gamma < 1$ versus $\\gamma = 1$ for the role of R&D stocks versus flows.",
    "gold_answer": "1. Traditional $\\gamma = 0.15$ implies slow R&D depreciation. Estimated $\\gamma$ (e.g., 0.38) implies faster obsolescence. \\n2. $\\gamma < 1$: R&D stocks (cumulative past R&D) drive innovations. \\n3. $\\gamma = 1$: R&D flows (current R&D) drive innovations. \\n4. Economic implication: Higher $\\gamma$ reduces persistence of R&D benefits, aligning with valuation-based approaches.",
    "question_context": "The estimate for $\\lambda$ indicates that the underlying profitability process $z$ jumps by an economically significant $22.2\\%$ when firms realize innovations. The estimate for $a$ translates to an average success rate of about $83\\%$ . Together, these imply an expected increase in the underlying profitability process due to R&D expenditures of $18.5\\%$ .\nThe estimated obsolescence rate for R&D investment, $\\gamma$ , is higher than the value of $15\\%$ traditionally employed in the production-function literature. Hall (2010) obtains depreciation rates in the range of $20{-}40\\%$ using a valuation-based approach.\nThe estimated curvature of the profit function, $\\theta$ , is lower than the estimates obtained by Cooper and Haltiwanger (2006). Assuming a capital share of $1/3$ and applying equation (2), one obtains an implied price elasticity of demand of 2.97 $(=1/0.337)$ .\n\nThe text presents estimation results for the structural model, including parameter estimates and matched moments for all R&D firms and selected R&D-intensive industries. Key findings include the economic significance of R&D parameters, heterogeneity across industries, and the model's performance in matching moments."
  },
  {
    "qid": "econ-empirical-1377-3-0-0",
    "question": "1) Derive the patient's utility function for hospital choice $u_{i l h t}$ and explain the role of each component, including $T_{i h t}$, $x_{h t}$, $\\nu_{i l t}$, and $\\gamma_{h}$.",
    "gold_answer": "The utility function for hospital choice is:\n\n$$\nu_{i l h t} = \\phi_{i l h t}(\\mathrm{Hospitals}) + \\varepsilon_{i l h t}\n$$\n\nwhere:\n\n$$\n\\phi_{i l h t}(\\mathrm{Hospitals}) = T_{i h t}\\lambda_{1} + T_{i h t}\\nu_{i l t}\\lambda_{2} + T_{i h t}x_{h t}\\lambda_{3} + x_{h t}\\nu_{i l t}\\lambda_{4} + \\mathbf{1}(i h_{t}=i h_{t-1})\\lambda_{5} + \\gamma_{h}\n$$\n\n- **$T_{i h t}$**: Distance from patient $i$'s location to hospital $h$.\n- **$x_{h t}$**: Observed hospital characteristics (e.g., quality, services).\n- **$\\nu_{i l t}$**: Observed patient characteristics (e.g., diagnosis severity).\n- **$\\gamma_{h}$**: Hospital fixed effects capturing unobserved time-invariant hospital quality.\n- **$\\mathbf{1}(i h_{t}=i h_{t-1})$**: Indicator for prior use of hospital $h$ (capturing loyalty or familiarity).\n- **$\\varepsilon_{i l h t}$**: Type 1 extreme value error term.",
    "question_context": "The final stage of the model involves patient $i$ enrolled in insurance plan $j$ choosing a provider in time $t$. The patient either has a condition that requires hospital care, $l$, in which case he or she chooses a hospital $h$ from among a set of in-network hospitals, $N_{j t}^{H}$, or the patient requires procedure $r$ from specialist type $s$, in which case he or she chooses physician practice $d$ among a set of practices of that specialty, $N_{j t}^{S}$.\nConsumer utility for patient of type $i$, with either illness $l$ or procedure $r$, from visiting a provider takes the following form: \n\n$$\n\\begin{array}{r l}&{u_{i l h t}=}\\ &{\\underbrace{T_{i h t}\\lambda_{1}+T_{i h t}\\nu_{i l t}\\lambda_{2}+T_{i h t}x_{h t}\\lambda_{3}+x_{h t}\\nu_{i l t}\\lambda_{4}+\\mathbf{1}\\big(i h_{t}=i h_{t-1}\\big)\\lambda_{5}+\\gamma_{h}}_{\\phi_{i l h t}(\\mathrm{Hospitals})}+\\varepsilon_{i l h t}}\\ &{u_{i r d t}^{s}=}\\ &{\\underbrace{T_{i d t}^{s}\\lambda_{1}+T_{i d t}^{s}\\nu_{i r t}\\lambda_{2}^{s}+T_{i d t}^{s}x_{d t}^{s}\\lambda_{3}^{s}+x_{d t}^{s}\\nu_{i r t}\\lambda_{4}^{s}+\\mathbf{1}\\big(i d_{t}^{s}=i d_{t-1}^{s}\\big)\\lambda_{5}^{s}+\\gamma_{d}^{s}}_{\\phi_{i r d t}^{s}(\\mathrm{PhysicianSpecially})}+\\varepsilon_{i r d t}^{s}}\\end{array}\n$$\nThe utility of household $I$ for plan $j$ at time $t$ is given by: \n\n$$\n\\begin{array}{r l}&{u_{I j t}=}\\ &{\\underbrace{-r_{I j t}\\alpha_{I}+E U_{I j t}^{H}\\beta_{1}+\\underset{s}{\\sum}E U_{I j t}^{s}\\beta_{2I}^{s}+\\mathbf{1}\\big(I j_{t}=I j_{t-1}\\big)\\beta_{3}+x_{j t}\\beta_{4}+\\eta_{j}}_{\\delta_{I j t}}+\\omega_{I j t}.}\\end{array}\n$$\nThe coefficients for network utility are specified as: \n\n$$\n\\beta_{2I}^{s}=\\beta_{2}^{s}+\\nu_{I}^{s},\n$$\n\nwhere $\\nu_{I}^{s}\\sim N(0,\\sigma^{s})$.\nThe market share of households of type $I$ for plan $j$ in market $t$ is derived as: \n\n$$\ns_{I j t}=\\int\\frac{e x p\\big(\\delta_{I j t}\\big)}{\\displaystyle\\sum_{k=1}^{J}e x p\\big(\\delta_{I k t}\\big)}F(\\beta)d\\beta.\n$$\n\nThe model describes a four-stage process involving employer product selection, premium setting, consumer plan choice, and provider selection based on illness. It incorporates detailed utility functions for both hospital and physician choices, accounting for observed characteristics, distances, and fixed effects."
  },
  {
    "qid": "econ-empirical-88-2-0-3",
    "question": "4) Derive the final estimable equation after applying the logistic transformation to the route share variables. Explain the assumptions about the error term in this transformed equation.",
    "gold_answer": "1. Start with the logistic transformation of the structural equation: $$L G S H A R E_{i j}=\\Gamma(X_{i j}^{1})+\\Omega(X_{i j}^{2})+\\epsilon_{i j}.$$\n2. Write the analogous equation for $L G S H A R E_{j i}$: $$L G S H A R E_{j i}=\\Gamma(X_{j i}^{1})+\\Omega(X_{j i}^{2})+\\epsilon_{j i}.$$\n3. Subtract the second equation from the first: $$L G S H A R E_{i j}-L G S H A R E_{j i}=\\Gamma(X_{i j}^{1})-\\Gamma(X_{j i}^{1})+\\epsilon_{i j}-\\epsilon_{j i}.$$\n4. Assume $\\Gamma$ is linear in $X^{1}$: $$L G S H A R E_{i j}-L G S H A R E_{j i}=\\gamma(X_{i j}^{1}-X_{j i}^{1})+\\epsilon_{i j}-\\epsilon_{j i}.$$\n5. The error term $\\epsilon_{i j}-\\epsilon_{j i}$ is assumed to be i.i.d. normal, as the differencing and transformation are intended to produce normally distributed errors.\n6. The intercept is omitted because differencing eliminates it, though an insignificant intercept is sometimes included to account for potential systematic biases in airport listing order.",
    "question_context": "The estimation of the effect of airport dominance takes advantage of the fact that many variables thought to affect route share take on the same value regardless of which end of a city-pair market is the origin of a customer's round-trip. Thus, if the structural equation for route share is additively separable in these factors, they can be eliminated from the estimation by studying the difference in an airline's share of the passengers originating at each endpoint of a route. I posit the following functional form: $$\\begin{array}{r}{S H A R E_{i j}=\\Gamma(X_{i j}^{1})+\\Omega(X_{i j}^{2})+\\epsilon_{i j},}\\end{array}$$ where $S H A R E_{i j}$ is an airline's share of the round-trip traffic on the route between point $\\romannumeral1$ and point j that originates at point i. The first set of right-hand-side variables, the vector $X^{1}$ , differs within a route as a function of the origination point of the passenger's trip, $X_{i j}^{1}\\neq$ $X_{j i}^{1}$ , while the second set of variables does not, $X_{i j}^{2}=X_{j i}^{2}$ .A stochastic component of route share by point of origin, $\\epsilon_{i j},$ is also assumed. If we then consider the difference in an airline's share of traffic on a route by point of origin, the second set of variables is eliminated: $$S H A R E_{i j}-S H A R E_{j i}=\\Gamma(X_{i j}^{1})-\\Gamma(X_{j i}^{1})+\\upepsilon_{i j}-\\upepsilon_{j i}.$$\nThis equation could be estimated as a linear function, but the estimates would suffer some bias due to the limited range of the dependent variable, [-1,1], which implies a nonnormal distribution of the error term. Since there is no obvious functional relationship that presents itself as superior to all others, this difficulty is most easily handled with a logistic transformation of each route share variable before the differencing. Thus, instead of (1), I assume that the underlying relationship is $$L G S H A R E_{i j}=\\Gamma(X_{i j}^{1})+\\Omega(X_{i j}^{2})+\\epsilon_{i j},$$ where $L G S H A R E_{i j}=\\ln[S H A R E_{i j}/(1-S H A R E_{i j})]$ .The difference in an airline's share on a route by point of origin—the equation thatIestimate--becomes $$L G S H A R E_{i j}-L G S H A R E_{j i}=\\gamma*[X_{i j}^{1}-X_{j i}^{1}]+\\epsilon_{i j}-\\epsilon_{j i}.$$\nThe logistic transformation is not defined when the variable is zero or one, so observations in which either by-origin route share, $S H A R E_{i j}$ or $S H A R E_{j i}$ , is zero or one must be excluded from the sample. In fact, also excluded from the results presented are routes on which the observed carrier has less than 5 percent or more than 95 percent of the total traffic on the route in both directions $(R U T S H A R E_{i j}<0.05$ or RUTSHARE;, > 0.95). Nearly all routes excluded due to the restriction from the logistic transformation would also be excluded on the second criterion. The justification for this sample selection is that if a carrier has less than 5 percent of the traffic, it is not really an active competitor in the market, while a share of more than 95 percent is indicative of monopoly. In neither of these cases is the model likely to have as much predictive power.\n\nThe estimation of the effect of airport dominance takes advantage of the fact that many variables thought to affect route share take on the same value regardless of which end of a city-pair market is the origin of a customer's round-trip. The structural equation for route share is additively separable in these factors, allowing them to be eliminated from the estimation by studying the difference in an airline's share of the passengers originating at each endpoint of a route."
  },
  {
    "qid": "econ-empirical-1024-2-0-3",
    "question": "4) The Cambridge Growth Project model is described briefly in Appendix A. Propose a structural framework for such a dynamic model, including key equations for GDP, employment, and unemployment.",
    "gold_answer": "1. **GDP**: \\(Y_t = A_t K_t^\\alpha L_t^{1-\\alpha}\\) (Cobb-Douglas).  \n2. **Employment**: \\(L_t = L_{t-1} (1 + g_L)\\), where \\(g_L\\) is labor growth.  \n3. **Unemployment**: \\(U_t = \\bar{L}_t - L_t\\), where \\(\\bar{L}_t\\) is labor force.  \n4. **Dynamic links**: Investment \\(\\rightarrow K_t\\), policy shocks \\(\\rightarrow A_t\\).",
    "question_context": "According to the standard view, over the period 1978-1985, GDP in constant 1975 prices will grow at an annual average rate of 1.0%, employment will fall by 0.6% p.a., unemployment will rise to 2.9 million (at least), the manufacturing sector will continue to decline, and the employment disparities between the regions are likely to persist.\nThe macro-economic model used for the simulation exercise is the dynamic model developed by members of the Cambridge Growth Project and described, all too briefly, in five pages in Appendix A.\n\nThis book presents a medium-term simulation exercise (1978-1985) comparing a standard view of the economy under present policies with selected alternatives. The Warwick Manpower Research Group (MRG) uses a dynamic model to project GDP growth, employment trends, and unemployment rates, providing 'points of reference' for policymakers."
  },
  {
    "qid": "econ-empirical-579-2-0-3",
    "question": "4) Using the data from Table 2, calculate the implied effect size of summer school on math achievement for Grade 5, given the reduced form estimate of 0.093 and the first-stage estimate of 0.385.",
    "gold_answer": "1. The reduced form estimate \\( \\pi_1 = 0.093 \\).  \n2. The first-stage estimate \\( \\pi_0 = 0.385 \\).  \n3. The effect size \\( \theta \\) is calculated as \\( \theta = \\pi_1 / \\pi_0 = 0.093 / 0.385 \\approx 0.241 \\).  \n4. This implies a 0.241 standard deviation increase in math achievement (s.e. 0.039).",
    "question_context": "Among students passing one of the achievement tests, barely failing the other generally results in a marked increase in the probability of attending summer school.\nIn both the 3rd and 5th grade, being mandated to summer school for math (i.e., scoring below the cutoff) is associated with a 38 percentage point (s.e.: .016) increase in the chance of attending summer school.\nIn both Grades 3 and 5, being mandated to summer school for reading causes about a 44% increase in the probability of summer school attendance.\n\nThe text discusses the impact of accountability policies on summer school attendance, focusing on discontinuities in the probability of attending summer school based on baseline math and reading scores."
  },
  {
    "qid": "econ-empirical-79-62-1-0",
    "question": "1) Using the data from Table I, calculate the net effect of rain on the supply of taxi rides, given the changes in occupancy rates and the number of cabs on the street.",
    "gold_answer": "1. **Occupancy rate effect**: \\[ 1 + 0.048 = 1.048 \\]\n2. **Supply reduction effect**: \\[ 1 - 0.071 = 0.929 \\]\n3. **Net effect**: \\[ 1.048 \\times 0.929 = 0.974 \\]\nThe net supply of taxi rides decreases by approximately 2.6% when it is raining.",
    "question_context": "Rain may have a number of effects on the market for taxi rides. First, it likely increases demand. This will make it easier for drivers to find passengers and increase hourly income. However, rain may decrease speed due to congestion and a deterioration in general driving conditions, which could result in lower earnings.\nThe hourly wage is not significantly correlated with whether it rained in Central Park. There is considerable systematic variation in average hourly earnings over the course of the day, week, month, and year, but accounting for whether it was raining does not significantly improve the fit of the model.\nTaxi occupancy rates are 4.8 percent higher when it is raining, accounting for systematic variation over time. Miles traveled with a passenger are about 2.4 percent lower when it is raining. The number of cabs on the street is about 7.1 percent lower when it is raining.\n\nThe text explores the hypothesis that target earnings behavior exacerbates the difficulty of finding a taxi in the rain, using empirical data to analyze the effects of rainfall on taxi market outcomes."
  },
  {
    "qid": "econ-empirical-1293-4-1-0",
    "question": "5) Critique the limitations of using AIC/BIC for selecting the number of components in a mixture of Gaussian processes. Propose an alternative method and justify its potential advantages.",
    "gold_answer": "1. AIC/BIC rely on likelihood penalties, which may not account for nonparametric complexity (e.g., kernel smoothing).\n2. Alternative: Use cross-validation or bootstrapping to estimate predictive accuracy directly.\n3. Advantages:\n   - Avoids assumptions about parameter counts.\n   - Better handles high-dimensional or functional data.\n4. Challenges: Computationally intensive but more robust for complex models.",
    "question_context": "The selection of the number of components is a challenging problem. In this article, we considered a computationally simple approach by fitting a multivariate normal mixtures to a partial data and demonstrated its effectiveness through supermarket data application.\nIt requires further research to adaptively select the number of mixture components using some more complicated methods. We may start with some likelihood-based approaches such as the information criterion method or penalized likelihood, however, a critical issue is to assess the model complexity, that is, the effective number of parameters.\n\nThe discussion highlights challenges in selecting the number of components and proposes future research directions, including model complexity and testing hypotheses in mixture models."
  },
  {
    "qid": "econ-empirical-952-3-1-0",
    "question": "3) Derive the concentration inequality in Lemma A.1 by applying Lemma 2.1 of Chung and Lu (2002), specifying the choices for $p_i$ and $a_i$.",
    "gold_answer": "1. **Setup**: Let $p_i = P\\{i\\in N_Z\\mid\\tilde{Z}\\} = \\pi_Z$ and $a_i = \\sum_{j\\in M_Z} \\tau_j(X_i,Z)1\\{Y_i=j\\}$. \\n2. **Bound Variance**: Compute $\\sum_{i\\in N} a_i^2 p_i \\leq \\overline{\\tau}^2(Z)n\\pi_Z$ using the fact that each student matches at most one college. \\n3. **Apply Chung-Lu**: Use Lemma 2.1 of Chung and Lu (2002) with $X_i = 1\\{i\\in N_Z\\}$ to obtain the exponential bound.",
    "question_context": "$$\\mathbf{P}\\left\\{\\left|n_{Z}\\hat{\\theta}(\\tau)-\\mathbf{E}\\left[n_{Z}\\hat{\\theta}(\\tau)\\mid\\tilde{Z},\\tilde{S}\\right]\\right|\\geq t\\mid\\tilde{Z}\\right\\}\\leq2\\exp\\left(-\\frac{t^{2}}{2\\overline{{\\tau}}^{2}(Z)n\\pi_{Z}+\\left(2\\overline{{\\tau}}(Z)t/3\\right)}\\right).$$\n$$\\sum_{i\\in N}a_{i}^{2}p_{i}=\\sum_{i\\in N}\\left(\\sum_{j\\in M_{Z}}\\tau_{j}(X_{i},Z)1\\{Y_{i}=j\\}\\right)^{2}\\pi_{Z}\\leq\\overline{{\\tau}}^{2}(Z)n\\pi_{Z}.$$\n\nThis lemma provides a concentration bound for the deviation of $n_Z\\hat{\\theta}(\\tau)$ from its conditional expectation, using assumptions from Chung and Lu (2002)."
  },
  {
    "qid": "econ-empirical-108-34-0-1",
    "question": "2) How does the rejection of the 2010 Greek anti-tax evasion bill align with the occupational backgrounds of parliamentarians? Use Table XIII to support your answer.",
    "gold_answer": "Table XIII shows that:\n1. **High-evasion professions dominate parliament**: Medicine, engineering, and education (high-evasion sectors) account for 61.7% of non-lawyer parliamentarians.\n2. **Self-interest hypothesis**: Parliamentarians from these professions may block reforms targeting their own industries.\n3. **Cumulative influence**: Including accounting/finance and tourism, over 80% of parliamentarians represent high-evasion sectors, explaining the bill's rejection.",
    "question_context": "The highest tax evaders are services requiring advanced degrees and certification and whose revenue depends on reputation (e.g., doctors, lawyers, engineers, journalists, tutors, accountants, and financial agents).\nIn 2010, a bill was proposed to the Greek Parliament that targeted a group of 11 occupations as most likely to evade and included provisions for audits of taxpayers in these professions that reported lower income than a prespecified threshold of €20,000.\n\nThe text discusses the distribution of tax-evading industries in Greece, highlighting that professions requiring advanced degrees and certification (e.g., doctors, lawyers, engineers) are among the highest tax evaders. This challenges the conventional view of tax evasion in formal or informal economies, suggesting a unique character of semiformal economies. The text also references a 2010 Greek bill targeting specific professions for audits, which was rejected, and explores the persistence of tax evasion in such economies."
  },
  {
    "qid": "econ-empirical-729-3-0-4",
    "question": "5) Analyze the robustness of the results to alternative modeling frameworks and discuss the implications for policy recommendations.",
    "gold_answer": "1. The study tests robustness by varying weather variable constructions and controlling for confounding factors. \n2. Results are consistent across specifications, suggesting that extreme heat shocks reliably reduce local employment. \n3. Policy implications include: \n   - Targeted support for wage workers during heatwaves. \n   - Programs to mitigate migration pressures caused by weather shocks. \n   - Investments in climate-resilient infrastructure to stabilize labor markets.",
    "question_context": "We estimate the following model: $$E_{i t}^{\\mathrm{s}}=f\\big(W_{m t};\\beta^{\\mathrm{s}}\\big)+\\gamma_{j t}+\\lambda_{i}+\\epsilon_{i t},$$ where $E_{i t}^{\\mathrm{s}}$ is a binary variable indicating whether individual $i$ is employed in sector $s$ in year $t.$ The regressors of interest, $W_{m t}$ are functions of weather in year $t$ and village m. Controls include both state-year $(\\gamma_{j t})$ and individual $(\\lambda_{\\mathrm{i}})$ fixed effects.\nWe begin by estimating the effects of GDDs, HDDs, precipitation $(P_{m t})$ and precipitation-squared on individual employment outcomes: $$E_{i t}^{\\mathrm{s}}=\\beta_{1}^{\\mathrm{s}}\\mathrm{HDD}_{m t}+\\beta_{2}^{\\mathrm{s}}\\mathrm{GDD}_{m t}+\\beta_{3}^{\\mathrm{s}}P_{m t}+\\beta_{4}^{\\mathrm{s}}P_{m t}^{\\mathrm{2}}+\\gamma_{j t}+\\lambda_{i}+\\epsilon_{i t}.$$\nAs shown in column (1), HDDs lead to a meaningful decrease in the probability of being employed locally, with an additional HDD reducing the probability of local work by $0.05\\%$. A one standard deviation increase in HDDs, which translates into an additional $36.5~\\mathrm{HDDs}$, would decrease the probability of local employment from roughly $47.8\\%$ to $45.9\\%$, or $4\\%$.\nThe reduction in local employment is largely driven by a reduction in local wage work. This is consistent with the theoretical prediction that hired labour is sensitive to weather shocks.\nThe results highlight the non-linearity of temperature impacts. An additional growing degree day has little impact on labour markets, while an increase in extreme temperatures has a real and significant impact.\n\nThe study employs a panel data approach to analyze the impact of weather on labor allocation, controlling for individual and state-year fixed effects. The model estimates the probability of employment in various sectors based on weather variables, including harmful degree days (HDDs), growing degree days (GDDs), and precipitation."
  },
  {
    "qid": "econ-empirical-719-0-1-3",
    "question": "4) Analyze the trade-offs between using blocking techniques and the proposed lightweight method for large datasets.",
    "gold_answer": "1. Blocking reduces computational demands by limiting comparisons but may discard true matches.\n2. The lightweight method preserves more comparisons but requires additional indicators.\n3. The choice depends on the dataset size and the prevalence of common field values.",
    "question_context": "Record linkage problems frequently require dealing with what we refer to as the common name problem, which is that many records (usually people) share very common names, birthplaces or other field values and are harder to link than records with uncommon names and field values.\nWe propose an alternative, lightweight method for treatment of very common names. We modify comparison data to include an indicator for very common names that otherwise spuriously produce exact matches in record pairs.\nThis modification slightly improves matching performance in the Union Army application, relative to a baseline model, and adds very little computational expense.\n\nThe text addresses the common name problem in record linkage, where many records share common field values, making matching difficult. It proposes lightweight methods to mitigate this issue without significant computational cost."
  },
  {
    "qid": "econ-empirical-168-1-0-0",
    "question": "1) Derive the refiner's optimal contract design problem under moral hazard, where the manager's effort \\( e \\) is unobservable but affects demand \\( Q(p,e) \\). Assume the refiner maximizes profit \\( \\pi = (w - c)Q(p,e) + F \\), where \\( w \\) is the wholesale price, \\( c \\) is marginal cost, and \\( F \\) is a fixed fee. The manager's utility is \\( U = pQ(p,e) - wQ(p,e) - C(e) + F \\), where \\( C(e) \\) is the cost of effort. Formulate the incentive compatibility (IC) and participation constraints (PC).",
    "gold_answer": "1. **IC Constraint**: The manager chooses \\( e \\) to maximize \\( U \\). First-order condition: \\( \\frac{\\partial U}{\\partial e} = (p - w)\\frac{\\partial Q}{\\partial e} - C'(e) = 0 \\).\n2. **PC Constraint**: \\( U \\geq \\bar{U} \\), where \\( \\bar{U} \\) is the reservation utility.\n3. Refiner's problem: Maximize \\( \\pi \\) subject to IC and PC. Substitute \\( F \\) from PC into \\( \\pi \\).\n4. Solve for \\( w \\) and \\( p \\) that satisfy IC and maximize \\( \\pi \\).",
    "question_context": "The refiner's contracting problem is complicated by limitations on what downstream choices can be covered by contract. The manager's effort may not be subject to direct control because it is unobservable.\nRefiners can control price directly only at stations operated under company-owned contracts. Whether this affects retail prices depends on what instruments the refiner can use to control pricing indirectly at other stations.\nOpen-dealer contracts should occur at stations where unobservable effort is important and income from ancillary service is a large share of downstream profit.\n\nGasoline retailing exemplifies a principal-agent problem in a vertical setting, where the station manager (agent) sells gasoline supplied by the refiner (principal). The contract governs investment sharing, asset management, and aligns incentives. Refiners design contracts to maximize profit, considering local demand elasticity, competition, and station characteristics. Three contractual forms exist: company-owned, lessee-dealer, and open-dealer, each with distinct control rights and incentive structures."
  },
  {
    "qid": "econ-empirical-1818-1-1-2",
    "question": "3) Discuss the implications of assuming \\(\\theta_{3}=0\\) for the independence of \\(y_{1i}^{*}\\) and \\(y_{2i}^{*}\\) and the potential bias in estimates.",
    "gold_answer": "1. **Implication**: \\(\\theta_{3}=0\\) implies conditional independence of \\(y_{1i}^{*}\\) and \\(y_{2i}^{*}\\) given \\(z_{i}\\).\n2. **Bias**: If \\(\\theta_{3}\\neq0\\) in reality, estimates under \\(\\theta_{3}=0\\) will be asymptotically biased. However, simulation-based methods (e.g., indirect inference) can correct this bias.",
    "question_context": "The generalized Tobit model is defined by \\(y_{i}=\\left\\{\\begin{array}{l l}{y_{1i}^{*}}&{\\mathrm{~if~}y_{2i}^{*}\\geq0}\\ {\\mathrm{missing~}}&{\\mathrm{~if~}y_{2i}^{*}<0}\\end{array},\\right.\\) with \\(y_{1i}^{*}=x_{i}^{\\prime}\\theta_{1}+\\sigma\\varepsilon_{i}\\), where \\(\\varepsilon_{i}\\sim\\aleph(0,1)\\).\nThe likelihood function is \\(\\prod_{i\\in I_{1}}\\frac{1}{\\sigma}\\varphi\\left(\\frac{y_{i}-x_{i}^{\\prime}\\theta_{1}}{\\sigma}\\right)\\operatorname*{Pr}\\left[y_{2i}^{*}\\geq0\\left|y_{i},z_{i},\\theta_{2},\\theta_{3}\\right.\\right]\\prod_{i\\in I_{0}}\\operatorname*{Pr}\\left[y_{2i}^{*}<0\\left|z_{i},\\theta\\right.\\right]\\).\nUnder the constraint \\(\\theta_{3}=0\\), the log-likelihood simplifies to a standard Tobit form, and the pseudo-score vector can be computed in closed-form.\n\nThis section illustrates the generalized Tobit model, focusing on its likelihood function and the implications of imposing parameter constraints."
  },
  {
    "qid": "econ-empirical-509-1-0-1",
    "question": "2) Prove Proposition 1: For TCP to hold when future preferences do not depend on unrealizable alternatives, current preferences must satisfy conditional weak independence (CWI).",
    "gold_answer": "Proof of Proposition 1:\n1. Assume TCP holds, i.e., $R^{s}[\\bar{x}, \\bar{y}_{\\sim s}] = Q^{s}[\\bar{x}, \\bar{y}_{\\sim s}]$ for all $s$ and $(\\bar{x}, \\bar{y}_{\\sim s})$.\n2. Since future preferences $Q^{s}$ do not depend on unrealizable alternatives, $Q^{s}[\\bar{x}, \\bar{y}_{\\sim s}]$ must be independent of $\\bar{y}_{\\sim s}$.\n3. Therefore, $R^{s}[\\bar{x}, \\bar{y}_{\\sim s}]$ must also be independent of $\\bar{y}_{\\sim s}$, which is the definition of conditional weak independence (CWI).\n4. Thus, current preferences $R$ must satisfy CWI for TCP to hold under the given condition.",
    "question_context": "Let us first consider the case of an individual decision maker $\\left(d m\\right)$ drawing up a plan for the next two periods. There is no uncertainty in his decision environment in the current period, while any one of $s$ possible states of nature may obtain in the second period (we will also let $s$ represent the set of states). Let $x\\in X$ represent his current action, and $y_{s}\\in Y_{s}$ his future action given that state $s\\in S$ obtains. Both $X$ and $Y_{s}$ are connected sets. A plan is a $(x,y)\\in X\\times Y,$ where $Y=\\prod Y_{s}^{2}$.\nDm's current preferences over plans $(x,y)$ are given by the continuous and strictly increasing ordering $R$ of $X\\times Y.$ His future preferences over actions $y_{s}$ in state $s$ given past consumption $x$ and planned consumption $\\begin{array}{r}{y_{\\sim s}\\in Y_{\\sim s}\\equiv\\prod_{s^{\\prime}\\neq s}Y_{s}}\\end{array}$ for states other than s, are described by the ordering $\\begin{array}{r}{\\mathcal{Q}^{s}[x,y_{\\sim s}];\\{R,Q\\}\\equiv\\{R,(Q^{\\bar{s}})\\dot{s}\\in S\\}}\\end{array}$ is said to represent dm's dynamic preferences.\nDEFINITION: $\\{R,Q\\}$ allow TCP if $R^{s}[\\bar{x},\\bar{y}_{\\sim s}]=Q^{s}[\\bar{x},\\bar{y}_{\\sim s}]$ for every $s$ and every $(\\bar{x},\\bar{y}_{\\sim s})\\in X\\times Y_{\\sim s}$.\nPROPOsITION 1: For TCP to hold when future preferences do not depend on unrealizable alternatives, current preferences must satisfy conditional weak independence (CWI).\nPROPOsiTION 2: Initial preferences R admit TCPiff there exists continuous and monotone increasingfunctions $\\begin{array}{r l}&{f\\colon X\\times R^{s}\\to R,}\\\\ &{u_{s}\\colon X\\times Y_{s}\\to R,\\quad s\\in S,\\quad s u c h t h a t}\\\\ &{u(x,y)=f(x,(u_{s}(x,y_{s})/s\\in S)).}\\end{array}$\n\nThe text discusses a two-period planning model for an individual decision maker (dm) with no uncertainty in the current period and s possible states of nature in the second period. The dm's preferences over plans are given by a continuous and strictly increasing ordering R. The future preferences over actions in each state are described by the ordering Q. The concept of time consistent planning (TCP) is introduced, which requires that the current preference ordering R, when restricted to future actions in each state, agrees with the future orderings Q."
  },
  {
    "qid": "econ-empirical-523-1-0-2",
    "question": "3) Show how the Kuhn-Tucker conditions for the utility maximization problem with nonnegativity constraints lead to the regime-switching conditions $\\pi_i(\\bar{v}) \\leq v_i$ for $i = 1, \\dots, l$.",
    "gold_answer": "1. The Lagrangean is: $$L = U(q) + \\lambda(1 - v q) + \\psi q,$$ where $\\psi_i \\geq 0$ are multipliers for the nonnegativity constraints.\n2. The Kuhn-Tucker conditions for $x_i = 0$ ($i = 1, \\dots, l$) are: $$\\frac{\\partial U}{\\partial q_i} - \\lambda v_i + \\psi_i = 0, \\quad \\psi_i \\geq 0.$$\n3. For $x_i = 0$, the virtual price is: $$\\pi_i(\\bar{v}) = \\frac{\\partial U/\\partial q_i}{\\lambda}.$$\n4. Substituting into the Kuhn-Tucker condition: $$\\pi_i(\\bar{v}) = v_i - \\frac{\\psi_i}{\\lambda} \\leq v_i,$$ since $\\psi_i \\geq 0$ and $\\lambda > 0$.",
    "question_context": "Let $H(v;\\theta,\\varepsilon)$ be an indirect utility function defined as $$H(v;\\theta,\\varepsilon)=\\operatorname*{max}_{q}\\left\\{U(q;\\theta,\\varepsilon)|v q=1\\right\\}$$ where $U(\\mathbf{\\lambda};\\pmb{\\theta},\\pmb{\\varepsilon})$ is a strictly quasi-concave utility function defined on $\\kappa$ commodities, $v$ is a vector of normalized market prices, $\\pmb{\\theta}$ a vector of unknown parameters, and $\\varepsilon$ a vector of random components.\nApplying Roy's Identity, the notional demand equations $D(v;\\theta,\\varepsilon)$ for a set of $\\kappa$ goods are $$q_{i}=\\frac{\\partial H(v;\\theta,\\varepsilon)}{\\partial v_{i}}\\Bigg/\\sum_{j=1}^{K}v_{j}\\frac{\\partial H(v;\\theta,\\varepsilon)}{\\partial v_{j}}$$ $$(i=1,\\ldots,K).$$\nThe notional demands $q_{i}$ are thus latent variables which correspond to a vector of nonnegative observed demands $\\left(x_{i}\\right)$ as follows. There exist vectors of positive virtual prices $\\pmb{\\pi}$ which can exactly support these zero demands (or any other allocation) as long as the preference function is strictly quasi-concave, continuous, and strictly monotonic (Neary and Roberts (1980)).\nIf demands for the first $\\imath$ goods are zero, the virtual prices $\\pi_{i}(v_{l+1},\\dots,v_{K})$ are solved from the equations $$0=\\partial H(\\pi_{1}(\\bar{v}),\\ldots,\\pi_{l}(\\bar{v}),\\bar{v};\\theta,\\varepsilon)/\\partial v_{i}$$ $$(i=1,\\ldots,l)$$ where $\\pi_{i}(\\bar{v})$ is the virtual price of the ith good and $\\bar{v}$ is the set of market prices of the positively consumed goods $l+1$ to $\\kappa.$\nThe remaining (positive) demands are $$x_{i}=\\frac{\\partial H(\\pi_{1}(\\bar{v}),\\ldots,\\pi_{l}(\\bar{v}),\\bar{v};\\theta,\\varepsilon)}{\\partial v_{i}}\\Bigg/{\\sum_{j=1}^{K}v_{j}\\frac{\\partial H(\\pi_{1}(\\bar{v}),\\ldots,\\pi_{l}(\\bar{v}),\\bar{v};\\theta,\\varepsilon)}{\\partial v_{j}}}$$\nThe regime in which the first $\\iota$ goods are not consumed is characterized by the conditions $$\\pi_{i}(\\bar{v})\\leqslant v_{i}$$ $$(i=1,\\ldots,l).$$\nThe Lagrangean function with utility maximization subject to nonnegativity constraints is $$L=U(q)+\\lambda\\left(1-v q\\right)+\\psi q$$ where $\\pmb{\\lambda}$ and $\\psi$ are Lagrange multipliers and the parameters $\\pmb\\theta$ and $\\pmb\\varepsilon$ are suppressed for notational simplicity.\nThe virtual price for good $i(i=1,\\dots,l)$ at $x$ is simply $$\\pi_{i}(\\bar{v})=\\frac{\\partial U(x)}{\\partial q_{i}}\\Bigg/\\lambda = v_{m}\\frac{\\partial U(x)}{\\partial q_{i}}\\Bigg/\\frac{\\partial U(x)}{\\partial q_{m}}.$$\n\nThis section introduces an indirect utility function and derives notional demand equations using Roy's Identity, discussing virtual prices and their role in demand regimes."
  },
  {
    "qid": "econ-empirical-196-5-0-3",
    "question": "4) Discuss how the introduction of elections into the model might affect the policy maker's incentive to consult advisers, considering both information acquisition and policy justification.",
    "gold_answer": "1. **Information Acquisition**: Elections may reduce the incentive to consult for information if the primary goal shifts to justifying policies.\n2. **Policy Justification**: Advisers may be used to legitimize decisions rather than improve policy quality.\n3. **Electoral Distortions**: The optimal adviser from the policy maker's perspective may diverge from the electorate's preferences, introducing additional agency problems.",
    "question_context": "We have shown that the more an adviser is biased towards one of the policy alternatives, the weaker her incentive to produce information. Hence, to maximise the quality of information, the policy maker should rely on unbiased advisers. However, if the policy maker is biased toward one of the policy alternatives, unbiased advisers manipulate information. To maximise the quality of the recommendation, the preferences of the policy maker and the adviser should coincide.\nAn important assumption underlying our analysis is that information about the effects of policies is not easily verified by policy makers because of a lack of time or expertise on the part of policy makers. Although we believe that this is a typical feature of the policy-making process, policy makers may occasionally be experts themselves or have acquired expertise over time, and hence may be less vulnerable to manipulation of information.\nThroughout the paper, we have assumed that an adviser is always willing to participate, i.e. we have ignored the advisers’ participation constraints. To identify an adviser’s participation constraint we must determine the adviser’s payoff if she does not participate. Clearly, this depends on the behaviour of the policy maker.\nFirst, the policy maker can commit himself on not consulting another adviser, if the selected adviser chooses not to participate. In that case the adviser participates if the expression in (2) is greater than zero for $e=e^{*}$ and $p<0$ .12 If (2) is smaller than zero, then the policy maker has to compensate the adviser.\nSecond, the policy maker cannot commit himself. In that case, the adviser must always be compensated for her effort when sufficient advisers are available. Without a sufficient reward, a free rider problem arises.\n\nThis paper has studied the delegation of information collection by policy makers to policy-motivated advisers. The analysis has centred on two agency problems: the uncertainty about the effort advisers put in producing information and the risk that advisers manipulate information or frame their recommendation."
  },
  {
    "qid": "econ-empirical-862-1-0-3",
    "question": "4) Analyze the robustness of West's conclusions when extending the framework to rational expectations and serially correlated shocks.",
    "gold_answer": "1. Under rational expectations, \\( E_{t-1}p_t \\) incorporates all available information.\n2. Serially correlated shocks introduce persistence, affecting variance dynamics.\n3. West's core condition (\\( \\beta > 1 \\)) remains but requires additional terms for shock persistence.",
    "question_context": "Bean uses a standard, simple aggregate demand curve and an aggregate supply curve similar to those found in the Fischer (1977) and Taylor (1980) contracting models. He measures the desirability of policies in terms of their ability to reduce the variance of output around a certain ‘full information’ level.\nThe purpose of this note is to consider whether these conclusions still hold if Bean's supply curve is replaced with a standard expectations augmented supply curve. It turns out that radically different conclusions can result.\nIn such a case, nominal income targeting is preferable to money supply targeting if and only if the elasticity of demand with respect to real balances is greater than one. This is true for both supply and demand shocks.\n\nThis section discusses the debate around nominal income targeting versus money supply targeting, contrasting the conclusions derived from different aggregate supply curves and policy desirability measures."
  },
  {
    "qid": "econ-empirical-785-5-3-0",
    "question": "6) Prove that in an economy with \\( S \\) states and \\( K < S \\) assets, generic existence of equilibrium requires rank\\([A] = K \\), where \\( A \\) is the payoff matrix.",
    "gold_answer": "1. Define excess demand \\( Z(p) \\).\n2. Apply Sard's theorem to show regular economies are dense.\n3. Use degree theory to establish existence for regular economies.",
    "question_context": "Approximate Arbitrage Pricing in Incomplete Financial Markets\n\nExamines existence of equilibria when asset markets don't span all states."
  },
  {
    "qid": "econ-empirical-785-4-0-2",
    "question": "3) Formulate the model for aggregate output with operating rates and inventories as buffers, and derive the equilibrium conditions for quasi-fixed factors.",
    "gold_answer": "1. Model: \\( Y_t = f(K_t, L_t, U_t, I_t) \\), where \\( U_t \\) is operating rate and \\( I_t \\) is inventory.\n2. Quasi-fixed factors imply adjustment costs: \\( K_{t+1} = (1-\\delta)K_t + I_t \\).\n3. Equilibrium: firms optimize \\( \\max \\sum \\beta^t [p_t Y_t - w_t L_t - c(I_t, K_t)] \\), yielding Euler equations for \\( K_t \\) and \\( I_t \\).",
    "question_context": "Estimation of a Fixed-Effect Cobb-Douglas System Using Panel Data\nMeasuring Technical Efficiency in Brazilian Manufacturing\nAggregate Output with Operating Rates and Inventories as Buffers Between Variable Final Demand and Quasi-Fixed Factors\nEstimating the Parameters of Flexible Functional Form Cost Functions When Some of the Input Shares are Relatively Small\n\nThe papers in this section discuss various econometric techniques and models for estimating production functions and measuring technical efficiency, with applications to different industries and countries."
  },
  {
    "qid": "econ-empirical-232-2-1-0",
    "question": "1) Derive the asymptotic distribution of \\( T_n \\) and show that \\( T_n \\xrightarrow{\\mathcal{L}} N(0, \\text{diag}(1/\\alpha_0, (\\kappa_\\eta - 1)/(4\\alpha_0^2))) \\).",
    "gold_answer": "1. Note that \\( T_n \\) is a sum of i.i.d. random variables.\n2. Apply the central limit theorem to \\( T_n \\).\n3. Compute the covariance matrix:\n   \\[ \\text{Var}(T_n) = \\text{diag}\\left(\\frac{1}{\\alpha_0}, \\frac{\\kappa_\\eta - 1}{4\\alpha_0^2}\\right) \\]\n4. Conclude that \\( T_n \\xrightarrow{\\mathcal{L}} N(0, \\text{diag}(1/\\alpha_0, (\\kappa_\\eta - 1)/(4\\alpha_0^2))) \\).",
    "question_context": "\\( H_n(u) = L_n(\\theta_0 + u) - L_n(\\theta_0) \\), where \\( u \\in \\Lambda := \\{u = (u_1, u_2, u_3)^T : u + \\theta_0 \\in \\Theta\\} \\).\n\\( H_n(\\hat{u}_n) = (\\sqrt{n} \\hat{v}_n)^T T_n + (\\sqrt{n} \\hat{v}_n)^T \\Omega (\\sqrt{n} \\hat{v}_n) + o_p(\\sqrt{n} \\|\\hat{v}_n\\| + n \\|\\hat{v}_n\\|^2) \\), where \\( T_n = -\\frac{1}{\\sqrt{n}} \\sum_{t=1}^{n} \\left(\\frac{\\text{sign}(\\eta_t)}{\\sqrt{\\alpha_0}}, \\frac{|\\eta_t| - 1}{2\\alpha_0}\\right)^T \\) and \\( \\Omega = \\text{diag}(f(0)/\\alpha_0, 1/(8\\alpha_0^2)) \\).\n\nThe proof involves reparameterizing the objective function \\( H_n(u) \\) and analyzing its asymptotic behavior using central limit theorem and Taylor expansions."
  },
  {
    "qid": "econ-empirical-899-2-0-0",
    "question": "1) Derive the fundamental recurrency relation of dynamic programming from the Principle of Optimality and explain why it fails in a dynamic game between rational agents with Muth-rational expectations.",
    "gold_answer": "1. **Principle of Optimality**: The optimal policy satisfies \\( V(x_0) = \\min_{u_0} \\{ g(x_0, u_0) + V(x_1) \\} \\), where \\( V \\) is the value function, \\( g \\) is the cost function, and \\( x_1 = f(x_0, u_0) \\).  \n2. **Recurrency Relation**: For a finite horizon problem, the Bellman equation is \\( V_t(x_t) = \\min_{u_t} \\{ g(x_t, u_t) + V_{t+1}(x_{t+1}) \\} \\).  \n3. **Failure in Dynamic Games**: Rational agents anticipate future policy actions, leading to time-inconsistency. The government's optimal plan at \\( t=0 \\) may not align with its plan at \\( t=1 \\) due to strategic interactions.",
    "question_context": "The Principle of Optimality of Dynamic Programming states that ‘An optimal policy has the property that, whatever the initial state and decision (i.e. control) are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision’ (Bellman, 1957).\nIn dynamic games between optimising agents endowed with rational expectations, it can happen that the optimal plan in subsequent periods is not the continuation of the first-period optimal plan over the remainder of the planning period: the optimal plan is not consistent.\nA two-period food disaster control example can illustrate this. There are two states of the world, good and bad, in period 2. In the bad state a major food occurs; in the good state there is no food. The true state will not be known until the beginning of period 2.\nBy suitably restricting the class of policy rules over which the government can optimise, it will in general be possible to write current decisions as functions only of current and past variables, even when both public and private agents ‘forecast efficiently conditional on their information sets’ (Brunner and Meltzer, 1977, p. 5).\n\nThe text discusses the inconsistency of optimal plans in Muth-rational models, where policy optimization is considered in dynamic games between rational agents. It highlights the failure of traditional optimal control techniques to account for the impact of future policy measures on current behavior through anticipation."
  },
  {
    "qid": "econ-empirical-1027-1-1-2",
    "question": "7) Derive the shadow price interpretation of the demand reservation price for a new factor. How does this relate to the firm's optimization problem?",
    "gold_answer": "1. **Shadow Price**: The reservation price is the Lagrange multiplier associated with the constraint of not using the new factor in the base period.\n2. **Optimization**: It represents the marginal cost of the constraint, i.e., the increase in costs due to the inability to use the factor.\n3. **Mathematical Form**: If $\\lambda$ is the shadow price, then $\\lambda = \\frac{\\partial C}{\\partial v_{\\text{new}}}$ evaluated at the reservation price.",
    "question_context": "The isomorphic case to that of new or disappearing goods in the analysis of the cost-of-living index or of the production-theoretic output price index is that of a new or disappearing factor of production.\nThe bounding property of a Paasche index will be preserved if the factor price so chosen is any price at or above its base period's demand reservation price.\nNew goods do therefore raise a question; the appearance of a new good, like any other technical change, will generally alter the isoquant which determines the production-theoretic input price index.\n\nThe text addresses the treatment of corner solutions in the context of input deflation, comparing it to the case of new or disappearing goods. It emphasizes the role of reservation prices and the bounding properties of Paasche indices."
  },
  {
    "qid": "econ-empirical-60-3-0-0",
    "question": "1) Derive the conditions under which the coefficients $\\phi_{1}$ and $\\phi_{2} - \\phi_{1}$ would be equal in magnitude but opposite in sign, as observed in the empirical results. What does this imply about the validity of the permanent income hypothesis?",
    "gold_answer": "1. Under the permanent income hypothesis, $\\phi_{2} = 0$, so $\\phi_{2} - \\phi_{1} = -\\phi_{1}$.  \n2. The empirical results show $\\phi_{2} - \\phi_{1} \\approx -\\phi_{1}$, which is consistent with the hypothesis.  \n3. The rejection of $\\phi_{2} - \\phi_{1} = 0$ contradicts the Keynesian model, which predicts no such relationship.  \n4. The equality in magnitude but opposite signs supports the lifecycle model's prediction that only unanticipated income changes affect consumption.",
    "question_context": "Column 7 indicates that the coefficient on the income change is only 0.0907 when only lagged $\\Delta X_{t}^{*}$ variables are used as instruments and is not significantly different from 0.\nThe point estimate of $\\phi_{2}\\mathrm{~-~}\\phi_{1}$ is $-0.211$. The hypothesis that it is 0 is rejected, which runs counter to the Keynesian consumption function.\nThe coefficients on the two income terms, which are estimates of $\\phi_{1}=B_{c}b$ and $-\\phi_{1}=-B_{c}b$, with $\\phi_{2}=0$ under the permanent income hypothesis, are in fact opposite in sign and similar in absolute value.\n\nThis section examines the role of predictable and unpredictable changes in income in the consumption function, testing hypotheses from the Keynesian and permanent income theories."
  },
  {
    "qid": "econ-empirical-1-1-1-0",
    "question": "3) Prove that the randomization ensures peers of a firm are random conditional on strata and subregion.",
    "gold_answer": "1. Let \\( S \\) be strata, \\( R \\) be subregion.  \n2. Random ranking within \\( S \\) and \\( R \\) ensures \\( P(\\text{peer assignment}) \\) is uniform.  \n3. Thus, peers are conditionally random.",
    "question_context": "In each subregion we divided firms into four strata: (a) small service, (b) big service, (c) small manufacturing, and (d) big manufacturing. In each strata of each subregion we randomly ranked firms.\nWe randomized treated firms into these groups in each subregion: (i) small firms in the same sector, (ii) large firms in the same sector, (iii) mixed-size firms in the same sector, and (iv) mixed size and mixed sector.\n\nAdditional interventions were introduced to measure peer effects and information diffusion, including stratified randomization by firm size and sector."
  },
  {
    "qid": "econ-empirical-865-2-2-0",
    "question": "5) Derive the total return on housing $r^{h}$ as a function of the housing capital gain $r^{h_{\\mathrm{cg}}}$, rental yield $r^{h_{r}}$, mortgage rate $r^{mort}$, and leverage ratio $\\mathrm{lev}(t)$.",
    "gold_answer": "The total return on housing $r^{h}$ is derived as:\n\n$$\nr^{h} = (1 - \\mathrm{lev}(t)) \\cdot r^{h_{\\mathrm{cg}}} + \\mathrm{lev}(t) \\cdot (r^{h_{r}} - r^{mort})\n$$\n\n**Steps**:\n1. The unleveraged capital gain is $(1 - \\mathrm{lev}(t)) \\cdot r^{h_{\\mathrm{cg}}}$.\n2. The leveraged rental yield net of mortgage costs is $\\mathrm{lev}(t) \\cdot (r^{h_{r}} - r^{mort})$.\n3. The total return combines these components.",
    "question_context": "Return on cash The rate of return on cash is set at the average real return on cash balances, which was $1.6\\%$ between 1952 and 2012 (see Table 1 of Barclays Capital (2012)).\nReturn on housing To specify the return on housing (given in equation (2)), we need to specify two rates of return $(r^{h_{\\mathrm{cg}}}$ —the housing capital gain and $r^{h_{r}}$ —the rental income from owner occupied housing), the mortgage interest rate $(r^{m o r t})$ , and the leverage ratio $(\\mathrm{lev}(t))$.\n\nThe model is parameterized using institutional data and existing literature. Key parameters include returns on assets, housing shares, and demographic assumptions."
  },
  {
    "qid": "econ-empirical-8-1-0-0",
    "question": "1) Derive the firm's profit maximization problem as presented in the text, explicitly stating the components of revenue, training cost, and wage bill.",
    "gold_answer": "1. **Revenue Component**: \n   $$ P_{j}^{0}\\big[T_{j}\\big(G_{j}\\big(w_{j}^{I}\\big)I_{j}+N_{j}\\big)\\big]^{1-\\frac{1}{\\varepsilon}} $$\n   This represents the revenue from selling output, where $P_{j}^{0}$ is the firm-specific price intercept, $T_{j}$ is physical productivity, $G_{j}(w_{j}^{I})I_{j}$ is the number of retained incumbents, and $N_{j}$ is the number of new hires.\n\n2. **Training Cost Component**: \n   $$ c\\left(\\frac{N_{j}}{I_{j}}\\right)I_{j} $$\n   This captures the cost of training and recruiting new workers, which depends on the gross hiring rate $\\frac{N_{j}}{I_{j}}$ and is scaled by the number of incumbents $I_{j}$.\n\n3. **Wage Bill Component**: \n   $$ w_{j}^{m}N_{j} + w_{j}^{I}G_{j}(w_{j}^{I})I_{j} $$\n   This includes the wages paid to new hires ($w_{j}^{m}N_{j}$) and retained incumbents ($w_{j}^{I}G_{j}(w_{j}^{I})I_{j}$).\n\n4. **Profit Maximization Problem**: \n   The firm maximizes profit by choosing $w_{j}^{I}$ and $N_{j}$:\n   $$ \\max_{\\{w_{j}^{I}, N_{j}\\}} \\left[ \\text{Revenue} - \\text{Training Cost} - \\text{Wage Bill} \\right]. $$",
    "question_context": "The firm chooses the number of new hires $N_{j}$ and an incumbent wage $w_{j}^{I}$ to maximize profits. Formally, its problem is to: \n$$\n\\begin{array}{r l}&{\\displaystyle\\operatorname*{max}_{\\left\\{w_{j}^{I},N_{j}\\right\\}}\\underbrace{P_{j}^{0}\\big[T_{j}\\big(G_{j}\\big(w_{j}^{I}\\big)I_{j}+N_{j}\\big)\\big]^{1-\\frac{1}{\\varepsilon}}}_{\\mathrm{revenue}}-\\underbrace{c\\left(\\frac{N_{j}}{I_{j}}\\right)I_{j}}_{\\mathrm{training~cost}}}\\ &{\\quad\\quad\\quad-\\underbrace{w_{j}^{m}N_{j}-w_{j}^{I}G\\big(w_{j}^{I}\\big)I_{j}}_{\\mathrm{wage~bill}}.}\\end{array}\n$$\nAt an optimum, the firm equates the marginal cost of a new hire to her marginal revenue product $(M R P_{j})$: \n$$\nw_{j}^{m}+c^{\\prime}\\left(\\frac{N_{j}}{I_{j}}\\right)=\\left(1-\\frac{1}{\\varepsilon}\\right)\\frac{P\\left(Q_{j}\\right)Q_{j}}{L_{j}}\\equiv M R P_{j}.\n$$\nFor incumbent wages, the first-order condition can be written: \n$$\n\\quad M R P_{j}=w_{j}^{I}+\\qquad\\underbrace{w_{j}^{I}-w_{j}^{m}}_{\\eta}\\qquad.\n$$\nSubtracting equations (1) and (2), we arrive at the following expression for the incumbent wage premium: \n$$\nw_{j}^{I}-w_{j}^{m}=\\frac{\\eta}{1+\\eta}c^{\\prime}\\left(\\frac{N_{j}}{I_{j}}\\right).\n$$\nPlugging equation (3) into equation (1) yields an expression for the incumbent wage that is useful for motivating our empirical rent-sharing specifications: \n$$\n\\begin{array}{c}{{w_{j}^{I}=\\displaystyle\\frac1{1+\\eta}w_{j}^{m}+\\displaystyle\\frac{\\eta}{1+\\eta}M R P_{j}}}\\\\ {{\\mathrm{~}=\\displaystyle(1-\\theta)w_{j}^{m}+\\theta M R P_{j},}}\\end{array}\n$$ \nwhere $\\begin{array}{r}{\\theta=\\frac{\\eta}{1+\\eta}}\\end{array}$.\n\nThis section presents a model of wage determination in innovative firms, focusing on how firm-specific productivity shocks propagate into wages. The model incorporates features like imperfect substitutability of workers, rent-sharing, and the role of patent decisions in identifying economic parameters."
  },
  {
    "qid": "econ-empirical-1004-0-0-0",
    "question": "1) Derive the implications of using polynomial trends versus stochastic trends in macroeconomic modeling, particularly in the context of productivity growth. Discuss the risks associated with each approach.",
    "gold_answer": "1. **Polynomial Trends**: \n   - Assume a deterministic form, e.g., \\( y_t = \\alpha + \beta t + \\gamma t^2 + \\epsilon_t \\). \n   - Risks: \n     - Misspecification if the true trend is stochastic. \n     - Poor out-of-sample prediction due to rigidity. \n     - Heavier penalization needed in model selection (Ploberger and Phillips, 1999). \n\n2. **Stochastic Trends**: \n   - Model as \\( y_t = y_{t-1} + \\epsilon_t \\) (unit root). \n   - Advantages: \n     - Captures persistent shocks (e.g., productivity innovations). \n     - More flexible in prediction. \n   - Limitations: \n     - Does not explain sources of change (Krugman's critique).",
    "question_context": "The study of trends brings together empirical-quantitative and theory-quantitative aspects of modeling and has, in turn, been empowered by that synergy.\nGreenspan (2000) recognises these processes, saying in the same Question and Answer session to Congress cited above that 'the fundamental root of this extraordinary successful economy that we have is, one, the synergies of a number of new technologies that have come together, coupled with a financial system and an economic system which has enabled those particular new technologies to be developed in a manner to very markedly enhance the standards of living of the American people.'\nAn alternate concept I favor (see Phillips, 1998a) is that trend formulations do no more than provide coordinate systems for capturing trend effects. These coordinate systems offer valid alternative ways of modeling trends, and each provides its own frame of reference for the trend behavior.\nPolynomial trends turn out to be riskier in prediction than stochastic trends and so they need to be more heavily penalized when we choose between models.\n\nThe text discusses the challenges in modeling trends in macroeconomic time series, particularly in the context of productivity growth and technological changes. It highlights the limitations of current trend mechanisms and suggests the need for new econometric formulations."
  },
  {
    "qid": "econ-empirical-565-4-0-0",
    "question": "1) Derive the welfare function $Z$ as given in the text, and explain the economic intuition behind its components: $CS$ (aggregate consumer surplus) and $\\pi$ (aggregate profit). How does the comparison between $Z^{*}$ (with NLP) and $Z^{\\prime}$ (without NLP) inform welfare analysis?",
    "gold_answer": "1. **Derivation of $Z$:** The welfare function is given by:\n   $$Z = \\frac{(n-1)CS + n\\pi}{n}$$\n   where:\n   - $CS$ is aggregate consumer surplus,\n   - $\\pi$ is aggregate profit ($\\pi = \\sum \\Pi$),\n   - $n$ is the number of firms.\n\n2. **Economic Intuition:**\n   - $CS$ captures the net benefit to consumers from purchasing the product at the prevailing prices.\n   - $\\pi$ represents the total profits earned by firms, reflecting producer surplus.\n   - The term $(n-1)CS$ suggests that welfare weighs consumer surplus more heavily as the number of firms increases, reflecting competitive pressures.\n\n3. **Welfare Comparison:**\n   - If $Z^{*} > Z^{\\prime}$, NLP leads to higher welfare.\n   - If $CS^{*} < CS^{\\prime}$ (consumer surplus is lower with NLP), then $\\pi^{*} > \\pi^{\\prime}$ (profits are higher with NLP) must hold for $Z^{*} > Z^{\\prime}$ to be true.\n   - This implies that NLP redistributes surplus from consumers to producers, but the net effect on welfare depends on the relative magnitudes.",
    "question_context": "Linear demands $(\\pmb{\\alpha}=\\textbf{I})$ constitute a reasonable ^central' case, and the zero costs assumption of Proposition 2 does not obviously lead to an expectation that insisting on I-unit packets would increase welfare. This is particularly so since the first-best outcome in the model with zero costs is for all types to consume two units of product.\nPart (ii) of Proposition 1 is an extension to a well-known and similar result for third-degree price discrimination between separate markets when demands are linear (see a whole range of contributions from Robinson (1933) to Shih et al. (1988)). Part (ii) is more remarkable and parallels that of Schmalensee (1981). For all permissible values of the parameters $b,c$ and $\\pmb{n}$ a welfare gain is made by prohibiting NLP.\nAlthough total units supplied are the same with or without NLP in the propositions, and thus the NLPCE could be made to mimic the UPCE by allowing resales, reselling is often limited in practice and has been assumed impossible in our model. Without reselling, some individuals consume products for which their reservation prices are less than the reservation prices of others who do not consume.\nIn Proposition 2 we see an extreme form of price discrimination where 1-unit packets are effectively removed from the market by being priced so that any case the equilibrium conditions (8) and (9) (NLP) and (13) (UP) are first order conditions from maximising $z$ freely (8) and (9)) and with the constraint of uniform pricing (13): $$Z=[\\left(n-\\mathrm{{I}}\\right)C S+n\\pi]/n$$ where $c s$ is aggregate consumer surplus and $\\pmb{\\pi}=\\pmb{\\Sigma}\\pmb{\\Pi}$ is aggregate profit. If $Z^{*}$ allows NLP and $\\mathbf{Z}^{\\prime}$ does not then $Z^{*}>Z^{\\prime}$ if the constraint is strictly binding. Now if $C S^{*}<C S^{\\prime}$ , we must have $\\pi^{*}>\\pi^{\\prime}$.\nSuppose the cost of a 2-unit packet is $_{2\\ell-5}$ where $\\mathfrak{s}$ is the cost saving or size economy from packaging and selling. Recalculation of $\\mathbf{c}_{1}^{*},\\bar{x_{2}^{*}},x_{1}^{\\prime},x_{2}^{\\prime},$ for the ${\\boldsymbol{\\alpha}}={\\boldsymbol{\\mathrm{~I~}}}}$ case, incorporating non-zero $s$ only affects $x_{2}^{\\ast}$ which decreases, implying that more 2-unit packets are sold in a NLPCE than before. Providing $s$ is not too big $(s/c$ $<\\mathsf{2}-\\mathsf{b})$ , part (i) of the Proposition 1 will still hold. Part (i) does not hold since output of units of product is now higher in a NLPCE than in a UPCE. The welfare calculation for part (i) trades off the unit-salesincreasing and cost-reducing effects of the size economy $s$ with the initial welfare loss from firms restricting I-unit sales to expand 2-unit sales. The value of $s$ becomes the crucial determining factor.\n\nThe numerical analysis and the two simple propositions that have been advanced in this paper show that an aggregate welfare improvement may be obtained by imposing linear pricing rules and if necessary ensuring the existence of small-size packets. These improvements are not Pareto improvements, since profits will decline. Although both propositions relate to a restricted model, the restrictions do not appear to prejudice the outcome unduly."
  },
  {
    "qid": "econ-empirical-1607-2-1-1",
    "question": "6) Explain why wage schedules are downward rigid ($w_{z,t+1}\\geq w_{z,t}$) in the absence of social preferences.",
    "gold_answer": "1. **Insurance**: Risk-averse workers prefer stable wages, leading to downward rigidity.\n2. **Competition**: When a worker's type is revealed as good ($z=g$), external firms bid up the wage, preventing decreases.\n3. **Equilibrium**: The initial wage $w_{z,1}$ is below expected productivity, and wages only increase (or stay constant) upon type revelation.",
    "question_context": "Proposition 2 (no social preferences). Assume that there are no social concerns $\\mathbf{\\partial}A\\equiv0,$ ) hen:\n\n(i) when $k=0$ , the firm–worker assignment is indeterminate;\n(ii) when $k>0$ , no worker ever leaves his initial employer, that is, the optimal retention policy is $\\rho_{t}=1$ for all $t$ ;\n(iii) for all $k\\geq0$ , the wage schedule is downward rigid, that is, $w_{z,t+1}\\geq w_{z,t}$ for all t and $z\\in\\{g,u\\}$ ; (iv) wages are stationary for a given type, that is, $w_{z,t+1}=w_{z,t}$ for all $t$ , and $z\\in\\{g,u\\}$ .\n\nThis section analyzes the model's predictions when social concerns are absent ($A\\equiv0$), extending Harris and Hölmström (1982) with mobility costs $k\\geq0$."
  },
  {
    "qid": "econ-empirical-1412-0-0-3",
    "question": "4) Discuss the robustness of NAM in the idiosyncratic risk model when riskier background risk leads to more risk-tolerant behavior. Provide a mathematical explanation for why NAM may fail in this case.",
    "gold_answer": "1. **Risk-Tolerant Behavior**: If riskier background risk leads to more risk-tolerant behavior (e.g., due to large risks relative to risk-free income), the DARA condition may not hold.\n2. **PAM Possibility**: In this case, high-risk agents may prefer to match with other high-risk agents, leading to positive assortative matching (PAM).\n3. **Mathematical Justification**: The failure of NAM occurs because the marginal utility of risk-sharing decreases, making PAM more efficient for large risks.",
    "question_context": "When preference belongs to the class of harmonic absolute risk aversion (HARA), the risk premium is perfectly transferable within each partnership; thus a stable match minimizes the social cost of risk.\nIn the systematic risk model, where agents are ranked by their holdings of a common risky asset, the convexity of the joint risk premium in joint risk size leads to negative assortative matching (NAM).\nIn the idiosyncratic risk model, where agents are ranked by their independent riskiness in the sense of second-order stochastic dominance (SSD), NAM arises when preference exhibits decreasing absolute risk aversion (DARA) in the sense of Ross and riskier background risk leads to more risk-averse behavior.\n\nThis section explores the sorting patterns in a two-sided matching market where agents facing different risks match to share them. When preferences belong to the class of harmonic absolute risk aversion (HARA), the risk premium is perfectly transferable within each partnership, leading to a stable match that minimizes the social cost of risk."
  },
  {
    "qid": "econ-empirical-992-0-0-1",
    "question": "2) What are the key challenges in modeling decision-making in a mixed economy, as discussed in Part II of the book? Provide a theoretical framework for analyzing fiscal decentralization.",
    "gold_answer": "Key challenges include:\n1. **Interdependence**: Public and private sectors interact, requiring joint optimization.\n2. **Incentives**: Aligning agents' objectives with social welfare.\n3. **Information**: Asymmetric information complicates mechanism design.\n\n**Framework for fiscal decentralization**:\n1. Let \\( U_i(\\mathbf{x}) \\) be utility of region \\( i \\) with allocation \\( \\mathbf{x} \\).\n2. Central planner maximizes \\( \\sum_i \\alpha_i U_i(\\mathbf{x}) \\) subject to resource constraint \\( \\sum_i \\mathbf{x}_i \\leq \\mathbf{X} \\).\n3. Shadow prices \\( \\lambda \\) emerge from Lagrangian, guiding local decisions.",
    "question_context": "The presentation is largely mathematical, making extensive use of vector calculus for constrained optimisation.\nPart II addresses decision making in a necessarily mixed, public and private sector, economy.\nPart III of the book is concerned with first-order welfare measures, appropriate where nonconvexities are absent.\nPart IV of the book is concerned with the evaluation of 'large' projects with associated indivisibilities and/or increasing returns to scale, so that marginal analysis is inappropriate.\n\nThe text discusses David A. Starrett's book 'Foundations of Public Economics', which synthesizes recent developments in normative welfare economics with applications in public finance. The book is mathematical, using vector calculus for constrained optimization, and is structured into four parts covering social objectives, decision-making in mixed economies, project appraisal, and large projects with indivisibilities."
  },
  {
    "qid": "econ-empirical-823-1-0-3",
    "question": "4) Derive the elasticity of substitution \\(\\sigma\\) for a homogeneous production function of degree one, and show how it relates to the change in the capital-output ratio \\(v\\) with respect to the saving propensity \\(s_c\\) in the S-M model.",
    "gold_answer": "1. The elasticity of substitution is given by: $$\\sigma = \\frac{f^{\\prime}(k f^{\\prime} - f)}{k f f^{\\prime\\prime}}.$$\n2. In the S-M model, the equilibrium capital-output ratio \\(v\\) depends on \\(s_c\\): $$v = k^* / f(k^*),$$ where \\(k^*\\) solves \\(f^{\\prime}(k^*) = n / s_c\\).\n3. Differentiate \\(v\\) with respect to \\(s_c\\): $$\\frac{d v}{d s_c} = \\frac{d v}{d k^*} \\frac{d k^*}{d s_c}.$$\n4. From \\(f^{\\prime}(k^*) = n / s_c\\), differentiate implicitly: $$f^{\\prime\\prime}(k^*) \\frac{d k^*}{d s_c} = -n / s_c^2.$$\n5. Solve for \\(d k^* / d s_c\\): $$\\frac{d k^*}{d s_c} = -n / [s_c^2 f^{\\prime\\prime}(k^*)].$$\n6. Substitute into \\(d v / d s_c\\): $$\\frac{d v}{d s_c} = \\frac{f(k^*) - k^* f^{\\prime}(k^*)}{f(k^*)^2} \\cdot \\frac{-n}{s_c^2 f^{\\prime\\prime}(k^*)}.$$\n7. Simplify using the definition of \\(\\sigma\\): $$\\frac{d v}{d s_c} = \\frac{\\sigma v}{s_c} > 0.$$\n\nInterpretation: The elasticity of substitution \\(\\sigma\\) measures how responsive the capital-output ratio is to changes in the saving propensity \\(s_c\\). A higher \\(\\sigma\\) implies a more significant adjustment in \\(v\\) for a given change in \\(s_c\\), reflecting greater flexibility in the production process.",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nThe equilibrium profit rate is uniform across capitalists and workers, and the growth rate of capital is uniform at the natural rate, given by: $$\\begin{array}{l}{{r\\equiv P/K=P_{i}/K_{i}}}\\\\ {{n=I/K=S_{i}/K_{i}}}\\end{array}\\quad i=c,w,$$\nCombining (3) and (4) yields a proportionality result between saving and profits for the two classes: $$S_{i}=\\left(n/r\\right)P_{i},\\:\\:\\:i=c,w.$$\nA necessary condition for a Pasinetti equilibrium is that the investment-output ratio be contained in the open interval defined by the difference between the saving propensities. If this condition holds, we may use (5) and the saving function for capitalists to obtain: $$s_{c}P_{c}=(n/r)P_{c}.$$\nEquation (6) may be solved for the equilibrium profit rate, yielding the Cambridge equation: $$P/Y=r K/Y=n v/s_{c},$$ $$W/Y=\\texttt{I}-(r K/Y)=(s_{c}-n v)/s_{c}.$$\nIn equilibrium, investment equals total saving: $$I=s_{w}(Y-P_{c})+s_{c}P_{c}.$$\nEquation (9) may be solved for the shares of capitalists' and workers' profits in total income: $$P_{c}/Y=(n v-s_{w})/(s_{c}-s_{w}),$$ $$\\begin{array}{r}{P_{w}/Y=s_{w}(s_{c}-n v)/s_{c}(s_{c}-s_{w}).}\\end{array}$$\nThe Pasinetti equilibrium value of the ratio of workers' income to capitalists' income is derived as: $$z\\equiv Y_{w}/Y_{c}=(Y-P_{c})/P_{c}=(s_{c}-n v)/(n v-s_{w}).$$\nThe essential difference between the Pasinetti and S-M systems is that in the former, the capital-output ratio \\(v\\) is constant, while in the latter, it is variable. The equilibrium value of \\(v\\) in the S-M system is derived by: $$r=f^{\\prime}(k)\\:=\\:n/s_{c}.$$\nThe change in the capital-output ratio with respect to the saving propensity is given by: $$d v/d s_{c}\\equiv v^{\\prime}(s_{c})=\\frac{d v}{d k}\\frac{d k}{d s_{c}}=\\frac{n(k^{*}f^{\\prime}-f)}{s_{c}^{2}f^{\\prime\\prime}f^{2}}>0.$$\nThe elasticity of substitution is given by: $$\\sigma=\\frac{f^{\\prime}(k f^{\\prime}-f)}{k f f^{\\prime\\prime}}.$$\nIn a Pasinetti equilibrium, the change in the capital-output ratio with respect to the saving propensity is: $$v^{\\prime}\\left(s_{c}\\right)=\\sigma v/s_{c}>0.$$\n\nThe Pasinetti and S-M models share several assumptions, including two steady-state conditions: uniform profit rates across capitalists and workers, and uniform capital growth rates at the natural rate. The Cambridge equation is derived under these assumptions, but its generality is debated due to the strength of the underlying assumptions."
  },
  {
    "qid": "econ-empirical-713-1-1-1",
    "question": "4) Show that the aggregate consumption change $\\Delta c_t$ in (19) follows an MA(1) process and derive the regression coefficient $\\hat{\\beta}$ in (20).",
    "gold_answer": "1. Aggregate consumption is $\\Delta c_t = \\frac{\\omega + r}{1 + r} \\varepsilon_t + (1 - \\omega) \\varepsilon_{t-1}$, which is MA(1).\n2. The regression coefficient is $\\hat{\\beta} = \\frac{\\text{cov}(\\Delta c_t, \\Delta y_{t-1})}{\\text{var}(\\Delta y_{t-1})} = 1 - \\omega$.\n3. This arises because $\\text{cov}(\\Delta c_t, \\Delta y_{t-1}) = (1 - \\omega) \\sigma_\\varepsilon^2$ and $\\text{var}(\\Delta y_{t-1}) = \\sigma_\\varepsilon^2$.",
    "question_context": "The optimal consumption response will have two parts: a response to the new innovation and a term that corrects for the error made in the previous period.\nThe change in aggregate consumption follows an MA(1) process: $\\Delta c_t = \\frac{\\omega + r}{1 + r} \\varepsilon_t + (1 - \\omega) \\varepsilon_{t-1}$.\n\nThe text extends the model to include lagged information about aggregate shocks, where individuals observe $y_{i t}$ and $\\varepsilon_{t-1}$, allowing them to infer $u_{i t-1}$."
  },
  {
    "qid": "econ-empirical-631-0-0-3",
    "question": "4) Discuss the theoretical implications of the authors' finding that a likelihood ratio test can be nearly efficient for the unit root hypothesis.",
    "gold_answer": "1) **Theoretical Implications**: The finding challenges the notion that likelihood ratio tests perform poorly for certain testing problems, as suggested by Lehmann (2006). \\n2) **Significance**: It demonstrates that likelihood ratio tests can achieve nearly efficient power for the unit root hypothesis, expanding the class of problems for which such tests are viable. \\n3) **Practical Impact**: This result provides a new tool for econometricians, offering a test that combines the interpretability of likelihood ratio tests with the power properties of nearly efficient tests.",
    "question_context": "Seemingly absent from the arsenal of currently available 'nearly efficient' testing procedures for the unit root hypothesis, that is, tests whose asymptotic local power functions are virtually indistinguishable from the Gaussian power envelope, is a test admitting a (quasi-)likelihood ratio interpretation.\nIn models with an unknown mean and/or a linear trend, the class of nearly efficient unit root tests does not contain the Dickey and Fuller (1979, 1981; henceforth DF) tests.\nThe purpose of this note is to propose and analyze such a test.\n\nThe paper discusses the development of a quasi-likelihood ratio unit root test that is nearly efficient, filling a gap in the existing arsenal of unit root tests. The authors highlight the significance of the unit root testing problem in time series econometrics and the absence of a likelihood ratio-based test in the class of nearly efficient tests."
  },
  {
    "qid": "econ-empirical-646-2-0-2",
    "question": "3) In the multinomial potential outcome model, prove that the mapping \\( \\nu \\mapsto (F_{g|u}(j; \\nu): j \\leq J) \\) is invertible under the condition that \\( \\varepsilon \\) has a non-negative density everywhere.",
    "gold_answer": "1. The mapping is defined by \\( F_{g|u}(j; \\nu) = \\Pr[\\nu_j + \\varepsilon_j \\geq \\nu_{j'} + \\varepsilon_{j'} \\forall j' \\leq J | U = u] \\).\n2. By Ruud (2000) and Ahn et al. (2017), this is a smooth function of \\( \\nu \\) if \\( \\varepsilon \\) has a non-negative density.\n3. The non-negative density ensures that small changes in \\( \\nu \\) lead to changes in \\( F_{g|u}(j; \\nu) \\), making the mapping locally invertible.\n4. Global invertibility follows from the strict monotonicity of \\( F_{g|u}(j; \\nu) \\) in \\( \\nu_j \\) under the full support condition.",
    "question_context": "Y=g(ν(X,D),ε)=ν1(X,D)+ν2(X,D)εforD∈{0,1}.\nFg|u(y;ν(x,d))=Pr[ν1(x,d)+ν2(x,d)ε≤y|U=u]=Fε|u(y−ν1(x,d)ν2(x,d))ford=0,1.\nY=g(ν(X,D),ε)=max{ν1(X,D)+ν2(X,D)ε,0}forD∈{0,1}.\nY=g(ν(X,D),ε)=argmaxj=0,1,…,Jyj,D∗, where yj,D∗=νj(X,D)+εjforj=1,2,...,J;y0,D∗=0.\nYd=maxj∈{a,b}yj,d∗andWd=argmaxj∈{a,b}yj,d∗ where yj,d∗=νj(X,d)+εjforj∈{a,b}.\n\nThis section presents examples where potential outcomes depend on multi-dimensional indices with endogenous treatment, illustrating the applicability of the general approach through transparent primitive conditions that imply Assumption A-5."
  },
  {
    "qid": "econ-empirical-546-0-0-2",
    "question": "3) Analyze the two countervailing effects on beliefs about a neighbor's type in a shrinking network and show why they balance each other.",
    "gold_answer": "The two effects are:\n\n1. **Classic Belief Updating**: As time passes and a neighbor does not stop, the player becomes more confident the neighbor is of type 2 (since type 1 players are more likely to stop).\n2. **Network Evolution Effect**: Even if the neighbor started as type 2, her other neighbor might have stopped, turning her into type 1.\n\nIn the line network, these effects balance because:\n- The probability that a type 2 neighbor becomes type 1 due to the other neighbor stopping exactly offsets the increased belief that the neighbor is type 2 due to not stopping.",
    "question_context": "Players are organized on line segments of random length and play an infinite horizon timing game. Each player has to decide when to take an action, we call 'stop.' The benefit of the action for an individual at date $t$ depends on the neighbors' past actions. Whenever a player stops, she increases the payoff of stopping of all her neighbors.\nEach link between two consecutive players is i.i.d drawn at date 0. The probability distribution of the network structure is common knowledge, but players do not observe the realization of the network structure but only their direct neighbors.\nThe benefit for an individual stopping at date $t$ depends on how many neighbors she has at that date. If a player stops at time $t$, and has $k$ neighbors at that date, her realized payoff is $\\Pi(k,t)=e^{-r t}B_{k}$, where $r$ is the rate of discounting and $B_{k}$ is the time invariant benefit of stopping for a player with $k$ neighbors.\n\nThe paper introduces a neighborhood structure in a waiting game, where the payoff of stopping increases when neighbors stop. This structure leads to dynamic evolution of the network, either as a shrinking or fragmenting network, and introduces an additional inefficiency linked to the order of stopping."
  },
  {
    "qid": "econ-empirical-634-0-2-0",
    "question": "1) Derive the Nash bargaining solution and discuss its axiomatic foundations as presented by Moulin.",
    "gold_answer": "The derivation involves:\n1. Define the bargaining problem and feasible set.\n2. State Nash's axioms (e.g., invariance, symmetry, independence of irrelevant alternatives).\n3. Solve for the solution that satisfies all axioms.\n4. Discuss Moulin's interpretation and extensions.",
    "question_context": "Moulin's book provides an analysis of a number of diverse problems in the theory of cooperative decision making. Part I of the book explores egalitarianism, classical utilitarianism, and the axiomatic bargaining theory. Part II is devoted to a discussion of some basic elements of the theory of cooperative games.\n\nThis section explores game theory applications and cooperative decision-making axioms, as discussed in the works of Aumann, Moulin, and others."
  },
  {
    "qid": "econ-empirical-674-2-1-3",
    "question": "4) How does the sampling of 1-min returns address asynchronicity, and what are the trade-offs?",
    "gold_answer": "Sampling 1-min returns:\n1. **Reduces Asynchronicity**: Aligns observations across assets at regular intervals.\n2. **Mitigates Microstructure Noise**: Avoids ultra-high-frequency noise while preserving jump detection.\n\n**Trade-offs**:\n1. May miss very short-lived jumps.\n2. Requires robustness checks for noise, as 1-min data still contains some microstructure effects.",
    "question_context": "High-frequency front-month futures contract prices are obtained from Tick Data Inc., including futures on the eMini-S&P 500 (ES), Crude Oil (CL), Euro/USD FX (EC), and 10-Year Treasury Notes (TY), from January 1 2007 to July 31 2012.\nTo mitigate the effect of asynchronicity, we sample each day 1-min returns from $6{:}00\\mathrm{p.m}$ . EST the previous day until $5{:}00~\\mathrm{p.m}$ . the next.\nWe proxy for news shocks or surprises by computing the scaled difference between the actual release and the survey expectations: News Shock Announced Quantityt − Median of Expectations Maximum of Expectations − Minimum of Expectations.\nFor Federal Open Market Committee (FOMC) and European Central Bank (ECB) meetings, we construct dummy variables using the pre-announced schedule of meetings.\n\nThe study uses high-frequency futures contract prices and macroeconomic news data to analyze the impact of jumps and news shocks on asset returns."
  },
  {
    "qid": "econ-empirical-25-3-1-0",
    "question": "5) Compute the marginal effect of financial development on entrepreneurship from the probit estimates in Table V, given a coefficient of 0.1072 (Column 1) and a sample mean probability of 14%. Interpret the economic magnitude.",
    "gold_answer": "1. **Marginal Effect**: For a probit model, the marginal effect is $\\phi(\\beta x) \\cdot \\beta$, where $\\phi$ is the standard normal PDF. At the mean, $\\phi(\\beta x) \\approx 0.39$ (assuming symmetry). Thus, the effect is $0.39 \\times 0.1072 \\approx 0.042$ or 4.2%.  \n2. **Interpretation**: A one-unit increase in financial development raises the probability of entrepreneurship by 4.2 percentage points, aligning with the Calabria-Marche comparison (5.6% difference).",
    "question_context": "In more financially developed regions the probability a person becomes self-employed is indeed higher, and this effect is statistically different from zero at the 1 percent level.\nMoving from Calabria (the most financially underdeveloped region) to Marche (the most financially developed) increases a person’s probability of starting his own business by 5.6 percentage points.\nThe IV coefficient is almost identical to the OLS counterpart and remains statistically different from zero.\n\nThe text examines how regional financial development influences entrepreneurship, using micro-level data and instrumental variables to address endogeneity."
  },
  {
    "qid": "econ-empirical-1555-0-3-1",
    "question": "8) How does the model reconcile the finding that MAJ systems can sometimes reduce inequality or increase public good provision relative to PR systems?",
    "gold_answer": "The model reconciles this by introducing the relative electoral sensitivity effect, which: \n1) Penalizes high-sensitivity localities in high-sensitivity districts under MAJ. \n2) Encourages more uniform allocations when districts are homogeneous. \n3) Reverses the standard result when sensitivity heterogeneity is large relative to contestability heterogeneity.",
    "question_context": "We explore how the electoral system modifies the incentive to continue targeting specific localities or groups, or to instead focus on public goods.\nPROPOSITION 6: If targetability is at the district level and districts are well-apportioned, then $G^{M A J}\\gtrless G^{P R}$ depends on the comparison of heterogeneity in sensitivities and contestabilities.\n\nThe paper extends the model to group-targeted interventions and public goods, showing how the relative sensitivity effect modifies classic findings in the literature."
  },
  {
    "qid": "econ-empirical-211-2-1-1",
    "question": "2) In Case 2 (\\(\\mu_{1}=0.85\\), \\(\\mu_{2}=0.75\\), \\(\\sigma_{1}=\\sigma_{2}=0.6\\)), why should we expect to reject \\(H_{0}^{2}\\) but not \\(H_{0}^{1}\\)? Provide a theoretical justification.",
    "gold_answer": "1. Lower \\(\\mu_{2}\\) shifts \\(X^{2}\\) leftward, increasing poverty gaps for \\(X^{2}\\) relative to \\(X^{1}\\).  \n2. Thus, \\(P_{2}(p;z_{2}) > P_{1}(p;z_{1})\\) for some \\(p\\), violating \\(H_{0}^{2}\\) but not \\(H_{0}^{1}\\).  \n3. Theorem 4.6 implies \\(H_{0}^{1}\\) is rejected less often than nominal size when \\(P_{2}\\) is below \\(P_{1}\\).  \n4. The test should reject \\(H_{0}^{2}\\) with probability exceeding nominal size as sample size grows.",
    "question_context": "We generate two sets of samples from two (possibly different) distributions: \\(X_{i}^{1}=\\exp(\\sigma_{1}Y_{1i}+\\mu_{1})\\) and \\(X_{j}^{2}=\\exp(\\sigma_{2}Y_{2j}+\\mu_{2})\\), where \\(Y_{1i}\\) and \\(Y_{2j}\\) are independent \\(N(0,1)\\).\nCase 1: \\(\\mu_{1}=\\mu_{2}=0.85\\) and \\(\\sigma_{1}=\\sigma_{2}=0.6\\). Case 2: \\(\\mu_{1}=0.85\\), \\(\\sigma_{1}=0.6\\), \\(\\mu_{2}=0.75\\), \\(\\sigma_{2}=0.6\\). Case 3: \\(\\mu_{1}=0.85\\), \\(\\sigma_{1}=0.6\\), \\(\\mu_{2}=0.85\\), \\(\\sigma_{2}=0.62\\). Case 4: \\(\\mu_{1}=0.85\\), \\(\\sigma_{1}=0.6\\), \\(\\mu_{2}=0.85\\), \\(\\sigma_{2}=0.7\\).\nThe poverty line is estimated using half the sample median: \\(\\psi_{z_{j}}(X_{j i};\\hat{z}_{j})=-0.5\\hat{f}_{j}(2\\hat{z}_{j})^{-1}\\big[1\\big(X_{j i}-2\\hat{z}_{j}\\big)-1/2\\big]\\), where \\(\\hat{f}_{j}(2\\hat{z}_{j})\\) is a nonparametric estimator.\n\nThis section presents Monte Carlo experiments to evaluate the small-sample performance of the PGP dominance tests, including size and power properties under various scenarios."
  },
  {
    "qid": "econ-empirical-1737-5-0-0",
    "question": "1) Derive the expression for roughness $R(t)$ at date $t \\geqslant z$ given traffic growth rate $g$ and initial traffic $N_{0}$. Show the steps leading to the equation $$R(t) = e^{m t}\\bigg\\{R_{0} + k\\bigg(X + N_{0}E\\int_{z}^{t}e^{g(u-z)}du\\bigg)\\bigg\\}.$$",
    "gold_answer": "1. Start with the definition of roughness as a function of traffic and time. \\n2. Assume traffic grows exponentially at rate $g$: $N(u) = N_{0}e^{g(u-z)}$. \\n3. The cumulative traffic up to time $t$ is $X + N_{0}E\\int_{z}^{t}e^{g(u-z)}du$. \\n4. Roughness increases with cumulative traffic and time, leading to the given equation.",
    "question_context": "The general case has traffic growing at steady rate $g$, and the case of zero traffic growth can be derived by setting $g=0$. Current traffic is $N_{0}$ at date $z$, and the equation for roughness at date $t\\geqslant z$ is $$R\\left(t\\right)=e^{m t}\\bigg\\{R_{0}+k\\bigg(X+N_{0}E\\int_{z}^{t}e^{g(u-z)}d u\\bigg)\\bigg\\},$$ where $X$ is cumulative traffic up to the current date.\nAt $t=M$, roughness reaches its critical level $\\overline{R}$: $$\\overline{R}e^{-m M}=R_{0}+k\\bigg\\{X+N_{0}E\\int_{z}^{M}e^{g(t-z)}d t\\bigg\\}.$$\nDifferentiate (A2) totally with respect to $X$ and evaluate at $M=T$: $$-m\\overline{R}e^{-m T}\\partial M/\\partial X=k+k N_{0}E e^{g(T-z)}\\partial M/\\partial X$$ which can be solved for $\\partial M/\\partial X$ in terms of $k$.\nThe cost of overlays depends on the required road strength after overlay, which can be found as follows. From (14) $k\\check{Y}(T)\\overset{!}{=}\\overline{R}e^{-m T}-R_{0}$, so that if $R_{0},\\overline{R}$, and $T$ are to remain constant, $1/k$ must be proportional to $Y(T)$.\nThe cost of an overlay of corrected structure number SNC, strength $s$ is given by Rolt (1981, p. 7) as $$C_{L}=C_{0}+\\beta S N C=C_{0}-\\beta+\\beta S.$$\nThe time average of equation (24) is thus $$\\frac{\\partial\\overline{F}}{\\partial X}=\\frac{1}{T}\\int_{0}^{T}\\frac{\\partial F}{\\partial X}d z=\\frac{\\mu}{N_{0}E T}\\bigg\\{\\int_{0}^{T}r e^{-(g+r)(T-z)}C(z)+e^{-r(T-z)}N_{0}(v^{*}-\\bar{v})\\bigg\\} + \\frac{b(\\overline{R}e^{-m T}-R_{0})}{T E\\phi(g)}e^{-g t}\\int_{0}^{T}\\big\\{e^{(g+m)z}-e^{-(r-g-m)T}e^{r z}\\big\\}d z$$ where $\\mu$ is given by (A4) and $v^{*}$ is given in (25).\n\nThe text presents a detailed mathematical model for road damage cost, incorporating traffic growth, roughness, and overlay costs. It includes differential equations and integrals to model the relationship between traffic, road strength, and maintenance costs."
  },
  {
    "qid": "econ-empirical-571-2-0-3",
    "question": "4) Derive the asymptotic covariance matrix estimator for the SML estimator, and explain why the sandwich estimator is preferred over the Hessian-based estimator.",
    "gold_answer": "1. The sandwich estimator is:\n   $$\\widehat{\\mathrm{Var}}(\\omega) = \\left[-\\frac{\\partial^2 \\log \\tilde{\\ell}}{\\partial \\omega \\partial \\omega^{\\prime}}\\right]^{-1} \\left[\\sum_{i=1}^{N} \\left(\\frac{\\partial \\log \\tilde{\\ell}_{i}}{\\partial \\omega}\\right) \\left(\\frac{\\partial \\log \\tilde{\\ell}_{i}}{\\partial \\omega}\\right)^{\\prime}\\right] \\left[-\\frac{\\partial^2 \\log \\tilde{\\ell}}{\\partial \\omega \\partial \\omega^{\\prime}}\\right]^{-1}.$$\n2. The sandwich estimator accounts for finite simulation noise ($K < \\infty$) and misspecification, whereas the Hessian-based estimator assumes exact likelihood evaluation and model correctness, leading to underestimation of variance (Newey and McFadden, 1994).",
    "question_context": "The likelihood function of the model equals $$\\ell=\\prod_{i=1}^{N}\\int_{\\eta_{i}}\\ell_{i}(\\eta_{i})\\phi(\\eta_{i};0,\\Sigma_{\\eta})\\mathrm{d}\\eta_{i},$$ with $$\\ell_{i}(\\eta_{i})=(2\\pi)^{-T_{i}/2}\\sigma_{i}^{-T_{i}}\\exp{\\left(-\\frac{1}{2}\\sum_{t=1}^{T_{i}}\\left(\\frac{e_{i,t}(\\eta_{i})}{X_{i,t-1}\\sigma_{i}}\\right)^{2}\\right)},$$ where $\\phi(\\eta_{i};0,\\Sigma_{\\eta})$ denotes the density function of a 5-variate normal distribution with mean 0 and covariance matrix $\\Sigma_{\\eta}$ evaluated at $\\eta_{i}$, $\\ell_{i}(\\boldsymbol{\\eta}_{i})$ is the likelihood contribution of article $i$ conditional on $\\eta_{i}$, and $e_{i,t}(\\eta_{i})$ is the (unstandardized) residual of (11) given $\\eta_{i}$. Note that $\\sigma_{i}^{2}$ also depends on $\\eta_{i}$, as from (12) it follows that $\\sigma_{i}^{2}=\\exp(Z_{i}^{\\prime}\\theta_{5}+\\eta_{5,i})$.\nThe integral in (13) cannot be solved analytically. To obtain parameter estimates we opt for simulated maximum likelihood, see for example Gourieroux and Montfort (1996). To reduce the variance of the likelihood simulator we use importance sampling, see Kloek and van Dijk (1978) and Geweke (1989). To this end, we rewrite the likelihood function as $$\\ell=\\prod_{i=1}^{N}\\int_{\\tilde{\\eta}_{i}}\\frac{\\ell_{i}(\\boldsymbol{\\Sigma}_{\\eta}^{1/2}\\tilde{\\eta}_{i})\\phi(\\tilde{\\eta}_{i};0,\\mathbf{I})}{g(\\tilde{\\eta}_{i};m_{i},S_{i})}g(\\tilde{\\eta}_{i};m_{i},S_{i})\\mathrm{d}\\tilde{\\eta}_{i},$$ where $\\Sigma_{\\eta}^{1/2}$ is the Choleski decomposition of $\\Sigma_{\\eta}$ and where $g(\\tilde{\\eta}_{i};m_{i},S_{i})$ denotes the importance function which is set to the normal density with mean $m_{i}$ and variance $S_{i}$.\nTo approximate the likelihood we use $$\\tilde{\\ell}=\\prod_{i=1}^{N}\\frac{1}{K}\\sum_{k=1}^{K}\\frac{\\ell_{i}(\\Sigma_{\\eta}^{1/2}\\tilde{\\eta}_{i}^{(k)})\\phi(\\tilde{\\eta}_{i}^{(k)};0,{\\bf I})}{g(\\tilde{\\eta}_{i}^{(k)};m_{i},S_{i})},$$ where $\\tilde{\\eta}_{i}^{(k)}$ is a draw from $g(\\tilde{\\eta}_{i};m_{i},S_{i})$.\nUnder the usual regularity conditions, the SML estimator is consistent for $N\\to\\infty$ and $K\\rightarrow\\infty$. Furthermore, the estimator is asymptotically normal distributed. The standard errors can be computed using the so-called sandwich or robust asymptotic covariance matrix estimator recommended by McFadden and Train (2000), see Newey and McFadden (1994) for a general discussion.\n\nThe parameters in the multi-level model are estimated using maximum likelihood estimation, with the likelihood function involving integrals that are solved via simulated maximum likelihood and importance sampling."
  },
  {
    "qid": "econ-empirical-1788-4-2-0",
    "question": "5) Prove Claim A.1: If \\( F_i \\succ_{SOSD} F_j \\), then \\( \\widetilde{F}_i^q \\succ_{SOSD} \\widetilde{F}_j^q \\) for all \\( q \\in (0,1) \\).",
    "gold_answer": "1. For \\( t \\leq F^{-1}(q) \\), \\( \\widetilde{F}_i^q(t) = \\frac{F_i(t)}{q} \\). \\n2. By SOSD, \\( \\int_0^t F_j(x) dx > \\int_0^t F_i(x) dx \\). \\n3. Thus, \\( \\int_0^t \\widetilde{F}_j^q(x) dx > \\int_0^t \\widetilde{F}_i^q(x) dx \\), proving \\( \\widetilde{F}_i^q \\succ_{SOSD} \\widetilde{F}_j^q \\).",
    "question_context": "If \\( F_i \\succ_{SOSD} F_j \\), then \\( \\widetilde{F}_i^q \\succ_{SOSD} \\widetilde{F}_j^q \\) for all \\( q \\in (0,1) \\).\nSocial welfare under \\( F_j \\) is: $$ SW(F_j) = \\int_0^{\\hat{p}_j} v_b(x) f_j(x) dx + \\int_{\\hat{p}_j}^{\\infty} x f_j(x) dx. $$\n\nThe lemma establishes dominance relations between distributions \\( F_i \\) and \\( F_j \\) under strict second-order stochastic dominance (SOSD) and their implications for social welfare."
  },
  {
    "qid": "econ-empirical-959-0-0-3",
    "question": "4) How does the CiC approach address the limitations of the principal stratification framework?",
    "gold_answer": "1. **Principal Stratification**: Focuses on subpopulations where the mediator is constant (not-affected at 0 or 1), often criticized for ignoring effects on those whose mediator is affected. \\n2. **CiC Approach**: Extends identification to subpopulations where the mediator is affected by the treatment (affected positively), providing a more comprehensive decomposition of total effects into direct and indirect components. \\n3. **Flexibility**: CiC allows for differential time trends across subpopulations, unlike strict common trend assumptions in some principal stratification methods.",
    "question_context": "We propose a novel approach for causal mediation analysis based on changes-in-changes assumptions restricting unobserved heterogeneity over time. This allows disentangling the causal effect of a binary treatment on a continuous outcome into an indirect effect operating through a binary intermediate variable (called mediator) and a direct effect running via other causal mechanisms.\nIdentification in the earlier mediation literature typically relied on linear models for the mediator and outcome equations and often neglected endogeneity issues. More recent contributions use more general identification approaches based on the potential outcome framework and take endogeneity issues explicitly into consideration.\n\nCausal mediation analysis aims to disentangle the total treatment effect into direct and indirect effects, where the indirect effect operates through a mediator. The article proposes a novel identification strategy based on changes-in-changes (CiC) assumptions, extending the approach by Athey and Imbens (2006) to mediation analysis."
  },
  {
    "qid": "econ-empirical-1414-0-0-2",
    "question": "3) Using the equation $\\hat{c}(w/r)/\\hat{c}p_{B}=1/r^{2}D$, analyze how the factor price differential between the North and South narrows.",
    "gold_answer": "To analyze the narrowing of the factor price differential:\n1. The equation $\\hat{c}(w/r)/\\hat{c}p_{B}=1/r^{2}D$ shows the sensitivity of the wage-rental ratio to changes in $p_{B}$.\n2. In the South, $p_{\\beta}$ rises, increasing $w/r$.\n3. In the North, $p_{a}$ falls (since $p_{a} = P_{s}$), decreasing $w/r$.\n4. The differential narrows because the wage-rental ratio increases in the South and decreases in the North.\n5. The condition $\\varepsilon > 1$ is not necessary for this result, as the differential narrows regardless of the elasticity condition.",
    "question_context": "Using a general equilibrium model of North-South trade Graciela Chichilnisky has concluded that if dualism and labour abundance in the South are sufficiently pronounced the export supply curve of that region will be negatively sloped and, as a result, the familiar neoclassical results of an increase in the North's demand for these exports and of an export policy in the South are reversed.\nWe show that the neoclassical results are valid whether or not the supply curve is downward sloping.\nReal wages in the South rise if $p_{B}=(1+s)\\bar{p}_{B}$ rises, i.e., if the deterioration of the terms of trade (lower $p_{a}$) is more than offset by the direct effect of the increase in $s$. This is the case if $\\varepsilon>1$ and this condition is satisfied.\nIt may be shown that $\\hat{c}(w/r)/\\hat{c}p_{B}=1/r^{2}D$. Since $p_{\\beta}$ rises in the South and falls in the North (where $p_{a}$ remains equal to $P_{s}$) the differential narrows.\n\nThe text discusses a general equilibrium model of North-South trade, focusing on the implications of a negatively sloped export supply curve in the South due to dualism and labor abundance. It contrasts the findings of Graciela Chichilnisky with the author's conclusions that neoclassical results hold regardless of the slope of the supply curve."
  },
  {
    "qid": "econ-empirical-1385-0-0-1",
    "question": "2) Derive the conditions under which procyclical fiscal policy could exacerbate business cycles, using a simple macroeconomic model.",
    "gold_answer": "Consider a simple Keynesian model: \\( Y = C + I + G \\). Procyclical fiscal policy implies \\( G \\) increases during expansions (raising \\( Y \\) further) and decreases during recessions (lowering \\( Y \\) more). This amplifies fluctuations in \\( Y \\).",
    "question_context": "In the past, while industrial countries have tended to pursue fiscal policy that is countercyclical or at worst acyclical, developing countries have tended to follow procyclical fiscal policy: they have increased spending (or cut taxes) during periods of expansion and cut spending (or raised taxes) during periods of recession.\nMany authors have documented that fiscal policy has tended to be more procyclical in developing countries than industrialized countries. Most studies look at the procyclicality of government spending because tax receipts are endogenous with respect to the business cycle.\n\nThe text discusses the cyclical behavior of fiscal policy across different income groups, highlighting the contrast between industrial and developing countries. Industrial countries tend to pursue countercyclical or acyclical fiscal policies, while developing countries often follow procyclical fiscal policies, exacerbating business cycles."
  },
  {
    "qid": "econ-empirical-1164-3-0-0",
    "question": "1) Derive the demand functions for public and private sector enrollments as presented in Table 2, and explain the interpretation of the elasticity terms in both sectors.",
    "gold_answer": "1. **Public Sector Demand Function**: \n   - The demand function is given by: \n     \\[ \\ln E_u/P = \\alpha + \\beta_1 \\ln T_u + \\beta_2 \\ln T_v + \\beta_3 \\ln Y + \\epsilon \\]\n     where \\(E_u/P\\) is public sector enrollment per 100 eligibles, \\(T_u\\) is public tuition, \\(T_v\\) is private tuition, and \\(Y\\) is income.\n   - **Elasticity Interpretation**: \n     - \\(\\beta_1 = -1.783\\) (price elasticity of public tuition) is significant and negative, indicating that higher public tuition reduces public enrollment.\n     - \\(\\beta_2 = 1.373\\) (cross-price elasticity) is significant and positive, indicating substitutability between public and private tuition.\n     - \\(\\beta_3 = 0.301\\) (income elasticity) is not statistically significant.\n\n2. **Private Sector Demand Function**: \n   - The demand function is given by: \n     \\[ \\ln E/P = \\alpha + \\beta_1 \\ln T_u + \\beta_2 \\ln T_v + \\beta_3 \\ln Y + \\epsilon \\]\n   - **Elasticity Interpretation**: \n     - \\(\\beta_1 = 0.202\\) (cross-price elasticity) is not significant.\n     - \\(\\beta_2 = -0.714\\) (price elasticity of private tuition) is not significant.\n     - \\(\\beta_3 = 1.099\\) (income elasticity) is significant and positive, indicating that higher income increases private enrollment.",
    "question_context": "The years between 1939 and 1947 have been omitted because of the effects of World War II on enrollments in IHEs. The war continued to have an effect after it had ended because of the educational benefits provided to veterans under the G.I. Bill.\nIn columns (1) and (2) of Table 2 we present estimates of the demand functions for the public and private sectors, respectively. The variables are in natural logarithms, enrollment ratios are in enrollments per 100 eligibles, and the tuition variables and income are in dollars, 1958 prices.\nIn the demand equation for the public sector, the signs of the elasticity terms are as expected from the theory. Although both price elasticity coefficients are statistically significant, the income elasticity coefficient is not significantly different from zero at any of the generally accepted levels of significance.\nIn the demand equation for the private sector, the signs of the elasticity terms are also as expected from the theory. Here, however, the price elasticity coefficients cannot be accepted as significantly different from zero on the basis of $t$-ratios.\nIn columns (3) and (4) of Table 2, the regression coefficients of $\\ln T_{u}$ are direct estimates of the sum of the own price and cross price elasticity terms in the public and private sector, respectively. Both of these coefficients are negative and significantly less than zero, indicating that own price effects are greater than cross price effects in each sector.\n\nThis section presents an empirical analysis of tuition data constraints and the effects of World War II on enrollments in higher education institutions (IHEs). The analysis covers specific years from 1927 to 1972, omitting 1939-1947 due to war effects. The G.I. Bill's impact on enrollment distribution is also discussed, with adjustments made to eliminate its influence on postwar data."
  },
  {
    "qid": "econ-empirical-630-2-0-2",
    "question": "3) Prove Property 3: If $k'>k$, then $\\Pi(k')>\\Pi(k)$ when $\\sigma\\rightarrow\\infty$ or $\\theta\\rightarrow0$.",
    "gold_answer": "1. **Case $\\sigma\\rightarrow\\infty$**: From (12), $\\beta_{k}\\rightarrow 1$. Then $a(k)=\\sigma\\sqrt{k\\beta_{k}}\\approx\\sigma\\sqrt{k}$.\\n2. From (15), $\\Pi(k)\\approx a(k)\\phi(0)=\\sigma\\sqrt{k}\\cdot\\frac{1}{\\sqrt{2\\pi}}$. Thus, $\\Pi(k')>\\Pi(k)$ since $k'>k$.\\n3. **Case $\\theta\\rightarrow0$**: $\\beta_{k}\\rightarrow 1$. The same logic applies as above.",
    "question_context": "Let $A$ denote the decision maker. Let $k$ denote his expertise breadth. That is, $\\#A=k\\leq n$. Unless stated otherwise, we assume that $k\\geq2$, which is required to ensure that there are two levels in the hierarchies under consideration here.\nThe expected value of this posterior of $X_{i}$, given $r_{i}$, is $\\operatorname{E}[X_{i}\\mid r_{i}]=\\mu+\\beta_{k}(r_{i}-\\mu)$, where $\\beta_{k}\\equiv\\sigma^{2}\\big/\\big(\\sigma^{2}+\\theta^{2}\\alpha_{k}^{2}\\big)$.\nThe profit is $\\Pi(k)\\equiv\\int_{P(\\rho(k))>0}P(\\rho(k))d\\Psi\\left[P(\\rho(k))\\right]$, where an explicit expression is $\\Pi(k)=(n\\mu-c)\\left[1-\\Phi\\left(\\frac{c-n\\mu}{a(k)}\\right)\\right]+a(k)\\phi\\left(\\frac{c-n\\mu}{a(k)}\\right)$, with $a(k)\\equiv\\sigma\\sqrt{k\\beta_{k}}$.\nThe organizational surplus is $\\Delta(k)\\equiv\\Pi(k)-\\Pi^{0}$, where $\\Pi^{0}\\equiv\\left\\{\\begin{array}{c l}{n\\mu-c,}&{\\mathrm{if}\\ n\\mu-c>0,}\\ {0,}&{\\mathrm{otherwise.}}\\end{array}\\right.$\nProperty 1: $\\partial\\Pi(k)/\\partial\\sigma>0$. Property 2: $\\partial\\Pi(k)/\\partial\\theta<0$. Property 3: If $k'>k$, then $\\Pi(k')>\\Pi(k)$ if $\\sigma\\rightarrow\\infty$ or $\\theta\\rightarrow0$.\nProposition 1: For two flat hierarchies with expertise breadths $k$ and $l$, the former is better if and only if $\\frac{k}{l}>\\frac{\\sigma^{2}+\\theta^{2}\\alpha_{k}^{2}}{\\sigma^{2}+\\theta^{2}\\alpha_{l}^{2}}$.\nProposition 2: The optimal expertise breadth $K(\\sigma,\\theta,n)$ is nondecreasing in $\\sigma$ and $n$, and non-increasing in $\\theta$.\n\nThe section analyzes flat hierarchies with two layers: an upper layer consisting of a decision maker and a lower layer of superspecialists. The profit of a flat hierarchy is derived based on the decision maker's expertise breadth, business unpredictability, and communication noisiness. Key properties of organizational profit and surplus are discussed, along with comparative statics of optimal flat hierarchies."
  },
  {
    "qid": "econ-empirical-403-2-0-1",
    "question": "2) Critically evaluate the monetarist claim that deficit spending ‘crowds out’ productive private investment. Distinguish between supply-constrained and money-constrained scenarios.",
    "gold_answer": "1. **Supply-Constrained Scenario**: When real output is limited by supply (e.g., full employment), any new demand (e.g., from deficit spending) competes for scarce resources, inevitably crowding out private investment. \n2. **Money-Constrained Scenario**: When output is constrained by insufficient money supply, deficit-financed demands can be accommodated by increasing money supply or velocity, avoiding crowding out. \n3. **Monetarist Oversight**: Popular monetarist claims often conflate these scenarios, ignoring that crowding out is not inevitable in money-constrained cases. \n4. **Policy Implication**: The effectiveness of deficit spending depends on the underlying constraint, a nuance often missing in ideological debates.",
    "question_context": "Monetarist doctrine says that deficits increase aggregate nominal spending only as they lead to increases of money supply. In countries with underdeveloped financial systems, printing money may be the only feasible way to finance deficits. But in countries like the United States and United Kingdom, any linkage must be political choice rather than technical necessity.\nMonetarists frequently charge deficit spending with ‘crowding out’ productive private investment. Popular versions of this charge are particularly disingenuous in failing to distinguish cases in which real output is supply-constrained from those in which it is merely money-constrained.\nThe new classical macro-economics has given a theoretical rationale for propositions that were previously matters of faith and empirical judgement. The grasp of the Invisible Hand is extended beyond micro-economic resource allocation to macro-economic optimality.\nThe message of the new classical macro-economics is not so much that Keynesian policies do Evil as that they do Nothing. Not quite: an alleged evil is that capricious shifts in policy rules confuse private agents and cause allocational distortions.\n\nMonetarism has become a central part of conservative ideology, often associated with opposition to government intervention in economic activities, redistribution of income, and budget deficits. The text explores the logical connections of monetarism to its ideological partners and critiques its theoretical and empirical foundations."
  },
  {
    "qid": "econ-empirical-582-3-0-3",
    "question": "4) Explain the transformation \\( M_{1}Y_{i} = \\widetilde{Y}_{i} \\) and derive its mean and variance under the conditional distribution.",
    "gold_answer": "1. The transformation \\( M_{1}Y_{i} = \\widetilde{Y}_{i} \\) is used to handle the singularity in the covariance matrix.\n2. The mean of \\( \\widetilde{Y}_{i} \\) is:\n   \\[\n   E(\\widetilde{Y}_{i}|\\hat{\\alpha}_{i}) = M_{1}X_{i}\\beta.\n   \\]\n3. The variance is:\n   \\[\n   V(\\widetilde{Y}_{i}|\\hat{\\alpha}_{i}) = \\widetilde{\\Sigma} = M_{1}\\Sigma M_{1}.\n   \\]\n4. This transformation reduces the rank of the covariance matrix to \\( T-1 \\), allowing for standard GLS estimation.",
    "question_context": "Consider the density of the ith observation, \\( f(Y_{i};X,\beta,\\Sigma,\\alpha_{i})=(2\\pi)^{-\\frac{1}{2}}\big|\\Sigma\big| \times\\exp{-\\frac{1}{2}(Y_{i}-X_{i}\beta-I_{T}\\alpha_{i})^{\\prime}}\\Sigma^{-1}(Y_{i}-X_{i}\beta-I_{T}\\alpha_{i}). \\)\nA natural statistic for \\( \\alpha_{i} \\) is \\( \\hat{\\alpha}_{i}=(1_{T}^{\\prime}\\Sigma^{-1}I_{T})^{-1}I_{T}\\Sigma^{-1}(Y_{i}-X_{i}\beta), \\) a weighted average of the elements of \\( Y_{i}-X_{i}\beta \\).\nThe joint distribution of \\( Y_{i} \\) and \\( \\hat{\\alpha}_{i} \\) is a singular normal distribution with mean and covariance \\( E{\binom{Y_{i}}{\\hat{\\alpha}_{i}}}={\binom{X_{i}\beta+I_{T}\\alpha_{i}}{\\alpha_{i}}}, \\qquad V{\binom{Y_{i}}{\\hat{\\alpha}_{i}}}={\\left[\begin{array}{l l}{\\Sigma}&{c l_{T}}\\ {c l_{T}^{\\prime}}&{c}\\end{array}\right]}, \\) where \\( c=({l}_{T}^{\\prime}{\\Sigma}^{-1}{l}_{T})^{-1}. \\)\nThe distribution of \\( Y_{i} \\) conditional on \\( \\hat{\\alpha}_{i} \\) has mean \\( E(Y_{i}\big|\\hat{\\alpha}_{i})=X_{i}\beta+I_{T}\\hat{\\alpha}_{i} \\) and variance \\( V(Y_{i}\big|\\hat{\\alpha}_{i})=\\Sigma_{c}=\\Sigma-c I_{T}{\\cal I}_{T}^{\\prime}. \\)\nThe transformation \\( M_{1}Y_{i}=\\widetilde{Y}_{i} \\) results in a distribution with mean \\( E(\\widetilde{Y}\big|\\hat{\\alpha}_{i})=M_{1}X_{i}\beta \\) and variance \\( V(\\widetilde{Y}_{i}\big|\\hat{\\alpha}_{i})=\\widetilde{\\cal{\tilde{\\Sigma}}}=M_{1}\\Sigma M_{1}. \\)\nThe nonlinear regression model considered is \\( y_{i t}=g(x_{i t},\theta)+\\alpha_{i}+\\varepsilon_{i t}, \\) which can be stacked over time for individual \\( i \\) to give \\( y_{i}=g(x_{i},\theta)+I_{T}\\alpha_{i}+\\varepsilon_{i}, \\) and stacked over individuals to \\( y=g(x,\theta)+Z\\alpha+\\varepsilon. \\)\nThe generalized least squares estimator minimizes \\( \\mathcal{Q}_{1}(\theta,z)=(y-g(x,\theta)-Z x)^{\\prime}(I\\otimes\\Sigma^{-1})(y-g(x,\theta)-Z x), \\) with respect to \\( \theta \\) and \\( \\propto \\) for \\( \\Sigma \\) known.\n\nThe conditional likelihood approach to estimation focuses attention on the density of the observables \\( Y_{i} \\) conditional on sufficient statistics for the incidental parameters \\( \\alpha_{i} \\). The text discusses the joint distribution of \\( Y_{i} \\) and \\( \\hat{\\alpha}_{i} \\), the conditional distribution of \\( Y_{i} \\) given \\( \\hat{\\alpha}_{i} \\), and the transformation to \\( \\widetilde{Y}_{i} \\) to handle singularity in the covariance matrix. It also extends the discussion to nonlinear regression models with additive effects."
  },
  {
    "qid": "econ-empirical-374-4-0-1",
    "question": "2) Show that the condition $T^{-1-2\\delta}\\sum_{t=2}^{T}z_{t-1}^{2}\\Rightarrow V_{z}$ implies that the variables satisfy Assumption 2.",
    "gold_answer": "1. Assumption 2 requires that the variables satisfy certain convergence properties, including the condition on the partial sums of $z_{t-1}^{2}$.\n2. Given $T^{-1-2\\delta}\\sum_{t=2}^{T}z_{t-1}^{2}\\Rightarrow V_{z}$, this implies that the normalized sum converges to a finite limit $V_{z}$.\n3. This convergence ensures that the variance of the partial sums is well-behaved asymptotically, which is a key requirement for Assumption 2.\n4. The parameter $\\delta$ controls the rate of convergence, ensuring that the sum does not diverge too quickly or too slowly.",
    "question_context": "Note that $\\begin{array}{r}{\\left(\\frac{1}{\\sqrt{T}}\\sum_{j=1}^{\\lceil r T\\rceil}u_{t},\\frac{1}{\\sqrt{T}}\\sum_{j=1}^{\\lceil r T\\rceil}v_{t}\\right)^{\\prime}\\implies(\\sigma_{u}W_{u}(r),\\sigma_{v}W_{v}}\\end{array}$ $(r))^{\\prime}$ where the r.h.s. is a bivariate Brownian motion with covariance $\\boldsymbol{\\Sigma}$ , and $\\begin{array}{r}{\\frac{1}{\\sqrt{T}}X_{[r T]}\\implies\\sigma_{v}J_{c}\\left(r\\right)}\\end{array}$ ; see Phillips and Durlauf (1986) and Phillips (1987).\nWe show that each of the variables satisfies Assumption 3 and the condition $T^{-1-2\\delta}\\sum_{t=2}^{T}z_{t-1}^{2}\\Rightarrow V_{z}$ , which implies that the discussed variables o bey Assumption 2. We take the route over Assumption 3 in order to avoid proving very similar results twice. Note that Assumption 3 involves the additional parameter $\\vartheta<1/2$ which does not play an explicit role for the VA procedure, but simplifies some of the derivations significantly.\nFor all four choices of $z_{t}$ , the condition $\\begin{array}{r}{\\frac{1}{T^{1/2+\\vartheta+\\delta}}\\sum_{j=1}^{[r T]}z_{j}\\Rightarrow Z(r)}\\end{array}$ implies $\\begin{array}{r}{\\frac{1}{T^{1.5+\\delta}}\\sum_{t=2}^{T}z_{t-1}x_{t-1}\\xrightarrow{p}0}\\end{array}$ Concretely, it is shown in the proof of Theorem 2 that $$ \\frac{1}{T^{\\vartheta+\\delta+1}}\\sum_{t=2}^{T}z_{t-1}x_{t-1}\\Rightarrow R_{z x}^{c}, $$ the key ingredient in establishing convergence to $R_{z x}^{c}$ being precisely the weak convergence $\\begin{array}{r}{\\frac{1}{T^{1/2+\\vartheta+\\delta}}\\sum_{j=1}^{[r T]}z_{j}\\Rightarrow Z(r)}\\end{array}$ . Then, given that $\\begin{array}{r}{\\vartheta<1/2,\\frac{1}{T^{1.5+\\delta}}\\sum_{t=2}^{T}z_{t-1}x_{t-1}{\\xrightarrow{p}}0}\\end{array}$ follows immediately.\n\nThe text discusses the convergence properties of partial sums involving bivariate Brownian motion and the implications for various assumptions in econometric models."
  },
  {
    "qid": "econ-empirical-433-4-0-2",
    "question": "3) Analyze the state variables' impact on output growth as shown in Fig. 2, focusing on the quadratic fit for consumption and stock returns.",
    "gold_answer": "1. **Top-left panels**: Confirm state variables impact output growth as per the model.\\n2. **Fig. 2(a)**: Consumption and stock returns respond positively to $x_{t}$, with quadratic fit indicating stronger response during downturns.\\n3. **Fig. 2(b)**: Output and consumption growth decrease with high volatility, aligning with Bloom (2009) and Nakamura et al. (2017).",
    "question_context": "Table 1 shows model selection criteria for variants of model (2) estimated using the particle filter EM algorithm with $N^{s}~=~5000$ particles and resampling threshold $E S S=0.75N^{s}$.\nFig. 1(a) plots the smoothed persistent output growth component $x_{t|T}$ and the aggregate log price–dividend ratio over time.\nFig. 1(b) plots the estimated volatilities of output growth and aggregate stock returns.\n\nThis section examines the relationship between economic growth risk and stock market volatility using long-run risk stochastic volatility models. The analysis includes model selection criteria, smoothed latent states, and comparisons of economic and financial volatility measures."
  },
  {
    "qid": "econ-empirical-77-5-2-1",
    "question": "6) Derive the market-clearing condition for U.S. variety $g$ given by $$q_{g}=\\left(a_{D g}D_{s}\\right)\\left(\\frac{p_{D g}}{P_{D s}}\\right)^{-\\eta}+\\sum_{i\\in\\mathcal{T}}\\delta_{i g}a_{i g}^{*}\\left(\\left(1+\\tau_{i g}^{*}\\right)p_{i g}^{X}\\right)^{-\\sigma^{*}},$$ and interpret each term.",
    "gold_answer": "1. The first term $\\left(a_{D g}D_{s}\\right)\\left(\\frac{p_{D g}}{P_{D s}}\\right)^{-\\eta}$ represents U.S. domestic demand for good $g$, where:\n   - $a_{D g}$ is a demand shock.\n   - $D_s$ is aggregate U.S. consumption of domestic goods in sector $s$.\n   - $P_{D s}$ is the price index of domestically produced goods.\n2. The second term $\\sum_{i\\in\\mathcal{T}}\\delta_{i g}a_{i g}^{*}\\left(\\left(1+\\tau_{i g}^{*}\\right)p_{i g}^{X}\\right)^{-\\sigma^{*}}$ sums foreign import demand across countries $i$, where:\n   - $\\delta_{ig}$ is the iceberg trade cost.\n   - $a_{ig}^*$ is a foreign demand shifter.\n   - $\\tau_{ig}^*$ is the retaliatory tariff.\n   - $p_{ig}^X$ is the export price.",
    "question_context": "The United States experiences a terms-of-trade gain in a sector if the price of products in that sector increases compared to the price of its imports. U.S. and foreign tariffs affect these prices by shifting world demand.\nThe extent of price changes due to tariffs depends on the elasticities of U.S. and foreign demands, which we have estimated, and on the sector-level elasticities of U.S. supply, which we have imposed through the model assumptions and the calibration.\n\nThe model analyzes short-run price effects of tariffs, assuming no primary factor mobility. Sector-level supply is upward-sloping, and prices are determined by U.S. supply and world demand."
  },
  {
    "qid": "econ-empirical-221-3-0-3",
    "question": "4) What is the 'double robustness' property of the weighted estimator in scenarios four and eight, and why is it significant?",
    "gold_answer": "The double robustness property means the weighted estimator consistently estimates the solution to (2.1) if either $\\mathrm{D}(\\boldsymbol{y}|\\boldsymbol{x})$ or $\\mathrm{P}(s=1|x)$ is correctly specified. This is significant because it provides robustness against model misspecification, ensuring consistency under broader conditions.",
    "question_context": "If features of an unconditional distribution, say $\\mathrm{D}(w)$, are of interest, unweighted estimators consistently estimate the parameters only if $\\mathrm{P}(s=1|w)=\\mathrm{P}(s=1)$ —that is, the data are ''missing completely at random'' (Rubin, 1976).\nIf Assumption 3.1 fails, the weighted estimator will be inconsistent for the parameters of an unconditional distribution.\nThe decision to weight is more subtle when we begin with the premise that some feature of a conditional distribution, $\\mathrm{D}(\\boldsymbol{y}|\\boldsymbol{x})$, is of interest.\nScenario four covers the important case where $\\mathrm{D}(\\boldsymbol{y}|\\boldsymbol{x})$ is misspecified yet we consistently estimate the solution to (2.1) using the weighted estimator.\nScenario seven is problematical for the weighted estimator and represents the strongest case against weighting.\n\nThe text discusses the conditions under which weighted estimators are desirable or undesirable, focusing on scenarios where features of unconditional or conditional distributions are of interest. It outlines various scenarios based on the correctness of model specifications and the nature of selection probabilities."
  },
  {
    "qid": "econ-empirical-862-2-0-3",
    "question": "4) Explain the economic intuition behind the condition $\\gamma > 1$ for nominal income targeting to be superior, using the graphical analysis from Fig. 1.",
    "gold_answer": "1. In Fig. 1, the slope of the demand curve $DD$ is $-\\gamma$, and the slope of the nominal income line $XX$ is $-1$.\n2. If $\\gamma < 1$, $DD$ is steeper than $XX$, meaning demand is less responsive to price changes.\n3. Under nominal income targeting, the monetary authority adjusts money supply to keep nominal income constant, leading to larger output fluctuations when demand is inelastic ($\\gamma < 1$).\n4. If $\\gamma > 1$, $DD$ is flatter than $XX$, meaning demand is more responsive to price changes.\n5. Nominal income targeting then stabilizes output more effectively because demand adjustments offset price changes, reducing output variance.",
    "question_context": "Aggregate demand and supply are given by:\n\n$$\n\\begin{array}{r l}&{y_{t}=\\gamma_{0}+\\gamma(m_{t}-\\hslash)+v_{t},}\\ &{y_{t}=y_{0}+\\beta(\\hslash-\\hslash^{e})+u_{t},}\\ &{\\hat{p}_{t}^{e}=\\left(\\mathrm{\\ar~}-\\theta\\right)\\hat{p}_{t-1}+\\theta\\hat{p}_{t-1}^{e}\\quad(0\\leqslant\\theta<\\mathrm{\\at},0<\\gamma,\\beta).}\\end{array}\n$$\n\n${\\pmb y}_{t}$ is log output, $m_{t}$ log money stock, $\\pmb{\\phi}_{t}$ log price level, $\\pmb{\\mathscr{p}}_{t}^{e}\\log_{\\mathbf{\\theta}}$ price expected by the public at period ${t-1,y_{0}}$ the natural rate of output, $\\pmb{v_{t}}$ and $\\pmb{u}_{t}$ serially uncorrelated disturbances.\nWith the constants set to zero, the reduced form of the model is:\n\n$$\n\\begin{array}{r l}&{y_{t}=(\\beta+\\gamma)^{-1}\\left(\\beta\\gamma m_{t}-\\beta\\gamma\\dot{p}_{t}^{e}+\\beta v_{t}+\\gamma u_{t}\\right),}\\ &{\\dot{p}_{t}=(\\beta+\\gamma)^{-1}\\left(\\gamma m_{t}+\\beta\\dot{p}_{t}^{e}+v_{t}-\\dot{u}_{t}\\right).}\\end{array}\n$$\nThe monetary authority must choose the period $t$ money supply $m_{t}$ before the shocks $\\pmb{u}_{t}$ and $\\pmb{v}_{t}$ are known. Policies are compared in terms of their ability to stabilise output, i.e. to minimise $\\mathbb{E}y_{t}^{2}.$\nUnder a constant money supply rule, say, $m_{t}=0$ for all $t$, the variance of output is given by:\n\n$$\n\\begin{array}{r l}&{\\mathrm{E}y_{t}^{2}=\\beta^{2}(\\beta+\\gamma)^{-2}\\left[2\\left(\\mathrm{I}-\\rho\\right)\\left(\\mathrm{I}-\\rho^{2}\\right)^{-1}\\mathrm{E}\\left(\\upsilon_{t}-u_{t}\\right)^{2}\\right]}\\ &{\\quad\\quad\\quad+2\\beta(\\beta+\\gamma)^{-1}\\mathrm{E}u_{t}(\\upsilon_{t}-u_{t})+\\mathrm{E}u_{t}^{2}.}\\end{array}\n$$\nUnder the nominal income rule, the variance of output is:\n\n$$\n\\begin{array}{r l}&{\\mathrm{E}y_{t}^{2}=\\beta^{2}(\\beta+\\gamma)^{-2}\\left[2\\left(\\mathrm{I}-\\nu\\right)\\left(\\mathrm{I}-\\nu^{2}\\right)^{-1}\\mathrm{E}\\left(\\nu_{t}-u_{t}\\right)^{2}\\right]}\\ &{\\mathrm{~~}+2\\beta(\\beta+\\gamma)^{-1}\\mathrm{E}u_{t}(v_{t}-u_{t})+\\mathrm{E}u_{t}^{2}.}\\end{array}\n$$\nA policy of targeting nominal income leads to a lower output variance than does a policy of targeting the money supply if and only if the elasticity of demand with respect to real balances is greater than one ($\\gamma>1$).\n\nThe text discusses the comparison between nominal income and money supply targets in monetary policy, using a detailed mathematical model of aggregate demand and supply. The model includes adaptive expectations and analyzes the impact of demand and supply shocks on output variance under different policy rules."
  },
  {
    "qid": "econ-empirical-1492-0-0-1",
    "question": "2) Evaluate the effectiveness of the Load on Top system in reducing marine oil pollution, and discuss potential technological improvements.",
    "gold_answer": "The Load on Top system reduces pollution by:\\n1. **Minimizing Discharge**: It allows residual oil to be separated and retained onboard.\\n2. **Cost-Effectiveness**: It is a practical solution for tankers.\\nImprovements could include:\\n- **Crude Oil Washing**: Replacing water with crude oil for tank cleaning, reducing waste.\\n- **Automation**: Enhancing detection and separation technologies.",
    "question_context": "The problem itseif is not dramatised, and the case for prevention is taken for granted. People directly concerned with the problem will probably find explanations of the limitations of international law enlightening, while those with more academic interests are likely to benefit particularly from the emphasis on technical questions in both prevention, cure, and detection.\nThe workings of the Load on Top system are well explained, but one would have liked more details on the most recent technological advances, such as the washing of tanks with crude oil instead of water.\nLegal progress, the author concludes, now depends largely on the erosion of two engrained principles of international law, exclusive 'flagstate jurisdiction' and 'freedom of the High Seas'.\n\nThe text discusses the international legal framework governing marine oil pollution, including the limitations of international law and recent technological advances in prevention and detection."
  },
  {
    "qid": "econ-empirical-976-3-0-3",
    "question": "4) Discuss the application of the indirect-MD approach in the oil supply shock example, including the selection and testing of proxies for non-target shocks.",
    "gold_answer": "1. **Proxies for non-target shocks**: The proxies $w_{t}:=(w_{t}^{R V},w_{t}^{B r})^{\\prime}$ are selected for $\\varepsilon_{2,t}=(\\varepsilon_{t}^{A D},\\varepsilon_{t}^{O S D})^{\\prime}$.\n2. **Bootstrap pre-test**: The multivariate normality test ($\\tau_{T,N}^{*}\\equiv DH$) yields a $p$-value of 0.67, failing to reject the null of relevance.\n3. **IRF estimation**: The indirect-MD approach yields IRFs (red lines in Fig. 2) with tighter confidence intervals (red shaded areas) compared to direct approaches.\n4. **Exogeneity check**: Correlations $\\widehat{Corr}(w_{t},z_{t})$ and $\\widehat{Corr}(w_{t},\\hat{\\varepsilon}_{t}^{S,Chol})$ are insignificant, supporting exogeneity.",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nThe following proposition establishes that the statistics $\\rho_{T}$ and $\\tau_{T,N}^{*}$ are asymptotically independent (as $T,N\\to\\infty.$ ).\nProposition 7 (Asymptotic Independence). Let $\\rho_{T}$ and $\\tau_{T,N}^{*}$ be as defined above. For any $x_{1},x_{2}\\in\\mathbb{R}$ and $T,N\\to\\infty.$ , it holds that\n\n$$\nP(\\{\\rho_{T}\\leq x_{1}\\}\\cap\\{\\tau_{T,N}^{*}\\leq x_{2}\\})-P(\\rho_{T}\\leq x_{1})P(\\tau_{T,N}^{*}\\leq x_{2})\\to0,\n$$\n\nprovided that the conditions of Proposition 5 or Proposition 6 hold.\n\nThe section discusses the importance of assessing the reliability of post-test inferences in proxy-SVAR models, particularly focusing on the coverage of confidence intervals for IRFs obtained via the indirect-MD approach. It introduces key statistics and propositions to establish asymptotic independence between bootstrap-based statistics and original sample estimates."
  },
  {
    "qid": "econ-empirical-909-2-0-1",
    "question": "2) Explain how to consistently estimate the asymptotic variance $\bar{\\rho}$ in (4.2), focusing on the challenges in evaluating the first term.",
    "gold_answer": "1. **Consistent Estimation**: The asymptotic variance $\bar{\\rho}$ is decomposed into three terms in (4.2). The last term can be consistently estimated using (3.14), and the middle term is straightforward to evaluate.\n2. **First Term Challenge**: The first term involves $\\mathbb{E}T\\left[\\sum_{t=1}^{T}(\\tilde{g_{t}}-g_{t})/T\\right]^{2}$, which requires knowledge of the $\\alpha_{t k}$ parameters. Without additional assumptions or hypotheses about these parameters, consistent estimation is not feasible. Special cases or additional restrictions are needed to proceed.",
    "question_context": "The tests we now investigate concern the values of $\bar{g}=\\mathsf{p l i m}\\hat{g}$. In this investigation, however, we shall consider $\\hat{g}$ defined as using $\\hat{\\beta}$ to calculate the estimated residuals rather than ${\\hat{\\beta}}^{I}$. All developments for the one apply to the other with suitable change of components arising from the estimating formula for ${\\hat{\\beta}}$.\nThe developments leading up to (3.16) and (3.17) provide the basis for the procedures considered. Let $\\pi_{t}=\\bar{q}^{\\prime}(\\vec{M}^{\\prime}\\vec{Q}_{,A}^{-1}\\vec{M})^{-1}\\bar{M}^{\\prime}\\bar{Q}_{,A}^{\\phantom{\\dagger}}z_{{\\cal H}\\bar{t}}^{\\prime}$. Then, as (3.17) indicates, we are interested in the asymptotic distribution of $\\sum_{t=1}^{T}(\\tilde{g_{t}}-g_{t} + \\pi_{t}\\eta_{t})/T$. The form of $\tilde{g_{t}}$ and its m-dependence mean that this expression is itself the average of $m$-dependent variables.\nThe major difficulties arise in evaluating the first term of (4.2). To do so usually requires some knowledge or hypothesis about the values of the $\\alpha_{t k}$. However, as we see in the next section, some exceptions may occur for special forms.\nThe statistic in (4.16) provides a test for a moving-average of length at most $m$ against the alternative that the length is between $m$ and $R$. When $m=0$, it is a test for whether there is any autocovariance at all.\n\nThe analysis focuses on estimating and testing hypotheses about the moving-average parameters and heteroscedasticity in the model. The tests concern the values of $\bar{g} = \\mathsf{plim}\\hat{g}$, where $\\hat{g}$ is defined using $\\hat{\\beta}$ to calculate residuals. The asymptotic distribution of $T^{\\frac{1}{2}}(\\hat{g} - \\vec{g})$ is derived under the extended Liapounov Central Limit Theorem."
  },
  {
    "qid": "econ-empirical-218-1-0-0",
    "question": "1) Derive the error correction form of the VAR model and explain the role of the matrices $\\alpha$ and $\\beta$ in the cointegration framework.",
    "gold_answer": "1. Start with the VAR($p$) model in levels: $$x_t = \\sum_{i=1}^p \\Pi_i x_{t-i} + \\varepsilon_t.$$  \n2. Subtract $x_{t-1}$ from both sides and rearrange: $$\\Delta x_t = (\\Pi - I)x_{t-1} + \\sum_{i=1}^{p-1} \\Gamma_i \\Delta x_{t-i} + \\varepsilon_t,$$ where $\\Pi = \\sum_{i=1}^p \\Pi_i$ and $\\Gamma_i = -\\sum_{j=i+1}^p \\Pi_j$.  \n3. Under cointegration, $\\Pi$ has reduced rank $r < k$ and can be decomposed as $\\Pi = \\alpha \\beta^{\\prime}$, where $\\alpha$ contains the adjustment coefficients and $\\beta$ the cointegrating vectors.  \n4. The term $\\alpha \\beta^{\\prime} x_{t-1}$ represents the error correction mechanism, with $\\beta^{\\prime} x_{t-1}$ being the stationary linear combinations.",
    "question_context": "Consider a vector autoregressive error correction model of order $\\pmb{p}$ for the $(k\\times1)$ vector time series $\\left\\{x_{t}\\right\\}$: $$\\varDelta x_{t}=\\sum_{i=1}^{p-1}{{r_{i}}}\\varDelta x_{t-i}+I x_{t-1}+\\mu+\\varPhi D_{t}+\\varepsilon_{t},\\qquad t=1,\\ldots,T,$$ where $\\pmb{\\varepsilon_{t}}$ denotes a $k$-dimensional normal variate with mean zero and nonsingular, p.d.s. covariance matrix $\\Sigma$, $\\mu$ is a vector of constant terms, and $D_{t}$ is a vector of other deterministic variables.\nWe assume that: (1) rank $(I)=r<k$ so that $I$ can be written as $\\alpha\\beta^{\\prime}$, where both $\\pmb{\\alpha}$ and $\\beta$ are $(k\\times r)$ matrices of full column rank; (2) $\\alpha_{\\perp}^{\\prime}(I-{r_{1}}-\\cdots-{r_{p-1}})\\beta_{\\perp}$ has full rank, with $\\pmb{\\alpha}_{\\perp}$ and $\\beta_{\\perp}$ $\\boldsymbol{k}\\times(\\boldsymbol{k}-\\boldsymbol{r})$ matrices of full rank which are orthogonal to $\\pmb{\\alpha}$ and $\\beta$.\n\nThe text discusses the Gaussian maximum likelihood framework based on complete VARs for cointegration analysis, focusing on the Johansen and Juselius approach."
  },
  {
    "qid": "econ-empirical-1054-2-0-3",
    "question": "4) Compare the ADPF's proposal density construction with the CUPF1 algorithm, highlighting why ADPF might outperform CUPF1 for DSGE models.",
    "gold_answer": "**ADPF vs. CUPF1**:\n\n1. **Proposal Density**:\n   - ADPF uses a Gaussian mixture to approximate multimodal $p(u_{t}|y_{t},x_{t-1})$, capturing multiple modes via Laplace approximations.\n   - CUPF1 relies on a single Gaussian proposal from the Unscented Kalman Filter, which cannot represent multimodality.\n\n2. **DSGE Performance**:\n   - DSGE models often produce multimodal densities due to nonlinearities. ADPF's mixture adapts to these modes, while CUPF1's Gaussian proposal leads to inaccurate sampling.\n   - The simulation study confirms ADPF's superiority for such models.",
    "question_context": "To construct $g(u_{t}|y_{t},x_{t-1})$ , used in Step (3) of the ADPF algorithm of Appendix A, we use a mixture density calculated in the following way. We are approximating the density \n\n$$\np(u_{t}|y_{t},x_{t-1})\\propto p(y_{t}|x_{t-1},u_{t})p(u_{t})\n$$  \n\nwhere $y_{t}$ and $x_{t-1}$ are known, as the previous state is given by the filter at time $(t-1)$ as $x_{t-1}^{k}$ . That is, we are conditioning on a particular particle $x_{t-1}^{k}$ , though we suppress the superscript $k$ from the subsequent notation here. Due to the nonlinear nature of the state equation, in (8), there are several modes of $p(u_{t}|y_{t},x_{t-1})$ . Intuitively, this is because a given observation $y_{t}$ could have been generated by several possible innovations $u_{t}$ . In the case of Eq. (9) above, for instance, a given $C_{t}$ could usually be explained by two values of $\\epsilon_{t}$ .\nWe shall denote the $M$ modes as $\\widehat{\\boldsymbol{u}}_{t}^{j}$ , where $j=1,\\ldots,M$ . Let $\\ell(u_{t})=\\log\\{p(y_{t}|u_{t},x_{t-1})p(u_{t})\\}$ . In  the neighbourhood of each mode $\\widehat{\\boldsymbol{u}}_{t}^{j}$ , the following Laplace approximation may be made:  \n\n$$\n\\ell(u_{t})\\simeq\\ell(\\widehat{u}_{t}^{j})+\\frac{1}{2}(u_{t}-\\widehat{u}_{t}^{j})^{T}[\\ell^{\\prime\\prime}(\\widehat{u}_{t}^{j})](u_{t}-\\widehat{u}_{t}^{j}),\n$$  \n\nwhere $\\ell^{\\prime\\prime}(\\widehat{u}_{t}^{j})=\\left.\\nabla^{2}\\ell(u_{t})\\right|_{\\widehat{u}_{t}^{j}}$ , a negative definite matrix of second derivatives of $\\ell(u_{t})$ evaluated at the particular mode $\\widehat{\\boldsymbol{u}}_{t}^{j}$ . Then we construct  \n\n$$\n\\begin{array}{c}{{g(u_{t}|y_{t},x_{t-1})=\\displaystyle\\sum_{j=1}^{M}\\lambda_{j}(2\\pi)^{-d/2}\\left|\\left.\\boldsymbol{\\Sigma}_{j}\\right|^{-{\\frac{1}{2}}}\\right.}}\\ {{\\mathrm{}}}\\ {{\\displaystyle\\qquad\\times\\exp\\left\\{-{\\frac{1}{2}}(u_{t}-\\widehat{u}_{t}^{j})^{T}\\boldsymbol{\\Sigma}_{j}^{-1}(u_{t}-\\widehat{u}_{t}^{j})\\right\\}}}\\ {{\\mathrm{}}}\\ {{\\displaystyle=\\sum_{j=1}^{M}\\lambda_{j}\\mathcal{N}_{n_{u}}(u_{t}|\\widehat{u}_{t}^{j};\\boldsymbol{\\Sigma}_{j}),}}\\end{array}\n$$  \n\nwhere $\\varSigma_{j}=-[\\ell^{\\prime\\prime}(\\widehat{u}_{t}^{j})]^{-1}$ and  \n\nWe normalise this so that $\\textstyle\\sum_{j=1}^{M}\\lambda_{j}=1$ .\n\nThe text discusses the construction of a mixture density for the ADPF algorithm, focusing on approximating the conditional density of innovations given observations and previous states. It details the use of Laplace approximations around multiple modes and the formation of a Gaussian mixture proposal density."
  },
  {
    "qid": "econ-empirical-956-1-1-1",
    "question": "6) Derive the efficiency bound for the estimator $\\widehat{\\pmb{\\theta}}_{n}^{\\mathrm{new}}$ under the null hypothesis.",
    "gold_answer": "1. The efficiency bound is given by the inverse of the Fisher information matrix.\n2. For $\\widehat{\\pmb{\\theta}}_{n}^{\\mathrm{new}}$, the optimal variance is: $$\\text{Var}(\\widehat{\\pmb{\\theta}}_{n}^{\\mathrm{new}})^{*} = (\\overline{{\\pmb{x}^{\\top}\\pmb{\\Sigma}_{\\theta_{0}\\theta_{0}}^{-1}\\pmb{x}}})^{-1}.$$\n3. This is achieved when $\\phi_{t} = \\pmb{\\Sigma}_{\\theta_{0}\\theta_{0},t}^{-1}$.",
    "question_context": "The optimal choice o fˆ $\\bar{\\phi}$ is $\\bar{\\Sigma}_{\\theta\\theta}^{-1}$ and the optimal variance is given by $$\\begin{array}{r}{\\mathrm{Var}(\\widehat{\\pmb{\\theta}}^{\\mathrm{LTT}})^{*}=(\\bar{{\\pmb x}}^{\\intercal}\\bar{\\pmb{\\Sigma}}_{{\\pmb{\\theta}_{0}\\pmb{\\theta}_{0}}}^{-1}\\bar{{\\pmb x}})^{-1}.}\\end{array}$$\nLemma 4 (Cauchy–Schwarz Inequality for Matrix-valued Processes). Let $A=(A_{t})_{t\\in[0,T]}$ and $B=(B_{t})_{t\\in[0,T]}$ be two matrix processes such that all the following matrix products are well-defined. Assume that $\\overline{{B^{\\intercal}B}}$ is invertible. The following matrix $$\\overline{{A^{\\intercal}A}}-\\overline{{A^{\\intercal}B}}(\\overline{{B^{\\intercal}B}})^{-1}\\overline{{B^{\\intercal}A}}$$ is positive semi-definite.\n\nThis section derives the efficiency bound for estimating the time-invariant parameter vector θ under the restricted model."
  },
  {
    "qid": "econ-empirical-108-21-2-1",
    "question": "7) Derive the TFP and output losses due to informational frictions in case 2 and explain the cross-country variation.",
    "gold_answer": "1. TFP loss: $a^* - a = \\frac{\\alpha^2 \\mathbb{V}}{2(1 - \\alpha^2)}$.  \n2. Output loss: $y^* - y = \\frac{a^* - a}{1 - \\alpha}$.  \n3. United States: $4\\%$ TFP loss due to low $\\mathbb{V}$.  \n4. India: $10\\%$ TFP loss due to high $\\mathbb{V}$.  \n5. Cross-country variation reflects differences in $\\mathbb{V}$ and $\\alpha$.",
    "question_context": "The United States has less volatile fundamental shocks and lower levels of noise both in private signals at the firm level (lower $\\sigma_{e}$ ) and generally in the stock market as well (lower $\\sigma_{v}$ and $\\sigma_{z}$ ).\nCompared to the full-information benchmark, in case 2, losses in steady-state TFP range from $4\\%$ in the United States to $10\\%$ in India, with corresponding output losses of $5{-}14\\%$.\n\nThis section presents the quantitative impact of informational frictions on uncertainty, misallocation, and aggregate productivity and output losses across countries."
  },
  {
    "qid": "econ-empirical-1438-2-0-1",
    "question": "2) Prove that the best response strategy $\\sigma_{t}(x,y)$ is given by $\\operatorname*{max}\\big(0,U_{t}(x,\\emptyset)-U_{t}(x,y)\\big)$, using the optimization over threshold strategies.",
    "gold_answer": "To prove this:\n1. The agent maximizes $U_{t}(x;\\sigma)$ with respect to $\\sigma_{t}(x,y)$.\n2. The derivative of $U_{t}(x;\\sigma)$ with respect to $\\sigma_{t}(x,y)$ is:\n$$-r_{t}(x,y)e^{-\\sigma_{t}(x,y)}\\big[U_{t}(x,y;\\sigma) + \\sigma_{t}(x,y) + 1\\big] + r_{t}(x,y)e^{-\\sigma_{t}(x,y)}\\big[1 - (U_{t}(x,\\emptyset;\\sigma))\\big]$$\n3. Setting the derivative to zero yields:\n$$U_{t}(x,y;\\sigma) + \\sigma_{t}(x,y) + 1 = U_{t}(x,\\emptyset;\\sigma) + 1$$\n4. Simplifying gives $\\sigma_{t}(x,y) = U_{t}(x,\\emptyset;\\sigma) - U_{t}(x,y;\\sigma)$.\n5. Since $\\sigma_{t}(x,y)\\geq0$, we take the maximum with 0.",
    "question_context": "In each period, any two agents may meet at random. The mass of meetings between agents of types $x$ and $y$ is given by meeting function $q_{t}\\bigl(x,y;\\mu^{X}\\bigr)$ and depend on the agent types as well as the mass distribution $\\mu^{X}\\in\\dot{\\mathbb{R}}_{+}^{X}$ of agents in the population.\nIf a type $x$ agent forms a match with a type $y$ agent, agent $x$ receives payoff equal to the systematic utility term $\\upsilon_{t}(x,y)$ plus the payoff shock. The agent's type in the next period becomes $x^{\\prime}$ with probability $P_{t}\\big(x^{\\prime}|x,y\\big)$ that may depend on her own current type as well as the type of her match partner.\nThe model is completely characterized by the systematic utility $\\upsilon$ , birth rates $Q$ transition probabilities $P$ , and meeting function $q_{t}$ . In general, the parameters of the model may depend on time.\n\nA continuum population of agents lives in discrete time $t=1,2,\\ldots$ In each period, each agent is characterized by type $x\\in X$ ,where $X$ is a finite set. Agent type is typically not permanent and may change depending on agent behavior. In each period, a mass $Q_{t}(x)$ of agents are born."
  },
  {
    "qid": "econ-empirical-406-0-1-3",
    "question": "4) Discuss the role of the GMC(4) condition in verifying the assumptions of Theorem 2.1 for GARCH processes.",
    "gold_answer": "The GMC(4) condition ($\\mathbb{E}(|X_n - X_n'|^4) \\leq C\\rho^n$) implies:\n\n- Exponential decay of $\\delta_4(k)$, ensuring $\\sum_{k=0}^\\infty \\delta_4(k) < \\infty$.\n- Finite fourth-order cumulant sums via moment bounds.\n- Covers common GARCH models (e.g., EGARCH) by verifying (8) via their geometric ergodicity.",
    "question_context": "We establish the consistency of the bootstrap approximation using the Hilbert space approach, which allows us to get the limiting distribution of the Cramér–von Mises statistic.\nTheorem 2.1. Assume the process $X_t$ admits the representation (4), $X_t\\in\\mathcal{L}^4$ and $\\sum_{k=0}^\\infty \\delta_4(k) < \\infty$. Further assume $\\sum_{k_1,k_2,k_3=-\\infty}^\\infty |\\text{cum}(X_0,X_{k_1},X_{k_2},X_{k_3})| < \\infty$. Then $S_n(\\lambda) - \\mathbb{E}\\{S_n(\\lambda)\\} \\Rightarrow S(\\lambda)$ in $L_2[0,\\pi]$.\n\nThe paper proposes a blockwise wild bootstrap to approximate the null distribution of the Cramér–von Mises statistic, addressing the limitations of asymptotic theory under dependence."
  },
  {
    "qid": "econ-empirical-845-0-0-2",
    "question": "3) Derive the Lipschitz continuity of the excess spending map under the assumption of quadratic concavity.",
    "gold_answer": "Given quadratic concavity, the excess spending map $E(\\lambda)$ is Lipschitz continuous because:\n1. The planner's problem has a unique solution $x^*(\\lambda)$ for each welfare weight $\\lambda$.\n2. Quadratic concavity implies the utility function is strongly concave, ensuring $x^*(\\lambda)$ is Lipschitz in $\\lambda$.\n3. Supporting prices $p^*(\\lambda) = DU_i(x_i^*(\\lambda))$ are Lipschitz due to the smoothness of $U_i$.\n4. Thus, $E(\\lambda) = \\sum_i (x_i^*(\\lambda) - e_i)$ is Lipschitz as a composition of Lipschitz functions.",
    "question_context": "One of the central features of classical models of competitive markets is the generic determinacy of competitive equilibria. For smooth economies with a finite number of commodities and a finite number of consumers, almost all initial endowments admit only a finite number of competitive equilibria, and these equilibria vary (locally) smoothly with endowments; thus equilibrium comparative statics are locally determinate.\nQuadratic concavity provides a quantitative measure of the extent to which distinct commodities are not perfect substitutes—globally, locally, or asymptotically. We use quadratic concavity to give a direct analysis of Pareto optima and supporting prices.\nA simple geometric argument shows that the solution to the planner's problem is Lipschitz; a parallel analysis of supporting prices establishes that the excess spending mapping is Lipschitz. Generic determinacy then follows by arguments similar to those in Shannon (1999).\nWe consider an exchange economy $\\mathcal{E}$ with $m$ consumers. The commodity space $X$ is a vector lattice endowed with a Hausdorff, locally convex topology $\\tau$. The price space $X^{*}$ is the topological dual of $X$ and is a sublattice of the order dual of $X$. Order intervals in $X$ are weakly compact.\n\nThe paper establishes the generic determinacy of competitive equilibria for economies with finitely many consumers and infinitely many commodities, introducing the concept of quadratic concavity to rule out preferences where goods are perfect substitutes."
  },
  {
    "qid": "econ-empirical-800-0-0-2",
    "question": "3) Derive the asymptotic distribution of the estimated tangency portfolio weights $\\hat{\\mathbf{w}}_{\\mathrm{TP}}$ under the assumption that returns follow a multivariate stationary Gaussian process.",
    "gold_answer": "1. **Asymptotic Normality**: By the delta method, $\\sqrt{n}(\\hat{\\mathbf{w}}_{\\mathrm{TP}} - \\mathbf{w}_{\\mathrm{TP}}) \\xrightarrow{d} \\mathcal{N}_{k}(0, \\mathbf{G}_{4}^{\\prime}\\pmb{\\Omega}_{1}\\mathbf{G}_{4})$, where $\\mathbf{G}_{4}$ is the Jacobian of $g_{4}(\\pmb{\\theta}_{1}) = \\alpha^{-1}\\pmb{\\Sigma}^{-1}(\\pmb{\\mu} - r_{f}\\mathbf{1})$.  \n2. **Jacobian Components**:  \n   - $\\frac{\\partial \\mathbf{w}_{\\mathrm{TP}}^{\\prime}}{\\partial \\pmb{\\mu}} = \\alpha^{-1}\\pmb{\\Sigma}^{-1}$  \n   - $\\frac{\\partial \\mathbf{w}_{\\mathrm{TP}}}{\\partial \\sigma_{v\\tau}} = -\\alpha^{-1}\\Delta_{v\\tau}(\\pmb{\\mu} - r_{f}\\mathbf{1})$  \n3. **Covariance Matrix**: For serially uncorrelated returns, $\\pmb{\\Omega}_{1}$ simplifies to:  \n   $$\\Omega_{1} = \\begin{pmatrix} \\pmb{\\Sigma} & 0 \\\\ 0 & \\text{Var}(\\text{vech}(\\hat{\\pmb{\\Sigma}})) \\end{pmatrix}.$$  \n4. **Final Form**: The asymptotic variance is:  \n   $$\\alpha^{-2}\\pmb{\\Sigma}^{-1} + \\alpha^{-2}\\left((\\pmb{\\mu} - r_{f}\\mathbf{1})^{\\prime}\\pmb{\\Sigma}^{-1}(\\pmb{\\mu} - r_{f}\\mathbf{1})\\pmb{\\Sigma}^{-1} + \\pmb{\\Sigma}^{-1}(\\pmb{\\mu} - r_{f}\\mathbf{1})(\\pmb{\\mu} - r_{f}\\mathbf{1})^{\\prime}\\pmb{\\Sigma}^{-1}\\right).$$",
    "question_context": "The weights of the expected quadratic utility (EU) optimal portfolio are given by: $${\\bf w}_{\\mathrm{EU}}={\\frac{{\\boldsymbol{\\Sigma}}^{-1}{\\mathbf{1}}}{{\\mathbf{1}}^{\\prime}{\\boldsymbol{\\Sigma}}^{-1}{\\mathbf{1}}}}+{\\boldsymbol{\\alpha}}^{-1}{\\mathbf{R}}{\\boldsymbol{\\mu}}\\quad\\mathrm{with}\\quad{\\bf R}={\\boldsymbol{\\Sigma}}^{-1}-{\\frac{{\\boldsymbol{\\Sigma}}^{-1}{\\mathbf{1}}{\\mathbf{1}}^{\\prime}{\\boldsymbol{\\Sigma}}^{-1}}{{\\mathbf{1}}^{\\prime}{\\boldsymbol{\\Sigma}}^{-1}{\\mathbf{1}}}}.$$\nThe weights of the global minimum variance (GMV) portfolio are given by: $${\\bf w}_{\\mathrm{GMV}}=\\frac{{\\bf\\Sigma}{\\bf\\Sigma}^{-1}{\\bf1}}{{\\bf1}^{\\prime}{\\bf\\Sigma}^{-1}{\\bf1}}.$$\nThe Sharpe ratio (SR) optimal weights are given by: $${\\bf w}_{\\mathrm{SR}}=\\frac{\\Sigma^{-1}\\pmb{\\mu}}{\\mathbf{1}^{\\prime}\\Sigma^{-1}\\pmb{\\mu}}$$ provided that $1^{\\prime}\\Sigma^{-1}\\pmb{\\mu}\\neq0$.\nThe weights of the tangency portfolio (TP) are given by: $${\\bf w}_{\\mathrm{TP}}=\\alpha^{-1}{\\bf\\Sigma}^{-1}(\\pmb{\\mu}-r_{f}\\pmb{1}).$$\n\nThe paper discusses the distributional properties of optimal portfolio weights under the assumption of normally distributed returns. It covers four types of portfolio weights: expected quadratic utility (EU), global minimum variance (GMV), Sharpe ratio optimal (SR), and tangency portfolio (TP) weights."
  },
  {
    "qid": "econ-empirical-1607-5-0-3",
    "question": "4) Using Proposition 7, explain why severance payments $s_u^*$ are positive only for workers of unknown type, while $s_g^* = 0$ for revealed-good workers.",
    "gold_answer": "1. **Proposition 7**: If $\\rho_u^* = 0$, then $-s_u^* = p q_u - w_t^* + \\pi_{t+1}^* - k$.  \n2. **Unknown Workers**: Positive $s_u^*$ compensates for moving costs ($k$) and wage loss.  \n3. **Good Workers**: If $\\rho_g^* = 0$, $s_g^* = 0$ because:  \n   - No social concerns ($A(\\widetilde{w}) = 0$).  \n   - Efficiency requires no compensation (Lemma 1).",
    "question_context": "One of the most well-accepted empirical findings is that wage increases are serially correlated through time, especially if associated with promotions or job changes (see Baker et al., 1994; and the discussion in Gibbons and Waldman, 1999, p. 1325). In our model, gradual promotions are a multiperiod response of wages to type revelation that naturally generates a strong serial correlation of wage increases within firms.\nA second implication of our model, which is different from Harris and Hölmström (1982), is that we can have real wage cuts. The fact that in HH wages are downward rigid is considered to be one of the major drawbacks of that model (e.g. see Gibbons and Waldman, 1999, p. 1347), because it is inconsistent with the substantial frequency of negative real-wage changes found by Baker et al. (1994). Moreover, our model makes a novel (empirically testable) prediction on wage cuts. They should typically be associated with changes in skill composition within firms.\nThird, a key implication of our model is that wage changes need not be connected with productivity changes. Insurance motives would also disconnect wage movements from productivity movements, by dampening the impact of the latter in the former. However, in our model, the wage may increase even when worker expected productivity actually declines. This is because of the impact of firm workforce composition and market pressures on envy costs and their translation into optimal wage schedules.\nOur simulation results show that the extent of skill segregation implied by the model decreases with mobility costs. This finding would allow us to explain part of the recent rise in inter-firm wage variance by an increase in segregation by skill, if there is a parallel change in mobility costs.\n\nThe discussion section analyzes empirical findings related to wage dynamics, promotions, and worker mobility, contrasting them with the implications of the model presented in the paper."
  },
  {
    "qid": "econ-empirical-611-1-0-2",
    "question": "3) Explain how the conditional joint density-distribution $f^{*}$ incorporates AR dynamics and the role of the log-likelihood function $L(\\theta)$ in estimation.",
    "gold_answer": "1. The conditional density $f^{*}$ integrates AR dynamics of order $r$ by summing over all possible state sequences and their probabilities:\n   $$\n   f^{*}(y_{t} | y_{t-1}, \\ldots, y_{-r}, \\mathbf{z}_{t}) = \\sum_{S_{t}, \\ldots, S_{t-r}} f(y_{t}, S_{t}, \\ldots, S_{t-r} | y_{t-1}, \\ldots, y_{-r}, \\mathbf{z}_{t}).\n   $$\n2. The log-likelihood function $L(\\theta) = \\sum_{t=1}^{T} \\ln[f^{*}(y_{t} | y_{t-1}, \\ldots, y_{-r}, \\mathbf{z}_{t}; \\theta)]$ is maximized to estimate the parameters $\\theta$, capturing the model's fit to the data.",
    "question_context": "The TVTP model with state-dependent means, predetermined right-side variables, and normally distributed errors yields\n\n$$\n\\begin{array}{r}{y_{t}=\\mu^{0}+\\Phi(L)\\big(y_{t-1}-\\mu^{S_{t-1}}\\big)+e_{t}\\quad\\mathrm{if~state~}0}\\ {=\\mu^{1}+\\Phi(L)\\big(y_{t-1}-\\mu^{S_{t-1}}\\big)+e_{t}\\quad\\mathrm{if~state~}1,}\\end{array}\n$$\n\nwhere $\\Phi(L)=\\phi_{1}+\\phi_{2}L+\\cdot\\cdot\\cdot+\\phi_{r}L^{r-1}$ is the lag polynomial, $\\mu^{S_{t}}=\\mu_{0}+\\mu_{1}S_{t}$ is the state-dependent mean, $e_{t}\\sim$ $N(0,\\sigma^{2})$ , and $S_{t}\\in\\{0,1\\}$ .\nThe two-point stochastic process on $s_{t}$ can be summarized by the transition matrix\n\n$$\n\\begin{array}{r l}&{P(S_{t}=s_{t}\\mid S_{t-1}=s_{t-1},\\mathbf{z}_{t})}\\ &{\\qquad=\\Lambda=\\left[\\begin{array}{l l}{q(\\mathbf{z}_{t})}&{1-p(\\mathbf{z}_{t})}\\ {1-q(\\mathbf{z}_{t})}&{p(\\mathbf{z}_{t})}\\end{array}\\right],}\\end{array}\n$$\n\nwhere the history of the economic-indicator variables is $\\scriptstyle{\\mathbf{z}}_{t}=$ $\\{z_{t},z_{t-1},\\ldots\\}$.\nIn this TVTP model, the parameters in Equation (1) and the transition probability parameters in Equation (2) are jointly estimated. The conditional joint density-distribution, $f$ summarizes the information in the data and explicitly links the transition probabilities to the estimation method and tests. With AR dynamics of order $r,$ the conditional density, $f^{*}$ ,is\n\n$$\n\\begin{array}{r l}&{(y_{1})\\left(y_{1}\\eta_{-1},\\ldots,y_{-r},z_{t}\\right)}\\ &{=\\displaystyle\\sum_{y_{t}\\in\\mathcal{R}_{t-1}^{n}}\\sum_{\\ell=0}\\zeta\\left(y_{t}\\nabla_{x}\\xi_{u}=\\xi_{t},\\right.}\\ &{\\qquad\\left.\\xi_{u-1}=\\xi_{t-1},\\ldots,\\xi_{-r},z_{t}\\right)}\\ &{\\qquad\\xi_{-r-1}=\\xi_{t-1},\\ldots,\\xi_{-r},z_{t}\\right)}\\ &{=\\displaystyle\\sum_{y_{t}\\in\\mathcal{R}_{t-1}^{n}}\\sum_{\\ell=0}\\zeta\\left(y_{t}|S_{u}\\xi_{u}=\\xi_{t},\\ldots,S_{-r},z_{t}\\right)}\\ &{\\qquad\\xi_{u}\\xi_{u}\\xi_{u}\\xi_{u}\\xi_{u-1},\\ldots,y_{-r},}\\ &{\\qquad\\xi_{\\ell}\\xi_{u}\\xi_{u}\\xi_{u}\\xi_{u-1},\\ldots,y_{-r}\\xi_{-r}\\right)}\\ &{\\qquad\\times P(S_{\\ell}\\xi_{u-1}|S_{\\ell-1},\\ldots,z_{t-1},z_{t})}\\ &{\\qquad\\times P(S_{\\ell-1}=\\xi_{t-1},\\ldots,S_{-r},z_{t}\\ldots,\\xi_{-r})}\\ &{\\qquad\\quad=\\xi_{t-1}{\\left.\\left[y_{t-1},\\ldots,y_{-r-1},z_{t-1}\\right]\\right.}}\\end{array}\n$$\n\nand the log-likelihood function is $\\begin{array}{r}{L(\\theta)=\\sum_{t=1}^{T}\\ln[f^{*}(y_{t}|}\\end{array}$ $y_{t-1},\\ldots,y_{-r},\\mathbf{z}_{t};\\theta)]$.\nEquation (3) shows exactly how the information in output growth and economic indicators, $\\scriptstyle\\mathbf{z}_{t}$ , affects the model's estimation and inference. Both sources of information enter in two ways, one directly and the other indirectly through the inference of the past states. The information in $\\pmb{y_{t}}$ and its lags directly influences the likelihood through the normal density, $\\hat{\\pmb f}$ : the lags of ${\\bf y}_{t}$ indirectly affect the likelihood through the information they provide about the past states $P(S_{t-1}=$ $s_{t-1},\\ldots,S_{t-r}=s_{t-r}\\left|y_{t-1},\\ldots,y_{-r},\\mathbf z_{t-1}\\right)$ :The economicindicator variables affect the transition probabilities, $P(S_{t}=$ $s_{t}\\left|S_{t-1}=s_{t-1},\\mathbf{z}_{t}\\right)$ , directly and the distribution of the states, $P(S_{t-1}=s_{t-1},\\ldots,S_{t-r}=s_{t-r}\\:\\vert y_{t-1},\\ldots,y_{-r},\\mathbf z_{t-1})$ indirectly.\n\nThe TVTP Markov-switching model of aggregate output growth, $y_{t}$, allows for distinct business-cycle phases with state-dependent means and for cyclical dynamics of aggregate output with the lagged predetermined variables. The model assumes that the state of the economy cannot be known with certainty, in the sense that the econometrician can neither observe the state of the economy nor deduce the state indirectly. The states, however, are assumed to be path dependent and to evolve according to a first-order Markov process with TVTP coefficients."
  },
  {
    "qid": "econ-empirical-945-4-1-2",
    "question": "7) Compare the British (class-based) and post-colonial (identity-based) paths to LD. How do the differing cleavage structures affect the stability of property vs. civil rights protections?",
    "gold_answer": "1. **British path**: Industrialization made class cleavages ($\\beta$) salient, enabling elite-mass bargains that bundled property and civil rights. \\n2. **Post-colonial path**: Identity cleavages ($\\alpha$) dominated, leading to unstable LD as elites prioritized property rights ($\\beta$) over minority protections. \\n3. **Model implication**: When $\\alpha \\gg \\beta$, LD stability requires external enforcement (e.g., military, capital flight).",
    "question_context": "Our framework treats the identity cleavage as unchanging and given. This is, of course, quite different from the actual practice of politics, where there may exist a multiplicity of possibly cross-cutting cleavages—revolving around ethnicity, race, religion or nationality.\nIf that were the case, then that would change decision-making calculus for each of the three groups. For example, the majority may be more inclined to moderate its demands if it knows that there is the possibility that it may end up in the losing minority group in the future.\nOur focus in this paper has been on the constitutive bargains that lie at the origin of different political regimes, liberal democracy in particular. It goes without saying that such bargains can obsolesce over time.\n\nThis section critiques two key assumptions of the model: (1) exogenous and static identity cleavages, and (2) perfect commitment to political agreements. It discusses how relaxing these assumptions could alter group decision-making."
  },
  {
    "qid": "econ-empirical-359-0-0-2",
    "question": "3) Formally analyze the implications of technological change and international trade on the demand for highly-skilled workers, as mentioned in the paper. How do these factors interact with the expansion of higher education?",
    "gold_answer": "The interaction can be modeled via a labor demand framework:\n\n\\[ D_H = f(Tech, Trade, HE) \\]\n\nwhere:\n- \\(D_H\\) is demand for highly-skilled workers,\n- \\(Tech\\) represents technological progress,\n- \\(Trade\\) captures international trade exposure,\n- \\(HE\\) is the supply of HE graduates.\n\n**Mechanisms:**\n1. **Tech-driven demand**: \\(\\frac{\\partial D_H}{\\partial Tech} > 0\\) due to skill-biased technical change.\n2. **Trade effects**: \\(\\frac{\\partial D_H}{\\partial Trade} > 0\\) if comparative advantage favors skilled sectors.\n3. **HE expansion**: May dilute returns if \\(\\frac{\\partial^2 D_H}{\\partial HE \\partial Tech} < 0\\) due to oversupply.",
    "question_context": "The results showed that there were average 'raw' returns to an undergraduate degree of around $21\\%$ for men and $39\\%$ for women. Controlling for ability at age seven, region, school type, family background, demographic characteristics and various other features of the job reduced the estimated return to around $17\\%$ for men and $37\\%$ for women.\nWe also attempted to investigate various subsidiary issues concerning the impact of HE on wages and produced some interesting findings. In particular, we found that the gender earnings gap was lower between men and women at various levels of higher educational attainment than it was between men and women with just A levels.\nThe demand conditions for such workers have also altered fundamentally since this cohort entered work. Technological change and increased international trade have meant that the demand for highly-skilled workers has grown alongside the growing participation in HE.\nIn addition, we have only considered the private financial returns to higher education, and not whatever other private returns or social returns may exist. Our finding of large private returns should imply that individuals will be able to contribute towards the cost of their education and still secure a net benefit (at least on average); however, without knowing the size of any social returns to HE it is impossible to say what the overall efficient mix of private and public funding should be.\n\nThis paper examines the effect of higher education on wages by comparing British men and women born in March 1958 who undertook higher education prior to 1991 with a group who obtained A levels but did not proceed into higher education. The study uses a 'proxy' or 'matching' approach to control for biases induced by unobserved individual characteristics, incorporating various individual and family background variables from the NCDS."
  },
  {
    "qid": "econ-empirical-1035-2-2-0",
    "question": "5) Derive the dose-response function for reform intensity, showing how the treatment effect varies with years of exposure to full pay reform.",
    "gold_answer": "1. Let \\( D_s \\) indicate years of full reform exposure (\\( s \\in \\{0,1,2,3\\} \\)).\n2. The dose-response model is:\n\\[ Y_{ik} = \\sum_{s=0}^3 \\tau_s D_s + \\gamma X_i + \\alpha_k + \\epsilon_{ik} \\]\n3. Table IV estimates:\n   - \\( \\tau_0 \\) ≈ 0 (no effect for partial reform)\n   - \\( \\tau_3 \\) = 0.082 (matriculation), 0.100 (university qualification)\n4. The convex response suggests cumulative exposure matters.",
    "question_context": "We define four treatment groups, ranging from three years of full reform to zero years of full reform (three years of partial reform)... The effect of being under a full differential pay system for two or three years has large and significant effect on all four outcomes... three years in high school under a full differential pay system causes an 8.2 percentage point increase in the matriculation rate and a 10.0 percentage point increase in the university qualified matriculation rate.\n\nThis section exploits variation in reform intensity (full vs. partial pay reform) and duration of exposure to estimate heterogeneous treatment effects."
  },
  {
    "qid": "econ-empirical-68-4-0-0",
    "question": "1) Derive the equilibrium conditions for the economy with trembles in private actions, assuming $\\varepsilon_{t}(j)$ has both aggregate and idiosyncratic components. Show how the central bank's ability to observe the aggregate component affects the equilibrium.",
    "gold_answer": "1. Start with the price-setting equation: $x_{t}(j)=\\tilde{x}_{t}(j)+\\varepsilon_{t}(j)$. \\n2. Decompose the error term: $\\varepsilon_{t}(j)=\\varepsilon_{t}^{a}+\\varepsilon_{t}^{i}(j)$, where $\\varepsilon_{t}^{a}$ is the aggregate component and $\\varepsilon_{t}^{i}(j)$ is the idiosyncratic component. \\n3. Aggregate prices: $x_{t}=\\int x_{t}(j) dj = \\tilde{x}_{t}+\\varepsilon_{t}^{a}$. \\n4. If the central bank observes $\\varepsilon_{t}^{a}$, it can adjust policies to offset its effect, leaving the equilibrium unchanged.",
    "question_context": "The actual price chosen by a price-setter, $x_{t}(j)$, differs from the intended price, $\\tilde{x}_{t}(j)$, by an additive error $\\varepsilon_{t}(j)$, so that $x_{t}(j)=\\tilde{x}_{t}(j)+\\varepsilon_{t}(j)$.\nThe central bank observes the price-setters’ choices with error, so that $\\hat{x}_{t}=x_{t}+\\varepsilon_{t}$, where the error $\\varepsilon_{t}$ is i.i.d. over time with mean zero and bounded support $[\\underline{{\\varepsilon}},\\bar{\\varepsilon}]$.\nThe King rule is of the form $i_{t}(h_{g t})=i_{t}^{*}+\\phi(1-\\alpha)(\\hat{x}_{t}-x_{t}^{*})$ with $\\phi>1$.\nThe realized value of the interest rate is given by $i_{t}=i_{t}^{*}+\\phi(1-\\alpha)\\varepsilon_{t}$, whereas the realized value of output is given by $y_{t}=y_{t}^{*}-\\psi\\phi(1-\\alpha)\\varepsilon_{t}$.\n\nThe text discusses the robustness of sophisticated policies to trembles in private actions and imperfect information. It shows that unique implementation can be achieved even with trembles and that the King–money hybrid rule ensures a unique equilibrium under imperfect information."
  },
  {
    "qid": "econ-empirical-950-2-0-0",
    "question": "1) Derive the expression for $A_{n}$ as given in Theorem 3.1, starting from the definition of $f_{n}(r,s)$ and the expected number of undirected pairs.",
    "gold_answer": "1. Start with the definition of $f_{n}(r,s) = \\int_{0}^{T} h K_{h,t}(s) K_{h,t}(r) w(t) dt$.\n2. Note that $A_{n}$ involves $f_{n}(r,r)$, which simplifies to $\\int_{0}^{T} h K_{h,t}(r)^2 w(t) dt$.\n3. The term $\\frac{1}{\\pi(r) \\mu_{n}(r; \\beta_0)}$ accounts for the sparsity and the conditional expectation.\n4. Integrate over $d N_{n,ij}(r)$, which counts the observed pairs, and sum over all pairs $(i,j) \\in V_n$.\n5. Normalize by $N \\gamma^2$ to account for the weighted time average and the kernel smoothing effect.",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nAssumption (SP): (Sparsity) We assume $p_{n}>0,\\pi(t)\\geq1_{\\{\\$ , and $\\pi$ continuous with $0\\leq{\\mathit{p}}_{n}\\pi(t)\\leq1$ for all $n\\geq1$ and $t\\in[0,T]$.\nTheorem 3.1. Suppose that Assumption (SP) and all the assumptions from Section 3.2 hold. Further, assume that model (1) holds with $\\alpha_{0}(t)=\\alpha(\\theta_{0},t)+c_{n}\\Delta_{n}(t)$ , where $c_{n}=\\left(N\\sqrt{h}\\right)^{-1/2}$ , and $\\Delta_{n}$ is uniformly bounded and continuously differentiable with uniformly bounded derivative (that is, uniformly in both $t$ and $n$ ). Then, as $n\\to\\infty$ , $$\\begin{array}{r l}&{\\frac{\\displaystyle N\\sqrt{h}\\left(T_{n}-\\frac{A_{n}}{N h}-\\int_{0}^{T}{\\left(\\int_{0}^{T}{K_{h,t}(s)c_{n}\\Delta_{n}(s)d s}\\right)^{2}w(t)d t}\\right)}{\\sqrt{B_{n}}}}\\ &{\\to\\:\\mathcal{N}(0,1),}\\end{array}$$ where with $\\begin{array}{r}{f_{n}(r,s):=\\int_{0}^{T}h K_{h,t}(s)K_{h,t}(r)w(t)d t,\\gamma=\\int_{0}^{T}\\int_{0}^{T}}\\end{array}$ $\\begin{array}{r}{K_{h,t}(s)\\frac{w(t)}{\\pi(s)}d s d t.}\\end{array}$ , and $\\begin{array}{r}{K^{(2)}:=\\frac{1}{2}\\|K\\star K\\|_{2}^{2}}\\end{array}$ , $$\\begin{array}{l}{{A_{n}:=\\displaystyle\\frac{1}{N\\gamma^{2}}\\sum_{i,j\\in V_{n}}\\int_{0}^{T}f_{n}(r,r)\\left(\\displaystyle\\frac{1}{\\pi(r)\\mu_{n}(r;\\beta_{0})}\\right)^{2}d N_{n,i j}(r),}}\\ {{B_{n}:=4K^{(2)}\\displaystyle\\int_{0}^{T}\\left(\\displaystyle\\frac{w(s)\\alpha_{0}(s)}{\\gamma\\pi(s)\\mu_{n}(s;\\beta_{0})}\\right)^{2}d s.}}\\end{array}$$\nRemark 3.2. We assume in Assumption (B) below that $r\\mapsto$ $\\mu_{n}(r,\\beta_{0})$ is uniformly bounded from below. Furthermore $\\gamma\\rightarrow$ $\\begin{array}{r}{\\int_{0}^{T}w(t)/\\pi(t)d t>0\\mathrm{if}\\pi}\\end{array}$ is continuous. Therefore, $B_{n}$ is bounded from below and does not influence the rate of convergence.\nRemark 3.3. Note that Theorem 3.1 is formulated for local alternatives, including the null hypothesis for $\\textstyle\\Delta_{n}\\equiv0$ . This allows us to use the stated asymptotic normality to formulate confidence intervals for the $L^{\\dot{2}}$ -norm of the smoothed and weighted $\\Delta_{n}$ as follows: Let $q_{1-\\alpha}$ denote the $(1-\\alpha)$ -quantile of a standard normal distribution. Then, asymptotically with probability at least $1-\\alpha$ , $$\\Vert K_{h}\\star(c_{n}\\Delta_{n})\\Vert_{w}\\geq D_{n}:=\\sqrt{\\operatorname*{max}\\left(T_{n}-\\frac{B_{n}^{\\frac{1}{2}}q_{1-\\alpha}}{N\\sqrt{h}}-\\frac{A_{n}}{N h},0\\right)},$$ where $\\begin{array}{r}{\\|K_{h}\\star(c_{n}\\Delta_{n})\\|_{w}^{2}:=\\int_{0}^{T}\\left(\\int_{0}^{T}K_{h,t}(s)c_{n}\\Delta_{n}(s)d s\\right)^{2}w(t)d t.}\\end{array}$ In other words, we can also test the hypotheses $H_{\\varepsilon}$ : $\\|K_{h}~\\star$ $(c_{n}\\Delta_{n})\\|_{w}\\leq\\varepsilon$ for different values of $\\varepsilon$ (reject $H_{\\varepsilon}$ if $D_{n}>\\varepsilon$ ). The right-hand side, that is, $D_{n}$ , in the above displayed formula is the largest value of $\\varepsilon$ for which $H_{\\varepsilon}$ would be rejected.\n\nThis section presents the main result of the paper, introducing notation and assumptions necessary for the theorem. It discusses the sparsity assumption, the expected number of undirected pairs, and the interpretation of N as a weighted time average. The theorem provides asymptotic normality for a specific statistic under local alternatives."
  },
  {
    "qid": "econ-empirical-856-2-0-2",
    "question": "3) Explain the economic significance of the disturbance term $u_{t+\\eta}$ in (2.35) and its properties under the model assumptions.",
    "gold_answer": "1. Disturbance term: $$u_{t+\\eta} = \\gamma (\\log C_{t+\\eta} - \\log C_t) + r_{t,t+\\eta}^\\omega - \\eta \\delta$$  \n2. **Properties**:  \n   - Satisfies $E[u_{t+\\eta} | I_t] = 0$ (moment restriction).  \n   - May exhibit conditional heteroskedasticity.  \n   - Stationary if $\\log U_t$ is stationary.  \n3. Only degenerate when γ = -1 (log utility).",
    "question_context": "We assume that a representative agent has logarithmic risk preferences and examine the following special case of the recursive utility function studied by Epstein and Zin: $$\\begin{array}{r l r}{\\mathrm{\\boldmath~v~}_{\\mathrm{t}}-\\left[(1-\\lambda)(\\mathsf{C}_{\\mathrm{t}})^{\\gamma+1}\\right.}&{+}&{\\left.\\lambda e x p[(\\gamma+1)E(1\\mathsf{o g U}_{\\mathsf{t}+\\eta}|I_{\\mathrm{\\widetilde{c}}})]\\right]^{1/(\\gamma+1)},}\\end{array}$$\nThe parameter Y governs intertemporal substitution of consumption, with the elasticity of substitution being -1/y. In the special case in which  = -l, preferences are state separable with a logarithmic period utility function.\nThe marginal rate of substitution of consumption between dates t and t+n is $$\\begin{array}{r l r}{\\Im\\Re S_{\\mathfrak{t},\\mathfrak{t}+\\eta}}&{{}=}&{\\lambda(\\mathsf{C}_{\\mathfrak{t}+\\eta}/\\mathsf{C}_{\\mathfrak{t}})^{\\gamma}e x p[(\\gamma+1)E(\\log\\mathfrak{t}_{\\mathfrak{t}+\\eta}|I_{\\mathfrak{t}})]/(\\mathfrak{U}_{\\mathfrak{t}+\\eta})^{\\gamma+1},}\\end{array}$$\nThe logarithm of the return on the wealth portfolio over the time interval t to $\\mathbf{\\hat{c}}+\\mathbf{\\hat{\\eta}}$ is $$\\begin{array}{r l r}{\\mathrm{\\bfv}_{\\mathrm{t,t+}\\eta}^{\\omega}}&{=}&{\\log[(\\omega_{\\mathrm{t+}\\eta}\\mathrm{\\boldmath~\\Omega~}+\\mathrm{\\boldmath~\\mathcal~{~c~}}_{\\mathrm{t+}\\eta})/\\omega_{\\mathrm{t}}].}\\end{array}$$\nThe standard Euler equation, $E\\{\\mathbb{M}\\mathbb{R}S_{\\mathrm{t},\\mathrm{t}+\\eta}\\mathrm{exp}(\\mathbf{v}_{\\mathrm{t},\\mathrm{t}+\\eta}^{\\omega})|\\boldsymbol{\\tau}_{\\mathrm{t}}\\}=1$, implies: $$\\mathbb{E}[\\gamma(\\dot{\\textbf{c}}_{\\mathfrak{t}+\\eta}\\cdot\\textbf{c}_{\\mathfrak{t}})+\\textbf{v}_{\\mathfrak{t},\\mathfrak{t}+\\eta}^{\\omega}|\\boldsymbol{\\tau}_{\\mathfrak{t}}]-\\eta\\delta-0.$$\n\nThis section derives a temporally aggregated log-linear ICAPM under non-expected utility preferences, focusing on the recursive utility function proposed by Epstein and Zin (1989b). The model assumes a representative agent with logarithmic risk preferences and examines the implications for intertemporal substitution and wealth dynamics."
  },
  {
    "qid": "econ-empirical-1803-2-0-0",
    "question": "1) Derive the condition under which it is socially optimal for the entrepreneur to invest, i.e., $\\tilde{n}\\geq n^{*}\\equiv\\frac{I}{1-c}$. Explain the intuition behind this condition.",
    "gold_answer": "1. **Socially Optimal Investment Condition**: The entrepreneur should invest if the revenue covers the total costs, i.e., $\\tilde{n} \\geq I + \\tilde{n}c$. Rearranging, we get $\\tilde{n}(1 - c) \\geq I$, or $\\tilde{n} \\geq \\frac{I}{1 - c} \\equiv n^{*}$. \\\\\n2. **Intuition**: The condition ensures that the revenue from selling to $\\tilde{n}$ consumers at a price of $1$ (since $\\nu_i \\in \\{0,1\\}$) covers both the fixed investment cost $I$ and the variable production costs $\\tilde{n}c$.",
    "question_context": "The entrepreneur needs an upfront investment of $I>0$ to develop her product. After developing it, the entrepreneur can produce the good at some marginal cost $c\\in[0,1)$. The entrepreneur is crucial for realizing the project and cannot sell her idea to outsiders.\nA consumer $i$ either values the good, $\\nu_{i}=1$, or not, $\\nu_{i}=0$. The $n$-dimensional vector $\\nu=(\\nu_{1},...,\\nu_{n})\\in\\mathcal{V}\\equiv\\{0,1\\}^{n}$ represents the valuation profile of the consumers. The probability that $\\tilde{n}$ consumers value the product is given by $\\operatorname{Pr}\\{\\tilde{n}\\}\\equiv\\sum_{\\substack{\\left\\{\\nu:\\sum_{i\\in N}\\nu_{i}=\\tilde{n}\\right\\}}}\\pi(\\nu)$.\nThe entrepreneur invests if the project's revenue, $\\tilde{n}$, covers the costs of production $I+{\\tilde{n}}c$, i.e., if $\\tilde{n}\\geq n^{*}\\equiv\\frac{I}{1-c}$. The ex ante expected aggregate surplus is $S^{*}=\\sum_{\\tilde{n}=\\lceil n^{*}\\rceil}^{n}\\operatorname*{Pr}\\left\\{\\tilde{n}\\right\\}[(1-c)\\tilde{n}-I]$.\nWith demand uncertainty, expected profits from investing are $\\bar{\\Pi}=\\left(\\sum_{\\tilde{n}=0}^{n}\\operatorname*{Pr}\\{\\tilde{n}\\}(1-c)\\tilde{n}\\right)-I$. The entrepreneur invests only if $\\overline{{\\Pi}}\\geq0$.\nAn all-or-nothing crowdfunding scheme $(p,T)$ yields the entrepreneur the expected profit $\\Pi^{c}(p,T)=\\sum_{\\tilde{n}=\\lceil T/p\\rceil}^{n}\\operatorname*{Pr}\\left\\{{\\tilde{n}}\\right\\}[(p-c){\\tilde{n}}-I]$.\nMoral hazard is introduced by assuming the entrepreneur can keep a share $\\alpha\\in[0,1]$ of the pledges without delivering the product. The entrepreneur runs if $\\alpha P>P-I-c P/p$.\nDeferred payments mitigate moral hazard by transferring only the required amount $I$ initially. The entrepreneur has no incentive to run if $P\\geq\\overline{{P}}\\equiv\\frac{(1+\\alpha)I}{1-c}=(1+\\alpha)n^{*}$.\nReducing the entrepreneur's information about demand can improve efficiency. A partially informative crowdfunding scheme deals with moral hazard more effectively by weakening the condition $E[P|P\\ge T]\\ge\\bar{P}$.\n\nThis section introduces a framework for analyzing crowdfunding, focusing on demand uncertainty, entrepreneurial moral hazard, and private cost information. The model considers a penniless entrepreneur who interacts with consumers to determine product valuation before making an investment decision."
  },
  {
    "qid": "econ-empirical-77-4-1-2",
    "question": "3) Analyze the robustness check that assesses the incidence of tariffs at different time horizons (two-month, three-month, and four-month levels). Why is this check important, and what do the results imply about the persistence of the pass-through effects?",
    "gold_answer": "1. The check is important because short-run pass-through may differ from medium- or long-run effects due to adjustment frictions.\n2. Aggregating the data to longer horizons smooths out high-frequency noise and captures more persistent effects.\n3. The results show no downward pressure on before-duty unit values even at these longer horizons.\n4. This implies that the complete pass-through is not a transient phenomenon but persists over time.\n5. The findings reinforce the conclusion that U.S. exporters do not adjust their before-duty prices in response to tariffs.",
    "question_context": "We first assess concerns that underlying trends or tariff anticipation bias the estimates. We also explore heterogeneity across sectors, compare the pass-through of tariffs to unit values at different time horizons, and examine how the results change with alternative sets of fixed effects.\nThe first robustness check controls for trends through panel fixed effects. We reestimate the variety-level specifications to include variety fixed effects and report the analog to Table IV in Online Appendix Table A.4 and the analog to Table VII in Online Appendix Table A.6. We assess long-run trends by reestimating the specifications with variety fixed effects using data from 2013:1 to 2019:4 in Online Appendix Tables A.5 and A.7. The results are essentially unchanged and remain consistent with the prior evidence that preexisting trends are unlikely to be confounding factors.\nThe second concern is that importers may have anticipated the changes in tariffs and shifted their purchasing decisions forward to avoid the duties. This would imply that even though tariffs have real effects on trade, regressions identified from contemporaneous changes in tariffs may produce biased elasticities. We check for anticipatory and delayed effects by allowing for leads and lags in variety-level reduced-form regressions:\n$$\\Delta\\ln y_{i g t}=\\alpha_{g t}+\\alpha_{i t}+\\alpha_{i s}+\\sum_{m=-6}^{m=6}\\beta_{m}^{y}\\left[\\ln\\left(1+\\tau_{i g,t-m}\\right)-\\ln\\left(1+\\tau_{i g,t-1-m}\\right)\\right]+\\epsilon_{i g t},$$\nThe bottom panel of Figure IV reports the results for exported varieties. In this specification, we find some evidence of tariff anticipation as U.S. export values increase in the month before tariffs change; however, we do not observe this pattern for export quantities. We also do not observe cumulative declines in before-duty export prices occurring as time elapses after the retaliations are implemented.\nThe baseline specifications impose common elasticities across sectors. However, these specifications potentially mask variation in the tariff pass-through result. For example, we may expect more differentiated products or less competitive sectors to exhibit less than complete pass-through. We may also expect demand elasticities to depend on inventories and whether goods are durable, which may allow buyers to postpone sales more easily. Other product characteristics, such as variation in price stickiness or stocks of inventories, could induce heterogeneous pass-through.\nOnline Appendix Table A.9 reports results from interacting the tariff changes with three different classifications of final versus intermediate goods. The top panel examines the pass-through of the import tariffs to import unit values, and the bottom panel examines pass-through of the retaliatory tariffs to U.S. export unit values. Column (1) uses the Broad Economic Categories (BEC) classification that categorizes sectors according to their end use. Column (2) uses an indicator for whether the HS product matches an entry line item in the BLS Consumer Price Index (CPI). Column (3) uses an indicator for whether each HS-10 product description contains the word “part” or “component.” Although each classification is imperfect, the results do not show statistical differences between final and intermediate goods.\nOnline Appendix Table A.10 examines interactions across 11 different measures of product or sector characteristics that have been used in the literature: (i) quality ladders from Khandelwal (2010); (ii) markups estimated by De Loecker, Eeckhout, and Unger (forthcoming); (iii) the coefficient of price variation; (iv) elasticities of substitution estimated by Broda and Weinstein (2006); (v) trade elasticities estimated by Caliendo and Parro (2015); (vi) contract intensity measured by Nunn (2007); (vii) inverse frequency of price adjustments from micro-data tabulated by Nakamura and Steinsson (2008) (a higher value indicates less frequent price adjustments); (viii) measures of industry upstreamness by Antras et al. (2012); (ix) inventory to sales ratios constructed from Census data; (x) differentiation developed by Rauch (1999); and (xi) an indicator for durable goods using the BEC classifications. The top panel reports the results for imports. The pattern across the 11 different metrics, along with the previous table, suggests that there is no meaningful heterogeneity in the complete pass-through result, at least with respect to the characteristics we have examined. Online Appendix Table A.11 reports the results for export unit values. Again we find no systematic evidence of heterogeneity with respect to observable characteristics.\nThe variety-level complete pass-through results may be a short-run phenomenon. We assess the incidence of the tariffs at different time horizons by aggregating the data to the two-month, three-month, and four-month levels, taking first differences, and reimplementing the reduced-form regression specifications. The results for before-duty import and export unit values are reported in Online Appendix Table A.12, with the baseline estimates from the monthly data replicated in column (1) of each panel. Even at these medium-term frequencies, we do not observe downward pressure on before-duty unit values in response to the tariff changes.\nOur baseline set of fixed effects—that is, product-time, country-time, and country-sector fixed effects—control for potentially confounding import demand and export supply shocks. However, if the trade war induces global or country-specific general-equilibrium responses to wages, these fixed effects may mask tariff pass-through effects in our regressions. Online Appendix Table A.13 reports the elasticity of before-duty unit values against the tariff changes controlling for different sets of fixed effects to explore this possibility. The top panel reports the import results and the bottom panel reports the export results, and column (1) of each panel reports the baseline estimates to facilitate comparisons across the alternate specifications. We do not observe any effect of the tariff on before-duty unit values across eight different sets of fixed effects, some of which exclude country-time or product-time fixed effects, in both the import and export data. We also extract the country-time $\\alpha_{i t}$ fixed effects in the baseline specification and regress them against monthly exchange rates, and do not find a statistically significant relationship (estimate is 0.11, std. err. $=0.19$ ). We also do not find a relationship between the country-time fixed effects and exchange rate changes for China (estimate is 0.04, std. err. $=0.04,$ ).\n\nThis section explores the robustness of the results by assessing concerns about underlying trends, tariff anticipation, heterogeneity across sectors, and alternative fixed effects."
  },
  {
    "qid": "econ-empirical-314-2-1-0",
    "question": "3) Prove Theorem 3 by showing that under $H_{1}$, the test statistic $Q$ diverges to $+\\infty$ as $T \\rightarrow \\infty$, ensuring $P(Q < \\alpha) \\rightarrow 1$.",
    "gold_answer": "1. Under $H_{1}$, $\\sqrt{T}\\widehat{\\mu}_{j} \\rightarrow -\\infty$ for some $j$ with $\\mu_{j} < 0$. \\n2. The smoothed indicator $\\psi(\\sqrt{T}\\widehat{\\mu}_{j}/\\widehat{\\theta}_{j})$ tends to 1, making the test statistic $Q$ large. \\n3. Thus, $Q$ exceeds any finite critical value $\\alpha$ with probability tending to 1.",
    "question_context": "Theorem 3 (Consistency): Given [A1]–[A6] with [D1]–[D4], under $H_{1}:\\mu_{j}<0$ for some $j\\in\\left\\{1,2,\\ldots,p\\right\\}$, $$ P(Q<\\alpha)\\longrightarrow1\\quad a s T\\longrightarrow\\infty. $$\nTheorem 4 (Local Power): Assume [A1], [A2], [A3], [A4], [A6] and [D1], [D3], [D4] hold with $\\mu_{j} = \\gamma_{j} + \\frac{c_{j}}{\\sqrt{T}}$. Define $\\tau\\equiv\\sum_{j=1}^{p}1\\{\\gamma_{j}=0\\}\\theta_{j}c_{j}$ and $\\kappa\\equiv\\sum_{i=1}^{p}\\sum_{j=1}^{p}1\\{\\gamma_{i}=0\\}1\\{\\gamma_{j}=0\\}\\theta_{i}\\theta_{j}v_{i j}$. Then, as $T\\longrightarrow\\infty$, $$ P(Q<\\alpha)\\longrightarrow\\varPhi(z_{\\alpha}-\\kappa^{-1/2}\\tau). $$\n\nThis section studies the asymptotic power properties of the test, including consistency against fixed alternatives and local power analysis."
  },
  {
    "qid": "econ-empirical-1559-5-0-1",
    "question": "2) Formally model the 'divorce prevention' mechanism using the educational attainment heterogeneity results from Table 8. Why does the coefficient for Reformk=2 (high) differ from Reformk=2 (low)?",
    "gold_answer": "1. Let $IPV_{it} = \\alpha + \\beta_1 Reform_t \\times HighEd_i + \\beta_2 Reform_t \\times LowEd_i + \\Gamma X_{it} + \\epsilon_{it}$. \\n2. Table 8 shows $\\beta_1^{high} = 0.033^{**}$ vs $\\beta_1^{low} = 0.009$ (insignificant). \\n3. Difference arises because: \\n   - High-education women have better outside options ($\\frac{\\partial IPV}{\\partial OutsideOptions} > 0$) \\n   - Low-education women face lower divorce threats ($\\frac{\\partial IPV}{\\partial Education} < 0$).",
    "question_context": "The results in Table 7 show that the overall effect of unilateral divorce on IPV is driven by women who remain married after the reform, which is presumably related to the fact that they represent 89% of the sample.\nIntact marriages are those that ‘survive’ the divorce reform. Thus, they were likely of better quality at the time of the reform than those that dissolve.\nThe reform could have caused or exacerbated any of these changes, but they might not have necessarily occurred immediately following easier divorce. This could explain the insignificant impact in the short run while the significant increase in the long run.\nA proxy for capturing outside options is a woman’s educational attainment. Thus, in order to further examine the proposed channel, I next analyse whether the effect of easier divorce on IPV varies by a woman’s educational attainment.\n\nThe section examines the mechanisms through which unilateral divorce laws affect intimate partner violence (IPV), contrasting short-term and long-term effects and exploring the role of marital status changes and educational attainment."
  },
  {
    "qid": "econ-empirical-207-1-0-1",
    "question": "2) Show how the reservation wage equation simplifies to $w^{r}=y+\\frac{\\beta p}{(1-\\beta)\\gamma}e^{-\\gamma w^{r}}$ under the exponential wage offer distribution $f(w)=\\gamma e^{-\\gamma w}$.",
    "gold_answer": "1. The expected value $E V$ is given by $E V = p \\int_{w^{r}}^{\\infty} V_{e}(w) f(w) dw + (1 - p \\Pi) V_{u}$. \\n2. Substitute $V_{e}(w) = \\frac{w}{1-\\beta}$ and $f(w) = \\gamma e^{-\\gamma w}$: $E V = \\frac{p \\gamma}{1-\\beta} \\int_{w^{r}}^{\\infty} w e^{-\\gamma w} dw + (1 - p \\Pi) V_{u}$. \\n3. Solve the integral: $\\int_{w^{r}}^{\\infty} w e^{-\\gamma w} dw = \\frac{(1 + \\gamma w^{r}) e^{-\\gamma w^{r}}}{\\gamma^2}$. \\n4. Substitute back into the reservation wage equation and simplify.",
    "question_context": "The worker maximizes the expected present value of income. Define the conditional value functions $V_{e}(w)$ and $V_{u}$ giving the values when employed and when unemployed (and searching). Under our assumptions, $V_{e}(w)=w/(1-\\beta)$.\nThe value function is $V(w)=\\operatorname*{max}\\{V_{e}(w),V_{u}\\}$. The value of remaining unemployed is $V_{u}= y+\\beta E V$, where $E$ is expectation over the offer arrivals (probability $p$) and offered wages [distribution $f(w)]$.\nThe implicit equation defining the reservation wage is $w^{r}=(1-\\beta)(y+\\beta E V)$. For the exponential wage offer distribution $f(w)=\\gamma e^{-\\gamma w}$, the equation becomes $w^{r}=y+\\frac{\\beta p}{(1-\\beta)\\gamma}e^{-\\gamma w^{r}}$.\n\nThe prototypal job-search model considers the optimal behavior of an unemployed worker actively seeking work. The worker is assumed to know the distribution of wage offers and maximizes the expected present value of income. The model includes key parameters such as the reservation wage, discount factor, and wage offer distribution."
  },
  {
    "qid": "econ-empirical-1187-2-0-0",
    "question": "1) Using the data from Table 1, derive the percentage point difference in the probability of not seeking care due to cost between women with less than a high school degree and women with college degrees. What does this imply about the impact of education on healthcare access?",
    "gold_answer": "1. From Table 1, women with less than a high school degree report a 15.2% probability of not seeking care due to cost, while women with college degrees report a 4.6% probability.\n2. The percentage point difference is calculated as \\(15.2\\% - 4.6\\% = 10.6\\%\\).\n3. This implies that lower education levels are associated with significantly higher barriers to healthcare access, likely due to factors such as lower income and higher uninsurance rates.",
    "question_context": "Table 1 reports sample statistics from the BRFSS on the receipt of mammography, physician breast exams, and two general measures of access to healthcare—whether a woman reported that she needed to see a doctor sometime in the past year and could not due to cost; and whether she has had a physician checkup in the past two years.\nAs expected, Medicare eligibility at age 65 leads to an abrupt decline in the probability of uninsurance, particularly among women with less than a high school education.\nTable 2 shows estimates of the effect of turning 65 on the use of health services using simple linear probability models controlling for education, race, region, and year effects.\nModels are then estimated interacting the dichotomous age 65 and over variable with education dummies. Results for the sum of these interactions and the age 65 dummy, reported in Table 2, show that the increase in the probability that a woman has had a recent mammogram or physician breast exam at age 65 varies strongly by education.\n\nThis section examines the disparities in the use of health services among women aged 50 to 80, focusing on the impact of Medicare eligibility at age 65. The analysis includes measures such as mammography, physician breast exams, and general access to healthcare, with a particular emphasis on differences by education level."
  },
  {
    "qid": "econ-empirical-19-1-1-3",
    "question": "4) Model the hospital's decision to opt into CJR during the voluntary period, assuming the hospital's expected profit under bundled payments is \\( \\pi_b \\) and under FFS is \\( \\pi_{\\text{FFS}} \\).",
    "gold_answer": "The hospital will opt in if: \n\\[ \\pi_b > \\pi_{\\text{FFS}} \\]\nwhere \\( \\pi_b = E[b_h - y_h] - C(q) \\) and \\( \\pi_{\\text{FFS}} = E[\\text{FFS payments}] - C(q) \\). Here, \\( C(q) \\) represents the cost of maintaining quality \\( q \\).",
    "question_context": "Under CJR, an episode begins with a hospital stay in a qualifying diagnosis-related group (DRG) and ends 90 days after hospital discharge. Medicare pays the hospital a predetermined bundle price for the episode.\nThe level and targeting of the bundle price are key design elements in a bundled payment program. Let \\( b_h \\) denote the average per episode bundle price at hospital \\( h \\) in a given year, and let \\( y_h \\) denote average per episode claims submitted that year.\nUnder CJR, Medicare tried to set the bundle price before each program year to be slightly lower than expected per episode claims under FFS. The bundle price included a small discount off a weighted average of historical hospital and regional per episode claims from three prior reference years.\n\nWe focus on the Medicare bundled payment program for hip and knee replacement, known as CJR. Hip and knee replacement is a large Medicare category; in 2014, Medicare covered almost half a million LEJR procedures, accounting for about 5% of Medicare admissions and inpatient spending."
  },
  {
    "qid": "econ-empirical-950-3-0-2",
    "question": "3) Prove that under Assumption (C), the model is Lipschitz continuous in parameters $\\beta$ and $\\theta$, using the given bounds on $L_{\\Psi}$ and $L_{\\alpha}$.",
    "gold_answer": "1. For $\\Psi$, $|\\Psi(x,\\beta_1) - \\Psi(x,\\beta_2)| \\leq L_{\\Psi} \\|\\beta_1 - \\beta_2\\|$ shows Lipschitz continuity in $\\beta$. \\n2. For $\\alpha$, $|\\alpha(\\theta_1,t) - \\alpha(\\theta_2,t)| \\leq L_{\\alpha}(t) \\|\\theta_1 - \\theta_2\\|$ shows Lipschitz continuity in $\\theta$. \\n3. Since $\\|L_{\\alpha}\\|_2 < \\infty$, the Lipschitz condition is integrable over $t$. \\n4. Continuity of $\\mu_n(\\cdot; \\beta_0)$ ensures the model does not change rapidly over time.",
    "question_context": "Assumption (VX): (Vertex Exchangeability) The tupels $(C_{n,i j},X_{n,i j},N_{n,i j})$ are vertex exchangeable, that is, their joint distribution does not change when the vertex labels are permuted.\nAssumption (KBW): (Kernel, Bandwidth, Weight) The kernel $K$ is bounded, symmetric about 0 and supported on $[-1,1]$ . The weight function $w$ is continuous and bounded with $\\mathbb{T}:=\\operatorname{supp}w\\subset(0,T)$ . The bandwidth $h$ fulfills $\\frac{\\log m}{h N}\rightarrow0.$ , ${\\sqrt{h}}\\log m\to0$ .\nAssumption (C): (Continuity and Boundedness of the Model) The link functions $\\Psi$ and $\\alpha$ are bounded, that is $\\|\\Psi(\\cdot;\beta_{0})\\|_{\\infty}<$ $\\infty$ and $\\|\\alpha(\theta_{0};\\cdot)\\|_{\\infty}<\\infty$ , and fulfill the following Lipschitz property $|\\Psi(x,\beta_{1})-\\Psi(x;\beta_{2})|\\le L_{\\Psi}\\|\beta_{1}-\beta_{2}\\|\\mathrm{~an}$ $|\\alpha(\theta_{1},t)-\\alpha(\theta_{2},t)|\\le L_{\\alpha}(t)\\|\theta_{1}-\theta_{2}\\|.$\nAssumption $(P)$ (Parametric Estimation) The estimators $\\hat{\theta}_{n}$ and ${\\widetilde{\beta}}_{n}$ are based on data independent of $(C_{n,i j},X_{n,i j},N_{n,i j})_{(i,j)\\in V_{n}\times V_{n}}$ and satisfy $\\|\\hat{\theta}_{n}-\theta_{0}\\|=\\hat{O}_{P}(N^{-1/2})$ , $\\|\\widetilde{\beta}_{n}-\beta_{0}\\|=O_{P}(N^{-1/2})$ and $\\mathbb{E}(\\|\\widetilde{\beta}_{n}-\beta_{0}\\|^{2})=O(N^{-1})$.\n\nThis section outlines the assumptions used to prove the main results, focusing on vertex exchangeability, kernel and bandwidth conditions, continuity and boundedness of the model, parametric estimation, local boundedness, weak correlation, momentary-m-dependence, and bounded moments."
  },
  {
    "qid": "econ-empirical-1505-2-0-1",
    "question": "2) Show that the quantity of good $y$ produced in exchange for a note is given by $y=\\omega^{-1}v\\left(\\phi\\right)$.",
    "gold_answer": "The quantity of good $y$ is determined by the seller's participation constraint: $$ -\\omega y+v\\left(\\phi\\right)\\ge0. $$ \\n1. Rearrange the inequality to solve for $y$: $$ v\\left(\\phi\\right)\\geq\\omega y. $$\\n2. Divide both sides by $\\omega$ to isolate $y$: $$ y\\leq\\omega^{-1}v\\left(\\phi\\right). $$\\n3. Since the buyer has all the bargaining power, the seller will produce exactly $y=\\omega^{-1}v\\left(\\phi\\right)$ to satisfy the constraint.",
    "question_context": "Let $m^{1}\\in[0,1]$ denote the invariant measure of buyers holding a note at the end of the first stage, let $m^{2}\\in[0,1]$ denote the invariant measure of sellers holding a note at the end of the second stage, and let $m^{3}\\in[0,1]$ denote the invariant volume of outstanding notes that are retired in the third stage. I only consider equilibria for which $m^{1}=1$ and $m^{2}=\\dot{m}^{3}=\\alpha\\lambda$ ,where $\\alpha\\in[0,1]$ denotes the probability that the seller will accept a privately issued note in exchange for his output.\nThe Bellman equations for a buyer are given by $$ V^{0}=-\\gamma+\\alpha\\lambda\\left[u\\left(y\\right)+\\beta V^{0}\\right]+\\left(1-\\alpha\\lambda\\right)\\beta V^{1}, $$ $$ \\begin{array}{r}{V^{1}=\\alpha\\lambda\\left[u\\left(y\\right)+\\beta V^{0}\\right]+\\left(1-\\alpha\\lambda\\right)\\beta V^{1}.}\\end{array} $$ Here, $y\\in\\mathbb{R}_{+}$ denotes the quantity of good $y$ that he will be able to purchase from the seller with whom he is matched in exchange for a note.\nThe Bellman equations for a seller are given by $$ W^{0}=\\beta\\left[\\lambda W^{1}+\\left(1-\\lambda\\right)W^{0}\\right], $$ $$ W^{1}=\\operatorname*{max}_{\\alpha\\in\\left[0,1\\right]}\\alpha\\left[-\\omega y+v\\left(\\phi\\right)\\right]+\\beta\\left[\\lambda W^{1}+\\left(1-\\lambda\\right)W^{0}\\right]. $$\nThe Bellman equations for a banker are given by $$ J^{0}=1-\\phi+\\beta\\left[\\alpha\\lambda J^{0}+(1-\\alpha\\lambda)J^{1}\\right], $$ $$ J^{1}=\\beta\\left[\\alpha\\lambda J^{0}+\\left(1-\\alpha\\lambda\\right)\\beta J^{1}\\right]. $$\nThe buyer's surplus from trade is given by $$ u\\left(y\\right)+\\beta V^{0}-\\beta V^{1}=u\\left(y\\right)-\\beta\\gamma, $$ and the seller's surplus from trade is given by $$ -\\omega y+v\\left(\\phi\\right). $$\nThe quantity of good $y$ produced in exchange for a note will be given by $$ y=\\omega^{-1}v\\left(\\phi\\right). $$\nThe banker's participation constraint is given by $$ J^{0}\\geq\\beta\\left[\\lambda J^{0}+(1-\\lambda)J^{1}\\right], $$ which simply requires $$ \\phi\\leq1. $$\nThe value associated with his best deviation ${\\boldsymbol{J}}^{d}$ is indeed bounded: $$ 1\\leq J^{d}\\leq\\frac{1-(1-\\lambda)^{2}\\beta}{1-(1-\\lambda)\\beta}. $$\nThe minimum value arises owing to the buyer's participation constraint, whereas the maximum value arises because of the banker's participation constraint. $$ v^{-1}\\left(\\omega u^{-1}\\left(\\frac{\\gamma\\left[1-\\beta\\left(1-\\lambda\\right)\\right]}{\\lambda}\\right)\\right)\\leq\\phi\\leq1. $$\n\nIn this section, I characterize stationary equilibrium allocations assuming that the members of the clearinghouse do not engage in any sort of risk-sharing scheme. Thus, to guarantee the solvency of each member bank, the clearinghouse will require each banker to keep in reserve the full face value of any note he has issued. In other words, each banker will have to adopt a $100\\%$ reserve policy to retain his membership."
  },
  {
    "qid": "econ-empirical-1516-0-1-1",
    "question": "2) Show how the mixture model example with exponential components satisfies the regularity conditions for the FLS estimator, particularly verifying Assumptions 1-3.",
    "gold_answer": "Verification:\n1. **Assumption 1**:\n   - \\( \\Gamma = [\\underline{\\gamma}, \\bar{\\gamma}] \\) compact\n   - \\( g_i(\\gamma) \\) continuous in \\( \\gamma \\)\n   - IID observations\n\n2. **Assumption 2**:\n   - \\( \\rho(\\gamma,\\theta) = \\theta_1 + \\theta_2\\frac{(\\gamma-1)}{(2\\gamma-1)^{1/2}} \\) is continuous\n   - Parameter space \\( \\Theta \\) compact\n\n3. **Assumption 3**:\n   - Boundedness: \\( |g_i(\\gamma)| \\leq \\frac{\\bar{\\gamma}(2\\underline{\\gamma}-1)^{1/2}}{\\underline{\\gamma}-1} \\)\n   - Derivatives bounded by same envelope",
    "question_context": "An empirical application conducts lifetime income path comparisons across different demographic groups according to years of work experience.\nFor example, the $S$-shaped curves or sigmoid functions are popularly employed as mean functions of business growth, crop yield, and learning outcome data.\nThe analysis of $p$-curves can be interpreted in terms of functional data. If we let $g_i(\\gamma):= K_h(\\gamma-x_i)$, the sample average becomes a standard kernel density function.\n\nThe methodology is applied to lifetime income path comparisons across demographic groups, mixture models, and p-curve analysis, demonstrating its practical utility in economic applications."
  },
  {
    "qid": "econ-empirical-1812-1-3-0",
    "question": "1) Prove that strategic convergence implies convergence in the product topology (i.e., weak convergence of $k$-order beliefs for all $k\\geq1$).",
    "gold_answer": "1. **Strategic Convergence**: Implies that for every game $G$ and action $a_i$, $a_i$ is rationalizable for $t_i$ iff it is $\\varepsilon$-rationalizable for $t_i^n$ for large $n$.  \n2. **Product Topology**: Convergence in the product topology means $t_i^n \\to t_i$ weakly at every $k$-order belief.  \n3. **Implication**: By Dekel et al. (2006), strategic convergence is equivalent to product topology convergence. The key step is showing that product topology convergence preserves best-reply properties across all games.",
    "question_context": "A sequence of types $t_{i}^{n}$ converges strategically to a type $t_{i}$ if for every game $G$ and every action $a_{i}$ of player $i$ in $G$, the following conditions are equivalent: (a) $a_{i}$ is rationalizable for $t_{i}$ in $G$; (b) for every $\\varepsilon>0$ there exists $N$ such that for every $n\\geq N$, $a_{i}$ is $\\varepsilon$-rationalizable for $t_{i}^{n}$ in $G$.\nThe strategic topology is the topology of strategic convergence on $T_{i}^{*}$.\nThe uniform strategic topology requires the rate of convergence $N$ to be independent of $G$.\n\nThis section defines strategic convergence and the strategic topology, which are used to analyze the robustness of rationalizable behavior under perturbations of beliefs."
  },
  {
    "qid": "econ-empirical-498-0-0-2",
    "question": "3) Derive the strategic-form payoff function $U_i: S_i \\times S_{-i} \\to \\mathbb{R}$ from the extensive-form game tuple $(N,A,Z,P,({\\mathcal{T}}_i,u_i)_{i\\in N})$.",
    "gold_answer": "1. **Terminal Histories**: For each $z \\in Z$, identify $(s_i, s_{-i}) \\in S(z)$.  \n2. **Payoff Assignment**: Define $U_i(s_i, s_{-i}) = u_i(z)$ for the unique $z$ induced by $(s_i, s_{-i})$.  \n3. **Linearity**: Extend to mixed strategies via $U_i(\\sigma_i, p) = \\sum_{t_i \\in S_i} \\sigma_i(t_i) U_i(t_i, p)$.",
    "question_context": "Structural rationality is defined as a strategy that maximizes a player's ex ante expected payoff with respect to some perturbation of beliefs, ensuring every information set is considered possible, albeit arbitrarily unlikely.\nThe game is represented by a tuple $(N,A,Z,P,({\\mathcal{T}}_{i},u_{i})_{i\\in N})$, where $N$ is the set of players, $A$ is the set of actions, $Z$ is the set of terminal histories, $P$ is the player function, ${\\mathcal{T}}_{i}$ is the collection of information sets for player $i$, and $u_{i}$ is the payoff function.\nA consistent conditional probability system (CCPS) for player $i$ is an array $\\mu=(\\mu(\\cdot|I))_{I\\in{\\mathbb{Z}}_{i}\\cup\\{\\phi\\}}\\in\\Delta(S_{-i})^{{\\mathbb{Z}}_{i}\\cup\\{\\phi\\}}$ with a sequence $(p^{k})_{k\\geq1}$ such that $\\lim_{k\\to\\infty}p^{k}(\\cdot|S_{-i}(I))=\\mu(\\cdot|I)$.\nA strategy $s_{i}\\in S_{i}$ is weakly sequentially rational given $\\mu$ if, for every $I\\in{\\mathcal{Z}}_{i}\\cup\\{\\phi\\}$ with $s_{i}\\in S_{i}(I)$, and all $t_{i}\\in S_{i}(I)$, $U_{i}(s_{i},\\mu(\\cdot|I))\\geq U_{i}(t_{i},\\mu(\\cdot|I))$.\n\nThe paper introduces structural rationality as a novel optimality criterion in dynamic games, addressing the limitations of sequential rationality in testing off-path beliefs. It leverages side bets and a variant of the strategy method for belief elicitation."
  },
  {
    "qid": "econ-empirical-885-2-0-3",
    "question": "4) Discuss the judicial concept of \"reasonable\" reliance in Fuller and Perdue (1936) and its relationship to the model's efficient reliance $r^{*}$.",
    "gold_answer": "1. **Reasonable Reliance**: Courts historically protect only \"reasonable\" reliance, which aligns with avoiding overinvestment.  \n2. **Efficient Reliance**: The model's $r^{*}$ formalizes this by maximizing contract value.  \n3. **Proxy**: \"Reasonable\" reliance acts as a judicial proxy for $r^{*}$, ensuring courts enforce only efficiency-enhancing expenditures.",
    "question_context": "Reliance expenditures are generally defined as outlays, not included in the contractual terms, that will be profitable if there is compliance with the contract and unprofitable otherwise.10 With multiple compliance and breach actions and with the buyer's payoff state-dependent $(x_{b}=x_{b}(a,r,\theta_{j},\\delta))$ , reliance could be unprofitable if a compliance action is taken or even profitable if a defaulting action is chosen. Thus, we need to broaden the definition of reliance for this more general model of contracts: reliance expenditures are outlays made by the buyer-—not included in the contractual terms-—that are expected to raise the buyer's payoff from the contract.\nThe treatment of reliance expenditures by various damage measures plays an important role in contracts. Shavell(1980) and Rogerson (1984) have shown that damage measures that return all reliance expenditures in the event of breach result in an overinvestment in reliance. Likewise, the failure to include reliance expenditures in damages may result in an underinvestment in reliance. The standard by which over- or underreliance is judged is the socially efficient level of reliance.\nDefinition 3. For a fixed-state partition ${\\boldsymbol{\\gamma}}~=~(\\gamma_{1},~\\ldots,~\\gamma_{k})$ and fixed-compliance sets $\\alpha_{1}$ ···; $\\alpha_{k}$ the socially optimal level of reliance $r^{*}$ is determined by the contract problem. Choose $r^{*}\\in R$ and $a\\in A$ to maximize $E U_{s}(x_{s}(a,\\theta_{j},r^{*},\\delta),a)$ , subject to constraints (1) and (3).\nThe socially optimal level of reliance is thus the amount that maximizes the value of the contract. Note that the damage measure $\\delta$ is fixed in Definition 3. In a given contract problem changing the damage measure leads to a different amount of socially efficient reliance. Assume that a solution for $r^{*}$ exists and is unique.\nIn this analysis I assume that the courts can judge the socially optimal level of reliance. Damage measures that protect reliance expenditures—-the expectations and reliance damage measures (see Section 4)—-will return only up to the socially optimal amount in the event of breach.11 That courts protect only effcient reliance may be implicit in the longstanding notion that only “reasonable\" reliance is protected (Fuller and Perdue, 1936, pp. 85-88). \"Reasonable” reliance may be the judicial proxy for efficient reliance.1² Furthermore, reliance expenditures are, by definition, not included in the contract terms. If the parties know that overreliance is an important possibility, they will have an incentive to incorporate the allowable reliance expenditures into the contract, and thus bypass the inefficiency.\n\nReliance expenditures are generally defined as outlays, not included in the contractual terms, that will be profitable if there is compliance with the contract and unprofitable otherwise. With multiple compliance and breach actions and with the buyer's payoff state-dependent, reliance could be unprofitable if a compliance action is taken or even profitable if a defaulting action is chosen. Thus, we need to broaden the definition of reliance for this more general model of contracts: reliance expenditures are outlays made by the buyer—not included in the contractual terms—that are expected to raise the buyer's payoff from the contract."
  },
  {
    "qid": "econ-empirical-836-2-0-2",
    "question": "3) Construct a valid joint probability mass function $p(T^{*}, T, z)$ under Assumption 2.2(i).",
    "gold_answer": "1. Start with the factorization $p(T^{*}, T, z) = p(T|T^{*})p(T^{*}|z)p(z)$.\n2. Use Assumption 2.2(i) to simplify $p(T|T^{*}, z) = p(T|T^{*})$.\n3. Ensure $p(T|T^{*})$ and $p(T^{*}|z)$ are valid probability mass functions.\n4. Verify compatibility with the observed distribution $p(T, z)$.",
    "question_context": "All of the results in this paper hold $\\mathbf{x}$ fixed. This allows us to completely ignore the presence of covariates in the proofs that follow. Accordingly we work in terms of scalars $\\alpha_{0},\\alpha_{1},\\beta,\\gamma$ $p_{k}$ , etc. rather than functions $\\alpha({\\bf x}),\\alpha_{1}({\\bf x}),\\beta({\\bf x}),p_{k}({\\bf x}).$ The former should be understood as the value of the latter evaluated at some particular $\\mathbf{x}.$ .\nProof of Lemma 2.1. Follows from a simple calculation using the law of total probability. □\nProof of Lemma 2.2. Immediate since $\\mathsf{C o v}(z,T)=(1-\\alpha_{0}-\\alpha_{1})\\mathsf{C o v}(z,T^{*})$ by Lemma 2.1. □\nProof of Theorem 2.1. To show that $\\alpha_{0}\\leq p_{k}\\leq1-\\alpha_{1}$ , substitute $p_{k}^{*}=0$ and $p_{k}^{*}=1$ , respectively, into Lemma 2.1 and rearrange. To show that $\\mathbb{E}[y|z=k]=c+\\beta(p_{k}-\\alpha_{0})/(1-\\alpha_{0}-\\alpha_{1}).$ take conditional expectations of Eq. (1) and apply Assumption 2.1(iii) and Lemma 2.1.\nTo prove sharpness we need to show that for any $(c,\\beta,\\alpha_{0},\\alpha_{1})$ that satisfy $\\alpha_{0}~\\le~p_{k}~\\le~1-\\alpha_{1}$ and $\\mathbb{E}[y|z=k]=$ $c+\\beta(p_{k}-\\alpha_{0})/(1-\\alpha_{0}-\\alpha_{1})$ we can construct a valid joint distribution for $(y,T,T^{*},z)$ that is compatible with the observed distribution of $(y,T,z)$ , provided that $p_{1}~\\neq~p_{0}$ .\nFor the first step, we need to construct a valid joint probability mass function $p(T^{*},T,z)$ with support set $\\{0,1\\}\\times\\{0,1\\}\\times$ {0, 1}. By Assumption 2.2(i), $p(T|T^{*},z)=p(T|T^{*})$ and hence\n\n$$\n\\begin{array}{r}{p(T^{*},T,z)=p(T|T^{*})p(T^{*}|z)p(z).}\\end{array}\n$$\nFor the second step, we need to construct a valid conditional distribution for $y$ given $(T,T^{*},z)$ . To begin we define the following notation:\n\n$$\n\\begin{array}{r l r l}&{\\mathbb{r}_{t k}\\equiv\\mathbb{P}(T^{*}=1|T=t,z=k)}&&{F_{t}(\\tau)\\equiv\\mathbb{P}(y\\leq\\tau|z=k)}\\ &{F_{t k}(\\tau)\\equiv\\mathbb{P}(y\\leq\\tau|T=t,z=k)}&&{F_{t k}^{\\prime*}(\\tau)\\equiv\\mathbb{P}(y\\leq\\tau|T^{*}=t^{*},T=t,z=k)}\\ &{G_{k}(\\tau)\\equiv\\mathbb{P}(\\varepsilon\\leq\\tau|z=k)}&&{G_{t k}^{\\prime*}(\\tau)\\equiv\\mathbb{P}(\\varepsilon\\leq\\tau|T^{*}=t^{*},T=t,z=k).}\\end{array}\n$$\nAssumption 2.1(i) imposes a relationship between $G_{t k}^{t^{*}}$ and $F_{t k}^{t^{*}}$ for each $t^{*}$ , namely\n\n$$\nG_{t k}^{0}(\\tau)=F_{t k}^{0}(\\tau+c),\\quad G_{t k}^{1}(\\tau)=F_{t k}^{1}(\\tau+c+\\beta)\n$$\n\nand thus we see that\n\n$$\n\\begin{array}{r l}&{G_{k}(\\tau)=r_{1k}p_{k}F_{1k}^{1}(\\tau+c+\\beta)+r_{0k}(1-p_{k})F_{0k}^{1}(\\tau+c+\\beta)}\\ &{\\quad\\quad+(1-r_{1k})p_{k}F_{1k}^{0}(\\tau+c)+(1-r_{0k})(1-p_{k})F_{0k}^{0}(\\tau+c)}\\end{array}\n$$\nProof of Corollary 2.1. The result follows by substituting the largest and smallest possible values for $\\alpha_{0}+\\alpha_{1}$ and taking the difference of the expressions for $\\mathbb{E}[y|z=k]$ . □\nProof of Theorem 2.2. The only difference between the conditions of Theorem 2.1 and those of 2.2 is that the latter imposes Assumption 2.2(iii) while the former does not. Accordingly, the present argument builds on the proof of Theorem 2.1 and relies on the notation defined within it.\n\nThe appendix provides detailed proofs for the results in the paper, focusing on partial identification results and the construction of valid joint distributions under various assumptions."
  },
  {
    "qid": "econ-empirical-83-2-0-3",
    "question": "4) Formalize the test for business-cycle confounding using lagged growth rates (Table IV). Why might significance weaken when lags are included?",
    "gold_answer": "The augmented model is:\n\\[ \\Delta G_{it} = \\alpha + \\beta D_{it} + \\sum_{k=1}^3 \\phi_k \\Delta G_{i,t-k} + \\epsilon_{it} \\]\n**Weakened significance**: If deregulation timing correlates with cyclical troughs (\\(\\phi_k \\) captures mean reversion), the true \\(\\beta\\) may be overstated in Table II. Lags absorb this autocorrelation, reducing \\(\\beta\\)'s magnitude and significance.",
    "question_context": "Each of the 35 deregulating states appears as a pair of points on the graph: the state's two-letter zip code name indicates the change in average annual growth rate of that state (the 'treatment state') following deregulation; a triangle appears directly above (or below) the state name indicating the change in the mean growth rate over the same period for all states that did not alter their regulatory regime during the 1972-1992 period (the 'control group').\nTable II presents estimates of the increase in growth following relaxation of intrastate branching restrictions. We present a regression of real, per capita growth based on personal income or gross state product on time and state fixed effects, and an indicator variable equal to one for states with no restrictions on branching via M&A.\nWe augment our growth model with two variables measuring the fiscal policies of the state government, the ratio of public investment to income and the ratio of tax receipts by the state government to total income (lagged one period).\nWe deal more formally with the problem of the timing of deregulation relative to the local business cycle by introducing lags of the dependent variable into our basic growth model. This model removes state-specific business cycle effects as well as national and regional cycles.\n\nThis section examines the robustness of growth results following branch deregulation, comparing treatment and control states to ensure results are not driven by outliers or coincident policy shifts."
  },
  {
    "qid": "econ-empirical-95-0-0-1",
    "question": "2) Calculate the expected duration a mimic's fund will stay in business if it offers a steady monthly return of $0.66\\%$ on top of a risk-free rate of $0.33\\%$, given the annual probability of failure is $7.6\\%$.",
    "gold_answer": "1. The annual probability of survival is $1 - 0.076 = 0.924$.\n2. The expected duration is the sum of the probabilities of surviving up to each year:\n   $$E[T] = \\sum_{t=1}^{\\infty} t \\cdot P(\\text{failure in year } t) = \\sum_{t=1}^{\\infty} t \\cdot (0.924)^{t-1} \\cdot 0.076.$$\n3. This is a geometric series with expected value:\n   $$E[T] = \\frac{1}{0.076} \\approx 13.16 \\text{ years}.$$",
    "question_context": "Let $Z_{t}=Z_{t}(m_{1},\\ldots,m_{t-1};x_{1},\\ldots,x_{t-1})$ be a random variable that describes how much net new money flows into the fund at the start of period $t$ as a function of the returns in prior periods.\nGiven any excess returns sequence $\\vec{m}\\geq\\vec{1}$ over $T$ years, a mimic can reproduce it with probability $1/\\prod_{1\\leq t\\leq T}m_{t}$ for all realizations of the benchmark returns.\nThe probability that such a fund will go under in any given year is $1-(1.0066)^{-12}=0.076$ or about $7.6\\%$.\nThe framework allows us to estimate how much a mimic can earn under different incentive structures; it also shows that commonly advocated reforms of the incentive structure cannot be relied upon to screen out unskilled risk-neutral managers who do not deliver excess returns to investors.\n\nThe analysis discusses how compensation mechanisms for skilled portfolio managers can be gamed by mimics, who take calculated risks without delivering value to investors. The framework incorporates flow-performance relationships without committing to a specific model, showing that mimics can attract the same amount of new money as skilled managers under certain conditions."
  },
  {
    "qid": "econ-empirical-1110-1-1-0",
    "question": "5) Critically evaluate the assumptions underlying the difference-in-difference (DiD) design in this study. What might violate the parallel trends assumption?",
    "gold_answer": "1. **Parallel Trends**: Assumes target/comparison groups would follow similar trends absent amnesty. Violations may occur if:\n   - Economic shocks disproportionately affect one group (e.g., sectoral downturns).\n   - Pre-existing trends differ (e.g., faster wage growth for high-skilled workers).\n2. **Testing**: Pre-treatment trend analysis (e.g., placebo tests) can validate the assumption.",
    "question_context": "To investigate whether NACARA affected the labor market outcomes of undocumented immigrants, I use a comparison group approach also referred to as difference-in-difference methodology.\nThe regression model for this analysis can be specified in the form of Equation 1.\nThe difference-in-difference estimate measures the effect of making a person eligible for amnesty, and not the effect of changing legal status.\n\nThis section outlines the empirical approach to estimate the causal effect of amnesty programs on labor market outcomes, focusing on the difference-in-difference methodology and data sources."
  },
  {
    "qid": "econ-empirical-1835-1-1-3",
    "question": "8) Prove that the KT statistic $\\mathrm{KT}$ is asymptotically equivalent to the Wald statistic $\\mathbf{W}$ using the relationship $\\mathcal{I}(\\beta^{0})[n^{1/2}(\\tilde{\\beta} - \\hat{\\beta})] \\cong n^{1/2}H(\\tilde{\\beta})^{\\prime}\\tilde{\\lambda}$.",
    "gold_answer": "1. From the relationship: $n^{1/2}(\\tilde{\\beta} - \\hat{\\beta}) \\cong \\mathcal{I}(\\beta^{0})^{-1}H(\\tilde{\\beta})^{\\prime}n^{1/2}\\tilde{\\lambda}$. \\\\ 2. Substitute into $\\mathbf{W}$: $\\mathbf{W} = n(\\tilde{\\beta} - \\hat{\\beta})^{\\prime}\\mathcal{I}(\\beta^{0})(\\tilde{\\beta} - \\hat{\\beta}) \\cong n\\tilde{\\lambda}^{\\prime}H(\\tilde{\\beta})\\mathcal{I}(\\beta^{0})^{-1}\\mathcal{I}(\\beta^{0})\\mathcal{I}(\\beta^{0})^{-1}H(\\tilde{\\beta})^{\\prime}\\tilde{\\lambda} = \\mathrm{KT}$. \\\\ 3. Thus, $\\mathbf{W} \\cong \\mathrm{KT}$ asymptotically.",
    "question_context": "The likelihood ratio statistic takes the usual form: $$ \\text{LR} = -2[L(\\tilde{\\beta}) - L(\\hat{\\beta})] = 2[L(\\hat{\\beta}) - L(\\tilde{\\beta})]. $$\nThe Wald statistic measures the magnitude of the difference between the restricted and unrestricted estimates of $\\beta_{n}^{0}$: $$ \\mathbf{W} = n(\\tilde{\\beta} - \\hat{\\beta})^{\\prime}\\mathcal{I}(\\beta^{0})(\\tilde{\\beta} - \\hat{\\beta}). $$\nThe Kuhn-Tucker statistic measures the magnitude of the Kuhn-Tucker multiplier vector: $$ \\mathrm{KT} = n\\tilde{\\lambda}^{\\prime}H(\\tilde{\\beta})\\mathcal{I}(\\beta^{0})^{-1}H(\\tilde{\\beta})^{\\prime}\\tilde{\\lambda}. $$\nBecause the difference between all of these statistics converges in probability to zero as $n\\rightarrow\\infty$, they all possess the same asymptotic distribution.\n\nThis section derives three test statistics for nonlinear inequality constraints and proves their asymptotic equivalence. The focus is on the likelihood ratio (LR), Wald, and Kuhn-Tucker (KT) statistics."
  },
  {
    "qid": "econ-empirical-1324-1-0-3",
    "question": "4) Using Definition 1.1, prove that conditional preference $\\succcurlyeq_A$ is independent of the choice of $h$ under Axioms A1 and A2.",
    "gold_answer": "1. Suppose $(f_A, h_{-A}) \\succcurlyeq (g_A, h_{-A})$ but $(g_A, d_{-A}) \\succ (f_A, d_{-A})$ for some $d$. \\\\ 2. By A2, $(f_A, h_{-A}) \\frac{1}{2} (g_A, d_{-A}) \\succcurlyeq (g_A, h_{-A}) \\frac{1}{2} (g_A, d_{-A})$. \\\\ 3. Similarly, $(g_A, h_{-A}) \\frac{1}{2} (g_A, d_{-A}) \\succ (g_A, h_{-A}) \\frac{1}{2} (f_A, d_{-A})$. \\\\ 4. By A1, this leads to a contradiction, proving independence of $h$.",
    "question_context": "A1 (Order). $\\succcurlyeq$ is transitive and complete on $\\mathcal{P}^{\\mathcal{Q}}$.\nA2 (Objective Independence). For all $f,g,h\\in\\mathcal{P}^{\\varOmega}$ and $\\alpha\\in(0,1],f\\succ$ (resp. $\\sim$ ) $g\\Rightarrow f\\alpha h\\succ$ (resp. t ) g:h.\nA3 (Non-triviality). There are f $;g\\in\\mathcal{P}^{\\mathcal{Q}}$ such that $f\\succ g$.\nA4 (Archimedean Property). $f\\succ g\\succ h\\Rightarrow\\exists\\alpha$ , $\\beta\\in(0,1)$ , $\\alpha>\\beta$ , such that $f\\alpha h\\succ g\\succ f\\beta h$.\nDefinition 1.1. Given $A\\subseteq\\varOmega,f,g\\in\\mathscr{P}^{\\varOmega},j$ $f$ is preferred to $g$ conditional on $A$ , written $f{\\succcurlyeq}_{\\mathcal{A}}g$ , if $(f_{A},h_{-A})\\succcurlyeq(g_{A},h_{-A})$ for some $h\\in\\mathcal{P}^{\\mathcal{Q}}$.\nTheorem 1.1. Under Axioms A1 and A2, the following property holds: (Strong Subjective Substitution) \\f, $g\\in\\mathcal{P}^{\\mathcal{Q}}$ , $\\forall A$ , $B\\subseteq{\\mathcal{Q}}$ with $A\\cap B=\\emptyset$ , if $f\\succcurlyeq_{A}g$ and $f\\succ_{B}$ (resp. $\\succcurlyeq_{B}$ ) $g$ then $f{\\succ}_{{A\\cup B}}$ (resp. $\\succcurlyeq_{A\\cup B}$ ) $g$ (cf. Blume et al. [5, Theorem 4.1]).\nDefinition 1.2. An event $A\\subseteq{\\mathcal{Q}}$ is Savage-null if $f\\sim_{A}g\\forall f,g\\in\\mathcal{P}^{\\Omega}$.\nA5 (Non-null-state Independence). For all states $\\omega$ , $\\omega^{\\prime}\\in\\varOmega$ which are not Savage-null and for any two constant acts $f,g\\in{\\mathcal{P}}^{\\varOmega}$ , $f\\succcurlyeq_{\\omega}g\\Leftrightarrow f\\succcurlyeq_{\\omega^{\\prime}}g$.\n\nThe following are the usual axioms for an expected-utility representation of preferences on $\\mathcal{P}^{\\mathcal{Q}}$. Let $\\succcurlyeq$ be a binary relation on $\\mathcal{P}^{\\itOmega^{\\itOmega}}$ (preference relation); $\\succ$ and $\\sim$ are defined in the usual way."
  },
  {
    "qid": "econ-empirical-1704-3-1-0",
    "question": "1) Derive $\\Pi_{F}^{\\prime}(\\bar{\\pi})$ and $\\Pi_{F}^{\\prime\\prime}(\\bar{\\pi})$ for $\\bar{\\pi} \\leq 0$ and interpret their economic implications.",
    "gold_answer": "1. For $\\bar{\\pi} \\leq 0$:\n   $$\\Pi_{F}^{\\prime}(\\bar{\\pi}) = -H(\\Delta(0) - \\bar{\\pi}) < 0,$$\n   $$\\Pi_{F}^{\\prime\\prime}(\\bar{\\pi}) = H^{\\prime}(\\Delta(0) - \\bar{\\pi}) < 0.$$\n2. The negative first derivative implies that increasing the minimum payoff reduces the buyer's expected payoff.\n3. The negative second derivative indicates diminishing returns to increasing $\\bar{\\pi}$.",
    "question_context": "For weakly negative values of $\\bar{\\pi}$, $x_{\\bar{\\pi}}(c)$ is a function with a single step at $\\Delta(0) - \\bar{\\pi}$, and so:\n\n$$\\Pi_{F}(\\bar{\\pi}) = \\int_{0}^{\\Delta(0) - \\bar{\\pi}} H(c)\\mathrm{d}c \\quad \\text{for all } \\bar{\\pi} \\leq 0.$$\nThe thresholds $\\underline{\\varepsilon}$ and $\\overline{\\varepsilon}$ are derived as:\n\n$$\\underline{\\varepsilon} = 0, \\quad \\overline{\\varepsilon} = \\frac{-\\Pi_{F}^{\\prime}(\\bar{\\pi}^{\\mathrm{max}})}{1 - \\Pi_{F}^{\\prime}(\\bar{\\pi}^{\\mathrm{max}})} < 1.$$\n\nThis section extends the analysis to cases where $\\bar{\\pi}^{\\mathrm{min}} < 0$, discussing the properties of $\\Pi_{F}(\\bar{\\pi})$ and deriving thresholds $\\underline{\\varepsilon}$ and $\\overline{\\varepsilon}$ for optimal mechanisms."
  },
  {
    "qid": "econ-empirical-571-1-0-0",
    "question": "1) Derive the differential equation for $\\bar{N}(t)$ from the hazard rate $\\frac{f(t)}{1-F(t)}=p+q F(t)$.",
    "gold_answer": "1. Start with the hazard rate: $\\frac{f(t)}{1-F(t)} = p + q F(t)$.\\n2. Recall that $\\bar{N}(t) = m F(t)$, so $F(t) = \\frac{\\bar{N}(t)}{m}$.\\n3. Substitute $F(t)$ into the hazard rate: $\\frac{f(t)}{1-\\frac{\\bar{N}(t)}{m}} = p + q \\frac{\\bar{N}(t)}{m}$.\\n4. Note that $f(t) = \\frac{\\mathrm{d}F(t)}{\\mathrm{d}t} = \\frac{1}{m} \\frac{\\mathrm{d}\\bar{N}(t)}{\\mathrm{d}t} = \\frac{\\bar{n}(t)}{m}$.\\n5. Substitute $f(t)$ into the equation: $\\frac{\\frac{\\bar{n}(t)}{m}}{1-\\frac{\\bar{N}(t)}{m}} = p + q \\frac{\\bar{N}(t)}{m}$.\\n6. Simplify the denominator: $1-\\frac{\\bar{N}(t)}{m} = \\frac{m-\\bar{N}(t)}{m}$.\\n7. The equation becomes: $\\frac{\\bar{n}(t)}{m-\\bar{N}(t)} = p + q \\frac{\\bar{N}(t)}{m}$.\\n8. Multiply both sides by $m-\\bar{N}(t)$: $\\bar{n}(t) = p(m-\\bar{N}(t)) + \\frac{q}{m}\\bar{N}(t)(m-\\bar{N}(t))$.",
    "question_context": "The hazard rate equals $\\frac{f(t)}{1-F(t)}=p+q F(t)$, where $p$ and $q$ are the parameters that determine the shape of the diffusion process.\nThe cumulative number of adopters at time $t$, denoted by $N(t)$, is a random variable with mean $\\bar{N}(t)=E[N(t)]=m F(t)$, where $t$ is measured in continuous time and $E$ denotes the expectation operator.\nThe function $\\bar{N}(t)$ obeys the differential equation $\\bar{n}(t)\\equiv\\frac{\\mathrm{d}\\bar{N}(t)}{\\mathrm{d}t}=p[m-\\bar{N}(t)]+\\frac{q}{m}\\bar{N}(t)[m-\\bar{N}(t)]$.\n\nThe Bass model assumes a population of $m$ potential adopters, where, in the context of citations, we will associate $m$ with the maturity level. In our context, adopters should be viewed as articles which cite the articles under scrutiny. The maturity level can be viewed as the total number of citations in the long run. For each adopter, the time to adoption is a random variable with a distribution function $F(t)$ and density $f(t)$."
  },
  {
    "qid": "econ-empirical-593-2-0-0",
    "question": "1) Derive the conditions under which the Newton-Raphson iterative scheme converges to two distinct solutions for $\\tilde{\\mu}$ and $\\tilde{\\sigma}$ in equation (5), given the parameter values $(\\mu,\\sigma_{1},\\sigma_{2},r_{1},r_{2})$.",
    "gold_answer": "1. **Initial Setup**: Start with the equation (5) to be solved numerically for $\\tilde{\\mu}$ and $\\tilde{\\sigma}$.\n2. **Newton-Raphson Method**: Apply the iterative scheme:\n   $$ x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)} $$\n   where $f(x)$ represents equation (5).\n3. **Convergence Conditions**: The method converges to two distinct solutions if:\n   - The function $f(x)$ has two real roots.\n   - The initial guesses $x_0$ are chosen such that they lie in the basins of attraction of different roots.\n4. **Verification**: Check the second derivative $f''(x)$ to ensure the roots are distinct and the function is well-behaved.",
    "question_context": "For given parameter values $(\\mu,\\sigma_{1},\\sigma_{2},r_{1},r_{2})$ we can calculate the inconsistency of the Tobit MLE's by solving (5) numerically for $\\tilde{\\mu}$ and $\\tilde{\\sigma}$.\nThe solution of (5) was found using the Newton-Raphson iterative scheme, from arbitrary starting values. Many sets of starting values were tried. In all cases precisely two sets of solutions to (5) were found, with one set having $\\tilde{\\sigma}$ negative and the other set having $\\tilde{\\sigma}$ positive.\nThe degree of inconsistency depends heavily on the degree of heteroskedasticity and on the degree of truncation of the sample. For given $\\sigma_{1}$, the inconsistency of $\\hat{\\mu}$ increases as the degree of truncation increases; that is, as $\\mu$ decreases.\nFor given $\\mu$, the inconsistency increases as the degree of heteroskedasticity increases, that is, as $\\sigma_{1}$ becomes further from one (the value of $\\sigma_{2}$).\n\nThe section discusses the numerical calculation of inconsistency in Tobit MLE estimates under heteroskedasticity, using parameter values $(\\mu,\\sigma_{1},\\sigma_{2},r_{1},r_{2})$. The Newton-Raphson iterative scheme is employed to solve equation (5) for $\\tilde{\\mu}$ and $\\tilde{\\sigma}$."
  },
  {
    "qid": "econ-empirical-374-0-0-2",
    "question": "3) Compare the local power properties of the original variable addition (VA) test and the modified VA test proposed in the paper. How does the choice of $z_t$ affect power?",
    "gold_answer": "The original VA test (e.g., $z_t = \\Delta x_t$) has power only in $1/\\sqrt{T}$ neighborhoods. The modified VA test improves power by constructing $z_t$ to mimic the persistence of $x_t$:\n1. **Short-memory VA**: $z_t = (1 - \\bar{\\alpha}L)^{-1} \\Delta x_t$ with $|\\bar{\\alpha}| < 1$. Power increases as $\\bar{\\alpha} \\to 1$.\n2. **Mild integration**: $z_t = (1 - \\alpha_T L)^{-1} \\Delta x_t$ with $\\alpha_T = 1 - a/T^\\eta$. Power improves with $\\eta$.\n3. **Long differences**: $z_t = x_t - x_{t-k_T}$ with $k_T = O(T^\\nu)$. Power rate is $T^{-(1+\\nu)/2}$.\nThe modified VA test achieves better power by balancing persistence and stationarity.",
    "question_context": "Valid inference in predictive regressions depends in a crucial manner on the degree of persistence of the predictor variables. The paper studies test procedures that are robust in the sense that their asymptotic null distributions are invariant to the persistence of the predictor, that is, the limiting distribution is the same irrespective of whether the regressors are stationary or (nearly) integrated.\nExisting procedures are often conservative (e.g. tests based on Bonferroni bounds), are based on highly restrictive assumptions (such as homoskedasticity or assuming an AR(1) process for the regressor) or fail to have power against alternatives in a $^1/T$ neighborhood of the null hypothesis.\n\nThe paper discusses robust inference in predictive regressions, focusing on test procedures that are invariant to the persistence of predictor variables. The authors highlight the limitations of existing methods and propose new approaches to improve asymptotic power."
  },
  {
    "qid": "econ-empirical-1154-1-3-0",
    "question": "5) Explain how migration in response to state minimum-wage increases biases the estimates of minimum-wage effects and the implications for comparing state and federal increases.",
    "gold_answer": "Migration biases the estimates as follows:\n1. For federal increases, migration cannot occur, so any adverse impact is overstated due to the lack of migration response.\n2. For state increases, migration of poor families into the state may increase labor market competition for residents, worsening outcomes in the treatment group and improving them in the control group, leading to an understatement of the minimum-wage's effects.\nThese biases imply that:\n1. Estimates from state increases may not fully capture the true effects due to migration.\n2. Comparisons between state and federal increases must account for these differing biases to accurately assess their relative impacts.",
    "question_context": "The presence of migration will lead our estimates to overstate the adverse impact of a federal minimum-wage increase (in response to which migration cannot really occur).\nFor state increases, our method likely understates the effects of the minimum wage because our sample includes some federal variation.\n\nThis section discusses potential biases in estimates due to migration in response to state minimum-wage increases and how these biases affect the comparison between state and federal increases."
  },
  {
    "qid": "econ-empirical-1510-1-1-1",
    "question": "2) Critically analyze Shonfield's critique of monetarist policies. What are the theoretical and empirical weaknesses of his arguments?",
    "gold_answer": "1. **Theoretical Critique**: Shonfield assumes that interventionist policies are inherently superior, ignoring the inefficiencies of government intervention.\n2. **Empirical Evidence**: Monetarist policies successfully reduced inflation in the US and UK, which Shonfield underemphasizes.\n3. **Long-term Effects**: Shonfield does not fully account for the long-term benefits of market liberalization.\n4. **Comparative Analysis**: His comparison may overlook unique contextual factors in each country.\n5. **Policy Alternatives**: He does not provide a clear alternative to monetarism that addresses its shortcomings.",
    "question_context": "The Use of Public Power examines the balance between public and private power during the boom years of the sixties and the uncertainties of the seventies, showing how the persistence of inflation, and popular reaction to it, offered conservative politicians a heaven-sent opportunity of reversing the trend toward increasing public expenditure.\nShonfield attacks the monetarist, non-interventionist policies of the Reagan and Thatcher governments and compares the economic achievements of the United States and Britain with those of France, Germany, Japan, and other Western-style mixed economies.\nHe critically appraises the future of the mixed economy.\n\nThe text examines the balance between public and private power during the economic boom of the 1960s and the uncertainties of the 1970s, critiquing monetarist policies and comparing economic achievements across nations."
  },
  {
    "qid": "econ-empirical-1177-0-1-0",
    "question": "5) Formally describe the matching algorithm used in the paper, including the distance metric for matching households across countries. How does this address selection bias?",
    "gold_answer": "1. The algorithm minimizes \\( D = \\sum_{i} (w_{i,US} - w_{i,Sweden})^2 \\) for initial wealth \\( w_i \\).\n2. Propensity score matching ensures \\( E[w_{US}|X] = E[w_{Sweden}|X] \\), balancing observables.\n3. Selection bias is reduced because matched samples satisfy ignorability \\( (Y_0, Y_1) \\perp T | X \\).",
    "question_context": "We utilize a matching algorithm that controls for differences in initial wealth distribution and demographic components.\nMatching on initial wealth alone compresses the U.S. wealth distribution, making quintile transitions more likely.\nMultivariate matching on age, family composition, and income does not significantly alter the results from univariate wealth matching.\n\nThe paper employs nonparametric sample matching methods to compare wealth dynamics in Sweden and the United States, controlling for initial wealth and demographic characteristics."
  },
  {
    "qid": "econ-empirical-1271-2-0-3",
    "question": "4) Show mathematically why the hypothesis of constant true velocity of money ($V_{\\mathrm{T}}$) is equivalent to $\\alpha=1$. Use the derived equations to support your answer.",
    "gold_answer": "1. From eq. (3.3), the true inflation rate is:\n   $$\\pi_{\\mathrm{T}} = \\pi + \\alpha(\\mathrm{d}M/M - \\pi - \\mathrm{d}R/R).$$\n2. The true velocity is $V_{\\mathrm{T}} = P_{\\mathrm{T}}R/M$. Its growth rate is:\n   $$\\mathrm{d}V_{\\mathrm{T}}/V_{\\mathrm{T}} = \\pi_{\\mathrm{T}} + \\mathrm{d}R/R - \\mathrm{d}M/M.$$\n3. Substitute $\\pi_{\\mathrm{T}}$:\n   $$\\mathrm{d}V_{\\mathrm{T}}/V_{\\mathrm{T}} = (1-\\alpha)(\\pi + \\mathrm{d}R/R - \\mathrm{d}M/M) = (1-\\alpha)\\mathrm{d}V/V.$$\n4. For $\\mathrm{d}V_{\\mathrm{T}}/V_{\\mathrm{T}} = 0$ (constant velocity), $\\alpha$ must equal 1. Thus, $\\alpha=1$ implies constant true velocity.",
    "question_context": "The complete model can be outlined as follows: \n\n$$\n\\begin{array}{r l}&{\\log q m^{\\underline{{\\delta}}}=a_{0}+a_{1}\\log y+a_{2}\\pi_{\\mathrm{r}}^{\\underline{{\\delta}}},}\\ &{\\pi_{\\mathrm{r}}-\\log P_{\\mathrm{r}}-\\log P_{\\mathrm{r}-1},}\\ &{\\log P_{\\mathrm{r}}-\\log P+\\alpha\\log(M_{2}/P R),}\\ &{\\pi_{\\mathrm{r}}^{\\underline{{\\delta}}}-\\pi_{\\mathrm{r}-1}^{\\underline{{\\delta}}}-\\beta(\\pi_{\\mathrm{r}}-\\pi_{\\mathrm{r}-1}^{\\underline{{\\delta}}}),0\\le\\beta\\le1,}\\ &{\\log q m-\\log q m_{-1}=\\tau(\\log q m^{\\underline{{\\delta}}}-\\log q m_{-1}),0\\le\\tau\\le1,}\\ &{q m=Q M/P_{\\mathrm{r}},}\\ &{y=R+((Q M-Q M_{-\\mathrm{r}})+(M_{\\mathrm{u}}-M_{1-1}))/P.}\\end{array}\n$$\n\nThe variables are defined again for convenience: \n\n$Q M=$ quasi-money balances (or savings deposits) $\\scriptstyle R=$ real retail sales, \n$\\begin{array}{r l}{\\pmb{P}}&{{}=}\\end{array}$ officially reported price index of retail sales, $M_{\\mathbf{H}}=$ household currency holding. \n$M_{2}=M_{\\mathrm{H}}+$ households bank deposits, \n$\\boldsymbol{y}_{}=$ real disposable income, \n\nsuperscript E denotes expectation and subscript T denotes true variables.\nFrom the complete model given by eqs. (3.1)-(3.7) we derive a reduced form for estimation: \n\n$$\nq m m=A_{0}+A_{1}y y+A_{2}\\pi_{\\mathrm{T}}+A_{3}q m m_{-1},\n$$\n\nwhere \n\n$$\n\\begin{array}{c l c r}{{q m m=\\log q m-(1-\\beta)\\log q m_{1},}}\\ {{}}\\ {{y y=\\log y-(1-\\beta)\\log y_{-1},}}\\end{array}\n$$\n\n$$\n\\begin{array}{r l}&{\\pi_{\\mathrm{T}}=\\pi+\\alpha(\\log{(M_{2}/P R)}-\\log{(M_{2}/P R)}-1),}\\ &{}\\ &{\\log{q m}=\\log{Q M}-\\log{P}-\\alpha\\log{(M_{2}/P R)},}\\ &{}\\ &{A_{0}=\\tau a_{0}\\beta,\\quad A_{1}=\\tau a_{1},\\quad A_{2}=\\tau a_{2}\\beta,\\quad A_{3}=1-\\tau.}\\end{array}\n$$\nThe parameter $\\pmb{\\alpha}$ is allowed to vary with increments of 0.01 over the range O-3, and for each value of $\\alpha_{1}$, we changed $\\beta$ between 0-1 (also with increments of 0.01). The values of $\\pmb{\\alpha}$ and $\\beta$ that maximized the log-likelihood function of the estimated parameters of eq. (3.8) were $\\pmb{\\alpha}=0.89$ and $\\beta=0.92$ with the following results: \n\n$$\n\\begin{array}{c}{{q m m=-0.286+0.392y y-0.548\\pi_{\\mathrm{T}}+0.639q m m_{-1},}}\\ {{(-3.03)(8.62)(-4.56)(14.99)}}\\end{array}\n$$\n\n$$\nR^{2}=0.993,H=0.26,\\mathrm{log-likelihood}=88.25.\n$$\nThe hypothesis that consumers believe that the true price level is the official price level is equivalent to asserting that $\\pmb{\\alpha=0}$. We therefore set $\\pmb{\\alpha=0}$ and search for a value for $\\beta$ by using the maximum likelihood technique mentioned above. We obtain the following results: \n\n$$\n\\begin{array}{c}{{q m m=-0.481~+~0.384y y~-~0.287\\pi_{\\mathrm{T}}~+~0.770q m m_{-1},}}\\ {{(-4.03)~(7.06)~(-1.11)~(22.86)}}\\end{array}\n$$\n\nThe log-likelihood is lower than that of eq. (3.9), and a likelihood ratio test decisively rejects the hypothesis $\\pmb{\\alpha=0}$ (the $\\chi^{2}$ statistic is 15.14, while the 0.99 value of $\\chi^{2}$ is 9.21).\nThe hypothesis of constant true velocity of money, $V_{\\mathrm{T}}= P_{\\mathrm{T}}R/M$, is equivalent to the hypothesis that $\\pmb{\\alpha}$ equals unity. Estimating eq. (3.8) with $\\alpha=1,~\\beta$ varying from 0 to 1, we obtain the following results: \n\n$$\n\\begin{array}{c}{{q m m=-0.154~+~0.387y y~-~0.570\\pi_{\\mathsf{T}}~+~0.615q m m_{-1},}}\\ {{(-1.68)~(8.72)~(-5.16)~(13.62)}}\\end{array}\n$$\n\n$$\n\\alpha=1.0,\\quad\\beta=0.9,\\quad H=0.23,\n$$\n\nThe likelihood ratio test cannot reject the hypothesis that $\\alpha$ equals unity (the $\\chi^{2}$ statistic is 0.48). Thus, ${\\mathrm{d}V_{\\mathrm{T}}}/V_{\\mathrm{T}}$ is insignificantly different from zero, i.e., the true velocity is stable over the sample period.\n\nThe model outlines the relationship between quasi-money balances, real disposable income, and inflation expectations in China from 1979 to 1988. It includes equations for demand for quasi-money, inflation rates, and adjustments for seasonal effects."
  },
  {
    "qid": "econ-empirical-1538-0-1-0",
    "question": "5) For Extension 1 with additive fixed-effects $\\eta_{i}$, derive the transformed model after removing the time-invariant heterogeneity and show how the GFE estimator can be applied to the transformed equation.",
    "gold_answer": "1. Transform by subtracting individual means: $$ y_{i t}-\\bar{y}_{i}=(x_{i t}-\\bar{x}_{i})^{\\prime}\\theta+(\\alpha_{g_{i}t}-\\bar{\\alpha}_{g_{i}})+(v_{i t}-\\bar{v}_{i}). $$ \\n2. The transformed model has structure: $$ \\ddot{y}_{i t}=\\ddot{x}_{i t}^{\\prime}\\theta+\\ddot{\\alpha}_{g_{i}t}+\\ddot{v}_{i t} $$ where $\\ddot{\\alpha}_{g_{i}t}=\\alpha_{g_{i}t}-\\frac{1}{T}\\sum_{s=1}^{T}\\alpha_{g_{i}s}$. \\n3. GFE is applied to estimate $\\theta$ and $\\{\\ddot{\\alpha}_{gt}\\}$ by minimizing: $$ \\sum_{i=1}^{N}\\sum_{t=1}^{T}(\\ddot{y}_{i t}-\\ddot{x}_{i t}^{\\prime}\\theta-\\ddot{\\alpha}_{g_{i}t})^{2}. $$ \\n4. Original $\\alpha_{gt}$ can be recovered up to a constant.",
    "question_context": "Extension 1: Unit-Specific Heterogeneity: The GFE framework can be combined with additive time-invariant fixed-effects: $$ y_{i t}=x_{i t}^{\\prime}\\theta+\\alpha_{g_{i}t}+\\eta_{i}+v_{i t}, $$ where $\\eta_{i}$ are $N$ unrestricted parameters.\nExtension 2: Heterogeneous Coefficients: Another extension is to allow for group-specific effects of covariates: $$ y_{i t}=x_{i t}^{\\prime}\\theta_{g_{i}}+\\alpha_{g_{i}t}+v_{i t}. $$\n\nThe paper establishes the statistical properties of the GFE estimator in an asymptotic framework where both N and T tend to infinity. It also considers several extensions including models with additive time-invariant fixed-effects and group-specific coefficients."
  },
  {
    "qid": "econ-empirical-206-3-0-0",
    "question": "1) Derive the number of parameters in the semiparametric model given by $(8+L)S-(L+2)$. Explain the role of $L$ and $S$ in this expression.",
    "gold_answer": "1. **Parameters in the model**: The model includes:\n   - $P_{si}$: 2 parameters ($p_{s0}, p_{s1}$)\n   - $G_{sit}$: 2 parameters ($g_{s0}, g_{s1}$)\n   - $H_{sit}$: 2 parameters ($h_{s0}, h_{s1}$)\n   - $\\theta_{si}$: $L+2$ parameters ($d_{s0}, d_{s1l}$ for $l=0,\\dots,L$)\n   Total per support point $s$: $2 + 2 + 2 + (L+2) = 8 + L$.\n2. **Across $S$ support points**: Total parameters = $S(8+L)$.\n3. **Constraints**: The probabilities $\\theta_{si}$ must sum to 1, imposing $L+2$ restrictions.\n4. **Final count**: $S(8+L) - (L+2)$.\n$L$ determines the dependence structure of $\\theta_{si}$ on $x_{il}$, and $S$ is the number of support points.",
    "question_context": "The obvious example is a linear index model: \n$$\n\\begin{array}{r l}&{P_{s i}=F_{0}(p_{s0}+p_{s1}x_{i0})}\\ &{G_{s i t}=F(g_{s0}+g_{s1}x_{i t})}\\ &{H_{s i t}=F(h_{s0}+h_{s1}x_{i t})}\\ &{\\theta_{s i}=F_{\\theta}\\left(d_{s0}+\\displaystyle\\sum_{l=0}^{L}d_{s1l}x_{i l}\\right)}\\end{array}\n$$\nwhere $F_{0},F$ and $F_{\\theta}$ are known cdf functions, such as the standard normal cdf or the standard logistic function.\nThe number of parameters is now $(8+L)S-(L+2)$. It does not depend on the number of values $x_{i t}$ can take. This reduces the number of parameters to identify with respect to the nonparametric case without altering the number of different moment conditions, $r_{x i t}(T,N_{x})$.\n\nThe semiparametric model allows for maximal heterogeneity across individuals without imposing a specific functional form on the relationship between covariates and outcomes. It uses known cumulative distribution functions (CDFs) to model probabilities."
  },
  {
    "qid": "econ-empirical-1837-2-0-3",
    "question": "4) Discuss the implications of local risk-neutrality in EUT/GEUT for the experiment's design, given the small payoffs (¥230 to £7). Why might subjects exhibit risk-averse behavior despite this?",
    "gold_answer": "1. **Local Risk-Neutrality**: EUT/GEUT imply linear utility for small payoff ranges, predicting risk-neutrality. However, 30-60% of subjects chose the safe option ($S$), contradicting this.\n2. **Possible Explanations**: \n   - Subjects may evaluate payoffs relative to current wealth (e.g., students with low wealth).\n   - PT/RDT avoid this issue via probability weighting, which doesn’t require local risk-neutrality.\n   - Behavioral factors (e.g., loss aversion, mental accounting) may override theoretical predictions.",
    "question_context": "The first column of the table specifies the set and pair of questions concerned. The next four columns provide a breakdown of the results into each of the four possible responses to a pair of questions RR, SS, RS and SR, so that the number 27 in the first row of column 4, for example, indicates that 27 people chose the $\\pmb R$ option in question 1 and the $s$ option in question 2. Only two of these response categories are consistent with EUT (RR and SS: columns 2 and 3).\nColumn 6 is the number of EUT violations $(R S+S R)$ expressed as a proportion of the total sample and shows that on average $30-40\\%$ of behaviour violates EUT.\nThe null hypothesis will be that RS and SR responses which violate EUT (columns 4 and 5) occur with equal frequency. A FO violation is observed when the $s R$ category (column 5) constitutes significantly more than $50\\%$ of EUT violations for any pair of questions. On the other hand, a FI violation is observed when the $\\pmb{R S}$ category (column 4) constitutes significantly more than $50\\%$ of the observed violations.\nColumn 7 is the proportion of EUT violations occurring in the direction predicted by fanning-out and column 8 gives a test statistic Z. A starred $z$ value indicates a rejection of the null at the $5\\%$ level of significance and the sign of $z$ indicates the direction of violation: a positive value indicates that the majority violation is consistent with FO.\nThese results offer no support to GEUT which predicts universal FO. Inspecting column 8 indicates that in 10 of the 13 pairwise comparisons, the majority of violations are consistent with FI and 8 of these are significant. There are only three cases where the majority violation is consistent with FO but none are significant.\nThere is a greater degree of support for PT and RDT. In contrast to GEUT and EUT both of these theories predict VF1. In all three vertical comparisons in the triangle the majority of violation is consistent with FI (the null hypothesis is decisively rejected in two cases).\nOne possible criticism of this experiment is that the pay-offs used were quite small (between $\\yen230$ and f7). This raises at least two potential difficulties. First, in EUT and GEUT, preferences are determined purely by the form of the relevant utility function. Since any smooth function tends towards linearity as we consider smaller and smaller segments, theories like this must entail that individuals will exhibit risk-neutral behaviour in choices where the pay-offs are all of a very similar magnitude. This property is sometimes called local risk-neutrality.\nThe RLIS is a device which has been widely used by experimenters as it has the twin advantages of enconomising on payments to participants while controlling for any wealth effects which might otherwise arise if subjects were paid according to their performance on each of a number of different tasks.\n\nThe results analyze pairwise comparisons of responses to questions under the Expected Utility Theory (EUT) framework, focusing on violations and systematic biases. The sample size is 124, with responses categorized into RR, SS, RS, and SR. EUT violations are quantified, and alternative theories like Generalized Expected Utility Theory (GEUT), Prospect Theory (PT), and Rank-Dependent Theory (RDT) are evaluated."
  },
  {
    "qid": "econ-empirical-879-1-1-1",
    "question": "6) Explain why mixed-strategy equilibria are ruled out in Lemma 1(ii).",
    "gold_answer": "**Reason**:\n- In a mixed-strategy equilibrium, firm $j$ randomizes bids to make firm $i$ indifferent between two bids for a given $k_i$.\n- However, this indifference cannot hold for all $k_i$ because profit trade-offs depend on $k_i$ (via quantity/price effects).\n- Thus, only pure-strategy equilibria exist.",
    "question_context": "LEMMA 1. If $\\overline{k} \\leq \\theta$, (i) capacity withholding is never optimal, $q_{i}^{*}(k_{i})=k_{i}$; (ii) all Bayesian Nash equilibria must be in pure strategies; (iii) the optimal price offer $b_{i}^{*}(k_{i})$ must be strictly decreasing in $k_{i}$.\n\nThis section analyzes the uniform-price auction equilibrium when firms' capacities are small ($\\overline{k} \\leq \\theta$). Key properties include no capacity withholding and strictly decreasing bid functions."
  },
  {
    "qid": "econ-empirical-861-4-0-1",
    "question": "2) Show that $\\overline{MSE} - \\mu = o_p(1)$, where $\\mu = \\mu_0 - 2\\mu_1$, and interpret the terms $\\mu_0$ and $\\mu_1$ in the context of MSE decomposition.",
    "gold_answer": "1. **Decomposition**: The difference $\\overline{MSE} - \\mu$ is expressed as:\n   $$\\overline{MSE} - \\mu = n^{-1}\\sum_{t=1}^n \\left[\\tilde{\\varphi}_n^2(X_t) + \\tilde{f}_n^2(X_t)r^2(X_t)\\right]f_n^{-2}(X_t)I_t - \\text{cross-terms}.$$\n2. **Asymptotic Negligibility**: By the uniform convergence rates and trimming conditions, each term in the decomposition is $o_p(1)$. For example, $\\sup|\\tilde{\\varphi}_n(x)| = o_p(b_n)$ implies $n^{-1}\\sum \\tilde{\\varphi}_n^2(X_t)I_t = o_p(1)$.\n3. **Interpretation**: $\\mu_0 = n^{-1}\\sum U_i^2 I_i$ estimates the irreducible error $E[(Y-r)^2]$, while $\\mu_1$ captures the bias from estimating $r(\\cdot)$ and $f(\\cdot)$. The term $-2\\mu_1$ adjusts for overfitting.",
    "question_context": "Define ${\\widetilde{f}}_{n}(x)=f_{n}(x)-f(x)$ and $\\tilde{\\varphi}_{n}(x)=\\varphi_{n}(x)-\\varphi(x)$. Define also $U_{\\iota}=Y_{\\iota}-r(X_{\\iota})$ and $I_{\\iota}=I[f(X_{\\iota})>b_{n}]$. From Bierens (1987, Section 2.3) and Assumptions 1,2,3, we have $$\\begin{array}{r l r}{\\lefteqn{\\operatorname*{sup}_{x\\in R^{p}}\\vert\\tilde{f_{n}}(x)\\vert=O_{p}(\\sqrt{n}h_{n}^{p})^{-1}+O_{p}(h_{n}^{(m+q)})}}&{\\mathrm{and}}&{\\operatorname*{sup}_{x\\in R^{p}}\\vert\\tilde{\\varphi}_{n}(x)\\vert=O_{p}(\\sqrt{n}h_{n}^{p})^{-1}+O_{p}(h_{n}^{(m+q)}).}\\end{array}$$\nFrom the assumptions on $b_{n}$, we obtain $$\\begin{array}{r l}&{b_{n}^{-1}\\underset{x\\in R^{p}}{\\operatorname*{sup}}\\vert\\tilde{f_{n}}(x)\\vert\\overset{p}{\\to}0\\quad\\quad\\mathrm{and}}&{b_{n}^{-1}\\underset{x\\in R^{p}}{\\operatorname*{sup}}\\vert\\tilde{\\varphi_{n}}(x)\\vert\\overset{p}{\\to}0.}\\end{array}$$\nLet $\\mu_{0}=n^{-1}\\sum_{\\iota=1}^{n}U_{\\iota}^{2}I_{\\iota};$ $\\mu_{1}=n^{-1}\\Sigma_{i=1}^{n}U_{i}[\\varphi_{n}(X_{i})-r(X_{i})f_{n}(X_{i})]f^{-1}(X_{\\iota})I_{\\iota}$ $\\mu=\\mu_{0}-2\\mu_{1}$ and $\\overline{{M S E}}=$ $n^{-1}\\Sigma_{\\iota=1}^{n}[Y_{\\iota}-r_{n}(X_{\\iota})]^{2}I_{i}$. We have $$\\begin{array}{l}{{\\displaystyle\\overline{{{\\cal M S E}}}-\\mu=n^{-1}\\sum_{t=1}^{n}\\{\\tilde{\\varphi}_{n}^{2}(X_{t})+\\tilde{f}_{n}^{2}(X_{t})r^{2}(X_{t})\\}f_{n}^{-2}(X_{t}){\\cal I}_{t}}}\\\\{{\\displaystyle\\qquad-2n^{-1}\\sum_{i=1}^{n}\\tilde{\\varphi}_{n}(X_{t})\\tilde{f}_{n}(X_{t})f_{n}^{-2}(X_{t})r(X_{t}){\\cal I}_{t}}}\\\\{{\\displaystyle\\qquad-2n^{-1}\\sum_{t=1}^{n}\\tilde{f}_{n}^{2}(X_{t})f_{n}^{-1}(X_{t})f^{-1}(X_{t}){\\cal U}_{t}r(X_{t}){\\cal I}_{t}}}\\\\{{\\displaystyle\\qquad+2n^{-1}\\sum_{t=1}^{n}\\tilde{\\varphi}_{n}(X_{t})\\tilde{f}_{n}(X_{t})f_{n}^{-1}(X_{t})f^{-1}(X_{t}){\\cal U}_{t}{\\cal I}_{t}.}}\\end{array}$$\n\nThis section establishes the consistency of the mean squared error (MSE) estimator under given assumptions, using kernel smoothing techniques and trimming conditions."
  },
  {
    "qid": "econ-empirical-672-0-0-1",
    "question": "2) Formulate a labor market equilibrium model that explains why real wage moderation in the U.S. led to employment growth while Europe experienced productivity-driven output growth without significant employment increases.",
    "gold_answer": "1. U.S. labor market:\n   - Wage moderation shifts labor demand: \\( W/P = MP_L = (1-\\alpha)A(K/L)^\\alpha \\)\n   - Lower real wages increase \\( L^* \\) along the labor supply curve\n2. Europe's rigid labor market:\n   - Institutional constraints fix \\( W/P \\) above equilibrium\n   - Firms substitute capital for labor: \\( K/L \\) rises, increasing productivity",
    "question_context": "The significant thing about the last four years is not so much that aggregate demand and output in Europe have risen rather slowly as inflation has fallen, but that output increase has been associated almost wholly with productivity increase rather than with employment increase - in strong contrast to the United States, where, against a background of product real wage moderation, growth has created employment rather than productivity rise.\n\nThe text discusses the contrasting trends in employment and productivity between Europe and the United States over a recent four-year period. In Europe, output growth has been primarily driven by productivity increases with minimal employment growth, whereas the United States has experienced employment growth alongside real wage moderation."
  },
  {
    "qid": "econ-empirical-93-0-0-0",
    "question": "1) Derive the condition under which a country converges to the world technology frontier growth rate in the Schumpeterian growth model with imperfect creditor protection. Highlight the role of financial development in this condition.",
    "gold_answer": "The condition for convergence is derived from the credit constraint faced by innovators. The key equation is the incentive-compatibility constraint: \n\n\\[ \\mu\\beta\\pi\\bar{A}_{t+1} - c N_{t} \\leq \\mu\\beta\\pi\\bar{A}_{t+1} - \\mu R \\cdot (N_{t} - w_{t}) \\]\n\nSolving this yields the investment limit: \n\n\\[ N_{t} \\leq \\frac{1 + r}{1 + r - c} w_{t} = \\nu w_{t} \\]\n\nwhere \\( \\nu \\) is the credit multiplier. Countries with \\( \\nu \\) above a critical threshold can invest sufficiently to converge to the frontier growth rate.",
    "question_context": "We introduce imperfect creditor protection in a multicountry Schumpeterian growth model. The theory predicts that any country with more than some critical level of financial development will converge to the growth rate of the world technology frontier, and that all other countries will have a strictly lower long-run growth rate.\nThe model implies a form of club convergence consistent with the broad facts outlined above. In the theory, countries above some threshold level of financial development will all converge to the same long-run growth rate, and all other countries will have strictly lower long-run growth rates.\n\nThe paper introduces imperfect creditor protection in a multicountry Schumpeterian growth model, predicting convergence to the world technology frontier growth rate for countries above a critical level of financial development, and divergence for others. It presents empirical evidence supporting these predictions through cross-country growth regressions."
  },
  {
    "qid": "econ-empirical-1173-3-0-0",
    "question": "1) Explain the limitations of using an ordered logit model for estimating regression coefficients in the context of athletic participation and earnings, and how the nonparametric method addresses these limitations.",
    "gold_answer": "1. **Ordered Logit Limitations**: \n   - Requires restrictive assumptions (e.g., proportional odds). \n   - Limits marginal effects to self-determined calculations. \n   - Difficult to investigate returns for specific occupations. \n2. **Nonparametric Advantages**: \n   - Flexible interpretation of coefficients. \n   - No need for restrictive distributional assumptions. \n   - Allows for heterogeneous effects across individuals.",
    "question_context": "When encountering a situation in which a regression must be estimated using a limited dependent variable, econometricians often use an ordered logit model. However, using this method not only requires several restrictive assumptions, but it also limits discussion to calculating self-determined marginal effects, and makes it difficult to investigate the returns to athletic participation for specific occupations. The nonparametric method we use allows for a more straightforward and flexible interpretation of the regression coefficients estimated.\nGiven the number of parameters obtained from the Generalized Kernel Estimation procedure, it is tricky to present results. Unfortunately no widely accepted presentation format exists. Therefore, in Figure 2, we give the mean, and the 25th, 50th, and 75th percentile along with their respective bootstrapped standard errors (labeled Quartile 1, 2, and 3), as well as a kernel density plot of the coefficients for the athletic participation variable for each athlete included in the data set (by definition, the athletic participation coefficient for all nonathletes is zero). Each coefficient represents the impact on earnings (category) for a one unit increase of the associated independent variable (in other words, ATHLETE going from O to 1).\nOne consideration is important in interpreting the ATHLETE coefficient. For the coefficient to be unbiased, ATHLETE must be truly exogenous-—-implying athletic status must be randomly assigned. However, it is possible students become athletes because they are innately motivated and disciplined-qualities that are unobservable but positively correlated with earnings (Duncan and Dunifon 1998). If this is the case, athletes may earn more not because universities provide value-added, but because better students become athletes.\nThe mean of the coefficients for the ATHLETE variable, 0.028, indicates that former college athletes are in a 0.028 higher earnings category than nonathletes, ceteris paribus. Because most income categories represent a $5,000 wage gap, this coefficient can be interpreted as approximately a $140 wage benefit.Although qualitatively similar, this is smaller than the 4 percent premium reported by LC. But the variation in individual wage premiums is more interesting. Less than half the college athletes actually receive a positive gain. The median of the coefficients (Q2) is negative, implying a skewed distribution with more than half of former college athletes actually earning lower wages than nonathletes, ceteris paribus.\n\nThe text discusses the use of a nonparametric method for estimating regression coefficients in the context of athletic participation and earnings, highlighting the limitations of traditional ordered logit models and the advantages of the nonparametric approach."
  },
  {
    "qid": "econ-empirical-634-8-1-2",
    "question": "3) Formulate a model to assess the welfare implications of the fixed-link duopoly policy versus a more competitive market structure.",
    "gold_answer": "Let \\( W = CS + PS \\) where \\( CS \\) is consumer surplus and \\( PS \\) is producer surplus. Under duopoly, \\( W_{duo} = CS_{duo} + PS_{duo} \\). Under competition, \\( W_{comp} = CS_{comp} + PS_{comp} \\). Compare \\( W_{duo} \\) and \\( W_{comp} \\) under assumptions about demand elasticity and cost structures.",
    "question_context": "The authors present a detailed account of recent telecommunications policy in the UK, focusing in particular on the period since the privatisation of BT in 1984.\nTheir thesis is that policy has been insufficiently pro-competitive, and unduly ready to rely on regulation, not the market, to deal with high profits, poor service, and monopoly abuses.\nThe main target of their criticisms is the 'fixed-link duopoly policy' involving the government, OFTEL, Mercury, and BT.\n\nThe text critiques UK telecommunications policy post-BT privatization, arguing for more competition and less regulation."
  },
  {
    "qid": "econ-empirical-882-0-0-3",
    "question": "4) Derive the RMSE metric used in Fig. 11 and explain why it remains below \\(0.07^{\\circ}C\\) across 200 parameter combinations of AOGCMs and carbon cycle models.",
    "gold_answer": "1. **RMSE Definition**: \\(\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (T_i^{\\text{MAGICC}} - T_i^{\\text{emulator}})^2}\\).  \n2. **Orthogonal Scenario Advantage**: Uncorrelated inputs ensure the emulator's Jacobian \\(J\\) is well-conditioned, limiting error propagation: \\(\\|\\Delta T\\| \\approx \\|J\\| \\cdot \\|\\Delta \\theta\\|\\).  \n3. **Parameter Robustness**: For each combination \\((\\theta_j, \\phi_k)\\), the emulator's linearity in parameters (e.g., \\(T = \\beta_0 + \\beta_1 E + \\beta_2 T_{\\text{lag}}\\)) ensures RMSE stability. Fig. 11 shows \\(\\text{RMSE} \\approx 0.04^{\\circ}C\\) (mean) due to orthogonality.",
    "question_context": "We propose a general emulation method for constructing low-dimensional approximations of complex dynamic climate models. Our method uses artificially designed uncorrelated CO₂ emissions scenarios, which are much better suited for the construction of an emulator than are conventional emissions scenarios.\nThe basic procedure for constructing a reduced model (often called an emulator) is to specify a set of emissions paths, use each one as input into a complex climate model, observe the resulting temperature paths, and use these simulated emissions and temperature data to specify an approximating dynamic system.\nWe construct uncorrelated CO₂ emissions scenarios, and show that, when used as input for simulations, they prove more efficient than conventional scenarios. In the demonstrative case of emulating a climate model, forecast errors decrease by almost half when we use uncorrelated scenarios.\n\nThe paper proposes a general emulation method for constructing low-dimensional approximations of complex dynamic climate models using orthogonal polynomials to design uncorrelated CO₂ emissions scenarios."
  },
  {
    "qid": "econ-empirical-1552-2-1-1",
    "question": "4) Using the cohort decomposition in Panel B, test the hypothesis that the treatment effect differs between the 1971–1973 and 1974–1977 cohorts. Construct a Wald statistic and interpret the results.",
    "gold_answer": "Wald test steps:\n1. Estimate unrestricted model with separate interactions: $Y_i = \\beta_0 + \\beta_1 \\text{East}_i + \\beta_2 \\text{Treated}_i + \\beta_3 (\\text{East}_i \\times \\text{Treated}_i \\times \\text{Cohort71-73}_i) + \\beta_4 (\\text{East}_i \\times \\text{Treated}_i \\times \\text{Cohort74-77}_i) + \\epsilon_i$.\n2. Restricted model imposes $\\beta_3 = \\beta_4$.\n3. Compute $W = (\\hat{\\beta}_3 - \\hat{\\beta}_4)^2 / (\\text{Var}(\\hat{\\beta}_3) + \\text{Var}(\\hat{\\beta}_4) - 2\\text{Cov}(\\hat{\\beta}_3, \\hat{\\beta}_4))$.\n4. Compare to $\\chi^2_1$: Results show no significant difference (both coefficients ≈ 0.02, similar standard errors).",
    "question_context": "The coefficient of main interest on the interaction term between East and Treatment is positive and significant, indicating that being enrolled at an older age in the East, and thereby receiving one year less of socialist education, increases the probability of attaining a college degree by 2.1 (column 1) to 2 (column 2) percentage points more than being enrolled at an older age in the West.\nPanel B of table 2 shows the results of specification (2) and decomposes the cohorts used in the analysis into the two cohort groups, 1971 to 1973 and 1974 to 1977, first using the full sample (column 2), and then splitting the sample into men (column 3) and women (column 4).\n\nThe analysis examines the impact of reunification on college graduation rates, using linear regressions with interaction terms between East/West residency and treatment status. Key findings include a significant positive effect of treatment (later school enrollment) on college graduation in the East, robust across cohorts and specifications."
  },
  {
    "qid": "econ-empirical-158-1-0-3",
    "question": "4) Using Table V, compute the difference in average episode length between individuals aged 25-44 and 45-64, and interpret the result in the context of health status and income.",
    "gold_answer": "From Table V: \\( E(T|25\\text{-}44) = 7.01 \\) days vs. \\( E(T|45\\text{-}64) = 8.52 \\) days. The difference of 1.51 days may reflect poorer health status and higher income (associated with longer episodes) in the older group, as per Table II.",
    "question_context": "The sample selection criteria are detailed in Table I.\nThe illnesses of individuals in the ill sample are acute conditions according to the International Classification of Diseases (ICD-9) and the condition code reported in the NMES data.\nThe acute conditions considered are infectious and parasitic diseases and respiratory conditions which include, for example, strep throat, viral infection, influenza, and the common cold.\nThese acute conditions account for almost 60% of all acute conditions (Adams and Benson (1992)).\nBecause the computational complexity associated with solving the dynamic programming problem depends on the length of the decision-making behavior, the ill sample excludes individuals with episodes that number 22 or more days.\n\nThe sample used to estimate the model of episodic behavior consists of males age 25 to 64 who reported being employed (but not self employed) during each of the interview rounds. The 'full sample' consists of 3797 males and includes both ill and well individuals. Individuals from the full sample who are observed to have an episode of acute illness make up the 'ill sample.' Estimation results are based on all individuals from the full sample, regardless of whether or not they are observed to have an acute illness episode."
  },
  {
    "qid": "econ-empirical-72-3-0-2",
    "question": "3) Formally model the relationship between tenure and nondefault participation rate, given that the rate increases from 10% at one month to 35% at one year and 50% at two years.",
    "gold_answer": "Let \\( r(t) \\) be the nondefault participation rate as a function of tenure \\( t \\) (in years).\n1. Data points: \\( r(1/12) = 10\\% \\), \\( r(1) = 35\\% \\), \\( r(2) = 50\\% \\).\n2. A possible model is logistic growth: \\( r(t) = \\frac{L}{1 + e^{-k(t-t_0)}}} \\), where \\( L \\) is the maximum rate, \\( k \\) is the growth rate, and \\( t_0 \\) is the inflection point.\n3. Fit parameters to match the observed data.",
    "question_context": "The overall default rate for the NEW cohort is 61 percent: six out of ten employees do nothing to change their savings behavior from the default specified by the company if no other action is taken.\nConditional on participation, 71 percent of the NEW cohort 401(k) participants are at the default contribution rate and fund allocation.\nOverall, the nondefault participation rate of the NEW cohort is 25 percent.\nMen have a lower default rate than women, older employees have a lower default rate than younger employees, and the default rate declines quite significantly with compensation.\nThe nondefault participation rates of the various demographic groups are highly correlated with the 401(k) participation rates of the WINDOW cohort (the correlation coefficient is 0.94).\nData from March 31, 2000, show a nondefault 401(k) participation rate of about 10 percent for employees with only one month of tenure. This increases quite substantially to about 35 percent for employees with one year of tenure, and to 50 percent for employees with two years of tenure.\nAlmost half of nondefault participants still have all of their balances invested in only one fund, a fraction much higher than that for any of the cohorts hired before automatic enrollment.\n\nThe text discusses the impact of automatic enrollment on 401(k) savings behavior, highlighting participant inertia and the prevalence of default savings choices among employees."
  },
  {
    "qid": "econ-empirical-1150-0-1-0",
    "question": "3) Derive the likelihood function for the logistic hazard model in Equation (1), accounting for right-censoring. Show how duration dependence enters via \\( g(\\cdot) \\).",
    "gold_answer": "1. Likelihood for individual \\( i \\): \\( L_i = [H(t_i)]^{d_i} [1-H(t_i)]^{1-d_i} \\prod_{k=1}^{t_i-1} (1-H(k)) \\), where \\( d_i \\) is divorce indicator.\n2. With \\( H(t) = \\frac{1}{1+e^{-X\\beta}} \\), log-likelihood becomes:\n   \\( \\ell = \\sum_i d_i X\\beta - \\log(1+e^{X\\beta}) + \\sum_{k=1}^{t_i-1} \\log(1-\\frac{1}{1+e^{-X\\beta}}) \\).\n3. \\( g(\\cdot) \\) enters through \\( X\\beta \\) as piecewise constant dummies.",
    "question_context": "The analysis uses a discrete-time hazard model: \\( H(t) = F[\\beta_0 + \\beta_1 \\text{FractionFemale\\_INDOCC} + ... + \\gamma_2 IND_{nt} + \\lambda Year_t + g(\\text{MarriageDuration})] \\)\n\nThe paper employs a discrete-time hazard model using 1990 Census and NLSY79 data, addressing endogeneity through fixed effects and instrumental variables."
  },
  {
    "qid": "econ-empirical-1795-0-1-2",
    "question": "7) How does the paper address endogeneity concerns in estimating the effect of financial development on convergence?",
    "gold_answer": "1. Uses the same instruments as Levine et al. (2000): legal origin and creditor rights.\n2. Tests robustness to alternative conditioning sets, outliers, and FD measures.\n3. Shows that results hold when controlling for potential omitted variables like schooling and institutions.",
    "question_context": "We add to their regression an interaction term between the log of initial per-capita GDP (relative to the United States) and financial development, and interpret a negative coefficient as evidence that low financial development makes convergence less likely.\nOur empirical results suggest that financial development is among the most powerful of these forces, especially considering that educational attainment, initial relative output, and a large number of other candidate variables do not have an analogous effect when included in the same regression with financial development.\n\nThe paper presents cross-country regression evidence supporting the model's predictions, focusing on the interaction between financial development and initial GDP."
  },
  {
    "qid": "econ-empirical-141-3-1-1",
    "question": "2) Explain the role of the spectral density matrix eigenvalues in determining $q$ and $\\tau$, as outlined in the text.",
    "gold_answer": "2. **Role of Eigenvalues**: \n   - The $q$ largest eigenvalues of $\\Sigma^{\\varDelta x}(\\theta)$ diverge linearly in $n$ at all frequencies, while the remaining $n-q$ stay bounded.\n   - At frequency $\\theta=0$, only the $\\tau$ largest eigenvalues diverge linearly in $n$, allowing separation of permanent ($\\tau$) and transitory ($q-\\tau$) shocks.",
    "question_context": "The values of $q$ and $\\tau$ can be determined by analyzing the behavior of the eigenvalues of the spectral density matrix. In particular, let $\\widehat{\\mathbf{r}}_{k}$ be the $n\\times n$ sample lag $k$ autocovariance matrix of the differenced data $\\varDelta\\mathbf{y}_{t}$ and consider the lag-window estimator of the spectral density matrix of $\\varDelta\\mathbf{y}_{t}$: $$\\widehat{\\pmb{\\Sigma}}^{\\varDelta y}(\\theta)=\\frac{1}{2\\pi}\\sum_{k=-B_{T}}^{B_{T}}\\left(1-\\frac{|k|}{B_{T}}\\right)\\widehat{\\mathbf{T}}_{k}e^{-i k\\theta}$$ where $B_{T}$ is a suitable bandwidth.\nProposition 3 (Consistency of Impulse–Response Functions based on VAR). Define $\\zeta_{n T,\\eta}=\\operatorname*{max}\\left(n^{-(1-\\eta)},n^{-1/2},T^{-1/2}\\right)$. Let Assumption 1 through 5 hold. Then, as $n, T\\to\\infty$, $\\mathsf{P}(\\widehat{q}=q)\\to1$ and $\\mathsf{P}(\\widehat{\\tau}=\\tau)\\to1$.\n\nThis section addresses the practical challenge of determining the number of common factors ($r$), common shocks ($q$), and common permanent shocks ($\\tau$) in non-stationary DFMs. It proposes methods based on spectral density matrix eigenvalues."
  },
  {
    "qid": "econ-empirical-71-5-1-0",
    "question": "3) Formalize a test for rational expectations using the forecast error \\( e_{t} = y_{t} - \\hat{y}_{t} \\), where \\( y_{t} \\) is realized GDP growth and \\( \\hat{y}_{t} \\) is the forecast. Show how household debt \\( \\Delta D_{t-4 \to t-1} \\) should enter the regression under the null of rational expectations.",
    "gold_answer": "1. Under rational expectations, \\( e_{t} \\) should be orthogonal to \\( \\Delta D_{t-4 \to t-1} \\).\n2. Regression: \\( e_{t} = \\alpha + \beta \\Delta D_{t-4 \to t-1} + \\epsilon_{t} \\).\n3. Null hypothesis: \\( \beta = 0 \\).\n4. Rejection of the null (as in Table VIII) implies forecasters ignore debt-driven risks.",
    "question_context": "Panels B and D of Figure VI confirm this result by replacing the growth forecast of the IMF and OECD with the forecast error. The forecast error is defined as the difference between realized and forecasted growth.\nTable VIII shows that the rise in household debt from four years ago to last year predicts forecasting errors.\n\nThis section examines whether forecast errors in GDP growth following household debt booms are consistent with rational expectations or behavioral biases."
  },
  {
    "qid": "econ-empirical-209-1-0-1",
    "question": "2) Using the model (14) for $u_t$, derive the expression for the minimum MSE forecast $\\tilde{u}_n(l)$ and explain its components.",
    "gold_answer": "The minimum MSE forecast $\\tilde{u}_n(l)$ is given by:\n\\[\n\\tilde{u}_n(l) = W_1(B)w_{n+l}^* + W_2(B)a_{n+l}^*\n\\]\nwhere:\n1. $W_1(B)w_{n+l}^*$ represents the contribution of the innovations $w_t$ from $x_t$.\n2. $W_2(B)a_{n+l}^*$ represents the contribution of the innovations $a_t$ from $y_t$.\nThe forecast combines these orthogonal components to minimize the MSE.",
    "question_context": "Necessary and sufficient conditions for $v_{1}(l)$ to be greater than $v_{2}(l)$ (for $x$ to be of value in predicting y) can be obtained from an analysis of the process $(x_{t},y_{t})$ in two stages [see Parzen (1969) and Haugh (1972)], (1) the univariate processes (5) and (8) for $x$ and y, and (2) the joint model for the univariate innovations $u_{t}\\operatorname{of}y$.\nTheorem 1. Single-period predictions of y from the distributed lag model (4) exploiting its relationship with an independent variable $x$ have smaller MSE than those from the model (8) incorporating only the past history of y, if any, and only if all, of the following equivalent conditions hold: (a) $W_{1}(B)$, in the model (14) for the innovations $u_{t}$ of $y$, is not identically 1 or zero. (b) It is not the case that $\\chi(B)=\\psi(B)$ in (7); and neither of the components of $u_{t}$ in (14) vanishes. (c) For some $k>0$, $\\rho_{w u}(k)\\neq0$.\n\nThis section discusses the conditions under which an indicator can be considered a leading indicator, focusing on the predictive value of variable x for y."
  },
  {
    "qid": "econ-empirical-221-2-0-1",
    "question": "2) Prove Theorem 4.1, showing that the IPW M-estimator is consistent for $\\theta_{\\mathrm{o}}$ under Assumption 4.1, even with a misspecified selection probability model.",
    "gold_answer": "1. By Assumption 4.1, $\\theta_{\\mathrm{o}}$ uniquely minimizes (4.1). \\n2. The IPW M-estimator minimizes the sample analog of (4.1): $N^{-1}\\sum_{i=1}^{N}[s_{i}/G(z_{i},\\hat{\\gamma})]q(w_{i},\\theta)$. \\n3. By uniform convergence and the continuity of the objective function, the estimator converges to the minimizer of the population objective function, which is $\\theta_{\\mathrm{o}}$.",
    "question_context": "Assumption 4.1. For $z$ defined in Assumption 3.1, and under parts (i), (ii), and (iv) of that assumption, $\\theta_{\\mathrm{o}}\\in\\Theta$ solves the problem $\\operatorname*{min}_{\\theta\\in\\Theta}\\operatorname{E}[q(w,\\theta)|z]$ for all $z\\in{Z}$.\nThe objective function for the weighted M-estimator in (3.4) now converges in probability uniformly to $\\operatorname{E}\\{[s_{i}/G(z_{i},\\gamma^{*})]q(w_{i},\\theta)\\}$, where $\\gamma^{*}$ denotes the plim of $\\hat{\\gamma}$ and $G(z_{i},\\gamma^{*})$ is not necessarily $p(z_{i})=\\mathsf{P}(s_{i}=1|z_{i})$.\nTheorem 4.1. Under Assumption 4.1, let $G(z,\\gamma)>0$ be a parametric model for $P(s=1|z)$, and let $\\hat{\\gamma}$ be any estimator such that $\\mathrm{plim}(\\hat{\\gamma})=\\gamma^{*}.$ for some $\\gamma^{*}\\in{\\cal{I}}$. In addition, assume that $\\theta_{\\mathrm{o}}$ is the unique minimizer of (4.1) over $\\boldsymbol{\\Theta}$, and assume the regularity conditions in Wooldridge (2002a, Theorem 5.1). Then the IPW M-estimator based on the possibly misspecified selection probabilities, $G(z_{i},\\hat{\\gamma})$, is consistent for $\\theta_{\\mathrm{o}}$.\nTheorem 4.2. Under Assumption 4.1, let $G(z,\\gamma)>0$ be a parametric model for $\\mathrm{P}(s=1|z)$, and let $\\hat{\\gamma}$ be any estimator such that $\\sqrt{N}(\\hat{\\gamma}-\\gamma^{*})=\\mathrm{O}_{\\mathrm{p}}(1)$ for some $\\gamma^{*}\\in{\\cal{I}}$. Assume that $q(w,\\theta)$ satisfies the regularity conditions from Theorem 3.1. Further, assume that $\\operatorname{E}[r(w_{i},\\theta_{\\mathrm{o}})|z_{i}]=0$. Let $\\hat{\\theta}_{\\mathrm{w}}$ denote the weighted $M$-estimator based on the estimated sampling probabilities $G(z_{i},\\hat{\\gamma})$, and let $\\tilde{\\theta}_{\\mathrm{w}}$ denote the weighted $M$-estimator based on $G(z_{i},\\gamma^{*})$. Then $\\mathrm{Avar}\\sqrt{N}(\\hat{{\\boldsymbol\\theta}}_{\\mathrm{w}}-{\\boldsymbol\\theta}_{0})=\\mathrm{Avar}\\sqrt{N}(\\tilde{{\\boldsymbol\\theta}}_{\\mathrm{w}}-{\\boldsymbol\\theta}_{0})=A_{\\mathrm{o}}^{-1}\\mathrm{E}(k_{i}k_{i}^{\\prime})A_{\\mathrm{o}}^{-1}$.\nTheorem 4.3. Let the assumptions of Theorem 4.2 hold. As before, let $p(z)=P(s=1|z)$, and, as a shorthand, write $G_{i}=G(z_{i},\\gamma^{*})$. Further, assume that the GCIME holds for the objective function $q(w,\\theta)$ in the population. Namely, for some $\\sigma_{\\mathrm{o}}^{2}>0$, $\\mathrm{E}[\\nabla_{\\theta}q(w,\\theta_{\\mathrm{o}})^{\\prime}\\nabla_{\\theta}q(w,\\theta_{\\mathrm{o}})|z]=\\sigma_{\\mathrm{o}}^{2}\\mathrm{E}[\\nabla_{\\theta}^{2}q(w,\\theta_{\\mathrm{o}})|z]\\equiv\\sigma_{\\mathrm{o}}^{2}J(z,\\theta_{\\mathrm{o}})$. Then $\\mathrm{Avar}\\sqrt{N}(\\hat{\\theta}_{\\mathrm{u}}-\\theta_{\\mathrm{o}})=\\sigma_{\\mathrm{o}}^{2}[E(p_{i}J_{i})]^{-1}$ and $\\mathrm{Avar}\\sqrt{N}(\\hat{\\theta}_{\\mathrm{w}}-\\theta_{\\mathrm{o}})-\\mathrm{Avar}\\sqrt{N}(\\hat{\\theta}_{\\mathrm{u}}-\\theta_{\\mathrm{o}})$ is positive semi-definite.\n\nThis section discusses estimation under exogenous selection, where certain kinds of sample selection do not cause bias in standard, unweighted estimators. The focus is on robust estimation under misspecified selection probability models and the implications for weighted and unweighted M-estimators."
  },
  {
    "qid": "econ-empirical-206-2-1-2",
    "question": "3) Derive the probability of a path $j$ given covariates $X_i = X$ and type-specific parameters $(P_{xs}, G_{xs}, H_{xs})$.",
    "gold_answer": "1. The probability is given by:\n$$p_{js|X} = P_{x_{i0}s}^{y_0^j}(1-P_{x_{i0}s})^{1-y_0^j} \\times \\prod_x G_{xs}^{n_{01|x}^j}(1-G_{xs})^{n_{00|x}^j}H_{xs}^{n_{11|x}^j}(1-H_{xs})^{n_{10|x}^j}$$\n2. Where:\n   - $n_{01|x}^j$ counts $0 \\to 1$ transitions given $x_{it} = x$ for path $j$\n   - $n_{00|x}^j$ counts $0 \\to 0$ transitions given $x_{it} = x$\n   - $n_{11|x}^j$ counts $1 \\to 1$ transitions given $x_{it} = x$\n   - $n_{10|x}^j$ counts $1 \\to 0$ transitions given $x_{it} = x$",
    "question_context": "For a binary $x_{i}$ , the time homogeneous first order Markov model is fully characterized by: $$\\begin{array}{r l}&{P_{0i}=\\operatorname*{Pr}\\left(y_{i0}=1\\mid x_{i}=0\\right);\\qquad P_{1i}=\\operatorname*{Pr}\\left(y_{i0}=1\\mid x_{i}=1\\right)}\\ &{G_{0i}=\\operatorname*{Pr}\\left(y_{i t}=1\\mid y_{i,t-1}=0,x_{i}=0\\right);}\\ &{H_{0i}=\\operatorname*{Pr}\\left(y_{i t}=1\\mid y_{i,t-1}=1,x_{i}=0\\right)}\\ &{G_{1i}=\\operatorname*{Pr}\\left(y_{i t}=1\\mid y_{i,t-1}=0,x_{i}=1\\right);}\\ &{H_{1i}=\\operatorname*{Pr}\\left(y_{i t}=1\\mid y_{i,t-1}=1,x_{i}=1\\right).}\\end{array}$$\n\nThe model is extended to include covariates, with different assumptions about the relationship between unobserved heterogeneity and covariates."
  },
  {
    "qid": "econ-empirical-1549-0-0-3",
    "question": "4) Using the results from Table 3, analyze the empirical power of the η(1) test versus the DF GLS test for ρ = 0.9 and ρ = 0.95. What do these results imply about the trade-offs between the two tests?",
    "gold_answer": "1. For ρ = 0.9: η(1) power = 0.4023, DF GLS power = 0.6763. The DF GLS test has higher power. \n2. For ρ = 0.95: η(1) power = 0.1657, DF GLS power = 0.2843. Both tests have low power, but DF GLS still performs better. \n3. The η(1) test may be preferred when size distortions are a concern, even at the cost of lower power. \n4. The DF GLS test is more powerful but may suffer from size distortions in finite samples.",
    "question_context": "In this note we have proposed a new nonparametric unit root test for trending processes that is based on normalized level crossings. The test is a generalization of the one proposed in Burridge and Guerre (1996), allowing for trended processes and more general forms of autocorrelated disturbances. This new test has an asymptotic distribution that is a standard Rayleigh distribution.\nWe have also shown that the test has good finite-sample properties and presents, for some but not all parametrizations of the DGP, better size properties than the DF GLS test considered in Ng and Perron (2001). Thus, the η(1) test proposed in this note can be considered a good complement to other unit root tests.\nDGP: x_t = ρx_{t-1} + ε_t, ε_t ~ iid N(0,1); T = 200. For the DF GLS test, 5% critical values computed from 50,000 replications were used.\n\nThis note proposes a new nonparametric unit root test for trending processes based on normalized level crossings, generalizing the test by Burridge and Guerre (1996). The test accommodates trended processes and more general forms of autocorrelated disturbances, with an asymptotic distribution that is a standard Rayleigh distribution. The test demonstrates good finite-sample properties and, for some parametrizations, better size properties than the DF GLS test (Ng and Perron, 2001)."
  },
  {
    "qid": "econ-empirical-1287-3-0-2",
    "question": "3) Explain the intuition behind the GNDT principle and how it differs from the static case.",
    "gold_answer": "1. GNDT states that the contract becomes efficient as soon as the agent reports a high type.\n2. Intuition: Distortions are only needed to screen high types, so once a high type is revealed, no further distortions are necessary.\n3. In the static case, efficiency is only achieved for the highest type in a single period.\n4. In the dynamic case, efficiency \"invades\" future periods after a high-type report due to the Markovian structure.\n5. Key difference: Dynamic GNDT implies efficiency propagates forward, while static GNDT is limited to one period.",
    "question_context": "When types follow a Markov process, the contract instantly becomes efficient as soon as the agent reports himself to be a high type: but now efficiency “invades\" the histories in which the agent subsequently reports himself to be a low type. This is the generalized no distortion at the top (GNDT) principle.\nA distortion persists on the lowest branch of the history tree (i.e., when the agent always declares to be a low type). By a simple manipulation of the formula in Proposition 2, the distortion can be written as $\\theta_{L}-\\dot{q}_{t}^{*}(\\theta_{L}|h_{t})$ $=\\Delta\\theta(\\mu_{H}/\\mu_{L})X(\\theta_{t},X_{t-1})\\{1-[\\operatorname*{Pr}(\\theta_{L}\\mid\\theta_{H})/\\l]\\}$ $\\mathrm{Pr}(\\theta_{L}|\\theta_{L})\\mathrm{]}\\}^{t=1}$ , since the efficient level of output with a low type is $\\theta_{L}$ .\nPROPOSITION 3: For any discount factor $\\delta\\in(0,1)$ ,the optimal contract converges over time to an efficient contract along any possible history.\n\nThe text discusses the Generalized No Distortion at the Top (GNDT) and Vanishing Distortion at the Bottom (VDB) principles in dynamic contracting with Markovian types. It explains how distortions are introduced to extract surplus from higher types and how these distortions evolve over time."
  },
  {
    "qid": "econ-empirical-1426-2-1-0",
    "question": "5) Prove that the matrix $\\Lambda^{0} = \\text{diag}\\{\\lambda^{s_{a}^{0}-d_{a}^{0}}\\}$ ensures $U_{t}$ is covariance stationary when $s_{a}^{0}$ differences are applied, given $-\\frac{1}{2} \\leq s_{a}^{0} - d_{a}^{0} < \\frac{1}{2}$.",
    "gold_answer": "Proof outline:\n1. For each component $a$, the memory parameter after differencing is $d_{a}^{0} - s_{a}^{0}$.\n2. The condition $-\\frac{1}{2} \\leq d_{a}^{0} - s_{a}^{0} < \\frac{1}{2}$ guarantees short memory (stationarity).\n3. The spectral density $f_{U}(\\lambda)$ exists because $\\int_{-\\pi}^{\\pi} |\\lambda|^{2(s_{a}^{0}-d_{a}^{0})} d\\lambda < \\infty$ under the given bounds.",
    "question_context": "Consider a real-valued $N$-dimensional vector process $X_{t}$ with memory parameter vector $d^{0} = (d_{1}^{0},...,d_{N}^{0})$. Define $s_{a}^{0} = \\lfloor d_{a}^{0} + \\frac{1}{2}\\rfloor$ and the stationary process $U_{t} = \\text{diag}\\{\\Delta^{s_{a}^{0}}\\}X_{t}$ with spectral density matrix $f_{U}(\\lambda)$.\nThe spectral density satisfies $f(\\lambda) \\sim \\Lambda^{0} R^{0} \\Lambda^{0}$ as $\\lambda \\rightarrow 0^{+}$, where $\\Lambda^{0} = \\text{diag}\\{\\lambda^{s_{a}^{0}-d_{a}^{0}}\\}$ and $R^{0}$ is positive definite.\n\nThis section extends the univariate framework to vector processes with heterogeneous memory parameters, introducing covariance stationarity through fractional differencing."
  },
  {
    "qid": "econ-empirical-959-1-1-2",
    "question": "6) Explain the role of the quantile-quantile transform $Q_{dm}(y)$ in estimating quantile treatment effects.",
    "gold_answer": "1. The quantile-quantile transform $Q_{dm}(y) = F_{Y_1|D=d,M=m}^{-1} \\circ F_{Y_0|D=d,M=m}(y)$ maps an outcome value $y$ from the baseline period ($T=0$) to the corresponding value in the follow-up period ($T=1$) for a given treatment $d$ and mediator state $m$.\\n2. This transform accounts for changes in the outcome distribution over time, holding treatment and mediator states fixed.\\n3. It is used to align the distributions of potential outcomes across time periods, enabling the estimation of QTEs by comparing quantiles of the transformed distributions.\\n4. The transform ensures that comparisons are made at the same relative ranks of the outcome distribution, addressing potential distributional shifts unrelated to the treatment or mediator.",
    "question_context": "We denote by $F_{Y_{t}(d,m)}(y)=\\operatorname*{Pr}(Y_{t}(d,m)\\leq y)$ the cumulative distribution function of $Y_{t}(d,m)$ at outcome level $y$ Its inverse, $F_{Y_{\\epsilon}(d,m)}^{-1}(q)=\\operatorname*{inf}\\{\\gamma:F_{Y_{t}(d,m)}(\\gamma)\\geq q\\}$ , is the quantile function of $Y_{t}(d,m)$ at rank $q$.\nThe total QTE are denoted by $\\Delta_{1}(q)=F_{Y_{1}(1,M(1))}^{-1}(q)-F_{Y_{1}(0,M(0))}^{-1}(q)$.\nThe direct QTE are denoted by $\\theta_{1}(q,d)=F_{Y_{1}(1,M(d))}^{-1}(q)-F_{Y_{1}(0,M(d))}^{-1}(q)$, and the indirect quantile effects, denoted by $\\delta_{1}(q,d)=F_{Y_{1}(d,M(1))}^{-1}(q)-F_{Y_{1}(d,M(0))}^{-1}(q)$.\n\nThis section extends the analysis to quantile treatment effects (QTE), focusing on the distributional impacts of the treatment and mediator. The quantile functions and transformations are introduced to define direct and indirect QTEs, conditional on principal strata."
  },
  {
    "qid": "econ-empirical-1440-0-2-1",
    "question": "6) Why might it be desirable to ban the use of some available information in certain cases, according to the essay?",
    "gold_answer": "Banning information may be desirable if: \n1) The information is of low quality and increases horizontal inequity significantly. \n2) The reduction in vertical inequity is small compared to the increase in horizontal inequity. \n3) The costs of collecting the information outweigh the equity and efficiency gains. \n4) Societal preferences strongly favor minimizing horizontal inequity over vertical inequity.",
    "question_context": "The general analysis of Section IHI and the examples in Section IV and Appendix B indicate that better information generally reduces both S and $\\pmb{V}_{:}$ , but it may well increase $H.$ Improvements in information are more likely to reduce $H_{:}$ the more substantial the improvement and the better the initial information.\nIf new information reduces both $\\pmb{H}$ and $\\pmb{v}$ ,there is no equity loss under our normative assumptions from permitting its use. On the other hand, if information about buyer-specific costs is initially poor, the use of additional low-quality information may raise $\\pmb{H}$ while lowering V.\n\nThe essay concludes with policy implications, emphasizing the importance of initial information quality and the trade-offs between horizontal and vertical inequity when considering information suppression."
  },
  {
    "qid": "econ-empirical-169-2-0-2",
    "question": "3) Interpret the Kruskall-Wallis test result ($\\mathrm{p}=0.994$) for differences in expected punishment across treatments and castes.",
    "gold_answer": "1. The Kruskall-Wallis test compares distributions of expected punishment across groups.  \n2. $\\mathrm{p}=0.994$ implies no significant differences between treatments or castes.  \n3. Conclusion: The conditional cooperation norm is consistent across all experimental conditions.",
    "question_context": "Our data give us two measures of punishment for defection: $p_{d}$ measures the absolute punishment of defectors (i.e. norm violators) and $\\mathit{p_{d}}-\\mathit{p_{c}}$ measures the extent to which defectors are more strongly punished than cooperators. We denote $\\mathit{p_{d}}-\\mathit{p_{c}}$ as relative punishment.\nIf there is a widely shared belief in the existence of a normative obligation to reciprocate cooperation, the beliefs of player B should reflect this obligation. Figure 3 presents player B’s beliefs about punishment.\nOn average, in each treatment and for each caste, player B expected that C would spend nearly 10 coins to punish B if he defected, and would spend almost nothing on punishment if B cooperated.\n\nThe study measures punishment for defection using two metrics: absolute punishment ($p_{d}$) and relative punishment ($\\mathit{p_{d}}-\\mathit{p_{c}}$). Player B's beliefs about punishment are used to infer the existence of a conditional cooperation norm."
  },
  {
    "qid": "econ-empirical-956-0-1-1",
    "question": "4) Derive the instantaneous volatility matrix $\\pmb{c}_t$ for the continuous-time regression model $dY_t' = \\beta_t dX_t' + dU_t$.",
    "gold_answer": "Given $Z = (X^\\top, Y^\\top)^\\top$, the instantaneous volatility matrix is:\n\n\\[\n\\pmb{c}_t = \\begin{pmatrix}\n\\pmb{c}_{XX,t} & \\pmb{c}_{XY,t} \\\\\n\\pmb{c}_{YX,t} & \\pmb{c}_{YY,t}\n\\end{pmatrix}\n\\]\n\nwhere $\\pmb{c}_{YX,t} = \\beta_t \\pmb{c}_{XX,t}$ under orthogonality $[X', U] \\equiv 0$.",
    "question_context": "Let $Z$ be a $d\\cdot$-dimensional stochastic process defined on some filtered probability space $(\\varOmega,\\mathcal{F},(\\mathcal{F}_{t})_{t\\in[0,T]},\\mathbb{P})$. Throughout this paper, we assume that the multivariate stochastic process $Z$ follows a multivariate Itô semimartingale.\nAssumption $(H F)$. The log-price process $Z$ and its volatility process c are given by (2.1) and (2.2), respectively. Let $r\\in[0,2]$ and $L$ be some positive finite number.\n\nThe paper assumes a multivariate Itô semimartingale model for the log-price process and derives the asymptotic properties of the estimators under high-frequency settings."
  },
  {
    "qid": "econ-empirical-413-1-0-2",
    "question": "3) Discuss why the first-order asymptotic analysis is insufficient for investigating the effect of shrinkage.",
    "gold_answer": "1. First-order asymptotics do not provide guidance on choosing the shrinkage parameter $s$.\n2. Similar to instrument selection, higher-order expansions are needed to determine optimal $s$.\n3. The Nagar-type asymptotic expansion is used to approximate the mean square error.",
    "question_context": "Assumption 1. $\\{y_{i},W_{i},x_{i}\\}$ are i.i.d., $E(\\epsilon_{i}^{2}|x_{i})=\\sigma_{\\epsilon}^{2}>0$ and $E(\\|\\eta_{i}\\|^{4}|x_{i})$ and $E(|\\epsilon_{i}|^{4}|x_{i})$ are bounded.\nAssumption 2. $(\\mathrm{i})\\bar{H}\\equiv E(f_{i}f_{i}^{\\prime})$ exists and is nonsingular. (ii) There exists $\\pi_{K}$ such that $E(\\|f(x)-\\pi_{K}(X^{\\prime},\\bar{Z}^{\\prime})\\|^{2})\\to0$ as $K\\rightarrow\\infty$ .\nAssumption 3. (i) $E\\{(\\epsilon,u^{\\prime})^{\\prime}(\\epsilon,u^{\\prime})|x_{i}\\}$ is constant. (ii) $(X,Z)^{\\prime}(X,Z)$ is nonsingular with probability one. (iii) $\\operatorname*{max}_{i\\leq N}P_{Z,i i}\\to_{p}0.($ iv) fi is bounded.\nTheorem 1. Suppose that Assumptions 1–3 are satisfied. If $(s K)^{2}/N$ $\\to_{p}0$ and either $s\\to_{p}1$ or $E(f_{i}Z_{i}^{\\prime})=0,$ then $\\hat{\\delta}_{t s l s,s}-\\delta\\to_{p}0$ and $\\sqrt{N}(\\hat{\\delta}_{t s l s,s}-\\delta)\\rightarrow_{d}N(0,\\sigma_{\\epsilon}^{2}\\bar{H}^{-1})$ .\n\nThis section discusses the asymptotic properties of the shrinkage TSLS estimator under specific assumptions, similar to those in Donald and Newey (2001)."
  },
  {
    "qid": "econ-empirical-51-0-0-2",
    "question": "3) Formally model the effect of family ownership and primogeniture on management quality. Discuss the trade-offs between principal-agent benefits and talent pool limitations.",
    "gold_answer": "1. **Principal-Agent Benefits**: Family ownership reduces agency costs. The owner's utility is:\n   $$ U = \\pi + \\theta V $$\n   where $V$ is private benefits and $\\theta$ measures their weight. Higher $\\theta$ aligns incentives but may reduce $\\pi$.\n2. **Talent Pool**: Primogeniture limits the manager talent pool. The expected quality of the manager is:\n   $$ E[Q] = \\alpha Q_{\\text{family}} + (1-\\alpha)Q_{\\text{market}} $$\n   where $\\alpha$ is the probability of choosing a family manager. Lower $Q_{\\text{family}}$ reduces $E[Q]$.\n3. **Net Effect**: The trade-off depends on the relative magnitudes of agency cost reduction and talent pool limitation.",
    "question_context": "We use an innovative survey tool to collect management practice data from 732 medium-sized firms in the United States, France, Germany, and the United Kingdom. These measures of managerial practice are strongly associated with firm-level productivity, profitability, Tobin’s $Q$ , and survival rates.\nManagement practices also display significant cross-country differences, with U.S. firms on average better managed than European firms, and significant within-country differences, with a long tail of extremely badly managed firms.\nWe find that poor management practices are more prevalent when product market competition is weak and/or when family-owned firms pass management control down to the eldest sons (primogeniture).\n\nThe paper examines the relationship between management practices and firm performance across different countries, focusing on medium-sized manufacturing firms in the U.S., U.K., France, and Germany. It highlights the significant variations in management practices and their impact on productivity, profitability, and other performance metrics."
  },
  {
    "qid": "econ-empirical-1332-4-0-2",
    "question": "3) Show that the transformed function $\\psi(x_{1},...,x_{m})=\\log\\varphi^{*}(\\exp(x_{1}),\\dots,\\exp(x_{m}))$ satisfies $\\psi(x)+\\psi(y)=\\psi(x+y)$ for $x,y\\leq0$.",
    "gold_answer": "1. **Transformation**: $\\psi(x)=\\log\\varphi^{*}(e^{x_{1}},\\dots,e^{x_{m}})$.  \n2. **Functional Equation**: From $\\varphi^{*}(a)/\\varphi^{*}(b)=\\varphi^{*}(a/b)$, take logs to get $\\psi(x-y)=\\psi(x)-\\psi(y)$.  \n3. **Rewrite**: For $x,y\\leq0$, set $x^{\\prime}=-x$, $y^{\\prime}=-y$ to get $\\psi(x^{\\prime}+y^{\\prime})=\\psi(x^{\\prime})+\\psi(y^{\\prime})$.",
    "question_context": "Let $\\varphi$ be an aggregator that satisfies the axioms. We shall prove that it is a geometric mean. By IIA, for all $t>0$ we may define a map $\\varphi_{t}\\colon[0,1]^{M}\\rightarrow[0,1]$ via $\\varphi_{t}\\bigl(f_{1}(t),\\dots,f_{m}(t)\\bigr)=\\varphi(f)(t)$, where each $f_{i}\\in{\\mathcal{N}}{\\mathcal{T}}.$\nBy the Pareto property, for all $t,s>0$ $\\varphi_{t}~=~\\varphi_{s}$: suppose that $f_{i}(t)\\mathrel{=}f_{i}(s)$ for all $i\\in M.$ Then $\\left(1,t\\right)$ is ranked the same as $\\left(1,s\\right)$ for all agents and therefore must be for the social ranking, so that $\\varphi_{t}\\big(f_{1}(t),\\ldots,f_{m}(t)\\big)=\\varphi_{s}\\big(f_{1}(s),\\ldots,f_{m}(s)\\big)$.\nNow, we want to claim that for all $a,b^{\\mathrm{~\\tiny~\\in~}}[0,1]^{M}$ with $a\\leq b$, we have $\\varphi^{*}(a)/\\varphi^{*}(b)=\\varphi^{*}(a_{1}/b_{1},...,a_{m}/b_{m})$. To this end, let $f_{1},\\dots,f_{m}\\in\\mathcal{N}\\mathbb{Z}$ for which $f_{i}(1)=b_{i}$ and $f_{i}(2)=a_{i}$.\nObserve that this is a form of the Cauchy functional equation. For $a,b\\in[0,1]^{M}$ with $a\\leq b$, we have $$\\frac{\\varphi^{*}(a)}{\\varphi^{*}(b)}=\\varphi^{*}\\Bigl(\\frac{a_{1}}{b_{1}},...,\\frac{a_{m}}{b_{m}}\\Bigr).$$\n\nThe proof establishes the necessity and sufficiency of axioms for an aggregator to be a geometric mean, utilizing Pareto and IIA properties."
  },
  {
    "qid": "econ-empirical-1018-2-1-1",
    "question": "6) Discuss the challenges in interpreting the mixed evidence for hypothesis (b), emphasizing the role of theoretical models in guiding empirical work.",
    "gold_answer": "1. Mixed evidence arises from omitted variables (e.g., demand elasticity).  \n2. Without theory, ideal data conditions (e.g., constant elasticity) are unclear.  \n3. Dasgupta and Stiglitz (1980a) show concentration-R&D link depends on innovation possibilities.  \n4. Empirical work must control for theoretical parameters to avoid spurious conclusions.",
    "question_context": "(a) Large firms undertake proportionately greater R& D than do small firms. (b) There is a positive association between the degree of concentration in an industry and innovative activity within it. (c) Growth in demand for the products of an industry stimulates R & D activity within it.\nThe evidence suggests that beyond a threshold size, (a) does not hold. Proportionately the size of firms does not influence R & D activity.\nHypothesis (b) is controversial, for the evidence is mixed. There is some evidence though that measures of R & D input relative to the industry's sales achieve their maximum at moderate levels of concentration.\nHypothesis (c) is the most intuitive. But it is by no means obvious. If demand is expected to grow, oligopoly profits might in any case be expected to increase. In this case one may argue that the incentive to undertake R & D expenditure will lessen, not increase.\n\nThis section reviews empirical hypotheses about firm size, industrial concentration, and R&D activity, highlighting the challenges of interpreting data without precise theoretical models."
  },
  {
    "qid": "econ-empirical-108-10-1-0",
    "question": "4) Formally derive the alternative normalization method using the hotel and restaurant industry as a benchmark, and explain how it affects the estimated bargaining effects.",
    "gold_answer": "1. Let $\\psi_{j}^{M}$ and $\\psi_{j}^{F}$ be the wage premiums for men and women at firm $j$. \\n2. Normalize such that the weighted average of $\\psi_{j}^{M}$ and $\\psi_{j}^{F}$ is 0 in the hotel/restaurant industry: \\n   $$ \\sum_{j \\in H} w_{j} \\psi_{j}^{M} = \\sum_{j \\in H} w_{j} \\psi_{j}^{F} = 0 $$ \\n3. This reduces $\\psi_{j}^{F}$ by 2 percentage points, increasing the bargaining effect by 0.021.",
    "question_context": "We considered an alternative normalization based on the assumption that firms in the hotel and restaurant industry pay zero rents to workers on average.\nThe estimated bargaining effects are uniformly larger, reflecting the fact that the mean of the estimated wage premiums for male workers is about the same under the baseline and alternative normalizations, but the mean of the estimated premiums for female workers falls by about 2 percentage points under the alternative normalization.\n\nThis section explores an alternative normalization method to estimate the bargaining effect, using the hotel and restaurant industry as a benchmark."
  },
  {
    "qid": "econ-empirical-1028-1-0-3",
    "question": "4) Compare the Bayesian estimation approach used in this paper with classical maximum likelihood methods, highlighting its advantages for estimating DSGE models with many structural parameters.",
    "gold_answer": "1. **Bayesian Estimation**: Combines prior distributions with the likelihood function to obtain posterior distributions for parameters. \n2. **Advantages**: \n   - Handles large parameter spaces efficiently. \n   - Incorporates prior information (e.g., from micro studies). \n   - Provides credible intervals for policy analysis. \n3. **Classical MLE**: Struggles with identification in high-dimensional DSGE models. \n4. **Example**: The posterior density \\( p(\\theta | Y) \\propto p(Y | \\theta) p(\\theta) \\) is more informative than MLE estimates \\( \\hat{\\theta} \\).",
    "question_context": "Money in our model has an informational role which facilitates the estimation of the unobserved shocks that drive potential output and thus the state of the economy.\nOur findings support the view that money has information value. This is reflected in higher precision in terms of unobserved model concepts such as the natural rate of output.\nMoreover, our results highlight how modelling money demand can provide insights about structural features of the economy that may be important for the design of interest rate rules.\nMoney demand shocks can confound monetary policy shocks to generate a perverse price response in vector autoregressions (VAR).\n\nThe literature review discusses the role of money in New Neoclassical Synthesis (NNS) models, highlighting its informational value and its implications for monetary policy design."
  },
  {
    "qid": "econ-empirical-290-4-1-1",
    "question": "4) Explain the empirical strategy used to compare risk profiles and wage-establishment age profiles by initial size and industry. What do the results imply about the role of risk premia?",
    "gold_answer": "1. The strategy involves grouping establishments by initial size/industry, estimating death rates and wage profiles by age, and testing for correlations between the two.\n2. Results show little correlation, implying risk premia do not explain wage differentials.\n3. For example, small establishments have steeply declining death rates but no corresponding wage profile differences, contradicting the hypothesis.",
    "question_context": "New establishments are characterized by high probabilities of going out of business and this is associated with high risks of job loss to their workers.15 It seems plausible therefore that the observed differences in starting wages in new establishments are compensating these workers for the unemployment risk.\nIf a measure of the risk a worker faces when starting a job with a particular employer were available, then controlling for this should eliminate the observed wage differential between new and old establishments, in so far as this differential is a compensation for the risk.\n\nThis section explores whether higher starting wages in new establishments compensate workers for the higher risk of job loss due to establishment failure. The analysis uses establishment death rates and wage profiles to test the compensating differentials hypothesis."
  },
  {
    "qid": "econ-empirical-1111-0-0-3",
    "question": "4) Analyze the welfare implications of minimum wage increases in a two-sector model (restaurants and non-restaurants). How does the incidence of the wage change differ between sectors?",
    "gold_answer": "1. Let sector 1 (restaurants) be labor-intensive and sector 2 (non-restaurants) be capital-intensive.\n2. Minimum wage binds in sector 1, causing unemployment \\( U = L_1^s(w_{min}) - L_1^d(w_{min}) \\).\n3. Sector 2 sees increased labor supply: \\( L_2^s(w) = L_2^s(w) + U \\), lowering \\( w_2 \\).\n4. Net welfare effect depends on the relative sizes of \\( \\Delta CS \\) (consumer surplus), \\( \\Delta PS \\) (producer surplus), and deadweight loss.",
    "question_context": "The Minimum Wage, Restaurant Prices, and Labor Market Structure by Daniel Aaronson, Eric French, and James MacDonald, published in The Journal of Human Resources, Summer 2008, Vol. 43, No. 3, pp. 688-720.\n\nThis study examines the impact of minimum wage increases on restaurant prices and labor market structure, providing empirical evidence on how firms adjust to wage mandates."
  },
  {
    "qid": "econ-empirical-1571-4-0-0",
    "question": "1) Derive the joint log-likelihood function used in the proof of Theorem 3 (a) and explain how the independence of $\\epsilon_{1}$ and $\\epsilon$ implies the independence of $\\hat{\\boldsymbol{C}}$ and $\\hat{\\pmb{\\alpha}}_{0}^{*}$.",
    "gold_answer": "1. **Joint Log-Likelihood Function**: The joint log-likelihood function is given by: $$\\sum_{i=1}^{n}\\log\\{f(y_{i}|m_{i}^{*},x_{i}^{*},\\alpha_{0}^{*},\\alpha_{1}^{*})\\}+\\sum_{i=1}^{n}\\log\\{f(m_{i}|x_{i}^{m},C)\\}.$$ This combines the likelihood of the outcome $y_i$ and the mediator $m_i$.  \n2. **Independence of Errors**: Since $\\epsilon_{1}$ and $\\epsilon$ are independent, the two terms in the log-likelihood are separable. This separability implies that the estimators $\\hat{\\boldsymbol{C}}$ (from $f(m_i|x_i^m, C)$) and $\\hat{\\pmb{\\alpha}}_{0}^{*}$ (from $f(y_i|m_i^{*},x_i^{*},\\alpha_0^{*},\\alpha_1^{*})$) are derived from independent components of the likelihood, ensuring their independence.",
    "question_context": "First, because $\\epsilon_{1}$ and $\\epsilon$ are independent, the joint log-likelihood function $$\\sum_{i=1}^{n}\\log\\{f(y_{i}|m_{i}^{*},x_{i}^{*},\\alpha_{0}^{*},\\alpha_{1}^{*})\\}+\\sum_{i=1}^{n}\\log\\{f(m_{i}|x_{i}^{m},C)\\}$$ implies that $\\hat{\\boldsymbol{C}}$ and $\\hat{\\pmb{\\alpha}}_{0}^{*}$ are independent, and therefore, $\\hat{\\mathbf{F}}(u)$ and $\\hat{\\pmb{\\alpha}}_{0}(u)$ are independent.\nThe asymptotic distribution of $\\hat{\\boldsymbol{\\beta}}(u)$ can be obtained as follows. For any $\\pmb{a}_{q\\times1}$ , we consider Cramer’s Device. Since $$\\begin{array}{r l}&{\\mathrm{cov}_{a}\\{\\mathrm{vec}(\\hat{\\Gamma}(u)^{T}-\\tilde{\\Gamma}(u)^{T})\\}}\\ &{\\quad\\mathrm=\\frac{1}{n}\\big[\\{I_{q}\\otimes d(u)^{T}\\}\\pmb{\\Sigma}_{x^{m}x^{m}}^{-1}\\{I_{q}\\otimes d(u)\\}\\big]\\otimes\\pmb{\\Sigma}_{\\epsilon},}\\end{array}$$ $\\operatorname{vec}(I_{p}\\hat{\\Gamma}(u)^{T}\\pmb{a})=(\\pmb{a}^{T}\\otimes I_{p})\\mathrm{vec}(\\hat{\\Gamma}(u)^{T})$ , and ve $\\begin{array}{r}{\\mathrm{c}(\\hat{\\Gamma}(u)^{T}{\\pmb a}-\\tilde{\\Gamma}(u)^{T}{\\pmb a})=}\\end{array}$ $({\\pmb a}^{T}\\otimes{\\pmb I}_{p})\\{\\mathrm{vec}(\\hat{\\pmb\\Gamma}(u)^{T})-\\mathrm{vec}(\\tilde{\\pmb\\Gamma}(u)^{T})\\}$ , we can get that $$\\begin{array}{r l}&{\\quad\\mathrm{cov}_{a}\\{\\mathrm{vec}(\\hat{\\Gamma}(u)^{T}a-\\tilde{\\Gamma}(u)^{T}a)\\}}\\ &{=\\mathrm{cov}_{a}[(a^{T}\\otimes I_{P})\\{\\mathrm{vec}(\\hat{\\Gamma}(u)^{T}-\\tilde{\\Gamma}(u)^{T})\\}]}\\ &{=(a^{T}\\otimes I_{P})\\Bigg(\\frac1n\\big[\\{I_{q}\\otimes d(u)^{T}\\}\\Sigma_{x^{m}x^{m}}^{-1}\\{I_{q}\\otimes d(u)\\}\\big]\\otimes\\Sigma_{\\epsilon}\\Bigg)(a\\otimes I_{P})}\\ &{=\\frac1n\\big[a^{T}\\{I_{q}\\otimes d(u)^{T}\\}\\Sigma_{x^{m}x^{m}}^{-1}\\{I_{q}\\otimes d(u)\\}\\a\\big]\\otimes\\Sigma_{\\epsilon}.}\\end{array}$$\nIf we let $\\tilde{{\\pmb\\theta}}(u)=\\tilde{\\bf\\Gamma}(u)^{T}{\\pmb a}$ and ${\\hat{\\pmb\\theta}}(u)={\\hat{\\mathbf{\\Gamma}}}(u)^{T}{\\pmb a}$ , then $$[\\mathrm{cov}_{a}\\{\\hat{\\pmb{\\theta}}(u)\\}]^{-1/2}(\\hat{\\pmb{\\theta}}(u)^{T}-\\tilde{\\pmb{\\theta}}(u)^{T})\\stackrel{D}{\\rightarrow}N(\\pmb{0},I),$$ where $\\begin{array}{r}{\\operatorname{cov}_{a}\\{\\hat{\\pmb\\theta}(u)\\}=\\frac{1}{n}\\big[{\\pmb a}^{T}\\{I_{q}\\otimes{\\pmb d}(u)^{T}\\}{\\pmb\\Sigma}_{x^{m}\\underline{{x}}^{m}}^{-1}\\{I_{q}\\otimes{\\pmb d}(u)\\}{\\pmb a}\\big]{\\pmb\\Sigma}_{\\epsilon}.}\\end{array}$\nTo obtain the asymptotic distribution of $\\hat{\\pmb{\\beta}}(u)=\\hat{\\pmb\\Gamma}(u)\\hat{\\pmb\\alpha}_{0}(u)$ by the delta method, let $\\pmb{\\Sigma}_{\\theta(u)}=\\mathrm{cov}_{a}\\{\\hat{\\pmb\\theta}(u)\\}$ and $\\Sigma_{\\pmb{\\alpha}_{0}(u)}=\\mathrm{cov}_{a}\\{\\hat{\\pmb{\\alpha}}_{0}(u)\\}=$ $\\begin{array}{r}{\\frac{1}{n}\\sigma_{1}^{2}\\{I_{p}\\otimes b_{0}(u)^{T}\\}\\Sigma_{m*m*.x*}^{-1}\\{I_{p}\\otimes b_{0}(u)\\}}\\end{array}$ for simplicity.\nSince $\\hat{\\pmb{\\alpha}}_{0}(u)$ and $\\hat{\\mathbf{{T}}}(u)$ are independent and $$\\left(\\stackrel{\\Sigma_{\\theta(u)}}{\\bf0}\\quad\\begin{array}{c c}{{\\bf0}}\\\\{{\\Sigma_{\\alpha_{0}(u)}}}\\end{array}\\right)^{-1/2}\\left\\{\\left(\\hat{\\theta}(u)^{T}\\right)-\\left(\\tilde{\\theta}(u)^{T}\\right)\\right\\}\\stackrel{D}{\\rightarrow}N\\left({\\bf0},I\\right),$$ we have $$\\begin{array}{r l}&{\\{\\tilde{\\alpha}_{0}(u)^{T}\\Sigma_{\\theta(u)}\\tilde{\\alpha}_{0}(u)+\\tilde{\\theta}(u)^{T}\\Sigma_{\\alpha_{0}(u)}\\tilde{\\theta}(u)\\}^{-1/2}\\{\\hat{\\theta}(u)^{T}\\hat{\\alpha}_{0}(u)}\\ &{\\quad\\quad-\\tilde{\\theta}(u)^{T}\\tilde{\\alpha}_{0}(u)\\}\\xrightarrow{D}N(\\mathbf{0},I),}\\end{array}$$ $$\\begin{array}{r l r}&{\\mathrm{where}\\quad\\tilde{\\alpha}(u)^{T}{\\Sigma}_{\\theta(u)}\\tilde{\\alpha}_{0}(u)}&{=}&{\\frac{1}{n}\\tilde{\\alpha}_{0}(u)^{T}{\\Sigma}_{\\epsilon}\\tilde{\\alpha}_{0}(u)a^{T}\\{I_{q}}\\\\ &{d(u)^{T}\\}{\\Sigma}_{x^{m}x^{m}}^{-1}\\{I_{q}\\quad\\otimes\\quad d(u)\\}\\a,\\quad\\mathrm{~and~}\\quad\\tilde{\\theta}(u)^{T}.\\quad{\\Sigma}_{\\alpha_{0}(u)}\\tilde{\\theta}(u)}\\\\ &{\\frac{1}{n}\\sigma_{1}^{2}a^{T}\\tilde{\\Gamma}(u)\\{I_{p}\\otimes b_{0}(u)^{T}\\}{\\Sigma}_{m*m*,x*}^{-1}\\{I_{p}\\otimes b_{0}(u)\\}\\tilde{\\Gamma}(u)^{T}a.~\\mathrm{Thus},}\\end{array}$$ $$\\tilde{\\pmb{\\alpha}}_{0}(u)^{T}\\pmb{\\Sigma}_{\\theta(u)}\\tilde{\\pmb{\\alpha}}_{0}(u)+\\tilde{\\pmb{\\theta}}(u)^{T}\\pmb{\\Sigma}_{\\alpha_{0}(u)}\\tilde{\\pmb{\\theta}}(u)$$ $$=\\frac{1}{n}\\pmb{a}^{T}\\big[\\Tilde{\\alpha}_{0}(u)^{T}\\pmb{\\Sigma}_{\\epsilon}\\Tilde{\\alpha}_{0}(u)\\{I_{q}\\otimes\\pmb{d}(u)^{T}\\}\\pmb{\\Sigma}_{x^{m}x^{m}}^{-1}\\{I_{q}\\otimes\\pmb{d}(u)\\}$$ $$+\\sigma_{1}^{2}\\tilde{\\Gamma}(u)\\{I_{\\boldsymbol{p}}\\otimes b_{0}(u)^{T}\\}\\Sigma_{m*m*,x*}^{-1}\\{I_{\\boldsymbol{p}}\\otimes b_{0}(u)\\}\\tilde{\\Gamma}(u)^{T}\\Big]a.$$\nSince $\\{\\hat{\\pmb{\\theta}}(u)^{T}\\hat{\\pmb{\\alpha}}_{0}(u)-\\tilde{\\pmb{\\theta}}(u)^{T}\\tilde{\\pmb{\\alpha}}_{0}(u)\\}=\\pmb{a}^{T}\\{\\hat{\\Gamma}(u)\\hat{\\pmb{\\alpha}}_{0}(u)-\\tilde{\\pmb{\\Gamma}}(u)\\tilde{\\pmb{\\alpha}}_{0}(u)\\},$ it follows that $$\\begin{array}{r l}&{\\quad[\\mathrm{cov}_{a}\\{\\hat{\\pmb{\\beta}}(u)\\}]^{-1/2}\\{\\hat{\\bf\\Gamma}(u)\\hat{\\pmb{\\alpha}}_{0}(u)-\\tilde{\\bf\\Gamma}(u)\\tilde{\\pmb{\\alpha}}_{0}(u)\\}}\\\\ &{}\\\\ &{\\quad\\mathrm=[\\mathrm{cov}_{a}\\{\\hat{\\pmb{\\beta}}(u)\\}]^{-1/2}\\{\\hat{\\pmb{\\beta}}(u)-\\tilde{\\pmb{\\beta}}(u)\\}}\\\\ &{}\\\\ &{\\quad\\xrightarrow D N\\{\\bf0,\\cal I\\},}\\end{array}$$ where $\\begin{array}{r}{\\mathrm{cov}_{a}\\{\\hat{\\beta}(u)\\}=\\frac{1}{n}[\\tilde{\\alpha}_{0}(u)^{T}\\Sigma_{\\epsilon}\\tilde{\\alpha}_{0}(u)\\{I_{q}\\otimes d(u)^{T}\\}\\Sigma_{x^{m}x^{m}}^{-1}\\{I_{q}\\otimes}\\end{array}$ d(u)}+ o²()1p bo(u)1. Em\\*m.x(lpbo(u)r().\n\nThis section provides a detailed proof of Theorem 3 (a), focusing on the asymptotic distribution of the estimator $\\hat{\\boldsymbol{\\beta}}(u)$ derived from the joint log-likelihood function. The proof leverages independence between $\\hat{\\boldsymbol{C}}$ and $\\hat{\\pmb{\\alpha}}_{0}^{*}$, and employs Cramer’s Device to derive the covariance structure."
  },
  {
    "qid": "econ-empirical-139-1-0-2",
    "question": "3) Using the findings of the article, formalize a theoretical model where social connections reduce default rates through monitoring and enforcement. Specify the key equations and equilibrium conditions.",
    "gold_answer": "1. **Monitoring cost**: Let \\(c(d_i, d_j)\\) be the cost for member \\(i\\) to monitor member \\(j\\), decreasing in social connection strength \\(d_{ij}\\).  \n2. **Default penalty**: Default by \\(j\\) imposes a reputational cost \\(\\phi(d_{ij})\\) on \\(i\\), increasing in \\(d_{ij}\\).  \n3. **Repayment condition**: Member \\(j\\) repays if \\(\\phi(d_{ij}) \\geq \\text{benefit of default}\\).  \n4. **Equilibrium**: All members monitor and enforce if \\(E[\\text{costs}] \\leq E[\\text{benefits}]\\).",
    "question_context": "Individuals with stronger social connections to their fellow group members (i.e., either living closer or being of a similar culture) have higher repayment and higher savings.\nFINCA-Peru’s process for assigning individuals to groups creates a natural experiment with quasi-random group formation. This quasi-random process provides the strategy for identifying social connections.\nThe savings and internal loan structure is very similar to a rotating savings and credit association since all members make small weekly deposits, and then each week a small fraction of the members receives large loans from the savings of everyone.\nFINCA’s operating philosophy encourages clients to develop solidarity or social capital. While this is evident from the meeting hall posters propagating the values of camaraderie, trust and teamwork, it is even more evident in the training materials provided to the employees and clients.\n\nThe article discusses the role of social connections in group lending, focusing on FINCA-Peru's quasi-random group formation process. It explores how social connections, such as geographic proximity and cultural similarity, influence loan repayment and savings rates through monitoring and enforcement mechanisms."
  },
  {
    "qid": "econ-empirical-1638-1-0-0",
    "question": "1) Derive the quasi-likelihood function $\\mathcal{L}_{n}(\\pmb{\\theta},\\eta)$ from the given probability density function $f(y;\\mathbf{x},z) = \\alpha(\\mathbf{x},z)(y/w_{n})^{-\\alpha(\\mathbf{x},z)}y^{-1}$.",
    "gold_answer": "1. Start with the given probability density function: $$ f(y;\\mathbf{x},z) = \\alpha(\\mathbf{x},z)(y/w_{n})^{-\\alpha(\\mathbf{x},z)}y^{-1}. $$\n2. Take the logarithm to obtain the log-likelihood: $$ \\log f(y;\\mathbf{x},z) = \\log(\\alpha(\\mathbf{x},z)) - \\alpha(\\mathbf{x},z)\\log(y/w_{n}) - \\log y. $$\n3. Substitute $\\log(\\alpha(\\mathbf{x},z)) = \\mathbf{x}^{\\top}\\pmb{\\theta} + \\eta(z)$: $$ \\log f(y;\\mathbf{x},z) = \\mathbf{x}^{\\top}\\pmb{\\theta} + \\eta(z) - \\exp(\\mathbf{x}^{\\top}\\pmb{\\theta} + \\eta(z))\\log(y/w_{n}) - \\log y. $$\n4. Sum over all observations $Y_i > w_n$ to obtain the quasi-likelihood: $$ \\mathcal{L}_{n}(\\pmb{\\theta},\\eta) = \\sum_{i=1}^{n} \\left\\{ \\log(Y_i/w_n)\\exp(\\mathbf{X}_i^{\\top}\\pmb{\\theta} + \\eta(Z_i)) - \\mathbf{X}_i^{\\top}\\pmb{\\theta} - \\eta(Z_i) \\right\\} I(Y_i > w_n). $$",
    "question_context": "Let $(Y_{i},\\mathbf{X}_{i},Z_{i})_{i=1}^{n}$ be independent observations of random variables $(Y,\\mathbf{X},Z)$ , where $Y\\in\\mathbb{R}$ is the response variable of interest and $(\\mathbf{X},Z)$ are covariates with $\\mathbf{X}\\in{\\mathcal{X}}\\subset\\mathbb{R}^{p}$ and, without loss of generality, $Z\\in{\\mathcal{Z}}\\subset\\mathbb{R}$ . In addition, let $F(y;\\mathbf{x},z)=\\operatorname{pr}(Y\\leq$ $y|\\textbf{X}=\\mathbf{x},Z=z)$ be the cumulative distribution function of $Y$ conditional on $(\\mathbf{X},Z)$ . In this article, we use $S(y;\\mathbf{x},z)=$ $1-F(y;\\mathbf{x},z)$ to denote the conditional survival function and study the following Pareto-type model $$ S(y;\\mathbf{x},z)=y^{-\\alpha(\\mathbf{x},z)}L(y;\\mathbf{x},z), $$ where $\\alpha(\\mathbf{x},z)$ is an unknown tail index function that characterizes the dependence of the tail behavior of $Y$ on $\\mathbf{X}$ and $Z$ .\nFor $\\alpha(\\mathbf{x},z)$ , we propose the following semiparametric tail index regression (STIR) model by denoting $\\alpha(\\mathbf{x},z)$ on the logarithmic scale as $$ \\log(\\alpha(\\mathbf{x},z))=\\mathbf{x}^{\\top}\\pmb\\theta+\\eta(z) $$ with an unknown parameter vector $\\begin{array}{l l l}{\\pmb{\\theta}}&{=}&{(\\theta_{1},\\dots,\\theta_{p})^{\\top}}\\end{array}$ and an unknown univariate smooth function $\\eta(\\cdot)$ .\nFor estimation, we apply the results in Severini and Staniswalis (1994) to the probability density function in (2) to obtain the following quasi-likelihood function $$ \\begin{array}{r l}&{\\mathcal{L}_{n}(\\pmb{\\theta},\\eta)=\\displaystyle\\sum_{i=1}^{n}\\left\\{\\log(Y_{i}/w_{n})\\exp(\\mathbf{X}_{i}^{\\top}\\pmb{\\theta}+\\eta(Z_{i}))\\right.}\\ &{\\qquad\\left.-\\mathbf{X}_{i}^{\\top}\\pmb{\\theta}-\\eta(Z_{i})\\right\\}I(Y_{i}>w_{n}).}\\end{array} $$\nTo estimate the function $\\eta(z)$ in (3), we adopt local linear smoothing in Fan and Gijbels (1996) by approximating $\\eta(Z_{i})$ via the first-order Taylor expansion at some $z$ satisfying $$ \\eta(Z_{i})=\\eta(z)+\\eta^{(1)}(z)(Z_{i}-z)+O_{P}(h_{n}^{2}) $$ for $\\mid Z_{i}-z\\mid\\leq h_{n}$ , where $h_{n}>0$ is a bandwidth approaching zero as the sample size $n\\rightarrow\\infty$ .\n\nThe text introduces a Pareto-type model for conditional survival functions with covariates, focusing on tail index regression and estimation techniques."
  },
  {
    "qid": "econ-empirical-1732-3-1-0",
    "question": "5) Derive the optimal contingent fee $\\hat{\\alpha}_{ik} = \\frac{1}{1 + n - m_k}$ for the affine compensation contract model.",
    "gold_answer": "1. The manager's wealth is $W_{ik} = \\alpha_{ik} Z_{ik} + \\sum_{j=1}^{h_k} \\delta_{ik} 1_{\\{\\phi_{ijk} \\neq 0\\}} - c$.  \n2. The optimal $\\alpha_{ik}$ ensures efficient risk sharing, giving each household and the manager an equal share of $Z_{ik}$.  \n3. With $h_k = n - m_k$ households, the optimal split is: $$\\hat{\\alpha}_{ik} = \\frac{1}{1 + h_k} = \\frac{1}{1 + n - m_k}.$$",
    "question_context": "We now allow the fund managers to use affine compensation contracts, but we maintain all of our other modeling assumptions. The $i$ th fund manager in group $k$ charges a fixed account fee $\\delta_{i k}$ in addition to the proportional fee $\\alpha_{i k}$ . Thus the date 3 wealth of the $i$ th manager in group $k$ is $$W_{i k}=\\alpha_{i k}Z_{i k}+\\sum_{j=1}^{h_{k}}\\delta_{i k}1_{\\{\\phi_{i j k}\\neq0\\}}-c,$$ where $Z_{i k}$ is from (2).\nProposition 5. Suppose the informed agents are allowed to use affine compensation contracts if they establish mutual funds. Then the optimal contingent management fee of the ith manager in group $k$ is $$\\hat{\\alpha}_{i k}={\\frac{1}{1+h_{k}}}={\\frac{1}{1+n-m_{k}}}$$ and the optimal fixed account fee of the ith manager in group $k$ is $$\\widehat{\\delta}_{i k}=\\frac{1}{2\\tau}\\log\\biggr[\\frac{\\mathcal{R}+m_{k}/\\sigma_{\\epsilon}^{2}}{\\mathcal{R}+(m_{k}-1)/\\sigma_{\\epsilon}^{2}}\\biggr],$$ where the price informativeness $\\mathcal{R}$ is $$\\mathcal{R}=\\frac{1}{\\sigma_{x}^{2}}+\\frac{1}{\\tau^{2}\\sigma_{u}^{2}\\sigma_{\\epsilon}^{4}}\\Bigg(\\int_{0}^{\\bar{k}}m_{k}(1+n-m_{k})d k\\Bigg)^{2}.$$\n\nThis section generalizes the model by allowing affine compensation contracts for mutual fund managers, which include both proportional and fixed fees. Key results include the optimal fee structure and its impact on equilibrium outcomes."
  },
  {
    "qid": "econ-empirical-678-1-0-2",
    "question": "3) Derive the conditional marginal distributions $\\pi_1(\\alpha | F_t, Y_t)$ and $\\pi_2(\\lambda_t | F_t, Y_t)$ given the joint distribution $$\\binom{\\alpha}{Y_{t}}F_{t}\\Biggr)\\sim\\mathrm{N}\\left[\\binom{S_{N}\\mu}{Z_{t}(\\mu+\\hat{\\lambda}_{t|t-1})},\\left(\\begin{array}{c c}{{\\phi_{11}}}&{{\\phi_{12}}}\\\\{{\\phi_{21}}}&{{\\phi_{22}}}\\end{array}\\right)\\right].$$",
    "gold_answer": "1. The joint distribution is: $$\\binom{\\alpha}{Y_t} \\sim \\mathsf{N}\\left(\\binom{S_N \\mu}{Z_t (\\mu + \\hat{\\lambda}_{t|t-1})}, \\begin{pmatrix} \\phi_{11} & \\phi_{12} \\\\ \\phi_{21} & \\phi_{22} \\end{pmatrix}\\right).$$\\n2. For $\\pi_1(\\alpha | F_t, Y_t)$, use the conditional mean formula: $\\alpha^{*} = S_N \\mu + \\phi_{12} \\phi_{22}^{-1} [Y_t - Z_t (\\mu + \\hat{\\lambda}_{t|t-1})]$.\\n3. The conditional variance is $V_{\\alpha}^{*} = \\phi_{11} - \\phi_{12} \\phi_{22}^{-1} \\phi_{21}$.\\n4. For $\\pi_2(\\lambda_t | F_t, Y_t)$, the mean is $\\lambda_t^{*} = \\hat{\\lambda}_{t|t-1} + \\hat{\\Omega}_{t|t-1} Z_t' \\phi_{22}^{-1} [Y_t - Z_t (\\mu + \\hat{\\lambda}_{t|t-1})]$.\\n5. The variance is $\\Omega_t^{*} = \\hat{\\Omega}_{t|t-1} - \\hat{\\Omega}_{t|t-1} Z_t' \\phi_{22}^{-1} Z_t \\hat{\\Omega}_{t|t-1}$.",
    "question_context": "From (11) the likelihood function is and the prior, given information up to $t$ , is $$\\begin{array}{r}{p(\\gamma_{t}|F_{t})=\\mathsf{N}(\\hat{\\gamma}_{t-1},\\hat{H}_{t-1}),}\\end{array}$$ where $\\hat{\\gamma}_{t-1}=S_{N}\\left(\\mu+\\hat{\\lambda}_{t|t-1}\\right)$ and $\\hat{H}_{t-1}=(S_{N}\\Psi S_{N}^{\\prime}+\\Delta)+S_{N}\\hat{\\Omega}_{t|t-1}S_{N}^{\\prime}$ . Standard calculations give us that the posterior $\\pi_{0}(\\gamma_{t}|F_{t},Y_{t})$ is normal with mean $\\gamma_{t}^{*}$ and variance $H_{t}^{*}$ where $$\\begin{array}{r l}&{\\gamma_{t}^{*}=H_{t}^{*}\\left(W_{t}^{\\prime}\\Sigma_{u}^{-1}Y_{t}+\\hat{H}_{t-1}^{-1}\\hat{\\gamma}_{t-1}\\right)}\\ &{\\qquad=\\hat{\\gamma}_{t-1}+\\hat{H}_{t-1}W_{t}^{\\prime}\\left[W_{t}\\hat{H}_{t-1}W_{t}^{\\prime}+\\Sigma_{u}\\right]^{-1}(Y_{t}-W_{t}\\hat{\\gamma}_{t-1}),}\\ &{H_{t}^{*}=\\left[\\hat{H}_{t-1}^{-1}+W_{t}^{\\prime}\\Sigma_{u}^{-1}W_{t}\\right]^{-1}}\\ &{\\qquad=\\hat{H}_{t-1}-\\hat{H}_{t-1}W_{t}^{\\prime}\\left[W_{t}\\hat{H}_{t-1}W_{t}^{\\prime}+\\Sigma_{u}\\right]^{-1}W_{t}\\hat{H}_{t-1}.}\\end{array}$$\n\nThe text discusses the likelihood function and prior distributions in a panel VAR model, focusing on fixed and random effects models. It details the posterior distributions and recursive estimation methods."
  },
  {
    "qid": "econ-empirical-10-3-0-3",
    "question": "4) Critically evaluate the methodological trade-offs between the cohort-based analysis in Figure VIII and a regression-based approach controlling for age and demographics, referencing the literature (e.g., Shin and Solon 2008).",
    "gold_answer": "1. Cohort analysis advantages: \n   - Transparency in interpreting raw mobility trends (Figure VIII). \n   - Avoids imputation biases from missing data. \n2. Regression disadvantages: \n   - Sensitivity to specification (Baker and Solon 2003). \n   - Complex interaction terms may obscure trends. \n3. Shin and Solon (2008) advocate for simplicity when structural models lack consensus.",
    "question_context": "We define the 'core long-term sample' in year $t$ as all individuals aged 25–60 in year $t$ with average earnings (using the standard wage indexation) from year $t-5$ to year $t+5$ above the minimum threshold.\nFigure VII displays the Gini coefficients for all workers, and for men and women separately based on those eleven-year average earnings from 1942 to 1999.\nFigure VIII displays the rank correlation between the eleven-year earnings spell centered in year $t$ and the eleven-year earnings spell after $T$ years (i.e., centered in year $t+T$ ) in the same sample of individuals present in the 'long-term core sample' in both year $t$ and year $t+T$.\n\nThis section examines long-term earnings inequality and mobility trends using eleven-year average earnings spans, focusing on cohort-based analysis to control for age structure and wage profile changes."
  },
  {
    "qid": "econ-empirical-74-0-0-3",
    "question": "4) Discuss the role of the no-renting assumption in the model and how it affects the monopolist's ability to price discriminate.",
    "gold_answer": "1. If the monopolist could rent the product, consumers would never leave the market, and the ratio of high to low consumers would remain constant.  \n2. Under the condition $\\alpha b_{1} > b_{2}$, the monopolist would always rent to high consumers only at a rent-level $\\alpha b_{1}$, extracting all their surplus.  \n3. This eliminates the need for periodic sales, as the monopolist can achieve the same profit as if he could commit to the high price forever.  \n4. The no-renting assumption is thus crucial for the cycling equilibrium to emerge.",
    "question_context": "A new cohort of consumers enters the market each period, interested in purchasing the good either immediately or after a delay. Within each cohort, consumers vary in their tastes for the good. Under broad conditions, the seller will vary the price over time. In most periods, he will charge a price just low enough to sell immediately to consumers with a high willingness to pay. Periodically, however, he will drop the price far enough to sell to an accumulated group of consumers with a low willingness to pay.\nThe optimal strategy of the monopolist involves a relatively high price in most periods, with sales only to newly entered consumers with a high willingness to pay. Periodically, however, the monopolist drops the price far enough to sell to the accumulated group of consumers with a low willingness to pay. Because all low-willingness consumers buy on a sale day, the price rises immediately after a sale day. Because some consumers wait for sales, volume increases on sale days.\nThe reservation price, call it $V_{1}$ of a type one consumer equals the discounted sum of per period values forever into the future: $V_{1}=\\sum_{t=0}^{\\infty}\\beta^{t}b_{1}=\\frac{b_{1}}{1-\\beta}$. Similarly, $V_{2}=b_{2}/(1-\\beta)$ is the reservation price of a type two consumer.\nThe cycle-creating forces would probably survive under limited consumer knowledge of the market; though the exact price pattern might not. As an extreme, suppose that consumers were gullible enough to believe that, after each sale, there would never be another. The monopolist could then get away with cycles consisting of $n-1$ periods at the high price followed by one period at the low price.\n\nThe paper presents a model where a monopoly seller of a durable good uses periodic sales as a means of price discrimination. New cohorts of consumers enter the market each period, with varying tastes for the good. The seller varies prices over time, charging high prices to consumers with high willingness to pay and periodically dropping prices to sell to accumulated low-willingness consumers."
  },
  {
    "qid": "econ-empirical-88-0-1-0",
    "question": "5) Explain the construction of the APTDOM variable and its instrumental role in identifying causal effects. How does excluding the ij route passengers address endogeneity concerns?",
    "gold_answer": "1. \\( APTDOM_{ij}^a \\) measures airline \\( a \\)'s share of originating passengers at airport \\( i \\), excluding those on the ij route.\n2. Exclusion prevents mechanical correlation between the dependent variable (route share) and the regressor.\n3. This design mimics a quasi-experiment where dominance at other routes serves as an instrument for local market power.",
    "question_context": "The data on share of CRS sales by metropolitan area are taken from the data appendix to the DOT's 1988 study of the effect of CRSs. The data for construction of the schedule convenience variable are taken from the May 15, 1986, Official Airline Guide.\n$L G S H A R E_{i j}^{a}=\\ln\\left[\\frac{S H A R E_{i j}^{a}}{1-S H A R E_{i j}^{a}}\\right]$\n\nThe paper employs a natural experiment design comparing round-trip tickets with identical routes but different origins to control for unobserved heterogeneity. Key variables include route shares, airport dominance metrics, and tourist orientation indices."
  },
  {
    "qid": "econ-empirical-556-0-1-1",
    "question": "6) Explain the construction of the SNP score generator for the multivariate case and its role in EMM estimation.",
    "gold_answer": "1. The SNP score generator is constructed using a polynomial P(z, x^*) to approximate the conditional density.\n2. The parameters μ_x and R_x are functions of lagged values x^*.\n3. The score generator is used to compute the expected score m(ρ, θ) under the stochastic volatility model.\n4. The SNP model's flexibility allows it to capture nonlinear features of the data, improving EMM estimation efficiency.",
    "question_context": "A multivariate stochastic volatility model for y_t is given by:\n$$\ny_t - μ_y = \\sum_{j=1}^{L_y} C_j (y_{t-j} - μ_y) + diag[exp(w_t)] R_y z_t,\n$$\n$$\nw_t - μ_w = \\sum_{j=1}^{L_w} A_j (w_{t-j} - μ_w) + R_w \\tilde{z}_t,\n$$\nwhere μ_y is an M×1 vector, the C_j are M×M matrices, and R_y is an M×M upper triangular matrix.\nThe SNP model is derived as:\n$$\nf(y | x, θ) = \\frac{\\{P[R_x^{-1}(y - μ_x), x^*]\\}^2 \\phi[R_x^{-1}(y - μ_x)]}{|det(R_x)| \\int [P(z, x^*)]^2 \\phi(z) dz},\n$$\nwhere μ_x = b_0 + B x^* and vech(R_x) = ρ_0 + P |x^*|.\n\nThe paper extends the univariate stochastic volatility model to a multivariate setting, fitting it to trivariate data consisting of stock returns, exchange rates, and interest rates. The model is specified with matrices for mean and volatility equations, and the estimation employs a multivariate SNP model as the score generator."
  },
  {
    "qid": "econ-empirical-1752-1-0-0",
    "question": "1) Derive the first-differenced form of the PVAR(1) model under fixed effects specification, starting from the original model $\\mathbf{w}_{i t} = \\pmb{\\mu}_{i} + \\Phi \\mathbf{w}_{i,t-1} + \\pmb{\\varepsilon}_{i t}$. Show the steps to eliminate $\\pmb{\\mu}_{i}$ and obtain $\\Delta\\mathbf{w}_{i t} = \\Phi \\Delta\\mathbf{w}_{i,t-1} + \\Delta\\pmb{\\varepsilon}_{i t}$.",
    "gold_answer": "1. Start with the original model: \n   $$ \\mathbf{w}_{i t} = \\pmb{\\mu}_{i} + \\Phi \\mathbf{w}_{i,t-1} + \\pmb{\\varepsilon}_{i t}. $$\n2. Lag the equation by one period: \n   $$ \\mathbf{w}_{i,t-1} = \\pmb{\\mu}_{i} + \\Phi \\mathbf{w}_{i,t-2} + \\pmb{\\varepsilon}_{i,t-1}. $$\n3. Subtract the lagged equation from the original: \n   $$ \\mathbf{w}_{i t} - \\mathbf{w}_{i,t-1} = \\Phi (\\mathbf{w}_{i,t-1} - \\mathbf{w}_{i,t-2}) + (\\pmb{\\varepsilon}_{i t} - \\pmb{\\varepsilon}_{i,t-1}). $$\n4. Simplify to obtain the first-differenced form: \n   $$ \\Delta\\mathbf{w}_{i t} = \\Phi \\Delta\\mathbf{w}_{i,t-1} + \\Delta\\pmb{\\varepsilon}_{i t}. $$",
    "question_context": "Under the fixed effects specification no restrictions need to be placed on the probability distribution function generating the individual-specific effects ${\\pmb{\\mu}}_{i}$ in \\~2+1! \\~or, in unrestricted form, $\\mathbf{a}_{i}$ in \\~2+3!!+ In particular, Assumptions \\~R1! and \\~R2! are no longer required+ It can then be allowed, for example, that \\~i! the individual effects are dependently distributed, \\~ii! the individual effects are heteroskedastic, \\~iii! the individual effects are \\~more generally! characterized by a joint probability distribution function with the number of unknown parameters increasing at the same rate as the number of cross-sectional observations in the panel, \\~iv! the individual effects do not have moments, and \\~v! the individual effects and the disturbances are correlated+\nFollowing standard practice the ${\\pmb\\mu}_{i}$ ’s can be eliminated by first-differencing \\~2+1!, namely,10 \n\n$$ \n\\Delta\\mathbf{w}_{i t}=\\Phi\\Delta\\mathbf{w}_{i,t-1}+\\Delta\\pmb{\\varepsilon}_{i t},\\qquadt=2,3,\\ldots,T. \n$$\nAssumption \\~F1!+ The following moment restrictions are satisfied: \n\n$$ \nE(\\pmb{\\kappa}_{i0}\\pmb{\\varepsilon}_{i1}^{\\prime})=\\mathbf{0}, \n$$ \n\nand \n\n$$ \nE(\\pmb{\\kappa}_{i0}\\Delta\\pmb{\\varepsilon}_{i t}^{\\prime})=\\mathbf{0},\\mathrm{for}t=2,3,...,T, \n$$ \n\nwhere $\\pmb{\\kappa}_{i0}=(\\mathbf{I}_{m}-\\pmb{\\Phi})\\pmb{\\xi}_{i0}$ +\n\nThe fixed effects specification allows for unrestricted distribution of individual-specific effects, accommodating dependent distributions, heteroskedasticity, and correlation with disturbances. The model is first-differenced to eliminate individual effects, leading to specific moment restrictions and distributional assumptions."
  },
  {
    "qid": "econ-empirical-176-3-0-2",
    "question": "3) Compare the accuracy of the sequence-space Jacobian (SSJ) method with the Reiter method. What are the key findings from Figure 4?",
    "gold_answer": "1. Both methods aim to solve the same first-order system of equations.  \n2. The SSJ method with automatic differentiation matches the Reiter method's solution up to 10 digits of accuracy.  \n3. Numerical differentiation introduces small errors, but two-sided numerical differentiation reduces these errors significantly.  \n4. Figure 4 shows that impulse responses from both methods are visually identical, with differences plotted in panels (b) and (d).",
    "question_context": "A general equilibrium model in the sequence space is characterized by a system of nonlinear equations $$\\mathbf{F}(\\mathbf{X},\\mathbf{Z})=0,$$ where $\\mathbf{Z}$ denotes the path of exogenous 'shocks,' with $\\mathbf{Z}_{t}$ an $n_{z}\\times1$ vector at each $t$, and $\\mathbf{X}$ denotes the path of endogenous variables, with $\\mathbf{X}_{t}$ an $n_{x}\\times1$ vector at each $t$.\nWe solve for the impulse responses of the model to first order around the steady state. By the implicit function theorem, the response of endogenous variables $d\\mathbf{X}$ to the shock $d\\mathbf{Z}$ is given by $$d\\mathbf{X}=-\\mathbf{F}_{\\mathbf{X}}^{-1}\\mathbf{F}_{\\mathbf{Z}}d\\mathbf{Z}\\equiv\\mathbf{G}d\\mathbf{Z},$$ where the Jacobians $\\mathbf{F_{X}}$ and $\\mathbf{F_{Z}}$ are evaluated at $(\\mathbf{X}^{s s},\\mathbf{Z}^{s s})$, and we define $\\mathbf{G}$ as the linear map from shocks $d\\mathbf{Z}$ to general equilibrium impulse responses $d\\mathbf{X}$.\nIn applications, it is common to reduce dimensionality by explicitly solving for some variables in terms of others. Suppose that $\\mathbf{F}$ is separated into $\\mathbf{F}_{1}$ and ${\\bf F}_{2}$, and that ${\\bf F}_{2}({\\bf X},{\\bf Z})=0$ can be solved in closed form to obtain $\\mathbf{X}$ as a function of some smaller vector $\\mathbf{U}$ of $n_{u}<n_{x}$ unknowns: $\\mathbf{X}=\\mathbf{M}(\\mathbf{U},\\mathbf{Z})$.\nWe use the implicit function theorem to solve for $d\\mathbf{X}$ in two steps: $$\\begin{array}{r l}&{d\\mathbf{U}=-\\mathbf{H}_{\\mathrm{U}}^{-1}\\mathbf{H}_{\\mathrm{Z}}d\\mathbf{Z},}\\ &{d\\mathbf{X}=\\mathbf{M}_{\\mathrm{U}}d\\mathbf{U}+\\mathbf{M}_{\\mathrm{Z}}d\\mathbf{Z}=\\mathbf{G}d\\mathbf{Z}.}\\end{array}$$\n\nThis section discusses solving for general equilibrium impulse responses in heterogeneous-agent models, focusing on the sequence-space Jacobian method and its accuracy compared to the Reiter method."
  },
  {
    "qid": "econ-empirical-1329-3-0-1",
    "question": "2) Explain why the coefficient on EXPSALES is significant and how its interpretation changes when outliers (plants with EXPSALES > 2) are removed.",
    "gold_answer": "1. EXPSALES measures production costs; higher costs increase closure probability.\n2. Initial coefficient: 0.040 ($t=2.48$), quasielasticity: 0.59%.\n3. After removing outliers (EXPSALES > 2), coefficient rises to 0.647 ($t=2.23$), quasielasticity: 9.3%.\n4. Outliers depress the average effect, masking the true sensitivity of closure to production costs.",
    "question_context": "The logit estimator in TSP (version 3.1b) was used to estimate coefficients, heteroskedastically robust standard errors and changes in the probability of closure with respect to changes in the explanatory variables $(X_{i})$. This 'quasielasticity' is calculated as $\\partial p/\\partial X_{i}=\\hat{p}(1-\\hat{p})\\beta_{i}$, where $\\hat{p}$ is the predicted probability of plant closure.\nTable 3 reports the logit estimation results and probability derivatives. Only CAPSALES is not statistically significant (the $t$-statistic is just outside the $5\\%$ level). The model as a whole is highly significant and correctly predicts $82\\%$ of the choices that firms made for their plants.\nPlants with high production costs, as measured by EXPSALES, were more likely to exit. The probability derivative at the mean rises to $9.3\\%$, which is the most elastic response of any variable.\nThe SHARE coefficient is negative and highly significant, indicating that larger plants were less likely to close. Increasing SHARE from two percentage points (the mean) to three would reduce closure probability by one percentage point.\nThe coefficient on YOUTH suggests that younger plants were more likely to exit. In the low CAPSALES subsample, the probability derivative was $2.9\\%$, in the high CAPSALES subsample it was $1.6\\%$.\nThe predicted probability of closure for a plant owned by a foreign controlled company was 4.7 percentage points below that of other plants.\nThe coefficient on MULTI shows that firms with more plants were more likely to close plants. Adding one plant at the mean of 5.88 plants per firm (a $17\\%$ increase), each plant owned by that firm would experience an increase in closure probability of 0.4 percentage points.\nThe industry characteristics have the expected effects: plants were more likely to close where the cuts in protection were greatest and where the protection had been based on import licensing.\nThe logit model was re-estimated with a set of plant size dummy variables interacting with SHARE, to see if the U-shaped pattern holds when controlling for other influences on closure.\n\nThe study analyzes plant-level data from the 1986/87 Economy-Wide Census of New Zealand, focusing on manufacturing units with at least 25 workers. The data was merged with the New Zealand Business Directory to track plant survival until November 1989. A logit model was employed to estimate the probability of plant closure, incorporating variables such as EXPSALES, SHARE, YOUTH, PIMPORT, and CAPSALES, among others."
  },
  {
    "qid": "econ-empirical-937-1-3-0",
    "question": "7) Derive the Lucas surprise supply function \\( y_t = y^n + \\alpha(\\pi_t - E_{t-1}\\pi_t) \\), clarifying the role of the variance ratio.",
    "gold_answer": "1. Firms observe local price \\( p_t = P_t + \\epsilon_t \\) but not aggregate \\( P_t \\).  \n2. Signal extraction: \\( E[P_t | p_t] = \\frac{\\sigma_P^2}{\\sigma_P^2 + \\sigma_\\epsilon^2} p_t \\).  \n3. Supply decision: \\( y_t = y^n + \\beta(E[P_t | p_t] - E_{t-1} P_t) \\).  \n4. Aggregating and using \\( \\pi_t = P_t - P_{t-1} \\) yields the result, where \\( \\alpha = \\beta \\cdot \\frac{\\sigma_P^2}{\\sigma_P^2 + \\sigma_\\epsilon^2} \\).",
    "question_context": "The slope of the Phillips curve in the Lucas model combines a structural parameter (supply elasticity) and a variance ratio affected by policy, leading to the Lucas critique.\n\nThe Lucas (1973) model derives a Phillips curve from imperfect information, emphasizing monetary non-neutrality and policy ineffectiveness."
  },
  {
    "qid": "econ-empirical-115-1-0-2",
    "question": "3) Prove that the one-step estimator is no less efficient than the two-step estimator for both $\\theta_{1}$ and $\\theta_{2}$.",
    "gold_answer": "To prove the one-step estimator is no less efficient than the two-step estimator:\n\n1. The one-step estimator uses all moment conditions simultaneously, while the two-step estimator uses them sequentially.\n2. The asymptotic variance of the one-step estimator is $(D^{\\prime}C^{-1}D)^{-1}$, which achieves the efficiency bound.\n3. The two-step estimator's asymptotic variance is $B C B^{\\prime}$, where $B$ captures the sequential estimation process.\n4. By the Gauss-Markov theorem, simultaneous estimation (one-step) is at least as efficient as sequential estimation (two-step) because it exploits all available information optimally.\n5. Thus, $\\mathbb{V}_{\\mathrm{two-step}} \\succeq \\mathbb{V}_{\\mathrm{one-step}}$ in the positive semi-definite sense.",
    "question_context": "Suppose that we can partition $\\theta$ into subsets of parameters $(\\theta_{1}^{\\prime},\\theta_{2}^{\\prime})^{\\prime}$ and $h(\\cdot)$ into subsets of functions $(h_{1}(\\cdot)^{\\prime},h_{2}(\\cdot)^{\\prime})^{\\prime}$ . If we suppress $w^{*}$ for notational convenience, we can write \n\n$$\n\\begin{array}{l}{{\\mathbb{E}}[h_{1}(\\theta_{1},\\theta_{2})]=0,}\\ {{\\mathbb{E}}[h_{2}(\\theta_{2})]=0,}\\end{array}\n$$ \n\nwhere $\\theta_{1}\\in\\Theta_{1},\\theta_{2}\\in\\Theta_{2},h_{1}(\\cdot)$ and $h_{2}(\\cdot)$ are $m_{1^{-}}$ and $m_{2}$ -vectors of known functions, respectively $(m=m_{1}+m_{2}$ . We consider the general case of overidentification, i.e., $m_{1}\\geq p_{1}$ and $m_{2}\\geq p_{2}$ . These identification conditions (plus the corresponding rank conditions assumed below) ensure that $\\theta_{2}$ is identified by (B) alone, and that, given $\\theta_{2},\\theta_{1}$ is identified by (A) alone, so that two-step estimation is possible.\nThe optimal weighting matrix for GMM will be the inverse of the following covariance matrix or its components: \n\n$$\nC=\\mathbb{V}[h(\\theta)]=\\left[{C_{11}\\quad C_{12}}\\right],\n$$ \n\nwhere we assume that $C$ is finite and nonsingular so its inverse exists: $C^{-1}={\\binom{C^{11}}{C^{21}}}\\quad C^{12}\\mathrm{]}.$ . Define the $(m_{1}+m_{2})\\times(p_{1}+p_{2})$ matrix of expected derivatives \n\n$$\nD=\\mathbb{E}\\nabla_{\\boldsymbol{\\theta}}h(\\boldsymbol{\\theta})=\\left[\\begin{array}{c c}{D_{11}}&{D_{12}}\\\\ {0}&{D_{22}}\\end{array}\\right].\n$$ \n\nWe assume that $D_{11}$ and $D_{22}$ are of full column rank so that $h_{2}$ alone identifies $\\theta_{2}$ , and $h_{1}$ alone identifies $\\theta_{1}$ given $\\theta_{2}$ .\nTheorem 2.1. Let Vone-step, Vtwo-step, $\\mathbb{V}_{\\mathtt{K N O W}-\\theta_{2}}$ , and $\\mathbb{V}_{\\mathrm{KNOW}-\\theta_{2}}$ -joint denote the asymptotic variance of the one-step, two-step, know- $\\cdot\\theta_{2}$ , and know- $\\cdot\\theta_{2}$ -joint estimators, respectively. Then, \n\n$$\n\\begin{array}{r l}&{\\mathbb{V}_{\\mathrm{ong-STEP}}=(D^{\\prime}C^{-1}D)^{-1}}\\\\ &{\\mathbb{V}_{\\mathrm{TW0-STEP}}=B C B^{\\prime}}\\\\ &{\\mathbb{V}_{\\mathrm{KNow-}\\theta_{2}}=(D_{11}^{\\prime}C_{11}^{-1}D_{11})^{-1}}\\\\ &{\\mathbb{V}_{\\mathrm{KNow-}\\theta_{2}\\mathrm{-}\\mathrm{JoINT}}=(D_{11}^{\\prime}C^{11}D_{11})^{-1}}\\end{array}\n$$ \n\nwhere $B$ is defined in Eq. (39) of the Appendix.\nTheorem 2.2. For the estimators defined in Definitions 2.1–2.4 with asymptotic variances given in Eqs. (6)–(9), respectively, the following statements hold: \n\n1. know- $\\cdot\\theta_{2}$ -joint is no less efficient than one-step, two-step, and know- $\\cdot\\theta_{2}$ .   \n2. If $C_{12}=0$ then know- $\\cdot\\theta_{2}$ -joint and know- $\\cdot\\theta_{2}$ are equally efficient [M-redundancy].   \n3. If $D_{12}=0$ then two-step and know- $\\cdot\\theta_{2}$ are equally efficient for $\\theta_{1}$ .   \n4. If $C_{12}~=~0$ and $D_{12}=0$ then one-step, two-step, know- $\\cdot\\theta_{2}$ - joint and know- $\\cdot\\theta_{2}$ are all equally efficient for $\\theta_{1}$ , and one-step and two-step are equally efficient for $\\theta_{2}$ [M/P-redundancy].   \n5. one-step is no less efficient than two-step (for both $\\theta_{1}$ and $\\theta_{2}$ ).   \n6. If $m_{1}=p_{1}$ then the one-step and two-step estimates of $\\theta_{2}$ are equal.   \n7. If $m_{1}~=~p_{1}$ and $m_{2}~=~p_{2}$ then the one-step and two-step estimates are equal (for both $\\theta_{1}$ and $\\theta_{2}$ ).   \n8. If $m_{1}~=~p_{1}$ and $C_{12}~=~0$ then the one-step and two-step estimates are equally efficient (for both $\\theta_{1}$ and $\\theta_{2}$ ).   \n9. If $D_{12}~=~C_{12}C_{22}^{-1}D_{22}$ then know- $\\cdot\\theta_{2}$ -joint and one-step are equally efficient for $\\theta_{1}$ ${}^{\\prime}P$ -redundancy], and one-step and twostep are equally efficient for $\\theta_{2}$ .   \n0. If $D_{12}=C_{12}C_{22}^{-1}D_{22}$ then one-step, two-step and know- $\\cdot\\theta_{2}$ - joint are no less efficient for $\\theta_{1}$ than know- $\\cdot\\theta_{2}$ .\n\nThe text discusses the partitioning of parameters and functions in GMM estimation, the conditions for identification, and the efficiency of different estimators under various conditions."
  },
  {
    "qid": "econ-empirical-620-1-1-1",
    "question": "2) Explain why bootstrap methods that only resample in the time dimension fail to capture the necessary bias term $\\varDelta_{\\delta}$.",
    "gold_answer": "The failure occurs because:\n1) Time-only resampling leads to $\\Gamma^{*} = 0$ as shown above\n2) The bias term $\\varDelta_{\\delta}$ depends on $\\Gamma_t$ through $\\Sigma_{\\tilde{F}}$\n3) With $\\Gamma^{*} = 0$, the bootstrap bias $\\varDelta_{\\delta}^{*} = 0$\n4) But the true asymptotic bias $-c\\varDelta_{\\delta} \\neq 0$ when $\\sqrt{T}/N \\to c \\neq 0$\n5) Thus the bootstrap distribution is incorrectly centered at zero\n6) This failure occurs even without cross-sectional dependence, whenever $c \\neq 0$ and $\\alpha \\neq 0$",
    "question_context": "Proposition 2.1. Suppose $e_{t}^{\\ast}\\sim$ i.i.d. $\\left\\{\\tilde{e}_{t}-\\overline{{\\tilde{e}}}\\right\\}$ for $t=1,\\ldots,T$ . Then $T^{*}=0$ for any $N,T$ .\n\nThe proof of Proposition 2.1 follows trivially from the first order conditions that define $\\tilde{\\boldsymbol{\\Lambda}}$ . In particular,\n\n$$\n{{\\cal{T}}^{*}}=\\frac{1}{T}\\sum_{t=1}^{T}\\frac{1}{N}{{\\tilde{\\cal{A}}}^{\\prime}{\\cal{V}}a r^{*}\\left({e_{t}^{*}}\\right){\\tilde{\\cal{A}}}},\n$$\n\nwhere\n\n$$\nV a r^{*}\\left(e_{t}^{*}\\right)=E^{*}\\left(e_{t}^{*}e_{t}^{*\\prime}\\right)=\\frac{1}{T}\\sum_{t=1}^{T}\\left(\\tilde{e}_{t}-\\bar{\\tilde{e}}\\right)\\left(\\tilde{e}_{t}-\\bar{\\tilde{e}}\\right)^{\\prime}=\\frac{\\tilde{e}^{\\prime}\\tilde{e}}{T}-\\frac{\\tilde{e}^{\\prime}u^{\\prime}\\tilde{e}}{T},\n$$\n\nand where $\\tilde{e}$ is a $T\\times N$ matrix with rows given by $\\tilde{{\\boldsymbol{e}}}_{t}^{\\prime}$ and $\\iota=(1,\\ldots,1)^{\\prime}$ is $T\\times1$ . It follows that\n\n$$\n{{\\varGamma}^{*}}=\\frac{1}{N}\\tilde{A}^{\\prime}V a r^{*}\\left({{e}_{t}^{*}}\\right)\\tilde{A}=\\frac{1}{N T}\\left({{\\tilde{A}}^{\\prime}{{\\tilde{e}}^{\\prime}}{{\\tilde{e}}}\\tilde{A}-{{\\tilde{A}}^{\\prime}}{{\\tilde{e}}^{\\prime}}{{u}^{\\prime}}{{\\tilde{e}}}\\tilde{A}}\\right)=0\n$$\n\nsince $\\tilde{\\boldsymbol{A}}^{\\prime}\\tilde{\\boldsymbol{e}}^{\\prime}=0$ by the first order conditions that define $\\left(\\tilde{F},\\tilde{\\boldsymbol{\\Lambda}}\\right)$ . Notice that this result holds for any possible value of $(N,T)$ .\nThe main implication of Proposition 2.1 is that the i.i.d. bootstrap distribution is centered at zero (i.e. $\\varDelta_{\\delta}^{*}=0$ because $\\varDelta\\v{\\delta}_{\\delta}*$ is a linear function of $T^{*}$ and ${\\cal T}^{*}=0$ ). Since the OLS estimator is asymptotically biased when the cross sectional dimension is relatively small compared to the time series dimension (i.e. when $\\sqrt{T}/N\\to c\\neq0;$ , the i.i.d. bootstrap does not replicate this important feature of the OLS distribution. Note that this failure of the i.i.d. bootstrap holds regardless of whether cross sectional dependence exists or not. The problem is not that the i.i.d. bootstrap does not capture cross sectional dependence. Rather the problem is that it induces a zero bias term which should be there even under cross sectional independence as long as $-c\\varDelta_{\\delta}\\neq0$ (i.e. as long as $c\\neq0$ and $p\\operatorname*{lim}\\hat{\\alpha}=H_{0}^{-1\\prime}\\alpha\\neq0)$ .\n\nThis section discusses the limitations of bootstrap methods that only resample in the time dimension for factor-augmented regression models, showing that they fail to capture the necessary cross-sectional dependence."
  },
  {
    "qid": "econ-empirical-108-69-0-2",
    "question": "3) Statistically reconcile the discrepancy between Lindahl (2005)'s mental health estimates and the current study's results, incorporating power analysis and effect size assumptions.",
    "gold_answer": "1. **Lindahl's Estimate**: 0.42 SD effect (SE = 0.19) for 1M SEK.  \n2. **Current Study**: Rejects effects > 0.06 SD.  \n3. **Power Analysis**: If true effect = 0.06, Lindahl's power = 6.2%.  \n4. **Overestimation Bias**: Significant results in low-power studies exaggerate effects by a factor of 7.4.",
    "question_context": "No variables in administrative records can be unambiguously interpreted as measures of parental investments or parenting quality, but our analysis plan specified five outcomes that may be of some relevance for testing theories of how wealth impacts children’s outcomes.\nFor three of the outcomes—duration of maternal leave, paternal leave, and school quality—we consider the estimates precise enough to conclude that the behavioral responses on these margins must be very small.\nOur results suggest that in a model of child development parameterized to match conditions in Sweden, the effect of permanent income on children’s outcomes is small.\nOur findings are also strikingly similar to those reported by Bleakley and Ferrie (forthcoming) in a study exploiting the randomized assignment of land in a lottery held in early nineteenth-century Georgia.\n\nThe study examines the impact of wealth shocks on parental behaviors and child outcomes, using Swedish administrative data. It focuses on variables like asset transfers, school quality, parental mental health, maternal smoking, and parental leave duration. The findings suggest minimal causal effects of wealth shocks on these outcomes, with precise estimates ruling out large effects."
  },
  {
    "qid": "econ-empirical-869-0-1-2",
    "question": "7) Discuss the implications of serially correlated unobservables on the estimation procedure and the robustness of the proposed estimators.",
    "gold_answer": "Serially correlated unobservables complicate estimation because:\n1. Agents condition their behavior on unobserved states, which depend on the current number of active firms.\n2. The paper shows that the impact of such unobservables depends on the data available, but the estimators remain robust if independent measures of profits are available.",
    "question_context": "All estimators are semiparametric. There is a first stage that provides a nonparametric estimate of the entry and continuation values and a second stage that treats these estimates as true values in a parametric estimation routine.\nThe computational burden of our estimator is comparable with that of the estimators for the simple static entry models.\nThe Monte Carlo results, when combined with a discussion of why they occur, end up being quite informative. Among the alternatives we consider, only one, perhaps two, should be used, and the best performing alternatives are also the least computationally burdensome.\n\nThe paper outlines semiparametric estimators for dynamic games, discussing their computational ease and robustness. It compares alternative estimators and their performance in Monte Carlo simulations."
  },
  {
    "qid": "econ-empirical-695-4-0-2",
    "question": "3) Show why the EPR setup cannot be used to send superluminal signals, despite the non-local correlations.",
    "gold_answer": "1. The measurement outcome on one side (e.g., right) is correlated with the outcome on the other side (left), but not with the detector setting on the left.\n2. The probability of measuring up on the right is always \\[ P(\\uparrow_R) = \\frac{1}{2} \\], regardless of the left detector's orientation.\n3. Since the detector setting on the left does not affect the outcome probabilities on the right, no information can be transmitted superluminally.",
    "question_context": "The quantum theory tells us that given the state of the system prior to measurement, the probability of getting a down-measurement on a given side is $1/2$, but the probability of getting an up-measurement on one side conditional on getting a down-measurement on the other side is 1 (likewise with up and down interchanged).\nBell considered settings of the detectors at various angles relative to one another in the Bohm version of EPR, and argued that if the setting could be chosen very quickly, the kind of locality that Einstein wanted required that the hidden state be independent of the settings of the detectors, as well as that the measurement results be independent conditional on the hidden states.\nBecause of the indeterministic nature of the measurement process, we cannot control the outcome of a measurement but only the nature of the measurement made. Thus, in the Bohm version of the EPR, we can control the orientation of the Stern-Gerlack apparatus.\n\nQuantum theory introduces non-locality, challenging classical notions of causation. The Einstein-Podolsky-Rosen (EPR) thought experiment and Bohm's simplified version illustrate this with spin 1/2 particles in a singlet state. Bell's theorem further shows that no local hidden variable theory can reproduce quantum mechanical probabilities, leading to the conclusion that nature is non-local."
  },
  {
    "qid": "econ-empirical-79-47-0-3",
    "question": "4) Analyze the impact of a jump J > s*/2 on the liquidity provider and stale-quote snipers, including the probability of sniping and the expected profits/losses.",
    "gold_answer": "1. **Liquidity Provider**:\n   - **Action**: Immediately cancels stale quotes at y_{t-} ± s*/2 and replaces them at y_t ± s*/2.\n   - **Loss**: If the provider loses the race (probability (N-1)/N), they lose E(J - s*/2 | J > s*/2).\n2. **Snipers**:\n   - **Action**: Each sniper attempts to trade at the stale quote y_{t-} ± s*/2.\n   - **Profit**: Each sniper wins the race with probability 1/N and earns J - s*/2.\n3. **Expected Outcomes**:\n   - Total expected loss for provider: λ_jump ⋅ Pr(J > s*/2) ⋅ E(J - s*/2 | J > s*/2) ⋅ (N-1)/N.\n   - Total expected profit for all snipers: λ_jump ⋅ Pr(J > s*/2) ⋅ E(J - s*/2 | J > s*/2) ⋅ (N-1)/N.",
    "question_context": "The signal y, and hence the fundamental value of security x, evolves as a compound Poisson jump process with arrival rate λ_jump and jump distribution F_jump. The jump distribution has finite bounded support and is symmetric with mean zero.\nInvestors arrive stochastically to the market with an inelastic need to either buy or sell a unit of x. The arrival process is Poisson with rate λ_invest, and, conditional on arrival, it is equally likely that the investor needs to buy versus sell.\nTrading firms (HFTs) have no intrinsic demand to buy or sell x. Their goal is to buy x at prices lower than y and sell x at prices higher than y. They act as both makers and takers of liquidity.\nIn equilibrium, the bid-ask spread s leaves trading firms indifferent between liquidity provision and stale-quote sniping. The equilibrium bid-ask spread s* is uniquely characterized by the solution to: λ_invest ⋅ (s/2) = λ_jump ⋅ Pr(J > s/2) ⋅ E(J - s/2 | J > s/2).\nIn the multi-unit demand model, the bid-ask spread for the kth unit of liquidity, s_k, is governed by indifference between liquidity provision and stale-quote sniping at the kth level of the book. Spreads are strictly increasing: s_1* < s_2* < ... < s_q*.\n\nThis section discusses the size of the prize in the speed race and critiques the continuous limit order book market design. It presents a model with a security x and a perfect public signal y, where y evolves as a compound Poisson jump process. The model includes investors and trading firms (HFTs), with trading firms acting as liquidity providers and stale-quote snipers."
  },
  {
    "qid": "econ-empirical-468-1-1-2",
    "question": "3) Explain the fixed effects estimator for $\\mu_{i}$ in the Poisson model. Why is the likelihood conditional on $\\sum_{s=1}^{T}n_{i s}$?",
    "gold_answer": "1. Fixed effects estimator for $\\mu_{i}$ is derived from FOC $\\partial\\ln\\mathcal{L}/\\partial\\mu_{i}=0$:\\n$$\\exp(\\mu_{i})=\\frac{\\sum_{t=1}^{T}n_{i t}}{\\sum_{t=1}^{T}\\exp(X_{i t}^{\\prime}\\beta)}.$$\\n2. The likelihood is conditional on $\\sum_{s=1}^{T}n_{i s}$ because:\\n   - $\\mu_{i}$ is absorbed into $p_{i t}=\\exp(X_{i t}^{\\prime}\\beta)/\\sum_{s=1}^{T}\\exp(X_{i s}^{\\prime}\\beta)$.\\n   - This avoids direct estimation of $\\mu_{i}$ for each firm, reducing dimensionality.",
    "question_context": "The log-likelihood of the basic Poisson model is given by: $$\\ln\\mathcal{L}=\\sum_{t=1}^{T}\\sum_{i=1}^{N}n_{i t}\\ln\\lambda_{i t}-\\ln(n_{i t}!)-\\lambda_{i t}.$$\nThe log-likelihood of the random effects model is: $$\\ln\\mathcal{L}=\\sum_{t=1}^{T}\\sum_{i=1}^{N}\\ln\\Gamma(\\delta+n_{i t})-\\ln\\Gamma(\\delta)-\\ln\\Gamma(n_{i t}+1)+n_{i t}\\ln\\left[\\frac{\\exp(\\mu_{0}+X_{i t}^{\\prime}\\beta)}{\\delta}\\right]-(n_{i t}+\\delta)\\ln\\left[1+\\frac{\\exp(\\mu_{0}+X_{i t}^{\\prime}\\beta)}{\\delta}\\right].$$\nThe log-likelihood of the fixed effects model is: $$\\ln\\mathcal{L}=\\sum_{t=1}^{T}\\sum_{i=1}^{N}n_{i t}\\ln\\left[p_{i t}\\sum_{s=1}^{T}n_{i s}\\right]-\\ln(n_{i t}!)-p_{i t}\\sum_{s=1}^{T}n_{i s}.$$\nThe latent-factor Poisson model is estimated by maximum simulated likelihood (MSL) method, integrating out latent variables: $$\\mathcal{L}=\\int_{\\mathbb{R}^{T}}\\prod_{t=1}^{T}\\prod_{i=1}^{N}\\frac{\\exp(-\\lambda_{i t})\\lambda_{i t}^{n_{i t}}}{n_{i t}!}f_{t}^{*}(l_{t}^{*}|l_{t-1}^{*})\\mathrm{d}L^{*}.$$\n\nThis section details the log-likelihood functions for various Poisson models, including basic, random effects, fixed effects, and latent-factor specifications, along with estimation methods."
  },
  {
    "qid": "econ-empirical-994-3-0-1",
    "question": "2) Derive the null hypothesis H_0: Aπ_(-J) ≤ b from the K-monotonicity and upper bound conditions given in Hypothesis (13).",
    "gold_answer": "1. **Hypothesis (13)**: States that g_s is K-monotone and (−1)^k g_s^(k) ≤ B_s^(k) for k=0,1,…,K.\n2. **Population proportions**: The restrictions on g_s translate to linear inequalities on π_(-J).\n3. **Matrix form**: These inequalities can be written as Aπ_(-J) ≤ b, where A encodes the K-monotonicity and bound constraints, and b contains the bounds B_s^(k).",
    "question_context": "We say a function ξ is K-monotone on some interval T if 0≤(−1)^k ξ^(k)(x) for every x∈ T and all k=0,1,…,K, where ξ^(k) is the kth derivative of ξ. By definition, a completely monotone function is K-monotone.\nConsider the null hypothesis: sK-monotone and (−1)^k g_s^(k) ≤ B_s^(k), for k=0,1,…,K, where s=1 for one-sided t-tests, s=2 for two-sided t-tests, and B_s^(k) is defined in Theorem 3.\nHypothesis (13) implies restrictions on the population proportions π := (π_1,…,π_J)′, which can be expressed as H_0: Aπ_(-J) ≤ b, where π_(-J) := (π_1,…,π_(J-1))′.\n\nThis section discusses the theoretical framework for testing K-monotonicity and establishing upper bounds on p-curves and their derivatives, as derived from Theorems 2 and 3."
  },
  {
    "qid": "econ-empirical-1424-0-0-2",
    "question": "3) Prove that the welfare result holds for both closed and open economies, provided TFP is calculated using domestic absorption rather than GDP. What is the key theoretical distinction?",
    "gold_answer": "1. In an open economy, GDP includes net exports, while absorption \\( C + I + G \\) reflects domestic demand.\n2. Household welfare depends on consumption \\( C \\) and investment \\( I \\), which are components of absorption.\n3. Derive the national income identity for absorption: \\( Y = C + I + G + NX \\).\n4. Show that TFP calculated from absorption correctly captures the resources available for domestic welfare, whereas GDP-based TFP includes foreign demand shocks irrelevant to domestic consumers.",
    "question_context": "We show that the welfare of a country’s infinitely lived representative consumer is summarized, to a first order, by total factor productivity (TFP), appropriately defined, and by the capital stock per capita.\nWelfare-relevant TFP needs to be constructed with prices and quantities as perceived by consumers, not firms. Thus, factor shares need to be calculated using after-tax wages and rental rates.\nOur result applies to both closed and open economies, provided TFP is calculated using domestic absorption instead of GDP as the measure of output.\n\nThe paper establishes that the welfare of a country's representative consumer can be approximated by total factor productivity (TFP) and capital stock per capita, under a wide range of economic conditions."
  },
  {
    "qid": "econ-empirical-226-0-0-3",
    "question": "4) Formally analyze the polar case where all workers must work the same hours, and derive the conditions under which layoffs are optimal.",
    "gold_answer": "1. **Model Setup**: \n   - Constraint: \\( h_i = h \\) for all \\( i \\). \n   - Firm maximizes \\( \\sum_{i} U_i(w_i, h) + \\lambda \\pi \\). \n2. **Optimality of Layoffs**: \n   - For some states of demand, \\( h^* \\) may be infeasibly low for high-productivity workers. \n   - Layoffs occur for workers where \\( U_i(0, 0) > U_i(w_i, h^*) \\). \n3. **Example**: \n   - Suppose \\( U_i(w, h) = \\sqrt{w} - h^2 \\). \n   - If \\( h^* < \\sqrt{w_i} \\), layoffs are optimal for worker \\( i \\).",
    "question_context": "This paper presents an implicit-contract model in which workers are allowed to differ in both their productive abilities and their preferences. It is shown that if firms are able to vary hours costlessly among their workers, workers are risk averse, and there are no outside payments to laid-off workers, then efficient contracts between firms and their workers will never provide for layoff unemployment.\nIf, however, such hours variations are not costless, layoffs are no longer generally inefficient since they are an alternative means by which firms can adjust the labor inputs of selected groups of workers.\nModels in the implicit-contract literature typically assume that all workers are exactly alike. This is not a very natural assumption since the long-term attachments between firms and workers required for the implicit-contract approach to make sense presumably exist as a result of costs to workers of finding alternative employment and costs to firms of replacing workers who quit--costs which would be minimal in a world of homogeneous workers.\n\nThis section introduces an implicit-contract model where workers differ in productive abilities and preferences, examining conditions under which layoffs become efficient."
  },
  {
    "qid": "econ-empirical-948-5-1-1",
    "question": "6) Prove PROPOSITION 6 by analyzing the cases $k<1$ (partial risk sharing) and $k\\geq1$ (full risk sharing).",
    "gold_answer": "1. **$k<1$**: Middle-$\\omega$ banks earn profits as intermediaries ($n_{E}(\\frac{1}{2})>0$).  \n2. **$k\\geq1$**: Extreme-$\\omega$ banks trade directly, eliminating intermediation ($n_{E}(\\frac{1}{2})=0$).  \n3. **Uniqueness**: Strategic substitutability in middle-$\\omega$ entry prevents multiplicity.",
    "question_context": "The equilibrium distribution of traders is symmetric: $\\Omega=\\{0,\\frac{1}{2},1\\}$, $\\pi(\\omega)=1/3$, and $n(0)=n(1)$.\nPROPOSITION 6: There exists a unique equilibrium with positive entry, $\\mu_{E}$. If $k<1$, $n_{E}(\\frac{1}{2})>0$; if $k\\geq1$, $n_{E}(\\frac{1}{2})=0$.\nCOROLLARY 2: When $c$ decreases, $\\mu_{E}(\\omega)$ increases for all $\\omega$, and $n_{E}(\\frac{1}{2})$ increases.\nCOROLLARY 3: When $k$ increases, $\\mu_{E}(\\frac{1}{2})$ is nonmonotonic: increasing for $k\\simeq0$ and vanishing for $k\\to1$.\n\nThis section presents a parametric model with three symmetric types to analyze equilibrium entry and comparative statics."
  },
  {
    "qid": "econ-empirical-79-13-0-3",
    "question": "4) How are the PanelViews surveys used to augment the Homescan data? What specific variables are derived from these surveys, and how are they utilized in the analysis?",
    "gold_answer": "PanelViews surveys augment Homescan data by providing:\n\n1. **Occupation data**: Matched to median earnings using BLS codes, grouped into categories (e.g., health care).\n2. **Product knowledge**: For headache remedies, respondents identify active ingredients (e.g., ibuprofen for Advil).\n3. **Attitudes**: Likert-scale responses on store-brand safety (1 = agree, 7 = disagree).\n4. **College major**: Health vs. non-health science majors.\n\nUsage:\n\n- Primary shopper characteristics (occupation, knowledge) are linked to purchase behavior.\n- Regression analysis controls for these variables to isolate information effects.",
    "question_context": "The data include purchases made on more than 77 million shopping trips by 125,114 households from 2004 to 2011. Panelist households are given optical scanners and are asked to scan the barcodes of all consumer packaged goods they purchase, regardless of outlet or store format.\nFor each purchase, we observe the date, the universal product classification (UPC) code, the transaction price, an identifier for the store chain in which the purchase was made, and the size of the item, which we convert to equivalent units specific to a given product category.\nNielsen supplies household demographic characteristics including the education of the household head, a categorical measure of household income, number of adults, race, age, household composition, home ownership, and the geographic market of residence.\nWe classify products as store brands using Nielsen-provided codes, supplemented with manual corrections. To compare store brands and national brands we aggregate products into comparable product groups, which are sets of products that are identical on all product attributes except for brand and item size.\n\nThe backbone of our data is the Nielsen Homescan Panel, which includes purchasing behavior for a panel of households from 2004 to 2011. To this we add data on occupation and product knowledge from two custom surveys administered to Nielsen Homescan panelists in September 2008 and October 2011."
  },
  {
    "qid": "econ-empirical-108-58-0-1",
    "question": "2) Formally show how pension wealth allocation is performed based on pension income and wages, and explain the 60-40% split between current pensioners and wage earners. How does this ensure consistency with SCF data?",
    "gold_answer": "1. Let \\( W_p \\) be total pension wealth. Allocate \\( 0.6W_p \\) to current pensioners proportionally to pensions received \\( P_i \\): \\( W_{p,i} = \\frac{P_i}{\\sum P_i} \\cdot 0.6W_p \\).\n2. Allocate \\( 0.4W_p \\) to wage earners proportionally to wages above the median \\( W_i \\): \\( W_{w,i} = \\frac{W_i}{\\sum W_i} \\cdot 0.4W_p \\).\n3. The 60-40 split is chosen to match the SCF's observed concentration of pension wealth (e.g., top 10% owns 53% in 2013). This split accounts for the fact that defined benefit pensions are less concentrated than defined contribution plans.",
    "question_context": "We infer the value of owner-occupied dwellings from property taxes paid. These taxes are itemized on tax returns by roughly the top one-third of the income distribution. Using information on total property taxes paid in the NIPAs, and consistent with what SCF data show, we estimate that itemizers own 75% of homes each year. We assume that homeowners all face the same effective property tax rate.\nPensions have been growing fast since the 1960s and now account for one-third of household wealth. Since many regulations prevent high income earners from contributing large amounts to their tax-deferred accounts, pension wealth is more evenly distributed than overall wealth. We allocate pension wealth based on how pension income and wages—both of which we observe at the micro-level—are distributed in such a way as to match the distribution of pension wealth in the SCF.\nAlthough interest from state and local government bonds is tax exempt, it has been reported on individual tax returns since 1987. Before 1987, we assume that it is distributed as in 1987, with 97% of municipal bonds belonging to the top 10% of the wealth distribution and 32% to the top 0.1%.\nOur estimates fully incorporate the wealth held by individuals through trusts. Trusts are entities set to distribute income—and possibly wealth—to individual beneficiaries and charities. Trust income distributed to individuals flows to the beneficiaries’ individual tax returns, directly to the dividend, realized capital gain, or interest lines for such income, and to Schedule E fiduciary income for other income such as rents and royalties.\nWe account for offshore wealth in supplementary series by assuming that it is distributed as trust income, that is, highly concentrated. Top wealth shares rise even more when including offshore wealth: the top 0.1% owned 23.0% of total wealth—instead of 22.1% in our baseline estimate—in 2012.\n\nThe analysis involves dealing with assets that do not generate taxable income, such as pensions and owner-occupied houses. These assets are sizable but do not significantly affect top wealth shares due to limited uncertainty in their distribution and their small fraction at the top end of the wealth distribution."
  },
  {
    "qid": "econ-empirical-939-0-0-3",
    "question": "4) For the Online News Popularity dataset, the PDP in Figure 6(a) shows a smaller causal effect of keywords than the marginal effect. Propose a causal diagram and structural equation model to explain this discrepancy.",
    "gold_answer": "1. **Causal diagram**: Let \\(K\\) (keywords), \\(C\\) (confounders, e.g., article quality), and \\(S\\) (shares). Assume \\(C \\rightarrow K\\), \\(C \\rightarrow S\\), and \\(K \\rightarrow S\\).  \n2. **Structural equations**: \\[ S = \\beta K + \\gamma C + \\epsilon \\]  \n3. Marginal effect \\(E[S|K]\\) captures \\(\\beta + \\gamma \\cdot \\text{bias}(C|K)\\), while PDP adjusts for \\(C\\), isolating \\(\\beta\\). The discrepancy implies \\(\\gamma \\neq 0\\) (confounding).",
    "question_context": "Friedman’s partial dependence plot has exactly the same formula as Pearl’s back-door adjustment.\nThree requirements to make causal interpretations: a model with good predictive performance, some domain knowledge in the form of a causal diagram, and suitable visualization tools.\nThe PDP and ICE plots can suggest causal hypotheses which should be verified by a more carefully designed study.\n\nThe article discusses the intersection of machine learning and causal inference, focusing on extracting causal interpretations from black-box models using tools like partial dependence plots (PDP) and individual conditional expectation (ICE) plots."
  },
  {
    "qid": "econ-empirical-497-3-0-2",
    "question": "3) Discuss the challenges and implications of instrumenting all variables in the logit analysis, as presented in columns 3 and 4 of Tables 2a and 2b.",
    "gold_answer": "1. Instrumenting all variables addresses potential endogeneity due to unobserved location-specific effects.\n2. Lack of sufficient instruments requires using lagged values, which may introduce collinearity or weak instruments.\n3. Results show surprising outcomes: DRS specification estimates 8.5 lives lost per execution, while DW estimates 9.2 net lives saved.\n4. Wide confidence intervals imply that neither estimate is statistically significant, and differences between specifications are not conclusive.",
    "question_context": "In Table 2 we present the results obtained from running the analysis assuming that the errors follow a logit distribution. We use the joint outcome probabilities implied by the individual choice problem as opposed to the conditional probabilities, so these results employ the theoretically correct set of deterrence variables.\nTables 2a and 2b show that the specification of the error term has very large effects on the estimated number of net lives saved. For the DRS specification, in Table 2a, moving from a linear to a logistic error distribution assumption reverses their claims: each execution is associated with an average increase of 1.8 murders.\nThe net utility function for this specification is given by a detailed equation incorporating various deterrence variables and individual-specific coefficients.\nUnder the assumption of logit errors, this gives us the following expression for the aggregate county-level murder rate, involving integration over the distribution of random coefficients.\n\nThe text discusses the impact of error term specification on the estimated effects of capital punishment on murder rates, comparing linear and logistic error distributions. It also explores the implications of instrumenting variables and introduces a random coefficients model to account for preference heterogeneity."
  },
  {
    "qid": "econ-empirical-626-2-0-4",
    "question": "5) Construct a mathematical model for brand switching behavior in the catsup category that can generate the diverse patterns observed (loyalty, gradual switching, and sporadic purchasing), incorporating both household heterogeneity and temporal dynamics.",
    "gold_answer": "1. **Model Framework**:\\n   $$\\begin{aligned} \\text{Pr}(\\text{Brand}_t = k) &= \\text{softmax}(\\theta_{t,k}) \\\\ \\theta_{t,k} &= \\alpha_k + \\beta_k t + \\epsilon_{k,t} \\end{aligned}$$\\n2. **Components**:\\n   - $\\alpha_k$: Baseline preference (heterogeneity)\\n   - $\\beta_k$: Trend component (captures gradual switching)\\n   - $\\epsilon_{k,t} \\sim \\text{AR}(1)$: Short-term fluctuations\\n3. **Special Cases**:\\n   - Loyalty: $\\beta_k = 0, \\text{Var}(\\epsilon) \\approx 0, \\alpha_{\\text{leader}} \\gg \\alpha_{\\text{others}}$\\n   - Gradual switching: $\\beta_{\\text{leader}} < 0, \\beta_{\\text{others}} > 0$\\n   - Sporadic: High $\\text{Var}(\\epsilon)$ with infrequent $\\theta_{\\text{leader}} > \\theta_{\\text{others}}$ spikes",
    "question_context": "The time series of the number of shopping trips per week is a good overall reflection of a customer's recorded shopping activities. For a given customer, this time series sheds light on when, how often, and how regularly the customer shops.\nEach customer's time series of the number of weekly shopping trips corresponds to a point in the Cartesian product space $\\mathbf{R}^{\\bar{5}2}$. The data for our customer sample then form a point cloud in this 52-dimensional space, and we seek the desired insights by inspection of this 52-dimensional point cloud through combinations of judiciously chosen low-dimensional representations.\nLet S be the $C\\times W$ matrix such that $S_{c,w}$ is the number of shopping trips for customer household $c$ during week $w$ and denote by $\\mathbf{S}_{*}$ the matrix obtained from S by first transforming each entry and then subtracting from each column the respective column mean. We chose a power transformation with an exponent of .1 to significantly reduce the skewness in the distribution across customers of weekly shopping trips.\nDenote by $\\mathbf{H}$ an orthogonal matrix and by $\\mathbf{D}$ a diagonal matrix with entries in decreasing order such that ${\\frac{1}{C-1}}{\\bf S}_{*}^{t}{\\bf S}_{*}={\\bf H}^{t}{\\bf D}{\\bf H}$.\nThe principal-components view suggests that customer C999 is an unusual customer given the distance in this plot between this customer and the remaining customers. The case-profile plot confirms that this is an unusual customer and also shows us in what sense: There are no recorded shopping trips for the first 28 weeks of the study period, but for the rest of the year this household's members make between 1 and 10 recorded shopping trips per week.\nInspection of the eigenvector corresponding to the first principal component suggested that this component is an aggregate measure of the number of shopping trips over the study period. Similarly, the second principal component is a measure of trend. Customers high on this dimension tend to increase the number of weekly shopping trips during the course of the study; for customers low on this dimension, the trend is negative.\nThe two variables 'total volume of the leading catsup brand purchased' and 'total volume of all other catsup brands purchased' reflect customer heterogeneity, which accounts for a considerable amount of the variation in weekly catsup purchases.\n\nThe data used in this example come from a midsized market equipped with technology to conduct tests on marketing-mix variables, such as product attributes, pricing, and promotion, for new as well as established brands. Records of shopping trips and purchases for about 3,500 panel households cover a period of about three years (1986 to 1988). These records were collected by issuing members of participating households identification cards that identified them as panel members. When they showed these cards at the checkout counters of participating stores, all of their purchases were recorded electronically. In addition to the purchase records, changes in household characteristics and the applicable marketing-mix variables were also recorded over this three-year period."
  },
  {
    "qid": "econ-empirical-560-2-0-1",
    "question": "2) Using Lemma 7, derive the conditions under which a binary relation $\\succcurlyeq$ satisfies probability trade-off consistency.",
    "gold_answer": "Lemma 7 states that $\\succcurlyeq$ satisfies probability trade-off consistency if and only if, for all rank-ordered sets $X=\\{x_{1},\\ldots,x_{n}\\}$, $Y=\\{y_{1},\\dots,y_{m}\\}$, $i\\in\\{2,\\ldots,n\\}$ such that $x_{i}\\succ x_{i-1}$, $j\\in\\left\\{2,\\dots,m\\right\\}$ such that $y_{j}\\succ y_{j-1}$, and for all $P^{*},Q^{*}\\in\\mathbb{P}_{X}^{*}$ and $R^{*},S^{*}\\in\\mathbb{P}_{Y}^{*}$, the implication $(6)\\Longrightarrow(7)$ holds. This ensures that the trade-offs are consistent across different measuring rods and axes.",
    "question_context": "If this independence condition is not satisfied, inconsistent orderings of probability trade-offs such as $[\\alpha;\\beta]\\succ^{t}[\\gamma;\\delta]$ and $\\left[\\gamma;\\delta\\right]\\succcurlyeq\\left[\\alpha;\\beta\\right]$ can be obtained.\nDEFINITION 6: We say that $\\succcurlyeq$ satisfies probability trade-off consistency if there do not exist probabilities $\\alpha,\\beta,\\gamma,\\delta$ such that both $[\\alpha;\\beta]\\succcurlyeq[\\gamma;\\delta]$ and $[\\gamma;\\delta]\\succ^{t}$ $[\\alpha;\\beta]$.\nLEMMA 7: The binary relation $\\succcurlyeq$ satisfies probability trade-off consistency if and only if, for all rank-ordered sets $X=\\{x_{1},\\ldots,x_{n}\\}$ ， $Y=\\{y_{1},\\dots,y_{m}\\}$ ， $i\\in\\{2,\\ldots,n\\}$ such that $x_{i}\\succ x_{i-1}$ ， $j\\in\\left\\{2,\\dots,m\\right\\}$ such that $y_{j}\\succ y_{j-1},P^{*},Q^{*}\\in\\mathbb{P}_{X}^{*}$ and $R^{*}$ $S^{*}\\in\\mathbb{P}_{Y}^{*}$.\nPROPOSITION 8: RDU implies probability trade-off consistency.\nTHEOREM 9: $L e t\\:\\approx b e$ a preference relation on $\\mathbb{P}^{s}$ . Then, RDU holds on $\\mathbb{P}^{s}$ if and only if the following conditions are satisfied: A1. Weak ordering; A2. Stochastic dominance; A3. Jensen-continuity; A4. Probability trade-off consistency.\n\nThis section discusses the derivation of probability trade-offs and their consistency, which is crucial for avoiding contradictory orderings in decision-making models. The text emphasizes the need for independence conditions to ensure consistent orderings and introduces probability trade-off consistency as a key condition."
  },
  {
    "qid": "econ-empirical-436-1-0-2",
    "question": "3) Evaluate the discussion on the INTERLINK model in the book. What are the limitations of the informal treatment of this model?",
    "gold_answer": "Limitations include:\n- **Lack of formal detail**: No listing of the model or its major equations.\n- **Quantitative shortcomings**: Issues in a quantitative area cannot be fully treated by words alone.\n- **Trust-based contention**: Readers must trust the authors' claims about uniform specification across countries.",
    "question_context": "The central section of the book discusses the need for economic forecasting at the international level and justifies the way in which the OECD Secretariat have chosen to tackle it.\nChapter 7 on Approaches to Economic Forecasting provides a textbook introduction to the subject, neatly set out with references, covering all the main questions which have been raised in recent years about the relation between clear economic structure and time series representation, the incorporation of rational expectations, the use of judgement and informal information and so on.\n\nThe book discusses the behavior of the international economy, the modeling of its behavior, the role of international coordination, and policy. It provides a description of the development of the international economy since the war and the role of governmental policy in that development."
  },
  {
    "qid": "econ-empirical-833-2-0-3",
    "question": "4) How does the structure of the Fifth World Congress proceedings facilitate the integration of theoretical and applied econometrics?",
    "gold_answer": "The structure facilitates integration by: \\n- **Separate Volumes**: Volume 1 covers theoretical material, while Volume 2 focuses on applied content, ensuring clear delineation. \\n- **Overlapping Themes**: Common themes across papers allow for cross-referencing and comparison between theoretical and applied perspectives. \\n- **Survey Articles**: The survey format bridges the gap between theory and application, providing a holistic view of the field.",
    "question_context": "The selection has been published in two volumes, the second of which contains topics of an applied econometrics content while the more theoretical material is covered in Volume 1.\nThe decision to expand the proceedings from the single volume of the Fourth Congress to the two volumes here, essentially doubling the number of papers reported, has produced undoubted benefits.\nThe papers are intended to be in the form of a survey article and hence are designed to give extensive coverage of the field.\nThe first three papers by Leamer (Econometric metaphors), Hendry (Econometric methodology: a personal perspective) and Sims (Making economics credible) are all concerned with the process of econometric modelling and testing, and in particular how the results should be interpreted.\n\nThe text discusses the publication of the Fifth World Congress of the Econometric Society, highlighting its expansion into two volumes to cover both theoretical and applied econometrics. It emphasizes the benefits of this expansion, including a wider spread of applications and overlapping topics that provide different approaches and interpretations. The papers are designed as survey articles to give extensive coverage of the field, making them accessible to both specialists and non-specialists."
  },
  {
    "qid": "econ-empirical-1587-2-2-1",
    "question": "6) Critically evaluate the statement that 'over 80% of the variance is due to symmetric noise' in the context of frontier estimation. What does this imply for policy recommendations based on efficiency scores?",
    "gold_answer": "1. **Interpretation**: High $$\\sigma^2$$ implies noise dominates inefficiency ($$u$$). Efficiency scores are unreliable.  \n2. **Policy**: If most variation is noise, targeting \"inefficient\" units may be misguided. ML-CE’s noise-adjusted frontiers are preferable.  \n3. **Caveat**: Standard errors (Table 3) show $$a$$ is poorly estimated in early years, further complicating inference.",
    "question_context": "The variance of the composed variable ln u is $$\\operatorname{var}\\left[\\ln u\\right]=\\sigma^{2}+1/(1+a)^{2}.$$\nFor six of the years included in the study, over 80% of the variance of the composed variable is due to the symmetric random variable.\nThe asymptotic standard errors in the composed error model are of the same order of magnitude but slightly smaller than in the average model.\n\nThe section compares the stability of parameter estimates across methods (LP*, ML, ML-CE) and discusses the variance decomposition of the composed error term."
  },
  {
    "qid": "econ-empirical-1540-1-0-3",
    "question": "4) Formally define a truncation $(\\hat{H}, \\hat{P})$ of a multistage game $(H, P)$ at period $T$ and verify that it satisfies the four given conditions.",
    "gold_answer": "A truncation $(\\hat{H}, \\hat{P})$ of $(H, P)$ at $T$ satisfies:  \n1. $\\hat{H} \\subseteq H$: The set of possible histories is a subset of the original game's histories.  \n2. $\\lambda_t \\hat{H} = \\lambda_t H$ for all $t < T$: The histories up to period $T-1$ are identical to those in the original game.  \n3. $\\#\\hat{A}_t(\\lambda_{t-1}x) = 1$ for all $t \\geq T$: From period $T$ onwards, players have no freedom to choose actions (only one outcome is possible).  \n4. $\\hat{P}_i$ is the restriction of $P_i$ to $\\hat{H}$: Payoffs in the truncated game are the same as in the original game for the remaining histories.",
    "question_context": "There is a finite number of players $i\\in I=\\left\\{1,2,...,N\\right\\}$ and an infinite number of time-periods $t\\in I N.$ In each period $t$ every player $i$ chooses an action $x_{t i}$ . These choices are made simultaneously. After each period all players observe this period's outcome.\nThe set of actions available to a player $i$ in some period $t$ may depend on the history of the game up to that period; however, regardless of the history of the game this set will always be a subset of some given 'ambient' set $S_{t i}$.\nAn outcome of the game in some period $t$ is therefore an $N$ -tuple $\\boldsymbol{x}_{t}=\\left(x_{t1},...,x_{t N}\\right)\\in S_{t}=S_{t1}\\boldsymbol{x}\\cdot\\cdot\\boldsymbol{\\cdot}\\boldsymbol{x}S_{t N}$ . An outcome of the whole game is a sequence $x=\\left(x_{0},x_{1},x_{2},\\ldots\\right)\\in S=\\left\\{0\\right\\}x\\aleph_{t\\in I N}S_{t}$ . (Here $x_{0}(=0)$ is included for notational convenience.)\nLet $x=(x_{0},x_{1},...,x_{t},...)\\in H$ and $t\\in I N_{0}$ . Then we write $\\lambda_{t}\\d_{x}$ for the $(t+1)$ tuple $(x_{0},x_{1},...,x_{t})$ and $\\mu_{t}x$ for the sequence $(x_{t},x_{t+1},x_{t+2},\\ldots).$ Also we define $\\lambda_{t}H=\\{\\lambda_{t}x\\mid x\\in H\\}$ and $\\mu_{t}H=\\left\\{\\mu_{t}x\\mid x\\in H\\right\\}$.\nGiven the set $H$ we can now introduce for all $t\\in I N$ a correspondence: $A_{t}\\colon\\lambda_{t-1}H\\to S_{t}$ which is defined by $A_{t}(\\lambda_{t-1}x)=\\{y_{t}\\in S_{t}\\mid y\\in H,\\lambda_{t-1}x=$ $\\lambda_{t-1}y\\}$ for all $x\\in H.$ So for every initial history $\\lambda_{t-1}x,A_{t}(\\lambda_{t-1}x)$ is the set of all period $t$ outcomes which are possible after the initial history $\\lambda_{t-1}x.$\nWe assume now that for all $t\\in I N_{\\mathrm{:}}$ $A_{t}$ can be written as the product of correspondences $A_{t i}\\colon\\lambda_{t-1}H\\to S_{t i}(i\\in I),$ i.e., that $A_{t}(\\lambda_{t-1}x)=$ $\\mathbb{X}_{i=1}^{N}A_{t i}{(\\lambda_{t-1}x)}$ holds for all $x\\in H.$ The correspondence $A_{t i}$ thus describes which actions are available to player $i$ in period t.\nFinally for every player $i\\in I$ there is a payoff-function $\\ P_{i}\\colon H\\to I R.$ We write $P$ for the $N.$ tuple $(P_{1},...,P_{N})$.\nA strategy $f_{i}$ for player $i$ is then a sequence $\\left(f_{t i}\\right)_{t\\in I N}$ of functions with $f_{t i}\\colon\\lambda_{t-\\mathrm{~i~}}H\\to S_{t i}$ for all $t\\in I N_{\\mathrm{:}}$ and $f_{t i}(\\lambda_{t-1}x)\\in A_{t i}(\\lambda_{t-1}x)$ for all $t\\in I N$ and $x\\in H$.\nA strategy combination $f=(f_{1},...,f_{N})$ is called a 'perfect &-equilibrium of $(H,P)^{\\mathfrak{N}}$ if there is no subgame of $\\{H,P\\}$ in which a player can gain more than c by deviating from $f.$ If $\\varepsilon=0$ a perfect &-equilibrium of $(H,P)$ is also called a 'perfect equilibrium of $\\{H,P\\}^{\\mathfrak{s}}$.\nA history $x\\in H$ is called a 'perfect g-equilibrium history of $(H,P)^{\\prime}$ if there is a perfect c-equilibrium $\\scriptstyle f$ of $(H,P)$ such that $x$ results if all players follow $f.$\nA strategy combination $f$ is called a 'generalised perfect &-equilibrium of $(H,P)^{\\prime}$ if there is no subgame of $\\{H,P\\}$ in which a player can gain more than & through a single period deviation from $f.$\nA truncation of $(H,P)$ at $T$ is a multistage game $(\\hat{H},\\hat{P})$ obtained from $(H,P)$ by removing the players' freedom to move from period $T$ onwards.\n\nThe text describes a multistage game with a finite number of players and infinite time periods. Players choose actions simultaneously in each period, and outcomes depend on the history of the game. The set of possible actions is constrained by the history, and payoffs are defined for each player. The text introduces concepts such as strategies, equilibria, and truncations of the game."
  },
  {
    "qid": "econ-empirical-66-3-2-0",
    "question": "7) For a CEO exercising options worth 2% of firm value (90th percentile), derive the total effect on assumed returns if the firm has a pension sensitivity of 1.11, using column (1) of Table VII.",
    "gold_answer": "1. Baseline effect: 0.027 × 2 = 5.4 basis points.  \n2. Interaction effect: 0.012 × 2 × 1.11 = 2.7 basis points.  \n3. Total effect = 5.4 + 2.7 = 8.1 basis points.",
    "question_context": "The coefficients in column (1) imply that CEO option exercise has a baseline effect of 5.4 basis points (2 times 0.027 percentage points) at a log sensitivity of zero. The interaction effect shows that for each additional point of log sensitivity, this effect is 2.4 basis points (2 times 0.012 percentage points) greater.\nThe results suggests that while assumed returns are unusually high at firm-year observations where the CEO is exercising large amounts of options relative to firm value, they may be lower in periods of large option grants.\n\nThis section links CEO option exercises/grants to assumed returns, with evidence of earnings manipulation in both directions."
  },
  {
    "qid": "econ-empirical-488-1-0-1",
    "question": "2) Construct the four-sector general equilibrium model from Chapter 7, specifying the roles of the produced commodity, exogenous factor, and money. How does this model differ from IS-LM?",
    "gold_answer": "1. **Model Setup**: \\n   - Commodity: \\( Y = F(K, L) \\), where \\( K \\) is exogenous. \\n   - Money: \\( M \\) enters utility \\( U(Y, M) \\). \\n   - Equilibrium: \\( Y = C + G \\), \\( M = \\overline{M} \\). \\n2. **Four-Sector Diagram**: Integrates labor, goods, money, and capital markets. \\n3. **IS-LM Contrast**: IS-LM assumes fixed prices and bonds; Sinclair's model endogenizes money and excludes bonds initially.",
    "question_context": "The main aim of the book is to provide in detail the ‘choice-theoretic basis for the various macroeconomic relations postulated by Keynes and developed by later writers’ (p. 1).\nChapters 1 to 4 present analyses of the consumer, producer, government and overseas sectors. The framework adopted is explicitly that of neoclassical optimisation.\nChapter 7 uses a simple general equilibrium model to analyse the macroeconomy with one produced commodity, two factors of production (one exogenously supplied), and money, analyzed via a four-sector diagram.\n\nThe book by P. J. N. Sinclair provides a detailed choice-theoretic basis for macroeconomic relations, integrating neoclassical optimization frameworks and addressing sectors like consumer, producer, government, and overseas with rigorous mathematical models."
  },
  {
    "qid": "econ-empirical-1575-0-0-2",
    "question": "3) Analyze the role of loan enforcement constraints in the ∩-shaped relationship between competition and trade credit provision.",
    "gold_answer": "Loan enforcement constraints become increasingly binding as the number of competitors grows. With more competitors, the threat of cutting off future credit is less effective, and social norms against default are harder to enforce. This leads to a reduction in trade credit provision in highly competitive markets, contributing to the ∩-shaped relationship.",
    "question_context": "Using firm-level data we investigate the relationship between trade credit and suppliers' market structure and find a ∩-shaped relationship between competition and trade credit, with a discontinuous increase in credit provision between monopoly and duopoly.\nWe construct a model in which suppliers initially post a cash price for up-front payments but are not able to commit ex ante to the terms of trade credit.\nThe fact that many monopolists appear unwilling to provide any trade credit to their clients seems particularly striking given that our data come from Indonesia — a country with poorly developed capital markets in which trade credit is often the only source of credit for small firms.\n\nThe paper investigates the relationship between trade credit and suppliers' market structure, finding a ∩-shaped relationship. Monopolists are more likely to not offer any trade credit compared to firms in competitive environments. The model highlights the inability of firms to commit ex ante to a trade credit price, leading to different behaviors between monopolists and competitive firms."
  },
  {
    "qid": "econ-empirical-1700-2-0-3",
    "question": "4) Derive the expressions for post-shock creation ($H_{0+}^{z}$) and the number of production units ($N_{0+}^{z}$) in the presence of zombies, starting from the equations $H_{0+}^{z} = \\frac{1}{2}(1 - N_{0+}^{z} + S)$ and $N_{0+}^{z} = H_{0+}^{z} + \\frac{1}{4}$.",
    "gold_answer": "1. Substitute $N_{0+}^{z} = H_{0+}^{z} + \\frac{1}{4}$ into the creation equation:\n   $$H_{0+}^{z} = \\frac{1}{2}\\left(1 - \\left(H_{0+}^{z} + \\frac{1}{4}\\right) + S\\right).$$\n2. Multiply both sides by 2:\n   $$2H_{0+}^{z} = 1 - H_{0+}^{z} - \\frac{1}{4} + S.$$\n3. Collect $H_{0+}^{z}$ terms:\n   $$3H_{0+}^{z} = \\frac{3}{4} + S.$$\n4. Solve for $H_{0+}^{z}$:\n   $$H_{0+}^{z} = \\frac{S}{3} + \\frac{1}{4}.$$\n5. Substitute back into $N_{0+}^{z} = H_{0+}^{z} + \\frac{1}{4}$:\n   $$N_{0+}^{z} = \\frac{S}{3} + \\frac{1}{2}.$$",
    "question_context": "The productivity of the incumbents varies over time and the current level of productivity for firm $i$ in year $t$, $Y_{i t}^{o}$ is: $Y_{i t}^{o}=A_{t}+A_{t}B+A_{t}\\varepsilon_{i t}^{o}=A_{t}(1+B+\\varepsilon_{i t}^{o})$, where $A_{t}$ represents the state of technology shared by all the incumbent production units at time $t,B$ is a potential shift parameter that can represent an aggregate productivity shock, and $\\varepsilon_{i t}^{o}$ is an idiosyncratic shock that is distributed uniformly on the unit interval.\nPotential entrants have technological advantage over incumbents, so that the productivity for a potential new firm is consistently higher than incumbents by $\\gamma A_{t}$: $Y_{i t}^{n}=A_{t}(1+\\gamma)+A_{t}B+A_{t}\\varepsilon_{i t}^{n}=A_{t}(1+\\gamma+B+\\varepsilon_{i t}^{n})$.\nBoth new and old units must incur a cost $A_{t}p(N_{t})$ in order to produce, where $N_{t}$ represents the number of production units in operation at time $t.$ The function $p(N)$ is increasing with respect to $N_{\\ast}$ and captures any reduction in profits due to congestion or competition.\nThe reservation productivity (normalized by the state of technology) of incumbents and potential entrants, respectively, is given by: $\\overline{y}^{o}-p(N)=0$ and $\\overline{y}^{n}-\\kappa-p(N)=0$.\nIn the presence of zombies, the post-shock destruction remains the same as the pre-shock level: $D_{0^{+}}^{z}=D_{0}=\\frac{1}{4}$.\n\nThe model analyzes the impact of zombie firms on economic restructuring through entry and exit decisions of firms, incorporating productivity shocks and subsidies to incumbents."
  },
  {
    "qid": "econ-empirical-1101-2-1-1",
    "question": "4) Explain the three advantages of using a conditional logit model for analyzing voting behavior in this context.",
    "gold_answer": "1. **Relative Characteristics**: The probability of voting against a contestant depends on the characteristics of all contestants, not just the target.\n2. **Interaction Effects**: Allows for interactions between voter and contestant characteristics (e.g., gender interactions).\n3. **Prediction Constraint**: Ensures the predicted number of votes sums to the number of contestants in each round.",
    "question_context": "The utility of player $i$ voting against contestant $j$ in round $r$ is modeled as: $$U_{i j r}=X_{i j r}\\upbeta_{r}+\\upvarepsilon_{i j r},$$ where $\\upbeta$ is a vector of coefficients to be estimated and $\\upvarepsilon_{i j r}$ is the unobserved preference individual $i$ has for voting against Contestant $j$. The probability of voting against contestant $j$ is given by: $$P_{i k r}=\\frac{\\exp{(X_{i j r}\\upbeta_{r})}}{\\sum_{k=1}^{N_{r}}\\exp{(X_{i k r}\\upbeta_{r})}}$$ where $N_{r}$ is the number of contestants in round $r$.\n\nConditional logit models are used to control for race and gender differences in contestants' abilities, allowing for a more nuanced analysis of voting behavior."
  },
  {
    "qid": "econ-empirical-488-1-1-1",
    "question": "2) Why might the omission of Barro and Friedman from the conversations limit the book's coverage of New Classical macroeconomics?",
    "gold_answer": "1. **Barro's Role**: Pioneered Ricardian equivalence and policy critique. \\n2. **Friedman's Role**: Bridged monetarism and rational expectations. \\n3. **Impact**: Absence leaves gaps in discussing foundational ideas like natural rate hypothesis.",
    "question_context": "The format is an introductory and a summary chapter separated by eleven chapters each recording a question and answer session with a specific economist.\nAnyone who has followed the evolution of the ‘Rational Expectations Revolution' in the pages of the academical journals will not find anything here that has not been said before, so far as economics is concerned.\n\nKlamer's book records conversations with leading economists on the Rational Expectations Revolution, though it lacks new theoretical insights or rigorous models."
  },
  {
    "qid": "econ-empirical-92-3-2-1",
    "question": "6) Derive the expected sign of the coefficient for unemployment's effect on smoking among pregnant Black mothers, given the selection mechanism observed in the Natality Files.",
    "gold_answer": "**Expected sign**: Negative. If low-SES Black women postpone fertility during recessions, the remaining pregnant Black mothers are positively selected (higher SES), leading to \\( \\text{Smoking} \\downarrow \\) as \\( \\text{Unemployment} \\uparrow \\).",
    "question_context": "More-educated Whites are less likely to be pregnant when the unemployment rate increases, whereas the opposite is true for Blacks... The number of drinks decreases for all pregnant women.\n\nBRFSS data (1985–2002) is used to analyze pregnancy selection and health behaviors (smoking, drinking) by race and education during recessions."
  },
  {
    "qid": "econ-empirical-1116-0-0-1",
    "question": "2) Explain the potential endogeneity issues in estimating the effect of parental English proficiency on child outcomes and how the instrumental variable strategy addresses them.",
    "gold_answer": "Endogeneity arises due to:\n1. **Omitted variable bias**: Unobserved factors like parental ability or cultural attitudes may affect both parental English and child outcomes.\n2. **Reverse causality**: Children's English proficiency may influence parental English.\n\nThe IV strategy addresses this by using age at arrival interacted with non-English-speaking country dummy as an instrument, which is exogenous conditional on controls and satisfies the exclusion restriction.",
    "question_context": "We interpret this as an effect of the parents’ English language skills and construct an instrumental variable for parental English proficiency. Estimates of the effect of parents’ English-speaking proficiency using two-stage least squares yield significant, positive results for children's English-speaking proficiency and preschool attendance, and significant, negative results for dropping out of high school and being below age-appropriate grade.\nParental English-language skills are correlated with many other variables that also affect child outcomes, such as parental ability, income, education, and cultural attitudes. Additionally, reverse causality is possible.\nWe identify the causal effect of parental English-language skills by taking advantage of the psychobiological phenomenon that younger children learn languages more easily than older children and adults.\n\nThe study examines the effect of immigrant parents' English-language proficiency on their children's educational outcomes using instrumental variable strategies to address endogeneity concerns."
  },
  {
    "qid": "econ-empirical-1208-4-1-1",
    "question": "4) Why do the parametric estimates for Hispanic and Asian women ($-0.05$ to $-0.09$ and $-0.12$ to $-0.14$) differ more from nonparametric estimates ($-0.004$ and $-0.02$) than those for white and black women?",
    "gold_answer": "Potential reasons:\n1. **Model Misspecification**: Parametric assumptions (e.g., quadratic experience) may not capture true wage determinants for Hispanic/Asian women.\n2. **Sample Heterogeneity**: Unobserved factors (e.g., immigration status, language proficiency) may vary nonlinearly within these groups.\n3. **Matching Rates**: Nonparametric estimates rely on exact matches, which may be rarer for Hispanic/Asian women, leading to different subsamples.",
    "question_context": "Table 7 presents the resulting estimated wage gaps--estimates that are directly comparable to those given in Panel C of Table 6. In the linear regression approaches, the unexplained log wage gap of $-0.11$ to $-0.13$ for black women is similar to the $-0.13$ gap we estimate using nonparametric methods.\nOur linear regression models include dummy variables for each of the 144 possible fields of study, dummy variables for degree, a quadratic in age, a quadratic in years of full-time experience, and an indicator for whether the respondent speaks English at home.\n\nThis section contrasts nonparametric and parametric methods for estimating wage gaps, highlighting the flexibility of nonparametric approaches and the implications of parametric assumptions."
  },
  {
    "qid": "econ-empirical-602-0-0-1",
    "question": "2) Prove that for scalar diffusions, the generator \\(\\mathcal{A}\\phi = \\mu\\phi' + \\frac{1}{2}\\sigma^2\\phi''\\) is the time-zero derivative of the semigroup of conditional expectation operators. Use the definition of the generator and the properties of the semigroup.",
    "gold_answer": "1. **Semigroup Definition**: The semigroup \\(\\{\\mathcal{T}_t\\}\\) is defined by \\(\\mathcal{T}_t \\phi(x) = E[\\phi(x_t) | x_0 = x]\\).  \n2. **Generator Definition**: The generator \\(\\mathcal{A}\\) is the limit \\(\\mathcal{A}\\phi(x) = \\lim_{t \\searrow 0} \\frac{\\mathcal{T}_t \\phi(x) - \\phi(x)}{t}\\).  \n3. **Apply Itô’s Lemma**: For a scalar diffusion \\(dx_t = \\mu(x_t)dt + \\sigma(x_t)dW_t\\), Itô’s Lemma gives \\(d\\phi(x_t) = \\mathcal{A}\\phi(x_t)dt + \\sigma(x_t)\\phi'(x_t)dW_t\\).  \n4. **Take Expectations**: \\(E[\\phi(x_t) | x_0 = x] = \\phi(x) + E\\left[\\int_0^t \\mathcal{A}\\phi(x_s)ds\\right]\\). Dividing by \\(t\\) and taking \\(t \\to 0\\) yields \\(\\mathcal{A}\\phi(x)\\).",
    "question_context": "Stationary diffusions that are ρ-mixing have mixing coefficients that decay exponentially to zero. When they fail to be ρ-mixing, they are still β-mixing and α-mixing; but coefficient decay is slower than exponential.\nThe ρ-mixing coefficients measure the temporal decay of maximally autocorrelated (nonlinear) functions of the Markov state. When ρ-mixing coefficients decay to zero, the spectral density of any process with finite second moments formed by taking a nonlinear function of the state has a continuous (and hence finite) spectral density at all frequencies, including frequency zero.\nThe generator of this scalar diffusion is known to be the differential operator: \\(\\mathcal{A}\\phi = \\mu\\phi' + \\frac{1}{2}\\sigma^2\\phi''\\).\nThe boundary behavior of a diffusion is characterized by the behavior of its scale function \\(S(x)\\): \\(S(x) = \\int_a^x s(y)dy\\, where \\, s(y) = \\exp\\left[-\\int_a^y \\frac{2\\mu(u)}{\\sigma^2(u)}du\\right]\\).\n\nThe paper explores how nonlinearities in drift and diffusion coefficients influence temporal dependence in diffusion models, using three measures of mixing: ρ-mixing, β-mixing, and α-mixing. It establishes conditions under which these mixing coefficients decay exponentially or sub-exponentially, and links these to the spectral properties of the processes."
  },
  {
    "qid": "econ-empirical-1415-0-0-3",
    "question": "4) Explain the role of the integral operator $S$ in the representation theorem. How does $(1-B)S$ relate to the identity, and why is this critical for rewriting the error correction terms?",
    "gold_answer": "1. The operator $S$ is defined as the cumulative sum: $$S_t(y) = \\sum_{\\tau=1}^t y_\\tau$$ for $t \\geq 1$.  \n2. The property $(1-B)S = I$ allows moving integrated terms to the left-hand side, e.g., rewriting $B\\theta(1-B)\\alpha(S)y_t$ as $\\Theta(1-B)\\alpha(S)y_t = \\sum_{h=1}^m \\theta^h(1-B)\\alpha^h(S)y_t$.  \n3. This decomposition is key to isolating contemporaneous cointegration terms with lagged dependencies.",
    "question_context": "The representation theorem introduces error correction terms among the right-hand-side variables of the VAR model which contain integrated series of various orders. This representation makes more precise the results of Yoo (1986) and generalizes the example treated in Granger and Lee (1989a, b).\nTheorem 1.1. If a process $y_{t}$ belongs to $\\mathcal{C}$ , then it satisfies an equation of the following form: $$\\phi(B)y_{t}=B\\theta(1-B)(\\alpha(S)y_{t}+\\beta(t))+\\varepsilon_{t},$$ where $\\phi(B)$ is an absolutely summable matrix series, $\\alpha(S)y_{t}+\\beta(t)$ is I(0), and $\\varepsilon_{t}$ is the innovation of $y_{t}$.\nTheorem 1.2 (Identification). For any process $y_{t}$ in $\\mathcal{C}$ , the representation (2) of Theorem 1.1 can be chosen with specific properties ensuring uniqueness up to a sign transformation.\n\nThe paper develops statistical tools for analyzing multivariate time series represented with a polynomial error correction model. It introduces an identification criterion for error correction terms and a multi-step estimation procedure involving principal component analysis."
  },
  {
    "qid": "econ-empirical-1463-1-0-0",
    "question": "1) Formally derive the labor supply function \\( L = f(w, Y, P) \\), where \\( w \\) is the net wage rate, \\( Y \\) is nonemployment income, and \\( P \\) represents preferences. Discuss the implications of using different time frames (e.g., daily, weekly, monthly) for measuring \\( L \\).",
    "gold_answer": "1. Start with the utility maximization problem: \\[ \\max U(C, L) \\text{ s.t. } C = wL + Y \\]\n2. Derive the first-order condition: \\[ \\frac{\\partial U}{\\partial L} = w \\frac{\\partial U}{\\partial C} \\]\n3. Solve for \\( L \\) to get the labor supply function. \n4. Implications of time frames:\n   - Shorter frames (daily) may capture volatility but miss trends.\n   - Longer frames (monthly) smooth out noise but may obscure short-term adjustments.",
    "question_context": "The basic labour supply model assumes that labour supply - usually measured by hours worked - depends on net wage rates, nonemployment income and preferences. There is almost no discussion of how these variables should be, or are, measured.\nShould, for example, hours be measured per day, per week, per month or per year? Should the definition of hours include second jobs, travel time, meal breaks? Do all of the experiments use the same definition, and if not does this affect comparisons between experiments?\n\nThe book discusses labor supply models in the context of U.S. income maintenance experiments, focusing on technical aspects and measurement challenges."
  },
  {
    "qid": "econ-empirical-441-4-0-1",
    "question": "2) The study reports a mean beta of \\(\\mathbf{I}\\cdot\\mathbf{O}\\mathbf{2}\\) with a cross-sectional standard deviation of \\(\\mathbf{o}\\cdot\\mathbf{i}5\\). Interpret these values in the context of the CAPM and discuss their implications for the study's findings.",
    "gold_answer": "1. **Mean Beta Interpretation**: The mean beta of \\(1.02\\) suggests that, on average, the stocks in the sample are slightly more volatile than the market (beta > 1). This implies they are expected to have higher systematic risk.\n\n2. **Standard Deviation Interpretation**: The cross-sectional standard deviation of \\(0.15\\) indicates moderate dispersion in beta estimates across stocks. This variability suggests differences in risk profiles among the stocks.\n\n3. **Implications for Findings**: The study assumes a beta of 1 in equation (4). The actual mean beta of \\(1.02\\) is close to this assumption, suggesting minimal bias. However, the dispersion in betas implies that some stocks may deviate significantly from the assumed beta, potentially affecting individual abnormal return calculations.",
    "question_context": "For each addition to the list, weekly abnormal returns and CARs were computed relative to the week of entry to the list (event date zero). Mean CARs and abnormal returns were then computed as an equally weighted average of the figures for all 132 additions.\nOver the 26 weeks following addition to the approved list, the cumulative abnormal return amounted to $095\\%$ - not nearly enough to cover the expenses of a round trip. Over this interval, the CAR varied between a low of $-\\mathbf{o}\\mathbf{\\cdot}\\mathbf{r}6\\%$ (in event week 1) and a high of $\\mathtt{I}{\\cdot}5\\mathtt{I}\\%$ (in event week 24).\nThe mean beta was $\\mathbf{I}\\cdot\\mathbf{O}\\mathbf{2}$ , and the cross-sectional standard deviation of the beta estimates was $\\mathbf{o}\\cdot\\mathbf{i}5$ . The unweighted mean beta of the go stocks in our study was $\\mathbf{I}\\cdot\\mathbf{O}3$ with a standard deviation of ${\\mathfrak{o}}{\\cdot}{\\mathfrak{i}}6.$\n\nThe study investigates abnormal performance following the addition of stocks to an approved list using an event study methodology. It computes weekly abnormal returns and cumulative abnormal returns (CARs) relative to the event date (week of entry to the list). The analysis includes 132 additions to the list, with mean CARs and abnormal returns calculated as equally weighted averages."
  },
  {
    "qid": "econ-empirical-1144-1-0-1",
    "question": "2) Derive the ratio of lead levels in water passed through iron pipes versus lead pipes based on the MSBH experiment and the Quam and Klein experiment. Compare the two ratios.",
    "gold_answer": "1. **MSBH experiment**: Iron pipe lead levels are \\( \\frac{1}{8} \\) to \\( \\frac{1}{3} \\) of lead pipe levels.\n2. **Quam and Klein experiment**: Iron pipe lead level is \\( \\frac{0.025}{0.1425} = \\frac{1}{5.7} \\).\n3. Comparison: MSBH shows a wider range (1/8 to 1/3) vs. Quam and Klein's consistent 1/5.7 ratio.",
    "question_context": "In the median Massachusetts town, the lead level found in water after ordinary use exceeded the modern guideline by a factor of 19; the lead level in water that stood in pipes overnight exceeded the modern guideline by a factor of 58.\nFor three cities-Hyde Park (old well); Lowell (Cook well); and Middleborough--the average lead levels in standing water exceeded the modern guideline by factors of 202, 169, and 411, respectively.\nWater samples passed through an iron pipe contained one-eighth to one-third the lead found in water passed through lead pipe (MSBH 1900, 491-97).\nThe average lead content for water that passed through the lead pipe was 0.1425 parts per liter of water; the average lead content for water passed through the iron pipe was .025, one-sixth the amount found in water from the lead pipe (Quam and Klein 1936).\nThe life of the typical lead service pipe was 35 years. By contrast, plain iron or steel pipe lasted 16 years; galvanized pipe 20 years; and cement lined pipe 28 years.\nFor cities with populations less than 8,000, 33 percent used lead service pipes. In contrast, for cities with populations between 30,000 and 300,000, 72 percent used lead pipes; and for the largest cities, those with populations greater than 300,000, all but one (94 percent) used lead service pipes (Troesken and Beeson 2003).\n\nThe text discusses lead levels in Massachusetts tap water in 1900, comparing them to modern EPA standards. It highlights the sources of lead, primarily from service pipes, and explores the reasons for the widespread use of lead pipes despite their toxicity."
  },
  {
    "qid": "econ-empirical-605-3-0-2",
    "question": "3) Prove that the equilibrium strategy μ² for player 2 in the game B(α¹, x) is unique and explain the role of the function a²(α¹, α², x) in this context.",
    "gold_answer": "1. **Uniqueness**: The equilibrium μ² is unique because each u²(α¹, α², x, ·) is strictly decreasing on [0, a²(α¹, α², x)].\n2. **Role of a²**: The function a²(α¹, α², x) ensures that both players' irrationality probabilities reach 1 at the same time, given μ²(α²) ≤ a²(α¹, α², x).",
    "question_context": "Let there be a continuum of firms, each with size S(n)...\nIf the game B(α¹, x) does not end at time 0, then it must be that player 2 has chosen some α² > 1 - ᾱ¹. We know from the analysis of Section 2, that after time 0 each player i must concede at rate λⁱ = rʲ(1 - αⁱ)/(α¹ + α² - 1) for j ≠ i.\nDefine T¹(α¹, α², x) = - (α¹ + α² - 1)/(r²(1 - α¹)) log x and T²(α¹, α², y) = - (α¹ + α² - 1)/(r¹(1 - α²)) log y.\nLet a²(α¹, α², x) be the unique value of a² such that T¹(α¹, α², x) = T²(α¹, α², y(a²)) where y(a²) = (z²π²(α²))/(z²π²(α²) + (1 - z²)a²).\n\nThis section provides a detailed proof of Proposition 2, focusing on the bargaining game B(α¹, x) where player 1 has a single irrational type and her probability of irrationality is x. The analysis involves determining equilibrium strategies and payoffs under specific conditions."
  },
  {
    "qid": "econ-empirical-1662-0-0-3",
    "question": "4) Compare the findings of Proposition 5 and Proposition 6 with Rochet and Tirole (2003). How does the inclusion of user heterogeneity ($\\gamma^b, \\gamma^s$) extend their insights?",
    "gold_answer": "1. **Rochet and Tirole (2003)**: Focuses on duopolistic associations maximizing transaction volume. Their model lacks heterogeneity parameters.\n\n2. **Extension with Heterogeneity**:\n   - $\\gamma^b$ and $\\gamma^s$ introduce scale effects, altering fee sensitivity.\n   - When $\\alpha^b$ or $\\alpha^s$ is small, heterogeneity amplifies fee shifts:\n     \\[ \\frac{\\partial \\hat{p}^b}{\\partial \\gamma^b} > 0, \\quad \\frac{\\partial \\hat{p}^s}{\\partial \\gamma^s} > 0 \\]\n   - Provides a more nuanced understanding of platform pricing under user diversity.",
    "question_context": "Proposition 4 suggests that this view by the authorities is true only when most of the cardholders are multihoming, whereby increasing interplatform competition indeed helps to reduce the interchange fee paid by the merchant side to the cardholder side. Notably, the reverse view is true when the fraction of singlehoming cardholders is sufficiently large, whereby increasing interplatform competition drives up the interchange fee instead.\n\nPayment card platforms offer benefits to cardholders and charge transaction fees to merchants. The equilibrium fees depend on the fraction of multihoming versus singlehoming cardholders."
  },
  {
    "qid": "econ-empirical-374-5-0-3",
    "question": "4) Explain the role of the matrix $$M_{T}$$ in the construction of the IV estimator $$\\widehat{\\pmb{\\beta}}_{i v}$$ and why it cancels out in the limiting distribution of the test statistic.",
    "gold_answer": "",
    "question_context": "$$Q_{T}=\\left(\\begin{array}{l l}{\\displaystyle\\sum_{t=2}^{T}\\mathbf{f}_{t}\\mathbf{f}_{t}^{\\prime}\\widehat{u}_{t}^{2}}&{\\displaystyle\\sum_{t=2}^{T}\\mathbf{f}_{t}\\mathbf{z}_{t-1}^{\\prime}\\widehat{u}_{t}^{2}}\\\\{\\displaystyle\\sum_{t=2}^{T}\\mathbf{z}_{t-1}\\mathbf{f}_{t}^{\\prime}\\widehat{u}_{t}^{2}}&{\\displaystyle\\sum_{t=2}^{T}\\mathbf{z}_{t-1}\\mathbf{z}_{t-1}^{\\prime}\\widehat{u}_{t}^{2}}\\end{array}\\right)$$\n$$\\left(\\sum_{t=2}^{T}\\xi_{t-1}\\xi_{t-1}^{\\prime}\\right)^{-1}=O_{p}\\left(T^{-2}\\right)\\quad\\mathrm{and}\\quad\\sum_{t=2}^{T}\\xi_{t-1}u_{t}=O_{p}\\left(T\\right).$$\n$$\\frac{1}{\\sqrt T}D_{T}^{-1}P_{T}=\\frac{1}{\\sqrt T}D_{T}^{-1}\\sum_{t=2}^{T}\\left(\\mathbf{f}_{t-1}\\right)u_{t}+o_{p}(1).$$\n$$\\widehat{\\pmb{\\beta}}_{i v}={M}_{T}^{-1}{P}_{T}$$\n$$\\mathcal{T}_{i v,K}^{w}=\\frac{1}{\\sqrt{T}}P_{T}^{\\prime}D_{\\mathbf{z}T}^{-1}\\left(\\frac{1}{T}D_{\\mathbf{z}T}^{-1}Q_{T}D_{\\mathbf{z}T}^{-1}\\right)^{-1}\\frac{1}{\\sqrt{T}}D_{\\mathbf{z}T}^{-1}P_{T}.$$\n$$\\frac{1}{\\sqrt{T}}\\sum_{t=2}^{T}\\left({D_{\\mathbf{f}T}^{-1}\\mathbf{f}_{t}y_{t}}\\atop{D_{\\mathbf{z}T}^{-1}\\mathbf{z}_{t-1}y_{t}}\\right)\\Rightarrow\\mathcal{M}\\mathcal{N}\\left(0,V_{\\mathbf{f}\\mathbf{z}u}\\right),$$\n\nThe proof involves advanced econometric techniques and matrix algebra to derive the limiting distribution of the test statistic under the null hypothesis."
  },
  {
    "qid": "econ-empirical-462-0-0-3",
    "question": "4) Discuss the implications of the linear-in-means model for policy makers. What should they consider when interpreting estimates from this model?",
    "gold_answer": "1. Policy makers should recognize that estimates from the linear-in-means model capture both direct and indirect (social) effects.\n2. Marginal effects are complex functions of parameters, so reporting these is essential for policy relevance.\n3. Consider whether the model is interpreted as a reduced form or as a game-theoretic best response.\n4. Be aware of the tradeoffs between automatic derivation of policy effects and the need for microfoundations.\n5. Ensure that the model's assumptions align with the policy context (e.g., rational agents, equilibrium behavior).",
    "question_context": "The article by Goldsmith-Pinkham and Imbens (2013) is an interesting contribution to the literature on social interactions. An important contribution of their article is to provide some answers to the question: what features of models of social interactions are empirically relevant?\nIn our comment on their article, we ask an alternative question: how should policy makers use the estimates from the linear-in-means model? This question concerns the relationship between the model used by the econometrician and policy interventions.\nWe first derive standard marginal effects of a change in a covariate. These are complicated functions of the model parameters. This suggests that it may be useful for applications of the linear-in-means model to report these policy relevant parameters.\nAnother interpretation views the linear-in-means model as a best response function from some game. This gives an autonomous interpretation to the parameters and uncovers the mechanisms underlying the interaction.\nThe existence of these two possibilities, either to use the results of Theorem 2.1 or to give an autonomous interpretation to the equations of the linear-in-means model, raises a tradeoff.\n\nThe article discusses the empirical relevance of models of social interactions and the use of linear-in-means model estimates by policy makers. It explores the relationship between the true data-generating process, the econometrician's model, and policy interventions."
  },
  {
    "qid": "econ-empirical-1080-1-1-1",
    "question": "2) Why is TSLS used instead of OLS to estimate the QAIDS model? Provide a detailed explanation of the instruments and first-stage results.",
    "gold_answer": "TSLS is used due to endogeneity concerns:\n1. **Endogeneity source**: For durable goods, total expenditure may be correlated with measurement error in expenditure shares, violating OLS assumptions.\n2. **Instruments**: Normal income and its square are used to instrument for log total expenditure and its square. These are based on usual earnings rather than current earnings, reducing endogeneity.\n3. **First-stage results**:\n   - For families with children: Adjusted \\( R^2 \\) values are 0.377 (log expenditure) and 0.368 (square of log expenditure).\n   - For households without children: Adjusted \\( R^2 \\) values are 0.332 and 0.321, respectively.\n4. **Instrument strength**: The instruments alone explain 35.3% and 34.7% of the variation in the endogenous variables for the with-children sample, indicating reasonable strength.",
    "question_context": "The budget share for each good can be estimated equation-by-equation using: \n\\[\nw_{i g}=\\alpha_{g}+\\gamma_{g}\\log p_{g}+\\mathfrak{g}_{1g}\\log\\left(\\frac{C_{i}}{I_{i}}\\right)+\\mathfrak{g}_{2g}\\left(\\log\\left(\\frac{C_{i}}{I_{i}}\\right)\\right)^{2}+\\Phi_{g}D_{i}+\\Theta_{g}H_{i}+u_{i g}\n\\]\nAddition of the square of log total consumption to the AIDS model of Deaton and Muellbauer (1980a,b), makes it more flexible, and is demonstrated to improve model fit in FES data by Banks, Blundell, and Lewbel (1997).\nGiven expenditure diary data for a two-week period, many households will record zero expenditures on some goods. The correct empirical model to apply depends on the mechanism generating those zeroes.\n\nThe basic empirical model is a Quadratic Almost Ideal Demand System (QAIDS), which extends the AIDS model by adding the square of log total consumption to improve flexibility and fit. The model is estimated using TSLS to address endogeneity concerns."
  },
  {
    "qid": "econ-empirical-93-3-0-1",
    "question": "2) Why is the Sargan test used to validate the legal-origins instruments? What does the null hypothesis of the Sargan test imply about the instruments' validity?",
    "gold_answer": "The Sargan test checks for overidentifying restrictions, ensuring instruments are uncorrelated with the error term. The null hypothesis is that the instruments are valid (uncorrelated with residuals). Large $p$-values in Table I (fourth row) fail to reject the null, supporting instrument validity by showing no evidence of omitted variable bias.",
    "question_context": "We tested the strength of our instruments with the usual $F$ -tests of joint significance in the first-stage regressions of $F$ and $\\boldsymbol{F}\\cdot(\\boldsymbol{y}-\\boldsymbol{y}_{1})$ . The $p$ -values reported in the first two rows of the lower panel of Table I indicate that the instruments passed this test at the 1 percent level in all three equations involving private credit, our preferred measure of financial development, in all equations involving bank assets and in all but one involving liquid liabilities.\nTo be valid, our legal-origins instruments must not affect growth through any channel other than finance, since otherwise the effects we are attributing to finance might actually be effects of these nonfinancial channels. This restriction might appear questionable because, for example, different legal systems could result in different regulatory environments that affect barriers to entry as argued by Djankov et al. [2000]. Therefore, we tested the restriction using the standard Sargan test, whose null hypothesis is that the instruments are uncorrelated with the IV residuals.\nAs a further test of our theory, we examined whether the effects of $F$ and $\\boldsymbol{F}\\cdot(\\boldsymbol{y}-\\boldsymbol{y}_{1})$ on per-capita GDP growth were work through productivity growth, as implied by the theory, instead of working just through capital accumulation. Specifically, we reestimated the basic growth equation (13) using productivity growth as the dependent variable instead of growth in per-capita GDP, and interpreting $y$ as the log of aggregate productivity in 1960 instead of the log of per-capita GDP.\n\nThis section discusses the validity and strength of instruments used in the analysis, focusing on financial development measures and their interaction with initial relative income. It also examines whether the effects of financial development on growth work through productivity growth."
  },
  {
    "qid": "econ-empirical-297-3-0-0",
    "question": "1) Derive the asymptotic properties of the Generalized Covariance estimator $\\tilde{\\phi}_{T}$ for the mixed VAR(p) process, referencing Proposition 7.",
    "gold_answer": "The asymptotic properties include consistency and asymptotic normality. Key steps:\n1. **Consistency**: $\\tilde{\\phi}_{T} \\xrightarrow{p} \\phi$ as $T \\rightarrow \\infty$ under regularity conditions.\n2. **Asymptotic Normality**: $\\sqrt{T}(\\tilde{\\phi}_{T} - \\phi) \\xrightarrow{d} N(0, \\Sigma)$, where $\\Sigma$ is the asymptotic covariance matrix.\n3. The proof relies on the portmanteau statistic's quadratic form and the ergodicity of the process.",
    "question_context": "The (Generalized) Covariance estimator $\\tilde{\\phi}_{T}$ can be applied to estimate $\\phi^{\\prime}=[(v e c\\phi_{1})^{\\prime},\\ldots,(v e c\\phi_{p})^{\\prime}]$ in the mixed VAR(p) process. The asymptotic results from Proposition 7 hold.\nThe GCov estimate is obtained by minimizing the portmanteau statistic computed from the above auto- and cross-correlations up to and including lag $H=10$: $\\hat{\\phi}=a r g m i n\\sum_{\\phi}^{4}\\sum_{\\substack{j=1}}^{4}\\sum_{k=1}^{10}[\\hat{\\rho}_{j,k}(h,\\phi)]^{2}$.\nThe errors $\\epsilon_{t}=(\\epsilon_{1,t},\\epsilon_{2,t})^{\\prime}$ are such that $\\epsilon_{1,t},\\epsilon_{2,t}$ are drawn independently in the same $t$-Student distribution with the degree of freedom $\\nu=4$, mean zero and variance equal to $\\nu/(\\nu-2)$.\nThe autoregressive matrix is equal to: $\\phi=A\\left(\\begin{array}{c c}{{J_{1}}}&{{0}}\\\\ {{0}}&{{J_{2}}}\\end{array}\\right)A^{-1}=\\left(\\begin{array}{c c}{{0.7}}&{{-1.3}}\\\\ {{0.0}}&{{2.0}}\\end{array}\\right)$.\n\nThis section discusses the application of the Generalized Covariance (GCov) estimator to estimate parameters in a mixed VAR(p) process, including simulation studies and empirical application to commodity futures."
  },
  {
    "qid": "econ-empirical-57-2-0-2",
    "question": "3) Formally test the null hypotheses H1 (Noise), H2 (Demographic Consistency), H3 (Expectational Consistency), and H4 (Demographic and Expectational Consistency) using the equation:\n\n$$x+e_{x}^{s}={\\sf\\beta}_{0}+{\\sf\\beta}_{1}[x+e_{x}^{0}]+{\\sf\\beta}_{2}D E L_{x}.$$",
    "gold_answer": "1. **H1 (Noise)**: $\\beta_{1}=\\beta_{2}=0$. Test if subjective life expectancy is random noise.\n2. **H2 (Demographic Consistency)**: $\\beta_{1}=1$, $\\beta_{0}=\\beta_{2}=0$. Test if subjective expectancy aligns perfectly with current life tables.\n3. **H3 (Expectational Consistency)**: $\\beta_{1}=0$, $\\beta_{2}=1$. Test if subjective expectancy reflects only extrapolated trends ($DEL_{x}$).\n4. **H4 (Joint Consistency)**: $\\beta_{0}=0$, $\\beta_{1}=\\beta_{2}=1$. Test if both demographic and expectational factors explain subjective expectancy.\n5. Results show H4 fits best, but a significant $\\beta_{0}$ indicates an upward bias beyond H4.",
    "question_context": "As one method of examining the shape of the subjective distributions, we fit the Weibull survival function, \n\n$$P_{i}(t\\vert p_{60,i}^{s};p_{80,i}^{s};x_{i})=\\exp{[-((t-x_{i})/\\uptheta_{i})^{c_{i}}]},\\qquad t>x_{v},$$\n\nwhere $i$ is a respondent, $c_{i}$ and $\\uptheta_{i}$ are parameters, and $\\mathcal{P}$ is the probability of survival to age $t.^{9}$\nThe fitted $c_{i}$ and $\\uptheta_{i}$ from (1) are then used to generate a distribution of subjective survival probabilities from which each respondent's subjective mean and variance can be calculated.\nAmong the over 80 percent of men aged 55 or less whose subjective survival probabilities are characterized well by a Weibull distribution, subjective distributions are fatter than the actuarial distribution.\n\nThe text discusses the examination of subjective survival distributions using the Weibull survival function, comparing subjective and actuarial survival probabilities."
  },
  {
    "qid": "econ-empirical-464-4-0-1",
    "question": "2) Show that the variance estimator $\\widehat{\\sigma}_{ML}^{2}$ for the MLE method follows a scaled chi-squared distribution $\\sigma_{0}^{2}\\chi_{T-1}^{2}/T$.",
    "gold_answer": "1. Define the residuals: $\\widehat{e} = y - \\widehat{m} = \\sigma_{0}(\\widetilde{e} - \\overline{e})$, where $\\widetilde{e} = e$. \\\\\n2. The matrix $M = I_{T} - 1(1^{\\prime}1)^{-1}1^{\\prime}$ is idempotent with $T-1$ degrees of freedom. \\\\\n3. The sum of squared residuals: $\\widehat{e}^{\\prime}\\widehat{e} = \\sigma_{0}^{2}e^{\\prime}Me$. \\\\\n4. Since $e\\sim \\mathcal{N}(0, I_{T})$, $e^{\\prime}Me \\sim \\chi_{T-1}^{2}$. \\\\\n5. Therefore, $\\widehat{\\sigma}_{ML}^{2} = \\frac{\\widehat{e}^{\\prime}\\widehat{e}}{T} \\sim \\sigma_{0}^{2}\\frac{\\chi_{T-1}^{2}}{T}$.",
    "question_context": "The data generating process is $y_{t}=m_{0}+\\sigma_{0}e_{t},e_{t}\\sim i i d\\mathcal{N}(0,1).$\nMLE: Define $\\begin{array}{r}{\\overline{{e}}=\\frac{1}{T}\\sum_{t=1}^{T}e_{t}}\\end{array}$ . Then the mean estimator is $\\widehat{m}=m_{0}+\\sigma_{0}\\overline{{{e}}}\\sim N(0,\\sigma_{0}^{2}/T)$ . For the variance estimator, $\\widehat{e}=y-\\widehat{m}=\\sigma_{0}(\\widetilde{e}-\\overline{{e}})=\\sigma_{0}M e,M=I_{T}-1(1^{\\prime}1)^{-1}1^{\\prime}$ is  aˆn idempotent matrix with $T-1$ degrees of freedom. Hence $\\widehat\\sigma_{M L}^{2}=\\widehat e^{\\prime}\\widehat e/T\\sim\\sigma_{0}^{2}\\chi_{T-1}^{2}$ .\nBˆC: Expˆreˆssed in terms of sufficient statistics $(\\widehat{m},\\widehat{\\sigma}^{2})$ , the joint density of $\\mathbf{y}$ is $$ p({\\bf y};m,\\sigma^{2})=(\\frac{1}{2\\pi\\sigma^{2}})^{T/2}\\exp\\biggl(-\\frac{\\sum_{t=1}^{T}(m-\\widehat{m})^{2}}{2\\sigma^{2}}\\times\\frac{-T\\widehat{\\sigma}^{2}}{2\\sigma^{2}}\\biggr). $$ The flat prior is $\\pi(m,\\sigma^{2})\\propto1.$ . The marginal posterior distribution for $\\sigma^{2}$ is $\\begin{array}{r}{p(\\sigma^{2}|\\mathbf{y})=\\int_{-\\infty}^{\\infty}p(\\mathbf{y}|m,\\sigma^{2})d m}\\end{array}$ . Using the result that $\\begin{array}{r}{\\int_{-\\infty}^{\\infty}\\exp(-\\frac{T}{2\\sigma^{2}}(m-\\widehat{m})^{2})d m=\\sqrt{2\\pi\\sigma^{2}}}\\end{array}$ we have $$ \\begin{array}{r l}&{p(\\sigma^{2}|\\mathbf{y})\\propto(2\\pi\\sigma^{2})^{-(T-1)/2}\\exp(-T\\widehat{\\sigma}^{2}/2\\sigma^{2})}\\ &{\\qquad\\sim\\mathrm{inv}r\\bigg(\\frac{T-3}{2},\\frac{T\\widehat{\\sigma}^{2}}{2}\\bigg).}\\end{array} $$ The mean of an $\\operatorname{inv}T(\\alpha,\\beta)$ is $\\frac{\\beta}{\\alpha_{-}1}$ . Hence the BC posterior is $\\begin{array}{r}{\\overline{{\\sigma}}_{B C}^{2}=E(\\sigma^{2}|\\mathbf{y})=\\widehat{\\sigma}^{2}\\frac{T}{T-5}}\\end{array}$ .\nSMD: The estimator equates the auxiliary statistics computed from the sample with the average of the statistics over simulations. Given $\\sigma$ , the mean estimator $\\widehat{\\b{m}}_{S}$ solves $\\begin{array}{r}{\\widehat{m}\\doteq\\widehat{m}_{S}+\\sigma\\frac{1}{S}\\sum_{s=1}^{S}\\overline{{e}}^{s}}\\end{array}$ . Since we use sufficient statistics, $\\widehat{m}$ is the ML estimator. Thus, $\\begin{array}{r}{\\widehat{m}_{S}\\sim\\mathcal{N}(m,\\frac{\\sigma_{0}^{2}}{T}+\\frac{\\sigma^{2}}{S T})}\\end{array}$ .ˆ Since $y_{t}^{s}-{\\overline{{y}}_{t}^{s}}=\\sigma(e_{t}^{s}-{\\overline{{e}}^{s}}).$ , the variance estimator ${\\widehat{\\sigma}}_{S}^{2}$ is the $\\sigma^{2}$ th aˆt solves $\\begin{array}{r}{\\widehat{\\sigma}^{2}=\\sigma^{2}(\\frac{1}{S T}{\\sum_{s=1}^{S}}{\\sum_{t=1}^{T}}(e_{t}^{s}-\\overline{{e}}^{s})^{2})}\\end{array}$ . Hence $$ \\widehat{\\sigma}_{S}^{2}=\\frac{\\widehat{\\sigma}^{2}}{\\frac{1}{S T}\\sum_{s}\\sum_{t}(\\widehat{e}_{t}^{s}-\\overline{{e}}^{s})^{2}}=\\sigma^{2}\\frac{\\chi_{T-1}^{2}/T}{\\chi_{S(T-1)}^{2}/(S T)}=\\sigma^{2}F_{T-1,S(T-1)}. $$ The mean of a $F_{d_{1},d_{2}}$ ranˆdom variable is $\\frac{d_{2}}{d_{2}-2}$ . Hence $\\begin{array}{r}{E(\\widehat{\\sigma}_{S M D}^{2})=\\sigma^{2}\\frac{(T-1)}{S(T-1)-2}}\\end{array}$ .\nLT: The LT is defined as $$ p_{\\mathrm{LT}}(\\sigma^{2}|\\widehat\\sigma^{2})\\propto\\mathbb{1}_{\\sigma^{2}\\geq0}\\exp\\left(-\\frac{T}{2}\\frac{\\left(\\widehat\\sigma^{2}-\\sigma^{2}\\right)^{2}}{2\\widehat\\sigma^{4}}\\right) $$ which implies For $X\\sim\\mathcal{N}(\\mu,\\sigma^{2})$ we have $\\begin{array}{r}{\\mathbb{E}(X|X>a)=\\mu+\\frac{\\phi(\\frac{a-\\mu}{\\sigma})}{1-\\phi(\\frac{a-\\mu}{\\sigma})}\\sigma}\\end{array}$ (Mills-Ratio). Hence: $$ \\mathbb{E}_{\\mathrm{LT}}(\\sigma^{2}|\\widehat\\sigma^{2})=\\widehat\\sigma^{2}+\\frac{\\phi(\\frac{0-\\widehat\\sigma^{2}}{\\sqrt{2/T}\\widehat\\sigma^{2}})}{1-\\phi(\\frac{0-\\widehat\\sigma^{2}}{\\sqrt{2/T}\\widehat\\sigma^{2}})}\\sqrt{2/T}\\widehat\\sigma^{2}=\\widehat\\sigma^{2}\\left(1+\\sqrt{\\frac{2}{T}}\\frac{\\phi(-\\sqrt{T/2})}{1-\\phi(-\\sqrt{T/2})}\\right). $$ $\\begin{array}{r}{\\kappa_{\\mathrm{LT}}=\\sqrt{\\frac{2}{T}}\\frac{\\phi(-\\sqrt{T/2})}{1-\\phi(-\\sqrt{T/2})}}\\end{array}$ . We have $\\mathbb{E}_{\\mathtt{L T}}(\\sigma^{2}|\\widehat{\\sigma}^{2})=\\widehat{\\sigma}^{2}\\left(1+\\kappa_{\\mathtt{L T}}\\right)$ . The expectation of the estimator is $$ \\mathbb{E}\\left(\\mathbb{E}_{\\mathrm{LT}}(\\sigma^{2}|\\widehat{\\sigma}^{2})\\right)=\\sigma^{2}\\frac{T-1}{T}\\left(1+\\kappa_{\\mathrm{LT}}\\right) $$ from which we deduce the bias of the estimator $$ \\mathbb{E}\\left(\\mathbb{E}_{\\mathrm{LT}}(\\sigma^{2}|\\widehat{\\sigma}^{2})\\right)-\\sigma^{2}=\\sigma^{2}\\left(\\frac{T-1}{T}\\kappa_{\\mathrm{LT}}-\\frac{1}{T}\\right). $$ The variance of the estimator is $\\textstyle2\\sigma^{4}{\\frac{T-1}{T^{2}}}(1+\\kappa_{\\mathrm{LT}})^{2}$ and the Mean-Squared Error (MSE) $$ \\sigma^{4}\\left(2{\\frac{T-1}{T^{2}}}(1+\\kappa_{\\mathrm{LT}})^{2}+\\left({\\frac{T-1}{T}}\\kappa_{\\mathrm{LT}}-{\\frac{1}{T}}\\right)^{2}\\right) $$ which is the squared bias of MLE plus terms that involve the Mills-Ratio (due to the truncation).\nSLT: The SLT is defined as $$ \\langle\\sigma^{2}|\\hat{\\sigma}^{2}\\rangle\\propto1_{\\sigma^{2}\\geq0}\\exp\\left(-\\frac{T}{2}\\frac{\\left(\\hat{\\sigma}^{2}-\\sigma^{2}\\frac{\\chi_{S(T-1)}^{2}}{S T}\\right)^{2}}{2\\hat{\\sigma}^{4}}\\right)=1_{\\sigma^{2}\\geq0}\\exp\\left(-\\frac{T[\\frac{\\chi_{S(T-1)}^{2}}{S T}]^{2}}{2}\\frac{\\left(\\hat{\\sigma}^{2}/\\frac{\\chi_{S(T-1)}^{2}}{S T}-\\sigma^{2}\\right)^{2}}{2\\hat{\\sigma}^{4}}\\right) $$ where $$ \\widehat{\\sigma}_{S}^{2}=\\sigma^{2}\\frac{1}{S}\\sum_{s=1}^{2}\\frac{1}{T}\\sum_{t=1}^{T}(e_{t}^{s}-\\overline{{e}}^{s})^{2}=\\sigma^{2}\\frac{\\chi_{S(T-1)}^{2}}{S T}. $$ This yields the slightly more complicated formula $$ \\sigma^{2}|\\widehat{\\sigma}^{2},(e^{s})_{s=1,\\ldots,S}\\sim\\mathcal{N}\\left(\\widehat{\\sigma}^{2}/\\frac{\\chi_{S(T-1)}^{2}}{S T},\\frac{2\\widehat{\\sigma}^{4}}{T}[\\frac{S T}{\\chi_{S(T-1)}^{2}}]^{2}\\right) $$ and the posterior mean becomes $$ \\begin{array}{r l}&{\\mathbb{E}_{\\mathrm{SIT}}(\\sigma^{2}|\\hat{\\sigma}^{2})=\\hat{\\sigma}^{2}\\frac{S T}{\\chi_{S(T-1)}^{2}}+\\cfrac{\\phi\\left(-\\cfrac{\\hat{\\sigma}^{2}S T/\\chi_{S(T-1)}^{2}}{\\sqrt{\\frac{2\\hat{\\sigma}^{4}}{T}(\\frac{\\chi_{S(T-1)}^{2}}{\\chi_{S(T-1)}^{2}})^{2}}}\\right)}{1-\\phi\\left(-\\cfrac{\\hat{\\sigma}^{2}S T/\\chi_{S(T-1)}^{2}}{\\sqrt{\\frac{2\\hat{\\sigma}^{4}}{T}(\\frac{\\chi_{S(T-1)}^{2}}{\\chi_{S(T-1)}^{2}})^{2}}}\\right)}\\sqrt{2/T}\\frac{S T}{\\chi_{S(T-1)}^{2}}\\hat{\\sigma}^{2}}\\ &{=\\hat{\\sigma}^{2}\\frac{S T}{\\chi_{S(T-1)}^{2}}+\\frac{\\phi\\left(-\\sqrt{T/2}\\right)}{1-\\phi\\left(-\\sqrt{T/2}\\right)}\\sqrt{2/T}\\frac{S T}{\\chi_{S(T-1)}^{2}}\\hat{\\sigma}^{2}.}\\end{array} $$ $\\begin{array}{r}{\\kappa_{\\mathrm{SLT}}=\\frac{\\phi(-\\sqrt{T/2})}{1-\\phi(-\\sqrt{T/2})}\\sqrt{2/T}\\frac{S T}{\\chi_{S(T-1)}^{2}}=\\kappa_{\\mathrm{LT}}\\frac{S T}{\\chi_{S(T-1)}^{2}}}\\end{array}$ (random). We can compute $$ \\mathbb{E}\\left(\\mathbb{E}_{\\mathtt{S L T}}(\\sigma^{2}|\\widehat{\\sigma}^{2})\\right)=\\sigma^{2}\\frac{S(T-1)}{S(T-1)-2}+\\sigma^{2}\\frac{T-1}{T}\\mathbb{E}(\\kappa_{\\mathtt{S L T}}) $$ and the bias $$ \\mathbb{E}\\left(\\mathbb{E}_{\\mathrm{SLT}}(\\sigma^{2}|\\widehat{\\sigma}^{2})\\right)-\\sigma^{2}=\\sigma^{2}\\frac{2}{S(T-1)-2}+\\sigma^{2}\\frac{T-1}{T}\\mathbb{E}(\\kappa_{\\mathrm{SLT}}) $$ which is the bias of SMD and the Mills-Ratio term that comes from taking the mean of the truncated normal rather than the mode. The variance is similar to the LT and the SMD $$ 2\\sigma^{4}\\kappa_{1}\\frac{1}{T-1}+2\\sigma^{4}\\mathbb{V}(\\kappa_{\\mathrm{SLT}})+4\\sigma^{4}\\frac{T-1}{T^{2}}\\mathrm{Cov}(\\kappa_{\\mathrm{SLT}},\\frac{S}{\\chi_{S(T-1)}^{2}}). $$ The extra term is due to $\\kappa_{\\mathsf{S L T}}$ being random. We could simplify further noting that $\\begin{array}{r}{\\kappa_{\\mathsf{S L T}}=\\kappa_{\\mathrm{LT}}\\frac{S T}{\\chi_{S(T-1)}^{2}},\\mathbb{E}(\\kappa_{\\mathsf{S L T}})=\\kappa_{\\mathrm{LT}}\\frac{S T}{S(T-1)-2},}\\end{array}$ MSE $$ \\begin{array}{r l}&{\\sigma^{4}\\bigg[\\frac{2}{S(T-1)-2}+\\frac{T-1}{T}\\mathbb{E}(\\kappa_{\\mathrm{SLT}})\\bigg]^{2}+2\\sigma^{4}\\kappa_{1}\\frac{1}{T-1}+2\\sigma^{4}\\mathbb{V}(\\kappa_{\\mathrm{SLT}})+4\\sigma^{4}\\frac{T-1}{T^{2}}\\mathrm{Cov}(\\kappa_{\\mathrm{SLT}},\\frac{S}{\\chi_{\\mathrm{S}(T-1)}^{2}}}\\ &{\\qquad=\\underbrace{2\\sigma^{4}\\bigg[\\frac{2}{[S(T-1)-2]^{2}}+\\kappa_{1}\\frac{1}{T-1}\\bigg]}_{\\mathrm{MSgfSMD}}+\\frac{(T-1)^{2}}{T^{2}}\\mathbb{E}(\\kappa_{\\mathrm{SLT}}^{2}+\\frac{4\\sigma^{4}}{S(T-1)-2})\\frac{T-1}{T}\\mathbb{E}(\\kappa_{\\mathrm{SLT}})}\\ &{\\qquad+2\\sigma^{4}\\mathbb{V}(\\kappa_{\\mathrm{SLT}})+4\\sigma^{4}\\frac{T-1}{T^{2}}\\mathrm{Cov}(\\kappa_{\\mathrm{SLT}},\\frac{S}{\\chi_{\\mathrm{S}(T-1)}^{2}}).}\\end{array} $$\nRS: The auxiliary statistic for each draw of simulated data is matched to the sample auxiliary statistic. Thus, $\\widehat{m}=m^{b}+\\sigma^{b}\\overline{{{e}}}^{b}$ . Thus conditional on $\\widehat{m}$ and $\\sigma^{2,b}$ , $m^{b}={\\widehat{m}}-\\sigma^{b}{\\overline{{e}}}^{b}\\sim{\\mathcal{N}}(0,\\sigma^{2,b}/T)$ . For the variance, $\\widehat{\\sigma}^{2,b}=\\sigma^{\\overline{{2}},b}\\sum_{t}(e_{t}^{b}-\\overline{{e}}^{b})^{\\overline{{2}}}/T$ . Hence $$ \\sigma^{2,b}=\\frac{\\widehat{\\sigma}^{2}}{\\sum_{t}(e_{t}^{b}-\\overline{{e}}^{b})^{2}/T}=\\sigma^{2}\\frac{\\sum_{t}(e_{t}-\\overline{{e}})^{2}/T}{\\sum_{t}(e_{t}^{b}-\\overline{{e}}^{b})^{2}/T}\\sim\\operatorname*{inv}{F}\\bigg(\\frac{T-1}{2},\\frac{T\\widehat{\\sigma}^{2}}{2}\\bigg). $$ Note that $\\begin{array}{r}{p_{B C}(\\sigma^{2}|\\widehat{\\sigma}^{2})\\sim\\operatorname*{inv}\\Gamma\\left(\\frac{T-3}{2},\\frac{T\\widehat{\\sigma}^{2}}{2}\\right)}\\end{array}$ under a flat prior, the Jacobian adjusts to the posterior to match the true posterior. To compˆute the posterior mean, we need to compute the Jacobian of the transformation: $\\begin{array}{r}{|\\psi_{\\theta}|^{-1}=\\frac{\\partial\\sigma^{2,s}}{\\partial\\widehat{\\sigma}^{2}}}\\end{array}$ .10 Since $\\begin{array}{r}{\\sigma^{2,b}=\\frac{T\\widehat\\sigma^{2}}{\\sum_{t}(e_{t}^{b}-\\overline{e}^{b})^{2}},|\\psi_{\\theta}|^{-1}=\\frac{T}{\\sum_{t}(e_{t}^{b}-\\overline{e}^{b})^{2}}.}\\end{array}$ Under the prior $p(\\sigma^{2,s})\\propto1$ , the posterior mean without the Jacobian transformation is $$ \\overline{{\\sigma}}^{2}=\\sigma^{2}\\frac{1}{B}\\sum_{b=1}^{B}\\frac{\\sum_{t}(e_{t}-\\overline{{e}})^{2}/T}{\\sum_{t}(e_{t}^{b}-\\overline{{e}}^{b})^{2}/T}\\overset{B\\to\\infty}{\\longrightarrow}\\widehat{\\sigma}^{2}\\frac{T}{T-3}. $$ The posterior mean after adjusting for the Jacobian transformation is $$ \\overline{{\\sigma}}_{R S}^{2}=\\frac{\\sum_{b=1}^{B}\\sigma^{2,b}\\cdot\\frac{T}{\\sum_{t}(e_{t}^{b}-\\overline{{e}}^{b})^{2}}}{\\sum_{b=1}^{B}1/\\sigma^{2,b}}=\\widehat{\\sigma}^{2}\\frac{\\sum_{b}(\\frac{T}{\\sum_{t}(e_{t}^{b}-\\overline{{e}}^{b})^{2}})^{2}}{\\sum_{b=1}\\sum_{t}(e_{t}^{b}-\\overline{{e}}^{b})^{2}/T}=T\\widehat{\\sigma}^{2}\\frac{\\frac{1}{B}\\sum_{b}(z^{b})^{2}}{\\frac{1}{B}\\sum_{b}z^{b}} $$ where $\\begin{array}{r}{1/z^{b}=\\sum_{t}(e_{t}^{b}-\\overline{{e}}^{b})^{2}}\\end{array}$ . As $B\\to\\infty$ , $\\textstyle{\\frac{1}{B}}\\sum_{b}(z^{b})^{2}{\\overset{p}{\\to}}E[(z^{b})^{2}]$ and $\\begin{array}{r}{\\frac15\\sum_{b}z^{b}\\xrightarrow{p}E[z^{b}].{\\mathrm{Now}}z^{b}\\sim\\operatorname*{inv}\\chi_{T-1}^{2}}\\end{array}$ with mean T 1 3 and variance $\\frac{2}{(T-3)^{2}(T-5)}$ giving $\\begin{array}{r}{E[(z^{b})^{2}]=\\frac{\\overline{{\\mathbf{\\theta}}}1}{(T-3)(T-5)}}\\end{array}$ . Hence as $B\\to\\infty$ , $\\begin{array}{r}{\\overline{{\\sigma}}_{R S,R}^{2}=\\widehat{\\sigma}^{2}\\frac{T}{T-5}=\\overline{{\\sigma}}_{B C}^{2}}\\end{array}$ .\nDerivation of the bias reducing prior. The bias of the MLE estimator has $\\mathbb{E}(\\widehat{\\boldsymbol{\\sigma}})~=~\\sigma^{2}~-~\\frac{1}{T}\\sigma^{2}$ and variance $V({\\widehat{\\sigma}}^{2})=$ $\\textstyle2\\sigma^{4}({\\frac{1}{T}}-{\\frac{1}{T^{2}}})$ . Since the auxiliary parameters coincide with the parameters ofˆ interest, $\\nabla_{\\boldsymbol{\\theta}}\\psi_{}({\\boldsymbol{\\theta}})$ and $\\nabla_{\\theta\\theta^{\\prime}}\\psi(\\theta)=0$ For $\\begin{array}{r}{Z\\sim\\hat{\\mathcal{N}}(0,1),A(v;\\sigma^{2})=\\sqrt{2}\\sigma^{2}(1-\\frac{1}{T})Z}\\end{array}$ . Thus $\\begin{array}{r}{\\partial_{\\sigma^{2}}A(v;\\sigma^{2})=\\sqrt{2}(1-\\frac{1}{T})Z,a^{s}=\\sqrt{2}\\sigma^{2}(1-\\frac{1}{T})(Z-Z^{s})}\\end{array}$ The terms in the asymptotic expansion are therefore $$ \\begin{array}{c}{{\\partial_{\\sigma^{2}}A(v^{s};\\sigma^{2})a^{s}=2\\sigma^{2}(1-\\displaystyle\\frac{1}{T})^{2}Z^{s}(Z-Z^{s})}}\\ {{\\Rightarrow\\mathbb{E}(\\partial_{\\sigma^{2}}A(v^{s};\\sigma^{2})a^{s})=-\\sigma^{2}2(1-\\displaystyle\\frac{1}{T})^{2}}}\\ {{{}V(a^{s})=4\\sigma^{4}(1-\\displaystyle\\frac{1}{T})^{2}}}\\ {{{}c o v(a^{s},a^{\\prime})=2(1-\\displaystyle\\frac{1}{T})^{2}\\sigma^{4}}}\\ {{(1-\\displaystyle\\frac{1}{S})V(a^{s})+\\displaystyle\\frac{S-1}{S}c o v(a^{s},a^{\\prime})=\\sigma^{4}(1-\\displaystyle\\frac{1}{T})^{2}\\Big(4(1-\\displaystyle\\frac{1}{S})+2\\displaystyle\\frac{S-1}{S}\\Big)=\\displaystyle\\frac{\\sigma^{2}S}{3(S-1)}.}}\\end{array} $$ Noting that $|\\partial_{\\widehat{\\sigma}^{2}}\\sigma^{2,b}|~\\propto~\\sigma^{2,b}$ , it is analytically simpler in this example to solve for the weights directly, i.e. $w(\\sigma^{2})~=$ $\\pi(\\sigma^{2})|\\partial_{\\widehat{\\sigma}^{2}}\\sigma^{2,b}|$ ˆrather than the bias reducing prior $\\pi$ itself. Thus the bias reducing prior satisfies $$ \\partial_{\\sigma^{2}}w(\\sigma^{2})=\\frac{-2\\sigma^{2}(1-\\frac{1}{T})^{2}}{\\sigma^{4}(1-\\frac{1}{T})^{2}\\Big(4(1-\\frac{1}{S})+2\\frac{S-1}{S}\\Big)}=-\\frac{1}{\\sigma^{2}}\\frac{2}{4(1-\\frac{1}{S})+2\\frac{S-1}{S}}. $$ Taking the integral on both sides we get: $$ \\log(w(\\sigma^{2}))\\propto-\\log(\\sigma^{2})\\Rightarrow w(\\sigma^{2})\\propto{\\frac{1}{\\sigma^{2}}}\\Rightarrow\\pi(\\sigma^{2})\\propto{\\frac{1}{\\sigma^{4}}} $$ which is the Jeffreys prior if there is no re-weighting and the square of the Jeffreys prior when we use the Jacobian to reweight. Since the estimator for the mean was unbiased, $\\pi(m)\\propto1$ is the prior for $m$ . The posterior mean under the Bias Reducing Prior $\\pi(\\sigma^{2,s})\\:=\\:1/\\sigma^{4,s}$ is the same as the posterior without weights but using the Jeffreys prior $\\pi(\\sigma^{2,s})=1/\\sigma^{2,s}$ : $$ \\overline{{\\sigma}}_{R S}^{2}=\\frac{\\sum_{s=1}^{S}\\sigma^{2,s}(1/\\sigma^{2,s})}{\\sum_{s=1}^{S}1/\\sigma^{2,s}}=\\frac{S}{\\sum_{s=1}^{S}1/\\sigma^{2,s}}=\\sigma^{2}\\frac{\\sum_{t=1}^{T}(e_{t}-\\overline{{e}})^{2}/T}{\\sum_{s=1}^{S}\\sum_{t=1}^{T}(e_{t}^{s}-\\overline{{e}}^{s})^{2}/(S T)}\\equiv\\widehat{\\sigma}_{S M D}^{2}. $$\nD.1. Further results for dynamic panel model with fixed effects Table 4 Dynamic panel $\\rho=0.9$ , $\\beta=1,\\sigma^{2}=2$\n\nThe text discusses various estimators for the mean and variance in a data generating process, including MLE, BC, SMD, LT, SLT, RS, and Bootstrap methods. It provides detailed mathematical derivations and properties of these estimators."
  },
  {
    "qid": "econ-empirical-937-4-0-3",
    "question": "4) Compare the implications of the intertemporal substitution model and the labour adjustment cost model for cyclical real wages and employment.",
    "gold_answer": "1. Intertemporal substitution model: workers adjust hours worked in response to temporary wage changes, leading to procyclical real wages and employment.\n2. Labour adjustment cost model: firms adjust labour input slowly, leading to acyclical or countercyclical real wages and procyclical employment.\n3. Key difference: adjustment costs dampen the firm's response to wage changes, while intertemporal substitution amplifies the worker's response.\n4. Empirical implication: adjustment cost models better explain observed acyclicality of real wages.",
    "question_context": "The real wage may differ from the marginal product of labour in the short run when there are costs of adjusting inputs. Labour adjustment costs reduce the response of hours of work to the real wage: the firm changes its labour input in response to a transitory shock to productivity or the wage, by less than it would without costs of adjustment.\nUnemployment is modelled in an equilibrium framework as resulting from search, where the search is not only that of unemployed workers looking for jobs but also that of vacancies looking for workers.\n\nThe real wage may differ from the marginal product of labour in the short run when there are costs of adjusting inputs. Labour adjustment costs reduce the response of hours of work to the real wage. Unemployment is modelled in an equilibrium framework as resulting from search, where the search is not only that of unemployed workers looking for jobs but also that of vacancies looking for workers."
  },
  {
    "qid": "econ-empirical-727-1-0-0",
    "question": "1) Derive the wage gap equation between two educational groups $s_1$ and $s_2$ from the earnings regression function $Y_t = \\alpha_t(s) + \\gamma_t a + \\epsilon_t$. Explain the role of the 'ability bias' term $\\gamma_t[\\mathrm{E}(a|s=s_1) - \\mathrm{E}(a|s=s_2)]$ in this context.",
    "gold_answer": "1. Start with the earnings regression function: $$ Y_t = \\alpha_t(s) + \\gamma_t a + \\epsilon_t $$ 2. Take the conditional expectation for groups $s_1$ and $s_2$: $$ \\mathrm{E}(Y_t|s=s_1) = \\alpha_t(s_1) + \\gamma_t \\mathrm{E}(a|s=s_1) $$ $$ \\mathrm{E}(Y_t|s=s_2) = \\alpha_t(s_2) + \\gamma_t \\mathrm{E}(a|s=s_2) $$ 3. Subtract the second equation from the first: $$ \\mathrm{E}(Y_t|s=s_1) - \\mathrm{E}(Y_t|s=s_2) = [\\alpha_t(s_1) - \\alpha_t(s_2)] + \\gamma_t[\\mathrm{E}(a|s=s_1) - \\mathrm{E}(a|s=s_2)] $$ 4. The 'ability bias' term $\\gamma_t[\\mathrm{E}(a|s=s_1) - \\mathrm{E}(a|s=s_2)]$ captures the wage differential due to differences in average ability between the two educational groups, weighted by the returns to ability $\\gamma_t$.",
    "question_context": "The earnings regression function is given by: $$ Y_{t}=\\alpha_{t}(s)+\\gamma_{t}a+\\epsilon_{t}, $$ where $Y_{t},$ a variable measuring earnings, is explained by years of schooling $S_{:}$ , ability $a$ , and a zero–mean stochastic disturbance $\\epsilon$.\nThe wage gap between groups with different educational attainments $s_{1}$ and $s_{2}$ can then be described by: $$ \\begin{array}{r l}&{\\mathrm{E}(Y_{t}|s=s_{1})-\\mathrm{E}(Y_{t}|s=s_{2})=\\alpha_{t}(s_{1})-\\alpha_{t}(s_{2})}\\ &{\\qquad+\\gamma_{t}[\\mathrm{E}(a|s=s_{1})-\\mathrm{E}(a|s=s_{2})].}\\end{array} $$\nIn the course of economic growth, the accumulation of physical capital alters factor prices and thus incentives to acquire human capital. As successively less talented individuals are inclined to upgrade their inherent abilities, the fraction of skilled workers rises. The changing ability structure within skill groups triggers a nonmonotone development of the wage differential by skills.\n\nThis paper analyses the effect of human-capital investments of heterogeneous individuals on the dynamics of the wage structure within a neoclassical growth model. The accumulation of physical capital changes relative factor prices and thus incentives to acquire skills, thereby altering the composition of the labour force. Without relying on exogenous shocks, our framework generates dynamics that resemble important observations on wage inequality (e.g., the non-monotone evolution of the skill premium). Additional incorporation of wage rigidities emphasises the trade off between residual wage inequality and employment opportunities for unskilled labour that is consistent with country-specific evidence."
  },
  {
    "qid": "econ-empirical-1638-4-0-2",
    "question": "3) Analyze the nonlinear effect of LEV on ROE using the fitted curve in Figure 4. Why does LEV exhibit a cubic polynomial trend, and what are the implications for firms with high leverage?",
    "gold_answer": "**Analysis of LEV's Nonlinear Effect:**\n1. The fitted curve for \\(\\eta(\\text{LEV})\\) shows a cubic polynomial trend, with significant \\(\\text{LEV}^3\\) term (\\(\\hat{\\theta}_{p+3} = 0.8070, p = 0.0309\\)).\n2. **Implications:**\n   - **High LEV:** Firms with high leverage experience amplified return volatility due to the nonlinear interaction between debt financing and operational risk.\n   - **Bankruptcy Risk:** The cubic trend suggests that extreme LEV values can lead to disproportionate risks, as debt amplifies both gains and losses.\n3. **Economic Rationale:** LEV enables control of resources beyond equity, but the imbalance between investment returns and interest rates introduces nonlinearity, as seen in the fitted curve.",
    "question_context": "Considered as desirable when above $15\\%$, it is an important metric for firms and is also one of the most important indices in investment decisions which is commonly affected by the following five economic variables. The first is profit margin ratio (PM), which can be seasonal, showing how efficiently a business operates. The second is asset turnover ratio (ATR) measuring how efficient a company is at using its assets to generate profits. More specifically, companies with low profit margins tend to have a high asset turnover, while those with high profit margins have a low asset turnover. Financial leverage (LEV) is the third variable referring to the amount of debt a company has for financing its operations, which perhaps has nonlinear impact on a firm’s development since LEV may generate profit and undertake finical risk simultaneously. That is, the influence of LEV on ROE may fluctuate and be accompanied by uncertainty. Moreover, a large LEV value carries a risk of bankruptcy. The fourth variable is total asset (ASSET) as an economic resource from which the future economic profit could be guaranteed. The final variable is historical sales growth rate (SGR), one of the simplest approaches for estimating future growth.\nUsing our methodology, the effective sample size is found to be $n_{0}~=~585$ determined by the selected threshold $w_{n}~=$ 0.185 through minimizing the distance criterion in (14), giving a sample fraction $n_{0}/n=19.95\\%$.\nFor the parametric component, the tail index estimates are reported in the upper panel of Table 3. We observe that both ASSET and ATR have significant positive effects and conclude that larger ASSET is helpful to the development of a company and that the enterprise with higher ATR can use its asset more effectively for generating profits.\nFor the nonparametric component, we display the fitted curve for LEV and the $95\\%$ pointwise bootstrap-based confidence bands in the left panel of Figure 4, in which the nonlinear effect of LEV on ROE is quite clear.\n\nThis section analyzes the return on equity (ROE) data using a semiparametric model to study the tail behavior, considering the nonlinear effect of financial leverage (LEV) and linear effects of other economic variables."
  },
  {
    "qid": "econ-empirical-293-0-2-1",
    "question": "2) Analyze the welfare implications of an 'imperfect' government that cannot directly control prices or impose taxes in an oligopolistic sector.",
    "gold_answer": "1. Model the government as choosing \\( n \\) to maximize \\( W \\), subject to inability to set \\( P \\) or \\( t \\). \\n2. The government's problem is \\( \\max_n W(n) \\), where \\( W(n) \\) is derived from the Cournot equilibrium. \\n3. Show that the second-best \\( n^{**} \\) approximates the first-best \\( n^* \\) when fixed costs are significant. \\n4. The welfare loss from imperfection is \\( W(n^*) - W(n^{**}) \\). \\n5. The result holds if \\( \\frac{dW}{dn} \\) is steep near \\( n^* \\).",
    "question_context": "The method is first to establish the ^excess competition’ theorem of oligopoly in a Cournot-Nash framework in partial equilibrium, and then to set oligopoly alongside competition in the economy at large.\nThat welfare is improved by restriction of numbers in the oligopolistic section is shown, under assumptions that Cournot behaviour suffices to describe oligopolists’ strategies, that consumers are of unvarying characteristics, and that factor intensities do not vary between the sectors.\nThe author then introduces the notion of competitive advantage through strategic commitment, involving sunk costs. This invites a sequential, two stage approach to competition in oligopoly.\n\nThe text examines the concept of 'excess competition' in oligopoly markets, showing how limiting the number of firms can improve welfare under certain conditions. It also explores the role of strategic commitment, such as R&D investment, in shaping competitive outcomes."
  },
  {
    "qid": "econ-empirical-494-1-1-2",
    "question": "3) Compare the power of the $\\hat{Z}_T$ test and the ADF test against STUR alternatives, using the results in Tables 1 and 2.",
    "gold_answer": "1. Table 1 shows the ADF test has low power (~6.8% rejection for $\\sigma_{\\eta}^2 = 0.0001$).\n2. Table 2 shows $\\hat{Z}_T$ has higher power (e.g., 15.2% rejection for $\\sigma_{\\eta}^2 = 0.0001$ at 5% nominal size).\n3. The $\\hat{Z}_T$ test's power increases with $\\sigma_{\\eta}^2$ and sample size, outperforming the ADF test, especially for larger $\\sigma_{\\eta}^2$ (e.g., 61.4% power for $\\sigma_{\\eta}^2 = 0.001$ at $T=1000$).",
    "question_context": "The model treated by Leybourne et al. is: $$x_{t}=\\sum_{i=1}^{q+1}\\delta_{i,t}x_{t-i}+\\varepsilon_{t},$$ where $\\delta_{1,t}=(a_{t}+\\pi_{1}), \\delta_{i,t}=(\\pi_{i}-a_{t}\\pi_{i-1}), i=2,\\ldots,q, \\delta_{q+1,t}=-a_{t}\\pi_{q}$.\nThe test statistic is: $$\\hat{Z}_{T}=T^{-3/2}\\hat{\\sigma}_{\\varepsilon}^{-2}\\hat{\\sigma}_{\\kappa}^{-1}\\sum_{t=q+3}^{T}\\left(\\sum_{j=q+2}^{t-1}\\hat{\\varepsilon}_{j}\\right)^{2}(\\hat{\\varepsilon}_{t}^{2}-\\hat{\\sigma}_{\\varepsilon}^{2}).$$\nThe test has good power against STUR alternatives for larger $\\sigma_{\\eta}^2$ and sample sizes, but power declines as $\\sigma_{\\eta}^2$ decreases.\n\nThis section introduces a test developed by Leybourne et al. (1994) for stochastic unit roots, which has better power properties than the ADF test against STUR alternatives."
  },
  {
    "qid": "econ-empirical-19-4-1-2",
    "question": "7) Show how the participation decision $B P_{h3} = 1 \\iff (b_{h3} - \\lambda_h e^{\\gamma_3}) + \\frac{\\omega_h}{2} + \\nu_h > 0$ is derived from the model.",
    "gold_answer": "The participation decision combines:\n\n1. **Net Benefit**: $(b_{h3} - \\lambda_{h3}) + \\frac{\\omega_h}{2}$.\n2. **Unobserved $\\lambda_{h3}$**: Hospitals use $\\lambda_h e^{\\gamma_3}$ (expected value).\n3. **Choice Shifter**: $\\nu_h$ captures unobserved heterogeneity.\n4. Thus:\n   $$B P_{h3} = 1 \\iff (b_{h3} - \\lambda_h e^{\\gamma_3}) + \\frac{\\omega_h}{2} + \\nu_h > 0$$\n   This mirrors the theoretical model but accounts for incomplete information.",
    "question_context": "For each hospital in the sample, we observe three periods of claims data $\\{y_{h1},y_{h2},y_{h3}\\}$ and three periods of bundled payment participation indicators $\\{B P_{h1},B P_{h2},B P_{h3}\\}$.\nThe model in Section IV defines each hospital by two hospital-specific parameters (level and slope). To accommodate the panel nature of the data, we maintain the assumption that hospitals are associated with these level and slope parameters, $\\{\\lambda_{h},\\omega_{h}\\}$.\nWe also assume that hospitals know their level type $\\lambda_{h}$ and the period-specific shifters $(\\gamma_{1}$ and $\\gamma_{3}$ ) but do not have any ex ante information about the random shock $\\epsilon_{h t}$.\n\nThe text outlines an econometric model to estimate hospital-level parameters $\\{\\lambda_h, \\omega_h\\}$ using claims data and participation decisions. It leverages random assignment to bundled payments for identification."
  },
  {
    "qid": "econ-empirical-489-7-0-1",
    "question": "2) Derive the conditions under which the covariance function $(t,s)\\longmapsto c_{P_{n}}(t,s)$ converges uniformly to a uniformly continuous function on $(T,d_{T})$.",
    "gold_answer": "1. The covariance function $c_{P_{n}}(t,s) = P_{n}f_{s,P_{n}}f_{t,P_{n}} - P_{n}f_{s,P_{n}}P_{n}f_{t,P_{n}}$ must satisfy:  \n   - **Uniform boundedness**: $\\sup_{P\\in\\mathcal{P}} \\|f_{t,P}\\|_{P,2} < \\infty$.  \n   - **Total boundedness of $(T,d_{T})$**: The metric space must be totally bounded to ensure compactness.  \n2. By the **Arzela-Ascoli theorem**, the set $\\{(t,s)\\longmapsto c_{P}(t,s): P\\in\\mathcal{P}\\}$ is relatively compact in $\\ell^{\\infty}(T\\times T)$ if it is uniformly bounded and equicontinuous.  \n3. The assumptions in equation (B.1) ensure that the entropy condition holds, guaranteeing uniform convergence.",
    "question_context": "Parts (a) and (b) are a direct consequence of Lemma B.2. In particular, Lemma B.2(a) implies stochastic equicontinuity under arbitrary subsequences $\\mathcal{P}_{n}\\in\\mathcal{P}$ , which implies part (b). Part (a) follows from Lemma B.2(b) by splitting an arbitrary sequence $\\pmb{n\\in\\mathbb{N}}$ into subsequences $\\pmb{n}\\in\\mathbb{N}^{\\prime}$ along each of which the covariance function $(t,s)\\longmapsto c_{P_{n}}(t,s):=$ $P_{n}f_{s,P_{n}}f_{t,P_{n}}-P_{n}f_{s,P_{n}}P_{n}f_{t,P_{n}}$ converges uniformly and therefore also pointwise to a uniformly continuous function on $(T,d_{T})$ . This convergence is possible because $\\{(t,s)\\longmapsto c_{P}(t,s):$ $P\\in\\mathcal{P}\\}$ is a relatively compact set in $\\ell^{\\infty}(T\\times T)$ in view of the Arzela-Ascoli theorem, the assumptions in equation (B.1), and total boundedness of $(T,d_{T})$ . By Lemma B.2(b), pointwise convergence of the covariance function implies weak convergence to a tight Gaussian process which may depend on the identity $\\mathbb{N}^{\\prime}$ of the subsequence. Since this argument applies to each such subsequence that split the overall sequence, part (b) follows.\n\nThe proof involves stochastic equicontinuity and weak convergence to a tight Gaussian process under specific conditions on the covariance function and the metric space."
  },
  {
    "qid": "econ-empirical-1066-5-0-2",
    "question": "3) Compare the forecast performance of UC-MA and UC models based on the RMSFE results in Table 5.",
    "gold_answer": "1. **Short-horizon forecasts**: UC-MA has an RMSFE of 92% of UC, indicating better performance. \n2. **Long-horizon forecasts**: UC-MA consistently outperforms UC, even at four-year horizons. \n3. **Conclusion**: The MA component improves forecast accuracy across all horizons.",
    "question_context": "We use each of the eleven models to produce both point and density $k$-step-ahead iterated forecasts with $k = 1, 4, 8, 12$ and 16. Specifically, given the data up to time $t$, denoted as $\\mathbf{y}_{1:t}$, we implement the MCMC sampler in Section 3.2 to obtain posterior draws given $\\mathbf{y}_{1:t}$. Then, we compute the predictive mean $\\mathbb{E}(y_{t+k} \\mid \\mathbf{y}_{1:t})$ as the point forecast and the predictive density $p(y_{t+k} \\mid \\mathbf{y}_{1:t})$ as the density forecast.\nThe metric used to evaluate the point forecasts is the root mean squared forecast error (RMSFE) defined as $$\\mathrm{RMSFE}=\\sqrt{\\frac{\\displaystyle\\sum_{t=t_{0}}^{T-k}(y_{t+k}^{0}-\\mathbb{E}(y_{t+k}\\mid\\mathbf{y}_{1:t}))^{2}}{\\displaystyle T-k-t_{0}+1}}.$$\nTo evaluate the density forecast $p(y_{t+k} \\mid \\mathbf{y}_{1:t})$, one natural measure is the predictive likelihood $p(y_{t+k} = y_{t+k}^{\\circ} \\mid \\mathbf{y}_{1:t})$, i.e., the predictive density of $y_{t+k}$ evaluated at the observed value $y_{t+k}^{0}$. We evaluate the density forecasts using the sum of log predictive likelihoods: $$\\sum_{t=t_{0}}^{T-k}\\log p(y_{t+k}=y_{t+k}^{0}\\mid\\mathbf{y}_{1:t}).$$\n\nThe section discusses a recursive out-of-sample forecasting exercise evaluating the performance of various models for forecasting US quarterly CPI inflation at different horizons. The models include UC, UCSV, AR(1), AR(2), and their variants with an MA component, along with versions having only constant variance for comparison."
  },
  {
    "qid": "econ-empirical-357-0-0-3",
    "question": "4) Analyze the welfare loss measure $\\mathcal{L}(\\phi_{c},\\phi_{p},\\Delta_{\\phi_{p}})$ and explain why it accounts for job creation costs.",
    "gold_answer": "1. **Definition:** $\\mathcal{L}(\\phi_{c},\\phi_{p},\\Delta_{\\phi_{p}})=(1-\\beta)\\left(\\frac{N_{\\mathrm{no-bc}}-E[N_{\\mathrm{bc}}]}{\\phi_{p}}\\right)$.  \n2. **Interpretation:** Compares discounted earnings without business cycles ($N_{\\mathrm{no-bc}}$) to expected earnings with cycles ($E[N_{\\mathrm{bc}}]$).  \n3. **Job Creation Costs:** The measure reflects reduced job duration due to cycles, which raises the effective cost of creation per unit of output.",
    "question_context": "The joint distribution of $\\phi_{c}$ and $\\phi_{p}$ has continuous support without point mass. The density is denoted by $f(\\phi_{c},\\phi_{p})$.\nProduction of an active job $i$, $y_{t}(i)$, is given by $y_{t}(i)=\\phi_{p}(i)\\phi_{p,t}$, where $\\boldsymbol{\\varPhi}_{p,t}$ is aggregate productivity.\nThe effort constraint is given by $\\phi_{p}\\phi_{p,t}\\geq\\chi$, where $\\chi=\\chi_{e}+\\mu$.\n\nThe paper introduces a model with job heterogeneity, job creation costs, and an agency problem that prevents first-best outcomes. The agency problem is based on the contractual fragility framework of Ramey and Watson (1997), where employers and employees face effort constraints."
  },
  {
    "qid": "econ-empirical-295-0-2-1",
    "question": "6) Discuss the implications of menu costs on the frequency of price adjustments in their model.",
    "gold_answer": "Menu costs introduce a threshold for price adjustments. Firms adjust prices only when the benefit exceeds the cost, leading to infrequent adjustments and price stickiness.",
    "question_context": "'Optimum Pricing Policy Under Stochastic Inflation,’ Eytan Sheshinski, Hebrew University, and Yoram Weiss, Tel Aviv University.\n\nEytan Sheshinski and Yoram Weiss examine optimal pricing strategies in the presence of stochastic inflation, focusing on firms' adjustment mechanisms."
  },
  {
    "qid": "econ-empirical-1669-2-0-2",
    "question": "3) Derive the mathematical relationship between relative mobility (rank-rank slope) and absolute mobility, and explain why the 1980-1982 US copula (with the lowest relative mobility rate) produces the highest absolute mobility estimates.",
    "gold_answer": "The rank-rank slope \\( \\beta \\) measures relative mobility: \\( E[Y_c | Y_p] = \\beta Y_p \\). Absolute mobility \\( M \\) depends on the joint distribution of \\( (Y_p, Y_c) \\).\n\nIf \\( \\beta \\) is low (low relative mobility), the copula implies weaker dependence between \\( Y_p \\) and \\( Y_c \\), increasing the probability that \\( Y_c > Y_p \\) (higher absolute mobility). Formally:\n\\[ M = P(Y_c > Y_p) = \\int_0^1 \\int_0^1 I(F_c^{-1}(q) > F_p^{-1}(p)) \\, c(p,q) \\, dp \\, dq \\]\nwhere \\( c(p,q) \\) is the copula density. A flatter copula (lower \\( \\beta \\)) increases \\( M \\).",
    "question_context": "The first source of potential error we consider is the assumption that one specific copula—often generated from a recent cohort where linked parent-child data are available—can be used to construct upward mobility estimates for earlier cohorts. This assumption will be unproblematic if either (i) the true copula (i.e., the rate of relative income mobility) is fairly stable over time or (ii) which copula is used has minimal impact on the calculated upward mobility rate.\nFigure 2 plots the true, linked-records rate of upward absolute mobility (solid line) as well as two versions of the estimated rate using the 'copula and marginals' approach with high-resolution $100\\times100$ copulas. The dotted line shows the 'copula and marginals' estimate using a copula constructed based on data from the exact cohort being estimated, while the dashed line uses the most recent copula for all cohorts in a given country. Both estimates hew closely to the true rate: across all four countries for which both approaches are implemented, the exact copula estimate never varies by more than 1.13 percentage points from the true value, and the most recent copula estimate never varies by more than 1.53 percentage points.\nThe area shaded in dark gray shows the range of estimates generated when we apply all $76~100\\times100$ empirical copulas that we observe across countries and birth cohorts in our sample to the marginal distributions for each country-cohort. Even though rates of relative income mobility varied substantially within our sample—the parent-child rank-rank slopes we observe range from 0.07 for the 1976 cohort in Norway to 0.34 for the 1980-1982 cohorts in the United States—the bounds constructed by considering all empirical copulas never extend more than 4.36 percentage points from the true value.\n\nThe text discusses the assumption that a specific copula can be used to construct upward mobility estimates for earlier cohorts, considering the stability of the true copula over time and the impact of the copula choice on the calculated upward mobility rate."
  },
  {
    "qid": "econ-empirical-761-2-0-3",
    "question": "4) Explain why the proposed test $\\widetilde{T}_{n}$ performs better than PP and ADF tests under MA(1) noise. Provide a mathematical intuition.",
    "gold_answer": "1. **Autocorrelation Decay**:\n   For MA(1) noise, $\\text{Cov}(\\varepsilon_{0}, \\varepsilon_{k}) = 0$ for $k \\geq 2$. Thus, including lags $M > 2$ does not improve $\\widetilde{T}_{n}$ under the null.\n\n2. **Test Statistic Adjustment**:\n   The New test statistic:\n   $$\\widetilde{T}_{n} = n\\sum_{k=1}^{M}\\frac{1-\\widehat{\\rho}(k)}{M(M+1)}$$\n   decreases for $M > 2$, making it more sensitive to deviations from the null hypothesis.\n\n3. **Empirical Results**:\n   Tables 6-7 show the New test outperforms PP and ADF for MA(1) noise, particularly for $\\alpha$ close to 1.",
    "question_context": "The critical values of the limit distribution in Theorem 2.2(ii) for $T_{n}$ with $M\\rightarrow\\infty$ are derived based on the empirical percentiles of $\bar{T_{n}}$ with i.i.d. $N(0,1)$ noise, sample size $n=10000$ and 10,000 repetitions.\nThe performance of the new test (New) $\\widetilde{T}_{n}$ is compared with those derived by the Phillips and Perron (PP, 1988), the Dickey and Fuller (DF, 1979) and the augmented Dickey–Fuller (ADF) tests methods.\nUnder the random-walk true model: $Y_{t}=Y_{t-1}+\\varepsilon_{t}$, we compute the PP $t$-statistics based on the regression: $Y_{t}=\\beta+\\alpha Y_{t-1}+\\varepsilon_{t}$.\nThe ADF test is computed by $t$-statistics based on model $\\nabla Y_{t}=\\beta+(\\alpha-1)Y_{t-1}+\\sum_{i=1}^{p}\\alpha_{i}\\nabla Y_{t-i}+e_{t}$.\n\nThis section evaluates the finite sample performance of the unit-root test statistic $\\widetilde{T}_{n}$ under various noise conditions and compares it with established tests like Phillips-Perron (PP), Dickey-Fuller (DF), and augmented Dickey-Fuller (ADF). Critical values are derived from empirical percentiles under different lag orders and noise types."
  },
  {
    "qid": "econ-empirical-1650-1-0-2",
    "question": "3) Explain why the Dufour test, which replaces the true $\\gamma$ with its OLS estimate $\\hat{\\gamma}$, may suffer from size distortions in finite samples despite being asymptotically valid.",
    "gold_answer": "3. **Dufour Test Size Distortions**: \n   - The Dufour test relies on replacing $\\gamma$ with $\\hat{\\gamma}$, introducing estimation error. \n   - In finite samples, this leads to non-negligible bias in the recursive residuals. \n   - The test statistic no longer follows the assumed distribution under $H_0$, causing size distortions. \n   - Asymptotically, the estimation error vanishes, restoring validity.",
    "question_context": "Consider the simple dynamic linear regression model \n\n$$ y_{t}=\\gamma y_{t-1}+\\beta_{1}x_{t1}+\\cdot\\cdot\\cdot+\\beta_{K}x_{t K}+u_{t} $$ \n\nwhere the disturbances $u_{t}$ are id $(0,\\sigma^{2})$ (not necessarily normal), $|\\gamma|<1$, $\\boldsymbol{u}_{t}$ is independent of $y_{t-j}\\left(j\\geqslant1\\right)$, and the pre-sample observation $y_{0}$ is some fixed and known number.\nThe CUSUM test for the stability of $\\delta$ is based on successive partial sums of recursive residuals $w_{r}$, which for $K+2\\leqslant r\\leqslant T$ are defined as \n\n$$ w_{r}=\\big(y_{r}-z_{r}^{\\prime}\\hat{\\delta}^{(r-1)}\\big)/f_{r}, $$ \n\nwhere \n\n$$ f_{r}=\\Big(1+z_{r}^{\\prime}\\big(Z^{(r-1)\\prime}Z^{(r-1)}\\big)^{-1}z_{r}\\Big)^{1/2}, $$ \n\nand $\\hat{\\delta}^{(r-1)}$ is the OLS estimate for $\\delta$ from the first $r-1$ observations.\nThe test statistic is \n\n$$ S=\\operatorname*{max}_{K+1<r\\leqslant T}\\left|\\frac{W^{(r)}}{\\sqrt{T-K-1}}\\right|\\left/\\left(1+2\\frac{r-K-1}{T-K-1}\\right),\\right. $$ \n\nwhere \n\n$$ W^{(r)}=\\frac{1}{\\hat{\\sigma}}\\sum_{t=K+2}^{r}w_{t} $$ \n\nis the cumulated sum of the recursive residuals, standardized by some consistent estimate $\\hat{\\pmb{\\sigma}}$ for the disturbance standard deviation $\\pmb{\\upsigma}$.\nTHEOREM 1: Let $W^{(r)}$ be as in (10), and let a be determined from (12). Then, under the conditions imposed at the beginning of this section, \n\n$$ \\operatorname*{lim}_{T\\to\\infty}\\operatorname*{Pr}\\left\\{\\operatorname*{max}_{K+1<r\\leqslant T}\\frac{W^{(r)}}{\\sqrt{T-K-1}}\\left/\\left(1+2\\frac{r-K-1}{T-K-1}\\right)\\geqslant a\\right\\}=\\frac{\\alpha}{2}, $$ \n\nwhether there is a lagged endogenous variable among the regressors or not.\n\nThe section presents a dynamic linear regression model with lagged endogenous variables and discusses the CUSUM test for parameter stability under various assumptions."
  },
  {
    "qid": "econ-empirical-1042-4-0-2",
    "question": "3) Describe the reversible-jump MCMC algorithm's application in comparing EGARCH models, including how proposal densities are constructed.",
    "gold_answer": "3. Steps:\n1. Initialize chain with model $m$ and parameters $\\theta_m$.\n2. Propose new model $m'$ with probability $j(m,m')$.\n3. Generate auxiliary variables $u$ from $q(u|m')$ to match dimensions.\n4. Accept/reject via Metropolis-Hastings ratio. Proposal densities $q(u|m')$ are multivariate normal with means/covariances from preliminary MCMC runs (e.g., $\\mathrm{N}(\\hat{\\mu}, \\hat{\\Sigma})$).",
    "question_context": "We illustrate our proposed methodology using $T=490$ weekly rates of the General Index of the Athens stock exchange over the period 1986-1996. If $G_{t}$ is the value of the General Index at time $t$, then we model the weekly rate $y_{t}=\\ln(G_{t}/G_{t-1}),t=1,\\ldots,T$.\nFor the parameters of the GARCH model, we used $\\pi(\\alpha_{0})\\:=\\:\\alpha_{0}^{-1},\\bar{\\pi}(\\sigma_{0}^{2})\\:=\\:\\sigma_{0}^{-2}$, and $U(0,1)$ priors for $\\alpha_{1}$ and $\\beta_{1}$. Under the Student-$t$ distribution, the degree of freedom $n$ $(n>2)$ is a parameter to be estimated, and we used as a prior the noninformative $\\pi(n)=(n-2)^{-1}$.\nStationarity conditions impose that $\\alpha_{1}+\\beta_{1}<1$ for the GARCH(1,1) model; this was taken into account by just rejecting, in the MCMC algorithm, all pairs of $(\\alpha_{1},\\beta_{1})$ that did not obey the preceding restriction.\nFor the parameters of the EGARCH models, we used $U(-1,1)$ priors for $\\beta_{j},j=1,2$, and normal or lognormal noninformative priors for the other parameters of the model taken as $\\mathrm{N}(0,10)$ for $\\alpha_{0},\\theta_{i},\\gamma_{i},i=1,2$, $\\mathrm{LN}(1.04\\cdot10^{22},2.93\\cdot10^{87})$ for $v$, and $\\mathrm{LN}(4.72\\cdot10^{18},6.008\\cdot10^{80})$ for $\\sigma_{0}^{2}$.\nThe output sample of every MCMC run was constructed as follows. First a large sample was taken and an initial (burn-in) part of it was discarded after a visual inspection of the time series plots of each parameter. Then the autocorrelation function of each parameter was investigated, and a decision was made about the lag intervals with which the sample should be collected to achieve a nearly noncorrelated sample.\nThe reversible-jump MCMC algorithm was applied for model selection among eight competing EGARCH models, with posterior probabilities and Bayes factors calculated. The best model for the Athens stock market was the EGARCH(2,1) model with posterior probability .4772.\n\nThe study applies GARCH and EGARCH models to weekly rates of the Athens Stock Exchange General Index from 1986-1996, using noninformative priors and MCMC methods for parameter estimation and model selection."
  },
  {
    "qid": "econ-empirical-1407-1-0-2",
    "question": "3) Prove that the charge $\\mu$ derived from $T$ is a probability charge, i.e., $\\mu(A) \\geqslant 0$ for all $A \\in \\mathcal{P}(N)$ and $\\mu(N) = 1$.",
    "gold_answer": "1. **Non-Negativity**: For any $A \\in \\mathcal{P}(N)$, define $f_A(n) = 1$ if $n \\in A$ and $0$ otherwise. Then:\n   $$T(f_A) = \\mu(A) \\geqslant 0,$$\n   since $T$ is positive (as it extends the Cesaro limit).\n2. **Total Measure**: For $A = N$, $f_N(n) = 1$ for all $n$. Thus:\n   $$T(f_N) = \\mu(N) = \\operatorname*{lim}_{N\\to\\infty}\\frac{1}{N}\\sum_{p=1}^{N}1 = 1.$$\n3. **Conclusion**: $\\mu$ satisfies the properties of a probability charge.",
    "question_context": "Each individual is labeled by an integer. An allocation is then a sequence $x=(x_{n})_{n\\geqslant1}$ , which we take in the space $l_{\\infty}$ of bounded sequences, endowed with the sup-norm $\\left\\Vert x\\right\\Vert=\\operatorname*{sup}_{n\\geqslant1}|x_{n}|$ . Let $T(x)$ be the result of adding the amounts of good held by all the agents in our denumerable economy. In an economy of size $N_{-}$ , it is convenient to attribute the weight $1/N$ to each agent. It allows to increase the size of the population while holding the aggregate endowment constant. In that context, it is natural to impose Assumption 1. $T(x)$ is equal to the Cesaro limit $\\operatorname*{lim}_{N\\to\\infty}1/N\\sum_{p=1}^{N}x_{p}$, when $x$ has a Cesaro limit.\nDuality allows to associate to any such $T$ an additive measure (or charge) $\\mu$ on the integers, which will play the same role as the Lebesgue measure in the case of a continuum of agents. The following proposition summarizes the technical results that lead from Assumption 1 to the charge $\\mu$ . Proposition 1. $A$ continuous linear functional $T$ satisfying Assumption 1 can be defined on $l_{\\infty}$ . $I t$ gives rise to a unique charge $\\mu$ on $(N,{\\mathcal{P}}(N))$ ) (where ${\\mathcal{P}}(N)$ is the power set of $N$ ) through $T(f)=\\int f(n)\\mu(d n)$. The charge $\\mu$ is a probability charge: $\\forall A\\in\\mathcal{P}(N),\\qquad\\mu(A)\\geqslant0, \\mu(N)=1.$\nProof. The existence of $T$ (which is a Banach limit) is a simple consequence of the Hahn-Banach theorem ([5, Theorem 11, p. 63]). The rest of the proposition follows from the identification of the dual of $l_{\\infty}$ , with the space of charges on $(N,\\mathcal{P}(N))$ ([5, Corollary 3, p. 259]). K There are many functionals $T$ satisfying Assumption 1 and correspondingly many charges $\\mu$ . From the point of view of this note, they all provide the same results (see the next section).\n\nThis section follows Feldman and Gilles ([6]), focusing on allocations in a denumerable economy where each individual is labeled by an integer. The allocation is represented as a bounded sequence in the space $l_{\\infty}$ with the sup-norm. The aggregate endowment is defined via a Cesaro limit, and duality is used to associate a charge $\\mu$ on the integers."
  },
  {
    "qid": "econ-empirical-1079-0-0-2",
    "question": "3) How would you test for heterogeneous effects of unemployment on different types of crime (e.g., property vs. violent crime)?",
    "gold_answer": "To test for heterogeneous effects:\\n1. Estimate separate regressions for each crime type:\\n\\[ CrimeType_{it} = \\beta_0 + \\beta_1 Unemployment_{it} + \\beta_2 X_{it} + \\alpha_i + \\gamma_t + \\epsilon_{it} \\]\\n2. Conduct Chow tests to compare coefficients across equations.\\n3. Include interaction terms between unemployment and crime-type indicators in a pooled regression.",
    "question_context": "Does Unemployment Increase Crime? Evidence from U.S. Data 1974-2000\nThe Journal of Human Resources, Spring, 2008, Vol. 43, No. 2 (Spring, 2008), pp. 413-436\nMing-Jen Lin\n\nThis study examines the relationship between unemployment and crime using U.S. data from 1974 to 2000. The author employs econometric techniques to analyze whether higher unemployment rates lead to increased crime rates, considering various control variables and potential endogeneity issues."
  },
  