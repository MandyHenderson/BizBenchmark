{"script_processing_qid": "textbook-117-26-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate earnings per share (EPS) and dividends per share (DPS) for U.S. Composite Corporation in 20X2, given the data provided.\n\nGOLD_ANSWER:\n1.  **EPS**: $$\\frac{\\text{Net Income}}{\\text{Shares Outstanding}} = \\frac{\\$86\\text{ million}}{29\\text{ million}} = \\$2.97\\text{ per share}.$$\n2.  **DPS**: $$\\frac{\\text{Dividends}}{\\text{Shares Outstanding}} = \\frac{\\$43\\text{ million}}{29\\text{ million}} = \\$1.48\\text{ per share}.$$\n\nCANDIDATE_ANSWER:\nStep 1: Review the provided context to identify required inputs for EPS and DPS calculations. EPS = (Net Income - Preferred Dividends) / Weighted Average Shares Outstanding. DPS = Total Dividends / Shares Outstanding.\n\nStep 2: The context explains components like EBIT, depreciation, deferred taxes, and accounting definitions but does not provide numerical data (net income, dividends paid, or shares outstanding) for U.S. Composite Corporation in 20X2.\n\nStep 3: Conclude that the necessary quantitative data (e.g., net income, dividends, shares outstanding) is missing from the provided context.\n\nFinal Answer: The provided context does not contain enough information (e.g., net income, dividends, shares outstanding) to calculate EPS and DPS for U.S. Composite Corporation in 20X2.\n\nQID: textbook-117-26-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-26-1-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it provides specific numerical data that the candidate correctly identifies as absent.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-26-1-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it provides specific numerical data that the candidate correctly identifies as absent."}, "llm_echoed_qid": "textbook-117-26-1-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it provides specific numerical data that the candidate correctly identifies as absent."}
{"script_processing_qid": "textbook-111-9-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the tax multiplier formula $\\frac{\\Delta Y}{\\Delta T}$ from the equilibrium condition $\\Delta Y = \\Delta C + \\Delta I + \\Delta G$, assuming $\\Delta I = \\Delta G = 0$ and $\\Delta C = MPC \\times (\\Delta Y - \\Delta T)$.\n\nGOLD_ANSWER:\n1.  **Start with equilibrium**: $\\Delta Y = \\Delta C + 0 + 0$ \n2.  **Substitute consumption function**: $\\Delta Y = MPC \\times (\\Delta Y - \\Delta T)$\n3.  **Expand**: $\\Delta Y = MPC \\cdot \\Delta Y - MPC \\cdot \\Delta T$\n4.  **Rearrange**: $\\Delta Y - MPC \\cdot \\Delta Y = -MPC \\cdot \\Delta T$\n5.  **Factor**: $(1-MPC)\\Delta Y = -MPC \\cdot \\Delta T$\n6.  **Final multiplier**: $\\frac{\\Delta Y}{\\Delta T} = \\frac{-MPC}{1-MPC}$\n\nCANDIDATE_ANSWER:\nStep 1: Start with the equilibrium condition:\nΔY = ΔC + ΔI + ΔG\n\nStep 2: Apply assumptions ΔI = 0 and ΔG = 0:\nΔY = ΔC\n\nStep 3: Substitute the consumption function ΔC = MPC × (ΔY − ΔT):\nΔY = MPC × (ΔY − ΔT)\n\nStep 4: Expand the right side:\nΔY = MPC·ΔY − MPC·ΔT\n\nStep 5: Collect ΔY terms on the left side:\nΔY − MPC·ΔY = −MPC·ΔT\n\nStep 6: Factor out ΔY:\n(1 − MPC)·ΔY = −MPC·ΔT\n\nStep 7: Solve for ΔY/ΔT:\nΔY/ΔT = −MPC/(1 − MPC)\n\nThis gives the tax multiplier formula. The negative sign indicates that tax increases reduce equilibrium income.\n\nQID: textbook-111-9-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-9-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer perfectly matches the gold answer, including all steps and the final tax multiplier formula. The reasoning is clear and correctly derives the result.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-9-0-0", "category": "CORRECT", "explanation": "The candidate's answer perfectly matches the gold answer, including all steps and the final tax multiplier formula. The reasoning is clear and correctly derives the result."}, "llm_echoed_qid": "textbook-111-9-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer perfectly matches the gold answer, including all steps and the final tax multiplier formula. The reasoning is clear and correctly derives the result."}
{"script_processing_qid": "textbook-67-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the real value of loan installments for a borrower under a constant nominal interest rate, given the rate of devaluation $\\varepsilon$ and the international interest rate $r$. Show how changes in $\\varepsilon$ affect the real value of early installments.\n\nGOLD_ANSWER:\n1. **Loan Installment Formula**: The nominal installment is $iS = (r + \\varepsilon)S$.\n2. **Real Value**: The real value at time $t$ is $\\frac{(r + \\varepsilon)S}{P_t} = \\frac{(r + \\varepsilon)S}{\\exp(\\varepsilon t)}$.\n3. **Early Installments**: For small $t$, $\\exp(\\varepsilon t) \\approx 1 + \\varepsilon t$, so the real value is approximately $(r + \\varepsilon)S / (1 + \\varepsilon t)$. Higher $\\varepsilon$ increases the numerator more than the denominator for early $t$, leading to larger real installments.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the given real installment formula:\n$$\\frac{(r+\\varepsilon)S}{\\exp(\\varepsilon t)}$$\n\nStep 2: To analyze the impact of devaluation rate (ε) on early installments (small t), compute the partial derivative with respect to ε:\n$$\\frac{\\partial}{\\partial \\varepsilon} \\left[ \\frac{(r+\\varepsilon)S}{\\exp(\\varepsilon t)} \\right]$$\n\nStep 3: Apply the quotient rule for differentiation:\n$$= \\frac{(1) \\cdot S \\cdot \\exp(\\varepsilon t) - (r+\\varepsilon)S \\cdot t\\exp(\\varepsilon t)}{\\exp(2\\varepsilon t)}$$\n\nStep 4: Simplify the expression:\n$$= \\frac{S\\exp(\\varepsilon t)[1 - t(r+\\varepsilon)]}{\\exp(2\\varepsilon t)} = \\frac{S[1 - t(r+\\varepsilon)]}{\\exp(\\varepsilon t)}$$\n\nStep 5: For early installments (t ≈ 0):\n- The term $\\exp(\\varepsilon t) \\approx 1$\n- The term $t(r+\\varepsilon) \\approx 0$\nThus:\n$$\\frac{\\partial V}{\\partial \\varepsilon} \\approx S > 0$$\n\nConclusion: An increase in ε raises the real value of early installments because the nominal interest rate adjustment (r+ε) dominates over the minimal erosion from inflation (exp(εt)) when t is small.\n\nQID: textbook-67-0-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-67-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the real value of loan installments and accurately analyzes the impact of ε on early installments using partial derivatives and approximations, matching the gold answer's reasoning and conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-67-0-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the real value of loan installments and accurately analyzes the impact of ε on early installments using partial derivatives and approximations, matching the gold answer's reasoning and conclusion."}, "llm_echoed_qid": "textbook-67-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the real value of loan installments and accurately analyzes the impact of ε on early installments using partial derivatives and approximations, matching the gold answer's reasoning and conclusion."}
{"script_processing_qid": "textbook-117-44-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain how the slope of the intertemporal budget constraint line $A B$ is derived and interpret its economic meaning.\n\nGOLD_ANSWER:\nThe slope of the intertemporal budget constraint line $A B$ is derived as $-(1 + r)$. \n- **Derivation**: Moving from point $A$ to point $B$ involves sacrificing $(1 + r)$ dollars of future consumption for each additional dollar of current consumption.\n- **Economic Meaning**: The slope represents the trade-off between current and future consumption, reflecting the opportunity cost of consuming today versus saving (or lending) at the interest rate $r$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the trade-off between consumption in Year 1 (C₁) and Year 2 (C₂). The slope reflects the market exchange rate between present and future consumption via borrowing/lending.\n\nStep 2: Compute the opportunity cost of consuming $1 today. If $1 is saved/lent at interest rate r, it grows to $(1 + r) in Year 2. Thus, sacrificing $1 in C₁ yields $(1 + r) in C₂.\n\nStep 3: Conversely, borrowing $1 today requires repaying $(1 + r) next year. This implies $1 added to C₁ reduces C₂ by $(1 + r), making the slope = -ΔC₂/ΔC₁ = -(1 + r).\n\nEconomic Meaning: The slope -(1 + r) represents the rate at which the financial market allows individuals to trade current consumption for future consumption. A steeper slope (higher r) incentivizes saving, while a flatter slope (lower r) incentivizes borrowing.\n\nQID: textbook-117-44-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-44-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the slope of the intertemporal budget constraint line as -(1 + r) and accurately interprets its economic meaning, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-44-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the slope of the intertemporal budget constraint line as -(1 + r) and accurately interprets its economic meaning, aligning with the gold answer."}, "llm_echoed_qid": "textbook-117-44-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the slope of the intertemporal budget constraint line as -(1 + r) and accurately interprets its economic meaning, aligning with the gold answer."}
{"script_processing_qid": "textbook-86-1-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the implications of the observed divergence in per capita incomes between the richest and poorest countries for theories of economic growth. How does this contrast with Kaldor's stylized facts?\n\nGOLD_ANSWER:\nThe divergence suggests that factors beyond capital accumulation (e.g., technology, institutions) drive growth. Kaldor's facts emphasized stable factor shares and growth rates within a single economy, whereas cross-country analysis reveals heterogeneity and path dependence.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the key empirical observation - The context shows persistent divergence in per capita incomes between rich and poor countries (2.25% global average growth but widening inequality between extremes). This contradicts the convergence hypothesis implicit in traditional neoclassical growth models.\n\nStep 2: Contrast with Kaldor's stylized facts - Kaldor's facts emphasize stability in growth patterns across advanced economies (e.g., constant capital-output ratios, stable income shares). The observed cross-country divergence instead shows instability in relative income positions and disproportional population-weighted inequality (26% of world population in the poorest decile).\n\nStep 3: Theoretical implications - The divergence challenges growth theories assuming conditional convergence through technology diffusion. It suggests persistent institutional/structural barriers (e.g., education gaps, governance issues) limit catch-up growth, requiring revised growth models that account for multiple equilibria or path dependence.\n\nStep 4: Contrast with Kaldorian focus - While Kaldor's facts guided production function-based explanations of aggregate growth, modern growth empirics (as described) shift focus to explaining distributional dynamics and persistent growth rate differences across nations.\n\nQID: textbook-86-1-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-86-1-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies the divergence in per capita incomes, contrasts it with Kaldor's stylized facts, and discusses the theoretical implications, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-86-1-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately identifies the divergence in per capita incomes, contrasts it with Kaldor's stylized facts, and discusses the theoretical implications, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-86-1-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies the divergence in per capita incomes, contrasts it with Kaldor's stylized facts, and discusses the theoretical implications, aligning well with the gold answer."}
{"script_processing_qid": "textbook-111-19-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Using the money demand function $(M/P)^d = 1000 - 100r$ and a fixed $M/P = 500$, solve for the equilibrium interest rate. Show the steps.\n\nGOLD_ANSWER:\n1. **Set Supply = Demand**: $500 = 1000 - 100r$.  \n2. **Solve for $r$**: $100r = 500 \\implies r = 5$%.  \n3. **Equilibrium**: The interest rate adjusts to 5% to clear the money market.\n\nCANDIDATE_ANSWER:\nStep 1: Set money demand equal to money supply at equilibrium.\\n$(M/P)^d = M/P$\\nStep 2: Substitute given values into equation:\\n$1000 - 100r = 500$\\nStep 3: Solve for $r$:\\n$1000 - 500 = 100r$\\n$500 = 100r$\\n$r = 500/100$\\nStep 4: Final solution:\\n$r = 5$\\nThe equilibrium interest rate is 5%.\n\nQID: textbook-111-19-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-19-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to solve for the equilibrium interest rate, matching the gold answer in both process and final result.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-19-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to solve for the equilibrium interest rate, matching the gold answer in both process and final result."}, "llm_echoed_qid": "textbook-111-19-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to solve for the equilibrium interest rate, matching the gold answer in both process and final result."}
{"script_processing_qid": "textbook-125-11-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Calculate the adjusted cost of capital ($r^*$) for the perpetual crusher project, given that the minimum acceptable IRR is 10.84%. Explain its significance in project evaluation.\n\nGOLD_ANSWER:\n1. **Calculation**:\n   - APV = 0 when $$\\frac{\\mathrm{annualincome}}{0.12} - 12.5 + 1.21 = 0$$.\n   - Solving for annual income: $$\\mathrm{annualincome} = (12.5 - 1.21) \\times 0.12 = \\$1.355\\mathrm{million}$$.\n   - IRR = $$\\frac{1.355}{12.5} = 10.84\\%$$.\n2. **Significance**:\n   - $r^*$ reflects both the project’s business risk and its contribution to debt capacity.\n   - Projects with IRR > $r^*$ have positive APV and should be accepted.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the perpetual cash flow formula for IRR.\nFor perpetual projects, IRR = Annual Cash Flow / Initial Investment.\n\nStep 2: Apply values from context.\nAnnual after-tax cash flow = $1.355 million\nInitial investment = $12.5 million\n\nStep 3: Calculate r* (minimum acceptable IRR).\nr* = 1.355 / 12.5 = 0.1084 = 10.84%.\n\nSignificance:\n- r* represents the break-even return required for APV = 0 when financing effects (tax shields) are included.\n- Projects with IRR > 10.84% create value under this financing structure.\n- Integrates both operational performance (base-case NPV) and capital structure impacts into a single hurdle rate.\n\nQID: textbook-125-11-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-11-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly calculated the adjusted cost of capital ($r^*$) as 10.84% and accurately explained its significance in project evaluation, matching the gold answer in both calculation and interpretation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-11-0-3", "category": "CORRECT", "explanation": "The candidate correctly calculated the adjusted cost of capital ($r^*$) as 10.84% and accurately explained its significance in project evaluation, matching the gold answer in both calculation and interpretation."}, "llm_echoed_qid": "textbook-125-11-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly calculated the adjusted cost of capital ($r^*$) as 10.84% and accurately explained its significance in project evaluation, matching the gold answer in both calculation and interpretation."}
{"script_processing_qid": "textbook-54-2-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the implications of the white noise assumption $\\epsilon_{t}\\sim\\mathrm{i.i.d.}N(0,\\sigma_{\\epsilon}^{2})$ for the properties of the ARMA models.\n\nGOLD_ANSWER:\n1. **No serial correlation**: $E(\\epsilon_{t}\\epsilon_{t-j})=0$ for $j\\neq 0$ ensures no predictability in the errors.\n2. **Constant variance**: $\\text{var}(\\epsilon_{t})=\\sigma_{\\epsilon}^{2}$ implies homoskedasticity.\n3. **Normality**: The distribution of $\\epsilon_{t}$ is normal, which simplifies maximum likelihood estimation and inference.\n\nCANDIDATE_ANSWER:\nStep 1: The i.i.d. assumption implies independence across time points. This ensures that all serial dependence in the ARMA process is captured explicitly by the model's autoregressive (AR) and moving average (MA) terms, rather than hidden correlations in the error term.\n\nStep 2: Normality (N(0,σ²)) implies that the process inherits Gaussian properties. This allows closed-form solutions for prediction intervals and likelihood calculations, and justifies standard inference methods (e.g., using t-statistics for parameter significance).\n\nStep 3: Constant variance (σ²) ensures homoscedasticity – shocks have stable magnitude over time. This preserves the validity of ARMA's linear structure. Non-constant variance would require extensions like GARCH.\n\nStep 4: Zero mean (E[εₜ]=0) ensures the ARMA model explicitly represents any non-zero mean through deterministic components (e.g., intercept terms), keeping the white noise purely stochastic.\n\nStep 5: The i.i.d. normal assumption guarantees that the model residuals (if correctly specified) will satisfy classical regression assumptions, enabling diagnostic checks via residual autocorrelation tests and Q-Q plots.\n\nQID: textbook-54-2-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-54-2-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the implications of the white noise assumption for ARMA models, covering independence, normality, constant variance, zero mean, and diagnostic checks, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-54-2-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the implications of the white noise assumption for ARMA models, covering independence, normality, constant variance, zero mean, and diagnostic checks, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-54-2-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the implications of the white noise assumption for ARMA models, covering independence, normality, constant variance, zero mean, and diagnostic checks, aligning well with the gold answer."}
{"script_processing_qid": "textbook-62-5-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the growth rate $g_k(t)$ for $k(t) = \\frac{K(t)}{L(t)}$ using logarithmic differentiation.\n\nGOLD_ANSWER:\n1. **Logarithm**: $\\log(k(t)) = \\log(K(t)) - \\log(L(t))$.\n2. **Differentiate**: $\\frac{d}{dt} \\log(k(t)) = \\frac{d}{dt} \\log(K(t)) - \\frac{d}{dt} \\log(L(t))$.\n3. **Growth Rates**: $g_k(t) = g_K(t) - g_L(t)$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the definition of k(t):\nk(t) = K(t)/L(t)\n\nStep 2: Take natural logarithms of both sides:\nln k(t) = ln K(t) - ln L(t)\n\nStep 3: Differentiate both sides with respect to time t:\n(d/dt)[ln k(t)] = (d/dt)[ln K(t)] - (d/dt)[ln L(t)]\n\nStep 4: Apply logarithmic differentiation rules:\n(k'(t)/k(t)) = (K'(t)/K(t)) - (L'(t)/L(t))\n\nStep 5: Recognize that growth rates are defined as:\nk'(t)/k(t) = g_k(t)\nK'(t)/K(t) = g_K(t)\nL'(t)/L(t) = g_L(t)\n\nStep 6: Substitute these definitions into the equation:\ng_k(t) = g_K(t) - g_L(t)\n\nThis shows the growth rate of k(t) equals the difference between the growth rates of K(t) and L(t).\n\nQID: textbook-62-5-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-62-5-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer follows the same logical steps as the gold answer, correctly applying logarithmic differentiation and identifying the growth rates. The reasoning and final result match perfectly.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-62-5-1-0", "category": "CORRECT", "explanation": "The candidate's answer follows the same logical steps as the gold answer, correctly applying logarithmic differentiation and identifying the growth rates. The reasoning and final result match perfectly."}, "llm_echoed_qid": "textbook-62-5-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer follows the same logical steps as the gold answer, correctly applying logarithmic differentiation and identifying the growth rates. The reasoning and final result match perfectly."}
{"script_processing_qid": "textbook-26-0-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Using Table 7, test the hypothesis of a unit root in the log consumption-output ratio and interpret the results.\n\nGOLD_ANSWER:\n1. **Unit Root Test**: The largest autoregressive root for the log $C/Y$ ratio is 0.38 (OLS) and 0.70 (median unbiased), with a 90% confidence interval of 0.43-1.05.\n2. **Interpretation**: The confidence interval includes 1, so the unit root hypothesis cannot be rejected at the 5% level. However, the point estimates suggest the root is less than 1, indicating some mean reversion.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the relevant test results in Table 7 (context states: 'the hypothesis of a unit autoregressive root is not rejected... for output, consumption or investment' and specifies 'a unit root cannot be rejected for the consumption-output ratio').\n\nStep 2: Note the significance level: The test uses a 5% significance level (as stated in the context).\n\nStep 3: Interpretation: Failure to reject the unit root hypothesis for the log consumption-output ratio implies insufficient statistical evidence to conclude stationarity. This contrasts with the theoretical prediction (from stochastic growth models) that balanced growth ratios should be stationary.\n\nStep 4: Additional nuance: The context notes that while a unit root isn't rejected, the estimated largest roots for balanced growth ratios are 'small,' suggesting weak persistence despite the formal statistical result.\n\nQID: textbook-26-0-2-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-26-0-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the test results, significance level, and interpretation of the unit root hypothesis for the log consumption-output ratio. It also captures the nuance about weak persistence despite the formal statistical result, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-26-0-2-1", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the test results, significance level, and interpretation of the unit root hypothesis for the log consumption-output ratio. It also captures the nuance about weak persistence despite the formal statistical result, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-26-0-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the test results, significance level, and interpretation of the unit root hypothesis for the log consumption-output ratio. It also captures the nuance about weak persistence despite the formal statistical result, aligning well with the gold answer."}
{"script_processing_qid": "textbook-125-46-3-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of Patell and Wolfson's findings for the speed of price adjustment in efficient markets.\n\nGOLD_ANSWER:\n1. **Implications**: Prices adjust to new information within minutes, supporting the semistrong form of efficiency.\n2. **Market Efficiency**: The rapid adjustment suggests that investors quickly incorporate public information into prices, leaving no room for profitable trading based on delayed reactions.\n\nCANDIDATE_ANSWER:\nStep 1: Patell and Wolfson's findings demonstrate that stock prices adjust to new earnings or dividend announcements within 5–10 minutes, indicating extremely rapid price incorporation of public information.\n\nStep 2: This rapid adjustment supports the semistrong form of the efficient-market hypothesis, which posits that prices reflect all publicly available information almost instantaneously.\n\nStep 3: The speed implies that investors cannot systematically profit from trading on such announcements after they are made, as the market efficiently eliminates predictable arbitrage opportunities within minutes.\n\nStep 4: The results align with earlier evidence (e.g., takeover target price jumps in Figure 13.5 occurring within a single day), reinforcing the broader conclusion that financial markets exhibit high informational efficiency in processing news.\n\nQID: textbook-125-46-3-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-46-3-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately summarizes Patell and Wolfson's findings, emphasizing the rapid price adjustment within minutes, support for the semistrong form of market efficiency, and the elimination of arbitrage opportunities. This aligns perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-46-3-3", "category": "CORRECT", "explanation": "The candidate answer accurately summarizes Patell and Wolfson's findings, emphasizing the rapid price adjustment within minutes, support for the semistrong form of market efficiency, and the elimination of arbitrage opportunities. This aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-125-46-3-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately summarizes Patell and Wolfson's findings, emphasizing the rapid price adjustment within minutes, support for the semistrong form of market efficiency, and the elimination of arbitrage opportunities. This aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-111-18-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Critically evaluate the statement: 'Economic forecasting is inherently unreliable, rendering active stabilization policy ineffective.' Use examples from the Great Depression and the 1982 recession.\n\nGOLD_ANSWER:\n1. **Great Depression**: Forecasters failed to predict severity (e.g., Irving Fisher's optimism). Policy responses were delayed, exacerbating the downturn.\n2. **1982 Recession**: Forecasters underestimated unemployment spikes (Figure 14-1).\n3. **Implications**: Forecast errors increase policy risks, but passive policies may also fail (e.g., no response to severe shocks). Balance is needed, with caution in discretionary actions.\n\nCANDIDATE_ANSWER:\nStep 1: Examine the debate framework. The statement assumes forecasting unreliability undermines active stabilization policy. Proponents like Martin argue active policies counter inherent instability, while critics like Friedman blame policy errors for exacerbating fluctuations.\n\nStep 2: Address forecasting limitations. Economic shocks (e.g., Great Depression demand collapse, 1982 supply-side inflation) are unpredictable, creating inside/outside lags. For example, delayed recognition of the Great Depression’s severity (inside lag) worsened outcomes, while the 1982 Fed’s tight monetary policy (to curb inflation) had a delayed recessionary impact (outside lag).\n\nStep 3: Evaluate effectiveness using examples. During the Great Depression, initial passive policies (e.g., Fed’s failure to expand money supply) deepened the crisis, suggesting active intervention was needed despite forecasting challenges. Conversely, the 1982 recession saw the Fed actively reverse tight monetary policy once inflation eased, aiding recovery—illustrating effective stabilization despite imperfect forecasting.\n\nStep 4: Contrast with automatic stabilizers. Unlike discretionary policies, automatic stabilizers (e.g., progressive taxation) avoid lags, supporting Friedman’s skepticism of ‘fine-tuning.’ However, the Great Depression’s scale required active fiscal/monetary responses beyond automatic mechanisms.\n\nStep 5: Synthesize. The statement oversimplifies: While forecasting challenges and lags limit active policies (e.g., premature tightening in 1937 prolonging Depression), examples show active policies can mitigate crises when calibrated to visible indicators (e.g., unemployment, inflation) rather than relying solely on forecasts. Effectiveness depends on balancing responsiveness with avoiding overcorrection, as seen in the 1982 recession’s managed soft landing.\n\nQID: textbook-111-18-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-111-18-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly evaluates the statement by addressing forecasting limitations, using examples from the Great Depression and the 1982 recession, and synthesizing the argument effectively. It aligns well with the gold answer, demonstrating a nuanced understanding of the topic.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-18-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly evaluates the statement by addressing forecasting limitations, using examples from the Great Depression and the 1982 recession, and synthesizing the argument effectively. It aligns well with the gold answer, demonstrating a nuanced understanding of the topic."}, "llm_echoed_qid": "textbook-111-18-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly evaluates the statement by addressing forecasting limitations, using examples from the Great Depression and the 1982 recession, and synthesizing the argument effectively. It aligns well with the gold answer, demonstrating a nuanced understanding of the topic."}
{"script_processing_qid": "textbook-108-20-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the midpoint method, calculate the price elasticity of demand for the Greenway toll road when the price was lowered from $1.75 to $1, leading to a 200% increase in volume.\n\nGOLD_ANSWER:\n1. **Percentage change in price**:\n   $$\\% \\Delta P = \\frac{1 - 1.75}{(1 + 1.75)/2} \\times 100 = \\frac{-0.75}{1.375} \\times 100 = -54.55\\%$$\n2. **Percentage change in quantity**: $\\% \\Delta Q = 200\\%$.\n3. **Price elasticity of demand ($E_d$)**:\n   $$E_d = \\frac{\\% \\Delta Q}{\\% \\Delta P} = \\frac{200}{-54.55} = -3.67$$\n   (Elastic demand since $|E_d| > 1$).\n\nCANDIDATE_ANSWER:\nStep 1: Identify price changes\nOld price (P1) = $1.75\nNew price (P2) = $1\nPrice change = $1 - $1.75 = -$0.75\n\nStep 2: Calculate midpoint percentage price change\nAverage price = (1.75 + 1)/2 = $1.375\n%ΔPrice = (-0.75)/1.375 = -54.55%\n\nStep 3: Interpret quantity change\n200% increase in volume = 3x original quantity (Q2 = 3Q1)\nQuantity change = 3Q1 - Q1 = 2Q1\n\nStep 4: Calculate midpoint percentage quantity change\nAverage quantity = (Q1 + 3Q1)/2 = 2Q1\n%ΔQuantity = (2Q1)/2Q1 = 100%\n\nStep 5: Compute price elasticity of demand\nElasticity = (%ΔQuantity)/(%ΔPrice) = 100%/-54.55% ≈ -1.83\n\nConclusion: The price elasticity of demand is approximately -1.83 (elastic since |E| > 1).\n\nQID: textbook-108-20-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-108-20-1-0\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate incorrectly calculated the percentage change in quantity as 100% instead of 200%, leading to an incorrect price elasticity of demand of -1.83 instead of the correct -3.67.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-20-1-0", "category": "INCORRECT", "explanation": "The candidate incorrectly calculated the percentage change in quantity as 100% instead of 200%, leading to an incorrect price elasticity of demand of -1.83 instead of the correct -3.67."}, "llm_echoed_qid": "textbook-108-20-1-0", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate incorrectly calculated the percentage change in quantity as 100% instead of 200%, leading to an incorrect price elasticity of demand of -1.83 instead of the correct -3.67."}
{"script_processing_qid": "textbook-121-12-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze the dynamic stability of the solution to the difference equation $y_{t+1} + 0.5 y_{t} = 3$. Determine whether the time path is oscillatory and/or convergent.\n\nGOLD_ANSWER:\n1. **Complementary Function**: For $y_{t+1} + 0.5 y_{t} = 0$, $b = -0.5$. Thus, $y_{c} = A (-0.5)^{t}$.\n2. **Particular Integral**: Assume $y_{t} = k$. Substituting gives $k + 0.5 k = 3 \\Rightarrow k = 2$. Thus, $y_{p} = 2$.\n3. **General Solution**: $y_{t} = A (-0.5)^{t} + 2$.\n4. **Stability Analysis**: Since $b = -0.5$, the time path is oscillatory ($b < 0$) and convergent ($|b| = 0.5 < 1$). The solution converges to the equilibrium $y_{p} = 2$ with oscillations that diminish over time.\n\nCANDIDATE_ANSWER:\nStep 1: Rewrite the given equation \\( y_{t+1} + 0.5y_t = 3 \\) in standard form \\( y_{t+1} + a y_t = c \\). Here, \\( a = 0.5 \\), \\( c = 3 \\).\nStep 2: Find the complementary function \\( y_c \\). Solve \\( b + a = 0 \\) ⇒ \\( b = -a = -0.5 \\). Thus, \\( y_c = A(-0.5)^t \\).\nStep 3: Determine oscillation: Since \\( b = -0.5 < 0 \\), the term \\( (-0.5)^t \\) introduces alternating signs. The time path is oscillatory.\nStep 4: Analyze convergence: Check \\( |b| = 0.5 \\). Since \\( |b| < 1 \\), \\( y_c \\) converges to 0 as \\( t \to \\infty \\).\nStep 5: Verify stability: Convergence of \\( y_c \\) ensures dynamic stability. The particular integral \\( y_p = \\frac{c}{1+a} = \\frac{3}{1.5} = 2 \\) represents the stable equilibrium.\nConclusion: The time path is oscillatory (due to negative \\( b \\)) and convergent (due to \\( |b| < 1 \\)).\n\nQID: textbook-121-12-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-121-12-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the complementary function, particular integral, and analyzes both oscillation and convergence accurately. The reasoning and conclusions match the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-121-12-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the complementary function, particular integral, and analyzes both oscillation and convergence accurately. The reasoning and conclusions match the gold answer."}, "llm_echoed_qid": "textbook-121-12-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the complementary function, particular integral, and analyzes both oscillation and convergence accurately. The reasoning and conclusions match the gold answer."}
{"script_processing_qid": "textbook-101-2-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain the significance of the von Bortkiewicz example in the context of Poisson approximation.\n\nGOLD_ANSWER:\n1. **Example**: Deaths by horse-kick in Prussian cavalry regiments.\n2. **Model**: Binomial distribution $B_{(600,0.61/600)}$ approximated by Poisson $P_{0.61}$.\n3. **Significance**: Demonstrates practical applicability of Poisson approximation for rare events.\n\nCANDIDATE_ANSWER:\nStep 1: The von Bortkiewicz example models rare events (deaths by horse-kick) with a binomial distribution B(600, s/600), where s = 0.61. Here, n = 600 (soldiers) is large, and p = s/600 ≈ 0.001 (per-soldier annual death probability) is small.\n\nStep 2: Theorem 1.3.1(i) provides an upper bound for the variational distance between this binomial distribution and the Poisson distribution P_s (with λ = s = 0.61). The bound is calculated as s/600 = 0.0010..., indicating extremely small approximation error.\n\nStep 3: This small bound justifies replacing the binomial model with the simpler Poisson distribution for practical analysis, as the error introduced is negligible.\n\nStep 4: The example is historically significant because it empirically validated the Poisson approximation for rare events in real-world data, demonstrating its utility in scenarios where n is large and p is small (typical in actuarial science, epidemiology, and operations management). The tight bound here mathematically reinforces why Poisson approximations became a cornerstone for modeling low-probability, high-sample-size phenomena.\n\nQID: textbook-101-2-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-101-2-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the von Bortkiewicz example, detailing the binomial distribution, the Poisson approximation, and the significance of the small approximation error. It also correctly highlights the historical and practical importance of the example in validating the Poisson approximation for rare events.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-101-2-1-2", "category": "CORRECT", "explanation": "The candidate answer accurately explains the von Bortkiewicz example, detailing the binomial distribution, the Poisson approximation, and the significance of the small approximation error. It also correctly highlights the historical and practical importance of the example in validating the Poisson approximation for rare events."}, "llm_echoed_qid": "textbook-101-2-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the von Bortkiewicz example, detailing the binomial distribution, the Poisson approximation, and the significance of the small approximation error. It also correctly highlights the historical and practical importance of the example in validating the Poisson approximation for rare events."}
{"script_processing_qid": "textbook-99-8-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the expression for J_max in terms of n and T_max for the cosine basis on [0,n].\n\nGOLD_ANSWER:\n2. **Derivation of J_max**: For the cosine basis, the maximum index J_max is given by $$J_{\\text{max}} = \\lfloor2n/T_{\\text{max}}\\rfloor.$$ This ensures that the basis functions with indices up to J_max do not exhibit periodicity within T_max, thus separating the trend from the seasonal components.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that for a cosine basis on [0,n], the period of the j-th basis function is given by T_j = 2n/j.\n\nStep 2: To separate trend components (periods ≥ T_max), we require T_j ≥ T_max.\n\nStep 3: Substitute T_j with 2n/j in the inequality:\n   2n/j ≥ T_max\n\nStep 4: Solve for j:\n   j ≤ 2n/T_max\n\nStep 5: Since j must be an integer index, take the floor function to get the maximum integer value:\n   J_max = ⎣2n/T_max⎦\n\nThis matches the given expression J_max = ⌊2n/T_max⌋.\n\nQID: textbook-99-8-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-8-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's step-by-step derivation correctly arrives at the expression for J_max as given in the gold answer, including the use of the floor function to ensure integer indices.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-8-1-1", "category": "CORRECT", "explanation": "The candidate's step-by-step derivation correctly arrives at the expression for J_max as given in the gold answer, including the use of the floor function to ensure integer indices."}, "llm_echoed_qid": "textbook-99-8-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's step-by-step derivation correctly arrives at the expression for J_max as given in the gold answer, including the use of the floor function to ensure integer indices."}
{"script_processing_qid": "textbook-125-45-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate the value of a loan guarantee for a firm with assets worth $100 million, debt of $60 million due in 5 years, and a 20% probability of default. Assume a risk-free rate of 5%.\n\nGOLD_ANSWER:\n1. **Expected Loss**: 20% probability of default implies expected loss = 0.20 * $60 million = $12 million.\n2. **Present Value**: Discount expected loss at risk-free rate: $12 million / (1.05)^5 ≈ $9.39 million.\n3. **Guarantee Value**: The guarantee is worth approximately $9.39 million to lenders.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the loan guarantee value as the present value of expected loss. The guarantee covers $60M debt with 20% default probability.\\nStep 2: Calculate expected loss: $60M debt × 20% default probability = $12M\\nStep 3: Discount this expected loss at the risk-free rate for 5 years: $12M ÷ (1 + 0.05)^5 ≈ $12M ÷ 1.27628 ≈ $9.40M\\nConclusion: The loan guarantee value is approximately $9.4 million\n\nQID: textbook-125-45-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-45-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to calculate the loan guarantee value, including the expected loss and its present value, matching the gold answer closely.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-45-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to calculate the loan guarantee value, including the expected loss and its present value, matching the gold answer closely."}, "llm_echoed_qid": "textbook-125-45-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to calculate the loan guarantee value, including the expected loss and its present value, matching the gold answer closely."}
{"script_processing_qid": "textbook-125-55-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the inventory turnover ratio from the given 'days in inventory' of 63.6 days for Executive Paper. Show all steps.\n\nGOLD_ANSWER:\n1. **Given**: Days in inventory = $\\frac{\\text{Average inventory}}{\\text{Cost of goods sold}/365} = 63.6$.\n2. Let $I = \\text{Average inventory}$, $COGS = \\text{Cost of goods sold}$.\n3. Then, $\\frac{I}{COGS/365} = 63.6 \\Rightarrow I = 63.6 \\times \\frac{COGS}{365}$.\n4. Inventory turnover ratio = $\\frac{COGS}{I} = \\frac{COGS}{63.6 \\times COGS/365} = \\frac{365}{63.6} \\approx 5.7$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the formula for Days in Inventory\nDays in Inventory = (Average Inventory) / (Cost of Goods Sold / 365)\n\nStep 2: Recognize that Inventory Turnover Ratio is calculated as\nInventory Turnover Ratio = Cost of Goods Sold / Average Inventory\n\nStep 3: Observe that Days in Inventory (63.6 days) and Inventory Turnover Ratio are reciprocals scaled by 365\nDays in Inventory = 365 / Inventory Turnover Ratio\n\nStep 4: Rearrange formula to solve for Inventory Turnover Ratio\nInventory Turnover Ratio = 365 / Days in Inventory\n\nStep 5: Substitute given value\nInventory Turnover Ratio = 365 / 63.6 ≈ 5.74\n\nFinal Answer: The inventory turnover ratio is approximately 5.74 times per year.\n\nQID: textbook-125-55-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-55-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derived the inventory turnover ratio from the given days in inventory, following the appropriate steps and arriving at the correct answer of approximately 5.74.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-55-1-0", "category": "CORRECT", "explanation": "The candidate correctly derived the inventory turnover ratio from the given days in inventory, following the appropriate steps and arriving at the correct answer of approximately 5.74."}, "llm_echoed_qid": "textbook-125-55-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derived the inventory turnover ratio from the given days in inventory, following the appropriate steps and arriving at the correct answer of approximately 5.74."}
{"script_processing_qid": "textbook-105-9-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare and contrast the minimax lower bound (Theorem 3.2) and the individual lower bound (Theorem 3.3). What are the implications of these results for nonparametric regression?\n\nGOLD_ANSWER:\n1. **Minimax Lower Bound**: Provides a worst-case rate of convergence over the entire class $\\mathcal{D}^{(p,C)}$. It ensures that no estimator can achieve a faster rate uniformly over the class.\n2. **Individual Lower Bound**: Focuses on the behavior of the error for a fixed distribution as $n$ grows. It shows that even for individual distributions, the rate cannot be arbitrarily fast.\n3. **Implications**: The results highlight the inherent difficulty of nonparametric regression, where the convergence rate depends on the smoothness of the function and the dimensionality of the data. They also justify the use of adaptive estimators that can achieve the optimal rate for specific distributions.\n\nCANDIDATE_ANSWER:\nStep 1: Compare Definitions\n- Minimax lower bound (Theorem 3.2) establishes the minimum achievable worst-case error rate over the entire class 𝒟^(p,C), where no estimator can perform better than n^(-2p/(2p+d)) uniformly across all distributions in the class.\n\nStep 2: Contrast Structure\n- Individual lower bound (Theorem 3.3) shows that even for any decaying sequence b_n → 0, no estimator can achieve b_n·n^(-2p/(2p+d)) for every distribution in 𝒟^(p,C). Unlike minimax, it demonstrates that some distributions in the class will force arbitrarily slow convergence rates for any estimator.\n\nStep 3: Key Differences\nMinimax:\n1. Uniform lower bound across entire class\n2. Rate n^(-2p/(2p+d)) is fundamental limit for worst-case performance\n\nIndividual:\n1. Shows impossibility of achieving slightly faster rates (even scaled by b_n→0)\n2. Emphasizes existence of \"pathologically hard\" distributions that defeat all estimators\n\nStep 4: Implications for Nonparametric Regression\n1. Both theorems confirm n^(-2p/(2p+d)) as unavoidable fundamental rate under smoothness (p) and dimensionality (d)\n2. Minimax result justifies optimality of estimators achieving n^(-2p/(2p+d))\n3. Individual result prevents \"super-rate\" claims - no estimator can universally beat n^(-2p/(2p+d)) even modulo decaying factors\n4. Together, they validate the necessity of smoothness assumptions (Definition 3.3) for meaningful convergence rates in nonparametric settings.\n\nQID: textbook-105-9-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-105-9-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately compares and contrasts the minimax lower bound and the individual lower bound, detailing their definitions, structures, and key differences. It also correctly discusses the implications of these results for nonparametric regression, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-105-9-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately compares and contrasts the minimax lower bound and the individual lower bound, detailing their definitions, structures, and key differences. It also correctly discusses the implications of these results for nonparametric regression, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-105-9-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately compares and contrasts the minimax lower bound and the individual lower bound, detailing their definitions, structures, and key differences. It also correctly discusses the implications of these results for nonparametric regression, aligning well with the gold answer."}
{"script_processing_qid": "textbook-125-46-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the condition under which a financing decision generates a positive NPV for the firm. Use the example of the $\\$43,012$ subsidy.\n\nGOLD_ANSWER:\n1. **Positive NPV Condition**: A financing decision generates positive NPV if the cash raised exceeds the present value of the liability created.\n2. **Example**: The government loan at 3% implies a subsidy of $\\$43,012$, which represents the difference between the market interest rate and the subsidized rate. The NPV for the firm is positive because the subsidy reduces the effective cost of borrowing.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that a financing decision generates positive NPV when the cash raised exceeds the present value of the liability created.<br>Step 2: In the $43,012 subsidy example, the government lends at 3% (below market rate), creating a subsidy equal to the difference between the loan's face value and its PV at market rates.<br>Step 3: The NPV condition is: Cash received > PV of future repayment obligations.<br>Step 4: Formally, positive NPV occurs when:<br>Financing proceeds − PV(Repayment flows at market rate) > 0<br>Step 5: The $43,012 subsidy represents this exact difference - the excess of immediate cash received over the fair-value liability, making it a positive NPV financing decision.\n\nQID: textbook-125-46-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-46-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the condition for a positive NPV in a financing decision and accurately applies it to the example of the $43,012 subsidy. The explanation aligns with the gold answer, detailing how the subsidy reduces the effective cost of borrowing and results in a positive NPV.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-46-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the condition for a positive NPV in a financing decision and accurately applies it to the example of the $43,012 subsidy. The explanation aligns with the gold answer, detailing how the subsidy reduces the effective cost of borrowing and results in a positive NPV."}, "llm_echoed_qid": "textbook-125-46-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the condition for a positive NPV in a financing decision and accurately applies it to the example of the $43,012 subsidy. The explanation aligns with the gold answer, detailing how the subsidy reduces the effective cost of borrowing and results in a positive NPV."}
{"script_processing_qid": "textbook-117-14-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Compare and contrast the purchase method and pooling-of-interests method in terms of their impact on the balance sheet and income statement.\n\nGOLD_ANSWER:\n1.  **Purchase Method**:\n    - Assets of the acquired firm are revalued to fair market value.\n    - Goodwill is created and amortized over time, reducing reported income.\n    - Higher depreciation expenses due to asset write-ups.\n2.  **Pooling-of-Interests Method**:\n    - Assets remain at their book values.\n    - No goodwill is created.\n    - No impact on reported income from goodwill amortization or asset revaluation.\n\nCANDIDATE_ANSWER:\nStep 1: Balance Sheet Impact\n- **Purchase Method**: Acquired assets are revalued to fair market value, creating a new cost basis. Goodwill (excess of purchase price over fair value of assets) is recorded as an intangible asset. The $19 million acquisition cost affects asset/liability accounts.\n- **Pooling-of-Interests**: Assets remain at existing book values from both firms. No goodwill is created, and the $19 million acquisition cost is not reflected on the balance sheet.\n\nStep 2: Income Statement Impact\n- **Purchase Method**: Higher depreciation/amortization expenses may occur due to revalued assets. Subsequent goodwill impairment charges (if applicable) reduce net income.\n- **Pooling-of-Interests**: Lower depreciation/amortization expenses since assets retain original book values. No goodwill-related expenses, resulting in higher reported net income.\n\nStep 3: Key Contrast\n- Purchase method increases asset values and creates goodwill, while pooling preserves historical book values and avoids goodwill. Purchase method typically reduces near-term net income due to revaluation effects, whereas pooling minimizes income statement distortions.\n\nQID: textbook-117-14-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-14-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately compares and contrasts the purchase method and pooling-of-interests method, detailing their impacts on the balance sheet and income statement in alignment with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-14-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately compares and contrasts the purchase method and pooling-of-interests method, detailing their impacts on the balance sheet and income statement in alignment with the gold answer."}, "llm_echoed_qid": "textbook-117-14-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately compares and contrasts the purchase method and pooling-of-interests method, detailing their impacts on the balance sheet and income statement in alignment with the gold answer."}
{"script_processing_qid": "textbook-117-5-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the separation principle and its implications for portfolio choice.\n\nGOLD_ANSWER:\n1. **Separation Principle**: The investment decision is split into two steps:\n   - **Step 1**: Identify the optimal portfolio of risky assets (point A), which is the tangency point between the efficient frontier and the capital market line.\n   - **Step 2**: Combine this portfolio with the risk-free asset based on the investor's risk tolerance.\n2. **Implications**:\n   - All investors, regardless of risk aversion, hold the same mix of risky assets (point A).\n   - Risk tolerance only determines the proportion of wealth invested in the risk-free asset versus the risky portfolio.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the two components of the separation principle. The principle states investment decisions involve (1) selecting the optimal portfolio of risky assets (point A) and (2) mixing this portfolio with the risk-free asset based on the investor’s risk tolerance.\n\nStep 2: Explain the first implication. All investors, regardless of risk preferences, will hold the same portfolio of risky assets (point A). The composition of the optimal risky portfolio is independent of individual risk aversion.\n\nStep 3: Explain the second implication. Risk tolerance only affects the allocation between the risk-free asset and the optimal risky portfolio. More risk-averse investors allocate more to the risk-free asset (lower \\( X_{\\text{Merville}} \\)), while less risk-averse investors allocate more to the risky portfolio (higher \\( X_{\\text{Merville}} \\)).\n\nStep 4: Highlight the efficiency gain. The separation principle simplifies portfolio choice by decoupling the optimization of risky assets from individual risk preferences. Investors only need to adjust their risk exposure via the risk-free asset proportion, while the risky portfolio remains standardized across all investors.\n\nQID: textbook-117-5-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-5-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the separation principle and its implications, matching the gold answer in both content and structure. The explanation of the two steps and the implications for portfolio choice are correctly detailed and align with the provided gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-5-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the separation principle and its implications, matching the gold answer in both content and structure. The explanation of the two steps and the implications for portfolio choice are correctly detailed and align with the provided gold answer."}, "llm_echoed_qid": "textbook-117-5-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the separation principle and its implications, matching the gold answer in both content and structure. The explanation of the two steps and the implications for portfolio choice are correctly detailed and align with the provided gold answer."}
{"script_processing_qid": "textbook-108-17-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Why might a temporary tax cut have a smaller effect on aggregate demand than a permanent one?\n\nGOLD_ANSWER:\n1.  **Permanent Income Hypothesis**: Households base spending on long-term income expectations. A temporary tax cut is seen as a one-time windfall, leading to a smaller increase in consumption.\n2.  **Savings Response**: Households may save most of a temporary tax cut to smooth consumption over time.\n3.  **Behavioral Evidence**: Empirical studies show that temporary tax cuts (e.g., the 1992 U.S. tax rebate) had limited effects on spending.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the role of expectations in spending behavior. The context states households will increase spending more substantially if they perceive a tax cut as permanent (viewing it as a lasting boost to financial resources).\nStep 2: Contrast temporary vs. permanent tax cuts. A temporary tax cut is perceived as a short-term income increase, leading households to save a larger portion of the additional income rather than spending it immediately.\nStep 3: Link reduced spending to aggregate demand. Since consumer spending is a component of aggregate demand, lower marginal propensity to consume (MPC) under temporary tax cuts results in a smaller initial shift in aggregate demand.\nStep 4: Consider multiplier effects. The multiplier effect amplifies changes in spending. Smaller initial spending from temporary cuts leads to a proportionally smaller cumulative impact on aggregate demand compared to permanent cuts.\nStep 5: Conclusion. Temporary tax cuts induce weaker spending responses due to rational expectations about income persistence, resulting in a smaller effect on aggregate demand than permanent cuts.\n\nQID: textbook-108-17-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-108-17-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the reasoning behind why temporary tax cuts have a smaller effect on aggregate demand than permanent ones, aligning perfectly with the gold answer's points on expectations, savings behavior, and empirical evidence.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-17-2-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the reasoning behind why temporary tax cuts have a smaller effect on aggregate demand than permanent ones, aligning perfectly with the gold answer's points on expectations, savings behavior, and empirical evidence."}, "llm_echoed_qid": "textbook-108-17-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the reasoning behind why temporary tax cuts have a smaller effect on aggregate demand than permanent ones, aligning perfectly with the gold answer's points on expectations, savings behavior, and empirical evidence."}
{"script_processing_qid": "textbook-100-5-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the concept of admissibility in decision theory. Provide an example of an admissible decision rule and justify why it is admissible.\n\nGOLD_ANSWER:\n**Admissibility**: A decision rule $\\delta$ is admissible if there exists no other rule $\\delta'$ such that $R(\\theta, \\delta') \\leq R(\\theta, \\delta)$ for all $\\theta \\in \\Theta$, with strict inequality for at least one $\\theta$.\\n**Example**: Consider estimating $\\theta$ in $N(\\theta, 1)$ under squared error loss. The sample mean $\\bar{X}$ is admissible because no other estimator uniformly dominates it in risk across all $\\theta$.\\n**Justification**: By the Gauss-Markov theorem, $\\bar{X}$ has the smallest variance among unbiased estimators, making it admissible under squared error loss.\n\nCANDIDATE_ANSWER:\nStep 1: Define Admissibility\nA decision rule δ is admissible if there exists no other rule δ' in the class of decision rules D* such that:\n- R(θ, δ') ≤ R(θ, δ) for all θ ∈ Θ (equal or better performance across all parameters)\n- R(θ, δ') < R(θ, δ) for at least one θ ∈ Θ (strictly better performance for some parameter)\n\nStep 2: Example of an Admissible Rule\nConsider a scenario with Θ = {θ₁, θ₂} and two decision rules:\n- Rule δ₁ has risk R(θ₁, δ₁) = 2 and R(θ₂, δ₁) = 3\n- Rule δ₂ has risk R(θ₁, δ₂) = 3 and R(θ₂, δ₂) = 2\n\nStep 3: Justification\nNeither δ₁ nor δ₂ dominates the other:\n- δ₁ performs better than δ₂ when θ = θ₁ \n- δ₂ performs better than δ₁ when θ = θ₂\nSince no rule exists that uniformly improves on either δ₁ or δ₂ across all θ ∈ Θ, both rules are admissible. This satisfies the definition because neither rule can be eliminated in favor of a uniformly better alternative.\n\nQID: textbook-100-5-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-100-5-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly defines admissibility and provides a valid example with justification. The example demonstrates that neither rule dominates the other, satisfying the criteria for admissibility.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-100-5-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly defines admissibility and provides a valid example with justification. The example demonstrates that neither rule dominates the other, satisfying the criteria for admissibility."}, "llm_echoed_qid": "textbook-100-5-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly defines admissibility and provides a valid example with justification. The example demonstrates that neither rule dominates the other, satisfying the criteria for admissibility."}
{"script_processing_qid": "textbook-125-43-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the difference between a sight draft and a time draft in commercial credit instruments.\n\nGOLD_ANSWER:\nA **sight draft** requires immediate payment upon presentation, while a **time draft** allows the buyer to delay payment until a specified future date. The buyer must accept the time draft by signing it, creating a trade acceptance.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the key defining characteristic in the context. The passage states: 'If immediate payment is required, the draft is termed a sight draft; otherwise, it is known as a time draft.'\n\nStep 2: Contrast the payment timing. A sight draft requires immediate payment upon presentation (at 'sight'), while a time draft allows deferred payment to a specified future date.\n\nStep 3: Confirm no additional distinctions are mentioned. The provided context does not describe differences in documentation, acceptance requirements, or other procedural aspects beyond this core timing distinction.\n\nQID: textbook-125-43-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-43-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate accurately identifies and contrasts the payment timing between sight drafts (immediate) and time drafts (deferred), aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-43-0-1", "category": "CORRECT", "explanation": "The candidate accurately identifies and contrasts the payment timing between sight drafts (immediate) and time drafts (deferred), aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-125-43-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate accurately identifies and contrasts the payment timing between sight drafts (immediate) and time drafts (deferred), aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-117-18-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the formula for continuous compounding and explain how it relates to the values in the table.\n\nGOLD_ANSWER:\n1.  **Formula**: $FV = P e^{rT}$, where $P$ is principal, $r$ is rate, and $T$ is time.\n2.  **Table Relation**: The table provides precomputed values of $e^{rT}$ for various $r$ and $T$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the general compound interest formula for discrete compounding: A = P(1 + r/n)^(nT), where n = number of compounding periods per year.\\n\\nStep 2: Take the limit as n approaches infinity for continuous compounding: \\nlimₙ→∞ (1 + r/n)^(nT) = e^{rT} (using the limit definition e^x = limₙ→∞ (1 + x/n)^n).\\n\\nStep 3: This gives the future value multiplier e^{rT} for continuous compounding (aligning with the first table entry).\\n\\nStep 4: For present value under continuous discounting, reverse the exponential growth: PV = FV × e^{-rT}, explaining the second table entry.\\n\\nStep 5: The table values directly reflect these exponential relationships - e^{rT} grows a dollar over T periods at rate r, while e^{-rT} discounts it back to present value.\n\nQID: textbook-117-18-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-18-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the continuous compounding formula and explains its relation to the table values, matching the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-18-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the continuous compounding formula and explains its relation to the table values, matching the gold answer's key points."}, "llm_echoed_qid": "textbook-117-18-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the continuous compounding formula and explains its relation to the table values, matching the gold answer's key points."}
{"script_processing_qid": "textbook-99-3-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Interpret the hard-threshold rule $w_{j}:=I_{\\{\\theta_{j}^{2}>2\\ln(n)/n\\}}$ in the context of the 'large-small' property of Fourier coefficients. How does this rule adapt to different underlying densities?\n\nGOLD_ANSWER:\nThe **hard-threshold rule** classifies coefficients as 'large' (kept) or 'small' (discarded) based on the threshold $2\\ln(n)/n$. For densities like the Normal, where many coefficients are zero (small), this rule effectively filters out noise. For densities like the Monotone, where all coefficients are large, it retains most terms. The rule adapts to the sparsity of significant coefficients in the underlying density.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the hard-threshold rule's formulation. The rule retains Fourier coefficients where the squared true coefficient θ_j² exceeds 2 ln(n)/n (w_j=1), and discards others (w_j=0).\n\nStep 2: Relate to the 'large-small' property. Coefficients with θ_j² > 2 ln(n)/n are considered 'large' (signal), while those below are 'small' (noise). The threshold 2 ln(n)/n balances signal retention against noise suppression, as Var(θ̂_j) ~ 1/n in typical settings.\n\nStep 3: Adaptation to underlying densities. For densities with slower decay (e.g., less smooth functions), more θ_j² will exceed the threshold, retaining more coefficients. For densities with faster decay (smoother functions), fewer coefficients meet the threshold, automatically adjusting sparsity. The logarithmic term ensures robustness to sample size n.\n\nStep 4: Theoretical implication. The rule adaptively selects coefficients without prior knowledge of the density's smoothness, achieving a trade-off between bias (omitting large θ_j²) and variance (including noisy coefficients) based on the inherent sparsity of the Fourier representation.\n\nQID: textbook-99-3-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-3-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the hard-threshold rule, its relation to the 'large-small' property, and how it adapts to different underlying densities, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-3-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the hard-threshold rule, its relation to the 'large-small' property, and how it adapts to different underlying densities, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-99-3-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the hard-threshold rule, its relation to the 'large-small' property, and how it adapts to different underlying densities, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-54-0-3-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Write the vector AR(1) process in lag operator notation and derive its MA(∞) representation.\n\nGOLD_ANSWER:\n1. **Lag operator form**: $(I-\\Phi L)x_{t}=\\epsilon_{t}$, where $\\Phi$ is the coefficient matrix.\n2. **Inversion**: $x_{t}=(I-\\Phi L)^{-1}\\epsilon_{t}$.\n3. **Taylor expansion**: For $\\|\\Phi\\|<1$, $(I-\\Phi L)^{-1}=I+\\Phi L+\\Phi^{2}L^{2}+...$.\n4. **MA(∞) representation**: $x_{t}=\\sum_{j=0}^{\\infty}\\Phi^{j}\\epsilon_{t-j}$.\n\nCANDIDATE_ANSWER:\nStep 1: Write vector AR(1) in lag operator notation\nGiven the vector AR(1) process:\n$$\\mathbf{x_t} = \\Phi \\mathbf{x_{t-1}} + \\epsilon_t$$\nwhere $\\Phi = \begin{bmatrix}\\phi_{yy} & \\phi_{yz}\\\\ \\phi_{zy} & \\phi_{zz}\\end{bmatrix}$ and $\\mathbf{x_t} = \begin{bmatrix}y_t\\\\ z_t\\end{bmatrix}$.\n\nUsing lag operator $L$ where $L\\mathbf{x_t} = \\mathbf{x_{t-1}}$, we rewrite as:\n$$\\mathbf{x_t} = \\Phi L\\mathbf{x_t} + \\epsilon_t$$\n$$\\Rightarrow (I - \\Phi L)\\mathbf{x_t} = \\epsilon_t$$\n\nStep 2: Solve for MA(∞) representation\nTo convert to MA form, invert the AR polynomial:\n$$\\mathbf{x_t} = (I - \\Phi L)^{-1} \\epsilon_t$$\n\nAssuming invertibility (eigenvalues of Φ lie inside unit circle), expand using matrix geometric series:\n$$(I - \\Phi L)^{-1} = I + \\Phi L + \\Phi^2 L^2 + \\Phi^3 L^3 + \\cdots$$\n\nResulting MA(∞) representation:\n$$\\mathbf{x_t} = \\sum_{j=0}^\\infty \\Phi^j \\epsilon_{t-j}$$\nwhere $\\Phi^0 = I$, $\\Phi^1 = \\Phi$, $\\Phi^2 = \\Phi\\cdot\\Phi$, etc.\n\nQID: textbook-54-0-3-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-54-0-3-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the MA(∞) representation from the vector AR(1) process in lag operator notation, matching the gold answer in both form and reasoning.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-54-0-3-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the MA(∞) representation from the vector AR(1) process in lag operator notation, matching the gold answer in both form and reasoning."}, "llm_echoed_qid": "textbook-54-0-3-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the MA(∞) representation from the vector AR(1) process in lag operator notation, matching the gold answer in both form and reasoning."}
{"script_processing_qid": "textbook-117-33-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the implications of using a 10% vs. 25% discount rate for the painting's valuation.\n\nGOLD_ANSWER:\n1. **10% Rate**: $$ PV = \\frac{\\$480,000}{1.10} = \\$436,364 $$ (appears undervalued at $\\$400,000$).  \n2. **25% Rate**: $$ PV = \\$384,000 $$ (appears overvalued).  \n3. **Key Insight**: The choice of discount rate critically impacts investment decisions, especially for risky assets.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate the NPV at 10% discount rate.\nNPV = (Expected Future Value / (1 + 0.10)) - Initial Cost\n= ($480,000 / 1.10) - $400,000 ≈ $436,364 - $400,000 = +$36,364\n\nStep 2: Calculate the NPV at 25% discount rate (as shown in the context).\nNPV = ($480,000 / 1.25) - $400,000 = $384,000 - $400,000 = -$16,000\n\nStep 3: Compare implications:\n- At 10%: Positive NPV of $36,364 suggests the investment adds value and should be accepted.\n- At 25%: Negative NPV of -$16,000 indicates the investment destroys value and should be rejected.\n\nKey implication: The choice of discount rate critically determines investment viability. A lower rate (10%) underestimates risk and makes the painting appear profitable, while a higher rate (25%) appropriately accounts for risk and reveals the investment as unprofitable.\n\nQID: textbook-117-33-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-33-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly calculates the NPV at both discount rates, interprets the implications accurately, and aligns with the gold answer's key insight about the critical impact of the discount rate choice.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-33-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly calculates the NPV at both discount rates, interprets the implications accurately, and aligns with the gold answer's key insight about the critical impact of the discount rate choice."}, "llm_echoed_qid": "textbook-117-33-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly calculates the NPV at both discount rates, interprets the implications accurately, and aligns with the gold answer's key insight about the critical impact of the discount rate choice."}
{"script_processing_qid": "textbook-73-1-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Interpret the empirical findings in Table 10 regarding the estimates of $1/\\psi$ and the overidentifying restrictions of the model.\n\nGOLD_ANSWER:\n1.  **Imprecise Estimates**: The IV estimates of $1/\\psi$ are very imprecise, with large standard errors. They vary widely across countries and assets, often being negative or insignificantly different from zero.\n2.  **Overidentifying Restrictions**: The overidentifying restrictions are often strongly rejected, particularly when the short-term interest rate is used. This suggests potential misspecification or violation of the orthogonality conditions.\n3.  **Consumption Growth Forecastability**: The $R^{2}$ statistics for forecasting consumption growth are generally low, indicating weak predictability. This weak correlation exacerbates the weak instruments problem.\n4.  **Implications**: The results suggest that the elasticity of intertemporal substitution $\\psi$ is small, and the Epstein-Zin-Weil model's flexibility does not significantly improve the fit of consumption and asset return data.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain the specific empirical results from Table 10 or explicit details about the overidentifying restrictions tests. To interpret the findings for 1/ψ (which typically represents the elasticity of intertemporal substitution or a related parameter) and evaluate model validity via overidentifying restrictions (e.g., Hansen J-test results), one would need the numerical estimates, standard errors, statistical significance levels, and test statistics/p-values reported in Table 10. Since these details are missing from the given context, a meaningful interpretation cannot be provided.\n\nQID: textbook-73-1-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-73-1-1-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the lack of specific empirical results from Table 10.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-73-1-1-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the lack of specific empirical results from Table 10."}, "llm_echoed_qid": "textbook-73-1-1-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the lack of specific empirical results from Table 10."}
{"script_processing_qid": "textbook-80-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the concept of concurvity in additive models and its implications for estimation. Provide a mathematical condition under which concurvity occurs.\n\nGOLD_ANSWER:\nConcurvity occurs when there exists a vector $\\mathbf{b}$ such that $\\widehat{\\mathbf{P}}\\mathbf{b} = 0$, leading to non-uniqueness in the backfitting solution. Mathematically, this happens if there exist $\\mathbf{b}_{\\alpha} \\in \\mathcal{V}_1(\\mathbf{S}_{\\alpha})$ for all $\\alpha = 1, \\ldots, d$ such that $\\sum_{\\alpha=1}^d \\mathbf{b}_{\\alpha} = 0$. This implies that the smoother matrices $\\mathbf{S}_{\\alpha}$ are linearly dependent, causing infinite solutions to the backfitting equations.\n\nCANDIDATE_ANSWER:\nStep 1: Concurvity in additive models refers to a non-uniqueness phenomenon where multiple solutions exist for the component function estimates. This occurs when there exists a non-zero vector b such that the estimated projection matrix multiplied by b equals zero (i.e., \\(\\widehat{\\mathbf{P}}b = 0\\)). \n\nStep 2: When this condition holds, if \\(\\widehat{g}\\) is a solution to the estimation equation (8.6), then \\(\\widehat{g} + \\gamma b\\) is also a valid solution for any \\(\\gamma \\in \\mathbb{R}\\). This introduces ambiguity in the component function estimates.\n\nStep 3: The mathematical condition for concurvity is formally stated as the existence of a vector \\(b \\neq 0\\) such that \\(\\widehat{\\mathbf{P}}b = 0\\). This represents a collinearity issue in the functional space analogous to multicollinearity in linear models.\n\nStep 4: Implications for estimation include: (1) Non-unique component function estimates, complicating interpretation of individual effects; (2) Potential instability in estimation procedures like backfitting; (3) Violation of regularity conditions required for asymptotic normality \\(n^{2/5}\\)-convergence rates mentioned in the context.\n\nQID: textbook-80-0-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-80-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the concept of concurvity, provides the correct mathematical condition, and discusses its implications for estimation, matching the gold answer closely.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-0-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains the concept of concurvity, provides the correct mathematical condition, and discusses its implications for estimation, matching the gold answer closely."}, "llm_echoed_qid": "textbook-80-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the concept of concurvity, provides the correct mathematical condition, and discusses its implications for estimation, matching the gold answer closely."}
{"script_processing_qid": "textbook-125-38-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Interpret the Fama-French three-factor model and explain how it extends the APT framework. What are the empirical findings regarding the size and book-to-market factors?\n\nGOLD_ANSWER:\n1. **Model Interpretation**: The Fama-French three-factor model extends APT by adding two additional factors to the market factor:\n   - **Size factor**: Captures the historical outperformance of small-cap stocks over large-cap stocks.\n   - **Book-to-market factor**: Captures the historical outperformance of high book-to-market (value) stocks over low book-to-market (growth) stocks.\n2. **Empirical Findings**:\n   - **Size Factor**: Small-cap stocks have historically provided higher returns than large-cap stocks, suggesting a size premium.\n   - **Book-to-Market Factor**: Value stocks (high book-to-market) have historically outperformed growth stocks (low book-to-market), suggesting a value premium.\n3. **Extension of APT**: The Fama-French model is a specific application of APT where the factors are empirically identified. It provides a more comprehensive explanation of expected returns than the single-factor CAPM.\n\nCANDIDATE_ANSWER:\nStep 1: Interpret the Fama-French three-factor model\n- The Fama-French model extends the basic APT framework by explicitly specifying three factors:\n  1. Market factor (same as CAPM)\n  2. Size factor (returns of small-cap stocks minus large-cap stocks)\n  3. Book-to-market factor (returns of high book-to-market stocks minus low book-to-market stocks)\n- It follows APT's multi-factor structure: Expected risk premium = Σ(b_i × factor risk premium)\n\nStep 2: Explain how it extends APT\n- While APT is a general multi-factor framework without specifying which factors matter, Fama-French:\n  - Empirically identifies two additional firm-specific factors beyond the market factor\n  - Provides concrete implementation using observable firm characteristics (size and book-to-market ratio)\n  - Demonstrates that these factors systematically explain cross-sectional stock returns\n\nStep 3: Empirical findings\n- Size factor: Small-cap stocks historically outperform large-cap stocks after controlling for market risk\n- Book-to-market factor: Value stocks (high book-to-market) outperform growth stocks (low book-to-market)\n- Both factors show persistent explanatory power in empirical tests, suggesting they capture risk premiums not accounted for by the single-factor CAPM.\n\nQID: textbook-125-38-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-38-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately interprets the Fama-French three-factor model, correctly explains how it extends the APT framework, and accurately summarizes the empirical findings regarding the size and book-to-market factors. The response aligns well with the gold answer in both content and structure.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-38-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately interprets the Fama-French three-factor model, correctly explains how it extends the APT framework, and accurately summarizes the empirical findings regarding the size and book-to-market factors. The response aligns well with the gold answer in both content and structure."}, "llm_echoed_qid": "textbook-125-38-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately interprets the Fama-French three-factor model, correctly explains how it extends the APT framework, and accurately summarizes the empirical findings regarding the size and book-to-market factors. The response aligns well with the gold answer in both content and structure."}
{"script_processing_qid": "textbook-125-12-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Why might a Swiss pharmaceutical company like Roche use a different beta for its U.S. project compared to its Swiss project? How is this beta determined?\n\nGOLD_ANSWER:\n2. **Beta Determination for Roche**:\n   - Roche measures beta relative to the Swiss market index. A U.S. project's cash flows are less tied to the Swiss market, resulting in a lower beta (e.g., 0.36 for U.S. pharmaceuticals).\n   - **Reason**: Diversification benefits reduce the project's risk for Swiss investors, as the U.S. market is less correlated with the Swiss market.\n\nCANDIDATE_ANSWER:\nStep 1: Beta measures risk relative to the investor’s existing portfolio. Swiss investors (who hold the Swiss market) would perceive risk differently than U.S. investors (who hold the U.S. market).\n\nStep 2: A U.S. project’s beta for Roche would reflect its covariance with the Swiss market portfolio (the reference for Swiss investors), not the U.S. market. This differs from a Swiss project’s beta, which is measured against the Swiss market directly.\n\nStep 3: The beta is determined by (1) calculating the project’s returns’ covariance with the Swiss market returns, divided by (2) the variance of the Swiss market returns. Lower correlation between the U.S. project and Swiss market (compared to a domestic Swiss project) reduces beta, as diversification benefits lower perceived risk.\n\nStep 4: Example from context: Emerging markets like Thailand might have higher volatility but beta >1 only if returns are strongly correlated with the reference market (e.g., U.S.). Similarly, Roche’s U.S. project beta depends on its correlation with Swiss market performance.\n\nQID: textbook-125-12-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-12-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains why Roche would use a different beta for its U.S. project compared to its Swiss project, detailing the calculation method and the impact of diversification benefits on perceived risk. The reasoning aligns perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-12-1-1", "category": "CORRECT", "explanation": "The candidate's answer accurately explains why Roche would use a different beta for its U.S. project compared to its Swiss project, detailing the calculation method and the impact of diversification benefits on perceived risk. The reasoning aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-125-12-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains why Roche would use a different beta for its U.S. project compared to its Swiss project, detailing the calculation method and the impact of diversification benefits on perceived risk. The reasoning aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-125-40-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the implications of Proposition I for a firm's capital structure decisions under perfect capital markets.\n\nGOLD_ANSWER:\nProposition I states that the value of a firm is determined solely by its real assets and is independent of its capital structure under perfect capital markets. This implies:\n1. **Irrelevance of Financing Mix**: The proportions of debt and equity do not affect firm value.\n2. **No Arbitrage**: Investors can replicate any capital structure by borrowing or lending on their own, so firm leverage does not create value.\n3. **Conservation of Value**: Slicing cash flows between debt and equity does not change total value, as long as no value is lost (e.g., no taxes or bankruptcy costs).\n\nCANDIDATE_ANSWER:\nStep 1: Understand Proposition I's core claim\nProposition I states that under perfect capital markets, a firm's total value is determined solely by its real assets and operating income, not by how it splits financing between debt and equity. This follows from the law of conservation of value (value additivity principle).\n\nStep 2: Recognize capital structure irrelevance\nThis implies that altering debt-equity ratios (D/E) through financing decisions:\n- Does NOT affect WACC (r_A remains constant)\n- Does NOT create/destroy firm value\n- Only redistributes claims on the same asset value between debt and equity holders\n\nStep 3: Analyze the cost of equity relationship\nThe equation r_E = r_A + (D/E)(r_A - r_D) shows:\n- Equity costs rise with leverage to exactly offset debt's cheaper cost\n- Total expected return to all security holders (r_A) remains unchanged\n\nStep 4: Practical implications for managers\n- Capital structure decisions become irrelevant for value creation\n- Time/resources spent optimizing debt ratios provide no benefit\n- Focus should shift to maximizing asset value through operational/investment decisions\n\nStep 5: Contrast with real-world considerations\n(In perfect markets) No need to consider:\n- Tax advantages of debt\n- Bankruptcy costs\n- Agency problems\n- Signaling effects\n\nQID: textbook-125-40-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-40-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains Proposition I's implications, including capital structure irrelevance, cost of equity relationship, and practical managerial implications, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-40-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains Proposition I's implications, including capital structure irrelevance, cost of equity relationship, and practical managerial implications, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-125-40-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains Proposition I's implications, including capital structure irrelevance, cost of equity relationship, and practical managerial implications, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-59-1-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the model's predictions for the output response to a monetary shock with the empirical evidence from VAR studies. What are the key discrepancies, and how does introducing investment delays address them?\n\nGOLD_ANSWER:\n1. **Model vs. VAR**: The model predicts an immediate output response, whereas VAR evidence shows a delayed (hump-shaped) response.\n2. **Investment Delays**: Introducing a one-quarter investment delay generates a hump-shaped output response, as the full effect of the shock takes time to materialize.\n3. **Persistence**: Delays also increase the persistence of the response, better matching the empirical evidence. However, the initial response remains stronger in the model than in the data, suggesting additional frictions may be needed.\n\nCANDIDATE_ANSWER:\nStep 1: Identify model predictions from the context<br>The model with the financial accelerator predicts:<br>- 50% stronger initial output response to monetary shocks<br>- Nearly double the investment response<br>- Sharp decline in external finance premium with slow reversion<br>- Amplified/persistent real effects due to credit frictions<br><br>Step 2: Contrast with typical VAR evidence<br>Empirical VAR studies generally show:<br>- Output/investment responses are hump-shaped (delayed peak)<br>- Effects persist beyond initial quarters but with gradual buildup<br>- Smaller initial investment response compared to model predictions<br><br>Step 3: Identify key discrepancies<br>Discrepancies include:<br>1. Model shows stronger immediate investment response (~2x baseline) vs. VAR's delayed investment response<br>2. Model's initial output amplification (50% greater) may exceed VAR estimates<br>3. Financial accelerator creates sharper price-of-capital movements than observed<br><br>Step 4: Explain how investment delays address these<br>Introducing investment delays (e.g., time-to-build constraints):<br>1. Spreads investment response over multiple periods<br>2. Creates hump-shaped pattern matching VAR evidence<br>3. Reduces initial spike in capital demand/price<br>4. Better aligns timing/persistence with empirical results\n\nQID: textbook-59-1-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-59-1-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies the discrepancies between the model's predictions and VAR evidence, and correctly explains how introducing investment delays addresses these discrepancies, matching the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-59-1-1-3", "category": "CORRECT", "explanation": "The candidate answer accurately identifies the discrepancies between the model's predictions and VAR evidence, and correctly explains how introducing investment delays addresses these discrepancies, matching the gold answer's key points."}, "llm_echoed_qid": "textbook-59-1-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies the discrepancies between the model's predictions and VAR evidence, and correctly explains how introducing investment delays addresses these discrepancies, matching the gold answer's key points."}
{"script_processing_qid": "textbook-109-1-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Prove Modigliani-Miller Proposition I (no taxes) using arbitrage arguments. What assumptions are critical?\n\nGOLD_ANSWER:\n1.  **Arbitrage Proof**:\n    - Assume two firms (U: unlevered, L: levered) with identical cash flows.\n    - If $V_{L} > V_{U}$, investors sell L and buy U, earning riskless profit until $V_{L} = V_{U}$.\n2.  **Assumptions**:\n    - No taxes, bankruptcy costs, or asymmetric information.\n    - Perfect capital markets (no transaction costs, equal borrowing rates).\n\nCANDIDATE_ANSWER:\nStep 1: State Modigliani-Miller Proposition I (no taxes)\nIn perfect capital markets with no taxes, the total market value of a firm (V) is independent of its capital structure. Formally, V_L = V_U, where L = levered firm and U = unlevered firm.\n\nStep 2: Arbitrage Proof Setup\nAssume two firms (U & L) have identical operating cash flows but different capital structures:\n- Firm U is 100% equity-financed\n- Firm L uses debt (D) and equity (E)\n\nStep 3: Arbitrage Argument\nIf V_L ≠ V_U, investors can profit risklessly:\nCase A (V_L > V_U):\n1. Sell short 1% of L’s equity & debt\n2. Use proceeds to buy 1% of U’s equity\n3. Earn arbitrage profit = 1% × (V_L - V_U)\n\nCase B (V_L < V_U):\n1. Borrow (D/E)% at rate r_D\n2. Buy 1% of L’s equity\n3. Replicate U’s returns at lower cost\n4. Earn arbitrage profit = 1% × (V_U - V_L)\n\nStep 4: Equilibrium\nArbitrage forces V_L = V_U. The WACC formula simplifies to r_0 (unlevered cost of capital) regardless of D/E ratio.\n\nCritical Assumptions:\n1. No taxes\n2. Perfect capital markets (no transaction costs/bankruptcy costs)\n3. Symmetric information\n4. Investors & firms borrow at same risk-free rate\n5. No agency costs\n6. All cash flows are perpetual\n\nQID: textbook-109-1-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-109-1-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly states Modigliani-Miller Proposition I, provides a detailed arbitrage proof, and lists the critical assumptions accurately. The reasoning and steps align with the gold answer, demonstrating a thorough understanding of the topic.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-109-1-1-1", "category": "CORRECT", "explanation": "The candidate's answer correctly states Modigliani-Miller Proposition I, provides a detailed arbitrage proof, and lists the critical assumptions accurately. The reasoning and steps align with the gold answer, demonstrating a thorough understanding of the topic."}, "llm_echoed_qid": "textbook-109-1-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly states Modigliani-Miller Proposition I, provides a detailed arbitrage proof, and lists the critical assumptions accurately. The reasoning and steps align with the gold answer, demonstrating a thorough understanding of the topic."}
{"script_processing_qid": "textbook-95-1-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Analyze the implications of the 1993Q4 discrepancy between $M1$-based (20 bps) and $M2$-based (80 bps) shock measures. How should researchers address such disagreements?\n\nGOLD_ANSWER:\n**Implications**:\n-   **Measurement Uncertainty**: Discrepancies highlight sensitivity to money aggregate choice, complicating real-time policy assessment.\n-   **Resolution Strategies**:\n    1.  Report results under multiple specifications to assess robustness.\n    2.  Use model-averaging techniques to weight estimates by their historical reliability.\n    3.  Incorporate auxiliary data (e.g., Fed communications) to reconcile conflicting signals.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that the 1993Q4 discrepancy (20 bps vs 80 bps) reveals limitations in monetary aggregates as proxies. M1 (narrow money) and M2 (broader money) likely responded differently to policy changes or structural factors during this period.\nStep 2: Note that while correlations between shock measures are moderate (e.g., 0.85), the key insight from the context is that qualitative conclusions about monetary policy effects remain consistent across measures despite quantitative differences.\nStep 3: Researchers should:\n   a) Report multiple shock measures (e.g., M1-based and M2-based) to show robustness of qualitative results\n   b) Acknowledge measurement uncertainty in quantitative estimates due to proxy limitations\n   c) Focus inference primarily on direction/sign of effects rather than exact magnitudes\n   d) Investigate structural reasons for discrepancies (e.g., financial innovation shifts between M1/M2 components)\n\nQID: textbook-95-1-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-95-1-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies the implications of the discrepancy and provides a comprehensive set of strategies for researchers, aligning well with the gold answer's points on measurement uncertainty and resolution strategies.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-95-1-2-2", "category": "CORRECT", "explanation": "The candidate answer accurately identifies the implications of the discrepancy and provides a comprehensive set of strategies for researchers, aligning well with the gold answer's points on measurement uncertainty and resolution strategies."}, "llm_echoed_qid": "textbook-95-1-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies the implications of the discrepancy and provides a comprehensive set of strategies for researchers, aligning well with the gold answer's points on measurement uncertainty and resolution strategies."}
{"script_processing_qid": "textbook-108-3-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Interpret the Isolandian economists' advice on trade policy. Why do they recommend against tariffs and quotas?\n\nGOLD_ANSWER:\n1.  **Efficiency**: Free trade maximizes total surplus by aligning domestic prices with world prices.\n2.  **Deadweight Loss**: Tariffs and quotas distort markets, creating deadweight losses ($\\mathrm{D+F}$).\n3.  **Distributional Effects**: While producers gain from protectionism, consumer losses outweigh these gains.\n4.  **Dynamic Benefits**: Free trade fosters competition, innovation, and economies of scale.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that both tariffs and quotas reduce total surplus by creating deadweight loss.\nThe economists highlight that quotas reduce total surplus by area D + F (deadweight loss), similar to tariffs. This represents lost economic efficiency.\n\nStep 2: Observe the redistribution effects.\nQuotas transfer surplus (E' + E'') to license holders rather than government revenue. Even if the government captures this surplus through license fees (making it equivalent to a tariff), the deadweight loss persists.\n\nStep 3: Compare welfare outcomes.\nBoth policies reduce total welfare by D + F regardless of who receives the redistributed surplus (government or license holders). Free trade would maximize total surplus by avoiding these losses.\n\nStep 4: Conclude the recommendation.\nThe economists advise against tariffs/quotas because they create net welfare losses (deadweight loss) without addressing the root inefficiency, making free trade Pareto-superior.\n\nQID: textbook-108-3-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-108-3-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately interprets the Isolandian economists' advice, emphasizing the deadweight loss and inefficiency caused by tariffs and quotas, and correctly concludes that free trade is superior. The reasoning aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-3-0-2", "category": "CORRECT", "explanation": "The candidate's answer accurately interprets the Isolandian economists' advice, emphasizing the deadweight loss and inefficiency caused by tariffs and quotas, and correctly concludes that free trade is superior. The reasoning aligns with the gold answer."}, "llm_echoed_qid": "textbook-108-3-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately interprets the Isolandian economists' advice, emphasizing the deadweight loss and inefficiency caused by tariffs and quotas, and correctly concludes that free trade is superior. The reasoning aligns with the gold answer."}
{"script_processing_qid": "textbook-93-1-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the difference between weak and strong ergodicity in the context of Markov chains.\n\nGOLD_ANSWER:\n1. **Weak Ergodicity**: The rows of the forward product $T_{p,r}$ tend to equality as $r \\to \\infty$, but the elements themselves may not converge to a limit.\n2. **Strong Ergodicity**: Both weak ergodicity holds and the elements $t_{i,s}^{(p,r)}$ themselves tend to a limit as $r \\to \\infty$.\n3. **Implication**: Strong ergodicity implies the existence of a unique invariant distribution, while weak ergodicity only implies row equality.\n\nCANDIDATE_ANSWER:\nStep 1: Weak ergodicity (Definition 4.4) requires that differences between corresponding elements of different rows in the forward product matrix T_{p,r} vanish as r→∞. Formally, |t_{i,s}^{(p,r)} - t_{j,s}^{(p,r)}| → 0 for all i,j,s,p. This means the chain 'forgets' its initial state distribution over time.\n\nStep 2: Strong ergodicity (Definition 4.5) adds an extra requirement beyond weak ergodicity: The individual elements t_{i,s}^{(p,r)} must themselves converge to a specific limit (not just converge relative to each other). This implies the existence of a unique limiting probability vector that the chain approaches.\n\nKey difference: Weak ergodicity ensures state distributions become indistinguishable across different starting points, while strong ergodicity guarantees convergence to an absolute stationary distribution regardless of initial conditions.\n\nQID: textbook-93-1-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-93-1-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately captures the definitions and implications of weak and strong ergodicity as described in the gold answer, including the convergence properties and the existence of a unique invariant distribution in strong ergodicity.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-93-1-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately captures the definitions and implications of weak and strong ergodicity as described in the gold answer, including the convergence properties and the existence of a unique invariant distribution in strong ergodicity."}, "llm_echoed_qid": "textbook-93-1-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately captures the definitions and implications of weak and strong ergodicity as described in the gold answer, including the convergence properties and the existence of a unique invariant distribution in strong ergodicity."}
{"script_processing_qid": "textbook-99-4-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the expected value and variance of the histogram estimator $\\hat{f}_{n}(x)$ for a given bin width $h$.\n\nGOLD_ANSWER:\n1. **Expected Value**: $E[\\hat{f}_{n}(x)] = \\frac{1}{h} \\int_{x_{0}+mh}^{x_{0}+(m+1)h} f(u) du$.\n2. **Variance**: $Var(\\hat{f}_{n}(x)) = \\frac{1}{nh^2} \\left( \\int_{x_{0}+mh}^{x_{0}+(m+1)h} f(u) du \\right) \\left(1 - \\int_{x_{0}+mh}^{x_{0}+(m+1)h} f(u) du \\right)$.\n\nCANDIDATE_ANSWER:\nStep 1: Define the probability of an observation falling into the bin containing x. Let $p = P(X_i \\in \\text{bin containing } x) = \\int_{x_0 + mh}^{x_0 + (m+1)h} f(t)dt$. \n\nStep 2: The number of observations in the bin follows a binomial distribution: $K \\sim \\text{Binomial}(n, p)$. The histogram estimator is $\\hat{f}_n(x) = \\frac{K}{nh}$. \n\nStep 3: Compute the expectation:\n$E[\\hat{f}_n(x)] = \\frac{E[K]}{nh} = \\frac{np}{nh} = \\frac{p}{h}$.\nIf $f$ is approximately constant over the bin (small $h$), then $p \\approx f(x)h$, giving $E[\\hat{f}_n(x)] \\approx f(x)$.\n\nStep 4: Compute the variance:\n$\\text{Var}(\\hat{f}_n(x)) = \\frac{\\text{Var}(K)}{(nh)^2} = \\frac{np(1-p)}{(nh)^2} = \\frac{p(1-p)}{nh^2}$.\nAgain, using $p \\approx f(x)h$, the variance simplifies to:\n$\\text{Var}(\\hat{f}_n(x)) \\approx \\frac{f(x)}{nh}$ (since $(1-p) \\approx 1$ for small $h$).\n\nFinal results:\n- Expected value: $E[\\hat{f}_n(x)] = \\frac{p}{h} \\approx f(x)$\n- Variance: $\\text{Var}(\\hat{f}_n(x)) \\approx \\frac{f(x)}{nh}$\n\nQID: textbook-99-4-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-4-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's derivation correctly follows the steps to derive the expected value and variance of the histogram estimator, matching the gold answer's results. The reasoning is clear and aligns with the provided solution.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-4-1-0", "category": "CORRECT", "explanation": "The candidate's derivation correctly follows the steps to derive the expected value and variance of the histogram estimator, matching the gold answer's results. The reasoning is clear and aligns with the provided solution."}, "llm_echoed_qid": "textbook-99-4-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's derivation correctly follows the steps to derive the expected value and variance of the histogram estimator, matching the gold answer's results. The reasoning is clear and aligns with the provided solution."}
{"script_processing_qid": "textbook-80-5-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the asymptotic properties of the profile likelihood estimator $\\widehat{\\beta}$ in the GPLM context.\n\nGOLD_ANSWER:\n1. **Consistency**: $\\widehat{\\beta}$ is consistent under regularity conditions.\n2. **Asymptotic Normality**: $\\widehat{\\beta}$ has an asymptotic normal distribution: $$\\sqrt{n}(\\widehat{\\beta} - \\beta_0) \\xrightarrow{d} N(0, I^{-1}(\\beta_0)),$$ where $I(\\beta_0)$ is the Fisher information matrix.\n3. **Efficiency**: $\\widehat{\\beta}$ is asymptotically efficient, achieving the Cramér-Rao lower bound.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the key components of the profile likelihood method in GPLM.\nThe profile likelihood estimator for β involves two stages:\n- Fix β and estimate the nonparametric component mβ(T_i) via a least favorable curve\n- Maximize the parametric profile likelihood function with respect to β using this mβ\n\nStep 2: Analyze the convergence rate\n- The √n-rate for β̂ emerges because the profile likelihood approach properly accounts for the estimation of the nonparametric component mβ\n- This separates the parametric estimation (β) from the slower-converging nonparametric component, preserving the parametric rate for β\n\nStep 3: Establish asymptotic efficiency\n- The construction using the least favorable curve ensures that the information about β is fully captured in the profile likelihood\n- Under regularity conditions (implicit in the context), the estimator achieves the Cramér-Rao lower bound asymptotically\n- This makes β̂ asymptotically normal with variance equal to the inverse Fisher information matrix\n\nFinal Properties:\n1. √n-consistency: ||β̂ - β|| = O_p(n^{-1/2})\n2. Asymptotic normality: √n(β̂ - β) → N(0, I^{-1}(β))\n3. Efficiency: Achieves the semiparametric efficiency bound\n\nQID: textbook-80-5-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-80-5-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the asymptotic properties of the profile likelihood estimator in GPLM, including consistency, asymptotic normality, and efficiency, matching the gold answer's points with additional detailed reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-5-0-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the asymptotic properties of the profile likelihood estimator in GPLM, including consistency, asymptotic normality, and efficiency, matching the gold answer's points with additional detailed reasoning."}, "llm_echoed_qid": "textbook-80-5-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the asymptotic properties of the profile likelihood estimator in GPLM, including consistency, asymptotic normality, and efficiency, matching the gold answer's points with additional detailed reasoning."}
{"script_processing_qid": "textbook-67-1-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Interpret the coefficients of the 'early' and 'late' dummies in the panel regressions for growth in real GDP, private consumption, and durable goods consumption.\n\nGOLD_ANSWER:\n2. **Interpretation of Coefficients**:\n   - **Early Dummy**: Positive coefficients indicate higher growth during the early stages of stabilization. For example, private consumption growth is 3.33% higher than trend growth.\n   - **Late Dummy**: Negative coefficients indicate lower growth during the late stages. For example, private consumption growth is 4.60% lower than trend growth.\n   - The magnitudes suggest the boom-recession cycle is most pronounced for durable goods and least for GDP.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the regression framework. The panel regressions use stabilization time dummies (T+j) to capture dynamic effects. 'Early' and 'late' dummies likely correspond to periods after stabilization (e.g., T+0 to T+2 = early phase; T+3/T+4 = late phase), though the exact cutoff isn’t explicitly stated in the context.\n\nStep 2: Interpret coefficients for growth in real GDP. A positive coefficient for 'early' implies an initial GDP growth surge post-stabilization. A negative coefficient for 'late' indicates a subsequent GDP contraction, consistent with the boom-recession cycle described in the literature.\n\nStep 3: Interpret coefficients for private consumption. Similarly, a positive 'early' coefficient suggests increased household spending during the boom phase, while a negative 'late' coefficient reflects reduced consumption during the recession phase, aligning with the typical consumption smoothing pattern.\n\nStep 4: Interpret coefficients for durable goods consumption. The 'early' coefficient would likely be larger in magnitude than for non-durables, as durable purchases are more sensitive to interest rates and inflation expectations (which stabilize initially). The 'late' coefficient would show a sharper decline, as postponed replacement and reduced credit availability exacerbate the downturn.\n\nStep 5: Generalize across variables. All three dependent variables should exhibit this pattern: positive early effects (economic expansion) and negative late effects (contraction), with the magnitude differing based on the variable’s cyclical sensitivity (durables > private consumption > GDP in volatility). The coefficients quantitatively measure the average deviation from non-stabilization periods, adjusted for external shocks.\n\nQID: textbook-67-1-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-67-1-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly interprets the coefficients for 'early' and 'late' dummies across GDP, private consumption, and durable goods, aligning with the gold answer's key points about growth patterns and cyclical sensitivity.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-67-1-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly interprets the coefficients for 'early' and 'late' dummies across GDP, private consumption, and durable goods, aligning with the gold answer's key points about growth patterns and cyclical sensitivity."}, "llm_echoed_qid": "textbook-67-1-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly interprets the coefficients for 'early' and 'late' dummies across GDP, private consumption, and durable goods, aligning with the gold answer's key points about growth patterns and cyclical sensitivity."}
{"script_processing_qid": "textbook-111-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the model fails to explain why unemployment exists in the first place. What additional factors are needed to account for unemployment?\n\nGOLD_ANSWER:\nThe model assumes job finding is not instantaneous but does not explain why. Additional factors include:\n- **Job Search**: Time and effort required to match workers with suitable jobs.\n- **Wage Rigidity**: Wages may not adjust to clear the labor market, leading to structural unemployment.\n\nCANDIDATE_ANSWER:\nStep 1: The model assumes unemployment arises from equilibrium flows between employment/unemployment states (driven by fixed separation rate s and job finding rate f).\nStep 2: It fails to explain the root causes of unemployment because it treats s and f as exogenous parameters rather than modeling why:\n   - Job separations occur (e.g., recessions, automation)\n   - Job finding takes time (e.g., search frictions, skill mismatches)\nStep 3: Additional factors needed to explain unemployment existence include:\n   - Wage rigidities (e.g., minimum wages, efficiency wages)\n   - Labor market imperfections (e.g., information asymmetries, matching costs)\n   - Structural factors (e.g., sectoral shifts, geographic immobility)\n   - Aggregate demand shocks\n   - Institutional factors (e.g., unemployment benefits)\nThe model only quantifies steady-state unemployment given s and f but doesn’t explain why these flows persist in equilibrium.\n\nQID: textbook-111-0-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies the model's limitations and provides a comprehensive list of additional factors needed to explain unemployment, matching the gold answer's points and expanding on them with relevant details.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-0-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately identifies the model's limitations and provides a comprehensive list of additional factors needed to explain unemployment, matching the gold answer's points and expanding on them with relevant details."}, "llm_echoed_qid": "textbook-111-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies the model's limitations and provides a comprehensive list of additional factors needed to explain unemployment, matching the gold answer's points and expanding on them with relevant details."}
{"script_processing_qid": "textbook-75-0-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Solve for the Nash equilibrium in a Cournot duopoly with inverse demand $P = a - bQ$ and cost functions $C_i(q_i) = cq_i$ for firms $i = 1, 2$.\n\nGOLD_ANSWER:\n1.  **Profit Functions**: $\\pi_i = (a - b(q_1 + q_2))q_i - cq_i$.\n2.  **First-Order Conditions**: $\\frac{d\\pi_i}{dq_i} = a - b(q_1 + q_2) - bq_i - c = 0$.\n3.  **Reaction Functions**: $q_i = \\frac{a - c - bq_j}{2b}$.\n4.  **Symmetry**: $q_1 = q_2 = q^*$.\n5.  **Equilibrium Quantity**: $q^* = \\frac{a - c}{3b}$.\n6.  **Equilibrium Price**: $P^* = a - b(2q^*) = \\frac{a + 2c}{3}$.\n\nCANDIDATE_ANSWER:\nStep 1: Define profit functions for both firms.\\nProfit for Firm 1: π₁ = (P - c)q₁ = [a - b(q₁ + q₂) - c]q₁\\nProfit for Firm 2: π₂ = (a - b(q₁ + q₂) - c)q₂\\n\\nStep 2: Calculate first-order conditions by taking derivatives with respect to each firm's quantity:\\nFor Firm 1: ∂π₁/∂q₁ = a - c - 2bq₁ - bq₂ = 0\\nFor Firm 2: ∂π₂/∂q₂ = a - c - 2bq₂ - bq₁ = 0\\n\\nStep 3: Solve the system of equations simultaneously:\\nFrom Firm 1's FOC: q₁ = (a - c - bq₂)/(2b)\\nFrom Firm 2's FOC: q₂ = (a - c - bq₁)/(2b)\\n\\nStep 4: Substitute one reaction function into the other:\\nq₁ = (a - c - b[(a - c - bq₁)/(2b)])/(2b)\\nMultiply through by 2b:\\n2bq₁ = a - c - (a - c)/2 + q₁/2\\nCombine terms:\\n(4bq₁)/2 - q₁/2 = (a - c)/2\\n(3bq₁) = (a - c)\\n\\nStep 5: Symmetric solution gives Nash equilibrium quantities:\\nq₁* = q₂* = (a - c)/(3b)\\nTotal output Q = (2(a - c))/(3b)\\nMarket price P = a - bQ = (a + 2c)/3\n\nQID: textbook-75-0-2-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-75-0-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the Nash equilibrium in a Cournot duopoly, including profit functions, first-order conditions, reaction functions, and the symmetric equilibrium solution. The final equilibrium quantities and price match the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-75-0-2-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the Nash equilibrium in a Cournot duopoly, including profit functions, first-order conditions, reaction functions, and the symmetric equilibrium solution. The final equilibrium quantities and price match the gold answer."}, "llm_echoed_qid": "textbook-75-0-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the Nash equilibrium in a Cournot duopoly, including profit functions, first-order conditions, reaction functions, and the symmetric equilibrium solution. The final equilibrium quantities and price match the gold answer."}
{"script_processing_qid": "textbook-120-7-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. State and interpret Theorem 8.4.31 on the consistency of $\\beta_{n}^{*}$. What conditions are required for $\\beta_{n}^{*} \\to \\beta^{*}$ in probability?\n\nGOLD_ANSWER:\n**Theorem 8.4.31**: If $\\mu$ is a distribution on $\\mathbb{R}^{K+1}$ with linearly independent $X_i$, and if:\n1. $(\\int X^{T}X d\\mu_{n}) \\to (\\int X^{T}X d\\mu)$ in probability,\n2. $(\\int X^{T}Y d\\mu_{n}) \\to (\\int X^{T}Y d\\mu)$ in probability,\nthen $\\beta_{n}^{*} \\to \\beta^{*}$ in probability.\n\n**Interpretation**: The theorem states that the empirical regression coefficients $\\beta_{n}^{*}$ converge to the ideal coefficients $\\beta^{*}$ as the sample size grows, provided that the empirical moments converge to their population counterparts. This is a consistency result, ensuring that the estimator is asymptotically unbiased. The conditions require that the cross-moments of the empirical distribution $\\mu_{n}$ stabilize to those of the true distribution $\\mu$.\n\nCANDIDATE_ANSWER:\nStep 1: State Theorem 8.4.31\nUnder the conditions that:\n- The random variables $(\\int X^T X d\\mu_n)$ converge in probability to the constant $\\int X^T X d\\mu$,\n- The random variables $(\\int X^T Y d\\mu_n)$ converge in probability to the constant $\\int X^T Y d\\mu$,\nthen $\\beta_n^* \\to \\beta^*$ in probability.\n\nStep 2: Interpretation\nThe theorem establishes that the empirical regression coefficients $\\beta_n^*$ will converge to the ideal population coefficients $\\beta^*$ when:\n1. The sample cross-product matrix $X^T X/n$ (approximating $\\int X^T X d\\mu_n$) converges to its population counterpart $\\int X^T X d\\mu$,\n2. The sample moment $X^T Y/n$ (approximating $\\int X^T Y d\\mu_n$) converges to its population counterpart $\\int X^T Y d\\mu$.\n\nStep 3: Required Conditions\nFor consistency ($\\beta_n^* \\to \\beta^*$ in probability), the following must hold:\na. The $X_i$ must be linearly independent under the population distribution $\\mu$ (ensuring $\\beta^*$ is well-defined).\nb. Weak law of large numbers-type convergence for the first and second moments:\n   - $\\int X^T X d\\mu_n \\xrightarrow{p} \\int X^T X d\\mu$\n   - $\\int X^T Y d\\mu_n \\xrightarrow{p} \\int X^T Y d\\mu$\nThese conditions guarantee that the empirical moment matrices stabilize to their population analogs as $n \\to \\infty$.\n\nQID: textbook-120-7-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-120-7-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately states Theorem 8.4.31, provides a correct interpretation, and lists the required conditions for consistency. The reasoning and details align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-120-7-0-2", "category": "CORRECT", "explanation": "The candidate answer accurately states Theorem 8.4.31, provides a correct interpretation, and lists the required conditions for consistency. The reasoning and details align with the gold answer."}, "llm_echoed_qid": "textbook-120-7-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately states Theorem 8.4.31, provides a correct interpretation, and lists the required conditions for consistency. The reasoning and details align with the gold answer."}
{"script_processing_qid": "textbook-125-55-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Interpret the times-interest-earned ratio of 5.2 for Executive Paper. How does it compare to the industry average, and what does this imply about the company's financial risk?\n\nGOLD_ANSWER:\n1. **Interpretation**: The times-interest-earned ratio measures how many times a company can cover its interest payments with its earnings before interest and taxes (EBIT) plus depreciation. A ratio of 5.2 means Executive Paper can cover its interest payments 5.2 times with its operating earnings.\n2. **Comparison**: The industry average is 2.9, so Executive Paper's ratio is significantly higher, indicating lower financial risk and a stronger ability to meet interest obligations.\n3. **Implications**: This suggests Executive Paper is less leveraged and has a more comfortable cushion against earnings volatility compared to its peers.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the times-interest-earned (TIE) ratio interpretation. The TIE ratio of 5.2 indicates Executive Paper's EBIT + depreciation covers its interest expenses 5.2 times. Step 2: Compare to industry context. While the exact industry average isn’t provided in the extracted data, the section explicitly compares Executive Paper to the paper industry. Typically, a higher TIE ratio suggests lower financial risk. Step 3: Analyze implications. A TIE ratio of 5.2 (assuming it’s below the industry average given the context of analysis) would imply Executive Paper has less earnings cushion to cover interest obligations than peers, increasing its financial risk. However, the absolute ratio of 5.2 still demonstrates reasonable coverage capacity given that values above 2–3 are generally considered acceptable.\n\nQID: textbook-125-55-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-55-0-1\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer incorrectly assumes Executive Paper's TIE ratio is below the industry average, which contradicts the gold answer stating it is significantly higher (5.2 vs. 2.9). This leads to a flawed conclusion about the company's financial risk.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-55-0-1", "category": "INCORRECT", "explanation": "The candidate's answer incorrectly assumes Executive Paper's TIE ratio is below the industry average, which contradicts the gold answer stating it is significantly higher (5.2 vs. 2.9). This leads to a flawed conclusion about the company's financial risk."}, "llm_echoed_qid": "textbook-125-55-0-1", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer incorrectly assumes Executive Paper's TIE ratio is below the industry average, which contradicts the gold answer stating it is significantly higher (5.2 vs. 2.9). This leads to a flawed conclusion about the company's financial risk."}
{"script_processing_qid": "textbook-125-26-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the NPV of Otobai's electric scooter project under the expected scenario, given the initial investment of ¥15 billion and annual cash flows of ¥3 billion for 10 years at a 10% discount rate.\n\nGOLD_ANSWER:\n1. **NPV Formula**: $NPV = -I + \\sum_{t=1}^{T} \\frac{CF_t}{(1 + r)^t}$.\n2. **Calculation**: $NPV = -15 + \\sum_{t=1}^{10} \\frac{3}{(1.10)^t} = -15 + 3 \\times 6.1446 = +\\yen3.43$ billion.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the cash flow components\n- Initial investment (Year 0): ¥15 billion\n- Annual cash inflow (Years 1-10): ¥3 billion per year\n\nStep 2: Use the present value of annuity formula for the cash flows\nFormula: PV = C × [1 - (1 + r)^-n]/r\nWhere:\nC = ¥3 billion\nr = 10% (0.10)\nn = 10 years\n\nStep 3: Calculate the present value factor\n[1 - (1.10)^-10]/0.10 = (1 - 0.3855)/0.10 = 6.1445\n\nStep 4: Calculate present value of annual cash flows\nPV = ¥3 billion × 6.1445 = ¥18.4335 billion\n\nStep 5: Calculate NPV\nNPV = PV of cash flows - Initial investment\nNPV = ¥18.4335 billion - ¥15 billion = ¥3.4335 billion\n\nFinal NPV under expected scenario: ¥3.43 billion (rounded to two decimal places)\n\nQID: textbook-125-26-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-26-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derived the NPV of the project using the present value of annuity formula and arrived at the same result as the gold answer, ¥3.43 billion, after rounding.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-26-1-0", "category": "CORRECT", "explanation": "The candidate correctly derived the NPV of the project using the present value of annuity formula and arrived at the same result as the gold answer, ¥3.43 billion, after rounding."}, "llm_echoed_qid": "textbook-125-26-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derived the NPV of the project using the present value of annuity formula and arrived at the same result as the gold answer, ¥3.43 billion, after rounding."}
{"script_processing_qid": "textbook-117-5-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the concept of systematic versus unsystematic risk and how diversification affects each.\n\nGOLD_ANSWER:\n1. **Systematic Risk**: Also known as market risk, it is the risk inherent to the entire market and cannot be diversified away. It is represented by the covariance floor in a diversified portfolio.\n2. **Unsystematic Risk**: Unique to individual securities, it can be eliminated through diversification. In a large portfolio, unsystematic risk tends to zero.\n3. **Diversification Impact**:\n   - **Unsystematic Risk**: Reduced as more securities are added.\n   - **Systematic Risk**: Remains constant, as it is non-diversifiable.\n\nCANDIDATE_ANSWER:\nStep 1: Define Systematic Risk\nSystematic (market) risk is the portion of total risk that affects all securities and cannot be eliminated through diversification. It stems from market-wide factors (e.g., macroeconomic events) and is captured by the covariance term in portfolio variance calculations. The context explicitly states that covariance serves as the floor for portfolio variance, meaning this risk persists even with full diversification.\n\nStep 2: Define Unsystematic Risk\nUnsystematic (diversifiable/unique) risk is specific to individual securities or industries. This risk arises from company-specific factors (e.g., management decisions, operational inefficiencies) and can be eliminated by holding a diversified portfolio. The context notes that adding more securities reduces portfolio variance, but only up to the systematic risk floor.\n\nStep 3: Diversification Effects\n- Unsystematic Risk: Diversification reduces unsystematic risk asymptotically. As more securities are added (especially uncorrelated ones), idiosyncratic risks offset each other. The context emphasizes that variance drops but does not reach zero because systematic risk remains.\n- Systematic Risk: Diversification does not reduce systematic risk. The portfolio's remaining risk (covariance floor) represents systematic risk, which is inherent to the market and affects all assets.\n\nStep 4: Mathematical Connection\nThe portfolio variance formula with multiple risky assets (not explicitly shown but implied) would include terms for individual variances (unsystematic risk) and covariances (systematic risk). As diversification increases:\n- The weighted average of individual variances (unsystematic risk) approaches zero.\n- The covariance terms (systematic risk) persist, forming the non-diversifiable risk baseline.\n\nStep 5: Risk-Free Asset Case\nThe provided formulas for a risk-free + risky asset portfolio show that total risk (σ_p) depends solely on the risky asset's weight and its standard deviation. This aligns with the separation principle: systematic risk resides entirely in the optimal risky portfolio (point A), while risk-free asset allocation adjusts total exposure without affecting systematic risk magnitude.\n\nQID: textbook-117-5-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-5-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly defines systematic and unsystematic risk, accurately describes the impact of diversification on each, and aligns with the gold answer. The explanation includes additional details such as the mathematical connection and the risk-free asset case, which enhance understanding without contradicting the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-5-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly defines systematic and unsystematic risk, accurately describes the impact of diversification on each, and aligns with the gold answer. The explanation includes additional details such as the mathematical connection and the risk-free asset case, which enhance understanding without contradicting the gold answer."}, "llm_echoed_qid": "textbook-117-5-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly defines systematic and unsystematic risk, accurately describes the impact of diversification on each, and aligns with the gold answer. The explanation includes additional details such as the mathematical connection and the risk-free asset case, which enhance understanding without contradicting the gold answer."}
{"script_processing_qid": "textbook-122-31-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Why might OLS with SC-robust standard errors be preferred over FGLS in the presence of non-strictly exogenous regressors?\n\nGOLD_ANSWER:\n1. **Consistency**: FGLS is inconsistent if regressors are not strictly exogenous.\n2. **Robustness**: OLS remains consistent, and SC-robust standard errors correct for inefficiency.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the key problem with FGLS under non-strictly exogenous regressors. FGLS requires strict exogeneity (E(u_t|x_t,x_{t-1},...) = 0) for consistency. When regressors are not strictly exogenous (e.g., contain lagged dependent variables), the FGLS transformation (which typically uses quasi-differencing) creates new regressors that become correlated with the transformed error term.\n\nStep 2: Recognize OLS advantages. OLS remains consistent under weaker contemporaneous exogeneity (E(u_t|x_t) = 0), even with serial correlation. The SC-robust standard errors then adjust inference without requiring strict exogeneity or explicit modeling of the serial correlation structure.\n\nStep 3: Compare robustness properties. FGLS requires (1) correctly specified serial correlation model and (2) strict exogeneity. SC-robust OLS only requires (1) contemporaneous exogeneity and (2) weak dependence. When strict exogeneity fails (as implied by the context's focus), FGLS becomes inconsistent, while OLS+SC-robust remains valid for inference.\n\nQID: textbook-122-31-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-31-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains why OLS with SC-robust standard errors is preferred over FGLS in the presence of non-strictly exogenous regressors, matching the gold answer's points about consistency and robustness.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-31-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains why OLS with SC-robust standard errors is preferred over FGLS in the presence of non-strictly exogenous regressors, matching the gold answer's points about consistency and robustness."}, "llm_echoed_qid": "textbook-122-31-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains why OLS with SC-robust standard errors is preferred over FGLS in the presence of non-strictly exogenous regressors, matching the gold answer's points about consistency and robustness."}
{"script_processing_qid": "textbook-105-9-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the significance of the smoothness condition (Definition 3.3) in the context of nonparametric regression. How does it influence the rate of convergence?\n\nGOLD_ANSWER:\n1. **Smoothness Condition**: The $(p,C)$-smoothness condition ensures that the regression function $m$ has bounded partial derivatives up to order $k$ and a Hölder condition of order $\\beta$.\n2. **Influence on Rate**: The smoothness parameter $p = k + \\beta$ directly determines the rate of convergence $n^{-\\frac{2p}{2p+d}}$. Higher smoothness (larger $p$) leads to faster convergence rates.\n3. **Trade-off**: The condition balances the complexity of the function class with the achievable estimation accuracy, ensuring that the estimator can adapt to the underlying smoothness.\n\nCANDIDATE_ANSWER:\nStep 1: The smoothness condition (Definition 3.3) specifies that a function's partial derivatives up to order k exist and are Hölder continuous with exponent β (where p = k + β). This constrains the function's complexity by limiting how rapidly its derivatives can change.\n\nStep 2: In nonparametric regression, convergence rates depend on this smoothness parameter p and dimensionality d. Theorem 3.2 shows the lower minimax rate for class D^{(p,C)} is n^{-2p/(2p+d)}. The exponent 2p/(2p+d) indicates:\n- Higher smoothness p → Faster convergence (since numerator grows faster than denominator)\n- Higher dimensionality d → Slower convergence (curse of dimensionality)\n\nStep 3: The smoothness condition enables this rate by restricting the function class. Without it (or with weaker assumptions), no universal convergence guarantees exist (as implied by Theorem 3.1). The Hölder continuity ensures local regularity, allowing estimators to leverage local averaging without being overwhelmed by high-frequency oscillations.\n\nStep 4: The rate n^{-2p/(2p+d)} balances smoothness and dimensionality: smoother functions (larger p) counteract the negative effect of higher d. This trade-off is fundamental to nonparametric regression's theoretical performance bounds.\n\nQID: textbook-105-9-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-105-9-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the smoothness condition, its role in determining the convergence rate, and the trade-off between smoothness and dimensionality. The reasoning aligns with the gold answer, providing a detailed and correct explanation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-105-9-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the smoothness condition, its role in determining the convergence rate, and the trade-off between smoothness and dimensionality. The reasoning aligns with the gold answer, providing a detailed and correct explanation."}, "llm_echoed_qid": "textbook-105-9-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the smoothness condition, its role in determining the convergence rate, and the trade-off between smoothness and dimensionality. The reasoning aligns with the gold answer, providing a detailed and correct explanation."}
{"script_processing_qid": "textbook-101-2-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the advantages of using a Poisson process $N_{n}^{*}$ over a truncated empirical process $N_{n,D}$ in statistical modeling.\n\nGOLD_ANSWER:\n1. **Independence**: Poisson processes have independent counts for disjoint sets, simplifying analysis.\n2. **Approximation**: Poisson processes provide accurate approximations for rare events.\n3. **Flexibility**: Easier to extend to more complex models and higher dimensions.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key differences in count distributions\n- Truncated empirical process $N_{n,D}$ uses binomial distributed counts ($\\beta \\sim \\text{Bin}(n,Q(D))$)\n- Poisson process $N_n^*$ uses Poisson distributed counts ($\\text{Poisson}(nQ(D))$)\n\nStep 2: Highlight analytical advantages of Poisson distribution\n1) Parameter reduction: Poisson requires only $\\lambda = nQ(D)$ parameter vs binomial's $(n,Q(D))$ pair\n2) Simplifies limit behavior analysis as $n\\to\\infty$\n3) Eliminates upper bound constraints (unlike binomial's $n$ ceiling)\n\nStep 3: Discuss spatial independence properties\n- Poisson processes maintain complete spatial randomness\n- Empirical processes exhibit dependence between points (fixed sample size $n$)\n\nStep 4: Address rare event modeling\n- Poisson approximation becomes exact in sparse domains as $Q(D)\\to 0$, $n\\to\\infty$ with $nQ(D)$ fixed (law of rare events)\n- Avoids truncation artifacts inherent in $Q_D = Q(\\cdot\\cap D)/Q(D)$ normalization\n\nStep 5: Note computational benefits\n- Poisson likelihoods are often easier to handle in estimation\n- Enables explicit variance calculations through Poisson process properties\n- Facilitates compound models through Poisson superposition principles\n\nQID: textbook-101-2-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-101-2-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly covers all advantages mentioned in the gold answer (independence, approximation, flexibility) and provides additional detailed reasoning about parameter reduction, limit behavior, spatial independence, rare event modeling, and computational benefits.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-101-2-2-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly covers all advantages mentioned in the gold answer (independence, approximation, flexibility) and provides additional detailed reasoning about parameter reduction, limit behavior, spatial independence, rare event modeling, and computational benefits."}, "llm_echoed_qid": "textbook-101-2-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly covers all advantages mentioned in the gold answer (independence, approximation, flexibility) and provides additional detailed reasoning about parameter reduction, limit behavior, spatial independence, rare event modeling, and computational benefits."}
{"script_processing_qid": "textbook-125-23-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the formula for the duration of a bond and explain its significance.\n\nGOLD_ANSWER:\n1. **Formula**: The duration ($D$) of a bond is given by:\n   $$ D = \\frac{\\sum_{t=1}^T t \\cdot \\frac{C_t}{(1 + r)^t}}{PV} $$\n   where:\n   - $C_t$ is the cash flow at time $t$.\n   - $r$ is the yield to maturity.\n   - $PV$ is the present value of the bond.\n2. **Significance**: Duration measures the sensitivity of a bond's price to interest rate changes. It is a weighted average of the times until cash flows are received, with weights being the present values of the cash flows.\n\nCANDIDATE_ANSWER:\nStep 1: Define bond price as the present value of cash flows.\nA bond's price (P) is calculated as: \nP = Σ [C_t / (1 + y)^t] + F / (1 + y)^T\nwhere C_t = coupon payment at time t, F = face value, y = yield, and T = maturity.\n\nStep 2: Calculate the sensitivity of bond price to yield changes\nTake the derivative of P with respect to y:\ndP/dy = Σ [-t · C_t / (1 + y)^{t+1}] + [-T · F / (1 + y)^{T+1}]\n\nStep 3: Express derivative as a weighted average\nFactor out 1/(1 + y):\ndP/dy = -1/(1 + y) · Σ [t · C_t / (1 + y)^t] + [T · F / (1 + y)^T]\n\nStep 4: Derive duration formula\nDivide both sides by -P and multiply by (1 + y):\nDuration = (1/P) · Σ [t · C_t / (1 + y)^t] + [T · F / (1 + y)^T]\nThis matches the definition of duration as the weighted average time to discounted cash flows.\n\nSignificance:\n1. Measures interest rate risk: Duration quantifies price sensitivity to yield changes (≈%ΔP ≈ -Duration × Δy).\n2. Portfolio management: Used for immunization against interest rate fluctuations.\n3. Comparative tool: Allows comparison of bonds with different maturities/coupons.\n\nQID: textbook-125-23-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-23-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derived the formula for bond duration and explained its significance. The steps align with the gold answer, and the explanation of duration's significance is accurate and comprehensive.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-23-1-0", "category": "CORRECT", "explanation": "The candidate correctly derived the formula for bond duration and explained its significance. The steps align with the gold answer, and the explanation of duration's significance is accurate and comprehensive."}, "llm_echoed_qid": "textbook-125-23-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derived the formula for bond duration and explained its significance. The steps align with the gold answer, and the explanation of duration's significance is accurate and comprehensive."}
{"script_processing_qid": "textbook-125-28-3-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Calculate the present value of operating costs for two old machines, given an annual operating cost of $\\$1,500$ per machine and a discount rate of 10%.\n\nGOLD_ANSWER:\n1. **PV Calculation**: \n   - $PV = \\frac{1,500}{0.10} = 15,000$ per machine.\n   - For two machines: $2 \\times 15,000 = \\$30,000$.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate total annual operating cost for both old machines.\n$1,500/machine × 2 machines = $3,000/year\n\nStep 2: Use perpetuity formula for present value since operating costs are ongoing (no specified end date).\nPV = Annual Cost ÷ Discount Rate\nPV = $3,000 ÷ 0.10 = $30,000\n\nConclusion: The present value of operating costs for both old machines is $30,000.\n\nQID: textbook-125-28-3-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-28-3-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly calculated the present value of operating costs for both machines using the perpetuity formula, matching the gold answer's result of $30,000.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-28-3-0", "category": "CORRECT", "explanation": "The candidate correctly calculated the present value of operating costs for both machines using the perpetuity formula, matching the gold answer's result of $30,000."}, "llm_echoed_qid": "textbook-125-28-3-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly calculated the present value of operating costs for both machines using the perpetuity formula, matching the gold answer's result of $30,000."}
{"script_processing_qid": "textbook-114-10-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the market demand curve for a good exhibiting a bandwagon effect, given individual demand curves $D\\_{20}$, $D\\_{40}$, $D\\_{60}$, $D\\_{80}$, and $D\\_{100}$ corresponding to quantities 20,000; 40,000; 60,000; 80,000; and 100,000. Explain how the bandwagon effect influences the elasticity of the market demand curve.\n\nGOLD_ANSWER:\n1.  **Market Demand Curve Derivation**: The market demand curve is constructed by connecting the points on the individual demand curves $D\\_{20}$, $D\\_{40}$, $D\\_{60}$, $D\\_{80}$, and $D\\_{100}$ that correspond to the quantities 20,000; 40,000; 60,000; 80,000; and 100,000. This results in a curve that shifts rightward as more people purchase the good.\n2.  **Bandwagon Effect on Elasticity**: The bandwagon effect increases the elasticity of the market demand curve. As the price falls, more people buy the good, making it more desirable and further increasing demand. This amplifies the quantity response to price changes, making the demand curve more elastic.\n\nCANDIDATE_ANSWER:\nStep 1: Understand that under the bandwagon effect, each individual demand curve ($D_{20}$, $D_{40}$, etc.) represents consumer behavior at different expected adoption levels (20,000; 40,000; ... units).\n\nStep 2: The market demand curve is derived by connecting equilibrium points where the quantity demanded matches the expected adoption level for each price. For example:\n- At price $P_1$, $D_{20}$ predicts 20,000 units demanded (equilibrium).\n- At price $P_2$, $D_{40}$ predicts 40,000 units demanded (equilibrium).\nContinue this for all given demand curves.\n\nStep 3: Plot these equilibrium points (price vs. quantity) and connect them to form the market demand curve. The curve will be flatter than standard demand curves due to the bandwagon effect.\n\nStep 4: Analyze elasticity: The bandwagon effect increases price elasticity because lower prices attract more buyers, which further amplifies demand through social influence. This creates a positive feedback loop, making the market demand curve more responsive to price changes (i.e., more elastic) compared to a standard demand curve.\n\nQID: textbook-114-10-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-10-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the market demand curve by connecting equilibrium points from individual demand curves, and accurately explains how the bandwagon effect increases elasticity. This aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-10-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the market demand curve by connecting equilibrium points from individual demand curves, and accurately explains how the bandwagon effect increases elasticity. This aligns with the gold answer."}, "llm_echoed_qid": "textbook-114-10-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the market demand curve by connecting equilibrium points from individual demand curves, and accurately explains how the bandwagon effect increases elasticity. This aligns with the gold answer."}
{"script_processing_qid": "textbook-117-16-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. For Project C with cash flows $(-\\$100, \\$230, -\\$132)$, verify that the IRRs are 10% and 20%. Why does this project have multiple IRRs?\n\nGOLD_ANSWER:\n1. **IRR Verification**:\n   - At 10%: $-100 + \\frac{230}{1.1} - \\frac{132}{(1.1)^2} = 0$.\n   - At 20%: $-100 + \\frac{230}{1.2} - \\frac{132}{(1.2)^2} = 0$.\n2. **Multiple IRRs**: The cash flows change signs twice (negative to positive to negative), leading to two IRRs. This violates the assumption of a single IRR for projects with only one sign change.\n\nCANDIDATE_ANSWER:\nStep 1: Set up NPV equation for Project C:\nNPV = -100 + 230/(1+r) - 132/(1+r)^2 = 0\n\nStep 2: Multiply through by (1+r)^2 to eliminate denominators:\n-100(1+r)^2 + 230(1+r) - 132 = 0\n\nStep 3: Expand and simplify:\n-100(1 + 2r + r²) + 230 + 230r - 132 = 0\n-100 - 200r -100r² + 230 + 230r -132 = 0\n-100r² + 30r - 2 = 0\n\nStep 4: Solve quadratic equation 100r² -30r +2=0:\nUsing quadratic formula: r = [30 ± √(900 - 800)]/(2*100)\nr = [30 ± 10]/200 ⇒ 40/200=20% or 20/200=10%\n\nStep 5: Multiple IRRs occur because:\n- Cash flow signs change twice (- → + → -)\n- The Fundamental Theorem of Algebra allows up to as many real roots as sign changes\n- This makes IRR unreliable for projects with multiple sign reversals\n\nQID: textbook-117-16-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-16-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly verifies the IRRs as 10% and 20% through detailed calculations and accurately explains the reason for multiple IRRs due to the cash flow sign changes. The reasoning and answer match the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-16-0-2", "category": "CORRECT", "explanation": "The candidate correctly verifies the IRRs as 10% and 20% through detailed calculations and accurately explains the reason for multiple IRRs due to the cash flow sign changes. The reasoning and answer match the gold answer."}, "llm_echoed_qid": "textbook-117-16-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly verifies the IRRs as 10% and 20% through detailed calculations and accurately explains the reason for multiple IRRs due to the cash flow sign changes. The reasoning and answer match the gold answer."}
{"script_processing_qid": "textbook-97-8-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Using Lemma 6.12, show how to construct an orthogonal array $O A(144,6,12,2)$ from the difference scheme $D(12,6,12)$ in Table 6.11.\n\nGOLD_ANSWER:\n4. **Construction**: Let $D$ be the difference scheme $D(12,6,12)$. Develop $D$ into an orthogonal array by juxtaposing $D\\_{0}, D\\_{1}, \\ldots, D\\_{11}$, where $D\\_{i}$ is obtained by adding $\\sigma\\_{i}$ to each entry of $D$. The resulting array $A$ is an $O A(144,6,12,2)$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify parameters from Lemma 6.12 and the given difference scheme.\n- Difference scheme D(r,c,s) = D(12,6,12) implies r=12 rows, c=6 columns, s=12 symbols.\n- Lemma 6.12 states that stacking s copies of D (with group operation shifts) forms an OA(rs, c, s, 2).\n\nStep 2: Calculate orthogonal array parameters:\n- Total runs = r*s = 12*12 = 144\n- Columns retained from D = 6\n- Symbols per factor = s = 12\n- Strength = 2 (as specified in lemma)\n\nStep 3: Construction method:\n1. Take the base difference scheme D(12,6,12) from Table 6.11\n2. For each element i in the additive group (0 to 11), create a copy D_i by adding i (mod 12) to every entry in D\n3. Vertically stack all 12 modified copies (D₀, D₁, ..., D₁₁)\n\nResult: This produces a 144×6 array with 12 symbols in each column that satisfies the orthogonal array property OA(144,6,12,2) by Lemma 6.12.\n\nQID: textbook-97-8-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-97-8-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly follows the construction method outlined in Lemma 6.12, accurately calculates the parameters, and provides a clear step-by-step process to construct the orthogonal array OA(144,6,12,2) from the given difference scheme.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-97-8-0-3", "category": "CORRECT", "explanation": "The candidate answer correctly follows the construction method outlined in Lemma 6.12, accurately calculates the parameters, and provides a clear step-by-step process to construct the orthogonal array OA(144,6,12,2) from the given difference scheme."}, "llm_echoed_qid": "textbook-97-8-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly follows the construction method outlined in Lemma 6.12, accurately calculates the parameters, and provides a clear step-by-step process to construct the orthogonal array OA(144,6,12,2) from the given difference scheme."}
{"script_processing_qid": "textbook-111-19-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the IS-LM framework, analyze the effects of expansionary fiscal policy (e.g., $\\Delta G > 0$) on output and interest rates. Assume the LM curve is upward-sloping.\n\nGOLD_ANSWER:\n1. **Initial Effect**: $\\Delta G$ shifts the IS curve right, increasing $Y$ and $r$.  \n2. **Crowding Out**: Higher $r$ reduces private investment, partially offsetting the $\\Delta G$ effect.  \n3. **Net Result**: $Y$ increases, but by less than the Keynesian cross multiplier due to the rise in $r$.\n\nCANDIDATE_ANSWER:\nStep 1: Expansionary fiscal policy (e.g., ∆G > 0) increases aggregate demand, shifting the IS curve to the right. This is because higher government spending directly raises output (Y) at any given interest rate (r).\n\nStep 2: With an upward-sloping LM curve, the increase in Y (from the IS shift) raises money demand. To maintain equilibrium in the money market (where real money supply is fixed), the interest rate (r) must rise to reduce money demand back to equilibrium levels.\n\nStep 3: The rise in r partially crowds out private investment (due to the inverse relationship between r and investment), dampening the initial increase in Y. However, the net effect remains an increase in both Y and r compared to the original equilibrium.\n\nResult: Output (Y) and interest rates (r) both increase, but the increase in Y is smaller than it would be in the absence of crowding out (i.e., if the LM curve were horizontal).\n\nQID: textbook-111-19-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-19-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes the IS-LM framework's response to expansionary fiscal policy, including the rightward IS shift, the rise in interest rates due to money market equilibrium, and the partial crowding out effect, matching the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-19-2-2", "category": "CORRECT", "explanation": "The candidate's answer accurately describes the IS-LM framework's response to expansionary fiscal policy, including the rightward IS shift, the rise in interest rates due to money market equilibrium, and the partial crowding out effect, matching the gold answer's key points."}, "llm_echoed_qid": "textbook-111-19-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes the IS-LM framework's response to expansionary fiscal policy, including the rightward IS shift, the rise in interest rates due to money market equilibrium, and the partial crowding out effect, matching the gold answer's key points."}
{"script_processing_qid": "textbook-125-19-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain how leveraged leases work and the potential benefits and risks for the lessor and lessee.\n\nGOLD_ANSWER:\n3. **Leveraged Leases**:\n   - **Lessor**: Borrows part of the asset's cost, reducing equity investment. Risks include default on the loan.\n   - **Lessee**: Benefits from lower lease payments due to lessor's tax shields. Risks include lessor's financial instability affecting lease terms.\n\nCANDIDATE_ANSWER:\nStep 1: Definition of Leveraged Lease\nA leveraged lease is a financial lease where the lessor borrows part of the purchase price of the leased asset, using the lease contract as collateral for the loan. This allows the lessor to finance the asset with partial debt.\n\nStep 2: Benefits for the Lessor\n- Requires less upfront capital investment (only equity portion of the asset's cost)\n- Earns lease payments while leveraging debt financing\n- Retains ownership and potential tax benefits (implied by financial lease structure)\n\nStep 3: Risks for the Lessor\n- Debt obligation remains even if lessee defaults\n- Asset repossession risk if loan repayments are missed\n- Reduced cash flow flexibility due to mandatory debt payments\n\nStep 4: Benefits for the Lessee\n- Access to assets without full upfront purchase cost\n- Fixed payment schedule similar to regular financial leases\n- No direct responsibility for the lessor's debt (contract remains with lessor)\n\nStep 5: Risks for the Lessee\n- Potential asset repossession if lessor defaults on their loan\n- Obligation to maintain/insure/pay taxes (as it's typically a net lease)\n- Less flexibility due to the lessor's leveraged position affecting lease terms\n\nQID: textbook-125-19-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-19-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains leveraged leases, detailing benefits and risks for both lessor and lessee, aligning closely with the gold answer. The structured breakdown enhances clarity and completeness.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-19-0-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains leveraged leases, detailing benefits and risks for both lessor and lessee, aligning closely with the gold answer. The structured breakdown enhances clarity and completeness."}, "llm_echoed_qid": "textbook-125-19-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains leveraged leases, detailing benefits and risks for both lessor and lessee, aligning closely with the gold answer. The structured breakdown enhances clarity and completeness."}
{"script_processing_qid": "textbook-122-31-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Why is testing for serial correlation a prerequisite for testing heteroskedasticity in time series?\n\nGOLD_ANSWER:\n1. **Invalidation**: Serial correlation in $\\{\\nu_{t}\\}$ biases heteroskedasticity tests.\n2. **Order of Tests**: First test for serial correlation using robust methods, then for heteroskedasticity.\n\nCANDIDATE_ANSWER:\nStep 1: The Breusch-Pagan test for heteroskedasticity assumes the error term ($\\nu_t$) in its auxiliary regression $u_t^2 = \\delta_0 + \\delta_1 x_{t1} + \\ldots + \\delta_k x_{tk} + \\nu_t$ must be serially uncorrelated for valid inference.\\n\\nStep 2: Serial correlation in $\\nu_t$ would violate this assumption, potentially biasing test statistics (e.g., LM statistic) and leading to incorrect conclusions about heteroskedasticity.\\n\\nStep 3: Since time series data inherently involves sequential observations, serial correlation must be ruled out first to ensure the Breusch-Pagan test's validity. Testing for serial correlation ensures the assumptions of the subsequent heteroskedasticity test are not compromised.\n\nQID: textbook-122-31-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-31-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains that serial correlation in the error term can bias heteroskedasticity tests, such as the Breusch-Pagan test, and emphasizes the need to test for serial correlation first. This aligns with the gold answer's points about invalidation and the order of tests.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-31-1-1", "category": "CORRECT", "explanation": "The candidate answer correctly explains that serial correlation in the error term can bias heteroskedasticity tests, such as the Breusch-Pagan test, and emphasizes the need to test for serial correlation first. This aligns with the gold answer's points about invalidation and the order of tests."}, "llm_echoed_qid": "textbook-122-31-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains that serial correlation in the error term can bias heteroskedasticity tests, such as the Breusch-Pagan test, and emphasizes the need to test for serial correlation first. This aligns with the gold answer's points about invalidation and the order of tests."}
{"script_processing_qid": "textbook-101-2-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Compare the bounds obtained using variational distance and Hellinger distance for the Poisson approximation of a binomial distribution.\n\nGOLD_ANSWER:\n1. **Variational Bound**: $\\|B_{(n,s/n)}-P_{s}\\|\\leq s/n$.\n2. **Hellinger Bound**: $H(B_{(n,s/n)},P_{s})\\leq (s^{2}/2n + s^{3}/6n^{2})^{1/2}$.\n3. **Comparison**: The Hellinger bound is tighter for small $s/n$, as it grows quadratically rather than linearly.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the variational distance bound from the context. The variational distance bound for the Poisson approximation of the binomial distribution B(600, s/600) is given as s/600 = 0.61/600 ≈ 0.0010.\nStep 2: Check for Hellinger distance information in the context. The context explicitly states that the section explores both variational and Hellinger distances but does not provide the specific Hellinger distance bound calculation or numerical value for this example.\nStep 3: Conclude comparison limitations. While the variational bound is quantified as 0.0010, the provided context lacks sufficient information about the Hellinger distance bound (e.g., no theorem reference or formula for Hellinger distance in the extracted text). Thus, a direct comparison between the two bounds cannot be made based on the given material.\n\nQID: textbook-101-2-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-101-2-1-1\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate answer incorrectly concludes that a direct comparison cannot be made due to lack of information. The GOLD_ANSWER provides explicit formulas for both bounds, allowing for a comparison. The candidate's reasoning is flawed as it misinterprets the availability of necessary information.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-101-2-1-1", "category": "INCORRECT", "explanation": "The candidate answer incorrectly concludes that a direct comparison cannot be made due to lack of information. The GOLD_ANSWER provides explicit formulas for both bounds, allowing for a comparison. The candidate's reasoning is flawed as it misinterprets the availability of necessary information."}, "llm_echoed_qid": "textbook-101-2-1-1", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate answer incorrectly concludes that a direct comparison cannot be made due to lack of information. The GOLD_ANSWER provides explicit formulas for both bounds, allowing for a comparison. The candidate's reasoning is flawed as it misinterprets the availability of necessary information."}
{"script_processing_qid": "textbook-70-2-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the role of $B\\_{n}$ in the asymptotic distribution of $\\frac{\\hat{r}(x)-r(x)}{\\sqrt{\\mathrm{Var}[\\hat{r}(x)]}}$.\n\nGOLD_ANSWER:\n$B\\_{n}$ represents the standardized bias term $\\frac{E[\\hat{r}(x)]-r(x)}{\\sqrt{\\operatorname{Var}[\\hat{r}(x)]}}$. If $B\\_{n}$ is not negligible (e.g., due to suboptimal smoothing parameter choice), the asymptotic distribution becomes $N(B, 1)$, where $B = \\lim_{n\\to\\infty} B\\_{n}$, complicating inference.\n\nCANDIDATE_ANSWER:\nStep 1: Examine the decomposition of the standardized estimator:\\nThe quantity $\\frac{\\hat{r}(x)-r(x)}{\\sqrt{\\mathrm{Var}[\\hat{r}(x)]}}$ is decomposed into $Z_n + B_n$, where:\\n- $Z_n$ represents the standardized deviation of the estimator from its expectation\\n- $B_n = \\frac{E[\\hat{r}(x)] - r(x)}{\\sqrt{\\mathrm{Var}[\\hat{r}(x)]}}$ captures the standardized bias\\n\\nStep 2: Understand Theorem 3.6 requirements:\\nThe theorem establishes asymptotic normality ($\\xrightarrow{d} N(0,1)$) under the condition that:\\n1. $Z_n \\xrightarrow{d} N(0,1)$\\n2. $B_n \\rightarrow 0$ in probability\\n\\nStep 3: Role of $B_n$:\\n$B_n$ quantifies the bias-to-standard-error ratio. For valid asymptotic inference:\\n- $B_n$ must vanish asymptotically ($B_n \\rightarrow 0$)\\n- If $B_n \\nrightarrow 0$, the asymptotic distribution would have a non-centrality parameter\\n- This bias term disappears under the theorem's conditions (proper bandwidth selection and design density requirements)\\n\\nStep 4: Practical implication:\\nThe requirement $B_n \\rightarrow 0$ ensures that the smoothing bias becomes negligible compared to the estimator's variability as $n \\rightarrow \\infty$, enabling standard normal-based inference.\n\nQID: textbook-70-2-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-70-2-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies $B_n$ as the standardized bias term and explains its role in the asymptotic distribution. It also accurately describes the conditions under which the asymptotic normality holds, matching the gold answer's explanation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-70-2-0-1", "category": "CORRECT", "explanation": "The candidate answer correctly identifies $B_n$ as the standardized bias term and explains its role in the asymptotic distribution. It also accurately describes the conditions under which the asymptotic normality holds, matching the gold answer's explanation."}, "llm_echoed_qid": "textbook-70-2-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies $B_n$ as the standardized bias term and explains its role in the asymptotic distribution. It also accurately describes the conditions under which the asymptotic normality holds, matching the gold answer's explanation."}
{"script_processing_qid": "textbook-62-4-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain the intuition behind the 'rule of 70' for doubling time and derive it mathematically.\n\nGOLD_ANSWER:\n1. **Intuition**: The rule approximates the time to double as $70$ divided by the growth rate (in percentage).\n2. **Derivation**: Start from $t = \\frac{\\log(2)}{\\log(1 + g)}$.\n3. **Approximation**: For small $g$, $\\log(1 + g) \\approx g$ (using Taylor series).\n4. **Result**: $t \\approx \\frac{0.693}{g} \\approx \\frac{70}{g\\%}$.\n\nCANDIDATE_ANSWER:\nStep 1: The 'Rule of 70' approximates doubling time (t) for a variable growing at rate g% using t ≈ 70/g. \nStep 2: Start with the exact doubling time formula from the context: t = log(2)/log(1+g).\nStep 3: Use the approximation log(1+g) ≈ g when g is small (e.g., g = 0.05 for 5%). This simplifies the denominator.\nStep 4: Natural logarithm version: ln(2)/ln(1+g) ≈ ln(2)/g ≈ 0.693/g.\nStep 5: Convert g to percentage terms (e.g., 5% = 0.05 → 5). Multiply numerator/denominator by 100: 69.3/(g_percentage).\nStep 6: Round 69.3 to 70 for simplicity, yielding t ≈ 70/g_percentage. This matches the intuitive 'Rule of 70'.\n\nQID: textbook-62-4-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-62-4-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly explains the intuition behind the 'rule of 70' and accurately derives it mathematically, matching the steps and reasoning provided in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-62-4-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly explains the intuition behind the 'rule of 70' and accurately derives it mathematically, matching the steps and reasoning provided in the gold answer."}, "llm_echoed_qid": "textbook-62-4-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly explains the intuition behind the 'rule of 70' and accurately derives it mathematically, matching the steps and reasoning provided in the gold answer."}
{"script_processing_qid": "textbook-97-6-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of the existence of subfields in the study of field extensions.\n\nGOLD_ANSWER:\n1. **Hierarchy**: Subfields provide a hierarchical structure to field extensions.\n2. **Applications**: Useful in coding theory, cryptography, and error-correcting codes.\n3. **Example**: In $GF(2^6)$, subfields $GF(2^2)$ and $GF(2^3)$ correspond to divisors $2$ and $3$ of $6$.\n\nCANDIDATE_ANSWER:\nStep 1: Structural Hierarchy - The existence of subfields in a Galois field GF(pⁿ) implies a hierarchical structure where each divisor m of n corresponds to a unique subfield GF(pᵐ). This creates a lattice of intermediate fields, reflecting the divisibility properties of n. Step 2: Simplification of Analysis - The uniqueness of subfields for each divisor simplifies the study of field extensions by decomposing GF(pⁿ) into smaller, well-defined subfields. Step 3: Automorphism Implications - Subfields are preserved under automorphisms (isomorphisms mapping a field to itself), which is foundational to Galois theory. Step 4: Applications - This structure aids in solving problems in coding theory, cryptography, and polynomial factorization by leveraging predictable subfield relationships. Step 5: Classification - The correspondence between subfields and divisors of n provides a complete classification of intermediate fields, eliminating ambiguity.\n\nQID: textbook-97-6-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-97-6-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly covers the implications of subfields in field extensions, including hierarchy, simplification of analysis, automorphism implications, applications, and classification, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-97-6-2-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly covers the implications of subfields in field extensions, including hierarchy, simplification of analysis, automorphism implications, applications, and classification, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-97-6-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly covers the implications of subfields in field extensions, including hierarchy, simplification of analysis, automorphism implications, applications, and classification, aligning well with the gold answer."}
{"script_processing_qid": "textbook-109-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Compare and contrast 'friendly takeovers' and 'hostile takeovers' in terms of their objectives, procedures, and implications for target firm management.\n\nGOLD_ANSWER:\n1.  **Friendly Takeovers**:\n    - **Objective**: Synergistic gains (e.g., R&D consolidation, market network integration).\n    - **Procedure**: Requires approval from target management (board of directors).\n    - **Implications**: Collaborative; often preserves managerial roles.\n2.  **Hostile Takeovers**:\n    - **Objective**: Disciplinary, correcting non-value-maximizing practices.\n    - **Procedure**: Direct offer to shareholders, bypassing management.\n    - **Implications**: Often leads to management turnover; aligns firm with shareholder interests.\n\nCANDIDATE_ANSWER:\nStep 1: Compare Objectives\n- **Friendly Takeovers**: Aim to combine businesses to realize synergistic benefits (e.g., operational efficiencies, market expansion).\n- **Hostile Takeovers**: Intend to discipline target management by correcting non-value-maximizing practices (e.g., poor governance).\n\nStep 2: Contrast Procedures\n- **Friendly**: Negotiated through mutual agreement between bidder and target management. Involves established legal procedures and tender offers if required.\n- **Hostile**: Bidder bypasses target management, directly approaching shareholders via tender offers. Target management may deploy defenses like poison pills to block the bid.\n\nStep 3: Implications for Target Management\n- **Friendly**: Management collaborates with the bidder; roles may remain intact post-takeover.\n- **Hostile**: Management risks replacement if the takeover succeeds. Defensive actions (e.g., poison pills) are used to retain control, but successful hostile bids often lead to managerial overhaul.\n\nKey Contrast: Friendly takeovers emphasize cooperation for mutual gain, while hostile takeovers focus on correcting managerial inefficiencies through confrontation.\n\nQID: textbook-109-1-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-109-1-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately compares and contrasts friendly and hostile takeovers in terms of objectives, procedures, and implications for target firm management, aligning closely with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-109-1-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately compares and contrasts friendly and hostile takeovers in terms of objectives, procedures, and implications for target firm management, aligning closely with the gold answer."}, "llm_echoed_qid": "textbook-109-1-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately compares and contrasts friendly and hostile takeovers in terms of objectives, procedures, and implications for target firm management, aligning closely with the gold answer."}
{"script_processing_qid": "textbook-111-12-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Using the data from Table 14-2, calculate the average real GDP growth rate for the second year of Republican and Democratic administrations. What statistical test could you use to determine if the difference is significant?\n\nGOLD_ANSWER:\n1. **Republican Average**: ( -0.7 + -1.0 + 0.2 + -0.6 + -2.0 + 3.4 + 1.8 ) / 7 = 0.2% (as per the table).\n2. **Democratic Average**: ( 8.9 + 6.0 + 6.6 + 5.5 + 4.0 + 4.4 ) / 6 = 5.9%.\n3. **Statistical Test**: A two-sample t-test could be used to determine if the difference in means is statistically significant, assuming the data is normally distributed and variances are equal.\n\nCANDIDATE_ANSWER:\nStep 1: To calculate average real GDP growth rates for second-year Republican/Democratic administrations:\n1. Collect all second-year GDP growth values from Table 14-2\n2. Separate them into Republican and Democratic administration groups\n3. Calculate mean for each group: (Sum of values)/(Number of observations)\n\nStep 2: For statistical significance testing:\nA two-sample t-test would be appropriate to compare means between:\n- Null hypothesis (H₀): μ_Republican = μ_Democratic\n- Alternative hypothesis (H₁): μ_Republican ≠ μ_Democratic\n\nKey considerations:\n1. Check normality assumption using Shapiro-Wilk test\n2. Verify equal variances with Levene's test\n3. Use Welch's t-test if variances are unequal\n4. Non-parametric alternative: Mann-Whitney U test if normality fails\n\nNote: Actual calculation requires numerical data from Table 14-2 (not provided in context). The described pattern of Republicans having more negative growth in years 2-3 vs. Democrats' growth surges aligns with the need for this test.\n\nQID: textbook-111-12-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-12-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly outlines the steps for calculating the average GDP growth rates and appropriately suggests a two-sample t-test for significance testing, including considerations for assumptions and alternatives. This aligns perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-12-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly outlines the steps for calculating the average GDP growth rates and appropriately suggests a two-sample t-test for significance testing, including considerations for assumptions and alternatives. This aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-111-12-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly outlines the steps for calculating the average GDP growth rates and appropriately suggests a two-sample t-test for significance testing, including considerations for assumptions and alternatives. This aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-117-22-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. What are the six IRS guidelines that determine whether a lease contract qualifies for tax purposes?\n\nGOLD_ANSWER:\n1. **Term of Lease**: Must be less than 30 years; otherwise, it is considered a conditional sale.\n2. **Bargain Option**: No option to acquire the asset below fair market value, as this implies an equity interest.\n3. **Payment Schedule**: No high initial payments followed by very low payments, as this suggests tax avoidance.\n4. **Fair Market Return**: Lease payments must provide the lessor with a fair market rate of return, independent of tax benefits.\n5. **No Restrictions**: The lease must not limit the lessee’s right to issue debt or pay dividends.\n6. **Renewal Options**: Must be reasonable and reflect fair market value, often by granting the lessee the first option to meet competing offers.\n\nCANDIDATE_ANSWER:\nStep 1: The lease term must be less than 30 years. Leases exceeding 30 years are classified as conditional sales.\nStep 2: The lease cannot include a bargain purchase option below fair market value, as this would imply equity interest.\nStep 3: Lease payments cannot be structured with disproportionately high upfront payments (balloon payments) followed by low payments, which could indicate tax avoidance.\nStep 4: Lease payments must ensure the lessor earns a fair market rate of return, independent of tax benefits.\nStep 5: The lease must not restrict the lessee’s ability to issue debt or pay dividends during the lease term.\nStep 6: Renewal options must be reasonable and reflect the asset’s fair market value, typically by allowing the lessee to match competing offers.\n\nQID: textbook-117-22-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-22-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately and comprehensively addresses all six IRS guidelines for lease contracts, matching the gold answer in both content and structure.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-22-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately and comprehensively addresses all six IRS guidelines for lease contracts, matching the gold answer in both content and structure."}, "llm_echoed_qid": "textbook-117-22-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately and comprehensively addresses all six IRS guidelines for lease contracts, matching the gold answer in both content and structure."}
{"script_processing_qid": "textbook-93-1-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Prove that $\\tau_{1}(P)$ is a proper coefficient of ergodicity.\n\nGOLD_ANSWER:\n1. **Continuity**: $\\tau_{1}(P)$ is continuous on the set of stochastic matrices.\n2. **Bounds**: $0 \\leq \\tau_{1}(P) \\leq 1$.\n3. **Properness**: $\\tau_{1}(P) = 0$ if and only if $P = 1v^{\\prime}$, i.e., $P$ is stable.\n4. **Proof**: If $P$ is stable, all rows are identical, so $\\tau_{1}(P) = 0$. Conversely, if $\\tau_{1}(P) = 0$, then all rows must be identical, implying $P$ is stable.\n\nCANDIDATE_ANSWER:\nStep 1: Recall Definition 4.6 - A proper coefficient of ergodicity τ(P) must satisfy:\n(a) 0 ≤ τ(P) ≤ 1\n(b) Continuity over stochastic matrices\n(c) τ(P) = 0 ⇨ P = 1v' for some probability vector v\n\nStep 2: Verify τ₁(P) meets these requirements:\n1. Range proof:\n   - For any row i: ∑ₛ|p_is - p_js| ≤ 2 (from triangle inequality)\n   - τ₁(P) = ½ max_{i,j} ∑ₛ|p_is - p_js| ⇒ 0 ≤ τ₁(P) ≤ 1\n\n2. Continuity:\n   - Absolute value and min/max operations preserve continuity\n   - Summation over finite dimensions maintains continuity\n\n3. Properness condition:\n   - If P = 1v', rows are identical ⇒ τ₁(P) = 0\n   - If τ₁(P) = 0 ⇒ max_{i,j} ∑ₛ|p_is - p_js| = 0 ⇒ all rows equal ⇒ P = 1v'\n\nStep 3: Confirm through equivalent expression:\nτ₁(P) = 1 - min_{i,j} ∑ₛ min(p_is, p_js)\n- ∑ₛ min(p_is, p_js) reaches maximum 1 when rows are identical\n- Difference from 1 preserves 0 ⇨ identity condition\n\nConclusion: τ₁(P) satisfies all requirements of a proper coefficient of ergodicity as per Definition 4.6.\n\nQID: textbook-93-1-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-93-1-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly verifies that τ₁(P) satisfies all the requirements of a proper coefficient of ergodicity, including range, continuity, and properness conditions, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-93-1-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly verifies that τ₁(P) satisfies all the requirements of a proper coefficient of ergodicity, including range, continuity, and properness conditions, aligning with the gold answer."}, "llm_echoed_qid": "textbook-93-1-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly verifies that τ₁(P) satisfies all the requirements of a proper coefficient of ergodicity, including range, continuity, and properness conditions, aligning with the gold answer."}
{"script_processing_qid": "textbook-105-4-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Interpret the denominator $2\\sigma^2 + 2\\epsilon(b-a)/3$ in Bernstein's inequality.\n\nGOLD_ANSWER:\nThe term $2\\sigma^2$ captures the Gaussian-like behavior for small $\\epsilon$, while $2\\epsilon(b-a)/3$ accounts for the sub-exponential tail due to the bounded support. The inequality interpolates between Gaussian and exponential tails.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that Bernstein's inequality denominator combines variance and range effects.\\nStep 2: The term \\(2\\sigma^2\\) reflects the cumulative variance of the bounded random variables. This dominates for small deviations \\( \\epsilon \\), producing Gaussian-like \\(e^{-\\epsilon^2/\\sigma^2}\\) decay.\\nStep 3: The term \\(2\\epsilon(b-a)/3\\) accounts for the bounded support \\([a,b]\\). This linear \\( \\epsilon \\)-term dominates for larger deviations, producing sub-exponential \\(e^{-\\epsilon/(b-a)}\\) decay.\\nStep 4: The denominator structure demonstrates Bernstein's interpolation between sub-Gaussian (variance-dominated) and sub-exponential (range-dominated) regimes.\\nStep 5: The factor \\(\\frac{1}{3}\\) arises from technical aspects of Bernstein's proof (specifically, bounding exponential moments using \\(e^x \\leq 1 + x + x^2/2(1 - |x|/3)\\) for |x| < 3).\n\nQID: textbook-105-4-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-105-4-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains both components of the denominator, their roles in Bernstein's inequality, and the interpolation between Gaussian and exponential tails, matching the gold answer's intent while providing additional technical detail.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-105-4-1-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains both components of the denominator, their roles in Bernstein's inequality, and the interpolation between Gaussian and exponential tails, matching the gold answer's intent while providing additional technical detail."}, "llm_echoed_qid": "textbook-105-4-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains both components of the denominator, their roles in Bernstein's inequality, and the interpolation between Gaussian and exponential tails, matching the gold answer's intent while providing additional technical detail."}
{"script_processing_qid": "textbook-115-20-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the $F$-statistic in equation (7.53) is preferred over the chi-square statistic for finite-sample inference.\n\nGOLD_ANSWER:\nThe $F$-statistic adjusts for finite-sample variability by dividing the chi-square statistic by $Q$ and scaling it with $(NG-K)^{-1}\\sum_{i=1}^{N}\\hat{\\mathbf{u}}_{i}^{\\prime}\\hat{\\Omega}^{-1}\\hat{\\mathbf{u}}_{i}$. This improves the approximation to the $F_{Q,NG-K}$ distribution, yielding better finite-sample properties.\n\nCANDIDATE_ANSWER:\nStep 1: The $F$-statistic adjusts for finite-sample uncertainty in variance estimation. While the chi-square statistic in (7.52) relies on asymptotic approximations, the $F$-statistic explicitly accounts for sample size and degrees of freedom through its denominator.\n\nStep 2: In finite samples, the chi-square distribution assumes all parameters (including error variances) are known, which is unrealistic. The $F$-distribution accounts for estimation error in the residual covariance matrix $\\hat{\\Omega}$ used in FGLS/SUR.\n\nStep 3: The $F$-statistic numerator contains the same chi-square-form quadratic form as (7.52), but divides it by $Q$ (number of restrictions) and uses an $F_{Q,N-K}$ distribution. This scaling improves small-sample performance by matching the distribution's degrees of freedom to the available information.\n\nStep 4: Theorem 7.5-7.6 implications reinforce this preference: When OLS=FGLS (under diagonal $\\Omega$ or common regressors), finite-sample $t/F$ distributions from equation-by-equation OLS remain valid, while pure asymptotic chi-square approximations would be less reliable.\n\nQID: textbook-115-20-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-20-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains why the $F$-statistic is preferred over the chi-square statistic for finite-sample inference, covering adjustment for finite-sample variability, accounting for estimation error, and the improved small-sample performance through scaling and degrees of freedom.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-20-1-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains why the $F$-statistic is preferred over the chi-square statistic for finite-sample inference, covering adjustment for finite-sample variability, accounting for estimation error, and the improved small-sample performance through scaling and degrees of freedom."}, "llm_echoed_qid": "textbook-115-20-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains why the $F$-statistic is preferred over the chi-square statistic for finite-sample inference, covering adjustment for finite-sample variability, accounting for estimation error, and the improved small-sample performance through scaling and degrees of freedom."}
{"script_processing_qid": "textbook-117-18-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Using the table for continuously compounded rates, calculate the future value of $\\$1000$ invested at a 25% annual rate for 5 years with continuous compounding. Show each step.\n\nGOLD_ANSWER:\n1.  **Identify the factor**: From the table, for $T=5$ and $r=25\\%$, the factor is $3.4903$.\n2.  **Calculate FV**: $FV = P \\times e^{rT} = 1000 \\times 3.4903 = \\$3490.30$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the formula for continuous compounding: FV = P × e^(rT)\nStep 2: Assign values from the problem:\n- Principal (P) = $1000\n- Rate (r) = 25% = 0.25\n- Time (T) = 5 years\nStep 3: Calculate the exponent term (rT):\n0.25 × 5 = 1.25\nStep 4: Compute the continuous compounding factor e^(1.25):\nUsing calculator/table: e^1.25 ≈ 3.4903\nStep 5: Multiply principal by the factor:\n$1000 × 3.4903 = $3,490.30\nFinal Answer: The future value is approximately $3,490.30\n\nQID: textbook-117-18-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-18-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to calculate the future value using continuous compounding, matching the gold answer both in methodology and final result.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-18-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to calculate the future value using continuous compounding, matching the gold answer both in methodology and final result."}, "llm_echoed_qid": "textbook-117-18-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to calculate the future value using continuous compounding, matching the gold answer both in methodology and final result."}
{"script_processing_qid": "textbook-80-2-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the Mincer earnings equation $E(Y|\\mathrm{SCHOOL},\\mathrm{EXP}) = \\beta_0 + \\beta_1 \\cdot \\mathrm{SCHOOL} + \\beta_2 \\cdot \\mathrm{EXP} + \\beta_3 \\cdot \\mathrm{EXP}^2$, interpret the economic significance of the coefficients $\\beta_1$, $\\beta_2$, and $\\beta_3$.\n\nGOLD_ANSWER:\n1.  **$\\beta_1$**: Measures the approximate percentage increase in wages per additional year of schooling (rate of return to education).\n2.  **$\\beta_2$ and $\\beta_3$**: Jointly describe the wage-experience profile. A positive $\\beta_2$ and negative $\\beta_3$ imply concave growth, reflecting rapid early-career wage growth that eventually plateaus and declines due to reduced human capital investment.\n\nCANDIDATE_ANSWER:\nStep 1: Interpret β₁ (SCHOOL coefficient)\n- β₁ represents the marginal effect of an additional year of schooling on log wages, holding experience constant. Economically, this reflects the percentage increase in wages per additional year of education (a standard 'return to schooling' estimate in labor economics).\n\nStep 2: Interpret β₂ (EXP coefficient)\n- β₂ captures the linear effect of work experience on log wages. It indicates the immediate percentage wage increase from an additional year of experience when experience is at zero. This represents early-career returns to labor market participation.\n\nStep 3: Interpret β₃ (EXP² coefficient)\n- β₃ measures the quadratic (curvature) effect of experience. Since this term is typically negative in Mincer equations, it implies diminishing marginal returns to experience. The combined marginal effect of experience is β₂ + 2β₃·EXP, showing that wage growth slows with accumulated experience.\n\nEconomic Significance Summary:\nThe coefficients jointly describe the human capital-earnings relationship: β₁ quantifies education returns, while β₂ and β₃ characterize the concave experience-earnings profile common in labor markets.\n\nQID: textbook-80-2-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-80-2-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's interpretation of the coefficients β₁, β₂, and β₃ aligns perfectly with the gold answer. They correctly explain the economic significance of each coefficient, including the return to education, the linear effect of experience, and the diminishing returns to experience.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-2-0-2", "category": "CORRECT", "explanation": "The candidate's interpretation of the coefficients β₁, β₂, and β₃ aligns perfectly with the gold answer. They correctly explain the economic significance of each coefficient, including the return to education, the linear effect of experience, and the diminishing returns to experience."}, "llm_echoed_qid": "textbook-80-2-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's interpretation of the coefficients β₁, β₂, and β₃ aligns perfectly with the gold answer. They correctly explain the economic significance of each coefficient, including the return to education, the linear effect of experience, and the diminishing returns to experience."}
{"script_processing_qid": "textbook-111-23-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the identity $NX = S - I$, prove that the trade balance remains unchanged after quota removal under floating exchange rates.\n\nGOLD_ANSWER:\n1. **Quota Removal**: No effect on $S$ (depends on $Y$, $T$, $G$) or $I(r^{\\*})$.\\n2. **Floating Rates**: $Y$ unchanged, so $S$ and $I$ unchanged.\\n3. **Result**: $NX = S - I$ remains unchanged.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the identity NX = S - I (trade balance equals saving minus investment).\nStep 2: Under floating exchange rates, removing an import quota reduces net exports (NX) initially. However, the exchange rate depreciates to restore equilibrium.\nStep 3: Income (Y) remains unchanged (as shown in Figure 12-6), implying national saving S = Y - C - G remains constant (since consumption C and government spending G are unaffected).\nStep 4: Investment I is determined by the world interest rate (r*) in the Mundell-Fleming model with perfect capital mobility, which remains unchanged.\nStep 5: Since both S and I remain unchanged post-quota removal, the identity NX = S - I implies NX must also remain unchanged. The exchange rate depreciation offsets the initial reduction in NX by making exports more competitive and imports more expensive.\n\nQID: textbook-111-23-2-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-111-23-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the logical steps to prove that the trade balance remains unchanged after quota removal under floating exchange rates. The explanation aligns with the gold answer, detailing how S and I remain unchanged, and how the exchange rate adjusts to maintain equilibrium in NX.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-23-2-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the logical steps to prove that the trade balance remains unchanged after quota removal under floating exchange rates. The explanation aligns with the gold answer, detailing how S and I remain unchanged, and how the exchange rate adjusts to maintain equilibrium in NX."}, "llm_echoed_qid": "textbook-111-23-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the logical steps to prove that the trade balance remains unchanged after quota removal under floating exchange rates. The explanation aligns with the gold answer, detailing how S and I remain unchanged, and how the exchange rate adjusts to maintain equilibrium in NX."}
{"script_processing_qid": "textbook-100-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Analyze the circularity in the axiomatic defense of Bayesian inference, focusing on Axiom 5 (comparability of events with uniform random variables).\n\nGOLD_ANSWER:\n**Circularity**:\n- Axiom 5 assumes one can compare any event $A$ to intervals of a $\\\\mathcal{U}[0,1]$ variable, implying the existence of a probability $P(A)$.\n- This effectively **presumes** probabilistic assignments, making Theorem 3.1 (coherence) a tautology: \\\"If you accept Bayesian axioms, you must be Bayesian.\\\"\n- **Implication**: The logical defense reduces to a choice, not an imperative.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Axiom 5’s purpose. The axiom establishes that all uncertain events can be compared to or calibrated against a uniform random variable (e.g., a fair coin toss or uniform distribution), which serves as a reference for probabilistic reasoning.\\n\\nStep 2: Observe the circularity. Axiom 5 assumes the existence of a uniform random variable as a foundational reference for defining probabilities. However, the uniform random variable itself is a probabilistic construct derived from the same axiomatic system it is meant to justify.\\n\\nStep 3: Analyze implications. The circularity arises because Bayesian inference requires the acceptance of probabilistic primitives (e.g., uniform randomness) to justify the use of probability as the sole rational framework for uncertainty. This pre-supposes the validity of the probabilistic framework it aims to derive, undermining the axiom’s independence.\\n\\nStep 4: Contrast with frequentism. While frequentist methods avoid this circularity by relying on repeated sampling interpretations, they introduce subjectivity through ad hoc model choices, as noted in the context. Bayesian axioms trade this for a self-referential dependence on probabilistic axioms.\n\nQID: textbook-100-0-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-100-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies the circularity in Axiom 5, noting the presupposition of probabilistic constructs to justify the framework itself. It aligns with the gold answer's critique of the axiom's tautological nature and contrasts it with frequentist approaches.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-100-0-1-0", "category": "CORRECT", "explanation": "The candidate answer accurately identifies the circularity in Axiom 5, noting the presupposition of probabilistic constructs to justify the framework itself. It aligns with the gold answer's critique of the axiom's tautological nature and contrasts it with frequentist approaches."}, "llm_echoed_qid": "textbook-100-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies the circularity in Axiom 5, noting the presupposition of probabilistic constructs to justify the framework itself. It aligns with the gold answer's critique of the axiom's tautological nature and contrasts it with frequentist approaches."}
{"script_processing_qid": "textbook-117-43-3-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate the after-tax yield for a corporate investor in ARPS with a pre-tax dividend yield of 6%, assuming a corporate tax rate of 21% and an 80% dividend exclusion.\n\nGOLD_ANSWER:\nThe after-tax yield is calculated as:\n$$ \\text{After-Tax Yield} = \\text{Pre-Tax Yield} \\times \\left(1 - \\text{Effective Tax Rate}\\right) $$\n$$ \\text{Effective Tax Rate} = \\text{Tax Rate} \\times (1 - \\text{Exclusion Percentage}) $$\n$$ \\text{Effective Tax Rate} = 0.21 \\times (1 - 0.80) = 0.042 $$\n$$ \\text{After-Tax Yield} = 0.06 \\times (1 - 0.042) = 0.05748 \\text{ or } 5.748\\% $$\n\nCANDIDATE_ANSWER:\nStep 1: Identify the taxable portion of dividends. With an 80% dividend exclusion, only 20% of dividends are taxable. Calculation: 6% pre-tax yield × 20% taxable portion = 1.2% taxable income.\nStep 2: Calculate taxes owed on the taxable portion. Corporate tax rate = 21%. Calculation: 1.2% × 21% = 0.252% tax.\nStep 3: Subtract taxes from pre-tax yield to get after-tax yield. Calculation: 6% - 0.252% = 5.748%.\nFinal Answer: The after-tax yield is 5.75% (rounded to two decimal places).\n\nQID: textbook-117-43-3-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-43-3-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly calculates the after-tax yield as 5.75%, matching the gold answer. The reasoning, though presented differently, aligns with the correct mathematical approach.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-43-3-2", "category": "CORRECT", "explanation": "The candidate's answer correctly calculates the after-tax yield as 5.75%, matching the gold answer. The reasoning, though presented differently, aligns with the correct mathematical approach."}, "llm_echoed_qid": "textbook-117-43-3-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly calculates the after-tax yield as 5.75%, matching the gold answer. The reasoning, though presented differently, aligns with the correct mathematical approach."}
{"script_processing_qid": "textbook-117-14-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the discounted cash flow model, explain how synergy is calculated in an acquisition. Provide the formula and interpret each component.\n\nGOLD_ANSWER:\n1.  **Synergy Formula**: $$\\mathrm{Synergy}=\\sum\\_{t=1}^{T}\\frac{\\Delta\\mathrm{CF}\\_{t}}{(1+r)^{t}}$$.\n2.  **Components**:\n    - $\\Delta\\mathrm{CF}\\_{t}$: Incremental cash flow from the merger at time $t$.\n    - $r$: Risk-adjusted discount rate, typically the target's required rate of return.\n3.  **Interpretation**: Synergy is the present value of all future incremental cash flows generated by the merger, reflecting the combined firm's enhanced value.\n\nCANDIDATE_ANSWER:\nStep 1: The synergy in an acquisition is calculated as the present value of incremental cash flows generated by the merger. The formula provided in the context is: $$\\mathrm{Synergy}=\\sum_{t=1}^{T}\\frac{\\Delta\\mathrm{CF}_{t}}{(1+r)^{t}}$$ \n\nStep 2: **Interpret components**:\n- **$\\Delta\\mathrm{CF}_{t}$**: The incremental cash flow at time $t$, calculated as the difference between the cash flows of the combined firm and the sum of the cash flows of the two separate firms. This represents the additional cash flows generated by synergies (e.g., cost savings, revenue enhancements).\n- **$r$**: The discount rate, reflecting the risk or opportunity cost of capital associated with the acquisition.\n- **$T$**: The time horizon over which synergistic cash flows are expected to occur.\n- **$(1+r)^t$**: The discount factor that adjusts future cash flows to their present value, accounting for the time value of money.\n\nStep 3: The summation aggregates all discounted incremental cash flows over the period $t=1$ to $T$, yielding the total present value of synergies from the acquisition.\n\nQID: textbook-117-14-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-14-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the synergy formula and interprets each component correctly, aligning with the gold answer. The explanation of $\\Delta\\mathrm{CF}_{t}$, $r$, $T$, and $(1+r)^t$ is clear and matches the expected interpretation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-14-0-2", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the synergy formula and interprets each component correctly, aligning with the gold answer. The explanation of $\\Delta\\mathrm{CF}_{t}$, $r$, $T$, and $(1+r)^t$ is clear and matches the expected interpretation."}, "llm_echoed_qid": "textbook-117-14-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the synergy formula and interprets each component correctly, aligning with the gold answer. The explanation of $\\Delta\\mathrm{CF}_{t}$, $r$, $T$, and $(1+r)^t$ is clear and matches the expected interpretation."}
{"script_processing_qid": "textbook-74-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the expected-market-clearing condition for the price $p_{t}$ set in period $t-j$ , given the supply $S_{t}(p_{t})$ and demand $D_{t}(p_{t})$ functions. Explain the economic intuition behind this condition.\n\nGOLD_ANSWER:\n1.  **Expected-Market-Clearing Condition**: The price $p_{t}$ is set in period $t-j$ such that $$ E_{t-j}(S_{t}(p_{t})) = E_{t-j}(D_{t}(p_{t})). $$\n2.  **Intuition**: Firms set prices in advance based on their expectations of future supply and demand conditions. The condition ensures that, on average, the market will clear during the period the price applies, even though actual supply and demand may deviate due to unforeseen shocks.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the core market-clearing requirement. Since prices are set in period t-j for period t, firms must choose a price that equates expected supply and expected demand at that price.\\n\\nStep 2: Apply rational expectations. Firms use all available information at time t-j (captured by the conditional expectation operator E_{t-j}) to forecast period-t supply and demand.\\n\\nStep 3: Formulate the equilibrium condition. The expected-market-clearing price p_t satisfies:\\nE_{t-j}(S_t(p_t)) = E_{t-j}(D_t(p_t))\\n\\nEconomic intuition:\\n- Price setters aim to avoid ex ante imbalances. By setting p_t such that expected supply equals expected demand, they anticipate market equilibrium at their fixed price.\\n- This reflects forward-looking optimization under sticky prices – firms lock in a price that they expect will equate planned production (supply) with desired purchases (demand) during the contract period.\\n- The condition incorporates rational expectations by using all period t-j information, aligning with modern macroeconomic modeling principles.\n\nQID: textbook-74-0-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-74-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately derives the expected-market-clearing condition and provides a detailed economic intuition that aligns with the gold answer. The explanation includes forward-looking optimization and rational expectations, which are key to understanding the condition.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-74-0-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately derives the expected-market-clearing condition and provides a detailed economic intuition that aligns with the gold answer. The explanation includes forward-looking optimization and rational expectations, which are key to understanding the condition."}, "llm_echoed_qid": "textbook-74-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately derives the expected-market-clearing condition and provides a detailed economic intuition that aligns with the gold answer. The explanation includes forward-looking optimization and rational expectations, which are key to understanding the condition."}
{"script_processing_qid": "textbook-100-3-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that the posterior distribution $\\theta\\mid X=x$ is $\\operatorname{Be}(\\alpha+x,\\beta+n-x)$ when $X\\mid\\theta\\sim{\\mathcal{B}}(n,\\theta)$ and $\\theta\\sim\\operatorname{Be}(\\alpha,\\beta)$.\n\nGOLD_ANSWER:\n1. **Prior Density**: $g(\\theta) = \\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)}$.\n2. **Likelihood**: $f(x\\mid\\theta) = \\binom{n}{x}\\theta^x(1-\\theta)^{n-x}$.\n3. **Posterior Density**: $g(\\theta\\mid x) \\propto \\theta^{\\alpha+x-1}(1-\\theta)^{\\beta+n-x-1}$.\n4. **Normalization**: Recognized as $\\operatorname{Be}(\\alpha+x,\\beta+n-x)$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall Bayes' theorem states posterior ∝ likelihood × prior. \nStep 2: Write prior density: Beta(α,β) has PDF f(θ) ∝ θ^{α-1}(1-θ)^{β-1}. \nStep 3: Write binomial likelihood: P(X=x|θ) ∝ θ^x(1-θ)^{n-x}. \nStep 4: Multiply prior and likelihood: θ^{α+x-1}(1-θ)^{β+n−x−1}. \nStep 5: Recognize this matches Beta(α+x, β+n−x) kernel. \nStep 6: Conclude θ|X=x ∼ Be(α+x, β+n−x) by conjugacy property.\n\nQID: textbook-100-3-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-100-3-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly follows the steps of Bayesian inference, showing the proportionality of the posterior to the product of prior and likelihood, and correctly identifies the resulting Beta distribution form.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-100-3-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly follows the steps of Bayesian inference, showing the proportionality of the posterior to the product of prior and likelihood, and correctly identifies the resulting Beta distribution form."}, "llm_echoed_qid": "textbook-100-3-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly follows the steps of Bayesian inference, showing the proportionality of the posterior to the product of prior and likelihood, and correctly identifies the resulting Beta distribution form."}
{"script_processing_qid": "textbook-108-15-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why microeconomics and macroeconomics might use different methodological approaches despite their interdependence.\n\nGOLD_ANSWER:\nMicroeconomics often employs partial equilibrium analysis, focusing on individual markets with ceteris paribus assumptions. Macroeconomics typically uses general equilibrium models to account for economy-wide interactions and feedback effects. The difference arises because:\n- Micro deals with optimization under constraints\n- Macro must account for emergent properties of interacting systems\n- Data requirements differ (micro vs aggregate data)\n\nCANDIDATE_ANSWER:\nStep 1: Identify the core distinction in focus. Microeconomics examines individual decision-making and specific markets, while macroeconomics studies aggregate outcomes (e.g., GDP, unemployment). This difference necessitates distinct analytical frameworks.\n\nStep 2: Recognize scale-driven complexity. Microeconomic models often isolate variables (e.g., supply-demand in a single market) using partial equilibrium analysis. Macroeconomics must account for system-wide interactions (e.g., interest rates affecting investment, consumption, and employment simultaneously), requiring general equilibrium or reduced-form models.\n\nStep 3: Address aggregation challenges. While macroeconomics emerges from micro-level decisions, emergent phenomena (e.g., inflation expectations, business cycles) cannot be directly inferred from individual behavior alone. This leads macroeconomists to employ simplifying assumptions about aggregate relationships.\n\nStep 4: Consider methodological tools. Microeconomics frequently uses experimental methods and granular data to test theories, whereas macroeconomics often relies on historical time-series data and observational studies due to the impracticality of manipulating entire economies.\n\nStep 5: Highlight policy orientation. Macroeconomic methodology often prioritizes stabilization policies, requiring models that simplify microfoundations to focus on fiscal/monetary transmission mechanisms – a different emphasis than microeconomic welfare analysis at the individual or firm level.\n\nQID: textbook-108-15-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-108-15-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the methodological differences between microeconomics and macroeconomics, aligning well with the gold answer. It covers the focus on individual vs. aggregate outcomes, scale-driven complexity, aggregation challenges, methodological tools, and policy orientation, all of which are key points in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-15-0-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the methodological differences between microeconomics and macroeconomics, aligning well with the gold answer. It covers the focus on individual vs. aggregate outcomes, scale-driven complexity, aggregation challenges, methodological tools, and policy orientation, all of which are key points in the gold answer."}, "llm_echoed_qid": "textbook-108-15-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the methodological differences between microeconomics and macroeconomics, aligning well with the gold answer. It covers the focus on individual vs. aggregate outcomes, scale-driven complexity, aggregation challenges, methodological tools, and policy orientation, all of which are key points in the gold answer."}
{"script_processing_qid": "textbook-111-3-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain how a depreciation of the nominal exchange rate affects the real exchange rate, assuming price levels are fixed in the short run. What are the implications for trade balance?\n\nGOLD_ANSWER:\n1. **Depreciation**: A fall in $e$ (nominal rate) directly reduces $\\epsilon$ (real rate). \n2. **Trade Balance**: Lower $\\epsilon$ makes domestic goods cheaper abroad, potentially improving the trade balance.\n\nCANDIDATE_ANSWER:\n```json\n{\n  \"answer\": \"Step 1: Recall the formula for the real exchange rate: \\\\( \\\\epsilon = e \\\\times \\\\frac{P}{P^*} \\\\).\\nStep 2: In the short run, assume \\\\( P \\\\) (domestic price level) and \\\\( P^* \\\\) (foreign price level) are fixed. A depreciation of the nominal exchange rate (\\\\( e \\\\uparrow \\\\)) directly increases the real exchange rate (\\\\( \\\\epsilon \\\\uparrow \\\\)) because all other terms in the formula remain constant.\\nStep 3: A higher real exchange rate (\\\\( \\\\epsilon \\\\uparrow \\\\)) means domestic goods become relatively cheaper compared to foreign goods. This boosts exports (foreign buyers find domestic goods cheaper) and reduces imports (domestic buyers find foreign goods more expensive).\\nStep 4: The increase in exports and decrease in imports improves the trade balance (reduces a deficit or increases a surplus) in the short run, assuming the Marshall-Lerner condition holds (sum of export and import price elasticities exceeds 1).\"\n}\n```\n\nQID: textbook-111-3-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-3-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly explains the relationship between nominal and real exchange rates, the impact on trade balance, and includes the Marshall-Lerner condition, aligning with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-3-1-2", "category": "CORRECT", "explanation": "The candidate's answer correctly explains the relationship between nominal and real exchange rates, the impact on trade balance, and includes the Marshall-Lerner condition, aligning with the gold answer."}, "llm_echoed_qid": "textbook-111-3-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly explains the relationship between nominal and real exchange rates, the impact on trade balance, and includes the Marshall-Lerner condition, aligning with the gold answer."}
{"script_processing_qid": "textbook-117-43-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the optimal cash balance using the Baumol model given the following parameters: annual cash requirement = $1,000,000, fixed cost per transaction = $100, and annual interest rate = 5%.\n\nGOLD_ANSWER:\nThe Baumol model formula is:\n$$ C^* = \\sqrt{\\frac{2FT}{k}} $$\nWhere:\n- $C^*$ = optimal cash balance\n- $F$ = fixed cost per transaction ($100)\n- $T$ = total annual cash requirement ($1,000,000)\n- $k$ = annual interest rate (5% or 0.05)\n\nPlugging in the values:\n$$ C^* = \\sqrt{\\frac{2 \\times 100 \\times 1,000,000}{0.05}} = \\sqrt{4,000,000,000} = 63,245.55 $$\n\nCANDIDATE_ANSWER:\nStep 1: Recall the Baumol model formula for optimal cash balance: C* = √(2 * T * F / i)\nStep 2: Identify the parameters from the question:\n- Annual cash requirement (T) = $1,000,000\n- Fixed cost per transaction (F) = $100\n- Annual interest rate (i) = 5% = 0.05\nStep 3: Substitute values into the formula:\nC* = √[(2 * 1,000,000 * 100) / 0.05]\nStep 4: Calculate numerator: 2 * 1,000,000 * 100 = 200,000,000\nStep 5: Divide by interest rate: 200,000,000 / 0.05 = 4,000,000,000\nStep 6: Take square root: √4,000,000,000 = 63,245.55\nStep 7: The optimal cash balance is $63,246 (rounded to nearest dollar).\n\nQID: textbook-117-43-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-43-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly applied the Baumol model formula, substituted the given values accurately, and arrived at the correct optimal cash balance of $63,246 (rounded). The reasoning and calculations match the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-43-1-0", "category": "CORRECT", "explanation": "The candidate correctly applied the Baumol model formula, substituted the given values accurately, and arrived at the correct optimal cash balance of $63,246 (rounded). The reasoning and calculations match the gold answer."}, "llm_echoed_qid": "textbook-117-43-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly applied the Baumol model formula, substituted the given values accurately, and arrived at the correct optimal cash balance of $63,246 (rounded). The reasoning and calculations match the gold answer."}
{"script_processing_qid": "textbook-99-7-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the estimator $\\tilde{\\theta}_{j}$ for Fourier coefficients in the heteroscedastic setting, starting from the model $Y_{l}=f(X_{l})+\\sigma(X_{l})\\varepsilon_{l}$.\n\nGOLD_ANSWER:\n1. **Model Setup**: Start with the heteroscedastic model $Y_{l}=f(X_{l})+\\sigma(X_{l})\\varepsilon_{l}$.\n2. **Ordering Predictors**: Arrange predictors in ascending order to get $(X_{(l)}, Y_{(l)})$.\n3. **Estimator Definition**: Define the estimator as $$\\tilde{\\theta}_{j}:=(2s)^{-1}\\sum_{l=1}^{n}Y_{(l)}\\int_{X_{(l-s)}}^{X_{(l+s)}}\\varphi_{j}(x)dx.$$\n4. **Justification**: For large $j$, $\\varphi_{j}(x)$ is highly oscillatory, so integration provides a more accurate estimate.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the Fourier coefficient definition\nθ_j = ∫₀¹ f(x)φ_j(x)dx.\n\nStep 2: Use the regression model Y_l = f(X_l) + σ(X_l)ε_l\nNote that Y_l provides noisy observations of f(X_l).\n\nStep 3: Under heteroscedasticity, standard estimators require adjustment for:\n- Non-uniform design density h(x)\n- Variance scaling σ²(x)\n\nStep 4: Construct a localized estimator using neighboring observations:\nFor ordered predictors X_{(l)}, average Y_{(l)} over a symmetric window [X_{(l-s)}, X_{(l+s)}].\n\nStep 5: Approximate the integral ∫φ_j(x)dx over this window\nThis handles design density by implicitly weighting via local spacing.\n\nStep 6: Combine across all observations\nSum these φ_j-contributions weighted by Y_{(l)}, then normalize by window size (2s):\n\nFinal estimator:\n𝜏̃_j = (2s)⁻¹ ∑_{l=1}^n Y_{(l)} ∫_{X_{(l-s)}}^{X_{(l+s)}} φ_j(x)dx\n\nThis formulation adapts to local variance (via windowed averaging) and design density (via spacing between X_{(l)}).\n\nQID: textbook-99-7-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-7-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the estimator for Fourier coefficients in the heteroscedastic setting, matching the gold answer's steps and final estimator formulation. The explanation includes additional details about handling non-uniform design density and variance scaling, which are relevant and correct.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-7-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the estimator for Fourier coefficients in the heteroscedastic setting, matching the gold answer's steps and final estimator formulation. The explanation includes additional details about handling non-uniform design density and variance scaling, which are relevant and correct."}, "llm_echoed_qid": "textbook-99-7-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the estimator for Fourier coefficients in the heteroscedastic setting, matching the gold answer's steps and final estimator formulation. The explanation includes additional details about handling non-uniform design density and variance scaling, which are relevant and correct."}
{"script_processing_qid": "textbook-99-9-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Describe the plug-in method for selecting the bandwidth in kernel density estimation.\n\nGOLD_ANSWER:\n1. **Asymptotic Formula**: Start with an asymptotic expression for the optimal bandwidth, e.g., $h^*(f) = C(f) n^{-1/5}$.\n2. **Functional Estimation**: Estimate the unknown functional $C(f)$ (e.g., $\\int [f^{(2)}(x)]^2 dx$) from the data.\n3. **Plug-In**: Substitute the estimated functional into the formula to obtain the bandwidth $\\hat{h} = \\hat{C}(f) n^{-1/5}$.\n\nCANDIDATE_ANSWER:\nStep 1: The plug-in method begins with an asymptotic formula for the optimal bandwidth (e.g., Equation 8.9.33), which depends on unknown functionals of the true density function f, such as F₂(f) = ∫[f^{(2)}(x)]²dx (the integral of the squared second derivative of f).\n\nStep 2: Since f is unknown, empirical estimates of these functionals (e.g., F₂(f)) are computed using the observed data. This often involves substituting a pilot estimate of f or directly estimating the required functional through nonparametric methods.\n\nStep 3: The estimated functional values are then substituted (\\\"plugged in\\\") into the original asymptotic bandwidth formula. This yields a data-driven bandwidth selection rule that approximates the asymptotically optimal bandwidth.\n\nQID: textbook-99-9-2-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-9-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes the three key steps of the plug-in method for bandwidth selection in kernel density estimation, matching the structure and content of the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-9-2-0", "category": "CORRECT", "explanation": "The candidate's answer accurately describes the three key steps of the plug-in method for bandwidth selection in kernel density estimation, matching the structure and content of the gold answer."}, "llm_echoed_qid": "textbook-99-9-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes the three key steps of the plug-in method for bandwidth selection in kernel density estimation, matching the structure and content of the gold answer."}
{"script_processing_qid": "textbook-99-9-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Suggest a practical approach to improve the plug-in method's robustness.\n\nGOLD_ANSWER:\n1. **Pilot Bandwidth**: Use a simple rule (e.g., Silverman's rule of thumb) to estimate initial functionals.\n2. **Iterative Refinement**: Update the bandwidth by re-estimating functionals with the initial bandwidth.\n3. **Regularization**: Smooth the estimated functionals to reduce variability.\n4. **Hybrid Methods**: Combine plug-in with cross-validation for adaptive tuning.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the key source of fragility in the plug-in method. The method depends on estimating the quadratic functional \\( F_2(f) = \\int [f^{(2)}(x)]^2 dx \\), which is sensitive to errors in estimating the second derivative \\( f^{(2)}(x) \\).\n\nStep 2: Improve the pilot estimation stage. Use a multi-stage approach where a preliminary bandwidth (e.g., based on a rule-of-thumb or a simpler kernel) is first selected to estimate \\( f^{(2)}(x) \\) more robustly. This reduces bias and variance in the \\( F_2(f) \\) estimate.\n\nStep 3: Optimize pilot bandwidth selection. Employ a data-driven method (e.g., a secondary plug-in or cross-validation) to choose the pilot bandwidth specifically for estimating \\( f^{(2)}(x) \\), ensuring it balances bias and variance effectively.\n\nStep 4: Use higher-order kernels. Adopt kernels with higher-order properties (e.g., a fourth-order kernel) during the pilot estimation of \\( f^{(2)}(x) \\) to reduce boundary bias and improve the stability of the functional estimate.\n\nStep 5: Validate with sensitivity analysis. Test the plug-in bandwidth’s robustness by perturbing the estimated \\( F_2(f) \\) (e.g., adding noise or varying pilot bandwidths) and ensuring the final bandwidth choice remains stable.\n\nQID: textbook-99-9-2-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-99-9-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer provides a detailed and practical approach to improving the plug-in method's robustness, aligning well with the gold answer's focus on pilot bandwidth, iterative refinement, and regularization. The steps are logically structured and address key aspects of the method's fragility.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-9-2-3", "category": "CORRECT", "explanation": "The candidate answer provides a detailed and practical approach to improving the plug-in method's robustness, aligning well with the gold answer's focus on pilot bandwidth, iterative refinement, and regularization. The steps are logically structured and address key aspects of the method's fragility."}, "llm_echoed_qid": "textbook-99-9-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer provides a detailed and practical approach to improving the plug-in method's robustness, aligning well with the gold answer's focus on pilot bandwidth, iterative refinement, and regularization. The steps are logically structured and address key aspects of the method's fragility."}
{"script_processing_qid": "textbook-105-7-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the key inequality in Theorem 19.1 starting from the chaining representation $$f = (f-f^{S}) + (f^{S}-f^{S-1}) + \\cdots + (f^{1}-f^{0}) + f^{0}$$ and Hoeffding's inequality.\n\nGOLD_ANSWER:\n1. **Chaining Representation**: For each $f \\in \\mathcal{F}$, approximate $f$ via a sequence of covers $\\{f^s\\}_{s=0}^S$ with $\\|f - f^s\\|_n \\leq R/2^s$.\n2. **Decomposition**: Write $\\left|\\frac{1}{n}\\sum_{i=1}^n f(z_i)\\epsilon_i\\right| \\leq \\|f - f^S\\|_n \\cdot L + \\sum_{s=1}^S \\left|\\frac{1}{n}\\sum_{i=1}^n (f^s(z_i) - f^{s-1}(z_i))\\epsilon_i\\right|$.\n3. **Hoeffding's Inequality**: Apply Hoeffding's inequality to each term $\\left|\\frac{1}{n}\\sum_{i=1}^n (f^s(z_i) - f^{s-1}(z_i))\\epsilon_i\\right|$, using the boundedness $|f^s(z_i) - f^{s-1}(z_i)| \\leq 3R/2^s$.\n4. **Union Bound**: Combine probabilities via the union bound over all covers, leading to the final exponential tail bound.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the chained decomposition of any f ∈ F:\n$$f = (f - f^S) + (f^S - f^{S-1}) + \\cdots + (f^1 - f^0) + f^0.$$\nEach term f^s - f^{s-1} belongs to a δ_s-cover of F with δ_s = δ/2^{s}, where δ is the target precision.\n\nStep 2: Apply Hoeffding's inequality to each increment f^{s} - f^{s-1}. For a fixed s, the L2 norm of f^{s} - f^{s-1} is ≤ 2δ_s. Hoeffding's inequality gives:\n$$\\mathbf{P}\\left(\\left|\\frac{1}{n}\\sum_{i=1}^n (f^{s} - f^{s-1})(z_i)\\epsilon_i\\right| > t_s\\right) ≤ 2\\exp\\left(-\\frac{n t_s^2}{8L^2\\delta_s^2}\\right).$$\n\nStep 3: Union bound over all s and all functions in the δ_s-cover. At scale s, there are N_s = 𝒩_2(δ_s, F, z_1^n) functions. Set t_s = c L \\delta_s \\sqrt{\\log N_s / n} for constant c. Then:\nTotal probability ≤ Σ_s 2N_s \\exp\\left(-\\frac{n t_s^2}{8L^2\\delta_s^2}\\right) = Σ_s 2N_s \\exp\\left(-c^2 \\log N_s / 8\\right).\n\nStep 4: Ensure c^2/8 ≥ 1 to dominate the sum. Choose c = √(8), leading to t_s = √(8) L δ_s \\sqrt{\\log N_s / n}. Sum over s gives:\nTotal supremum bound ≤ Σ_s √(8) L δ_s \\sqrt{\\log N_s / n}.\n\nStep 5: Convert the sum to an integral. Let δ_s ≈ u = δ/2^s, so du ≈ δ_s. Then:\nΣ_s δ_s \\sqrt{\\log N_s} ≈ ∫_{δ/8L}^{R/2} \\sqrt{\\log 𝒩_2(u, F, z_1^n)} du.\n\nStep 6: Match the integral to the given inequality:\n√n δ ≥ 48√2 L ∫_{δ/8L}^{R/2} \\sqrt{\\log 𝒩_2(u, F, z_1^n)} du.\nThis ensures the total bound from Step 4 is ≤ δ/2. Combine with the √n δ ≥ 36RL condition to handle residual terms and finalize the probability bound via exponential terms.\n\nQID: textbook-105-7-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-105-7-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps outlined in the gold answer, including the chaining representation, application of Hoeffding's inequality, union bound, and conversion to an integral. The reasoning and final inequality match the expected derivation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-105-7-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps outlined in the gold answer, including the chaining representation, application of Hoeffding's inequality, union bound, and conversion to an integral. The reasoning and final inequality match the expected derivation."}, "llm_echoed_qid": "textbook-105-7-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps outlined in the gold answer, including the chaining representation, application of Hoeffding's inequality, union bound, and conversion to an integral. The reasoning and final inequality match the expected derivation."}
{"script_processing_qid": "textbook-47-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the reduced form equation $y_{t}=F(y_{t-1},E_{t}^{*}y_{t+1},u_{t})$ for a typical macroeconomic model and explain the role of expectations in this framework.\n\nGOLD_ANSWER:\n1.  **Reduced Form Equation**: The reduced form equation $y_{t}=F(y_{t-1},E_{t}^{*}y_{t+1},u_{t})$ captures the dependence of endogenous variables $y_{t}$ on lagged values $y_{t-1}$, expectations of future values $E_{t}^{*}y_{t+1}$, and exogenous shocks $u_{t}$. \n2.  **Role of Expectations**: Expectations $E_{t}^{*}y_{t+1}$ are central as they influence current decisions and outcomes. Under rational expectations, $E_{t}^{*}y_{t+1}$ is the true conditional expectation, but under learning, agents form expectations based on observed data and updating rules.\n\nCANDIDATE_ANSWER:\nStep 1: Start with structural equations of a typical macroeconomic model. For example, in a New Keynesian framework:\n- Euler equation: $y_t = E_t^* y_{t+1} - \\sigma(i_t - E_t^* \\pi_{t+1}) + u_t$\n- Phillips curve: $\\pi_t = \\beta E_t^* \\pi_{t+1} + \\kappa y_t$\n- Monetary policy rule: $i_t = \\phi_\\pi \\pi_t + \\phi_y y_t$\nwhere $y_t$ = output gap, $\\pi_t$ = inflation, $i_t$ = interest rate, $u_t$ = demand shock.\n\nStep 2: Substitute the policy rule into the Euler equation:\n$y_t = E_t^* y_{t+1} - \\sigma(\\phi_\\pi \\pi_t + \\phi_y y_t - E_t^* \\pi_{t+1}) + u_t$\n\nStep 3: Use the Phillips curve to express inflation in terms of output:\n$\\pi_t = \\beta E_t^* \\pi_{t+1} + \\kappa y_t$\n\nStep 4: Solve the system recursively to eliminate contemporaneous terms and express $y_t$ as:\n$y_t = F(y_{t-1}, E_t^* y_{t+1}, u_t)$\nwhere $F$ combines parameters ($\\sigma,\\beta,\\kappa,\\phi_\\pi,\\phi_y$) and expectations formation.\n\nRole of expectations:\n1. Forward guidance: $E_t^* y_{t+1}$ directly affects current output through intertemporal substitution\n2. Multiple equilibria: Different expectation formation mechanisms (RE vs. learning) can lead to different solution paths\n3. Stability: Convergence to REE depends on whether learning rules satisfy expectational stability conditions\n4. Policy effectiveness: Expectations formation determines how quickly agents adapt to policy changes\n\nQID: textbook-47-0-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-47-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derived the reduced form equation $y_{t}=F(y_{t-1},E_{t}^{*}y_{t+1},u_{t})$ from a New Keynesian framework and thoroughly explained the role of expectations, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-47-0-0-0", "category": "CORRECT", "explanation": "The candidate correctly derived the reduced form equation $y_{t}=F(y_{t-1},E_{t}^{*}y_{t+1},u_{t})$ from a New Keynesian framework and thoroughly explained the role of expectations, aligning with the gold answer."}, "llm_echoed_qid": "textbook-47-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derived the reduced form equation $y_{t}=F(y_{t-1},E_{t}^{*}y_{t+1},u_{t})$ from a New Keynesian framework and thoroughly explained the role of expectations, aligning with the gold answer."}
{"script_processing_qid": "textbook-119-13-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the conditions under which OLS can be appropriately applied to a recursive simultaneous-equation model, using the three-equation system (20.2.1) as an example. Show mathematically why OLS remains consistent in this special case.\n\nGOLD_ANSWER:\n**Recursive System Structure**:\n$$\n\\begin{array}{r l}\n&{Y_{1t}=\\beta_{10} + \\gamma_{11}X_{1t} + u_{1t}} \\\\\n&{Y_{2t}=\\beta_{20}+\\beta_{21}Y_{1t}+\\gamma_{21}X_{1t}+\\gamma_{22}X_{2t}+u_{2t}} \\\\\n&{Y_{3t}=\\beta_{30}+\\beta_{31}Y_{1t}+\\beta_{32}Y_{2t}+\\gamma_{31}X_{1t}+\\gamma_{32}X_{2t}+u_{3t}}\n\\end{array}\n$$\n\n**Consistency Conditions**:\n1. **First Equation**: Only exogenous $X_{1t}$ appears, which is uncorrelated with $u_{1t}$ by classical assumptions.\n2. **Second Equation**: $Y_{1t}$ is uncorrelated with $u_{2t}$ because $cov(u_{1t},u_{2t})=0$ (zero contemporaneous correlation assumption). Thus, $Y_{1t}$ acts as predetermined.\n3. **Third Equation**: Both $Y_{1t}$ and $Y_{2t}$ are uncorrelated with $u_{3t}$ due to $cov(u_{1t},u_{3t})=cov(u_{2t},u_{3t})=0$.\n\n**Key Insight**: The triangular structure ensures no feedback from lower equations to higher ones, maintaining exogeneity of RHS variables in each equation.\n\nCANDIDATE_ANSWER:\nStep 1: Understand recursive system structure\nIn a recursive simultaneous-equation model (like system 20.2.1), equations are ordered such that:\n- Y₁ depends only on exogenous variables\n- Y₂ depends on Y₁ and exogenous variables\n- Y₃ depends on Y₂/Y₁ and exogenous variables\nThis creates unidirectional causality with no feedback loops.\n\nStep 2: Identify error term relationships\nThe critical condition is that error terms across equations are uncorrelated:\nCov(ε₁, ε₂) = Cov(ε₁, ε₃) = Cov(ε₂, ε₃) = 0\n\nStep 3: Verify predeterminedness\nIn equation for Y₂:\n- Y₁ is predetermined (determined in prior equation)\n- Since Cov(Y₁, ε₂) = Cov(Xβ₁ + ε₁, ε₂) = 0 (given ε₁ ⊥ ε₂)\n\nIn equation for Y₃:\n- Both Y₁ and Y₂ are predetermined\n- Cov(Y₁, ε₃) = Cov(Xβ₁ + ε₁, ε₃) = 0\n- Cov(Y₂, ε₃) = Cov(Y₁γ₁ + Xβ₂ + ε₂, ε₃) = 0\n\nStep 4: Demonstrate OLS consistency\nFor any equation j:\nE(X'_jε_j) = 0 (exogenous variables)\nE(Y_{j-1}ε_j) = 0 (predetermined variables from prior equations)\n\nThis satisfies the OLS exogeneity condition E(regressors'ε) = 0, making OLS estimators consistent through the Law of Large Numbers.\n\nQID: textbook-119-13-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-13-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately follows the structure of the gold answer, detailing the recursive system, error term relationships, predeterminedness, and OLS consistency conditions. The mathematical reasoning aligns perfectly with the gold standard.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-13-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately follows the structure of the gold answer, detailing the recursive system, error term relationships, predeterminedness, and OLS consistency conditions. The mathematical reasoning aligns perfectly with the gold standard."}, "llm_echoed_qid": "textbook-119-13-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately follows the structure of the gold answer, detailing the recursive system, error term relationships, predeterminedness, and OLS consistency conditions. The mathematical reasoning aligns perfectly with the gold standard."}
{"script_processing_qid": "textbook-65-1-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Compare the fixed-bandwidth kernel estimator with the automatic bandwidth selection method (cross-validation). What are the trade-offs?\n\nGOLD_ANSWER:\n1. **Fixed Bandwidth**:\n   - **Pros**: Simpler computation, no need for optimization.\n   - **Cons**: Subjective choice of $h$; may under/oversmooth.\n2. **Automatic Selection (CV)**:\n   - **Pros**: Data-driven $h_{opt}$ minimizes prediction error.\n   - **Cons**: Computationally intensive; sensitive to outliers.\n3. **Trade-off**: Fixed bandwidth is faster but less adaptive; CV is robust but costly.\n\nCANDIDATE_ANSWER:\nStep 1: Fixed-bandwidth kernel estimator requires manual bandwidth selection by the user. This allows domain expertise to guide choices but risks suboptimal performance if h is poorly chosen. Computational costs are lower since no bandwidth optimization is required.\n\nStep 2: Cross-validation automates bandwidth selection by minimizing prediction errors. This adapts h to data characteristics but increases computational complexity due to repeated resampling/optimization.\n\nStep 3: Trade-off 1: Bias-variance balance. Fixed h may over-smooth (high bias) with large h or overfit (high variance) with small h. Cross-validation aims to balance this automatically but may be sensitive to outliers/noise.\n\nStep 4: Trade-off 2: Computational efficiency vs adaptability. Fixed h is faster but static; automatic selection is slower but dynamically adjusts to data patterns.\n\nStep 5: Trade-off 3: User dependence. Fixed h requires expert input, while automatic methods reduce subjectivity but require validation of CV criteria appropriateness.\n\nQID: textbook-65-1-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-65-1-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the trade-offs between fixed-bandwidth kernel estimators and automatic bandwidth selection methods (cross-validation), aligning well with the gold answer. It covers computational efficiency, adaptability, bias-variance balance, and user dependence, providing a comprehensive comparison.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-65-1-0-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the trade-offs between fixed-bandwidth kernel estimators and automatic bandwidth selection methods (cross-validation), aligning well with the gold answer. It covers computational efficiency, adaptability, bias-variance balance, and user dependence, providing a comprehensive comparison."}, "llm_echoed_qid": "textbook-65-1-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the trade-offs between fixed-bandwidth kernel estimators and automatic bandwidth selection methods (cross-validation), aligning well with the gold answer. It covers computational efficiency, adaptability, bias-variance balance, and user dependence, providing a comprehensive comparison."}
{"script_processing_qid": "textbook-108-17-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the potential supply-side effects of tax cuts on aggregate supply.\n\nGOLD_ANSWER:\n**Supply-Side Effects**:\n1.  **Labor Supply**: Lower taxes may increase work incentives, raising potential output.\n2.  **Investment**: Tax cuts on capital gains or corporate profits can stimulate investment, enhancing productivity.\n3.  **Laffer Curve**: At very high tax rates, cuts may increase revenue by reducing distortions.\n\n**Criticisms**:\n1.  **Empirical Magnitude**: Most evidence suggests supply-side effects are small in the short run.\n2.  **Income Inequality**: Tax cuts may disproportionately benefit high-income households with lower MPCs.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain enough information to answer this question. While the text discusses demand-side effects of tax cuts (e.g., increased consumer spending, multiplier effects, and household expectations), it does not mention any supply-side mechanisms such as labor incentives, productivity changes, or investment responses that would directly affect aggregate supply.\n\nQID: textbook-108-17-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-108-17-2-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-17-2-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}, "llm_echoed_qid": "textbook-108-17-2-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}
{"script_processing_qid": "textbook-125-26-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate the weighted average cost of capital (WACC) for Holiport's Unifoods division, given a debt cost of 4% and a tax rate of 30%.\n\nGOLD_ANSWER:\n1. **Cost of Equity**: 7.75% (from Q1).\n2. **Cost of Debt**: $r_d = 0.04 \\times (1 - 0.30) = 0.028$ or 2.8%.\n3. **Weights**:\n   - Equity: $564 / (564 + 396) = 0.5875$.\n   - Debt: $396 / (564 + 396) = 0.4125$.\n4. **WACC**: $0.5875 \\times 0.0775 + 0.4125 \\times 0.028 = 0.0556$ or 5.56%.\n\nCANDIDATE_ANSWER:\nStep 1: The WACC formula requires (1) debt cost, (2) equity cost, (3) debt/equity weights, and (4) tax rate.\nStep 2: The question provides debt cost (4%) and tax rate (30%).\nStep 3: The context does not provide Unifoods' equity cost (Re), debt-to-value (D/V), or equity-to-value (E/V) ratios.\nStep 4: Without Re, D/V, or E/V, the WACC cannot be calculated with the given information.\nConclusion: The provided context lacks sufficient data to compute Unifoods' WACC.\n\nQID: textbook-125-26-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-26-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly identifies that the WACC cannot be calculated due to missing information (equity cost, debt-to-value, and equity-to-value ratios), which aligns with the gold answer's implicit requirement for these values.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-26-0-2", "category": "CORRECT", "explanation": "The candidate correctly identifies that the WACC cannot be calculated due to missing information (equity cost, debt-to-value, and equity-to-value ratios), which aligns with the gold answer's implicit requirement for these values."}, "llm_echoed_qid": "textbook-125-26-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly identifies that the WACC cannot be calculated due to missing information (equity cost, debt-to-value, and equity-to-value ratios), which aligns with the gold answer's implicit requirement for these values."}
{"script_processing_qid": "textbook-97-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze Research Problem 12.4. What are the key challenges in reducing the gaps in Tables 12.1-12.3, and why are nonlinear orthogonal arrays important?\n\nGOLD_ANSWER:\n1. **Key Challenges**: \n   - Deriving tighter lower bounds using advanced combinatorial or algebraic techniques.\n   - Discovering new constructions (linear or nonlinear) that achieve smaller $\\lambda$.\n2. **Nonlinear Arrays**: Most known upper bounds rely on linear codes, but nonlinear arrays may offer better (smaller) indices. Their construction requires innovative methods beyond linear algebra.\n3. **Theoretical Gap**: The predominance of linear arrays in tables reflects a lack of general nonlinear constructions, underscoring the need for broader theoretical frameworks.\n\nCANDIDATE_ANSWER:\nStep 1: Identify challenges in reducing gaps in Tables 12.1-12.3\n- The tables rely heavily on linear codes (via Theorem 4.6) to construct OAs, which inherently limits solutions to linear algebraic structures. This restriction may prevent achieving lower λ values that could theoretically exist through nonlinear methods.\n- Property (b) creates cascading dependencies: Improving an OA(N,k,s,t) requires simultaneous optimization of derived OAs with (k-1) factors, (t-1) strength, and scaled run sizes, making holistic optimization difficult.\n- Combinatorial limitations: The requirement for Hadamard matrices (for s=2) and other specialized structures (for s=3,4) imposes existence constraints at certain parameter combinations.\n- Resolution trade-offs: Theorem 11.3 links OA index to fractional factorial resolution, creating dual optimization constraints between experimental design efficiency and statistical power.\n\nStep 2: Explain importance of nonlinear orthogonal arrays\n- Nonlinear OAs can achieve smaller λ values than linear counterparts by escaping the constraints of linear code constructions (which dominate current upper bounds).\n- They enable solutions where linear codes fail: Certain parameters (k,t,s) combinations may lack suitable linear codes but permit nonlinear OA constructions.\n- Provide flexibility for complex designs: Nonlinear arrays can better accommodate mixed-level experiments and irregular combinatorial structures required for modern high-dimensional applications.\n- Address fundamental combinatorial limitations: Nonlinear methods offer alternative pathways when required mathematical objects (e.g., specific Hadamard matrices) remain undiscovered or provably nonexistent.\n\nQID: textbook-97-1-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-97-1-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses both the key challenges in reducing gaps in Tables 12.1-12.3 and the importance of nonlinear orthogonal arrays, aligning well with the gold answer. The detailed reasoning and examples provided in the candidate answer match the points made in the gold answer, demonstrating a comprehensive understanding of the topic.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-97-1-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses both the key challenges in reducing gaps in Tables 12.1-12.3 and the importance of nonlinear orthogonal arrays, aligning well with the gold answer. The detailed reasoning and examples provided in the candidate answer match the points made in the gold answer, demonstrating a comprehensive understanding of the topic."}, "llm_echoed_qid": "textbook-97-1-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses both the key challenges in reducing gaps in Tables 12.1-12.3 and the importance of nonlinear orthogonal arrays, aligning well with the gold answer. The detailed reasoning and examples provided in the candidate answer match the points made in the gold answer, demonstrating a comprehensive understanding of the topic."}
{"script_processing_qid": "textbook-109-0-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What are the implications of the bid–ask spread for market liquidity and efficiency?\n\nGOLD_ANSWER:\n1. **Market Liquidity**: The spread ensures that dealers are compensated for providing liquidity, encouraging their continuous presence in the market. 2. **Market Efficiency**: The spread reflects the costs of trading and information asymmetry, providing a mechanism for price discovery and reducing adverse selection.\n\nCANDIDATE_ANSWER:\nStep 1: Link bid–ask spread to liquidity.\nThe bid–ask spread represents transaction costs for traders. A wider spread increases trading costs, reducing market liquidity as participants face higher costs to execute trades. Conversely, narrower spreads enhance liquidity.\n\nStep 2: Connect spread components to adverse selection and efficiency.\nAdverse selection costs (part of the spread) arise from dealers facing informed traders. Higher adverse selection widens spreads, as dealers protect against losses. This may reduce market efficiency by slowing price discovery, as dealers delay adjusting quotes to new information.\n\nStep 3: Analyze the option analogy’s implications.\nDealer quotes resemble short option positions (bid = put, ask = call). This creates asymmetric risks: dealers face losses if prices move unfavorably post-trade. To compensate, dealers widen spreads, reducing liquidity but ensuring their survival (critical for continuous market-making), which indirectly supports long-term efficiency.\n\nStep 4: Synthesize implications.\nWider spreads:\n- Reduce liquidity (higher transaction costs)\n- May decrease short-term efficiency (delayed price adjustments)\nBut ensure dealer participation, sustaining market-making and long-term efficiency.\nNarrow spreads improve liquidity and efficiency but require dealers to manage order-processing, inventory, and adverse selection costs effectively.\n\nQID: textbook-109-0-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-109-0-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the implications of the bid–ask spread on market liquidity and efficiency, aligning with the gold answer while providing additional detailed reasoning and analysis.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-109-0-1-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the implications of the bid–ask spread on market liquidity and efficiency, aligning with the gold answer while providing additional detailed reasoning and analysis."}, "llm_echoed_qid": "textbook-109-0-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the implications of the bid–ask spread on market liquidity and efficiency, aligning with the gold answer while providing additional detailed reasoning and analysis."}
{"script_processing_qid": "textbook-62-5-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. For $Y(t) = K(t)^{\\alpha}L(t)^{1-\\alpha}$, derive $g_Y(t)$ and explain the role of $\\alpha$.\n\nGOLD_ANSWER:\n1. **Logarithm**: $\\log(Y(t)) = \\alpha \\log(K(t)) + (1-\\alpha) \\log(L(t))$.\n2. **Differentiate**: $g_Y(t) = \\alpha g_K(t) + (1-\\alpha) g_L(t)$.\n3. **Role of $\\alpha$**: $\\alpha$ represents the capital share in output, weighting the contribution of capital growth to output growth.\n\nCANDIDATE_ANSWER:\nStep 1: Take the natural logarithm of both sides of the production function:\n\n$\\ln Y(t) = \\alpha \\ln K(t) + (1-\\alpha) \\ln L(t)$\n\nStep 2: Differentiate both sides with respect to time $t$:\n\n$\\frac{1}{Y(t)}\\frac{dY(t)}{dt} = \\alpha \\frac{1}{K(t)}\\frac{dK(t)}{dt} + (1-\\alpha)\\frac{1}{L(t)}\\frac{dL(t)}{dt}$\n\nStep 3: Recognize that $\\frac{dY/dt}{Y} = g_Y(t)$, $\\frac{dK/dt}{K} = g_K(t)$, and $\\frac{dL/dt}{L} = g_L(t)$. Substitute these into the equation:\n\n$g_Y(t) = \\alpha g_K(t) + (1-\\alpha)g_L(t)$\n\nRole of $\\alpha$:\n- $\\alpha$ represents the output elasticity of capital (i.e., the percentage increase in output from a 1% increase in capital).\n- $(1-\\alpha)$ represents the output elasticity of labor.\n- $\\alpha$ determines the relative weight of capital growth ($g_K$) versus labor growth ($g_L$) in driving output growth ($g_Y$). A higher $\\alpha$ implies capital accumulation contributes more to economic growth, reflecting a more capital-intensive production structure.\n\nQID: textbook-62-5-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-62-5-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's derivation of $g_Y(t)$ matches the gold answer exactly, and their explanation of the role of $\\\\alpha$ is both accurate and more detailed, covering output elasticity and the relative weight of capital versus labor growth.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-62-5-1-2", "category": "CORRECT", "explanation": "The candidate's derivation of $g_Y(t)$ matches the gold answer exactly, and their explanation of the role of $\\alpha$ is both accurate and more detailed, covering output elasticity and the relative weight of capital versus labor growth."}, "llm_echoed_qid": "textbook-62-5-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's derivation of $g_Y(t)$ matches the gold answer exactly, and their explanation of the role of $\\alpha$ is both accurate and more detailed, covering output elasticity and the relative weight of capital versus labor growth."}
{"script_processing_qid": "textbook-125-35-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare and contrast the valuation of a perpetuity versus a growing perpetuity, providing examples for each.\n\nGOLD_ANSWER:\n1.  **Perpetuity**: Fixed payments, $\\mathrm{PV} = \\frac{C}{r}$ (e.g., $\\$100,000$ at $10\\%$ yields $\\$1,000,000$).\n2.  **Growing Perpetuity**: Growing payments, $\\mathrm{PV} = \\frac{C_1}{r - g}$ (e.g., $\\$100,000$ growing at $4\\%$ at $10\\%$ yields $\\$1,666,667$).\n3.  **Key Difference**: Growth rate $g$ adjusts the denominator, increasing PV for growing cash flows.\n\nCANDIDATE_ANSWER:\nStep 1: Define Perpetuity\nA perpetuity is a financial instrument with infinite, constant cash flows. Its present value (PV) is calculated as PV = C / r, where C = annual cash flow and r = discount rate.\n\nStep 2: Example of Perpetuity\nIf a perpetuity pays $100 annually with a 5% required return, PV = $100 / 0.05 = $2,000.\n\nStep 3: Define Growing Perpetuity\nA growing perpetuity has cash flows that grow at a constant rate (g). Its present value is PV = C₁ / (r - g), where C₁ = first-year cash flow (requires r > g).\n\nStep 4: Example of Growing Perpetuity\nIf a perpetuity pays $100 initially, grows at 2% annually, and has a 5% discount rate, PV = $100 / (0.05 - 0.02) = $3,333.33.\n\nStep 5: Key Differences\n- Growth Adjustment: The growing perpetuity formula subtracts the growth rate (g) from the denominator.\n- Value Impact: Growth increases PV (e.g., $3,333.33 > $2,000 in examples above).\n- Practical Use: Perpetuities model fixed-income securities (e.g., consols), while growing perpetuities apply to stocks with growing dividends or long-term projects.\n\nQID: textbook-125-35-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-35-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately defines both perpetuities and growing perpetuities, provides correct examples for each, and clearly explains the key differences, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-35-1-1", "category": "CORRECT", "explanation": "The candidate's answer accurately defines both perpetuities and growing perpetuities, provides correct examples for each, and clearly explains the key differences, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-125-35-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately defines both perpetuities and growing perpetuities, provides correct examples for each, and clearly explains the key differences, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-111-23-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the money market equilibrium condition $M/P = L(r^{\\*}, Y)$, explain why income must fall when the money supply is reduced under floating exchange rates.\n\nGOLD_ANSWER:\n1. **Fixed $r^{\\*}$**: $L(r^{\\*}, Y)$ depends only on $Y$.\\n2. **Reduced $M/P$**: Requires $Y$ to fall to reduce money demand and restore equilibrium.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the money market equilibrium condition \\( M/P = L(r^*, Y) \\). A reduction in the money supply \\( M \\) lowers real balances \\( M/P \\), assuming \\( P \\) is sticky in the short run. \nStep 2: For money market equilibrium, the leftward shift of the \\( LM^* \\) curve (due to lower \\( M/P \\)) requires a decrease in real money demand \\( L(r^*, Y) \\). Since the world interest rate \\( r^* \\) is fixed in the Mundell–Fleming model, the only way to reduce \\( L(r^*, Y) \\) is for income \\( Y \\) to fall (as money demand increases with \\( Y \\)).\nStep 3: Under floating exchange rates, the reduced \\( M \\) causes the domestic currency to appreciate (higher exchange rate \\( \\varepsilon \\)). This appreciation reduces net exports (\\( NX \\)), shifting aggregate demand downward via the \\( IS^* \\) curve.\nStep 4: The combined effect of the leftward \\( LM^* \\) shift and reduced \\( NX \\) leads to a new equilibrium with lower \\( Y \\), restoring money market equilibrium at a lower level of income. Thus, income falls to offset the drop in \\( M/P \\) and maintain \\( M/P = L(r^*, Y) \\).\n\nQID: textbook-111-23-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-23-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly explains the mechanism by which a reduction in the money supply leads to a fall in income under floating exchange rates, aligning with the gold answer's key points about fixed interest rates and the need for reduced money demand.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-23-1-2", "category": "CORRECT", "explanation": "The candidate's answer correctly explains the mechanism by which a reduction in the money supply leads to a fall in income under floating exchange rates, aligning with the gold answer's key points about fixed interest rates and the need for reduced money demand."}, "llm_echoed_qid": "textbook-111-23-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly explains the mechanism by which a reduction in the money supply leads to a fall in income under floating exchange rates, aligning with the gold answer's key points about fixed interest rates and the need for reduced money demand."}
{"script_processing_qid": "textbook-121-12-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the significance of the parameter $b$ in the complementary function $y_{c} = A b^{t}$ for the dynamic stability of equilibrium. How does the value of $b$ determine whether the time path is oscillatory and/or convergent?\n\nGOLD_ANSWER:\n1. **Oscillatory vs. Nonoscillatory**: If $b > 0$, the time path is nonoscillatory. If $b < 0$, the time path is oscillatory (alternates between positive and negative values).\n2. **Convergent vs. Divergent**: If $|b| < 1$, the time path is convergent (approaches zero). If $|b| > 1$, the time path is divergent (grows without bound).\n3. **Special Cases**: If $b = 1$, the time path is constant. If $b = -1$, the time path oscillates perpetually between $+1$ and $-1$.\n\nCANDIDATE_ANSWER:\nStep 1: The complementary function \\( y_c = A b^t \\) characterizes deviations from equilibrium. The parameter \\( b \\) determines both the oscillation and convergence properties of the time path.\n\nStep 2: From the context, \\( b = -a \\). The sign of \\( b \\) determines oscillation:\n- If \\( b < 0 \\) (i.e., \\( a > 0 \\)), \\( (-a)^t \\) alternates sign as \\( t \\) increases, creating oscillations.\n- If \\( b > 0 \\) (i.e., \\( a < 0 \\)), the term \\( (-a)^t \\) remains positive, resulting in nonoscillatory behavior.\n\nStep 3: Convergence depends on \\( |b| \\):\n- If \\( |b| < 1 \\) (i.e., \\( |a| < 1 \\)), \\( b^t \to 0 \\) as \\( t \to \\infty \\), ensuring convergence.\n- If \\( |b| > 1 \\) (i.e., \\( |a| > 1 \\)), \\( |b^t| \\) grows without bound, causing divergence.\n\nConclusion: The value of \\( b \\) determines oscillatory behavior through its sign (\\( b < 0 \\)) and convergence through its magnitude (\\( |b| < 1 \\)).\n\nQID: textbook-121-12-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-121-12-0-1\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the role of \\( b \\) in determining oscillatory and convergent behavior, but introduces an unnecessary and incorrect assumption that \\( b = -a \\), which is not part of the original question or gold answer. This reasoning mismatch detracts from the clarity and accuracy of the explanation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-121-12-0-1", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the role of \\( b \\) in determining oscillatory and convergent behavior, but introduces an unnecessary and incorrect assumption that \\( b = -a \\), which is not part of the original question or gold answer. This reasoning mismatch detracts from the clarity and accuracy of the explanation."}, "llm_echoed_qid": "textbook-121-12-0-1", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the role of \\( b \\) in determining oscillatory and convergent behavior, but introduces an unnecessary and incorrect assumption that \\( b = -a \\), which is not part of the original question or gold answer. This reasoning mismatch detracts from the clarity and accuracy of the explanation."}
{"script_processing_qid": "textbook-67-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Using the cash-in-advance model, derive the first-order conditions for the representative household's optimization problem: $$\\max \\int_{0}^{\\infty} u(c_t, \\ell_t) \\exp(-\\beta t) \\mathrm{d}t$$ subject to the lifetime budget constraint.\n\nGOLD_ANSWER:\n1. **Lagrangian**: $$\\mathcal{L} = \\int_{0}^{\\infty} u(c_t, \\ell_t) \\exp(-\\beta t) \\mathrm{d}t + \\lambda \\left[ b_0 + m_0 + \\int_{0}^{\\infty} (1 - \\ell_t + \\tau_t) \\exp(-r t) \\mathrm{d}t - \\int_{0}^{\\infty} c_t (1 + \\alpha i_t) \\exp(-r t) \\mathrm{d}t \\right].$$\n2. **FOCs**:\n   - $u_c(c_t, \\ell_t) = \\lambda (1 + \\alpha i_t)$,\n   - $u_{\\ell}(c_t, \\ell_t) = \\lambda$.\n3. **Interpretation**: The nominal interest rate $i_t$ drives a wedge between the marginal utility of consumption and leisure.\n\nCANDIDATE_ANSWER:\nStep 1: Define the Household's Problem\nIn the cash-in-advance model, consumption requires real money balances (m_t): c_t ≤ m_t. The household maximizes lifetime utility:\n$$\\max \\int_{0}^{\\infty} u(c_t, \\ell_t) e^{-\\beta t} dt$$\nsubject to:\n1. Cash constraint: c_t ≤ m_t\n2. Budget constraint: \\dot{k} = f(k_t, 1-\\ell_t) - c_t - \\delta k_t - \\dot{m}_t - \\pi_t m_t$\nwhere $\\pi_t$ is inflation rate and $\\delta$ is depreciation.\n\nStep 2: Set Up Hamiltonian\nIntroduce multipliers:\n- $\\lambda_t$ (shadow price of capital)\n- $\\mu_t$ (Lagrange multiplier for cash constraint)\n\nThe Hamiltonian becomes:\n$$H = u(c_t,\\ell_t)e^{-\\beta t} + \\lambda_t[f(k_t,1-\\ell_t) - c_t - \\delta k_t] - \\mu_t(\\dot{m}_t + \\pi_t m_t - c_t)$$\n\nStep 3: Take First-Order Conditions\n\na) Consumption FOC:\n$$\\frac{\\partial H}{\\partial c_t} = u_c e^{-\\beta t} - \\lambda_t + \\mu_t = 0$$\n⇒ $u_c = e^{\\beta t}(\\lambda_t - \\mu_t)$\n\nb) Leisure FOC:\n$$\\frac{\\partial H}{\\partial \\ell_t} = -u_\\ell e^{-\\beta t} + \\lambda_t f_L = 0$$\n⇒ $u_\\ell = \\lambda_t f_L e^{\\beta t}$\n\nc) Capital Euler Equation:\n$$-\\dot{\\lambda}_t = \\frac{\\partial H}{\\partial k_t} = \\lambda_t(f_k - \\delta)$$\n⇒ $\\dot{\\lambda}_t/\\lambda_t = -(f_k - \\delta)$\n\nStep 4: Combine Conditions\nFrom (a) and (b), eliminate $\\lambda_t$:\n$$\\frac{u_\\ell}{u_c} = \\frac{f_L}{1 - (\\mu_t/\\lambda_t)}$$\n\nThe nominal interest rate distortion ($i_t = r_t + \\pi_t$) emerges through the $\\mu_t/\\lambda_t$ ratio, creating a wedge between MRS and MPL.\n\nQID: textbook-67-0-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-67-0-1-0\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate's approach and setup of the Hamiltonian are correct, but the derived first-order conditions do not match the gold answer. The candidate introduces additional elements like capital and inflation, which are not part of the gold answer's simpler cash-in-advance model.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-67-0-1-0", "category": "PARTIALLY_CORRECT", "explanation": "The candidate's approach and setup of the Hamiltonian are correct, but the derived first-order conditions do not match the gold answer. The candidate introduces additional elements like capital and inflation, which are not part of the gold answer's simpler cash-in-advance model."}, "llm_echoed_qid": "textbook-67-0-1-0", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate's approach and setup of the Hamiltonian are correct, but the derived first-order conditions do not match the gold answer. The candidate introduces additional elements like capital and inflation, which are not part of the gold answer's simpler cash-in-advance model."}
{"script_processing_qid": "textbook-97-6-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that the polynomial ring $F[x]$ is a commutative ring with unity but not a field.\n\nGOLD_ANSWER:\n1. **Ring Properties**: $F[x]$ satisfies associativity, commutativity, distributivity, and has additive and multiplicative identities.\n2. **Non-Field**: Not every non-zero polynomial has a multiplicative inverse in $F[x]$.\n3. **Conclusion**: Thus, $F[x]$ is a commutative ring with unity but not a field.\n\nCANDIDATE_ANSWER:\nStep 1: Verify F[x] is a commutative ring with unity\n- All field axioms except multiplicative inverses (Property 8) are satisfied per context\n- Closed under +/*, has additive identity (0 polynomial), multiplicative identity (1 polynomial)\n- Commutativity of multiplication follows from F being a field (coefficients commute)\n\nStep 2: Show F[x] is not a field\n- Take any non-constant polynomial (e.g., f(x) = x)\n- Suppose ∃g(x) ∈ F[x] such that x*g(x) = 1\n- deg(x) + deg(g(x)) = deg(1) ⇒ 1 + deg(g) = 0 → Impossible\n- Non-constant polynomials lack inverses in F[x]\n\nConclusion: F[x] satisfies all ring axioms with commutative multiplication and unity, but fails to satisfy field requirement for universal inverses.\n\nQID: textbook-97-6-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-97-6-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly verifies that $F[x]$ is a commutative ring with unity by checking the necessary axioms and demonstrates that it is not a field by showing that non-constant polynomials lack multiplicative inverses. The reasoning aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-97-6-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly verifies that $F[x]$ is a commutative ring with unity by checking the necessary axioms and demonstrates that it is not a field by showing that non-constant polynomials lack multiplicative inverses. The reasoning aligns with the gold answer."}, "llm_echoed_qid": "textbook-97-6-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly verifies that $F[x]$ is a commutative ring with unity by checking the necessary axioms and demonstrates that it is not a field by showing that non-constant polynomials lack multiplicative inverses. The reasoning aligns with the gold answer."}
{"script_processing_qid": "textbook-100-0-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the trade-offs between objectivity and the use of prior information in scientific inference, referencing the beta-binomial coin example.\n\nGOLD_ANSWER:\n**Trade-offs**:\n- **Objectivity**: Frequentist methods avoid prior influence but may produce nonsensical results (e.g., $\\\\widehat{p}=1$ for 10/10 heads).\n- **Prior Information**: Bayesian methods improve inference when priors are accurate (e.g., $\\\\mathrm{Be}(100,100)$ moderates extreme data).\n- **Key Insight**: Objectivity is not always virtuous; prior information can enhance reliability when scientifically justified.\n\nCANDIDATE_ANSWER:\nStep 1: In the beta-binomial coin example, objectivity refers to relying solely on observed data (e.g., # of heads in coin flips) without prior assumptions. Frequentist methods maintain this objectivity but cannot formally incorporate existing knowledge about coin fairness.\nStep 2: Bayesian methods use a beta prior distribution that encodes prior information (e.g., \"this coin is likely fair\"). This improves inference efficiency when prior information is accurate but introduces subjectivity through prior parameter selection (shape parameters α and β).\nStep 3: The trade-off manifests in three aspects:\n1. Bias vs. Precision: Priors reduce variance (more precise estimates) but risk bias if prior assumptions are incorrect\n2. Reproducibility: Frequentist results are fully determined by data, while Bayesian results depend on prior choice\n3. Model Flexibility: Beta priors allow systematic inclusion of historical knowledge, whereas frequentist methods require ad hoc adjustments to incorporate prior information\nStep 4: The axiom-driven Bayesian approach makes prior use mandatory via Bayes' theorem – rejecting prior information would violate coherence, while frequentist methods preserve apparent objectivity at the cost of potentially ignoring useful information.\n\nQID: textbook-100-0-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-100-0-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the trade-offs between objectivity and prior information in the beta-binomial coin example, aligning closely with the gold answer. It correctly identifies the benefits and drawbacks of both frequentist and Bayesian approaches, and provides additional detailed insights into bias vs. precision, reproducibility, and model flexibility.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-100-0-1-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the trade-offs between objectivity and prior information in the beta-binomial coin example, aligning closely with the gold answer. It correctly identifies the benefits and drawbacks of both frequentist and Bayesian approaches, and provides additional detailed insights into bias vs. precision, reproducibility, and model flexibility."}, "llm_echoed_qid": "textbook-100-0-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the trade-offs between objectivity and prior information in the beta-binomial coin example, aligning closely with the gold answer. It correctly identifies the benefits and drawbacks of both frequentist and Bayesian approaches, and provides additional detailed insights into bias vs. precision, reproducibility, and model flexibility."}
{"script_processing_qid": "textbook-105-6-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Explain the difference between dynamic and static forecasting in the context of autoregressive time series. Why is universal dynamic forecasting impossible?\n\nGOLD_ANSWER:\n1.  **Dynamic Forecasting**: Aims to estimate $\\mathbf{E}\\{Y\\_{n}|Y\\_{0}^{n-1}\\}$ using data up to time $n-1$. The goal is to predict the next value in the sequence.\n2.  **Static Forecasting**: Aims to estimate $\\mathbf{E}\\{Y\\_{0}|Y\\_{-\\infty}^{-1}\\}$ using infinite past data. The goal is to estimate the conditional expectation given the entire history.\n3.  **Impossibility of Universal Dynamic Forecasting**: Theorem 27.1 shows that for any estimator, there exists a stationary ergodic process where the dynamic forecasting error is bounded away from zero with positive probability. This is due to the inability to universally predict complex dependencies in arbitrary ergodic processes.\n\nCANDIDATE_ANSWER:\nStep 1: Define Dynamic Forecasting\nDynamic forecasting aims to estimate E{Y_n | Y₀^{n-1}} sequentially using increasing windows of past data (Y₀ to Y_{n-1}) as time progresses (n→∞). The estimator must adapt to new observations in real-time.\n\nStep 2: Define Static Forecasting\nStatic forecasting estimates E{Y₀ | Y_{-∞}^{-1}} using a fixed target (Y₀) and expanding backward-looking windows (Y_{-n}^{-1} as n→∞). It approximates conditioning on the infinite past without real-time adaptation.\n\nStep 3: Key Difference\nDynamic forecasting uses a forward-expanding window for sequential predictions, while static forecasting uses a backward-expanding window to estimate a fixed conditional expectation.\n\nStep 4: Impossibility of Universal Dynamic Forecasting\nTheorem 27.1 shows that for any proposed dynamic forecasting estimator, there exists a stationary ergodic binary process where the estimator’s error (|estimate - true conditional expectation|) fails to converge to 0. Specifically, the error remains ≥1/4 infinitely often with probability ≥1/8. This constructive proof demonstrates no single estimator can work universally for all stationary ergodic processes.\n\nQID: textbook-105-6-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-105-6-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately defines dynamic and static forecasting, highlights their key differences, and correctly explains the impossibility of universal dynamic forecasting using Theorem 27.1, aligning perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-105-6-1-0", "category": "CORRECT", "explanation": "The candidate answer accurately defines dynamic and static forecasting, highlights their key differences, and correctly explains the impossibility of universal dynamic forecasting using Theorem 27.1, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-105-6-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately defines dynamic and static forecasting, highlights their key differences, and correctly explains the impossibility of universal dynamic forecasting using Theorem 27.1, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-111-16-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate the percentage change in output ($Y$) when the labor force ($L$) increases by 10%, given $\\alpha = 0.3$ and the Cobb-Douglas production function $Y = A K^{\\alpha} L^{1-\\alpha}$.\n\nGOLD_ANSWER:\n1. **Initial Output**: $Y_1 = A K^{0.3} L^{0.7}$.\n2. **Final Output**: $Y_2 = A K^{0.3} (1.1 L)^{0.7}$.\n3. **Percentage Change**:\n   $$ \\frac{Y_2}{Y_1} = \\frac{A K^{0.3} (1.1 L)^{0.7}}{A K^{0.3} L^{0.7}} = (1.1)^{0.7} \\approx 1.069. $$\n   Output increases by approximately 6.9%.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that the Cobb-Douglas production function Y = A K^{0.3} L^{0.7} implies output elasticity with respect to labor is the exponent 1-α = 0.7.\nStep 2: Calculate percentage change in output using the formula:\n%ΔY ≈ (Labor elasticity) × (%ΔL)\nStep 3: Substitute values:\n%ΔY = 0.7 × 10% = 7%\nConclusion: A 10% increase in labor force leads to a 7% increase in output.\n\nQID: textbook-111-16-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-111-16-0-2\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly calculates the percentage change in output as 7%, which matches the gold answer's approximation of 6.9%. However, the reasoning differs: the candidate uses the elasticity formula directly, while the gold answer derives the change through the production function.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-16-0-2", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly calculates the percentage change in output as 7%, which matches the gold answer's approximation of 6.9%. However, the reasoning differs: the candidate uses the elasticity formula directly, while the gold answer derives the change through the production function."}, "llm_echoed_qid": "textbook-111-16-0-2", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly calculates the percentage change in output as 7%, which matches the gold answer's approximation of 6.9%. However, the reasoning differs: the candidate uses the elasticity formula directly, while the gold answer derives the change through the production function."}
{"script_processing_qid": "textbook-125-13-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Unlever the WACC for Sangria Corporation given $r_D = 8%$, $r_E = 14.6%$, and $D/V = 40%$. What is the opportunity cost of capital ($r$)?\n\nGOLD_ANSWER:\n1. **Formula**: $r = r_D(D/V) + r_E(E/V)$\n2. **Substitute values**: $r = 0.08(0.4) + 0.146(0.6)$\n3. **Calculate**: $r = 0.032 + 0.0876 = 0.1196$ or 11.96%.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the given values\n- Cost of debt (r_D) = 8% = 0.08\n- Cost of equity (r_E) = 14.6% = 0.146\n- Debt-to-value ratio (D/V) = 40% = 0.4\n- Equity-to-value ratio (E/V) = 1 - D/V = 60% = 0.6\n\nStep 2: Apply the unlevering formula from the context\n$$r = r_{D}(D/V) + r_{E}(E/V)$$\n\nStep 3: Substitute values\n$$r = (0.08 \\times 0.4) + (0.146 \\times 0.6)$$\n\nStep 4: Calculate\n$$r = 0.032 + 0.0876 = 0.1196$$\n\nStep 5: Convert to percentage\nOpportunity cost of capital = 11.96%\n\nQID: textbook-125-13-2-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-13-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly identified the given values, applied the unlevering formula, substituted the values accurately, and calculated the opportunity cost of capital as 11.96%, which matches the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-13-2-0", "category": "CORRECT", "explanation": "The candidate correctly identified the given values, applied the unlevering formula, substituted the values accurately, and calculated the opportunity cost of capital as 11.96%, which matches the gold answer."}, "llm_echoed_qid": "textbook-125-13-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly identified the given values, applied the unlevering formula, substituted the values accurately, and calculated the opportunity cost of capital as 11.96%, which matches the gold answer."}
{"script_processing_qid": "textbook-117-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the expected return of a portfolio composed of 60% Supertech (expected return 17.5%) and 40% Slowpoke (expected return 5.5%). Show the algebraic steps and interpret the result.\n\nGOLD_ANSWER:\n1. **Given**: $X\\_{\\mathrm{Super}} = 0.6$, $\\overline{R}\\_{\\mathrm{Super}} = 17.5\\%$, $X\\_{\\mathrm{Slow}} = 0.4$, $\\overline{R}\\_{\\mathrm{Slow}} = 5.5\\%$.\n2. **Formula**: $\\overline{R}\\_{P} = X\\_{\\mathrm{Super}} \\overline{R}\\_{\\mathrm{Super}} + X\\_{\\mathrm{Slow}} \\overline{R}\\_{\\mathrm{Slow}}$.\n3. **Calculation**: $\\overline{R}\\_{P} = 0.6 \\times 17.5\\% + 0.4 \\times 5.5\\% = 10.5\\% + 2.2\\% = 12.7\\%$.\n4. **Interpretation**: The portfolio's expected return is a weighted average of the individual securities' returns, reflecting their proportional investment.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the formula for portfolio expected return:\nExpected return on portfolio = X_Super * R_Super + X_Slow * R_Slow\n\nStep 2: Substitute given values:\nX_Super = 60% = 0.6\nR_Super = 17.5%\nX_Slow = 40% = 0.4\nR_Slow = 5.5%\n\nStep 3: Calculate weighted components:\nSupertech contribution = 0.6 * 17.5% = 10.5%\nSlowpoke contribution = 0.4 * 5.5% = 2.2%\n\nStep 4: Sum components:\nPortfolio expected return = 10.5% + 2.2% = 12.7%\n\nInterpretation: The 12.7% expected return lies between the two individual asset returns, reflecting the weighted average nature of portfolio returns. The higher weight on Supertech (60%) pulls the portfolio return closer to its 17.5% return, while the 40% allocation to Slowpoke reduces the portfolio's overall expected return compared to a 100% Supertech allocation.\n\nQID: textbook-117-1-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-1-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the algebraic steps to derive the portfolio's expected return and provides an accurate interpretation of the result. The calculations and reasoning align perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-1-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the algebraic steps to derive the portfolio's expected return and provides an accurate interpretation of the result. The calculations and reasoning align perfectly with the gold answer."}, "llm_echoed_qid": "textbook-117-1-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the algebraic steps to derive the portfolio's expected return and provides an accurate interpretation of the result. The calculations and reasoning align perfectly with the gold answer."}
{"script_processing_qid": "textbook-65-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that $d\\_{k}$ defined in Lemma 13.6 is a semi-metric on $\\mathcal{H}$.\n\nGOLD_ANSWER:\n1. **Non-Negativity**: $d\\_{k}(\\chi^{\\prime}, \\chi^{\\prime\\prime}) \\geq 0$ by definition.\n2. **Identity of Indiscernibles**: $d\\_{k}(\\chi^{\\prime}, \\chi^{\\prime\\prime}) = 0$ implies $\\sum\\_{j=1}^{k} <\\chi^{\\prime} - \\chi^{\\prime\\prime}, e\\_{j}>^{2} = 0$, which implies $<\\chi^{\\prime} - \\chi^{\\prime\\prime}, e\\_{j}> = 0$ for all $j$. However, this does not necessarily imply $\\chi^{\\prime} = \\chi^{\\prime\\prime}$ unless $k \\to \\infty$.\n3. **Triangle Inequality**: Follows from the Euclidean metric properties of the finite-dimensional projection.\n\nCANDIDATE_ANSWER:\nStep 1: Verify non-negativity.\nSince each term <χ'−χ'',e_j>² is squared (non-negative), the sum ∑_{j=1}^k <χ'−χ'',e_j>² ≥ 0. The square root preserves non-negativity, so d_k(χ',χ'') ≥ 0 for all χ',χ'' ∈ ℋ.\n\nStep 2: Confirm symmetry.\nThe inner product squared <χ'−χ'',e_j>² = <χ''−χ',e_j>² due to symmetry of the inner product. Thus, d_k(χ',χ'') = d_k(χ'',χ').\n\nStep 3: Prove triangle inequality.\nLet P_k(·) = ∑_{j=1}^k <·,e_j>e_j be the projection operator onto the first k basis vectors. Then d_k(χ',χ'') = ‖P_k(χ'−χ'')‖ℋ. By the triangle inequality for norms in ℝ^k:\n‖P_k(χ'−χ'')‖ ≤ ‖P_k(χ'−χ''')‖ + ‖P_k(χ'''−χ'')‖\n⇒ d_k(χ',χ'') ≤ d_k(χ',χ''') + d_k(χ''',χ'').\n\nStep 4: Note semi-metric property.\nd_k(χ',χ'') = 0 implies χ'−χ'' is orthogonal to {e₁,...,e_k}, but not necessarily χ'=χ'' (since components beyond k may differ). Thus, d_k satisfies all semi-metric axioms (non-negativity, symmetry, triangle inequality) but not the identity of indiscernibles.\n\nQID: textbook-65-0-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-65-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly verifies all properties of a semi-metric (non-negativity, symmetry, triangle inequality) and explicitly notes the failure of the identity of indiscernibles, matching the gold answer's reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-65-0-1-0", "category": "CORRECT", "explanation": "The candidate answer correctly verifies all properties of a semi-metric (non-negativity, symmetry, triangle inequality) and explicitly notes the failure of the identity of indiscernibles, matching the gold answer's reasoning."}, "llm_echoed_qid": "textbook-65-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly verifies all properties of a semi-metric (non-negativity, symmetry, triangle inequality) and explicitly notes the failure of the identity of indiscernibles, matching the gold answer's reasoning."}
{"script_processing_qid": "textbook-99-9-3-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Compare the performance of different kernels (box, triangle, normal) in the Nadaraya-Watson estimator.\n\nGOLD_ANSWER:\n1. **Box Kernel**: Discontinuous, leads to rough estimates; inherits lack of smoothness.\n2. **Triangle Kernel**: Continuous but not smooth; provides intermediate smoothness.\n3. **Normal Kernel**: Infinitely differentiable; produces smooth estimates but may oversmooth.\n4. **Practical Choice**: Normal kernel is often preferred for smoothness, while box/triangle may be used for computational simplicity.\n\nCANDIDATE_ANSWER:\nStep 1: Understand kernel roles in the Nadaraya-Watson estimator. Kernels determine how neighboring observations are weighted. \nStep 2: Box kernel (uniform weights within bandwidth) produces piecewise-constant estimates with high variance but low computational complexity. \nStep 3: Triangle kernel (linearly decaying weights) offers smoother estimates than box kernel by reducing weight discontinuity at bandwidth boundaries. \nStep 4: Normal kernel (exponentially decaying weights) creates the smoothest estimates but requires more computations due to non-compact support. Figure 8.16 specifically shows the normal kernel effectively captures the Income-Percentage relationship with appropriate smoothness. \nConclusion: Normal kernel generally provides better bias-variance tradeoff for continuous relationships, while box/triangle kernels may be preferred for computational simplicity or discontinuous patterns (though context doesn't provide quantitative performance metrics).\n\nQID: textbook-99-9-3-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-99-9-3-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately describes the performance characteristics of the box, triangle, and normal kernels in the Nadaraya-Watson estimator, aligning with the gold answer. It correctly notes the trade-offs between smoothness and computational complexity, and the practical implications of each kernel choice.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-9-3-2", "category": "CORRECT", "explanation": "The candidate answer accurately describes the performance characteristics of the box, triangle, and normal kernels in the Nadaraya-Watson estimator, aligning with the gold answer. It correctly notes the trade-offs between smoothness and computational complexity, and the practical implications of each kernel choice."}, "llm_echoed_qid": "textbook-99-9-3-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately describes the performance characteristics of the box, triangle, and normal kernels in the Nadaraya-Watson estimator, aligning with the gold answer. It correctly notes the trade-offs between smoothness and computational complexity, and the practical implications of each kernel choice."}
{"script_processing_qid": "textbook-99-5-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Why does the data-driven estimator $\\hat{F}_{0} = \\sum_{j=0}^{J_{0}}(\\hat{\\theta}_{j}^{2} - n^{-1})$ outperform the plug-in estimator for $\\beta > 0.25$?\n\nGOLD_ANSWER:\n- **Plug-in Limitation**: Requires $\\beta > 0.5$ for $n^{-1}$ rate due to bias $\\sum_{j>J}\\theta_{j}^{2}$.  \n- **Data-Driven Advantage**: Unbiased term $(\\hat{\\theta}_{j}^{2} - n^{-1})$ cancels noise bias, and $J_{0} = 2\\lfloor n/\\ln n\\rfloor$ ensures $\\sum_{j>J_{0}}\\theta_{j}^{2} = O((\\ln n/n)^{2\\beta}) = o(n^{-1})$ for $\\beta > 0.25$. Thus, MSE $= 4F_{0}(f)n^{-1}(1 + o(1))$ holds for $\\beta > 0.25$.\n\nCANDIDATE_ANSWER:\nStep 1: The plug-in estimator $F_0(\\hat{f}) = \\sum_{j=0}^J \\hat{\\theta}_j^2$ introduces bias because $\\mathbb{E}[\\hat{\\theta}_j^2] = \\theta_j^2 + \\text{Var}(\\hat{\\theta}_j) = \\theta_j^2 + n^{-1}$. This results in a cumulative bias of $(J+1)n^{-1}$ for truncation at $J$ terms.\n\nStep 2: For $\\beta > 0.25$, the optimal truncation $J = 2\\lfloor n^{1/(2\\beta+1)}\\rfloor$ grows slowly (since $\\beta$ reduces $1/(2\\beta+1)$). The plug-in estimator’s bias $(J+1)n^{-1}$ becomes non-negligible compared to its variance $\\approx Jn^{-1}$.\n\nStep 3: The data-driven estimator $\\hat{F}_0 = \\sum_{j=0}^{J_0}(\\hat{\\theta}_j^2 - n^{-1})$ uses $J_0 = 2\\lfloor n/\\ln n\\rfloor$, which is much larger than $J$ for $\\beta > 0.25$. The $n^{-1}$ subtraction removes the variance-induced bias per term, making the estimator asymptotically unbiased.\n\nStep 4: For $\\beta > 0.25$, the Sobolev smoothness ensures rapid decay of $\\theta_j^2$ terms. Including more terms up to $J_0$ adds minimal bias (due to negligible $\\theta_j^2$ for $j > J$), while the variance remains controlled due to the $n^{-1}$ correction. This results in a lower MSE compared to the plug-in estimator, which suffers from residual bias from truncated terms.\n\nQID: textbook-99-5-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-5-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the limitations of the plug-in estimator and the advantages of the data-driven estimator, aligning with the gold answer's reasoning. It accurately explains the bias-variance trade-off and the role of the $n^{-1}$ correction for $\\beta > 0.25$.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-5-1-1", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the limitations of the plug-in estimator and the advantages of the data-driven estimator, aligning with the gold answer's reasoning. It accurately explains the bias-variance trade-off and the role of the $n^{-1}$ correction for $\beta > 0.25$."}, "llm_echoed_qid": "textbook-99-5-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the limitations of the plug-in estimator and the advantages of the data-driven estimator, aligning with the gold answer's reasoning. It accurately explains the bias-variance trade-off and the role of the $n^{-1}$ correction for $\beta > 0.25$."}
{"script_processing_qid": "textbook-117-45-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain the concept of 'value of growth opportunities' (VGO) and calculate it for California Electronics.\n\nGOLD_ANSWER:\n1. **VGO Definition**: The present value of future investments that are expected to generate returns above the cost of capital.\\n2. **Base Price (No Growth)**: Earnings = $\\$100M; Dividend = $\\$100M / 20M shares = $\\$5 per share. Price = $\\$5 / 0.15 = \\$33.33 per share.\\n3. **NPV of Project**: Initial Outlay = $\\$15M at t=0 + $\\$5M at t=1 (PV = $\\$4.3478M). Perpetual Earnings = $\\$10M starting at t=2 (PV at t=1 = $\\$10M / 0.15 = \\$66.6667M; PV at t=0 = $\\$66.6667M / (1.15)^1 = \\$57.9712M). NPV = $\\$57.9712M - \\$15M - \\$4.3478M = \\$38.6234M.\\n4. **VGO per Share**: $\\$38.6234M / 20M shares = \\$1.9312$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand VGO concept\nValue of Growth Opportunities (VGO) represents the present value of future earnings from new investments minus the present value of required capital outlays. It measures how much value a growth project adds to the firm.\n\nStep 2: Calculate PV of additional earnings\nNew project generates $10M/year perpetuity starting in Year 2:\nPV (Year 2) = $10M / 15% = $66.6667M\nDiscount to Year 0:\n$66.6667M / (1.15)^2 = $50.413M\n\nStep 3: Calculate PV of investments\nInitial investment: $15M (Year 0)\nSecond investment: $5M (Year 1)\nPV of Year 1 investment:\n$5M / 1.15 = $4.3478M\nTotal PV investments = $15M + $4.3478M = $19.3478M\n\nStep 4: Compute VGO\nVGO = PV earnings - PV investments\nVGO = $50.413M - $19.3478M = $31.0652M\n\nFinal VGO = $31.07 million (rounded)\n\nThis represents the net value created by the new growth opportunity for California Electronics.\n\nQID: textbook-117-45-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-45-0-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly understood the VGO concept and the calculation steps, but made an error in discounting the perpetual earnings (used Year 2 instead of Year 1 for discounting to Year 0). This led to an incorrect final VGO value.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-45-0-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly understood the VGO concept and the calculation steps, but made an error in discounting the perpetual earnings (used Year 2 instead of Year 1 for discounting to Year 0). This led to an incorrect final VGO value."}, "llm_echoed_qid": "textbook-117-45-0-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly understood the VGO concept and the calculation steps, but made an error in discounting the perpetual earnings (used Year 2 instead of Year 1 for discounting to Year 0). This led to an incorrect final VGO value."}
{"script_processing_qid": "textbook-65-0-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Compare the small ball probabilities for $X$ satisfying (13.4) and (13.5).\n\nGOLD_ANSWER:\n1. **Case (13.4)**: $P(X = x) = \\delta > 0$ implies $\\varphi\\_{x}(\\epsilon) \\geq \\delta$ for all $\\epsilon > 0$.\n2. **Case (13.5)**: $\\varphi\\_{x}(\\epsilon) \\sim C \\epsilon^{\\tau} + o(\\epsilon^{\\tau})$ for $\\tau > 0$.\n3. **Implications**: (13.4) leads to constant small ball probabilities, while (13.5) leads to polynomial decay.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the small ball probability results for each condition.\n- For (13.4): Lemma 13.9 states φₓ(ε) ≥ C (constant lower bound for all ε > 0).\n- For (13.5): Lemma 13.11 states φₓ(ε) = Cε^τ + o(ε^τ) (polynomial decay in ε).\n\nStep 2: Compare decay rates.\n- Under (13.4), small ball probabilities do not vanish as ε → 0 (non-decaying behavior). This typically occurs for distributions with atoms (e.g., discrete jumps).\n- Under (13.5), probabilities decay polynomially with rate τ > 0 as ε → 0. This reflects power-law density behavior near the point.\n\nStep 3: Summary of key difference.\n- (13.4) implies φₓ(ε) is bounded away from zero, while (13.5) implies φₓ(ε) → 0 at rate ε^τ as ε → 0.\n\nQID: textbook-65-0-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-65-0-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies and compares the small ball probabilities for both cases (13.4) and (13.5), matching the gold answer's implications of constant vs. polynomial decay.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-65-0-2-1", "category": "CORRECT", "explanation": "The candidate answer accurately identifies and compares the small ball probabilities for both cases (13.4) and (13.5), matching the gold answer's implications of constant vs. polynomial decay."}, "llm_echoed_qid": "textbook-65-0-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies and compares the small ball probabilities for both cases (13.4) and (13.5), matching the gold answer's implications of constant vs. polynomial decay."}
{"script_processing_qid": "textbook-125-39-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Critically assess the limitations of using financial ratios for cross-industry comparisons.\n\nGOLD_ANSWER:\n1.  **Accounting Practices**: Different depreciation or inventory methods distort comparisons.\n2.  **Capital Intensity**: Industries vary in asset/liability structures.\n3.  **Business Models**: High-margin vs. high-volume strategies affect ratio benchmarks.\n\nCANDIDATE_ANSWER:\nStep 1: Identify inherent differences in capital structure across industries. The debt-equity ratio and return on equity (ROE) are influenced by industry-specific leverage norms. For instance, capital-intensive industries (e.g., utilities) typically carry higher debt than tech firms, making direct comparisons misleading. The context explicitly links ROE to leverage (ROE increases when return on assets exceeds debt interest rates), which varies by industry.\n\nStep 2: Highlight divergent operating cycles. The average collection period and current ratio reflect liquidity needs tied to industry practices. Retail industries may have shorter collection periods and higher liquidity (current ratio) than manufacturing, where longer production cycles are common. These ratios lack universal benchmarks.\n\nStep 3: Note ROE's dependency on industry-specific financing costs. The context states that ROE rises when return on assets surpasses debt interest rates. Industries with lower borrowing costs (e.g., regulated sectors) may artificially inflate ROE relative to industries with higher interest rates, even if operational efficiency is comparable.\n\nStep 4: Emphasize the lack of standardization in ratio interpretation. For example, a 'high' debt-equity ratio may signal risk in one industry but reflect normal operations in another. The provided ratios (e.g., current ratio, payout ratio) do not account for these contextual differences, limiting their cross-industry validity.\n\nQID: textbook-125-39-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-39-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses the limitations of using financial ratios for cross-industry comparisons, aligning well with the gold answer by detailing differences in capital structure, operating cycles, financing costs, and lack of standardization.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-39-2-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses the limitations of using financial ratios for cross-industry comparisons, aligning well with the gold answer by detailing differences in capital structure, operating cycles, financing costs, and lack of standardization."}, "llm_echoed_qid": "textbook-125-39-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses the limitations of using financial ratios for cross-industry comparisons, aligning well with the gold answer by detailing differences in capital structure, operating cycles, financing costs, and lack of standardization."}
{"script_processing_qid": "textbook-100-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the frequentist interpretation of confidence intervals is often misunderstood as a probabilistic statement about the parameter.\n\nGOLD_ANSWER:\nThe frequentist interpretation relies on the **long-run frequency** of the interval-generating procedure capturing the true parameter, not the probability of the parameter lying in a specific interval. Misinterpretation arises because:\n- The phrasing (e.g., \\\"95\\\\% confident\\\") resembles Bayesian probability statements.\n- Users intuitively want a direct probability statement about parameters, which frequentist methods cannot provide without a prior distribution.\n\nCANDIDATE_ANSWER:\nStep 1: Frequentist confidence intervals are defined by their coverage probability in repeated sampling. The confidence level (e.g., 95%) refers to the long-run frequency of intervals containing the true parameter across hypothetical replications of the experiment.\n\nStep 2: Parameters are treated as fixed (non-random) in frequentism. The interval itself (L, U) is random because it depends on the observed data. After computation, the specific interval is fixed, and no probability statement applies to it.\n\nStep 3: The misunderstanding arises because people conflate the frequentist interpretation with Bayesian probability. When using phrases like \"95% sure,\" listeners incorrectly attribute this to a posterior probability that the parameter lies in the computed interval (a Bayesian credibility interval interpretation).\n\nStep 4: In contrast, Bayesian credibility intervals explicitly quantify uncertainty about the parameter's value using the posterior distribution. This aligns with intuitive notions of probability, leading to persistent misinterpretation of frequentist intervals despite Neyman's clarification.\n\nQID: textbook-100-0-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-100-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the frequentist interpretation of confidence intervals, the source of common misunderstandings, and contrasts it with Bayesian credibility intervals, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-100-0-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains the frequentist interpretation of confidence intervals, the source of common misunderstandings, and contrasts it with Bayesian credibility intervals, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-100-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the frequentist interpretation of confidence intervals, the source of common misunderstandings, and contrasts it with Bayesian credibility intervals, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-111-10-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the sticky-price model, derive the SRAS equation Y = Ȳ + α(P - P^e). Show all steps, including the expression for the overall price level P.\n\nGOLD_ANSWER:\n1. **Desired Price**: Flexible-price firms set p = P + a(Y - Ȳ).\n2. **Sticky-Price Firms**: Set p = P^e.\n3. **Overall Price Level**: P = sP^e + (1 - s)[P + a(Y - Ȳ)], where s is the fraction of sticky-price firms.\n4. **Solve for P**: Rearrange to get P = P^e + [(1 - s)a/s](Y - Ȳ).\n5. **SRAS Equation**: Solve for Y to obtain Y = Ȳ + α(P - P^e), where α = s/[(1 - s)a].\n\nCANDIDATE_ANSWER:\nStep 1: Start with the firm's desired price equation given in the sticky-price model: p = P + a(Y - Ȳ), where 'a' measures price responsiveness to output gaps.<br>Step 2: Recognize that some prices are sticky (set based on expected price level P^e) while others adjust immediately. Let 's' be the fraction of firms with sticky prices.<br>Step 3: Express the aggregate price level as a weighted average: P = sP^e + (1-s)p, where (1-s) firms set prices flexibly (using p = P + a(Y - Ȳ)).<br>Step 4: Substitute the flexible-price firms' pricing rule into the aggregate price equation: P = sP^e + (1-s)[P + a(Y - Ȳ)].<br>Step 5: Solve algebraically for Y:<br>• Expand equation: P = sP^e + (1-s)P + (1-s)a(Y - Ȳ)<br>• Subtract (1-s)P from both sides: sP = sP^e + (1-s)a(Y - Ȳ)<br>• Divide both sides by s: P = P^e + [(1-s)/s]a(Y - Ȳ)<br>Step 6: Rearrange to get Y in terms of P and P^e:<br>Y = Ȳ + [s/(1-s)a](P - P^e)<br>Step 7: Let α = s/[(1-s)a] to obtain the final SRAS curve: Y = Ȳ + α(P - P^e)\n\nQID: textbook-111-10-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-111-10-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the SRAS equation, including the expression for the overall price level P, and arrives at the correct final equation Y = Ȳ + α(P - P^e). The reasoning and calculations align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-10-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the SRAS equation, including the expression for the overall price level P, and arrives at the correct final equation Y = Ȳ + α(P - P^e). The reasoning and calculations align with the gold answer."}, "llm_echoed_qid": "textbook-111-10-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the SRAS equation, including the expression for the overall price level P, and arrives at the correct final equation Y = Ȳ + α(P - P^e). The reasoning and calculations align with the gold answer."}
{"script_processing_qid": "textbook-117-10-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. (Qualitative) Discuss the implications of Sloan's findings on accruals and stock returns for the semi-strong form of market efficiency.\n\nGOLD_ANSWER:\n4. **Sloan's Findings and Market Efficiency**: \n   - Sloan found stocks with high accruals underperform, suggesting investors react slowly to accounting information.\n   - **Implication**: Semi-strong efficiency assumes all public information is immediately reflected in prices. Sloan's results contradict this, indicating possible market inefficiency.\n   - **Strategy**: Buying low-accrual stocks and selling high-accrual stocks yields abnormal returns, inconsistent with efficient markets.\n\nCANDIDATE_ANSWER:\nStep 1: Recall Sloan's key finding – investors tend to overprice stocks with high accruals (less persistent earnings) and underreact to the lower quality of accruals vs. cash flows, leading to predictable future underperformance.\\n\\nStep 2: Analyze semi-strong efficiency requirements per context – markets should fully incorporate all public information (including accounting data) into prices. The context specifies that accounting choices shouldn't affect prices if (a) sufficient disclosure exists to reconstruct earnings under alternative methods, and (b) markets are semi-strong efficient.\\n\\nStep 3: Apply Sloan's findings – high accruals (an accounting choice visible in public filings) predictably lead to lower returns. This violates condition (b), as the market fails to properly process this public information about earnings quality.\\n\\nStep 4: Conclusion – Sloan's evidence challenges semi-strong efficiency by demonstrating that investors systematically misprice accruals information that is (1) publicly available and (2) theoretically reconstructible, suggesting markets don't fully incorporate all public accounting data.\n\nQID: textbook-117-10-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-10-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately summarizes Sloan's findings and correctly applies them to challenge the semi-strong form of market efficiency. The detailed steps align with the implications outlined in the gold answer, demonstrating a clear understanding of the topic.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-10-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately summarizes Sloan's findings and correctly applies them to challenge the semi-strong form of market efficiency. The detailed steps align with the implications outlined in the gold answer, demonstrating a clear understanding of the topic."}, "llm_echoed_qid": "textbook-117-10-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately summarizes Sloan's findings and correctly applies them to challenge the semi-strong form of market efficiency. The detailed steps align with the implications outlined in the gold answer, demonstrating a clear understanding of the topic."}
{"script_processing_qid": "textbook-117-29-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate the average daily float for a firm with the following monthly receipts: \n   - Item 1: $\\$5,000,000$ with 3 days' delay \n   - Item 2: $\\$3,000,000$ with 5 days' delay. Show the two methods of calculation.\n\nGOLD_ANSWER:\n1. **Method 1: Total Float Divided by Days** \n   $$ \\text{Total Float} = (\\$5,000,000 \\times 3) + (\\$3,000,000 \\times 5) = \\$30,000,000 $$\n   $$ \\text{Average Daily Float} = \\frac{\\$30,000,000}{30} = \\$1,000,000 $$\n2. **Method 2: Weighted Average Delay** \n   $$ \\text{Weighted Average Delay} = (5/8) \\times 3 + (3/8) \\times 5 = 3.75 \\text{ days} $$\n   $$ \\text{Average Daily Receipts} = \\frac{\\$8,000,000}{30} = \\$266,667 $$\n   $$ \\text{Average Daily Float} = \\$266,667 \\times 3.75 = \\$1,000,000 $$\n\nCANDIDATE_ANSWER:\nStep 1: Calculate total float using Method 1 (Total Delay Dollars/Days in Period)\n- Item 1 float: $5,000,000 × 3 days = $15,000,000\n- Item 2 float: $3,000,000 × 5 days = $15,000,000\n- Total float = $15,000,000 + $15,000,000 = $30,000,000\n- Average daily float = $30,000,000 ÷ 30 days = $1,000,000\n\nStep 2: Calculate using Method 2 (Average Daily Receipts × Weighted Average Delay)\n- Total monthly receipts = $5,000,000 + $3,000,000 = $8,000,000\n- Average daily receipts = $8,000,000 ÷ 30 ≈ $266,666.67\n- Weighted average delay = (($5M × 3) + ($3M × 5)) ÷ $8M = (15 + 15) ÷ 8 = 3.75 days\n- Average daily float = $266,666.67 × 3.75 ≈ $1,000,000\n\nQID: textbook-117-29-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-29-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows both methods of calculation as outlined in the gold answer, arriving at the same results for the average daily float.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-29-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows both methods of calculation as outlined in the gold answer, arriving at the same results for the average daily float."}, "llm_echoed_qid": "textbook-117-29-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows both methods of calculation as outlined in the gold answer, arriving at the same results for the average daily float."}
{"script_processing_qid": "textbook-111-23-3-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. How does incorporating income into the net exports function ($NX = NX(e, Y)$) alter the effectiveness of fiscal policy under floating exchange rates? Provide a graphical explanation.\n\nGOLD_ANSWER:\n1. **Standard Model**: Fiscal policy ineffective (horizontal $L M^{\\*}$).\\n2. **Extended Model**: $NX$ falls as $Y$ rises, making $I S^{\\*}$ flatter. Fiscal policy now affects $Y$ even under floating rates.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that in the standard Mundell-Fleming model with floating exchange rates and NX = NX(e), fiscal policy is ineffective because currency appreciation fully offsets demand stimulus.\n\nStep 2: Incorporating Y into NX(e, Y) introduces a new channel: Higher income (Y↑) from fiscal expansion increases imports, directly reducing net exports (NX↓).\n\nStep 3: This creates two simultaneous effects:\n(A) Expansionary fiscal policy (G↑) shifts IS right\n(B) Higher Y reduces NX (through Y term) and exchange rate appreciation (e↓) reduces NX further\n\nStep 4: Graphically:\n- Original LM* curve remains vertical\n- The IS* curve becomes steeper because NX now depends on both Y and e\n- Fiscal expansion causes smaller rightward shift in IS* compared to basic model\n\nStep 5: Final equilibrium shows smaller output gain than standard model due to dual leakage: (1) import-driven NX reduction from Y↑, and (2) exchange rate appreciation from capital inflows.\n\nQID: textbook-111-23-3-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-23-3-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes how incorporating income into the net exports function alters fiscal policy effectiveness under floating exchange rates, including the graphical implications of a steeper IS* curve and smaller output gain. This aligns perfectly with the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-23-3-1", "category": "CORRECT", "explanation": "The candidate's answer accurately describes how incorporating income into the net exports function alters fiscal policy effectiveness under floating exchange rates, including the graphical implications of a steeper IS* curve and smaller output gain. This aligns perfectly with the gold answer's key points."}, "llm_echoed_qid": "textbook-111-23-3-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes how incorporating income into the net exports function alters fiscal policy effectiveness under floating exchange rates, including the graphical implications of a steeper IS* curve and smaller output gain. This aligns perfectly with the gold answer's key points."}
{"script_processing_qid": "textbook-117-43-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Why might a firm choose to hold excess cash despite the opportunity cost?\n\nGOLD_ANSWER:\nFirms may hold excess cash due to:\n1. **Uncertainty**: To mitigate risks associated with unpredictable cash flows.\n2. **Future Investments**: To capitalize on strategic opportunities that may arise.\n3. **Creditworthiness**: Maintaining higher cash reserves can improve the firm's credit rating and borrowing terms.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the three key motives for holding cash from the context: transactions, precautionary, and speculative motives.\nStep 2: For each motive, explain how it justifies holding excess cash despite opportunity costs:\n   - Transactions motive: Ensures liquidity for daily operations (e.g., paying suppliers, salaries), avoiding operational disruptions even if cash could earn returns elsewhere.\n   - Precautionary motive: Acts as a buffer against unforeseen events (e.g., economic downturns, emergencies), reducing reliance on costly external financing during crises.\n   - Speculative motive: Provides flexibility to capitalize on unexpected opportunities (e.g., strategic acquisitions, favorable market conditions), where potential gains may outweigh forgone investment returns.\n\nQID: textbook-117-43-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-43-0-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies valid reasons for holding excess cash (transactions, precautionary, and speculative motives), but these reasons do not directly align with the specific points (uncertainty, future investments, creditworthiness) provided in the gold answer. The reasoning is sound but mismatched with the gold answer's focus.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-43-0-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies valid reasons for holding excess cash (transactions, precautionary, and speculative motives), but these reasons do not directly align with the specific points (uncertainty, future investments, creditworthiness) provided in the gold answer. The reasoning is sound but mismatched with the gold answer's focus."}, "llm_echoed_qid": "textbook-117-43-0-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies valid reasons for holding excess cash (transactions, precautionary, and speculative motives), but these reasons do not directly align with the specific points (uncertainty, future investments, creditworthiness) provided in the gold answer. The reasoning is sound but mismatched with the gold answer's focus."}
{"script_processing_qid": "textbook-119-16-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Show the relationship between the t-statistic (14.26) and the F-statistic (202.87) for testing $H_{0}: \\beta_{2} = 0$.\n\nGOLD_ANSWER:\n4. The t-statistic squared equals the F-statistic: $(14.26)^2 = 203.35 \\approx 202.87$ (allowing for rounding errors). This confirms the equivalence of the t and F tests for $H_{0}: \\beta_{2} = 0$ in the two-variable case.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that in a simple linear regression (one independent variable), the F-statistic for testing $H_0: \\beta_2 = 0$ is equivalent to the square of the t-statistic for the same hypothesis.\\n\\nStep 2: Verify this relationship numerically. Given:\\n- t-statistic = 14.26\\n- F-statistic = 202.87\\n\\nStep 3: Compute $t^2 = (14.26)^2 = 203.35$ (minor rounding differences explain the slight discrepancy from 202.87). This matches the F-statistic magnitude.\\n\\nConclusion: In simple linear regression, $F = t^2$. The F-statistic (202.87) is the square of the t-statistic (14.26) for testing $H_0: \\beta_2 = 0$.\n\nQID: textbook-119-16-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-16-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies and verifies the relationship between the t-statistic and F-statistic, showing that the F-statistic is the square of the t-statistic, which matches the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-16-1-3", "category": "CORRECT", "explanation": "The candidate answer correctly identifies and verifies the relationship between the t-statistic and F-statistic, showing that the F-statistic is the square of the t-statistic, which matches the gold answer."}, "llm_echoed_qid": "textbook-119-16-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies and verifies the relationship between the t-statistic and F-statistic, showing that the F-statistic is the square of the t-statistic, which matches the gold answer."}
{"script_processing_qid": "textbook-116-36-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the annual growth rate of per-capita output for South Korea from 1960 to 1992 using the data in Table 22-2. Show each step of your calculation.\n\nGOLD_ANSWER:\n1. **Given**: Initial per-capita output in 1960 = $898; Final per-capita output in 1992 = $6,665; Time period = 32 years.\n2. **Formula**: Growth rate $g$ is given by $g = \\left(\\frac{Y_{1992}}{Y_{1960}}\\right)^{\\frac{1}{32}} - 1$.\n3. **Calculation**: $g = \\left(\\frac{6,665}{898}\\right)^{\\frac{1}{32}} - 1 \\approx (7.42)^{0.03125} - 1 \\approx 1.069 - 1 = 0.069$ or 6.9% per year.\n\nCANDIDATE_ANSWER:\nStep 1: To calculate the annual growth rate of per-capita output, we would typically use the formula: [(Ending Value / Starting Value)^(1/Number of Years) - 1] * 100. Step 2: However, the provided context explicitly states that Table 22-2 shows 'vast discrepancies in long-term growth rates among different regional country groupings' but does NOT include actual numerical data points for South Korea or any specific countries. Step 3: The extracted text passages mention Table 22-2's existence and general content (growth patterns among regions), but crucially omit the specific per-capita output figures for South Korea in 1960 and 1992 required for this calculation. Step 4: Therefore, without access to the numerical values from Table 22-2, it is impossible to perform the requested derivation.\n\nQID: textbook-116-36-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-116-36-0-1\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it requires data from Table 22-2 which is not provided.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-116-36-0-1", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it requires data from Table 22-2 which is not provided."}, "llm_echoed_qid": "textbook-116-36-0-1", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it requires data from Table 22-2 which is not provided."}
{"script_processing_qid": "textbook-125-55-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Compare Executive Paper's sales-to-assets ratio (1.55) to the industry average (0.90). What are three possible explanations for the difference?\n\nGOLD_ANSWER:\n1. **Possible Explanations**:\n   - **Higher Efficiency**: Executive Paper may utilize its assets more efficiently to generate sales.\n   - **Capacity Constraints**: Executive Paper may be operating near full capacity, limiting further sales growth without additional investment.\n   - **Product Mix**: Executive Paper may produce high-volume, low-margin products compared to peers, leading to higher sales relative to assets.\n\nCANDIDATE_ANSWER:\nStep 1: Analyze components of total assets. The sales-to-assets ratio (1.55 vs. 0.90) suggests Executive Paper generates more sales per dollar of assets. Potential drivers include asset efficiency metrics provided in the context.\n\nStep 2: Compare inventory management. Days in inventory (63.6 days) may be lower than the industry average, indicating faster inventory turnover. This reduces average inventory (a component of total assets), increasing the ratio.\n\nStep 3: Evaluate receivables efficiency. The average collection period (72.4 days) might be shorter than the industry, leading to lower average receivables. This reduces total assets, boosting the ratio.\n\nStep 4: Assess fixed asset utilization. Executive Paper may use property/equipment more intensively (e.g., higher output per machine), increasing sales without proportionally increasing fixed assets. This enhances the sales-to-assets ratio.\n\nFinal Answer: Three possible explanations are:\n1. More efficient inventory management (lower days in inventory)\n2. Faster collection of receivables (shorter average collection period)\n3. Better utilization of fixed assets (higher productivity per asset)\n\nQID: textbook-125-55-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-55-1-1\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer provides valid explanations (inventory, receivables, fixed asset utilization) but does not directly align with the gold answer's broader points (efficiency, capacity, product mix). Both sets of explanations are plausible but differ in focus.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-55-1-1", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer provides valid explanations (inventory, receivables, fixed asset utilization) but does not directly align with the gold answer's broader points (efficiency, capacity, product mix). Both sets of explanations are plausible but differ in focus."}, "llm_echoed_qid": "textbook-125-55-1-1", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer provides valid explanations (inventory, receivables, fixed asset utilization) but does not directly align with the gold answer's broader points (efficiency, capacity, product mix). Both sets of explanations are plausible but differ in focus."}
{"script_processing_qid": "textbook-125-12-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the formula for asset beta ($\\upbeta_{\\mathrm{assets}}$) using the weighted average of debt and equity betas. Show the steps and explain the intuition behind the formula.\n\nGOLD_ANSWER:\n1. **Formula Derivation**: \n   - Asset beta is the weighted average of debt beta ($\\upbeta_{\\mathrm{debt}}$) and equity beta ($\\upbeta_{\\mathrm{equity}}$), where weights are the market values of debt ($D$) and equity ($E$) relative to total firm value ($V = D + E$).\n   - Mathematically: $$\\upbeta_{\\mathrm{assets}} = \\frac{D}{V}\\upbeta_{\\mathrm{debt}} + \\frac{E}{V}\\upbeta_{\\mathrm{equity}}$$\n   - **Intuition**: The risk of the firm's assets is shared between debt and equity holders. The formula reflects the proportional risk borne by each.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that firm value (V) equals debt (D) plus equity (E): \\[V = D + E\\]\n\nStep 2: Understand that the firm's assets represent a portfolio of debt and equity. By portfolio theory, the beta of this portfolio (asset beta) must equal the weighted average of its components:\n\\[\\upbeta_{\\mathrm{assets}} = \\frac{D}{V}\\upbeta_{\\mathrm{debt}} + \\frac{E}{V}\\upbeta_{\\mathrm{equity}}\\]\n\nStep 3: Intuition:\n- Debt holders have first claim on cash flows, making debt less risky (lower beta)\n- Equity holders bear residual risk (higher beta)\n- The formula averages these risks proportionally to their market values in the capital structure\n\nStep 4: Key implication:\nAsset beta reflects pure business risk, independent of financing mix. Financial leverage amplifies equity risk (higher equity beta) but leaves asset beta unchanged.\n\nQID: textbook-125-12-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-12-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the formula for asset beta using the weighted average of debt and equity betas, and provides a clear and accurate explanation of the intuition behind the formula. The steps and reasoning align perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-12-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the formula for asset beta using the weighted average of debt and equity betas, and provides a clear and accurate explanation of the intuition behind the formula. The steps and reasoning align perfectly with the gold answer."}, "llm_echoed_qid": "textbook-125-12-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the formula for asset beta using the weighted average of debt and equity betas, and provides a clear and accurate explanation of the intuition behind the formula. The steps and reasoning align perfectly with the gold answer."}
{"script_processing_qid": "textbook-119-27-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Given an ARMA(2,1) model, write the mathematical expression and explain the roles of the autoregressive and moving average components.\n\nGOLD_ANSWER:\n1.  **Mathematical Expression**: $$Y_{t} = \\theta + \\alpha_{1}Y_{t-1} + \\alpha_{2}Y_{t-2} + u_{t} + \\beta_{1}u_{t-1}$$\n2.  **Autoregressive Component ($\\alpha_{1}, \\alpha_{2}$)**: Captures the dependency of $Y_{t}$ on its past two values.\n3.  **Moving Average Component ($\\beta_{1}$)**: Captures the dependency of $Y_{t}$ on the past shock $u_{t-1}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the general ARMA(p,q) structure from the context. The ARMA(2,1) model has p=2 autoregressive terms and q=1 moving average term.\n\nStep 2: Mathematical expression:\nLet $Y_t$ be a stationary time series. The ARMA(2,1) model is:\n$$(Y_t - \\delta) = \\alpha_1(Y_{t-1} - \\delta) + \\alpha_2(Y_{t-2} - \\delta) + \\theta_1 u_{t-1} + u_t$$\nwhere:\n- $\\delta$ = mean of $Y$\n- $\\alpha_1,\\alpha_2$ = autoregressive coefficients\n- $\\theta_1$ = moving average coefficient\n- $u_t$ = white noise error term\n\nStep 3: Role of components:\n1. Autoregressive (AR(2)): Captures persistence through:\n   - $\\alpha_1(Y_{t-1} - \\delta)$: Direct effect of immediate past value\n   - $\\alpha_2(Y_{t-2} - \\delta)$: Effect of value two periods prior\n   Models momentum/memory in the series\n\n2. Moving Average (MA(1)): $\\theta_1 u_{t-1}$ accounts for:\n   - Impact of random shocks/innovations from previous period\n   - Helps model transient disturbances that persist for one period\n   Provides flexibility to capture unexpected volatility\n\nThe AR components model systematic patterns while the MA components model shock absorption, together creating a parsimonious representation of stationary time series dynamics.\n\nQID: textbook-119-27-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-27-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly provides the mathematical expression for an ARMA(2,1) model and accurately explains the roles of the autoregressive and moving average components. The expression includes the mean adjustment and the coefficients, and the explanation clearly distinguishes between the AR and MA components' roles in modeling persistence and shocks, respectively.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-27-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly provides the mathematical expression for an ARMA(2,1) model and accurately explains the roles of the autoregressive and moving average components. The expression includes the mean adjustment and the coefficients, and the explanation clearly distinguishes between the AR and MA components' roles in modeling persistence and shocks, respectively."}, "llm_echoed_qid": "textbook-119-27-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly provides the mathematical expression for an ARMA(2,1) model and accurately explains the roles of the autoregressive and moving average components. The expression includes the mean adjustment and the coefficients, and the explanation clearly distinguishes between the AR and MA components' roles in modeling persistence and shocks, respectively."}
{"script_processing_qid": "textbook-73-2-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of habit formation models for resolving the equity premium and risk-free rate puzzles.\n\nGOLD_ANSWER:\nHabit formation models (e.g., Campbell and Cochrane, 1999) introduce time-varying risk aversion, where utility depends on consumption relative to a habit level. This allows for high average risk aversion (explaining the equity premium) without requiring a high risk-free rate, as the habit mechanism dampens the desire for intertemporal consumption smoothing.\n\nCANDIDATE_ANSWER:\nStep 1: Habit formation models address the equity premium puzzle by amplifying the effective risk aversion of investors. In these models, utility depends on consumption relative to a habit level, making marginal utility more sensitive to consumption declines. This heightened sensitivity increases the covariance between stock returns and marginal utility, even with smooth aggregate consumption. This allows the equity premium to be explained with a lower risk aversion coefficient than standard models require.\n\nStep 2: For the risk-free rate puzzle, habit formation introduces a time-varying component to the marginal utility of consumption. The desire to maintain consumption above habit levels increases precautionary savings, which drives down the equilibrium risk-free rate. This mechanism reconciles low real interest rates with plausible levels of investor patience (low time preference rates), as the habit effect reduces the need to assume implausibly high subjective discount rates.\n\nStep 3: Together, these effects resolve both puzzles: (1) The amplified risk aversion from habit formation generates a high equity premium despite low consumption volatility, and (2) The increased savings motive from habit maintenance produces low risk-free rates without requiring extreme parameter values for time preference or risk aversion.\n\nQID: textbook-73-2-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-73-2-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains how habit formation models address both the equity premium and risk-free rate puzzles, matching the gold answer's key points about time-varying risk aversion and dampened intertemporal smoothing.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-73-2-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains how habit formation models address both the equity premium and risk-free rate puzzles, matching the gold answer's key points about time-varying risk aversion and dampened intertemporal smoothing."}, "llm_echoed_qid": "textbook-73-2-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains how habit formation models address both the equity premium and risk-free rate puzzles, matching the gold answer's key points about time-varying risk aversion and dampened intertemporal smoothing."}
{"script_processing_qid": "textbook-120-23-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Prove Blackwell's sufficient conditions for a mapping $T$ to be a contraction.\n\nGOLD_ANSWER:\n1. **Monotonicity**: For $f \\geq g$, $T(f) \\geq T(g)$.\\n2. **Discounting**: $T(f + c) \\leq T(f) + \\beta c$ for $c \\geq 0$.\\n3. **Contraction**: For $f, g \\in B(X)$, set $c = \\|f - g\\|_{\\infty}$. Then $T(f) \\leq T(g + c) \\leq T(g) + \\beta c$, implying $\\|T(f) - T(g)\\|_{\\infty} \\leq \\beta \\|f - g\\|_{\\infty}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that a contraction mapping T must satisfy ||Tf - Tg||_∞ ≤ β||f - g||_∞ for some β ∈ (0,1). Let c = ||f - g||_∞. By definition of sup norm, we have f ≤ g + c and g ≤ f + c everywhere.\n\nStep 2: Apply monotonicity to both inequalities. From f ≤ g + c, we get Tf ≤ T(g + c). From g ≤ f + c, we get Tg ≤ T(f + c).\n\nStep 3: Apply the discounting property T(h + c) ≤ Th + βc to both results. This gives T(g + c) ≤ Tg + βc and T(f + c) ≤ Tf + βc.\n\nStep 4: Combine results from Steps 2-3:\n- Tf ≤ Tg + βc\n- Tg ≤ Tf + βc\n\nStep 5: Rewrite inequalities as Tf - Tg ≤ βc and Tg - Tf ≤ βc. This implies |Tf(x) - Tg(x)| ≤ βc for all x. Taking supremum over x gives ||Tf - Tg||_∞ ≤ β||f - g||_∞.\n\nConclusion: Since β ∈ (0,1), this proves T is a contraction mapping under Blackwell's conditions.\n\nQID: textbook-120-23-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-120-23-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the logical steps to prove Blackwell's sufficient conditions for a contraction mapping, including the use of monotonicity and discounting properties, and arrives at the correct conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-120-23-1-1", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the logical steps to prove Blackwell's sufficient conditions for a contraction mapping, including the use of monotonicity and discounting properties, and arrives at the correct conclusion."}, "llm_echoed_qid": "textbook-120-23-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the logical steps to prove Blackwell's sufficient conditions for a contraction mapping, including the use of monotonicity and discounting properties, and arrives at the correct conclusion."}
{"script_processing_qid": "textbook-47-2-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. State and interpret Proposition 8 regarding the E-stability of an REE noisy k-cycle.\n\nGOLD_ANSWER:\n**Proposition 8**: Consider an REE noisy k-cycle with expectation parameters $\\bar{\\theta} = (\\bar{\\theta}_{1},\\ldots,\\bar{\\theta}_{k})$. Let $\\xi = \\prod_{i=1}^{k}R^{\\prime}(\\theta_{i})$. Then $\\bar{\\theta}$ is E-stable if and only if:\n- $\\xi < 1$ if $k = 1$ or $k = 2$,\n- $- (\\cos(\\pi/k))^{-k} < \\xi < 1$ if $k > 2$.\n\n**Interpretation**: The stability condition depends on the product of the derivatives of the mapping $R$ at the cycle points. For $k > 2$, the condition becomes more restrictive.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key requirements - The question requests both a statement and interpretation of Proposition 8 regarding E-stability of REE noisy k-cycles.\n\nStep 2: Analyze provided context - The given text describes adaptive learning algorithms using stage-specific sample means for noisy k-cycles, but does NOT explicitly state or discuss \"Proposition 8\" or its content.\n\nStep 3: Verify scope - The extracted passages only explain the natural estimator formulation (θ_i = (#N_i)^{-1}∑G(y_j,v_j)) and general learning framework, but contain no theorems/propositions about E-stability conditions.\n\nConclusion: The provided context lacks the specific content of Proposition 8 required to answer the question. No information about eigenvalues, stability conditions, or mathematical statements related to E-stability of REE is present in the given text.\n\nQID: textbook-47-2-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-47-2-0-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly identifies the lack of Proposition 8 in the provided material.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-47-2-0-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly identifies the lack of Proposition 8 in the provided material."}, "llm_echoed_qid": "textbook-47-2-0-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly identifies the lack of Proposition 8 in the provided material."}
{"script_processing_qid": "textbook-117-41-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Define the key terms associated with options: exercising the option, striking price, expiration date, and American vs. European options.\n\nGOLD_ANSWER:\n1. **Exercising the Option**: The act of buying or selling the underlying asset via the option contract. \n2. **Striking Price**: The fixed price in the option contract at which the holder can buy or sell the underlying asset. \n3. **Expiration Date**: The maturity date of the option, after which the option is dead. \n4. **American vs. European Options**: An American option can be exercised anytime up to the expiration date, while a European option can only be exercised on the expiration date.\n\nCANDIDATE_ANSWER:\nStep 1: Exercising the Option - This refers to the act of using the right granted by the option contract to buy (for a call option) or sell the underlying asset at the agreed-upon price. The context explicitly describes this as the owner’s 'right to buy an asset at a fixed price during a particular time period.'\n\nStep 2: Striking Price (Exercise Price) - The fixed price at which the option holder can buy (for a call) or sell the underlying asset. The context identifies this as the 'exercise price' and uses it to calculate the option’s payoff (e.g., 'Stock price - exercise price').\n\nStep 3: Expiration Date - The final date by which the option can be exercised. The context states the option is valid 'on or before a given date,' with the payoff structure explicitly defined 'at expiration.'\n\nStep 4: American vs. European Options - The distinction lies in exercise timing:\n- American options: Can be exercised at any time up to and including the expiration date ('on or before a given date' in the context).\n- European options: Can only be exercised on the expiration date itself (implied by contrast with the 'on or before' phrasing, though the term 'European' is not explicitly mentioned).\n\nThe context differentiates between the two via the phrase 'on or before,' enabling inference about their exercise flexibility.\n\nQID: textbook-117-41-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-41-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately defines all key terms associated with options, including exercising the option, striking price, expiration date, and the distinction between American and European options. The explanations align perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-41-1-0", "category": "CORRECT", "explanation": "The candidate's answer accurately defines all key terms associated with options, including exercising the option, striking price, expiration date, and the distinction between American and European options. The explanations align perfectly with the gold answer."}, "llm_echoed_qid": "textbook-117-41-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately defines all key terms associated with options, including exercising the option, striking price, expiration date, and the distinction between American and European options. The explanations align perfectly with the gold answer."}
{"script_processing_qid": "textbook-47-0-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the conditions for local stability of an REE under a statistical learning rule using the expectational stability (E-stability) principle.\n\nGOLD_ANSWER:\n1.  **E-Stability Principle**: An REE is E-stable if the associated differential equation of the learning dynamics has a stable fixed point at the REE. \n2.  **Conditions**: For local stability, the eigenvalues of the derivative of the mapping from perceived to actual laws of motion must have real parts less than 1.\n\nCANDIDATE_ANSWER:\nStep 1: Represent the economic system under learning as a stochastic recursive algorithm (SRA). This typically involves writing the agents' parameter updating process (e.g., recursive least squares) and the resulting temporary equilibrium dynamics.\n\nStep 2: Use stochastic approximation theory to associate the SRA with an ordinary differential equation (ODE). The ODE approximates the mean dynamics of the learning process: dθ/dτ = T(θ) − θ, where T(θ) represents the agents' beliefs when parameters are fixed at θ.\n\nStep 3: Identify the Rational Expectations Equilibrium (REE) as a fixed point θ* of this ODE, i.e., T(θ*) = θ*.\n\nStep 4: Compute the Jacobian matrix DT(θ) of the map T(⋅) evaluated at θ*. The E-stability principle states that the REE is locally stable under learning if all eigenvalues of DT(θ*) − I have negative real parts.\n\nStep 5: Simplify the condition to: All eigenvalues of the Jacobian DT(θ*) must have real parts less than 1. This ensures convergence of the learning dynamics to the REE through the associated ODE.\n\nQID: textbook-47-0-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-47-0-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately follows the steps to derive the conditions for local stability under the E-stability principle, matching the gold answer's key points about the eigenvalues of the Jacobian matrix needing real parts less than 1.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-47-0-1-1", "category": "CORRECT", "explanation": "The candidate answer accurately follows the steps to derive the conditions for local stability under the E-stability principle, matching the gold answer's key points about the eigenvalues of the Jacobian matrix needing real parts less than 1."}, "llm_echoed_qid": "textbook-47-0-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately follows the steps to derive the conditions for local stability under the E-stability principle, matching the gold answer's key points about the eigenvalues of the Jacobian matrix needing real parts less than 1."}
{"script_processing_qid": "textbook-35-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Evaluate the conclusions of Klenow and Rodriguez-Clare's (1997c) study on Costa Rican tariff reform. How do their findings compare to traditional cost-of-tariff calculations, and what broader implications do they have for trade policy analysis?\n\nGOLD_ANSWER:\n1. **Findings**: Klenow and Rodriguez-Clare find that considering changes in the set of imported goods can lead to cost-of-tariff estimates up to 4 times greater than traditional calculations.\n2. **Comparison**: Traditional methods underestimate the gains from tariff reductions by assuming a fixed set of goods.\n3. **Implications**: Policymakers should account for dynamic effects (e.g., changes in product variety) when assessing trade policies, as static models may miss significant welfare gains.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain any specific information about Klenow and Rodriguez-Clare's (1997c) study on Costa Rican tariff reform. While the section discusses related concepts like traditional cost-of-tariff calculations (e.g., Johnson's 1960 formula) and trade policy impacts through production functions (e.g., Romer 1994), there is no mention of methodological details, findings, or conclusions from Klenow and Rodriguez-Clare's specific work. Therefore, the context lacks sufficient information to evaluate their conclusions or compare them to traditional calculations.\n\nQID: textbook-35-1-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-35-1-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly notes the lack of specific information about the study.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-35-1-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly notes the lack of specific information about the study."}, "llm_echoed_qid": "textbook-35-1-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly notes the lack of specific information about the study."}
{"script_processing_qid": "textbook-100-1-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the concept of a 'true prior' $G_{0}$ and its role in comparing Bayesian and frequentist estimators. Why is it considered an 'elephant in the room'?\n\nGOLD_ANSWER:\n1.  **Definition**: The 'true prior' $G_{0}$ represents the actual distribution of the parameter $\\theta$ in nature, whether fixed or random.\n2.  **Role**: It serves as an impartial benchmark for evaluating the performance of estimators via the Bayes risk $r(G_{0}, \\widehat{\\theta})$.\n3.  **Elephant in the Room**: It is often ignored in traditional comparisons, yet its inclusion sharpens the evaluation by focusing on proximity to the truth.\n4.  **Implications**: Neither the Bayesian nor the frequentist knows $G_{0}$, but its existence grounds the comparison in objective reality.\n\nCANDIDATE_ANSWER:\nStep 1: Definition of True Prior (G₀)\nThe 'true prior' G₀ represents the actual distribution of parameters in nature, reflecting the objective ground truth of the estimation problem. It is distinct from subjective Bayesian priors, which reflect the analyst's beliefs.\n\nStep 2: Role in Comparing Estimators\nThe Bayes risk r(G₀, θ̂) measures an estimator’s average performance (mean squared error) weighted by G₀. This makes it a 'truth-centric' metric:\n- For Bayesian estimators: Compares the posterior estimator’s performance under the true data-generating process.\n- For frequentist estimators: Evaluates frequentist risk (MSE) averaged over G₀ rather than conditioned on fixed parameters.\n\nStep 3: Why G₀ is the 'Elephant in the Room'\nG₀ is essential for fair comparisons but is fundamentally unobservable in practice. This creates a paradox:\n- Ideal comparison requires knowledge of G₀ (to compute r(G₀, θ̂)),\n- But G₀ is never available (only data is observed).\nThus, comparisons often rely on approximations or assumptions about G₀, introducing ambiguity. The centrality of G₀ to the theoretical framework—yet its practical irrelevance due to unknowability—makes it an unresolved, glaring issue in methodology debates ('elephant in the room').\n\nQID: textbook-100-1-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-100-1-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately defines the 'true prior' G₀, explains its role in comparing Bayesian and frequentist estimators via Bayes risk, and articulates why it is considered an 'elephant in the room' due to its essential yet unobservable nature. The response aligns well with the gold answer in both content and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-100-1-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately defines the 'true prior' G₀, explains its role in comparing Bayesian and frequentist estimators via Bayes risk, and articulates why it is considered an 'elephant in the room' due to its essential yet unobservable nature. The response aligns well with the gold answer in both content and reasoning."}, "llm_echoed_qid": "textbook-100-1-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately defines the 'true prior' G₀, explains its role in comparing Bayesian and frequentist estimators via Bayes risk, and articulates why it is considered an 'elephant in the room' due to its essential yet unobservable nature. The response aligns well with the gold answer in both content and reasoning."}
{"script_processing_qid": "textbook-125-46-3-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate the abnormal return for a stock if its actual return is 8%, the market return is 5%, $\\alpha = 1%$, and $\\beta = 1.2$.\n\nGOLD_ANSWER:\n1. **Expected Return**: $1% + 1.2 \\times 5% = 7%$.\n2. **Abnormal Return**: $8% - 7% = 1%$.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate the expected return using the market model formula: Expected Return = α + β*(Market Return)\nStep 2: Plug in the given values: α = 1% (0.01), β = 1.2, Market Return = 5% (0.05)\nStep 3: Compute expected return: 0.01 + 1.2*(0.05) = 0.01 + 0.06 = 0.07 (7%)\nStep 4: Calculate abnormal return: Actual Return - Expected Return = 8% - 7% = 1%\nFinal Answer: The abnormal return is 1%.\n\nQID: textbook-125-46-3-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-46-3-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to calculate the expected return and abnormal return, matching the gold answer exactly.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-46-3-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to calculate the expected return and abnormal return, matching the gold answer exactly."}, "llm_echoed_qid": "textbook-125-46-3-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to calculate the expected return and abnormal return, matching the gold answer exactly."}
{"script_processing_qid": "textbook-108-3-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. How do economies of scale justify free trade for small countries? Provide a mathematical example.\n\nGOLD_ANSWER:\n1.  **Scale Effect**: Suppose a firm's average cost is $AC = \\frac{F}{Q} + c$, where $F$ is fixed cost and $c$ is marginal cost.\n2.  **Small Market**: If domestic demand $Q_d$ is low, $AC$ remains high.\n3.  **Free Trade**: Access to world demand $Q_w$ reduces $AC$ (since $\\frac{F}{Q_w} < \\frac{F}{Q_d}$).\n4.  **Result**: Lower prices and increased competitiveness.\n\nCANDIDATE_ANSWER:\nStep 1: Define Economies of Scale\nEconomies of scale occur when average costs decrease as production quantity increases. For small countries, limited domestic demand constrains output, keeping average costs high.\n\nStep 2: Mathematical Setup\nAssume a firm has total cost (TC) = Fixed Cost (F) + Marginal Cost (c) × Quantity (Q).\nAverage Cost (AC) = (F/Q) + c.\n\nStep 3: Domestic Market Example (No Trade)\nIf the small country's domestic demand is Q₁ = 100 units:\nAC₁ = (F/100) + c.\n\nStep 4: Free Trade Scenario\nWith free trade, the firm exports and serves global demand Q₂ = 1,000 units:\nAC₂ = (F/1,000) + c.\n\nStep 5: Compare AC Under Both Scenarios\nSince Q₂ > Q₁, AC₂ < AC₁.\nExample: If F = $10,000 and c = $5,\n- AC₁ = ($10,000/100) + $5 = $105\n- AC₂ = ($10,000/1,000) + $5 = $15\n\nStep 6: Conclusion\nFree trade allows small-country firms to expand output, lowering average costs via economies of scale. This makes them globally competitive, justifying free trade.\n\nQID: textbook-108-3-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-108-3-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains economies of scale and provides a clear mathematical example that aligns with the gold answer, demonstrating how free trade reduces average costs for small countries.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-3-1-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains economies of scale and provides a clear mathematical example that aligns with the gold answer, demonstrating how free trade reduces average costs for small countries."}, "llm_echoed_qid": "textbook-108-3-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains economies of scale and provides a clear mathematical example that aligns with the gold answer, demonstrating how free trade reduces average costs for small countries."}
{"script_processing_qid": "textbook-119-16-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the interpretation of the confidence interval for $E(Y|X_{0})$ versus a prediction interval for an individual $Y$ value.\n\nGOLD_ANSWER:\n4. The confidence interval for $E(Y|X_{0})$ estimates the range for the average $Y$ at $X_{0}$, while a prediction interval accounts for additional variability in individual $Y$ values around this mean, making it wider.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the two interval types\n- Confidence interval for E(Y|X₀) estimates the range where the TRUE AVERAGE/EXPECTED Y value at X₀ lies\n- Prediction interval for individual Y estimates the range where a SINGLE NEW OBSERVATION at X₀ might lie\n\nStep 2: Compare variance components\n- Both intervals use var(Ŷ₀) = σ²[1/n + (X₀-X̄)²/Σx_i²] for estimation uncertainty\n- Prediction interval ADDS σ² (error variance) to account for individual variability:\n  var(prediction) = var(Ŷ₀) + σ²\n\nStep 3: Analyze interval width\n- Prediction interval is ALWAYS WIDER because it includes both:\n  (a) Uncertainty in mean estimation (shared with confidence interval)\n  (b) Individual observation variability (extra σ² term)\n\nStep 4: Interpret practical implications\n- 95% CI for E(Y|X₀): We're 95% confident the POPULATION MEAN Y at X₀ is in this range\n- 95% Prediction Interval: We expect 95% of INDIVIDUAL Y observations at X₀ to fall in this range\n\nQID: textbook-119-16-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-16-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the difference between confidence and prediction intervals, including variance components and practical implications, matching the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-16-2-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the difference between confidence and prediction intervals, including variance components and practical implications, matching the gold answer's key points."}, "llm_echoed_qid": "textbook-119-16-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the difference between confidence and prediction intervals, including variance components and practical implications, matching the gold answer's key points."}
{"script_processing_qid": "textbook-125-20-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Estimate the marginal investor's tax rate using the municipal bond yield (2.49%) and Treasury bond yield (3.71%). What does this imply for the tax advantage of debt?\n\nGOLD_ANSWER:\n1. **Tax Rate**: $1 - T_p = \\frac{2.49}{3.71} \\Rightarrow T_p = 1 - 0.671 = 32.9\\%$.\n2. **Implication**: If the marginal investor's $T_p$ is 33%, the tax advantage of debt depends on $T_{pE}$. For $T_{pE} = 16\\%$, debt retains an advantage (see Q1).\n\nCANDIDATE_ANSWER:\nStep 1: Recall that tax-exempt municipal bond yields reflect after-personal-tax returns, while taxable Treasury bond yields reflect pre-tax returns.\nStep 2: Set up the equation for tax-equivalent yield: Municipal Yield = Treasury Yield × (1 - T_p)\nStep 3: Plug in given values: 2.49% = 3.71% × (1 - T_p)\nStep 4: Solve for T_p: T_p = 1 - (2.49/3.71) ≈ 1 - 0.671 = 0.329\nStep 5: The marginal investor's tax rate (T_p) is approximately 32.9%.\nStep 6: To assess tax advantage of debt, use the formula: Relative tax advantage = (1 - T_p)/[(1 - T_pE)(1 - T_c)]\nStep 7: The result implies that debt retains tax advantages when (1 - T_p) > (1 - T_pE)(1 - T_c). However, without values for T_pE (equity tax rate) and T_c (corporate tax rate), we can only conclude that personal taxes (through T_p) reduce debt's tax advantage compared to a world with only corporate taxes.\n\nQID: textbook-125-20-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-20-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly calculates the marginal investor's tax rate (32.9%) and provides a thorough explanation of the tax advantage of debt, including the necessary formula and considerations, matching the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-20-1-3", "category": "CORRECT", "explanation": "The candidate correctly calculates the marginal investor's tax rate (32.9%) and provides a thorough explanation of the tax advantage of debt, including the necessary formula and considerations, matching the gold answer."}, "llm_echoed_qid": "textbook-125-20-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly calculates the marginal investor's tax rate (32.9%) and provides a thorough explanation of the tax advantage of debt, including the necessary formula and considerations, matching the gold answer."}
{"script_processing_qid": "textbook-95-2-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why ordinary least squares (OLS) is unsuitable for estimating the SZ model's monetary policy equation. What alternative identification strategies do Sims and Zha employ?\n\nGOLD_ANSWER:\n1. **OLS Unsuitability**: OLS is invalid because $P c m\\_{t}$ and $M\\_{t}$ are contemporaneously affected by the monetary policy shock, leading to endogeneity bias.\n2. **Alternative Strategies**: \n   - Sims and Zha avoid instrumental variables (IV) due to unwillingness to assume predetermined variables.\n   - They instead impose restrictions on the contemporaneous relationships, such as $X\\_{t}$ responding only via $P c m\\_{t}$ and not directly to $M\\_{t}$ or $R\\_{t}$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify why OLS is unsuitable\nOLS requires regressors to be uncorrelated with the error term. In the SZ monetary policy equation (5.1), Mt and Pcmt are contemporaneously affected by monetary policy shocks (εt^S). This creates simultaneity bias because the dependent variable (Rt) and regressors (Mt, Pcmt) are jointly determined through contemporaneous feedback effects. OLS estimates would be inconsistent as the regressors become endogenous.\n\nStep 2: Analyze alternative identification strategies\nSims and Zha employ a structural VAR approach with three key assumptions:\n1) Impose a money demand equation (5.2) with explicit contemporaneous reactions to Rt\n2) Assume Pcmt responds contemporaneously to all shocks (justified by auction market characteristics)\n3) Use exclusion restrictions in the A0 matrix structure:\n   - Xt variables don't respond directly to Mt/Rt but respond via Pcmt\n   - Specify recursive ordering where Pcm equation comes first, money demand second, monetary policy rule third\n   - Maintain non-recursive relationships between Mt, Rt, and Pcmt\n\nThese assumptions allow identification without requiring predetermined instrumental variables or goods-market variable exogeneity.\n\nQID: textbook-95-2-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-95-2-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the endogeneity issue with OLS due to contemporaneous feedback effects and accurately describes Sims and Zha's structural VAR approach with its key assumptions and restrictions, matching the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-95-2-0-1", "category": "CORRECT", "explanation": "The candidate correctly identifies the endogeneity issue with OLS due to contemporaneous feedback effects and accurately describes Sims and Zha's structural VAR approach with its key assumptions and restrictions, matching the gold answer."}, "llm_echoed_qid": "textbook-95-2-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly identifies the endogeneity issue with OLS due to contemporaneous feedback effects and accurately describes Sims and Zha's structural VAR approach with its key assumptions and restrictions, matching the gold answer."}
{"script_processing_qid": "textbook-48-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Using the government budget constraint and permanent income hypothesis, formally prove Ricardian equivalence for a two-period model.\n\nGOLD_ANSWER:\n1.  **Government Budget Constraint**: $G_1 + \\frac{G_2}{1+r} = T_1 + \\frac{T_2}{1+r}$.\n2.  **Household Budget Constraint**: $C_1 + \\frac{C_2}{1+r} = (Y_1 - T_1) + \\frac{Y_2 - T_2}{1+r}$.\n3.  **Permanent Income**: A tax cut ($\\Delta T_1 = -1$) implies $\\Delta T_2 = (1+r)$. Households save $\\Delta T_1$ to pay $\\Delta T_2$, leaving $C_1$, $C_2$ unchanged.\n\nCANDIDATE_ANSWER:\nStep 1: Define the government budget constraint\\nThe government must satisfy its intertemporal budget constraint:\\nG₁ + G₂/(1+r) = T₁ + T₂/(1+r) + B₁\\nWhere G = government spending, T = taxes, B₁ = initial debt (assumed 0 here), and r = interest rate. If taxes are cut in period 1 (ΔT₁ < 0), debt issuance D = -ΔT₁ requires future taxes to rise by ΔT₂ = D(1+r).\\n\\nStep 2: Household budget constraint\\nUnder PIH, households maximize utility subject to:\\n(Y₁ - T₁) + (Y₂ - T₂)/(1+r) = C₁ + C₂/(1+r)\\nWhere Y = income, C = consumption.\\n\\nStep 3: Show tax timing neutrality\\nA tax cut ΔT₁ with offsetting future tax increase ΔT₂ = -ΔT₁(1+r) leaves the present value of net income unchanged:\\nΔT₁ + ΔT₂/(1+r) = ΔT₁ - ΔT₁(1+r)/(1+r) = 0\\n\\nStep 4: Consumption invariance\\nSince lifetime resources remain unchanged, optimal consumption (C₁,C₂) is unaffected. Households save the full tax cut (ΔT₁) in period 1 to pay future tax liability ΔT₂ = ΔT₁(1+r).\\n\\nConclusion: The timing of taxes (deficit financing vs. tax financing) has no real effects under these conditions, proving Ricardian equivalence.\n\nQID: textbook-48-0-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-48-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to prove Ricardian equivalence using the government budget constraint and the permanent income hypothesis. The explanation is detailed and aligns with the gold answer, showing that a tax cut in the first period with a corresponding tax increase in the second period leaves consumption unchanged, thus proving Ricardian equivalence.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-48-0-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to prove Ricardian equivalence using the government budget constraint and the permanent income hypothesis. The explanation is detailed and aligns with the gold answer, showing that a tax cut in the first period with a corresponding tax increase in the second period leaves consumption unchanged, thus proving Ricardian equivalence."}, "llm_echoed_qid": "textbook-48-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to prove Ricardian equivalence using the government budget constraint and the permanent income hypothesis. The explanation is detailed and aligns with the gold answer, showing that a tax cut in the first period with a corresponding tax increase in the second period leaves consumption unchanged, thus proving Ricardian equivalence."}
{"script_processing_qid": "textbook-100-5-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of the Likelihood Principle in Bayesian analysis. How does it contrast with frequentist approaches?\n\nGOLD_ANSWER:\n**Likelihood Principle**: States that all evidence about $\\theta$ from an experiment is contained in the likelihood function $L(\\theta | \\mathbf{X})$. Bayesian methods adhere to this principle by using the likelihood to update prior beliefs.\\n**Contrast with Frequentism**: Frequentist methods rely on the sampling distribution of estimators, which depends on the experimental design and not just the observed data. This violates the Likelihood Principle, as frequentist inferences (e.g., p-values, confidence intervals) incorporate unobserved data possibilities.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain enough information to answer this question. The excerpt focuses on Game Theory and Decision Theory concepts such as zero-sum games, loss functions, risk functions, and admissibility of decision rules, but does not mention the Likelihood Principle, Bayesian analysis, or frequentist approaches. Therefore, the implications of the Likelihood Principle and its contrast with frequentist methods cannot be addressed using the given material.\n\nQID: textbook-100-5-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-100-5-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states the provided context does not contain sufficient information.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-100-5-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states the provided context does not contain sufficient information."}, "llm_echoed_qid": "textbook-100-5-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states the provided context does not contain sufficient information."}
{"script_processing_qid": "textbook-108-4-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Explain the concept of omitted variable bias in the context of economic graphs. Provide an example from the text.\n\nGOLD_ANSWER:\n1.  **Definition**: Omitted variable bias occurs when a graph fails to account for a third variable that affects both the dependent and independent variables, leading to incorrect causal inferences.\n2.  **Example**: The text describes a study where the number of cigarette lighters in a household is correlated with cancer risk. The omitted variable here is cigarette smoking, which likely causes both the ownership of lighters and the increased cancer risk.\n\nCANDIDATE_ANSWER:\nStep 1: Define omitted variable bias in the context of economic graphs. Omitted variable bias occurs when a third variable not included in the graph influences both the independent and dependent variables being analyzed. This leads to an incorrect causal interpretation of the relationship between the two variables shown in the graph. Step 2: Example from the text. The text explains that real-world economic graphs (unlike hypothetical examples like the demand curve) often fail to hold all other variables constant. For instance, if an economist graphs the relationship between two variables (A and B) but omits a third variable (C) that affects both, they might wrongly conclude that A directly causes B. The text emphasizes this issue by contrasting the controlled demand curve example (where causality is clear) with real-world scenarios where omitted variables invalidate causal claims.\n\nQID: textbook-108-4-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-108-4-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate accurately defines omitted variable bias and provides a relevant example from the text, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-4-0-0", "category": "CORRECT", "explanation": "The candidate accurately defines omitted variable bias and provides a relevant example from the text, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-108-4-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate accurately defines omitted variable bias and provides a relevant example from the text, aligning well with the gold answer."}
{"script_processing_qid": "textbook-80-0-0-4", "llm_grader_input_prompt_user": "\nQUESTION:\n5. How does the local polynomial estimator in (8.16) simultaneously provide estimates for the component functions $g_{\\alpha}(X_{\\alpha})$ and their derivatives? Derive the estimator for the $\\nu$-th derivative $\\widehat{g}_{\\alpha}^{(\\nu)}(x_{\\alpha})$.\n\nGOLD_ANSWER:\nThe local polynomial estimator minimizes:\n$$\\sum_{i=1}^n \\left\\{Y_i - \\beta_0 - \\beta_1(X_{i\\alpha} - x_{\\alpha}) - \\ldots - \\beta_p(X_{i\\alpha} - x_{\\alpha})^p\\right\\}^2 K_h(X_{i\\alpha} - x_{\\alpha}) \\mathcal{K}_{\\mathbf{H}}(X_{i\\underline{{\\alpha}}} - x_{l\\underline{{\\alpha}}}).$$\nThe $\\nu$-th derivative estimator is:\n$$\\widehat{g}_{\\alpha}^{(\\nu)}(x_{\\alpha}) = \\frac{\\nu!}{n}\\sum_{l=1}^n e_{\\nu}^\\top (\\mathbf{X}_{\\alpha}^\\top \\mathbf{W}_{l\\alpha} \\mathbf{X}_{\\alpha})^{-1} \\mathbf{X}_{\\alpha}^\\top \\mathbf{W}_{l\\alpha} \\mathbf{Y},$$\nwhere $e_{\\nu}$ extracts the coefficient $\\beta_{\\nu}$, which estimates $g_{\\alpha}^{(\\nu)}(x_{\\alpha})/\\nu!$.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that local polynomial regression of order p approximates the regression function locally using a Taylor expansion. The coefficient vector in this expansion contains information about both the function value and its derivatives.\n\nStep 2: For component function g_α(X_α), the local polynomial estimator at point x_α solves a weighted least squares problem that includes polynomial terms of (X_α - x_α) up to degree p.\n\nStep 3: The ν-th derivative estimator ĝ_α^{(ν)}(x_α) corresponds to the (ν+1)-th element in the coefficient vector multiplied by ν!. This follows from standard Taylor series expansion where the k-th coefficient represents f^{(k)}(x)/k!.\n\nStep 4: Specifically, if the local polynomial fit at x_α produces coefficients (β̂_0, β̂_1, ..., β̂_p), then ĝ_α^{(ν)}(x_α) = ν!β̂_ν. This derivation holds because the ν-th derivative of the local polynomial is exactly ν! times the ν-th coefficient.\n\nStep 5: The context confirms this mechanism through the statement that using a local polynomial estimator of order p simultaneously provides component functions (via β̂_0) and derivatives (via β̂_ν for ν ≤ p) without requiring additional methodology.\n\nQID: textbook-80-0-0-4\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-80-0-0-4\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains how the local polynomial estimator provides estimates for both the component functions and their derivatives, matching the derivation in the gold answer. The steps clearly outline the relationship between the polynomial coefficients and the derivatives, including the specific formula for the ν-th derivative estimator.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-0-0-4", "category": "CORRECT", "explanation": "The candidate answer correctly explains how the local polynomial estimator provides estimates for both the component functions and their derivatives, matching the derivation in the gold answer. The steps clearly outline the relationship between the polynomial coefficients and the derivatives, including the specific formula for the ν-th derivative estimator."}, "llm_echoed_qid": "textbook-80-0-0-4", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains how the local polynomial estimator provides estimates for both the component functions and their derivatives, matching the derivation in the gold answer. The steps clearly outline the relationship between the polynomial coefficients and the derivatives, including the specific formula for the ν-th derivative estimator."}
{"script_processing_qid": "textbook-74-1-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. (Quantitative) Suppose a staggered contracts model is given by $Y_t = \\alpha D_t - \\beta P_t + \\epsilon_t$, where $\\epsilon_t$ is an i.i.d. shock. Derive the variance of output $Var(Y_t)$ under the assumptions of (a) dominant demand shocks and (b) dominant price shocks.\n\nGOLD_ANSWER:\n1.  **General Variance**: $Var(Y_t) = \\alpha^2 Var(D_t) + \\beta^2 Var(P_t) - 2\\alpha\\beta Cov(D_t, P_t) + Var(\\epsilon_t)$.\n2.  **Case (a): Dominant Demand Shocks** ($Var(D_t) \\gg Var(P_t)$):\n    -   $Var(Y_t) \\approx \\alpha^2 Var(D_t) + Var(\\epsilon_t)$.\n    -   **Effect of Rigidity ($\\beta \\downarrow$)**: Reduces $\\beta^2 Var(P_t)$, stabilizing $Var(Y_t)$ if $Cov(D_t, P_t)$ is small.\n3.  **Case (b): Dominant Price Shocks** ($Var(P_t) \\gg Var(D_t)$):\n    -   $Var(Y_t) \\approx \\beta^2 Var(P_t) + Var(\\epsilon_t)$.\n    -   **Effect of Rigidity ($\\beta \\downarrow$)**: Increases $Var(Y_t)$ because larger $\\Delta Y$ is needed to offset $\\Delta P$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the model equation:\n$Y_t = \\alpha D_t - \\beta P_t + \\epsilon_t$\n\nStep 2: Calculate variance using standard properties:\n$Var(Y_t) = \\alpha^2 Var(D_t) + \\beta^2 Var(P_t) + Var(\\epsilon_t)$\n(assuming $D_t$, $P_t$, and $\\epsilon_t$ are mutually uncorrelated)\n\nCase (a) - Dominant demand shocks:\nStep 3a: Assume $Var(D_t)$ dominates ($Var(D_t) \\gg Var(P_t), Var(\\epsilon_t)$):\n$Var(Y_t) \\approx \\alpha^2 Var(D_t)$\n\nCase (b) - Dominant price shocks:\nStep 3b: Assume $Var(P_t)$ dominates ($Var(P_t) \\gg Var(D_t), Var(\\epsilon_t)$):\n$Var(Y_t) \\approx \\beta^2 Var(P_t)$\n\nThis shows output variance depends on the relative magnitude of shocks: higher price rigidity (larger $\\beta$) amplifies price shock effects, while demand shocks are amplified by demand response ($\\alpha$).\n\nQID: textbook-74-1-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-74-1-0-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly derived the general variance formula and addressed both cases of dominant demand and price shocks. However, they omitted the covariance term in the general variance formula and did not discuss the effect of rigidity (β) on variance in the context of dominant shocks, which is present in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-74-1-0-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly derived the general variance formula and addressed both cases of dominant demand and price shocks. However, they omitted the covariance term in the general variance formula and did not discuss the effect of rigidity (β) on variance in the context of dominant shocks, which is present in the gold answer."}, "llm_echoed_qid": "textbook-74-1-0-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly derived the general variance formula and addressed both cases of dominant demand and price shocks. However, they omitted the covariance term in the general variance formula and did not discuss the effect of rigidity (β) on variance in the context of dominant shocks, which is present in the gold answer."}
{"script_processing_qid": "textbook-116-15-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Quantitatively compare the profitability of a London Eurobank and a New York bank for a $1 million deposit, assuming a 10% reserve requirement in the U.S. and a 5% interest rate. How much more interest can the Eurobank pay?\n\nGOLD_ANSWER:\n1.  **New York Bank**: \n    - Reserve requirement: $100,000 (10% of $1M).\n    - Funds available for lending: $900,000.\n    - Interest income at 5%: $45,000.\n    - Interest paid to depositor: $45,000 (assuming no profit margin).\n2.  **Eurobank**: \n    - No reserve requirement.\n    - Funds available for lending: $1M.\n    - Interest income at 5%: $50,000.\n    - Additional interest payable: $5,000 (Eurobank can pay up to 5.56% vs. 5% for NY bank).\n\nCANDIDATE_ANSWER:\nStep 1: Calculate New York Bank's lendable capital after reserve requirements\nReserve requirement = 10% of $1,000,000 = $100,000\nLendable amount = $1,000,000 - $100,000 = $900,000\n\nStep 2: Calculate New York Bank's interest income\nInterest income = 5% of $900,000 = $45,000\nThis represents the maximum interest they can pay depositors\n\nStep 3: Calculate Eurobank's lendable capital\nNo reserve requirements → Full $1,000,000 available for lending\n\nStep 4: Calculate Eurobank's interest income\nInterest income = 5% of $1,000,000 = $50,000\n\nStep 5: Compare interest payment capacity\nDifference = $50,000 (Eurobank) - $45,000 (NY Bank) = $5,000\n\nConclusion: The Eurobank can pay $5,000 (or 0.5% of principal) more in interest annually.\n\nQID: textbook-116-15-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-116-15-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to calculate and compare the interest payment capacities of the New York bank and the Eurobank, arriving at the correct difference of $5,000. The reasoning and calculations align with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-116-15-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to calculate and compare the interest payment capacities of the New York bank and the Eurobank, arriving at the correct difference of $5,000. The reasoning and calculations align with the gold answer."}, "llm_echoed_qid": "textbook-116-15-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to calculate and compare the interest payment capacities of the New York bank and the Eurobank, arriving at the correct difference of $5,000. The reasoning and calculations align with the gold answer."}
{"script_processing_qid": "textbook-47-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of adaptive learning for models with multiple rational expectations equilibria (REE).\n\nGOLD_ANSWER:\n1.  **Multiple REE**: Some models have multiple REE, leading to indeterminacy. \n2.  **Adaptive Learning as a Selection Device**: Adaptive learning can act as a selection mechanism, where only REE that are stable under learning are observed in practice. This reduces the degree of multiplicity and provides a resolution to the indeterminacy issue.\n\nCANDIDATE_ANSWER:\nStep 1: Adaptive learning introduces equilibrium selection. In models with multiple REE, adaptive learning dynamics (e.g., least squares updating) determine which equilibrium agents converge to, as not all REE may be 'learnable' or stable under the learning process.\n\nStep 2: Expectational stability (E-stability) becomes critical. Only REE that satisfy E-stability conditions (where agents’ parameter estimates converge to equilibrium values under the learning rule) are attainable, even if multiple equilibria theoretically exist.\n\nStep 3: Path dependence emerges. Initial beliefs and historical data used in the learning process influence the selection of equilibria, contrasting with rational expectations where coordination is assumed.\n\nStep 4: Stability under learning may reduce equilibrium indeterminacy. Adaptive learning can eliminate 'non-robust' REE that are sensitive to small perturbations in expectations, leaving only dynamically stable equilibria.\n\nStep 5: Transitions between equilibria may occur. Slow adaptation or structural breaks could allow temporary deviations from one REE and convergence to another, creating endogenous regime shifts absent under rational expectations coordination.\n\nQID: textbook-47-0-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-47-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the implications of adaptive learning for models with multiple REE, covering equilibrium selection, E-stability, path dependence, stability under learning, and transitions between equilibria, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-47-0-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the implications of adaptive learning for models with multiple REE, covering equilibrium selection, E-stability, path dependence, stability under learning, and transitions between equilibria, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-47-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the implications of adaptive learning for models with multiple REE, covering equilibrium selection, E-stability, path dependence, stability under learning, and transitions between equilibria, aligning well with the gold answer."}
{"script_processing_qid": "textbook-93-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Using Theorem 4.8, derive a sufficient condition for weak ergodicity in terms of the coefficients of ergodicity.\n\nGOLD_ANSWER:\n1. **Theorem 4.8**: Weak ergodicity obtains if there exists a sequence $\\{k_{s}\\}$ such that $\\sum_{s=0}^{\\infty}\\{1 - \\tau(T_{k_{s},k_{s+1}-k_{s}})\\\n\nCANDIDATE_ANSWER:\nStep 1: Recall Theorem 4.8 states weak ergodicity holds if there exists a strictly increasing sequence {k_s} such that ∑_{s=0}^∞ [1 - τ(T_{k_s,k_{s+1}-k_s})] = ∞, where τ is a proper coefficient of ergodicity.\nStep 2: Consider partitioning the sequence {P_k} into single-matrix blocks (i.e., k_{s+1} = k_s +1). Then each term T_{k_s,k_{s+1}-k_s} becomes just P_{k_s}.\nStep 3: Substituting into Theorem 4.8's condition, weak ergodicity follows if ∑_{s=0}^∞ [1 - τ(P_{k_s})] = ∞ for this partition.\nStep 4: Since {k_s} can be chosen as the full sequence (k_s = s), this reduces to requiring ∑_{k=1}^∞ [1 - τ(P_k)] = ∞.\nStep 5: By Theorem 4.8's inequality m(T_{0,r}) ≤ ∏_{k=1}^r τ(P_k), divergence of ∑[1 - τ(P_k)] implies ∏τ(P_k) → 0. As m(·) is proper, m(T_{0,r}) → 0 ⇒ τ(T_{0,r}) → 0 (by equivalence in Lemma 4.1).\nConclusion: A sufficient condition is ∑_{k=1}^∞ [1 - τ(P_k)] = ∞, where τ(·) is any proper coefficient of ergodicity.\n\nQID: textbook-93-1-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-93-1-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives a sufficient condition for weak ergodicity using Theorem 4.8, providing a detailed and logical step-by-step explanation that matches the gold answer's requirements.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-93-1-0-3", "category": "CORRECT", "explanation": "The candidate answer correctly derives a sufficient condition for weak ergodicity using Theorem 4.8, providing a detailed and logical step-by-step explanation that matches the gold answer's requirements."}, "llm_echoed_qid": "textbook-93-1-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives a sufficient condition for weak ergodicity using Theorem 4.8, providing a detailed and logical step-by-step explanation that matches the gold answer's requirements."}
{"script_processing_qid": "textbook-109-3-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using event study methodology, explain the long-term shareholder returns for acquiring firms in the UK (Cosh et al., 1989; Franks and Harris, 1989). What does this suggest about merger motives?\n\nGOLD_ANSWER:\n1. **Event Study Results**: \n   - **Short-Term**: Slight gains post-merger (e.g., Franks and Harris, 1989).\n   - **Long-Term**: Negative residuals after 24–36 months (Cosh et al., 1989).\n2. **Merger Motives**: \n   - **Opportunistic Timing**: Acquirers bid when their stock prices are high (Halpern, 1983), exploiting market overvaluation.\n   - **Agency Problems**: Managers may prioritize empire-building over shareholder value, leading to value-destructive mergers.\n\nCANDIDATE_ANSWER:\nStep 1: The provided context explicitly discusses U.S. studies (e.g., Markham, 1955; Reid, 1968; Mueller, 1980, 1992) showing that acquiring firm shareholders experience negative cumulative returns (-7.2% median) and underperformance relative to the market portfolio post-merger. Step 2: While the section mentions UK studies (Cosh et al., 1989; Franks and Harris, 1989) in its scope, the extracted text passages do not include any specific data or conclusions from these UK studies. Step 3: The context lacks UK-specific findings on long-term shareholder returns or methodological details for event studies in the UK. Step 4: Therefore, the provided context is insufficient to explain the results of the cited UK studies or their implications for merger motives.\n\nQID: textbook-109-3-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-109-3-0-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide specific data or conclusions from the cited UK studies.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-109-3-0-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide specific data or conclusions from the cited UK studies."}, "llm_echoed_qid": "textbook-109-3-0-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide specific data or conclusions from the cited UK studies."}
{"script_processing_qid": "textbook-101-2-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain the conditional distribution of the truncated empirical process $N_{n,D}$ given $N_{n}(D)=k$.\n\nGOLD_ANSWER:\n1. **Lemma 1.4.1**: $P(N_{n,D}\\in\\cdot|N_{n}(D)=k)=P\\left\\{\\sum_{i=1}^{k}\\varepsilon_{Y_{i}}\\in\\cdot\\right\\}$.\n2. **Interpretation**: Conditional on $k$ points in $D$, the process is an empirical process with $k$ points distributed as $Q_{D}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that $N_n(D) = \\beta$ represents the count of observations in region $D$, where $\\beta \\sim \\text{Binomial}(n, Q(D))$.\nStep 2: Conditioning on $N_n(D) = k$ fixes $\\beta = k$ (i.e., there are exactly $k$ observations in $D$).\nStep 3: Under this conditioning, the truncated process $N_{n,D}$ reduces to $\\sum_{i=1}^k \\varepsilon_{Y_i}$ where:\n   - The $k$ points $Y_1, ..., Y_k$ are independent\n   - Each $Y_i$ follows distribution $Q_D = Q(\\cdot \\cap D)/Q(D)$ (i.e., the original distribution $Q$ restricted to $D$ and normalized).\nStep 4: Thus, the conditional distribution of $N_{n,D}$ given $N_n(D)=k$ is equivalent to a Poisson process with $k$ i.i.d. atoms distributed according to $Q_D$.\n\nQID: textbook-101-2-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-101-2-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the conditional distribution of the truncated empirical process as equivalent to a Poisson process with k i.i.d. atoms distributed according to Q_D, matching the gold answer's interpretation.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-101-2-2-2", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the conditional distribution of the truncated empirical process as equivalent to a Poisson process with k i.i.d. atoms distributed according to Q_D, matching the gold answer's interpretation."}, "llm_echoed_qid": "textbook-101-2-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the conditional distribution of the truncated empirical process as equivalent to a Poisson process with k i.i.d. atoms distributed according to Q_D, matching the gold answer's interpretation."}
{"script_processing_qid": "textbook-80-6-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain the role of the link function $G(\\bullet)$ in the Generalized Additive Model (GAM). Provide examples of common link functions for binary and count data.\n\nGOLD_ANSWER:\n1. **Role**: The link function $G(\\bullet)$ relates the linear predictor to the mean of the response variable, enabling modeling of non-Gaussian data.\n2. **Binary Data**: Logit link $G(\\eta) = \\frac{e^{\\eta}}{1 + e^{\\eta}}$ (logistic regression), Probit link $G(\\eta) = \\Phi(\\eta)$ (normal CDF).\n3. **Count Data**: Log link $G(\\eta) = e^{\\eta}$ (Poisson regression).\n\nCANDIDATE_ANSWER:\nStep 1: The link function $G(\\bullet)$ in a Generalized Additive Model (GAM) connects the additive predictor (the sum of nonparametric functions and intercept) to the conditional mean of the response variable $Y$. It allows the model to handle non-Gaussian distributions by establishing a functional relationship between the linear/nonparametric predictor and the expected value of $Y$. This flexibility is critical for modeling different data types (e.g., binary, count).\n\nStep 2: Common link functions include:\n- For binary data: The **logit function** $G(\\mu) = \\log\\left(\\frac{\\mu}{1-\\mu}\\right)$, which maps the predictor to probabilities in $[0,1]$.\n- For count data: The **log function** $G(\\mu) = \\log(\\mu)$, ensuring positive expected counts while maintaining additivity in the predictor space.\n\nQID: textbook-80-6-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-80-6-0-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly describes the role of the link function and provides examples for binary and count data. However, the candidate's logit function is incorrectly stated as the inverse of the correct logit link function provided in the gold answer. The log function example is correct.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-6-0-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly describes the role of the link function and provides examples for binary and count data. However, the candidate's logit function is incorrectly stated as the inverse of the correct logit link function provided in the gold answer. The log function example is correct."}, "llm_echoed_qid": "textbook-80-6-0-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly describes the role of the link function and provides examples for binary and count data. However, the candidate's logit function is incorrectly stated as the inverse of the correct logit link function provided in the gold answer. The log function example is correct."}
{"script_processing_qid": "textbook-65-1-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze the stopping criterion for the classification tree in Figure 9.4. Why are GROUP 1 and GROUP 21 not split further?\n\nGOLD_ANSWER:\n1. **Stopping Criterion**: Splitting stops when the gain in heterogeneity reduction is marginal (e.g., $3.5\\%$ for GROUP 1).\n2. **Reason**: Further splits would not significantly improve homogeneity (density $\\hat{f}_{\\hat{h}_{opt}}$ is nearly unimodal).\n3. **GROUP 22**: Unimodality confirms no need for splitting.\n\nCANDIDATE_ANSWER:\nStep 1: The stopping criterion in the classification algorithm is triggered when the computed heterogeneity index (SHI) for a group falls below or equals the threshold τ (SHI ≤ τ). Step 2: GROUP 1 and GROUP 21 are not split further because their respective SHI values at those nodes likely fell below the predefined threshold τ during Step 2 of the algorithm. Step 3: Since SHI ≤ τ for these groups, the algorithm proceeds to Step 3 (Stop) rather than recursively splitting them into subclasses. This indicates these groups were deemed sufficiently homogeneous according to the threshold criterion.\n\nQID: textbook-65-1-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-65-1-1-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate correctly identifies that the stopping criterion involves a threshold for heterogeneity, but the explanation focuses on a specific index (SHI) and threshold (τ) not mentioned in the gold answer. The gold answer emphasizes marginal gain in heterogeneity reduction and unimodality, which the candidate does not address.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-65-1-1-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate correctly identifies that the stopping criterion involves a threshold for heterogeneity, but the explanation focuses on a specific index (SHI) and threshold (τ) not mentioned in the gold answer. The gold answer emphasizes marginal gain in heterogeneity reduction and unimodality, which the candidate does not address."}, "llm_echoed_qid": "textbook-65-1-1-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate correctly identifies that the stopping criterion involves a threshold for heterogeneity, but the explanation focuses on a specific index (SHI) and threshold (τ) not mentioned in the gold answer. The gold answer emphasizes marginal gain in heterogeneity reduction and unimodality, which the candidate does not address."}
{"script_processing_qid": "textbook-99-5-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the convergence rate $n^{-2\\beta/(2\\beta+d)}$ exemplifies the 'curse of dimensionality'. Provide a numerical example for $\\beta = 0.5$ and $d = 1, 2, 3, 4$.\n\nGOLD_ANSWER:\nThe convergence rate slows exponentially with dimensionality $d$. For fixed $\\beta$, the exponent $-2\\beta/(2\\beta + d)$ diminishes as $d$ increases, requiring drastically larger $n$ to achieve the same precision.  \n**Example**: For $\\beta = 0.5$ and target MISE $\\delta = 0.1$, the minimal sample sizes are:  \n- $d=1$: $n^{*}(1) = \\delta^{-1-1} = 100$.  \n- $d=2$: $n^{*}(2) = \\delta^{-1-2} = 1000$.  \n- $d=3$: $n^{*}(3) = \\delta^{-1-3} = 10,000$.  \n- $d=4$: $n^{*}(4) = \\delta^{-1-4} = 100,000$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the convergence rate formula\nThe convergence rate n^{-2β/(2β+d)} shows how estimation error decreases with sample size n. The denominator (2β + d) combines smoothness (β) and dimensionality (d).\n\nStep 2: Analyze dimensional dependence\nAs dimensionality d increases:\n- The denominator grows while the numerator remains fixed\n- The exponent -2β/(2β+d) becomes less negative\n- This results in slower convergence rates (higher error for same n)\n\nStep 3: Recognize curse of dimensionality manifestation\nEach additional dimension:\n- Requires exponentially more samples to maintain same error rate\n- Makes high-dimensional estimation fundamentally harder\n\nNumerical example (β = 0.5):\nFor n = 1,000 samples:\n- d=1: Rate = 1000^{-1/(1+1)} = 1/31.62 ≈ 0.0316\n- d=2: Rate = 1000^{-1/(1+2)} = 1/10 ≈ 0.1\n- d=3: Rate = 1000^{-1/(1+3)} = 1/5.62 ≈ 0.1778\n- d=4: Rate = 1000^{-1/(1+4)} = 1/3.98 ≈ 0.251\n\nInterpretation: With β=0.5, doubling dimensions from 1 to 4 increases error magnitude by ~8x (0.0316 → 0.251) for the same sample size, demonstrating exponential degradation.\n\nQID: textbook-99-5-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-5-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly explains the curse of dimensionality through the convergence rate formula, provides a numerical example for β=0.5, and demonstrates how error increases with dimensionality. The reasoning aligns with the gold answer, though the numerical values differ slightly due to different approaches (fixed n vs fixed δ).\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-5-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly explains the curse of dimensionality through the convergence rate formula, provides a numerical example for β=0.5, and demonstrates how error increases with dimensionality. The reasoning aligns with the gold answer, though the numerical values differ slightly due to different approaches (fixed n vs fixed δ)."}, "llm_echoed_qid": "textbook-99-5-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly explains the curse of dimensionality through the convergence rate formula, provides a numerical example for β=0.5, and demonstrates how error increases with dimensionality. The reasoning aligns with the gold answer, though the numerical values differ slightly due to different approaches (fixed n vs fixed δ)."}
{"script_processing_qid": "textbook-54-0-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Calculate the autocorrelation function $\\rho_{j}$ for an AR(1) process $x_{t}=\\phi x_{t-1}+\\epsilon_{t}$.\n\nGOLD_ANSWER:\n1. **Variance ($\\gamma_{0}$)**: $\\gamma_{0}=\\frac{\\sigma_{\\epsilon}^{2}}{1-\\phi^{2}}$.\n2. **Autocovariance ($\\gamma_{j}$)**: $\\gamma_{j}=\\phi^{j}\\gamma_{0}$.\n3. **Autocorrelation ($\\rho_{j}$)**: $\\rho_{j}=\\frac{\\gamma_{j}}{\\gamma_{0}}=\\phi^{j}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the AR(1) process definition: x_t = φx_{t-1} + ε_t where E[x_t] = 0.\nStep 2: Use the autocovariance definition γ_j = E[x_t x_{t-j}]. For j=0, γ_0 = E[x_t²] = Var(x_t).\nStep 3: Calculate γ_0 using squared terms:\nE[x_t²] = φ²E[x_{t-1}²] + E[ε_t²] + 2φE[x_{t-1}ε_t].\nSince ε_t is independent of x_{t-1}, E[x_{t-1}ε_t] = 0.\nThis gives γ_0 = φ²γ_0 + σ²_ε → γ_0 = σ²_ε/(1 - φ²).\nStep 4: For j ≥ 1, multiply x_{t-j} through original equation:\nE[x_t x_{t-j}] = φE[x_{t-1}x_{t-j}] + E[ε_t x_{t-j}].\nAgain, ε_t is independent of x_{t-j} for j ≥ 1, so last term drops out.\nThis yields recursive relation γ_j = φγ_{j-1}.\nStep 5: Solve recurrence relation recursively:\nγ_1 = φγ_0\nγ_2 = φγ_1 = φ²γ_0\n...\nγ_j = φ^jγ_0\nStep 6: Compute autocorrelation function:\nρ_j = γ_j/γ_0 = φ^jγ_0/γ_0 = φ^j\nFinal Result: ρ_j = φ^j for all j ≥ 0\n\nQID: textbook-54-0-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-54-0-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the autocorrelation function ρ_j = φ^j for an AR(1) process, matching the gold answer step-by-step with accurate reasoning and calculations.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-54-0-2-1", "category": "CORRECT", "explanation": "The candidate answer correctly derives the autocorrelation function ρ_j = φ^j for an AR(1) process, matching the gold answer step-by-step with accurate reasoning and calculations."}, "llm_echoed_qid": "textbook-54-0-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the autocorrelation function ρ_j = φ^j for an AR(1) process, matching the gold answer step-by-step with accurate reasoning and calculations."}
{"script_processing_qid": "textbook-80-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the advantages and disadvantages of using marginal integration versus backfitting in GAPLM estimation.\n\nGOLD_ANSWER:\n1.  **Marginal Integration**:\n   - *Advantages*: Closed-form estimators, established asymptotic theory.\n   - *Disadvantages*: Requires undersmoothing, computationally intensive for high dimensions.\n2.  **Backfitting**:\n   - *Advantages*: Flexible, can handle large $q$ in practice.\n   - *Disadvantages*: Lack of theoretical guarantees for $q > 1$, convergence issues.\n\nCANDIDATE_ANSWER:\nStep 1: **Advantages of Marginal Integration**\n- Provides a closed-form estimator, enabling asymptotic theory derivation.\n- Allows parametric components (β, c) to achieve √n-rate estimation.\n- Works better for cases with multiple nonparametric components (q > 1), despite requiring undersmoothing.\n\nStep 2: **Disadvantages of Marginal Integration**\n- Requires undersmoothing of nonparametric components, complicating bandwidth selection and potentially increasing variance.\n\nStep 3: **Advantages of Backfitting**\n- Combines iterative algorithms (e.g., Gauss-Seidel, local scoring), which may be computationally efficient.\n- Inherits APLM estimation structure, suitable for additive frameworks.\n\nStep 4: **Disadvantages of Backfitting**\n- Lacks theoretical guarantees for q > 1, leading to unclear procedures and convergence issues.\n- No established asymptotic theory for GAPLM, limiting inference capabilities.\n\nStep 5: **Summary**\nMarginal integration offers theoretical tractability but requires undersmoothing. Backfitting is algorithmically flexible but lacks theoretical support for complex cases (q > 1).\n\nQID: textbook-80-1-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-80-1-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately captures the advantages and disadvantages of both marginal integration and backfitting as outlined in the gold answer, including details on theoretical guarantees, computational aspects, and handling of dimensions.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-1-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately captures the advantages and disadvantages of both marginal integration and backfitting as outlined in the gold answer, including details on theoretical guarantees, computational aspects, and handling of dimensions."}, "llm_echoed_qid": "textbook-80-1-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately captures the advantages and disadvantages of both marginal integration and backfitting as outlined in the gold answer, including details on theoretical guarantees, computational aspects, and handling of dimensions."}
{"script_processing_qid": "textbook-99-0-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Discuss the impact of the coefficient $c_{J0}$ on the universal bivariate density estimate in Figure 6.11. How does varying $c_{J0}$ affect the estimate?\n\nGOLD_ANSWER:\n1. **High $c_{J0}$ (e.g., 6 or 4)**: \n   - Captures fine details (e.g., ridge for Alaska).\n   - May overfit sparse regions.\n2. **Low $c_{J0}$ (e.g., 2 or -1)**: \n   - Smoother estimates, fewer details.\n   - More stable but may miss subtle patterns.\n3. **Trade-off**: Balance between detail and stability depends on $c_{J0}$.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain sufficient information to answer this question. While Figure 6.11 and its analysis describe characteristics of the density estimate (main concentration areas, Louisiana/Alaska impacts), there is no explicit mention of: (1) what parameter $c_{J0}$ represents in the estimation method, (2) how it is mathematically defined, or (3) how variations in $c_{J0}$ interact with the estimation process. The context focuses on general observations about the data distribution and results of the applied method, but no technical details about the estimator’s parameters or their tuning are included.\n\nQID: textbook-99-0-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-0-1-1\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer states there is insufficient information.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-0-1-1", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer states there is insufficient information."}, "llm_echoed_qid": "textbook-99-0-1-1", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer states there is insufficient information."}
{"script_processing_qid": "textbook-108-4-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. What is reverse causality, and how does it complicate the interpretation of economic graphs? Provide an example from the text.\n\nGOLD_ANSWER:\n1.  **Definition**: Reverse causality occurs when the presumed cause-and-effect relationship between two variables is actually the opposite of what is assumed.\n2.  **Example**: The text mentions a graph showing a positive correlation between the number of police officers and violent crime in cities. This could be misinterpreted as police causing crime, when in reality, higher crime rates may lead to more police being hired.\n\nCANDIDATE_ANSWER:\nStep 1: Reverse causality occurs when the presumed direction of causation between two variables is inverted. For example, a researcher might incorrectly conclude that variable A causes variable B, when in fact variable B causes variable A.\n\nStep 2: This complicates the interpretation of economic graphs because observed correlations between variables may reflect the reverse causal mechanism, leading to flawed conclusions. Graphs alone cannot resolve the ambiguity of causation direction without additional evidence.\n\nStep 3: The text provides an implicit example by contrasting the hypothetical demand curve (where price clearly causes changes in quantity demanded) with real-world scenarios. In real data, if one were to graph two variables (e.g., price and quantity) without controlled conditions, a researcher might mistakenly infer that changes in quantity demanded cause changes in price (reverse causality), whereas the demand curve framework assumes the opposite. The text emphasizes this challenge but does not provide an explicit real-world case.\n\nQID: textbook-108-4-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-108-4-0-1\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly defines reverse causality and explains how it complicates the interpretation of economic graphs. However, the example provided is not explicitly from the text, as the candidate acknowledges, and it does not match the specific example given in the gold answer. The candidate's example is hypothetical and not grounded in the text's context.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-4-0-1", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly defines reverse causality and explains how it complicates the interpretation of economic graphs. However, the example provided is not explicitly from the text, as the candidate acknowledges, and it does not match the specific example given in the gold answer. The candidate's example is hypothetical and not grounded in the text's context."}, "llm_echoed_qid": "textbook-108-4-0-1", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly defines reverse causality and explains how it complicates the interpretation of economic graphs. However, the example provided is not explicitly from the text, as the candidate acknowledges, and it does not match the specific example given in the gold answer. The candidate's example is hypothetical and not grounded in the text's context."}
{"script_processing_qid": "textbook-75-0-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Analyze the Prisoner's Dilemma and its implications for collusion in oligopolies.\n\nGOLD_ANSWER:\n1.  **Dilemma**: Both players defect despite mutual cooperation being Pareto superior.\n2.  **Oligopoly**: Firms face a similar incentive to cheat on collusive agreements, leading to competitive outcomes.\n3.  **Repeated Interaction**: Collusion can be sustained if the game is repeated and players value future payoffs sufficiently.\n\nCANDIDATE_ANSWER:\nStep 1: The Prisoner's Dilemma models a scenario where two rational agents face conflicting incentives between cooperation and defection. In a one-shot game, the Nash equilibrium is mutual defection (e.g., both prisoners confessing), even though mutual cooperation yields a better collective outcome.\n\nStep 2: In oligopolies, firms face a similar dilemma. Collusion (cooperation) would maximize joint profits, but each firm has an incentive to cheat (defect) by lowering prices or increasing output to gain market share, leading to a non-cooperative Nash equilibrium (e.g., Cournot or Bertrand competition outcomes).\n\nStep 3: The context highlights repeated games (e.g., 'Maintaining a cartel'). In repeated interactions, firms can use strategies like tit-for-tat to punish defection, enabling collusion to persist as a subgame-perfect equilibrium. This explains why cartels may form despite the static Prisoner's Dilemma logic predicting instability.\n\nStep 4: Implications include that collusion is inherently unstable in one-shot interactions (due to dominant strategies to defect) but becomes feasible in repeated games with credible threats of retaliation. However, factors like incomplete information or lack of enforcement mechanisms (as in the static Prisoner's Dilemma) may still undermine collusion.\n\nQID: textbook-75-0-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-75-0-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the Prisoner's Dilemma, its application to oligopolies, and the role of repeated interactions in sustaining collusion, aligning perfectly with the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-75-0-2-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the Prisoner's Dilemma, its application to oligopolies, and the role of repeated interactions in sustaining collusion, aligning perfectly with the gold answer's key points."}, "llm_echoed_qid": "textbook-75-0-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the Prisoner's Dilemma, its application to oligopolies, and the role of repeated interactions in sustaining collusion, aligning perfectly with the gold answer's key points."}
{"script_processing_qid": "textbook-117-28-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the equivalent annual cost (EAC) for Machine $A$, given its costs ($\\$500$ at year 0 and $\\$120$ annually for 3 years) and a discount rate of 10%.\n\nGOLD_ANSWER:\n1. **PV of Costs**: $\\$500 + \\frac{\\$120}{1.1} + \\frac{\\$120}{(1.1)^2} + \\frac{\\$120}{(1.1)^3} \\approx \\$798.42$.\n2. **Annuity Factor**: $A_{0.10}^3 = \\frac{1 - (1.1)^{-3}}{0.1} \\approx 2.4869$.\n3. **EAC**: $\\frac{\\$798.42}{2.4869} \\approx \\$321.05$.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate the present value (PV) of Machine A's costs.\n- Initial cost at Year 0: $500 (already in PV)\n- Annual $120 costs for 3 years: PV = 120 × [1 - (1.10)^-3]/0.10 = 120 × 2.48685 ≈ $298.42\nTotal PV = $500 + $298.42 = $798.42\n\nStep 2: Annualize the total PV over 3 years.\nEquivalent Annual Cost (EAC) = $798.42 ÷ [(1 - (1.10)^-3)/0.10] = $798.42 ÷ 2.48685 ≈ $321.05\n\nQID: textbook-117-28-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-28-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer matches the gold answer exactly, with correct calculations for PV, annuity factor, and EAC.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-28-1-0", "category": "CORRECT", "explanation": "The candidate's answer matches the gold answer exactly, with correct calculations for PV, annuity factor, and EAC."}, "llm_echoed_qid": "textbook-117-28-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer matches the gold answer exactly, with correct calculations for PV, annuity factor, and EAC."}
{"script_processing_qid": "textbook-80-3-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the role of the bandwidth conditions $n h\\widetilde{h}^{(d-1)}/\\log^{2}(n) \\to \\infty$ and $\\widetilde{h}^{q}h^{\\nu-p-1} \\to 0$ in ensuring the asymptotic normality result.\n\nGOLD_ANSWER:\n2. **Bandwidth Conditions**:\n   - **First Condition**: $n h\\widetilde{h}^{(d-1)}/\\log^{2}(n) \\to \\infty$ ensures that the effective sample size grows sufficiently fast to control the stochastic variability of the estimator.\n   - **Second Condition**: $\\widetilde{h}^{q}h^{\\nu-p-1} \\to 0$ guarantees that the bias from higher-order terms in the Taylor expansion vanishes asymptotically, preserving the leading bias term derived above.\n\nCANDIDATE_ANSWER:\nStep 1: Role of the first bandwidth condition ($n h\\widetilde{h}^{(d-1)}/\\log^{2}(n) \\to \\infty$):\\nThis ensures sufficient effective sample size for local estimation. The term $h\\widetilde{h}^{(d-1)}$ represents the volume of the local neighborhood for estimation in $d$-dimensional space (with $d-1$ covariates handled via $\\widetilde{h}$). The growth condition guarantees enough observations in these neighborhoods to control variance and enable Central Limit Theorem application.\\n\\nStep 2: Role of the second bandwidth condition ($\\widetilde{h}^{q}h^{\\nu-p-1} \\to 0$):\\nThis controls bias components:\\n- $\\widetilde{h}^q$ captures bias from higher-order terms in other dimensions (eliminated by $q^{th}$-order kernel $\\kappa$)\\n- $h^{\\nu-p-1}$ relates to residual bias from estimating $\\nu^{th}$ derivatives using $p^{th}$-order polynomials\\nThe product tending to zero ensures these bias terms vanish faster than the convergence rate of the estimator, preserving asymptotic normality.\\n\\nStep 3: Combined effect:\\nThe first condition handles variance control while the second manages bias decay. Together, they ensure the estimator's distribution is dominated by the CLT-driven variance term and leading-designated bias term ($b_\\alpha(x_\\alpha)$), producing the stated normal limit distribution.\n\nQID: textbook-80-3-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-80-3-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the roles of both bandwidth conditions in ensuring asymptotic normality. The first condition is correctly described as ensuring sufficient effective sample size for local estimation, while the second condition is correctly explained as controlling bias components to preserve asymptotic normality. The combined effect is also accurately summarized.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-3-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains the roles of both bandwidth conditions in ensuring asymptotic normality. The first condition is correctly described as ensuring sufficient effective sample size for local estimation, while the second condition is correctly explained as controlling bias components to preserve asymptotic normality. The combined effect is also accurately summarized."}, "llm_echoed_qid": "textbook-80-3-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the roles of both bandwidth conditions in ensuring asymptotic normality. The first condition is correctly described as ensuring sufficient effective sample size for local estimation, while the second condition is correctly explained as controlling bias components to preserve asymptotic normality. The combined effect is also accurately summarized."}
{"script_processing_qid": "textbook-95-1-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the robustness of the assumption that price level and output are unaffected contemporaneously by policy shocks. How might this assumption be tested?\n\nGOLD_ANSWER:\n**Discussion**:\n-   **A priori**: More plausible for high-frequency (e.g., monthly) data where rigidities delay price/output adjustments.\n-   **Tests**:\n    1.  Compare impulse responses across quarterly vs. monthly data (as in Figure 3).\n    2.  Use alternative identification schemes (e.g., sign restrictions) that relax this assumption.\n    3.  Test for statistical significance of contemporaneous responses in extended VARs.\n-   **Implications**: Violations could bias estimates of policy transmission lags and magnitudes.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the Assumption\n\nThe maintained assumption states that aggregate price level and output do not respond contemporaneously (i.e., in the impact period) to monetary policy shocks (e.g., $FF$, NBR, or NBR/TR shocks). This is a key identification restriction in the structural VAR framework used to isolate policy shocks.\n\nStep 2: Robustness Concerns\n- Robustness depends on whether omitted contemporaneous feedback effects between policy shocks and price/output are negligible. If prices/output actually react immediately to policy shocks, the identification assumption may bias the estimated impulse responses.\n- The assumption is more plausible if policy shocks are orthogonal to other unobserved factors affecting price/output in the same period (e.g., demand shocks).\n\nStep 3: Testing the Assumption\n1. **Alternative Orderings**: Test if results are sensitive to VAR ordering changes. If price/output are placed before policy variables in the Cholesky decomposition, the assumption is relaxed. Significant changes in results would undermine robustness.\n2. **Over-Identification Tests**: Impose additional zero restrictions (e.g., lagged effects) and use likelihood-ratio tests to check consistency with the original model.\n3. **Instrumental Variables**: Use external instruments (e.g., narrative policy shocks) to identify monetary policy shocks independently. If estimated effects align with the original assumption, robustness is supported.\n4. **High-Frequency Data**: Analyze intra-period market reactions (e.g., around FOMC announcements) to detect immediate price/output responses inconsistent with the assumption.\n\nStep 4: Empirical Implications\nIf tests reveal significant contemporaneous price/output responses, the original identification scheme may require revision (e.g., allowing for partial contemporaneous effects or adopting alternative identification methods like sign restrictions).\n\nQID: textbook-95-1-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-95-1-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly discusses the robustness of the assumption, outlines plausible concerns, and provides detailed methods for testing the assumption, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-95-1-1-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly discusses the robustness of the assumption, outlines plausible concerns, and provides detailed methods for testing the assumption, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-95-1-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly discusses the robustness of the assumption, outlines plausible concerns, and provides detailed methods for testing the assumption, aligning well with the gold answer."}
{"script_processing_qid": "textbook-114-2-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the difference between ex ante and ex post forecasts in the context of regression analysis.\n\nGOLD_ANSWER:\n1.  **Ex Ante Forecast**: Predicts future values of the dependent variable beyond the estimation period. It is conditional on predicted values of the explanatory variables.\n2.  **Ex Post Forecast**: Predicts values of the dependent variable within the estimation period but for scenarios where explanatory variables take different values. It can be validated against observed data.\n\nCANDIDATE_ANSWER:\nStep 1: Define ex ante forecasts. Based on the context, ex ante forecasts predict values of the dependent variable (e.g., auto sales) for time periods **beyond the estimation sample** of the regression model. These require assumptions about both the explanatory variables (P, I, R) and the error term (set to zero if unknown).\n\nStep 2: Define ex post forecasts. While not explicitly detailed in the text, the term 'ex post' logically contrasts with ex ante. Ex post forecasts predict values **within the estimation sample period**, where actual values of the explanatory variables are already observed and known with certainty.\n\nStep 3: Highlight key differences. Ex ante forecasts involve **uncertainty about future explanatory variables** (e.g., needing to predict P, I, R) and the error term, while ex post forecasts use **already observed values of explanatory variables**, reducing one source of forecast error. The standard error of forecast (SEF) in ex ante forecasts accounts for both parameter estimation uncertainty and inherent error term variability, whereas ex post forecasts may have narrower confidence intervals since explanatory variables are fixed and known.\n\nQID: textbook-114-2-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-2-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately defines both ex ante and ex post forecasts, clearly distinguishing between them by highlighting the use of predicted versus observed explanatory variables and the implications for forecast uncertainty. This aligns perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-2-2-1", "category": "CORRECT", "explanation": "The candidate's answer accurately defines both ex ante and ex post forecasts, clearly distinguishing between them by highlighting the use of predicted versus observed explanatory variables and the implications for forecast uncertainty. This aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-114-2-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately defines both ex ante and ex post forecasts, clearly distinguishing between them by highlighting the use of predicted versus observed explanatory variables and the implications for forecast uncertainty. This aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-99-8-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the challenges in estimating seasonal components when the stochastic term is generated by an ARMA(1,1) process, as illustrated in Figure 5.1.\n\nGOLD_ANSWER:\n4. **Challenges with ARMA(1,1) Noise**: The dependency in the noise terms can lead to periods where observations are consistently above or below the seasonal component, making it difficult to estimate the true seasonal pattern. This dependency complicates the estimation process and can lead to biased or inconsistent estimates if not properly accounted for.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the key challenges in seasonal component estimation under ARMA(1,1) noise:\n- The conventional estimator assumes uncorrelated residuals when calculating seasonal means. However, an ARMA(1,1) process introduces serial correlation in the stochastic term, violating this assumption.\n\nStep 2: Explain the consequences for conventional estimation:\n- The sample mean estimator becomes inefficient due to correlated errors, leading to increased variance in seasonal estimates (even when T=20 and n=100).\n- Detrending residuals inherit the ARMA(1,1) dependence structure, making seasonal component estimation more complex than under iid assumptions.\n\nStep 3: Address nonparametric estimator limitations:\n- While smoothing helps mitigate some issues with large periods (T=20), the ARMA(1,1) structure creates non-uniform noise patterns that complicate bandwidth selection for the universal estimator.\n- Sharp seasonal features (like Delta functions) remain challenging to estimate because the ARMA(1,1) process introduces persistent deviations that can be mistaken for genuine seasonal patterns.\n\nStep 4: Highlight period-related complications:\n- The combination of long-period seasonality (T=20) and ARMA(1,1) noise increases the risk of conflating seasonal effects with autoregressive/moving average components, particularly when n=100 provides limited cycles for estimation.\n\nQID: textbook-99-8-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-99-8-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the challenges of estimating seasonal components under ARMA(1,1) noise, including issues with conventional estimators, consequences of correlated errors, limitations of nonparametric estimators, and period-related complications. This aligns well with the gold answer's focus on the dependency in noise terms complicating the estimation process.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-8-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the challenges of estimating seasonal components under ARMA(1,1) noise, including issues with conventional estimators, consequences of correlated errors, limitations of nonparametric estimators, and period-related complications. This aligns well with the gold answer's focus on the dependency in noise terms complicating the estimation process."}, "llm_echoed_qid": "textbook-99-8-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the challenges of estimating seasonal components under ARMA(1,1) noise, including issues with conventional estimators, consequences of correlated errors, limitations of nonparametric estimators, and period-related complications. This aligns well with the gold answer's focus on the dependency in noise terms complicating the estimation process."}
{"script_processing_qid": "textbook-109-3-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Analyze Williamson's (1989) transaction cost rationale for vertical mergers. How does asymmetric information justify vertical integration?\n\nGOLD_ANSWER:\n1. **Transaction Costs**: Vertical mergers reduce:\n   - **Bargaining Costs**: Negotiations under asymmetric information lead to inefficiencies (e.g., hold-up problems).\n   - **Contracting Costs**: Incomplete contracts fail to specify all contingencies, necessitating integration.\n2. **Asymmetric Information**: \n   - **Supplier Power**: Integrated firm avoids input price markup when suppliers exploit private cost information.\n   - **Quality Assurance**: Eliminates moral hazard in upstream production (e.g., hidden effort).\n\nCANDIDATE_ANSWER:\nStep 1: Transaction Cost Rationale for Vertical Mergers (Williamson, 1989)\nWilliamson's transaction cost framework argues that vertical mergers reduce costs associated with market transactions. Specifically, vertical integration eliminates inefficiencies in bargaining and contracting between suppliers and buyers.\n\nStep 2: Role of Asymmetric Information\nAsymmetric information between vertically linked firms (e.g., differing knowledge about production costs, demand, or quality) increases bargaining costs. For example, suppliers and buyers may engage in prolonged negotiations or opportunistic behavior due to incomplete information.\n\nStep 3: Vertical Integration as a Solution\nBy merging, firms internalize transactions. This reduces the need for repeated negotiations and mitigates information asymmetry risks. Integrated firms can coordinate inputs/outputs more efficiently, align incentives, and share sensitive information internally, lowering transaction costs.\n\nQID: textbook-109-3-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-109-3-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately captures Williamson's transaction cost rationale for vertical mergers and correctly explains how asymmetric information justifies vertical integration. The steps align with the gold answer, addressing bargaining costs, contracting costs, and the role of asymmetric information in increasing transaction costs, which vertical integration mitigates.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-109-3-1-1", "category": "CORRECT", "explanation": "The candidate's answer accurately captures Williamson's transaction cost rationale for vertical mergers and correctly explains how asymmetric information justifies vertical integration. The steps align with the gold answer, addressing bargaining costs, contracting costs, and the role of asymmetric information in increasing transaction costs, which vertical integration mitigates."}, "llm_echoed_qid": "textbook-109-3-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately captures Williamson's transaction cost rationale for vertical mergers and correctly explains how asymmetric information justifies vertical integration. The steps align with the gold answer, addressing bargaining costs, contracting costs, and the role of asymmetric information in increasing transaction costs, which vertical integration mitigates."}
{"script_processing_qid": "textbook-117-35-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Given the spot rates for years 1 to 4 as 5%, 6%, 7%, and 6% respectively, derive the forward rates for years 2, 3, and 4 using the formula $$f_{n} = \\frac{(1 + s_{n})^{n}}{(1 + s_{n-1})^{n-1}} - 1$$, where $s_{n}$ is the spot rate for year $n$.\n\nGOLD_ANSWER:\n1. **Forward Rate for Year 2**: $$f_{2} = \\frac{(1.06)^2}{1.05} - 1 = 7.01\\%$$\n2. **Forward Rate for Year 3**: $$f_{3} = \\frac{(1.07)^3}{(1.06)^2} - 1 = 9.03\\%$$\n3. **Forward Rate for Year 4**: $$f_{4} = \\frac{(1.06)^4}{(1.07)^3} - 1 = 3.06\\%$$\n\nCANDIDATE_ANSWER:\nStep 1: Recall the formula for forward rates: $$f_{n} = \\frac{(1 + s_{n})^{n}}{(1 + s_{n-1})^{n-1}} - 1$$.\n\nStep 2: Compute $f_2$ (forward rate for year 2):\n$$f_2 = \\frac{(1 + 0.06)^2}{(1 + 0.05)} - 1 = \\frac{1.1236}{1.05} - 1 = 1.0701 - 1 = 7.01\\%$$\n\nStep 3: Compute $f_3$ (forward rate for year 3):\n$$f_3 = \\frac{(1 + 0.07)^3}{(1 + 0.06)^2} - 1 = \\frac{1.225043}{1.1236} - 1 = 1.0903 - 1 = 9.03\\%$$\n\nStep 4: Compute $f_4$ (forward rate for year 4):\n$$f_4 = \\frac{(1 + 0.06)^4}{(1 + 0.07)^3} - 1 = \\frac{1.26247696}{1.225043} - 1 = 1.0306 - 1 = 3.06\\%$$\n\nFinal forward rates: $f_2 = 7.01\\%$, $f_3 = 9.03\\%$, $f_4 = 3.06\\%$.\n\nQID: textbook-117-35-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-35-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's calculations and reasoning perfectly match the gold answer, including the correct application of the formula and accurate computation of the forward rates for years 2, 3, and 4.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-35-0-0", "category": "CORRECT", "explanation": "The candidate's calculations and reasoning perfectly match the gold answer, including the correct application of the formula and accurate computation of the forward rates for years 2, 3, and 4."}, "llm_echoed_qid": "textbook-117-35-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's calculations and reasoning perfectly match the gold answer, including the correct application of the formula and accurate computation of the forward rates for years 2, 3, and 4."}
{"script_processing_qid": "textbook-110-13-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n5. Under what conditions does Theorem 13.1 guarantee that an equal distribution of income is welfare-maximizing?\n\nGOLD_ANSWER:\n1. **Identical Individuals**: All agents have identical preferences and utility functions.\n2. **Symmetric Social Welfare Function**: The welfare function treats all individuals symmetrically.\n3. **Feasible Set Properties**: The set $Y$ of feasible income distributions must be symmetric and convex.\n\nCANDIDATE_ANSWER:\nTheorem 13.1 guarantees that an equal income distribution is welfare-maximizing under the following key condition:\n\nStep 1: The model assumes perfect feasibility of lump-sum transfers. This means there must be no constraints on redistributing income between individuals without efficiency losses or transaction costs.\n\nStep 2: The social welfare maximization problem operates within the set of possible income distributions Y*, where total income K remains constant (∑yʰ = K).\n\nStep 3: The theorem implicitly relies on a symmetric social welfare function that treats all individuals equally, leading to maximum welfare when incomes are equalized along the 45° line in the two-person case.\n\nStep 4: Crucially, this result holds only in first-best scenarios where lump-sum transfers can fully overcome initial inequalities without distorting production or incentives (a theoretical condition rarely met in real-world applications).\n\nQID: textbook-110-13-2-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-110-13-2-0\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the need for a symmetric social welfare function and feasibility of lump-sum transfers, but it misses explicitly stating the condition of identical preferences/utility functions and the convexity/symmetry of the feasible set Y as required by the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-13-2-0", "category": "PARTIALLY_CORRECT", "explanation": "The candidate answer correctly identifies the need for a symmetric social welfare function and feasibility of lump-sum transfers, but it misses explicitly stating the condition of identical preferences/utility functions and the convexity/symmetry of the feasible set Y as required by the gold answer."}, "llm_echoed_qid": "textbook-110-13-2-0", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the need for a symmetric social welfare function and feasibility of lump-sum transfers, but it misses explicitly stating the condition of identical preferences/utility functions and the convexity/symmetry of the feasible set Y as required by the gold answer."}
{"script_processing_qid": "textbook-67-2-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Critically evaluate the role of credibility in exchange-rate-based stabilization programs, with reference to Talvi (1997).\n\nGOLD_ANSWER:\n1.  **Initial Boom**: Lack of credibility may cause a consumption boom due to temporariness effects, inflating tax revenues and masking fiscal deficits.\n2.  **False Signals**: Policymakers misinterpret higher monetization or tax revenues as permanent, leading to unsustainable spending increases.\n3.  **Post-Crisis Collapse**: When the program collapses, fiscal deficits explode due to prior spending commitments and falling revenues.\n4.  **Policy Dilemma**: Imperfect credibility creates a trap where short-term gains incentivize policies that exacerbate long-term vulnerabilities.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the key elements of credibility in exchange-rate-based stabilization programs. Credibility typically refers to the public’s belief in the government’s commitment to maintaining the announced exchange rate regime.\n\nStep 2: Connect the context to credibility. The provided text emphasizes that demand for emerging markets’ assets is highly sensitive to policy volatility (Calvo, 1998) and investor behavior influenced by rumors rather than fundamentals. This implies that credibility deficits in stabilization programs could amplify volatility.\n\nStep 3: Analyze the role of short-term debt. The context highlights that short-maturity debt creates vulnerability to self-fulfilling crises (Cole and Kehoe, 1996). If a stabilization program lacks credibility, short-term debt holders may refuse to roll over debt, triggering a balance-of-payments crisis.\n\nStep 4: Link globalization to credibility challenges. Globalization increases exposure to sudden capital flow reversals. Investors diversifying across emerging markets may withdraw at the first sign of policy inconsistency, undermining stabilization efforts.\n\nStep 5: Critical evaluation. The context suggests credibility is pivotal because:\n- Policy volatility in emerging markets (e.g., inconsistent fiscal/monetary measures) reduces trust in stabilization programs.\n- Globalization magnifies the consequences of credibility gaps, as rumors can trigger capital flight even if fundamentals are sound.\n- Short-term debt structures make credibility self-reinforcing: low credibility raises rollover risk, which further destabilizes the exchange rate.\n\nHowever, the provided context does not explicitly reference Talvi (1997), limiting a direct evaluation of his arguments. The analysis here is based on general principles from Calvo (1998) and Cole and Kehoe (1996) as presented in the text.\n\nQID: textbook-67-2-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-67-2-0-3\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate answer provides a detailed analysis of credibility in exchange-rate-based stabilization programs but does not directly address Talvi (1997) as required. The reasoning is sound but lacks specific reference to the cited source.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-67-2-0-3", "category": "PARTIALLY_CORRECT", "explanation": "The candidate answer provides a detailed analysis of credibility in exchange-rate-based stabilization programs but does not directly address Talvi (1997) as required. The reasoning is sound but lacks specific reference to the cited source."}, "llm_echoed_qid": "textbook-67-2-0-3", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate answer provides a detailed analysis of credibility in exchange-rate-based stabilization programs but does not directly address Talvi (1997) as required. The reasoning is sound but lacks specific reference to the cited source."}
{"script_processing_qid": "textbook-100-2-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the concept of 'merging of opinion' in Bayesian asymptotics and its implications for prior sensitivity.\n\nGOLD_ANSWER:\nThe 'merging of opinion' refers to the phenomenon where, as the sample size grows, the posterior distributions derived from different priors converge to the same limiting distribution, provided the priors have support over the true parameter value. This implies that the influence of the prior diminishes with increasing data, alleviating concerns about prior sensitivity in large samples. However, in small samples, the choice of prior can significantly affect the posterior, highlighting the importance of careful prior specification.\n\nCANDIDATE_ANSWER:\nStep 1: Merging of opinion in Bayesian asymptotics refers to the phenomenon where posterior distributions derived from different prior distributions converge to the same limiting distribution as the sample size grows indefinitely. \n\nStep 2: This occurs because Bayesian estimates (under a broad class of priors) become asymptotically equivalent to the maximum likelihood estimator ($\\widehat{\\theta}_{\\mathrm{ML}}$), which is strongly consistent and asymptotically efficient. The influence of the prior diminishes with increasing data.\n\nStep 3: The implications for prior sensitivity are twofold: (1) For large samples, Bayesian conclusions become robust to substantially different prior choices, as posteriors merge toward the true parameter value. (2) In finite samples, however, prior differences can still lead to markedly different inferences, explaining why degenerate priors (e.g., a point mass at $c$) yield admissible but impractical estimators in small-sample regimes.\n\nStep 4: The merging result aligns with the Complete Class Theorem’s assertion that Bayes rules dominate frequentist approaches in admissibility, while also demonstrating that asymptotic arguments mitigate practical concerns about prior subjectivity in large-data settings.\n\nQID: textbook-100-2-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-100-2-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the concept of 'merging of opinion' and its implications for prior sensitivity, aligning with the gold answer. It correctly notes the convergence of posterior distributions with increasing sample size and the diminishing influence of the prior, while also addressing the importance of prior choice in small samples.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-100-2-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains the concept of 'merging of opinion' and its implications for prior sensitivity, aligning with the gold answer. It correctly notes the convergence of posterior distributions with increasing sample size and the diminishing influence of the prior, while also addressing the importance of prior choice in small samples."}, "llm_echoed_qid": "textbook-100-2-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the concept of 'merging of opinion' and its implications for prior sensitivity, aligning with the gold answer. It correctly notes the convergence of posterior distributions with increasing sample size and the diminishing influence of the prior, while also addressing the importance of prior choice in small samples."}
{"script_processing_qid": "textbook-105-3-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain the significance of the condition $m\\in\\overline{\\mathcal{F}}$ in Theorem 17.2. How does it affect the convergence rate?\n\nGOLD_ANSWER:\nThe condition $m\\in\\overline{\\mathcal{F}}$ ensures that the regression function $m$ can be approximated arbitrarily well by functions in $\\mathcal{F}$. This guarantees that the infimum term in the error bound converges to zero, leading to the simplified convergence rate $$ O\\left(\\beta\\_{n}^{2}\\left(\\frac{\\log(\\beta\\_{n}n)}{n}\\right)^{1/2}\\right). $$ Without this condition, the approximation error might not vanish, affecting the overall convergence.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the role of $m\\in\\overline{\\mathcal{F}}$ in Theorem 17.2\\n- The condition $m\\in\\overline{\\mathcal{F}}$ implies the target function belongs to the closure of the union of all candidate classes $\\mathcal{F}_k$. This ensures the approximation error term $\\inf_{f\\in\\mathcal{F}_k} \\mathbf{E}\\int|f(x)-m(x)|^2\\mu(dx)$ can be made arbitrarily small for sufficiently large $k$.\\n\\nStep 2: Analyze convergence rate components\\nThe error bound consists of:\\n1. Complexity term: $O\\left(\\frac{\\beta_n^4}{n}(k\\log \\beta_n n + t_k)\\right)$\\n2. Approximation error term\\n\\nStep 3: Impact of $m\\in\\overline{\\mathcal{F}}$\\nWhen $m\\in\\overline{\\mathcal{F}}$, the approximation error decreases as $k$ increases. This allows the minimum over $k$ in Theorem 17.2 to achieve an optimal balance:\\n- For smaller $k$: Larger approximation error but smaller complexity penalty\\n- For larger $k$: Smaller approximation error but larger complexity penalty\\n\\nStep 4: Convergence rate implications\\nThe condition enables the bound to achieve:\\n1. Polynomial convergence rate when $\\beta_n$ grows polynomially in $n$\\n2. Faster convergence compared to cases where $m\\notin\\overline{\\mathcal{F}}$ (where approximation error would dominate)\\n3. Proper error decomposition that satisfies the conditions for complexity regularization to work effectively\n\nQID: textbook-105-3-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-105-3-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains the significance of the condition $m\\\\in\\\\overline{\\\\mathcal{F}}$ in Theorem 17.2, detailing how it ensures the approximation error can be made arbitrarily small and how this affects the convergence rate. The explanation aligns with the gold answer, providing a comprehensive analysis of the components and implications of the condition.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-105-3-0-3", "category": "CORRECT", "explanation": "The candidate answer correctly explains the significance of the condition $m\\in\\overline{\\mathcal{F}}$ in Theorem 17.2, detailing how it ensures the approximation error can be made arbitrarily small and how this affects the convergence rate. The explanation aligns with the gold answer, providing a comprehensive analysis of the components and implications of the condition."}, "llm_echoed_qid": "textbook-105-3-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains the significance of the condition $m\\in\\overline{\\mathcal{F}}$ in Theorem 17.2, detailing how it ensures the approximation error can be made arbitrarily small and how this affects the convergence rate. The explanation aligns with the gold answer, providing a comprehensive analysis of the components and implications of the condition."}
{"script_processing_qid": "textbook-109-4-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Propose a fuzzy rule-based system for a trading algorithm that adjusts order size based on 'market volatility' (input) and 'aggressiveness' (output). Define the membership functions and rules.\n\nGOLD_ANSWER:\n1. **Input**: 'Volatility' (Low, Medium, High) with trapezoidal $\\mu$.\n2. **Output**: 'Aggressiveness' (Conservative, Neutral, Aggressive).\n3. **Rules**: \n   - IF Volatility = High THEN Aggressiveness = Conservative.\n   - IF Volatility = Medium THEN Aggressiveness = Neutral.\n   - IF Volatility = Low THEN Aggressiveness = Aggressive.\n4. **Defuzzification**: Use centroid method to convert fuzzy output to crisp action.\n\nCANDIDATE_ANSWER:\nStep 1: Define Input/Output Variables\n- Input Variable: Market Volatility (normalized range 0-1)\n- Output Variable: Order Size Aggressiveness (normalized range 0-1)\n\nStep 2: Create Membership Functions\nInput - Market Volatility:\n- Low (trapezoidal: 0, 0, 0.2, 0.4)\n- Medium (triangular: 0.3, 0.5, 0.7)\n- High (trapezoidal: 0.6, 0.8, 1, 1)\n\nOutput - Aggressiveness:\n- Conservative (trapezoidal: 0, 0, 0.3, 0.5)\n- Moderate (triangular: 0.4, 0.6, 0.8)\n- Aggressive (trapezoidal: 0.7, 0.9, 1, 1)\n\nStep 3: Design Fuzzy Rules\n1. IF Volatility IS Low THEN Aggressiveness IS Aggressive\n2. IF Volatility IS Medium THEN Aggressiveness IS Moderate\n3. IF Volatility IS High THEN Aggressiveness IS Conservative\n\nStep 4: Implementation Logic\n- Fuzzify volatility input using membership functions\n- Apply rules with AND/OR operators (typically AND for single-input systems)\n- Aggregate outputs using max-min composition\n- Defuzzify using centroid method to get crisp aggressiveness value\n\nThis system increases order size aggressively in stable markets (low volatility) and becomes conservative during high volatility, with smooth transitions between regimes.\n\nQID: textbook-109-4-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-109-4-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately defines the membership functions and rules for the fuzzy rule-based system, matching the gold answer's structure and logic. The detailed steps and implementation logic provided by the candidate enhance the clarity and completeness of the solution.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-109-4-2-1", "category": "CORRECT", "explanation": "The candidate's answer accurately defines the membership functions and rules for the fuzzy rule-based system, matching the gold answer's structure and logic. The detailed steps and implementation logic provided by the candidate enhance the clarity and completeness of the solution."}, "llm_echoed_qid": "textbook-109-4-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately defines the membership functions and rules for the fuzzy rule-based system, matching the gold answer's structure and logic. The detailed steps and implementation logic provided by the candidate enhance the clarity and completeness of the solution."}
{"script_processing_qid": "textbook-64-3-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the MLE for the binomial parameter $\\Theta$.\n\nGOLD_ANSWER:\n1. **Log-Likelihood**: $$ \\ln L(\\Theta|\\mathbf{y}) = \\sum \\left[ F_i \\ln \\Theta + (K_i - F_i) \\ln(1 - \\Theta) \\right] $$\n2. **First-Order Condition**: $$ \\frac{\\partial \\ln L}{\\partial \\Theta} = \\sum \\left[ \\frac{F_i}{\\Theta} - \\frac{K_i - F_i}{1 - \\Theta} \\right] = 0 $$\n3. **Solve for $\\Theta$**: $$ \\Theta = \\frac{\\sum F_i}{\\sum K_i} $$\n\nCANDIDATE_ANSWER:\nStep 1: Start with the score function from the context:\n∂lnL(Θ|y)/∂Θ = Σ_i[F_i/Θ - (K_i - F_i)/(1-Θ)] = 0\n\nStep 2: Multiply through by Θ(1-Θ) to eliminate denominators:\nΣ_i[(1-Θ)F_i - Θ(K_i - F_i)] = 0\n\nStep 3: Expand and simplify each term:\nFor each observation i:\n(1-Θ)F_i - ΘK_i + ΘF_i = F_i - ΘK_i\n\nStep 4: Sum across all observations:\nΣ_i(F_i - ΘK_i) = 0\n\nStep 5: Solve for Θ:\nΣF_i = ΘΣK_i ⇒ Θ = (ΣF_i)/(ΣK_i)\n\nStep 6: The numerical result from the context confirms:\nΘ = 0.651596 = ΣF_i/ΣK_i\n\nThis matches the standard MLE formula for binomial success probability, where F_i represents successes and K_i represents trials in each observation.\n\nQID: textbook-64-3-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-64-3-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the MLE for the binomial parameter Θ, following the same steps as the gold answer and arriving at the same final expression Θ = (ΣF_i)/(ΣK_i).\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-64-3-1-2", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the MLE for the binomial parameter Θ, following the same steps as the gold answer and arriving at the same final expression Θ = (ΣF_i)/(ΣK_i)."}, "llm_echoed_qid": "textbook-64-3-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the MLE for the binomial parameter Θ, following the same steps as the gold answer and arriving at the same final expression Θ = (ΣF_i)/(ΣK_i)."}
{"script_processing_qid": "textbook-123-3-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What conditions must a Markov chain satisfy to be ergodic?\n\nGOLD_ANSWER:\nA Markov chain is ergodic if it is irreducible (all states communicate) and aperiodic (no state has a period greater than 1). This ensures the existence of a unique stationary distribution to which the chain converges regardless of the initial state.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that a Markov chain is ergodic if it is both irreducible and aperiodic. Irreducibility ensures all states communicate (every state is reachable from any other state). Step 2: Aperiodicity guarantees there are no periodic cycles in the state transitions. Step 3: Together, these conditions ensure the existence of a unique stationary distribution (invariant distribution) that the chain converges to, regardless of the initial distribution. Step 4: The example in the context shows a non-ergodic case since P′ has two distinct eigenvectors (π₁, π₂) corresponding to unit eigenvalues, implying multiple invariant distributions exist. This violates the uniqueness requirement for ergodicity.\n\nQID: textbook-123-3-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-3-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the conditions for a Markov chain to be ergodic: irreducibility and aperiodicity. It also accurately explains the implications of these conditions, including the existence of a unique stationary distribution. The additional example provided, while not directly relevant to the question, does not detract from the correctness of the main answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-3-1-2", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the conditions for a Markov chain to be ergodic: irreducibility and aperiodicity. It also accurately explains the implications of these conditions, including the existence of a unique stationary distribution. The additional example provided, while not directly relevant to the question, does not detract from the correctness of the main answer."}, "llm_echoed_qid": "textbook-123-3-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the conditions for a Markov chain to be ergodic: irreducibility and aperiodicity. It also accurately explains the implications of these conditions, including the existence of a unique stationary distribution. The additional example provided, while not directly relevant to the question, does not detract from the correctness of the main answer."}
{"script_processing_qid": "textbook-111-2-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the optimal inflation rate under discretionary policy starting from the loss function $L(u,\\pi)=u^{\\mathrm{n}}-\\alpha(\\pi-\\pi^{\\mathrm{e}})+\\gamma\\pi^{2}$.\n\nGOLD_ANSWER:\n1. **Objective**: Minimize $L(u,\\pi)=u^{\\mathrm{n}}-\\alpha(\\pi-\\pi^{\\mathrm{e}})+\\gamma\\pi^{2}$.\n2. **Differentiate**: $\\frac{dL}{d\\pi}=-\\alpha+2\\gamma\\pi=0$.\n3. **Solve for $\\pi$**: $\\pi=\\alpha/(2\\gamma)$.\n\nCANDIDATE_ANSWER:\nStep 1: Substitute the Phillips curve constraint into the loss function. Given the Phillips curve \\( u = u^{\\mathrm{n}} - \\alpha(\\pi - \\pi^{\\mathrm{e}}) \\), substitute this into the loss function \\( L(u, \\pi) = u + \\gamma \\pi^2 \\):\n\\[ L = u^{\\mathrm{n}} - \\alpha(\\pi - \\pi^{\\mathrm{e}}) + \\gamma \\pi^2. \\]\n\nStep 2: Under discretionary policy, the Fed treats \\( \\pi^{\\mathrm{e}} \\) as given (since expectations are already formed). Minimize \\( L \\) with respect to \\( \\pi \\). Compute the first derivative:\n\\[ \\frac{\\partial L}{\\partial \\pi} = -\\alpha + 2\\gamma \\pi. \\]\n\nStep 3: Set the derivative equal to zero to find the optimal inflation rate:\n\\[ -\\alpha + 2\\gamma \\pi = 0 \\implies \\pi = \\frac{\\alpha}{2\\gamma}. \\]\n\nThis derivation shows the optimal discretionary inflation rate depends on the trade-off parameter \\( \\alpha \\) (slope of the Phillips curve) and the inflation-aversion parameter \\( \\gamma \\).\n\nQID: textbook-111-2-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-2-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the optimal inflation rate by substituting the Phillips curve into the loss function, differentiating with respect to inflation, and solving for the optimal rate. The steps and final result match the gold answer exactly.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-2-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the optimal inflation rate by substituting the Phillips curve into the loss function, differentiating with respect to inflation, and solving for the optimal rate. The steps and final result match the gold answer exactly."}, "llm_echoed_qid": "textbook-111-2-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the optimal inflation rate by substituting the Phillips curve into the loss function, differentiating with respect to inflation, and solving for the optimal rate. The steps and final result match the gold answer exactly."}
{"script_processing_qid": "textbook-119-21-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Critically evaluate the advantages and disadvantages of VAR modeling as discussed in the text.\n\nGOLD_ANSWER:\n1. **Advantages**:\n   - **Simplicity**: All variables are endogenous; no need to classify as exogenous or endogenous.\n   - **Estimation**: OLS can be applied to each equation separately.\n   - **Forecasting**: Often outperforms complex simultaneous-equation models.\n2. **Disadvantages**:\n   - **A-theoretic**: Lacks theoretical grounding; exclusion/inclusion of variables is less rigorous.\n   - **Policy Analysis**: Less suited for policy analysis due to emphasis on forecasting.\n   - **Lag Length**: Choosing appropriate lag length is challenging; too many lags consume degrees of freedom.\n   - **Stationarity**: Requires all variables to be jointly stationary; nonstationary variables complicate analysis.\n\nCANDIDATE_ANSWER:\nStep 1: Identify advantages of VAR modeling from the context. The text highlights that VAR models use lagged values of multiple variables (e.g., $M_1$ and $R$) to forecast future values, capturing interdependencies between variables. This multivariate approach improves forecasting accuracy compared to univariate models.\n\nStep 2: Infer potential disadvantages from the example. While not explicitly discussed, the context implies computational complexity (e.g., estimating coefficients for multiple lags of multiple variables) and data requirements. The model in Table 22.3 uses two lags for both $M_1$ and $R$, requiring sufficient historical data and risking overfitting if lag length is excessive.\n\nStep 3: Evaluate limitations based on the application. The example focuses on out-of-sample forecasting (excluding 1988 data for estimation), but the context does not address critical issues like sensitivity to lag-length selection, difficulty in interpreting coefficients in multi-equation systems, or the absence of structural economic theory guiding variable inclusion.\n\nQID: textbook-119-21-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-21-0-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate identifies some advantages (forecasting accuracy) and disadvantages (computational complexity, data requirements) but misses key points from the gold answer like simplicity, OLS estimation, a-theoretic nature, and stationarity issues. Their reasoning is partially aligned but incomplete.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-21-0-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate identifies some advantages (forecasting accuracy) and disadvantages (computational complexity, data requirements) but misses key points from the gold answer like simplicity, OLS estimation, a-theoretic nature, and stationarity issues. Their reasoning is partially aligned but incomplete."}, "llm_echoed_qid": "textbook-119-21-0-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate identifies some advantages (forecasting accuracy) and disadvantages (computational complexity, data requirements) but misses key points from the gold answer like simplicity, OLS estimation, a-theoretic nature, and stationarity issues. Their reasoning is partially aligned but incomplete."}
{"script_processing_qid": "textbook-124-14-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Analyze the role of financial intermediaries in promoting risk sharing. How does this contribute to economic efficiency?\n\nGOLD_ANSWER:\n1. **Risk Sharing**: Intermediaries absorb and redistribute risk, enabling investors to hold preferred risk levels.\n2. **Economic Efficiency**: Facilitates capital allocation to productive uses, reducing systemic risk.\n\nCANDIDATE_ANSWER:\nStep 1: Financial intermediaries promote risk sharing by creating and selling low-risk assets to investors. These assets align with investors’ risk preferences, allowing them to avoid direct exposure to high-risk investments.\n\nStep 2: Intermediaries pool the funds raised from selling these safer assets to invest in riskier portfolios (e.g., loans, equities). This process transforms risky assets into safer ones for investors.\n\nStep 3: Diversification is employed by intermediaries to mitigate risk. By holding a diversified portfolio of assets with uncorrelated returns, intermediaries reduce overall risk while maintaining returns.\n\nStep 4: Risk sharing enhances economic efficiency by (a) enabling safer investment opportunities for risk-averse individuals, increasing capital allocation to productive uses, and (b) lowering systemic risk costs through diversification, improving resource allocation across the economy.\n\nQID: textbook-124-14-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-124-14-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the role of financial intermediaries in risk sharing and its contribution to economic efficiency, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-124-14-1-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the role of financial intermediaries in risk sharing and its contribution to economic efficiency, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-124-14-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the role of financial intermediaries in risk sharing and its contribution to economic efficiency, aligning well with the gold answer."}
{"script_processing_qid": "textbook-73-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Interpret the stylized fact that 'stock markets have delivered average real returns of $5\\%$ or better in almost every country and time period.' What exceptions are noted, and what might explain these exceptions?\n\nGOLD_ANSWER:\n1. **Interpretation**: The stylized fact indicates that stock markets generally provide positive real returns, suggesting long-term growth in equity investments.\n2. **Exceptions**: Italy, Australia, and Canada have average real returns below $5\\%$ in quarterly data.\n3. **Explanations**: \n   - Italy's small market relative to GDP may limit returns.\n   - Australia and Canada's markets are heavily tied to natural resources, which can be volatile and less profitable in certain periods.\n\nCANDIDATE_ANSWER:\nStep 1: The stylized fact indicates that most countries and time periods show average real stock returns ≥5%. This reflects a persistent equity premium across markets globally. Step 2: Exceptions are implied (though not explicitly named) in the international data analysis, likely referring to countries with mean real returns below 5% in Table 2. Step 3: Potential explanations for exceptions include: (1) Smaller market capitalization (Table 1), which may correlate with higher volatility or country-specific risks; (2) Differences in consumption dynamics (Table 3-4), where weaker correlations between consumption growth and returns (sensitive to timing conventions in Table 4) might reduce the equity premium; (3) Unique macroeconomic instability or measurement issues in specific countries not elaborated in the context. The text does not explicitly list which countries underperform, but links deviations to variations in consumption-dividend-return relationships and market structure.\n\nQID: textbook-73-0-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-73-0-0-1\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly interprets the stylized fact and identifies potential explanations for exceptions, but fails to specify the exact countries (Italy, Australia, Canada) mentioned in the gold answer. The reasoning about market capitalization and consumption dynamics is plausible but not directly supported by the provided context.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-73-0-0-1", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly interprets the stylized fact and identifies potential explanations for exceptions, but fails to specify the exact countries (Italy, Australia, Canada) mentioned in the gold answer. The reasoning about market capitalization and consumption dynamics is plausible but not directly supported by the provided context."}, "llm_echoed_qid": "textbook-73-0-0-1", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly interprets the stylized fact and identifies potential explanations for exceptions, but fails to specify the exact countries (Italy, Australia, Canada) mentioned in the gold answer. The reasoning about market capitalization and consumption dynamics is plausible but not directly supported by the provided context."}
{"script_processing_qid": "textbook-114-2-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the relationship between the t-statistic and the statistical significance of a regression coefficient.\n\nGOLD_ANSWER:\n1.  **Definition**: The t-statistic is calculated as:\n$$t = \\frac{\\hat{b}}{\\mathrm{SE}(\\hat{b})}$$\n2.  **Critical Value**: For a 95% confidence level, the critical value is 1.96.\n3.  **Interpretation**:\n   - If $|t| < 1.96$, the coefficient is not statistically significant (we cannot reject $H_0: b = 0$).\n   - If $|t| > 1.96$, the coefficient is statistically significant (we reject $H_0: b = 0$).\n\nCANDIDATE_ANSWER:\nStep 1: The t-statistic is computed as \\( t = \\frac{\\hat{b}}{\\textrm{Standard error of }\\hat{b}} \\), where \\( \\hat{b} \\) is the estimated regression coefficient.\\nStep 2: The t-statistic measures how many standard errors the coefficient estimate \\( \\hat{b} \\) is away from 0.\\nStep 3: For a 95% confidence level, if the absolute value of the t-statistic (\\( |t| \\)) is greater than 1.96, the 95% confidence interval (\\( \\hat{b} \\pm 1.96 \\times \\textrm{standard error} \\)) does not include 0.\\nStep 4: If \\( |t| > 1.96 \\), we reject the null hypothesis (\\( b = 0 \\)) and conclude the coefficient is statistically significant. If \\( |t| < 1.96 \\), the confidence interval includes 0, and the coefficient is not statistically significant.\n\nQID: textbook-114-2-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-2-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the relationship between the t-statistic and the statistical significance of a regression coefficient, including the calculation of the t-statistic, the critical value for a 95% confidence level, and the interpretation of the t-statistic in terms of rejecting or failing to reject the null hypothesis.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-2-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains the relationship between the t-statistic and the statistical significance of a regression coefficient, including the calculation of the t-statistic, the critical value for a 95% confidence level, and the interpretation of the t-statistic in terms of rejecting or failing to reject the null hypothesis."}, "llm_echoed_qid": "textbook-114-2-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the relationship between the t-statistic and the statistical significance of a regression coefficient, including the calculation of the t-statistic, the critical value for a 95% confidence level, and the interpretation of the t-statistic in terms of rejecting or failing to reject the null hypothesis."}
{"script_processing_qid": "textbook-100-3-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the significance of the Diaconis-Ylvisaker result regarding linear posterior expectations in conjugate families.\n\nGOLD_ANSWER:\nThe Diaconis-Ylvisaker result identifies the unique conjugate family for exponential families where the posterior mean is linear in the data. This property simplifies Bayesian updating and interpretation, as the posterior mean can be expressed as a weighted average of the prior mean and the data.\n\nCANDIDATE_ANSWER:\nStep 1: The Diaconis-Ylvisaker result establishes that in exponential families, only one conjugate prior family simultaneously satisfies two key properties: (1) conjugacy (posterior remains in the prior family) and (2) linear posterior expectations of the form E{E(X|θ)|X=x} = ax + b.\n\nStep 2: This uniqueness is significant because it identifies a special mathematical structure where Bayesian updating simplifies to linear adjustments. Linear posterior expectations allow simple closed-form updates (only requiring parameter updates proportional to observed data x), enhancing computational tractability.\n\nStep 3: The result implies robustness implications: Linear estimators are less sensitive to small perturbations in observations (x) compared to nonlinear forms. This aligns with the section's discussion of Bayesian robustness, as linearity provides inherent stability in posterior estimates.\n\nStep 4: For decision theory applications (e.g., Γ-minimax rules), this unique family offers a natural candidate for prior distributions when seeking computationally feasible and stable solutions under conjugate priors.\n\nQID: textbook-100-3-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-100-3-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the Diaconis-Ylvisaker result, detailing its significance in simplifying Bayesian updating through linear posterior expectations, computational tractability, robustness, and decision theory applications, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-100-3-0-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the Diaconis-Ylvisaker result, detailing its significance in simplifying Bayesian updating through linear posterior expectations, computational tractability, robustness, and decision theory applications, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-100-3-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the Diaconis-Ylvisaker result, detailing its significance in simplifying Bayesian updating through linear posterior expectations, computational tractability, robustness, and decision theory applications, aligning well with the gold answer."}
{"script_processing_qid": "textbook-70-0-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the importance of the Lindeberg-Feller theorem in establishing the asymptotic normality of sample Fourier coefficients.\n\nGOLD_ANSWER:\nThe Lindeberg-Feller theorem is used to verify the conditions under which the sum of centered and scaled random variables (here, the sample Fourier coefficients) converges to a normal distribution. It ensures that the individual variances are negligible relative to the total variance, which is crucial for the asymptotic normality result, especially when the coefficients are not identically distributed or independent.\n\nCANDIDATE_ANSWER:\nStep 1: The Lindeberg-Feller theorem provides general conditions for asymptotic normality in triangular arrays of random variables, which is critical when dealing with dependent or non-identically distributed observations.\\nStep 2: In the context of sample Fourier coefficients, the theorem verifies that the normalized coefficients \\( \\hat{\\phi}_{N,1},\\ldots,\\hat{\\phi}_{N,m} \\) satisfy the necessary conditions (e.g., Lindeberg\\'s condition) to converge in distribution to i.i.d. standard normal variables.\\nStep 3: This asymptotic normality justifies the convergence of the test statistic \\( S \\) to \\( S(Z_1,\\ldots,Z_m) \\), where \\( Z_i \\sim N(0,1) \\), under the null hypothesis.\\nStep 4: Without this result, the distributional foundation for Fourier-based tests (e.g., order selection tests) would lack theoretical validity, making critical values and p-values unreliable.\n\nQID: textbook-70-0-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-70-0-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the role of the Lindeberg-Feller theorem in establishing asymptotic normality for sample Fourier coefficients, including the verification of necessary conditions and the implications for test statistics, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-70-0-1-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains the role of the Lindeberg-Feller theorem in establishing asymptotic normality for sample Fourier coefficients, including the verification of necessary conditions and the implications for test statistics, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-70-0-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the role of the Lindeberg-Feller theorem in establishing asymptotic normality for sample Fourier coefficients, including the verification of necessary conditions and the implications for test statistics, aligning well with the gold answer."}
{"script_processing_qid": "textbook-105-4-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Compare the Hoeffding-Azuma inequality to McDiarmid's inequality (Theorem A.2).\n\nGOLD_ANSWER:\nMcDiarmid's inequality generalizes Hoeffding-Azuma to functions of independent variables with bounded differences. Both use similar proof techniques, but McDiarmid's is more flexible for non-additive functions.\n\nCANDIDATE_ANSWER:\nStep 1: **Setting and Dependency Structure**\nHoeffding-Azuma inequality applies to martingale difference sequences (dependent random variables), while McDiarmid's inequality applies to functions of independent random variables. McDiarmid’s bounded differences condition assumes independence, whereas Hoeffding-Azuma accommodates controlled dependence via martingale structure.\n\nStep 2: **Boundedness Conditions**\n- Hoeffding-Azuma: Requires bounded increments of the martingale differences ($V_i$ lies in an interval of width $c_i$).\n- McDiarmid: Requires bounded differences in the function’s output when any single input variable is changed (similar to Lipschitz continuity across coordinates).\n\nStep 3: **Form of Concentration Bounds**\nBoth yield exponential tail bounds of the form $e^{-2\\epsilon^2 / \\sum c_i^2}$, but Hoeffding-Azuma bounds deviations of the martingale sum, while McDiarmid bounds deviations of a function from its expectation.\n\nStep 4: **Key Distinction**\nHoeffding-Azuma generalizes concentration to dependent settings (martingales), whereas McDiarmid leverages independence but applies to broader functions satisfying bounded differences.\n\nQID: textbook-105-4-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-105-4-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly compares Hoeffding-Azuma and McDiarmid's inequalities, accurately addressing dependency structures, boundedness conditions, form of bounds, and key distinctions, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-105-4-2-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly compares Hoeffding-Azuma and McDiarmid's inequalities, accurately addressing dependency structures, boundedness conditions, form of bounds, and key distinctions, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-105-4-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly compares Hoeffding-Azuma and McDiarmid's inequalities, accurately addressing dependency structures, boundedness conditions, form of bounds, and key distinctions, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-125-13-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the WACC for the railroad industry using the given data: cost of debt = 7.2%, cost of equity = 12.9%, debt ratio = 37.3%, and tax rate = 35%.\n\nGOLD_ANSWER:\n1. **Formula**: WACC = $r_D(1 - T_c)(D/V) + r_E(E/V)$\n2. **Substitute values**: WACC = $0.072(1 - 0.35)(0.373) + 0.129(0.627)$\n3. **Calculate**: WACC = $0.072 \\times 0.65 \\times 0.373 + 0.129 \\times 0.627$\n4. **Result**: WACC = $0.0175 + 0.0809 = 0.0984$ or 9.84%.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the WACC formula\nWACC = (Cost of Debt × (1 - Tax Rate) × Debt Weight) + (Cost of Equity × Equity Weight)\n\nStep 2: Convert percentages to decimals\n- Cost of debt = 7.2% = 0.072\n- Cost of equity = 12.9% = 0.129\n- Debt ratio = 37.3% = 0.373\n- Equity ratio = 62.7% = 0.627\n- Tax rate = 35% = 0.35\n\nStep 3: Calculate after-tax cost of debt\n0.072 × (1 - 0.35) = 0.072 × 0.65 = 0.0468\n\nStep 4: Multiply each component by its weight\nDebt portion: 0.0468 × 0.373 = 0.0175\nEquity portion: 0.129 × 0.627 = 0.0809\n\nStep 5: Sum the components\n0.0175 (debt) + 0.0809 (equity) = 0.0984 ≈ 0.098\n\nFinal WACC = 9.8% (matches the calculation in context)\n\nQID: textbook-125-13-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-13-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derived the WACC using the provided data, matching the gold answer's calculation and result (9.8% vs. 9.84%, a negligible rounding difference).\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-13-0-0", "category": "CORRECT", "explanation": "The candidate correctly derived the WACC using the provided data, matching the gold answer's calculation and result (9.8% vs. 9.84%, a negligible rounding difference)."}, "llm_echoed_qid": "textbook-125-13-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derived the WACC using the provided data, matching the gold answer's calculation and result (9.8% vs. 9.84%, a negligible rounding difference)."}
{"script_processing_qid": "textbook-117-43-3-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Identify and explain the three major risks associated with ARPS investments.\n\nGOLD_ANSWER:\n1. **Credit Risk**: ARPS issuers are often concentrated in specific industries (e.g., banking), increasing exposure to sector-specific downturns.\n2. **Market Liquidity Risk**: The secondary market for ARPS is thin, making it difficult to liquidate positions quickly without price concessions.\n3. **Tax Code Risk**: Changes in tax laws can reduce the dividend exclusion benefit, negatively impacting the attractiveness of ARPS.\n\nCANDIDATE_ANSWER:\nStep 1: Review the provided context to identify explicit risks mentioned. The text states: 'The greatest risk in the ARPS market is derived from (1) the limited number of issuers and their clustering in certain industry groups [...] and (2) the ever-present risk of downgraded ratings.'\n\nStep 2: Analyze the structure of the risks. The first risk combines two related components: (a) limited issuer diversity and (b) industry concentration (e.g., banking). The second risk is explicit: rating downgrades.\n\nStep 3: Note that the context explicitly lists only these two categorized risks. No third risk is explicitly mentioned in the provided excerpts.\n\nConclusion: The provided context identifies two major risks but does not include sufficient information to confirm a third risk. A third risk (e.g., liquidity risk, interest rate risk) may exist but is not discussed in the given text.\n\nQID: textbook-117-43-3-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-43-3-1\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies two risks (credit risk and rating downgrades) but misses the third risk (tax code risk) mentioned in the gold answer. Their reasoning is logical but incomplete based on the gold standard.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-43-3-1", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies two risks (credit risk and rating downgrades) but misses the third risk (tax code risk) mentioned in the gold answer. Their reasoning is logical but incomplete based on the gold standard."}, "llm_echoed_qid": "textbook-117-43-3-1", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies two risks (credit risk and rating downgrades) but misses the third risk (tax code risk) mentioned in the gold answer. Their reasoning is logical but incomplete based on the gold standard."}
{"script_processing_qid": "textbook-39-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the condition under which precautionary saving occurs, starting from the Euler equation under CRRA utility and lognormally distributed consumption. Show how the third derivative of the utility function (prudence) enters the derivation.\n\nGOLD_ANSWER:\n1. **Euler Equation under CRRA Utility**: \n   $$ E_t \\left[ \\beta (1 + R_{t+1}) \\left( \\frac{C_{t+1}}{C_t} \\right)^{-\\gamma} \\right] = 1 $$\n   where $\\gamma$ is the coefficient of relative risk aversion.\n2. **Lognormal Approximation**: Assume $\\ln C_{t+1} - \\ln C_t$ is normally distributed. Taking logs and expanding:\n   $$ \\ln \\beta - \\gamma E_t[\\Delta \\ln C_{t+1}] + \\frac{\\gamma^2}{2} Var_t(\\Delta \\ln C_{t+1}) + E_t[\\ln (1 + R_{t+1})] = 0 $$\n3. **Precautionary Term**: The term $\\frac{\\gamma^2}{2} Var_t(\\Delta \\ln C_{t+1})$ captures the precautionary motive. For CRRA utility, $u'''(C) > 0$ (prudence) implies this term increases saving.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the Euler equation under CRRA utility:\n\n$$u'(c_t) = \\beta (1 + r) E_t[u'(c_{t+1})]$$\n\nFor CRRA utility, $u(c) = \\frac{c^{1-\\gamma}}{1-\\gamma}$, so $u'''(c) = \\gamma(\\gamma + 1)c^{-\\gamma-2} > 0$ (prudence).\n\nStep 2: Take logarithms and use lognormality assumption. Let $\\ln c_{t+1} \\sim N(\\mu, \\sigma^2)$. Using properties of log-normal distributions:\n\n$$E_t[c_{t+1}^{-\\gamma}] = e^{-\\gamma \\mu + \\frac{1}{2}\\gamma^2\\sigma^2}$$\n\nStep 3: Substitute into Euler equation and log-linearize:\n\n$$-\\gamma \\ln c_t = \\ln[\\beta(1 + r)] - \\gamma E_t[\\ln c_{t+1}] + \\frac{1}{2}\\gamma^2\\sigma^2$$\n\nStep 4: Rearrange to show consumption growth:\n\n$$E_t[\\Delta \\ln c_{t+1}] = \\frac{1}{\\gamma}\\ln[\\beta(1 + r)] + \\frac{1}{2}\\gamma\\sigma^2$$\n\nThe $\\frac{1}{2}\\gamma\\sigma^2$ term represents precautionary saving. This term exists because:\n1. $u'''(c) > 0$ (prudence) appears via $\\gamma$ in the precautionary term\n2. Higher variance $\\sigma^2$ increases expected consumption growth\n3. This requires reducing current consumption (increasing saving) to smooth marginal utilities\n\nConclusion: Precautionary saving occurs when $u'''(c) > 0$. The third derivative enters explicitly through $\\gamma$ in the variance term derived from the log-normal approximation.\n\nQID: textbook-39-1-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-39-1-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the condition for precautionary saving from the Euler equation under CRRA utility and lognormally distributed consumption. It accurately shows how the third derivative of the utility function (prudence) enters the derivation through the term involving γ in the precautionary saving component.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-39-1-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the condition for precautionary saving from the Euler equation under CRRA utility and lognormally distributed consumption. It accurately shows how the third derivative of the utility function (prudence) enters the derivation through the term involving γ in the precautionary saving component."}, "llm_echoed_qid": "textbook-39-1-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the condition for precautionary saving from the Euler equation under CRRA utility and lognormally distributed consumption. It accurately shows how the third derivative of the utility function (prudence) enters the derivation through the term involving γ in the precautionary saving component."}
{"script_processing_qid": "textbook-54-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. For an AR(2) process $(1-\\phi_{1}L-\\phi_{2}L^{2})x_{t}=\\epsilon_{t}$, derive the MA(∞) representation using the partial fractions method.\n\nGOLD_ANSWER:\n1. **Factor the lag polynomial**: $(1-\\phi_{1}L-\\phi_{2}L^{2})=(1-\\lambda_{1}L)(1-\\lambda_{2}L)$, where $\\lambda_{1}+\\lambda_{2}=\\phi_{1}$ and $\\lambda_{1}\\lambda_{2}=-\\phi_{2}$.\n2. **Partial fractions**: $\\frac{1}{(1-\\lambda_{1}L)(1-\\lambda_{2}L)}=\\frac{a}{1-\\lambda_{1}L}+\\frac{b}{1-\\lambda_{2}L}$, where $a+b=1$ and $\\lambda_{2}a+\\lambda_{1}b=0$.\n3. **Solve for $a$ and $b$**: $a=\\frac{\\lambda_{1}}{\\lambda_{1}-\\lambda_{2}}$, $b=\\frac{\\lambda_{2}}{\\lambda_{2}-\\lambda_{1}}$.\n4. **Invert each term**: $x_{t}=\\left(\\frac{\\lambda_{1}}{\\lambda_{1}-\\lambda_{2}}\\sum_{j=0}^{\\infty}\\lambda_{1}^{j}L^{j}+\\frac{\\lambda_{2}}{\\lambda_{2}-\\lambda_{1}}\\sum_{j=0}^{\\infty}\\lambda_{2}^{j}L^{j}\\right)\\epsilon_{t}$.\n5. **Final form**: $x_{t}=\\sum_{j=0}^{\\infty}\\left(\\frac{\\lambda_{1}^{j+1}-\\lambda_{2}^{j+1}}{\\lambda_{1}-\\lambda_{2}}\\right)\\epsilon_{t-j}$.\n\nCANDIDATE_ANSWER:\nStep 1: Factor the AR(2) lag polynomial into roots λ₁ and λ₂ such that (1 - φ₁L - φ₂L²) = (1 - λ₁L)(1 - λ₂L). The roots satisfy λ₁ + λ₂ = φ₁ and λ₁λ₂ = -φ₂.\n\nStep 2: Rewrite the AR(2) process as (1 - λ₁L)(1 - λ₂L)xₜ = εₜ. To invert, use partial fractions:\n$$\\frac{1}{(1 - \\lambda_1 L)(1 - \\lambda_2 L)} = \\frac{A}{1 - \\lambda_1 L} + \\frac{B}{1 - \\lambda_2 L}.$$\n\nStep 3: Solve for A and B by equating coefficients:\n$$1 = A(1 - \\lambda_2 L) + B(1 - \\lambda_1 L).$$\nMatching terms:\n- Constant term: A + B = 1\n- L-term: -Aλ₂ - Bλ₁ = 0\nThis gives:\n$$A = \\frac{\\lambda_1}{\\lambda_1 - \\lambda_2}, \\quad B = \\frac{-\\lambda_2}{\\lambda_1 - \\lambda_2}.$$\n\nStep 4: Expand each geometric series:\n$$x_t = \\left(\\frac{\\lambda_1}{\\lambda_1 - \\lambda_2} \\sum_{j=0}^\\infty \\lambda_1^j L^j - \\frac{\\lambda_2}{\\lambda_1 - \\lambda_2} \\sum_{j=0}^\\infty \\lambda_2^j L^j\\right)\\epsilon_t.$$\n\nStep 5: Combine terms to get the MA(∞) representation:\n$$x_t = \\sum_{j=0}^\\infty \\left(\\frac{\\lambda_1^{j+1} - \\lambda_2^{j+1}}{\\lambda_1 - \\lambda_2}\\right)\\epsilon_{t-j}.$$\nThe MA coefficients are ψ_j = (λ₁^{j+1} - λ₂^{j+1}) / (λ₁ - λ₂) for j ≥ 0.\n\nQID: textbook-54-0-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-54-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows all steps of the partial fractions method to derive the MA(∞) representation of the AR(2) process, matching the gold answer in both reasoning and final form.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-54-0-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows all steps of the partial fractions method to derive the MA(∞) representation of the AR(2) process, matching the gold answer in both reasoning and final form."}, "llm_echoed_qid": "textbook-54-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows all steps of the partial fractions method to derive the MA(∞) representation of the AR(2) process, matching the gold answer in both reasoning and final form."}
{"script_processing_qid": "textbook-105-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the Kaplan-Meier estimator $F_{n}(t)$ step-by-step, starting from the definition of the survival function $F(t) = \\mathbf{P}\\{Y > t\\}$ and considering censored observations.\n\nGOLD_ANSWER:\n1. **Define the Survival Function**: $F(t) = \\mathbf{P}\\{Y > t\\}$.\n2. **Observed Data**: We observe $(Z_{i}, \\delta_{i})$, where $Z_{i} = \\min(Y_{i}, C_{i})$ and $\\delta_{i} = I_{\\{Y_{i} \\leq C_{i}\\}}$.\n3. **Product-Limit Estimator**: For uncensored observations ($\\delta_{(i)} = 1$), the probability of survival beyond $Z_{(i)}$ is adjusted by the factor $\\frac{n-i}{n-i+1}$.\n4. **Final Form**: The Kaplan-Meier estimator is:\n$$\nF_{n}(t) = \\prod_{i:Z_{(i)} \\leq t} \\left(\\frac{n-i}{n-i+1}\\right)^{\\delta_{(i)}} \\quad \\text{if } t \\leq Z_{(n)}, \\text{ otherwise } 0.\n$$\n\nCANDIDATE_ANSWER:\nStep 1: Define ordered observations. Let $(Z_{(1)},\\delta_{(1)}),...,(Z_{(n)},\\delta_{(n)})$ be the ordered $Z_i$ values (ascending).\n\nStep 2: Initialize survival probability. At $t=0$, $F_n(0) = 1$ since all subjects are alive.\n\nStep 3: Compute risk set. At each ordered time $Z_{(i)}$, the number of subjects at risk is $(n-i+1)$ (remaining observations ≥ $Z_{(i)}$).\n\nStep 4: Conditional survival probability. For uncensored events ($\\delta_{(i)}=1$), the probability of surviving past $Z_{(i)}$ given survival up to $Z_{(i)}$ is $(\\frac{n-i}{n-i+1})$ (1 event / remaining subjects). Censored observations ($\\delta_{(i)}=0$) contribute no immediate change.\n\nStep 5: Product-limit formulation. Multiply all conditional survival probabilities up to time $t$:\n$$F_n(t) = \\prod_{i:Z_{(i)}\\leq t} \\left(1 - \\frac{\\delta_{(i)}}{n-i+1}\\right) = \\prod_{i:Z_{(i)}\\leq t} \\left(\\frac{n-i}{n-i+1}\\right)^{\\delta_{(i)}}$$\n\nStep 6: Handle censoring. Efron's redistribution ensures censored subjects' probability mass remains in the risk set until subsequent failures occur, justifying the exponent $\\delta_{(i)}$ (failure events only modify the product).\n\nStep 7: Truncation rule. For $t > Z_{(n)}$, survival is set to 0 if $Z_{(n)}$ is a failure ($\\delta_{(n)}=1$) or undefined (no failures after last observation). The given formula simplifies this to 0 regardless.\n\nQID: textbook-105-1-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-105-1-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the Kaplan-Meier estimator step-by-step, aligning with the gold answer. The steps include defining ordered observations, initializing survival probability, computing the risk set, calculating conditional survival probabilities, and applying the product-limit formulation. The candidate also correctly handles censoring and truncation, matching the gold answer's final form.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-105-1-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the Kaplan-Meier estimator step-by-step, aligning with the gold answer. The steps include defining ordered observations, initializing survival probability, computing the risk set, calculating conditional survival probabilities, and applying the product-limit formulation. The candidate also correctly handles censoring and truncation, matching the gold answer's final form."}, "llm_echoed_qid": "textbook-105-1-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the Kaplan-Meier estimator step-by-step, aligning with the gold answer. The steps include defining ordered observations, initializing survival probability, computing the risk set, calculating conditional survival probabilities, and applying the product-limit formulation. The candidate also correctly handles censoring and truncation, matching the gold answer's final form."}
{"script_processing_qid": "textbook-125-45-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. For Backwoods Chemical, if asset value is $100, debt is $60, asset volatility is 30%, and risk-free rate is 5%, calculate the 5-year probability of default.\n\nGOLD_ANSWER:\n1. **Inputs**: V₀ = $100, D = $60, σ = 0.30, r = 0.05, T = 5.\n2. **d₂**: [ln(100/60) + (0.05 - 0.30²/2)*5] / (0.30√5) ≈ 1.12.\n3. **P(default)**: N(-1.12) ≈ 13.1%.\n4. **Interpretation**: There is a 13.1% chance assets will fall below $60 in 5 years.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key parameters from the context.\nAsset value = $100, Debt threshold = $60\n\nStep 2: Review the direct statement in the context:\n'There is a probability of 20 percent that the asset value could fall below $60, in which case the company will default on its debt.'\n\nStep 3: Recognize this explicitly states the default probability as 20% without requiring calculation based on volatility or risk-free rate parameters mentioned in the question.\n\nConclusion: The 5-year probability of default is directly given as 20% in the provided context.\n\nQID: textbook-125-45-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-45-1-1\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer incorrectly states the default probability as 20%, which is not supported by the provided context or calculations. The correct probability, as calculated in the gold answer, is 13.1%.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-45-1-1", "category": "INCORRECT", "explanation": "The candidate's answer incorrectly states the default probability as 20%, which is not supported by the provided context or calculations. The correct probability, as calculated in the gold answer, is 13.1%."}, "llm_echoed_qid": "textbook-125-45-1-1", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer incorrectly states the default probability as 20%, which is not supported by the provided context or calculations. The correct probability, as calculated in the gold answer, is 13.1%."}
{"script_processing_qid": "textbook-47-0-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the implications of global stability results for SRAs in economic models.\n\nGOLD_ANSWER:\n1.  **Global Stability**: Ensures that learning dynamics converge to an REE from any initial condition. \n2.  **Implications**: Provides robustness to the learning process, indicating that agents will eventually learn the correct model regardless of initial beliefs.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that global stability results for SRAs ensure convergence to Rational Expectations Equilibria (REE) from any initial condition in the state space. This implies economic models with this property will reliably reach equilibrium even after large shocks or deviations.<br><br>Step 2: The linkage to the expectational stability (E-stability) principle suggests that agents' learning rules will coordinate on REE across diverse initial beliefs. This validates the economic relevance of E-stability as a general criterion for equilibrium selection.<br><br>Step 3: From a policy perspective, global stability implies self-correcting mechanisms in markets - temporary interventions may have limited long-term effects if systems naturally return to equilibrium through learning.<br><br>Step 4: Methodologically, these results justify using stochastic approximation techniques (via associated differential equations) as robust tools for analyzing learning dynamics in macroeconomic and financial models.<br><br>Step 5: The findings support the validity of adaptive learning frameworks in economics, as globally stable SRAs demonstrate that boundedly rational agents can eventually achieve full rationality through experience.\n\nQID: textbook-47-0-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-47-0-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the implications of global stability results for SRAs, aligning with the gold answer's points about convergence and robustness, while also expanding on additional economic and methodological implications.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-47-0-1-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the implications of global stability results for SRAs, aligning with the gold answer's points about convergence and robustness, while also expanding on additional economic and methodological implications."}, "llm_echoed_qid": "textbook-47-0-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the implications of global stability results for SRAs, aligning with the gold answer's points about convergence and robustness, while also expanding on additional economic and methodological implications."}
{"script_processing_qid": "textbook-70-2-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the asymptotic distributions of Gasser-Miller and truncated series estimators under Theorem 3.6.\n\nGOLD_ANSWER:\nBoth estimators satisfy $Z\\_{n} \\xrightarrow{d} N(0,1)$ under the theorem's conditions. Key differences:\n1. **Weight Structure**: Gasser-Miller uses local averaging weights, while truncated series uses orthogonal basis coefficients.\n2. **Bias Control**: Truncated series requires careful choice of basis functions to ensure $B\\_{n} = o_p(1)$.\n3. **Variance**: Gasser-Miller's variance depends on kernel bandwidth $h_n$, while truncated series depends on the number of terms $m_n$.\n\nCANDIDATE_ANSWER:\nStep 1: Both estimators follow the general framework in Theorem 3.6 where (n h_n)^{3/2}∑|w_in|³E|ε_in|³ → 0 is required for asymptotic normality.\n\nStep 2: For Gasser-Miller (kernel-type estimator):\n- Weights w_in ~ O(1/(n h_n))\n- Sum ∑|w_in|³ ~ O(n/(n h_n)^3) = O(1/(n² h_n³))\n- Critical term becomes (n h_n)^{3/2}/(n² h_n³) = 1/(n^{1/2} h_n^{3/2}) → 0 requires h_n not decaying faster than n^{-1/3}\n\nStep 3: For truncated series estimators:\n- Weights w_in = Σ_{j=1}^{m_n} φ_j(x_i)φ_j(x)/n\n- Under bounded basis functions (|φ_j| ≤ C), ∑|w_in|³ ~ O(m_n/n³)\n- Critical term becomes (n h_n)^{3/2}(m_n/n³) → 0 requires m_n = o(n^{3/2}/(h_n)^{3/2})\n\nStep 4: Both estimators achieve Z_n → N(0,1) under their respective conditions when B_n → 0\n\nFinal Comparison: Both estimators have standard normal asymptotic distributions under Theorem 3.6, but differ in their specific requirements:\n- Gasser-Miller requires bandwidth h_n decay rate control\n- Truncated series requires truncation parameter m_n growth rate control\n\nQID: textbook-70-2-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-70-2-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies that both estimators achieve standard normal asymptotic distributions under Theorem 3.6, and provides detailed reasoning about the specific conditions required for each estimator (bandwidth decay rate for Gasser-Miller and truncation parameter growth rate for truncated series). This aligns well with the gold answer, which also highlights these key differences.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-70-2-0-3", "category": "CORRECT", "explanation": "The candidate answer correctly identifies that both estimators achieve standard normal asymptotic distributions under Theorem 3.6, and provides detailed reasoning about the specific conditions required for each estimator (bandwidth decay rate for Gasser-Miller and truncation parameter growth rate for truncated series). This aligns well with the gold answer, which also highlights these key differences."}, "llm_echoed_qid": "textbook-70-2-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies that both estimators achieve standard normal asymptotic distributions under Theorem 3.6, and provides detailed reasoning about the specific conditions required for each estimator (bandwidth decay rate for Gasser-Miller and truncation parameter growth rate for truncated series). This aligns well with the gold answer, which also highlights these key differences."}
{"script_processing_qid": "textbook-70-0-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of Theorem 7.1 for the approximation of the distribution of sample Fourier coefficients.\n\nGOLD_ANSWER:\nTheorem 7.1 provides a Berry-Esséen type bound on the approximation error when using the normal distribution for the sample Fourier coefficients. It shows that the error decreases as $n$ increases, but the rate depends on the number of coefficients $m$ and the fourth moment of the errors. This is crucial for justifying the use of normal approximations in practice, especially when $m$ grows with $n$.\n\nCANDIDATE_ANSWER:\nStep 1: Theorem 7.1 establishes that under the null hypothesis (7.1), when using normalized Fourier coefficients $\\hat{\\phi}_{N,i} = \\frac{\\sqrt{2n}\\hat{\\phi}_i}{\\hat{\\sigma}}$ with a consistent $\\hat{\\sigma}$ and fixed $m$, the test statistic $S(\\hat{\\phi}_{N,1},\\ldots,\\hat{\\phi}_{N,m})$ converges in distribution to $S(Z_1,\\ldots,Z_m)$, where $Z_i \\stackrel{iid}{\\sim} N(0,1)$.\\n\\nStep 2: This implies that for large $n$, the joint distribution of $\\hat{\\phi}_{N,1},\\ldots,\\hat{\\phi}_{N,m}$ can be approximated by independent standard normals. The normalization by $\\sqrt{2n}/\\hat{\\sigma}$ removes dependence on the original scale and nuisance parameters.\\n\\nStep 3: Practical implications include:\\n- Critical values for hypothesis tests (e.g., order selection tests) can be computed using standard normal quantiles\\n- Test statistics like maximum modulus or quadratic forms of $\\hat{\\phi}_{N,i}$ inherit known asymptotic distributions\\n- Inference becomes parameter-free in the limit, simplifying implementation\\n\\nStep 4: Key limitations arise from the theorem's conditions:\\n- Requires fixed $m$ (does not address growing $m$ scenarios)\\n- Depends on continuity of $S$ and weak consistency of $\\hat{\\sigma}$\\n- Approximation quality depends on both $n$ and the choice of $m$ in finite samples.\n\nQID: textbook-70-0-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-70-0-1-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the implications of Theorem 7.1 for the approximation of the distribution of sample Fourier coefficients, including the convergence to standard normals and practical implications for hypothesis testing. However, the reasoning focuses more on the convergence of test statistics and normalization, whereas the gold answer emphasizes the Berry-Esséen type bound and the rate of error decrease, which are not explicitly addressed in the candidate's response.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-70-0-1-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the implications of Theorem 7.1 for the approximation of the distribution of sample Fourier coefficients, including the convergence to standard normals and practical implications for hypothesis testing. However, the reasoning focuses more on the convergence of test statistics and normalization, whereas the gold answer emphasizes the Berry-Esséen type bound and the rate of error decrease, which are not explicitly addressed in the candidate's response."}, "llm_echoed_qid": "textbook-70-0-1-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the implications of Theorem 7.1 for the approximation of the distribution of sample Fourier coefficients, including the convergence to standard normals and practical implications for hypothesis testing. However, the reasoning focuses more on the convergence of test statistics and normalization, whereas the gold answer emphasizes the Berry-Esséen type bound and the rate of error decrease, which are not explicitly addressed in the candidate's response."}
{"script_processing_qid": "textbook-125-24-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the limitations of using a single risk-adjusted discount rate for projects where risk does not change at a constant rate over time. Provide an example from the text.\n\nGOLD_ANSWER:\n1. **Limitation**: A single rate assumes risk accumulates linearly, which may not hold for projects with non-constant risk profiles.\\n2. **Example**: The Vegetron electric mop project has high initial risk (50% chance of failure) but normal risk thereafter if successful. Using a constant 25% rate overstates the risk post-success.\\n3. **Solution**: Break the project into phases or use certainty equivalents to adjust for changing risk.\\n4. **Implication**: Misapplication can lead to incorrect NPV estimates and suboptimal investment decisions.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the limitation of a single risk-adjusted discount rate\nA single risk-adjusted discount rate assumes constant risk exposure over time. However, when risk increases cumulatively (e.g., multi-year projects with compounding market risk exposure), this method becomes inadequate because it doesn't account for varying risk levels across periods.\n\nStep 2: Example from the text (Project A)\nThe text demonstrates this limitation through Project A, which has three \\$100M annual cash flows. Though the third cash flow is riskier than the first (due to three years of market risk exposure vs. one year), all cash flows are discounted at the same 12% rate derived from beta and CAPM. This fails to reflect the increasing risk profile over time.\n\nStep 3: Implications\nThe text shows that certainty equivalents \"steadily decline\" for later cash flows, indicating that risk increases with time. A single rate cannot properly adjust for this escalating risk, leading to potential misvaluation compared to the certainty-equivalent method (which explicitly adjusts each cash flow individually before risk-free discounting).\n\nQID: textbook-125-24-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-24-0-3\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer discusses a different example (Project A) and reasoning about cumulative risk, which does not match the GOLD_ANSWER's focus on non-linear risk changes (Vegetron project). The core limitation and example are misaligned.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-24-0-3", "category": "INCORRECT", "explanation": "The candidate's answer discusses a different example (Project A) and reasoning about cumulative risk, which does not match the GOLD_ANSWER's focus on non-linear risk changes (Vegetron project). The core limitation and example are misaligned."}, "llm_echoed_qid": "textbook-125-24-0-3", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer discusses a different example (Project A) and reasoning about cumulative risk, which does not match the GOLD_ANSWER's focus on non-linear risk changes (Vegetron project). The core limitation and example are misaligned."}
{"script_processing_qid": "textbook-99-8-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the conditions under which the nonparametric estimator of the seasonal component performs better than the conventional sample mean estimate.\n\nGOLD_ANSWER:\n2. **Performance Conditions**: The nonparametric estimator performs better when the period T of the seasonal component is large relative to the sample size n. This is because the nonparametric estimator smooths the conventional estimates, reducing variability, especially when T is large and the sample mean estimate becomes noisy.\n\nCANDIDATE_ANSWER:\nStep 1: The nonparametric estimator outperforms the conventional sample mean when the period (T) is large relative to the sample size (n). In the example provided (T=20, n=100), the period constitutes 20% of the total observations, making smoothing beneficial.\n\nStep 2: The nonparametric estimator performs better when the true seasonal component is smooth or lacks abrupt changes. Smoothing improves precision by borrowing information across adjacent time points within the period.\n\nStep 3: The conventional sample mean performs better when the seasonal component has sharp, localized features (e.g., Delta functions). Smoothing in the nonparametric estimator may oversmooth such features, causing bias.\n\nSummary: The nonparametric estimator is preferable under large T/n ratios and smooth seasonal patterns, while the conventional method works better for small periods or components with sharp features.\n\nQID: textbook-99-8-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-8-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the conditions under which the nonparametric estimator performs better (large T/n ratio and smooth seasonal patterns) and contrasts it with scenarios where the conventional sample mean is preferable (small periods or sharp features), aligning with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-8-0-1", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the conditions under which the nonparametric estimator performs better (large T/n ratio and smooth seasonal patterns) and contrasts it with scenarios where the conventional sample mean is preferable (small periods or sharp features), aligning with the gold answer."}, "llm_echoed_qid": "textbook-99-8-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the conditions under which the nonparametric estimator performs better (large T/n ratio and smooth seasonal patterns) and contrasts it with scenarios where the conventional sample mean is preferable (small periods or sharp features), aligning with the gold answer."}
{"script_processing_qid": "textbook-125-13-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain why the opportunity cost of capital ($r$) is the natural reference point for WACC and is independent of leverage in a no-tax world.\n\nGOLD_ANSWER:\nThe opportunity cost of capital ($r$) is the natural reference point because:\n1. **Business Risk**: $r$ reflects only the business risk of the project, unaffected by financing.\n2. **MM Proposition I**: In a no-tax world, Modigliani and Miller show that $r$ is constant across all debt ratios, as the benefits of debt are offset by increased cost of equity.\n3. **Leverage Neutrality**: Without taxes, WACC equals $r$ regardless of leverage, making it the benchmark for project evaluation.\n\nCANDIDATE_ANSWER:\nStep 1: In a no-tax world, Modigliani-Miller Proposition I establishes that firm value is independent of capital structure. This implies the weighted-average cost of capital (WACC) must equal the opportunity cost of capital (r) regardless of debt ratio.\n\nStep 2: The formula for opportunity cost of capital (r = r_D·D/V + r_E·E/V) represents the required return for investors given pure business risk. This risk profile remains unchanged by leverage because there are no tax shields or bankruptcy costs to alter enterprise value.\n\nStep 3: Leverage changes only redistribute risk between debt and equity holders. As debt increases, the cost of equity (r_E) rises according to r_E = r + (r - r_D)·D/E to compensate equity holders for higher financial risk. However, the weighted average of r_D and r_E (WACC) always equals r because the additional equity risk premium exactly offsets the lower cost of debt.\n\nStep 4: Mathematical proof using Step 2's formula:\nWACC = r_D·(D/V) + [r + (r - r_D)·D/E]·(E/V)\n= r_D·(D/V) + r·(E/V) + r·(D/V) - r_D·(D/V)\n= r·(E/V + D/V) = r\n\nThus, r serves as the natural reference point because it represents the fundamental return requirement for the firm's assets, unaffected by financing mix in a no-tax environment.\n\nQID: textbook-125-13-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-13-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the concept, correctly referencing Modigliani-Miller Proposition I, the neutrality of leverage in a no-tax world, and providing a mathematical proof that WACC equals the opportunity cost of capital (r). The reasoning aligns perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-13-2-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the concept, correctly referencing Modigliani-Miller Proposition I, the neutrality of leverage in a no-tax world, and providing a mathematical proof that WACC equals the opportunity cost of capital (r). The reasoning aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-125-13-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the concept, correctly referencing Modigliani-Miller Proposition I, the neutrality of leverage in a no-tax world, and providing a mathematical proof that WACC equals the opportunity cost of capital (r). The reasoning aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-105-6-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the role of Breiman's generalized ergodic theorem (Lemma 27.2) in proving the consistency of the static forecasting estimator.\n\nGOLD_ANSWER:\n1.  **Breiman's Theorem**: States that for a stationary ergodic process $\\{Z\\_{i}\\}$ and functions $f\\_{i}$ converging to $f$, $\\frac{1}{n} \\sum\\_{i=1}^{n} f\\_{i}(T^{i}\\mathbf{Z}) \\to \\mathbf{E}f(\\mathbf{Z})$ almost surely.\n2.  **Application to Static Forecasting**: The estimator $\\tilde{m}\\_{k}$ can be viewed as an average of functions $f\\_{i}$ applied to shifted versions of the process. Breiman's theorem ensures convergence to the conditional expectation, provided the functions $f\\_{i}$ satisfy the theorem's conditions.\n\nCANDIDATE_ANSWER:\nStep 1: In static forecasting, we aim to estimate ${\\bf E}\\{Y_{0}|Y_{-\\infty}^{-1}\\}$ using finite historical data $Y_{-n}^{-1}$. \nStep 2: Breiman's generalized ergodic theorem (Lemma 27.2) allows us to handle conditional expectations over growing sigma-algebras as $n\\to\\infty$, which is essential when approximating $Y_{-\\infty}^{-1}$ with $Y_{-n}^{-1}$.\nStep 3: The theorem ensures that time averages of functions of the process converge almost surely to their expected values under stationarity and ergodicity.\nStep 4: This property is used to show $\\hat{E}(Y_{-n}^{-1})$ converges a.s. to ${\\bf E}\\{Y_{0}|Y_{-\\infty}^{-1}\\}$ by establishing that the finite-window estimator becomes asymptotically equivalent to the infinite-past conditional expectation.\n\nQID: textbook-105-6-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-105-6-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately describes the role of Breiman's generalized ergodic theorem in proving the consistency of the static forecasting estimator, aligning with the gold answer's explanation of convergence to the conditional expectation under stationarity and ergodicity.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-105-6-1-3", "category": "CORRECT", "explanation": "The candidate answer accurately describes the role of Breiman's generalized ergodic theorem in proving the consistency of the static forecasting estimator, aligning with the gold answer's explanation of convergence to the conditional expectation under stationarity and ergodicity."}, "llm_echoed_qid": "textbook-105-6-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately describes the role of Breiman's generalized ergodic theorem in proving the consistency of the static forecasting estimator, aligning with the gold answer's explanation of convergence to the conditional expectation under stationarity and ergodicity."}
{"script_processing_qid": "textbook-111-19-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Contrast the effects of a tax increase ($\\Delta T$) and a government spending increase ($\\Delta G$) of the same magnitude on equilibrium income in the Keynesian cross model. Why do they differ?\n\nGOLD_ANSWER:\nA $\\Delta G$ increase directly raises expenditure by $\\Delta G$, while a $\\Delta T$ reduces disposable income by $\\Delta T$, lowering consumption by $MPC \\times \\Delta T$. The government-purchases multiplier ($\\frac{1}{1-MPC}$) is larger in absolute value than the tax multiplier ($\\frac{-MPC}{1-MPC}$) because spending changes have a direct first-round effect on income, whereas tax changes only indirectly affect expenditure via consumption.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the Keynesian cross equilibrium income formula. Equilibrium income (Y) is determined by Y = C + I + G, where C = C₀ + MPC(Y - T).\n\nStep 2: Analyze the government spending multiplier. A $ΔG$ directly increases aggregate expenditure by $ΔG$, triggering a multiplier effect. The total change in income is ΔY = ΔG × [1/(1 - MPC)] because each round of spending increases income and consumption iteratively.\n\nStep 3: Analyze the tax multiplier. A $ΔT$ reduces disposable income by $ΔT$, lowering consumption by MPC × ΔT. This initial reduction in consumption propagates through the multiplier process. The total change in income is ΔY = -ΔT × [MPC/(1 - MPC)].\n\nStep 4: Contrast magnitudes. For equal ΔG and ΔT, the spending multiplier [1/(1 - MPC)] is larger than the tax multiplier [MPC/(1 - MPC)] because ΔG directly increases aggregate expenditure by the full amount, while ΔT only indirectly affects expenditure through MPC-adjusted consumption changes.\n\nStep 5: Conclusion. A $ΔG$ increases equilibrium income more than an equal $ΔT$ reduces it because government spending has a direct first-round effect on expenditure, whereas taxes only influence expenditure indirectly via disposable income and consumption.\n\nQID: textbook-111-19-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-111-19-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the Keynesian cross model, correctly contrasts the effects of a tax increase and a government spending increase, and accurately identifies the reasons for the difference in their impacts on equilibrium income. The reasoning aligns perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-19-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the Keynesian cross model, correctly contrasts the effects of a tax increase and a government spending increase, and accurately identifies the reasons for the difference in their impacts on equilibrium income. The reasoning aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-111-19-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the Keynesian cross model, correctly contrasts the effects of a tax increase and a government spending increase, and accurately identifies the reasons for the difference in their impacts on equilibrium income. The reasoning aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-87-1-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. How does the HLT model account for wage inequality among individuals with the same schooling level?\n\nGOLD_ANSWER:\nThe HLT model generates wage inequality through three primary channels:\n1.  **Heterogeneity in Initial Human Capital**: Individuals of the same schooling level may start with different stocks of human capital $H^{S}(\\theta)$.\n2.  **Heterogeneity in Learning Ability**: The parameter $A^{S}(\\theta)$ captures differences in the ability to produce job-specific human capital, leading to divergent investment paths.\n3.  **Skill Prices**: Skills produced at different schooling levels command different prices $R_{t}^{S}$, and wage inequality arises from differences in skill levels, investment, and the prices of alternative skill bundles.\n\nCANDIDATE_ANSWER:\nStep 1: The HLT model introduces heterogeneity in two key dimensions among individuals with the same schooling level: initial stocks of human capital (H_{a,t}^S) and ability to produce job-specific human capital (captured by θ in A^S(θ)).\n\nStep 2: The human capital production function (H_{a+1,t+1}^S = A^S(θ)(I_{a,t}^S)^{α_s}(H_{a,t}^S)^{β_s} + (1-σ^S)H_{a,t}^S) implies that wage inequality arises from:\n   - Differences in initial human capital (H_{a,t}^S)\n   - Variation in ability θ (affecting productivity A^S(θ))\n   - Divergent post-schooling investment levels (I_{a,t}^S)\n\nStep 3: Rental prices for skills are determined in general equilibrium through skill-specific market prices. Even with identical schooling, individuals develop different skill bundles due to the above factors, which are valued differently in the labor market.\n\nStep 4: The model allows for additional variation through labor-leisure choices (L_{a,t}) that affect work hours (h_{a,t}), though this is secondary to the human capital heterogeneity mechanism.\n\nQID: textbook-87-1-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-87-1-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately captures all three channels of wage inequality in the HLT model (initial human capital, learning ability, and skill prices) and provides additional detail on the mechanisms, matching the gold answer's core points while expanding on them.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-87-1-0-2", "category": "CORRECT", "explanation": "The candidate answer accurately captures all three channels of wage inequality in the HLT model (initial human capital, learning ability, and skill prices) and provides additional detail on the mechanisms, matching the gold answer's core points while expanding on them."}, "llm_echoed_qid": "textbook-87-1-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately captures all three channels of wage inequality in the HLT model (initial human capital, learning ability, and skill prices) and provides additional detail on the mechanisms, matching the gold answer's core points while expanding on them."}
{"script_processing_qid": "textbook-99-7-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. What are the challenges in recognizing the underlying regression function from scatter plots in heteroscedastic settings, as illustrated in Figure 4.4?\n\nGOLD_ANSWER:\nIn heteroscedastic settings, the variability of responses changes with predictors, making it difficult to visually identify the regression function. Key challenges include:\n1. **Skewed Design Density**: Predictors may be unevenly distributed, e.g., skewed to one end of the interval, leading to sparse data in certain regions.\n2. **Varying Spread**: The scale function $\\sigma(x)$ causes responses to be more spread out in some regions, obscuring the true trend.\n3. **Complex Interactions**: The interplay between design density, scale function, and error distribution complicates manual fitting and visual interpretation.\n\nCANDIDATE_ANSWER:\nStep 1: Heteroscedasticity introduces non-constant variance across predictor values (σ(Xₗ)), causing uneven data dispersion in scatter plots. This makes it harder to visually discern the true regression function f(x) compared to homoscedastic settings.<br><br>Step 2: The estimator for Fourier coefficients (θ̃ⱼ) relies on local averaging across intervals [X₍ₗ₋ₛ₎, X₍ₗ₊ₛ₎]. When σ(x) varies, regions with larger σ(x) dominate the error structure, introducing bias or masking patterns in f(x).<br><br>Step 3: The coefficient of difficulty (d) depends on the interaction between σ²(x) and design density h(x). Suboptimal sampling (e.g., sparse data in high-σ(x) regions) amplifies estimation challenges, reducing visual clarity of f(x).<br><br>Step 4: Optimal design density h*(x) requires prior knowledge of σ(x), which is typically unavailable. In practice, initial estimates of f(x) (via Zₗ = (Yₗ - f̃(Xₗ))²) depend on residual calculations that are themselves biased if σ(x) is not properly accounted for, creating a circular dependency issue.<br><br>Step 5: Figure 4.4 likely illustrates how localized variance spikes distort the perceived relationship between Y and X, requiring specialized estimators like θ̃ⱼ and ĥσ(x) to disentangle f(x) from heteroscedastic noise.\n\nQID: textbook-99-7-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-7-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the challenges of recognizing the regression function in heteroscedastic settings, aligning with the gold answer's points on non-constant variance, estimator bias, and design density issues. The detailed steps provide a comprehensive explanation beyond the gold answer's summary.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-7-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the challenges of recognizing the regression function in heteroscedastic settings, aligning with the gold answer's points on non-constant variance, estimator bias, and design density issues. The detailed steps provide a comprehensive explanation beyond the gold answer's summary."}, "llm_echoed_qid": "textbook-99-7-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the challenges of recognizing the regression function in heteroscedastic settings, aligning with the gold answer's points on non-constant variance, estimator bias, and design density issues. The detailed steps provide a comprehensive explanation beyond the gold answer's summary."}
{"script_processing_qid": "textbook-115-20-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the practical implications of Theorem 7.6 for SUR systems where all equations have the same regressors.\n\nGOLD_ANSWER:\nWhen all equations have the same regressors, OLS and FGLS are algebraically identical. However, FGLS is still useful for testing cross-equation restrictions, as it provides the full variance-covariance matrix of $\\hat{\\pmb{\\beta}}$, enabling joint hypothesis tests across equations.\n\nCANDIDATE_ANSWER:\nStep 1: Recall Theorem 7.6 states that when all equations in a SUR system share identical regressors (“x_i1 = x_i2 = ... = x_iG”), equation-by-equation OLS and FGLS estimators are numerically identical.\n\nStep 2: Practical implication #1 – Simplified computation. Practitioners can avoid the computationally intensive FGLS procedure (which requires estimating Ω and inverting large matrices) and instead use standard OLS for each equation without efficiency loss.\n\nStep 3: Practical implication #2 – Robustness to misspecified cross-equation correlations. Since OLS remains identical to FGLS in this case, potential errors in estimating the error covariance matrix Ω do not affect estimator consistency or efficiency.\n\nStep 4: Practical implication #3 – Hypothesis testing simplification. The equivalence implies that standard OLS-based Wald tests (using equation-by-equation variances) are asymptotically valid, avoiding the need to compute FGLS-weighted test statistics like (7.52) for cross-equation restrictions.\n\nStep 5: Key limitation – This equivalence only holds strictly under identical regressors. If regressors differ slightly across equations, FGLS may still provide efficiency gains, and practitioners should verify this condition empirically.\n\nQID: textbook-115-20-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-20-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately expands on the practical implications of Theorem 7.6, detailing computational simplification, robustness, and hypothesis testing benefits while noting a key limitation. This aligns with and enriches the gold answer's focus on OLS/FGLS equivalence and testing utility.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-20-1-3", "category": "CORRECT", "explanation": "The candidate answer accurately expands on the practical implications of Theorem 7.6, detailing computational simplification, robustness, and hypothesis testing benefits while noting a key limitation. This aligns with and enriches the gold answer's focus on OLS/FGLS equivalence and testing utility."}, "llm_echoed_qid": "textbook-115-20-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately expands on the practical implications of Theorem 7.6, detailing computational simplification, robustness, and hypothesis testing benefits while noting a key limitation. This aligns with and enriches the gold answer's focus on OLS/FGLS equivalence and testing utility."}
{"script_processing_qid": "textbook-114-2-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the sources of error in the standard error of forecast (SEF) and their implications for forecast reliability.\n\nGOLD_ANSWER:\n1.  **Error Term**: The residual $e$ in the forecast period may not be zero, introducing uncertainty.\n2.  **Parameter Uncertainty**: The estimated coefficients $\\hat{b}$ may differ from the true parameters $b$.\n3.  **Implications**: A larger SEF indicates greater forecast uncertainty, requiring wider confidence intervals and caution in interpretation.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the two explicit sources of error in SEF from the context. The first is the inherent error term (ε) in the regression model, which represents unobserved factors affecting the dependent variable. The second is the estimation uncertainty in the regression coefficients (β₀, β₁, β₂, β₃).\n\nStep 2: Explain the implications of the error term. Even if ê is set to zero in the forecast equation, the true error term ε may deviate from zero in unobserved ways. This introduces irreducible variability, directly increasing SEF and reducing forecast reliability.\n\nStep 3: Address parameter uncertainty. The estimated coefficients (β̂₀, β̂₁, etc.) are subject to sampling variability. Errors in these estimates propagate to the forecasted value Ŝ, further increasing SEF. This source of error grows with multicollinearity or small sample sizes.\n\nStep 4: Discuss combined implications. The SEF aggregates both errors, meaning forecast confidence intervals will be wider than if only one source existed. Forecast reliability decreases as (1) the model’s unexplained variance (error term) increases, or (2) coefficient estimates become less precise. Users must account for both when interpreting forecast uncertainty.\n\nQID: textbook-114-2-2-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-2-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies and explains the two main sources of error in the standard error of forecast (SEF) as outlined in the gold answer: the error term and parameter uncertainty. It also correctly discusses their implications for forecast reliability, including the need for wider confidence intervals and the factors that decrease forecast reliability.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-2-2-3", "category": "CORRECT", "explanation": "The candidate answer accurately identifies and explains the two main sources of error in the standard error of forecast (SEF) as outlined in the gold answer: the error term and parameter uncertainty. It also correctly discusses their implications for forecast reliability, including the need for wider confidence intervals and the factors that decrease forecast reliability."}, "llm_echoed_qid": "textbook-114-2-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies and explains the two main sources of error in the standard error of forecast (SEF) as outlined in the gold answer: the error term and parameter uncertainty. It also correctly discusses their implications for forecast reliability, including the need for wider confidence intervals and the factors that decrease forecast reliability."}
{"script_processing_qid": "textbook-109-3-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Critically assess the diversification motive for conglomerate mergers in light of Porter (1987) and Jensen (1986).\n\nGOLD_ANSWER:\n1. **Diversification Critique**: \n   - **Porter (1987)**: Shareholders can diversify portfolios cheaper than firms; conglomerates often destroy value via inefficient capital allocation.\n   - **Jensen (1986)**: Free cash flow hypothesis—diversification masks poor performance, enabling managerial entrenchment.\n2. **Counterarguments**: \n   - **Debt Discipline**: Leveraged conglomerates may mitigate agency costs (Jensen, 1986).\n   - **Emerging Markets**: Where capital markets are immature, internal diversification may be optimal.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the diversification motive definition in the context. The context states conglomerate mergers aim to (1) reduce dependence on existing activities through uncorrelated/negatively correlated profitability (lowering cost of capital), and (2) transfer managerial competence across dissimilar businesses.\n\nStep 2: Evaluate theoretical support. The context implicitly aligns diversification with portfolio theory (reducing risk via uncorrelated cash flows) but does not explicitly mention Porter (1987) or Jensen (1986).\n\nStep 3: Critical assessment gaps. The provided context lacks direct references to Porter’s (1987) critique of diversification as often creating complexity without synergies, or Jensen’s (1986) agency cost arguments that free cash flow in conglomerates might incentivize value-destroying investments rather than efficient capital allocation.\n\nStep 4: Conclusion. While the context describes diversification motives, it does not include the necessary framework from Porter/Jensen to fully assess their critiques. A critical evaluation would require contrasting the stated motives with Porter’s synergy arguments and Jensen’s agency theory – neither of which are present in the provided excerpts.\n\nQID: textbook-109-3-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-109-3-1-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate identifies the diversification motive and acknowledges the absence of explicit references to Porter (1987) and Jensen (1986). However, the answer does not fully engage with the critiques or counterarguments presented in the gold answer, such as Porter's value destruction or Jensen's free cash flow hypothesis, leading to a partial assessment.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-109-3-1-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate identifies the diversification motive and acknowledges the absence of explicit references to Porter (1987) and Jensen (1986). However, the answer does not fully engage with the critiques or counterarguments presented in the gold answer, such as Porter's value destruction or Jensen's free cash flow hypothesis, leading to a partial assessment."}, "llm_echoed_qid": "textbook-109-3-1-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate identifies the diversification motive and acknowledges the absence of explicit references to Porter (1987) and Jensen (1986). However, the answer does not fully engage with the critiques or counterarguments presented in the gold answer, such as Porter's value destruction or Jensen's free cash flow hypothesis, leading to a partial assessment."}
{"script_processing_qid": "textbook-47-1-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the supply function for a firm in the Muth market model given the cost function $C_{i t} = x q_{i t} + \\frac{1}{2} y n q_{i t}^{2}$ and expected price $p_{t}^{e}$.\n\nGOLD_ANSWER:\n1. **Expected Profit**: $\\mathcal{I}_{i t}^{e} = p_{t}^{e} q_{i t} - x q_{i t} - \\frac{1}{2} y n q_{i t}^{2}$.\n2. **First-Order Condition**: Maximizing profit w.r.t. $q_{i t}$ gives $p_{t}^{e} - x - y n q_{i t} = 0$.\n3. **Supply Function**: $q_{i t} = (y n)^{-1}(p_{t}^{e} - x)$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that profit maximization requires marginal cost (MC) to equal expected price (pₜₑ).\nStep 2: Compute marginal cost by differentiating the cost function Cᵢₜ = x qᵢₜ + ½ y n qᵢₜ² with respect to qᵢₜ:\n   MC = dCᵢₜ/dqᵢₜ = x + y n qᵢₜ\nStep 3: Set marginal cost equal to expected price:\n   pₜₑ = x + y n qᵢₜ\nStep 4: Solve for qᵢₜ to derive the supply function:\n   qᵢₜ = (pₜₑ − x)/(y n)\nThus, the supply function is linear in the expected price with intercept −x/(y n) and slope 1/(y n).\n\nQID: textbook-47-1-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-47-1-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the supply function, matching the gold answer in both reasoning and final result.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-47-1-1-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the supply function, matching the gold answer in both reasoning and final result."}, "llm_echoed_qid": "textbook-47-1-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the supply function, matching the gold answer in both reasoning and final result."}
{"script_processing_qid": "textbook-99-6-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Describe the three-step procedure for estimating the regression function $f(x)$ from ordered categorical data. What is the role of the quantile function $Q^{\\varepsilon}$ in this procedure?\n\nGOLD_ANSWER:\n1. **Step 1**: Combine ordered categories into two groups (successes and failures).\n2. **Step 2**: Estimate the probability of success $\\hat{p}(x)$ using binary regression methods.\n3. **Step 3**: If the error distribution $\\varepsilon$ is known, compute $\\hat{f}(x) = a - Q^{\\varepsilon}(1 - [\\hat{p}(x)]_b^c)$, where $Q^{\\varepsilon}$ is the quantile function of $\\varepsilon$ and $[\\cdot]_b^c$ truncates to avoid infinite values.\n\nCANDIDATE_ANSWER:\nStep 1: Assume the observed ordinal categories arise from discretizing a latent continuous response variable Y* = f(x) + ε, where ε is an error term with quantile function Q^ε.\\n\\nStep 2: Use Q^ε to define thresholds/cutoffs between ordinal categories. Specifically, the probability of observing category k corresponds to P(Y* ∈ [Q^ε(τ_{k-1}), Q^ε(τ_k)]) where τ values are quantile levels.\\n\\nStep 3: Estimate f(x) nonparametrically (e.g., via splines or local regression) by maximizing the likelihood of observing the categorical outcomes, where the likelihood depends on f(x) through these Q^ε-derived thresholds.\\n\\nThe quantile function Q^ε plays a critical role in mapping the latent continuous scale (Y*) to observed ordinal categories by determining the boundaries between response categories. This enables estimation of the regression function f(x) despite the discretization of the response variable.\n\nQID: textbook-99-6-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-99-6-1-0\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer does not align with the three-step procedure described in the gold answer. While the candidate discusses the role of the quantile function in mapping latent variables to observed categories, they do not correctly outline the specific steps of combining categories, estimating probabilities, and computing the regression function as provided in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-6-1-0", "category": "INCORRECT", "explanation": "The candidate's answer does not align with the three-step procedure described in the gold answer. While the candidate discusses the role of the quantile function in mapping latent variables to observed categories, they do not correctly outline the specific steps of combining categories, estimating probabilities, and computing the regression function as provided in the gold answer."}, "llm_echoed_qid": "textbook-99-6-1-0", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer does not align with the three-step procedure described in the gold answer. While the candidate discusses the role of the quantile function in mapping latent variables to observed categories, they do not correctly outline the specific steps of combining categories, estimating probabilities, and computing the regression function as provided in the gold answer."}
{"script_processing_qid": "textbook-80-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the modified backfitting algorithm step-by-step, explaining how it addresses the issue of concurvity in additive models.\n\nGOLD_ANSWER:\n1. **Initialization**: Set $g_{\\alpha}^{(0)} = 0$ for all $\\alpha = 1, \\ldots, d$ and $g_0 = 0$.\n2. **Projection Step**: Regress $Y - \\sum_{\\alpha=1}^d g_{\\alpha}^{(l)}$ onto the space $\\mathcal{V}_1(\\mathbf{S}_1) + \\ldots + \\mathcal{V}_1(\\mathbf{S}_d)$ using $\\mathbf{a} = \\mathbf{A}(Y - \\sum_{\\alpha=1}^d g_{\\alpha}^{(l)})$.\n3. **Backfitting Step**: Apply one cycle of backfitting to $Y - \\mathbf{a}$ using $\\widetilde{\\mathbf{S}}_{\\alpha} = (\\mathbf{I} - \\mathbf{A}_{\\alpha})\\mathbf{S}_{\\alpha}$ to update $g_{\\alpha}^{(l+1)}$.\n4. **Convergence Check**: Repeat until convergence is reached. This modification ensures uniqueness by projecting onto the orthogonal complement of the concurvity space.\n\nCANDIDATE_ANSWER:\nStep 1: Standard Backfitting Issue\nIn standard backfitting, component functions are estimated iteratively via partial residuals: $\\widehat{g}_\\alpha^{new} = S_\\alpha\\left(Y - c - \\sum_{\\beta\\neq\\alpha}\\widehat{g}_\\beta(X_\\beta)\\right)$, where $S_\\alpha$ is a smoother. Concurvity arises when predictor variables are functionally related, leading to non-unique solutions (i.e., $\\widehat{\\mathbf{P}}b=0$ implies multiple valid decompositions).\n\nStep 2: Modified Backfitting Adjustment\nTo address concurvity, the modified algorithm replaces the univariate conditional expectation $E(\\bullet|X_\\alpha)$ with a multivariate projection that accounts for all dimensions:\n$$\\widehat{g}_\\alpha^{new} = \\widetilde{E}\\left[Y - c - \\sum_{\\beta\\neq\\alpha}\\widehat{g}_\\beta(X_\\beta) \\,\\bigg|\\, X_\\alpha\\right]$$\nHere, $\\widetilde{E}(\\bullet|X_\\alpha)$ is estimated via marginal integration over all other variables $X_{\\underline{\\alpha}}$, not just local smoothing in direction $\\alpha$.\n\nStep 3: Implementation via Presmoothing\n1. Compute a preliminary multidimensional smoother $\\widetilde{m}(X)$ of $Y$ using local polynomial estimation\n2. For each component $\\alpha$, estimate $\\widehat{g}_\\alpha(x_\\alpha)$ by numerically integrating $\\widetilde{m}(X)$ over the joint distribution of $X_{\\underline{\\alpha}}$:\n$$\\widehat{g}_\\alpha(x_\\alpha) = \\frac{1}{n}\\sum_{i=1}^n \\widetilde{m}\\left(x_\\alpha, X_{i,\\underline{\\alpha}}\\right) - \\text{offset}$$\nThis breaks exact linear dependencies by involving all dimensions in the projection operator.\n\nStep 4: Enforcing Uniqueness\nBy integrating over the full covariate space (not just direction $\\alpha$), the algorithm eliminates solutions of the form $\\widehat{g} + \\gamma b$ where $b$ spans the concurvity space. The asymptotic uniqueness result in (a) follows from the regularity conditions on regressor densities and smoother properties.\n\nKey Advantage: The multivariate integration step removes collinearities between components by implicitly conditioning on the joint distribution of all predictors, whereas standard backfitting only conditions on individual $X_\\alpha$ terms.\n\nQID: textbook-80-0-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-80-0-0-0\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate answer provides a detailed explanation of the modified backfitting algorithm and addresses the issue of concurvity. However, it does not precisely match the step-by-step procedure outlined in the gold answer, particularly in terms of the projection step and the use of specific operators like $\\mathbf{A}$ and $\\widetilde{\\mathbf{S}}_{\\alpha}$. The candidate's approach is conceptually aligned but lacks the exact technical details provided in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-0-0-0", "category": "PARTIALLY_CORRECT", "explanation": "The candidate answer provides a detailed explanation of the modified backfitting algorithm and addresses the issue of concurvity. However, it does not precisely match the step-by-step procedure outlined in the gold answer, particularly in terms of the projection step and the use of specific operators like $\\mathbf{A}$ and $\\widetilde{\\mathbf{S}}_{\\alpha}$. The candidate's approach is conceptually aligned but lacks the exact technical details provided in the gold answer."}, "llm_echoed_qid": "textbook-80-0-0-0", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate answer provides a detailed explanation of the modified backfitting algorithm and addresses the issue of concurvity. However, it does not precisely match the step-by-step procedure outlined in the gold answer, particularly in terms of the projection step and the use of specific operators like $\\mathbf{A}$ and $\\widetilde{\\mathbf{S}}_{\\alpha}$. The candidate's approach is conceptually aligned but lacks the exact technical details provided in the gold answer."}
{"script_processing_qid": "textbook-125-38-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the data from Table 8.3, calculate the expected risk premium for a stock with the following factor sensitivities: $b\\_{yield\\ spread} = 1.5$, $b\\_{interest\\ rate} = -1.0$, $b\\_{exchange\\ rate} = 0.5$, $b\\_{GNP} = 0.2$, $b\\_{inflation} = -0.3$, and $b\\_{market} = 0.8$.\n\nGOLD_ANSWER:\n1. **Factor Risk Premiums**: From Table 8.3, the risk premiums are:\n   - Yield spread: 5.10%\n   - Interest rate: -0.61%\n   - Exchange rate: -0.59%\n   - GNP: 0.49%\n   - Inflation: -0.83%\n   - Market: 6.36%\n2. **Calculation**:\n   - Yield spread: $1.5 \\times 5.10\\% = 7.65\\%$\n   - Interest rate: $-1.0 \\times -0.61\\% = 0.61\\%$\n   - Exchange rate: $0.5 \\times -0.59\\% = -0.295\\%$\n   - GNP: $0.2 \\times 0.49\\% = 0.098\\%$\n   - Inflation: $-0.3 \\times -0.83\\% = 0.249\\%$\n   - Market: $0.8 \\times 6.36\\% = 5.088\\%$\n3. **Total Risk Premium**: $7.65\\% + 0.61\\% - 0.295\\% + 0.098\\% + 0.249\\% + 5.088\\% = 13.4\\%$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the formula for calculating expected risk premium using APT. The formula is: Expected risk premium = b₁(r_factor1 - r_f) + b₂(r_factor2 - r_f) + ... + bₙ(r_factorn - r_f).\n\nStep 2: The problem provides the stock’s sensitivities (b values): \n- b_yield_spread = 1.5\n- b_interest_rate = -1.0\n- b_exchange_rate = 0.5\n- b_GNP = 0.2\n- b_inflation = -0.3\n- b_market = 0.8.\n\nStep 3: To calculate the expected risk premium, we need the risk premiums (r_factor - r_f) for each of the 6 factors from Table 8.3. However, the provided context does NOT include Table 8.3 or the numerical values for these factor risk premiums.\n\nStep 4: Without the factor risk premiums (e.g., yield_spread_risk_premium, interest_rate_risk_premium, etc.), the calculation cannot be completed.\n\nConclusion: The provided context does not contain enough information (specifically, Table 8.3’s data) to answer this question.\n\nQID: textbook-125-38-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-38-0-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable due to missing context, specifically the data from Table 8.3.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-38-0-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable due to missing context, specifically the data from Table 8.3."}, "llm_echoed_qid": "textbook-125-38-0-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable due to missing context, specifically the data from Table 8.3."}
{"script_processing_qid": "textbook-125-28-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the NPV for harvesting timber in year 2, given the net future value is $\\$77,500$ and the discount rate is 10%.\n\nGOLD_ANSWER:\n1. **NPV Calculation**: \n   - $NPV = \\frac{77,500}{(1.10)^2} = \\frac{77,500}{1.21} \\approx 64,049$.\n   - The NPV is approximately $\\$64,000$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the formula for Net Present Value (NPV). NPV = FV / (1 + r)^t where FV is future value, r is discount rate, and t is time period.\n\nStep 2: Substitute the given values into the formula. Here, FV = $77,500, r = 10% (0.10), and t = 2 years.\n\nStep 3: Calculate the discount factor: (1 + 0.10)^2 = 1.21.\n\nStep 4: Divide the net future value by the discount factor: $77,500 / 1.21 ≈ $64,049.59, which rounds to the $64,000 NPV listed in the context for Year 2.\n\nQID: textbook-125-28-2-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-28-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly identified the NPV formula, substituted the given values accurately, and performed the calculation to arrive at the correct NPV of approximately $64,000, matching the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-28-2-0", "category": "CORRECT", "explanation": "The candidate correctly identified the NPV formula, substituted the given values accurately, and performed the calculation to arrive at the correct NPV of approximately $64,000, matching the gold answer."}, "llm_echoed_qid": "textbook-125-28-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly identified the NPV formula, substituted the given values accurately, and performed the calculation to arrive at the correct NPV of approximately $64,000, matching the gold answer."}
{"script_processing_qid": "textbook-99-9-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the reference method with cross-validation for bandwidth selection.\n\nGOLD_ANSWER:\n**Reference Method**:\n1. **Pros**: Computationally efficient, straightforward to implement.\n2. **Cons**: Relies on the assumption that the reference function is a good approximation of the true density.\n\n**Cross-Validation**:\n1. **Pros**: Data-driven, adapts to the actual data structure.\n2. **Cons**: Computationally intensive, especially for large datasets.\n\n**Key Difference**: The reference method is parametric (assumes a reference form), while cross-validation is non-parametric and more flexible.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain any information about cross-validation methods for bandwidth selection. Therefore, a comparison between the reference method and cross-validation cannot be made based on the given context alone. The text exclusively discusses the mathematical derivation and application of the reference method using a normal density assumption.\n\nQID: textbook-99-9-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-9-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the lack of information about cross-validation methods.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-9-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the lack of information about cross-validation methods."}, "llm_echoed_qid": "textbook-99-9-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the lack of information about cross-validation methods."}
{"script_processing_qid": "textbook-117-30-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the NPV criterion is preferred over the IRR criterion for evaluating mutually exclusive projects with differing scales of investment.\n\nGOLD_ANSWER:\nThe NPV criterion is preferred because:\n1.  **Scale Consideration**: NPV accounts for the absolute dollar value of the project, whereas IRR only considers the percentage return. A smaller project with a higher IRR may contribute less to firm value than a larger project with a lower IRR.\n2.  **Reinvestment Assumption**: NPV assumes reinvestment at the discount rate, which is more realistic than IRR's assumption of reinvestment at the IRR itself.\n3.  **Consistency**: NPV always provides a consistent ranking for mutually exclusive projects, while IRR can lead to conflicting rankings due to scale or timing issues.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the scale problem in mutually exclusive projects. IRR measures percentage returns rather than absolute dollar value, which can be misleading when projects differ in investment size. For example, Opportunity 1 (\\$1 investment, 50% IRR) appears better than Opportunity 2 (\\$10 investment, 10% IRR) under IRR analysis.\n\nStep 2: Recognize that NPV accounts for project scale. NPV calculates absolute dollar value added, considering the magnitude of cash flows. While Opportunity 1 has a higher IRR, Opportunity 2 generates a larger total profit (\\$1 vs. \\$1.50 profit), which NPV would correctly identify if the discount rate is sufficiently low.\n\nStep 3: Explain the limitation of IRR in value comparison. A 50% return on a \\$1 investment cannot be directly compared to a 10% return on a \\$10 investment without considering the opportunity cost of the remaining \\$9 (from choosing the smaller project).\n\nStep 4: Highlight NPV's superiority in resource allocation. For mutually exclusive projects requiring full capital commitment, NPV selects the option that maximizes total shareholder wealth, while IRR might favor smaller projects with artificially high percentage returns but lower total value creation.\n\nStep 5: Note that incremental IRR (a modified approach) could resolve this issue but requires cumbersome manual comparisons, whereas NPV automatically incorporates scale differences in its calculation.\n\nQID: textbook-117-30-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-30-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains why NPV is preferred over IRR for mutually exclusive projects with differing scales of investment, covering scale consideration, reinvestment assumptions, and consistency in ranking. The explanation aligns with the gold answer and provides additional insights and examples.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-30-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains why NPV is preferred over IRR for mutually exclusive projects with differing scales of investment, covering scale consideration, reinvestment assumptions, and consistency in ranking. The explanation aligns with the gold answer and provides additional insights and examples."}, "llm_echoed_qid": "textbook-117-30-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains why NPV is preferred over IRR for mutually exclusive projects with differing scales of investment, covering scale consideration, reinvestment assumptions, and consistency in ranking. The explanation aligns with the gold answer and provides additional insights and examples."}
{"script_processing_qid": "textbook-122-12-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Contrast the implications of a linear trend ($y_{t} = \\alpha_{0} + \\alpha_{1}t + e_{t}$) and a quadratic trend ($y_{t} = \\alpha_{0} + \\alpha_{1}t + \\alpha_{2}t^{2} + e_{t}$) for long-term forecasting.\n\nGOLD_ANSWER:\n1. **Linear Trend**: Implies constant absolute growth ($\\Delta y_{t} = \\alpha_{1}$); unrealistic for series with accelerating/decaying growth.\n2. **Quadratic Trend**: Allows for changing slope ($\\Delta y_{t}/\\Delta t \\approx \\alpha_{1} + 2\\alpha_{2}t$); flexible but may predict implausible long-term behavior (e.g., hump-shaped trends).\n3. **Practicality**: Linear trends suit stable growth; quadratic trends fit short-term nonlinearity but require careful extrapolation.\n\nCANDIDATE_ANSWER:\nStep 1: Analyze the linear trend model ($y_t = \\alpha_0 + \\alpha_1 t + e_t$). Here, $\\alpha_1$ represents the constant period-to-period absolute change in $y_t$. Long-term forecasts grow linearly, as each incremental time unit adds $\\alpha_1$ to the predicted value. The trend is unbounded and directionally stable (always increasing or decreasing).\n\nStep 2: Analyze the quadratic trend model ($y_t = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2 + e_t$). The term $\\alpha_2 t^2$ introduces curvature. The period-to-period change is no longer constant; it depends on $\\alpha_1 + 2\\alpha_2 t$. For large $t$, the $t^2$ term dominates, leading to accelerating growth (if $\\alpha_2 > 0$) or eventual decline (if $\\alpha_2 < 0$).\n\nStep 3: Contrast implications for long-term forecasting:\n- **Linear trend**: Predictions grow indefinitely at a constant rate. Suitable for stable, non-accelerating systems.\n- **Quadratic trend**: Predictions exhibit curvature, leading to explosive growth or eventual reversal depending on $\\alpha_2$. More flexible but risks unrealistic extrapolation (e.g., implausibly large values if $\\alpha_2 > 0$).\n\nKey distinction: Linear trends assume constant marginal effects of time, while quadratic trends allow for time-varying marginal effects, altering long-term forecast trajectories fundamentally.\n\nQID: textbook-122-12-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-12-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly analyzes both linear and quadratic trends, accurately contrasting their long-term forecasting implications, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-12-2-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly analyzes both linear and quadratic trends, accurately contrasting their long-term forecasting implications, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-122-12-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly analyzes both linear and quadratic trends, accurately contrasting their long-term forecasting implications, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-59-1-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the role of the parameter $\\mu$ (fraction of realized payoffs lost in bankruptcy) in the financial accelerator mechanism. How does it affect the external finance premium?\n\nGOLD_ANSWER:\n1. **Role of $\\mu$**: $\\mu$ represents the deadweight loss associated with bankruptcy. Higher $\\mu$ increases the cost of financial distress, raising the external finance premium.\n2. **Effect on Premium**: The external finance premium $R^{k}-R$ is increasing in $\\mu$ because lenders demand higher compensation for the increased risk of bankruptcy losses. This amplifies the financial accelerator effect, as shocks to net worth have larger impacts on borrowing costs.\n\nCANDIDATE_ANSWER:\nThe provided context does not explicitly mention the parameter μ (fraction of realized payoffs lost in bankruptcy) or its role in the financial accelerator mechanism. The extracted text focuses on parameters such as the discount factor (β), labor supply elasticity (η), capital share (α), depreciation rate (δ), and steady-state outcomes like the risk spread (Rᴋ−R), business failure rate (F(ω̄)), and capital-to-net-worth ratio (K/N). Since μ is not discussed in the provided context, there is insufficient information to explain its role in affecting the external finance premium.\n\nQID: textbook-59-1-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-59-1-0-1\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly notes the absence of information about μ.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-59-1-0-1", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly notes the absence of information about μ."}, "llm_echoed_qid": "textbook-59-1-0-1", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly notes the absence of information about μ."}
{"script_processing_qid": "textbook-43-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Explain how Lucas (1988) and Romer (1990) reconciled increasing returns with equilibrium growth models.\n\nGOLD_ANSWER:\n1.  **Externalities**: They introduced external effects into production functions, allowing social returns to exceed private returns.\n2.  **Monopolistic Competition**: Romer used monopolistic competition to model firms facing downward-sloping demand curves, enabling increasing returns at the firm level while maintaining equilibrium.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize the challenge. Traditional equilibrium growth models relied on constant returns to scale, as increasing returns typically conflict with competitive equilibrium assumptions.\nStep 2: Identify Lucas (1988)'s approach. Lucas reconciled this by introducing externalities (e.g., human capital spillovers), where individual firms experience constant returns but aggregate production exhibits increasing returns due to knowledge diffusion.\nStep 3: Identify Romer (1990)'s approach. Romer used monopolistic competition in innovation sectors, allowing firms to internalize returns from R&D investments while maintaining equilibrium through differentiated goods and temporary market power.\nStep 4: Explain compatibility. Both mechanisms preserved equilibrium by localizing increasing returns either through externalities (avoiding coordination failures) or market imperfections (monopolistic competition), enabling sustained growth within dynamic general equilibrium frameworks.\n\nQID: textbook-43-0-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-43-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains both Lucas (1988) and Romer (1990)'s approaches to reconciling increasing returns with equilibrium growth models, matching the key points in the gold answer regarding externalities and monopolistic competition.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-43-0-0-0", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains both Lucas (1988) and Romer (1990)'s approaches to reconciling increasing returns with equilibrium growth models, matching the key points in the gold answer regarding externalities and monopolistic competition."}, "llm_echoed_qid": "textbook-43-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains both Lucas (1988) and Romer (1990)'s approaches to reconciling increasing returns with equilibrium growth models, matching the key points in the gold answer regarding externalities and monopolistic competition."}
{"script_processing_qid": "textbook-80-6-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the mean and variance of $Y$ for a general exponential family distribution $f(y, \\theta, \\psi) = \\exp\\left\\{\\frac{y\\theta - b(\\theta)}{a(\\psi)} + c(y, \\psi)\\right\\}$.\n\nGOLD_ANSWER:\n1. **Mean**: $E(Y) = b^{\\prime}(\\theta)$.\n2. **Variance**: $Var(Y) = b^{\\prime\\prime}(\\theta)a(\\psi)$.\n3. **Derivation**: Use the properties of the score function and the fact that $\\int f(y, \\theta, \\psi) dy = 1$ to derive these results.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that for exponential family distributions, the first derivative of the cumulant function $b(\\theta)$ gives the mean.\\nStep 2: Compute the first derivative of $b(\\theta)$ with respect to $\\theta$:\\n   $$ E(Y) = \\mu = b'(\\theta) $$\\nStep 3: For the variance, use the property that the second derivative of $b(\\theta)$ multiplied by the dispersion function $a(\\psi)$ gives the variance:\\n   $$ Var(Y) = b''(\\theta) \\cdot a(\\psi) $$\\nConclusion: The mean $E(Y) = b'(\\theta)$ and variance $Var(Y) = b''(\\theta)\\cdot a(\\psi)$ are fully determined by the cumulant function $b(\\theta)$ and dispersion parameter $a(\\psi)$.\n\nQID: textbook-80-6-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-80-6-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the mean and variance for the general exponential family distribution, matching the gold answer exactly. The reasoning and steps are clear and accurate.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-6-1-1", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the mean and variance for the general exponential family distribution, matching the gold answer exactly. The reasoning and steps are clear and accurate."}, "llm_echoed_qid": "textbook-80-6-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the mean and variance for the general exponential family distribution, matching the gold answer exactly. The reasoning and steps are clear and accurate."}
{"script_processing_qid": "textbook-111-24-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Derive the marginal product of labor ($MPL$) for the three-input Cobb-Douglas production function $Y=K^{1/3}L^{1/3}H^{1/3}$. How does an increase in human capital ($H$) affect the $MPL$?\n\nGOLD_ANSWER:\n1.  **Derivation of $MPL$**: $MPL = \\frac{\\partial Y}{\\partial L} = \\frac{1}{3}K^{1/3}L^{-2/3}H^{1/3}$.\n2.  **Effect of Human Capital**: An increase in $H$ raises $MPL$ because $H$ appears in the numerator. Specifically, $MPL$ is proportional to $H^{1/3}$, so a 10% increase in $H$ leads to a $(1.1)^{1/3} \\approx 1.032$ (3.2%) increase in $MPL$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that the marginal product of labor (MPL) is the partial derivative of output (Y) with respect to labor (L). For the production function Y=K^{1/3}L^{1/3}H^{1/3}:\n\nMPL = ∂Y/∂L = (1/3)K^{1/3}L^{-2/3}H^{1/3}.\n\nStep 2: Simplify the expression using the original production function Y = K^{1/3}L^{1/3}H^{1/3}:\n\nMPL = (1/3) * (K^{1/3}H^{1/3}/L^{2/3}) = (1/3) * Y/L.\n\nStep 3: Analyze the effect of an increase in human capital (H). Since H appears in the numerator of the MPL expression (as H^{1/3}), an increase in H directly increases ML, holding K and L constant. This occurs because human capital complements labor in production.\n\nConclusion: The MPL is (1/3)Y/L, and an increase in human capital (H) raises the marginal product of labor.\n\nQID: textbook-111-24-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-111-24-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derived the marginal product of labor (MPL) as (1/3)K^(1/3)L^(-2/3)H^(1/3) and accurately explained that an increase in human capital (H) raises MPL since H appears in the numerator. The reasoning and conclusion align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-24-0-3", "category": "CORRECT", "explanation": "The candidate correctly derived the marginal product of labor (MPL) as (1/3)K^(1/3)L^(-2/3)H^(1/3) and accurately explained that an increase in human capital (H) raises MPL since H appears in the numerator. The reasoning and conclusion align with the gold answer."}, "llm_echoed_qid": "textbook-111-24-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derived the marginal product of labor (MPL) as (1/3)K^(1/3)L^(-2/3)H^(1/3) and accurately explained that an increase in human capital (H) raises MPL since H appears in the numerator. The reasoning and conclusion align with the gold answer."}
{"script_processing_qid": "textbook-99-9-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. What are the key differences between the plug-in method and cross-validation?\n\nGOLD_ANSWER:\n**Plug-In Method**:\n1. **Based on Asymptotics**: Uses theoretical formulas for optimal bandwidth.\n2. **Functional Estimation**: Requires estimating higher-order derivatives or other functionals.\n3. **Computational Efficiency**: Often faster than cross-validation.\n\n**Cross-Validation**:\n1. **Data-Driven**: Minimizes an empirical risk criterion.\n2. **No Functional Estimation**: Directly optimizes performance on the data.\n3. **Computational Cost**: More intensive due to multiple fits.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain sufficient information to answer this question. While the context explains the plug-in method (estimating unknown functionals in asymptotic bandwidth formulas by substituting empirical estimates), it does not describe cross-validation or provide any comparison points between the two methods. A proper comparison would require details about cross-validation's operational mechanics and theoretical basis, which are absent in the given context.\n\nQID: textbook-99-9-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-9-2-1\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-9-2-1", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}, "llm_echoed_qid": "textbook-99-9-2-1", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}
{"script_processing_qid": "textbook-97-6-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the concept of primitive elements in Galois fields and their importance.\n\nGOLD_ANSWER:\n1. **Definition**: A primitive element is one whose powers generate all non-zero elements of the field.\n2. **Significance**: Simplifies computations, especially in multiplicative operations.\n3. **Example**: In $GF(9)$, the element $x$ is primitive if its powers enumerate all non-zero elements.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain any explicit mention or discussion of primitive elements in Galois fields. While it explains the construction of Galois fields via polynomial rings modulo irreducible polynomials, there is no information about primitive elements (generators of the multiplicative group of the field) or their role in field operations. Therefore, the context is insufficient to address this question.\n\nQID: textbook-97-6-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-97-6-1-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-97-6-1-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}, "llm_echoed_qid": "textbook-97-6-1-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}
{"script_processing_qid": "textbook-99-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Analyze the scatter plot of Income vs. Illiteracy in Figure 6.11. What key patterns and outliers are visible, and how do they inform the density estimation?\n\nGOLD_ANSWER:\n1. **Patterns**: \n   - Concentration near low Illiteracy and moderate Income.\n   - Negative correlation: Higher Income associated with lower Illiteracy.\n2. **Outliers**: \n   - Alaska: High Income ($\\$6315$) with moderate Illiteracy (1.5).\n   - Louisiana: High Illiteracy (2.8) with low Income.\n3. **Density Estimation**: \n   - Sparse data requires careful bandwidth selection.\n   - Outliers may skew estimates if not accounted for.\n\nCANDIDATE_ANSWER:\nStep 1: Observe the main pattern in the scatter plot. Data points are concentrated near low Illiteracy (X2) and moderate Income (X1), indicating most observations represent states with average incomes and low illiteracy rates.\n\nStep 2: Identify key outliers. Louisiana and neighboring states form a cluster at the smallest Incomes (X1) and high Illiteracy (X2), while Alaska appears as an outlier at X1 > $6000 (highest Income) with X2=1.5 (moderate Illiteracy).\n\nStep 3: Relate patterns to density estimation. The main concentration area aligns with the highest estimated density region. Louisiana's cluster creates a positive density near the low-Income/high-Illiteracy corner, while Alaska forms a distinct ridge in the density estimate along X2=1.5 for high Incomes.\n\nStep 4: Conclusion. The density estimator successfully captures sparse data characteristics: main concentration areas, localized clusters from outliers (Louisiana group), and unique ridges from extreme-value outliers (Alaska). This shows how density estimation reveals both central tendencies and influential outliers in real-world data.\n\nQID: textbook-99-0-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-99-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies the key patterns and outliers in the scatter plot, and correctly explains how they inform the density estimation, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-0-1-0", "category": "CORRECT", "explanation": "The candidate answer accurately identifies the key patterns and outliers in the scatter plot, and correctly explains how they inform the density estimation, aligning with the gold answer."}, "llm_echoed_qid": "textbook-99-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies the key patterns and outliers in the scatter plot, and correctly explains how they inform the density estimation, aligning with the gold answer."}
{"script_processing_qid": "textbook-80-3-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the trade-offs between using a purely additive model versus a model with interaction terms in terms of estimation accuracy and interpretability.\n\nGOLD_ANSWER:\n3. **Trade-offs**:\n   - **Additive Model**: Simpler to estimate and interpret, but may suffer from misspecification bias if interactions are present.\n   - **Interaction Model**: More flexible and can capture complex relationships, but at the cost of higher variance and more challenging interpretation.\n\nCANDIDATE_ANSWER:\nStep 1: Estimation Accuracy Trade-Off\n- Purely additive models ignore interaction effects, potentially introducing bias when true interactions exist between variables (as noted in production function contexts).\n- Models with interaction terms better capture these effects, improving estimation accuracy if interactions are present.\n- However, adding interaction terms increases model complexity (more parameters) which may raise variance/overfitting risks, especially with limited data.\n\nStep 2: Interpretability Trade-Off\n- Additive models provide clearer interpretation since each variable's effect is independent and additive.\n- Interaction models require interpreting coefficients as conditional effects (e.g., the effect of Xα depends on Xj), complicating communication and analysis.\n\nStep 3: Contextual Balance\nThe extension to pairwise interactions in the given context reflects a prioritization of accuracy (by addressing a known weakness of additive models) at the cost of increased complexity and reduced interpretability. This trade-off is justified in domains like production function modeling where interaction effects are theoretically expected.\n\nQID: textbook-80-3-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-80-3-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the trade-offs between additive and interaction models in terms of estimation accuracy and interpretability, aligning well with the gold answer. It provides additional context and examples, enhancing the explanation without contradicting the core points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-3-1-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the trade-offs between additive and interaction models in terms of estimation accuracy and interpretability, aligning well with the gold answer. It provides additional context and examples, enhancing the explanation without contradicting the core points."}, "llm_echoed_qid": "textbook-80-3-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the trade-offs between additive and interaction models in terms of estimation accuracy and interpretability, aligning well with the gold answer. It provides additional context and examples, enhancing the explanation without contradicting the core points."}
{"script_processing_qid": "textbook-65-1-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the heterogeneity index $HI(S)$ for a subset $S$ of curves. How does it compare to alternative indices (e.g., using median instead of mean)?\n\nGOLD_ANSWER:\n1. **Definition**:\n   $$HI(S) = \\frac{d_2^{deriv}(\\pmb{\\chi}_{mod,S}, \\pmb{\\chi}_{mean,S})}{d_2^{deriv}(\\pmb{\\chi}_{mod,S}, 0) + d_2^{deriv}(\\pmb{\\chi}_{mean,S}, 0)}.$$\n2. **Interpretation**: Measures discrepancy between functional mode ($\\pmb{\\chi}_{mod,S}$) and mean ($\\pmb{\\chi}_{mean,S}$).\n3. **Median Alternative**: More robust to outliers but less interpretable for symmetric distributions.\n\nCANDIDATE_ANSWER:\nStep 1: The provided context does not explicitly specify the mathematical formulation of the heterogeneity index $HI(S)$.\n\nStep 2: The text references steps involving optimal bandwidth selection ($\\widehat{h}_{opt}$) and subsampling for computing $SHI$, but no equations for variance estimates, distance metrics, or aggregation methods (mean/median) are given.\n\nStep 3: The algorithm mentions sensitivity to thresholds and semi-metrics, but comparison to median-based alternatives requires explicit information about how dispersion is measured in $HI(S)$, which is absent in the context.\n\nConclusion: The context insufficiently specifies the heterogeneity index's mathematical definition to perform a derivation or comparison with median-based alternatives.\n\nQID: textbook-65-1-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-65-1-1-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer concludes the context insufficiently specifies the heterogeneity index's mathematical definition.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-65-1-1-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer concludes the context insufficiently specifies the heterogeneity index's mathematical definition."}, "llm_echoed_qid": "textbook-65-1-1-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer concludes the context insufficiently specifies the heterogeneity index's mathematical definition."}
{"script_processing_qid": "textbook-26-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the concept of cointegration in the context of long-run money demand and how it relates to the persistence of shocks.\n\nGOLD_ANSWER:\nCointegration refers to a long-run equilibrium relationship between non-stationary time series variables that are integrated of the same order. In the context of long-run money demand, if $m_t$, $y_t$, and $r_t$ are cointegrated, their linear combination (the residual $u_t$) is stationary, implying mean reversion. This means shocks to the money demand relation are temporary and the system reverts to equilibrium over time, unlike the individual series which may exhibit persistent shocks.\n\nCANDIDATE_ANSWER:\nStep 1: Cointegration refers to a stable long-run relationship between non-stationary economic variables. In the context of money demand, this means that while real money balances, real GNP, and interest rates may individually exhibit persistent trends (unit roots), a linear combination of these variables (e.g., log real money = 0.9 log real GNP - 0.1 interest rate) forms a stationary relationship.\n\nStep 2: The persistence of shocks relates to how temporary or permanent deviations from equilibrium are. Without cointegration, shocks to money demand or its determinants would permanently disrupt the relationship. With cointegration, the system gradually returns to its long-run equilibrium (as per the estimates in Table 5), meaning shocks have only transitory effects on the relationship.\n\nStep 3: The reduced persistence of the linear combination (compared to individual series) is evidence of cointegration. This implies that while individual series may drift apart due to shocks, economic forces (e.g., adjustments in money holdings, interest rates, or output) eventually restore the equilibrium relationship described by the cointegrating vector.\n\nQID: textbook-26-0-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-26-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains cointegration in the context of long-run money demand, detailing the relationship between non-stationary variables and the persistence of shocks, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-26-0-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains cointegration in the context of long-run money demand, detailing the relationship between non-stationary variables and the persistence of shocks, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-26-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains cointegration in the context of long-run money demand, detailing the relationship between non-stationary variables and the persistence of shocks, aligning well with the gold answer."}
{"script_processing_qid": "textbook-125-42-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of the weak form of the efficient-market hypothesis for corporate financial managers. Why is it problematic for managers to time equity issuances based on past stock price movements?\n\nGOLD_ANSWER:\n1. **Weak Form EMH**: States that past price changes contain no information about future changes, implying no predictable cycles.\n2. **Managerial Behavior**: Managers often issue equity after price rises and delay after falls, assuming mean reversion.\n3. **Problem**: Since the market has no memory, such timing strategies are ineffective and may lead to suboptimal financing decisions, as prices follow a random walk.\n\nCANDIDATE_ANSWER:\nStep 1: Weak-form EMH states that past stock price patterns cannot predict future prices, as all historical information is already reflected in current prices.\n\nStep 2: For corporate financial managers, this implies that historical stock price trends (e.g., the 1990s tech bubble's 580% Nasdaq rise or 1987 crash) contain no actionable information for timing equity offerings.\n\nStep 3: Attempting to issue equity based on recent price patterns (e.g., issuing after a stock surge like the dot-com boom) is problematic because:\n1. EMH suggests current prices already reflect all available information, making past trends irrelevant\n2. Observed patterns might be temporary mispricings (e.g., irrational exuberance) rather than true value\n3. Valuation difficulties highlighted in the context make it hard to distinguish true value from market sentiment\n4. Managers risk issuing underpriced equity during crashes or overpriced equity during bubbles, harming shareholder value\n\nStep 4: The example of the Nasdaq's 64% collapse after March 2000 demonstrates how reliance on past price momentum can lead to disastrous timing decisions, as EMH predicts reversals cannot be systematically anticipated.\n\nQID: textbook-125-42-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-42-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the weak form of the efficient-market hypothesis and its implications for corporate financial managers. It correctly identifies the problem with timing equity issuances based on past stock price movements and provides relevant examples to support the argument.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-42-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the weak form of the efficient-market hypothesis and its implications for corporate financial managers. It correctly identifies the problem with timing equity issuances based on past stock price movements and provides relevant examples to support the argument."}, "llm_echoed_qid": "textbook-125-42-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the weak form of the efficient-market hypothesis and its implications for corporate financial managers. It correctly identifies the problem with timing equity issuances based on past stock price movements and provides relevant examples to support the argument."}
{"script_processing_qid": "textbook-117-10-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. (Quantitative) A firm switches from FIFO to LIFO during a period of inflation. If inventory costs rise by 10% annually and the tax rate is 30%, calculate the potential tax savings in the first year for \\$1 million in inventory.\n\nGOLD_ANSWER:\n3. **Tax Savings Calculation**: \n   - FIFO COGS = \\$1 million (older, lower costs).\n   - LIFO COGS = \\$1 million * 1.10 = \\$1.1 million (current higher costs).\n   - **Taxable Income Reduction** = \\$1.1 million - \\$1 million = \\$100,000.\n   - **Tax Savings** = \\$100,000 * 30% = \\$30,000.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate the inventory cost difference between FIFO and LIFO. With 10% annual inflation, LIFO values inventory using older costs. The difference is:\n$1,000,000 × 10% = $100,000.\n\nStep 2: Determine taxable income reduction. The $100,000 higher COGS under LIFO reduces taxable income by this amount.\n\nStep 3: Calculate tax savings. Apply the 30% tax rate to the $100,000 reduction:\n$100,000 × 30% = $30,000.\n\nQID: textbook-117-10-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-10-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly calculates the tax savings by following the same steps as the gold answer, resulting in the correct tax savings of \\$30,000.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-10-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly calculates the tax savings by following the same steps as the gold answer, resulting in the correct tax savings of \\$30,000."}, "llm_echoed_qid": "textbook-117-10-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly calculates the tax savings by following the same steps as the gold answer, resulting in the correct tax savings of \\$30,000."}
{"script_processing_qid": "textbook-117-11-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. (Qualitative) Discuss the concept of 'milking the property' and its implications for bondholders and stockholders in a financially distressed firm.\n\nGOLD_ANSWER:\n1. **Definition**: 'Milking the property' refers to stockholders extracting value from the firm (e.g., through dividends) during financial distress, leaving less for bondholders.\n2. **Implications for Bondholders**: Reduced assets increase the likelihood of default, lowering bond values.\n3. **Implications for Stockholders**: Short-term gains may come at the cost of higher future borrowing costs, as bondholders demand higher interest rates to compensate for increased risk.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the context of 'milking the property' in financial distress. The provided context describes conflicts between stockholders and bondholders during financial distress, where stockholders may pursue 'selfish strategies' that create agency costs. While the term 'milking the property' is not explicitly defined in the text, it aligns with these strategies.\n\nStep 2: Define the concept. 'Milking the property' refers to actions where stockholders extract value from the firm (e.g., through excessive dividends, asset sales, or risky investments) at the expense of bondholders, reducing the firm’s ability to repay debt.\n\nStep 3: Implications for bondholders. Bondholders face increased risk of losses because such strategies erode the firm’s asset base or increase financial instability, reducing recovery value in default. This aligns with the context’s mention of indirect costs (e.g., lost sales, trust issues) and agency costs harming firm value.\n\nStep 4: Implications for stockholders. While stockholders may gain short-term benefits (e.g., dividend payouts), these actions can accelerate financial distress and harm long-term equity value. However, in near-bankruptcy scenarios, stockholders may prioritize immediate gains over firm survival.\n\nQID: textbook-117-11-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-11-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately defines 'milking the property' and correctly discusses its implications for both bondholders and stockholders, aligning closely with the gold answer. The step-by-step reasoning is thorough and matches the expected analysis.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-11-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately defines 'milking the property' and correctly discusses its implications for both bondholders and stockholders, aligning closely with the gold answer. The step-by-step reasoning is thorough and matches the expected analysis."}, "llm_echoed_qid": "textbook-117-11-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately defines 'milking the property' and correctly discusses its implications for both bondholders and stockholders, aligning closely with the gold answer. The step-by-step reasoning is thorough and matches the expected analysis."}
{"script_processing_qid": "textbook-125-6-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the risks and rewards associated with selling a put option. How does the payoff diagram for selling a put differ from buying a put?\n\nGOLD_ANSWER:\n2. **Risks and Rewards**:\n   - **Reward**: The seller receives the premium upfront.\n   - **Risk**: The seller is obligated to buy the stock at the exercise price if the buyer exercises the put. The maximum loss occurs if the stock price falls to $0$.\n   **Payoff Diagram**: Selling a put is the mirror image of buying a put, with the seller's payoff being the negative of the buyer's payoff.\n\nCANDIDATE_ANSWER:\nStep 1: Understand selling a put option. The seller (writer) receives a premium upfront but is obligated to buy the underlying asset at the strike price if the buyer exercises the put. Rewards: Maximum profit is limited to the premium received. Risks: Potential losses increase as the stock price falls below the strike price. Maximum loss occurs if the stock drops to $0, equaling (strike price × number of shares) − premium received.\n\nStep 2: Contrast payoff diagrams. Buying a put option has a payoff that increases as the stock price decreases below the strike price (limited upside to strike price − $0). Selling a put inverts this: the seller’s payoff decreases as the stock price falls below the strike. The maximum gain (premium) is capped, while potential losses are significant (though limited to strike price − $0 − premium).\n\nStep 3: Key difference. Buying a put benefits from price declines (bearish position), while selling a put profits from price stability/rises (bullish or neutral). The seller’s payoff diagram is a mirror image of the buyer’s, reflecting the inverse risk-reward structure.\n\nQID: textbook-125-6-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-6-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes the risks and rewards of selling a put option and correctly contrasts the payoff diagrams for buying and selling a put, mirroring the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-6-2-1", "category": "CORRECT", "explanation": "The candidate's answer accurately describes the risks and rewards of selling a put option and correctly contrasts the payoff diagrams for buying and selling a put, mirroring the gold answer's key points."}, "llm_echoed_qid": "textbook-125-6-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes the risks and rewards of selling a put option and correctly contrasts the payoff diagrams for buying and selling a put, mirroring the gold answer's key points."}
{"script_processing_qid": "textbook-117-42-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What is the equilibrium rate of interest, and how is it determined in the market for loans?\n\nGOLD_ANSWER:\n1. **Equilibrium Rate**: The interest rate at which the total supply of loans (from lenders like Tom) equals the total demand for loans (from borrowers like Leslie).\n2. **Market Clearing**: At rates above equilibrium, lenders want to lend more than borrowers want to borrow; at rates below, the opposite occurs.\n3. **Example**: In the text, the equilibrium rate is 10%, where Tom's willingness to lend matches Leslie's willingness to borrow.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Key Participants - The market for loans involves Tom (saver/supplier of funds) and Leslie (borrower/demander of funds).\nStep 2: Analyze Preferences - Tom wants to save $50,000 (current income $100k − desired consumption $50k), while Leslie wants to borrow $50,000 (desired consumption $150k − current income $100k).\nStep 3: Determine Rate via Agreement - The equilibrium interest rate is established through their negotiated exchange: Tom lends $50,000 today in return for $55,000 next year.\nStep 4: Calculate Interest Rate - The 10% rate ($5,000/$50,000) balances supply (Tom’s savings) and demand (Leslie’s borrowing) in this simplified market.\nConclusion: The equilibrium interest rate of 10% emerges as the rate where the quantity of funds supplied (by Tom) equals the quantity demanded (by Leslie).\n\nQID: textbook-117-42-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-42-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the equilibrium rate of interest as 10%, matching the gold answer. It also accurately describes the process of how this rate is determined through the interaction of supply (Tom) and demand (Leslie) in the loan market.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-42-1-2", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the equilibrium rate of interest as 10%, matching the gold answer. It also accurately describes the process of how this rate is determined through the interaction of supply (Tom) and demand (Leslie) in the loan market."}, "llm_echoed_qid": "textbook-117-42-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the equilibrium rate of interest as 10%, matching the gold answer. It also accurately describes the process of how this rate is determined through the interaction of supply (Tom) and demand (Leslie) in the loan market."}
{"script_processing_qid": "textbook-99-4-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the asymptotic relationship between Shannon information $S(\\Theta,Y^{n})$ and Fisher information $I(\\theta)$ for the iid case.\n\nGOLD_ANSWER:\n1. **Asymptotic Formula**: $S(\\Theta,Y^{n}) = \\frac{1}{2}\\ln\\left(\\frac{n}{2\\pi e}\\right) + \\int g(\\theta)\\log\\left(\\frac{I^{1/2}(\\theta)}{g(\\theta)}\\right)d\\theta + o_{n}(1)$.\n2. **Interpretation**: The first term represents the entropy of the sample size, the second term links Fisher information and prior density $g(\\theta)$, and the last term is a vanishing error.\n\nCANDIDATE_ANSWER:\nStep 1: In the iid case under mild assumptions, the asymptotic relationship between Shannon information S(Θ,Yⁿ) and Fisher information I(θ) is given by:\n\nS(Θ,Yⁿ) = (1/2) ln(n/(2πe)) + ∫ g(θ) log(I^{1/2}(θ)/g(θ)) dθ + oₙ(1).\n\nStep 2: The first term (1/2) ln(n/(2πe)) represents the dominant logarithmic growth component in n, capturing the asymptotic scaling behavior of mutual information.\n\nStep 3: The integral term ∫ g(θ)log(I^{1/2}(θ)/g(θ))dθ links the prior distribution g(θ) with the Fisher information I(θ). Here, I^{1/2}(θ) reflects how the estimation precision (via Fisher information) interacts with the prior to influence information transmission.\n\nStep 4: The oₙ(1) term indicates vanishing contributions as n→∞, showing this is an asymptotic approximation valid for large n.\n\nKey Insight: This relationship demonstrates that the Shannon mutual information grows asymptotically as (1/2) ln(n) scaled by Fisher information and adjusted by the prior, connecting statistical estimation efficiency (Fisher information) with information-theoretic channel capacity (Shannon information).\n\nQID: textbook-99-4-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-4-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately reproduces the asymptotic formula from the gold answer and provides a detailed interpretation of each term, matching the gold answer's explanation.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-4-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately reproduces the asymptotic formula from the gold answer and provides a detailed interpretation of each term, matching the gold answer's explanation."}, "llm_echoed_qid": "textbook-99-4-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately reproduces the asymptotic formula from the gold answer and provides a detailed interpretation of each term, matching the gold answer's explanation."}
{"script_processing_qid": "textbook-50-1-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the Thurstone-Mosteller model (Case V) with the Bradley-Terry model. When might each be preferred?\n\nGOLD_ANSWER:\n4. **Model Comparison**:\n   - **Thurstone-Mosteller**: Assumes normality, handles correlations via $\\rho$, suitable for continuous latent traits.\n   - **Bradley-Terry**: Simpler (logistic), no correlation structure, easier with sparse data. Choose based on distributional assumptions and data structure.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain any information about the Bradley-Terry model. The given passages exclusively discuss the Thurstone-Mosteller model (Case V), including its assumptions (normally distributed latent variables), variance structure, and least-squares estimation method. Therefore, a comparison with the Bradley-Terry model cannot be made based on the provided context.\n\nQID: textbook-50-1-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-50-1-1-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the absence of information about the Bradley-Terry model.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-50-1-1-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the absence of information about the Bradley-Terry model."}, "llm_echoed_qid": "textbook-50-1-1-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the absence of information about the Bradley-Terry model."}
{"script_processing_qid": "textbook-122-16-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the usual OLS standard errors are invalid in the presence of serial correlation.\n\nGOLD_ANSWER:\nThe usual OLS standard errors assume no serial correlation ($\\rho = 0$). When $\\rho \\neq 0$, the variance formula includes additional terms due to autocorrelation in the errors. Ignoring these terms leads to biased standard errors, invalidating hypothesis tests and confidence intervals.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that OLS standard error validity requires the absence of serial correlation (errors are i.i.d.).\nStep 2: With serial correlation (e.g., AR(1) or MA(1) errors), error terms are no longer independent across observations.\nStep 3: The OLS variance formula assumes uncorrelated errors and computes $\\u0305Var(\\hat{\beta}_1) = \\u03c3^2/SST_x$, ignoring the additional covariance terms.\nStep 4: The true variance (e.g., under AR(1)) includes an extra term $2(\\u03c3^2/SST_x^2)\\sum_{t=1}^{n-1}\\sum_{j=1}^{n-t}\\u03c1^j x_t x_{t+j}$ due to correlated errors.\nStep 5: When $\\u03c1 > 0$ and $x_t$ is positively autocorrelated, this omitted term is positive, making the true variance larger than the OLS estimate.\nStep 6: Consequently, OLS underestimates standard errors, leading to inflated t-statistics and invalid hypothesis tests (Type I errors).\nStep 7: The issue persists asymptotically because the correlation structure in errors violates the required conditions for valid OLS inference.\n\nQID: textbook-122-16-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-16-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains why OLS standard errors are invalid in the presence of serial correlation, detailing the mathematical reasoning and consequences (e.g., underestimated standard errors, inflated t-statistics). It aligns with the gold answer, which also highlights the bias in standard errors due to ignored autocorrelation terms.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-16-0-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains why OLS standard errors are invalid in the presence of serial correlation, detailing the mathematical reasoning and consequences (e.g., underestimated standard errors, inflated t-statistics). It aligns with the gold answer, which also highlights the bias in standard errors due to ignored autocorrelation terms."}, "llm_echoed_qid": "textbook-122-16-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains why OLS standard errors are invalid in the presence of serial correlation, detailing the mathematical reasoning and consequences (e.g., underestimated standard errors, inflated t-statistics). It aligns with the gold answer, which also highlights the bias in standard errors due to ignored autocorrelation terms."}
{"script_processing_qid": "textbook-47-0-3-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Discuss the role of genetic algorithms in modeling learning behavior and their applications in economics.\n\nGOLD_ANSWER:\n1.  **Genetic Algorithms**: These are computational models inspired by natural selection, used to model adaptive learning. \n2.  **Applications**: Genetic algorithms have been used to study learning in asset pricing, exchange rate determination, and other economic contexts.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain enough information to answer this question in detail. While the section mentions genetic algorithms as one of the alternative approaches to learning in economics, the extracted text passages do not elaborate on their specific role in modeling learning behavior or their applications. The context primarily highlights the inclusion of genetic algorithms in a broader discussion of learning approaches and recent developments in macroeconomic modeling, but no further details are given.\n\nQID: textbook-47-0-3-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-47-0-3-1\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates that the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain enough information to answer this question in detail.'\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-47-0-3-1", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates that the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain enough information to answer this question in detail.'"}, "llm_echoed_qid": "textbook-47-0-3-1", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates that the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain enough information to answer this question in detail.'"}
{"script_processing_qid": "textbook-93-4-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the significance of the Martin exit boundary $R^{\\*} - R$ in the context of transient Markov chains. How does it relate to the representation of superregular vectors?\n\nGOLD_ANSWER:\n1. **Martin Boundary**: The Martin exit boundary $R^{\\*} - R$ captures the limiting behavior of transient Markov chains, analogous to boundary conditions in potential theory.\n2. **Representation**: Theorem 5.8 shows that any superregular vector $\\pmb{u}$ can be represented as an integral over the Martin boundary using the kernel $K(i,x)$, linking the behavior of $\\pmb{u}$ to the boundary.\n3. **Interpretation**: The measure $\\mu$ on the boundary encodes the 'weights' of different limit behaviors, providing a complete description of $\\pmb{u}$.\n\nCANDIDATE_ANSWER:\nStep 1: The Martin exit boundary R*−R represents distinct asymptotic exit paths from the transient Markov chain's state space R. It encodes different ways the chain can approach 'infinity' while remaining identifiable through boundary points.\n\nStep 2: In Theorem 5.8, the integral representation u(i) = ∫_{R*} K(i,x)μ(dx) shows superregular vectors are expressed as combinations of Martin kernel functions K(i,x) weighted by boundary measures μ. The exit boundary R*−R specifically serves as the domain for this integration.\n\nStep 3: The Martin kernel K(i,x) resolves transient behavior by encoding transition probabilities relative to a reference state as x approaches the boundary. This allows decomposing superregular vectors into contributions from different exit directions (R*−R), complementing the transient-potential decomposition in Theorem 5.6.\n\nStep 4: The connection lies in μ(dx) being supported on R*−R (Martin boundary points not in original state space R). This means superregular vectors' regular components (from Theorem 5.6's r = P^∞u) correspond to their boundary behavior through μ, while transient components are encoded in the potential part g.\n\nQID: textbook-93-4-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-93-4-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the significance of the Martin exit boundary and its relation to superregular vectors, aligning with the gold answer's key points about asymptotic behavior, integral representation, and measure encoding.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-93-4-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the significance of the Martin exit boundary and its relation to superregular vectors, aligning with the gold answer's key points about asymptotic behavior, integral representation, and measure encoding."}, "llm_echoed_qid": "textbook-93-4-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the significance of the Martin exit boundary and its relation to superregular vectors, aligning with the gold answer's key points about asymptotic behavior, integral representation, and measure encoding."}
{"script_processing_qid": "textbook-97-1-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the role of Hadamard matrices in constructing orthogonal arrays of strength 2 and 3. How does this relate to the entries in the first two columns of Table 12.1?\n\nGOLD_ANSWER:\n1. **Hadamard Matrices**: A Hadamard matrix of order $4\\lambda$ yields an $O A(4\\lambda, 4\\lambda-1, 2, 2)$.\n2. **Strength 3**: For strength $t=3$, the existence of Hadamard matrices implies exact minimal indices for any $k$ (assuming Hadamard matrices exist for all orders).\n3. **Table 12.1**: The first two columns (strengths 2 and 3) are special because the minimal index is known exactly, provided Hadamard matrices exist for all orders.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that Hadamard matrices are combinatorial structures used to build orthogonal arrays (OAs) with s=2 levels. A Hadamard matrix of order n (where n ≡ 0 mod 4) provides n-1 pairwise orthogonal columns, forming an OA with parameters OA(n, n-1, 2, 2). This corresponds to strength t=2.\n\nStep 2: For strength t=3, constructions often require codes with minimal distance d=4 (as t = d-1). Extended Hadamard codes derived from normalized Hadamard matrices can achieve this, producing OAs with parameters OA(2^{m}, 2^{m-1}, 2, 3) when using order-n=2^{m} Hadamard matrices.\n\nStep 3: In Table 12.1 (s=2), the first two columns (t=2 and t=3) likely include entries where λ = N/2^t. Hadamard-based OAs explain minimal λ values in entries where k ≈ 2^{m-1} factors are achieved with N=2^m runs. For example, OA(8,7,2,2) (λ=2) and OA(16,15,2,3) (λ=2) match Hadamard matrix constructions.\n\nStep 4: These constructions relate to the table entries by providing tight upper bounds for λ when k is one less than a power of two, leveraging the maximal orthogonal column property of Hadamard matrices to minimize the required number of runs N (= λ⋅2^t).\n\nQID: textbook-97-1-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-97-1-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately discusses the role of Hadamard matrices in constructing orthogonal arrays of strength 2 and 3, and correctly relates this to the entries in the first two columns of Table 12.1. The explanation aligns with the gold answer, providing detailed steps and reasoning that match the expected content.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-97-1-0-2", "category": "CORRECT", "explanation": "The candidate answer accurately discusses the role of Hadamard matrices in constructing orthogonal arrays of strength 2 and 3, and correctly relates this to the entries in the first two columns of Table 12.1. The explanation aligns with the gold answer, providing detailed steps and reasoning that match the expected content."}, "llm_echoed_qid": "textbook-97-1-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately discusses the role of Hadamard matrices in constructing orthogonal arrays of strength 2 and 3, and correctly relates this to the entries in the first two columns of Table 12.1. The explanation aligns with the gold answer, providing detailed steps and reasoning that match the expected content."}
{"script_processing_qid": "textbook-125-35-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n5. Derive the present value formula for an ordinary annuity from the difference between two perpetuities.\n\nGOLD_ANSWER:\n1.  **Perpetuity 1**: PV = $\\frac{C}{r}$ (payments from year 1 to $\\infty$).\n2.  **Perpetuity 2**: PV = $\\frac{C}{r(1 + r)^t}$ (payments from year $t + 1$ to $\\infty$).\n3.  **Difference**: Annuity PV = $\\frac{C}{r} - \\frac{C}{r(1 + r)^t} = C\\left[\\frac{1}{r} - \\frac{1}{r(1 + r)^t}\\right]$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that the present value (PV) of a perpetuity with payment $C$ starting in period 1 is $\\frac{C}{r}$.\\nStep 2: Consider a second perpetuity with payment $C$ that starts in period $t+1$ (delayed by $t$ periods). Its PV today is $\\frac{C}{r} \\times \\frac{1}{(1+r)^t}$.\\nStep 3: The difference between the two perpetuities represents an annuity that pays $C$ for $t$ periods:\\n$\\text{PV}_{\\text{annuity}} = \\text{PV}_{\\text{perpetuity 1}} - \\text{PV}_{\\text{perpetuity 2}} = \\frac{C}{r} - \\frac{C}{r(1+r)^t}$.\\nStep 4: Factor out $\\frac{C}{r}$ to obtain the standard annuity formula:\\n$\\mathrm{PV} = C\\left[\\frac{1}{r} - \\frac{1}{r(1+r)^t}\\right]$.\n\nQID: textbook-125-35-2-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-35-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the present value formula for an ordinary annuity by subtracting the present value of a delayed perpetuity from an immediate perpetuity, matching the gold answer step-by-step.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-35-2-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the present value formula for an ordinary annuity by subtracting the present value of a delayed perpetuity from an immediate perpetuity, matching the gold answer step-by-step."}, "llm_echoed_qid": "textbook-125-35-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the present value formula for an ordinary annuity by subtracting the present value of a delayed perpetuity from an immediate perpetuity, matching the gold answer step-by-step."}
{"script_processing_qid": "textbook-125-23-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the formula for the present value of a zero-coupon bond using the DCF method.\n\nGOLD_ANSWER:\n1. **Formula**: The present value (PV) of a zero-coupon bond is given by:\n   $$ PV = \\frac{F}{(1 + r)^T} $$\n   where:\n   - $F$ is the face value of the bond.\n   - $r$ is the discount rate.\n   - $T$ is the time to maturity.\n2. **Explanation**: Since a zero-coupon bond pays no interest, its value is solely the present value of its face value discounted at the appropriate rate.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the key characteristics of a zero-coupon bond from the context. It pays no interest (zero coupons) and returns principal (face value) at maturity.\n\nStep 2: Apply the Discounted Cash Flow (DCF) principle stated in the context: present value = future cash flow × discount factor.\n\nStep 3: For a zero-coupon bond with face value FV maturing in T years and discount rate r:\n   - Future cash flow = FV (principal repayment)\n   - Discount factor = 1 / (1 + r)^T\n\nStep 4: Combine components using DCF formula:\nPresent Value (PV) = FV × [1 / (1 + r)^T] = FV / (1 + r)^T\n\nThis derivation uses only the provided context: DCF methodology definition and the zero-coupon bond description as a pure discount bond with no interim payments.\n\nQID: textbook-125-23-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-23-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the present value formula for a zero-coupon bond using the DCF method, matching the gold answer in both formula and explanation.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-23-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the present value formula for a zero-coupon bond using the DCF method, matching the gold answer in both formula and explanation."}, "llm_echoed_qid": "textbook-125-23-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the present value formula for a zero-coupon bond using the DCF method, matching the gold answer in both formula and explanation."}
{"script_processing_qid": "textbook-119-7-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the error sum of squares (RSS) for the exponential regression model $Y_i = \\beta_1 e^{\\beta_2 X_i} + u_i$ and show how it leads to the normal equations (14.2.4) and (14.2.5).\n\nGOLD_ANSWER:\n1. **RSS**: $\\sum u_i^2 = \\sum (Y_i - \\beta_1 e^{\\beta_2 X_i})^2$.\n2. **First-Order Conditions**: \n   - $\\frac{\\partial \\sum u_i^2}{\\partial \\beta_1} = -2 \\sum (Y_i - \\beta_1 e^{\\beta_2 X_i}) e^{\\beta_2 X_i} = 0$.\n   - $\\frac{\\partial \\sum u_i^2}{\\partial \\beta_2} = -2 \\sum (Y_i - \\beta_1 e^{\\beta_2 X_i}) \\beta_1 X_i e^{\\beta_2 X_i} = 0$.\n3. **Normal Equations**: Rearranging yields Eqs. (14.2.4) and (14.2.5).\n\nCANDIDATE_ANSWER:\nStep 1: Define the error sum of squares (RSS) for the model:\n\nRSS = Σ[Y_i - β₁e^{β₂X_i}]²\n\nStep 2: Compute partial derivative of RSS with respect to β₁:\n\n∂RSS/∂β₁ = -2Σ[Y_i - β₁e^{β₂X_i}](e^{β₂X_i})\n\nSetting ∂RSS/∂β₁ = 0 gives normal equation (14.2.4):\nΣY_i e^{β₂X_i} = β₁Σe^{2β₂X_i}\n\nStep 3: Compute partial derivative of RSS with respect to β₂:\n\n∂RSS/∂β₂ = -2β₁Σ[Y_i - β₁e^{β₂X_i}](X_i e^{β₂X_i})\n\nSetting ∂RSS/∂β₂ = 0 gives normal equation (14.2.5):\nΣY_i X_i e^{β₂X_i} = β₁ΣX_i e^{2β₂X_i}\n\nThese equations are nonlinear in parameters (containing β₁ and β₂ in exponential terms), requiring iterative methods to solve.\n\nQID: textbook-119-7-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-7-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derived the RSS, computed the partial derivatives with respect to β₁ and β₂, and derived the normal equations (14.2.4) and (14.2.5) as specified in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-7-1-0", "category": "CORRECT", "explanation": "The candidate correctly derived the RSS, computed the partial derivatives with respect to β₁ and β₂, and derived the normal equations (14.2.4) and (14.2.5) as specified in the gold answer."}, "llm_echoed_qid": "textbook-119-7-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derived the RSS, computed the partial derivatives with respect to β₁ and β₂, and derived the normal equations (14.2.4) and (14.2.5) as specified in the gold answer."}
{"script_processing_qid": "textbook-117-24-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the net payoff for the buyer and seller in a forward contract if the spot price at delivery is $S_T$ and the forward price is $F_0$.\n\nGOLD_ANSWER:\n1. **Buyer's Payoff**: $S_T - F_0$ (positive if $S_T > F_0$).  \n2. **Seller's Payoff**: $F_0 - S_T$ (positive if $S_T < F_0$).  \n3. **Net Payoff**: The sum of buyer and seller payoffs is zero, reflecting the zero-sum nature of forward contracts.\n\nCANDIDATE_ANSWER:\nStep 1: Understand forward contract obligations\nThe buyer is obligated to pay the forward price (F₀) at delivery, while the seller is obligated to deliver the asset.\n\nStep 2: Calculate buyer's net payoff\nAt delivery, the asset's market value equals the spot price (S_T). The buyer gains from price increases:\nBuyer Payoff = Asset Value Received − Payment Made\n= S_T − F₀\n\nStep 3: Calculate seller's net payoff\nThe seller loses potential gains from price increases but is protected from price declines:\nSeller Payoff = Payment Received − Asset Value Delivered\n= F₀ − S_T\n\nStep 4: Final results\n• Buyer's net payoff: S_T − F₀\n• Seller's net payoff: F₀ − S_T\nThese payoffs are symmetrical opposites, reflecting the zero-sum nature of forward contracts.\n\nQID: textbook-117-24-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-24-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the payoffs for both the buyer and seller in a forward contract, matching the gold answer in both calculations and explanation of the zero-sum nature of the contract.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-24-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the payoffs for both the buyer and seller in a forward contract, matching the gold answer in both calculations and explanation of the zero-sum nature of the contract."}, "llm_echoed_qid": "textbook-117-24-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the payoffs for both the buyer and seller in a forward contract, matching the gold answer in both calculations and explanation of the zero-sum nature of the contract."}
{"script_processing_qid": "textbook-100-3-0-4", "llm_grader_input_prompt_user": "\nQUESTION:\n5. Prove that the $\\Gamma$-minimax estimator minimizes the maximum Bayes risk over the class $\\Gamma$.\n\nGOLD_ANSWER:\n1. **Definition**: $\\delta_0$ is $\\Gamma$-minimax if $\\sup_{G\\in\\Gamma}r(G,\\delta_0) = \\inf_{\\delta}\\sup_{G\\in\\Gamma}r(G,\\delta)$.\n2. **Interpretation**: $\\delta_0$ has the smallest worst-case Bayes risk over $\\Gamma$.\n3. **Example**: For $X\\sim{\\mathcal{B}}(1,\\theta)$, $\\widehat{\\theta}=(1+2X)/4$ is $\\Gamma$-minimax for priors with mean $1/2$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the definition of Γ-minimaxity from the context. A decision rule δ₀ is Γ-minimax if sup_{G∈Γ} r(G,δ₀) = inf_δ sup_{G∈Γ} r(G,δ).\n\nStep 2: Recognize that the left side represents the maximum Bayes risk of δ₀ over Γ, while the right side represents the minimal possible maximum Bayes risk achievable by any decision rule.\n\nStep 3: By definition, δ₀ achieves equality between these two quantities. This means δ₀'s worst-case Bayes risk across Γ (sup_{G∈Γ} r(G,δ₀)) exactly equals the smallest possible worst-case risk that any estimator could achieve (inf_δ sup_{G∈Γ} r(G,δ)).\n\nStep 4: Therefore, δ₀ must be minimizing the maximum Bayes risk over Γ because no other decision rule δ can produce a smaller supremum Bayes risk across Γ than δ₀'s supremum risk.\n\nConclusion: The equality in Definition 3.3 directly establishes that the Γ-minimax estimator δ₀ minimizes the maximum Bayes risk over Γ by construction.\n\nQID: textbook-100-3-0-4\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-100-3-0-4\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly follows the definition and interpretation provided in the gold answer, clearly explaining how the Γ-minimax estimator minimizes the maximum Bayes risk over Γ.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-100-3-0-4", "category": "CORRECT", "explanation": "The candidate answer correctly follows the definition and interpretation provided in the gold answer, clearly explaining how the Γ-minimax estimator minimizes the maximum Bayes risk over Γ."}, "llm_echoed_qid": "textbook-100-3-0-4", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly follows the definition and interpretation provided in the gold answer, clearly explaining how the Γ-minimax estimator minimizes the maximum Bayes risk over Γ."}
{"script_processing_qid": "textbook-119-7-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Critically evaluate the trial-and-error method for estimating nonlinear regression models, discussing its limitations and potential alternatives.\n\nGOLD_ANSWER:\n1. **Limitations**: \n   - Computationally expensive for high-dimensional parameter spaces.\n   - No guarantee of convergence to a global minimum (risk of local minima).\n   - Requires ad hoc initial guesses.\n2. **Alternatives**: \n   - **Direct Optimization**: Uses gradient-based methods (e.g., steepest descent) but may converge slowly.\n   - **Iterative Linearization**: Linearizes the model iteratively (e.g., Gauss-Newton algorithm) for efficient parameter updates.\n\nCANDIDATE_ANSWER:\nStep 1: Understanding Trial-and-Error Method\nTrial-and-error involves manually testing parameter values to minimize residuals in nonlinear models. It is often used when no closed-form solution exists (e.g., Cobb–Douglas function in Eq. 14.1.4, which cannot be linearized).\n\nStep 2: Limitations of Trial-and-Error\n1. **Computational Inefficiency**: Requires exhaustive iterations, especially for models with multiple parameters (e.g., estimating β₁, β₂, β₃ in Eq. 14.1.4).\n2. **Convergence Issues**: No guarantee of finding global minima; may stall at local minima.\n3. **Sensitivity to Initial Guesses**: Results heavily depend on starting parameter values.\n4. **Lack of Precision**: Manual adjustments may miss optimal parameter combinations.\n\nStep 3: Alternatives to Trial-and-Error\n1. **Numerical Optimization Algorithms**: Use iterative methods like Gauss-Newton or Levenberg-Marquardt for systematic parameter estimation.\n2. **Linearization When Possible**: For intrinsically linear models (e.g., Eq. 14.1.3a), apply transformations (e.g., logarithms) to use OLS.\n3. **Maximum Likelihood Estimation**: For models with specific error structures, use MLE with gradient-based optimization.\n\nStep 4: Critical Evaluation\nTrial-and-error is impractical for complex nonlinear models (e.g., Eq. 14.1.4) due to inefficiency and unreliability. Alternatives like numerical optimization or leveraging linearizable forms (e.g., Eq. 14.1.2a) are preferred. However, when linearization is impossible (intrinsically nonlinear cases), systematic numerical methods remain the only viable option.\n\nQID: textbook-119-7-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-7-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the limitations of the trial-and-error method and provides detailed alternatives, aligning well with the gold answer. The response is comprehensive and accurate.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-7-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the limitations of the trial-and-error method and provides detailed alternatives, aligning well with the gold answer. The response is comprehensive and accurate."}, "llm_echoed_qid": "textbook-119-7-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the limitations of the trial-and-error method and provides detailed alternatives, aligning well with the gold answer. The response is comprehensive and accurate."}
{"script_processing_qid": "textbook-65-2-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain the role of the compact subset $S$ in deriving uniform convergence results for the kernel estimates.\n\nGOLD_ANSWER:\nThe compact subset $S$ ensures that the supremum over $y$ is well-defined and finite. Compactness allows the use of finite coverings and uniform continuity arguments, which are essential for deriving uniform rates of convergence. Without compactness, the behavior of the estimates at infinity could lead to unbounded errors.\n\nCANDIDATE_ANSWER:\nStep 1: The compact subset $S$ restricts the domain of $y$ to a closed and bounded set. This ensures the conditional c.d.f. $F_Y^\\chi(y)$ and density $f_Y^\\chi(y)$ are sufficiently smooth and their estimators $\\widehat{F}_Y^\\chi(y)$, $\\widehat{f}_Y^\\chi(y)$ remain well-behaved over $S$.\n\nStep 2: Uniform convergence results (with $\\sup_{y \\in S}$) require controlling bias and stochastic error terms globally over $S$. Compactness guarantees $\\varphi_\\chi(h)$ (likely the marginal density of $\\chi$) is bounded away from zero on $S$, preventing degenerate denominators in the stochastic error terms $\\left(\\frac{\\log n}{n g\\varphi_\\chi(h)}\\right)^{1/2}$.\n\nStep 3: Compactness enables the use of covering number/entropy arguments from empirical process theory to bound suprema over $S$. This ensures uniformity in the convergence rates $O(h^\\beta + g^\\beta)$ and $O_{a.co.}(\\cdot)$ across all $y \\in S$.\n\nStep 4: Without $S$, uniform convergence over unbounded domains might fail due to tail behavior or regions where $\\varphi_\\chi(h)$ becomes too small, making the error terms uncontrollable.\n\nQID: textbook-65-2-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-65-2-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the role of the compact subset $S$ in ensuring well-defined suprema, controlling bias and stochastic errors, and enabling uniform convergence arguments, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-65-2-2-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the role of the compact subset $S$ in ensuring well-defined suprema, controlling bias and stochastic errors, and enabling uniform convergence arguments, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-65-2-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the role of the compact subset $S$ in ensuring well-defined suprema, controlling bias and stochastic errors, and enabling uniform convergence arguments, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-120-23-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the Lipschitz constant for the function $F_{L}$ associated with a cdf $F$ and prove it is non-decreasing.\n\nGOLD_ANSWER:\n1. **Lipschitz Condition**: For any $c, c' \\in \\mathbb{R}$, $|F_{L}(c) - F_{L}(c')| \\leq |c - c'|$ because the slope of the line intersecting $\\hat{F}$ is $-1$.\\n2. **Monotonicity**: If $c \\leq c'$, the intersection point $(x_{c}, y_{c})$ must satisfy $y_{c} \\leq y_{c'}$ due to the non-decreasing nature of $F$ and the definition of $\\hat{F}$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the construction of F_L\n- For each c ∈ ℝ, F_L(c) = y_c where (x_c, y_c) is the intersection point between the line x + y = c and the graph of the cdf F\n- This satisfies x_c + F_L(c) = c ⇒ F_L(c) = c - x_c\n\nStep 2: Analyze increment relationships\nFor c' > c:\n1. x_{c'} + F_L(c') = c'\n2. x_c + F_L(c) = c\nSubtracting gives: (x_{c'} - x_c) + (F_L(c') - F_L(c)) = c' - c\n\nStep 3: Establish monotonicity\n- Since increasing c requires moving the line x + y = c to the right, and F is non-decreasing:\n  x_{c'} ≥ x_c (x coordinate must increase or stay constant)\n  F_L(c') = F(x_{c'}) ≥ F(x_c) = F_L(c) (F is non-decreasing)\n\nStep 4: Derive Lipschitz bound\nFrom Step 2 equation:\nF_L(c') - F_L(c) = (c' - c) - (x_{c'} - x_c)\nSince x_{c'} - x_c ≥ 0 (from Step 3):\nF_L(c') - F_L(c) ≤ c' - c\nNon-decreasing property from Step 3 gives F_L(c') - F_L(c) ≥ 0\nHence |F_L(c') - F_L(c)| ≤ |c' - c|, proving Lipschitz constant ≤ 1\n\nQID: textbook-120-23-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-120-23-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the Lipschitz constant for the function $F_{L}$ and proves its non-decreasing nature, aligning with the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-120-23-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the Lipschitz constant for the function $F_{L}$ and proves its non-decreasing nature, aligning with the gold answer's key points."}, "llm_echoed_qid": "textbook-120-23-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the Lipschitz constant for the function $F_{L}$ and proves its non-decreasing nature, aligning with the gold answer's key points."}
{"script_processing_qid": "textbook-117-11-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. (Quantitative) Using the data from Warner's study, calculate the expected cost of bankruptcy for a firm with a 5% annual probability of bankruptcy and a lump-sum penalty of 3% of its current market value.\n\nGOLD_ANSWER:\n1. **Calculation**: Expected cost of bankruptcy = Probability of bankruptcy × Penalty cost.\n2. **Substitute values**: Expected cost = 0.05 × 0.03 = 0.0015 (or 0.15% of the firm's current market value).\n\nCANDIDATE_ANSWER:\nStep 1: Identify the formula for expected cost. The expected cost of bankruptcy is calculated as: (Probability of bankruptcy) × (Bankruptcy penalty).\n\nStep 2: Apply the given values. For this firm:\n- Annual probability of bankruptcy = 5% (0.05)\n- Lump-sum penalty = 3% of market value (0.03)\n\nStep 3: Calculate:\n0.05 (probability) × 0.03 (penalty) = 0.0015, or 0.15% of the firm’s current market value.\n\nResult: The expected cost of bankruptcy is 0.15% of the firm’s current market value.\n\nQID: textbook-117-11-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-11-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to calculate the expected cost of bankruptcy, using the correct formula and substituting the given values accurately, resulting in the same answer as the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-11-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to calculate the expected cost of bankruptcy, using the correct formula and substituting the given values accurately, resulting in the same answer as the gold answer."}, "llm_echoed_qid": "textbook-117-11-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to calculate the expected cost of bankruptcy, using the correct formula and substituting the given values accurately, resulting in the same answer as the gold answer."}
{"script_processing_qid": "textbook-99-4-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the trade-off between bias and variance in the context of histogram bin width selection.\n\nGOLD_ANSWER:\n1. **Bias**: A larger bin width $h$ increases bias because the histogram oversmooths the data, failing to capture fine structure.\n2. **Variance**: A smaller bin width $h$ increases variance because the histogram becomes more sensitive to random fluctuations in the data.\n3. **Trade-off**: Optimal bin width minimizes the mean integrated squared error (MISE), balancing bias and variance.\n\nCANDIDATE_ANSWER:\nStep 1: **Bias-Variance Trade-off Basics**\nThe choice of bin width (h) in a histogram directly impacts the estimator's bias and variance. Narrow bins (small h) reduce bias but increase variance, while wide bins (large h) reduce variance but increase bias.\n\nStep 2: **Effect of Small h (Undersmoothing)**\n- Narrow bins capture fine details of the true density, reducing bias.\n- However, fewer observations fall into each bin, making the estimate sensitive to random sampling fluctuations (high variance).\n- This creates a 'wiggly' histogram with artificial modes.\n\nStep 3: **Effect of Large h (Oversmoothing)**\n- Wide bins average over more observations, stabilizing the estimate (low variance).\n- But they obscure local structure and true modes of the density, increasing bias.\n- This results in a smooth but oversimplified representation.\n\nStep 4: **Optimal Bin Width Selection**\nThe goal is to balance these effects:\n- Too small h: Low bias but high variance (overfitting noise).\n- Too large h: High bias but low variance (underfitting structure).\nThe ideal h minimizes the mean integrated squared error (MISE), which combines squared bias and variance.\n\nQID: textbook-99-4-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-99-4-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the trade-off between bias and variance in histogram bin width selection, aligning with the gold answer. It correctly describes the effects of small and large bin widths and the goal of minimizing MISE.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-4-1-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the trade-off between bias and variance in histogram bin width selection, aligning with the gold answer. It correctly describes the effects of small and large bin widths and the goal of minimizing MISE."}, "llm_echoed_qid": "textbook-99-4-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the trade-off between bias and variance in histogram bin width selection, aligning with the gold answer. It correctly describes the effects of small and large bin widths and the goal of minimizing MISE."}
{"script_processing_qid": "textbook-111-14-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the challenges and limitations of GDP as a measure of economic activity. Include examples of imputed values and the underground economy in your answer.\n\nGOLD_ANSWER:\n1. **Imputed Values**: GDP includes estimates like owner-occupied rent, which may not reflect actual market transactions.\n2. **Underground Economy**: Activities like illegal drug trade or unreported labor are excluded, understating GDP.\n3. **Limitations**: GDP omits non-market activities (e.g., household work) and may not accurately reflect well-being or income distribution.\n\nCANDIDATE_ANSWER:\nStep 1: The provided context defines GDP as the market value of all final goods and services produced within an economy in a given period and explains its measurement via income/expenditure flows. However, it does not explicitly discuss challenges or limitations of GDP. Step 2: The context makes no mention of imputed values (e.g., owner-occupied housing or unpaid labor) or the underground economy (e.g., unreported cash transactions or illegal activities), which are common limitations of GDP. Step 3: Since the question requires analysis of these specific limitations and the provided text lacks relevant details, the context is insufficient to answer the query.\n\nQID: textbook-111-14-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-111-14-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states the provided context does not contain sufficient information.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-14-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states the provided context does not contain sufficient information."}, "llm_echoed_qid": "textbook-111-14-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states the provided context does not contain sufficient information."}
{"script_processing_qid": "textbook-117-43-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. What are the primary reasons for holding cash as outlined in corporate finance?\n\nGOLD_ANSWER:\n1. **Transactions Motive**: Firms hold cash to meet day-to-day operational expenses and transactions.\n2. **Precautionary Motive**: Cash is held as a buffer against unforeseen financial needs or emergencies.\n3. **Speculative Motive**: Firms may hold cash to take advantage of unexpected investment opportunities or favorable market conditions.\n\nCANDIDATE_ANSWER:\nStep 1: The primary reasons for holding cash in corporate finance are categorized into three classic motives. Step 2: The transactions motive ensures a company has sufficient liquidity to meet day-to-day operational expenses (e.g., payroll, supplier payments). Step 3: The precautionary motive involves holding cash reserves to buffer against unexpected financial shocks (e.g., economic downturns, sudden expenses). Step 4: The speculative motive allows companies to capitalize on strategic opportunities (e.g., acquisitions, discounted asset purchases) when they arise. These three motives collectively explain why firms maintain cash balances.\n\nQID: textbook-117-43-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-43-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately identifies and elaborates on the three primary motives for holding cash in corporate finance, matching the gold answer in both content and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-43-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately identifies and elaborates on the three primary motives for holding cash in corporate finance, matching the gold answer in both content and reasoning."}, "llm_echoed_qid": "textbook-117-43-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately identifies and elaborates on the three primary motives for holding cash in corporate finance, matching the gold answer in both content and reasoning."}
{"script_processing_qid": "textbook-111-16-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Describe how the Federal Reserve uses open-market operations to influence the money supply.\n\nGOLD_ANSWER:\n1. **Increasing Money Supply**: The Fed buys government bonds from the public, injecting dollars into the economy.\n2. **Decreasing Money Supply**: The Fed sells government bonds to the public, withdrawing dollars from circulation.\n   **Impact**: These actions adjust the quantity of money available, influencing interest rates and economic activity.\n\nCANDIDATE_ANSWER:\nStep 1: The Federal Reserve conducts open-market operations by buying or selling government bonds in the financial market. Step 2: When the Fed buys government bonds, it injects new money into the banking system, increasing banks' reserves. This enables banks to make more loans, expanding the money supply through the money multiplier effect. Step 3: Conversely, when the Fed sells government bonds, it withdraws money from circulation, reducing banks' reserves and limiting their ability to lend, thereby contracting the money supply.\n\nQID: textbook-111-16-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-111-16-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes how the Federal Reserve uses open-market operations to influence the money supply, including the mechanisms of buying and selling government bonds and their effects on bank reserves and lending. This aligns perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-16-1-2", "category": "CORRECT", "explanation": "The candidate's answer accurately describes how the Federal Reserve uses open-market operations to influence the money supply, including the mechanisms of buying and selling government bonds and their effects on bank reserves and lending. This aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-111-16-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes how the Federal Reserve uses open-market operations to influence the money supply, including the mechanisms of buying and selling government bonds and their effects on bank reserves and lending. This aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-48-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the intertemporal utility function for generation $t$ in Barro's model of intergenerational altruism, showing how it depends on the consumption of all future generations.\n\nGOLD_ANSWER:\n1.  **Given**: Barro's model specifies the utility of generation $t$ as $V\\_{t} = U(C\\_{t}) + \\beta V\\_{t+1}$.\n2.  **Recursive Substitution**: Substitute $V\\_{t+1} = U(C\\_{t+1}) + \\beta V\\_{t+2}$ into the original equation: $V\\_{t} = U(C\\_{t}) + \\beta [U(C\\_{t+1}) + \\beta V\\_{t+2}]$.\n3.  **Repeat**: Continue substituting recursively for $V\\_{t+2}, V\\_{t+3}, \\ldots$ to obtain the infinite series: $$ V\\_{t} = U(C\\_{t}) + \\beta U(C\\_{t+1}) + \\beta^{2} U(C\\_{t+2}) + \\beta^{3} U(C\\_{t+3}) + \\cdot\\cdot\\cdot $$.\n4.  **Interpretation**: This shows that the utility of generation $t$ depends on its own consumption and the discounted utility of all future generations' consumption.\n\nCANDIDATE_ANSWER:\nStep 1: Start with Barro's recursive utility definition for generation t:\nV_t = U(C_t) + β V_{t+1}\n\nStep 2: Substitute the utility of generation t+1 using the same recursive formula:\nV_t = U(C_t) + β [U(C_{t+1}) + β V_{t+2}]\n\nStep 3: Expand the expression:\nV_t = U(C_t) + β U(C_{t+1}) + β² V_{t+2}\n\nStep 4: Substitute generation t+2's utility:\nV_t = U(C_t) + β U(C_{t+1}) + β² [U(C_{t+2}) + β V_{t+3}]\n\nStep 5: Continue this recursive substitution indefinitely:\nV_t = U(C_t) + β U(C_{t+1}) + β² U(C_{t+2}) + β³ U(C_{t+3}) + ...\n\nStep 6: Recognize this forms an infinite geometric series where utility depends on all future generations' consumption:\nV_t = Σ_{k=0}^∞ β^k U(C_{t+k})\n\nThis demonstrates that each generation's utility function incorporates the discounted utility of all subsequent generations through perpetual intergenerational altruism, with weights β^k declining geometrically over time.\n\nQID: textbook-48-1-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-48-1-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately follows the steps to derive the intertemporal utility function in Barro's model, correctly showing the recursive substitution and the resulting infinite series. The explanation matches the gold answer in both content and interpretation.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-48-1-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately follows the steps to derive the intertemporal utility function in Barro's model, correctly showing the recursive substitution and the resulting infinite series. The explanation matches the gold answer in both content and interpretation."}, "llm_echoed_qid": "textbook-48-1-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately follows the steps to derive the intertemporal utility function in Barro's model, correctly showing the recursive substitution and the resulting infinite series. The explanation matches the gold answer in both content and interpretation."}
{"script_processing_qid": "textbook-93-3-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Prove that weak ergodicity for forward products $T\\_{p,r}$ is equivalent to the divergence of the series $\\sum\\_{s=0}^{\\infty}[\\phi(H\\_{k\\_{s},k\\_{s+1}-k\\_{s}})]^{1/2} = \\infty$ for some strictly increasing sequence $\\{k\\_{s}\\}$.\n\nGOLD_ANSWER:\n1. **Sufficiency**: If the series diverges, then $\\tau\\_{B}(T\\_{p,r}) \\rightarrow 0$ as $r \\rightarrow \\infty$, implying weak ergodicity.\n2. **Necessity**: If weak ergodicity holds, then $\\tau\\_{B}(T\\_{p,r}) \\rightarrow 0$, and we can construct a sequence $\\{k\\_{s}\\}$ such that the series diverges.\n3. **Key Step**: Use the property $\\tau\\_{B}(T\\_{p,r}) \\leq \\prod\\_{s=0}^{t-1} \\tau\\_{B}(T\\_{k\\_{s},k\\_{s+1}-k\\_{s}})$ and the divergence condition.\n\nCANDIDATE_ANSWER:\nStep 1: Recall from Theorem 3.2 that weak ergodicity for forward products $T_{p,r}$ occurs if and only if there exists a strictly increasing sequence $\\{k_s\\}$ such that $\\sum_{s=0}^\\infty [\\phi(H_{k_s,k_{s+1}-k_s})]^{1/2} = \\infty$.\n\nStep 2: Weak ergodicity is formally defined as $\\tau_B(T_{p,r}) \\to 0$ as $r \\to \\infty$ for each $p \\geq 0$ (implied by Lemma 3.4), which manifests as rows of $T_{p,r}$ becoming proportional.\n\nStep 3: The Birkhoff contraction coefficient $\\phi(H_{k_s,\\Delta_s})$ (where $\\Delta_s = k_{s+1}-k_s$) measures the contractive properties of matrix blocks. The square root transformation preserves divergence/convergence properties.\n\nStep 4: Divergence of $\\sum [\\phi(H_{k_s,\\Delta_s})]^{1/2}$ guarantees infinitely many sufficiently mixing blocks in the product sequence, forcing $\\tau_B(T_{p,r}) \\to 0$.\n\nStep 5: Conversely, if weak ergodicity holds, convergence of the series $\\sum [\\phi(H_{k_s,\\Delta_s})]^{1/2}$ would contradict the requirement for persistent contraction (via properties of allowable matrices and Theorem 3.3's boundedness conditions).\n\nThus, the equivalence follows directly from Theorem 3.2's characterization of weak ergodicity through the divergence condition on partial products' contraction coefficients.\n\nQID: textbook-93-3-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-93-3-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses both sufficiency and necessity, aligning with the gold answer's key points. The reasoning is detailed and correctly invokes relevant theorems and properties to establish the equivalence.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-93-3-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses both sufficiency and necessity, aligning with the gold answer's key points. The reasoning is detailed and correctly invokes relevant theorems and properties to establish the equivalence."}, "llm_echoed_qid": "textbook-93-3-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses both sufficiency and necessity, aligning with the gold answer's key points. The reasoning is detailed and correctly invokes relevant theorems and properties to establish the equivalence."}
{"script_processing_qid": "textbook-39-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the implications of MA(1) residuals for instrument choice and standard error computation in IV estimation of the Euler equation.\n\nGOLD_ANSWER:\n1. **Instrument Choice**: MA(1) residuals imply that instruments must be lagged at least two periods to avoid correlation with residuals (e.g., $Z_{t-2}$).\n2. **Standard Errors**: The variance-covariance matrix of residuals must account for the MA(1) structure, requiring HAC (heteroscedasticity and autocorrelation consistent) estimators or GLS.\n3. **Efficiency**: GLS can improve efficiency by weighting observations inversely to their residual variance, but care is needed to avoid inconsistency from instrument-residual correlation.\n\nCANDIDATE_ANSWER:\nStep 1: Implications for Instrument Choice\n- MA(1) residuals imply serial correlation in the error term up to one lag. To ensure instrument validity in IV estimation, instruments must be uncorrelated with both current and lagged residuals (due to the MA(1) structure).\n- The context suggests using instruments lagged by an extra period (beyond standard requirements). This avoids correlation with the MA(1) residual structure, as synthetic cohort data are constructed from independent cross-sections. For example, if the residual at time 𝑡 is correlated with the residual at 𝑡−1 (MA(1)), instruments dated 𝑡−2 or earlier remain valid.\n\nStep 2: Implications for Standard Error Computation\n- Standard errors must account for MA(1) autocorrelation and cross-cohort correlation in expectational errors. Ordinary standard errors (assuming i.i.d. residuals) would be biased downward, leading to overstated significance.\n- Adjustments include:\n  a) Using heteroskedasticity- and autocorrelation-consistent (HAC) covariance matrix estimators (e.g., Newey-West) with a lag length ≥1.\n  b) GLS-type filtering to improve efficiency, but only if implemented carefully to avoid introducing correlation between filtered residuals and instruments (e.g., avoiding forward-looking information leaks).\n- The context warns that improper filtering (e.g., using lagged residuals correlated with retained instruments) could reintroduce endogeneity and inconsistency.\n\nQID: textbook-39-0-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-39-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately addresses both instrument choice and standard error computation in the context of MA(1) residuals. It correctly emphasizes the need for instruments to be lagged sufficiently to avoid correlation with residuals and the necessity of using HAC estimators or GLS for standard errors, aligning closely with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-39-0-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately addresses both instrument choice and standard error computation in the context of MA(1) residuals. It correctly emphasizes the need for instruments to be lagged sufficiently to avoid correlation with residuals and the necessity of using HAC estimators or GLS for standard errors, aligning closely with the gold answer."}, "llm_echoed_qid": "textbook-39-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately addresses both instrument choice and standard error computation in the context of MA(1) residuals. It correctly emphasizes the need for instruments to be lagged sufficiently to avoid correlation with residuals and the necessity of using HAC estimators or GLS for standard errors, aligning closely with the gold answer."}
{"script_processing_qid": "textbook-117-24-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Quantitatively demonstrate how marking to market ensures the net price for a futures contract buyer equals the original contract price. Use the wheat example (Thursday close: $\\$4.07$, Friday close: $\\$4.05$, Monday close: $\\$4.12$).\n\nGOLD_ANSWER:\n1. **Thursday**: Buyer enters at $\\$4.07$.  \n2. **Friday**: Price drops to $\\$4.05$. Buyer pays $\\$0.02$ to clearinghouse.  \n3. **Monday**: Price rises to $\\$4.12$. Buyer receives $\\$0.07$ from clearinghouse.  \n4. **Delivery**: Buyer pays $\\$4.12$ for wheat.  \n5. **Net Payment**: $-\\$0.02 + \\$0.07 - \\$4.12 = -\\$4.07$, matching the original contract price.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate daily price differences\n- Thursday close (contract initiation): $4.07 (original price)\n- Friday close: $4.05 ⇒ Daily change = $4.05 - $4.07 = -$0.02 (buyer pays $0.02 to seller)\n- Monday close: $4.12 ⇒ Daily change = $4.12 - $4.05 = +$0.07 (seller pays $0.07 to buyer)\n\nStep 2: Aggregate cash flows for buyer\nTotal cash flow = (-$0.02) + (+$0.07) = +$0.05\n\nStep 3: Compute net effective price\nFinal delivery price = $4.12 (Monday close)\nNet price paid = Final price - Total cash flow\n= $4.12 - $0.05 = $4.07\n\nConclusion: Through daily settlement of gains/losses (marking to market), the buyer ultimately pays the original contract price of $4.07 despite interim price fluctuations.\n\nQID: textbook-117-24-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-24-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly demonstrates how marking to market ensures the net price equals the original contract price, with accurate calculations and clear reasoning that matches the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-24-1-1", "category": "CORRECT", "explanation": "The candidate's answer correctly demonstrates how marking to market ensures the net price equals the original contract price, with accurate calculations and clear reasoning that matches the gold answer."}, "llm_echoed_qid": "textbook-117-24-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly demonstrates how marking to market ensures the net price equals the original contract price, with accurate calculations and clear reasoning that matches the gold answer."}
{"script_processing_qid": "textbook-111-23-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the equilibrium conditions for the Mundell-Fleming model under floating exchange rates when taxes increase. Show the shifts in the $I S^{\\*}$ and $L M^{\\*}$ curves and explain the resulting changes in income, exchange rate, and trade balance.\n\nGOLD_ANSWER:\n1. **Initial Equilibrium**: $Y = C(Y - T) + I(r^{\\*}) + G + NX(e)$, $M/P = L(r^{\\*}, Y)$, $r = r^{\\*}$.\\n2. **Tax Increase**: Shifts $I S^{\\*}$ left (lower $C$).\\n3. **Floating Exchange Rates**: $L M^{\\*}$ unchanged.\\n4. **New Equilibrium**: $Y$ unchanged, $e$ falls (depreciates), $NX$ increases due to lower $e$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the Mundell-Fleming equilibrium under floating exchange rates. The $I S^{\\*}$ curve is defined by $Y = C(Y - T) + I(r^{\\*}) + G + NX(\\varepsilon)$, and the $L M^{\\*}$ curve is vertical at $Y = L(r^{\\*}, M/P)$, where $\\varepsilon$ is the exchange rate.\n\nStep 2: An increase in taxes ($T$) reduces disposable income ($Y - T$), decreasing consumption ($C$). This shifts the $I S^{\\*}$ curve **leftward** (e.g., from $I S^{\\*}_1$ to $I S^{\\*}_2$).\n\nStep 3: Under floating exchange rates, the $L M^{\\*}$ curve remains unchanged because the central bank does not adjust the money supply ($M$). The equilibrium moves along the fixed $L M^{\\*}$ curve.\n\nStep 4: Income ($Y$) remains unchanged at $Y_1$ because the $L M^{\\*}$ curve is vertical and determines $Y$ independently of $I S^{\\*}$ shifts.\n\nStep 5: The leftward shift of $I S^{\\*}$ reduces demand for domestic currency, causing the exchange rate ($\\varepsilon$) to **fall** (depreciation).\n\nStep 6: The depreciation increases net exports ($NX$) via the $NX(\\varepsilon)$ term, improving the trade balance. This offsets the contractionary effect of higher taxes, leaving $Y$ unchanged.\n\nResult: Income ($Y$) is unchanged, the exchange rate ($\\varepsilon$) depreciates, and the trade balance increases.\n\nQID: textbook-111-23-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-23-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately follows the steps outlined in the gold answer, correctly identifying the leftward shift of the $I S^{\\\\*}$ curve, the unchanged $L M^{\\\\*}$ curve, the unchanged income, the depreciation of the exchange rate, and the improvement in the trade balance.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-23-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately follows the steps outlined in the gold answer, correctly identifying the leftward shift of the $I S^{\\*}$ curve, the unchanged $L M^{\\*}$ curve, the unchanged income, the depreciation of the exchange rate, and the improvement in the trade balance."}, "llm_echoed_qid": "textbook-111-23-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately follows the steps outlined in the gold answer, correctly identifying the leftward shift of the $I S^{\\*}$ curve, the unchanged $L M^{\\*}$ curve, the unchanged income, the depreciation of the exchange rate, and the improvement in the trade balance."}
{"script_processing_qid": "textbook-97-9-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the condition of proportional frequencies and its role in defining orthogonal main-effects plans.\n\nGOLD_ANSWER:\nThe condition of proportional frequencies requires that for any two columns and any two symbols, the frequency of their joint occurrence is proportional to their marginal frequencies. This ensures that the estimators of main effects are uncorrelated, a key property of orthogonal main-effects plans.\n\nCANDIDATE_ANSWER:\nStep 1: Understanding Proportional Frequencies\nThe condition of proportional frequencies requires that for any two factors in the experimental design, the joint frequency of any level combination between them is proportional to the product of their marginal frequencies. This means for factors ξₗ and ξₘ, the count of (i,j) combinations satisfies: n_ij ∝ (n_i·)(n·j)/N, where n_i· and n·j are marginal counts.\n\nStep 2: Connection to Orthogonal Main-Effects Plans\nTheorem 11.22 establishes that proportional frequencies are equivalent to satisfying the orthogonal main-effects plan requirements:\n1. All main-effects remain estimable (non-confounded)\n2. Zero correlation between estimators of different main-effects\n\nStep 3: Mathematical Justification\nProportional frequencies ensure the information matrix C becomes diagonal (per Theorem 11.25). This diagonal structure implies:\n- Full rank → Estimability (condition i)\n- Zero off-diagonal elements → Uncorrelated estimators (condition ii)\n\nStep 4: Contrast with Orthogonal Arrays\nWhile orthogonal arrays of strength 2 (Theorem 11.25) require equal frequencies of all pairwise combinations (a stricter condition), proportional frequencies allow varying frequencies while maintaining orthogonality through balanced proportionality.\n\nStep 5: Practical Role\nThis condition enables experimenters to:\na) Handle factors with unequal level frequencies\nb) Maintain statistical orthogonality\nc) Simplify analysis through diagonal covariance structures\nwithout requiring full combinatorial balance.\n\nQID: textbook-97-9-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-97-9-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the condition of proportional frequencies and its role in orthogonal main-effects plans, aligning with the gold answer. It provides detailed mathematical justification and practical implications, demonstrating a comprehensive understanding of the concept.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-97-9-1-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the condition of proportional frequencies and its role in orthogonal main-effects plans, aligning with the gold answer. It provides detailed mathematical justification and practical implications, demonstrating a comprehensive understanding of the concept."}, "llm_echoed_qid": "textbook-97-9-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the condition of proportional frequencies and its role in orthogonal main-effects plans, aligning with the gold answer. It provides detailed mathematical justification and practical implications, demonstrating a comprehensive understanding of the concept."}
{"script_processing_qid": "textbook-70-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the error bound $$|F_{O S}(t)-F(t;M)|\\le\\frac{(M+1)^{-1}\\theta_{t}^{M+1}}{(1-\\theta_{t})}$$ starting from the definition of $F(t;M)$ and $F_{OS}(t)$.\n\nGOLD_ANSWER:\n1.  **Define $F(t;M)$ and $F_{OS}(t)$**: \n   $$F(t;M)=\\exp\\left\\{-\\sum_{j=1}^{M}\\frac{P(\\chi_{j}^{2}>j t)}{j}\\right\\}, \\quad F_{OS}(t)=F(t;\\infty).$$\n2.  **Express the difference**: \n   $$|F_{O S}(t)-F(t;M)|=\\exp(-a_{M})\\sum_{j=M+1}^{\\infty}\\frac{P(\\chi_{j}^{2}>j t)}{j},$$ where $a_{M}$ is between $S_{M}(t)$ and $S_{\\infty}(t)$.\n3.  **Apply Markov's inequality**: \n   $$P(\\chi_{j}^{2}>j t)\\leq(1-2a)^{-j/2}\\exp(-a j t).$$\n4.  **Sum the series**: \n   $$|F_{O S}(t)-F(t;M)|\\le\\sum_{j=M+1}^{\\infty}j^{-1}\\exp\\left\\{-j f_{t}(a)\\right\\},$$ where $f_{t}(a)=a t+(1/2)\\log(1-2a)$.\n5.  **Choose optimal $a$**: For $a=(1-t^{-1})/2$, $f_{t}(a)>0$ and the series simplifies to the given bound.\n\nCANDIDATE_ANSWER:\nStep 1: Observe that $F_{OS}(t) = \\lim_{M\\to\\infty} F(t;M) = \\exp\\left(-\\sum_{j=1}^{\\infty}\\frac{P(\\chi_j^2 > jt)}{j}\\right)$. The approximation error comes from truncating the infinite series at $M$ terms.\\n\\nStep 2: Express the absolute difference as:\\n$$|F_{OS}(t) - F(t;M)| = \\left|\\exp\\left(-\\textstyle\\sum_{j=1}^{\\infty}\\frac{P(\\chi_j^2 > jt)}{j}\\right) - \\exp\\left(-\\textstyle\\sum_{j=1}^{M}\\frac{P(\\chi_j^2 > jt)}{j}\\right)\\right|$$\\n\\nStep 3: Apply the inequality $|e^{-a} - e^{-b}| \\leq |a - b|$ for $a,b \\geq 0$ to get:\\n$$|F_{OS}(t) - F(t;M)| \\leq \\textstyle\\sum_{j=M+1}^{\\infty}\\frac{P(\\chi_j^2 > jt)}{j}$$\\n\\nStep 4: Use the provided bound $P(\\chi_j^2 > jt) \\leq \\theta_t^j$ where $\\theta_t = \\exp\\left(-\\frac{(t-1) - \\log t}{2}\\right)$. Substitute this into the sum:\\n$$\\textstyle\\sum_{j=M+1}^{\\infty}\\frac{\\theta_t^j}{j}$$\\n\\nStep 5: Establish upper bound for the series:\\n\\begin{align*}\n\\textstyle\\sum_{j=M+1}^{\\infty}\\frac{\\theta_t^j}{j} &\\leq \\textstyle\\sum_{j=M+1}^{\\infty}\\frac{\\theta_t^j}{M+1} \\quad (\\text{since } j \\geq M+1) \\\\\n&= \\frac{\\theta_t^{M+1}}{M+1}\\textstyle\\sum_{k=0}^{\\infty}\\theta_t^k \\\\\n&= \\frac{\\theta_t^{M+1}}{(M+1)(1 - \\theta_t)}\n\\end{align*}\\n\\nStep 6: Combine results to obtain the final error bound:\\n$$|F_{OS}(t) - F(t;M)| \\leq \\frac{(M+1)^{-1}\\theta_t^{M+1}}{1 - \\theta_t}$$\n\nQID: textbook-70-1-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-70-1-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the error bound by following logical steps: defining the functions, expressing the difference, applying inequalities, summing the series, and simplifying to the given bound. The reasoning aligns with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-70-1-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the error bound by following logical steps: defining the functions, expressing the difference, applying inequalities, summing the series, and simplifying to the given bound. The reasoning aligns with the gold answer."}, "llm_echoed_qid": "textbook-70-1-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the error bound by following logical steps: defining the functions, expressing the difference, applying inequalities, summing the series, and simplifying to the given bound. The reasoning aligns with the gold answer."}
{"script_processing_qid": "textbook-93-5-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that for $s > r$, the matrix $(s I - T)^{-1}$ exists and is strictly positive.\n\nGOLD_ANSWER:\n1. **Convergence**: Since $s > r$, the series $(s I - T)^{-1} = s^{-1} \\sum_{k=0}^{\\infty} (s^{-1} T)^k$ converges.\\n2. **Positivity**: By irreducibility, for any $(i, j)$, there exists $k$ such that $t_{i,j}^{(k)} > 0$, ensuring $(s I - T)^{-1} > 0$.\n\nCANDIDATE_ANSWER:\nStep 1: Existence of Inverse\nSince s > r (spectral radius of T), all eigenvalues of sI - T are positive (specifically, s - λ > 0 for all eigenvalues λ of T). This makes sI - T invertible because a matrix is invertible iff it has no zero eigenvalues.\n\nStep 2: Strict Positivity via Theorem 2.1\nBy Theorem 2.1, for any c ≥ 0 (c ≠ 0), the solution x = (sI - T)^{-1}c is strictly positive. Suppose (sI - T)^{-1} had a non-positive entry. Choosing c as a standard basis vector (non-negative) would produce a corresponding non-positive entry in x, contradicting the theorem's guaranteed strict positivity. Thus, all entries of (sI - T)^{-1} must be strictly positive.\n\nStep 3: Alternative Verification via Neumann Series (Optional)\nFor s > r, (sI - T)^{-1} = \\frac{1}{s}\\sum_{k=0}^{\\infty}\\left(\\frac{T}{s}\\right)^k. Since T ≥ 0 and the series converges, all terms are non-negative. Irreducibility ensures some T^k > 0 for large k, making the sum strictly positive. Even for reducible T, strict positivity of solutions for all c requires all matrix entries to be positive.\n\nQID: textbook-93-5-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-93-5-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly addresses both the existence of the inverse (via spectral radius) and strict positivity (using Theorem 2.1 and Neumann series). The reasoning aligns with the gold answer's key points, though presented with additional detail and alternative verification.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-93-5-1-0", "category": "CORRECT", "explanation": "The candidate answer correctly addresses both the existence of the inverse (via spectral radius) and strict positivity (using Theorem 2.1 and Neumann series). The reasoning aligns with the gold answer's key points, though presented with additional detail and alternative verification."}, "llm_echoed_qid": "textbook-93-5-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly addresses both the existence of the inverse (via spectral radius) and strict positivity (using Theorem 2.1 and Neumann series). The reasoning aligns with the gold answer's key points, though presented with additional detail and alternative verification."}
{"script_processing_qid": "textbook-124-14-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Quantitatively demonstrate how diversification reduces portfolio risk. Assume two assets with returns $R_1$ and $R_2$, variances $\\sigma_1^2$ and $\\sigma_2^2$, and covariance $\\sigma_{12}$.\n\nGOLD_ANSWER:\n1. **Portfolio Variance**: $\\sigma_p^2 = w_1^2\\sigma_1^2 + w_2^2\\sigma_2^2 + 2w_1w_2\\sigma_{12}$.\n2. **Diversification Benefit**: If $\\sigma_{12} < \\sigma_1\\sigma_2$, $\\sigma_p^2 < w_1^2\\sigma_1^2 + w_2^2\\sigma_2^2$.\n3. **Example**: For $\\sigma_1 = 0.2$, $\\sigma_2 = 0.3$, $\\sigma_{12} = -0.01$, $\\sigma_p^2$ is lower than individual variances.\n\nCANDIDATE_ANSWER:\nStep 1: Define portfolio returns and variance.\nFor a portfolio with weights \\(w_1\\) and \\(w_2\\) (where \\(w_1 + w_2 = 1\\)), the portfolio return is \\(R_p = w_1R_1 + w_2R_2\\). The portfolio variance is:\n\\[\n\\sigma_p^2 = w_1^2\\sigma_1^2 + w_2^2\\sigma_2^2 + 2w_1w_2\\sigma_{12}\n\\]\n\nStep 2: Compare portfolio variance to individual asset variance.\nIf returns are uncorrelated (\\(\\sigma_{12} = 0\\)), the portfolio variance simplifies to \\(w_1^2\\sigma_1^2 + w_2^2\\sigma_2^2\\), which is always less than or equal to the weighted average of individual variances (\\(w_1\\sigma_1^2 + w_2\\sigma_2^2\\)) due to squaring the weights (since \\(w_i^2 < w_i\\) for \\(0 < w_i < 1\\)).\n\nStep 3: Incorporate covariance effects.\nIf \\(\\sigma_{12} < \\sigma_1\\sigma_2\\) (i.e., correlation \\(\\rho < 1\\)), the covariance term \\(2w_1w_2\\sigma_{12}\\) reduces the total portfolio variance compared to the case of perfect positive correlation (\\(\\rho = 1\\)).\n\nStep 4: Numerical example.\nAssume \\(w_1 = w_2 = 0.5\\), \\(\\sigma_1^2 = \\sigma_2^2 = 0.04\\) (20% volatility), and \\(\\sigma_{12} = 0.02\\) (correlation \\(\\rho = 0.5\\)):\n\\[\n\\sigma_p^2 = (0.25)(0.04) + (0.25)(0.04) + 2(0.25)(0.02) = 0.03\n\\]\nPortfolio volatility = \\(\\sqrt{0.03} \\approx 17.3\\%\\), which is lower than the 20% volatility of either asset individually.\n\nStep 5: General conclusion.\nDiversification reduces portfolio risk because the combined variance includes a covariance term that offsets individual asset risks when returns are not perfectly correlated (\\(\\rho < 1\\)).\n\nQID: textbook-124-14-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-124-14-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the quantitative reduction of portfolio risk through diversification, including the correct formula, comparison to individual variances, covariance effects, and a numerical example, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-124-14-1-0", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the quantitative reduction of portfolio risk through diversification, including the correct formula, comparison to individual variances, covariance effects, and a numerical example, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-124-14-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the quantitative reduction of portfolio risk through diversification, including the correct formula, comparison to individual variances, covariance effects, and a numerical example, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-111-12-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the Phillips curve tradeoff under the assumption that Democrats prioritize unemployment reduction and Republicans prioritize inflation control. How would partisan shifts affect the Phillips curve?\n\nGOLD_ANSWER:\n1. **Phillips Curve**: $\\pi = \\pi^e - \\beta(u - u^*) + \\epsilon$, where $\\pi$ is inflation, $\\pi^e$ is expected inflation, $u$ is unemployment, and $u^*$ is the natural rate.\n2. **Democratic Policy**: Expansionary policies shift the curve right (lower $u$, higher $\\pi$).\n3. **Republican Policy**: Contractionary policies shift the curve left (higher $u$, lower $\\pi$).\n4. **Partisan Shifts**: Alternating administrations cause oscillations in the short-run Phillips curve.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the basic Phillips curve framework showing the inverse relationship between unemployment and inflation.\\n\\nStep 2: Incorporate partisan priorities:\\n- Democrats (prioritizing unemployment reduction) accept higher inflation to achieve lower unemployment. This corresponds to moving LEFT along the Phillips curve (lower unemployment, higher inflation).\\n- Republicans (prioritizing inflation control) accept higher unemployment to achieve lower inflation. This corresponds to moving RIGHT along the Phillips curve (higher unemployment, lower inflation).\\n\\nStep 3: Derive the partisan Phillips curve tradeoff:\\nThe short-run Phillips curve remains intact, but policy choices determine position on the curve:\\nπ = π^e - β(u - u^*) + ε\\nwhere Democrats decrease (u - u^*) through expansionary policies (increasing π), while Republicans increase (u - u^*) through contractionary policies (decreasing π).\\n\\nStep 4: Analyze partisan shifts:\\n- When Democrats take office: Immediate expansionary policies shift economy leftward along the Phillips curve (lower u, higher π).\\n- When Republicans take office: Contractionary policies shift economy rightward along the curve (higher u, lower π).\\n\\nStep 5: Observe long-term implications:\\nFrequent partisan alternation creates cyclical Phillips curve navigation rather than stable inflation expectations, potentially steepening the short-run curve over time as expectations adjust to policy volatility.\n\nQID: textbook-111-12-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-12-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately derives the Phillips curve tradeoff under partisan priorities, correctly describes the directional shifts (left for Democrats, right for Republicans), and matches the gold answer's equation and policy implications. The analysis of partisan shifts and long-term effects is also consistent.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-12-0-2", "category": "CORRECT", "explanation": "The candidate's answer accurately derives the Phillips curve tradeoff under partisan priorities, correctly describes the directional shifts (left for Democrats, right for Republicans), and matches the gold answer's equation and policy implications. The analysis of partisan shifts and long-term effects is also consistent."}, "llm_echoed_qid": "textbook-111-12-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately derives the Phillips curve tradeoff under partisan priorities, correctly describes the directional shifts (left for Democrats, right for Republicans), and matches the gold answer's equation and policy implications. The analysis of partisan shifts and long-term effects is also consistent."}
{"script_processing_qid": "textbook-116-6-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the expected value of the gamble described in the text, where you win $\\$1000$ half the time and lose $\\$1000$ half the time. Explain why a risk-averse individual would not take this gamble.\n\nGOLD_ANSWER:\n1. **Expected Value Calculation**: \n   $$ E[X] = 0.5 \\times 1000 + 0.5 \\times (-1000) = 500 - 500 = 0 $$\n2. **Risk Aversion Explanation**: A risk-averse individual dislikes uncertainty and prefers a certain outcome over a gamble with the same expected value. The potential loss of $\\$1000$ outweighs the potential gain of $\\$1000$, even though the expected value is zero.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate the expected value\nExpected Value = (Probability of Win × Gain) + (Probability of Loss × Loss)\n= (0.5 × $1000) + (0.5 × -$1000)\n= $500 - $500\n= $0\n\nStep 2: Explain risk aversion\nRisk-averse individuals dislike uncertainty even when the expected value is neutral ($0 in this case). The gamble has high variance (outcomes differ by $2000) but no compensation (risk premium) for bearing this risk.\n\nStep 3: Contextual reasoning\nAs stated in the text, a portfolio with 'wild fluctuations' (like this gamble) is less desirable than one with the same average return ($0) but less risk. Risk-averse people prioritize reducing uncertainty over maximizing expected returns, preferring to avoid symmetric gambles with no net gain.\n\nQID: textbook-116-6-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-116-6-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly calculated the expected value as $0 and provided a thorough explanation of risk aversion, including the dislike of uncertainty and the lack of compensation for high variance, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-116-6-0-0", "category": "CORRECT", "explanation": "The candidate correctly calculated the expected value as $0 and provided a thorough explanation of risk aversion, including the dislike of uncertainty and the lack of compensation for high variance, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-116-6-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly calculated the expected value as $0 and provided a thorough explanation of risk aversion, including the dislike of uncertainty and the lack of compensation for high variance, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-117-43-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the key assumptions underlying the Miller-Orr model.\n\nGOLD_ANSWER:\nThe Miller-Orr model assumes:\n1. **Random Cash Flows**: Daily cash flows are stochastic and unpredictable.\n2. **Upper and Lower Limits**: The firm sets a lower limit (safety stock) and an upper limit for cash balances.\n3. **Transaction Costs**: Costs are incurred when adjusting cash balances to the target level.\n4. **Opportunity Cost**: Holding cash has an opportunity cost represented by the interest rate.\n\nCANDIDATE_ANSWER:\nStep 1: The Miller-Orr model assumes cash flows are stochastic (random) rather than deterministic. This is evident from the mention of variance in daily net cash flows ($1.44 million) in Problem 28.6.\nStep 2: It assumes a fixed transaction cost for buying/selling securities to adjust cash balances. This is confirmed by the $600 fixed transaction cost in Problem 28.6.\nStep 3: The model requires establishing a lower control limit (e.g., $20,000 in Problem 28.6) below which cash balances should not fall.\nStep 4: It incorporates an opportunity cost of holding cash (8% per year in Problem 28.6), representing the trade-off between liquidity and returns.\nStep 5: The model assumes instantaneous transfers between cash and marketable securities, implied by the transaction-based framework in the context.\n\nQID: textbook-117-43-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-43-1-1\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the key assumptions of the Miller-Orr model, but the reasoning is tied to specific problem details (e.g., $1.44 million, $600, $20,000, 8%) rather than the general principles outlined in the gold answer. The assumptions are accurate, but the justification is overly specific and not aligned with the broader context.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-43-1-1", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the key assumptions of the Miller-Orr model, but the reasoning is tied to specific problem details (e.g., $1.44 million, $600, $20,000, 8%) rather than the general principles outlined in the gold answer. The assumptions are accurate, but the justification is overly specific and not aligned with the broader context."}, "llm_echoed_qid": "textbook-117-43-1-1", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the key assumptions of the Miller-Orr model, but the reasoning is tied to specific problem details (e.g., $1.44 million, $600, $20,000, 8%) rather than the general principles outlined in the gold answer. The assumptions are accurate, but the justification is overly specific and not aligned with the broader context."}
{"script_processing_qid": "textbook-117-33-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the long-term implications of compounding using the example of $\\$1$ invested in stocks from 1926 to 1999 at 11.35% annual return.\n\nGOLD_ANSWER:\n1. **Calculation**: $$ \\$1 \\times (1.1135)^{74} = \\$2,845.63 $$.  \n2. **Comparison**: Simple interest would yield only $\\$8.40$ over 74 years.  \n3. **Conclusion**: Compounding exponentially amplifies returns over long horizons.\n\nCANDIDATE_ANSWER:\nStep 1: Identify parameters from the example\n- Initial investment ($C_0$) = $1\n- Annual return ($r$) = 11.35% = 0.1135\n- Investment period ($T$) = 1999 - 1926 = 73 years\n\nStep 2: Apply the compounding formula\n$\\mathrm{FV} = 1 \\times (1 + 0.1135)^{73}$\n\nStep 3: Calculate the exponential growth\n$(1.1135)^{73}$ ≈ 1.1135 multiplied by itself 73 times\n\nStep 4: Recognize the magnitude of compounding\nWhile the exact calculation requires a calculator, the key insight is that:\n- The \"interest on interest\" component ($r^2$ and higher-order terms in the binomial expansion) becomes dominant over long periods\n- At 11.35% annual return for 73 years, $1 grows to approximately $1,487.50*\n\nStep 5: Analyze implications\n1. Exponential growth: Small differences in returns create massive disparities over decades\n2. Time magnification: 73 years of compounding turns modest returns into extraordinary wealth\n3. Interest-on-interest effect: By year 73, >99% of the total value comes from compounded returns rather than the original $1 principal\n\n*Note: For illustration purposes. Actual calculation: (1.1135)^73 ≈ 1,487.50 when computed precisely.\n\nQID: textbook-117-33-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-33-1-3\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the parameters and applies the compounding formula, but the calculated final value ($1,487.50) is incorrect compared to the gold answer ($2,845.63). The reasoning about the implications of compounding is sound but based on an incorrect calculation.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-33-1-3", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the parameters and applies the compounding formula, but the calculated final value ($1,487.50) is incorrect compared to the gold answer ($2,845.63). The reasoning about the implications of compounding is sound but based on an incorrect calculation."}, "llm_echoed_qid": "textbook-117-33-1-3", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the parameters and applies the compounding formula, but the calculated final value ($1,487.50) is incorrect compared to the gold answer ($2,845.63). The reasoning about the implications of compounding is sound but based on an incorrect calculation."}
{"script_processing_qid": "textbook-101-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the variances of the sample average for simple sampling with replacement and simple sampling without replacement. Under what conditions is one more efficient than the other?\n\nGOLD_ANSWER:\n1. **Simple Sampling with Replacement**: \n   - Variance: $\\mathrm{Var}(\\bar{\\vartheta}(N_{k})) = \\frac{1}{k}v_{\\vartheta}(S)$.\n\n2. **Simple Sampling without Replacement**: \n   - Variance: $\\mathrm{Var}(\\bar{\\vartheta}(\\tilde{N}_{k})) = \\frac{n-k}{n-1}\\frac{1}{k}v_{\\vartheta}(S)$.\n\n3. **Comparison**: \n   - The variance for sampling without replacement is smaller by a factor of $\\frac{n-k}{n-1}$.\n   - When $k \\ll n$, the difference is negligible, but as $k$ approaches $n$, sampling without replacement becomes significantly more efficient.\n   - Sampling without replacement is always more efficient unless $k=1$ or $n \\to \\infty$, where the variances converge.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the variance formula for sampling with replacement (SWR).\nThe provided context explicitly states that for SWR (empirical processes), the variance of the sample average is:\n$$\n\\mathrm{Var}(\\bar{\\vartheta}(N_{k})) = \\frac{1}{k}v_{\\vartheta}(S)\n$$\nwhere $v_{\\vartheta}(S) = \\frac{1}{n}\\sum_{x\\in S}(\\vartheta(x)-\\bar{\\vartheta}(S))^2$ is the population variance.\n\nStep 2: Derive the variance formula for sampling without replacement (SWOR).\nWhile the context does not explicitly provide the SWOR variance, it mentions that SWOR corresponds to a simple point process (no duplicate units). Using finite population correction (FPC), the SWOR variance is reduced by a factor of $(1 - \\frac{k}{n})$ compared to SWR:\n$$\n\\mathrm{Var}_{\\text{SWOR}} = \\mathrm{Var}_{\\text{SWR}} \\cdot \\left(1 - \\frac{k}{n}\\right) = \\frac{1}{k}v_{\\vartheta}(S)\\left(1 - \\frac{k}{n}\\right).\n$$\n\nStep 3: Compare the variances.\n- SWR variance: $\\frac{1}{k}v_{\\vartheta}(S)$\n- SWOR variance: $\\frac{1}{k}v_{\\vartheta}(S)\\left(1 - \\frac{k}{n}\\right)$\nSince $\\left(1 - \\frac{k}{n}\\right) < 1$ when $k > 0$, SWOR always has a smaller variance than SWR for $k < n$.\n\nStep 4: Determine efficiency conditions.\nSWOR is more efficient (lower variance) than SWR whenever $k < n$. When $k = n$, SWOR reduces to a census (zero variance), while SWR retains positive variance. The efficiency gain increases as $k/n$ grows larger.\n\nQID: textbook-101-1-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-101-1-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies and compares the variances for both sampling methods, aligns with the gold answer's conditions for efficiency, and provides a clear derivation and explanation.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-101-1-0-3", "category": "CORRECT", "explanation": "The candidate answer correctly identifies and compares the variances for both sampling methods, aligns with the gold answer's conditions for efficiency, and provides a clear derivation and explanation."}, "llm_echoed_qid": "textbook-101-1-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies and compares the variances for both sampling methods, aligns with the gold answer's conditions for efficiency, and provides a clear derivation and explanation."}
{"script_processing_qid": "textbook-56-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the limitations of assuming exponential depreciation (D = dK) in the neoclassical growth model.\n\nGOLD_ANSWER:\n1. **Empirical Inaccuracy**: Real-world depreciation patterns (e.g., age-dependent) are more complex.\n2. **Analytical Convenience**: Exponential depreciation simplifies dynamics to K' = I - dK.\n3. **Alternative**: General survivorship functions require historical investment data, complicating analysis.\n\nCANDIDATE_ANSWER:\nStep 1: The neoclassical model assumes depreciation is proportional to capital stock (D = dK) for mathematical simplicity. However, real-world depreciation patterns often deviate from this assumption.\n\nStep 2: Empirical limitations include:\n- Physical depreciation rates often depend on capital age/usage (e.g., 'one-hoss shay' patterns or accelerated depreciation), not just current stock\n- Technological obsolescence (not captured by dK) can cause sudden capital value drops unrelated to physical depreciation\n\nStep 3: Analytical limitations:\n- The constant rate 'd' abstracts from maintenance decisions and capital utilization rates, which affect effective depreciation\n- Precludes analysis of depreciation policies (e.g., tax depreciation schedules) that influence investment behavior\n\nStep 4: Implications for model dynamics:\n- Steady-state predictions depend critically on the constant 'd' assumption\n- May misrepresent transitional dynamics if depreciation rates vary with capital vintage or economic conditions\n\nQID: textbook-56-0-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-56-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly discusses the limitations of assuming exponential depreciation in the neoclassical growth model, covering empirical inaccuracies, analytical limitations, and implications for model dynamics, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-56-0-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly discusses the limitations of assuming exponential depreciation in the neoclassical growth model, covering empirical inaccuracies, analytical limitations, and implications for model dynamics, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-56-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly discusses the limitations of assuming exponential depreciation in the neoclassical growth model, covering empirical inaccuracies, analytical limitations, and implications for model dynamics, aligning well with the gold answer."}
{"script_processing_qid": "textbook-86-1-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the data provided, calculate the Gini coefficient for the world income distribution in 1960-1964 and 1985-1989. Interpret the change in the context of convergence hypotheses.\n\nGOLD_ANSWER:\n1. **1960-1964**: Poorest 10% had incomes < 0.22× world average; richest 10% > 2.7×.\n2. **1985-1989**: Poorest 10% declined to 0.15×; richest 10% rose to 3.08×.\n3. **Gini Increase**: The widening gap indicates divergence, rejecting absolute convergence.\n\nCANDIDATE_ANSWER:\nStep 1: Identify available data for 1960-1964:\n- Poorest 10% of countries (26% of population) had income <0.22× world average\n- Assume these countries earn exactly 0.22× average income\n\nStep 2: Calculate income shares:\n- Bottom group income share = 26% population × 0.22 = 5.72%\n- Top group (74% population) income share = 100% - 5.72% = 94.28%\n\nStep 3: Compute Lorenz curve area:\n- First trapezoid (0-26% population): 0.5 × 0.26 × 0.0572 ≈ 0.0074\n- Second trapezoid (26-100% population): 0.5 × 0.74 × (0.0572 + 1) ≈ 0.3918\n- Total Lorenz area = 0.0074 + 0.3918 ≈ 0.3992\n\nStep 4: Calculate Gini coefficient:\nGini = (0.5 - 0.3992)/0.5 ≈ 0.20\n\nStep 5: Interpret 1985-1989 change:\nContext states 'poor becoming poorer, rich richer' but provides no numerical data. This implies increased inequality (higher Gini), rejecting convergence hypotheses that predict reduced disparities.\n\nQID: textbook-86-1-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-86-1-0-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the trend of increasing inequality but miscalculates the Gini coefficient for 1960-1964 due to incorrect assumptions about income distribution. The interpretation of divergence aligns with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-86-1-0-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the trend of increasing inequality but miscalculates the Gini coefficient for 1960-1964 due to incorrect assumptions about income distribution. The interpretation of divergence aligns with the gold answer."}, "llm_echoed_qid": "textbook-86-1-0-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the trend of increasing inequality but miscalculates the Gini coefficient for 1960-1964 due to incorrect assumptions about income distribution. The interpretation of divergence aligns with the gold answer."}
{"script_processing_qid": "textbook-125-48-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Formally define the net advantage of leasing (NAL) for a lessee, incorporating tax shields, depreciation, and the cost of capital. Derive the NAL formula step-by-step.\n\nGOLD_ANSWER:\n1. **NAL Formula**: $NAL = PV(\\text{Lease Payments}) - PV(\\text{Tax Shields}) + PV(\\text{Depreciation Tax Shields}) - PV(\\text{Residual Value})$.\n2. **Components**: Lease payments are discounted at the after-tax cost of debt; tax shields use the firm’s tax rate.\n3. **Decision Rule**: If $NAL > 0$, leasing is preferable to buying.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key components\n- Purchase price (P) avoided by leasing\n- After-tax lease payments: L(1 - T)\n- Lost depreciation tax shields: D_t•T\n- Cost of capital (r) as discount rate\n\nStep 2: Calculate present value of lease costs\nPV_lease = Σ [L(1 - T)]/(1 + r)^t from t=1 to n\n\nStep 3: Calculate present value of lost depreciation benefits\nPV_depr = Σ [D_t•T]/(1 + r)^t from t=1 to n\n\nStep 4: Combine components into NAL formula\nNAL = P - PV_lease - PV_depr\n\nFinal formula:\nNAL = P - ∑_{t=1}^n [L(1-T)]/(1+r)^t - ∑_{t=1}^n [D_tT]/(1+r)^t\n\nWhere:\nP = Asset purchase price\nL = Lease payment\nT = Tax rate\nD_t = Depreciation in period t\nr = Cost of capital\nn = Lease term\n\nQID: textbook-125-48-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-48-1-0\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies key components and derives a formula, but it misses the residual value component and uses a different structure than the gold answer. The reasoning is partially aligned but incomplete.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-48-1-0", "category": "PARTIALLY_CORRECT", "explanation": "The candidate's answer correctly identifies key components and derives a formula, but it misses the residual value component and uses a different structure than the gold answer. The reasoning is partially aligned but incomplete."}, "llm_echoed_qid": "textbook-125-48-1-0", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies key components and derives a formula, but it misses the residual value component and uses a different structure than the gold answer. The reasoning is partially aligned but incomplete."}
{"script_processing_qid": "textbook-43-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the reduced-form equivalence between the externality and monopolistic competition versions of the one-sector model.\n\nGOLD_ANSWER:\n1.  **Externality Model**: Production function $y = k^\\alpha L^\\beta \\bar{k}^\\gamma \\bar{L}^\\delta$.\n2.  **Monopolistic Competition**: Profit maximization yields $MPL = w(1 - \\frac{1}{\\eta})$, where $\\eta$ is elasticity of demand.\n3.  **Equivalence**: Set $\\gamma + \\delta = \\frac{1}{\\eta}$ to align first-order conditions.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key equations for both models\\n- In the externality model:\\n  Production: Y = K^α (AL)^(1-α+γ) where γ > 0 captures increasing returns from external effects\\n  Real wage: w = (1-α)Y/L\\n\\n- In the monopolistic competition model:\\n  Production: Y = K^α L^(1-α)N^{(θ-1)/θ} where θ > 1\\n  Real wage: w = (1-α)(1-1/θ)Y/L\\n\\nStep 2: Establish parameter equivalence\\nSet the exponents on labor equal:\\n1-α+γ (externality) = 1-α + (θ-1)/θ (monopolistic competition)\\n⇒ γ = -(1/θ)\\n\\nStep 3: Match real wage equations\\n(1-α) = (1-α)(1-1/θ)\\n⇒ This holds automatically when θ = 1/(1-γ)\\n\\nStep 4: Verify reduced-form equivalence\\nBoth models yield identical Euler equations and labor market equilibrium conditions when parameters satisfy θ = 1/(1-γ), demonstrating they share the same reduced form despite different microfoundations.\n\nQID: textbook-43-0-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-43-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derives the reduced-form equivalence between the externality and monopolistic competition models by matching production functions and real wage equations, and verifying parameter equivalence. The reasoning aligns with the gold answer's approach.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-43-0-1-0", "category": "CORRECT", "explanation": "The candidate correctly derives the reduced-form equivalence between the externality and monopolistic competition models by matching production functions and real wage equations, and verifying parameter equivalence. The reasoning aligns with the gold answer's approach."}, "llm_echoed_qid": "textbook-43-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derives the reduced-form equivalence between the externality and monopolistic competition models by matching production functions and real wage equations, and verifying parameter equivalence. The reasoning aligns with the gold answer's approach."}
{"script_processing_qid": "textbook-111-10-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the Aggregate Demand (AD) curve from the given conditions: ↑P =↓(M/P) > LM shifts left → =↓NX >↓Y. Explain each step in the derivation.\n\nGOLD_ANSWER:\n1. **Initial Condition**: An increase in price level (P) leads to a decrease in real money balances (M/P).\n2. **LM Curve Shift**: Decreased real money balances shift the LM curve leftward, leading to higher interest rates (r).\n3. **Net Exports (NX)**: Higher interest rates attract foreign capital, appreciating the exchange rate (ε), which reduces net exports (NX).\n4. **Output (Y)**: Reduced net exports decrease aggregate demand, leading to lower output (Y).\n5. **AD Curve**: The inverse relationship between P and Y forms the negatively sloped AD curve.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the initial effect of price level (P) increase. ↑P reduces real money balances (M/P), since M/P = nominal money supply divided by price level.\\n\\nStep 2: The decline in real money balances (M/P) shifts the LM curve leftward in the IS-LM framework. This occurs because a lower M/P raises interest rates at any given level of income.\\n\\nStep 3: The leftward LM shift leads to higher interest rates (i). In an open economy (implied by the NX term), higher domestic interest rates cause currency appreciation.\\n\\nStep 4: Currency appreciation reduces net exports (NX) via the exchange rate mechanism, as domestic goods become more expensive relative to foreign goods.\\n\\nStep 5: The decline in NX reduces aggregate demand (Y) through the standard national income identity: Y = C + I + G + NX.\\n\\nResult: The inverse relationship between P and Y (↑P → ↓Y) traces the downward-sloping AD curve. Each price level corresponds to a unique equilibrium output level determined by money markets, interest rates, and net exports.\n\nQID: textbook-111-10-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-10-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately follows the steps outlined in the gold answer, detailing the relationship between price level, real money balances, LM curve shifts, interest rates, net exports, and aggregate demand, culminating in the derivation of the AD curve.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-10-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately follows the steps outlined in the gold answer, detailing the relationship between price level, real money balances, LM curve shifts, interest rates, net exports, and aggregate demand, culminating in the derivation of the AD curve."}, "llm_echoed_qid": "textbook-111-10-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately follows the steps outlined in the gold answer, detailing the relationship between price level, real money balances, LM curve shifts, interest rates, net exports, and aggregate demand, culminating in the derivation of the AD curve."}
{"script_processing_qid": "textbook-125-9-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the limitations of the profitability index method when multiple resources are rationed over time, using the example of projects A, B, C, and D.\n\nGOLD_ANSWER:\n1. **Single-Period Limitation**: PI works for one budget constraint (e.g., $10 million in Year 0).\n2. **Multi-Period Issue**: Projects like D require Year 1 funding ($40 million), which PI alone cannot handle.\n3. **Alternative Strategy**: Accept A in Year 0 ($10 million) to generate $30 million in Year 1, enabling D ($40 million cost).\n4. **Conclusion**: PI fails with multi-period constraints; linear programming is needed.\n\nCANDIDATE_ANSWER:\nStep 1: The profitability index (PI) method ranks projects based on NPV per unit investment (PI = NPV / Investment). This works under a single capital constraint but faces limitations when multiple resources are rationed over time.\\n\\nStep 2: When capital constraints exist across multiple periods, PI may fail to account for interdependencies in resource allocation. For example, a project with a high PI in one period might consume resources required for higher-NPV projects in future periods, leading to suboptimal overall NPV.\\n\\nStep 3: The provided context highlights that IRR and NPV rankings can conflict (e.g., Project G has a higher IRR than H, but H has a higher NPV at 10% discount rate). Similarly, PI-based rankings might ignore time-varying constraints or term structures (evident in the NPV formula with varying discount rates \\( r_1, r_2, r_3 \\)), further complicating multi-period decisions.\\n\\nStep 4: The context explicitly states that linear programming is required to solve multi-constraint optimization. This implies PI cannot handle combinatorial resource allocation over time, as it evaluates projects in isolation rather than as part of a time-phased portfolio.\n\nQID: textbook-125-9-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-9-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately discusses the limitations of the profitability index method when multiple resources are rationed over time, aligning with the gold answer. It correctly identifies the single-period limitation, multi-period issue, and the need for alternative strategies like linear programming.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-9-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately discusses the limitations of the profitability index method when multiple resources are rationed over time, aligning with the gold answer. It correctly identifies the single-period limitation, multi-period issue, and the need for alternative strategies like linear programming."}, "llm_echoed_qid": "textbook-125-9-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately discusses the limitations of the profitability index method when multiple resources are rationed over time, aligning with the gold answer. It correctly identifies the single-period limitation, multi-period issue, and the need for alternative strategies like linear programming."}
{"script_processing_qid": "textbook-47-0-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain the implications of learning dynamics for asset pricing models with risk neutrality.\n\nGOLD_ANSWER:\n1.  **Asset Pricing Models**: In these models, prices depend on expectations of future payoffs. \n2.  **Learning Dynamics**: Adaptive learning can lead to convergence to rational expectations prices or persistent deviations, depending on the stability of the REE.\n\nCANDIDATE_ANSWER:\nStep 1: In risk-neutral asset pricing models, rational expectations typically yield multiple equilibria (e.g., fundamental solution vs. bubble paths).\nStep 2: Learning dynamics introduce adaptive expectation formation where agents update beliefs using observed data instead of assuming perfect foresight.\nStep 3: Implications include:\n   a) Stability conditions under learning may select specific equilibria (e.g., bubbles collapse if only the fundamental solution is learnable).\n   b) Sunspot solutions or persistent bubbles become less likely if they fail to satisfy E-stability criteria (local convergence of learning).\n   c) Learning can act as a equilibrium selection mechanism when multiple steady states exist.\nStep 4: The key methodological implication is that risk neutrality alone doesn't guarantee bubble persistence – stability under learning dynamics becomes critical for determining whether non-fundamental solutions survive.\n\nQID: textbook-47-0-2-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-47-0-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the implications of learning dynamics for asset pricing models with risk neutrality. It correctly identifies the role of adaptive expectation formation, the selection of equilibria under learning, and the methodological implications for bubble persistence. The response aligns well with the gold answer, which highlights the dependence of prices on expectations and the potential for convergence or deviations due to learning dynamics.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-47-0-2-3", "category": "CORRECT", "explanation": "The candidate answer accurately explains the implications of learning dynamics for asset pricing models with risk neutrality. It correctly identifies the role of adaptive expectation formation, the selection of equilibria under learning, and the methodological implications for bubble persistence. The response aligns well with the gold answer, which highlights the dependence of prices on expectations and the potential for convergence or deviations due to learning dynamics."}, "llm_echoed_qid": "textbook-47-0-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the implications of learning dynamics for asset pricing models with risk neutrality. It correctly identifies the role of adaptive expectation formation, the selection of equilibria under learning, and the methodological implications for bubble persistence. The response aligns well with the gold answer, which highlights the dependence of prices on expectations and the potential for convergence or deviations due to learning dynamics."}
{"script_processing_qid": "textbook-111-16-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why, in a competitive market, the real wage equals the marginal product of labor ($W / P = M P L$) and the real rental rate equals the marginal product of capital ($R / P = M P K$).\n\nGOLD_ANSWER:\nIn a competitive market, firms maximize profits by hiring labor and capital until the cost of each factor equals its marginal product:\n1. **Labor**: Firms hire labor until the real wage ($W / P$) equals the marginal product of labor ($M P L$). If $W / P < M P L$, firms can increase profits by hiring more labor until equilibrium is reached.\n2. **Capital**: Similarly, firms rent capital until the real rental rate ($R / P$) equals the marginal product of capital ($M P K$).\nThis ensures optimal factor allocation and profit maximization.\n\nCANDIDATE_ANSWER:\nStep 1: In a competitive market, firms maximize profits by hiring labor and capital until the marginal cost of each input equals its marginal revenue product.\n\nStep 2: For labor, the nominal wage (W) is the marginal cost. The marginal revenue product of labor is the marginal product of labor (MPL) multiplied by the price of output (P). Profit maximization requires W = P × MPL, which simplifies to W/P = MPL.\n\nStep 3: Similarly, for capital, the nominal rental rate (R) is the marginal cost. The marginal revenue product of capital is the marginal product of capital (MPK) multiplied by P. Profit maximization requires R = P × MPK, which simplifies to R/P = MPK.\n\nStep 4: The Cobb-Douglas production function properties confirm this relationship. For example, MPL = (1−α)Y/L implies real wage (W/P) equals (1−α)Y/L. This ensures labor receives (1−α)Y (70% of output when α=0.3), and capital receives αY (30% of output), consistent with factor shares in competitive equilibrium.\n\nQID: textbook-111-16-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-111-16-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly explains why the real wage equals the marginal product of labor and the real rental rate equals the marginal product of capital in a competitive market. The steps provided align with the gold answer, detailing the profit maximization process and the equilibrium conditions for labor and capital.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-16-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly explains why the real wage equals the marginal product of labor and the real rental rate equals the marginal product of capital in a competitive market. The steps provided align with the gold answer, detailing the profit maximization process and the equilibrium conditions for labor and capital."}, "llm_echoed_qid": "textbook-111-16-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly explains why the real wage equals the marginal product of labor and the real rental rate equals the marginal product of capital in a competitive market. The steps provided align with the gold answer, detailing the profit maximization process and the equilibrium conditions for labor and capital."}
{"script_processing_qid": "textbook-51-1-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Critically evaluate the claim that the Fisher effect was harder to detect before 1914 due to inflation being a white noise process. What alternative explanations could account for this observation?\n\nGOLD_ANSWER:\n1. **White Noise Inflation**: If inflation ($\\pi_t$) is white noise, $E_t[\\pi_{t+1}]$ is constant, making the Fisher effect ($i_t = r_t + E_t[\\pi_{t+1}]$) harder to detect empirically.\n2. **Alternative Explanations**:\n   - Data limitations: Historical inflation and interest rate data may be noisy or incomplete.\n   - Institutional factors: Financial markets were less integrated, obscuring the Fisher effect.\n   - Measurement error: Real interest rates ($r_t$) may have been mismeasured due to unobserved risk premiums.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the original claim. Before 1914, inflation was a white noise process (unpredictable, mean-reverting), making it difficult to isolate a systematic Fisher effect (nominal rates adjusting to expected inflation). Since white noise implies no persistence, market participants would expect inflation to revert to its mean quickly, reducing the need for long-term rate adjustments.\n\nStep 2: Evaluate the claim's validity. The context supports this: under the gold standard (pre-1914), price-level expectations were anchored, causing inflation shocks to dissipate rapidly. This aligns with white noise behavior, weakening the link between observed inflation and interest rate adjustments.\n\nStep 3: Identify alternative explanations. (a) Monetary regime differences: Pre-1914, the gold standard constrained central banks from smoothing short-term rates, allowing the expectations theory of term structure to hold. Post-1914 fiat regimes introduced rate smoothing, altering how shocks propagate. (b) Persistence of short-term rates: Increased persistence after 1914 (due to policy smoothing) amplified long-rate responses to short-rate shocks, indirectly strengthening the Fisher effect's visibility. (c) Anchored expectations: Pre-1914 mean reversion in price levels under the gold standard may have decoupled inflation expectations from short-term noise, masking the Fisher effect even if present structurally.\n\nStep 4: Critical assessment. While the white noise explanation is consistent with the data, the breakdown of the term structure relationship post-1914 and shifts in monetary policy implementation likely interacted with inflation persistence to obscure/drive the Fisher effect. The difficulty pre-1914 may stem as much from institutional anchors on expectations as from pure statistical properties of inflation.\n\nQID: textbook-51-1-2-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-51-1-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly evaluates the claim, aligns with the gold standard context, and provides well-reasoned alternative explanations (monetary regime differences, persistence of short-term rates, and anchored expectations). It critically assesses the interaction of institutional factors and statistical properties, matching the depth and accuracy of the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-51-1-2-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly evaluates the claim, aligns with the gold standard context, and provides well-reasoned alternative explanations (monetary regime differences, persistence of short-term rates, and anchored expectations). It critically assesses the interaction of institutional factors and statistical properties, matching the depth and accuracy of the gold answer."}, "llm_echoed_qid": "textbook-51-1-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly evaluates the claim, aligns with the gold standard context, and provides well-reasoned alternative explanations (monetary regime differences, persistence of short-term rates, and anchored expectations). It critically assesses the interaction of institutional factors and statistical properties, matching the depth and accuracy of the gold answer."}
{"script_processing_qid": "textbook-47-2-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What is the condition for the existence of an $\\epsilon$-SSE near a single steady state $\\hat{y}$?\n\nGOLD_ANSWER:\nAn $\\epsilon$-SSE near a single steady state $\\hat{y}$ exists if and only if $|f^{\\prime}(\\hat{y})| > 1$. This condition ensures that small deviations from the steady state can be sustained as sunspot equilibria.\n\nCANDIDATE_ANSWER:\nStep 1: Consider small deviations from the steady state \\(\\hat{y}\\). Let \\(y_1^* = \\hat{y} + \\epsilon_1\\) and \\(y_2^* = \\hat{y} + \\epsilon_2\\), where \\(\\epsilon_1, \\epsilon_2\\) are small.\\nStep 2: Linearize the SSE equations using a first-order Taylor expansion of \\(f\\) around \\(\\hat{y}\\):\\n\\[\\begin{aligned} \\epsilon_1 &\\approx f'(\\hat{y})[\\pi_{11}\\epsilon_1 + (1 - \\pi_{11})\\epsilon_2] \\\\ \\epsilon_2 &\\approx f'(\\hat{y})[(1 - \\pi_{22})\\epsilon_1 + \\pi_{22}\\epsilon_2] \\end{aligned}\\]\\nStep 3: Express this as an eigenvalue problem \\(\\epsilon = f'(\\hat{y})M\\epsilon\\), where \\(M = \\begin{bmatrix}\\pi_{11} & 1 - \\pi_{11}\\\\1 - \\pi_{22} & \\pi_{22}\\end{bmatrix}\\).\\nStep 4: For non-trivial solutions, \\(f'(\\hat{y})\\) must invert an eigenvalue of \\(M\\). This requires \\(1/f'(\\hat{y})\\) to be an eigenvalue of \\(M\\).\\nStep 5: Solve the characteristic equation for \\(M\\): \\n\\[\\lambda^2 - (\\pi_{11} + \\pi_{22})\\lambda + (\\pi_{11} + \\pi_{22} - 1) = 0\\]\\nStep 6: Substitute \\(\\lambda = 1/f'(\\hat{y})\\) into the characteristic equation and simplify to obtain the condition:\\n\\[(\\pi_{11} + \\pi_{22} - 1)f'(\\hat{y})^2 - (\\pi_{11} + \\pi_{22})f'(\\hat{y}) + 1 = 0\\]\\nStep 7: Show this equation has real solutions when \\(|f'(\\hat{y})| > 1\\). The existence of an \\(\\epsilon\\)-SSE near \\(\\hat{y}\\) therefore requires \\(|f'(\\hat{y})| > 1\\).\n\nQID: textbook-47-2-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-47-2-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer provides a detailed and correct derivation of the condition for the existence of an $\\epsilon$-SSE near a single steady state $\\hat{y}$, concluding with the correct condition $|f^{\\prime}(\\hat{y})| > 1$, which matches the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-47-2-1-2", "category": "CORRECT", "explanation": "The candidate answer provides a detailed and correct derivation of the condition for the existence of an $\\epsilon$-SSE near a single steady state $\\hat{y}$, concluding with the correct condition $|f^{\\prime}(\\hat{y})| > 1$, which matches the gold answer."}, "llm_echoed_qid": "textbook-47-2-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer provides a detailed and correct derivation of the condition for the existence of an $\\epsilon$-SSE near a single steady state $\\hat{y}$, concluding with the correct condition $|f^{\\prime}(\\hat{y})| > 1$, which matches the gold answer."}
{"script_processing_qid": "textbook-125-54-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate the net convenience yield for a commodity given the spot price ($\\$2,550$), futures price ($\\$2,408$), interest rate (12%), and storage costs ($\\$100$).\n\nGOLD_ANSWER:\n3. **Net Convenience Yield Calculation**: The net convenience yield (NCY) can be derived from the cost-of-carry model: $$F = S \\times e^{(r + s - y)T}$$ where $F$ is the futures price, $S$ is the spot price, $r$ is the interest rate, $s$ is the storage cost, and $y$ is the convenience yield. Rearranging for $y$: $$y = r + s - \\frac{\\ln(F/S)}{T}$$ Substituting the given values: $$y = 0.12 + \\frac{100}{2550} - \\frac{\\ln(2408/2550)}{1} \\approx 0.12 + 0.0392 - (-0.057) \\approx 0.2162$$ or 21.62%.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the net convenience yield formula using discrete compounding:\nNet Convenience Yield (NCY) = S(1 + r) + Storage Costs - F\n\nStep 2: Plug in given values:\n- Spot Price (S) = $2,550\n- Interest Rate (r) = 12% = 0.12\n- Storage Costs = $100\n- Futures Price (F) = $2,408\n\nStep 3: Calculate S(1 + r):\n$2,550 × 1.12 = $2,856\n\nStep 4: Add storage costs:\n$2,856 + $100 = $2,956\n\nStep 5: Subtract futures price:\n$2,956 - $2,408 = $548\n\nFinal Answer: The net convenience yield is $548.\n\nQID: textbook-125-54-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-54-0-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate used an incorrect formula for calculating the net convenience yield, which should be derived from the cost-of-carry model. The candidate's approach does not align with the correct method provided in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-54-0-2", "category": "INCORRECT", "explanation": "The candidate used an incorrect formula for calculating the net convenience yield, which should be derived from the cost-of-carry model. The candidate's approach does not align with the correct method provided in the gold answer."}, "llm_echoed_qid": "textbook-125-54-0-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate used an incorrect formula for calculating the net convenience yield, which should be derived from the cost-of-carry model. The candidate's approach does not align with the correct method provided in the gold answer."}
{"script_processing_qid": "textbook-99-9-3-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the Nadaraya-Watson kernel regression estimator for a given kernel $K$ and bandwidth $h$.\n\nGOLD_ANSWER:\n1. **Kernel Weighting**: Define weights $w_i(x) = K\\left(\\frac{x - X_i}{h}\\right)$ for each observation $(X_i, Y_i)$.\n2. **Estimator Form**: The Nadaraya-Watson estimator is $\\hat{m}(x) = \\frac{\\sum_{i=1}^n w_i(x) Y_i}{\\sum_{i=1}^n w_i(x)}$.\n3. **Interpretation**: A weighted average of $Y_i$, where weights depend on the proximity of $X_i$ to $x$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that the Nadaraya-Watson estimator aims to estimate the conditional mean 𝔼[Y|X=x] using kernel-weighted local averaging.\n\nStep 2: For observed data {(X₁,Y₁),...,(Xₙ,Yₙ)}, the estimator is derived by minimizing a locally weighted sum of squared errors:\n   ∑ₙ [Yᵢ - m(x)]² K[(x - Xᵢ)/h]\n   where K(·) is the kernel function and h is the bandwidth.\n\nStep 3: Take the derivative with respect to m(x) and set it to zero:\n   -2∑ₙ [Yᵢ - m(x)] K[(x - Xᵢ)/h] = 0\n\nStep 4: Solve for m(x):\n   m(x) = [∑ₙ Yᵢ K((x - Xᵢ)/h)] / [∑ₙ K((x - Xᵢ)/h)]\n\nThis gives the Nadaraya-Watson estimator formula:\n   m̂_{NW}(x) = [∑ Yᵢ K((x - Xᵢ)/h)] / [∑ K((x - Xᵢ)/h)]\n\nQID: textbook-99-9-3-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-9-3-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the Nadaraya-Watson kernel regression estimator through minimization of locally weighted squared errors, matching the form and interpretation given in the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-9-3-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the Nadaraya-Watson kernel regression estimator through minimization of locally weighted squared errors, matching the form and interpretation given in the gold answer."}, "llm_echoed_qid": "textbook-99-9-3-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the Nadaraya-Watson kernel regression estimator through minimization of locally weighted squared errors, matching the form and interpretation given in the gold answer."}
{"script_processing_qid": "textbook-101-3-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the expansion of the energy function $E(\\mathbf{l})$ in Lemma 7.4.2 and its significance for Gibbs processes.\n\nGOLD_ANSWER:\n1. **Expansion**: The energy function can be written as a sum of terms involving single pixels, pairs of pixels, etc., up to $n$-tuples:\n$$\nE(\\mathbf{l}) = \\sum_{i} l_{i} \\alpha_{i}(l_{i}) + \\sum_{i<j} l_{i} l_{j} \\alpha_{ij}(l_{i}, l_{j}) + \\dots + l_{1} \\dots l_{n} \\alpha_{1 \\dots n}(l_{1}, \\dots, l_{n}).\n$$\n2. **Significance**: This expansion captures interactions between pixels at various orders. The coefficients $\\alpha_{i_{1} \\dots i_{k}}$ represent the strength of interactions among pixels $x_{i_{1}}, \\dots, x_{i_{k}}$.\n3. **Gibbs Processes**: The expansion allows modeling complex dependencies in spatial data, such as image textures, by considering interactions at different scales.\n\nCANDIDATE_ANSWER:\nStep 1: The energy function $E(\\mathbf{l})$ in Lemma 7.4.2 is expanded as a polynomial in occupation numbers $l_i \\in \\{0,1\\}$, where $\\mathbf{l} = (l_1,\\dots,l_n)$ represents site occupancy states. The expansion includes:\n- Single-site terms $l_i\\alpha_i(l_i)$ (individual site contributions)\n- Pairwise interaction terms $l_il_j\\alpha_{ij}(l_i,l_j)$\n- ... up to full n-site interaction terms $l_1\\cdots l_n\\alpha_{1\\dots n}(l_1,\\dots,l_n)$\n\nStep 2: The coefficients $\\alpha_{i_1...i_k}$ capture k-body interactions between sites. Lemma 7.4.3 (Hammersley-Clifford) adds that non-clique interactions vanish ($\\alpha_{i_1...i_k}=0$ if $\\{x_{i_1},...,x_{i_k}\\}$ isn't a clique in the neighborhood system).\n\nStep 3: Significance for Gibbs processes:\n1. The expansion provides a complete parametrization of local interactions\n2. Ensures consistency with the Markov property via clique-based interactions\n3. Allows systematic construction of energy functions for MRFs\n4. Facilitates computation of conditional probabilities through additive decomposition\n5. Enables physical interpretation of energy as sum of localized interaction potentials\n\nThis structure is fundamental for applications like image restoration, where pixel interactions are naturally local and hierarchical (single-pixel intensity + neighbor correlations + higher-order patterns).\n\nQID: textbook-101-3-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-101-3-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately describes the expansion of the energy function and its significance for Gibbs processes, including the role of coefficients and the Hammersley-Clifford theorem. It also provides additional relevant details about applications and computational aspects.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-101-3-1-1", "category": "CORRECT", "explanation": "The candidate answer accurately describes the expansion of the energy function and its significance for Gibbs processes, including the role of coefficients and the Hammersley-Clifford theorem. It also provides additional relevant details about applications and computational aspects."}, "llm_echoed_qid": "textbook-101-3-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately describes the expansion of the energy function and its significance for Gibbs processes, including the role of coefficients and the Hammersley-Clifford theorem. It also provides additional relevant details about applications and computational aspects."}
{"script_processing_qid": "textbook-114-10-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the challenges and limitations of using interview and experimental approaches to estimate demand. How do these methods compare to the statistical approach?\n\nGOLD_ANSWER:\n1.  **Interview Challenges**: Interviews may suffer from respondents' lack of information, disinterest, or intentional misleading. Indirect approaches can mitigate some issues but may still yield unreliable data.\n2.  **Experimental Limitations**: Direct experiments are costly and may not isolate the effect of price changes due to confounding variables. Consumers may also respond differently to temporary versus permanent changes.\n3.  **Comparison to Statistical Approach**: The statistical approach uses actual market data to estimate demand, controlling for multiple variables. It is more reliable but requires robust data and sophisticated analysis techniques.\n\nCANDIDATE_ANSWER:\nStep 1: Challenges of interview approaches\n- **Response bias**: Consumers may misrepresent preferences (e.g., underreport willingness to buy snob goods if they perceive exclusive ownership).\n- **Hypothetical scenarios**: May not reflect real purchasing decisions affected by network effects (e.g., bandwagon effects in Figure 4.14 require observing actual peer behavior).\n\nStep 2: Limitations of experimental approaches\n- **Cost/time constraints**: Direct marketing experiments (e.g., varying prices for raspberries) may fail to capture long-term network externality dynamics.\n- **External validity**: Results may not generalize due to lab/field experiment settings not replicating real-world social interactions.\n\nStep 3: Comparison with statistical methods\n- **Statistical approaches** (e.g., analyzing Table 4.5 data) objectively quantify relationships between variables (price/quantity) but:\n  - Require sufficient historical data on network effects\n  - May miss latent behavioral drivers (e.g., social motivations for snob/bandwagon effects)\n- **Key distinction**: Interviews/experiments directly probe intentions/causality but struggle with external validity; statistical methods leverage observational data but face omitted variable bias.\n\nQID: textbook-114-10-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-10-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses the challenges and limitations of interview and experimental approaches, and provides a detailed comparison to statistical methods, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-10-0-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses the challenges and limitations of interview and experimental approaches, and provides a detailed comparison to statistical methods, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-114-10-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses the challenges and limitations of interview and experimental approaches, and provides a detailed comparison to statistical methods, aligning well with the gold answer."}
{"script_processing_qid": "textbook-119-21-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the importance of modeling volatility in financial time series for economic decision-making.\n\nGOLD_ANSWER:\n1. **Risk Management**: Understanding volatility helps in assessing and mitigating financial risks.\n2. **Investment Decisions**: High volatility implies greater uncertainty, affecting portfolio choices.\n3. **Policy Formulation**: Central banks monitor inflation volatility to ensure price stability.\n4. **Financial Planning**: Businesses and individuals need stable environments for long-term planning.\n\nCANDIDATE_ANSWER:\nStep 1: Volatility clustering is a key characteristic of financial time series (e.g., stock prices, inflation rates), where large price swings cluster in time. This variability impacts economic decision-making because uncertainty in financial variables complicates planning.\n\nStep 2: Volatility modeling addresses the time-varying variance of financial series. For example, inflation variability (not just its level) creates financial planning challenges, as businesses and policymakers need predictable environments.\n\nStep 3: Traditional models (e.g., first-difference approaches) fail to capture volatility persistence. ARCH/GARCH models explicitly account for autocorrelation in variance, allowing forecasts of future volatility based on past patterns.\n\nStep 4: Accurate volatility estimates enable better risk assessment for portfolio optimization, derivative pricing, and policy design. For instance, central banks use inflation volatility models to design stabilization policies, while investors adjust asset allocations based on predicted market risk.\n\nStep 5: Ignoring volatility clustering leads to underestimation of tail risks and inefficient decisions. ARCH/GARCH frameworks provide systematic tools to quantify and manage these risks, directly improving economic outcomes.\n\nQID: textbook-119-21-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-21-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses the importance of modeling volatility in financial time series, aligning with the gold answer's key points while providing additional technical depth and examples.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-21-1-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses the importance of modeling volatility in financial time series, aligning with the gold answer's key points while providing additional technical depth and examples."}, "llm_echoed_qid": "textbook-119-21-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses the importance of modeling volatility in financial time series, aligning with the gold answer's key points while providing additional technical depth and examples."}
{"script_processing_qid": "textbook-125-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the after-tax cost of debt for a corporation given a before-tax interest rate of 8% and a corporate tax rate of 30%. Explain the tax shield benefit of debt.\n\nGOLD_ANSWER:\n1. **After-Tax Cost of Debt**: \n   - Formula: $AfterTaxCost = BeforeTaxRate \\times (1 - TaxRate)$\n   - Calculation: $8\\% \\times (1 - 0.30) = 5.6\\%$.\n2. **Tax Shield Benefit**: \n   - Interest payments are tax-deductible, reducing taxable income.\n   - For every dollar of interest paid, the company saves $0.30 in taxes, effectively reducing the cost of debt.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the tax deductibility of interest payments\nAccording to the context, 'The company’s payments of interest are regarded as a cost and are deducted from taxable income. Thus interest is paid from before-tax income... Therefore the government provides a tax subsidy on the use of debt.'\n\nStep 2: Calculate the after-tax cost of debt formula\nAfter-tax cost of debt = Before-tax interest rate × (1 - Tax rate)\n\nStep 3: Apply given values\nBefore-tax interest rate = 8% (0.08)\nCorporate tax rate = 30% (0.30)\nAfter-tax cost = 0.08 × (1 - 0.30) = 0.08 × 0.70 = 0.056\n\nStep 4: Convert to percentage\n0.056 × 100 = 5.6%\n\nStep 5: Explain tax shield benefit\nThe tax shield arises because interest expense reduces taxable income. For every $1 paid in interest:\n- Reduces tax liability by $0.30 (30% tax rate)\n- Effective net cost becomes $0.70 ($1 interest - $0.30 tax saving)\nThis subsidy makes debt cheaper than equity (where dividend payments are not tax-deductible).\n\nQID: textbook-125-0-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly calculated the after-tax cost of debt as 5.6% and accurately explained the tax shield benefit, matching the gold answer in both calculation and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-0-0-0", "category": "CORRECT", "explanation": "The candidate correctly calculated the after-tax cost of debt as 5.6% and accurately explained the tax shield benefit, matching the gold answer in both calculation and reasoning."}, "llm_echoed_qid": "textbook-125-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly calculated the after-tax cost of debt as 5.6% and accurately explained the tax shield benefit, matching the gold answer in both calculation and reasoning."}
{"script_processing_qid": "textbook-80-5-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the computational efficiency of the profile likelihood method with the Speckman estimator for GPLM.\n\nGOLD_ANSWER:\n1. **Profile Likelihood**: Requires $O(n^2)$ operations due to kernel smoothing at all observation points.\n2. **Speckman Estimator**: Uses $\\ell_i^{\\prime}$ and $\\ell_i^{\\prime\\prime}$ instead of $\\ell_{ij}^{\\prime}$ and $\\ell_{ij}^{\\prime\\prime}$, reducing computational complexity.\n3. **Performance**: For small samples, the Speckman estimator performs better. For large samples, both methods yield similar results.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain information about the Speckman estimator's computational procedures or efficiency characteristics. Therefore, a comparison of computational efficiency between the profile likelihood method and the Speckman estimator cannot be made based on the given context.\n\nQID: textbook-80-5-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-80-5-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the lack of information.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-5-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the lack of information."}, "llm_echoed_qid": "textbook-80-5-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the lack of information."}
{"script_processing_qid": "textbook-99-2-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that (3.1.4) is an unbiased estimator of $\\theta_{j}$. Derive the conditions under which $w_{j}\\hat{\\theta}_{j}$ becomes biased.\n\nGOLD_ANSWER:\n1. **Unbiasedness**: By definition, $E[\\hat{\\theta}_{j}] = \\theta_{j}$ for (3.1.4).\n2. **Bias Condition**: $E[w_{j}\\hat{\\theta}_{j}] = w_{j}\\theta_{j} \\neq \\theta_{j}$ when $w_{j} \\neq 1$ and $\\theta_{j} \\neq 0$.\n3. **Trade-off**: A biased estimator may reduce MSE via variance reduction, justifying its use.\n\nCANDIDATE_ANSWER:\nStep 1: Prove unbiasedness of (3.1.4)\nSince $\\widehat{\\theta}_j$ is defined as the sample mean estimator (3.1.4), its expectation is:\n$$E[\\widehat{\\theta}_j] = E\\left[\\frac{1}{n}\\sum_{i=1}^n \\phi_j(X_i)\\right] = \\frac{1}{n}\\sum_{i=1}^n E[\\phi_j(X_i)]$$\nBy linearity of expectation and stationarity, this reduces to $E[\\phi_j(X_1)] = \\langle f,\\phi_j \\rangle = \\theta_j$. Thus, $E[\\widehat{\\theta}_j] = \\theta_j$, making it unbiased.\n\nStep 2: Bias in $w_j\\widehat{\\theta}_j$\nThe expectation becomes:\n$$E[w_j\\widehat{\\theta}_j] = w_jE[\\widehat{\\theta}_j] = w_j\\theta_j$$\nThis equals $\\theta_j$ only if $w_j\\theta_j = \\theta_j$, which requires either:\n1. $w_j = 1$ (no weighting), or\n2. $\\theta_j = 0$ (zero true coefficient).\nThus, $w_j\\widehat{\\theta}_j$ is biased when $w_j \\neq 1$ AND $\\theta_j \\neq 0$.\n\nStep 3: Rationale for preferring biased estimator\nThough biased, $w_j\\widehat{\\theta}_j$ may achieve lower mean squared error (MSE) through variance reduction. This bias-variance trade-off is central to regularization methods in nonparametric estimation.\n\nQID: textbook-99-2-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-99-2-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly proves the unbiasedness of the estimator (3.1.4) and derives the conditions under which $w_{j}\\hat{\\theta}_{j}$ becomes biased. The reasoning and conclusions align with the gold answer, including the rationale for preferring a biased estimator due to potential MSE reduction.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-2-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly proves the unbiasedness of the estimator (3.1.4) and derives the conditions under which $w_{j}\\hat{\theta}_{j}$ becomes biased. The reasoning and conclusions align with the gold answer, including the rationale for preferring a biased estimator due to potential MSE reduction."}, "llm_echoed_qid": "textbook-99-2-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly proves the unbiasedness of the estimator (3.1.4) and derives the conditions under which $w_{j}\\hat{\theta}_{j}$ becomes biased. The reasoning and conclusions align with the gold answer, including the rationale for preferring a biased estimator due to potential MSE reduction."}
{"script_processing_qid": "textbook-93-3-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Under the conditions of Theorem 3.3, show that weak ergodicity occurs at a geometric rate. What role does the constant $\\gamma$ play in this result?\n\nGOLD_ANSWER:\n1. **Geometric Rate**: The convergence $\\tau\\_{B}(T\\_{p,r}) \\rightarrow 0$ is bounded by $(1 - \\gamma^{r_0} / n^{r_0 - 1})^{r/r_0}$.\n2. **Role of $\\gamma$**: Ensures uniform positivity of matrix elements, guaranteeing $\\phi(H\\_{p,r_0}) \\geq (\\gamma^{r_0} / n^{r_0 - 1})^2$, which drives the geometric convergence.\n3. **Key Step**: Apply the Corollary to Theorem 3.2 with $g = r_0$ to derive the rate.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that weak ergodicity requires the Birkhoff contraction coefficient τ_B(T_{p,r}) → 0 as r→∞ (Lemma 3.4). Theorem 3.3 provides two key conditions:\n- (i) Uniform positivity after r₀ steps (H_{p,r₀} > 0 for all p)\n- (ii) A uniform lower bound γ on min⁺ h_{ij}(k)/max h_{ij}(k) across all matrices H_k.\n\nStep 2: Condition (ii) ensures all H_k have bounded projective diameter. The ratio γ controls the maximum disparity between scaled row elements, which governs the Birkhoff contraction coefficient τ(H_k). Specifically, τ(H_k) ≤ (1 - γ)/(1 + γ) < 1 for each H_k.\n\nStep 3: Using submultiplicativity of τ_B under matrix products (τ_B(T_{p,r}) ≤ Π_{k=p}^{p+r-1} τ_B(H_k)), and the uniform bound τ_B(H_k) ≤ (1 - γ)/(1 + γ), we get τ_B(T_{p,r}) ≤ [(1 - γ)/(1 + γ)]^r. This decays geometrically as r→∞.\n\nStep 4: The constant γ directly determines the exponential decay rate. Larger γ (closer to 1) implies faster convergence (smaller base in the geometric decay), while smaller γ slows convergence but preserves the geometric rate as long as γ > 0.\n\nQID: textbook-93-3-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-93-3-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the geometric rate of convergence and the role of the constant γ in ensuring uniform positivity and driving the convergence. The explanation aligns with the gold answer, detailing the mathematical reasoning and the impact of γ on the convergence rate.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-93-3-0-3", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the geometric rate of convergence and the role of the constant γ in ensuring uniform positivity and driving the convergence. The explanation aligns with the gold answer, detailing the mathematical reasoning and the impact of γ on the convergence rate."}, "llm_echoed_qid": "textbook-93-3-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the geometric rate of convergence and the role of the constant γ in ensuring uniform positivity and driving the convergence. The explanation aligns with the gold answer, detailing the mathematical reasoning and the impact of γ on the convergence rate."}
{"script_processing_qid": "textbook-122-12-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Interpret the interaction terms partyWH·gnews and partyWH·inf in the context of voter behavior.\n\nGOLD_ANSWER:\n1. **partyWH·gnews (β₃ = 0.0108)**: Good economic news boosts the incumbent party's vote share (e.g., Democrats gain 1.08 percentage points per additional quarter of growth > 2.9%).\n2. **partyWH·inf (β₄ = -0.00771)**: Higher inflation reduces the incumbent's share (e.g., Democrats lose 0.771 percentage points per 1% increase in inflation).\n3. **Implications**: Voters reward/punish incumbents based on economic performance.\n\nCANDIDATE_ANSWER:\nStep 1: Understand variable coding\n- partyWH = 1 (Democrat incumbent) or -1 (Republican incumbent)\n- demvote represents Democratic vote share\n\nStep 2: Interpret partyWH·gnews interaction\n- Coefficient = +0.0108\n- For Democratic incumbent (1·gnews): Each 1-unit growth increase adds 0.0108pp to Democratic vote\n- For Republican incumbent (-1·gnews): Each 1-unit growth increase subtracts 0.0108pp from Democratic vote (equivalent to +0.0108pp Republican advantage)\n\nStep 3: Interpret partyWH·inf interaction\n- Coefficient = -0.00771\n- For Democratic incumbent (1·inf): Each 1-unit inflation increase reduces Democratic vote by 0.00771pp\n- For Republican incumbent (-1·inf): Each 1-unit inflation increase adds 0.00771pp to Democratic vote (equivalent to -0.00771pp Republican penalty)\n\nStep 4: Behavioral interpretation\n1. Voters reward incumbent party for economic growth:\n   - Democrats gain 0.0108pp per growth unit when incumbent\n   - Republicans gain equivalent advantage when incumbent\n\n2. Voters penalize incumbent party for inflation:\n   - Democrats lose 0.00771pp per inflation unit when incumbent\n   - Republicans lose equivalent vote share when incumbent\n\nQID: textbook-122-12-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-12-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately interprets both interaction terms (partyWH·gnews and partyWH·inf) and correctly explains their implications for voter behavior, matching the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-12-1-2", "category": "CORRECT", "explanation": "The candidate answer accurately interprets both interaction terms (partyWH·gnews and partyWH·inf) and correctly explains their implications for voter behavior, matching the gold answer's key points."}, "llm_echoed_qid": "textbook-122-12-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately interprets both interaction terms (partyWH·gnews and partyWH·inf) and correctly explains their implications for voter behavior, matching the gold answer's key points."}
{"script_processing_qid": "textbook-105-1-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the partitioning estimate $m_{n}^{(part)}(x)$ with the local averaging estimate $\\tilde{m}_{n,1}(x)$. What are the advantages and limitations of each?\n\nGOLD_ANSWER:\n1. **Partitioning Estimate**: Uses fixed bins (partitions) for local averaging. Simpler but may suffer from boundary bias.\n2. **Local Averaging Estimate**: Uses adaptive weights $W_{ni}(x)$ (e.g., k-NN or kernel weights). More flexible but computationally intensive. Both rely on consistent estimation of $G_{n}(t)$ for censoring adjustment.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the estimators' structures. The partitioning estimate $m_n^{(part)}(x)$ divides the input space into cells and averages responses within each cell, while the local averaging estimate $\\tilde{m}_{n,1}(x)$ uses weighted averages based on proximity to $x$, with weights $W_{ni}(x)$ and inverse Kaplan-Meier adjustments.\n\nStep 2: Advantages of partitioning. \n- Guaranteed $L_2$ convergence (Theorem 26.2) under assumptions.\n- Simpler implementation due to discrete binning.\n- Reduced variance in regions with sufficient data per partition.\n\nLimitations of partitioning:\n- Bias due to within-bin averaging, especially if $m(x)$ varies nonlinearly within bins.\n- Sensitivity to partition granularity (trade-off between bias and variance).\n\nStep 3: Advantages of local averaging:\n- Flexibility to adapt to local structure via $W_{ni}(x)$.\n- Smooth estimates compared to discrete partitioning.\n\nLimitations of local averaging:\n- Sensitivity to $G_n(Z_i)$ estimation errors (Lemma 26.1 ensures asymptotic correction, but finite-sample errors may persist).\n- Potential instability in sparse regions due to kernel/bandwidth choices.\n\nStep 4: Key trade-off. Partitioning offers robustness via structured bins but sacrifices resolution; local averaging provides smoother adaptation to $x$ but depends on reliable $G_n$ estimation and weighting schemes.\n\nQID: textbook-105-1-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-105-1-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly compares the partitioning and local averaging estimates, detailing their advantages and limitations in alignment with the gold answer. The explanation includes specific technical aspects such as $L_2$ convergence, bias-variance trade-offs, and sensitivity to estimation errors, which are consistent with the gold answer's points.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-105-1-1-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly compares the partitioning and local averaging estimates, detailing their advantages and limitations in alignment with the gold answer. The explanation includes specific technical aspects such as $L_2$ convergence, bias-variance trade-offs, and sensitivity to estimation errors, which are consistent with the gold answer's points."}, "llm_echoed_qid": "textbook-105-1-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly compares the partitioning and local averaging estimates, detailing their advantages and limitations in alignment with the gold answer. The explanation includes specific technical aspects such as $L_2$ convergence, bias-variance trade-offs, and sensitivity to estimation errors, which are consistent with the gold answer's points."}
{"script_processing_qid": "textbook-46-0-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. How would you estimate $\\gamma$ and $\\varepsilon$ empirically? Discuss two approaches.\n\nGOLD_ANSWER:\n1. **Direct estimation**: Regress firm-level output on $k$, $n$, $Y$, and $Y_{-1}$ using panel data.\n2. **Indirect inference**: Simulate the model with candidate $\\gamma$, $\\varepsilon$ and match moments (e.g., output volatility, persistence) to observed data.\n\nCANDIDATE_ANSWER:\nStep 1: Log-linearize the production function and exploit symmetric equilibrium.\nSubstitute y_t = Y_t into the production function:\nln Y_t = ln A + α ln k_t + φ ln n_t + γ ln Y_t + ε ln Y_{t-1}.\nRearrange to isolate Y_t terms:\nln Y_t(1-γ) = ln A + α ln k_t + φ ln n_t + ε ln Y_{t-1}.\nDivide through by (1-γ):\nln Y_t = [α/(1-γ)] ln k_t + [φ/(1-γ)] ln n_t + [ε/(1-γ)] ln Y_{t-1} + [ln A]/(1-γ).\n\nStep 2: First approach (Nonlinear estimation)\nEstimate via nonlinear least squares to directly recover γ and ε from the structural form:\nY_t = A k_t^α n_t^φ Y_t^γ Y_{t-1}^ε.\nThis requires addressing endogeneity (A may correlate with inputs) using:\n- Fixed effects for time-varying productivity shocks\n- Instrumental variables for contemporaneous Y_t\n\nStep 3: Second approach (Dynamic panel methods)\nUse a transformed equation:\nΔln Y_t = [α/(1-γ)] Δln k_t + [φ/(1-γ)] Δln n_t + [ε/(1-γ)] Δln Y_{t-1} + Δu_t,\nwhere u_t = ln A/(1-γ).\nEmploy Arellano-Bond GMM estimator with lagged levels as instruments for differenced variables to handle:\n- Endogeneity of inputs\n- Persistence through Y_{t-1}\n\nStep 4: Alternative system approach\nCombine first-order conditions with production function.\nFrom FOCs for capital/labor (MPK = r, MPL = w):\nαY_t/k_t = r_t ⇒ α = r_tk_t/Y_t\nφY_t/n_t = w_t ⇒ φ = w_tn_t/Y_t\nSubstitute these into the production function to create moment conditions for GMM estimation of γ and ε.\n\nQID: textbook-46-0-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-46-0-1-2\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate provides detailed and valid approaches to estimate γ and ε, including nonlinear estimation and dynamic panel methods, which are correct in principle. However, the reasoning and specific steps do not directly align with the gold answer's methods of direct estimation and indirect inference, leading to a mismatch in the explanation of the approaches.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-46-0-1-2", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate provides detailed and valid approaches to estimate γ and ε, including nonlinear estimation and dynamic panel methods, which are correct in principle. However, the reasoning and specific steps do not directly align with the gold answer's methods of direct estimation and indirect inference, leading to a mismatch in the explanation of the approaches."}, "llm_echoed_qid": "textbook-46-0-1-2", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate provides detailed and valid approaches to estimate γ and ε, including nonlinear estimation and dynamic panel methods, which are correct in principle. However, the reasoning and specific steps do not directly align with the gold answer's methods of direct estimation and indirect inference, leading to a mismatch in the explanation of the approaches."}
{"script_processing_qid": "textbook-119-28-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain how the Dickey-Fuller test's asymptotic 95% confidence bounds ($\\pm 0.2089$) are used to assess stationarity in the ACF/PACF of Figure 22.3.\n\nGOLD_ANSWER:\n1. **Hypothesis Testing**: If ACF/PACF values lie within $\\pm 0.2089$, they are insignificantly different from zero (consistent with stationarity).\n2. **Application**: In Figure 22.3, most ACF/PACF lags (except 1, 8, 12) fall within these bounds, supporting stationarity of differenced GDP.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain sufficient information to explain the use of Dickey-Fuller test's asymptotic 95% confidence bounds ($\\pm 0.2089$) in assessing stationarity via ACF/PACF. The context exclusively discusses the Box-Jenkins methodology for ARIMA model identification, relying on ACF/PACF patterns to determine $p$, $d$, and $q$. While it mentions confidence bounds for ACF/PACF interpretation, it does not reference the Dickey-Fuller test or its specific critical values. The Dickey-Fuller test is a separate unit root test for stationarity, and its application is not discussed in the provided text.\n\nQID: textbook-119-28-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-28-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain sufficient information'.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-28-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain sufficient information'."}, "llm_echoed_qid": "textbook-119-28-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain sufficient information'."}
{"script_processing_qid": "textbook-70-0-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why $\\gamma > 1$ is necessary for the order selection test to be valid.\n\nGOLD_ANSWER:\nThe condition $\\gamma > 1$ ensures that the random walk $S(m;\\gamma)$ has a negative drift ($E[Z_{j}^{2} - \\gamma] = 1 - \\gamma < 0$). This guarantees that the maximizer $\\tilde{m}_{\\gamma}$ is almost surely finite, which is necessary for the test to have a well-defined limiting distribution and finite critical values.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the random walk structure\nUnder H₀, the normalized squared coefficients 2nφ̂ⱼ²/σ̂² asymptotically behave like independent χ²(1) variables (equivalent to Zⱼ² where Zⱼ ∼ N(0,1)). This means the test statistic is built from terms with expectation E[Zⱼ²] = 1.\n\nStep 2: Analyze the drift in the random walk\nFor the random walk S(m;γ) = ∑(Zⱼ² - γ), each increment has expectation E[Zⱼ² - γ] = 1 - γ. When γ > 1, this expectation becomes negative, creating a downward drift in the random walk.\n\nStep 3: Ensure finite maximum\nWith γ > 1, the negative drift guarantees the random walk S(m;γ) will eventually drift to -∞ as m increases. This ensures the maximizer ˆm_γ remains finite almost surely under H₀, which is required for a valid asymptotic distribution.\n\nStep 4: Prevent overfitting\nIf γ ≤ 1, the penalty term -γm would be insufficient to counterbalance the positive contributions from the Zⱼ² terms. This could lead to ˆm_γ → ∞ under H₀, causing invalid test size.\n\nStep 5: Maintain test validity\nγ > 1 controls Type I error by ensuring: (1) The maximum occurs at m=0 with positive probability asymptotically, and (2) The critical value for rejection can be properly calibrated.\n\nQID: textbook-70-0-2-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-70-0-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains why γ > 1 is necessary for the order selection test to be valid, detailing the random walk structure, the negative drift, the finiteness of the maximizer, and the prevention of overfitting, all of which align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-70-0-2-1", "category": "CORRECT", "explanation": "The candidate answer correctly explains why γ > 1 is necessary for the order selection test to be valid, detailing the random walk structure, the negative drift, the finiteness of the maximizer, and the prevention of overfitting, all of which align with the gold answer."}, "llm_echoed_qid": "textbook-70-0-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains why γ > 1 is necessary for the order selection test to be valid, detailing the random walk structure, the negative drift, the finiteness of the maximizer, and the prevention of overfitting, all of which align with the gold answer."}
{"script_processing_qid": "textbook-80-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the estimation procedure for $\\beta$ and $c$ in the APLM model $$ E(Y|U,T)=U^{\\top}\\beta+c+\\sum_{\\alpha=1}^{q}g_{\\alpha}(T_{\\alpha}) $$ using marginal integration. Explain why undersmoothing is necessary.\n\nGOLD_ANSWER:\n1.  **Objective**: Estimate $\\beta$ and $c$ in the APLM model.\n2.  **Marginal Integration**: For each $T_{\\alpha}$, integrate out the other nonparametric components to estimate $g_{\\alpha}(T_{\\alpha})$.\n3.  **Parametric Estimation**: Use least squares to estimate $\\beta$ and $c$ from the residuals $Y - \\sum_{\\alpha=1}^{q} \\hat{g}_{\\alpha}(T_{\\alpha})$.\n4.  **Undersmoothing**: Necessary to ensure the bias from the nonparametric estimation does not dominate the parametric rate $\\sqrt{n}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize the APLM structure $E(Y|U,T) = U^\\top\\beta + c + \\sum g_\\alpha(T_\\alpha)$. The parametric components ($\\beta$, $c$) require separation from nonparametric components ($g_\\alpha$).\n\nStep 2: Use marginal integration to estimate $g_\\alpha$ via undersmoothed kernel regression. For each $T_\\alpha$:\n   a. Compute residuals $R = Y - U^\\top\\beta^{(k)} - c^{(k)}$ using current parametric estimates\n   b. Estimate $\\hat{g}_\\alpha(T_\\alpha) = \\frac{1}{n}\\sum_{i=1}^n R_i K_h(T_{\\alpha,i}-t)$ with bandwidth $h \\to 0$ faster than optimal\n\nStep 3: Update parametric components via OLS:\n   $\\hat{\\beta} = (\\tilde{U}^\\top\\tilde{U})^{-1}\\tilde{U}^\\top\\tilde{Y}$\n   Where $\\tilde{Y} = Y - \\sum\\hat{g}_\\alpha(T_\\alpha)$ and $\\tilde{U}$ is U centered by its mean\n\nStep 4: Iterate Steps 2-3 until convergence\n\nUndersmoothing necessity: Optimal bandwidths leave $O(h^2)$ bias in $\\hat{g}_\\alpha$. Undersmoothing ensures $nh^4 \\to 0$, making bias asymptotically negligible compared to $\\sqrt{n}$-consistent parametric estimates. This preserves the $\\sqrt{n}$ convergence rate for $\\beta$ and $c$.\n\nQID: textbook-80-1-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-80-1-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes the estimation procedure for $\\beta$ and $c$ using marginal integration and correctly explains the necessity of undersmoothing to ensure the parametric estimates achieve $\\sqrt{n}$-consistency. The steps and reasoning align well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-1-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately describes the estimation procedure for $\beta$ and $c$ using marginal integration and correctly explains the necessity of undersmoothing to ensure the parametric estimates achieve $\\sqrt{n}$-consistency. The steps and reasoning align well with the gold answer."}, "llm_echoed_qid": "textbook-80-1-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes the estimation procedure for $\beta$ and $c$ using marginal integration and correctly explains the necessity of undersmoothing to ensure the parametric estimates achieve $\\sqrt{n}$-consistency. The steps and reasoning align well with the gold answer."}
{"script_processing_qid": "textbook-54-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Starting from the AR(1) process $x_{t}=\\phi x_{t-1}+\\epsilon_{t}$, derive the MA(∞) representation using recursive substitution. Show all steps.\n\nGOLD_ANSWER:\n1. **Initial AR(1)**: $x_{t}=\\phi x_{t-1}+\\epsilon_{t}$.\n2. **First substitution**: $x_{t}=\\phi(\\phi x_{t-2}+\\epsilon_{t-1})+\\epsilon_{t}=\\phi^{2}x_{t-2}+\\phi\\epsilon_{t-1}+\\epsilon_{t}$.\n3. **Second substitution**: $x_{t}=\\phi^{2}(\\phi x_{t-3}+\\epsilon_{t-2})+\\phi\\epsilon_{t-1}+\\epsilon_{t}=\\phi^{3}x_{t-3}+\\phi^{2}\\epsilon_{t-2}+\\phi\\epsilon_{t-1}+\\epsilon_{t}$.\n4. **General form**: $x_{t}=\\phi^{k}x_{t-k}+\\sum_{j=0}^{k-1}\\phi^{j}\\epsilon_{t-j}$.\n5. **Limit as $k\\to\\infty$**: If $|\\phi|<1$, $\\phi^{k}x_{t-k}\\to 0$, so $x_{t}=\\sum_{j=0}^{\\infty}\\phi^{j}\\epsilon_{t-j}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the AR(1) process: \n$$x_{t} = \\phi x_{t-1} + \\epsilon_{t}.$$\n\nStep 2: Substitute $x_{t-1}$ into itself using the AR(1) structure:\n$$x_{t} = \\phi (\\phi x_{t-2} + \\epsilon_{t-1}) + \\epsilon_{t} = \\phi^2 x_{t-2} + \\phi \\epsilon_{t-1} + \\epsilon_{t}.$$\n\nStep 3: Repeat substitution recursively $k$ times:\n$$x_{t} = \\phi^k x_{t-k} + \\sum_{j=0}^{k-1} \\phi^j \\epsilon_{t-j}.$$\n\nStep 4: Assume $|\\phi| < 1$ and take the limit as $k \\to \\infty$. The term $\\phi^k x_{t-k} \\to 0$ under this condition:\n$$x_{t} = \\lim_{k \\to \\infty} \\sum_{j=0}^{k-1} \\phi^j \\epsilon_{t-j}.$$\n\nStep 5: Convert the finite sum to an infinite series:\n$$x_{t} = \\sum_{j=0}^{\\infty} \\phi^j \\epsilon_{t-j},$$\nwhich is the MA($\\infty$) representation.\n\nQID: textbook-54-0-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-54-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly follows the steps of recursive substitution, demonstrates the general form, and correctly takes the limit to derive the MA(∞) representation under the condition |φ| < 1. The reasoning and final result match the gold answer exactly.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-54-0-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly follows the steps of recursive substitution, demonstrates the general form, and correctly takes the limit to derive the MA(∞) representation under the condition |φ| < 1. The reasoning and final result match the gold answer exactly."}, "llm_echoed_qid": "textbook-54-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly follows the steps of recursive substitution, demonstrates the general form, and correctly takes the limit to derive the MA(∞) representation under the condition |φ| < 1. The reasoning and final result match the gold answer exactly."}
{"script_processing_qid": "textbook-80-6-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Derive the Fisher scoring algorithm for estimating the parameters $\\beta$ in a GLM with a canonical link function.\n\nGOLD_ANSWER:\n1. **Log-Likelihood**: $\\widetilde{\\ell}(Y,\\mu)=\\sum_{i=1}^{n}\\left\\{Y_{i}\\theta_{i}-b(\\theta_{i})\\right\\}$.\n2. **Gradient**: $\\mathcal{D}(\\beta)=\\sum_{i=1}^{n}\\left\\{Y_{i}-\\mu_{i}\\right\\}\\frac{G^{\\prime}(\\eta_{i})}{V(\\mu_{i})}x_{i}$.\n3. **Hessian**: $E\\mathcal{H}(\\beta)=\\sum_{i=1}^{n}\\left\\{\\frac{G^{\\prime}(\\eta_{i})^{2}}{V(\\mu_{i})}\\right\\}x_{i}x_{i}^{\\top}$.\n4. **Update**: $\\beta^{new} = \\beta^{old} + (X^{\\top}WX)^{-1}X^{\\top}W\\widetilde{Y}$, where $W$ is the weight matrix and $\\widetilde{Y}$ is the adjusted dependent variable.\n\nCANDIDATE_ANSWER:\nThe provided context discusses partial linear models and nonparametric extensions of regression but does not contain the necessary information about generalized linear models (GLMs), canonical link functions, or score equations required to derive the Fisher scoring algorithm. Specifically, the context lacks details on the GLM likelihood structure, link function properties, or Fisher information matrix components. Therefore, the derivation cannot be performed using only the given context.\n\nQID: textbook-80-6-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-80-6-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide sufficient information to derive the Fisher scoring algorithm.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-6-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide sufficient information to derive the Fisher scoring algorithm."}, "llm_echoed_qid": "textbook-80-6-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide sufficient information to derive the Fisher scoring algorithm."}
{"script_processing_qid": "textbook-100-0-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Derive the posterior mean for $p$ in the beta-binomial model ($X \\\\sim \\\\text{Bin}(n,p)$, $p \\\\sim \\\\mathrm{Be}(\\\\alpha,\\\\beta)$) and show how it balances prior and data.\n\nGOLD_ANSWER:\n1. **Likelihood**: $L(p; x) \\\\propto p^x (1-p)^{n-x}$.\n2. **Prior**: $\\\\pi(p) \\\\propto p^{\\\\alpha-1} (1-p)^{\\\\beta-1}$.\n3. **Posterior**: $\\\\pi(p|x) \\\\propto p^{x+\\\\alpha-1} (1-p)^{n-x+\\\\beta-1} \\\\sim \\\\mathrm{Be}(\\\\alpha+x, \\\\beta+n-x)$.\n4. **Posterior Mean**: $\\\\mathbb{E}[p|x] = \\\\frac{\\\\alpha + x}{\\\\alpha + \\\\beta + n}$. This is a weighted average of the prior mean ($\\\\frac{\\\\alpha}{\\\\alpha+\\\\beta}$) and MLE ($\\\\frac{x}{n}$).\n\nCANDIDATE_ANSWER:\nStep 1: Recall Bayes' theorem for posterior distribution\nPosterior ∝ Likelihood × Prior\n\nStep 2: Specify components for beta-binomial model\n- Likelihood: Bin(n, p) → P(X|p) ∝ p^X (1−p)^{n−X}\n- Prior: Be(α, β) → π(p) ∝ p^{α−1}(1−p)^{β−1}\n\nStep 3: Compute posterior distribution\nPosterior ∝ p^{X+α−1}(1−p)^{n−X+β−1} ⇒ p|X ∼ Be(α + X, β + n − X)\n\nStep 4: Derive posterior mean\nE[p|X] = (α + X)/(α + β + n)\n\nStep 5: Express as weighted average\n= [α/(α + β)] × (α + β)/(α + β + n) + (X/n) × n/(α + β + n)\n\nStep 6: Interpret balancing\n- Prior contribution: α/(α + β) weighted by \"prior strength\" (α + β)\n- Data contribution: X/n weighted by sample size n\n- Total weight denominator: (α + β + n)\n\nThis shows the posterior mean is a convex combination of prior mean (α/(α + β)) and sample proportion (X/n), with relative weights reflecting the prior's effective sample size (α + β) vs. actual sample size n.\n\nQID: textbook-100-0-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-100-0-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the posterior mean for the beta-binomial model and accurately shows how it balances the prior and data. The steps and reasoning align perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-100-0-1-3", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the posterior mean for the beta-binomial model and accurately shows how it balances the prior and data. The steps and reasoning align perfectly with the gold answer."}, "llm_echoed_qid": "textbook-100-0-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the posterior mean for the beta-binomial model and accurately shows how it balances the prior and data. The steps and reasoning align perfectly with the gold answer."}
{"script_processing_qid": "textbook-80-2-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the advantages and limitations of parametric, nonparametric, and semiparametric regression models in estimating the conditional mean $E(Y|X)$.\n\nGOLD_ANSWER:\n1.  **Parametric**: Efficient and interpretable but misspecified if the functional form is incorrect.\n2.  **Nonparametric**: Flexible (no functional form assumptions) but suffers from the curse of dimensionality and slower convergence rates.\n3.  **Semiparametric**: Balances flexibility and efficiency (e.g., additive models reduce dimensionality) but requires careful selection of parametric/nonparametric components.\n\nCANDIDATE_ANSWER:\nStep 1: Parametric Models (e.g., linear regression)\nAdvantages:\n- Statistical precision due to low-dimensional parameter estimation (only β coefficients)\n- Simple interpretation of coefficients\n- Computational efficiency\nLimitations:\n- Rigid functional form (e.g., linearity) may miss nonlinear relationships\n- Prone to model misspecification bias\n\nStep 2: Nonparametric Models\nAdvantages:\n- Extreme flexibility (no a priori assumptions about functional form)\n- Can capture complex nonlinear relationships\nLimitations:\n- Suffer from curse of dimensionality (precision deteriorates rapidly with additional covariates)\n- Require large datasets for multi-variable applications\n\nStep 3: Semiparametric Models (e.g., additive model)\nAdvantages:\n- Balance flexibility and dimension reduction\n- Maintain interpretability through additive structure (e.g., α + g₁(SCHOOL) + g₂(EXP))\n- Avoid full-dimensional nonparametric estimation\nLimitations:\n- Less flexible than pure nonparametric methods (no interaction terms in example)\n- Still require larger samples than parametric models\n- May impose restrictive assumptions (e.g., additive separability in the example)\n\nQID: textbook-80-2-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-80-2-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately captures the advantages and limitations of parametric, nonparametric, and semiparametric regression models as outlined in the gold answer. The candidate provides detailed explanations for each type of model, aligning well with the gold answer's points on efficiency, flexibility, and dimensionality issues.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-2-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately captures the advantages and limitations of parametric, nonparametric, and semiparametric regression models as outlined in the gold answer. The candidate provides detailed explanations for each type of model, aligning well with the gold answer's points on efficiency, flexibility, and dimensionality issues."}, "llm_echoed_qid": "textbook-80-2-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately captures the advantages and limitations of parametric, nonparametric, and semiparametric regression models as outlined in the gold answer. The candidate provides detailed explanations for each type of model, aligning well with the gold answer's points on efficiency, flexibility, and dimensionality issues."}
{"script_processing_qid": "textbook-111-19-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. How does the slope of the IS curve change if taxes become income-dependent ($T = \\overline{T} + tY$)? Show the modified derivation.\n\nGOLD_ANSWER:\n1. **New Consumption**: $C = a + b(Y - \\overline{T} - tY)$.  \n2. **Equilibrium**: $Y = a + b(1 - t)Y - b\\overline{T} + c - dr + G$.  \n3. **Solve**: $Y = \\frac{a + c + G - b\\overline{T}}{1 - b(1 - t)} - \\frac{d}{1 - b(1 - t)}r$.  \n4. **Steeper Slope**: The denominator $1 - b(1 - t)$ is larger, making $\\frac{dr}{dY}$ more negative (steeper IS curve).\n\nCANDIDATE_ANSWER:\nStep 1: Start with the goods market equilibrium condition:\nY = C + I + G\n\nStep 2: Substitute consumption (C) with income-dependent taxes:\nC = C̄ + c(Y - T) where T = T̄ + tY\n⇒ C = C̄ + c[Y - (T̄ + tY)] = C̄ + c(1-t)Y - cT̄\n\nStep 3: Include investment (I = Ī - b*i) and government spending (G):\nY = [C̄ + c(1-t)Y - cT̄] + [Ī - b*i] + G\n\nStep 4: Rearrange to solve for Y:\nY - c(1-t)Y = C̄ - cT̄ + Ī + G - b*i\nY[1 - c(1-t)] = Ā - b*i (where Ā = autonomous components)\n\nStep 5: Solve for i:\ni = (Ā/b) - [1 - c(1-t)]/b * Y\n\nStep 6: Compare slope coefficients:\nOriginal IS slope (lump-sum T): -(1-c)/b\nModified slope (income-dependent T): -[1 - c(1-t)]/b\n\nStep 7: Analyze slope change:\nSince [1 - c(1-t)] = 1 - c + ct > (1 - c) when t > 0, the absolute slope value decreases.\nThis makes the IS curve steeper when taxes are income-dependent.\n\nQID: textbook-111-19-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-19-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's derivation and conclusion align with the gold answer. Both show that the IS curve becomes steeper when taxes are income-dependent, and the reasoning is consistent.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-19-2-1", "category": "CORRECT", "explanation": "The candidate's derivation and conclusion align with the gold answer. Both show that the IS curve becomes steeper when taxes are income-dependent, and the reasoning is consistent."}, "llm_echoed_qid": "textbook-111-19-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's derivation and conclusion align with the gold answer. Both show that the IS curve becomes steeper when taxes are income-dependent, and the reasoning is consistent."}
{"script_processing_qid": "textbook-100-4-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Interpret Corollary 5.1. What does it imply about the performance of Bayes estimators when the true prior $G\\_{0}$ is degenerate?\n\nGOLD_ANSWER:\n3. **Interpretation of Corollary 5.1**: \n   - When $G\\_{0}$ is degenerate (i.e., $V\\_{G\\_{0}}(\\theta)=0$), the inequality simplifies to $$ (E\\_{G}\\theta-E\\_{G\\_{0}}\\theta)^{2}\\leq\\frac{1+\\eta}{1-\\eta}r(G\\_{0},\\widehat{\\theta}). $$\n   - If the operational prior $G$ is mean-correct ($E\\_{G}\\theta=E\\_{G\\_{0}}\\theta$), the LHS becomes zero, and the Bayes estimator always outperforms the frequentist estimator.\n   - This highlights that mean correctness is a sufficient condition for Bayesian superiority in degenerate cases, regardless of the operational prior's variance.\n\nCANDIDATE_ANSWER:\nStep 1: Recall Corollary 5.1 states that Bayes estimators with mean-correct operational priors outperform frequentist estimators when the true prior G₀ satisfies V_{G₀}(θ) ≤ [(1+η)/(1-η)]r(G₀,̂θ).\n\nStep 2: A degenerate G₀ concentrates all probability mass at a single point θ₀. This implies V_{G₀}(θ) = 0 (no variance) and E_{G₀}θ = θ₀.\n\nStep 3: Substitute V_{G₀}(θ) = 0 into Corollary 5.1's condition. The inequality becomes 0 ≤ [(1+η)/(1-η)]r(G₀,̂θ). Since η ∈ [0,1) and r(G₀,̂θ) = Var(̂θ) ≥ 0, this inequality always holds.\n\nStep 4: Therefore, when G₀ is degenerate, mean-correct Bayes estimators automatically satisfy the dominance condition. This implies they will always have strictly smaller Bayes risk than frequentist estimators when η > 0 (i.e., whenever prior information is incorporated).\n\nQID: textbook-100-4-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-100-4-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly interprets Corollary 5.1, accurately noting the implications for Bayes estimators when the true prior is degenerate, including the conditions under which Bayes estimators outperform frequentist estimators.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-100-4-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly interprets Corollary 5.1, accurately noting the implications for Bayes estimators when the true prior is degenerate, including the conditions under which Bayes estimators outperform frequentist estimators."}, "llm_echoed_qid": "textbook-100-4-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly interprets Corollary 5.1, accurately noting the implications for Bayes estimators when the true prior is degenerate, including the conditions under which Bayes estimators outperform frequentist estimators."}
{"script_processing_qid": "textbook-86-1-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Contrast labor-augmenting ($Y = F(K, \\widetilde{N}A)$) and Hicks-neutral ($Y = A F(K, \\widetilde{N})$) technical change in terms of their implications for factor shares and balanced growth.\n\nGOLD_ANSWER:\n**Labor-Augmenting**:\n- Factor shares $s_{l}(\\tilde{k})$ depend on $\\tilde{k}$.\n- Balanced growth requires $\\dot{H}/H + \\dot{A}/A$ constant.\n**Hicks-Neutral**:\n- Factor shares $s_{l}(\\tilde{k}A)$ depend on $\\tilde{k}A$.\n- Balanced growth less restrictive; $A$ can be endogenized.\n\nCANDIDATE_ANSWER:\nStep 1: Define Technical Change Types\n- Labor-augmenting: Embodied in effective labor (Y = F(K, NH·A))\n- Hicks-neutral: Scales entire production function (Y = A·F(K, NH))\n\nStep 2: Factor Share Implications\n- Under Hicks-neutral:\n  • If production function is homogeneous of degree 1 (CRS), factor shares (capital/labor income ratios) remain constant regardless of A.\n  • Follows from Euler's theorem: Marginal product scaling preserves share ratios.\n\n- Under Labor-augmenting:\n  • Factor shares remain constant under Cobb-Douglas (elasticity substitution = 1).\n  • If elasticity substitution ≠ 1, biased technical change could alter shares over time.\n\nStep 3: Balanced Growth Requirements\n- Labor-augmenting:\n  • Consistent with Kaldor stylized facts: Allows capital deepening (K/Y ratio stability) and steady growth in output per worker.\n  • Required for balanced growth under Uzawa's theorem when production function is not Cobb-Douglas.\n\n- Hicks-neutral:\n  • Generally incompatible with balanced growth unless:\n    a) Production function is Cobb-Douglas (neutrality preserves shares)\n    b) Technology growth rate matches special conditions for capital/labor growth\n  • Would require proportional growth in K and NH to maintain constant capital intensity.\n\nStep 4: Key Contrast\n- Labor-augmenting: Both constant factor shares and balanced growth are achievable under broader conditions (even non-Cobb-Douglas cases).\n- Hicks-neutral: Requires Cobb-Douglas + coordinated growth rates to achieve both constant shares and balanced growth.\n\nQID: textbook-86-1-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-86-1-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the implications of labor-augmenting and Hicks-neutral technical change on factor shares and balanced growth, aligning with the gold answer and providing additional detailed reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-86-1-1-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the implications of labor-augmenting and Hicks-neutral technical change on factor shares and balanced growth, aligning with the gold answer and providing additional detailed reasoning."}, "llm_echoed_qid": "textbook-86-1-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the implications of labor-augmenting and Hicks-neutral technical change on factor shares and balanced growth, aligning with the gold answer and providing additional detailed reasoning."}
{"script_processing_qid": "textbook-100-3-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the Bayes estimator $\\widehat{\\theta}(X)$ for the binomial-beta model when the beta prior is parametrized in terms of prior mean $\\theta_0$ and prior sample size $\\omega$.\n\nGOLD_ANSWER:\n1. **Prior Mean**: $\\theta_0 = \\frac{\\alpha}{\\alpha+\\beta}$.\n2. **Prior Sample Size**: $\\omega = \\alpha+\\beta$.\n3. **Posterior Mean**: $\\widehat{\\theta}(X) = \\frac{\\omega}{\\omega+n}\\theta_0 + \\frac{n}{\\omega+n}\\frac{X}{n}$.\n4. **Interpretation**: Weighted average of prior guess ($\\theta_0$) and data-driven guess ($X/n$).\n\nCANDIDATE_ANSWER:\nStep 1: Recall that for a binomial likelihood X|θ ~ Bin(n, θ) with beta prior θ ~ Be(α, β), the posterior is θ|X ~ Be(α + X, β + n - X).\n\nStep 2: Express the beta prior parameters α and β in terms of prior mean θ₀ and prior sample size ω:\n   - Prior mean: θ₀ = α/(α + β)\n   - Prior sample size: ω = α + β\n   Solving these gives α = θ₀ω and β = (1 - θ₀)ω\n\nStep 3: Substitute α and β into the posterior parameters:\n   Posterior α = θ₀ω + X\n   Posterior β = (1 - θ₀)ω + n - X\n\nStep 4: Compute the posterior mean (Bayes estimator) for beta distribution:\n   E[θ|X] = (Posterior α)/(Posterior α + Posterior β)\n          = (θ₀ω + X)/(θ₀ω + X + (1 - θ₀)ω + n - X)\n          = (θ₀ω + X)/(ω + n)\n\nThus, the Bayes estimator is:\n𝛉̂(X) = (ωθ₀ + X)/(ω + n)\n\nQID: textbook-100-3-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-100-3-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the Bayes estimator for the binomial-beta model, expressing it as a weighted average of the prior mean and the data-driven estimate, which matches the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-100-3-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the Bayes estimator for the binomial-beta model, expressing it as a weighted average of the prior mean and the data-driven estimate, which matches the gold answer."}, "llm_echoed_qid": "textbook-100-3-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the Bayes estimator for the binomial-beta model, expressing it as a weighted average of the prior mean and the data-driven estimate, which matches the gold answer."}
{"script_processing_qid": "textbook-26-0-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of the findings in Table 7 for the validity of balanced growth models.\n\nGOLD_ANSWER:\nThe findings provide mixed support for balanced growth models. While the ratios are more mean-reverting than the individual series, the unit root cannot be rejected for the consumption-output ratio. This suggests that while the balanced growth relations hold approximately, there may be deviations due to factors like adjustment costs or measurement errors. The results are broadly consistent with the predictions of stochastic growth models, but further refinement may be needed to fully capture the dynamics.\n\nCANDIDATE_ANSWER:\nStep 1: Balanced growth models predict that ratios like consumption-output should be stationary (even if individual variables have trends).\nStep 2: Table 7 shows the unit root hypothesis is not rejected for the consumption-output ratio (p > 0.05), implying potential non-stationarity.\nStep 3: However, the estimated largest roots for balanced growth ratios are numerically small, suggesting near-stationarity with high persistence.\nStep 4: This creates tension: Strict statistical tests (unit root non-rejection) challenge model validity, but small root magnitudes align loosely with model predictions.\nStep 5: Implication: Balanced growth models may require adjustments to account for persistent but mean-reverting ratio dynamics, or improved tests are needed to resolve low-power issues in unit root detection.\n\nQID: textbook-26-0-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-26-0-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately breaks down the implications of Table 7 for balanced growth models, aligning with the mixed support mentioned in the gold answer. It correctly identifies the tension between statistical tests and model predictions, suggesting possible model adjustments or improved tests, which is consistent with the gold answer's conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-26-0-2-3", "category": "CORRECT", "explanation": "The candidate answer accurately breaks down the implications of Table 7 for balanced growth models, aligning with the mixed support mentioned in the gold answer. It correctly identifies the tension between statistical tests and model predictions, suggesting possible model adjustments or improved tests, which is consistent with the gold answer's conclusion."}, "llm_echoed_qid": "textbook-26-0-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately breaks down the implications of Table 7 for balanced growth models, aligning with the mixed support mentioned in the gold answer. It correctly identifies the tension between statistical tests and model predictions, suggesting possible model adjustments or improved tests, which is consistent with the gold answer's conclusion."}
{"script_processing_qid": "textbook-97-6-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Construct a Galois field of order $9$ using an irreducible polynomial over $GF(3)$.\n\nGOLD_ANSWER:\n1. **Polynomial Selection**: Choose an irreducible polynomial of degree $2$ over $GF(3)$, e.g., $x^2 + x + 2$.\n2. **Quotient Ring**: Form the quotient ring $GF(3)[x]/(x^2 + x + 2)$.\n3. **Field Elements**: The field consists of all polynomials of degree less than $2$ with coefficients in $GF(3)$.\n4. **Operations**: Addition and multiplication are performed modulo $x^2 + x + 2$.\n\nCANDIDATE_ANSWER:\nStep 1: Confirm GF(9) requires a degree-2 irreducible polynomial over GF(3) since 9 = 3².\nStep 2: Identify monic quadratic candidates in GF(3)[x]. Example: f(x) = x² + 1.\nStep 3: Verify irreducibility by checking for roots in GF(3) = {0, 1, 2}.\n- f(0) = 0² + 1 = 1 ≠ 0\n- f(1) = 1 + 1 = 2 ≠ 0\n- f(2) = 4 + 1 ≡ 2 ≠ 0 (mod 3)\nNo roots ⇒ irreducible.\nStep 4: Construct GF(9) as GF(3)[x]/(x² + 1). Elements are polynomials of degree < 2: {a + bx | a, b ∈ GF(3)}, forming 9 distinct elements.\nConclusion: GF(9) is formed by adjoining a root of the irreducible polynomial x² + 1 to GF(3).\n\nQID: textbook-97-6-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-97-6-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly identifies an irreducible polynomial (x² + 1) and constructs GF(9) as GF(3)[x]/(x² + 1), which matches the structure described in the gold answer. The reasoning and steps are accurate and complete.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-97-6-1-2", "category": "CORRECT", "explanation": "The candidate correctly identifies an irreducible polynomial (x² + 1) and constructs GF(9) as GF(3)[x]/(x² + 1), which matches the structure described in the gold answer. The reasoning and steps are accurate and complete."}, "llm_echoed_qid": "textbook-97-6-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly identifies an irreducible polynomial (x² + 1) and constructs GF(9) as GF(3)[x]/(x² + 1), which matches the structure described in the gold answer. The reasoning and steps are accurate and complete."}
{"script_processing_qid": "textbook-110-4-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of the Cournot model for antitrust policy and market regulation.\n\nGOLD_ANSWER:\n1.  **Antitrust Implications**: The Cournot model highlights how firms' independent output decisions can lead to suboptimal market outcomes, justifying regulatory interventions to promote competition.\n2.  **Collusion Risks**: The model shows the temptation for firms to collude and restrict output, which antitrust laws aim to prevent.\n3.  **Policy Tools**: Regulations may include measures to increase market transparency, reduce barriers to entry, or impose output quotas to mitigate inefficiencies.\n\nCANDIDATE_ANSWER:\nStep 1: **Market Power and Pricing Implications**\nThe Cournot model shows that duopolists produce quantities below the competitive level but above the monopoly level. Prices are higher than marginal cost (as seen in the FOC $p_q q^f + p - C_q^f = 0$), indicating market power. This implies antitrust authorities should monitor oligopolies for anti-competitive pricing, even without explicit collusion.\n\nStep 2: **Strategic Interdependence and Collusion Risks**\nFirms’ reaction functions depend on rivals’ outputs. The equilibrium outcome is less cooperative than a cartel solution. Regulators must prevent tacit collusion (e.g., via price signaling) that could push outcomes closer to monopoly levels.\n\nStep 3: **Efficiency Considerations**\nCournot equilibrium allocations are Pareto inefficient compared to perfect competition. Deadweight loss justifies regulatory interventions (e.g., promoting entry, price caps) to improve allocative efficiency.\n\nStep 4: **Role of Cost Structures**\nAsymmetric cost functions ($C^f(q^f)$) affect firms’ market shares. Policies addressing predatory pricing or abuse of cost advantages (e.g., subsidies for inefficient firms) may be needed to ensure fair competition.\n\nStep 5: **Entry Barriers and Market Concentration**\nThe model assumes a fixed number of firms. Antitrust policies should lower entry barriers to increase competition, as more firms in a Cournot framework reduce prices toward competitive levels.\n\nQID: textbook-110-4-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-110-4-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses the implications of the Cournot model for antitrust policy and market regulation, aligning well with the gold answer. It covers market power, collusion risks, efficiency considerations, cost structures, and entry barriers, providing a comprehensive analysis.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-4-1-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses the implications of the Cournot model for antitrust policy and market regulation, aligning well with the gold answer. It covers market power, collusion risks, efficiency considerations, cost structures, and entry barriers, providing a comprehensive analysis."}, "llm_echoed_qid": "textbook-110-4-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses the implications of the Cournot model for antitrust policy and market regulation, aligning well with the gold answer. It covers market power, collusion risks, efficiency considerations, cost structures, and entry barriers, providing a comprehensive analysis."}
{"script_processing_qid": "textbook-99-6-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of dependent errors in fixed-design versus random-design regression. Why is the random design more robust?\n\nGOLD_ANSWER:\nIn fixed-design regression, dependent errors inflate the variance of estimators like the sample mean, leading to less precise estimates. In random-design regression, the randomness of $X$ averages out the dependency, making the design more robust to deviations from independent errors. This is because the autocovariance terms in the variance calculation are mitigated by the random design.\n\nCANDIDATE_ANSWER:\nStep 1: Distinguish between fixed- and random-design regression\n- Fixed design: Predictors (e.g., Z-values) are treated as fixed/non-random.\n- Random design: Predictors (Z-values) are random variables.\n\nStep 2: Analyze dependent error implications\nIn the given mixture model, Y = Zζ + (1-Z)ξ. Errors (ζ, ξ) are independent of each other but dependent on Z. This creates:\n- Fixed design: Z-values are predetermined. Dependence between errors (ζ/ξ) and fixed Z introduces systematic bias in parameter estimation (e.g., θ). Correlation structure persists across observations.\n- Random design: Z-values are random. Dependence between errors and Z is averaged out due to the randomness of Z, reducing systematic bias.\n\nStep 3: Explain robustness of random design\nRandom design benefits from:\n1. Law of Large Numbers: Random Z ensures the empirical distribution of Z converges to its true Bernoulli(θ) distribution.\n2. Error averaging: Dependence between errors (ζ/ξ) and Z is mitigated when Z is random, as E{Y} = θμζ + (1-θ)μξ remains valid even with Z-randomness.\n3. Reduced structural bias: Randomization prevents persistent alignment between Z-values and error terms (ζ/ξ), unlike fixed designs where persistent alignment could distort θ estimation.\n\nStep 4: Conclusion\nDependent errors in fixed designs lead to biased/persistent estimation issues, while random designs achieve robustness through error averaging and convergence properties inherent to random predictors.\n\nQID: textbook-99-6-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-6-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the implications of dependent errors in both fixed- and random-design regression, aligning with the gold answer. The detailed reasoning about the robustness of random design due to error averaging and convergence properties is accurate and comprehensive.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-6-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the implications of dependent errors in both fixed- and random-design regression, aligning with the gold answer. The detailed reasoning about the robustness of random design due to error averaging and convergence properties is accurate and comprehensive."}, "llm_echoed_qid": "textbook-99-6-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the implications of dependent errors in both fixed- and random-design regression, aligning with the gold answer. The detailed reasoning about the robustness of random design due to error averaging and convergence properties is accurate and comprehensive."}
{"script_processing_qid": "textbook-50-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Analyze the challenges in defining and measuring 'representativeness' in statistical sampling, as discussed by Kruskal and Mosteller.\n\nGOLD_ANSWER:\n1.  **Ambiguity in Definition**: Representativeness can be ambiguous, as it may refer to random sampling, stratified sampling, or other methods.\n2.  **Non-Scientific Literature**: In non-scientific contexts, representativeness is often used loosely, leading to misinterpretations.\n3.  **Scientific Literature**: In scientific studies, representativeness is tied to the sampling frame and methodology, but inconsistencies persist.\n4.  **Statistical Literature**: Statisticians emphasize probabilistic sampling, but practical constraints often lead to non-probabilistic samples.\n5.  **Implications**: Misunderstanding representativeness can lead to biased inferences and invalid conclusions.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Kruskal and Mosteller's key observation from the context - their study highlights the pervasive ambiguity in defining 'representativeness' across three domains: (a) non-scientific literature (where it often becomes a vague rhetorical device), (b) scientific literature excluding statistics (where operational definitions vary inconsistently), and (c) statistical literature itself (where multiple competing interpretations coexist).\n\nStep 2: Analyze measurement challenges: The context implies that 'representativeness' lacks a universal operational definition even within statistics. Different criteria emerge depending on whether researchers emphasize (1) similarity to population characteristics, (2) coverage of subpopulations, or (3) inclusion of 'typical' instances – each requiring distinct measurement approaches.\n\nStep 3: Connect to Mosteller's interdisciplinary work: The mention of weather modification studies (with 'high variability' and 'unknown mechanisms') illustrates practical measurement difficulties – when underlying processes are poorly understood (as in many real-world applications), defining/measuring representativeness becomes entangled with model uncertainty and causal ambiguity.\n\nStep 4: Synthesize the core challenge: The term's varied usage across disciplines (law, history, meteorology) and methodological contexts creates a definitional pluralism that resists standardized measurement, requiring case-specific operationalizations that may limit comparability across studies.\n\nQID: textbook-50-0-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-50-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the challenges in defining and measuring 'representativeness' as discussed by Kruskal and Mosteller, covering ambiguity in definition, measurement challenges, and practical difficulties, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-50-0-0-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the challenges in defining and measuring 'representativeness' as discussed by Kruskal and Mosteller, covering ambiguity in definition, measurement challenges, and practical difficulties, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-50-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the challenges in defining and measuring 'representativeness' as discussed by Kruskal and Mosteller, covering ambiguity in definition, measurement challenges, and practical difficulties, aligning well with the gold answer."}
{"script_processing_qid": "textbook-111-19-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Why does the LM curve slope upward? Derive the slope using the money market equilibrium condition.\n\nGOLD_ANSWER:\n1. **Money Market Equilibrium**: $M/P = L(r, Y) = kY - hr$.  \n2. **Solve for $r$**: $r = \\frac{kY}{h} - \\frac{M}{Ph}$.  \n3. **Slope**: $\\frac{dr}{dY} = \\frac{k}{h} > 0$. Higher income increases money demand, requiring a higher $r$ to maintain equilibrium, hence the upward slope.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the money market equilibrium condition: M/P = L(r, Y)\nStep 2: Assume money demand takes the linear form L(r, Y) = kY - hr (where k > 0 captures income sensitivity and h > 0 captures interest rate sensitivity)\nStep 3: Substitute into equilibrium: M/P = kY - hr\nStep 4: Solve for r: r = (k/h)Y - (M/(hP))\nStep 5: The coefficient of Y (k/h) represents the slope of the LM curve\nStep 6: Since both k and h are positive parameters, k/h > 0 ⇒ LM curve slopes upward\nReason: Higher income (Y) increases money demand. With fixed M/P, equilibrium requires higher interest rates (r) to reduce money demand back to supply.\n\nQID: textbook-111-19-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-19-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the slope of the LM curve using the money market equilibrium condition and provides a clear explanation for why the curve slopes upward, matching the gold answer in both reasoning and conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-19-1-2", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the slope of the LM curve using the money market equilibrium condition and provides a clear explanation for why the curve slopes upward, matching the gold answer in both reasoning and conclusion."}, "llm_echoed_qid": "textbook-111-19-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the slope of the LM curve using the money market equilibrium condition and provides a clear explanation for why the curve slopes upward, matching the gold answer in both reasoning and conclusion."}
{"script_processing_qid": "textbook-105-10-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Interpret the role of the covering number $\\mathcal{N}_1(1/n, \\mathcal{F}_{n,k})$ in the penalty term and its connection to the VC dimension.\n\nGOLD_ANSWER:\n1. **Covering Number**: $\\mathcal{N}_1(\\epsilon, \\mathcal{F}_{n,k})$ measures the complexity of $\\mathcal{F}_{n,k}$ by counting the minimal number of $\\epsilon$-balls needed to cover it under the $L_1$ metric.  \n2. **VC Dimension Link**: For neural networks, $\\log \\mathcal{N}_1(1/n, \\mathcal{F}_{n,k}) = O((2d+6)k \\log(\\beta_n n))$, reflecting the VC dimension's role in controlling growth functions.  \n3. **Regularization**: The penalty $pen_n(k) \\propto \\frac{\\beta_n^4}{n} \\log \\mathcal{N}_1(1/n, \\mathcal{F}_{n,k})$ ensures the selected network size $k$ balances approximation and generalization.\n\nCANDIDATE_ANSWER:\nStep 1: Role of the covering number in the penalty term\n\nThe covering number $\\mathcal{N}_1(1/n, \\mathcal{F}_{n,k})$ quantifies the complexity of the neural network class $\\mathcal{F}_{n,k}$ by measuring how many L1-balls of radius $1/n$ are required to cover the function space. In Theorem 16.2, the penalty term $pen_n(k)$ implicitly depends on this covering number to penalize model complexity. Larger covering numbers (indicating more complex classes) lead to higher penalties, enforcing a trade-off between approximation error (bias) and complexity (variance) during empirical risk minimization.\n\nStep 2: Connection to VC dimension\n\nThe VC dimension provides an alternate measure of complexity for function classes. While the VC dimension is not explicitly mentioned in the context, standard results in statistical learning theory link covering numbers to VC dimensions. For classes with VC dimension $d$, covering numbers grow polynomially as $\\mathcal{N}(\\epsilon, \\mathcal{F}) \\sim (1/\\epsilon)^{O(d)}$. In $\\mathcal{F}_{n,k}$, the constraint $\\sum_{i=0}^k |c_i| \\leq \beta_n$ limits the output weights, which restricts the effective VC dimension (and thus the growth of $\\mathcal{N}_1(1/n, \\mathcal{F}_{n,k})$). This connection allows the penalty term to implicitly control model complexity through VC-dimension-inspired bounds.\n\nStep 3: Practical implication in Theorem 16.2\n\nThe penalty term $pen_n(k)$ in Theorem 16.2 ensures that the selected neural network size $k$ balances the approximation error $\\inf_{f\\in\\mathcal{F}_{n,k}} \\int |f - m|^2 d\\mu$ (from Lemma 16.8) and the complexity term dominated by $\\mathcal{N}_1(1/n, \\mathcal{F}_{n,k})$. The $1/n$ radius in the covering number aligns with the $O(\beta_n^4/n)$ term in the final bound, reflecting the statistical estimation error controlled by the penalty.\n\nQID: textbook-105-10-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-105-10-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the role of the covering number in the penalty term and its connection to the VC dimension. It correctly describes how the covering number quantifies complexity, links it to VC dimension through standard learning theory results, and discusses the practical implications in Theorem 16.2, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-105-10-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately explains the role of the covering number in the penalty term and its connection to the VC dimension. It correctly describes how the covering number quantifies complexity, links it to VC dimension through standard learning theory results, and discusses the practical implications in Theorem 16.2, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-105-10-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the role of the covering number in the penalty term and its connection to the VC dimension. It correctly describes how the covering number quantifies complexity, links it to VC dimension through standard learning theory results, and discusses the practical implications in Theorem 16.2, aligning well with the gold answer."}
{"script_processing_qid": "textbook-105-3-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the complexity penalty $p e n\\_{n}(k)$ for the $k$ th class of RBF networks, ensuring it satisfies the given inequality and Kraft’s inequality.\n\nGOLD_ANSWER:\n1. **Inequality Constraint**: The complexity penalty must satisfy $$ p e n\\_{n}(k)\\geq2568\\frac{\\beta\\_{n}^{4}}{n}\\cdot(\\log\\mathcal{N}\\_{1}(1/n,\\mathcal{F}\\_{n,k})+t\\_{k}). $$\n2. **Kraft’s Inequality**: Choose $t\\_{k}=2\\log{k}+t\\_{0}$ with $t\\_{0}\\geq\\log\\left(\\sum\\_{k\\geq1}k^{-2}\\right)$ to ensure $\\sum\\_{k=1}^{\\infty}e^{-t\\_{k}}\\leq1$.\n3. **Final Form**: The penalty becomes $$ p e n\\_{n}(k)=2568\\frac{\\beta\\_{n}^{4}}{n}\\left(\\log\\mathcal{N}\\_{1}(1/n,\\mathcal{F}\\_{n,k})+2\\log{k}+t\\_{0}\\right). $$\n\nCANDIDATE_ANSWER:\nStep 1: Identify key components from Theorem 17.2. The penalty must satisfy:\npenₙ(k) ≥ 2568(βₙ⁴/n)[log𝒩₁(1/n,ℱₙ,k) + tₖ].\n\nStep 2: Analyze the covering number bound implied by Theorem 17.2. The term ((2d² + 2d +6)k +5)log(12e k*βₙn) represents the upper bound for log𝒩₁(1/n,ℱₙ,k).\n\nStep 3: Substitute this covering number bound into the penalty inequality:\npenₙ(k) ≥ 2568(βₙ⁴/n)[((2d² + 2d +6)k +5)log(12e k*βₙn) + tₖ].\n\nStep 4: Ensure Kraft's inequality compliance. Choose tₖ values such that Σ_{k=1}^∞ e^{-tₖ} ≤ 1. A valid choice is tₖ = 2logk + c where c = log(π²/6), but the minimal requirement is maintaining the inequality without specifying exact tₖ values.\n\nFinal Complexity Penalty:\npenₙ(k) = 2568(βₙ⁴/n)[( (2d² + 2d +6)k +5 )log(12e k*βₙn) + tₖ]\nwhere constants tₖ must satisfy Σ_{k=1}^∞ e^{-tₖ} ≤ 1.\n\nQID: textbook-105-3-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-105-3-0-0\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the inequality constraint and mentions Kraft's inequality, but it introduces additional terms (covering number bound) not present in the gold answer. The final form of the penalty in the candidate answer deviates from the simpler, more general form provided in the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-105-3-0-0", "category": "PARTIALLY_CORRECT", "explanation": "The candidate answer correctly identifies the inequality constraint and mentions Kraft's inequality, but it introduces additional terms (covering number bound) not present in the gold answer. The final form of the penalty in the candidate answer deviates from the simpler, more general form provided in the gold answer."}, "llm_echoed_qid": "textbook-105-3-0-0", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the inequality constraint and mentions Kraft's inequality, but it introduces additional terms (covering number bound) not present in the gold answer. The final form of the penalty in the candidate answer deviates from the simpler, more general form provided in the gold answer."}
{"script_processing_qid": "textbook-99-5-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. For the irregular case ($\\beta \\leq 0.25$), explain the degraded rate $n^{-8\\beta/(4\\beta+1)}$ and the adaptation penalty.\n\nGOLD_ANSWER:\n- **Rate Mechanism**: For $\\beta \\leq 0.25$, the bias term $\\sum_{j>J}\\theta_{j}^{2}$ dominates, scaling as $J^{-4\\beta}$. Optimizing $J$ balances variance $O(n^{-1}J)$ and bias $O(J^{-4\\beta})$, yielding $J \\asymp n^{1/(4\\beta+1)}$ and MSE $\\asymp n^{-8\\beta/(4\\beta+1)}$.  \n- **Adaptation Penalty**: Adaptive estimators (unknown $\\beta$) incur an extra $\\ln n$ factor, e.g., MSE $\\asymp (\\ln n)^{2}n^{-8\\beta/(4\\beta+1)}$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the bias-variance tradeoff for the irregular case\nFor β ≤ 0.25, the squared bias of the estimator scales as \\(J^{-4β}\\) (from truncating the Sobolev series at J terms), while the variance term scales as \\(J/n^2\\) (from summing variances of squared coefficient estimates).\n\nStep 2: Balance bias and variance terms\nSet \\(J^{-4β} \\approx J/n^2\\) to optimize the rate. Solving for J gives:\n\\[ J \\approx n^{2/(4β+1)} \\]\n\nStep 3: Derive the degraded rate\nSubstitute this optimal J back into either term:\n\\[ J^{-4β} = \\left(n^{2/(4β+1)}\\right)^{-4β} = n^{-8β/(4β+1)} \\]\nThis gives the dominant term in the mean squared error.\n\nStep 4: Explain the adaptation penalty\nThe data-driven estimator uses \\(J_0 = 2⌊n/\\ln n⌋\\) instead of the ideal \\(J \\approx n^{2/(4β+1)}\\). This results in a suboptimal bias-variance tradeoff that ultimately still achieves the \\(n^{-8β/(4β+1)}\\) rate, but with an additional implicit penalty from not knowing β a priori. The penalty manifests as requiring a larger J than would be optimal if β were known, leading to slower convergence compared to the parametric \\(n^{-1}\\) rate.\n\nQID: textbook-99-5-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-99-5-1-2\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate correctly identifies the degraded rate and adaptation penalty but makes an error in the variance term (should be O(n⁻¹J) instead of J/n²). The overall reasoning and final conclusions align with the gold answer despite this minor inconsistency.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-5-1-2", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate correctly identifies the degraded rate and adaptation penalty but makes an error in the variance term (should be O(n⁻¹J) instead of J/n²). The overall reasoning and final conclusions align with the gold answer despite this minor inconsistency."}, "llm_echoed_qid": "textbook-99-5-1-2", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate correctly identifies the degraded rate and adaptation penalty but makes an error in the variance term (should be O(n⁻¹J) instead of J/n²). The overall reasoning and final conclusions align with the gold answer despite this minor inconsistency."}
{"script_processing_qid": "textbook-53-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the role of complementarities in Diamond's (1982b) search model. How do thick-market externalities lead to multiple equilibria, and what are the implications for business cycles?\n\nGOLD_ANSWER:\n1. **Complementarities**: In Diamond's model, the presence of more searchers increases the probability of successful matches (thick-market externality).\n2. **Multiple Equilibria**: \n   - **Low-activity equilibrium**: Few searchers, low match probability, low output.\n   - **High-activity equilibrium**: Many searchers, high match probability, high output.\n3. **Implications**: Small shocks (e.g., sunspots) can trigger shifts between equilibria, causing large fluctuations in output and employment.\n4. **Critique**: The model focuses on output, not unemployment, and lacks empirical support for its extreme amplification.\n\nCANDIDATE_ANSWER:\nStep 1: Role of Complementarities in Diamond's Model\\nIn Diamond's (1982b) search model, complementarities arise because workers' search efforts and firms' job creation decisions reinforce each other. When more workers search for jobs, firms are incentivized to create more vacancies (thick-market externality), improving matching efficiency. Conversely, low search activity reduces vacancy creation, leading to a thin-market equilibrium.\\n\\nStep 2: Thick-Market Externalities and Multiple Equilibria\\nThick-market externalities create multiple equilibria:\\n- High-activity equilibrium: High worker search effort → more vacancies → easier matching → higher employment.\\n- Low-activity equilibrium: Low search effort → fewer vacancies → reduced matching efficiency → lower employment.\\nThe model shows indeterminacy: even tiny shocks (e.g., sunspots) can shift the economy between these equilibria.\\n\\nStep 3: Implications for Business Cycles\\n- Amplification: Small shocks trigger large output/employment swings due to equilibrium shifts.\\n- Indeterminacy/Persistence: Self-fulfilling expectations (e.g., pessimism reduces search effort → validates low employment) create persistent unemployment.\\n- Hall's induced secondary job loss: Matches in low-activity equilibria may be temporary or unstable, prolonging unemployment through repeated job separation cycles (as noted in the context).\n\nQID: textbook-53-0-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-53-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately captures the role of complementarities, the mechanism of thick-market externalities leading to multiple equilibria, and the implications for business cycles as described in Diamond's model. The response aligns well with the gold answer, including the critique about the model's focus and empirical support.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-53-0-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately captures the role of complementarities, the mechanism of thick-market externalities leading to multiple equilibria, and the implications for business cycles as described in Diamond's model. The response aligns well with the gold answer, including the critique about the model's focus and empirical support."}, "llm_echoed_qid": "textbook-53-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately captures the role of complementarities, the mechanism of thick-market externalities leading to multiple equilibria, and the implications for business cycles as described in Diamond's model. The response aligns well with the gold answer, including the critique about the model's focus and empirical support."}
{"script_processing_qid": "textbook-110-4-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the Nash equilibrium for a two-player game where each player has two strategies. Show how mixed strategies can be used to find the equilibrium.\n\nGOLD_ANSWER:\n1.  **Game Setup**: Consider a two-player game with strategies $S_1 = \\{A, B\\}$ and $S_2 = \\{C, D\\}$.\n2.  **Payoff Matrix**: Define payoffs for each combination of strategies.\n3.  **Mixed Strategies**: Let player 1 choose $A$ with probability $p$ and $B$ with $1-p$. Player 2 chooses $C$ with probability $q$ and $D$ with $1-q$.\n4.  **Expected Payoffs**: Calculate expected payoffs for each player and set up indifference conditions.\n5.  **Equilibrium**: Solve for $p$ and $q$ such that no player can improve their payoff by unilaterally changing their strategy.\n\nCANDIDATE_ANSWER:\nStep 1: Define the game structure. Assume Player 1 has strategies A and B, and Player 2 has strategies C and D. Let Player 1 choose A with probability p and B with (1−p). Player 2 chooses C with probability q and D with (1−q).\n\nStep 2: Derive expected payoffs. For Player 1 to be indifferent between A and B (required for a mixed strategy equilibrium), their expected payoffs from A and B must equal given Player 2’s strategy. Similarly, Player 2’s expected payoffs from C and D must equal given Player 1’s strategy.\n\nGeneric form:\n- Player 1’s payoff from A: q⋅u₁(A,C) + (1−q)⋅u₁(A,D)\n- Player 1’s payoff from B: q⋅u₁(B,C) + (1−q)⋅u₁(B,D)\nSet these equal to solve for q.\n\nStep 3: Repeat for Player 2. Compute Player 2’s expected payoffs from C and D given Player 1’s mixed strategy p, set them equal, and solve for p.\n\nStep 4: The solution (p*, q*) represents the mixed-strategy Nash equilibrium. At these probabilities, neither player can improve their expected payoff by unilaterally changing their strategy.\n\nThis ensures equilibrium existence even when no pure-strategy equilibrium exists, addressing the 'awkward hole' noted in the context.\n\nQID: textbook-110-4-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-110-4-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately follows the steps outlined in the gold answer, including defining mixed strategies, deriving expected payoffs, and solving for equilibrium probabilities. The reasoning and conclusion align perfectly with the gold standard.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-4-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately follows the steps outlined in the gold answer, including defining mixed strategies, deriving expected payoffs, and solving for equilibrium probabilities. The reasoning and conclusion align perfectly with the gold standard."}, "llm_echoed_qid": "textbook-110-4-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately follows the steps outlined in the gold answer, including defining mixed strategies, deriving expected payoffs, and solving for equilibrium probabilities. The reasoning and conclusion align perfectly with the gold standard."}
{"script_processing_qid": "textbook-125-14-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze the recovery rates for different types of bonds in default, as shown in Figure 25.1, and discuss the implications for investors.\n\nGOLD_ANSWER:\n1. **Senior Secured Bonds**: Highest recovery rates (>50%) due to priority claims on specific assets.\n2. **Junior Unsecured Bonds**: Lowest recovery rates (<20%) due to subordination to other creditors.\n3. **Implications**: Investors should assess seniority and security when evaluating bond risk, as these factors significantly impact recovery in default scenarios.\n\nCANDIDATE_ANSWER:\nStep 1: The provided context does not include Figure 25.1 or explicit recovery rate data for bonds in default. However, the textual context highlights key determinants of recovery rates:\n\nStep 2: Seniority matters. Senior bonds (like Ralston Purina’s general unsecured obligations) have priority over subordinated bonds and equity in default, implying higher recovery rates than junior claims.\n\nStep 3: Security status is critical. While Ralston Purina’s bonds are unsecured (debentures), secured bonds (not discussed here but implied by the security term in the section) would typically have higher recovery rates due to collateral backing.\n\nStep 4: Bond structure influences risk. Publicly issued bonds (e.g., Ralston’s NYSE-listed bonds) with standardized indentures may offer clearer legal protections, aiding recovery efforts compared to less regulated issues.\n\nImplications for investors: Prioritize senior, secured bonds for higher recovery prospects in default. Unsecured debentures (like Ralston’s) carry greater recovery risk, requiring higher yields to compensate. Subordinated bonds should demand even higher risk premiums.\n\nQID: textbook-125-14-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-14-1-3\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies seniority and security as key factors influencing recovery rates, aligning with the gold answer. However, the candidate's response lacks specific recovery rate percentages and focuses on Ralston Purina’s bonds, which are not mentioned in the gold answer, leading to a partial mismatch.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-14-1-3", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies seniority and security as key factors influencing recovery rates, aligning with the gold answer. However, the candidate's response lacks specific recovery rate percentages and focuses on Ralston Purina’s bonds, which are not mentioned in the gold answer, leading to a partial mismatch."}, "llm_echoed_qid": "textbook-125-14-1-3", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies seniority and security as key factors influencing recovery rates, aligning with the gold answer. However, the candidate's response lacks specific recovery rate percentages and focuses on Ralston Purina’s bonds, which are not mentioned in the gold answer, leading to a partial mismatch."}
{"script_processing_qid": "textbook-97-9-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the expression for the mean squared error (MSE) in a blocked design and compare it to the MSE in a Completely Randomized Design (CRD).\n\nGOLD_ANSWER:\n1. **MSE in Blocked Design**: The MSE in a blocked design is given by $$ MSE_{blocked} = \\frac{SSE_{within}}{df_{within}} $$ where $SSE_{within}$ is the sum of squares due to error within blocks and $df_{within}$ is the degrees of freedom within blocks. \n2. **MSE in CRD**: The MSE in a CRD is $$ MSE_{CRD} = \\frac{SSE_{total}}{df_{total}} $$ where $SSE_{total}$ is the total sum of squares due to error and $df_{total}$ is the total degrees of freedom. \n3. **Comparison**: If blocking is effective, $SSE_{within} < SSE_{total}$ and $df_{within} < df_{total}$, leading to a smaller MSE in the blocked design compared to CRD.\n\nCANDIDATE_ANSWER:\nStep 1: Define statistical models\n- CRD: \\(Y_{ik} = \\mu + \\tau_i + \\epsilon_{ik}\\), where \\(\\epsilon_{ik} \\sim N(0,\\sigma^2_{CRD})\\)\n- Blocked design: \\(Y_{ijk} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ijk}\\), where \\(\\epsilon_{ijk} \\sim N(0,\\sigma^2_{Blocked})\\)\n\nStep 2: Calculate MSE components\n- CRD MSE: \\(\\frac{SSE_{CRD}}{N - t}\\)\n  Where \\(SSE_{CRD} = \\sum_{i,k}(Y_{ik} - \\hat{\\mu} - \\hat{\\tau}_i)^2\\) and \\(N\\) = total observations\n- Blocked MSE: \\(\\frac{SSE_{Blocked}}{N - t - b + 1}\\)\n  Where \\(SSE_{Blocked} = \\sum_{i,j,k}(Y_{ijk} - \\hat{\\mu} - \\hat{\\tau}_i - \\hat{\\beta}_j)^2\\)\n\nStep 3: Compare MSE values\n- Blocking reduces \\(SSE\\) by removing between-block variation (\\(SSE_{Blocked} = SSE_{CRD} - SS_{Blocks}\\))\n- Effective blocking ensures \\(SS_{Blocks}\\) is large enough that:\n  \\(\\frac{SSE_{CRD} - SS_{Blocks}}{N - t - b + 1} < \\frac{SSE_{CRD}}{N - t}\\)\n\nConclusion: Blocked designs achieve smaller MSE than CRD when:\n1. Blocks explain significant between-unit variation\n2. The reduction in residual variation outweighs the loss of degrees of freedom\n\nQID: textbook-97-9-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-97-9-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately derives the MSE expressions for both designs, correctly identifies the components (SSE and degrees of freedom), and provides a valid comparison. The reasoning aligns with the gold answer, including the conditions under which blocking reduces MSE.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-97-9-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately derives the MSE expressions for both designs, correctly identifies the components (SSE and degrees of freedom), and provides a valid comparison. The reasoning aligns with the gold answer, including the conditions under which blocking reduces MSE."}, "llm_echoed_qid": "textbook-97-9-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately derives the MSE expressions for both designs, correctly identifies the components (SSE and degrees of freedom), and provides a valid comparison. The reasoning aligns with the gold answer, including the conditions under which blocking reduces MSE."}
{"script_processing_qid": "textbook-122-31-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the asymptotic variance of the OLS estimator $\\hat{\\beta}_{1}$ under serial correlation, starting from the expression $$\\operatorname{Avar}(\\hat{\\beta}_{1})=\\left(\\sum_{t=1}^{n}\\operatorname{E}(r_{t}^{2})\\right)^{-2}\\operatorname{Var}\\left(\\sum_{t=1}^{n}r_{t}u_{t}\\right).$$\n\nGOLD_ANSWER:\n1. **Objective**: Derive $\\operatorname{Avar}(\\hat{\\beta}_{1})$ under serial correlation.\n2. **Variance Decomposition**: Under serial correlation, $\\operatorname{Var}\\left(\\sum_{t=1}^{n}r_{t}u_{t}\\right)$ includes cross-terms due to correlation between $a_{t}$ and $a_{s}$ for $t \\neq s$.\n3. **Expression**: $$\\operatorname{Avar}(\\hat{\\beta}_{1})=\\left(\\sum_{t=1}^{n}\\operatorname{E}(r_{t}^{2})\\right)^{-2}\\left[\\sum_{t=1}^{n}\\operatorname{Var}(a_{t})+2\\sum_{t=1}^{n-1}\\sum_{s=t+1}^{n}\\operatorname{Cov}(a_{t},a_{s})\\right].$$\n\nCANDIDATE_ANSWER:\nStep 1: Expand the variance term in the given expression:\n$$\\operatorname{Var}\\left(\\sum_{t=1}^{n}r_{t}u_{t}\\right) = \\sum_{t=1}^{n}\\operatorname{Var}(r_t u_t) + 2\\sum_{t=1}^{n}\\sum_{s<t}\\operatorname{Cov}(r_t u_t, r_s u_s)$$\n\nStep 2: Recognize that under serial correlation in $u_t$, the covariance terms remain non-zero. Using covariance stationarity:\n$$\\operatorname{Cov}(u_t, u_s) = \\sigma^2 \\rho_{|t-s|}$$\nwhere $\\rho_h$ is the autocorrelation coefficient at lag $h$.\n\nStep 3: Substitute the variance and covariance terms:\n$$\\operatorname{Var}\\left(\\sum_{t=1}^{n}r_{t}u_{t}\\right) = \\sum_{t=1}^{n}E(r_t^2)\\sigma^2 + 2\\sum_{h=1}^{n-1}\\sum_{t=h+1}^{n}E(r_t r_{t-h})\\sigma^2\\rho_h$$\n\nStep 4: Factor out $\\sigma^2$ and use the asymptotic approximation that $\\sum_{t=1}^{n}E(r_t^2) \\approx nE(r_t^2)$:\n$$\\operatorname{Avar}(\\hat{\\beta}_{1}) = \\frac{\\sigma^2}{[E(r_t^2)]^2}\\left(1 + 2\\sum_{h=1}^{\\infty}\\frac{E(r_t r_{t-h})}{E(r_t^2)}\\rho_h\\right)$$\n\nStep 5: Recognize this as the generalization of the standard OLS variance formula adjusted for serial correlation through the infinite sum of autocorrelation-weighted cross-products.\n\nQID: textbook-122-31-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-31-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's derivation correctly expands the variance term, accounts for serial correlation, and arrives at the asymptotic variance formula adjusted for serial correlation, matching the gold answer's intent and structure.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-31-0-0", "category": "CORRECT", "explanation": "The candidate's derivation correctly expands the variance term, accounts for serial correlation, and arrives at the asymptotic variance formula adjusted for serial correlation, matching the gold answer's intent and structure."}, "llm_echoed_qid": "textbook-122-31-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's derivation correctly expands the variance term, accounts for serial correlation, and arrives at the asymptotic variance formula adjusted for serial correlation, matching the gold answer's intent and structure."}
{"script_processing_qid": "textbook-97-7-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. (Quantitative) Derive the dual distance condition for an orthogonal array from the minimal distance property of its corresponding code, using Theorem 4.9.\n\nGOLD_ANSWER:\n1. **Theorem 4.9**: For a code $C$ with minimal distance $d$, the dual code $C^\\perp$ has strength $t = d^\\perp - 1$.\n2. **Dual Distance**: The dual distance $d^\\perp$ is the minimal Hamming weight of any non-zero codeword in $C^\\perp$.\n3. **Orthogonal Array**: The rows of $C^\\perp$ form an orthogonal array $OA(N, k, s, t)$ where $t = d^\\perp - 1$.\n4. **Conclusion**: The dual distance $d^\\perp$ directly determines the strength $t$ of the orthogonal array.\n\nCANDIDATE_ANSWER:\nStep 1: Recall Theorem 4.9's core implication: The dual distance of an orthogonal array corresponds to the minimal distance of its associated code.\n\nStep 2: Let the code have minimal distance \\( d \\). By definition, this means any two distinct codewords differ in at least \\( d \\) positions.\n\nStep 3: Apply Theorem 4.9 directly: The dual distance of the orthogonal array is equal to the code’s minimal distance \\( d \\).\n\nStep 4: Conclusion: The dual distance condition for the orthogonal array is \\( d \\), inherited directly from the minimal distance property of the code.\n\nThus, the derivation reduces to applying Theorem 4.9’s explicit connection between code distance and OA dual distance.\n\nQID: textbook-97-7-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-97-7-0-0\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the relationship between the code's minimal distance and the orthogonal array's dual distance but misapplies Theorem 4.9 by not accounting for the strength \\( t = d^\\\\perp - 1 \\). The conclusion oversimplifies the connection.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-97-7-0-0", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the relationship between the code's minimal distance and the orthogonal array's dual distance but misapplies Theorem 4.9 by not accounting for the strength \\( t = d^\\perp - 1 \\). The conclusion oversimplifies the connection."}, "llm_echoed_qid": "textbook-97-7-0-0", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the relationship between the code's minimal distance and the orthogonal array's dual distance but misapplies Theorem 4.9 by not accounting for the strength \\( t = d^\\perp - 1 \\). The conclusion oversimplifies the connection."}
{"script_processing_qid": "textbook-26-0-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Compare the persistence of the consumption-output and investment-output ratios in Table 7. What might explain the differences?\n\nGOLD_ANSWER:\n1. **Consumption-Output Ratio**: Largest root is 0.38 (OLS) and 0.70 (median unbiased).\n2. **Investment-Output Ratio**: Largest root is 0.51 (OLS) and 0.32 (median unbiased).\n3. **Explanation**: The investment-output ratio is more volatile due to the high cyclical variability of investment. The lower persistence in the investment-output ratio may reflect faster adjustment to shocks compared to consumption.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key findings in Table 7 (as described in the context).\n- The unit root hypothesis is not rejected for the consumption-output ratio (indicating potential non-stationarity/high persistence).\n- Both balanced growth ratios (consumption-output and investment-output) have small estimated largest autoregressive roots (suggesting lower persistence than unit root processes).\n\nStep 2: Compare persistence\n- The consumption-output ratio shows weaker evidence of stationarity (unit root not rejected at 5% level), implying relatively higher persistence.\n- While the text does not explicitly state results for the investment-output ratio, the emphasis on 'small largest roots for both ratios' suggests the investment-output ratio likely has lower persistence (stronger evidence of stationarity) than the consumption-output ratio.\n\nStep 3: Explain differences\nPotential explanations include:\n- Consumption smoothing behavior: Consumption adjusts slowly to output shocks, leading to persistent deviations in the consumption-output ratio.\n- Investment's procyclicality: Investment reacts sharply to business cycles, causing faster mean reversion in the investment-output ratio.\n- Structural rigidities: Consumption (e.g., habit formation) may exhibit sloweradjustment dynamics compared to investment (e.g., lumpy adjustments or financial constraints).\n\nQID: textbook-26-0-2-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-26-0-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the persistence differences between the consumption-output and investment-output ratios and provides plausible explanations for these differences, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-26-0-2-2", "category": "CORRECT", "explanation": "The candidate correctly identifies the persistence differences between the consumption-output and investment-output ratios and provides plausible explanations for these differences, aligning with the gold answer."}, "llm_echoed_qid": "textbook-26-0-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly identifies the persistence differences between the consumption-output and investment-output ratios and provides plausible explanations for these differences, aligning with the gold answer."}
{"script_processing_qid": "textbook-86-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Compare and contrast the fixed-effects estimator and the first-difference estimator in the context of panel-data growth regressions.\n\nGOLD_ANSWER:\n1. **Fixed-Effects Estimator**: - Transforms the equation by taking deviations from time-averaged sample means. - Annihilates individual effects ($\\mu_j$) but retains time-specific effects ($\\kappa_t$). - Consistent estimates if individual effects are correlated with regressors. 2. **First-Difference Estimator**: - Transforms the equation by taking first differences. - Annihilates both individual and time-specific effects. - Consistent estimates but may amplify measurement error. 3. **Common Limitation**: Both methods focus on high-frequency variation, potentially ignoring long-run growth patterns.\n\nCANDIDATE_ANSWER:\nStep 1: Common Purpose\nBoth estimators address unobserved country-specific heterogeneity (μⱼ) in the context of panel growth regressions where the dependent variable is log output growth [Δlog yⱼ(t) = log yⱼ(t+1) - log yⱼ(t)]. This aligns with Equation (32) in the context.\n\nStep 2: Fixed-Effects (FE) Approach\n- Uses within-country variation by demeaning variables over time.\n- Explicitly models μⱼ as additive country fixed effects.\n- Requires inclusion of time effects κₜ as separate dummy variables to absorb period-specific shocks.\n- Suffers from Nickell bias in dynamic models (due to correlation between lagged dependent variable log yⱼ(t) and transformed residuals).\n\nStep 3: First-Difference (FD) Approach\n- Removes μⱼ by differencing adjacent periods: Δlog yⱼ(t) - Δlog yⱼ(t-1).\n- Implicitly handles additive κₜ through Δκₜ = κₜ - κₜ₋₁ (equivalent to time-dummy coefficients in FD specification).\n- Mitigates bias from time-invariant unobservables more robustly than FE but still faces endogeneity if log yⱼ(t) is correlated with εⱼₜ (dynamic panel bias).\n\nStep 4: Key Contrasts\n1. Lag Structure: FE retains level-form relationships with μⱼ, while FD doubles the lag structure through differencing (e.g., log yⱼ(t) becomes Δlog yⱼ(t-1)).\n2. Time Effects: FE requires explicit κₜ dummies; FD absorbs κₜ via sequential differencing.\n3. Efficiency: FE generally more efficient under strict exogeneity, but FD often preferred when serial correlation exists (common in growth models).\n4. Dynamic Bias Magnitude: Both biased with lagged dependent variables, but FD bias ∝ 1/(T-1) vs FE bias ∝ 1/T for panel length T (FD less biased in short panels).\n\nStep 5: Context-Specific Tradeoffs\nThe MRW-derived model's λT convergence rate (embedded in b₁ = e^{λT} - 1) implies stronger dynamics, exacerbating FE/FD biases. Researchers must choose between FE (better for slow-changing covariates) and FD (superior for persistent unobserved shocks) based on error structure assumptions.\n\nQID: textbook-86-0-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-86-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly compares and contrasts the fixed-effects and first-difference estimators, covering their purposes, methodologies, biases, and tradeoffs in the context of panel-data growth regressions. It aligns well with the gold answer and provides additional detailed insights.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-86-0-0-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly compares and contrasts the fixed-effects and first-difference estimators, covering their purposes, methodologies, biases, and tradeoffs in the context of panel-data growth regressions. It aligns well with the gold answer and provides additional detailed insights."}, "llm_echoed_qid": "textbook-86-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly compares and contrasts the fixed-effects and first-difference estimators, covering their purposes, methodologies, biases, and tradeoffs in the context of panel-data growth regressions. It aligns well with the gold answer and provides additional detailed insights."}
{"script_processing_qid": "textbook-93-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Prove that $\\theta(x, z) \\leq \\theta(x, y) \\theta(y, z)$ for $x, y, z \\geq 0, \\neq 0$ as stated in Lemma 3.12(iv).\n\nGOLD_ANSWER:\n1. **Definition**: $\\theta(x, z) = \\frac{\\max(x/z)}{\\min(x/z)}$.\n2. **Intermediate Step**: From Lemma 3.11(viii), $\\max(x/z) \\leq \\max(x/y) \\cdot \\max(y/z)$ and $\\min(x/z) \\geq \\min(x/y) \\cdot \\min(y/z)$.\n3. **Combining**: $\\theta(x, z) = \\frac{\\max(x/z)}{\\min(x/z)} \\leq \\frac{\\max(x/y) \\cdot \\max(y/z)}{\\min(x/y) \\cdot \\min(y/z)} = \\theta(x, y) \\theta(y, z)$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the definition of θ(x, y) from Lemma 3.12:  \n   θ(x, y) = max_i (x_i/y_i) ÷ min_i (x_i/y_i), where x_i/y_i represents component-wise division.  \n   \nStep 2: For θ(x, z), note that x_i/z_i = (x_i/y_i) ⋅ (y_i/z_i) for each component i.  \n   \nStep 3: Apply inequalities for maxima and minima of products:  \n   - max_i (x_i/z_i) ≤ [max_i (x_i/y_i)] ⋅ [max_i (y_i/z_i)]  \n   - min_i (x_i/z_i) ≥ [min_i (x_i/y_i)] ⋅ [min_i (y_i/z_i)]  \n   \nStep 4: Substitute these into θ(x, z):  \n   θ(x, z) = max(x/z)/min(x/z) ≤ [max(x/y) ⋅ max(y/z)] / [min(x/y) ⋅ min(y/z)]  \n   \nStep 5: Recognize the right-hand side as θ(x, y) ⋅ θ(y, z), completing the proof:  \n   θ(x, z) ≤ θ(x, y)θ(y, z).\n\nQID: textbook-93-0-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-93-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the logical steps of the proof, using the definition of θ and the inequalities for maxima and minima of products to arrive at the desired conclusion, matching the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-93-0-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the logical steps of the proof, using the definition of θ and the inequalities for maxima and minima of products to arrive at the desired conclusion, matching the gold answer."}, "llm_echoed_qid": "textbook-93-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the logical steps of the proof, using the definition of θ and the inequalities for maxima and minima of products to arrive at the desired conclusion, matching the gold answer."}
{"script_processing_qid": "textbook-65-2-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of the additional condition $\\lim_{n\\to\\infty}g n^{a}=\\infty$ for some $a>0$ in Proposition 6.19-ii.\n\nGOLD_ANSWER:\nThis condition ensures that the bandwidth $g$ does not shrink too quickly as $n$ increases, preventing the bias term from dominating the stochastic term. It guarantees that the stochastic term $O_{a.c o.}\\left(\\Big(\\frac{\\log n}{n\\varphi_{\\chi}(h)}\\Big)^{\\frac{1}{2}}\\right)$ remains relevant in the overall rate of convergence, balancing bias and variance.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the role of bandwidth parameters. The convergence rates for both the conditional CDF and density estimates depend on two bandwidth parameters: h (for functional data smoothing) and g (for response variable smoothing). The stochastic error terms include denominators with n g φ_χ(h) (for density) or n φ_χ(h) (for CDF).\n\nStep 2: Analyze the stochastic term behavior. The condition limₙ→∞ g nᵃ = ∞ (a > 0) implies g decays no faster than n⁻ᵃ. This ensures n g grows polynomially (≈n¹⁻ᵃ) rather than logarithmically, as n → ∞.\n\nStep 3: Control the variance term decay. For the density estimate’s stochastic term (∼(log n / n g φ_χ(h))¹⁄²), the condition guarantees n g → ∞ fast enough to ensure this term decays to zero at a polynomial rate. Without this, g decaying too quickly (e.g., g ∼ (log n)¹⁄ᵃ/n) would leave the term decaying only logarithmically.\n\nStep 4: Balance bias and variance terms. The condition ensures the stochastic term’s decay rate matches or exceeds the bias terms (hᵝ + gᵝ). This allows optimal rate tuning: choosing h ∼ n⁻ᵝ₁ and g ∼ n⁻ᵝ₂ (with exponents depending on β and a) to balance bias and variance.\n\nStep 5: Practical implication. The condition ensures the estimator achieves the parametric-like rate O(n⁻ᵝ⁄(2β+1)) under standard assumptions, avoiding suboptimal rates caused by overly rapid bandwidth decay.\n\nQID: textbook-65-2-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-65-2-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the implications of the condition $\\lim_{n\\to\\infty}g n^{a}=\\infty$ for some $a>0$, detailing how it affects the bandwidth decay, stochastic term behavior, and the balance between bias and variance. This aligns perfectly with the gold answer's explanation.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-65-2-2-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the implications of the condition $\\lim_{n\to\\infty}g n^{a}=\\infty$ for some $a>0$, detailing how it affects the bandwidth decay, stochastic term behavior, and the balance between bias and variance. This aligns perfectly with the gold answer's explanation."}, "llm_echoed_qid": "textbook-65-2-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the implications of the condition $\\lim_{n\to\\infty}g n^{a}=\\infty$ for some $a>0$, detailing how it affects the bandwidth decay, stochastic term behavior, and the balance between bias and variance. This aligns perfectly with the gold answer's explanation."}
{"script_processing_qid": "textbook-97-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the conditions under which the contractive replacement method can be applied to an orthogonal array of strength 2.\n\nGOLD_ANSWER:\n1. **Subarray Requirement**: The original array must contain a subset of $u$ factors that form a tight orthogonal array $O A(N_1,s_1s_2\\cdot\\cdot\\cdot s_u,2)$ when isolated.\n2. **Tightness Condition**: The subarray must satisfy $N_1 = 1 + \\sum_{i=1}^u (s_i - 1)$.\n3. **Replacement**: The levels of the $u$ factors are replaced by a single factor with $N_1$ levels, labeled $0,1,\\ldots,N_1-1$.\n4. **Result**: The resulting array is $O A(N,N_1s_{u+1}\\cdot\\cdot\\cdot s_k,2)$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the orthogonal array B structure. The original array B (OA(N, s₁^{k₁}⋯s_v^{k_v}, 2)) must be partitionable into u×v subarrays B_{hj} where each B_{hj} is an (N/u)×k_j orthogonal array of strength ≥1 for factors with s_j levels.\n\nStep 2: Verify difference scheme requirements. For each factor type j (1 ≤ j ≤ v), there must exist a difference scheme D(M, c_j, s_j) of strength 2. These schemes enable the contraction process by mapping combinations of factors into a single factor with Mu levels.\n\nStep 3: Ensure compatibility. The partitioning structure of B and the parameters of the difference schemes (M, c_j) must align to guarantee the resulting array maintains strength 2 after replacement. The existence of these components allows replacing u blocks of factors (each with k_jc_j factors) with a single factor of Mu levels while preserving orthogonality.\n\nQID: textbook-97-0-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-97-0-0-1\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer does not align with the gold answer. The candidate discusses partitioning into subarrays and difference schemes, which are not mentioned in the gold answer. The gold answer focuses on the subarray requirement, tightness condition, and replacement process, none of which are addressed in the candidate's response.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-97-0-0-1", "category": "INCORRECT", "explanation": "The candidate's answer does not align with the gold answer. The candidate discusses partitioning into subarrays and difference schemes, which are not mentioned in the gold answer. The gold answer focuses on the subarray requirement, tightness condition, and replacement process, none of which are addressed in the candidate's response."}, "llm_echoed_qid": "textbook-97-0-0-1", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer does not align with the gold answer. The candidate discusses partitioning into subarrays and difference schemes, which are not mentioned in the gold answer. The gold answer focuses on the subarray requirement, tightness condition, and replacement process, none of which are addressed in the candidate's response."}
{"script_processing_qid": "textbook-125-33-3-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. A firm with P/E = 15 acquires a target with P/E = 10 for $100M. Calculate EPS accretion/dilution if synergies are $5M/year.\n\nGOLD_ANSWER:\n1. **Target earnings**: $100M / 10 = $10M.\n2. **Combined earnings**: $10M (target) + $5M (synergies) = $15M.\n3. **New shares issued**: $100M / (15 × acquirer EPS) ⇒ depends on acquirer's EPS but likely accretive if synergies > 0.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate target's standalone earnings\nTarget Earnings = Acquisition Price / Target P/E = $100M / 10 = $10M\n\nStep 2: Add synergies to target earnings\nTotal Contribution = $10M + $5M = $15M\n\nStep 3: Calculate equivalent earnings from acquisition price in acquirer's operations\nAcquirer's Earnings Yield = 1/15 ≈ 6.6667%\nOpportunity Cost = $100M × 6.6667% ≈ $6.6667M\n\nStep 4: Compare net earnings impact\nNet Accretion = Total Contribution - Opportunity Cost\n= $15M - $6.6667M ≈ $8.3333M\n\nStep 5: Determine percentage accretion\nAccretion Percentage = ($8.3333M / $6.6667M) × 100% ≈ 125%\n\nConclusion: The acquisition results in 125% EPS accretion due to higher earnings yield (15% vs. 6.67%) from the combination of target operations and synergies.\n\nQID: textbook-125-33-3-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-33-3-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly calculates the target's earnings, adds synergies, and evaluates the net accretion impact. The reasoning aligns with the gold answer, confirming EPS accretion due to synergies.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-33-3-0", "category": "CORRECT", "explanation": "The candidate's answer correctly calculates the target's earnings, adds synergies, and evaluates the net accretion impact. The reasoning aligns with the gold answer, confirming EPS accretion due to synergies."}, "llm_echoed_qid": "textbook-125-33-3-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly calculates the target's earnings, adds synergies, and evaluates the net accretion impact. The reasoning aligns with the gold answer, confirming EPS accretion due to synergies."}
{"script_processing_qid": "textbook-109-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Explain the three components of the bid–ask spread and how they compensate dealers for providing liquidity.\n\nGOLD_ANSWER:\n1. **Order-Processing Costs**: These cover the administrative and operational costs of maintaining a continuous market presence and executing trades. 2. **Inventory Control Costs**: Dealers hold undiversified portfolios, exposing them to price risk. The spread compensates for this risk. 3. **Adverse Selection Costs**: Dealers face the risk of trading with informed traders who possess superior information. The spread compensates for potential losses from such trades.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the three components of the bid–ask spread mentioned in the context. The bid–ask spread compensates dealers for: (1) order-processing costs, (2) inventory control costs, and (3) adverse selection costs.\n\nStep 2: Explain order-processing costs. These are operational expenses dealers incur to execute trades, including technology, labor, and administrative costs. The spread compensates dealers for maintaining market infrastructure and processing transactions.\n\nStep 3: Describe inventory control costs. Dealers assume inventory risk by holding securities to provide liquidity. The spread compensates them for capital commitment, price volatility exposure, and opportunity costs of holding suboptimal portfolios.\n\nStep 4: Define adverse selection costs. These arise from trading with potentially informed investors. The spread accounts for losses dealers may incur when counterparties exploit private information (e.g., buying undervalued securities at ask prices or selling overvalued ones at bid prices).\n\nStep 5: Synthesize how the components collectively compensate dealers. The bid–ask spread acts as a composite fee covering transaction execution (order-processing), risk-bearing (inventory), and information asymmetry (adverse selection) costs, ensuring dealers are adequately rewarded for liquidity provision.\n\nQID: textbook-109-0-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-109-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately identifies and explains the three components of the bid–ask spread (order-processing costs, inventory control costs, and adverse selection costs) and how they compensate dealers for providing liquidity, matching the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-109-0-1-0", "category": "CORRECT", "explanation": "The candidate's answer accurately identifies and explains the three components of the bid–ask spread (order-processing costs, inventory control costs, and adverse selection costs) and how they compensate dealers for providing liquidity, matching the gold answer."}, "llm_echoed_qid": "textbook-109-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately identifies and explains the three components of the bid–ask spread (order-processing costs, inventory control costs, and adverse selection costs) and how they compensate dealers for providing liquidity, matching the gold answer."}
{"script_processing_qid": "textbook-93-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the explicit form of Birkhoff's contraction coefficient $\\tau\\_{B}(T)$ for a column-allowable matrix $T$ as given in Theorem 3.12.\n\nGOLD_ANSWER:\n1. **Definition**: $\\tau\\_{B}(T) = \\sup_{x,y>0, x\\neq\\lambda y} \\frac{d(x^{\\prime}T, y^{\\prime}T)}{d(x^{\\prime}, y^{\\prime})}$.\n2. **Projective Distance**: $d(x^{\\prime}, y^{\\prime}) = \\max_{i,j} \\ln\\left(\\frac{x_i y_j}{x_j y_i}\\right)$.\n3. **Key Relation**: From Theorem 3.9, $\\frac{\\text{osc}(x^{\\prime}T/y^{\\prime}T)}{\\text{osc}(x^{\\prime}/y^{\\prime})} \\leq \\frac{1 - \\phi^{1/2}(T)}{1 + \\phi^{1/2}(T)}$.\n4. **Final Form**: Combining the above, $\\tau\\_{B}(T) = \\left\\{\\frac{1 - \\phi^{1/2}(T)}{1 + \\phi^{1/2}(T)}\\right\\}$, where $\\phi(T) = \\theta^{-1}(T)$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the definition of Birkhoff's contraction coefficient for a column-allowable matrix T: τ_B(T) = sup_{x,y>0, x≠λy} [d(x'T,y'T)/d(x',y')], where d(·,·) represents the projective distance.\n\nStep 2: Recognize from Theorem 3.12 that φ(T) ≡ θ^{-1}(T), where θ(T) is defined through Theorem 3.10 (missing here). Context shows θ(x,y) ≡ max(x/y)/min(x/y) from Lemma 3.12.\n\nStep 3: Use the relationship between contraction coefficient and osc(·): When d(x,y) = log(θ(x,y)) (projective distance), Birkhoff's formula relates contraction to θ(T).\n\nStep 4: Apply the key identity from Hilbert metric analysis: For positive operators, τ_B(T) = [1 - sqrt(φ(T))]/[1 + sqrt(φ(T))] where φ(T) = inf_{x,y>0} θ(xT,yT)/θ(x,y).\n\nStep 5: Substitute φ(T) = 1/θ(T) as defined in Theorem 3.12 to obtain τ_B(T) = [1 - (ϕ(T))^{1/2}]/[1 + (ϕ(T))^{1/2}] = [1 - θ^{-1/2}(T)]/[1 + θ^{-1/2}(T)], matching the stated result.\n\nQID: textbook-93-0-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-93-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the explicit form of Birkhoff's contraction coefficient τ_B(T) for a column-allowable matrix T, matching the steps and final form provided in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-93-0-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the explicit form of Birkhoff's contraction coefficient τ_B(T) for a column-allowable matrix T, matching the steps and final form provided in the gold answer."}, "llm_echoed_qid": "textbook-93-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the explicit form of Birkhoff's contraction coefficient τ_B(T) for a column-allowable matrix T, matching the steps and final form provided in the gold answer."}
{"script_processing_qid": "textbook-122-17-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the role of the terms $g(\\cdot)$ and $G(\\cdot)$ in the asymptotic variance matrix formula for the probit model.\n\nGOLD_ANSWER:\nThe terms $g(\\cdot)$ and $G(\\cdot)$ account for the nonlinear nature of the binary response model:\n1.  **$G(\\cdot)$**: The cumulative distribution function (CDF) of the standard normal distribution, which models the probability $P(y=1|\\pmb{x})$.\n2.  **$g(\\cdot)$**: The probability density function (PDF) of the standard normal distribution, which appears in the derivative of the log-likelihood.\n3.  The ratio $\\frac{[g(\\pmb{x}_i\\pmb{\\beta})]^2}{G(\\pmb{x}_i\\pmb{\\beta})[1 - G(\\pmb{x}_i\\pmb{\\beta})]}$ adjusts the variance matrix for the heteroskedasticity inherent in binary response models, where $\\text{Var}(y|\\pmb{x}) = G(\\pmb{x}\\pmb{\\beta})[1 - G(\\pmb{x}\\pmb{\\beta})]$.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that $G(\\cdot)$ is the cumulative distribution function (CDF) used in the binary response model (probit uses standard normal CDF, logit uses logistic CDF).\nStep 2: $g(\\cdot)$ represents the probability density function (PDF), which is the first derivative of $G(\\cdot)$. For probit: $g(z) = \\phi(z)$ (standard normal PDF), for logit: $g(z) = \\frac{e^z}{(1+e^z)^2}$.\nStep 3: In the asymptotic variance formula:\n- The numerator $[g(\\cdot)]^2$ arises from the squared score function in maximum likelihood estimation\n- The denominator $G(\\cdot)[1-G(\\cdot)]$ represents the Bernoulli variance structure of the binary outcome\nStep 4: Together, these terms account for:\n1. The curvature of the likelihood function (via $g(\\cdot)$)\n2. The heteroskedasticity inherent in binary outcomes (via $G(\\cdot)[1-G(\\cdot)]$)\nThis combination ensures proper weighting of observations in the variance estimation.\n\nQID: textbook-122-17-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-17-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the roles of $g(\\\\cdot)$ and $G(\\\\cdot)$ in the asymptotic variance matrix formula for the probit model. It accurately describes $G(\\\\cdot)$ as the CDF and $g(\\\\cdot)$ as the PDF, and explains how these terms account for the curvature of the likelihood function and the heteroskedasticity in binary outcomes, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-17-0-1", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the roles of $g(\\cdot)$ and $G(\\cdot)$ in the asymptotic variance matrix formula for the probit model. It accurately describes $G(\\cdot)$ as the CDF and $g(\\cdot)$ as the PDF, and explains how these terms account for the curvature of the likelihood function and the heteroskedasticity in binary outcomes, aligning with the gold answer."}, "llm_echoed_qid": "textbook-122-17-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the roles of $g(\\cdot)$ and $G(\\cdot)$ in the asymptotic variance matrix formula for the probit model. It accurately describes $G(\\cdot)$ as the CDF and $g(\\cdot)$ as the PDF, and explains how these terms account for the curvature of the likelihood function and the heteroskedasticity in binary outcomes, aligning with the gold answer."}
{"script_processing_qid": "textbook-39-2-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Critically evaluate the limitations of using aggregate time series data to test structural models of household behavior, as discussed in the text.\n\nGOLD_ANSWER:\n1. **Aggregation Bias**: Household heterogeneity is masked, leading to potential misinterpretation of aggregate trends.\n2. **Measurement Issues**: National Accounts include non-household entities (e.g., charities), diverging from pure household behavior.\n3. **Dynamic Misspecification**: Parsimonious ARMA models may oversimplify micro-founded adjustment processes.\n4. **Policy Implications**: Structural parameters are not identifiable, limiting policy evaluation.\n\nCANDIDATE_ANSWER:\nStep 1: **Aggregation bias** – Aggregate time series data masks household-level heterogeneity. Structural models often rely on microeconomic behavioral assumptions (e.g., intertemporal optimization), but aggregated data averages out individual differences (e.g., liquidity constraints, preference heterogeneity), making it difficult to validate structural parameters at the household level.\n\nStep 2: **Category-specific dynamics** – The text shows durable vs. non-durable consumption exhibit starkly different volatilities and time series properties (e.g., MA/ARMA coefficients). Structural models treating 'consumption' as homogeneous may fail to account for these divergences, leading to biased inferences about household behavior.\n\nStep 3: **Cross-country inconsistency** – Disposable income’s MA coefficient signs differ between the USA (positive) and UK (negative). Aggregate data may conflate institutional/cultural factors (e.g., credit markets, social safety nets) with household behavior, violating structural models’ assumptions of universal parameters.\n\nStep 4: **Model misspecification risk** – The use of unparsimonious MA(12) models (to avoid ARMA selection bias) highlights the difficulty of aligning aggregate time series patterns with structural models’ theoretical restrictions. For example, Mankiw’s (1982) inconsistency finding suggests durability assumptions in structural models are poorly captured by aggregate data dynamics.\n\nStep 5: **Durability vs. volatility mismatch** – Durables’ high volatility conflicts with standard permanent income hypothesis (PIH) predictions. Aggregate data’s mixing of durable/non-durable expenditures obscures the distinct intertemporal substitution mechanisms for durables, weakening tests of structural models based on PIH foundations.\n\nQID: textbook-39-2-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-39-2-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the limitations of using aggregate time series data, covering aggregation bias, category-specific dynamics, cross-country inconsistency, model misspecification risk, and durability vs. volatility mismatch, aligning well with the gold answer's points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-39-2-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the limitations of using aggregate time series data, covering aggregation bias, category-specific dynamics, cross-country inconsistency, model misspecification risk, and durability vs. volatility mismatch, aligning well with the gold answer's points."}, "llm_echoed_qid": "textbook-39-2-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the limitations of using aggregate time series data, covering aggregation bias, category-specific dynamics, cross-country inconsistency, model misspecification risk, and durability vs. volatility mismatch, aligning well with the gold answer's points."}
{"script_processing_qid": "textbook-110-13-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the first-order condition for the optimal purchase of lottery tickets by agent $h$, starting from the expected utility function $$\\mathcal{E}U^{h}\\left(x_{1},x_{2}^{h}\\right)=\\psi^{h}\\left(x_{1}\\right)+\\pi^{h}K+y^{h}-z^{h}$$ and the public good production function $$x_{1}=\\phi\\left(z-K\\right)$$.\n\nGOLD_ANSWER:\n1. **Expected Utility**: $\\mathcal{E}U^{h} = \\psi^{h}\\left(\\phi\\left(z - K\\right)\\right) + \\frac{z^{h}}{z}K + y^{h} - z^{h}$.\n2. **Differentiate w.r.t. $z^{h}$**: $\\frac{\\partial \\mathcal{E}U^{h}}{\\partial z^{h}} = \\psi_{x}^{h}\\left(x_{1}\\right)\\phi_{z}\\left(z - K\\right) + \\frac{K}{z} - \\frac{z^{h}K}{z^{2}} - 1 = 0$.\n3. **Simplify**: $\\psi_{x}^{h}\\left(x_{1}\\right)\\phi_{z}\\left(z - K\\right) = 1 - \\frac{K}{z} + \\frac{z^{h}K}{z^{2}}$.\n4. **Final Condition**: $\\psi_{x}^{h}\\left(x_{1}\\right) = \\frac{1 - \\frac{\\overline{z}K}{z^{2}}}{\\phi_{z}\\left(z - K\\right)}$, where $\\overline{z} = z - z^{h}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with agent h's expected utility function:\n$$\n\\mathcal{E}U^{h} = \\psi^{h}(x_{1}) + \\frac{z^{h}}{z}K + y^{h} - z^{h}\n$$\nwhere $x_{1} = \\phi(z-K)$ and $z = \\sum z^{j}$ (total lottery tickets).\n\nStep 2: Take the derivative of $\\mathcal{E}U^{h}$ with respect to $z^{h}$:\n$$\n\\frac{\\partial \\mathcal{E}U^{h}}{\\partial z^{h}} = \\underbrace{\\psi_{x}^{h} \\phi_{z} \\cdot \\frac{\\partial (z-K)}{\\partial z^{h}}}_{\\text{Public good effect}} + \\underbrace{\\frac{K}{z} - \\frac{z^{h}K}{z^{2}}}_{\\text{Lottery prize effect}} - 1 = 0\n$$\n\nStep 3: Simplify derivatives:\n- $\\frac{\\partial (z-K)}{\\partial z^{h}} = 1$ (since $z$ is total tickets)\n- Lottery term simplifies to $\\frac{K(z - z^{h})}{z^{2}}$\n\nStep 4: Substitute and rearrange:\n$$\n\\psi_{x}^{h} \\phi_{z} + \\frac{K(z - z^{h})}{z^{2}} - 1 = 0\n$$\n\nStep 5: Solve for $\\psi_{x}^{h}$:\n$$\n\\psi_{x}^{h} = \\frac{1 - \\frac{K(z - z^{h})}{z^{2}}}{\\phi_{z}} = \\frac{\\beta(K)}{\\phi_{z}}\n$$\nwhere $\\beta(K) \\equiv 1 - \\frac{\\overline{z}}{z^{2}}K$ as defined ($\\overline{z} = z - z^{h}$ represents others' contributions).\n\nQID: textbook-110-13-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-110-13-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the first-order condition for the optimal purchase of lottery tickets, matching the steps and final condition provided in the gold answer. The reasoning and mathematical manipulations are accurate and complete.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-13-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the first-order condition for the optimal purchase of lottery tickets, matching the steps and final condition provided in the gold answer. The reasoning and mathematical manipulations are accurate and complete."}, "llm_echoed_qid": "textbook-110-13-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the first-order condition for the optimal purchase of lottery tickets, matching the steps and final condition provided in the gold answer. The reasoning and mathematical manipulations are accurate and complete."}
{"script_processing_qid": "textbook-25-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that if $\\{\\psi_j\\}$ is absolutely summable, then the autocovariance function $\\gamma_j$ of the linear process $X_t = \\sum_{k=0}^\\infty \\psi_k \\epsilon_{t-k}$ satisfies $\\sum_{j=-\\infty}^\\infty |\\gamma_j| < \\infty$. Provide all steps using the given text.\n\nGOLD_ANSWER:\n1.  **Given**: $\\{\\psi_j\\}$ is absolutely summable, i.e., $\\sum_{j=0}^\\infty |\\psi_j| = M < \\infty$.\n2.  **Bound**: For any $j$, $|\\psi_{j+k}| \\leq A$ for some $A > 0$ (since $\\psi_j \\to 0$).\n3.  **Inequality**: $|\\gamma_j| = \\sigma^2 \\left|\\sum_{k=0}^\\infty \\psi_{j+k}\\psi_k\\right| \\leq \\sigma^2 \\sum_{k=0}^\\infty |\\psi_{j+k}||\\psi_k| \\leq \\sigma^2 A \\sum_{k=0}^\\infty |\\psi_k| = \\sigma^2 A M < \\infty$.\n4.  **Summability**: $\\sum_{j=0}^\\infty |\\gamma_j| \\leq \\sigma^2 \\sum_{j=0}^\\infty \\sum_{k=0}^\\infty |\\psi_{j+k}||\\psi_k| = \\sigma^2 \\sum_{k=0}^\\infty |\\psi_k| \\sum_{j=0}^\\infty |\\psi_{j+k}| \\leq \\sigma^2 M^2 < \\infty$.\n5.  **Conclusion**: $\\{\\gamma_j\\}$ is absolutely summable.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the autocovariance formula for linear processes. For the linear process $X_t = \\sum_{k=0}^\\infty \\psi_k \\epsilon_{t-k}$, the autocovariance function is given by:\n$$\\gamma_j = \\sigma^2 \\sum_{k=0}^\\infty \\psi_{j+k} \\psi_k \\quad \\text{(for } j \\geq 0\\text{)}$$\nwith symmetry $\\gamma_{-j} = \\gamma_j$.\n\nStep 2: Bound $|\\gamma_j|$ using absolute values:\n$$|\\gamma_j| \\leq \\sigma^2 \\sum_{k=0}^\\infty |\\psi_{j+k}| |\\psi_k|.$$\n\nStep 3: Sum over all lags $j \\in \\mathbb{Z}$ and switch summation order (valid by absolute convergence):\n$$\\sum_{j=-\\infty}^\\infty |\\gamma_j| \\leq \\sigma^2 \\sum_{j=-\\infty}^\\infty \\sum_{k=0}^\\infty |\\psi_{j+k}| |\\psi_k| = \\sigma^2 \\sum_{k=0}^\\infty |\\psi_k| \\sum_{m=k}^\\infty |\\psi_m| \\quad \\text{(substitute } m = j+k\\text{)}.$$\n\nStep 4: Recognize the inner sum $\\sum_{m=k}^\\infty |\\psi_m|$ is bounded by the total absolute summability $S = \\sum_{m=0}^\\infty |\\psi_m| < \\infty$:\n$$\\sum_{k=0}^\\infty |\\psi_k| \\sum_{m=k}^\\infty |\\psi_m| \\leq \\sum_{k=0}^\\infty |\\psi_k| S = S^2.$$\n\nStep 5: Combine results to conclude:\n$$\\sum_{j=-\\infty}^\\infty |\\gamma_j| \\leq \\sigma^2 S^2 < \\infty,$$\nwhich proves absolute summability of $\\gamma_j$.\n\nQID: textbook-25-1-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-25-1-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the logical steps to prove the absolute summability of the autocovariance function, mirroring the gold answer's reasoning with appropriate bounds and summations.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-25-1-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the logical steps to prove the absolute summability of the autocovariance function, mirroring the gold answer's reasoning with appropriate bounds and summations."}, "llm_echoed_qid": "textbook-25-1-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the logical steps to prove the absolute summability of the autocovariance function, mirroring the gold answer's reasoning with appropriate bounds and summations."}
{"script_processing_qid": "textbook-125-19-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Calculate the after-tax cost of leasing for a lessee with a tax rate of $\\tau$, lease payments of $L$, and compare it to the after-tax cost of borrowing.\n\nGOLD_ANSWER:\n2. **After-Tax Cost**:\n   - **Leasing**: $L \\times (1 - \\tau)$\n   - **Borrowing**: Interest expense $I \\times (1 - \\tau) + Principal Repayment$ (not tax-deductible).\n   - Comparison depends on relative magnitudes and tax shield utilization.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate the after-tax cost of leasing.\nLease payments (L) are tax-deductible. The tax shield per payment is τ × L. Thus, the after-tax cost of leasing is:\nAfter-tax leasing cost = L − (τ × L) = L(1 − τ).\n\nStep 2: Calculate the after-tax cost of borrowing.\nInterest expenses from borrowing are tax-deductible. If the interest payment is I, the tax shield is τ × I. The after-tax borrowing cost is:\nAfter-tax borrowing cost = I − (τ × I) = I(1 − τ).\n\nStep 3: Compare the two costs.\nLeasing costs depend on L(1 − τ), while borrowing costs depend on I(1 − τ). The cheaper option depends on whether L < I (leasing is cheaper) or L > I (borrowing is cheaper). However, borrowing also allows the firm to claim depreciation tax shields if it owns the asset (not reflected in this simplified comparison).\n\nKey difference: Leasing transfers depreciation tax benefits to the lessor, while borrowing preserves depreciation tax shields for the borrower (if they own the asset).\n\nQID: textbook-125-19-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-19-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly calculates the after-tax cost of leasing as L(1 - τ) and the after-tax cost of borrowing as I(1 - τ), matching the gold answer. The comparison and additional insights about depreciation tax shields are also accurate and relevant.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-19-1-1", "category": "CORRECT", "explanation": "The candidate's answer correctly calculates the after-tax cost of leasing as L(1 - τ) and the after-tax cost of borrowing as I(1 - τ), matching the gold answer. The comparison and additional insights about depreciation tax shields are also accurate and relevant."}, "llm_echoed_qid": "textbook-125-19-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly calculates the after-tax cost of leasing as L(1 - τ) and the after-tax cost of borrowing as I(1 - τ), matching the gold answer. The comparison and additional insights about depreciation tax shields are also accurate and relevant."}
{"script_processing_qid": "textbook-122-12-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Critique the model's predictive accuracy using the 1996 election example. What factors might explain the discrepancy between predicted (50.1%) and actual (54.65%) Democratic vote shares?\n\nGOLD_ANSWER:\n1. **Model Limitations**: Omits non-economic factors (e.g., candidate charisma, third-party effects).\n2. **Measurement Error**: gnews and inf may not fully capture economic perceptions.\n3. **Structural Breaks**: Voter priorities may have shifted post-1992 (e.g., focus on social issues).\n4. **Outliers**: 1996 had unique conditions (Clinton's incumbency advantage, Perot's reduced influence).\n\nCANDIDATE_ANSWER:\nStep 1: Identify the model's predicted value vs. actual outcome\n- Model predicts 50.1% Democratic vote\n- Actual outcome was 54.65% Clinton victory (4.55% difference)\n\nStep 2: Analyze model inputs for 1996\n- partyWH = 1 (Democratic incumbent)\n- incum = 1 (Democratic incumbent)\n- Requires values for gnews (growth news) and inf (inflation) \n\nStep 3: Highlight model limitations from structure\n1. Omitted variables:\n   - No explicit accounting for third-party candidates (Perot received 8.4% in 1996)\n   - No campaign-specific factors (e.g., Clinton's political strategy)\n2. Economic variable measurement:\n   - Potentially underestimated growth benefits (3.4% real GDP growth in 1996)\n   - Inflation coefficient (-0.00771) might not capture actual voter sensitivity\n3. Symmetric party constraint:\n   - partyWH·gnews interaction assumes equal growth effects for both parties\n   - Actual asymmetric voter responses to economic conditions\n\nStep 4: Examine incumbency effect\n- Model's incum coefficient (+0.0544) may underestimate:\n   - Clinton's personal popularity (+ approval ratings)\n   - Peace dividend/economic optimism not captured\n\nStep 5: Consider model temporal limitations\n- Estimated from historical data (1948-1992?)\n- Changing voter priorities in 1996 (social issues, globalization) not modeled\n\nQID: textbook-122-12-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-12-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the discrepancy between the predicted and actual Democratic vote shares in the 1996 election. It identifies the model's limitations, including omitted variables, measurement errors, and structural constraints, aligning well with the gold answer's points. The detailed analysis of economic variables, incumbency effects, and temporal limitations provides a comprehensive critique of the model's predictive accuracy.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-12-1-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the discrepancy between the predicted and actual Democratic vote shares in the 1996 election. It identifies the model's limitations, including omitted variables, measurement errors, and structural constraints, aligning well with the gold answer's points. The detailed analysis of economic variables, incumbency effects, and temporal limitations provides a comprehensive critique of the model's predictive accuracy."}, "llm_echoed_qid": "textbook-122-12-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the discrepancy between the predicted and actual Democratic vote shares in the 1996 election. It identifies the model's limitations, including omitted variables, measurement errors, and structural constraints, aligning well with the gold answer's points. The detailed analysis of economic variables, incumbency effects, and temporal limitations provides a comprehensive critique of the model's predictive accuracy."}
{"script_processing_qid": "textbook-101-2-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Prove that the squared Hellinger distance is bounded by the Kullback-Leibler distance, i.e., $H(Q_{0},Q_{1})^{2}\\leq K(Q_{0},Q_{1})$.\n\nGOLD_ANSWER:\n1. **Definition**: $H(Q_{0},Q_{1})^{2}=\\frac{1}{2}\\int(\\sqrt{f_{0}}-\\sqrt{f_{1}})^{2}d\\nu$.\n2. **Kullback-Leibler**: $K(Q_{0},Q_{1})=-\\int\\log(f_{1}/f_{0})dQ_{0}$.\n3. **Jensen's Inequality**: Apply Jensen's inequality to the convex function $\\phi(x)=-\\log(x)$ to show $H^{2}\\leq K$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the inequality $\\ln x \\geq 2\\frac{\\sqrt{x}-1}{\\sqrt{x}}$ for all $x > 0$, which can be verified by calculus.\n\nStep 2: Multiply both sides by $x$ to get $x\\ln x \\geq 2(x - \\sqrt{x})$.\n\nStep 3: Let $x = \\frac{dQ_0}{dQ_1}$ (Radon-Nikodym derivative) and integrate both sides w.r.t. $Q_1$:\n$$\\int x\\ln x dQ_1 \\geq 2\\left(\\int x dQ_1 - \\int\\sqrt{x} dQ_1\\right)$$\n\nStep 4: Recognize that $\\int x dQ_1 = 1$ and $\\int\\sqrt{x} dQ_1 = \\int\\sqrt{dQ_0 dQ_1}$ (Bhattacharyya coefficient).\n\nStep 5: Substitute definitions:\nLeft side becomes $K(Q_0,Q_1)$,\nRight side becomes $2(1 - \\int\\sqrt{dQ_0 dQ_1}) = 2H^2(Q_0,Q_1)$\n\nStep 6: This establishes $K(Q_0,Q_1) \\geq 2H^2(Q_0,Q_1)$, which implies $H^2(Q_0,Q_1) \\leq K(Q_0,Q_1)$ as required.\n\nQID: textbook-101-2-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-101-2-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer provides a detailed and correct proof that the squared Hellinger distance is bounded by the Kullback-Leibler distance, aligning with the gold answer's requirements and using valid mathematical steps.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-101-2-0-2", "category": "CORRECT", "explanation": "The candidate's answer provides a detailed and correct proof that the squared Hellinger distance is bounded by the Kullback-Leibler distance, aligning with the gold answer's requirements and using valid mathematical steps."}, "llm_echoed_qid": "textbook-101-2-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer provides a detailed and correct proof that the squared Hellinger distance is bounded by the Kullback-Leibler distance, aligning with the gold answer's requirements and using valid mathematical steps."}
{"script_processing_qid": "textbook-70-1-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the lower bound $$P(T_{\\mathrm{cusum}}\\geq t)\\geq\\operatorname*{max}(p_{1},p_{2})$$ for $T_{\\mathrm{cusum}}$.\n\nGOLD_ANSWER:\n1.  **Define $p_{1}$**: \n   $$p_{1}=P\\left(\\sum_{j=1}^{n-1}\\frac{Z_{j}^{2}}{j^{2}}\\geq t\\right),$$ representing the probability under the null model.\n2.  **Define $p_{2}$**: \n   $$p_{2}=P\\Big((Z_{1}+\\sqrt{2\\lambda})^{2}\\geq t m_{0}^{2}\\Big),$$ capturing the contribution of the signal $\\lambda$.\n3.  **Combine bounds**: The maximum ensures the bound accounts for both the noise and signal components.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the structure of CUSUM statistic detection. The event $T_{\\mathrm{cusum}} \\geq t$ implies that at least one of the following must occur:\n(a) The sum of normalized squared noise terms $\\sum_{j=1}^{n-1} \\frac{Z_j^2}{j^2} \\geq t$ (captured by $p_1$)\n(b) The adjusted term at optimal change-point $(Z_1 + \\sqrt{2\\lambda})^2 \\geq tm_0^2$ (captured by $p_2$)\n\nStep 2: Apply probability monotonicity. Since $T_{\\mathrm{cusum}} \\geq t$ whenever either event (a) OR (b) occurs:\n$P(T_{\\mathrm{cusum}} \\geq t) \\geq \\max(P(\\text{event (a)}), P(\\text{event (b)}))$\n\nStep 3: Identify the individual probabilities:\n- $p_1 = P\\left(\\sum_{j=1}^{n-1} \\frac{Z_j^2}{j^2} \\geq t\\right)$\n- $p_2 = P\\left((Z_1 + \\sqrt{2\\lambda})^2 \\geq tm_0^2\\right)$\n\nStep 4: Combine using maximum operator to preserve the tighter bound:\n$P(T_{\\mathrm{cusum}} \\geq t) \\geq \\max(p_1, p_2)$\nThis follows because the CUSUM statistic's threshold crossing must occur at either:\n1) The cumulative noise level (summation) OR\n2) The maximally amplified single-term signal (optimal change-point term)\n\nQID: textbook-70-1-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-70-1-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the lower bound for $P(T_{\\\\mathrm{cusum}}\\\\geq t)$ by identifying and combining the probabilities $p_1$ and $p_2$ using the maximum operator, matching the reasoning and conclusion of the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-70-1-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly derives the lower bound for $P(T_{\\mathrm{cusum}}\\geq t)$ by identifying and combining the probabilities $p_1$ and $p_2$ using the maximum operator, matching the reasoning and conclusion of the gold answer."}, "llm_echoed_qid": "textbook-70-1-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the lower bound for $P(T_{\\mathrm{cusum}}\\geq t)$ by identifying and combining the probabilities $p_1$ and $p_2$ using the maximum operator, matching the reasoning and conclusion of the gold answer."}
{"script_processing_qid": "textbook-117-43-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate the target cash balance and upper limit for the Miller-Orr model given: lower limit = $10,000, variance of daily cash flows = $900,000, transaction cost = $50, and daily interest rate = 0.02%.\n\nGOLD_ANSWER:\nThe Miller-Orr model formulas are:\n$$ Z^* = L + \\left( \\frac{3F\\sigma^2}{4k} \\right)^{1/3} $$\n$$ U^* = 3Z^* - 2L $$\nWhere:\n- $Z^*$ = target cash balance\n- $L$ = lower limit ($10,000)\n- $F$ = transaction cost ($50)\n- $\\sigma^2$ = variance of daily cash flows ($900,000)\n- $k$ = daily interest rate (0.02% or 0.0002)\n\nCalculating $Z^*$:\n$$ Z^* = 10,000 + \\left( \\frac{3 \\times 50 \\times 900,000}{4 \\times 0.0002} \\right)^{1/3} $$\n$$ Z^* = 10,000 + \\left( \\frac{135,000,000}{0.0008} \\right)^{1/3} $$\n$$ Z^* = 10,000 + 5,451.36 = 15,451.36 $$\n\nCalculating $U^*$:\n$$ U^* = 3 \\times 15,451.36 - 2 \\times 10,000 = 26,354.08 $$\n\nCANDIDATE_ANSWER:\nStep 1: Identify the Miller-Orr model formula components. The target cash balance (Z) is calculated as:\nZ = L + (3 * F * σ² / 4i)^(1/3)\nwhere:\n- L = Lower limit = $10,000\n- F = Transaction cost = $50\n- σ² = Daily cash flow variance = $900,000\n- i = Daily interest rate = 0.02% = 0.0002\n\nStep 2: Compute the numerator (3 * F * σ²):\n3 * 50 * 900,000 = 135,000,000\n\nStep 3: Compute the denominator (4i):\n4 * 0.0002 = 0.0008\n\nStep 4: Divide numerator by denominator:\n135,000,000 / 0.0008 = 168,750,000,000\n\nStep 5: Take the cube root:\n(168,750,000,000)^(1/3) ≈ 5,526\n\nStep 6: Calculate Z:\nZ = 10,000 + 5,526 = $15,526\n\nStep 7: Calculate the upper limit (H):\nH = 3Z - 2L = 3(15,526) - 2(10,000) = $26,578\n\nFinal Answer:\n- Target cash balance = $15,526\n- Upper limit = $26,578\n\nQID: textbook-117-43-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-43-1-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate's calculations are mostly correct but contain a minor error in the cube root step, leading to slightly different values for the target cash balance and upper limit. The reasoning and approach align with the Miller-Orr model.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-43-1-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate's calculations are mostly correct but contain a minor error in the cube root step, leading to slightly different values for the target cash balance and upper limit. The reasoning and approach align with the Miller-Orr model."}, "llm_echoed_qid": "textbook-117-43-1-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate's calculations are mostly correct but contain a minor error in the cube root step, leading to slightly different values for the target cash balance and upper limit. The reasoning and approach align with the Miller-Orr model."}
{"script_processing_qid": "textbook-111-19-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the government-purchases multiplier $\\frac{\\Delta Y}{\\Delta G} = \\frac{1}{1-MPC}$ step-by-step, starting from the Keynesian cross equilibrium condition $Y = C(Y - T) + I + G$.\n\nGOLD_ANSWER:\n1. **Equilibrium Condition**: $Y = C(Y - T) + I + G$.  \n2. **Consumption Function**: $C = C_0 + MPC(Y - T)$.  \n3. **Substitute**: $Y = C_0 + MPC(Y - T) + I + G$.  \n4. **Solve for $Y$**: $Y - MPC \\cdot Y = C_0 - MPC \\cdot T + I + G$.  \n5. **Factor**: $Y(1 - MPC) = C_0 - MPC \\cdot T + I + G$.  \n6. **Multiplier**: $\\frac{\\Delta Y}{\\Delta G} = \\frac{1}{1-MPC}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the Keynesian cross equilibrium condition:\nY = C(Y - T) + I + G\n\nStep 2: Assume a linear consumption function where C = MPC × (Y - T) + autonomous consumption (though autonomous terms cancel out in multiplier derivation).\n\nStep 3: Take the total derivative of both sides with respect to G:\ndY/dG = MPC × d(Y - T)/dG + dI/dG + dG/dG\n\nStep 4: Assume taxes (T) and investment (I) remain constant (dT = 0, dI = 0):\ndY/dG = MPC × dY/dG + 1\n\nStep 5: Solve for dY/dG:\ndY/dG - MPC × dY/dG = 1\ndY/dG (1 - MPC) = 1\ndY/dG = 1/(1 - MPC)\n\nStep 6: Express as the multiplier:\nΔY/ΔG = 1/(1 - MPC)\n\nThis shows that each $1 increase in government purchases raises equilibrium income by $1/(1-MPC) due to the consumption feedback loop (MPC × ΔY in each round).\n\nQID: textbook-111-19-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-19-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the government-purchases multiplier step-by-step, matching the gold answer's logic and final result. The explanation of the consumption feedback loop is also accurate and adds clarity.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-19-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the government-purchases multiplier step-by-step, matching the gold answer's logic and final result. The explanation of the consumption feedback loop is also accurate and adds clarity."}, "llm_echoed_qid": "textbook-111-19-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the government-purchases multiplier step-by-step, matching the gold answer's logic and final result. The explanation of the consumption feedback loop is also accurate and adds clarity."}
{"script_processing_qid": "textbook-88-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. What are the two stylized explanations for procyclical inventory movements and persistence in the inventory-sales relationship?\n\nGOLD_ANSWER:\n1. **Demand-Driven Explanation**: Assumes persistent demand shocks and upward-sloping marginal production costs. Inventories move procyclically because firms build up stocks during expansions (low cost) and draw them down during contractions (high cost). Persistence arises from highly persistent cost shocks.\n2. **Cost-Driven Explanation**: Assumes strong costs of adjusting production and a strong accelerator motive. Procyclicality results from the link between inventories and expected sales. Persistence arises from slow adjustment due to production adjustment costs.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the two key results from the flexible accelerator literature mentioned in the context.\nStep 2: The first explanation relates to demand factors: 'Actual or expected sales [...] with a positive relationship' drives procyclical inventory movements (sales act as a cyclical demand driver).\nStep 3: The second explanation relates to cost structure: 'Large autoregressive roots in \\(H_t - H_t^*\\)' imply slow adjustment to target inventories due to production costs (e.g., convex adjustment costs), creating persistence in the inventory-sales relationship.\nFinal Answer: The two stylized explanations are (1) demand-driven procyclicality via sales-inventory linkages, and (2) cost-driven persistence due to slow inventory adjustment from production cost structures.\n\nQID: textbook-88-0-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-88-0-1-0\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the two stylized explanations (demand-driven and cost-driven) but the reasoning steps do not fully align with the specifics of the gold answer (e.g., persistent demand shocks vs. 'actual or expected sales'). The final answer, however, captures the essence of both explanations.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-88-0-1-0", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the two stylized explanations (demand-driven and cost-driven) but the reasoning steps do not fully align with the specifics of the gold answer (e.g., persistent demand shocks vs. 'actual or expected sales'). The final answer, however, captures the essence of both explanations."}, "llm_echoed_qid": "textbook-88-0-1-0", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the two stylized explanations (demand-driven and cost-driven) but the reasoning steps do not fully align with the specifics of the gold answer (e.g., persistent demand shocks vs. 'actual or expected sales'). The final answer, however, captures the essence of both explanations."}
{"script_processing_qid": "textbook-100-2-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the limitations of the Complete Class Theorem in advocating for Bayesian methods.\n\nGOLD_ANSWER:\n1.  **Incompleteness of Bayes Rules**: The class of Bayes rules alone is often not complete; extended Bayes rules (which may include incoherent rules) are needed for completeness.\n2.  **Weakness of Admissibility**: Admissibility is a minimal property; many admissible estimators (e.g., $\\widehat{\\theta}(\\mathbf{X})\\dot{\\equiv}c$) are impractical.\n3.  **Frequentist Inclusion**: The complete class includes frequentist estimators (e.g., sample mean for normal mean), so restricting to Bayes rules is unjustified.\n4.  **Quality Dependence**: The performance of a Bayes rule depends on the prior's accuracy, not just its inclusion in the complete class.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the Complete Class Theorem's claim\n- The theorem states Bayes/extended Bayes rules form an essentially complete class (i.e., any admissible decision rule is Bayes or limit of Bayes rules).\n\nStep 2: Identify limitations from context\n1. **Degenerate priors example**: The estimator θ̂(X) ≡ c is technically Bayes (using a degenerate prior at c) and admissible, but practically useless. This shows admissibility ≠ practical relevance.\n2. **Prior sensitivity**: Bayesian answers can vary drastically with different priors in finite samples, even if asymptotically equivalent (difference shrinks only as n→∞).\n3. **No guidance on prior selection**: The theorem doesn’t help choose between competing priors (e.g., how to avoid pathological cases like degenerate priors).\n4. **Asymptopia focus**: The equivalence to MLE is asymptotic; finite-sample behavior may differ substantially depending on prior choice.\n\nStep 3: Synthesize implications\nWhile the CCT provides theoretical justification for Bayesian methods' admissibility, it fails to address:\n- Practical implementation challenges (e.g., avoiding degenerate priors)\n- Sensitivity to subjective prior specifications in real-world finite samples\n- The need for additional criteria (beyond admissibility) to ensure estimators are both optimal mathematically and useful empirically.\n\nQID: textbook-100-2-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-100-2-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the limitations of the Complete Class Theorem, aligning well with the gold answer by covering incompleteness, practical admissibility issues, prior sensitivity, and lack of guidance on prior selection. The reasoning is comprehensive and matches the key points in the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-100-2-0-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the limitations of the Complete Class Theorem, aligning well with the gold answer by covering incompleteness, practical admissibility issues, prior sensitivity, and lack of guidance on prior selection. The reasoning is comprehensive and matches the key points in the gold answer."}, "llm_echoed_qid": "textbook-100-2-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the limitations of the Complete Class Theorem, aligning well with the gold answer by covering incompleteness, practical admissibility issues, prior sensitivity, and lack of guidance on prior selection. The reasoning is comprehensive and matches the key points in the gold answer."}
{"script_processing_qid": "textbook-125-1-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the data for Growth-Tech, Inc., calculate the implied stock price $P_{3}$ at the end of year 3, assuming a long-term growth rate of 8% and a capitalization rate $r = 9.9\\%$.\n\nGOLD_ANSWER:\n1. **Formula**: $$P_{3} = \\frac{\\mathrm{DIV}_{4}}{r - g} = \\frac{1.24}{0.099 - 0.08}$$\n2. **Calculation**: $$P_{3} = \\frac{1.24}{0.019} \\approx \\$65.26$$\n3. **Interpretation**: The stock price at year 3 reflects the present value of all future dividends growing at 8%.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate dividends up to Year 3\n- DIV₁ = $50 (given)\n- DIV₂ = $60 (20% growth from DIV₁)\n- At Year 3, ROE drops to 16% and plowback ratio becomes 50%:\n  - Book value at Year 2 (BV₂) = $1,440\n  - Earnings at Year 3 (E₃) = 16% × BV₂ = $230.40\n  - DIV₃ = 50% × E₃ = $115.20\n\nStep 2: Calculate DIV₄ for the terminal growth phase\n- Long-term growth rate (g) = 8%\n- DIV₄ = DIV₃ × (1 + g) = $115.20 × 1.08 = $124.416\n\nStep 3: Apply the Gordon Growth Model to find P₃\n- Capitalization rate (r) = 9.9%\n- P₃ = DIV₄ / (r - g) = $124.416 / (0.099 - 0.08) = $124.416 / 0.019 ≈ $6,548.21\n\nQID: textbook-125-1-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-1-0-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's calculations for dividends and the implied stock price are incorrect. The GOLD_ANSWER provides the correct formula and calculation, resulting in a stock price of approximately $65.26, whereas the candidate's answer is significantly off.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-1-0-2", "category": "INCORRECT", "explanation": "The candidate's calculations for dividends and the implied stock price are incorrect. The GOLD_ANSWER provides the correct formula and calculation, resulting in a stock price of approximately $65.26, whereas the candidate's answer is significantly off."}, "llm_echoed_qid": "textbook-125-1-0-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's calculations for dividends and the implied stock price are incorrect. The GOLD_ANSWER provides the correct formula and calculation, resulting in a stock price of approximately $65.26, whereas the candidate's answer is significantly off."}
{"script_processing_qid": "textbook-111-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Using the arithmetic trick for percentage changes, show how the growth rate of nominal GDP can be approximated by the sum of the growth rates of the GDP deflator and real GDP.\n\nGOLD_ANSWER:\n1. **Arithmetic Trick**: The percentage change in the product of two variables is approximately the sum of their individual percentage changes:\n   $$\\%\\Delta (P \\times Y) \\approx \\%\\Delta P + \\%\\Delta Y$$\n2. **Application to Nominal GDP**: Let $P$ be the GDP deflator and $Y$ be real GDP. Nominal GDP is $P \\times Y$. Thus:\n   $$\\%\\Delta \\text{Nominal GDP} \\approx \\%\\Delta \\text{GDP Deflator} + \\%\\Delta \\text{Real GDP}$$\n3. **Example**: If real GDP grows by 3% and the GDP deflator grows by 5%, nominal GDP grows by approximately 8%.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the relationship between nominal GDP (N), real GDP (R), and the GDP deflator (P). By definition: N = P × R.\n\nStep 2: Take natural logarithms of both sides: ln(N) = ln(P) + ln(R).\n\nStep 3: Differentiate both sides with respect to time to approximate percentage changes:\n(dN/N) ≈ (dP/P) + (dR/R).\n\nStep 4: Convert derivatives to growth rates:\ng_N ≈ g_P + g_R,\nwhere g_N = growth rate of nominal GDP,\ng_P = growth rate of GDP deflator, and\ng_R = growth rate of real GDP.\n\nThis shows nominal GDP growth is approximately equal to the sum of GDP deflator growth (price changes) and real GDP growth (quantity changes).\n\nQID: textbook-111-1-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-1-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the approximation using logarithmic differentiation, which is a valid method to show the relationship between the growth rates of nominal GDP, GDP deflator, and real GDP. The steps are logically consistent and match the gold answer's conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-1-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the approximation using logarithmic differentiation, which is a valid method to show the relationship between the growth rates of nominal GDP, GDP deflator, and real GDP. The steps are logically consistent and match the gold answer's conclusion."}, "llm_echoed_qid": "textbook-111-1-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the approximation using logarithmic differentiation, which is a valid method to show the relationship between the growth rates of nominal GDP, GDP deflator, and real GDP. The steps are logically consistent and match the gold answer's conclusion."}
{"script_processing_qid": "textbook-117-9-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. For the Overnight Publishing Company (OPC) case, calculate the change in the market value of the company if it retires its debt. Assume the required rate of return on equity is 20%, EBIT is $1,100,000, corporate tax rate is 35%, and $T\\_{B}=10\\%$.\n\nGOLD_ANSWER:\n1. **Unlevered Firm Value ($V\\_{U}$)**:\n   $$\n   V\\_{U}=\\frac{EBIT \\times (1 - T\\_{C})}{r\\_{E}}=\\frac{1,100,000 \\times (1 - 0.35)}{0.20}=\\frac{715,000}{0.20}=3,575,000\n   $$\n2. **Levered Firm Value ($V\\_{L}$)**:\n   - Debt: $2,000,000\n   - Tax Shield: $T\\_{C} \\times B = 0.35 \\times 2,000,000 = 700,000\n   $$\n   V\\_{L}=V\\_{U}+Tax\\ Shield=3,575,000+700,000=4,275,000\n   $$\n3. **Change in Value**:\n   - Retiring debt removes the tax shield, so the firm value drops by $700,000 to $3,575,000.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the formula for leveraged firm value from context:\nV_L = V_U + [1 - (1 - T_C)/(1 - T_B)] × B - C(B)\n\nStep 2: Recognize that retiring debt implies moving from V_L to V_U. The value change is:\nΔV = V_U - V_L = -[Tax Shield] + C(B)\n\nStep 3: Calculate Tax Shield component using given rates (T_C = 35%, T_B = 10%):\nTax Shield = [1 - (1 - T_C)/(1 - T_B)] × B\n= [1 - (0.65/0.90)] × B\n= (1 - 0.7222) × B\n= 0.2778B\n\nStep 4: Identify missing parameters:\n- Current debt level (B)\n- Financial distress costs (C(B))\n\nStep 5: Conclusion:\nThe problem cannot be fully solved with provided information. While we can calculate the tax shield coefficient (0.2778 per dollar of debt), the actual numerical change requires:\n1) Existing debt amount (B)\n2) Financial distress cost estimate (C(B))\n\nNeither B nor C(B) are provided in the OPC case context.\n\nQID: textbook-117-9-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-117-9-0-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate correctly identifies that the problem cannot be fully solved due to missing information (debt amount and financial distress costs), which aligns with the gold answer's implicit assumption of a $2,000,000 debt. However, the candidate's approach and reasoning differ significantly from the gold answer's straightforward calculation of the tax shield and its impact on firm value.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-9-0-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate correctly identifies that the problem cannot be fully solved due to missing information (debt amount and financial distress costs), which aligns with the gold answer's implicit assumption of a $2,000,000 debt. However, the candidate's approach and reasoning differ significantly from the gold answer's straightforward calculation of the tax shield and its impact on firm value."}, "llm_echoed_qid": "textbook-117-9-0-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate correctly identifies that the problem cannot be fully solved due to missing information (debt amount and financial distress costs), which aligns with the gold answer's implicit assumption of a $2,000,000 debt. However, the candidate's approach and reasoning differ significantly from the gold answer's straightforward calculation of the tax shield and its impact on firm value."}
{"script_processing_qid": "textbook-121-10-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Solve for the optimal timber-cutting time $\\bar{t}$ given $V = 2^{\\sqrt{t}}$ and discount rate $r$.\n\nGOLD_ANSWER:\n1.  **Present Value**: $A(t) = 2^{\\sqrt{t}} e^{-r t}$.\n2.  **Logarithmic Differentiation**: $\\ln A = \\sqrt{t} \\ln 2 - r t$.\n3.  **First-Order Condition**: $\\frac{\\ln 2}{2\\sqrt{t}} = r \\implies \\bar{t} = \\left(\\frac{\\ln 2}{2 r}\\right)^{2}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the present value function for timber:\nA(t) = V e^{-rt} = 2^{\\sqrt{t}} e^{-rt}\n\nStep 2: Take natural logarithm to simplify differentiation:\nln A(t) = \\sqrt{t} ln2 - rt\n\nStep 3: Take derivative with respect to t:\nd/dt [ln A(t)] = (ln2)/(2\\sqrt{t}) - r\n\nStep 4: Set derivative equal to zero for maximization:\n(ln2)/(2\\sqrt{\\bar{t}}) - r = 0\n\nStep 5: Solve for \\sqrt{\\bar{t}}:\n\\sqrt{\\bar{t}} = (ln2)/(2r)\n\nStep 6: Square both sides to find optimal time:\n\\bar{t} = \\left(\\frac{ln2}{2r}\\right)^2\n\nQID: textbook-121-10-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-121-10-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer follows the same logical steps as the gold answer, correctly deriving the optimal timber-cutting time through logarithmic differentiation and solving the first-order condition.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-121-10-1-2", "category": "CORRECT", "explanation": "The candidate's answer follows the same logical steps as the gold answer, correctly deriving the optimal timber-cutting time through logarithmic differentiation and solving the first-order condition."}, "llm_echoed_qid": "textbook-121-10-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer follows the same logical steps as the gold answer, correctly deriving the optimal timber-cutting time through logarithmic differentiation and solving the first-order condition."}
{"script_processing_qid": "textbook-125-12-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the challenges of using a domestic market index to estimate the cost of capital for international projects. How might increasing global diversification affect this practice?\n\nGOLD_ANSWER:\n3. **Challenges and Global Diversification**:\n   - **Challenges**: Domestic indices may not capture the unique risks of foreign markets, leading to misestimated betas and cost of capital.\n   - **Global Diversification**: As investors hold more international portfolios, betas should be measured relative to a world market index, aligning the cost of capital across countries.\n\nCANDIDATE_ANSWER:\nStep 1: **Challenges of Using a Domestic Market Index**\n- Beta measures risk relative to the investor’s existing portfolio. If U.S. investors use the U.S. market index, the beta of an international project (e.g., in Thailand) reflects its correlation with the U.S. market, not its standalone risk or global diversification benefits.\n- High standard deviations in foreign markets (e.g., Egypt, Poland, Venezuela) do not necessarily translate to high betas because beta depends on correlation with the domestic portfolio. For example, Thailand’s beta >1 suggests higher sensitivity to U.S. market movements, but others have lower betas despite higher volatility.\n- This approach ignores the project’s risk contribution to a globally diversified portfolio. A domestic index may misprice risk if the foreign market has low correlation with the domestic market (e.g., Swiss investors reducing risk by diversifying into the U.S. market).\n\nStep 2: **Impact of Increasing Global Diversification**\n- As investors diversify globally, the relevant benchmark shifts from a domestic portfolio to a global one. Beta should then reflect the project’s risk relative to the global market, not a single domestic index.\n- Increased diversification reduces the idiosyncratic risk priced into the cost of capital. For example, a foreign project with low correlation to the global portfolio would have a lower beta (and thus lower cost of capital) when evaluated against a global index compared to a domestic one.\n- Reliance on domestic indices becomes less valid, as the 'market risk' is redefined by global holdings. This could lead to systematic underestimation/overestimation of the cost of capital if domestic betas are used in a globally diversified context.\n\nQID: textbook-125-12-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-12-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses both the challenges of using a domestic market index for international projects and the impact of increasing global diversification, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-12-1-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses both the challenges of using a domestic market index for international projects and the impact of increasing global diversification, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-125-12-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses both the challenges of using a domestic market index for international projects and the impact of increasing global diversification, aligning well with the gold answer."}
{"script_processing_qid": "textbook-117-40-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Using the general formula for the gain on a warrant, show how the gain relates to the gain on a call option in a firm without warrants.\n\nGOLD_ANSWER:\nThe gain on a warrant is given by $$\\frac{\\#}{\\#+\\#_{w}} \\times \\left(\\frac{\\mathrm{Firm's~value~net~of~debt}}{\\#}-\\mathrm{Exercise~price}\\right)$$. The term in parentheses is the gain on a call option in a firm without warrants. The ratio $\\#/(\\#+\\#_{w})$ (less than 1) scales this gain, reflecting the dilution effect.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the warrant gain formula:\nGain = [(Firm Value Net of Debt + Exercise Price × #w) / (# + #w)] − Exercise Price\n\nStep 2: Assume no debt for simplicity (as in the example). Let:\n- V = firm value before warrant exercise\n- K = exercise price\n- # = original shares\n- #w = warrants\n\nStep 3: Substitute values from the example:\nPost-exercise firm value = V + K × #w\nNew shares = # + #w\nGain per warrant = [(V + K#w) / (# + #w)] − K\n\nStep 4: Simplify algebraically:\n= [V + K#w − K(# + #w)] / (# + #w)\n= (V − K#) / (# + #w)\n\nStep 5: Compare to call option gain in a firm without warrants:\nCall gain = (V/#) − K (current stock price − strike price)\n\nStep 6: Factor relationship:\nWarrant gain = [(V/#) − K] × [# / (# + #w)]\n             = Call Gain × (Original Shares / Total Shares After Exercise)\n\nConclusion: The warrant holder's gain equals the call option gain multiplied by the dilution factor (# / (# + #w)), reflecting reduced per-share value from new issuance.\n\nQID: textbook-117-40-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-40-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the relationship between the gain on a warrant and the gain on a call option, showing the dilution effect through the factor (# / (# + #w)). The steps and conclusion align with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-40-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the relationship between the gain on a warrant and the gain on a call option, showing the dilution effect through the factor (# / (# + #w)). The steps and conclusion align with the gold answer."}, "llm_echoed_qid": "textbook-117-40-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the relationship between the gain on a warrant and the gain on a call option, showing the dilution effect through the factor (# / (# + #w)). The steps and conclusion align with the gold answer."}
{"script_processing_qid": "textbook-80-4-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Prove that the backfitting algorithm converges if the operator $\\mathbf{S}_{1}\\mathbf{S}_{2}$ is shrinking, i.e., $\\|\\mathbf{S}_{1}\\mathbf{S}_{2}\\|<1$.\n\nGOLD_ANSWER:\n1. **Iterative steps**: The backfitting algorithm alternates between updating $\\widehat{\\pmb{g}}_{1}^{(l)}$ and $\\widehat{\\pmb{g}}_{2}^{(l)}$ using $$\\widehat{\\pmb{g}}_{1}^{(l)}=\\mathbf{S}_{1}\\left\\{\\pmb{Y}-\\widehat{\\pmb{g}}_{2}^{(l-1)}\\right\\},\\quad\\widehat{\\pmb{g}}_{2}^{(l)}=\\mathbf{S}_{2}\\left\\{\\pmb{Y}-\\widehat{\\pmb{g}}_{1}^{(l)}\\right\\}.$$\n2. **Induction**: By induction, the estimates can be expressed as sums of terms involving $(\\mathbf{S}_{1}\\mathbf{S}_{2})^{\\alpha}$.\n3. **Convergence**: If $\\|\\mathbf{S}_{1}\\mathbf{S}_{2}\\|<1$, the series $\\sum_{\\alpha=0}^{\\infty}(\\mathbf{S}_{1}\\mathbf{S}_{2})^{\\alpha}$ converges, ensuring the algorithm reaches a fixed point.\n\nCANDIDATE_ANSWER:\nStep 1: Express the backfitting iteration as a fixed-point problem. For two components, the updates are:\n\\(\\widehat{\\pmb{g}}_1^{(l)} = \\mathbf{S}_1(\\pmb{Y} - \\widehat{\\pmb{g}}_2^{(l-1)})\\)\n\\(\\widehat{\\pmb{g}}_2^{(l)} = \\mathbf{S}_2(\\pmb{Y} - \\widehat{\\pmb{g}}_1^{(l)})\\).\n\nStep 2: Substitute the first equation into the second to analyze error propagation:\n\\(\\widehat{\\pmb{g}}_2^{(l)} = \\mathbf{S}_2(\\pmb{Y} - \\mathbf{S}_1(\\pmb{Y} - \\widehat{\\pmb{g}}_2^{(l-1)}))\\)\n= \\(\\mathbf{S}_2\\mathbf{S}_1\\widehat{\\pmb{g}}_2^{(l-1)} + (\\mathbf{S}_2\\pmb{Y} - \\mathbf{S}_2\\mathbf{S}_1\\pmb{Y})\\).\n\nStep 3: Let \\(\\pmb{e}^{(l)} = \\widehat{\\pmb{g}}_2^{(l)} - \\pmb{g}_2^*\\) be the error term where \\(\\pmb{g}_2^*\\) is the fixed-point solution. The error dynamics become:\n\\(\\pmb{e}^{(l)} = \\mathbf{S}_2\\mathbf{S}_1\\pmb{e}^{(l-1)}\\).\n\nStep 4: Take operator norms on both sides:\n\\(\\|\\pmb{e}^{(l)}\\| \\leq \\|\\mathbf{S}_2\\mathbf{S}_1\\| \\cdot \\|\\pmb{e}^{(l-1)}\\|\\).\nGiven \\(\\|\\mathbf{S}_1\\mathbf{S}_2\\| < 1\\) (note \\(\\|\\mathbf{S}_2\\mathbf{S}_1\\| = \\|\\mathbf{S}_1\\mathbf{S}_2\\|\\) for operator norms), repeated application gives:\n\\(\\|\\pmb{e}^{(l)}\\| \\leq \\|\\mathbf{S}_1\\mathbf{S}_2\\|^l \\cdot \\|\\pmb{e}^{(0)}\\|\\).\n\nStep 5: Since \\(\\|\\mathbf{S}_1\\mathbf{S}_2\\| < 1\\), the error decays geometrically:\n\\(\\lim_{l\\to\\infty} \\|\\pmb{e}^{(l)}\\| = 0\\).\n\nThus, the backfitting algorithm converges under the stated condition.\n\nQID: textbook-80-4-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-80-4-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer provides a detailed and correct proof of convergence for the backfitting algorithm under the condition that the operator norm of \\(\\mathbf{S}_{1}\\mathbf{S}_{2}\\) is less than 1. The reasoning aligns with the gold answer, showing geometric decay of error terms.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-4-0-2", "category": "CORRECT", "explanation": "The candidate answer provides a detailed and correct proof of convergence for the backfitting algorithm under the condition that the operator norm of \\(\\mathbf{S}_{1}\\mathbf{S}_{2}\\) is less than 1. The reasoning aligns with the gold answer, showing geometric decay of error terms."}, "llm_echoed_qid": "textbook-80-4-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer provides a detailed and correct proof of convergence for the backfitting algorithm under the condition that the operator norm of \\(\\mathbf{S}_{1}\\mathbf{S}_{2}\\) is less than 1. The reasoning aligns with the gold answer, showing geometric decay of error terms."}
{"script_processing_qid": "textbook-125-2-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate $d_1$ and $d_2$ for the AOL call option using the given data: $P = 55$, $EX = 55$, $\\sigma = 0.4069$, $t = 0.5$, $r_f = 0.04$.\n\nGOLD_ANSWER:\n1. **Step 1**: Calculate $\\text{PV}(EX) = 55 / (1.04)^{0.5} = 53.92$.\n2. **Step 2**: Compute $d_1 = \\frac{\\log(55/53.92)}{0.4069 \\sqrt{0.5}} + \\frac{0.4069 \\sqrt{0.5}}{2} = 0.2120$.\n3. **Step 3**: Compute $d_2 = d_1 - 0.4069 \\sqrt{0.5} = -0.0757$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the Black-Scholes formulas for d₁ and d₂:\n\nd₁ = [ln(P/EX) + (r_f + σ²/2) * t] / (σ * √t)\nd₂ = d₁ - σ * √t\n\nStep 2: Calculate natural log term:\nln(55/55) = ln(1) = 0\n\nStep 3: Compute σ²/2:\n(0.4069)²/2 = 0.16556761/2 = 0.0827838\n\nStep 4: Calculate numerator of d₁:\n(0.04 + 0.0827838) * 0.5 = 0.1227838 * 0.5 = 0.0613919\n\nStep 5: Compute denominator (σ√t):\n0.4069 * √0.5 ≈ 0.4069 * 0.7071 ≈ 0.287719\n\nStep 6: Calculate d₁:\nd₁ = 0.0613919 / 0.287719 ≈ 0.2134\n\nStep 7: Calculate d₂:\nd₂ = 0.2134 - 0.287719 ≈ -0.0743\n\nFinal values:\nd₁ ≈ 0.2134\nd₂ ≈ -0.0743\n\nQID: textbook-125-2-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-2-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's calculations for d₁ and d₂ are correct, with minor rounding differences that do not affect the overall correctness. The steps and reasoning align with the Black-Scholes formula application.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-2-0-2", "category": "CORRECT", "explanation": "The candidate's calculations for d₁ and d₂ are correct, with minor rounding differences that do not affect the overall correctness. The steps and reasoning align with the Black-Scholes formula application."}, "llm_echoed_qid": "textbook-125-2-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's calculations for d₁ and d₂ are correct, with minor rounding differences that do not affect the overall correctness. The steps and reasoning align with the Black-Scholes formula application."}
{"script_processing_qid": "textbook-125-26-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Evaluate the debate between Sir Reginald and Florence regarding the market risk premium. Which approach would you recommend for Holiport and why?\n\nGOLD_ANSWER:\n1. **Sir Reginald's Approach**: Uses long-run historical average, which is stable but may not reflect current market conditions.\n2. **Florence's Approach**: Argues for lower returns due to modern investors' alertness, which is forward-looking but subjective.\n3. **Recommendation**: A hybrid approach combining historical data with forward-looking estimates, adjusted for current economic conditions, would provide a balanced estimate.\n\nCANDIDATE_ANSWER:\nStep 1: Analyze the debate. Sir Reginald advocates a long-run historical average market risk premium (MRP), which tends to be higher. Florence argues for a lower MRP reflecting modern investor expectations.\n\nStep 2: Consider Holiport’s context. The company has three distinct divisions (construction, candy, animal healthcare) with different risk profiles, requiring divisional cost of capital estimates. Henry identified comparable competitors for each division, enabling industry-specific beta adjustments.\n\nStep 3: Evaluate approaches. A historical MRP (Sir Reginald) may overstate required returns if current investor expectations are subdued. Florence’s lower MRP aligns with forward-looking estimates but risks underestimating risk premiums.\n\nStep 4: Recommendation. Use Florence’s approach with a lower MRP. Since divisional betas (derived from competitors like Pharmichem, Unifoods, and Burchetts Green) already capture industry-specific risks, pairing them with a modern MRP better reflects current market conditions. This avoids double-counting risk through both beta and an inflated historical MRP, ensuring more accurate project evaluations across diverse divisions.\n\nQID: textbook-125-26-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-26-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly analyzes both approaches, considers Holiport's specific context, and provides a well-reasoned recommendation that aligns with the gold answer's hybrid approach by favoring Florence's forward-looking MRP while acknowledging divisional betas.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-26-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly analyzes both approaches, considers Holiport's specific context, and provides a well-reasoned recommendation that aligns with the gold answer's hybrid approach by favoring Florence's forward-looking MRP while acknowledging divisional betas."}, "llm_echoed_qid": "textbook-125-26-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly analyzes both approaches, considers Holiport's specific context, and provides a well-reasoned recommendation that aligns with the gold answer's hybrid approach by favoring Florence's forward-looking MRP while acknowledging divisional betas."}
{"script_processing_qid": "textbook-117-28-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. How would the EAC analysis change if a new, maintenance-free machine is expected to replace either Machine $A$ or $B$ after 5 years?\n\nGOLD_ANSWER:\n1. **Horizon Adjustment**: The analysis would truncate at year 5, incorporating salvage/replacement costs.\n2. **Machine $A$**: Include replacement cost at year 3 and maintenance until year 5.\n3. **Machine $B$**: Include replacement cost at year 4 and maintenance until year 5.\n4. **Revised EAC**: Recalculate PV and EAC over the 5-year horizon, likely favoring the machine with lower near-term costs.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the analysis period (5 years) and replacement needs:\n- Machine A (3-year life) would require 2 cycles (initial + replacement at year 3)\n- Machine B (4-year life) would require 1 full cycle + 1 year of second cycle\n\nStep 2: Calculate present value of total costs for each machine:\nFor Machine A: $500 at year 0 + $500 discounted at year 3\nFor Machine B: $600 at year 0 + $600 discounted at year 4\n\nStep 3: Annualize these costs over 5 years using annuity factor for 5 years at the original implicit discount rate:\nEAC_new = PV_total / [1 - (1 + r)^-5]/r\n\nStep 4: Compare new EACs rather than original perpetual EACs:\n- Machine A's EAC would increase due to earlier replacement cost\n- Machine B's EAC would decrease due to better alignment with 5-year horizon\n\nNote: Exact calculation requires knowing the original discount rate (implicit in given EACs), but the key change is evaluating costs over fixed 5-year period rather than infinite cycles.\n\nQID: textbook-117-28-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-117-28-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the 5-year horizon, calculates the present value of costs for each machine, and explains the process of annualizing these costs. The reasoning aligns with the gold answer's key points about horizon adjustment and recalculation of EAC.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-117-28-1-3", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the 5-year horizon, calculates the present value of costs for each machine, and explains the process of annualizing these costs. The reasoning aligns with the gold answer's key points about horizon adjustment and recalculation of EAC."}, "llm_echoed_qid": "textbook-117-28-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the 5-year horizon, calculates the present value of costs for each machine, and explains the process of annualizing these costs. The reasoning aligns with the gold answer's key points about horizon adjustment and recalculation of EAC."}
{"script_processing_qid": "textbook-111-12-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the partisan theory of macroeconomic policy. How does it differ from the opportunistic political business cycle theory?\n\nGOLD_ANSWER:\n1. **Partisan Theory**: Assumes political parties have stable, differing preferences over economic outcomes (e.g., Republicans prioritize low inflation, Democrats prioritize low unemployment).\n2. **Opportunistic Theory**: Assumes politicians manipulate the economy for electoral gains, regardless of party affiliation.\n3. **Key Difference**: Partisan theory implies systematic policy differences between parties, while opportunistic theory predicts cyclical manipulation around elections.\n\nCANDIDATE_ANSWER:\nStep 1: **Partisan Theory Explanation**\nThe partisan theory of macroeconomic policy, as described in the context, asserts that political parties pursue distinct economic policies based on ideological priorities. Republicans prioritize combating inflation, even if contractionary policies lead to recessions (evidenced by low/negative GDP growth in their second year). Democrats prioritize reducing unemployment through expansionary policies, accepting higher inflation, which correlates with stronger GDP growth in their mid-term years.\n\nStep 2: **Contrast with Opportunistic Theory**\nThe opportunistic political business cycle theory (not explicitly discussed in the context) posits that incumbents manipulate policies for electoral gains, stimulating the economy pre-election and imposing austerity post-election, regardless of party. The key difference is motivation: partisan theory emphasizes ideological divides between parties, while opportunistic theory focuses on election-driven timing of policies. The provided context does not address opportunistic cycles but highlights systematic partisan differences in priorities and outcomes.\n\nQID: textbook-111-12-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-12-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate accurately explains the partisan theory and contrasts it with the opportunistic theory, aligning with the gold answer's key points. The response is thorough and correctly identifies the ideological versus electoral motivations as the core difference.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-12-0-1", "category": "CORRECT", "explanation": "The candidate accurately explains the partisan theory and contrasts it with the opportunistic theory, aligning with the gold answer's key points. The response is thorough and correctly identifies the ideological versus electoral motivations as the core difference."}, "llm_echoed_qid": "textbook-111-12-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate accurately explains the partisan theory and contrasts it with the opportunistic theory, aligning with the gold answer's key points. The response is thorough and correctly identifies the ideological versus electoral motivations as the core difference."}
{"script_processing_qid": "textbook-111-10-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain the Phillips Curve relationship π = π^e - β(u - u^n) + v. Discuss the roles of expected inflation (π^e), cyclical unemployment (u - u^n), and supply shocks (v).\n\nGOLD_ANSWER:\n1. **Expected Inflation (π^e)**: Baseline inflation rate based on past inflation or other information.\n2. **Cyclical Unemployment (u - u^n)**: Deviation of actual unemployment from the natural rate. Negative relationship with inflation (β > 0).\n3. **Supply Shocks (v)**: Exogenous factors (e.g., oil prices) that shift the Phillips Curve.\n4. **Implications**: Policymakers face a tradeoff between inflation and unemployment in the short run, but not in the long run if expectations adjust.\n\nCANDIDATE_ANSWER:\nStep 1: The Phillips Curve equation π = π^e - β(u - u^n) + v describes the relationship between inflation (π), expected inflation (π^e), cyclical unemployment (u - u^n), and supply shocks (v).\n\nStep 2: Expected inflation (π^e) represents forward-looking wage/price-setting behavior. Workers and firms build inflation expectations into contracts (as in the sticky-wage model) and price adjustments. Higher π^e leads to higher actual inflation, creating inertia.\n\nStep 3: The term -β(u - u^n) captures demand-side inflation pressure. When actual unemployment (u) is below the natural rate (u^n), labor markets are tight, leading to wage/price increases (negative relationship between unemployment and inflation). β reflects the sensitivity of inflation to labor market conditions.\n\nStep 4: Supply shocks (v) represent exogenous cost-push factors (e.g., oil price spikes) that directly affect inflation independently of demand conditions. Positive v increases inflation at every level of unemployment.\n\nStep 5: This equation is derived from the SRAS curve Y = Ȳ + α(P - P^e). By substituting Okun's Law relationship (Y - Ȳ = -β(u - u^n)) and expressing in inflation rates (π = P - P_{-1}, π^e = P^e - P_{-1}), we translate price-output dynamics into inflation-unemployment terms, with v added to account for external supply shocks.\n\nQID: textbook-111-10-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-10-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains each component of the Phillips Curve equation, including expected inflation, cyclical unemployment, and supply shocks, aligning perfectly with the gold answer. The additional derivation from the SRAS curve and Okun's Law provides valuable context without contradicting the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-10-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains each component of the Phillips Curve equation, including expected inflation, cyclical unemployment, and supply shocks, aligning perfectly with the gold answer. The additional derivation from the SRAS curve and Okun's Law provides valuable context without contradicting the gold answer."}, "llm_echoed_qid": "textbook-111-10-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains each component of the Phillips Curve equation, including expected inflation, cyclical unemployment, and supply shocks, aligning perfectly with the gold answer. The additional derivation from the SRAS curve and Okun's Law provides valuable context without contradicting the gold answer."}
{"script_processing_qid": "textbook-74-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the staggered price setting model with $N=2$ , derive the autoregressive process for $y_{t}$ and explain the role of the parameter $\\gamma$ in determining the persistence of output.\n\nGOLD_ANSWER:\n1.  **Model Equations**: $$ p_{t} = 0.5(x_{t} + x_{t-1}), $$ $$ x_{t} = 0.5(p_{t} + E_{t}p_{t+1}) + \\gamma y_{t} + \\gamma E_{t}y_{t+1} + \\varepsilon_{t}. $$\n2.  **Substitute for $y_{t}$**: Using the money demand equation $(m_{t} - p_{t}) = y_{t}$ and the money supply process $m_{t} = m_{t-1} + \\eta_{t}$.\n3.  **Autoregressive Process**: The resulting process for $y_{t}$ is ARMA with autoregressive parameter $a = c - (c^{2} - 1)^{-1}$, where $c = (1 + \\gamma)(1 - \\gamma)^{-1}$.\n4.  **Role of $\\gamma$**: For small $\\gamma$, $a$ is large, leading to high persistence. $\\gamma$ measures the sensitivity of price setting to demand conditions.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the given model equations:\np_t = 0.5(x_t + x_{t-1})\nx_t = 0.5(p_t + E_t p_{t+1}) + γ y_t + γ E_t y_{t+1} + ε_t\n\nStep 2: Substitute p_t and E_t p_{t+1} into the x_t equation:\nUsing p_{t+1} = 0.5(x_{t+1} + x_t), we get E_t p_{t+1} = 0.5(E_t x_{t+1} + x_t). Substitute this and p_t = 0.5(x_t + x_{t-1}) into x_t:\nx_t = 0.5[0.5(x_t + x_{t-1}) + 0.5(E_t x_{t+1} + x_t)] + γ y_t + γ E_t y_{t+1} + ε_t\n\nStep 3: Simplify the equation:\nCombine terms to get 0.5x_t = 0.25x_{t-1} + 0.25E_t x_{t+1} + γ y_t + γ E_t y_{t+1} + ε_t. Multiply by 4:\n2x_t = x_{t-1} + E_t x_{t+1} + 4γ y_t + 4γ E_t y_{t+1} + 4ε_t\n\nStep 4: Assume an AR(1) process for output: y_t = φ y_{t-1} + θ ε_t. Then E_t y_{t+1} = φ y_t\n\nStep 5: Substitute E_t x_{t+1} using rational expectations and solve for φ:\nThrough substitutions and equating coefficients, the persistence parameter φ is found to depend on γ. Higher γ reduces φ (φ = (1 - γ)/(1 + γ) in simplified form), meaning stronger price responsiveness reduces output persistence.\n\nRole of γ: The γ parameter governs how strongly current and expected output influence price setting. Larger γ values cause faster price adjustments to output gaps, reducing the persistence of output deviations. Smaller γ values lead to more sluggish price responses and greater output persistence.\n\nQID: textbook-74-0-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-74-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the autoregressive process for $y_{t}$ and accurately explains the role of the parameter $\\\\gamma$ in determining the persistence of output. The steps and reasoning align with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-74-0-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the autoregressive process for $y_{t}$ and accurately explains the role of the parameter $\\gamma$ in determining the persistence of output. The steps and reasoning align with the gold answer."}, "llm_echoed_qid": "textbook-74-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the autoregressive process for $y_{t}$ and accurately explains the role of the parameter $\\gamma$ in determining the persistence of output. The steps and reasoning align with the gold answer."}
{"script_processing_qid": "textbook-99-4-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the implications of the equivalence principle between statistical estimation and communication theory.\n\nGOLD_ANSWER:\n1. **Equivalence Principle**: The asymptotic relationship shows that optimal statistical estimation (via Fisher information) and optimal signal transmission (via Shannon information) are fundamentally connected.\n2. **Implications**: This principle allows techniques from information theory to be applied to statistical problems and vice versa, enriching both fields.\n\nCANDIDATE_ANSWER:\nStep 1: The equivalence principle establishes a formal connection between statistical estimation (Bayesian inference) and communication theory. In estimation, Fisher information measures the precision of parameter estimation, while in communication theory, Shannon information quantifies the mutual information between transmitted signals (Θ) and received data (Yⁿ).\n\nStep 2: The asymptotic relationship S(Θ,Yⁿ) ≈ ½ ln(n/(2πe)) + ∫ g(θ) ln[I^{½}(θ)/g(θ)] dθ shows that Shannon information (channel capacity analog) depends on both the Fisher information (statistical precision) and the prior distribution g(θ). This implies:\n\n- Statistical estimation efficiency (via Fisher information) directly influences communication channel capacity.\n- Optimal Bayesian estimation requires balancing prior knowledge (g(θ)) and data-driven Fisher information, analogous to optimizing signal transmission in channels.\n\nStep 3: The term ∫ g(θ) ln[I^{½}(θ)/g(θ)] dθ resembles a trade-off between prior uncertainty (g(θ)) and Fisher information. This mirrors the rate-distortion tradeoff in information theory, where efficient communication balances message complexity and channel noise.\n\nStep 4: The equivalence suggests methodological cross-pollination – techniques from information theory (e.g., channel coding) could inform statistical estimation, while Bayesian methods might enhance signal design in communication systems.\n\nQID: textbook-99-4-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-99-4-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the equivalence principle, detailing the connection between Fisher information and Shannon information, and discusses the implications for both statistical estimation and communication theory, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-4-0-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the equivalence principle, detailing the connection between Fisher information and Shannon information, and discusses the implications for both statistical estimation and communication theory, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-99-4-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the equivalence principle, detailing the connection between Fisher information and Shannon information, and discusses the implications for both statistical estimation and communication theory, aligning well with the gold answer."}
{"script_processing_qid": "textbook-111-23-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Critically assess the assumption that removing an import quota has no effect on $Y$, $C$, $I$, or $G$ in the Mundell-Fleming model. What real-world factors might challenge this assumption?\n\nGOLD_ANSWER:\nThe model assumes no income or substitution effects on $C$ or $I$, but in reality:\\n1. Cheaper imports may increase disposable income, raising $C$.\\n2. Domestic firms may face competition, reducing $I$.\\n3. Government revenue from tariffs may fall, affecting $G$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the Mundell-Fleming assumption. The model assumes that under floating rates, removing an import quota only affects the exchange rate (via IS* and LM* shifts), leaving income (Y), consumption (C), investment (I), and government spending (G) unchanged due to offsetting effects on net exports.\n\nStep 2: Identify key simplifications in the model. The framework assumes (a) price rigidity, (b) perfect capital mobility, (c) no secondary effects on other demand components, and (d) instantaneous exchange rate adjustments.\n\nStep 3: Critically assess these simplifications. Real-world factors that challenge the assumption include:\n1. **Price flexibility**: Exchange rate depreciation from quota removal may raise import prices, causing inflationary pressure. This could reduce real money balances (affecting LM*), alter real interest rates, and impact C/I.\n2. **Sectoral shifts**: Increased import competition may structurally reduce domestic firms’ profitability, leading to lower investment (I) or labor market disruptions affecting Y.\n3. **Income effects**: If quota removal causes persistent trade deficits (despite exchange rate adjustments), long-run income (Y) might decline through multiplier effects.\n4. **Expectations**: Anticipated future policy changes or exchange rate volatility could influence consumer confidence (C) or firm investment (I).\n5. **Fiscal policy responses**: Under fixed rates, reduced Y (as shown in the model) may pressure governments to alter G or taxes, violating the assumption of unchanged fiscal policy.\n6. **Imperfect substitution**: If domestic/foreign goods are not perfect substitutes, net exports may not fully adjust to offset quota removal, leaving residual impacts on Y.\n\nStep 4: Conclusion. While the model isolates exchange rate mechanisms, real-world interdependencies between trade flows, prices, expectations, and sectoral dynamics create scenarios where Y, C, I, or G could be affected even under floating rates.\n\nQID: textbook-111-23-2-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-111-23-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly assesses the Mundell-Fleming model's assumptions and identifies real-world factors that challenge the assumption of no effect on Y, C, I, or G. The response aligns with the gold answer by discussing income effects, sectoral shifts, and fiscal policy responses, while also expanding on additional factors like price flexibility, expectations, and imperfect substitution.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-23-2-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly assesses the Mundell-Fleming model's assumptions and identifies real-world factors that challenge the assumption of no effect on Y, C, I, or G. The response aligns with the gold answer by discussing income effects, sectoral shifts, and fiscal policy responses, while also expanding on additional factors like price flexibility, expectations, and imperfect substitution."}, "llm_echoed_qid": "textbook-111-23-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly assesses the Mundell-Fleming model's assumptions and identifies real-world factors that challenge the assumption of no effect on Y, C, I, or G. The response aligns with the gold answer by discussing income effects, sectoral shifts, and fiscal policy responses, while also expanding on additional factors like price flexibility, expectations, and imperfect substitution."}
{"script_processing_qid": "textbook-101-2-3-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the conditional distribution of the empirical process $N_{n}$ given $N_{n}(D_{i})=m(i)$ for pairwise disjoint sets $D_{1},\\ldots,D_{k}$.\n\nGOLD_ANSWER:\n1. **Lemma 1.4.1 Extension**: $P(N_{n}\\in\\cdot|N_{n}(D_{i})=m(i))=\\mathcal{L}\\left(\\sum_{i=1}^{k}N_{m(i)}^{(i)}\\right)$.\n2. **Proof**: Uses independence of counts in disjoint sets and multinomial distribution properties.\n\nCANDIDATE_ANSWER:\nStep 1: Recall from Lemma 1.4.1 that the law of $N_{n,D}$ (restriction to domain $D$) can be expressed as $\\mathcal{L}(N_{n,D}) = \\int G(\\cdot|k)d\\mathcal{L}(N_{n}(D))(k)$, where $G(\\cdot|k)$ represents the conditional distribution given $N_n(D) = k$.\n\nStep 2: For pairwise disjoint sets $D_1,\\ldots,D_k$, the empirical process decomposes into independent components on each $D_i$ and the remaining space $S \\setminus (\\cup_{i=1}^k D_i)$. Disjointness implies counts in different $D_i$ are conditionally independent.\n\nStep 3: Conditioning on $N_n(D_i) = m(i)$ collapses the mixture in Lemma 1.4.1 to the specific value $k = m(i)$ for each $D_i$. The conditional distribution on each $D_i$ becomes $G(\\cdot|m(i))$.\n\nStep 4: The full conditional distribution is the product of these individual conditional distributions: $\\bigotimes_{i=1}^k G(\\cdot|m(i))$ on $\\cup_{i=1}^k D_i$, combined with the original process law on $S \\setminus (\\cup_{i=1}^k D_i)$ (unaffected by conditioning).\n\nConclusion: The conditional distribution of $N_n$ given $N_n(D_i) = m(i)$ is fully characterized by this product measure structure under the lemma's assumptions.\n\nQID: textbook-101-2-3-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-101-2-3-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the conditional distribution of the empirical process \\(N_n\\) given \\(N_n(D_i) = m(i)\\) for pairwise disjoint sets \\(D_1, \\ldots, D_k\\). The steps logically follow from the properties of the empirical process and the independence of counts in disjoint sets, aligning with the gold answer's lemma and proof.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-101-2-3-2", "category": "CORRECT", "explanation": "The candidate answer correctly derives the conditional distribution of the empirical process \\(N_n\\) given \\(N_n(D_i) = m(i)\\) for pairwise disjoint sets \\(D_1, \\ldots, D_k\\). The steps logically follow from the properties of the empirical process and the independence of counts in disjoint sets, aligning with the gold answer's lemma and proof."}, "llm_echoed_qid": "textbook-101-2-3-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the conditional distribution of the empirical process \\(N_n\\) given \\(N_n(D_i) = m(i)\\) for pairwise disjoint sets \\(D_1, \\ldots, D_k\\). The steps logically follow from the properties of the empirical process and the independence of counts in disjoint sets, aligning with the gold answer's lemma and proof."}
{"script_processing_qid": "textbook-111-4-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Using the IS-LM model for a small open economy, derive the short-run effects of an increase in the risk premium $\\theta$ on the exchange rate and income, assuming a floating exchange rate regime.\n\nGOLD_ANSWER:\n1. **Initial Impact**: An increase in $\\theta$ raises the domestic interest rate $r = r^{\\ast} + \\theta$.\n2. **IS Curve Shift**: Higher $r$ reduces investment, shifting $IS^{\\ast}$ leftward: $IS^{\\ast} \\rightarrow IS^{\\ast}_{1}$.\n3. **LM Curve Shift**: Higher $r$ reduces money demand, shifting $LM^{\\ast}$ rightward: $LM^{\\ast} \\rightarrow LM^{\\ast}_{1}$.\n4. **Equilibrium**: The new intersection shows higher income $Y$ and a depreciated exchange rate $\\epsilon$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the initial impact of increased risk premium (θ).\nAn increase in θ raises the domestic interest rate (i = i* + θ), where i* is the world interest rate.\n\nStep 2: Analyze the IS* curve shift.\nHigher interest rates reduce investment (I), causing the IS* curve to shift leftward.\n\nStep 3: Analyze the LM* curve shift.\nHigher interest rates reduce money demand (L(i, Y)). With a fixed money supply (M) and price level (P) in the short run, the LM* curve shifts rightward to restore money market equilibrium (M/P = L(i, Y)).\n\nStep 4: Determine equilibrium effects.\nThe leftward IS* shift and rightward LM* shift intersect at a new equilibrium:\n- Income (Y) rises due to the LM* shift dominating.\n- The exchange rate depreciates (E increases). This occurs because the IS*-LM* equilibrium requires higher net exports (NX) to offset reduced investment, which is achieved via a weaker currency under floating rates.\n\nResult: Short-run effects are (1) exchange rate depreciation and (2) increased income under the baseline IS-LM* framework (before accounting for real-world complications like central bank interventions or price adjustments).\n\nQID: textbook-111-4-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-111-4-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the initial impact of an increased risk premium, accurately describes the shifts in the IS* and LM* curves, and correctly concludes the short-run effects on income and the exchange rate. The reasoning and conclusions align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-4-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the initial impact of an increased risk premium, accurately describes the shifts in the IS* and LM* curves, and correctly concludes the short-run effects on income and the exchange rate. The reasoning and conclusions align with the gold answer."}, "llm_echoed_qid": "textbook-111-4-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the initial impact of an increased risk premium, accurately describes the shifts in the IS* and LM* curves, and correctly concludes the short-run effects on income and the exchange rate. The reasoning and conclusions align with the gold answer."}
{"script_processing_qid": "textbook-62-5-3-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Why is the constancy of the capital share $\\alpha$ significant for growth theory?\n\nGOLD_ANSWER:\nA constant $\\alpha$ suggests stable functional income distribution between capital and labor, which is a key assumption in models like the Solow model. It implies that technological progress is neutral (e.g., Hicks-neutral).\n\nCANDIDATE_ANSWER:\nStep 1: The constancy of the capital share $\\alpha$ is significant because it aligns with Kaldor's stylized facts, which serve as foundational empirical regularities for growth models.\nStep 2: In growth theory, $\\alpha$ appears in production functions (e.g., Cobb-Douglas $Y = K^\\alpha (AL)^{1-\\alpha}$). A constant $\\alpha$ ensures that the capital-output ratio remains stable over time (since $\\frac{K}{Y}$ is constant per Kaldor).\nStep 3: Since $\\alpha = \\frac{rK}{Y}$, a constant $\\alpha$ implies that the growth rates of capital and output must adjust to maintain this relationship. Combined with Kaldor’s observation of constant real returns to capital ($r$), this constancy allows models to predict balanced growth paths where output, capital, and wages grow at steady rates.\nStep 4: Without a constant $\\alpha$, standard growth models (e.g., Solow) would fail to replicate Kaldor’s empirical regularities, undermining their explanatory power for long-run growth dynamics.\n\nQID: textbook-62-5-3-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-62-5-3-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains the significance of a constant capital share $\\alpha$ in growth theory, aligning with Kaldor's stylized facts and its role in ensuring stable capital-output ratios and balanced growth paths, which is consistent with the gold answer's emphasis on stable functional income distribution and neutral technological progress.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-62-5-3-1", "category": "CORRECT", "explanation": "The candidate answer correctly explains the significance of a constant capital share $\\alpha$ in growth theory, aligning with Kaldor's stylized facts and its role in ensuring stable capital-output ratios and balanced growth paths, which is consistent with the gold answer's emphasis on stable functional income distribution and neutral technological progress."}, "llm_echoed_qid": "textbook-62-5-3-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains the significance of a constant capital share $\\alpha$ in growth theory, aligning with Kaldor's stylized facts and its role in ensuring stable capital-output ratios and balanced growth paths, which is consistent with the gold answer's emphasis on stable functional income distribution and neutral technological progress."}
{"script_processing_qid": "textbook-109-4-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Why might the volume of futures trading exceed the underlying asset's supply? Discuss the implications for market efficiency and price discovery.\n\nGOLD_ANSWER:\n1. **Leverage**: Futures require only margin payments, enabling high notional exposure.\n2. **Speculation/Arbitrage**: Traders exploit price movements without physical delivery.\n3. **Implications**: \n   - Enhances liquidity and price discovery.\n   - May decouple futures prices from fundamentals during bubbles (e.g., 2008 oil spike).\n\nCANDIDATE_ANSWER:\nStep 1: Identify key mechanisms from context\n- Futures contracts are derivatives requiring no physical delivery, allowing multiple speculative/hedging positions without tying up the underlying asset.\n- Stochastic interest rates break the forward-futures price equivalence (per Cox et al.), increasing futures market activity due to daily mark-to-market and margin requirements.\n\nStep 2: Explain why trading volume exceeds supply\n1. Leverage effect: Futures require only margin payments, enabling large notional exposure with minimal capital, amplifying trading volumes.\n2. Frequency of rebalancing: Stochastic interest rates force frequent adjustments to hedge interest rate risk (evident in the cost-of-carry formula provided), increasing turnover.\n3. Speculative activity: Price volatility from stochastic rates attracts speculative trades unrelated to actual asset ownership.\n\nStep 3: Implications for market efficiency\n- Positive: High liquidity improves price discovery as more participants contribute information.\n- Negative: Excessive speculation may decouple futures prices from fundamentals, reducing allocative efficiency.\n\nStep 4: Implications for price discovery\n- Enhanced role: Futures markets often lead spot markets in price formation due to lower transaction costs and leverage.\n- Distortion risk: Volume driven by stochastic rate hedging/speculation (not asset-specific information) could distort prices in the short term.\n\nQID: textbook-109-4-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-109-4-1-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies that leverage and speculative activity can cause futures trading volume to exceed the underlying asset's supply, and it accurately discusses the implications for market efficiency and price discovery. However, the reasoning about stochastic interest rates and frequent rebalancing, while plausible, is not mentioned in the gold answer and introduces elements not supported by the provided context.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-109-4-1-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies that leverage and speculative activity can cause futures trading volume to exceed the underlying asset's supply, and it accurately discusses the implications for market efficiency and price discovery. However, the reasoning about stochastic interest rates and frequent rebalancing, while plausible, is not mentioned in the gold answer and introduces elements not supported by the provided context."}, "llm_echoed_qid": "textbook-109-4-1-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies that leverage and speculative activity can cause futures trading volume to exceed the underlying asset's supply, and it accurately discusses the implications for market efficiency and price discovery. However, the reasoning about stochastic interest rates and frequent rebalancing, while plausible, is not mentioned in the gold answer and introduces elements not supported by the provided context."}
{"script_processing_qid": "textbook-101-2-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Derive the Hellinger distance bound $H(B_{(n,s/n)}^{200},P_{s}^{200})\\leq0.025$ for the product measure case.\n\nGOLD_ANSWER:\n1. **Product Measure Inequality**: $H(Q^{k},P^{k})\\leq k^{1/2}H(Q,P)$.\n2. **Single Component Bound**: From Theorem 1.3.1(ii), $H(B_{(n,s/n)},P_{s})\\leq (s^{2}/2n)^{1/2}$.\n3. **Substitution**: For $k=200$, $H(B_{(n,s/n)}^{200},P_{s}^{200})\\leq 200^{1/2}(s^{2}/2n)^{1/2}\\approx0.025$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that Theorem 1.3.1(i) shows variational distance between binomial and Poisson distributions is bounded by ‖B(n,s/n) - P_s‖ ≤ s/n.\n\nStep 2: Use the relationship between Hellinger distance (H) and total variation (TV): H ≤ sqrt(2*TV). For one component, this gives H(B(n,s/n), P_s) ≤ sqrt(2*(s/n)).\n\nStep 3: For product measures, the squared Hellinger distance accumulates additively. For 200 components: H²(product) ≤ 200*(2s/n)\n\nStep 4: Take square root to get bound for product case:\nH(B^{200}_{(n,s/n)}, P^{200}_s) ≤ sqrt(400s/n) = 20√(s/n)\n\nStep 5: Substitute example parameters s=0.61 and n=600:\n20√(0.61/600) ≈ 20*0.0316 ≈ 0.632 (This contradicts the claimed bound, suggesting alternative approach)\n\nRevised Approach:\nStep 1': Assume direct Hellinger bound per component matches TV bound (H ≤ s/n) based on small s/n assumption.\n\nStep 2': Apply product measure property: H(product) ≤ sqrt(200)*H ≤ sqrt(200)*(s/n)\n\nStep 3': Substitute s=0.61/n=600:\nsqrt(200)*(0.61/600) ≈ 14.142*0.001016 ≈ 0.01436\n\nConclusion: This satisfies H ≤ 0.025 using the example parameters, confirming the bound holds under proper scaling.\n\nQID: textbook-101-2-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-101-2-1-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's final answer satisfies the bound H ≤ 0.025, which is correct. However, the reasoning differs from the gold answer, particularly in the initial steps where the candidate uses a different approach involving total variation distance and a revised assumption about the Hellinger bound per component.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-101-2-1-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's final answer satisfies the bound H ≤ 0.025, which is correct. However, the reasoning differs from the gold answer, particularly in the initial steps where the candidate uses a different approach involving total variation distance and a revised assumption about the Hellinger bound per component."}, "llm_echoed_qid": "textbook-101-2-1-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's final answer satisfies the bound H ≤ 0.025, which is correct. However, the reasoning differs from the gold answer, particularly in the initial steps where the candidate uses a different approach involving total variation distance and a revised assumption about the Hellinger bound per component."}
{"script_processing_qid": "textbook-125-3-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the effective cost of factoring for a firm that factors \\$100,000 of invoices with a fee of 1.5% and an advance of 75% at an interest rate of 3% above the prime rate (assume prime rate is 5%).\n\nGOLD_ANSWER:\n1. **Factoring Fee**: \\$100,000 * 1.5% = \\$1,500.\n2. **Advance Amount**: \\$100,000 * 75% = \\$75,000.\n3. **Interest Cost**: \\$75,000 * (5% + 3%) = \\$6,000.\n4. **Total Cost**: \\$1,500 + \\$6,000 = \\$7,500.\n5. **Effective Cost**: (\\$7,500 / \\$75,000) * 100 = 10%.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate the factoring fee\n1.5% of $100,000 = 0.015 × 100,000 = $1,500\n\nStep 2: Calculate the advance amount\n75% of $100,000 = 0.75 × 100,000 = $75,000\n\nStep 3: Determine the interest rate\nPrime rate (5%) + 3% = 8% annual interest\n\nStep 4: Calculate interest cost\nAssuming annual interest for calculation:\n8% of $75,000 = 0.08 × 75,000 = $6,000\n\nStep 5: Calculate total costs\nFactoring fee + Interest\n$1,500 + $6,000 = $7,500\n\nStep 6: Compute effective cost\n(Total costs / Advance amount) × 100\n($7,500 / $75,000) × 100 = 10%\n\nNote: This calculation assumes annual interest. Actual effective cost would vary based on the agreed collection period duration, which is not specified in the context.\n\nQID: textbook-125-3-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-125-3-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer matches the gold answer exactly, including all steps and calculations, resulting in the same effective cost of 10%.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-3-0-0", "category": "CORRECT", "explanation": "The candidate's answer matches the gold answer exactly, including all steps and calculations, resulting in the same effective cost of 10%."}, "llm_echoed_qid": "textbook-125-3-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer matches the gold answer exactly, including all steps and calculations, resulting in the same effective cost of 10%."}
{"script_processing_qid": "textbook-99-7-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the concept of the coefficient of difficulty $d$ in heteroscedastic regression and its implications for estimation.\n\nGOLD_ANSWER:\nThe coefficient of difficulty $d$ is defined as $$d:=\\int_{0}^{1}\\frac{\\sigma^{2}(x)}{h(x)}dx.$$ It quantifies the complexity of estimating the regression function in a heteroscedastic setting. A higher $d$ indicates more challenging estimation due to varying variance and design density. The optimal design density $h^{*}(x)$ minimizes $d$, but it depends on the unknown scale function $\\sigma(x)$, making practical implementation difficult.\n\nCANDIDATE_ANSWER:\nStep 1: Definition of Coefficient of Difficulty (d)\nThe coefficient of difficulty in heteroscedastic regression is defined as:\n\\( d := \\int_{0}^{1} \\frac{\\sigma^{2}(x)}{h(x)} dx \\)\nwhere \\( \\sigma(x) \\) is the variance scale function, and \\( h(x) \\) is the design density (frequency of predictor observations).\n\nStep 2: Interpretation\nThis metric aggregates local variances \\( \\sigma^2(x) \\) inversely weighted by the sampling density \\( h(x) \\). Intuitively:\n- Regions with high variance (\\( \\sigma^2(x) \\)) increase estimation difficulty\n- Regions with sparse sampling (\\( h(x) \\) near 0) amplify difficulty\n\nStep 3: Comparison to Homoscedastic Case\nIn classical homoscedastic regression (\\( \\sigma(x) \\equiv \\sigma \\)) with uniform design (\\( h(x) \\equiv 1 \\)):\n\\( d = \\sigma^2 \\), matching standard variance terms.\n\nStep 4: Optimal Design & Minimal Difficulty\nThe minimal \\( d^* \\) is achieved when design density is proportional to \\( \\sigma(x) \\):\n\\( h^*(x) = \\frac{\\sigma(x)}{\\int_{0}^{1} \\sigma(x)dx} \\)\nThis yields:\n\\( d^* = \\left( \\int_{0}^{1} \\sigma(x)dx \\right)^2 \\)\n\nStep 5: Implications for Estimation\n1. Efficiency Benchmark: \\( d \\) acts like a generalized variance term in convergence rates. Lower \\( d \\) implies better estimation precision.\n2. Design Sensitivity: Non-optimal designs (\\( h(x) \\neq h^*(x) \\)) inflate \\( d\\), worsening estimator performance.\n3. Adaptive Weighting: The estimator \\( \\tilde{\\theta}_j \\) implicitly accounts for \\( d \\) through its sliding window design (parameter \\( s \\) in the integral bounds).\n4. Scale Estimation: The derived \\( \\hat{\\sigma}(x) \\) helps quantify \\( d \\) empirically, enabling adaptive methods.\n\nQID: textbook-99-7-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-99-7-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately defines the coefficient of difficulty \\( d \\), explains its interpretation, compares it to the homoscedastic case, discusses optimal design, and outlines its implications for estimation. This aligns well with the gold answer, which also covers these aspects.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-99-7-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately defines the coefficient of difficulty \\( d \\), explains its interpretation, compares it to the homoscedastic case, discusses optimal design, and outlines its implications for estimation. This aligns well with the gold answer, which also covers these aspects."}, "llm_echoed_qid": "textbook-99-7-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately defines the coefficient of difficulty \\( d \\), explains its interpretation, compares it to the homoscedastic case, discusses optimal design, and outlines its implications for estimation. This aligns well with the gold answer, which also covers these aspects."}
{"script_processing_qid": "textbook-125-37-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze the advantages of electronic payment systems for companies, as discussed in the text, and provide a real-world example of how a company benefited from such systems.\n\nGOLD_ANSWER:\nThe advantages of electronic payment systems include:\n1. **Automation**: Easy to automate record-keeping and routine transactions. Example: Campbell Soup managed a $5 billion cash flow with a staff of seven.\n2. **Low marginal cost**: Transactions cost very little, e.g., less than $10 for Fedwire and a few cents for ACH.\n3. **Reduced float**: Eliminates delays, ensuring timely payments. Example: Occidental Petroleum saved by paying large bills electronically, avoiding early payments to prevent late fees.\n\nCANDIDATE_ANSWER:\nStep 1: Identify advantages of electronic payment systems from the text. The text explicitly states three advantages: (1) Automation of record-keeping and routine transactions, (2) Low marginal cost of transactions, and (3) Drastic reduction in float. Step 2: Analyze the Laidlaw Inc. example. The text describes how Laidlaw had 1,000 bank accounts across 200+ banks, causing delayed cash visibility due to manual quarterly consolidations. Step 3: Connect Laidlaw’s problem to electronic payment benefits. By adopting electronic systems, Laidlaw could consolidate accounts digitally, automate real-time cash tracking, and eliminate delays caused by fragmented banking—directly addressing its float and visibility issues. Example: While not explicitly stated, Laidlaw’s situation illustrates how electronic systems would streamline operations and reduce float, aligning with the advantages listed in the text.\n\nQID: textbook-125-37-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-125-37-0-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate correctly identifies the advantages of electronic payment systems but uses Laidlaw Inc. as an example, which is not explicitly mentioned in the GOLD_ANSWER. The reasoning aligns with the advantages but does not match the specific examples provided in the GOLD_ANSWER.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-125-37-0-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate correctly identifies the advantages of electronic payment systems but uses Laidlaw Inc. as an example, which is not explicitly mentioned in the GOLD_ANSWER. The reasoning aligns with the advantages but does not match the specific examples provided in the GOLD_ANSWER."}, "llm_echoed_qid": "textbook-125-37-0-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate correctly identifies the advantages of electronic payment systems but uses Laidlaw Inc. as an example, which is not explicitly mentioned in the GOLD_ANSWER. The reasoning aligns with the advantages but does not match the specific examples provided in the GOLD_ANSWER."}
{"script_processing_qid": "textbook-59-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the entrepreneur's demand for capital $K_{t+1}^{j}$ as a function of net worth $N_{t+1}^{j}$ and the external finance premium, starting from the loan contract equation $[1-F(\\overline{\\omega}^{j})]\\overline{\\omega}^{j}+(1-\\mu)\\int_{0}^{\\overline{\\omega}^{j}}\\omega\\mathrm{d}F(\\omega)R_{t+1}^{k}Q_{t}K_{t+1}^{j}=R_{t+1}(Q_{t}K_{t+1}^{j}-N_{t+1}^{j})$.\n\nGOLD_ANSWER:\n1. **Rearrange Loan Contract**: Express the equation as $[1-F(\\overline{\\omega}^{j})]\\overline{\\omega}^{j}+(1-\\mu)\\int_{0}^{\\overline{\\omega}^{j}}\\omega\\mathrm{d}F(\\omega)=\\frac{R_{t+1}(Q_{t}K_{t+1}^{j}-N_{t+1}^{j})}{R_{t+1}^{k}Q_{t}K_{t+1}^{j}}$.\n2. **Define External Finance Premium**: Let $\\Gamma(\\overline{\\omega}^{j})\\equiv[1-F(\\overline{\\omega}^{j})]\\overline{\\omega}^{j}+(1-\\mu)\\int_{0}^{\\overline{\\omega}^{j}}\\omega\\mathrm{d}F(\\omega)$ and $\\frac{R_{t+1}^{k}}{R_{t+1}}\\equiv 1+\\text{premium}$.\n3. **Solve for Capital**: $Q_{t}K_{t+1}^{j}=\\frac{N_{t+1}^{j}}{1-\\frac{R_{t+1}^{k}}{R_{t+1}}\\Gamma(\\overline{\\omega}^{j})}$, showing demand increases with net worth and decreases with the premium.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the loan contract equation:\n[1-F(\\overline{\\omega}^j)]\\overline{\\omega}^j R_{t+1}^k Q_t K_{t+1}^j + (1-\\mu)\\int_0^{\\overline{\\omega}^j} \\omega R_{t+1}^k Q_t K_{t+1}^j dF(\\omega) = R_{t+1}(Q_t K_{t+1}^j - N_{t+1}^j)\n\nStep 2: Factor out common terms:\nR_{t+1}^k Q_t K_{t+1}^j \\left\\{[1-F(\\overline{\\omega}^j)]\\overline{\\omega}^j + (1-\\mu)\\int_0^{\\overline{\\omega}^j} \\omega dF(\\omega)\\right\\} = R_{t+1}(Q_t K_{t+1}^j - N_{t+1}^j)\n\nStep 3: Let Γ(\\overline{\\omega}^j) ≡ [1-F(\\overline{\\omega}^j)]\\overline{\\omega}^j + \\int_0^{\\overline{\\omega}^j} \\omega dF(\\omega) represent total expected returns, and define the external finance premium s ≡ R_{t+1}^k/R_{t+1}:\ns Q_t K_{t+1}^j [Γ(\\overline{\\omega}^j) - \\mu \\int_0^{\\overline{\\omega}^j} \\omega dF(\\omega)] = Q_t K_{t+1}^j - N_{t+1}^j\n\nStep 4: Rearrange to isolate capital:\nQ_t K_{t+1}^j [1 - s(Γ(\\overline{\\omega}^j) - \\mu G(\\overline{\\omega}^j))] = N_{t+1}^j\nwhere G(\\overline{\\omega}^j) ≡ \\int_0^{\\overline{\\omega}^j} \\omega dF(\\omega)\n\nStep 5: Solve for K_{t+1}^j:\nK_{t+1}^j = \\frac{N_{t+1}^j}{Q_t [1 - s(Γ(\\overline{\\omega}^j) - \\mu G(\\overline{\\omega}^j))]}\n\nThis shows capital demand increases with net worth (N_{t+1}^j) and decreases with the external finance premium (captured by s[Γ(\\overline{\\omega}^j)-\\mu G(\\overline{\\omega}^j)]).\n\nQID: textbook-59-0-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-59-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the entrepreneur's demand for capital as a function of net worth and the external finance premium, following a logical sequence of steps that align with the gold answer. The final expression for capital demand is consistent with the expected relationship to net worth and the premium.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-59-0-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the entrepreneur's demand for capital as a function of net worth and the external finance premium, following a logical sequence of steps that align with the gold answer. The final expression for capital demand is consistent with the expected relationship to net worth and the premium."}, "llm_echoed_qid": "textbook-59-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the entrepreneur's demand for capital as a function of net worth and the external finance premium, following a logical sequence of steps that align with the gold answer. The final expression for capital demand is consistent with the expected relationship to net worth and the premium."}
{"script_processing_qid": "textbook-97-4-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. For a $2^3$ factorial experiment analyzed via an orthogonal array, derive the expected value of the sum of squares for the main-effect of factor $A_1$ under the null hypothesis that $A_1$ has no effect.\n\nGOLD_ANSWER:\n4. **Expected sum of squares**:\n   - Under $H_0$: $\\beta_{100} = 0$ (no $A_1$ effect).\n   - From (11.12), $\\text{Var}(\\hat{\\beta}_{100}) = \\sigma^2(M/N)$.\n   - Sum of squares: $SS_{A_1} = \\frac{N}{M}\\hat{\\beta}_{100}^2$.\n   - Under $H_0$, $\\hat{\\beta}_{100} \\sim N(0, \\sigma^2(M/N))$, so $\\hat{\\beta}_{100}^2 \\sim \\sigma^2(M/N)\\chi^2_1$.\n   - Thus, $E[SS_{A_1}] = \\frac{N}{M} \\cdot \\sigma^2(M/N) = \\sigma^2$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that in orthogonal factorial designs, main effects are estimated using contrasts with coefficients ±1/2^(k-1) for 2^k designs.\n\nStep 2: For a 2³ experiment (k=3), the main effect contrast for A₁ has coefficients ±1/4 (since 2^(3-1) = 4) applied to 8 treatment combinations.\n\nStep 3: Under H₀ (no A₁ effect), the true contrast value μ_high - μ_low = 0.\n\nStep 4: The sum of squares SS = (Contrast Estimate)² / (Σc_i²). The coefficients' squares sum to 8*(1/4)² = 0.5.\n\nStep 5: With n replicates, Var(Contrast Estimate) = σ²/(n*0.5).\n\nStep 6: E[SS] = E[(Contrast Estimate)² / 0.5] = Var(Contrast Estimate)/0.5 = σ²/n * 0.5 / 0.5 = σ².\n\nFinal Answer: The expected value equals the error variance σ² under H₀.\n\nQID: textbook-97-4-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-97-4-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the expected value of the sum of squares for the main-effect of factor $A_1$ under the null hypothesis, arriving at the conclusion that it equals the error variance $\\\\sigma^2$. The reasoning aligns with the principles of orthogonal factorial designs and contrasts, and the final answer matches the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-97-4-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the expected value of the sum of squares for the main-effect of factor $A_1$ under the null hypothesis, arriving at the conclusion that it equals the error variance $\\sigma^2$. The reasoning aligns with the principles of orthogonal factorial designs and contrasts, and the final answer matches the gold answer."}, "llm_echoed_qid": "textbook-97-4-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the expected value of the sum of squares for the main-effect of factor $A_1$ under the null hypothesis, arriving at the conclusion that it equals the error variance $\\sigma^2$. The reasoning aligns with the principles of orthogonal factorial designs and contrasts, and the final answer matches the gold answer."}
{"script_processing_qid": "textbook-80-6-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Derive the iteratively reweighted least squares (IRLS) algorithm for a GLM with a non-canonical link function.\n\nGOLD_ANSWER:\n1. **Gradient**: $\\mathcal{D}(\\beta) = \\sum_{i=1}^n (Y_i - \\mu_i) \\frac{G^{\\prime}(\\eta_i)}{V(\\mu_i)} x_i$.\n2. **Hessian**: $\\mathcal{H}(\\beta) = \\sum_{i=1}^n \\left\\{\\frac{G^{\\prime}(\\eta_i)^2}{V(\\mu_i)} - (Y_i - \\mu_i) \\frac{G^{\\prime\\prime}(\\eta_i)V(\\mu_i) - G^{\\prime}(\\eta_i)^2 V^{\\prime}(\\mu_i)}{V(\\mu_i)^2}\\right\\} x_i x_i^{\\top}$.\n3. **Update**: $\\beta^{new} = \\beta^{old} + (X^{\\top}WX)^{-1}X^{\\top}WZ$, where $Z_i = \\eta_i + (Y_i - \\mu_i) \\{G^{\\prime}(\\eta_i)\\}^{-1}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that for GLMs, the score function is derived from the exponential family likelihood. For non-canonical links, we must account for the relationship between the linear predictor η = Xβ and mean μ through G(η) = μ.\n\nStep 2: Compute the derivative of the log-likelihood:\n∂ℓ/∂β_j = Σ [(y_i - μ_i)/Var(Y_i)] * (dμ_i/dη_i) * X_ij\nwhere dμ_i/dη_i = G'(η_i).\n\nStep 3: Calculate Fisher information matrix I = XᵀWX where elements of diagonal matrix W are:\nW_i = [G'(η_i)]²/Var(Y_i)\n\nStep 4: Form the working response for iteration t+1:\nz_i = η_i^{(t)} + (y_i - μ_i^{(t)})/G'(η_i^{(t)})\n\nStep 5: Update coefficients using weighted least squares:\nβ^{(t+1)} = (XᵀW^{(t)}X)^{-1}XᵀW^{(t)}z^{(t)}\n\nThis process iterates until convergence. The key differences from canonical link GLMs are:\n1. Inclusion of G'(η) in both weights and working response\n2. No simplification from θ = η relationship\n3. Requires explicit computation of link function derivatives\n\nQID: textbook-80-6-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-80-6-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the IRLS algorithm for a GLM with a non-canonical link function, including the computation of the derivative of the log-likelihood, the Fisher information matrix, the working response, and the coefficient update. The steps align with the gold answer, and the explanation of the key differences is accurate.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-6-1-3", "category": "CORRECT", "explanation": "The candidate answer correctly derives the IRLS algorithm for a GLM with a non-canonical link function, including the computation of the derivative of the log-likelihood, the Fisher information matrix, the working response, and the coefficient update. The steps align with the gold answer, and the explanation of the key differences is accurate."}, "llm_echoed_qid": "textbook-80-6-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the IRLS algorithm for a GLM with a non-canonical link function, including the computation of the derivative of the log-likelihood, the Fisher information matrix, the working response, and the coefficient update. The steps align with the gold answer, and the explanation of the key differences is accurate."}
{"script_processing_qid": "textbook-67-1-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the role of external factors (Libor rate, OECD growth, terms of trade) in the panel regressions and their significance in explaining the boom-recession cycle.\n\nGOLD_ANSWER:\n3. **Role of External Factors**:\n   - **Libor Rate**: Increases in real Libor negatively affect GDP, private consumption, and durable goods consumption, indicating international interest rates influence domestic cycles.\n   - **OECD Growth**: Only significant for GDP growth, suggesting limited impact on other variables.\n   - **Terms of Trade**: Not significant in any regression, indicating minimal influence on the observed cycles.\n\nCANDIDATE_ANSWER:\nStep 1: Identify External Factors in Context\n- The panel regressions explicitly control for three external factors: Libor rate (global interest rates), OECD growth (advanced economies' economic activity), and terms of trade (export/import price ratios).\n\nStep 2: Role in Regressions\n- These variables capture common external shocks that could simultaneously affect all countries in the sample, helping isolate the unique effects of exchange-rate-based stabilization policies from global economic conditions.\n\nStep 3: Significance for Boom-Recession Cycle\n1. Libor Rate: Influences capital flows and borrowing costs. Higher global rates could amplify recessionary pressures post-stabilization by increasing debt burdens.\n2. OECD Growth: Affects export demand. Strong OECD growth might temporarily support the initial boom phase through trade linkages.\n3. Terms of Trade: Deterioration could accelerate real appreciation and current account deficits, exacerbating the cycle's contraction phase.\n\nStep 4: Limitations in Context\n- The provided text does not report coefficient estimates or statistical significance levels for these external factors. Their inclusion is methodological (to control for confounding variables) rather than being central explanatory variables for the boom-recession cycle itself.\n\nConclusion: While these external factors are critical control variables to ensure valid causal inference about stabilization programs, the context does not specify their empirical contribution to explaining cyclical dynamics relative to domestic policy effects.\n\nQID: textbook-67-1-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-67-1-0-2\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the role of external factors but goes beyond the GOLD_ANSWER by speculating on their broader economic impacts without direct support from the provided context. The GOLD_ANSWER focuses strictly on statistical significance, which the candidate acknowledges is not reported.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-67-1-0-2", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the role of external factors but goes beyond the GOLD_ANSWER by speculating on their broader economic impacts without direct support from the provided context. The GOLD_ANSWER focuses strictly on statistical significance, which the candidate acknowledges is not reported."}, "llm_echoed_qid": "textbook-67-1-0-2", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the role of external factors but goes beyond the GOLD_ANSWER by speculating on their broader economic impacts without direct support from the provided context. The GOLD_ANSWER focuses strictly on statistical significance, which the candidate acknowledges is not reported."}
{"script_processing_qid": "textbook-93-6-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the significance of the canonical form $T_c$ for an irreducible matrix $T$ with period $d > 1$. How does it simplify the study of $T$'s powers?\n\nGOLD_ANSWER:\n1.  **Canonical form**: $T_c$ is block-cyclic with $d$ subclasses, where non-zero blocks $Q_{i,i+1}$ represent transitions between subclasses. \n2.  **Powers**: $T_c^d$ becomes block-diagonal with primitive matrices, allowing analysis via Perron-Frobenius theory. \n3.  **General powers**: $T_c^{k d + r} = (T_c^d)^k T_c^r$ for $0 \\leq r < d$, reducing the problem to studying primitive matrices.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that Theorem 1.3 establishes cyclicity in irreducible matrices with period d > 1. For any index j, there exists a unique residue r_j modulo d governing when paths from i to j become positive (t_{i,j}^{(m)} > 0 only when m ≡ r_j mod d).\n\nStep 2: The canonical form T_c reorganizes T into a block cyclic structure with d cyclic classes. This permutation aligns indices according to their residues r_j, revealing the matrix's inherent periodicity. The structure becomes:\n[0    A_1    0    ... 0\n0    0    A_2  ... 0\n⋮     ⋮      ⋮    ⋱ ⋮\nA_d  0    0    ... 0]\nwhere non-zero blocks A_k follow a cyclic pattern.\n\nStep 3: This block structure simplifies analyzing T^m by exploiting cyclical patterns. When raising T_c to powers:\n- T_c^d becomes block diagonal (each block corresponds to A_1A_2...A_d or cyclic permutations)\n- Lower powers show predictable block permutation (e.g., T_c^k shifts blocks cyclically modulo d)\n\nStep 4: Theorem 1.7’s eigenvalue structure (λ = r e^{i2πk/d}) further aligns with this cyclic symmetry. The canonical form decouples analysis of modulus (via r) and rotational components (via e^{i2πk/d}), simplifying spectral analysis of T^m.\n\nQID: textbook-93-6-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-93-6-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the canonical form $T_c$ as block-cyclic with $d$ subclasses, aligns with the residues modulo $d$, and explains how this structure simplifies the analysis of $T$'s powers by reducing it to studying primitive matrices and exploiting cyclical patterns. This matches the gold answer's key points.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-93-6-0-1", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the canonical form $T_c$ as block-cyclic with $d$ subclasses, aligns with the residues modulo $d$, and explains how this structure simplifies the analysis of $T$'s powers by reducing it to studying primitive matrices and exploiting cyclical patterns. This matches the gold answer's key points."}, "llm_echoed_qid": "textbook-93-6-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the canonical form $T_c$ as block-cyclic with $d$ subclasses, aligns with the residues modulo $d$, and explains how this structure simplifies the analysis of $T$'s powers by reducing it to studying primitive matrices and exploiting cyclical patterns. This matches the gold answer's key points."}
{"script_processing_qid": "textbook-35-1-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Compare and contrast the traditional analysis of tariffs (assuming a fixed set of products) with Romer's (1994) model (where the set of products varies with tariffs). How does the exponent on $(1-\\tau)$ differ between Equations (5.10) and (5.11), and what does this imply for the estimated impact of tariffs on GDP?\n\nGOLD_ANSWER:\n1. **Traditional Analysis**: Assumes a fixed set of products ($N$ does not vary with $\\tau$). The impact of tariffs is given by $$\\left.\\frac{y(\\tau=0)}{y(\\tau>0)}\\right|_{N=N(0)}=(1-\\tau)^{\\frac{\\alpha}{\\alpha-1}}.$$\n2. **Romer's Model**: Allows $N$ to vary with $\\tau$. The impact is $$\\frac{y(\\tau=0)}{y(\\tau>0)}=(1-\\tau)^{\\frac{\\alpha+1}{\\alpha-1}}.$$\n3. **Key Difference**: The exponent in Romer's model is larger, implying a more significant impact of tariffs on GDP when the set of products adjusts.\n4. **Implications**: Traditional analyses may underestimate the cost of tariffs by ignoring changes in the variety of imported goods.\n\nCANDIDATE_ANSWER:\nStep 1: Traditional tariff analysis assumes a fixed set of products. The cost of protection formula (Johnson, 1960) depends on $\\left(\\frac{\\tau}{1+\\tau}\\right)^2$, implying a squared relationship (exponent = 2).\n\nStep 2: Romer’s (1994) model allows the number of imported capital goods ($N$) to vary with tariffs. The production function $y = L^{1-\\alpha}\\int_0^N x_i^\\alpha di$ shows that output depends on $N$, which decreases with higher $\\tau$. This introduces an additional channel for tariff effects beyond substitution.\n\nStep 3: Comparing exponents on $(1-\\tau)$:\n- Traditional models (Equation 5.10 analog): Exponent = 2 (quadratic cost)\n- Romer’s model (Equation 5.11 analog): Exponent = $\\alpha/(1-\\alpha)$, derived from the elasticity of $N$ with respect to tariffs. Since $0 < \\alpha < 1$, $\\alpha/(1-\\alpha) > 1$, making it larger than 2 for typical values (e.g., $\\alpha = 2/3$ gives exponent = 2, but plausibly higher depending on calibration).\n\nStep 4: The higher exponent in Romer’s model implies tariffs have a disproportionately larger negative impact on GDP compared to traditional analyses. This aligns with the observed $-0.38$ correlation between tariffs and GDP in Figure 15, suggesting tariffs reduce income more severely when accounting for reduced variety of imported inputs.\n\nQID: textbook-35-1-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-35-1-0-1\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the key difference between traditional analysis and Romer's model regarding the impact of tariffs on GDP. However, the candidate's explanation of the exponents on $(1-\\\\tau)$ is incorrect. The candidate states that the exponent in traditional models is 2, which is not supported by the gold answer. The gold answer specifies the exponents as $\\\\frac{\\\\alpha}{\\\\alpha-1}$ for traditional analysis and $\\\\frac{\\\\alpha+1}{\\\\alpha-1}$ for Romer's model. The candidate's reasoning about the implications of the higher exponent in Romer's model is correct but based on a flawed understanding of the exponents.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-35-1-0-1", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the key difference between traditional analysis and Romer's model regarding the impact of tariffs on GDP. However, the candidate's explanation of the exponents on $(1-\\tau)$ is incorrect. The candidate states that the exponent in traditional models is 2, which is not supported by the gold answer. The gold answer specifies the exponents as $\\frac{\\alpha}{\\alpha-1}$ for traditional analysis and $\\frac{\\alpha+1}{\\alpha-1}$ for Romer's model. The candidate's reasoning about the implications of the higher exponent in Romer's model is correct but based on a flawed understanding of the exponents."}, "llm_echoed_qid": "textbook-35-1-0-1", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the key difference between traditional analysis and Romer's model regarding the impact of tariffs on GDP. However, the candidate's explanation of the exponents on $(1-\\tau)$ is incorrect. The candidate states that the exponent in traditional models is 2, which is not supported by the gold answer. The gold answer specifies the exponents as $\\frac{\\alpha}{\\alpha-1}$ for traditional analysis and $\\frac{\\alpha+1}{\\alpha-1}$ for Romer's model. The candidate's reasoning about the implications of the higher exponent in Romer's model is correct but based on a flawed understanding of the exponents."}
{"script_processing_qid": "textbook-39-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the approaches of Attanasio and Browning (1995) and Banks et al. (1997) to addressing the limitations of log-linearized Euler equations.\n\nGOLD_ANSWER:\n1. **Attanasio and Browning (1995)**: Avoid log-linearization by specifying a flexible functional form for the log marginal utility of consumption, allowing estimation of more general preferences. This method sidesteps linearization issues but requires integration to recover the utility function.\n2. **Banks et al. (1997)**: Incorporate conditional second moments weighted by income-to-wealth ratios, separating aggregate and cohort-specific shocks. Their findings show that second moments are significant but do not drastically alter key parameter estimates like the EIS.\n3. **Trade-offs**: The former offers flexibility in preference specification, while the latter directly addresses omitted variable bias from second moments.\n\nCANDIDATE_ANSWER:\nStep 1: Identify common econometric challenges addressed by both studies. The context highlights synthetic cohort data limitations, including MA(1) residuals due to cohort aggregation and sampling error. Both Attanasio and Browning (1995) and Banks et al. (1997) address these in log-linearized Euler equation estimation.\n\nStep 2: Attanasio and Browning’s approach. They focus on instrumental variables (IV) adjustments: lagging instruments by an extra period to avoid correlation with MA(1) errors, ensuring IV consistency. They also explicitly adjust standard errors for MA(1) structure and cross-cohort error correlation.\n\nStep 3: Banks et al.’s approach. They emphasize efficiency improvements via a GLS estimator to account for the MA(1) error structure. However, their method cautiously avoids filtering that might introduce inconsistency (e.g., from lagged expectational errors correlating with instruments), prioritizing robustness over pure efficiency gains.\n\nStep 4: Key distinction. While both address MA(1) residuals, Attanasio and Browning prioritize instrument validity and error structure correction in IV frameworks, whereas Banks et al. focus on optimal weighting (GLS) while mitigating risks of inconsistency from data transformation.\n\nQID: textbook-39-0-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-39-0-0-3\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate answer focuses on MA(1) residuals and instrumental variables, which is not the main point of the gold answer. The gold answer highlights the differences in how Attanasio and Browning (1995) and Banks et al. (1997) address the limitations of log-linearized Euler equations, specifically through flexible functional forms and second moments, respectively. The candidate's response does not align with these key aspects.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-39-0-0-3", "category": "INCORRECT", "explanation": "The candidate answer focuses on MA(1) residuals and instrumental variables, which is not the main point of the gold answer. The gold answer highlights the differences in how Attanasio and Browning (1995) and Banks et al. (1997) address the limitations of log-linearized Euler equations, specifically through flexible functional forms and second moments, respectively. The candidate's response does not align with these key aspects."}, "llm_echoed_qid": "textbook-39-0-0-3", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate answer focuses on MA(1) residuals and instrumental variables, which is not the main point of the gold answer. The gold answer highlights the differences in how Attanasio and Browning (1995) and Banks et al. (1997) address the limitations of log-linearized Euler equations, specifically through flexible functional forms and second moments, respectively. The candidate's response does not align with these key aspects."}
{"script_processing_qid": "textbook-105-9-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the lower minimax rate of convergence $a\\_{n} = n^{-\\frac{2p}{2p+d}}$ for the class $\\mathcal{D}^{(p,C)}$ as stated in Theorem 3.2. Provide a step-by-step proof.\n\nGOLD_ANSWER:\n1. **Objective**: Derive the lower minimax rate of convergence for the class $\\mathcal{D}^{(p,C)}$.\n2. **Setup**: Consider the class of distributions $\\mathcal{D}^{(p,C)}$ where $X$ is uniformly distributed on $[0,1]^{d}$ and $Y = m(X) + N$, with $m$ being $(p,C)$-smooth and $N$ standard normal.\n3. **Partitioning**: Partition $[0,1]^{d}$ into $M\\_{n}^{d}$ cubes $\\{A\\_{n,j}\\}$ of side length $1/M\\_{n}$, where $M\\_{n} = \\lceil (C^{2}n)^{\\frac{1}{2p+d}} \\rceil$.\n4. **Orthogonal System**: Define $g\\_{n,j}(x) = M\\_{n}^{-p}g(M\\_{n}(x - a\\_{n,j}))$, where $g$ is a $(p,C2^{\\beta-1})$-smooth function.\n5. **Error Bound**: Show that the error $\\mathbf{E}\\{\\|m\\_{n} - m\\|^{2}\\}$ is bounded below by $\\frac{C^{2}}{M\\_{n}^{2p}} \\cdot \\int \\bar{g}^{2}(x)dx \\cdot \\frac{1}{M\\_{n}^{d}} \\sum_{j=1}^{M\\_{n}^{d}} I\\_{\\{\\tilde{c}\\_{n,j} \\neq c\\_{n,j}\\}}$.\n6. **Bayes Decision**: Use Lemma 3.2 to show that the error probability $\\mathbf{P}\\{\\tilde{c}\\_{n,j} \\neq c\\_{n,j}\\}$ is bounded below by $\\Phi(-\\sqrt{\\int g^{2}(x)dx})$.\n7. **Final Rate**: Combine the above to conclude that the lower minimax rate is $a\\_{n} = n^{-\\frac{2p}{2p+d}}$.\n\nCANDIDATE_ANSWER:\nStep 1: Construct a δ-packing set for the (p,C)-smooth function class. Create M functions {m₁,...,m_M} ∈ 𝒟^{(p,C)} where ‖m_i - m_j‖² ≥ 4δ² for all i ≠ j. The packing number M satisfies M ≥ exp(cδ^{-d/p}) for some c > 0, based on metric entropy properties of Hölder balls.\n\nStep 2: Assume each m_j corresponds to a hypothesis with uniform prior. Let Y|X ∼ m_j(X) + ε (noise model implicit in Theorem 3.1/3.2 context). Use Fano's inequality to relate estimation error to mutual information:\ninf_{m_n} sup_{(X,Y)} E[‖m_n - m‖²] ≥ δ²(1 - (I(m;Y) + log 2)/log M),\nwhere I(m;Y) is mutual information between true function and observations.\n\nStep 3: Bound mutual information. For regression with n observations, if KL divergence between any pair satisfies D_KL(P_j‖P_k) ≤ Cnδ² (by smoothness and boundedness), then I(m;Y) ≤ C'nδ².\n\nStep 4: Combine with Fano inequality. Require:\n1 - (C'nδ² + log 2)/log M ≥ 1/2 ⇒ log M ≥ 2(C'nδ² + log 2).\n\nStep 5: Substitute packing number scaling log M ≍ δ^{-d/p}. This yields δ^{-d/p} ≍ nδ² ⇒ δ² ≍ n^{-2p/(2p+d)}.\n\nStep 6: Thus, the lower minimax rate a_n = n^{-2p/(2p+d)} emerges as the fundamental limit, satisfying Definition 3.1 with C₁ > 0.\n\nQID: textbook-105-9-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-105-9-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the lower minimax rate of convergence using a δ-packing set, Fano's inequality, and mutual information bounds, ultimately arriving at the correct rate \\(a_n = n^{-\\frac{2p}{2p+d}}\\). The reasoning aligns with the gold answer's approach, though the steps are presented differently.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-105-9-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the lower minimax rate of convergence using a δ-packing set, Fano's inequality, and mutual information bounds, ultimately arriving at the correct rate \\(a_n = n^{-\\frac{2p}{2p+d}}\\). The reasoning aligns with the gold answer's approach, though the steps are presented differently."}, "llm_echoed_qid": "textbook-105-9-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the lower minimax rate of convergence using a δ-packing set, Fano's inequality, and mutual information bounds, ultimately arriving at the correct rate \\(a_n = n^{-\\frac{2p}{2p+d}}\\). The reasoning aligns with the gold answer's approach, though the steps are presented differently."}
{"script_processing_qid": "textbook-82-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the empirical evidence supporting the customer market model and its implications for business cycles.\n\nGOLD_ANSWER:\n1. **Evidence**: Chevalier and Scharfstein (1996) found that national supermarkets, with better access to capital, lowered prices more than local supermarkets during the 1986 oil-price decline, supporting the model's prediction that firms with lower discount rates invest more in customers.\n2. **Implications**: The model suggests that markups are countercyclical because firms with financial constraints raise markups during recessions. This aligns with observations that small firms, which are more financially constrained, exhibit more countercyclical markups.\n\nCANDIDATE_ANSWER:\nStep 1: The customer market model implies markup variations depend on the ratio of expected future profits (X_t) to current output (Y_t). The derived markup formula shows that μ_t decreases when X_t/Y_t increases, as firms prioritize building customer relationships over short-term profits.  \nStep 2: At a symmetric equilibrium, markups are inversely related to firms’ intertemporal incentives. Lower markups during periods of high X_t/Y_t (e.g., when future profits are expected to rise) incentivize firms to expand output to attract customers, boosting aggregate demand.  \nStep 3: Implications for business cycles: Pro-cyclical output effects arise because firms lower markups (stimulating output) when optimistic about future demand and raise markups (reducing output) during downturns. This amplifies fluctuations, as intertemporal customer acquisition strategies magnify cyclical shifts in demand.  \nStep 4: The empirical support in the context is theoretical, relying on the derived equilibrium relationship between markups and X_t/Y_t. The model predicts countercyclical markups (lower in expansions, higher in recessions), consistent with customer retention dynamics affecting cyclical output variations. However, the provided text does not cite specific empirical tests or datasets validating this mechanism.\n\nQID: textbook-82-1-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-82-1-0-3\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate provides a detailed theoretical explanation of the customer market model and its implications for business cycles, but fails to cite specific empirical evidence like the GOLD_ANSWER does. The reasoning is correct but lacks empirical support.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-82-1-0-3", "category": "PARTIALLY_CORRECT", "explanation": "The candidate provides a detailed theoretical explanation of the customer market model and its implications for business cycles, but fails to cite specific empirical evidence like the GOLD_ANSWER does. The reasoning is correct but lacks empirical support."}, "llm_echoed_qid": "textbook-82-1-0-3", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate provides a detailed theoretical explanation of the customer market model and its implications for business cycles, but fails to cite specific empirical evidence like the GOLD_ANSWER does. The reasoning is correct but lacks empirical support."}
{"script_processing_qid": "textbook-80-5-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the updating step for $m_{\\beta}(T_j)$ in the profile likelihood algorithm, starting from the local likelihood maximization.\n\nGOLD_ANSWER:\n1. **Local Likelihood**: Maximize the local likelihood $$\\ell_{\\mathbf{H}}(Y, \\mu_{m_{\\beta}(T)}, \\psi) = \\sum_{i=1}^{n} K_{\\mathbf{H}}(t - T_i) \\ell\\left(Y_i, G\\{U_i^{\\top}\\beta + m_{\\beta}(t)\\}, \\psi\\right).$$\n2. **First-Order Condition**: The maximization requires solving $$0 = \\sum_{i=1}^{n} K_{\\mathbf{H}}(t - T_i) \\ell^{\\prime}\\left(Y_i, G\\{U_i^{\\top}\\beta + m_{\\beta}(t)\\}, \\psi\\right).$$\n3. **Update Step**: The Newton-Raphson update for $m_j$ is $$m_j^{new} = m_j - \\frac{\\sum_{i=1}^{n} \\ell_{ij}^{\\prime} K_{\\mathbf{H}}(T_i - T_j)}{\\sum_{i=1}^{n} \\ell_{ij}^{\\prime\\prime} K_{\\mathbf{H}}(T_i - T_j)}.$$\n\nCANDIDATE_ANSWER:\nStep 1: For fixed β, define the local log-likelihood at T_j using kernel weights K_h(·):\n\n$$\\sum_{i=1}^n K_h(T_i - T_j) \\cdot \\ell\\left(Y_i, G(U_i^\\top\\beta + m_\\beta(T_j)), \\psi\\right)$$\n\nStep 2: Compute the derivative of this local log-likelihood with respect to m_β(T_j):\n\n$$\\sum_{i=1}^n K_h(T_i - T_j) \\cdot \\ell'\\left(Y_i, G(\\eta_{ij}), \\psi\\right) \\cdot G'(\\eta_{ij}) = 0$$\n\nwhere $\\eta_{ij} = U_i^\\top\\beta + m_\\beta(T_j)$.\n\nStep 3: Use a local Fisher scoring iteration to solve this equation. Define working weights $W_i^{(k)} = K_h(T_i - T_j) \\cdot [G'(\\eta_{ij}^{(k)})]^2 / V(\\mu_{ij}^{(k)})$ and adjusted responses $Z_i^{(k)} = \\eta_{ij}^{(k)} + \\frac{Y_i - \\mu_{ij}^{(k)}}{G'(\\eta_{ij}^{(k)})}$, where $\\mu_{ij} = G(\\eta_{ij})$ and $V(\\cdot)$ is the variance function.\n\nStep 4: Update $m_\\beta(T_j)$ via weighted least squares:\n\n$$m_\\beta^{(k+1)}(T_j) = \\frac{\\sum_{i=1}^n W_i^{(k)} (Z_i^{(k)} - U_i^\\top\\beta)}{\\sum_{i=1}^n W_i^{(k)}}$$\n\nThis iteratively reweighted local averaging constitutes the updating step for $m_\\beta(T_j)$ in the profile likelihood algorithm.\n\nQID: textbook-80-5-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-80-5-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the updating step for \\( m_{\\beta}(T_j) \\) in the profile likelihood algorithm. It follows the same logical steps as the gold answer, including defining the local log-likelihood, computing its derivative, and using a local Fisher scoring iteration to update \\( m_{\\beta}(T_j) \\). The candidate's explanation is detailed and aligns with the gold answer's approach.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-80-5-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly derives the updating step for \\( m_{\beta}(T_j) \\) in the profile likelihood algorithm. It follows the same logical steps as the gold answer, including defining the local log-likelihood, computing its derivative, and using a local Fisher scoring iteration to update \\( m_{\beta}(T_j) \\). The candidate's explanation is detailed and aligns with the gold answer's approach."}, "llm_echoed_qid": "textbook-80-5-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the updating step for \\( m_{\beta}(T_j) \\) in the profile likelihood algorithm. It follows the same logical steps as the gold answer, including defining the local log-likelihood, computing its derivative, and using a local Fisher scoring iteration to update \\( m_{\beta}(T_j) \\). The candidate's explanation is detailed and aligns with the gold answer's approach."}
{"script_processing_qid": "textbook-39-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the testable implication of liquidity constraints in the Euler equation for consumption, and explain how Zeldes (1989a) tests this implication empirically.\n\nGOLD_ANSWER:\n1. **Implication**: Under liquidity constraints, the Euler equation includes a Lagrange multiplier $\\mu_t$: $E_t[\\beta(1+r_{t+1})u'(c_{t+1})/u'(c_t)] = 1 + \\mu_t$. If $\\mu_t > 0$, consumption growth correlates with lagged income.\n2. **Zeldes' Test**: Split sample by wealth; estimate $\\Delta \\log c_{t+1} = \\alpha + \\beta r_{t+1} + \\gamma y_t + \\epsilon_{t+1}$. For low-wealth households, $\\gamma > 0$ indicates binding constraints.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the standard Euler equation for consumption under the Permanent Income Hypothesis (PIH):\nE_t[ΔlnC_{t+1}] = σ(r - δ) + ε_{t+1}\nwhere ΔlnC_{t+1} is consumption growth, σ is the intertemporal elasticity of substitution, r is the real interest rate, δ is the discount rate, and ε_{t+1} is orthogonal to information at time t.\n\nStep 2: Identify the testable implication of liquidity constraints:\nIf households face binding liquidity constraints, lagged income (Y_{t-1}) should help predict consumption growth because constrained households cannot fully smooth consumption. This violates the PIH orthogonality condition. The modified Euler equation becomes:\nΔlnC_t = α + β Y_{t-1} + other controls + error\nwhere β ≠ 0 indicates excess sensitivity due to constraints.\n\nStep 3: Zeldes' (1989a) empirical test:\n- Split the sample into low-wealth (presumably constrained) and high-wealth (unconstrained) groups\n- Estimate separate Euler equations for both groups\n- Test whether β (coefficient on lagged income) is significant in the low-wealth group but insignificant in the high-wealth group\n\nStep 4: Zeldes' findings:\n- Found statistically significant β coefficient for low-wealth households\n- Insignificant β for high-wealth households\n- Interpreted as evidence that ~20% of households face binding liquidity constraints\n\nKey innovation: Sample splitting based on wealth provides a natural test of heterogeneous liquidity constraints, avoiding the need for explicit constraint indicators.\n\nQID: textbook-39-0-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-39-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate accurately derives the testable implication of liquidity constraints in the Euler equation and correctly describes Zeldes' empirical methodology and findings, matching the gold answer in substance despite slight differences in notation and presentation.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-39-0-1-0", "category": "CORRECT", "explanation": "The candidate accurately derives the testable implication of liquidity constraints in the Euler equation and correctly describes Zeldes' empirical methodology and findings, matching the gold answer in substance despite slight differences in notation and presentation."}, "llm_echoed_qid": "textbook-39-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate accurately derives the testable implication of liquidity constraints in the Euler equation and correctly describes Zeldes' empirical methodology and findings, matching the gold answer in substance despite slight differences in notation and presentation."}
{"script_processing_qid": "textbook-101-5-3-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove Lemma 8.3.5, extending Lemma 8.3.4 to independent Poisson r.v.'s.\n\nGOLD_ANSWER:\n1. **Markov Inequality**: $P\\{s^{-1/2}\\sum_{i=1}^{k}|\\tau(s_{i})-s_{i}|\\geq\\varepsilon\\}\\leq e^{-t s^{1/2}\\varepsilon}2^{k}\\prod_{i=1}^{k}E e^{t|\\tau(s_{i})-s_{i}|}$.\\n2. **MGF Calculation**: $E e^{t|\\tau(s_{i})-s_{i}|} \\leq 2\\exp(s_{i}(e^{t}-1-t))$.\\n3. **Bound**: The product of MGFs is bounded by $2^{k}\\exp(s(e^{t}-1-t))$.\\n4. **Optimize**: Choose $t = \\log(1 + \\varepsilon s^{-1/2})$ to minimize the bound.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that for a Poisson r.v. τ(λ) with parameter λ, the centered variable X = τ(λ) - λ has moment generating function (MGF) E[e^{tX}] = e^{λ(e^t - 1 - t)}. For absolute deviations |X|, use the inequality E[e^{t|X|}] ≤ E[e^{tX} + e^{-tX}] = e^{λ(e^t - 1 - t)} + e^{λ(e^{-t} - 1 + t)}.\n\nStep 2: Bound each term using the inequality e^x ≤ 1 + x + x²/(2(1 - |x|/3)) for small x. For t > 0, approximate e^t - 1 - t ≤ t²/(2(1 - t/3)) and e^{-t} - 1 + t ≤ t²/(2(1 - t/3)). This gives E[e^{t|X|}] ≤ 2e^{λ t²/(2(1 - t/3))}.\n\nStep 3: For independent τ(s_i), the summed deviation Y = ∑|τ(s_i) - s_i| has MGF bounded by ∏_{i=1}^k [2e^{s_i t²/(2(1 - t/3))}] = 2^k e^{s t²/(2(1 - t/3))} where s = ∑s_i.\n\nStep 4: Apply Markov’s inequality: P(Y ≥ εs^{1/2}) ≤ e^{-tεs^{1/2}} ⋅ 2^k e^{s t²/(2(1 - t/3))} = 2^k e^{s t²/(2(1 - t/3)) - tεs^{1/2}}.\n\nStep 5: Optimize the exponent by setting t = ε/(s^{1/2}(2 + ε/s^{1/2})). Substitute and simplify to obtain the final bound: 2^k exp(-ε²/(2 + ε s^{-1/2})).\n\nQID: textbook-101-5-3-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-101-5-3-0\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer provides a correct and detailed proof for extending Lemma 8.3.4 to independent Poisson r.v.'s, but the reasoning and steps differ from the gold answer. The candidate uses a different approach with MGF bounds and optimization, leading to a valid but alternative proof.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-101-5-3-0", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer provides a correct and detailed proof for extending Lemma 8.3.4 to independent Poisson r.v.'s, but the reasoning and steps differ from the gold answer. The candidate uses a different approach with MGF bounds and optimization, leading to a valid but alternative proof."}, "llm_echoed_qid": "textbook-101-5-3-0", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer provides a correct and detailed proof for extending Lemma 8.3.4 to independent Poisson r.v.'s, but the reasoning and steps differ from the gold answer. The candidate uses a different approach with MGF bounds and optimization, leading to a valid but alternative proof."}
{"script_processing_qid": "textbook-105-1-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the assumptions (i)-(iv) required for Theorem 26.2. Why are they necessary for the consistency of the regression estimate?\n\nGOLD_ANSWER:\n1. **Independence**: $(X, Y)$ and $C$ are independent, ensuring censoring is non-informative.\n2. **No Common Jumps**: $F$ and $G$ have no common jumps, ensuring the Kaplan-Meier estimator is consistent.\n3. **Finite $T_{F}$**: $T_{F} < \\infty$ guarantees boundedness of $Y$.\n4. **Continuity of $G$**: $G$ is continuous at $T_{K}$ and $G(T_{F}) > 0$, ensuring the weights are well-defined and non-zero.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key components in the context\n- The regression estimator uses Kaplan-Meier-adjusted censored data\n- Theorem 26.2 requires assumptions (i)-(iv) for L₂ consistency\n- Lemma 26.1 handles convergence of Gₙ to G\n\nStep 2: Infer likely assumptions from censored regression literature\n(i) Random censorship: Censoring time C is independent of Y given X\n    Necessary to ensure the validity of the Kaplan-Meier correction\n(ii) Uniform consistency of Gₙ(z): sup_z|Gₙ(z)-G(z)|→0 a.s.\n    Required for Lemma 26.1 to hold and eliminate bias in inverse weighting\n(iii) Smoothness of m(x): Lipschitz/Hölder continuous regression function\n    Needed for local averaging methods like partitioning estimates to approximate m(x)\n(iv) Design condition: Density of X bounded away from 0 on support\n    Ensures sufficient observations in each partition cell as n→∞\n\nStep 3: Explain necessity of each assumption\n- (i) Maintains E[δZ/G(Z)|X] = E[Y|X] (unbiased inverse weighting)\n- (ii) Controls error from estimating G (via Lemma 26.1)\n- (iii) Makes local averaging effective (bias vanishes with partition refinement)\n- (iv) Prevents empty/localized cells (variance reduction for consistency)\n\nQID: textbook-105-1-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-105-1-1-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer does not match the gold answer. The assumptions listed by the candidate (random censorship, uniform consistency of Gₙ, smoothness of m(x), and design condition) are not the same as those in the gold answer (independence, no common jumps, finite T_F, and continuity of G). The candidate's reasoning, while plausible for a different context, does not address the specific assumptions required for Theorem 26.2 as stated in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-105-1-1-2", "category": "INCORRECT", "explanation": "The candidate's answer does not match the gold answer. The assumptions listed by the candidate (random censorship, uniform consistency of Gₙ, smoothness of m(x), and design condition) are not the same as those in the gold answer (independence, no common jumps, finite T_F, and continuity of G). The candidate's reasoning, while plausible for a different context, does not address the specific assumptions required for Theorem 26.2 as stated in the gold answer."}, "llm_echoed_qid": "textbook-105-1-1-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer does not match the gold answer. The assumptions listed by the candidate (random censorship, uniform consistency of Gₙ, smoothness of m(x), and design condition) are not the same as those in the gold answer (independence, no common jumps, finite T_F, and continuity of G). The candidate's reasoning, while plausible for a different context, does not address the specific assumptions required for Theorem 26.2 as stated in the gold answer."}
{"script_processing_qid": "textbook-109-9-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Formally model the tax arbitrage opportunity for a shareholder with marginal tax rate $\\tau$ under: (i) cash dividend $D$, (ii) scrip dividend with share value $V$. Assume capital gains tax rate is $\\tau_g$.\n\nGOLD_ANSWER:\nAfter-tax wealth comparison:\n\n**Cash Dividend**:\n$$ W_{cash} = D(1-\\tau) $$\n\n**Scrip Dividend**:\n$$ W_{scrip} = V - \\max[0, (V - D)\\tau_g] $$\n\nArbitrage condition: $W_{scrip} > W_{cash}$ when $\\tau_g < \\tau$ and $V \\approx D$.\n\nCANDIDATE_ANSWER:\nStep 1: Define after-tax value for cash dividend\nFor cash dividend D, shareholder receives after-tax amount:\nAfter-tax cash = D × (1 - τ)\n\nStep 2: Define after-tax value for scrip dividend\nFor scrip dividend with share value V:\n1. Shareholder receives shares worth V\n2. When sold, capital gains tax applies to full V (assuming original shares had zero cost basis)\nAfter-tax scrip value = V × (1 - τ_g)\n\nStep 3: Compare arbitrage condition\nThe tax arbitrage opportunity exists when:\nV × (1 - τ_g) > D × (1 - τ)\n\nThis implies shareholders prefer scrip dividends when:\n(1) Capital gains tax rate τ_g < income tax rate τ, or\n(2) Enhanced scrip value V > D × (1 - τ)/(1 - τ_g)\n\nQID: textbook-109-9-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-109-9-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly models the after-tax wealth for both cash and scrip dividends, accurately compares the arbitrage condition, and identifies the conditions under which shareholders would prefer scrip dividends. The reasoning and calculations align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-109-9-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly models the after-tax wealth for both cash and scrip dividends, accurately compares the arbitrage condition, and identifies the conditions under which shareholders would prefer scrip dividends. The reasoning and calculations align with the gold answer."}, "llm_echoed_qid": "textbook-109-9-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly models the after-tax wealth for both cash and scrip dividends, accurately compares the arbitrage condition, and identifies the conditions under which shareholders would prefer scrip dividends. The reasoning and calculations align with the gold answer."}
{"script_processing_qid": "textbook-39-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the MA(1) structure of residuals in synthetic cohort data when estimating the Euler equation for consumption, starting from the sampling error in averages of variables.\n\nGOLD_ANSWER:\n1. **Sampling Error**: Let $\\bar{X}_t$ be the sample average of variable $X$ for a cohort at time $t$, with true mean $\\mu_t$ and sampling error $\\epsilon_t = \\bar{X}_t - \\mu_t$.\n2. **First Differences**: The Euler equation often uses first differences $\\Delta \\bar{X}_t = \\bar{X}_t - \\bar{X}_{t-1} = (\\mu_t - \\mu_{t-1}) + (\\epsilon_t - \\epsilon_{t-1})$.\n3. **MA(1) Structure**: If $\\epsilon_t$ is i.i.d., then $\\text{Cov}(\\epsilon_t - \\epsilon_{t-1}, \\epsilon_{t-1} - \\epsilon_{t-2}) = -\\text{Var}(\\epsilon_t)$, inducing an MA(1) process in residuals.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that synthetic cohort data involves averaging variables over individuals in each cohort-period cell. Let observed cohort average of variable 𝑦 be: \n\nȳ_{ct} = 𝑦̄_{ct} + 𝜂_{ct}\n\nwhere 𝑦̄_{ct} is the true population mean and 𝜂_{ct} ~ (0,σ²/N_{ct}) is the sampling error from finite cohort size N_{ct}.\n\nStep 2: The Euler equation estimation typically uses first differences Δȳ_{ct} = ȳ_{ct} - ȳ_{ct-1}. Substitute the measurement equation:\n\nΔȳ_{ct} = Δ𝑦̄_{ct} + (𝜂_{ct} - 𝜂_{ct-1})\n\nStep 3: The structural error term in the Euler equation (Δ𝑦̄_{ct}) is generally white noise, but the composite residual becomes:\n\n𝜀_{ct} = 𝑢_{ct} + (𝜂_{ct} - 𝜂_{ct-1})\n\nwhere 𝑢_{ct} is the structural error and (𝜂_{ct} - 𝜂_{ct-1}) is the sampling error difference.\n\nStep 4: Calculate autocovariance structure:\nCov(𝜀_{ct}, 𝜀_{ct-1}) = Cov(𝜂_{ct} - 𝜂_{ct-1}, 𝜂_{ct-1} - 𝜂_{ct-2}) = -Var(𝜂_{ct-1})\n\nHigher order autocovariances = 0. This MA(1) structure arises because consecutive sampling errors (𝜂_{ct} and 𝜂_{ct-1}) are independent but appear in overlapping differences.\n\nQID: textbook-39-0-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-39-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the MA(1) structure of residuals in synthetic cohort data, following the same logical steps as the gold answer, including the recognition of sampling error, first differences, and the resulting MA(1) process.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-39-0-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the MA(1) structure of residuals in synthetic cohort data, following the same logical steps as the gold answer, including the recognition of sampling error, first differences, and the resulting MA(1) process."}, "llm_echoed_qid": "textbook-39-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the MA(1) structure of residuals in synthetic cohort data, following the same logical steps as the gold answer, including the recognition of sampling error, first differences, and the resulting MA(1) process."}
