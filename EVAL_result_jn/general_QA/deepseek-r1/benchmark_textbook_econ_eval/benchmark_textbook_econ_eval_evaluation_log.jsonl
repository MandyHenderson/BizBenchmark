{"script_processing_qid": "textbook-119-14-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Define perfect multicollinearity and provide an example using a hypothetical dataset.\n\nGOLD_ANSWER:\n1. **Definition**: Perfect multicollinearity occurs when there is an exact linear relationship among the explanatory variables in a regression model, i.e., $\\lambda_1 X_1 + \\lambda_2 X_2 + \\dots + \\lambda_k X_k = 0$ for some constants $\\lambda_i$ not all zero.\n2. **Example**: Consider $X_2 = [10, 15, 18, 24, 30]$ and $X_3 = [50, 75, 90, 120, 150]$. Here, $X_3 = 5 X_2$, leading to perfect multicollinearity.\n\nCANDIDATE_ANSWER:\nStep 1: Define perfect multicollinearity using the formal condition from the context. Perfect multicollinearity occurs when there exists an exact linear relationship among explanatory variables in a regression model, i.e., λ₁X₁ + λ₂X₂ + ... + λₖXₖ = 0, where λ₁, λ₂,...,λₖ are constants (not all zero) and X₁ is the intercept term (a column of 1s).\n\nStep 2: Provide a hypothetical dataset example. Consider a regression model with:\n- X₁ = 1 (intercept)\n- X₂ = Years of education\n- X₃ = 2 × X₂ + 5 (e.g., a variable artificially defined as twice education years plus 5).\n\nStep 3: Demonstrate the perfect linear relationship. Here, λ₁ = -5, λ₂ = -2, and λ₃ = 1 satisfy -5X₁ - 2X₂ + X₃ = 0 for all observations. Including X₁, X₂, and X₃ in the same regression would create perfect multicollinearity because X₃ is an exact linear combination of X₁ and X₂.\n\nQID: textbook-119-14-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-14-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately defines perfect multicollinearity and provides a valid hypothetical example demonstrating the concept, matching the gold answer's requirements.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-14-1-0", "category": "CORRECT", "explanation": "The candidate's answer accurately defines perfect multicollinearity and provides a valid hypothetical example demonstrating the concept, matching the gold answer's requirements."}, "llm_echoed_qid": "textbook-119-14-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately defines perfect multicollinearity and provides a valid hypothetical example demonstrating the concept, matching the gold answer's requirements."}
{"script_processing_qid": "textbook-84-2-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Given matrices $\\mathbf{A}$ and $\\mathbf{B}$ as defined in the text, verify the product $\\mathbf{A}\\mathbf{B}$ by performing the matrix multiplication step-by-step.\n\nGOLD_ANSWER:\n1. **Matrix Multiplication**:\n   - $\\mathbf{A}$ is $2\\times3$ and $\\mathbf{B}$ is $3\\times3$.\n   - The element in the first row and first column of $\\mathbf{A}\\mathbf{B}$ is computed as: $2\\cdot0 + (-1)\\cdot1 + 7\\cdot3 = 0 -1 + 21 = 20$.\n   - Similarly, compute all other elements to confirm the result: $\\left(\\begin{array}{l l l}{20}&{-6}&{12}\\\\{5}&{36}&{-24}\\end{array}\\right)$.\n\nCANDIDATE_ANSWER:\nStep 1: Confirm matrix dimensions - A is 2×3 and B is 3×3, so AB is valid and will be 2×3\nStep 2: Compute element (1,1): (2)(0) + (-1)(1) + (7)(3) = 0 -1 +21 = 20\nStep 3: Compute element (1,2): (2)(1) + (-1)(8) + (7)(0) = 2 -8 +0 = -6\nStep 4: Compute element (1,3): (2)(6) + (-1)(0) + (7)(0) = 12 +0 +0 = 12\nStep 5: Compute element (2,1): (-4)(0) + (5)(1) + (0)(3) = 0 +5 +0 =5\nStep 6: Compute element (2,2): (-4)(1) + (5)(8) + (0)(0) = -4 +40 +0 =36\nStep 7: Compute element (2,3): (-4)(6) + (5)(0) + (0)(0) = -24 +0 +0 =-24\nFinal result matches given matrix: ⎛20  -6   12⎞\n                                      ⎝5   36  -24⎠\n\nQID: textbook-84-2-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-84-2-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the step-by-step matrix multiplication process and arrives at the same result as the gold answer, including all intermediate calculations and the final matrix.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-84-2-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the step-by-step matrix multiplication process and arrives at the same result as the gold answer, including all intermediate calculations and the final matrix."}, "llm_echoed_qid": "textbook-84-2-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the step-by-step matrix multiplication process and arrives at the same result as the gold answer, including all intermediate calculations and the final matrix."}
{"script_processing_qid": "textbook-120-24-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain why the sequence $x_n$ generated by Newton’s bisection method is Cauchy.\n\nGOLD_ANSWER:\n1. **Interval Length**: $r_n = |a_n - b_n| = \\frac{1}{2^{n-1}} r_1$.\n2. **Convergence**: Since $\\frac{1}{2^{n-1}} \\to 0$, $r_n \\to 0$.\n3. **Cauchy Criterion**: For any $\\epsilon > 0$, there exists $M$ such that for all $n, m \\geq M$, $d(x_n, x_m) \\leq r_M < \\epsilon$.\n\nCANDIDATE_ANSWER:\nStep 1: Observe that in Newton’s bisection method, the interval length at step n is |I_n| = (b_n - a_n) = (b_0 - a_0)/2^n. This length converges to 0 as n → ∞.\nStep 2: For any n, m ≥ N (assuming m ≥ n without loss of generality), both x_n and x_m lie within the interval I_N = [a_N, b_N]. The distance between x_n and x_m satisfies |x_n - x_m| ≤ |I_N| = (b_0 - a_0)/2^N.\nStep 3: Given any ε > 0, choose N such that (b_0 - a_0)/2^N < ε. Then, for all n, m ≥ N, |x_n - x_m| < ε by the inequality in Step 2.\nStep 4: This satisfies the definition of a Cauchy sequence: For every ε > 0, there exists N such that |x_n - x_m| < ε for all n, m ≥ N. Thus, the sequence x_n is Cauchy.\n\nQID: textbook-120-24-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-120-24-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the reasoning of the gold answer, detailing the interval length convergence and applying the Cauchy criterion accurately to demonstrate the sequence is Cauchy.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-120-24-1-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the reasoning of the gold answer, detailing the interval length convergence and applying the Cauchy criterion accurately to demonstrate the sequence is Cauchy."}, "llm_echoed_qid": "textbook-120-24-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the reasoning of the gold answer, detailing the interval length convergence and applying the Cauchy criterion accurately to demonstrate the sequence is Cauchy."}
{"script_processing_qid": "textbook-106-5-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the data from Big Bob’s Bagel Bin (Table 13-3), calculate the marginal cost (MC) for producing the 7th bagel and explain how it relates to the average total cost (ATC) at that quantity.\n\nGOLD_ANSWER:\n1.  **Calculation**: From Table 13-3, the total cost (TC) for 6 bagels is $5.80, and for 7 bagels, it is $6.60. Thus, the marginal cost for the 7th bagel is:\n   $$ MC = TC_7 - TC_6 = 6.60 - 5.80 = 0.80 $$\n2.  **Relationship to ATC**: At 7 bagels, the ATC is $0.95. Since MC ($0.80) is less than ATC ($0.95), the ATC is falling at this point. This aligns with the principle that when MC < ATC, ATC decreases.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the requirement - Calculate marginal cost (MC) for the 7th bagel using Table 13-3 data.\nStep 2: Observe that the provided context only references Thirsty Thelma’s Lemonade Stand (Table 13-2) and general cost curve relationships. No data from Big Bob’s Bagel Bin (Table 13-3) is included in the context.\nStep 3: Conclude that insufficient data exists to calculate MC for the 7th bagel. The provided context only explains theoretical relationships (e.g., MC intersects ATC at its minimum point) but lacks numerical data for bagel production costs.\nStep 4: General relationship explanation - If MC < ATC at a given quantity, ATC decreases. If MC > ATC, ATC increases. At Q=7, the comparison between MC and ATC would determine whether ATC is rising/falling, but this cannot be quantified without Table 13-3 data.\n\nQID: textbook-106-5-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-5-0-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the lack of necessary data from Table 13-3.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-5-0-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the lack of necessary data from Table 13-3."}, "llm_echoed_qid": "textbook-106-5-0-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the lack of necessary data from Table 13-3."}
{"script_processing_qid": "textbook-72-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the role of the worker's share of match surplus ($\\beta$) in determining equilibrium unemployment. How does it compare to the Hosios condition?\n\nGOLD_ANSWER:\nAn increase in $\\beta$ shifts $C C$ downward and $D D$ upward, reducing $\\theta$ but ambiguously affecting $R$. The effect on $R$ depends on the sign of $\\beta - \\eta$, where $\\eta$ is the matching function elasticity. If $\\beta = \\eta$ (Hosios condition), search externalities are internalized, serving as a benchmark for efficient outcomes.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain specific information about the worker's share of match surplus (β) or its role in determining equilibrium unemployment. The discussion focuses on parameters like labor supply price (b) and general productivity (p) affecting the job destruction line (DD) and equilibrium unemployment. Similarly, the Hosios condition (which relates bargaining power to matching function elasticity) is not mentioned in the given context. Therefore, the necessary details to address this question are absent from the provided material.\n\nQID: textbook-72-0-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-72-0-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer states the necessary details are absent.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-72-0-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer states the necessary details are absent."}, "llm_echoed_qid": "textbook-72-0-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer states the necessary details are absent."}
{"script_processing_qid": "textbook-106-1-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Given the demand and supply schedules for Frisbees, calculate the equilibrium price and quantity. Then, determine the new market price and quantity sold if a price floor $2 above the equilibrium price is imposed.\n\nGOLD_ANSWER:\n1. **Equilibrium Price and Quantity**: From the table, equilibrium occurs at $8 (quantity demanded = quantity supplied = 6 million).\n2. **Price Floor**: $8 + $2 = $10.\n3. **New Quantity Sold**: At $10, quantity demanded is 2 million, and quantity supplied is 12 million, leading to a surplus of 10 million Frisbees.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain enough information to answer this question. While the context mentions demand and supply schedules for Frisbees and references price floors, it does not include the actual numerical data from the schedules required to calculate equilibrium price/quantity or analyze a price floor's effects. To solve this problem, we would need specific price-quantity pairs showing how many units are demanded and supplied at various price levels.\n\nQID: textbook-106-1-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-106-1-0-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it references a table that is not provided.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-1-0-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it references a table that is not provided."}, "llm_echoed_qid": "textbook-106-1-0-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it references a table that is not provided."}
{"script_processing_qid": "textbook-119-20-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the simultaneity bias in the demand-supply model using a graph.\n\nGOLD_ANSWER:\n**Graphical Explanation**: 1. **Demand Shock**: A positive $u_{1t}$ shifts demand curve up, increasing $P_t$ and $Q_t$. 2. **Supply Shock**: A positive $u_{2t}$ shifts supply curve up, increasing $P_t$ but decreasing $Q_t$. 3. **Bias**: OLS conflates these effects, leading to biased estimates of $\\alpha_1$ and $\\beta_1$.\n\nCANDIDATE_ANSWER:\nStep 1: Visualize the demand-supply framework. Plot price (P) on the vertical axis and quantity (Q) on the horizontal axis. Draw a downward-sloping demand curve (Q^d = α₀ + α₁P + u₁) and upward-sloping supply curve (Q^s = β₀ + β₁P + u₂).\n\nStep 2: Identify equilibrium. The intersection of the two curves determines equilibrium price (P*) and quantity (Q*).\n\nStep 3: Introduce simultaneity bias. When demand shocks (u₁) occur, they shift the demand curve, altering both P and Q. Similarly, supply shocks (u₂) shift the supply curve. This creates a feedback loop where:\n- Higher u₁ increases Q^d at all prices → demand shifts right → new equilibrium has higher P and Q\n- Higher u₂ increases Q^s at all prices → supply shifts right → new equilibrium has lower P and higher Q\n\nStep 4: Demonstrate OLS failure. Data points observed in practice are equilibrium combinations (P,Q) from simultaneous shifts of both curves. The OLS regression line through these points will neither trace the true demand nor supply curve, as P becomes correlated with both u₁ and u₂:\n- When u₁ increases → P increases (positive correlation)\n- When u₂ increases → P decreases (negative correlation)\n\nStep 5: Show bias direction. For the demand equation, OLS estimates of α₁ become biased because P is positively correlated with u₁ (demand shocks directly increase both P and Q). This flattens the estimated demand curve compared to the true relationship.\n\nQID: textbook-119-20-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-20-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains simultaneity bias in the demand-supply model, aligning with the gold answer by detailing demand and supply shocks, equilibrium shifts, and OLS bias. The graphical and conceptual steps are accurately described.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-20-1-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains simultaneity bias in the demand-supply model, aligning with the gold answer by detailing demand and supply shocks, equilibrium shifts, and OLS bias. The graphical and conceptual steps are accurately described."}, "llm_echoed_qid": "textbook-119-20-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains simultaneity bias in the demand-supply model, aligning with the gold answer by detailing demand and supply shocks, equilibrium shifts, and OLS bias. The graphical and conceptual steps are accurately described."}
{"script_processing_qid": "textbook-114-14-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What are the limitations of using warranties as signals of quality? Discuss potential market failures.\n\nGOLD_ANSWER:\n1. **Costly Signaling**: If the cost difference between high and low-quality producers is not significant, warranties may not effectively signal quality.\n2. **Consumer Misinterpretation**: Consumers may not always correctly interpret warranties as signals of quality.\n3. **Market Failure**: In cases where warranties are too costly for even high-quality producers, the market may fail to signal quality, leading to adverse selection.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key limitations of warranties as quality signals from the context. The text notes warranties are costly for low-quality producers due to higher servicing costs, but this mechanism assumes rational behavior and clear cost differentiation. Limitations arise when these assumptions fail.\n\nStep 2: Limitation 1 – Cost mimicry: If low-quality producers can imitate warranty terms without incurring proportionally higher costs (e.g., through deceptive practices like planned obsolescence or outsourcing repairs cheaply), the signaling mechanism breaks down.\n\nStep 3: Limitation 2 – Moral hazard: Warranties may encourage excessive consumer risk-taking (e.g., careless product use), increasing claim rates. This raises costs for high-quality producers, potentially forcing them to reduce warranty scope, weakening the signal.\n\nStep 4: Limitation 3 – Adverse selection: If consumers cannot distinguish credible warranties, low-quality producers might exploit this by offering superficial warranties. This creates a 'lemons market' where high-quality producers withdraw warranties to avoid being pooled with low-quality ones, reducing market efficiency.\n\nStep 5: Market failure 1 – Signaling collapse: If warranties lose credibility due to the above limitations, asymmetric information re-emerges, leading to underproduction of high-quality goods and overpricing of low-quality goods.\n\nStep 6: Market failure 2 – Externalities: Misleading warranties impose costs on honest firms (via reputation spillovers) and consumers (via repair delays or hidden fees), creating negative externalities that regulators may need to address.\n\nQID: textbook-114-14-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-14-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses the limitations of warranties as signals of quality and discusses potential market failures, aligning well with the gold answer while providing additional detailed reasoning and examples.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-14-0-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses the limitations of warranties as signals of quality and discusses potential market failures, aligning well with the gold answer while providing additional detailed reasoning and examples."}, "llm_echoed_qid": "textbook-114-14-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses the limitations of warranties as signals of quality and discusses potential market failures, aligning well with the gold answer while providing additional detailed reasoning and examples."}
{"script_processing_qid": "textbook-114-2-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate the 95% confidence interval for the forecasted auto sales of $188.7 billion, given a standard error of forecast (SEF) of $8.2 billion.\n\nGOLD_ANSWER:\n1.  **Critical Value**: 1.96 for a 95% confidence interval.\n2.  **Margin of Error**:\n$$1.96 \\times 8.2 = 16.072$$\n3.  **Confidence Interval**:\n$$188.7 \\pm 16.072 = [172.628, 204.772]$$\n4.  **Interpretation**: We are 95% confident that the true sales value lies between $172.6 billion and $204.8 billion.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the critical value for a 95% confidence interval. Since regression forecasts typically use the normal distribution for large samples, we use the z-score of 1.96.\nStep 2: Apply the confidence interval formula: Forecast ± (Critical Value × SEF)\nStep 3: Compute the margin of error: 1.96 × $8.2 billion = $16.072 billion\nStep 4: Calculate the interval bounds:\nLower bound = $188.7 billion - $16.072 billion = $172.628 billion\nUpper bound = $188.7 billion + $16.072 billion = $204.772 billion\nFinal 95% confidence interval: [$172.6 billion, $204.8 billion] (rounded to one decimal place)\n\nQID: textbook-114-2-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-2-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to calculate the 95% confidence interval, matches the gold answer's calculations, and provides the same final interval bounds.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-2-2-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to calculate the 95% confidence interval, matches the gold answer's calculations, and provides the same final interval bounds."}, "llm_echoed_qid": "textbook-114-2-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to calculate the 95% confidence interval, matches the gold answer's calculations, and provides the same final interval bounds."}
{"script_processing_qid": "textbook-123-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of duality for computational implementation, as mentioned in the text.\n\nGOLD_ANSWER:\n1. **Computational Efficiency**: The duality allows the same set of computer programs to solve both Kalman filtering and optimal linear regulator problems by simply reinterpreting the input matrices.\n2. **Practical Benefit**: This symmetry reduces the need for separate implementations, saving computational resources and ensuring consistency in solutions.\n\nCANDIDATE_ANSWER:\nStep 1: The duality between the Kalman filter and optimal linear regulator problems establishes a mathematical equivalence under specific matrix reinterpretations and time-index transformations (Theorem B.6.1).\nStep 2: Computational implementations can leverage this symmetry by reusing the same algorithms for both problems. For example, an existing solver for the optimal linear regulator can be applied to the Kalman filter problem by transposing system matrices (e.g., using $A_{t_{1}-1-t}^\\prime$ instead of $A_t$) and reversing time indices.\nStep 3: This reduces code redundancy, as separate implementations for filtering and control are unnecessary. Developers can maintain a single codebase for solving Riccati equations in both contexts.\nStep 4: Numerical methods optimized for solving regulator problems (e.g., matrix inversion/iteration techniques) directly benefit Kalman filter implementations, improving computational efficiency and reliability through shared optimizations.\n\nQID: textbook-123-1-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-1-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately elaborates on the implications of duality for computational implementation, matching the gold answer's points about computational efficiency and practical benefits while providing additional technical details.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-1-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately elaborates on the implications of duality for computational implementation, matching the gold answer's points about computational efficiency and practical benefits while providing additional technical details."}, "llm_echoed_qid": "textbook-123-1-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately elaborates on the implications of duality for computational implementation, matching the gold answer's points about computational efficiency and practical benefits while providing additional technical details."}
{"script_processing_qid": "textbook-36-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Graphically illustrate the effect of a lump-sum tax on the budget constraint of a consumer. Derive the new budget line equation.\n\nGOLD_ANSWER:\n1. **Original Budget Line**: $p_1x_1 + p_2x_2 = m$.\n2. **Lump-Sum Tax**: Reduces income to $m - T$.\n3. **New Budget Line**: $p_1x_1 + p_2x_2 = m - T$.\n4. **Graph**: Parallel inward shift of the budget line, with intercepts $(m - T)/p_1$ and $(m - T)/p_2$.\n\nCANDIDATE_ANSWER:\nThe provided context discusses market equilibrium, comparative statics, and Pareto efficiency but does not contain any information about budget constraints, lump-sum taxes, or their graphical effects. Therefore, there is insufficient information in the given context to derive the new budget line equation or illustrate the effect of a lump-sum tax on a consumer's budget constraint.\n\nQID: textbook-36-0-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-36-0-0-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the lack of relevant information.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-36-0-0-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the lack of relevant information."}, "llm_echoed_qid": "textbook-36-0-0-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the lack of relevant information."}
{"script_processing_qid": "textbook-3-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that if a preference relation > is transitive and complete, then the induced indifference relation ~ is symmetric and transitive.\n\nGOLD_ANSWER:\n1. **Symmetry**: Suppose x ~ y. By definition, x > y and y > x. Hence y > x and x > y, so y ~ x. Thus ~ is symmetric.\n2. **Transitivity**: Suppose x ~ y and y ~ z. Then x > y, y > x, y > z, and z > y. By transitivity of >, x > z and z > x. Hence x ~ z. Thus ~ is transitive.\n\nCANDIDATE_ANSWER:\nStep 1: Recall definitions:\n- Completeness: For all x, y ∈ X, either x ≳ y or y ≳ x.\n- Transitivity: For all x, y, z ∈ X, if x ≳ y and y ≳ z, then x ≳ z.\n- Indifference (~): x ~ y ⇨ x ≳ y and y ≳ x.\n\nStep 2: Prove symmetry of ~:\n- Suppose x ~ y. By definition, this means x ≳ y ∧ y ≳ x.\n- This immediately implies y ≳ x ∧ x ≳ y, which is y ~ x.\n\nStep 3: Prove transitivity of ~:\n- Assume x ~ y and y ~ z.\n- From x ~ y: x ≳ y (1) and y ≳ x (2).\n- From y ~ z: y ≳ z (3) and z ≳ y (4).\n- Apply transitivity to (1) & (3): x ≳ z (5).\n- Apply transitivity to (4) & (2): z ≳ x (6).\n- From (5) & (6): x ≳ z ∧ z ≳ x ⇨ x ~ z.\n\nConclusion: Therefore, ~ inherits symmetry from its definition and transitivity via the transitivity of ≳.\n\nQID: textbook-3-0-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-3-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly proves both symmetry and transitivity of the indifference relation ~ using the definitions and properties provided. The reasoning aligns perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-3-0-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly proves both symmetry and transitivity of the indifference relation ~ using the definitions and properties provided. The reasoning aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-3-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly proves both symmetry and transitivity of the indifference relation ~ using the definitions and properties provided. The reasoning aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-83-1-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the recurrence relation $T_{m+1}(x) = 2x T_m(x) - T_{m-1}(x)$ from the trigonometric definition of Chebyshev polynomials.\n\nGOLD_ANSWER:\n1. **Trigonometric Identity**: Use $\\cos((m+1)\\theta) + \\cos((m-1)\\theta) = 2\\cos\\theta \\cos(m\\theta)$.\\n2. **Substitute**: Let $\\theta = \\arccos x$, so $T_{m+1}(x) + T_{m-1}(x) = 2x T_m(x)$.\\n3. **Rearrange**: Obtain $T_{m+1}(x) = 2x T_m(x) - T_{m-1}(x)$.\n\nCANDIDATE_ANSWER:\nStep 1: Let θ = arccos x ⇒ x = cos θ.\nStep 2: Using trigonometric definitions:\nT_{m+1}(x) = cos((m+1)θ)\nT_m(x) = cos(mθ)\nT_{m-1}(x) = cos((m-1)θ)\nStep 3: Apply cosine addition formula:\ncos((m+1)θ) = cos(mθ + θ) = cos(mθ)cosθ - sin(mθ)sinθ\nStep 4: Similarly for subtraction:\ncos((m-1)θ) = cos(mθ - θ) = cos(mθ)cosθ + sin(mθ)sinθ\nStep 5: Add equations from Steps 3 and 4:\ncos((m+1)θ) + cos((m-1)θ) = 2cos(mθ)cosθ\nStep 6: Rearrange to isolate T_{m+1}(x):\ncos((m+1)θ) = 2cos(mθ)cosθ - cos((m-1)θ)\nStep 7: Substitute back using x = cosθ and Chebyshev definitions:\nT_{m+1}(x) = 2xT_m(x) - T_{m-1}(x)\n\nQID: textbook-83-1-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-83-1-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the recurrence relation for Chebyshev polynomials, matching the gold answer's approach and conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-83-1-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the recurrence relation for Chebyshev polynomials, matching the gold answer's approach and conclusion."}, "llm_echoed_qid": "textbook-83-1-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the recurrence relation for Chebyshev polynomials, matching the gold answer's approach and conclusion."}
{"script_processing_qid": "textbook-112-32-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare and contrast the educational outcomes for the common people versus those of rank and fortune in a civilized society, according to Smith.\n\nGOLD_ANSWER:\n1. **Common People**: They have little time or resources for education, and their trades are too simple to exercise their minds. Their labor is constant and severe, leaving no leisure for intellectual pursuits.\n2. **People of Rank and Fortune**: They have ample time and resources to acquire a broad education before entering their professions. Their employments are complex and exercise their minds, preventing intellectual torpor.\n3. **Outcome**: The common people remain intellectually stagnant, while the elite continue to develop their capabilities, perpetuating social inequality.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Smith’s analysis of common people’s education. The division of labor confines them to repetitive, simple tasks, leaving no occasion to develop intellectual faculties like understanding or invention. Their labor conditions inherently limit educational outcomes. Step 2: Examine outcomes for those of rank and fortune. The text highlights unrestrained competition in education (likely accessible to the wealthy) as producing high-quality instruction. However, institutional endowments corrupt public teachers and hinder private education quality. Step 3: Contrast the two groups. Common people face systemic intellectual stagnation due to labor constraints, while the privileged class benefits from competitive educational markets (despite institutional flaws). Step 4: Conclude that Smith implies a stark disparity: the wealthy retain access to refined education fostering mental growth, whereas laborers suffer diminished intellectual development due to their socio-economic roles.\n\nQID: textbook-112-32-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-32-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies and contrasts the educational outcomes for common people and those of rank and fortune as described by Smith. It correctly notes the intellectual stagnation of common people due to labor constraints and the access to refined education for the wealthy, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-32-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately identifies and contrasts the educational outcomes for common people and those of rank and fortune as described by Smith. It correctly notes the intellectual stagnation of common people due to labor constraints and the access to refined education for the wealthy, aligning with the gold answer."}, "llm_echoed_qid": "textbook-112-32-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies and contrasts the educational outcomes for common people and those of rank and fortune as described by Smith. It correctly notes the intellectual stagnation of common people due to labor constraints and the access to refined education for the wealthy, aligning with the gold answer."}
{"script_processing_qid": "textbook-94-6-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. How do average and marginal cost curves behave in the short run and long run?\n\nGOLD_ANSWER:\n4.  **Short-Run Curves**:\n- **Average Cost (SAC)**: U-shaped due to decreasing average fixed costs and eventually increasing average variable costs.\n- **Marginal Cost (SMC)**: Intersects SAC at its minimum point.\n5.  **Long-Run Curves**:\n- **Average Cost (LAC)**: Also U-shaped, reflecting economies and diseconomies of scale.\n- **Marginal Cost (LMC)**: Intersects LAC at its minimum point.\n6.  **Constant Returns to Scale**: If production exhibits constant returns, LAC = LMC = constant.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain sufficient information about short run versus long run distinctions in cost curve behavior. While the context establishes properties of cost functions (homogeneity, concavity, etc.) and mentions concepts like average/marginal costs, it does not explicitly discuss time horizons (short run vs. long run) or their impact on the shape of these curves. The derivation of short-run vs. long-run cost curve behavior typically requires explicit assumptions about fixed/variable inputs, which are absent from the given context.\n\nQID: textbook-94-6-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-94-6-1-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain sufficient information'.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-94-6-1-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain sufficient information'."}, "llm_echoed_qid": "textbook-94-6-1-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain sufficient information'."}
{"script_processing_qid": "textbook-21-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that if preferences are homothetic, then the aggregate demand function depends only on prices and aggregate wealth.\n\nGOLD_ANSWER:\n1. **Homothetic Preferences**: For homothetic preferences, the demand function satisfies $x_{i}(p, w_{i}) = w_{i}x_{i}(p, 1)$.\n2. **Aggregate Demand**: $x(p, w) = \\sum_{i} x_{i}(p, w_{i}) = \\sum_{i} w_{i}x_{i}(p, 1)$.\n3. **Wealth Distribution**: Since $\\sum_{i} w_{i} = w$, aggregate demand can be written as $x(p, w) = w \\sum_{i} \\alpha_{i}x_{i}(p, 1)$, where $\\alpha_{i} = w_{i}/w$.\n4. **Conclusion**: Aggregate demand depends only on prices and aggregate wealth $w$, not on the distribution of wealth among consumers.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that homothetic preferences imply Engel curves are linear through the origin (i.e., demand functions satisfy x(p, αw) = αx(p, w) for any α > 0).\nStep 2: Individual demand for agent i with wealth w_i can thus be expressed as x_i(p, w_i) = w_i · x_i(p, 1).\nStep 3: Aggregate demand under wealth distribution {w₁,...,w_I} becomes X(p, W) = Σ_{i=1}^I [w_i · x_i(p, 1)] where W = Σ_{i=1}^I w_i.\nStep 4: Since preferences are homothetic (and identically scaled), x_i(p, 1) = x_j(p, 1) for all agents i,j. Let x(p, 1) denote this common vector.\nStep 5: Aggregate demand simplifies to X(p, W) = (Σ_{i=1}^I w_i) · x(p, 1) = W · x(p, 1), which depends only on prices (p) and aggregate wealth (W).\nStep 6: This holds for any wealth distribution {w_i} as long as total W remains constant – redistribution does not affect X(p, W).\n\nQID: textbook-21-0-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-21-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the logical steps to prove that aggregate demand depends only on prices and aggregate wealth under homothetic preferences, matching the reasoning and conclusion of the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-21-0-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the logical steps to prove that aggregate demand depends only on prices and aggregate wealth under homothetic preferences, matching the reasoning and conclusion of the gold answer."}, "llm_echoed_qid": "textbook-21-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the logical steps to prove that aggregate demand depends only on prices and aggregate wealth under homothetic preferences, matching the reasoning and conclusion of the gold answer."}
{"script_processing_qid": "textbook-114-14-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What are the limitations of the mechanisms (e.g., takeovers, managerial labor market) that aim to mitigate the principal-agent problem?\n\nGOLD_ANSWER:\n1. **Takeovers**: May be motivated by personal gain rather than efficiency, leading to suboptimal outcomes.\n2. **Managerial Labor Market**: May not work perfectly due to long-term contracts and retirement nearness, reducing the incentive to perform.\n3. **Monitoring Costs**: High costs of monitoring can limit the effectiveness of these mechanisms.\n\nCANDIDATE_ANSWER:\nStep 1: **Takeovers as a mechanism**\n- Limitations: Takeovers require accurate market valuation to discipline managers. However, asymmetric information (mentioned in the context) makes it difficult for principals to distinguish between poor management and external factors affecting firm performance. Costly monitoring exacerbates this issue, as owners may lack resources to identify underperforming managers.\n\nStep 2: **Managerial labor market**\n- Limitations: Reputation-based incentives rely on observable performance metrics. Incomplete information (highlighted in the context) prevents full visibility into managerial decisions, allowing agents to manipulate short-term indicators (e.g., cutting R&D to boost profits) while harming long-term value. This creates a misalignment between labor market signals and true owner interests.\n\nStep 3: **General constraints**\n- Both mechanisms assume rational market responses, but costly monitoring (explicitly noted in the context) limits owners' ability to verify agent actions. This reduces the effectiveness of market-based discipline mechanisms. Additionally, managers may prioritize takeover defense strategies (e.g., poison pills) over value creation, further undermining these solutions.\n\nQID: textbook-114-14-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-14-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the limitations of takeovers and the managerial labor market, aligning well with the gold answer. It also expands on general constraints, providing a comprehensive analysis that matches the gold answer's points while adding depth.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-14-2-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the limitations of takeovers and the managerial labor market, aligning well with the gold answer. It also expands on general constraints, providing a comprehensive analysis that matches the gold answer's points while adding depth."}, "llm_echoed_qid": "textbook-114-14-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the limitations of takeovers and the managerial labor market, aligning well with the gold answer. It also expands on general constraints, providing a comprehensive analysis that matches the gold answer's points while adding depth."}
{"script_processing_qid": "textbook-112-20-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the economic incidence of a tax on the sale of land, considering the relative elasticities of supply and demand as described by Adam Smith.\n\nGOLD_ANSWER:\n1.  **Seller's Elasticity**: The seller is often necessitous, implying inelastic supply (must sell regardless of price).\n2.  **Buyer's Elasticity**: The buyer is not under necessity, implying more elastic demand (can choose not to buy if price is too high).\n3.  **Tax Incidence**: The burden falls more heavily on the less elastic party (seller). Mathematically, the tax burden ratio is inversely proportional to the elasticities: $\\frac{T_s}{T_d} = \\frac{E_d}{E_s}$, where $E_d$ is demand elasticity and $E_s$ is supply elasticity.\n4.  **Result**: Since $E_s < E_d$, $T_s > T_d$; the seller bears most of the tax burden.\n\nCANDIDATE_ANSWER:\nStep 1: Analyze the context provided. Adam Smith argues that taxes on land sales fall entirely on sellers because sellers are 'under necessity of selling' (indicating inelastic supply), while buyers face no urgency (indicating more elastic demand).\n\nStep 2: Apply the standard tax incidence formula. The economic burden of a tax is determined by the relative elasticities of supply (Es) and demand (Ed):\n- Seller’s burden = Ed / (Ed + Es)\n- Buyer’s burden = Es / (Ed + Es)\n\nStep 3: Infer elasticities from the context. Smith describes sellers as having near-zero elasticity of supply (Es ≈ 0) because they 'must take such a price as they can get.' Buyers exhibit non-zero elasticity (Ed > 0) since they 'will only give such a price as they like.'\n\nStep 4: Substitute elasticities into the formula. With Es ≈ 0:\n- Seller’s burden ≈ Ed / (Ed + 0) = 100%\n- Buyer’s burden ≈ 0 / (Ed + 0) = 0%\n\nStep 5: Conclusion. The entire tax burden falls on sellers due to their perfectly inelastic supply position, consistent with Smith’s argument that the tax 'must be frequently very cruel and oppressive' to necessitous sellers.\n\nQID: textbook-112-20-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-20-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately follows Adam Smith's reasoning, correctly identifies the elasticities, and applies the tax incidence formula to conclude that sellers bear the entire tax burden due to perfectly inelastic supply. This matches the gold answer in both reasoning and conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-20-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately follows Adam Smith's reasoning, correctly identifies the elasticities, and applies the tax incidence formula to conclude that sellers bear the entire tax burden due to perfectly inelastic supply. This matches the gold answer in both reasoning and conclusion."}, "llm_echoed_qid": "textbook-112-20-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately follows Adam Smith's reasoning, correctly identifies the elasticities, and applies the tax incidence formula to conclude that sellers bear the entire tax burden due to perfectly inelastic supply. This matches the gold answer in both reasoning and conclusion."}
{"script_processing_qid": "textbook-107-12-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. For the production function $y=K^{1/2}L^{1/2}$, derive the optimal input combination for WEW to produce 10 widgets at minimal cost. Then, calculate the additional cost incurred by WNW due to the CEO's constraint.\n\nGOLD_ANSWER:\n1. **Optimal Inputs**: For WEW, minimize $C = K + L$ subject to $K^{1/2}L^{1/2} = 10$.\n2. **First-Order Conditions**: $\\frac{\\partial C}{\\partial K} = 1 - \\lambda \\frac{L^{1/2}}{2K^{1/2}} = 0$ and $\\frac{\\partial C}{\\partial L} = 1 - \\lambda \\frac{K^{1/2}}{2L^{1/2}} = 0$.\n3. **Solution**: $K = L = 10$. Total cost for WEW: $10 + 10 = \\$20$.\n4. **WNW Constraint**: Uses $L = 20$ (twice WEW's labor). Solve $K^{1/2}(20)^{1/2} = 10$ to get $K = 5$.\n5. **Additional Cost**: WNW's cost is $5 + 20 = \\$25$. The difference is $\\$5$ (option d).\n\nCANDIDATE_ANSWER:\nStep 1: Identify WEW's cost minimization problem.\nProduction function: y = K^(1/2)L^(1/2) = 10\nInput prices: w = $1 (labor), r = $1 (capital)\n\nStep 2: Set up optimization condition (MRTS = input price ratio).\nMRTS = MPL/MPK = (K/L) = w/r = 1 ⇒ K = L\n\nStep 3: Substitute K = L into production constraint:\n10 = (K*L)^(1/2) ⇒ 100 = K*L ⇒ K = L = 10\nWEW's optimal inputs: K = 10, L = 10\nTotal cost = 10*$1 + 10*$1 = $20\n\nStep 4: Calculate WNW's inputs under constraint:\nL_WNW = 2*L_WEW = 20\nProduction constraint with L = 20:\n10 = (K*20)^(1/2) ⇒ K = 5\n\nStep 5: Compute WNW's total cost:\nK = 5, L = 20\nTotal cost = 20*$1 + 5*$1 = $25\n\nStep 6: Calculate cost difference:\n$25 (WNW) - $20 (WEW) = $5 additional cost\n\nQID: textbook-107-12-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-12-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer matches the gold answer in all aspects, including the derivation of optimal inputs, the calculation of costs, and the determination of the additional cost due to the CEO's constraint.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-12-0-1", "category": "CORRECT", "explanation": "The candidate's answer matches the gold answer in all aspects, including the derivation of optimal inputs, the calculation of costs, and the determination of the additional cost due to the CEO's constraint."}, "llm_echoed_qid": "textbook-107-12-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer matches the gold answer in all aspects, including the derivation of optimal inputs, the calculation of costs, and the determination of the additional cost due to the CEO's constraint."}
{"script_processing_qid": "textbook-112-47-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the 'invisible hand' mechanism as described by Adam Smith in this context.\n\nGOLD_ANSWER:\n1.  **Individual Intent**: Individuals aim to maximize personal gain by supporting domestic industry and directing it toward high-value production.\n2.  **Unintended Outcome**: This behavior, though self-interested, inadvertently promotes societal welfare by maximizing aggregate revenue and employment.\n3.  **Market Efficiency**: The 'invisible hand' ensures optimal resource allocation without centralized planning, as individuals naturally prefer efficient, high-value industries.\n\nCANDIDATE_ANSWER:\nStep 1: Adam Smith argues that individuals naturally prioritize domestic industry over foreign trade when allocating capital. This choice is driven by self-interest (seeking security and personal gain) rather than societal benefit.\n\nStep 2: By directing capital to maximize the value of domestic production ('as its produce may be of the greatest value'), individuals unintentionally promote greater domestic employment and national revenue. This occurs even though these outcomes are not their explicit goal.\n\nStep 3: Smith calls this unintended coordination the 'invisible hand' mechanism. It demonstrates how decentralized self-interest in free markets achieves socially optimal outcomes (stronger domestic industry) more effectively than deliberate regulations like monopolies or trade restrictions, which he criticizes as 'useless or hurtful' interventions.\n\nQID: textbook-112-47-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-47-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately captures the three key components of the 'invisible hand' mechanism as described by Adam Smith: individual intent, unintended societal benefit, and market efficiency. The explanation aligns well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-47-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately captures the three key components of the 'invisible hand' mechanism as described by Adam Smith: individual intent, unintended societal benefit, and market efficiency. The explanation aligns well with the gold answer."}, "llm_echoed_qid": "textbook-112-47-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately captures the three key components of the 'invisible hand' mechanism as described by Adam Smith: individual intent, unintended societal benefit, and market efficiency. The explanation aligns well with the gold answer."}
{"script_processing_qid": "textbook-84-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the interpretation of $\\beta_{1}/100$ as the percentage point change in voteA for a one percent increase in expendA, starting from the equation $\\Delta\\nu o t e A \\approx (\\beta_{1}/100)(\\%\\Delta e x p e n d A)$.\n\nGOLD_ANSWER:\n1. **Given**: $\\Delta\\nu o t e A \\approx (\\beta_{1}/100)(\\%\\Delta e x p e n d A)$.\n2. **Interpretation**: For a $1\\%$ increase in expendA ($\\%\\Delta e x p e n d A = 1$), $\\Delta\\nu o t e A \\approx (\\beta_{1}/100)(1) = \\beta_{1}/100$.\n3. **Conclusion**: $\\beta_{1}/100$ represents the percentage point change in voteA for a $1\\%$ increase in expendA, ceteris paribus.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the original relationship: ΔvoteA ≈ (β₁/100)(%ΔexpendA).\nStep 2: Recognize that %ΔexpendA = 1 represents a 1% increase in expenditure by Candidate A.\nStep 3: Substitute %ΔexpendA = 1 into the equation: ΔvoteA ≈ (β₁/100)(1) = β₁/100.\nStep 4: Interpret this result: When expendA increases by 1%, voteA changes by β₁/100 percentage points (since voteA is already measured in percentage terms).\nConclusion: β₁/100 represents the percentage point change in voteA for a one percent increase in expendA, holding other factors constant.\n\nQID: textbook-84-1-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-84-1-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the interpretation of β₁/100 as the percentage point change in voteA for a one percent increase in expendA, matching the gold answer in both reasoning and conclusion.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-84-1-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the interpretation of β₁/100 as the percentage point change in voteA for a one percent increase in expendA, matching the gold answer in both reasoning and conclusion."}, "llm_echoed_qid": "textbook-84-1-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the interpretation of β₁/100 as the percentage point change in voteA for a one percent increase in expendA, matching the gold answer in both reasoning and conclusion."}
{"script_processing_qid": "textbook-111-21-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Calculate NNP from GNP given that depreciation is 10% of GNP.\n\nGOLD_ANSWER:\nIf depreciation is 10% of GNP, then:\n$$ \\text{NNP} = \\text{GNP} - 0.10 \\times \\text{GNP} = 0.90 \\times \\text{GNP} $$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the formula for NNP from the context: NNP = GNP - Depreciation.\nStep 2: Calculate depreciation as 10% of GNP: Depreciation = 0.10 × GNP.\nStep 3: Substitute depreciation into the NNP formula: NNP = GNP - (0.10 × GNP) = 0.90 × GNP.\nFinal Answer: NNP equals 90% of GNP when depreciation is 10% of GNP.\n\nQID: textbook-111-21-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-21-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive NNP from GNP given the depreciation rate, matching the gold answer's reasoning and final result.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-21-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive NNP from GNP given the depreciation rate, matching the gold answer's reasoning and final result."}, "llm_echoed_qid": "textbook-111-21-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive NNP from GNP given the depreciation rate, matching the gold answer's reasoning and final result."}
{"script_processing_qid": "textbook-107-11-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain why Al has a comparative advantage in producing geegaws while Bill has a comparative advantage in producing doodads, given their production rates.\n\nGOLD_ANSWER:\n1. **Al's Opportunity Costs**: \n   - 1 geegaw = 2 doodads (12/6), 1 doodad = 0.5 geegaws.\n   - **Bill's Opportunity Costs**: 1 geegaw = 4 doodads (12/3), 1 doodad = 0.25 geegaws.\n2. **Comparative Advantage**: Al gives up fewer doodads to produce geegaws (2 vs. Bill's 4), so he specializes in geegaws. Bill gives up fewer geegaws to produce doodads (0.25 vs. Al's 0.5), so he specializes in doodads.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate opportunity costs for each producer. For Al: Producing 1 geegaw costs 12/6 = 2 doodads. Producing 1 doodad costs 6/12 = 0.5 geegaws. For Bill: Producing 1 geegaw costs 12/3 = 4 doodads. Producing 1 doodad costs 3/12 = 0.25 geegaws.\n\nStep 2: Compare opportunity costs. Al has lower opportunity cost in geegaws (2 doodads vs Bill's 4 doodads). Bill has lower opportunity cost in doodads (0.25 geegaws vs Al's 0.5 geegaws).\n\nStep 3: Determine comparative advantage. Al should specialize in geegaws (where his OC is lower), Bill should specialize in doodads (where his OC is lower).\n\nQID: textbook-107-11-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-107-11-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly calculates the opportunity costs, compares them, and determines the comparative advantage for both Al and Bill, matching the gold answer in both reasoning and conclusion.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-11-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly calculates the opportunity costs, compares them, and determines the comparative advantage for both Al and Bill, matching the gold answer in both reasoning and conclusion."}, "llm_echoed_qid": "textbook-107-11-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly calculates the opportunity costs, compares them, and determines the comparative advantage for both Al and Bill, matching the gold answer in both reasoning and conclusion."}
{"script_processing_qid": "textbook-114-32-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Using the example of the baseball team with a franchise, explain how producer surplus includes economic rent. What would happen to the producer surplus if more franchises were allowed?\n\nGOLD_ANSWER:\n1. **Producer Surplus and Economic Rent**: The producer surplus for the baseball team includes its economic profit and the economic rent from the franchise. The rent arises because the franchise is scarce and valuable.\n2. **Impact of More Franchises**: If more franchises were allowed, the scarcity value of the franchise would decrease, reducing the economic rent component of the producer surplus. The team's producer surplus would decrease as competition increases.\n\nCANDIDATE_ANSWER:\nStep 1: Define key terms. Producer surplus is the difference between the market price received and the marginal cost of production. Economic rent is the payment for a scarce input exceeding the minimum required to retain it.\n\nStep 2: Apply to the baseball franchise example. The franchise itself is a scarce input (e.g., limited licenses). The team earns economic rent equal to the difference between what it is willing to pay for the franchise and the minimum amount needed to keep the franchise operational (e.g., covering basic costs).\n\nStep 3: Link producer surplus to economic rent. In the long run, the team’s producer surplus includes all economic rents from its scarce inputs (e.g., the franchise). Since the franchise is scarce, it generates persistent rent, contributing to producer surplus even when economic profit is zero.\n\nStep 4: Effect of allowing more franchises. Increasing franchises reduces the scarcity of this input. With more franchises available, the economic rent per franchise decreases (as competition lowers the premium teams can earn). Consequently, the producer surplus for existing teams would decline because the rent component of their surplus diminishes.\n\nQID: textbook-114-32-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-32-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the relationship between producer surplus and economic rent using the baseball franchise example, and correctly describes the impact of allowing more franchises on producer surplus, aligning with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-32-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the relationship between producer surplus and economic rent using the baseball franchise example, and correctly describes the impact of allowing more franchises on producer surplus, aligning with the gold answer."}, "llm_echoed_qid": "textbook-114-32-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the relationship between producer surplus and economic rent using the baseball franchise example, and correctly describes the impact of allowing more franchises on producer surplus, aligning with the gold answer."}
{"script_processing_qid": "textbook-106-11-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the conditions under which a good is classified as a public good, private good, common resource, or natural monopoly, using the properties of excludability and rivalry.\n\nGOLD_ANSWER:\n1. **Public Good**: Neither excludable nor rival (e.g., national defense).\n2. **Private Good**: Both excludable and rival (e.g., ice-cream cone).\n3. **Common Resource**: Rival but not excludable (e.g., fish in the ocean).\n4. **Natural Monopoly**: Excludable but not rival (e.g., fire protection in a small town).\n\nCANDIDATE_ANSWER:\nStep 1: Identify the two key properties of goods:\n- Excludability: Whether individuals can be prevented from using the good.\n- Rivalry: Whether one person’s use reduces availability for others.\n\nStep 2: Classify goods based on combinations of these properties:\n1. Private Goods: Excludable AND Rival (e.g., food, clothing).\n2. Public Goods: NOT Excludable AND NOT Rival (e.g., national defense, public parks).\n3. Common Resources: NOT Excludable BUT Rival (e.g., fish stocks, clean air).\n4. Natural Monopolies: Excludable BUT NOT Rival (e.g., cable TV, utilities).\n\nStep 3: Derive classification conditions:\n- If a good is both excludable and rival → Private Good.\n- If a good is neither excludable nor rival → Public Good.\n- If a good is rival but not excludable → Common Resource.\n- If a good is excludable but not rival → Natural Monopoly.\n\nQID: textbook-106-11-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-106-11-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the properties of excludability and rivalry and accurately classifies goods into public goods, private goods, common resources, and natural monopolies based on these properties. The examples provided also align with the definitions.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-11-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the properties of excludability and rivalry and accurately classifies goods into public goods, private goods, common resources, and natural monopolies based on these properties. The examples provided also align with the definitions."}, "llm_echoed_qid": "textbook-106-11-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the properties of excludability and rivalry and accurately classifies goods into public goods, private goods, common resources, and natural monopolies based on these properties. The examples provided also align with the definitions."}
{"script_processing_qid": "textbook-123-18-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Show how the $j$-step pricing kernel $Q_{j}(s_{t+j}|s_{t})$ can be computed recursively from the one-step kernel.\n\nGOLD_ANSWER:\n1. **Base Case**: $Q_{1}(s_{t+1}|s_{t})$ is given. \n2. **Recursion**: $Q_{j}(s_{t+j}|s_{t}) = \\sum_{s_{t+1}} Q_{1}(s_{t+1}|s_{t}) Q_{j-1}(s_{t+j}|s_{t+1})$. \n3. **Verification**: This satisfies the no-arbitrage condition.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that the j-step pricing kernel represents the price of transitioning from state $s_t$ at time $t$ to state $s_{t+j}$ at time $t+j$.\n\nStep 2: Observe the recursive formula provided: $Q_j(s_{t+j}|s_t) = \\sum_{s_{t+1}} Q_{j-1}(s_{t+j}|s_{t+1}) Q_1(s_{t+1}|s_t)$. This shows that a j-step kernel can be computed by:\n  (i) First considering all possible intermediate states $s_{t+1}$ one period ahead\n  (ii) Multiplying the 1-step kernel $Q_1(s_{t+1}|s_t)$ (transition to $s_{t+1}$)\n  (iii) Combining it with the $(j-1)$-step kernel $Q_{j-1}(s_{t+j}|s_{t+1})$ (remaining transitions from $s_{t+1}$ to $s_{t+j}$)\n\nStep 3: Initialize with the baseline case $Q_1(s_{t+1}|s_t)$ (given). For each subsequent $j > 1$, iteratively apply the formula by:\n  1. Computing $Q_2(s_{t+2}|s_t) = \\sum_{s_{t+1}} Q_1(s_{t+2}|s_{t+1}) Q_1(s_{t+1}|s_t)$\n  2. Then $Q_3(s_{t+3}|s_t) = \\sum_{s_{t+1}} Q_2(s_{t+3}|s_{t+1}) Q_1(s_{t+1}|s_t)$\n  3. Continue recursively until reaching $Q_j$\n\nThis process effectively performs nested summation across all intermediate states at each time step, propagating uncertainty through the state space in a Markovian manner.\n\nQID: textbook-123-18-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-18-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains the recursive computation of the j-step pricing kernel, matching the gold answer's structure and logic. It details the base case, recursive step, and verification process accurately.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-18-1-2", "category": "CORRECT", "explanation": "The candidate answer correctly explains the recursive computation of the j-step pricing kernel, matching the gold answer's structure and logic. It details the base case, recursive step, and verification process accurately."}, "llm_echoed_qid": "textbook-123-18-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains the recursive computation of the j-step pricing kernel, matching the gold answer's structure and logic. It details the base case, recursive step, and verification process accurately."}
{"script_processing_qid": "textbook-106-15-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the concept of diminishing marginal utility and its role in the utilitarian argument for income redistribution.\n\nGOLD_ANSWER:\nDiminishing marginal utility implies that as income increases, the additional utility from each extra dollar decreases. This justifies redistribution because transferring income from the rich (with lower marginal utility) to the poor (with higher marginal utility) increases total societal utility. Utilitarians advocate for redistribution up to the point where the marginal utility loss from taxation equals the marginal utility gain from transfers, accounting for incentive distortions.\n\nCANDIDATE_ANSWER:\nStep 1: Diminishing marginal utility refers to the principle that as a person’s income increases, the additional utility (happiness or satisfaction) gained from each extra dollar of income decreases. For example, a poor person derives more utility from an additional dollar than a wealthy person.\\n\\nStep 2: In the utilitarian framework, this principle justifies income redistribution. Transferring a dollar from a wealthy individual (who experiences a small utility loss) to a poor individual (who gains a larger utility increase) raises total societal utility.\\n\\nStep 3: However, utilitarians recognize that excessive redistribution reduces incentives to work and invest, potentially shrinking total income. Thus, the government balances marginal utility gains from equality against efficiency losses, stopping redistribution at the point where total utility is maximized—not pursuing full equality.\n\nQID: textbook-106-15-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-15-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains diminishing marginal utility and its role in utilitarian arguments for income redistribution, including the balance between utility gains and efficiency losses, matching the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-15-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains diminishing marginal utility and its role in utilitarian arguments for income redistribution, including the balance between utility gains and efficiency losses, matching the gold answer."}, "llm_echoed_qid": "textbook-106-15-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains diminishing marginal utility and its role in utilitarian arguments for income redistribution, including the balance between utility gains and efficiency losses, matching the gold answer."}
{"script_processing_qid": "textbook-123-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the individual's optimization problem under employment lotteries and explain why it relaxes the constraint on $\\psi$.\n\nGOLD_ANSWER:\n1. **Optimization Problem**: The individual maximizes $u(c) - \\psi A$ subject to $c \\leq \\psi\\gamma$ and $\\psi \\in [0,1]$.\n2. **Relaxation of Constraint**: Unlike the original problem where $\\psi$ could only be 0 or 1, the lottery allows $\\psi$ to take any value in $[0,1]$, enabling smoother trade-offs between consumption and labor.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the key elements of the optimization problem under employment lotteries. The individual maximizes utility $u(c)$ net of work disutility $\\psi A$, subject to $c \\leq \\psi\\gamma$ (budget constraint) and $\\psi \\in [0,1]$ (lottery probability).\n\nStep 2: Compare to the discrete labor case without lotteries. In standard indivisible labor models, $\\psi$ is restricted to $\\{0,1\\}$ (either work full-time or not at all). Employment lotteries relax this by allowing $\\psi$ to represent a probability of employment in $[0,1]$.\n\nStep 3: Explain the relaxation. By allowing $\\psi$ to take any value between 0 and 1, individuals can select convex combinations of employment states (e.g., 60% chance of working). This convexifies the choice set, enabling better optimization of the trade-off between consumption ($c = \\psi\\gamma$) and disutility ($\\psi A$) compared to the binary constraint.\n\nStep 4: Welfare implication. The relaxation improves welfare because it allows marginal rate of substitution between consumption and leisure to equal the marginal rate of transformation ($\\gamma/A$) through interior solutions, which is impossible under the original $\\psi \\in \\{0,1\\}$ restriction.\n\nQID: textbook-123-0-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the optimization problem under employment lotteries and explains the relaxation of the constraint on $\\psi$. It accurately compares the lottery case to the discrete labor case and provides a clear explanation of the welfare implications.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-0-1-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the optimization problem under employment lotteries and explains the relaxation of the constraint on $\\psi$. It accurately compares the lottery case to the discrete labor case and provides a clear explanation of the welfare implications."}, "llm_echoed_qid": "textbook-123-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the optimization problem under employment lotteries and explains the relaxation of the constraint on $\\psi$. It accurately compares the lottery case to the discrete labor case and provides a clear explanation of the welfare implications."}
{"script_processing_qid": "textbook-62-4-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the implications of a 1% vs. 5% annual growth rate over 100 years using the compound growth formula.\n\nGOLD_ANSWER:\n1. **Formula**: $Y_{t} = (1 + g)^{t}Y_{0}$.\n2. **Case 1%**: $Y_{100} = (1.01)^{100}Y_{0} \\approx 2.705Y_{0}$.\n3. **Case 5%**: $Y_{100} = (1.05)^{100}Y_{0} \\approx 131.5Y_{0}$.\n4. **Interpretation**: Small differences in growth rates lead to vastly different outcomes over long periods.\n\nCANDIDATE_ANSWER:\nStep 1: Apply the compound growth formula for both rates:\nFor 1% growth: Y₁₀₀ = (1 + 0.01)¹⁰⁰ × Y₀ ≈ 2.7048 × Y₀\nFor 5% growth: Y₁₀₀ = (1 + 0.05)¹⁰⁰ × Y₀ ≈ 131.50 × Y₀\n\nStep 2: Compare outcomes:\n- 5% growth produces ≈48.6 times greater output than 1% growth after 100 years (131.50/2.7048 ≈48.6)\n\nStep 3: Analyze doubling times using t = log(2)/log(1+g):\n- At 1%: t ≈ 69.66 years (GDP doubles once in 100 years)\n- At 5%: t ≈ 14.21 years (GDP doubles ≈7 times in 100 years)\n\nImplication: Small rate differences lead to enormous disparities over long horizons due to exponential compounding. A 5% rate yields 131x initial GDP vs 2.7x at 1%, demonstrating growth rate sensitivity in long-term economic outcomes.\n\nQID: textbook-62-4-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-62-4-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly applied the compound growth formula, calculated the outcomes for both growth rates, and provided a detailed comparison and analysis of the implications, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-62-4-0-3", "category": "CORRECT", "explanation": "The candidate correctly applied the compound growth formula, calculated the outcomes for both growth rates, and provided a detailed comparison and analysis of the implications, aligning with the gold answer."}, "llm_echoed_qid": "textbook-62-4-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly applied the compound growth formula, calculated the outcomes for both growth rates, and provided a detailed comparison and analysis of the implications, aligning with the gold answer."}
{"script_processing_qid": "textbook-119-9-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Explain the Lagrange Multiplier (LM) test for adding variables and how it compares to Ramsey's RESET test.\n\nGOLD_ANSWER:\n1. **LM Test Steps**:\n   - Estimate the restricted model and obtain residuals.\n   - Regress residuals on all regressors (including those in the restricted model).\n   - Compute $n R^{2}$ and compare to chi-square distribution.\n2. **Comparison to RESET**:\n   - LM test requires specifying the alternative model (e.g., including $X\\_{i}^{2}$ and $X\\_{i}^{3}$).\n   - RESET does not require specifying an alternative model but may lack power.\n\nCANDIDATE_ANSWER:\nStep 1: The Lagrange Multiplier (LM) test for adding variables involves the following procedure:\n   a. Estimate the restricted model (excluding the variables of interest) via OLS and save residuals (ï��_i).\n   b. Regress these residuals on all regressors from both restricted and unrestricted models.\n   c. Compute the test statistic nR², which asymptotically follows a chi-square distribution with degrees of freedom equal to the number of excluded variables being tested.\n\nStep 2: Ramsey's RESET test focuses on detecting general specification errors (e.g., omitted nonlinear terms or incorrect functional form). While the context does not detail its exact steps, it is typically implemented by adding powers of fitted values from the original model and testing their joint significance.\n\nStep 3: Key comparison:\n   - The LM test specifically evaluates whether excluded variables should be added to the model.\n   - Ramsey's RESET test evaluates broader functional form misspecification (e.g., missing quadratic terms or interactions) rather than omitted variables explicitly.\n   - Both use an auxiliary regression and nR²/chi-square framework, but they target different types of specification errors.\n\nQID: textbook-119-9-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-9-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately describes the steps of the Lagrange Multiplier (LM) test and correctly compares it to Ramsey's RESET test, highlighting their distinct purposes and methodologies. The explanation aligns well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-9-1-0", "category": "CORRECT", "explanation": "The candidate answer accurately describes the steps of the Lagrange Multiplier (LM) test and correctly compares it to Ramsey's RESET test, highlighting their distinct purposes and methodologies. The explanation aligns well with the gold answer."}, "llm_echoed_qid": "textbook-119-9-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately describes the steps of the Lagrange Multiplier (LM) test and correctly compares it to Ramsey's RESET test, highlighting their distinct purposes and methodologies. The explanation aligns well with the gold answer."}
{"script_processing_qid": "textbook-119-5-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the variance of $\\hat{\\beta}_2$ for the regression-through-the-origin model under the classical assumptions.\n\nGOLD_ANSWER:\n1. **Variance Definition**: $\\mathrm{var}(\\hat{\\beta}_2) = E[(\\hat{\\beta}_2 - \\beta_2)^2]$.\n2. **Substitute**: $\\hat{\\beta}_2 - \\beta_2 = \\frac{\\sum X_i u_i}{\\sum X_i^2}$.\n3. **Expand**: $E\\left[\\left(\\frac{\\sum X_i u_i}{\\sum X_i^2}\\right)^2\\right] = \\frac{\\sum X_i^2 \\sigma^2}{(\\sum X_i^2)^2} = \\frac{\\sigma^2}{\\sum X_i^2}$ (assuming homoscedasticity and no autocorrelation).\n\nCANDIDATE_ANSWER:\nStep 1: Start with the expression for the estimator deviation\nFrom the context, we have: \n$$\n\\hat{\\beta}_2 - \\beta_2 = \\frac{\\sum X_i u_i}{\\sum X_i^2}\n$$\n\nStep 2: Compute the variance\n$$\n\\text{var}(\\hat{\\beta}_2) = E\\left[(\\hat{\\beta}_2 - \\beta_2)^2\\right] = E\\left[\\left(\\frac{\\sum X_i u_i}{\\sum X_i^2}\\right)^2\\right]\n$$\n\nStep 3: Expand the squared term\n$$\n= \\frac{1}{(\\sum X_i^2)^2} E\\left[\\left(\\sum X_i u_i\\right)^2\\right]\n$$\n\nStep 4: Apply classical assumptions\nUnder CLRM assumptions (uncorrelated errors with $E(u_i u_j)=0$ for $i\\neq j$, const variance $E(u_i^2)=\\sigma^2$):\n$$\nE\\left[\\left(\\sum X_i u_i\\right)^2\\right] = \\sum X_i^2 E(u_i^2) = \\sigma^2 \\sum X_i^2\n$$\n\nStep 5: Final simplification\n$$\n\\text{var}(\\hat{\\beta}_2) = \\frac{\\sigma^2 \\sum X_i^2}{(\\sum X_i^2)^2} = \\frac{\\sigma^2}{\\sum X_i^2}\n$$\nThis matches the variance expression derived in the provided context.\n\nQID: textbook-119-5-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-5-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the variance of $\\hat{\\beta}_2$ under the classical assumptions, matching the gold answer in both reasoning and final result.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-5-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the variance of $\\hat{\beta}_2$ under the classical assumptions, matching the gold answer in both reasoning and final result."}, "llm_echoed_qid": "textbook-119-5-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the variance of $\\hat{\beta}_2$ under the classical assumptions, matching the gold answer in both reasoning and final result."}
{"script_processing_qid": "textbook-115-4-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain how to compute the LM statistic using the auxiliary regression approach under homoskedasticity.\n\nGOLD_ANSWER:\n1. **Regress Restricted Residuals**: Run the regression $$\\tilde{u}_{i}\\mathrm{~on~}\\nabla_{\\boldsymbol{\\theta}}\\tilde{m}_{i},\\quad i=1,2,\\ldots,N,$$ where $\\tilde{u}_{i}$ are the residuals from the restricted model and $\\nabla_{\\boldsymbol{\\theta}}\\tilde{m}_{i}$ is the gradient of the unrestricted mean function evaluated at the restricted estimates.\n2. **Compute $R_{u}^{2}$**: Obtain the uncentered $R$-squared $R_{u}^{2}$ from this regression.\n3. **LM Statistic**: The LM statistic is $L M=N R_{u}^{2}$, which asymptotically follows $\\chi_{Q}^{2}$ under $\\mathrm{H}_{0}$.\n\nCANDIDATE_ANSWER:\nStep 1: Estimate the restricted model under H₀ to obtain the restricted residuals (denoted as residuals).\nStep 2: Compute the gradient of the unrestricted model's mean function with respect to all parameters, evaluated at the restricted parameter estimates. This forms the regressor matrix in the auxiliary regression.\nStep 3: Regress the restricted residuals (from Step 1) on the gradient matrix (from Step 2) using an OLS regression.\nStep 4: Compute the uncentered R-squared (R_u²) from this auxiliary regression. The uncentered R-squared uses the total sum of squares without demeaning the dependent variable.\nStep 5: Multiply R_u² by the sample size N to obtain the LM statistic: LM = N × R_u²\nStep 6: Compare the LM statistic to the critical value from a χ² distribution with Q degrees of freedom (where Q is the number of restrictions under H₀).\n\nQID: textbook-115-4-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-4-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately follows the steps outlined in the gold answer, including regressing restricted residuals on the gradient, computing the uncentered R-squared, and deriving the LM statistic. The explanation is detailed and matches the gold answer's methodology.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-4-2-1", "category": "CORRECT", "explanation": "The candidate answer accurately follows the steps outlined in the gold answer, including regressing restricted residuals on the gradient, computing the uncentered R-squared, and deriving the LM statistic. The explanation is detailed and matches the gold answer's methodology."}, "llm_echoed_qid": "textbook-115-4-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately follows the steps outlined in the gold answer, including regressing restricted residuals on the gradient, computing the uncentered R-squared, and deriving the LM statistic. The explanation is detailed and matches the gold answer's methodology."}
{"script_processing_qid": "textbook-107-6-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Analyze the prisoners' dilemma scenario where both prisoners believe the other will deny the crime. Why is the statement 'both would deny the crime' false?\n\nGOLD_ANSWER:\n1. **Prisoners' Dilemma**: Both players have a dominant strategy to confess, leading to a suboptimal outcome where both confess, even though mutual denial would yield a better payoff.\n2. **Rationality**: Even if a prisoner believes the other will deny, confessing still yields a higher payoff (lower sentence), making denial not a Nash equilibrium.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that in the prisoners' dilemma, the dominant strategy for each player is to confess (defect), not deny (cooperate).\nStep 2: If both prisoners believe the other will deny, their optimal response (given the payoff structure of the prisoners' dilemma) is to confess, as this ensures a better individual outcome regardless of the other's action.\nStep 3: The statement 'both would deny the crime' is false because rational players in a prisoners' dilemma will not follow through on denying if they believe the other will deny—they will instead play their dominant strategy (confess).\nStep 4: This aligns with the definition of Nash equilibrium in the context provided: a situation where everyone plays a dominant strategy (confessing, in this case) constitutes the equilibrium, not mutual denial.\n\nQID: textbook-107-6-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-6-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the dominant strategy (confessing) and explains why mutual denial is not a Nash equilibrium, aligning perfectly with the gold answer's reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-6-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the dominant strategy (confessing) and explains why mutual denial is not a Nash equilibrium, aligning perfectly with the gold answer's reasoning."}, "llm_echoed_qid": "textbook-107-6-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the dominant strategy (confessing) and explains why mutual denial is not a Nash equilibrium, aligning perfectly with the gold answer's reasoning."}
{"script_processing_qid": "textbook-118-16-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the model $y_{t} = \\gamma_{1} + \\gamma_{2}\\log X_{t} + u_{t}$ is considered a linear regression model despite the nonlinear transformation of $X_{t}$.\n\nGOLD_ANSWER:\nThe model is linear in the parameters $\\gamma_{1}$ and $\\gamma_{2}$, even though $X_{t}$ is transformed nonlinearly. The key feature of a linear regression model is linearity in the parameters, not necessarily in the variables.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the definition of linear regression. A model is linear if it is linear in parameters (coefficients), not necessarily linear in variables.\\nStep 2: Examine the model structure $y_t = \\gamma_1 + \\gamma_2\\log X_t + u_t$. The parameters $\\gamma_1$ and $\\gamma_2$ enter the equation in a linear, additive form.\\nStep 3: Recognize that $\\log X_t$ represents a nonlinear transformation of the variable $X_t$, but the coefficients $\\gamma_1$ and $\\gamma_2$ remain linear parameters.\\nStep 4: Contrast with genuinely nonlinear models like $y_t = e^{\\beta_1}X_{t2}^{\\beta_2} + u_t$, where parameters appear in nonlinear forms (e.g., exponents).\\nConclusion: The model is considered linear because the nonlinearity only applies to the independent variable $X_t$, not to the parameters $\\gamma_1$ and $\\gamma_2$.\n\nQID: textbook-118-16-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-16-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies that the model is linear in parameters, which is the defining feature of a linear regression model, despite the nonlinear transformation of the variable. The explanation is thorough and aligns with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-16-0-1", "category": "CORRECT", "explanation": "The candidate answer correctly identifies that the model is linear in parameters, which is the defining feature of a linear regression model, despite the nonlinear transformation of the variable. The explanation is thorough and aligns with the gold answer."}, "llm_echoed_qid": "textbook-118-16-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies that the model is linear in parameters, which is the defining feature of a linear regression model, despite the nonlinear transformation of the variable. The explanation is thorough and aligns with the gold answer."}
{"script_processing_qid": "textbook-118-8-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Why is the RRF called 'restricted' compared to the URF?\n\nGOLD_ANSWER:\n1. **Exclusion restrictions**: The RRF embeds zeros in $\\boldsymbol{\\Gamma}$ and $\\boldsymbol{B}$ from the SF, reducing the parameter space.\n2. **Cross-equation constraints**: Coefficients in $\\boldsymbol{B}\\boldsymbol{\\Gamma}^{-1}$ are linked across equations, unlike the URF's unrestricted $\\boldsymbol{\\Pi}$.\n3. **Efficiency**: The RRF is more efficient if restrictions are valid, but misspecified restrictions lead to inconsistency.\n\nCANDIDATE_ANSWER:\nStep 1: Compare the RRF and URF equations. The RRF is derived from the structural form (Y = WBΓ⁻¹ + V) where parameters are combinations of structural parameters (B and Γ). The URF (Y = WΠ + V) treats Π as unrestricted parameters with no structural constraints.\n\nStep 2: Observe parameter constraints. The RRF inherits exclusion restrictions from the structural form (e.g., zero restrictions in B or Γ), reducing the number of free parameters in Π = BΓ⁻¹. The URF imposes no such restrictions, leaving Π fully flexible.\n\nStep 3: Note identification status. When equations are under- or over-identified (not just identified), the RRF's Π parameters retain structural restrictions (e.g., cross-equation constraints), while the URF’s Π parameters remain unrestricted. This makes the RRF structurally 'restricted' compared to the URF.\n\nQID: textbook-118-8-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-8-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the key differences between RRF and URF, including parameter constraints and identification status, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-8-2-2", "category": "CORRECT", "explanation": "The candidate answer accurately explains the key differences between RRF and URF, including parameter constraints and identification status, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-118-8-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the key differences between RRF and URF, including parameter constraints and identification status, aligning well with the gold answer."}
{"script_processing_qid": "textbook-114-25-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the marginal rate of substitution (MRS) formula from the definition provided in the text. Show how it relates to the slope of the indifference curve.\n\nGOLD_ANSWER:\n1. **Definition of MRS**: The MRS of food ($F$) for clothing ($C$) is the maximum amount of clothing a consumer is willing to give up for one additional unit of food.\n2. **Mathematical Formulation**: $MRS = -\\frac{\\Delta C}{\\Delta F}$, where $\\Delta C$ is the change in clothing and $\\Delta F$ is the change in food.\n3. **Slope of Indifference Curve**: The MRS equals the absolute value of the slope of the indifference curve at any point. For example, in Figure 3.5, the slope between points A and B is $-6/1$, so $MRS = 6$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that indifference curves represent constant utility (U) levels. Along an indifference curve, the total differential of utility equals zero: dU = 0.\n\nStep 2: For a utility function U(x,y) with goods x and y, compute the total differential:\ndU = (∂U/∂x)dx + (∂U/∂y)dy = 0\n\nStep 3: Rearrange to find the slope of the indifference curve (dy/dx):\n(∂U/∂x)dx = -(∂U/∂y)dy\n=> dy/dx = -(∂U/∂x)/(∂U/∂y)\n\nStep 4: Recognize that the marginal rate of substitution (MRS) is defined as the negative of this slope (the rate at which y is substituted for x while maintaining utility):\nMRS = -dy/dx = (∂U/∂x)/(∂U/∂y)\n\nKey relationship: The MRS equals the ratio of marginal utilities (MU_x/MU_y), which defines the slope of the indifference curve. The ordinal utility framework ensures this ratio remains valid even under monotonic utility transformations.\n\nQID: textbook-114-25-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-25-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the MRS formula using the total differential of the utility function and relates it to the slope of the indifference curve, aligning with the gold answer's explanation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-25-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the MRS formula using the total differential of the utility function and relates it to the slope of the indifference curve, aligning with the gold answer's explanation."}, "llm_echoed_qid": "textbook-114-25-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the MRS formula using the total differential of the utility function and relates it to the slope of the indifference curve, aligning with the gold answer's explanation."}
{"script_processing_qid": "textbook-37-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. What is the implication of offering a wage w < 0 in this model? How does this compare to the perfect information equilibrium?\n\nGOLD_ANSWER:\n1.  **Implication**: A wage w < 0 leads to underemployment, as some workers of type θ < 0 will not work.\n2.  **Comparison**: Under perfect information, all workers of type θ ≤ 0 would work, as firms would offer wages matching their productivity. The adverse selection leads to inefficiency.\n\nCANDIDATE_ANSWER:\nStep 1: In this adverse selection model, offering w < 0 allows market clearing by balancing firm demand and worker supply. At w < 0:\\n- Only workers with θ ≤ θ' (where θ' < 0) will accept employment\\n- Firms' expected productivity E[θ|θ ≤ θ'] equals the wage w through equilibrium adjustment\\n\\nStep 2: The key implication is underemployment compared to perfect information equilibrium:\\n- Under perfect information, all workers with θ ≤ 6 would be employed at w = θ\\n- With asymmetric information, even some workers with θ < 6 (who should optimally work) remain unemployed due to the required w < 0\\n\\nStep 3: Comparison:\\nPerfect Information: Efficient allocation (all θ ≤ 6 employed)\\nAsymmetric Information: Market clears at w < 0 but with:\\n1. Lower wages than fundamental productivity\\n2. Pareto-inefficient underemployment of productive workers\\n3. Distorted worker participation (θ ≤ θ' < 6 vs θ ≤ 6)\n\nQID: textbook-37-0-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-37-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately describes the implications of offering a wage w < 0, including underemployment and comparison to the perfect information equilibrium. It aligns with the gold answer in both content and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-37-0-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately describes the implications of offering a wage w < 0, including underemployment and comparison to the perfect information equilibrium. It aligns with the gold answer in both content and reasoning."}, "llm_echoed_qid": "textbook-37-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately describes the implications of offering a wage w < 0, including underemployment and comparison to the perfect information equilibrium. It aligns with the gold answer in both content and reasoning."}
{"script_processing_qid": "textbook-122-20-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the assumptions under which fixed effects estimation is consistent.\n\nGOLD_ANSWER:\n3. **Assumptions**: Fixed effects estimation is consistent if:\n   - **Strict Exogeneity**: $E[u_{it} | union_{i1}, ..., union_{iT}, a_i] = 0$ for all $t$.\n   - **No Perfect Collinearity**: The transformed regressors $(union_{it} - \\overline{union_{i}})$ must have sufficient variation.\n   - **Time-Varying Errors**: $u_{it}$ must be uncorrelated with $union_{it}$ after conditioning on $a_i$.\n\nCANDIDATE_ANSWER:\nStep 1: Fixed effects (FE) estimation removes time-constant unobserved heterogeneity (a_i) through within-transformation (demeaning). This addresses the omitted variable bias present in pooled OLS when a_i is correlated with regressors (e.g., unionit).\n\nStep 2: For FE to be consistent, two critical assumptions must hold:\n1. **Strict exogeneity conditional on a_i**: After controlling for a_i, the idiosyncratic error term (ε_it) must be uncorrelated with all regressors (unionit and other X's) across all time periods. Formally: E[ε_it | X_i1, X_i2, ..., X_iT, a_i] = 0 for all t.\n2. **Time-varying variation**: There must be sufficient within-individual variation in the regressors (e.g., union status changes over time for some workers) to estimate parameters.\n\nStep 3: The context specifically highlights that FE eliminates bias from a_i being correlated with unionit. However, FE consistency still requires assumption (1) – that no time-varying omitted variables correlated with unionit remain in ε_it after removing a_i.\n\nQID: textbook-122-20-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-20-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the key assumptions for fixed effects estimation to be consistent, including strict exogeneity conditional on a_i and the need for time-varying variation in regressors. These align with the gold answer's requirements of strict exogeneity and no perfect collinearity.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-20-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the key assumptions for fixed effects estimation to be consistent, including strict exogeneity conditional on a_i and the need for time-varying variation in regressors. These align with the gold answer's requirements of strict exogeneity and no perfect collinearity."}, "llm_echoed_qid": "textbook-122-20-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the key assumptions for fixed effects estimation to be consistent, including strict exogeneity conditional on a_i and the need for time-varying variation in regressors. These align with the gold answer's requirements of strict exogeneity and no perfect collinearity."}
{"script_processing_qid": "textbook-113-23-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Using the three effects, derive the aggregate-demand curve mathematically.\n\nGOLD_ANSWER:\n1.  **Wealth Effect**: $C = C(Y - T, W)$, where $W = \\frac{M}{P}$ (real money balances). $P \\downarrow \\Rightarrow W \\uparrow \\Rightarrow C \\uparrow$.\n2.  **Interest-Rate Effect**: $P \\downarrow \\Rightarrow r \\downarrow \\Rightarrow I(r) \\uparrow$.\n3.  **Exchange-Rate Effect**: $P \\downarrow \\Rightarrow r \\downarrow \\Rightarrow E \\uparrow \\Rightarrow NX(\\epsilon) \\uparrow$.\n4.  **Aggregate Demand**: $AD = C + I + G + NX$. All components increase as $P \\downarrow$, yielding a downward-sloping $AD$ curve: $AD = AD(P, M, G, T, \\dots)$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the aggregate demand equation: Y = C + I + G + NX\nStep 2: Express each component as a function of price level (P):\n   - Consumption (C) decreases as P increases (wealth effect: C = C(P) where dC/dP < 0)\n   - Investment (I) decreases as P increases (interest-rate effect: I = I(P) where dI/dP < 0)\n   - Net exports (NX) decrease as P increases (exchange-rate effect: NX = NX(P) where dNX/dP < 0)\nStep 3: Combine these relationships mathematically:\n   Y = C(P) + I(P) + G + NX(P)\nStep 4: Since all three components (C, I, NX) are inversely related to P, total output Y decreases as P increases. This inverse relationship between Y and P constitutes the downward-sloping aggregate-demand curve.\n\nQID: textbook-113-23-2-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-113-23-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the aggregate-demand curve by showing how each component (C, I, NX) is inversely related to the price level (P), resulting in a downward-sloping AD curve. The reasoning aligns perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-113-23-2-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the aggregate-demand curve by showing how each component (C, I, NX) is inversely related to the price level (P), resulting in a downward-sloping AD curve. The reasoning aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-113-23-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the aggregate-demand curve by showing how each component (C, I, NX) is inversely related to the price level (P), resulting in a downward-sloping AD curve. The reasoning aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-14-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Solve the differential equation $(\\mathbf{v\\_{i}}-b(\\mathbf{v\\_{i}}))/\\overline{{\\mathbf{v}}}-b^{\\prime}(\\mathbf{v\\_{i}})\\mathbf{v\\_{i}}/\\overline{{\\mathbf{v}}}=0$ to show that the equilibrium bidding function is $b(v) = v/2$.\n\nGOLD_ANSWER:\n1.  **Rewrite the Equation**: The differential equation can be rewritten as $(v_i - b(v_i)) - b^{\\prime}(v_i)v_i = 0$.\n2.  **Solve the ODE**: This is a first-order linear ODE. Rearrange to $b^{\\prime}(v_i) + \\frac{1}{v_i}b(v_i) = 1$.\n3.  **Integrating Factor**: The integrating factor is $e^{\\int \\frac{1}{v_i} dv_i} = v_i$.\n4.  **Solution**: Multiply through by the integrating factor: $v_i b^{\\prime}(v_i) + b(v_i) = v_i$. This simplifies to $\\frac{d}{dv_i}(v_i b(v_i)) = v_i$. Integrate both sides: $v_i b(v_i) = \\frac{v_i^2}{2} + C$. For $b(0) = 0$, $C = 0$. Thus, $b(v_i) = \\frac{v_i}{2}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the given differential equation from the FOC:\n$$\\frac{\\mathbf{v_i} - b(\\mathbf{v_i})}{\\overline{\\mathbf{v}}} - \\frac{b'(\\mathbf{v_i})\\mathbf{v_i}}{\\overline{\\mathbf{v}}} = 0.$$\n\nStep 2: Multiply through by $\\overline{\\mathbf{v}}$ to eliminate denominators:\n$$(\\mathbf{v_i} - b(\\mathbf{v_i})) - b'(\\mathbf{v_i})\\mathbf{v_i} = 0.$$\n\nStep 3: Rearrange terms:\n$$\\mathbf{v_i} - b(\\mathbf{v_i}) = \\mathbf{v_i} \\cdot b'(\\mathbf{v_i}).$$\n\nStep 4: Recognize the left-hand side as the derivative of $\\mathbf{v_i} \\cdot b(\\mathbf{v_i})$ using the product rule:\n$$\\frac{d}{d\\mathbf{v_i}}[\\mathbf{v_i} \\cdot b(\\mathbf{v_i})] = \\mathbf{v_i} \\cdot b'(\\mathbf{v_i}) + b(\\mathbf{v_i}).$$\n\nStep 5: Substitute into the rearranged equation:\n$$\\mathbf{v_i} - b(\\mathbf{v_i}) = \\frac{d}{d\\mathbf{v_i}}[\\mathbf{v_i} \\cdot b(\\mathbf{v_i})] - b(\\mathbf{v_i}).$$\nSimplify to:\n$$\\mathbf{v_i} = \\frac{d}{d\\mathbf{v_i}}[\\mathbf{v_i} \\cdot b(\\mathbf{v_i})].$$\n\nStep 6: Integrate both sides with respect to $\\mathbf{v_i}$:\n$$\\int \\mathbf{v_i} \\, d\\mathbf{v_i} = \\int d[\\mathbf{v_i} \\cdot b(\\mathbf{v_i})],$$\nwhich yields:\n$$\\frac{1}{2}\\mathbf{v_i}^2 + C = \\mathbf{v_i} \\cdot b(\\mathbf{v_i}).$$\n\nStep 7: Solve for $b(\\mathbf{v_i})$:\n$$b(\\mathbf{v_i}) = \\frac{1}{2}\\mathbf{v_i} + \\frac{C}{\\mathbf{v_i}}.$$\n\nStep 8: Apply boundary condition $b(0) = 0$ (implied by uniform valuations on $[0,\\overline{\\mathbf{v}}]$). To avoid divergence at $\\mathbf{v_i}=0$, set $C=0$:\n$$b(\\mathbf{v_i}) = \\frac{\\mathbf{v_i}}{2}.$$\n\nQID: textbook-14-0-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-14-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to solve the differential equation, including rewriting, rearranging, integrating, and applying the boundary condition to arrive at the equilibrium bidding function $b(v) = v/2$.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-14-0-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to solve the differential equation, including rewriting, rearranging, integrating, and applying the boundary condition to arrive at the equilibrium bidding function $b(v) = v/2$."}, "llm_echoed_qid": "textbook-14-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to solve the differential equation, including rewriting, rearranging, integrating, and applying the boundary condition to arrive at the equilibrium bidding function $b(v) = v/2$."}
{"script_processing_qid": "textbook-13-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the limiting distribution of the Dickey-Fuller $\\rho$ statistic $T\\cdot(\\hat{\\rho}^{\\mu}-1)$ under the null hypothesis of a unit root.\n\nGOLD_ANSWER:\n1. **Regression Model**: Consider the regression $y\\_{t} = \\alpha^* + \\rho y\\_{t-1} + \\varepsilon\\_{t}$.\n2. **OLS Estimate**: The OLS estimate $\\hat{\\rho}^{\\mu}$ is biased downward under the null $\\rho = 1$.\n3. **Limiting Distribution**: $T\\cdot(\\hat{\\rho}^{\\mu}-1) \\to DF\\_{\\rho}^{\\mu} \\equiv \\frac{\\frac{1}{2}\\left([W^{\\mu}(1)]^2 - [W^{\\mu}(0)]^2 - 1\\right)}{\\int_0^1 [W^{\\mu}(r)]^2 dr}$, where $W^{\\mu}(\\cdot)$ is a demeaned Wiener process.\n\nCANDIDATE_ANSWER:\nStep 1: Under H₀ (ρ = 1), the true DGP is a driftless random walk: yₜ = yₜ₋₁ + εₜ, with y₀ = α + z₀. The Dickey-Fuller regression includes a constant: yₜ = μ + ρyₜ₋₁ + εₜ.\n\nStep 2: The OLS estimator deviation (ρ̂^μ - 1) can be expressed as:\n  (∑yₜ₋₁εₜ) / (∑yₜ₋₁² - T⁻¹(∑yₜ₋₁)²)\n\nStep 3: Scale numerator and denominator by T. Under H₀, partial sums of εₜ behave like a Wiener process W(r). Using FCLT and CMT:\n  - T⁻¹∑yₜ₋₁εₜ ⇒ ½[W(1)² - 1] - W(1)∫₀¹W(r)dr\n  - T⁻²∑yₜ₋₁² ⇒ ∫₀¹W(r)²dr\n  - T⁻³(∑yₜ₋₁)² ⇒ [∫₀¹W(r)dr]²\n\nStep 4: Combine scaled terms. The limiting distribution becomes:\n  [½(W(1)² - 1) - W(1)∫₀¹W(r)dr] / [∫₀¹W(r)²dr - (∫₀¹W(r)dr)²]\n\nStep 5: This matches the Dickey-Fuller ρ distribution (DF_ρ) for models with a constant (demeaned Wiener process), as noted in Proposition 9.3. Critical values for T→∞ in Table 9.1 correspond to this non-standard distribution.\n\nQID: textbook-13-0-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-13-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the limiting distribution of the Dickey-Fuller ρ statistic under the null hypothesis of a unit root, matching the gold answer's description of the distribution involving a demeaned Wiener process.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-13-0-1-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the limiting distribution of the Dickey-Fuller ρ statistic under the null hypothesis of a unit root, matching the gold answer's description of the distribution involving a demeaned Wiener process."}, "llm_echoed_qid": "textbook-13-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the limiting distribution of the Dickey-Fuller ρ statistic under the null hypothesis of a unit root, matching the gold answer's description of the distribution involving a demeaned Wiener process."}
{"script_processing_qid": "textbook-9-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the fixed-effects estimator using the Frisch-Waugh Theorem, showing that it can be written as $(\\mathbf{F}^{\\prime}\\mathbf{M}_{\\mathbf{D}}\\mathbf{F})^{-1}\\mathbf{F}^{\\prime}\\mathbf{M}_{\\mathbf{D}}\\mathbf{y}$.\n\nGOLD_ANSWER:\n1. **First Step**: Regress $\\mathbf{y}$ on $\\mathbf{D}$ to obtain residuals $\\mathbf{M}_{\\mathbf{D}}\\mathbf{y}$.\n2. **Second Step**: Regress each column of $\\mathbf{F}$ on $\\mathbf{D}$ to obtain residuals $\\mathbf{M}_{\\mathbf{D}}\\mathbf{F}$.\n3. **Final Step**: Regress $\\mathbf{M}_{\\mathbf{D}}\\mathbf{y}$ on $\\mathbf{M}_{\\mathbf{D}}\\mathbf{F}$ to obtain the fixed-effects estimator $\\widehat{\\pmb{\\beta}} = (\\mathbf{F}^{\\prime}\\mathbf{M}_{\\mathbf{D}}\\mathbf{F})^{-1}\\mathbf{F}^{\\prime}\\mathbf{M}_{\\mathbf{D}}\\mathbf{y}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the Frisch-Waugh Theorem for partitioned regressions. When regressing y on [W1 W2], the estimator for W2's coefficients can be obtained by first regressing y on W1 to get residuals (M_{W1}y), then regressing those residuals on M_{W1}W2 (residuals of W2 after regressing on W1).\n\nStep 2: In this model, W = [D F] where D are dummy variables. The OLS estimator for θ = [α; β] is (W'W)^{-1}W'y. However, the fixed-effects estimator focuses on β.\n\nStep 3: Apply Frisch-Waugh to isolate β:\n- First residualize y and F by projecting out D: M_Dy = residualized y, M_DF = residualized F.\n- Then regress M_Dy on M_DF. The coefficient estimator becomes (F'M_DF)^{-1}F'M_Dy.\n\nStep 4: Verify dimensions: M_D (Mn×Mn) annihilates D's variation. F (Mn×dim(β)) and y (Mn×1) are properly transformed. The inverse (F'M_DF)^{-1} exists under standard rank conditions.\n\nStep 5: Conclusion: The Frisch-Waugh approach confirms the fixed-effects estimator for β is (F'M_DF)^{-1}F'M_Dy as required.\n\nQID: textbook-9-0-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-9-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly applies the Frisch-Waugh Theorem to derive the fixed-effects estimator, matching the steps and final expression provided in the gold answer. The explanation is detailed and aligns with the theoretical framework.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-9-0-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly applies the Frisch-Waugh Theorem to derive the fixed-effects estimator, matching the steps and final expression provided in the gold answer. The explanation is detailed and aligns with the theoretical framework."}, "llm_echoed_qid": "textbook-9-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly applies the Frisch-Waugh Theorem to derive the fixed-effects estimator, matching the steps and final expression provided in the gold answer. The explanation is detailed and aligns with the theoretical framework."}
{"script_processing_qid": "textbook-114-31-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze the impact of Kodak's entry into the instant camera market on Polaroid's pricing strategy and profitability.\n\nGOLD_ANSWER:\nKodak's entry reduced Polaroid's monopoly power, affecting its pricing strategy:\n1. **Reduced Market Power**: Polaroid could no longer set prices as high due to competition.\n2. **Price Adjustments**: Polaroid lowered camera prices to remain competitive, as demand became more elastic.\n3. **Film Pricing**: Film prices likely decreased, but Polaroid retained some monopoly power due to compatibility issues.\n4. **Profit Impact**: Overall profitability declined as Polaroid could extract less consumer surplus, but it still benefited from its film monopoly to some extent.\n\nCANDIDATE_ANSWER:\nStep 1: Understand Polaroid's monopoly structure pre-Kodak entry.  \nPolaroid had a monopoly on both cameras (priced at $T$) and film (priced at $P$), with film sales ($Q$) tied to camera sales ($n$), since film was only usable in Polaroid cameras. Profit was maximized via a two-part tariff: the camera acted as an upfront 'entry fee,' and film sales generated recurring revenue.  \n\nStep 2: Assess Kodak's entry into the camera market.  \nKodak’s entry broke Polaroid’s camera monopoly, introducing competition in the camera market. However, Polaroid retained its film monopoly, as Kodak cameras could not use Polaroid film.  \n\nStep 3: Impact on Polaroid’s pricing strategy.  \n- Camera price ($T$): Polaroid likely reduced $T$ to remain competitive with Kodak, as camera demand became price-sensitive.  \n- Film price ($P$): To compensate for lower camera revenue, Polaroid would increase $P$ to extract more profit from its film monopoly (similar to the 'razor-and-blades' model).  \n\nStep 4: Impact on profitability.  \n- Reduced camera sales ($n$) and lower $T$ decreased revenue from cameras.  \n- Higher $P$ increased film revenue per unit, but total film sales ($Q$) depend on $n$ (since $Q = n \times$ film usage per camera). If $n$ fell significantly due to Kodak’s competition, the gain from higher $P$ might not offset losses from lower $n$ and $T$, reducing overall profit $\\pi$.  \n\nStep 5: Conclusion.  \nKodak’s entry forced Polaroid to shift profitability reliance from cameras to film. However, the interdependence of $n$ and $Q$ in the profit equation $\\pi = P Q + n T - C_1(Q) - C_2(n)$ meant that reduced camera sales likely eroded profitability despite higher film prices.\n\nQID: textbook-114-31-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-31-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly analyzes the impact of Kodak's entry on Polaroid's pricing strategy and profitability, aligning with the gold answer's key points. The detailed reasoning about the shift from camera to film profitability and the interdependence of sales components is accurate and well-supported.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-31-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly analyzes the impact of Kodak's entry on Polaroid's pricing strategy and profitability, aligning with the gold answer's key points. The detailed reasoning about the shift from camera to film profitability and the interdependence of sales components is accurate and well-supported."}, "llm_echoed_qid": "textbook-114-31-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly analyzes the impact of Kodak's entry on Polaroid's pricing strategy and profitability, aligning with the gold answer's key points. The detailed reasoning about the shift from camera to film profitability and the interdependence of sales components is accurate and well-supported."}
{"script_processing_qid": "textbook-94-4-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the elasticity of substitution ($\\sigma$) for the Cobb-Douglas production function and explain why it equals 1.\n\nGOLD_ANSWER:\n1. **Given**: $MRTS = -\\frac{a}{1-a}\\frac{x_2}{x_1}$ and $\\frac{x_2}{x_1} = -\\frac{1-a}{a}MRTS$.\n2. **Logarithmic Transformation**: $\\ln\\left(\\frac{x_2}{x_1}\\right) = \\ln\\left(\\frac{1-a}{a}\\right) + \\ln|MRTS|$.\n3. **Elasticity Definition**: $\\sigma = \\frac{d\\ln(x_2/x_1)}{d\\ln|MRTS|} = 1$ (derivative of the log equation with respect to $\\ln|MRTS|$).\n4. **Interpretation**: The Cobb-Douglas function has a unitary elasticity of substitution, meaning the ratio of inputs changes proportionally with the MRTS.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the Cobb-Douglas MRTS expression provided in context: \n$$M R T S=-\\frac{a}{1-a}\\frac{x_{2}}{x_{1}}.$$ \n\nStep 2: Solve for the input ratio $\\frac{x_2}{x_1}$:\n$$\\frac{x_2}{x_1} = -\\frac{1-a}{a} M R T S.$$\n\nStep 3: Take natural logarithms of both sides:\n$$\\ln\\left(\\frac{x_2}{x_1}\\right) = \\ln\\left(\\frac{1-a}{a}\\right) + \\ln|M R T S|.$$\n\nStep 4: Apply the definition of elasticity of substitution:\n$$\\sigma = \\frac{d\\ln(x_2/x_1)}{d\\ln|M R T S|}.$$\n\nStep 5: Differentiate the logarithmic expression. The term $\\ln\\left(\\frac{1-a}{a}\\right)$ is a constant with respect to $\\ln|MRTS|$, so its derivative is 0. The derivative of $\\ln|MRTS|$ with respect to itself is 1.\n$$\\sigma = 0 + 1 = 1.$$\n\nExplanation: The Cobb-Douglas production function has an elasticity of substitution of 1 because the relationship between $\\ln(x_2/x_1)$ and $\\ln|MRTS|$ is linear with a slope of 1. This implies a constant proportional response of the input ratio to changes in MRTS.\n\nQID: textbook-94-4-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-94-4-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the elasticity of substitution for the Cobb-Douglas production function and explains why it equals 1, matching the gold answer in both steps and reasoning.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-94-4-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the elasticity of substitution for the Cobb-Douglas production function and explains why it equals 1, matching the gold answer in both steps and reasoning."}, "llm_echoed_qid": "textbook-94-4-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the elasticity of substitution for the Cobb-Douglas production function and explains why it equals 1, matching the gold answer in both steps and reasoning."}
{"script_processing_qid": "textbook-119-32-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the savings-income regression example, compute the $F$ statistic for the Chow test and interpret the results.\n\nGOLD_ANSWER:\n1. **Given**: $\\mathrm{RSS_{R}}=23,248.30$, $\\mathrm{RSS_{UR}}=11,790.252$, $k=2$, $n_{1}+n_{2}-2k=22$.\n2. **Compute $F$**: $$F=\\frac{(23,248.30-11,790.252)/2}{(11,790.252)/22}=10.69$$.\n3. **Critical Value**: For $F_{0.01}(2,22)=5.72$, the computed $F$ exceeds the critical value.\n4. **Conclusion**: Reject $H_{0}$; there is evidence of structural change in the savings-income relationship.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the required values from the context.\n- RSS_R (restricted) = 23,248.30\n- RSS_UR (unrestricted) = 11,790.252\n- k = 2 parameters\n- n1 + n2 - 2k = 22 (degrees of freedom for denominator)\n\nStep 2: Compute the F statistic using the Chow test formula:\n$$F = \\frac{(\\mathrm{RSS_R} - \\mathrm{RSS_{UR}})/k}{\\mathrm{RSS_{UR}}/(n_1 + n_2 - 2k)}$$\n\nStep 3: Substitute values into the formula:\n$$F = \\frac{(23,248.30 - 11,790.252)/2}{11,790.252/22}$$\n\nStep 4: Calculate numerator and denominator:\n- Numerator: (11,458.048)/2 = 5,729.024\n- Denominator: 11,790.252/22 ≈ 535.9205\n\nStep 5: Final F value:\n$$F = \\frac{5,729.024}{535.9205} ≈ 10.69$$\n\nInterpretation: The computed F statistic is 10.69 with 2 (numerator) and 22 (denominator) degrees of freedom. This value exceeds typical critical F values at standard significance levels (e.g., 3.44 for α = 0.05), leading to rejection of the null hypothesis. This indicates a structural change in the savings-income regression model across the two subperiods.\n\nQID: textbook-119-32-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-32-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly computes the F statistic as 10.69, matches the gold answer's calculation, and provides a valid interpretation of the results, including the rejection of the null hypothesis due to structural change.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-32-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly computes the F statistic as 10.69, matches the gold answer's calculation, and provides a valid interpretation of the results, including the rejection of the null hypothesis due to structural change."}, "llm_echoed_qid": "textbook-119-32-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly computes the F statistic as 10.69, matches the gold answer's calculation, and provides a valid interpretation of the results, including the rejection of the null hypothesis due to structural change."}
{"script_processing_qid": "textbook-122-33-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the approximation $\\log(x_{1}) - \\log(x_{0}) \\approx (x_{1} - x_{0})/x_{0}$ for small changes in $x$ using calculus.\n\nGOLD_ANSWER:\n1. **Objective**: Approximate $\\log(x_{1}) - \\log(x_{0})$ for small $\\Delta x = x_{1} - x_{0}$.\n2. **Taylor Expansion**: The first-order Taylor expansion of $\\log(x)$ around $x_{0}$ is:\n   $$ \\log(x) \\approx \\log(x_{0}) + \\frac{1}{x_{0}}(x - x_{0}). $$\n3. **Evaluate at $x_{1}$**:\n   $$ \\log(x_{1}) \\approx \\log(x_{0}) + \\frac{1}{x_{0}}(x_{1} - x_{0}). $$\n4. **Rearrange**:\n   $$ \\log(x_{1}) - \\log(x_{0}) \\approx \\frac{x_{1} - x_{0}}{x_{0}}. $$\n5. **Conclusion**: For small $\\Delta x$, the difference in logs approximates the proportionate change in $x$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that the derivative of the natural logarithm function is d/dx [log(x)] = 1/x.\nStep 2: Use a first-order Taylor approximation around x₀ for log(x₁):\n   log(x₁) ≈ log(x₀) + (1/x₀)(x₁ - x₀)\nStep 3: Subtract log(x₀) from both sides:\n   log(x₁) - log(x₀) ≈ (x₁ - x₀)/x₀\nStep 4: This shows that for small changes (Δx = x₁ - x₀), the log difference approximates the relative change Δx/x₀.\n\nQID: textbook-122-33-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-33-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps of using a first-order Taylor expansion to derive the approximation, matching the gold answer's reasoning and conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-33-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps of using a first-order Taylor expansion to derive the approximation, matching the gold answer's reasoning and conclusion."}, "llm_echoed_qid": "textbook-122-33-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps of using a first-order Taylor expansion to derive the approximation, matching the gold answer's reasoning and conclusion."}
{"script_processing_qid": "textbook-108-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss how the introduction of new goods and unmeasured quality changes affect the accuracy of the CPI.\n\nGOLD_ANSWER:\n1. **New Goods**: The introduction of new goods (e.g., VCRs) increases consumer variety and effectively lowers the cost of maintaining a standard of living. However, the CPI does not immediately include new goods, so it fails to reflect this improvement.\n\n2. **Unmeasured Quality Changes**: If the quality of a good improves (e.g., a car with better mileage), the value of a dollar increases because consumers get more for the same price. Conversely, quality deterioration reduces the dollar's value. The BLS attempts to adjust for quality changes, but these adjustments are often imperfect due to the difficulty of measuring quality.\n\nCANDIDATE_ANSWER:\nStep 1: Impact of New Goods\n- When new goods are introduced, consumers gain access to greater variety, increasing their purchasing power (each dollar becomes more valuable).\n- However, CPI calculations typically lag in incorporating new goods into the basket. This delay means the CPI fails to capture the welfare improvement from new options, resulting in an overstatement of the cost of living.\n\nStep 2: Impact of Unmeasured Quality Changes\n- If product quality improves without a price increase, the CPI does not adjust for the enhanced value, leading to an overestimation of inflation (since consumers get more utility per dollar).\n- Conversely, if quality deteriorates while prices remain constant, the CPI underestimates inflation (consumers pay the same price for inferior goods, reducing their real purchasing power).\n\nStep 3: Combined Effect\n- Both factors create upward bias in CPI-measured inflation: New goods and quality improvements are inadequately reflected, making the index less accurate as a cost-of-living measure.\n\nQID: textbook-108-1-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-108-1-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately addresses both the impact of new goods and unmeasured quality changes on CPI, aligning closely with the gold answer in content and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-1-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately addresses both the impact of new goods and unmeasured quality changes on CPI, aligning closely with the gold answer in content and reasoning."}, "llm_echoed_qid": "textbook-108-1-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately addresses both the impact of new goods and unmeasured quality changes on CPI, aligning closely with the gold answer in content and reasoning."}
{"script_processing_qid": "textbook-120-24-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Show that the sequence $x_n = \\sum_{t=0}^n \\alpha^t$ converges to $\\frac{1}{1-\\alpha}$ when $|\\alpha| < 1$.\n\nGOLD_ANSWER:\n1. **Partial Sum**: $x_n = \\frac{1 - \\alpha^{n+1}}{1 - \\alpha}$.\n2. **Limit**: As $n \\to \\infty$, $\\alpha^{n+1} \\to 0$ by Lemma 3.5.1.\n3. **Conclusion**: $x_n \\to \\frac{1}{1 - \\alpha}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the finite geometric series formula. For \\( x_n = \\sum_{t=0}^n \\alpha^t \\), the closed-form expression is \\( x_n = \\frac{1 - \\alpha^{n+1}}{1 - \\alpha} \\).\n\nStep 2: Apply Lemma 3.5.1. Since \\( |\\alpha| < 1 \\), \\( \\alpha^{n+1} \\to 0 \\) as \\( n \\to \\infty \\).\n\nStep 3: Take the limit of \\( x_n \\):\n\\[ \\lim_{n \\to \\infty} x_n = \\lim_{n \\to \\infty} \\frac{1 - \\alpha^{n+1}}{1 - \\alpha} = \\frac{1 - 0}{1 - \\alpha} = \\frac{1}{1 - \\alpha} \\].\n\nConclusion: By the geometric series formula and Lemma 3.5.1, \\( x_n \\) converges to \\( \\frac{1}{1 - \\alpha} \\).\n\nQID: textbook-120-24-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-120-24-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly follows the steps of the gold answer, including the use of the geometric series formula and Lemma 3.5.1 to show convergence to the desired limit.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-120-24-1-1", "category": "CORRECT", "explanation": "The candidate answer correctly follows the steps of the gold answer, including the use of the geometric series formula and Lemma 3.5.1 to show convergence to the desired limit."}, "llm_echoed_qid": "textbook-120-24-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly follows the steps of the gold answer, including the use of the geometric series formula and Lemma 3.5.1 to show convergence to the desired limit."}
{"script_processing_qid": "textbook-114-4-3-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare and contrast private goods, public goods, and common property resources.\n\nGOLD_ANSWER:\n1. **Private Goods**: Excludable and rival (e.g., food).\n2. **Public Goods**: Non-excludable and non-rival (e.g., national defense).\n3. **Common Property**: Non-excludable but rival (e.g., fisheries), leading to overuse.\n\nCANDIDATE_ANSWER:\nStep 1: Define key attributes\n- **Excludability**: Whether individuals can be prevented from using the good.\n- **Rivalry**: Whether one person's use diminishes availability for others.\n\nStep 2: Compare characteristics\n- **Private goods**:\n  - Excludable (e.g., purchased through markets)\n  - Rival (consumption reduces quantity available)\n  - No market failure (efficiently allocated in competitive markets)\n\n- **Public goods**:\n  - Non-excludable (cannot prevent free riders, per 'nonexclusive goods' in context)\n  - Nonrival (use by one doesn’t reduce availability, per 'nonrival goods')\n  - Market failure occurs (due to free riders and underprovision, per '651-652')\n\n- **Common property resources**:\n  - Non-excludable (similar to public goods)\n  - Rival (overuse depletes resource, e.g., fish stocks)\n  - Market failure due to negative externalities (e.g., overconsumption/tragedy of the commons, per '645-646')\n\nStep 3: Contrast outcomes & solutions\n- Private goods: Efficiently provided by markets.\n- Public goods: Require government provision (due to free riders and market underprovision, per '654-656').\n- Common property resources: Require regulation/property rights (to address overuse, per '641-645').\n\nQID: textbook-114-4-3-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-4-3-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly defines and contrasts private goods, public goods, and common property resources, aligning perfectly with the gold answer. The detailed reasoning and examples provided enhance the clarity and accuracy of the response.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-4-3-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly defines and contrasts private goods, public goods, and common property resources, aligning perfectly with the gold answer. The detailed reasoning and examples provided enhance the clarity and accuracy of the response."}, "llm_echoed_qid": "textbook-114-4-3-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly defines and contrasts private goods, public goods, and common property resources, aligning perfectly with the gold answer. The detailed reasoning and examples provided enhance the clarity and accuracy of the response."}
{"script_processing_qid": "textbook-122-11-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain how the F-distribution is used to test the joint significance of multiple regression coefficients.\n\nGOLD_ANSWER:\n1. **SSR Comparison**: The F-statistic compares restricted and unrestricted sum of squared residuals (SSR) in regression.\n2. **Distribution**: Under null, $F = [(SSR_r - SSR_{ur})/q] / [SSR_{ur}/(n - k - 1)] \\sim F_{q,n-k-1}$, where $q$ is the number of restrictions.\n3. **Decision Rule**: Reject null if $F > F_{\\alpha,q,n-k-1}$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the F-statistic structure. In regression joint hypothesis testing, the F-statistic is constructed as a ratio of two scaled chi-square variables:\n\nF = [(SSR_restricted - SSR_unrestricted)/q] / [SSR_unrestricted/(n - k - 1)]\n\nwhere:\n- SSR = sum of squared residuals\n- q = number of restrictions (coefficients being tested)\n- k = total number of predictors in unrestricted model\n- n = sample size\n\nStep 2: Recognize the distributional basis. Under the null hypothesis (all tested coefficients = 0):\n1. The numerator (SSR_diff/q) follows χ²_q / q\n2. The denominator (unrestricted error variance) follows χ²_{n-k-1}/(n-k-1)\n\nStep 3: Apply the F-distribution definition from context. Since these components are independent chi-square variables divided by their degrees of freedom, their ratio follows F_{q, n-k-1}.\n\nStep 4: Perform hypothesis testing. Compare the calculated F-statistic to critical values from F_{q, n-k-1} distribution:\n- If F > F_critical, reject null hypothesis\n- This indicates at least one tested coefficient is statistically significant\n\nThe F-test simultaneously evaluates whether multiple coefficients jointly differ from zero, controlling for Type I error rates better than multiple t-tests.\n\nQID: textbook-122-11-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-11-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the construction of the F-statistic, its distribution under the null hypothesis, and the decision rule for joint significance testing, matching the gold answer's key points with additional clarity and detail.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-11-2-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the construction of the F-statistic, its distribution under the null hypothesis, and the decision rule for joint significance testing, matching the gold answer's key points with additional clarity and detail."}, "llm_echoed_qid": "textbook-122-11-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the construction of the F-statistic, its distribution under the null hypothesis, and the decision rule for joint significance testing, matching the gold answer's key points with additional clarity and detail."}
{"script_processing_qid": "textbook-108-9-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. How does the Fed use open-market operations to influence the money supply? Provide a step-by-step explanation.\n\nGOLD_ANSWER:\n1.  **Increase Money Supply**: The Fed buys government bonds from the public, injecting dollars into the economy.\n2.  **Decrease Money Supply**: The Fed sells government bonds to the public, removing dollars from circulation.\n\nCANDIDATE_ANSWER:\nStep 1: The Federal Reserve decides to either expand or contract the money supply based on economic objectives. For open-market operations, this involves buying or selling U.S. government bonds.\n\nStep 2: When the Fed wants to INCREASE the money supply: \n- It purchases government bonds from banks or the public using newly created money. \n- Banks receive cash in exchange for bonds, increasing their reserves. \n- With more reserves, banks can make more loans, which expands the money supply through the fractional-reserve banking multiplier effect.\n\nStep 3: When the Fed wants to DECREASE the money supply:\n- It sells government bonds from its portfolio to banks or the public. \n- Buyers pay with existing money from bank accounts. \n- Banks lose reserves, reducing their ability to make loans. \n- This contraction of reserves leads to a reduction in the money supply via the same multiplier effect in reverse.\n\nStep 4: The Federal Open Market Committee (FOMC) oversees these transactions, which are executed daily by the Fed's trading desk at the New York Federal Reserve Bank.\n\nQID: textbook-108-9-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-108-9-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the process of open-market operations, including the steps for increasing and decreasing the money supply, and the role of the FOMC. It aligns with the gold answer while providing additional detail.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-9-1-2", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the process of open-market operations, including the steps for increasing and decreasing the money supply, and the role of the FOMC. It aligns with the gold answer while providing additional detail."}, "llm_echoed_qid": "textbook-108-9-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the process of open-market operations, including the steps for increasing and decreasing the money supply, and the role of the FOMC. It aligns with the gold answer while providing additional detail."}
{"script_processing_qid": "textbook-106-8-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the supply schedule in Table 4-4, calculate the elasticity of supply when the price rises from $\\$1.00$ to $\\$1.50$. Interpret the result.\n\nGOLD_ANSWER:\n1.  **Percentage change in quantity supplied**: (2 - 1) / 1 * 100 = 100%.\n2.  **Percentage change in price**: (1.50 - 1.00) / 1.00 * 100 = 50%.\n3.  **Elasticity of supply**: 100% / 50% = 2.\n4.  **Interpretation**: The supply is elastic (elasticity > 1), meaning the quantity supplied is highly responsive to price changes.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain the specific numerical values from Table 4-4 required to calculate the elasticity of supply. While the context mentions Ben’s supply schedule and describes the general relationship between price and quantity supplied (e.g., quantity increases as price rises), it does not include the exact quantities supplied at $1.00 and $1.50. Elasticity calculations require these values to compute the percentage changes in quantity and price. Without the table data, the calculation cannot be performed.\n\nQID: textbook-106-8-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-106-8-0-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it requires specific numerical values from Table 4-4 which are not provided.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-8-0-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it requires specific numerical values from Table 4-4 which are not provided."}, "llm_echoed_qid": "textbook-106-8-0-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it requires specific numerical values from Table 4-4 which are not provided."}
{"script_processing_qid": "textbook-3-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain the Weak Axiom of Revealed Preference (WARP) and its implications for choice behavior.\n\nGOLD_ANSWER:\n1. **WARP**: If x is chosen over y in some budget set, then y cannot be chosen over x in any other budget set where both are available.\n2. **Implications**: WARP ensures consistency in choices. If violated, it implies that the individual's choices cannot be rationalized by a stable preference relation.\n3. **Example**: If C({x, y}) = {x} and C({x, y, z}) = {y}, then WARP is violated because x is chosen over y in one set but y is chosen over x in another.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain enough information to answer this question. While the text discusses preference relations (>, ≥), transitivity, and utility representation, it does not explicitly mention or define the Weak Axiom of Revealed Preference (WARP) or its implications for choice behavior. The passages focus on proving properties of preference orderings (e.g., transitivity) and utility function existence, but no explicit connection to revealed preference theory or WARP is made in the given text.\n\nQID: textbook-3-0-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-3-0-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates that the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-3-0-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates that the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}, "llm_echoed_qid": "textbook-3-0-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates that the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}
{"script_processing_qid": "textbook-122-34-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Show that the AR(1) model $y_t = \\beta_0 + \\beta_1 y_{t-1} + u_t$ is dynamically complete under the assumption $\\mathrm{E}(u_t|y_{t-1}, y_{t-2}, \\dots) = 0$.\n\nGOLD_ANSWER:\n3. **AR(1) Dynamic Completeness**: Given $\\mathrm{E}(u_t|y_{t-1}, y_{t-2}, \\dots) = 0$, the conditional expectation is $\\mathrm{E}(y_t|y_{t-1}, y_{t-2}, \\dots) = \\beta_0 + \\beta_1 y_{t-1}$. This shows that only one lag of $y$ is needed, making the model dynamically complete. The errors $u_t$ are serially uncorrelated.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that a model is dynamically complete if its error term (ut) satisfies E(ut|It−1)=0, where It−1 represents all information available at t−1 (including past y values).\\n\\nStep 2: For the AR(1) model yt=β0+β1yt−1+ut, the assumption E(ut|yt−1,yt−2,...)=0 implies ut is unpredictable using any past information contained in {yt−1,yt−2,...}.\\n\\nStep 3: Since the model includes only yt−1 as a regressor and ut is unpredictable given the entire history {yt−1,yt−2,...}, no additional lags of yt would help predict yt when conditioned on yt−1.\\n\\nStep 4: This satisfies the formal definition of dynamic completeness: E(yt|yt−1,yt−2,...) = E(yt|yt−1) = β0+β1yt−1, meaning the model fully captures the conditional expectation structure.\\n\\nStep 5: Therefore, under the given assumption, the AR(1) specification contains all relevant past information needed to model yt, making it dynamically complete.\n\nQID: textbook-122-34-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-34-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains the concept of dynamic completeness in the context of the AR(1) model. It accurately uses the given assumption to show that the model fully captures the conditional expectation structure, making it dynamically complete. The reasoning aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-34-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly explains the concept of dynamic completeness in the context of the AR(1) model. It accurately uses the given assumption to show that the model fully captures the conditional expectation structure, making it dynamically complete. The reasoning aligns with the gold answer."}, "llm_echoed_qid": "textbook-122-34-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains the concept of dynamic completeness in the context of the AR(1) model. It accurately uses the given assumption to show that the model fully captures the conditional expectation structure, making it dynamically complete. The reasoning aligns with the gold answer."}
{"script_processing_qid": "textbook-51-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the deflationary prospect posed by the gold standard in a growing world economy. How does this relate to the exhaustibility of gold as a resource?\n\nGOLD_ANSWER:\n1.  **Deflationary Pressure**: As the economy grows, the demand for money increases. If the money supply is tied to gold (a finite resource), the supply of money may not keep pace with demand, leading to deflation.\n2.  **Exhaustibility**: Gold is durable and exhaustible, meaning its supply grows slowly. This rigidity contrasts with fiat money, which can be expanded to meet growing demand.\n3.  **Historical Context**: The deflationary tendency of the gold standard was a key reason for its abandonment, as it exacerbated economic downturns (e.g., the Great Depression).\n\nCANDIDATE_ANSWER:\nStep 1: The gold standard ties the money supply to a finite stock of gold, a durable but exhaustible resource. In a growing economy, the production of goods/services increases over time.\n\nStep 2: Economic growth requires a proportional expansion of the money supply to maintain price stability. However, gold reserves grow slowly due to mining constraints and physical scarcity, making gold supply inelastic.\n\nStep 3: With fixed/limited gold reserves, the money supply cannot expand adequately to match growing economic output. This creates excess demand for money relative to goods, leading to deflation (declining prices).\n\nStep 4: Gold’s exhaustibility exacerbates this issue – its finite nature prevents sufficient long-term supply growth to keep pace with economic expansion, making deflation an inherent structural risk under the gold standard.\n\nQID: textbook-51-0-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-51-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the deflationary pressure caused by the gold standard in a growing economy, highlighting the inelasticity of gold supply and its exhaustibility. The reasoning aligns perfectly with the gold answer, covering all key points.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-51-0-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the deflationary pressure caused by the gold standard in a growing economy, highlighting the inelasticity of gold supply and its exhaustibility. The reasoning aligns perfectly with the gold answer, covering all key points."}, "llm_echoed_qid": "textbook-51-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the deflationary pressure caused by the gold standard in a growing economy, highlighting the inelasticity of gold supply and its exhaustibility. The reasoning aligns perfectly with the gold answer, covering all key points."}
{"script_processing_qid": "textbook-108-7-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Given a linear demand curve, why does elasticity vary along the curve even though the slope is constant?\n\nGOLD_ANSWER:\nElasticity varies because it is the ratio of percentage changes in quantity and price, not the ratio of absolute changes (slope). At higher prices and lower quantities, a given percentage change in price corresponds to a larger percentage change in quantity, making demand more elastic. Conversely, at lower prices and higher quantities, the same percentage change in price corresponds to a smaller percentage change in quantity, making demand less elastic.\n\nCANDIDATE_ANSWER:\nStep 1: Recall elasticity formula using midpoint method:\nElasticity = [(Q₂ - Q₁)/Midpoint(Q)] ÷ [(P₂ - P₁)/Midpoint(P)]\n\nStep 2: Observe that while the slope (ΔP/ΔQ) remains constant in a linear demand curve, the ratios (ΔQ/Midpoint(Q)) and (ΔP/Midpoint(P)) change along the curve:\n- At high prices (low quantities): ΔQ/Midpoint(Q) is large (low base Q) while ΔP/Midpoint(P) is small (high base P)\n- At low prices (high quantities): ΔQ/Midpoint(Q) is small (high base Q) while ΔP/Midpoint(P) is large (low base P)\n\nStep 3: Demonstrate with example:\nIf price decreases from $10→$8 (Midpoint P = $9):\n%ΔP = (2/9) ≈ 22%\nIf quantity increases from 10→20 (Midpoint Q = 15):\n%ΔQ = (10/15) ≈ 67%\nElasticity ≈ 67/22 ≈ 3.0 (elastic)\n\nAt lower price $4→$2 (Midpoint P = $3):\n%ΔP = (2/3) ≈ 67%\nQuantity increases from 40→50 (Midpoint Q = 45):\n%ΔQ = (10/45) ≈ 22%\nElasticity ≈ 22/67 ≈ 0.33 (inelastic)\n\nStep 4: Conclusion: The varying weight of price/quantity changes relative to their midpoints causes elasticity to change along a linear curve, even with constant slope.\n\nQID: textbook-108-7-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-108-7-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly explains why elasticity varies along a linear demand curve by using the midpoint method and demonstrating with examples. This aligns with the gold answer, which also emphasizes the role of percentage changes in price and quantity.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-7-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly explains why elasticity varies along a linear demand curve by using the midpoint method and demonstrating with examples. This aligns with the gold answer, which also emphasizes the role of percentage changes in price and quantity."}, "llm_echoed_qid": "textbook-108-7-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly explains why elasticity varies along a linear demand curve by using the midpoint method and demonstrating with examples. This aligns with the gold answer, which also emphasizes the role of percentage changes in price and quantity."}
{"script_processing_qid": "textbook-108-20-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n6. Provide an example of two goods with a negative cross-price elasticity and explain the economic intuition.\n\nGOLD_ANSWER:\n**Example**: Computers and software.\n   - **Intuition**: A price increase in computers reduces demand for software because they are used together. The negative cross-price elasticity reflects this complementary relationship.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the cross-price elasticity formula and its interpretation.\nCross-price elasticity (CPE) = (% change in quantity demanded of good 1) / (% change in price of good 2). A negative CPE indicates the goods are complements.\n\nStep 2: Select complementary goods where increased price of one reduces demand for the other.\nExample: Printers and ink cartridges.\n\nStep 3: Explain the relationship.\nIf the price of printers (good 2) increases, fewer printers are purchased. Since printers and ink are used together, demand for ink cartridges (good 1) decreases.\n\nEconomic intuition: Complements are consumed jointly. A price increase for one raises the effective cost of the bundle, reducing consumption of both goods. This creates an inverse relationship reflected in the negative CPE.\n\nQID: textbook-108-20-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-108-20-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly identifies complementary goods (printers and ink cartridges) and provides a clear explanation of the negative cross-price elasticity, matching the economic intuition described in the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-20-2-1", "category": "CORRECT", "explanation": "The candidate correctly identifies complementary goods (printers and ink cartridges) and provides a clear explanation of the negative cross-price elasticity, matching the economic intuition described in the gold answer."}, "llm_echoed_qid": "textbook-108-20-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly identifies complementary goods (printers and ink cartridges) and provides a clear explanation of the negative cross-price elasticity, matching the economic intuition described in the gold answer."}
{"script_processing_qid": "textbook-114-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Explain why coffee prices spike immediately after a freeze in Brazil but return to prefreeze levels within a few years. Discuss the roles of short-run and long-run supply elasticities.\n\nGOLD_ANSWER:\n1. **Short-run**: Supply is inelastic immediately after a freeze due to fixed coffee bean quantities. Demand is also inelastic, leading to sharp price increases.\n2. **Intermediate-run**: Supply becomes more elastic as trees are harvested more intensively, and demand adjusts as consumers alter habits, causing prices to decline.\n3. **Long-run**: Supply fully adjusts as new trees replace damaged ones, restoring equilibrium prices.\n\nCANDIDATE_ANSWER:\nStep 1: Short-run supply elasticity is low (inelastic) because coffee trees take 3-4 years to mature. A freeze immediately reduces coffee bean supply, but farmers cannot quickly replace damaged trees or increase production. This creates a large short-term price spike.\n\nStep 2: In the long run (3-4 years), supply becomes highly elastic. Farmers replant trees, expand cultivation areas, and global competitors increase production in response to sustained high prices. This restores supply to prefreeze levels.\n\nStep 3: Demand elasticity reinforces this pattern. While short-term demand remains relatively inelastic (consumers still buy coffee despite higher prices), long-term demand becomes more elastic as consumers may adapt to temporary price changes.\n\nCombined effect: The price spike is sharp but temporary because supply constraints are resolved through replanting and market adjustments in the long run when supply elasticity increases substantially.\n\nQID: textbook-114-1-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-1-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the short-run and long-run supply elasticities, the immediate price spike due to inelastic supply, and the eventual price stabilization as supply becomes more elastic in the long run. The explanation aligns well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-1-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the short-run and long-run supply elasticities, the immediate price spike due to inelastic supply, and the eventual price stabilization as supply becomes more elastic in the long run. The explanation aligns well with the gold answer."}, "llm_echoed_qid": "textbook-114-1-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the short-run and long-run supply elasticities, the immediate price spike due to inelastic supply, and the eventual price stabilization as supply becomes more elastic in the long run. The explanation aligns well with the gold answer."}
{"script_processing_qid": "textbook-112-20-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the text, explain how stamp duties in England and Holland differ in their proportionality to the value of the property transferred.\n\nGOLD_ANSWER:\n1.  **England**: Stamp duties are not proportional to property value; they vary by deed type (e.g., £6 per sheet for crown grants).\n2.  **Holland**: Stamp duties are proportional for testaments (e.g., stamps cost from 3 pence to 300 florins based on property value).\n3.  **Implications**: England's system is less equitable, as it does not account for value, while Holland's aligns tax burden with ability to pay.\n\nCANDIDATE_ANSWER:\nStep 1: Review the provided context to identify mentions of stamp duties in England and Holland. Step 2: Observe that the context discusses general principles of taxes on property transference (e.g., their incidence on buyers/sellers and economic effects) but does not explicitly describe the specific structures of stamp duties in England and Holland. Step 3: Conclude that the provided text lacks direct information about how stamp duties in these two countries differ in proportionality to property value. The context focuses on theoretical implications rather than comparative institutional details.\n\nQID: textbook-112-20-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-20-0-2\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate correctly identifies the lack of specific information in the context but does not address the gold answer's detailed comparison of England and Holland's stamp duties. The reasoning focuses on missing context rather than engaging with the provided answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-20-0-2", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate correctly identifies the lack of specific information in the context but does not address the gold answer's detailed comparison of England and Holland's stamp duties. The reasoning focuses on missing context rather than engaging with the provided answer."}, "llm_echoed_qid": "textbook-112-20-0-2", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate correctly identifies the lack of specific information in the context but does not address the gold answer's detailed comparison of England and Holland's stamp duties. The reasoning focuses on missing context rather than engaging with the provided answer."}
{"script_processing_qid": "textbook-114-31-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why Polaroid's monopoly on film was crucial for the success of its two-part tariff strategy.\n\nGOLD_ANSWER:\nPolaroid's monopoly on film ensured that it could set the price of film above marginal cost without competitive pressure. This was essential because:\n1. **Consumer Heterogeneity**: With diverse consumer demands, a high film price allowed Polaroid to extract more surplus from high-demand consumers.\n2. **Profit Source**: Film sales generated most of Polaroid's profits, as the camera acted as an entry fee to access film consumption.\n3. **No Substitutes**: Without competitors, consumers had no alternative film sources, ensuring Polaroid could maintain high film prices.\n\nCANDIDATE_ANSWER:\nStep 1: The two-part tariff strategy requires Polaroid to sell cameras (fixed fee 'T') and film (per-unit price 'P').\nStep 2: Polaroid’s monopoly on film ensured no competitors could supply alternative film, eliminating substitution possibilities for consumers.\nStep 3: Without film competition, Polaroid could optimally set high per-unit film prices (P) to extract consumer surplus from repeated film purchases tied to camera ownership.\nStep 4: If film were not monopolized, rival firms could undercut Polaroid’s film prices, reducing film profits (PQ − C₁(Q)) and weakening the profitability of the two-part tariff.\nStep 5: Film monopoly also prevented consumers from bypassing Polaroid’s film after purchasing the camera, ensuring ongoing revenue streams critical to the strategy’s success.\n\nQID: textbook-114-31-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-31-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the necessity of Polaroid's film monopoly for its two-part tariff strategy, aligning with the gold answer's points on lack of substitutes, profit extraction, and consumer heterogeneity.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-31-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the necessity of Polaroid's film monopoly for its two-part tariff strategy, aligning with the gold answer's points on lack of substitutes, profit extraction, and consumer heterogeneity."}, "llm_echoed_qid": "textbook-114-31-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the necessity of Polaroid's film monopoly for its two-part tariff strategy, aligning with the gold answer's points on lack of substitutes, profit extraction, and consumer heterogeneity."}
{"script_processing_qid": "textbook-119-11-3-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the properties of a good point estimator, including unbiasedness and efficiency.\n\nGOLD_ANSWER:\n1.  **Unbiasedness**: $E(\\hat{\\theta}) = \\theta$ (no systematic error).\n2.  **Efficiency**: Minimum variance among unbiased estimators (most precise).\n3.  **Consistency**: $\\hat{\\theta}$ converges to $\\theta$ as $n \\to \\infty$.\n4.  **Sufficiency**: Uses all relevant information in the sample.\n\nCANDIDATE_ANSWER:\nStep 1: Define a Good Point Estimator\nA good point estimator, denoted as \\(\\hat{\\theta}\\), is a statistic derived from sample data (e.g., \\(\\bar{X}\\)) used to estimate an unknown population parameter \\(\\theta\\). Its quality depends on properties like unbiasedness and efficiency.\n\nStep 2: Unbiasedness\nAn estimator is **unbiased** if its expected value equals the true parameter: \\(E(\\hat{\\theta}) = \\theta\\). For example, the sample mean \\(\\bar{X}\\) is an unbiased estimator of the population mean \\(\\mu\\) because \\(E(\\bar{X}) = \\mu\\).\n\nStep 3: Efficiency\nAn estimator is **efficient** if it has the smallest variance among all unbiased estimators for \\(\\theta\\). Efficiency ensures precision: a lower variance means estimates are clustered more tightly around the true value. For instance, if \\(\\bar{X}\\) and another estimator \\(\\hat{\\theta}_2\\) are both unbiased for \\(\\mu\\), but \\(\\text{Var}(\\bar{X}) < \\text{Var}(\\hat{\\theta}_2)\\), then \\(\\bar{X}\\) is more efficient.\n\nStep 4: Combined Criteria\nA good estimator balances these properties. Unbiasedness guarantees accuracy (no systematic error), while efficiency ensures minimal dispersion. However, practical trade-offs may exist (e.g., biased estimators with very low variance might sometimes be preferable).\n\nQID: textbook-119-11-3-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-11-3-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly discusses the properties of a good point estimator, including unbiasedness and efficiency, and provides clear examples. It aligns well with the gold answer, covering all key points and adding practical insights.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-11-3-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly discusses the properties of a good point estimator, including unbiasedness and efficiency, and provides clear examples. It aligns well with the gold answer, covering all key points and adding practical insights."}, "llm_echoed_qid": "textbook-119-11-3-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly discusses the properties of a good point estimator, including unbiasedness and efficiency, and provides clear examples. It aligns well with the gold answer, covering all key points and adding practical insights."}
{"script_processing_qid": "textbook-43-0-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. How do transversality conditions rule out explosive paths in the absence of externalities?\n\nGOLD_ANSWER:\nExploding capital and shadow prices violate $\\lim_{t\\to\\infty} e^{-rt} p_t k_t = 0$, as over-accumulation cannot be justified by future consumption.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the role of transversality conditions. These conditions ensure that the present value of future variables (e.g., capital stock) remains bounded, preventing infinite growth or divergence from equilibrium paths.\n\nStep 2: In models without externalities, production typically exhibits decreasing returns to scale. This results in standard downward-sloping labor demand and upward-sloping labor supply curves, ensuring a unique equilibrium.\n\nStep 3: Without externalities, the system’s eigenvalues (from linearized dynamics) will generally have one stable and one unstable root. Transversality conditions require trajectories to converge to the steady state along the stable manifold, ruling out paths diverging toward infinity (explosive paths).\n\nStep 4: By contrast, models with externalities (as in Benhabib and Farmer) induce upward-sloping labor demand curves, creating multiple equilibria. In such cases, even with transversality conditions, multiple convergent paths remain admissible due to altered eigenvalue structures (e.g., two stable roots), allowing indeterminacy.\n\nStep 5: Thus, in the absence of externalities, transversality conditions uniquely pin down the equilibrium path by excluding explosive trajectories that violate boundedness requirements, while preserving saddle-path stability.\n\nQID: textbook-43-0-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-43-0-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains how transversality conditions rule out explosive paths in the absence of externalities, aligning with the gold answer's focus on boundedness and equilibrium convergence. The reasoning is detailed and accurate.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-43-0-1-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains how transversality conditions rule out explosive paths in the absence of externalities, aligning with the gold answer's focus on boundedness and equilibrium convergence. The reasoning is detailed and accurate."}, "llm_echoed_qid": "textbook-43-0-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains how transversality conditions rule out explosive paths in the absence of externalities, aligning with the gold answer's focus on boundedness and equilibrium convergence. The reasoning is detailed and accurate."}
{"script_processing_qid": "textbook-37-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the welfare implications of the competitive equilibrium under adverse selection compared to the perfect information benchmark.\n\nGOLD_ANSWER:\n1.  **Adverse Selection Equilibrium**: Leads to underemployment or overemployment, depending on wage offers, resulting in Pareto inefficiency.\n2.  **Perfect Information Benchmark**: Achieves full employment with wages equal to productivity, maximizing social welfare. The adverse selection introduces deadweight loss due to misallocation of labor.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the perfect information benchmark\nUnder perfect information, firms can observe worker productivity (θ). The equilibrium wage equals worker productivity (w = θ), and all workers with θ ≤ 6 (threshold) are employed - achieving full efficiency with no underemployment.\n\nStep 2: Analyze adverse selection conditions\nWith asymmetric information:\n- Workers self-select into the labor market based on wages\n- Firms set wages based on expected productivity (E[θ]) of participating workers\n\nStep 3: Compare market outcomes\nAt any wage w ≥ 0:\n- Expected productivity E[θ|θ ≤ w] < w (due to adverse selection)\n- Firms demand fewer workers than supply → market fails to clear\n\nStep 4: Equilibrium adjustment\nOnly w < 0 clears the market, but this causes:\n- Underemployment: Some workers with θ ≤ 6 (who would work under perfect information) abstain\n- Deadweight loss: Socially productive matches (θ ≤ 6) fail to occur\n\nStep 5: Welfare implications\nCompared to perfect information:\n1. Reduced employment levels despite productive workers being available\n2. Inefficient allocation of labor resources\n3. Lower total economic surplus due to missed mutually beneficial transactions\n4. Distorted wage signals fail to reflect true productivity\n\nQID: textbook-37-0-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-37-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the welfare implications of competitive equilibrium under adverse selection compared to the perfect information benchmark, aligning with the gold answer. It correctly identifies underemployment, deadweight loss, and inefficient labor allocation as key issues arising from adverse selection.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-37-0-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the welfare implications of competitive equilibrium under adverse selection compared to the perfect information benchmark, aligning with the gold answer. It correctly identifies underemployment, deadweight loss, and inefficient labor allocation as key issues arising from adverse selection."}, "llm_echoed_qid": "textbook-37-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the welfare implications of competitive equilibrium under adverse selection compared to the perfect information benchmark, aligning with the gold answer. It correctly identifies underemployment, deadweight loss, and inefficient labor allocation as key issues arising from adverse selection."}
{"script_processing_qid": "textbook-115-23-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the conditions under which the three classical tests (Wald, LR, LM) have the same limiting distribution under local alternatives.\n\nGOLD_ANSWER:\n1. **Conditions**: The three classical tests have the same limiting distribution under local alternatives when the unconditional information matrix equality holds. This requires that the density is correctly specified.\n\nCANDIDATE_ANSWER:\nStep 1: The three classical tests (Wald, LR, LM) derive their asymptotic equivalence from the information matrix equality. This equality holds under correct model specification of the density function.\n\nStep 2: Under regularity conditions for MLE (e.g., smoothness of the likelihood function, consistency, and asymptotic normality of estimators), all three test statistics converge to the same limiting chi-square distribution under the null hypothesis.\n\nStep 3: For equivalence under local alternatives (i.e., parameter deviations shrinking to zero at rate 1/√N), the key requirement remains correct model specification. This ensures the score, Hessian, and outer product of scores terms in the LM statistic formulations asymptotically coincide with the information matrix.\n\nStep 4: Specifically, when the density is correctly specified, the asymptotic equivalence of the LM statistics (based on Hessian, expected Hessian, or outer product of scores) and the LR/Wald statistics follows from the information matrix equality −E[H_i] = E[s_i s_i']. This allows all three tests to share the same non-central chi-square distribution under local alternatives.\n\nStep 5: Therefore, the critical conditions are: (1) Correct specification of the likelihood function (enabling the information matrix equality), and (2) Standard MLE regularity conditions (as referenced in Sections 12.6.1 and 12.6.3 of the context).\n\nQID: textbook-115-23-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-23-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately details the conditions under which the three classical tests have the same limiting distribution under local alternatives, including correct model specification and standard MLE regularity conditions, matching the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-23-1-3", "category": "CORRECT", "explanation": "The candidate answer accurately details the conditions under which the three classical tests have the same limiting distribution under local alternatives, including correct model specification and standard MLE regularity conditions, matching the gold answer."}, "llm_echoed_qid": "textbook-115-23-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately details the conditions under which the three classical tests have the same limiting distribution under local alternatives, including correct model specification and standard MLE regularity conditions, matching the gold answer."}
{"script_processing_qid": "textbook-94-5-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. What is the reaction function in the Cournot model, and how is it used to determine equilibrium?\n\nGOLD_ANSWER:\n1. **Reaction function**: $Q_{j}(\\hat{q}_{-j})$ gives firm $j$'s optimal output as a function of the total output of other firms.\n2. **Equilibrium**: A Cournot equilibrium occurs when each firm's output is a best response to the others' outputs, i.e., $q_{j} = Q_{j}(\\hat{q}_{-j})$ for all $j$. This is found at the intersection of the reaction functions.\n\nCANDIDATE_ANSWER:\nStep 1: In the Cournot model, a reaction function represents firm j's profit-maximizing output level (q_j) given the output levels of all other firms. It is derived by maximizing firm j's profit function (π_j = p(y_j, Y)q_j - c(q_j)), where Y is total market output and p depends on Y.\n\nStep 2: To derive the reaction function mathematically:\n1. Take the first-order derivative of π_j with respect to q_j\n2. Set it equal to zero (FOC: ∂π_j/∂q_j = 0)\n3. Solve for q_j as a function of other firms' outputs\n\nStep 3: Cournot equilibrium occurs where all firms' reaction functions intersect. This means:\n- Each firm's output level q_j* is a best response to other firms' equilibrium outputs\n- No firm has an incentive to unilaterally deviate from q_j*\n\nStep 4: For a duopoly case with firms 1 and 2:\n1. Solve q₁ = f₁(q₂) from firm 1's FOC\n2. Solve q₂ = f₂(q₁) from firm 2's FOC\n3. Find (q₁*, q₂*) that simultaneously satisfy both equations\n\nThis equilibrium represents a Nash equilibrium in quantities, where all firms mutually optimize their outputs given competitors' choices.\n\nQID: textbook-94-5-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-94-5-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the reaction function in the Cournot model and how equilibrium is determined, matching the gold answer's key points and providing additional detailed steps.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-94-5-1-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the reaction function in the Cournot model and how equilibrium is determined, matching the gold answer's key points and providing additional detailed steps."}, "llm_echoed_qid": "textbook-94-5-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the reaction function in the Cournot model and how equilibrium is determined, matching the gold answer's key points and providing additional detailed steps."}
{"script_processing_qid": "textbook-114-8-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Compare and contrast the Cournot and Stackelberg models in terms of firm behavior and market outcomes.\n\nGOLD_ANSWER:\n1. **Cournot Model**: Firms choose quantities simultaneously, resulting in a Nash equilibrium with lower profits.\n2. **Stackelberg Model**: The leader firm chooses quantity first, exploiting the follower's reaction to achieve higher profits.\n\nCANDIDATE_ANSWER:\nStep 1: **Model Structure**\nCournot involves simultaneous decision-making (both firms set output independently at the same time), while Stackelberg is sequential (one firm sets output first, the other follows).\n\nStep 2: **Firm Behavior**\n- Cournot: Firms act as symmetric competitors with no leadership advantage; both make decisions based on assumptions about the rival’s output.\n- Stackelberg: The leader (first mover) commits to an output level first, leveraging strategic dominance. The follower observes the leader’s choice and optimizes accordingly.\n\nStep 3: **Market Outcomes**\n- Cournot: Results in a Nash equilibrium where both firms produce less than the competitive output but more than a monopoly, splitting profits equally (if symmetric).\n- Stackelberg: The leader produces more output than in Cournot, while the follower produces less. The leader earns higher profits than the follower, and total industry output is higher than in Cournot (leading to lower prices).\n\nStep 4: **Strategic Implications**\n- In the context’s cereal example, simultaneous decisions (Cournot-like) lead to both firms introducing sweet cereal and incurring losses. Sequential moves (Stackelberg) allow Firm 1 to deter Firm 2 from entering the sweet cereal market or force Firm 2 into a less profitable strategy, improving Firm 1’s outcome.\n\nQID: textbook-114-8-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-8-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately describes the differences between the Cournot and Stackelberg models in terms of firm behavior and market outcomes, matching the gold answer's key points and expanding with additional detail.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-8-1-1", "category": "CORRECT", "explanation": "The candidate answer accurately describes the differences between the Cournot and Stackelberg models in terms of firm behavior and market outcomes, matching the gold answer's key points and expanding with additional detail."}, "llm_echoed_qid": "textbook-114-8-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately describes the differences between the Cournot and Stackelberg models in terms of firm behavior and market outcomes, matching the gold answer's key points and expanding with additional detail."}
{"script_processing_qid": "textbook-110-4-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the Cournot-Nash equilibrium is inefficient compared to the joint-profit maximization outcome.\n\nGOLD_ANSWER:\n1.  **Cournot-Nash Equilibrium**: Each firm maximizes its own profit given the other's output, leading to higher total output and lower prices than the joint-profit maximization outcome.\n2.  **Joint-Profit Maximization**: Firms act as a monopolist, restricting output to maximize combined profits, resulting in higher prices and lower output.\n3.  **Inefficiency**: The Cournot equilibrium is inefficient because firms do not internalize the negative externality of their output decisions on the other firm's profits, leading to a prisoner's dilemma-like outcome.\n\nCANDIDATE_ANSWER:\nStep 1: Understand Firm Behavior in Cournot-Nash\nIn Cournot competition, Firm 1 maximizes its profit assuming Firm 2's output (q²) is fixed. Its first-order condition (FOC) is:\np_q(q)q¹ + p(q) - C_q¹(q¹) = 0\nwhere p_q(q) = derivative of price w.r.t. total output. The term p_q(q)q¹ reflects how Firm 1's output expansion reduces market price for all units sold (including its own).\n\nStep 2: Compare to Joint-Profit Maximization\nIn joint-profit maximization, firms coordinate to maximize total industry profits. The FOC for total output would include BOTH firms' effects on price:\np_q(q)[q¹ + q²] + p(q) - C_q¹(q¹) = 0\nSimilarly for Firm 2. Here, firms internalize the price impact of output changes on each other.\n\nStep 3: Identify the Externality\nThe Cournot FOC only accounts for the price reduction caused by Firm 1's own output (p_q q¹). In contrast, joint maximization accounts for the full price reduction (p_q [q¹ + q²]). Firms in Cournot overproduce because they ignore the negative externality their output imposes on their rival's revenues.\n\nStep 4: Resulting Inefficiency\nThis externality causes Cournot-Nash equilibrium output to be higher and prices lower than under joint-profit maximization. Higher output reduces industry profits below the collusive optimum, making Cournot equilibrium Pareto-inefficient from the firms' perspective.\n\nQID: textbook-110-4-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-110-4-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the Cournot-Nash equilibrium's inefficiency compared to joint-profit maximization, detailing firm behavior, the externality, and the resulting market outcomes, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-4-1-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the Cournot-Nash equilibrium's inefficiency compared to joint-profit maximization, detailing firm behavior, the externality, and the resulting market outcomes, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-110-4-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the Cournot-Nash equilibrium's inefficiency compared to joint-profit maximization, detailing firm behavior, the externality, and the resulting market outcomes, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-114-32-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze the implications of a decreasing-cost industry (not explicitly discussed in the text) on the shape of the long-run supply curve. How would input prices behave, and what would the curve look like?\n\nGOLD_ANSWER:\n1. **Decreasing-Cost Industry**: Input prices decrease as the industry expands, possibly due to economies of scale in input production or technological spillovers.\n2. **Shape of Supply Curve**: The long-run supply curve would be downward-sloping because higher output leads to lower per-unit costs.\n3. **Implications**: Firms could produce more at lower prices, potentially leading to natural monopolies or significant economies of scale. This scenario is less common and not discussed in the text.\n\nCANDIDATE_ANSWER:\nStep 1: Contrast with provided context. The text explicitly describes constant-cost (horizontal LR supply curve) and increasing-cost (upward-sloping LR supply curve) industries. A decreasing-cost industry represents the inverse dynamic of an increasing-cost industry.\n\nStep 2: Infer input price behavior. In a decreasing-cost industry, input prices would decline as the industry expands due to factors like economies of scale in input production or technological spillovers.\n\nStep 3: Derive LR supply curve shape. Falling input prices reduce firms' long-run average costs as industry output expands. This creates a downward-sloping long-run supply curve – greater industry output can be sustained at progressively lower equilibrium prices.\n\nStep 4: Example mechanism. If industry expansion leads to cheaper raw materials (e.g., via bulk purchasing or specialized suppliers), new entrants could produce at lower costs than existing firms, pulling the market price downward as output increases.\n\nQID: textbook-114-32-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-32-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes the implications of a decreasing-cost industry, including the behavior of input prices and the resulting downward-sloping long-run supply curve, matching the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-32-1-3", "category": "CORRECT", "explanation": "The candidate's answer accurately describes the implications of a decreasing-cost industry, including the behavior of input prices and the resulting downward-sloping long-run supply curve, matching the gold answer."}, "llm_echoed_qid": "textbook-114-32-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes the implications of a decreasing-cost industry, including the behavior of input prices and the resulting downward-sloping long-run supply curve, matching the gold answer."}
{"script_processing_qid": "textbook-112-48-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Quantify the employment impact of distant colonial trade using Smith's example of a £1000 capital with 5-year returns versus annual returns.\n\nGOLD_ANSWER:\n1.  **Annual Returns Case**:\n   - Capital maintains labor: $L = \\frac{\\£1000}{w}$ per year, where $w$ is wage rate.\n\n2.  **5-Year Returns Case**:\n   - Effective capital: $\\frac{\\£1000}{5} = \\£200$/year.\n   - Labor maintained: $L' = \\frac{\\£200}{w}$.\n\n3.  **Employment Ratio**:\n   - $\\frac{L'}{L} = \\frac{1}{5}$.\n   - Thus, distant trade reduces employment by 80% compared to annual-return trade.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Smith's key principle - Capital that circulates/reinvests more frequently sustains more employment.\nStep 2: Compare employment impacts of capital return intervals:\n- Annual return (£1000 reinvested yearly): Maintains full £1000-worth of employment per year\n- 5-year return (£1000 tied up for 5 years): Only maintains £200-worth of employment annually (1000/5)\nStep 3: Calculate total employment difference over 5 years:\n- Annual return: 5 × £1000 = £5000 total employment capacity\n- 5-year return: 5 × £200 = £1000 total employment capacity\nStep 4: Quantify the employment reduction: Distant colonial trade (5-year cycle) supports only 20% (1/5) of the employment that annual-recycling capital would generate.\n\nQID: textbook-112-48-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-48-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows Smith's principle, accurately compares the employment impacts of different return intervals, and quantifies the employment reduction, matching the gold answer's conclusion that distant trade reduces employment by 80% compared to annual-return trade.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-48-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows Smith's principle, accurately compares the employment impacts of different return intervals, and quantifies the employment reduction, matching the gold answer's conclusion that distant trade reduces employment by 80% compared to annual-return trade."}, "llm_echoed_qid": "textbook-112-48-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows Smith's principle, accurately compares the employment impacts of different return intervals, and quantifies the employment reduction, matching the gold answer's conclusion that distant trade reduces employment by 80% compared to annual-return trade."}
{"script_processing_qid": "textbook-106-6-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the price elasticity of supply might vary along a supply curve, especially in industries with limited production capacity.\n\nGOLD_ANSWER:\nThe price elasticity of supply can vary along the supply curve due to production constraints. At low levels of output, firms may have idle capacity, making supply highly elastic as small price increases can quickly ramp up production. As output approaches maximum capacity, further increases require significant investments (e.g., new factories), making supply less elastic. This is illustrated in Figure 5-7, where elasticity decreases as quantity supplied rises.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the formula for price elasticity of supply (PES). It equals the percentage change in quantity supplied divided by the percentage change in price. The midpoint method ensures unbiased calculations.\n\nStep 2: Recognize that PES depends on proportional changes. At low production levels (left side of the supply curve), firms often have underutilized capacity. A small absolute increase in quantity supplied (e.g., from 9,000 to 11,000 gallons) represents a large percentage change, leading to higher PES (e.g., 2.0 in the example).\n\nStep 3: As output increases toward maximum capacity (right side of the supply curve), firms face physical constraints (e.g., fixed factory size, limited machinery). The same absolute increase in quantity (e.g., 2,000 gallons) becomes a smaller percentage of the larger base quantity (e.g., moving from 20,000 to 22,000 gallons yields a 9.5% change instead of 20%), reducing PES.\n\nStep 4: In industries with limited capacity, this effect is amplified. Near full capacity, firms cannot increase output significantly even with large price increases, making supply increasingly inelastic (lower PES) at higher quantities.\n\nQID: textbook-106-6-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-106-6-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains why the price elasticity of supply varies along the supply curve, particularly in industries with limited production capacity. The step-by-step reasoning aligns with the gold answer, detailing how elasticity decreases as output approaches maximum capacity due to production constraints.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-6-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains why the price elasticity of supply varies along the supply curve, particularly in industries with limited production capacity. The step-by-step reasoning aligns with the gold answer, detailing how elasticity decreases as output approaches maximum capacity due to production constraints."}, "llm_echoed_qid": "textbook-106-6-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains why the price elasticity of supply varies along the supply curve, particularly in industries with limited production capacity. The step-by-step reasoning aligns with the gold answer, detailing how elasticity decreases as output approaches maximum capacity due to production constraints."}
{"script_processing_qid": "textbook-102-2-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compute the marginal effect of the 301st truck on the participation rate using the probit model.\n\nGOLD_ANSWER:\n1. **Probit equation**: $z_i = -2.3116 + 0.010646 \\times 300 = 0.8822$.\\n2. **Density**: $\\phi(0.8822) = 0.2703$.\\n3. **Marginal effect**: $0.010646 \\times 0.2703 = 0.00288$.\\n4. **Interpretation**: The 301st truck increases the participation rate by approximately 0.288 percentage points.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate z_i value at T=300 trucks\nz_i = -2.18098 + 0.0098898*(300) = -2.18098 + 2.96694 = 0.78596\n\nStep 2: Compute standard normal PDF φ(z_i)\nφ(0.78596) = (1/√(2π))e^{-(0.78596)^2/2} ≈ 0.2928\n\nStep 3: Multiply by given coefficient\nMarginal effect = 0.010646 * 0.2928 ≈ 0.003116\n\nInterpretation: The 301st truck increases participation probability by approximately 0.31 percentage points (0.003116 in decimal terms). This calculation follows the provided formula where the marginal effect in probit models depends on the density φ(z_i) at the specified truck count.\n\nQID: textbook-102-2-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-102-2-1-3\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate used incorrect coefficients (-2.18098 and 0.0098898) in Step 1 instead of the correct ones (-2.3116 and 0.010646) provided in the gold answer, leading to an incorrect marginal effect calculation.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-102-2-1-3", "category": "INCORRECT", "explanation": "The candidate used incorrect coefficients (-2.18098 and 0.0098898) in Step 1 instead of the correct ones (-2.3116 and 0.010646) provided in the gold answer, leading to an incorrect marginal effect calculation."}, "llm_echoed_qid": "textbook-102-2-1-3", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate used incorrect coefficients (-2.18098 and 0.0098898) in Step 1 instead of the correct ones (-2.3116 and 0.010646) provided in the gold answer, leading to an incorrect marginal effect calculation."}
{"script_processing_qid": "textbook-118-11-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of the angle between two vectors on their scalar product.\n\nGOLD_ANSWER:\n1. **Parallel Vectors**: If $\\theta = 0$, $\\cos\\theta = 1$, and $\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\|\\mathbf{x}\\|\\|\\mathbf{y}\\|$.\n2. **Orthogonal Vectors**: If $\\theta = 90^\\circ$, $\\cos\\theta = 0$, and $\\langle \\mathbf{x}, \\mathbf{y} \\rangle = 0$.\n3. **Opposite Vectors**: If $\\theta = 180^\\circ$, $\\cos\\theta = -1$, and $\\langle \\mathbf{x}, \\mathbf{y} \\rangle = -\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|$.\n4. **General Case**: The scalar product varies continuously with $\\theta$, reflecting the degree of alignment between the vectors.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the geometric formula for the scalar product:\nThe scalar product ⟨x, y⟩ equals ‖x‖‖y‖cosθ, where θ is the angle between the vectors.\n\nStep 2: Angle θ = 0° (parallel vectors):\n- cosθ = 1 → scalar product simplifies to α‖x‖² (where y = αx).\n- Implication: Maximal positive value, indicating perfect alignment.\n\nStep 3: Angle θ = 90° (orthogonal vectors):\n- cosθ = 0 → scalar product becomes 0.\n- Implication: Vectors are perpendicular, providing a natural test for orthogonality.\n\nStep 4: Angle θ = 180° (antiparallel vectors):\n- cosθ = -1 → scalar product equals -‖x‖‖y‖.\n- Implication: Minimal negative value, indicating diametrically opposed directions.\n\nStep 5: General case for unit vectors (‖x‖ = ‖y‖ = 1):\n- Scalar product equals cosθ (e.g., for coordinates (1,0) and (cosθ, sinθ)).\n- Implication: Directly measures directional similarity via cosθ ∈ [-1,1].\n\nStep 6: Intermediate angles:\n- Values of cosθ between -1 and 1 quantify the degree of alignment:\n  • cosθ > 0: Acute angle (vectors share directional components).\n  • cosθ < 0: Obtuse angle (vectors oppose directionally).\n\nStep 7: Higher-dimensional generalization:\n- The angle interpretation remains valid in Eⁿ, allowing geometric intuition in abstract spaces.\n\nQID: textbook-118-11-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-11-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly discusses the implications of the angle between two vectors on their scalar product, covering all key cases (parallel, orthogonal, opposite, and general) as outlined in the gold answer. The reasoning is accurate and aligns with the expected mathematical principles.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-11-1-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly discusses the implications of the angle between two vectors on their scalar product, covering all key cases (parallel, orthogonal, opposite, and general) as outlined in the gold answer. The reasoning is accurate and aligns with the expected mathematical principles."}, "llm_echoed_qid": "textbook-118-11-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly discusses the implications of the angle between two vectors on their scalar product, covering all key cases (parallel, orthogonal, opposite, and general) as outlined in the gold answer. The reasoning is accurate and aligns with the expected mathematical principles."}
{"script_processing_qid": "textbook-119-11-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. For a binomial random variable $X \\sim Bin(n,p)$, derive the mean and variance using the properties of Bernoulli trials.\n\nGOLD_ANSWER:\n1.  **Mean**: $E(X) = np$ (sum of $n$ independent Bernoulli trials, each with mean $p$).\n2.  **Variance**: $Var(X) = np(1-p)$ (sum of $n$ independent Bernoulli trials, each with variance $p(1-p)$).\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that a Binomial random variable X is the sum of n independent Bernoulli trials. Let Xi ~ Bernoulli(p) for i = 1 to n, where Xi = 1 (success) with probability p and Xi = 0 (failure) with probability q = 1-p. Then, X = X₁ + X₂ + ... + Xₙ.\n\nStep 2: Compute the mean using linearity of expectation:\nE(X) = E(X₁ + X₂ + ... + Xₙ) = E(X₁) + E(X₂) + ... + E(Xₙ)\nSince each Xi has E(Xi) = p,\nE(X) = n * p\n\nStep 3: Compute the variance using the property that variances add for independent variables:\nVar(X) = Var(X₁ + X₂ + ... + Xₙ) = Var(X₁) + Var(X₂) + ... + Var(Xₙ)\nSince each Xi has Var(Xi) = p * q,\nVar(X) = n * p * q\n\nQID: textbook-119-11-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-11-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derives the mean and variance of a binomial random variable using the properties of Bernoulli trials, matching the gold answer exactly in both reasoning and final expressions.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-11-2-1", "category": "CORRECT", "explanation": "The candidate correctly derives the mean and variance of a binomial random variable using the properties of Bernoulli trials, matching the gold answer exactly in both reasoning and final expressions."}, "llm_echoed_qid": "textbook-119-11-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derives the mean and variance of a binomial random variable using the properties of Bernoulli trials, matching the gold answer exactly in both reasoning and final expressions."}
{"script_processing_qid": "textbook-112-13-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the conditions under which a free trade agreement between two countries (e.g., France and England) could be mutually beneficial, even if the balance of trade favors one country.\n\nGOLD_ANSWER:\n1. **Comparative Advantage**: If France produces wine and linen more efficiently (cheaper/better) than Portugal and Germany, England benefits by importing these goods from France, even if the bilateral trade balance favors France.\n2. **Re-exportation**: A portion of French goods imported by England can be re-exported to other countries, generating additional revenue.\n3. **Net Benefit**: The overall cost of imports may decrease if French goods are cheaper, leading to a net benefit for England despite the trade imbalance with France.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize Adam Smith's rejection of mercantilist balance-of-trade logic. The provided context explicitly states there is \"no certain criterion\" to assess which country benefits more from trade, undermining the premise that a trade imbalance inherently creates unilateral benefits.<br><br>Step 2: Establish comparative advantage as the basis for mutual gains. Though not explicitly mentioned in the context, Smith's broader principles (implied via criticism of monopoly distortions) suggest specialization increases total output. Both countries benefit by exporting goods where they have relative efficiency.<br><br>Step 3: Remove trade restrictions. The context condemns taxes aimed at reducing imports as \"destructive of revenue and freedom of trade.\" An FTA eliminates such distortions, allowing optimal resource allocation regardless of nominal trade balance.<br><br>Step 4: Consider dynamic effects. The context warns against sudden policy changes (\"introduced slowly, gradually\"), implying mutual benefit requires phased adjustment periods to reallocate factors of production efficiently.<br><br>Condition Synthesis: An FTA is mutually beneficial if (1) comparative advantage enables specialization gains exceeding trade imbalance effects, (2) trade barriers are removed to enable efficient exchange, and (3) transitional policies mitigate short-term disruptions to entrenched interests (per Smith's warning about abrupt changes).\n\nQID: textbook-112-13-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-13-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the conditions for mutual benefit in a free trade agreement, aligning with the gold answer's emphasis on comparative advantage and net benefits, while also expanding on dynamic effects and policy considerations.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-13-0-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the conditions for mutual benefit in a free trade agreement, aligning with the gold answer's emphasis on comparative advantage and net benefits, while also expanding on dynamic effects and policy considerations."}, "llm_echoed_qid": "textbook-112-13-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the conditions for mutual benefit in a free trade agreement, aligning with the gold answer's emphasis on comparative advantage and net benefits, while also expanding on dynamic effects and policy considerations."}
{"script_processing_qid": "textbook-112-6-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the conditions under which a builder would remain in the trade, given the building-rent and the market rate of interest.\n\nGOLD_ANSWER:\n1.  **Building-Rent Condition**: The building-rent must cover:\n    - The opportunity cost of capital (market interest rate).\n    - The cost of maintaining the house (or replacing the capital over time).\n2.  **Mathematical Formulation**: Let $r$ be the market interest rate, $C$ the capital invested, and $M$ the maintenance cost. The building-rent $R$ must satisfy:\n    $$ R \\geq r \\cdot C + M $$\n3.  **Equilibrium Profit**: If $R$ exceeds this threshold, capital flows into building until profits normalize. If $R$ falls below, capital exits until equilibrium is restored.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the two key components of building-rent as defined in the context. Building-rent must cover (a) the market interest rate the builder could earn by lending capital elsewhere, and (b) the ongoing repair costs to maintain the property.\n\nStep 2: Formalize the conditions mathematically. Let:\n- \\( r \\) = market rate of interest\n- \\( C \\) = capital invested in building\n- \\( M \\) = annual maintenance/repair costs\n\nFor the builder to remain in the trade, the total building-rent (\\( BR \\)) must satisfy:\n\\( BR \\geq (r \times C) + M \\)\n\nStep 3: Interpret the inequality. If \\( BR < (r \times C) + M \\), the builder would exit the trade because:\n1. The interest component (\\( r \times C \\)) represents opportunity cost – insufficient returns compared to alternative investments.\n2. The maintenance component (\\( M \\)) ensures structural viability – unmet repairs would erode long-term profitability.\n\nStep 4: Relate to tax implications (as discussed in the context). Even with taxes on house-rent, builders will remain only if the post-tax building-rent still satisfies \\( BR_{\\text{post-tax}} \\geq (r \times C) + M \\). If not, market forces (reduced supply of builders) would eventually restore equilibrium by raising rents.\n\nQID: textbook-112-6-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-6-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies and formalizes the conditions for a builder to remain in the trade, matching the gold answer's key points and mathematical formulation. The additional step about tax implications, while not in the gold answer, does not contradict it and provides a reasonable extension.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-6-0-0", "category": "CORRECT", "explanation": "The candidate answer accurately identifies and formalizes the conditions for a builder to remain in the trade, matching the gold answer's key points and mathematical formulation. The additional step about tax implications, while not in the gold answer, does not contradict it and provides a reasonable extension."}, "llm_echoed_qid": "textbook-112-6-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies and formalizes the conditions for a builder to remain in the trade, matching the gold answer's key points and mathematical formulation. The additional step about tax implications, while not in the gold answer, does not contradict it and provides a reasonable extension."}
{"script_processing_qid": "textbook-112-29-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the effects of spending on durable goods versus hospitality on national opulence, according to Smith.\n\nGOLD_ANSWER:\n1.  **Durable Goods**: Spending on houses, furniture, etc., benefits future generations and supports productive labor.\n2.  **Hospitality**: Profuse spending on feasts and servants is wasteful and supports unproductive labor.\n3.  **Long-term Impact**: Durable goods contribute to sustained economic growth, while hospitality does not.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Smith's key arguments in the provided context. The text emphasizes that capital accumulation drives economic growth by increasing productive labor and improving tools/division of labor. Durable goods spending is explicitly praised for supporting accumulation and frugality.<br>Step 2: Contrast durable goods with hospitality (implied to represent unproductive/service-oriented spending). Durable goods (e.g., machinery, tools) enhance productive capacity through lasting investments, creating multiplier effects for future output.<br>Step 3: Hospitality spending (as perishable/experiential consumption) lacks this durable quality. While not explicitly named in the text, hospitality aligns with expenditures that do not 'facilitate and abridge labour' or improve 'division of employment,' making them less favorable for capital accumulation.<br>Step 4: Conclusion based on context: Smith views durable goods spending as superior for national opulence because it directly supports capital formation and productivity gains, whereas hospitality spending (as non-durable consumption) lacks these growth-enhancing properties.\n\nQID: textbook-112-29-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-29-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies Smith's arguments, contrasting the benefits of durable goods spending (supporting capital accumulation and productivity) with the less favorable nature of hospitality spending (non-durable and less growth-enhancing). This aligns perfectly with the gold answer's points.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-29-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately identifies Smith's arguments, contrasting the benefits of durable goods spending (supporting capital accumulation and productivity) with the less favorable nature of hospitality spending (non-durable and less growth-enhancing). This aligns perfectly with the gold answer's points."}, "llm_echoed_qid": "textbook-112-29-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies Smith's arguments, contrasting the benefits of durable goods spending (supporting capital accumulation and productivity) with the less favorable nature of hospitality spending (non-durable and less growth-enhancing). This aligns perfectly with the gold answer's points."}
{"script_processing_qid": "textbook-85-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain the concept of sample selection bias in truncated regression models and how it affects the consistency of OLS estimators.\n\nGOLD_ANSWER:\n1. **Sample Selection Bias**: In truncated regression models, observations are included only if the dependent variable $y_{t}$ meets a certain criterion (e.g., $y_{t} > c$). This truncation alters the distribution of $y_{t}\\mid\\mathbf{x}_{t}$, leading to a bias in the OLS estimator. \\n2. **Effect on OLS**: The conditional mean of $y_{t}$ given $\\mathbf{x}_{t}$ and truncation is: \\n   $$ \\mathrm{E}(y_{t}\\mid\\mathbf{x}_{t}, y_{t} > c) = \\mathbf{x}_{t}^{\\prime}\\pmb{\\beta}_{0} + \\sigma_{0}\\lambda\\left(\\frac{c - \\mathbf{x}_{t}^{\\prime}\\pmb{\\beta}_{0}}{\\sigma_{0}}\\right), $$ \\n   where $\\lambda(v)$ is the inverse Mill's ratio. The term $\\sigma_{0}\\lambda(v)$ introduces a nonlinearity, causing OLS to be inconsistent for $\\pmb{\\beta}_{0}$. \\n3. **Solution**: Maximum likelihood estimation accounts for the truncation by incorporating the correction term into the likelihood function, ensuring consistent and efficient estimates.\n\nCANDIDATE_ANSWER:\nStep 1: Define sample selection bias in truncated regression models\nSample selection bias occurs when the observed data is non-randomly truncated based on the dependent variable value (e.g., only observing outcomes above/below certain thresholds). This creates a non-representative sample of the population.\n\nStep 2: Explain the inconsistency mechanism\nIn truncated regression models, the truncation creates a correlation between the error term and explanatory variables because the truncation condition (based on y_t) implicitly restricts the error term distribution. This violates the OLS assumption that E[ε|X] = 0.\n\nStep 3: Demonstrate mathematical consequence\nThe conditional expectation in truncated samples becomes E[y_t|x_t] = x_t'θ + E[ε_t|ε_t > c - x_t'θ] (for truncation at c). This additional term creates specification bias because OLS assumes E[ε_t|x_t] = 0.\n\nStep 4: Show inconsistency formally\nSince the OLS estimator θ_ols = (X'X)^{-1}X'y depends on biased conditional expectations, plim θ_ols ≠ θ_0 due to the non-zero correlation between x_t and the truncated error terms.\n\nStep 5: Conclusion\nThe resulting bias persists asymptotically, making OLS estimators inconsistent. Proper maximum likelihood estimation (as discussed in the context's QR models) is required to account for the truncated likelihood structure.\n\nQID: textbook-85-0-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-85-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately defines sample selection bias in truncated regression models, explains the inconsistency mechanism, demonstrates the mathematical consequence, and shows the formal inconsistency of OLS estimators. The explanation aligns well with the gold answer, including the need for maximum likelihood estimation to address the issue.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-85-0-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately defines sample selection bias in truncated regression models, explains the inconsistency mechanism, demonstrates the mathematical consequence, and shows the formal inconsistency of OLS estimators. The explanation aligns well with the gold answer, including the need for maximum likelihood estimation to address the issue."}, "llm_echoed_qid": "textbook-85-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately defines sample selection bias in truncated regression models, explains the inconsistency mechanism, demonstrates the mathematical consequence, and shows the formal inconsistency of OLS estimators. The explanation aligns well with the gold answer, including the need for maximum likelihood estimation to address the issue."}
{"script_processing_qid": "textbook-115-11-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. How can you test the null hypothesis $H_{0}: \\gamma = 0$ in the presence of generated regressors?\n\nGOLD_ANSWER:\nTo test $H_{0}: \\gamma = 0$, use the usual $t$ statistic on $\\hat{q}$ in the OLS regression. Under $H_{0}$, this statistic has a limiting standard normal distribution. The test requires the homoskedasticity assumption $\\text{E}(u^{2}|\\mathbf{x},q) = \\sigma^{2}$. For heteroskedasticity, use a robust $t$ statistic.\n\nCANDIDATE_ANSWER:\nTo test $H_0: \\gamma = 0$ with generated regressors:\n\nStep 1: Estimate $\\hat{\\delta}$ in the first stage using observable data $\\mathbf{w}$ (e.g., via MLE, NLS, or other appropriate method).\nStep 2: Compute generated regressors $\\hat{q}_i = f(\\mathbf{w}_i, \\hat{\\delta})$.\nStep 3: Run the OLS regression $y$ on $x_1, x_2, \\dots, x_K, \\hat{q}_i$ to obtain $\\hat{\\gamma}$ and its naive OLS standard error (which is biased downward).\nStep 4: Adjust standard errors to account for first-stage uncertainty in $\\hat{\\delta}$. Two common approaches:\n   a) **Two-step correction**: Use a robust variance estimator (e.g., Murphy-Topel) that incorporates estimation uncertainty from both stages.\n   b) **Bootstrap**: Resample data, re-estimate $\\hat{\\delta}$ and $\\hat{\\gamma}$ in each sample, then compute the standard error of $\\hat{\\gamma}$ across bootstrap replications.\nStep 5: Form the t-statistic $t = \\hat{\\gamma}/\\text{adjusted SE}(\\hat{\\gamma})$ and compare to critical values from the standard normal distribution.\n\nKey intuition: The standard OLS variance formula assumes $\\hat{q}_i$ is fixed and ignores the variability from estimating $\\delta$, necessitating adjusted inference methods.\n\nQID: textbook-115-11-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-11-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer provides a detailed and correct procedure for testing the null hypothesis $H_0: \\gamma = 0$ in the presence of generated regressors, including the necessary adjustments for standard errors and the use of robust variance estimators or bootstrap methods, which aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-11-0-3", "category": "CORRECT", "explanation": "The candidate answer provides a detailed and correct procedure for testing the null hypothesis $H_0: \\gamma = 0$ in the presence of generated regressors, including the necessary adjustments for standard errors and the use of robust variance estimators or bootstrap methods, which aligns with the gold answer."}, "llm_echoed_qid": "textbook-115-11-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer provides a detailed and correct procedure for testing the null hypothesis $H_0: \\gamma = 0$ in the presence of generated regressors, including the necessary adjustments for standard errors and the use of robust variance estimators or bootstrap methods, which aligns with the gold answer."}
{"script_processing_qid": "textbook-94-7-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the marginal rate of technical substitution (MRTS) for a Cobb-Douglas production function $f(x_{1}, x_{2}) = x_{1}^{\\alpha}x_{2}^{1-\\alpha}$.\n\nGOLD_ANSWER:\n1. **Differentiate the production function**: \n   $$\\frac{\\partial f}{\\partial x_{1}} = \\alpha x_{1}^{\\alpha-1}x_{2}^{1-\\alpha} = \\alpha \\left(\\frac{x_{2}}{x_{1}}\\right)^{1-\\alpha}$$\n   $$\\frac{\\partial f}{\\partial x_{2}} = (1-\\alpha)x_{1}^{\\alpha}x_{2}^{-\\alpha} = (1-\\alpha)\\left(\\frac{x_{1}}{x_{2}}\\right)^{\\alpha}$$\n2. **Compute MRTS**: \n   $$MRTS = -\\frac{\\partial f/\\partial x_{1}}{\\partial f/\\partial x_{2}} = -\\frac{\\alpha}{1-\\alpha}\\frac{x_{2}}{x_{1}}$$\n\nCANDIDATE_ANSWER:\nStep 1: Recall that MRTS is defined as the absolute value of the slope of the isoquant, calculated as the ratio of marginal products: MRTS = MP₁/MP₂.\n\nStep 2: Compute marginal product of input 1 (MP₁):\nMP₁ = ∂f/∂x₁ = αx₁^{α-1}x₂^{1-α} = α(x₂/x₁)^{1-α}\n\nStep 3: Compute marginal product of input 2 (MP₂):\nMP₂ = ∂f/∂x₂ = (1-α)x₁^{α}x₂^{-α} = (1-α)(x₁/x₂)^{α}\n\nStep 4: Form the ratio MP₁/MP₂:\n(α(x₂/x₁)^{1-α}) / ((1-α)(x₁/x₂)^{α}) = (α/(1-α)) * (x₂/x₁)^{1-α+α}\n\nStep 5: Simplify exponents:\n(x₂/x₁)^{1} = x₂/x₁\n\nFinal MRTS:\nMRTS = (α/(1-α)) * (x₂/x₁)\n\nQID: textbook-94-7-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-94-7-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derived the MRTS for the Cobb-Douglas production function, following the same steps and arriving at the same result as the gold answer. The reasoning and calculations are accurate.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-94-7-0-0", "category": "CORRECT", "explanation": "The candidate correctly derived the MRTS for the Cobb-Douglas production function, following the same steps and arriving at the same result as the gold answer. The reasoning and calculations are accurate."}, "llm_echoed_qid": "textbook-94-7-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derived the MRTS for the Cobb-Douglas production function, following the same steps and arriving at the same result as the gold answer. The reasoning and calculations are accurate."}
{"script_processing_qid": "textbook-122-23-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the first-order conditions are necessary but not sufficient for minimizing the sum of squared residuals. How can we verify that the solution indeed minimizes the SSR?\n\nGOLD_ANSWER:\n1. **Necessary Conditions**: The first-order conditions ensure that the gradient is zero at the critical point, but they do not distinguish between minima, maxima, or saddle points.\\n2. **Verification**:\\n   - Expand $Q(b_0, b_1)$ around $(\\hat{\\beta}_0, \\hat{\\beta}_1)$.\\n   - Show that the Hessian matrix is positive definite, or equivalently, that the second derivative with respect to $b_1$ is positive.\\n   - Alternatively, show that $Q(b_0, b_1) \\geq Q(\\hat{\\beta}_0, \\hat{\\beta}_1)$ for all $(b_0, b_1)$.\n\nCANDIDATE_ANSWER:\nStep 1: First-order conditions (FOCs) are necessary because they identify critical points where the gradient is zero (potential minima/maxima/saddle points). In OLS, these yield the normal equations that define β̂₀ and β̂₁.\n\nStep 2: FOCs alone are not sufficient because they cannot distinguish between minima and maxima. For example, a critical point could theoretically be a maximum or saddle point in a non-convex function.\n\nStep 3: Verification is achieved by analyzing the structure of the SSR function. The provided context rewrites the SSR as:\nQ(b₀,b₁) = Q(β̂₀,β̂₁) + Σ[(β̂₀−b₀) + (β̂₁−b₁)x_i]²\n\nStep 4: The term Σ[(β̂₀−b₀) + (β̂₁−b₁)x_i]² is always non-negative (sum of squares). Thus, Q(b₀,b₁) ≥ Q(β̂₀,β̂₁) for all (b₀,b₁), with equality only at (β̂₀,β̂₁). This proves the solution is a global minimum.\n\nQID: textbook-122-23-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-23-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains why first-order conditions are necessary but not sufficient for minimizing the sum of squared residuals. It also accurately describes the verification process by showing that the SSR function is always greater than or equal to its value at the critical point, proving it is a global minimum. This aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-23-0-1", "category": "CORRECT", "explanation": "The candidate answer correctly explains why first-order conditions are necessary but not sufficient for minimizing the sum of squared residuals. It also accurately describes the verification process by showing that the SSR function is always greater than or equal to its value at the critical point, proving it is a global minimum. This aligns with the gold answer."}, "llm_echoed_qid": "textbook-122-23-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains why first-order conditions are necessary but not sufficient for minimizing the sum of squared residuals. It also accurately describes the verification process by showing that the SSR function is always greater than or equal to its value at the critical point, proving it is a global minimum. This aligns with the gold answer."}
{"script_processing_qid": "textbook-122-11-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Prove that if $T \\sim t_n$, then $T^2 \\sim F_{1,n}$ using the definitions of t and F distributions.\n\nGOLD_ANSWER:\n1. **Definition**: $T = Z / \\sqrt{X/n}$ where $Z \\sim \\operatorname{Normal}(0,1)$, $X \\sim \\chi_n^2$.\n2. **Square**: $T^2 = Z^2 / (X/n)$. Since $Z^2 \\sim \\chi_1^2$, $T^2 = (\\chi_1^2/1) / (\\chi_n^2/n) \\sim F_{1,n}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the definition of a t-distribution. If $T \\sim t_n$, then $T = \\frac{Z}{\\sqrt{V/n}}$ where:\n- $Z \\sim N(0,1)$ (standard normal)\n- $V \\sim \\chi^2_n$ (chi-square with $n$ degrees of freedom)\n- $Z$ and $V$ are independent.\n\nStep 2: Square both sides of the t-distribution definition:\n$T^2 = \\frac{Z^2}{V/n}$.\n\nStep 3: Observe that $Z^2 \\sim \\chi^2_1$ since the square of a standard normal variable follows a chi-square distribution with 1 degree of freedom.\n\nStep 4: Rewrite $T^2$ using the F-distribution definition from the context:\n$T^2 = \\frac{\\chi^2_1/1}{\\chi^2_n/n}$.\n\nStep 5: Compare to the general F-distribution formula $F = \\frac{(X_1/k_1)}{(X_2/k_2)}$ where $X_1 \\sim \\chi^2_{k_1}$ and $X_2 \\sim \\chi^2_{k_2}$. Here, $X_1 = Z^2$ ($k_1 = 1$) and $X_2 = V$ ($k_2 = n$).\n\nConclusion: By definition, $T^2$ follows an $F_{1,n}$ distribution.\n\nQID: textbook-122-11-2-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-11-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to prove that if $T \\\\sim t_n$, then $T^2 \\\\sim F_{1,n}$. The reasoning aligns with the definitions of the t and F distributions, and the conclusion is accurate.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-11-2-1", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to prove that if $T \\sim t_n$, then $T^2 \\sim F_{1,n}$. The reasoning aligns with the definitions of the t and F distributions, and the conclusion is accurate."}, "llm_echoed_qid": "textbook-122-11-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to prove that if $T \\sim t_n$, then $T^2 \\sim F_{1,n}$. The reasoning aligns with the definitions of the t and F distributions, and the conclusion is accurate."}
{"script_processing_qid": "textbook-107-6-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that a situation where everyone is playing a dominant strategy must be a Nash equilibrium.\n\nGOLD_ANSWER:\n1. **Definition**: A dominant strategy is one that yields the highest payoff for a player, regardless of what the other players do.\n2. **Nash Equilibrium**: A set of strategies where no player can benefit by unilaterally changing their strategy.\n3. **Proof**: If all players are playing a dominant strategy, no player can improve their payoff by changing their strategy, as the dominant strategy is already optimal for any strategy of the others. Thus, it satisfies the Nash equilibrium condition.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the definitions from the context.\n- A dominant strategy is one that is optimal for a player regardless of other players' strategies.\n- A Nash equilibrium occurs when no player can benefit by changing their strategy while others keep theirs unchanged.\n\nStep 2: Assume all players use dominant strategies. By definition, each player’s dominant strategy maximizes their payoff irrespective of others’ choices.\n\nStep 3: To disprove a Nash equilibrium, at least one player must gain by deviating. But since their current strategy is already a dominant strategy (optimal against any opponents’ strategies), deviation cannot improve their payoff.\n\nStep 4: Conclude that mutual dominant strategy play inherently satisfies the Nash equilibrium condition: no unilateral deviation is profitable.\n\nQID: textbook-107-6-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-107-6-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the logical steps to prove that a situation where everyone is playing a dominant strategy must be a Nash equilibrium. The reasoning aligns with the gold answer, clearly stating that no player can benefit by unilaterally changing their strategy when all are using dominant strategies.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-6-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the logical steps to prove that a situation where everyone is playing a dominant strategy must be a Nash equilibrium. The reasoning aligns with the gold answer, clearly stating that no player can benefit by unilaterally changing their strategy when all are using dominant strategies."}, "llm_echoed_qid": "textbook-107-6-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the logical steps to prove that a situation where everyone is playing a dominant strategy must be a Nash equilibrium. The reasoning aligns with the gold answer, clearly stating that no player can benefit by unilaterally changing their strategy when all are using dominant strategies."}
{"script_processing_qid": "textbook-112-26-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze the historical examples provided (Egypt, Bengal, China) to explain why early civilizations developed near navigable waterways.\n\nGOLD_ANSWER:\n1.  **Egypt**: Nile's canals facilitated inland navigation, enabling trade and early agricultural/industrial development.\n2.  **Bengal/China**: Ganges and other rivers created extensive canal networks, supporting commerce and opulence.\n3.  **General Principle**: Navigable waterways reduce transportation costs, expand markets, and encourage specialization, leading to early civilization.\n\nCANDIDATE_ANSWER:\nThe provided context does not explicitly mention the historical examples of Egypt, Bengal, or China. While Adam Smith's theory explains that navigable waterways facilitate market expansion by lowering transportation costs (a key driver of division of labor and economic development), the absence of direct references to these civilizations in the given text passages makes it impossible to analyze their specific historical contexts. To answer this question, additional details about how these civilizations utilized waterways in relation to market size or division of labor would be required.\n\nQID: textbook-112-26-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-26-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the absence of direct references to the civilizations in the provided context.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-26-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the absence of direct references to the civilizations in the provided context."}, "llm_echoed_qid": "textbook-112-26-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the absence of direct references to the civilizations in the provided context."}
{"script_processing_qid": "textbook-122-10-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the 90% confidence interval for $\\beta_{j}$ given $\\hat{\\beta}_{j} = 1.2$, $\\mathrm{se}(\\hat{\\beta}_{j}) = 0.3$, and $df = 25$.\n\nGOLD_ANSWER:\n1. **Critical Value**: For $df = 25$, the 95th percentile of the t-distribution is $c = 1.71$.\n2. **CI Calculation**: $$ \\text{CI} = [1.2 - 1.71 \\cdot 0.3, 1.2 + 1.71 \\cdot 0.3] = [0.687, 1.713]. $$\n3. **Interpretation**: We are 90% confident that the true $\\beta_{j}$ lies between 0.687 and 1.713.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the required confidence level and corresponding percentile. For a 90% confidence interval, we need the 95th percentile (since 100% - 90% = 10%, split equally into two tails).\n\nStep 2: Use df = 25 to find the critical t-value (c) from the t-distribution table. For df = 25 and 95th percentile (one-tailed), c ≈ 1.708.\n\nStep 3: Calculate the margin of error:\nMargin of Error = c × se(β̂_j) = 1.708 × 0.3 ≈ 0.512\n\nStep 4: Construct the confidence interval:\n1.2 ± 0.512 ⇒ (1.2 - 0.512, 1.2 + 0.512) ⇒ (0.688, 1.712)\n\nThe 90% confidence interval for β_j is (0.688, 1.712).\n\nQID: textbook-122-10-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-10-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer is correct. They accurately calculated the 90% confidence interval for β_j using the correct critical value, margin of error, and interval construction. The slight difference in the interval bounds (0.688, 1.712 vs. 0.687, 1.713) is due to rounding and is negligible.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-10-1-0", "category": "CORRECT", "explanation": "The candidate's answer is correct. They accurately calculated the 90% confidence interval for β_j using the correct critical value, margin of error, and interval construction. The slight difference in the interval bounds (0.688, 1.712 vs. 0.687, 1.713) is due to rounding and is negligible."}, "llm_echoed_qid": "textbook-122-10-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer is correct. They accurately calculated the 90% confidence interval for β_j using the correct critical value, margin of error, and interval construction. The slight difference in the interval bounds (0.688, 1.712 vs. 0.687, 1.713) is due to rounding and is negligible."}
{"script_processing_qid": "textbook-118-15-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the difference between linear independence and statistical independence in the context of econometrics.\n\nGOLD_ANSWER:\n**Linear independence** refers to the property of vectors (or matrix columns) where no vector can be expressed as a linear combination of others. **Statistical independence** refers to the property of random variables where the occurrence of one does not affect the probability distribution of another. In econometrics, linear independence ensures the invertibility of $\\boldsymbol{X}^{\\top}\\boldsymbol{X}$, while statistical independence is often an assumption about error terms.\n\nCANDIDATE_ANSWER:\nStep 1: Define Linear Independence in Context\nLinear independence in econometrics refers to the relationship between regressors (columns of matrix X). Columns are linearly independent if none can be expressed as a linear combination of others (e.g., no exact multicollinearity). This ensures invertibility of XᵀX for OLS estimation.\n\nStep 2: Example of Linear Dependence\nIf Xb = 0 for a non-zero vector b, the columns are linearly dependent. This makes XᵀX singular, preventing OLS estimation via (XᵀX)⁻¹Xᵀy.\n\nStep 3: Statistical Independence Not Addressed in Context\nThe provided text explicitly discusses linear independence but does not define or address statistical independence (typically a probabilistic concept where variables do not influence each other’s distributions).\n\nConclusion\nThe context only allows contrasting linear independence (a linear algebra property critical for OLS identifiability) with the absence of information about statistical independence. No direct comparison can be made using the provided material.\n\nQID: textbook-118-15-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-15-0-1\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly explains linear independence and its importance in econometrics but fails to address statistical independence, which is a key part of the question. The conclusion acknowledges the absence of information about statistical independence but does not provide a definition or contrast it with linear independence as required.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-15-0-1", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly explains linear independence and its importance in econometrics but fails to address statistical independence, which is a key part of the question. The conclusion acknowledges the absence of information about statistical independence but does not provide a definition or contrast it with linear independence as required."}, "llm_echoed_qid": "textbook-118-15-0-1", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly explains linear independence and its importance in econometrics but fails to address statistical independence, which is a key part of the question. The conclusion acknowledges the absence of information about statistical independence but does not provide a definition or contrast it with linear independence as required."}
{"script_processing_qid": "textbook-119-24-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Interpret the $95\\%$ confidence interval for $\\sigma^{2}$ in the context of repeated sampling.\n\nGOLD_ANSWER:\nIn repeated sampling, $95\\%$ of the constructed intervals will contain the true $\\sigma^{2}$. For any specific interval, the probability that it contains the true $\\sigma^{2}$ is either 0 or 1.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that the confidence interval formula [(n-2)σ̂²/χ²_{α/2}, (n-2)σ̂²/χ²_{1−α/2}] is constructed using critical values from the χ² distribution under the repeated sampling framework.\n\nStep 2: Understand that σ̂² (sample error variance) varies across different samples, making the interval bounds random variables.\n\nStep 3: Note that the probability statement Pr[(n−2)σ̂²/χ²_{0.025} ≤ σ² ≤ (n−2)σ̂²/χ²_{0.975}] = 0.95 holds before any specific sample is drawn.\n\nStep 4: Conclude that in repeated sampling, 95% of all possible intervals constructed this way would contain the true (fixed) σ². This means the confidence interval procedure has a 95% coverage rate over infinitely many applications under identical conditions.\n\nQID: textbook-119-24-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-24-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly interprets the $95\\\\%$ confidence interval for $\\\\sigma^{2}$ in the context of repeated sampling, aligning with the gold answer. It accurately describes the process of constructing the interval and the probabilistic interpretation of the interval's coverage of the true $\\\\sigma^{2}$.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-24-1-1", "category": "CORRECT", "explanation": "The candidate answer correctly interprets the $95\\%$ confidence interval for $\\sigma^{2}$ in the context of repeated sampling, aligning with the gold answer. It accurately describes the process of constructing the interval and the probabilistic interpretation of the interval's coverage of the true $\\sigma^{2}$."}, "llm_echoed_qid": "textbook-119-24-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly interprets the $95\\%$ confidence interval for $\\sigma^{2}$ in the context of repeated sampling, aligning with the gold answer. It accurately describes the process of constructing the interval and the probabilistic interpretation of the interval's coverage of the true $\\sigma^{2}$."}
{"script_processing_qid": "textbook-119-2-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Interpret the beta coefficient $\\hat{\\beta}_{2}^{*} = 0.9387$ in the context of the standardized regression of GPDI on GDP.\n\nGOLD_ANSWER:\nA one standard deviation increase in the (standardized) GDP leads to an average increase of 0.9387 standard deviations in the (standardized) GPDI. This indicates a strong positive relationship between the two variables.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that both GPDI (dependent variable) and GDP (independent variable) are standardized in this regression. Standardized variables have a mean of 0 and standard deviation of 1.\nStep 2: Interpret the beta coefficient (β̂₂* = 0.9387) in the context of standardized units. A one standard deviation increase in the standardized GDP corresponds to a 0.9387 standard deviation increase in the standardized GPDI, holding other variables constant.\nConclusion: The coefficient indicates that a 1-SD increase in GDP is associated with a 94% (+0.9387 SD) increase in GPDI when both variables are measured in standardized units.\n\nQID: textbook-119-2-2-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-2-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly interprets the beta coefficient in the context of standardized regression, accurately stating that a one standard deviation increase in GDP leads to a 0.9387 standard deviation increase in GPDI. The explanation aligns with the gold answer, including the recognition of standardized units and the positive relationship between the variables.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-2-2-1", "category": "CORRECT", "explanation": "The candidate answer correctly interprets the beta coefficient in the context of standardized regression, accurately stating that a one standard deviation increase in GDP leads to a 0.9387 standard deviation increase in GPDI. The explanation aligns with the gold answer, including the recognition of standardized units and the positive relationship between the variables."}, "llm_echoed_qid": "textbook-119-2-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly interprets the beta coefficient in the context of standardized regression, accurately stating that a one standard deviation increase in GDP leads to a 0.9387 standard deviation increase in GPDI. The explanation aligns with the gold answer, including the recognition of standardized units and the positive relationship between the variables."}
{"script_processing_qid": "textbook-114-12-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain why the markup over marginal cost decreases as the absolute value of the elasticity of demand increases.\n\nGOLD_ANSWER:\nA higher absolute elasticity means demand is more sensitive to price changes. The monopolist cannot significantly raise prices without losing many customers, so the optimal markup shrinks. In the limit of perfect elasticity ($|E_d| \\to \\infty$), the markup approaches zero, aligning with competitive markets where $P = MC$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the profit-maximizing condition derived from the context: MR = MC. From the marginal revenue derivation, we have MR = P + P(1/E_d). Setting this equal to marginal cost gives P + P(1/E_d) = MC.\n\nStep 2: Rearrange the equation to isolate the markup term (P - MC)/P:\n(P - MC)/P = -1/E_d\n\nStep 3: Observe that the left side (P - MC)/P represents the markup percentage over marginal cost relative to price. The right side -1/E_d depends on the elasticity's absolute value since E_d is inherently negative in demand curves.\n\nStep 4: Recognize that as |E_d| (absolute elasticity) increases, the magnitude of 1/|E_d| decreases. For example, if |E_d| rises from 2 to 4, 1/|E_d| decreases from 0.5 to 0.25.\n\nStep 5: Conclude that a higher |E_d| (more elastic demand) reduces the markup fraction (P - MC)/P, forcing firms to set prices closer to marginal cost to avoid losing sales in elastic markets.\n\nQID: textbook-114-12-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-12-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly explains the relationship between markup over marginal cost and the absolute value of the elasticity of demand. It follows a logical step-by-step derivation, showing how increased elasticity leads to a reduced markup, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-12-1-2", "category": "CORRECT", "explanation": "The candidate's answer correctly explains the relationship between markup over marginal cost and the absolute value of the elasticity of demand. It follows a logical step-by-step derivation, showing how increased elasticity leads to a reduced markup, aligning with the gold answer."}, "llm_echoed_qid": "textbook-114-12-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly explains the relationship between markup over marginal cost and the absolute value of the elasticity of demand. It follows a logical step-by-step derivation, showing how increased elasticity leads to a reduced markup, aligning with the gold answer."}
{"script_processing_qid": "textbook-115-17-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the test statistic for $H_0: \\sigma_c^2 = 0$ based on the expression $N^{-1/2}\\sum_{i=1}^N \\sum_{t=1}^{T-1} \\sum_{s=t+1}^T \\hat{v}_{it} \\hat{v}_{is}$ and show its asymptotic normality under the null.\n\nGOLD_ANSWER:\n1. **Statistic**: $S = N^{-1/2}\\sum_{i=1}^N \\sum_{t=1}^{T-1} \\sum_{s=t+1}^T \\hat{v}_{it} \\hat{v}_{is}$.\n2. **Variance**: Under $H_0$, $S$ has variance $\\operatorname{E}\\left(\\sum_{t=1}^{T-1} \\sum_{s=t+1}^T v_{it} v_{is}\\right)^2$.\n3. **Asymptotics**: By CLT, $S$ converges to $N(0, \\text{Var}(S))$ under $H_0$.\n\nCANDIDATE_ANSWER:\nStep 1: Under H₀: σ_c² = 0, the errors {v_it} are serially uncorrelated with E[v_{it}v_{is}] = 0 for t ≠ s. The test statistic is based on the scaled sum of residual cross-products: \n\nT_N = N^{-1/2} ∑_{i=1}^N ∑_{t=1}^{T-1} ∑_{s=t+1}^T v̂_{it}v̂_{is}\n\nStep 2: Since residuals v̂_{it} consistently estimate v_{it}, T_N behaves asymptotically like N^{-1/2} ∑_{i=1}^N ∑_{t=1}^{T-1} ∑_{s=t+1}^T v_{it}v_{is}. Under H₀, each v_{it}v_{is} has mean 0 and finite variance (by strict exogeneity of x_{it}).\n\nStep 3: By the cross-sectional independence assumption, {∑_{t<s}v_{it}v_{is}} are i.i.d. across i. The Central Limit Theorem implies:\n\nN^{-1/2} ∑_{i=1}^N [∑_{t<s}v_{it}v_{is}] →^d Normal(0, τ²)\n\nwhere τ² = Var(∑_{t<s}v_{it}v_{is}) = E[(∑_{t<s}v_{it}v_{is})²] (since E[∑_{t<s}v_{it}v_{is}] = 0).\n\nStep 4: τ² can be consistently estimated by N^{-1}∑_{i=1}^N [∑_{t<s}v̂_{it}v̂_{is}]². The standardized statistic:\n\nT_N / sqrt(τ̂²) →^d Normal(0,1)\n\nThis establishes asymptotic normality under H₀.\n\nQID: textbook-115-17-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-17-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the test statistic, explains its asymptotic behavior under the null hypothesis, and justifies the use of the Central Limit Theorem to establish asymptotic normality. The steps align with the gold answer, providing a detailed and accurate explanation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-17-1-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the test statistic, explains its asymptotic behavior under the null hypothesis, and justifies the use of the Central Limit Theorem to establish asymptotic normality. The steps align with the gold answer, providing a detailed and accurate explanation."}, "llm_echoed_qid": "textbook-115-17-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the test statistic, explains its asymptotic behavior under the null hypothesis, and justifies the use of the Central Limit Theorem to establish asymptotic normality. The steps align with the gold answer, providing a detailed and accurate explanation."}
{"script_processing_qid": "textbook-48-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Using the Poterba and Summers (1987) argument, explain why Ricardian equivalence might hold approximately in the short run even without intergenerational altruism.\n\nGOLD_ANSWER:\n1.  **Time Horizon**: If a tax cut is financed by debt with a 5% interest rate, 77% of future taxes occur within 30 years, limiting intergenerational redistribution.\n2.  **Marginal Propensity to Consume**: Life-cycle consumers have a small marginal propensity to consume out of wealth, so the redistribution effect on consumption is minimal.\n3.  **Private Saving**: Households may increase saving by roughly the size of the deficit, offsetting the tax cut.\n4.  **Conclusion**: Short-run deviations from Ricardian equivalence due to intergenerational redistribution are small, making it approximately valid for some policies.\n\nCANDIDATE_ANSWER:\nStep 1: Ricardian equivalence generally relies on intergenerational altruism (as in Barro's model) to ensure households internalize future tax liabilities. However, Poterba and Summers (1987) argue that even without altruism, equivalence might hold approximately in the short run.<br><br>Step 2: Key insight: If government debt issued today is expected to be repaid during the lifetime of the current generation (rather than passed to future generations), households will treat debt-financed tax cuts as equivalent to future tax increases.<br><br>Step 3: In this scenario, households save the tax cut to prepare for the anticipated tax increase during their own lifetime. This creates no net wealth effect and preserves consumption patterns, mimicking Ricardian equivalence.<br><br>Step 4: This short-run approximation breaks down in the long run if debt repayment extends beyond the current generation's lifespan (requiring true intergenerational transfers), but holds temporarily when fiscal timing aligns with household lifespans.\n\nQID: textbook-48-1-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-48-1-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains Poterba and Summers' argument, emphasizing the alignment of debt repayment with the current generation's lifespan and the resulting short-run approximation of Ricardian equivalence. This aligns with the key points in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-48-1-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately explains Poterba and Summers' argument, emphasizing the alignment of debt repayment with the current generation's lifespan and the resulting short-run approximation of Ricardian equivalence. This aligns with the key points in the gold answer."}, "llm_echoed_qid": "textbook-48-1-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains Poterba and Summers' argument, emphasizing the alignment of debt repayment with the current generation's lifespan and the resulting short-run approximation of Ricardian equivalence. This aligns with the key points in the gold answer."}
{"script_processing_qid": "textbook-79-0-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain why any mean-preserving spread of a distribution $F$ is second-order stochastically dominated by $F$. Provide an economic interpretation.\n\nGOLD_ANSWER:\n3. A mean-preserving spread transfers probability mass from the center of $F$ to the tails, increasing variance without changing the mean. For risk-averse agents, this reduces expected utility because they dislike dispersion. Thus, $F$ SOSD its mean-preserving spreads, as the original distribution is preferred due to lower risk.\n\nCANDIDATE_ANSWER:\nStep 1: Recall Definitions\n- A mean-preserving spread (MPS) redistributes probability mass away from the center of distribution F while preserving the mean, creating greater dispersion.\n- Distribution G second-order stochastically dominates (SSD) distribution F if ∫₀ˣ [G(t) − F(t)] dt ≤ 0 for all x (i.e., F has less downside risk).\n\nStep 2: Link MPS to SSD\nAny MPS of F creates a riskier distribution G with the same mean but higher variance. For risk-averse agents with concave utility u''(x) < 0, E[u(G)] ≤ E[u(F)] because increased dispersion reduces expected utility under risk aversion.\n\nStep 3: Formalize SSD Relationship\nSince MPS implies G differs from F through increased risk without changing the mean, the integral condition for SSD (∫₀ˣ [F(t) − G(t)] dt ≥ 0 ∀x) holds because F concentrates more probability mass in lower wealth regions compared to G's spread-out tails.\n\nEconomic Interpretation:\nRisk-averse decision makers prefer the original distribution F over its mean-preserving spread G because F offers the same expected return with lower risk exposure. The MPS introduces compensated risk (same mean) that is undesirable to concave utility maximizers.\n\nQID: textbook-79-0-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-79-0-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains why any mean-preserving spread of a distribution F is second-order stochastically dominated by F, including the economic interpretation. The reasoning aligns with the gold answer, detailing the redistribution of probability mass, the impact on risk-averse agents, and the formal SSD relationship.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-79-0-1-2", "category": "CORRECT", "explanation": "The candidate answer correctly explains why any mean-preserving spread of a distribution F is second-order stochastically dominated by F, including the economic interpretation. The reasoning aligns with the gold answer, detailing the redistribution of probability mass, the impact on risk-averse agents, and the formal SSD relationship."}, "llm_echoed_qid": "textbook-79-0-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains why any mean-preserving spread of a distribution F is second-order stochastically dominated by F, including the economic interpretation. The reasoning aligns with the gold answer, detailing the redistribution of probability mass, the impact on risk-averse agents, and the formal SSD relationship."}
{"script_processing_qid": "textbook-67-0-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Derive the government's intertemporal budget constraint in the context of a stabilization program and discuss how it interacts with the household's budget constraint to determine equilibrium outcomes.\n\nGOLD_ANSWER:\n1. **Government Constraint**: $$\\int_{0}^{\\infty} \\tau_t \\exp(-r t) \\mathrm{d}t = g_0 + \\int_{0}^{\\infty} g_t \\exp(-r t) \\mathrm{d}t,$$ where $\\tau_t$ is taxes and $g_t$ is spending.\n2. **Household Constraint**: As in the text, includes taxes and spending.\n3. **Interaction**: Combined, they determine the feasible paths for $c_t$, $\\ell_t$, and $g_t$. Fiscal adjustments affect household wealth and thus consumption and labor supply.\n\nCANDIDATE_ANSWER:\nStep 1: Formulate the government's intertemporal budget constraint (IBC). The government must satisfy that the present value of future primary surpluses (taxes minus spending) plus seigniorage equals its initial debt. Mathematically: \\(B_0 = \\sum_{t=0}^\\infty \\frac{T_t - G_t + S_t}{(1 + r)^t}\\), where \\(B_0\\) is initial debt, \\(T_t\\) taxes, \\(G_t\\) spending, \\(S_t\\) seigniorage, and \\(r\\) the discount rate. In stabilization programs, reductions in inflation tax (a component of \\(S_t\\)) or future spending cuts (lower \\(G_t\\)) affect this constraint.\n\nStep 2: Link the government's IBC to household behavior. Households face their own IBC: \\(\\sum_{t=0}^\\infty \\frac{C_t}{(1 + r)^t} = W_0 + \\sum_{t=0}^\\infty \\frac{Y_t - T_t}{(1 + r)^t}\\), where \\(C_t\\) is consumption, \\(W_0\\) initial wealth, and \\(Y_t\\) income. If Ricardian equivalence fails (as in Helpman & Razin, 1987), changes in \\(T_t\\) or \\(S_t\\) directly alter household wealth perceptions.\n\nStep 3: Analyze interactions. A stabilization program (e.g., cutting \\(G_t\\) or reducing inflation tax) modifies the government's IBC. Households interpret this as a wealth increase (due to non-Ricardian effects), raising \\(C_t\\). In Rebelo & Végh (1995), reduced government tradable consumption raises private tradable consumption, causing real appreciation (via higher non-tradable demand) and improved current account (lower public absorption of tradables).\n\nStep 4: Equilibrium outcomes. The joint satisfaction of both IBCs determines variables like consumption, real exchange rates, and current account. For example, fiscal tightening (lower \\(G_t\\)) relaxes the government's IBC but induces a private consumption boom if households perceive higher net wealth, leading to real appreciation and investment/current account adjustments as resources shift between sectors.\n\nQID: textbook-67-0-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-67-0-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately derives the government's intertemporal budget constraint and effectively links it to household behavior, discussing the interaction and equilibrium outcomes in a manner consistent with the gold answer. The explanation includes relevant economic concepts and references, demonstrating a thorough understanding of the topic.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-67-0-2-3", "category": "CORRECT", "explanation": "The candidate's answer accurately derives the government's intertemporal budget constraint and effectively links it to household behavior, discussing the interaction and equilibrium outcomes in a manner consistent with the gold answer. The explanation includes relevant economic concepts and references, demonstrating a thorough understanding of the topic."}, "llm_echoed_qid": "textbook-67-0-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately derives the government's intertemporal budget constraint and effectively links it to household behavior, discussing the interaction and equilibrium outcomes in a manner consistent with the gold answer. The explanation includes relevant economic concepts and references, demonstrating a thorough understanding of the topic."}
{"script_processing_qid": "textbook-115-16-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the Wooldridge (1995a) and Honoré and Kyriazidou (2000b) methods for handling attrition. Highlight the trade-offs between robustness and efficiency.\n\nGOLD_ANSWER:\n1. **Wooldridge (1995a)**: \n   - Uses parametric assumptions (normality). \n   - More efficient under correct specification. \n   - Less robust to misspecification. \n2. **Honoré and Kyriazidou (2000b)**: \n   - Nonparametric, no distributional assumptions. \n   - More robust but less efficient. \n   - Requires larger samples for precision. \n3. **Trade-off**: Choose Wooldridge for efficiency when assumptions hold; Honoré and Kyriazidou for robustness.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Key Features of Each Method\n- Wooldridge (1995a) relies on parametric assumptions (e.g., joint normality of Δu_it and v_it) to model attrition bias using inverse Mills ratios (Heckman-type correction).\n- Honoré & Kyriazidou (2000b) likely employs semiparametric or instrumental variables (IV) approaches, avoiding strict distributional assumptions.\n\nStep 2: Robustness Trade-off\n- Wooldridge’s method is less robust because it requires correct specification of the error distribution (e.g., normality). Violations lead to inconsistent estimates.\n- Honoré & Kyriazidou’s method is more robust: It avoids parametric assumptions (if semiparametric) or relies on IV exclusion restrictions (if IV-based), which are weaker than joint normality.\n\nStep 3: Efficiency Trade-off\n- Wooldridge’s method is more efficient under correct assumptions, as parametric models achieve lower variance asymptotically.\n- Honoré & Kyriazidou’s method sacrifices efficiency for robustness: Semiparametric/IV estimators typically have larger standard errors or require stronger identification assumptions (e.g., valid IVs).\n\nStep 4: Summary of Trade-offs\n- Wooldridge: Efficiency gains under parametric correctness vs. vulnerability to misspecification.\n- Honoré & Kyriazidou: Robustness to distributional misspecification or attrition mechanisms vs. higher data requirements (strong IVs) or reduced precision.\n\nQID: textbook-115-16-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-16-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies the key features of both methods, correctly highlights the trade-offs between robustness and efficiency, and aligns with the gold answer in terms of content and reasoning.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-16-1-3", "category": "CORRECT", "explanation": "The candidate answer accurately identifies the key features of both methods, correctly highlights the trade-offs between robustness and efficiency, and aligns with the gold answer in terms of content and reasoning."}, "llm_echoed_qid": "textbook-115-16-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies the key features of both methods, correctly highlights the trade-offs between robustness and efficiency, and aligns with the gold answer in terms of content and reasoning."}
{"script_processing_qid": "textbook-115-8-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the heteroskedasticity-robust variance matrix estimator $\\hat{\\mathbf{A}}^{-1}\\hat{\\mathbf{B}}\\hat{\\mathbf{A}}^{-1}/N$.\n\nGOLD_ANSWER:\n1.  **Estimate $\\mathbf{A}$**: $\\hat{\\mathbf{A}} = N^{-1}\\sum_{i=1}^{N}\\mathbf{x}_{i}^{\\prime}\\mathbf{x}_{i}$.\n2.  **Estimate $\\mathbf{B}$**: $\\hat{\\mathbf{B}} = N^{-1}\\sum_{i=1}^{N}\\hat{u}_{i}^{2}\\mathbf{x}_{i}^{\\prime}\\mathbf{x}_{i}$.\n3.  **Combine**: $\\operatorname{Avar}(\\hat{\\pmb{\\beta}}) = \\hat{\\mathbf{A}}^{-1}\\hat{\\mathbf{B}}\\hat{\\mathbf{A}}^{-1}/N = (\\mathbf{X}^{\\prime}\\mathbf{X})^{-1}\\left(\\sum_{i=1}^{N}\\hat{u}_{i}^{2}\\mathbf{x}_{i}^{\\prime}\\mathbf{x}_{i}\\right)(\\mathbf{X}^{\\prime}\\mathbf{X})^{-1}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the asymptotic distribution under heteroskedasticity. The asymptotic variance of √N(β̂−β) is A⁻¹BA⁻¹, where:\n  - A = E(x'x)\n  - B = E(u²x'x)\n\nStep 2: Compute estimators for A and B:\n  - Â = (1/N)∑x_i'x_i (sample analog of A)\n  - B̂ = (1/N)∑û_i²x_i'x_i (uses squared residuals û_i² to account for heteroskedasticity)\n\nStep 3: Form the sandwich estimator for asymptotic variance:\n  Asy.Var(√N(β̂−β)) ≈ Â⁻¹B̂Â⁻¹\n\nStep 4: Scale by 1/N to get the variance matrix for β̂ (not √Nβ̂):\n  Var(β̂) ≈ (Â⁻¹B̂Â⁻¹)/N\n\nThis gives the heteroskedasticity-robust variance matrix estimator Â⁻¹B̂Â⁻¹/N, where the middle term B̂ replaces the homoskedastic σ²A with a heteroskedasticity-consistent residual-weighted average of x_i'x_i.\n\nQID: textbook-115-8-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-8-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the heteroskedasticity-robust variance matrix estimator step-by-step, matching the gold answer's structure and formulas (Â⁻¹B̂Â⁻¹/N). The explanation also clarifies the scaling by 1/N and the role of B̂ in handling heteroskedasticity.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-8-1-2", "category": "CORRECT", "explanation": "The candidate answer correctly derives the heteroskedasticity-robust variance matrix estimator step-by-step, matching the gold answer's structure and formulas (Â⁻¹B̂Â⁻¹/N). The explanation also clarifies the scaling by 1/N and the role of B̂ in handling heteroskedasticity."}, "llm_echoed_qid": "textbook-115-8-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the heteroskedasticity-robust variance matrix estimator step-by-step, matching the gold answer's structure and formulas (Â⁻¹B̂Â⁻¹/N). The explanation also clarifies the scaling by 1/N and the role of B̂ in handling heteroskedasticity."}
{"script_processing_qid": "textbook-114-12-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Using the markup pricing rule $\\frac{P - MC}{P} = -\\frac{1}{E_d}$, calculate the optimal price if $MC = \\$9$ and the elasticity of demand is $-4$.\n\nGOLD_ANSWER:\n1. **Markup Rule**: $\\frac{P - 9}{P} = -\\frac{1}{-4} = 0.25$.\n2. **Solve for P**: $P - 9 = 0.25P \\implies 0.75P = 9 \\implies P = \\frac{9}{0.75} = \\$12$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the given markup pricing formula: \\[(P - MC)/P = -1/E_d\\]\nStep 2: Substitute MC = $9 and E_d = -4:\\[(P - 9)/P = -1/(-4) = 1/4\\]\nStep 3: Multiply both sides by P to eliminate the denominator:\\[P - 9 = P/4\\]\nStep 4: Rearrange to isolate P:\\[P - P/4 = 9\\]\\[3P/4 = 9\\]\nStep 5: Solve for P:\\[P = 9 \\times (4/3) = \\$12\\]\nThe optimal price is $12.\n\nQID: textbook-114-12-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-12-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to solve for the optimal price using the markup pricing rule, arriving at the correct answer of $12.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-12-1-1", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to solve for the optimal price using the markup pricing rule, arriving at the correct answer of $12."}, "llm_echoed_qid": "textbook-114-12-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to solve for the optimal price using the markup pricing rule, arriving at the correct answer of $12."}
{"script_processing_qid": "textbook-43-1-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain the role of sunspot shocks in the monetary transmission mechanism under indeterminate equilibria.\n\nGOLD_ANSWER:\n1.  **Multiple equilibria**: Indeterminacy allows sunspots to influence $C\\_{t+1}$ and $m\\_{t+1}$, which are not predetermined.\n2.  **Propagation mechanism**: Sunspots act as additional shocks, enriching the model's dynamics beyond fundamental shocks.\n3.  **Empirical fit**: The model can match observed data by selecting sunspot processes that align with residual correlations.\n\nCANDIDATE_ANSWER:\nStep 1: Under indeterminate equilibria, the matrix A in Farmer's (1997) model has all four roots inside the unit circle (based on calibrated parameters), violating the unique equilibrium condition.\n\nStep 2: When equilibria are indeterminate, the rational expectations solution is not uniquely determined by fundamentals. This allows sunspot shocks (non-fundamental disturbances like self-fulfilling beliefs, represented by the e_{t+1} terms in vector B) to influence economic variables.\n\nStep 3: In the monetary transmission mechanism, sunspot shocks propagate through the system dynamics via matrix B. They affect future values of money growth (μ), consumption (C), capital (K), and real balances (m) in the difference equation structure.\n\nStep 4: This creates multiple equilibrium paths where monetary policy effects depend on expectations. VAR analysis would detect these sunspot-driven fluctuations as empirically relevant shocks, even though they lack direct economic fundamentals.\n\nQID: textbook-43-1-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-43-1-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the role of sunspot shocks in the monetary transmission mechanism under indeterminate equilibria, detailing how they influence economic variables and create multiple equilibrium paths, which aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-43-1-1-2", "category": "CORRECT", "explanation": "The candidate answer accurately explains the role of sunspot shocks in the monetary transmission mechanism under indeterminate equilibria, detailing how they influence economic variables and create multiple equilibrium paths, which aligns with the gold answer."}, "llm_echoed_qid": "textbook-43-1-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the role of sunspot shocks in the monetary transmission mechanism under indeterminate equilibria, detailing how they influence economic variables and create multiple equilibrium paths, which aligns with the gold answer."}
{"script_processing_qid": "textbook-115-22-3-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the implications of part c for empirical research. What alternatives exist when all durations are censored?\n\nGOLD_ANSWER:\n1.  **Implications**: Estimation is infeasible without additional information, such as variation in censoring times or covariates.\n2.  **Alternatives**: Use external data to inform the baseline hazard or consider alternative sampling designs.\n\nCANDIDATE_ANSWER:\nStep 1: Recall from part c that with all durations censored and a Weibull model containing only a constant term, the log-likelihood cannot be maximized for real parameter values. This occurs because the likelihood function becomes monotonic in parameters (e.g., approaches its supremum as α → 0 or β → ∞), preventing identification of finite MLE estimates.\n\nStep 2: Implications for empirical research:\n- Standard parametric duration models (like Weibull) become inestimable under full right-censoring with flow sampling\n- Researchers cannot recover baseline hazard/shape parameters\n- Results highlight fundamental identification problem when no uncensored durations are observed\n\nStep 3: Alternatives when all durations are censored:\n1. Obtain supplementary data with uncensored durations (e.g., stock sampling)\n2. Use external information to fix parameters (e.g., impose α from prior studies)\n3. Switch to semi-parametric approaches (e.g., Cox model with external benchmark hazards)\n4. Alternative sampling designs (e.g., combine flow and stock samples)\n5. Bayesian methods with strongly informative priors (if credible prior information exists)\n\nQID: textbook-115-22-3-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-22-3-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately discusses the implications of part c for empirical research, noting the inestimability of standard parametric models under full right-censoring and the identification problem. The alternatives provided are comprehensive and align well with the gold answer, including the use of external data, alternative sampling designs, and semi-parametric approaches.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-22-3-2", "category": "CORRECT", "explanation": "The candidate answer accurately discusses the implications of part c for empirical research, noting the inestimability of standard parametric models under full right-censoring and the identification problem. The alternatives provided are comprehensive and align well with the gold answer, including the use of external data, alternative sampling designs, and semi-parametric approaches."}, "llm_echoed_qid": "textbook-115-22-3-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately discusses the implications of part c for empirical research, noting the inestimability of standard parametric models under full right-censoring and the identification problem. The alternatives provided are comprehensive and align well with the gold answer, including the use of external data, alternative sampling designs, and semi-parametric approaches."}
{"script_processing_qid": "textbook-24-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Using the footloose entrepreneur model, derive the conditions under which overagglomeration or underagglomeration occurs. What welfare implications does this have for immobile factors?\n\nGOLD_ANSWER:\n4.  **Conditions**:\n   - **Overagglomeration**: Occurs when the relative stock of immobile factors is large and agglomeration forces are strong ($\\frac{L_{immobile}}{L_{mobile}} > \\gamma$ and $\\phi > \\phi^*$).\n   - **Underagglomeration**: Occurs when immobile factors are scarce and agglomeration forces are weak.\n\n   **Welfare Implications**:\n   - Immobile factors in the periphery face higher prices and lower wages under overagglomeration.\n   - First-best allocations require targeting both monopoly power and pecuniary externalities.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the footloose entrepreneur (FE) model framework. Entrepreneurs are mobile across regions, while immobile factors (e.g., unskilled labor) remain fixed. Agglomeration forces stem from circular causality between firm location and market size (via home market effects and price-index linkages).\n\nStep 2: Identify market equilibrium vs. social optimum. Market outcomes depend on individual entrepreneurs' location decisions based on private returns, while social optimum considers pecuniary externalities affecting all agents (including immobile factors).\n\nStep 3: Derive overagglomeration condition. Occurs when:<br/>- Private incentives to agglomerate exceed social benefits<br/>- Caused by entrepreneurs ignoring negative externalities on periphery consumers (higher import prices) and immobile factors in the core (congestion effects)<br/>- More likely at intermediate trade costs where agglomeration forces are strong but not fully internalized\n\nStep 4: Derive underagglomeration condition. Occurs when:<br/>- Social benefits of agglomeration exceed private incentives<br/>- Arises from coordination failures or threshold effects (Baldwin's policy traps)<br/>- More likely at very high/low trade costs where multiple equilibria exist but market selects suboptimal dispersion\n\nStep 5: Welfare implications for immobile factors:<br/>- Overagglomeration: Immobile factors in periphery face higher prices (welfare loss), while core factors benefit from lower prices but face congestion<br/>- Undergglomeration: Both regions lose potential gains from scale economies, keeping prices higher than optimal in all locations<br/>- Optimal agglomeration balances these effects through policy interventions addressing pecuniary externalities\n\nQID: textbook-24-0-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-24-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses both the conditions for overagglomeration and underagglomeration, aligning with the gold answer. It also correctly details the welfare implications for immobile factors, including the effects on prices and wages, matching the gold answer's points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-24-0-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses both the conditions for overagglomeration and underagglomeration, aligning with the gold answer. It also correctly details the welfare implications for immobile factors, including the effects on prices and wages, matching the gold answer's points."}, "llm_echoed_qid": "textbook-24-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses both the conditions for overagglomeration and underagglomeration, aligning with the gold answer. It also correctly details the welfare implications for immobile factors, including the effects on prices and wages, matching the gold answer's points."}
{"script_processing_qid": "textbook-122-27-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. How does an increase in income affect the demand curve? Provide both a graphical and mathematical explanation.\n\nGOLD_ANSWER:\n1. **Mathematical Explanation**: An increase in income shifts the intercept of the demand function upward. For example, if income increases by $\\$100$, the new intercept becomes $120 + 0.03(100) = 123$, but the slope remains $-9.8$.\n2. **Graphical Explanation**: In the price-quantity plane, the demand curve shifts parallel upward, indicating higher quantity demanded at every price level. The slope (steepness) of the curve remains unchanged.\n\nCANDIDATE_ANSWER:\nStep 1: Graphical Explanation\nAn increase in income causes a rightward parallel shift of the demand curve in price-quantity space. The original demand curve (drawn at income = $900 in Figure A.2) shows the relationship between price and quantity with fixed income. When income rises, the quantity demanded increases at every price level, resulting in a new demand curve positioned to the right of the original one. The slope (-9.8) remains unchanged because it represents the partial effect of price holding income constant.\n\nStep 2: Mathematical Explanation\nLet the demand function be Q = α - 9.8P + βY, where β > 0 (assuming normal goods). The original demand curve at Y = $900 is Q = (α + 900β) - 9.8P. An income increase to Y' > $900 changes the intercept to (α + Y'β) while maintaining the same slope (-9.8). This results in a new parallel demand curve equation: Q = (α + Y'β) - 9.8P. The horizontal shift magnitude equals βΔY, where ΔY is the income change.\n\nQID: textbook-122-27-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-27-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly explains both the graphical and mathematical effects of an increase in income on the demand curve. The graphical explanation accurately describes a rightward parallel shift, and the mathematical explanation correctly adjusts the intercept while maintaining the slope, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-27-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly explains both the graphical and mathematical effects of an increase in income on the demand curve. The graphical explanation accurately describes a rightward parallel shift, and the mathematical explanation correctly adjusts the intercept while maintaining the slope, aligning with the gold answer."}, "llm_echoed_qid": "textbook-122-27-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly explains both the graphical and mathematical effects of an increase in income on the demand curve. The graphical explanation accurately describes a rightward parallel shift, and the mathematical explanation correctly adjusts the intercept while maintaining the slope, aligning with the gold answer."}
{"script_processing_qid": "textbook-41-0-3-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What is the significance of the boundary behavior of the aggregate excess demand function?\n\nGOLD_ANSWER:\n3. **Boundary Behavior**: As the price of a good approaches zero, the excess demand for that good must diverge to infinity. This ensures that all goods are desirable and prevents zero prices in equilibrium.\n\nCANDIDATE_ANSWER:\nThe provided context does not explicitly discuss the significance of the boundary behavior of the aggregate excess demand function. While the section mentions that boundary behavior is one of the examined properties (alongside homogeneity and Walras' law), the extracted text passages only state that certain propositions (iii) and (i) follow from homogeneity and continuity. No further details about the implications or practical relevance of boundary behavior are included in the given context. Therefore, the information required to address this question is not available in the provided material.\n\nQID: textbook-41-0-3-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-41-0-3-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-41-0-3-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context."}, "llm_echoed_qid": "textbook-41-0-3-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context."}
{"script_processing_qid": "textbook-108-2-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain how forward-looking parents could theoretically offset the burden of government debt on their children. Discuss the limitations of this argument in practice.\n\nGOLD_ANSWER:\n1. **Mechanism**: Parents save more/leave larger bequests to compensate for future tax burdens.\n2. **Limitations**: Not all parents are farsighted; bequests may not fully offset debt; liquidity constraints may prevent optimal saving.\n\nCANDIDATE_ANSWER:\nStep 1: **Theoretical Offset Mechanism**\nForward-looking parents could theoretically offset government debt burdens on their children by increasing personal savings. If current deficits imply future tax increases for the next generation, parents might save more today (e.g., through larger bequests or investments) to compensate. This behavior aligns with the Ricardian equivalence hypothesis, where private savings adjustments neutralize intergenerational redistribution from debt.\n\nStep 2: **Practical Limitations**\n1. **Other Redistributive Policies**: Fiscal policies beyond debt (e.g., Social Security, education spending) also redistribute income across generations. Parents cannot easily offset these combined effects through private savings alone.\n2. **Liquidity Constraints**: Many households lack sufficient income to increase savings, limiting their ability to leave larger bequests.\n3. **Myopia & Short-Termism**: Parents may prioritize current consumption over future generations’ welfare due to cognitive biases or imperfect information about long-term fiscal impacts.\n4. **Dynamic Fiscal Context**: If government debt grows slower than national income (as noted in the context), its burden may remain manageable without parental intervention, reducing perceived urgency to act.\n5. **Political Uncertainty**: Future tax policies or debt management decisions are unpredictable, complicating parents’ ability to calculate and offset liabilities accurately.\n\nQID: textbook-108-2-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-108-2-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes the theoretical mechanism (Ricardian equivalence) and provides a comprehensive list of practical limitations, including liquidity constraints, myopia, and political uncertainty, which align with and expand upon the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-2-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately describes the theoretical mechanism (Ricardian equivalence) and provides a comprehensive list of practical limitations, including liquidity constraints, myopia, and political uncertainty, which align with and expand upon the gold answer."}, "llm_echoed_qid": "textbook-108-2-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes the theoretical mechanism (Ricardian equivalence) and provides a comprehensive list of practical limitations, including liquidity constraints, myopia, and political uncertainty, which align with and expand upon the gold answer."}
{"script_processing_qid": "textbook-118-8-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Prove that system estimation reduces to equation-by-equation IV estimation when all equations are just identified.\n\nGOLD_ANSWER:\n1. **Just-identified equations**: For each equation $i$, the number of instruments ($l$) equals the number of parameters ($k_i$). Thus, $P\\_{W}X\\_{i}$ spans the same space as $W$, and there exists a matrix $J_i$ such that $P\\_{W}X\\_{i}J_i = W$.\n2. **Decoupling**: Premultiplying the $i^{\\text{th}}$ estimating equation by $J_i^{\\top}$ and summing over $i$ yields $$ W^{\\top}({\\pmb y}\\_{m}-X\\_{m}\\beta\\_{m})={\\pmb0}, $$ which is the equation-by-equation IV estimator. No efficiency gain arises because all moment conditions are exhausted.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that system estimation (like 3SLS) generally requires solving coupled equations due to error term correlations (Σ matrix).\nStep 2: Under just identification, the number of instruments equals the number of regressors in every equation. This removes overidentification restrictions.\nStep 3: The provided context shows that in this case, estimating equations simplify to W^⊤(y_m − X_mβ_m) = 0 for each equation m=1,...,g.\nStep 4: This equation matches the standard IV moment condition E[W^⊤u_m] = 0 for each individual equation.\nStep 5: Since these equations are decoupled (no cross-equation terms remain), solving them equation-by-equation produces identical results to full system estimation.\nStep 6: Therefore, system estimation collapses to equation-wise IV estimation under full just-identification.\n\nQID: textbook-118-8-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-8-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately follows the reasoning in the gold answer, clearly explaining how system estimation simplifies to equation-by-equation IV estimation under just-identification conditions. Both highlight the decoupling of equations and the equivalence of moment conditions.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-8-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately follows the reasoning in the gold answer, clearly explaining how system estimation simplifies to equation-by-equation IV estimation under just-identification conditions. Both highlight the decoupling of equations and the equivalence of moment conditions."}, "llm_echoed_qid": "textbook-118-8-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately follows the reasoning in the gold answer, clearly explaining how system estimation simplifies to equation-by-equation IV estimation under just-identification conditions. Both highlight the decoupling of equations and the equivalence of moment conditions."}
{"script_processing_qid": "textbook-123-24-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What conditions must the matrices $A$, $B$, $Q$, and $R$ satisfy for the Riccati equation to have a unique stabilizing solution?\n\nGOLD_ANSWER:\nFor the Riccati equation to have a unique stabilizing solution, the following conditions must hold:\n1. **Controllability**: The pair $(A, B)$ must be controllable.\n2. **Observability**: The pair $(A, C)$ must be observable, where $Q = C^{\\prime}C$.\n3. **Positive Definiteness**: $Q$ must be positive semidefinite, and $R$ must be positive definite.\nThese conditions ensure that the cost function is convex and the optimal control problem is well-posed.\n\nCANDIDATE_ANSWER:\nStep 1: The Riccati equation (18.A.6) is derived from the optimal linear regulator problem with system matrices \\( A, B \\) and penalty matrices \\( Q, R \\).\n\nStep 2: For uniqueness and stability of the solution \\( P \\), the closed-loop system matrix \\( A - BF \\) (where \\( F = \beta(Q + \beta B'PB)^{-1}B'PA \\)) must have eigenvalues strictly inside the unit circle (\\u201cstabilizing\\u201d).\n\nStep 3: Standard conditions (implied by the context\\u2019s optimal control framework):\n- \\( Q \\) must be positive semi-definite (ensures non-negative state penalties).\n- \\( R \\) must be positive definite (ensures strict convexity in control).\n- \\( (A, B) \\) must be stabilizable (to guarantee existence of a stabilizing \\( F \\)).\n- \\( (A, C) \\) must be detectable, where \\( Q = C'C \\) (ensures observability of unstable modes).\n\nThese conditions ensure the algebraic Riccati equation has a unique positive semi-definite stabilizing solution \\( P \\).\n\nQID: textbook-123-24-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-24-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the necessary conditions for the Riccati equation to have a unique stabilizing solution, including the positive semi-definiteness of \\( Q \\), positive definiteness of \\( R \\), stabilizability of \\( (A, B) \\), and detectability of \\( (A, C) \\). These align with the gold answer's requirements.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-24-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the necessary conditions for the Riccati equation to have a unique stabilizing solution, including the positive semi-definiteness of \\( Q \\), positive definiteness of \\( R \\), stabilizability of \\( (A, B) \\), and detectability of \\( (A, C) \\). These align with the gold answer's requirements."}, "llm_echoed_qid": "textbook-123-24-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the necessary conditions for the Riccati equation to have a unique stabilizing solution, including the positive semi-definiteness of \\( Q \\), positive definiteness of \\( R \\), stabilizability of \\( (A, B) \\), and detectability of \\( (A, C) \\). These align with the gold answer's requirements."}
{"script_processing_qid": "textbook-114-4-3-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the socially optimal level of output in the presence of a negative externality and compare it to the market equilibrium.\n\nGOLD_ANSWER:\n1. **Social Optimum**: Occurs where marginal social cost (MSC) equals marginal social benefit (MSB).\n2. **Market Equilibrium**: Occurs where marginal private cost (MPC) equals marginal private benefit (MPB), leading to overproduction if MSC > MPC.\n\nCANDIDATE_ANSWER:\nStep 1: Define key concepts. A negative externality occurs when a firm's production imposes costs on third parties not reflected in market prices. Market equilibrium occurs where Marginal Private Cost (MPC) equals Marginal Private Benefit (MPB). Socially optimal output requires Marginal Social Cost (MSC = MPC + Marginal External Cost) to equal Marginal Social Benefit (MSB, equivalent to MPB when no positive externality exists).\n\nStep 2: Compare equilibria. In market equilibrium (Q_market), MPC = MPB. With a negative externality, MSC > MPC at all quantities. The socially optimal quantity (Q_optimal) occurs where MSC = MSB, which is smaller than Q_market because firms do not internalize external costs.\n\nStep 3: Graphical interpretation. The market overproduces by (Q_market - Q_optimal), creating a deadweight loss equal to the area between MSC and MPB curves from Q_optimal to Q_market. This shows unaccounted social costs make market equilibrium inefficient.\n\nQID: textbook-114-4-3-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-4-3-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately defines key concepts, correctly compares market equilibrium and socially optimal output in the presence of a negative externality, and provides a clear graphical interpretation. It aligns perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-4-3-0", "category": "CORRECT", "explanation": "The candidate answer accurately defines key concepts, correctly compares market equilibrium and socially optimal output in the presence of a negative externality, and provides a clear graphical interpretation. It aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-114-4-3-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately defines key concepts, correctly compares market equilibrium and socially optimal output in the presence of a negative externality, and provides a clear graphical interpretation. It aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-114-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Why does majority rule voting not necessarily lead to the efficient provision of public goods? How does the median voter theorem explain this?\n\nGOLD_ANSWER:\n1. **Median Voter Theorem**: Under majority rule, the outcome reflects the preferences of the median voter, not the aggregate willingness to pay.\n2. **Efficiency**: Public goods are efficiently provided when the sum of individual marginal benefits equals marginal cost. The median voter's preference may not align with this condition.\n3. **Example**: If the median voter prefers less spending than the efficient level, the outcome will be under-provision of the public good.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain information about majority rule voting, public goods provision, or the median voter theorem. The discussion focuses on externalities, market failures, and corrective mechanisms like government interventions (emissions standards/fees) and private bargaining (Coase Theorem). Therefore, the context is insufficient to address why majority voting may fail to achieve efficient public goods provision or how the median voter theorem relates to this issue.\n\nQID: textbook-114-0-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-0-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-0-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}, "llm_echoed_qid": "textbook-114-0-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}
{"script_processing_qid": "textbook-111-18-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Analyze the limitations of macroeconometric models in forecasting recessions, referencing the 1982 recession prediction failures.\n\nGOLD_ANSWER:\n1. **Model Errors**: Misspecification of equations (e.g., omitted variables like oil shocks).\n2. **Exogenous Assumptions**: Incorrect forecasts of policy/prices (e.g., 1982 Fed actions).\n3. **1982 Case**: Models failed to predict unemployment spikes due to unanticipated shocks and adaptive expectations.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key limitations of macroeconometric models mentioned in the context. The models depend on (1) the validity of their structural equations and (2) accurate assumptions about exogenous variables. Both factors directly impact forecasting reliability.\n\nStep 2: Connect these limitations to the 1982 recession prediction failures. If the model equations failed to capture structural changes in the economy (e.g., shifts in consumer behavior or financial markets), or if exogenous variable assumptions (e.g., policy responses, oil prices) were incorrect, forecasts would be inaccurate. The 1982 recession involved unexpected monetary tightening and debt crises, which models likely failed to anticipate due to these limitations.\n\nStep 3: Highlight broader challenges. Even with leading indicators like stock prices or unemployment claims (part of the index), macroeconometric models struggle to integrate real-time structural breaks or nonlinear crisis dynamics, leading to false signals or delayed recession predictions. This explains why leading indicators alone (e.g., in 1982) may conflict with model outputs, compounding forecasting errors.\n\nQID: textbook-111-18-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-18-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the limitations of macroeconometric models, including structural equation validity and exogenous variable assumptions, and connects these to the 1982 recession prediction failures, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-18-1-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the limitations of macroeconometric models, including structural equation validity and exogenous variable assumptions, and connects these to the 1982 recession prediction failures, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-111-18-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the limitations of macroeconometric models, including structural equation validity and exogenous variable assumptions, and connects these to the 1982 recession prediction failures, aligning well with the gold answer."}
{"script_processing_qid": "textbook-12-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the condition under which Jones will choose artificial production over natural production, given the CRS technologies and the cost structures provided.\n\nGOLD_ANSWER:\n1. **Objective**: Compare costs of artificial vs. natural production.\n2. **Artificial Production Cost**: $\\mathbb{H}(\\mathsf{p}_{\\mathsf{m}} + \\mathrm{w})$.\n3. **Natural Production Cost**: $\\mathbb{H}(\\mathsf{b}\\mathsf{p}_{\\mathsf{b}} + \\mathrm{k}\\mathrm{w})$.\n4. **Decision Rule**: Choose artificial production if $\\mathbb{H}(\\mathsf{p}_{\\mathsf{m}} + \\mathrm{w}) < \\mathbb{H}(\\mathsf{b}\\mathsf{p}_{\\mathsf{b}} + \\mathrm{k}\\mathrm{w})$, simplifying to $\\mathrm{P_{m}} < \\mathrm{b}\\mathrm{p_{b}} + (\\mathrm{k} - 1)\\mathrm{w}$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify cost structures under CRS technologies\n- Artificial production cost per honey unit: (pₘ + w)\n- Natural production cost per honey unit: (b p_b + k w)\n\nStep 2: Compare unit costs for technology choice\nJones will prefer artificial production when:\n(pₘ + w) < (b p_b + k w)\n\nStep 3: Rearrange the inequality to isolate pₘ\nSubtract w from both sides:\npₘ < b p_b + k w - w\n\nStep 4: Factor out w from the last two terms\npₘ < b p_b + (k - 1)w\n\nThis gives the required condition for choosing artificial production: pₘ < b p_b + (k - 1)w\n\nQID: textbook-12-0-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-12-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the cost structures, compares the unit costs, and derives the condition for choosing artificial production, matching the gold answer exactly.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-12-0-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the cost structures, compares the unit costs, and derives the condition for choosing artificial production, matching the gold answer exactly."}, "llm_echoed_qid": "textbook-12-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the cost structures, compares the unit costs, and derives the condition for choosing artificial production, matching the gold answer exactly."}
{"script_processing_qid": "textbook-118-14-3-0", "llm_grader_input_prompt_user": "\nQUESTION:\n7. State the identification condition for NLS and explain its relation to the second-order condition for a minimum.\n\nGOLD_ANSWER:\n1. **Identification Condition**: $\\hat{X}^{\\top}\\hat{X}$ must be nonsingular, ensuring a unique minimum for $\\mathrm{SSR}(\\beta)$.\n2. **Second-Order Condition**: A positive definite $\\hat{X}^{\\top}\\hat{X}$ ensures the Hessian is positive definite, confirming a local minimum.\n\nCANDIDATE_ANSWER:\nStep 1: The identification condition for NLS requires that the matrix $\\hat{X}^{\\top}\\hat{X}$ (the Hessian matrix of the sum-of-squares function) must be nonsingular. This ensures the parameters are uniquely identifiable from the data.\\n\\nStep 2: The second-order condition for a local minimum requires the Hessian matrix (here, $\\hat{X}^{\\top}\\hat{X}$) to be positive definite. A positive definite matrix is necessarily nonsingular.\\n\\nStep 3: Thus, the identification condition (nonsingular $\\hat{X}^{\\top}\\hat{X}$) coincides with the second-order sufficient condition for a minimum (positive definite $\\hat{X}^{\\top}\\hat{X}$). The nonsingularity ensures both parameter identifiability and the existence of a unique local minimum in the NLS estimation.\n\nQID: textbook-118-14-3-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-14-3-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly states the identification condition for NLS (nonsingular $\\hat{X}^{\\top}\\hat{X}$) and its relation to the second-order condition for a minimum (positive definite $\\hat{X}^{\\top}\\hat{X}$). The explanation aligns with the gold answer, demonstrating a clear understanding of the concepts.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-14-3-0", "category": "CORRECT", "explanation": "The candidate answer correctly states the identification condition for NLS (nonsingular $\\hat{X}^{\top}\\hat{X}$) and its relation to the second-order condition for a minimum (positive definite $\\hat{X}^{\top}\\hat{X}$). The explanation aligns with the gold answer, demonstrating a clear understanding of the concepts."}, "llm_echoed_qid": "textbook-118-14-3-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly states the identification condition for NLS (nonsingular $\\hat{X}^{\top}\\hat{X}$) and its relation to the second-order condition for a minimum (positive definite $\\hat{X}^{\top}\\hat{X}$). The explanation aligns with the gold answer, demonstrating a clear understanding of the concepts."}
{"script_processing_qid": "textbook-106-20-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain how cross-price elasticity can be used to identify substitutes and complements.\n\nGOLD_ANSWER:\n1. **Substitutes**: $E_{XY} > 0$ (price increase of Y increases demand for X).\n2. **Complements**: $E_{XY} < 0$ (price increase of Y decreases demand for X).\n\nCANDIDATE_ANSWER:\nStep 1: Define cross-price elasticity - Cross-price elasticity measures the percentage change in quantity demanded of one good (Good X) relative to a percentage change in price of another good (Good Y).\nStep 2: Substitute identification - If the cross-price elasticity is positive, it means an increase in Good Y's price leads to an increase in Good X's demand (consumers switch to Good X when Good Y becomes more expensive). This indicates substitute goods (e.g., tea and coffee).\nStep 3: Complement identification - If the cross-price elasticity is negative, it means an increase in Good Y's price leads to a decrease in Good X's demand (both goods are consumed together). This indicates complementary goods (e.g., printers and ink).\nStep 4: Interpretation - The sign (+/-) of the elasticity directly reveals the relationship: substitutes have EPXY > 0, complements have EPXY < 0.\n\nQID: textbook-106-20-2-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-106-20-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly defines cross-price elasticity and accurately explains how positive and negative values indicate substitutes and complements, respectively, aligning perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-20-2-1", "category": "CORRECT", "explanation": "The candidate answer correctly defines cross-price elasticity and accurately explains how positive and negative values indicate substitutes and complements, respectively, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-106-20-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly defines cross-price elasticity and accurately explains how positive and negative values indicate substitutes and complements, respectively, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-112-3-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze Smith's policy conclusion that nations should not artificially prioritize carrying trade over home/foreign consumption trade. What modern economic concepts challenge this view?\n\nGOLD_ANSWER:\n1. **Smith's View**:\n   - Natural capital flows optimize labor support and annual produce.\n   - Artificial incentives distort comparative advantage (e.g., British shipping subsidies).\n\n2. **Modern Challenges**:\n   - **Externalities**: Carrying trade may generate positive externalities (e.g., technology spillovers, geopolitical influence).\n   - **Dynamic Comparative Advantage**: State intervention (e.g., South Korea’s shipping industry) can create long-term competitive edges.\n   - **Global Value Chains**: Modern carrying trade (e.g., container shipping) is integral to GVCs, enhancing overall productivity beyond Smith’s static model.\n\n3. **Synthesis**:\n   - Smith’s warning against distortion holds, but contemporary trade complexity necessitates nuanced policy frameworks.\n\nCANDIDATE_ANSWER:\nStep 1: Understand Smith's Core Argument\nSmith argues that capital allocation impacts domestic productivity:\n- Home trade replaces two domestic capitals (supports domestic labor twice).\n- Foreign consumption trade replaces one domestic and one foreign capital (supports domestic labor once).\n- Carrying trade (transporting goods between foreign nations) supports only foreign labor, withdrawing capital entirely from domestic production.\nHe concludes nations should not artificially prioritize carrying trade, as it offers the least domestic economic benefit.\n\nStep 2: Identify Modern Economic Challenges\nModern concepts challenging this view include:\n1. **Comparative Advantage** (Ricardo/Heckscher-Ohlin): Specializing in carrying trade (e.g., shipping/logistics) may align with a nation’s comparative advantage, increasing global efficiency and long-term gains despite reduced direct domestic labor support.\n2. **Economies of Scale** (New Trade Theory): Carrying trade participation might enable scale efficiencies in transportation/logistics sectors, indirectly boosting domestic productivity through infrastructure development.\n3. **Global Value Chains**: Carrying trade integrates nations into networks that facilitate technology/knowledge spillovers and diversification benefits, which Smith’s static capital-replacement model ignores.\n4. **Service Sector Dynamics**: Modern economies value service sectors (e.g., shipping, finance). Carrying trade may enhance high-value service exports, contributing to GDP growth beyond Smith’s focus on agriculture/manufacturing.\n\nStep 3: Synthesis\nWhile Smith emphasizes immediate domestic capital recycling, modern economics acknowledges dynamic gains from specialization, global integration, and service-sector growth—all of which justify strategic participation in carrying trade under certain conditions.\n\nQID: textbook-112-3-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-3-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses Smith's argument and effectively identifies modern economic concepts that challenge his view, including comparative advantage, economies of scale, global value chains, and service sector dynamics. The synthesis aligns well with the gold answer, acknowledging the nuanced policy frameworks required in contemporary trade.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-3-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses Smith's argument and effectively identifies modern economic concepts that challenge his view, including comparative advantage, economies of scale, global value chains, and service sector dynamics. The synthesis aligns well with the gold answer, acknowledging the nuanced policy frameworks required in contemporary trade."}, "llm_echoed_qid": "textbook-112-3-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses Smith's argument and effectively identifies modern economic concepts that challenge his view, including comparative advantage, economies of scale, global value chains, and service sector dynamics. The synthesis aligns well with the gold answer, acknowledging the nuanced policy frameworks required in contemporary trade."}
{"script_processing_qid": "textbook-84-1-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of the statement: 'In large samples we do not have to worry about omitted variable bias.' Is it true or false? Justify.\n\nGOLD_ANSWER:\n1. **False**: Omitted variable bias persists asymptotically if the omitted variable is correlated with included regressors.\n2. **Reason**: $\\mathrm{plim}(\\tilde{\\beta}_{1}) = \\beta_{1} + \\beta_{2}\\delta_{1} \\neq \\beta_{1}$ unless $\\delta_{1} = 0$ or $\\beta_{2} = 0$.\n3. **Conclusion**: Large samples do not eliminate bias from omitted variables; they only reduce sampling variability.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the statement's claim. The assertion implies that omitted variable bias (OVB) disappears asymptotically (in large samples).\n\nStep 2: Analyze the provided context. The derivation shows that under the given assumptions (including E(u) = 0 and no correlation between x₁ and u), plim(β₁_hat) = β₁. This ensures consistency of OLS estimators asymptotically.\n\nStep 3: Identify the flaw in the statement. Omitted variable bias violates the assumption that E(u|x₁) = 0 (exogeneity). If u contains omitted variables correlated with x₁, then plim(β₁_hat) ≠ β₁ even in large samples. The law of large numbers and asymptotic normality do not resolve this issue unless OVB is explicitly addressed.\n\nStep 4: Conclusion. The statement is false. OVB persists in large samples because it violates the exogeneity condition required for consistency. Larger samples reduce sampling variability but do not eliminate systematic bias caused by omitted variables correlated with regressors.\n\nQID: textbook-84-1-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-84-1-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the statement as false, explains the persistence of omitted variable bias in large samples due to violated exogeneity, and aligns with the gold answer's reasoning and conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-84-1-1-3", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the statement as false, explains the persistence of omitted variable bias in large samples due to violated exogeneity, and aligns with the gold answer's reasoning and conclusion."}, "llm_echoed_qid": "textbook-84-1-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the statement as false, explains the persistence of omitted variable bias in large samples due to violated exogeneity, and aligns with the gold answer's reasoning and conclusion."}
{"script_processing_qid": "textbook-118-3-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the advantages and limitations of using the joint test statistic $\\tau\\_{3,4}$ for skewness and excess kurtosis compared to separate tests. How does the asymptotic independence of $\\tau\\_{3}$ and $\\tau\\_{4}$ under the null facilitate this test?\n\nGOLD_ANSWER:\n1.  **Advantages**: \n    - **Joint Power**: $\\tau\\_{3,4}$ can detect deviations from normality due to either skewness or kurtosis, increasing test power.\n    - **Simplified Interpretation**: The $\\chi^{2}(2)$ distribution provides a single critical value for the joint test.\n2.  **Limitations**: \n    - **Directionality Loss**: The squared form obscures the direction of skewness/kurtosis (e.g., positive vs. negative).\n    - **Specificity**: Rejection does not indicate which feature (skewness or kurtosis) caused the deviation.\n3.  **Asymptotic Independence**: Under the null, $\\tau\\_{3}$ and $\\tau\\_{4}$ are uncorrelated, ensuring that $\\tau\\_{3,4}=\\tau\\_{3}^{2}+\\tau\\_{4}^{2}$ has a clean $\\chi^{2}(2)$ distribution without cross-term complications.\n\nCANDIDATE_ANSWER:\nStep 1: Advantages of τ₃,₄\n- Jointly controls Type I error: Combines skewness (τ₃) and kurtosis (τ₄) tests into a single χ²(2) framework, avoiding multiple testing issues.\n- Comprehensive normality assessment: Simultaneously tests both third and fourth moment deviations, critical for detecting general non-normality.\n\nStep 2: Limitations of τ₃,₄\n- Reduced directional specificity: A significant result does not indicate whether skewness, kurtosis, or both caused rejection.\n- Potential power trade-off: May underperform against alternatives where only one moment deviates (e.g., pure skewness) compared to focused single tests.\n\nStep 3: Role of asymptotic independence\n1. Under H₀ (normality), τ₃ and τ₄ are asymptotically independent N(0,1) variables.\n2. Independence implies τ₃² + τ₄² ~ χ²(2) exactly, without covariance adjustments.\n3. This simplifies critical value calculation compared to correlated test statistics requiring variance-covariance estimation.\n\nQID: textbook-118-3-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-3-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately captures the advantages (joint Type I error control, comprehensive assessment) and limitations (directional specificity loss, power trade-offs) of the joint test statistic τ₃,₄. It also correctly explains the role of asymptotic independence in simplifying the test's distribution under the null, matching the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-3-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately captures the advantages (joint Type I error control, comprehensive assessment) and limitations (directional specificity loss, power trade-offs) of the joint test statistic τ₃,₄. It also correctly explains the role of asymptotic independence in simplifying the test's distribution under the null, matching the gold answer's key points."}, "llm_echoed_qid": "textbook-118-3-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately captures the advantages (joint Type I error control, comprehensive assessment) and limitations (directional specificity loss, power trade-offs) of the joint test statistic τ₃,₄. It also correctly explains the role of asymptotic independence in simplifying the test's distribution under the null, matching the gold answer's key points."}
{"script_processing_qid": "textbook-55-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Show that strong and weak Pareto efficiency are equivalent under strongly monotone preferences.\n\nGOLD_ANSWER:\n1.  **Strongly Monotone Preferences**: Imply $u_{i}(x_{i}^{\\prime}) > u_{i}(x_{i})$ if $x_{i}^{\\prime} \\gg x_{i}$.\n2.  **Redistribution**: For any feasible allocation $(x^{\\prime},y^{\\prime})$ that is a weak Pareto improvement, redistribute a small amount $\\epsilon$ of commodity $s$ from consumer $k$ to others.\n3.  **Continuity**: By continuity of utility functions, $u_{i}(x_{i}^{\\prime\\prime}) > u_{i}(x_{i})$ for all $i$ for small enough $\\epsilon$.\n4.  **Conclusion**: Thus, any weak Pareto improvement can be made a strong Pareto improvement, proving equivalence.\n\nCANDIDATE_ANSWER:\nStep 1: Show Strong Pareto Efficiency implies Weak Pareto Efficiency.\n- By definition, a strongly Pareto efficient allocation prevents any feasible improvement where at least one agent is better off without harming others. This automatically prohibits any scenario where all agents are simultaneously better off (the requirement for weak Pareto efficiency).\n\nStep 2: Demonstrate Weak Pareto Efficiency implies Strong Pareto Efficiency under strong monotonicity.\n- Assume an allocation is weakly Pareto efficient but not strongly Pareto efficient. Then, there exists a feasible allocation (x', y') where at least one agent (k) is strictly better off, and others are no worse off.\n- By strong monotonicity, add a small positive amount of all goods to every other agent’s bundle in (x', y'). This creates a new feasible allocation (x'', y'') where all agents are strictly better off (due to monotonicity), contradicting weak Pareto efficiency.\n- Thus, weak Pareto efficiency necessitates strong Pareto efficiency under strongly monotone preferences.\n\nConclusion: Equivalence is established bidirectionally under the given assumptions.\n\nQID: textbook-55-0-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-55-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly demonstrates the equivalence between strong and weak Pareto efficiency under strongly monotone preferences, covering both directions of the implication with valid reasoning and steps that align with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-55-0-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly demonstrates the equivalence between strong and weak Pareto efficiency under strongly monotone preferences, covering both directions of the implication with valid reasoning and steps that align with the gold answer."}, "llm_echoed_qid": "textbook-55-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly demonstrates the equivalence between strong and weak Pareto efficiency under strongly monotone preferences, covering both directions of the implication with valid reasoning and steps that align with the gold answer."}
{"script_processing_qid": "textbook-119-11-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the expected value and variance of a Bernoulli random variable with success probability $p$.\n\nGOLD_ANSWER:\n1.  **Expected Value**: $E(X) = 1 \\cdot p + 0 \\cdot (1-p) = p$.\n2.  **Variance**: $Var(X) = E(X^2) - [E(X)]^2 = p - p^2 = p(1-p) = pq$.\n\nCANDIDATE_ANSWER:\nStep 1: Expected Value Derivation\nFor a Bernoulli random variable:\n- Outcome 1 occurs with probability P(X=1) = p\n- Outcome 0 occurs with probability P(X=0) = 1-p\n\nUsing the definition of expected value:\nE(X) = Σ [x_i · P(X = x_i)]\n= (1 · p) + (0 · (1-p))\n= p + 0\n= p\n\nStep 2: Variance Derivation\nFirst compute E(X²):\nE(X²) = (1² · p) + (0² · (1-p)) = p\n\nThen use the variance formula:\nvar(X) = E(X²) - [E(X)]²\n= p - p²\n= p(1-p) = pq\n(where q ≡ 1-p by definition)\n\nQID: textbook-119-11-2-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-11-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives both the expected value and variance of a Bernoulli random variable, matching the gold answer in both steps and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-11-2-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives both the expected value and variance of a Bernoulli random variable, matching the gold answer in both steps and reasoning."}, "llm_echoed_qid": "textbook-119-11-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives both the expected value and variance of a Bernoulli random variable, matching the gold answer in both steps and reasoning."}
{"script_processing_qid": "textbook-106-4-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Model the Tragedy of the Commons using a mathematical framework. Derive the socially optimal and privately optimal levels of resource use.\n\nGOLD_ANSWER:\n1. **Private Optimum**: Each shepherd maximizes private profit $\\pi_i = p \\cdot s_i - c(s_i)$, where $s_i$ is sheep, $p$ is price, and $c(s_i)$ is cost. The solution is $p = c'(s_i)$.\n2. **Social Optimum**: Social planner maximizes total welfare $\\sum \\pi_i - D(S)$, where $D(S)$ is damage from overgrazing. The solution is $p = c'(s_i) + D'(S)$.\n3. **Inefficiency**: Privately, shepherds ignore $D'(S)$, leading to overgrazing ($S_{private} > S_{social}$).\n\nCANDIDATE_ANSWER:\nStep 1: Define Variables\nLet:\n- N = Number of identical users (herders)\n- q_i = Resource use by individual i\n- Q = ∑q_i = Total resource use (total sheep)\n- B(Q) = Marginal private benefit per unit of resource use\n- C(Q) = Marginal private cost per unit of resource use (increasing in Q)\n\nStep 2: Private Optimization\nEach user maximizes private profit π_i = B*q_i - C(Q)*q_i.\nFirst-order condition (FOC) for individual i:\n∂π_i/∂q_i = B - C(Q) - q_i*C'(Q) = 0\n\nIn symmetric equilibrium (q_i = Q/N):\nB = C(Q) + (Q/N)*C'(Q)\n\nFor large N → Private equilibrium simplifies to:\nB = C(Q_p)\n(Q_p = Privately optimal quantity)\n\nStep 3: Social Optimization\nSocial planner maximizes total welfare W = B*Q - Q*C(Q)\nFOC:\ndW/dQ = B - C(Q) - Q*C'(Q) = 0\n\nSocially optimal quantity Q_s satisfies:\nB = C(Q_s) + Q_s*C'(Q_s)\n\nStep 4: Compare Outcomes\nSince C'(Q) > 0 (increasing marginal cost):\nQ_s < Q_p\n\nThis shows private actors overuse the resource because they ignore the negative externality term Q*C'(Q) in their decision-making.\n\nQID: textbook-106-4-2-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-4-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly models the Tragedy of the Commons, deriving both private and social optima with appropriate mathematical frameworks (FOCs) and clearly showing the inefficiency (Q_s < Q_p) due to ignored externalities. The reasoning aligns perfectly with the gold answer's core concepts.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-4-2-0", "category": "CORRECT", "explanation": "The candidate's answer correctly models the Tragedy of the Commons, deriving both private and social optima with appropriate mathematical frameworks (FOCs) and clearly showing the inefficiency (Q_s < Q_p) due to ignored externalities. The reasoning aligns perfectly with the gold answer's core concepts."}, "llm_echoed_qid": "textbook-106-4-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly models the Tragedy of the Commons, deriving both private and social optima with appropriate mathematical frameworks (FOCs) and clearly showing the inefficiency (Q_s < Q_p) due to ignored externalities. The reasoning aligns perfectly with the gold answer's core concepts."}
{"script_processing_qid": "textbook-63-2-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. How does the choice of error distribution (e.g., normal vs. uniform) impact the performance of the Box-Pierce and Ljung-Box tests in Monte Carlo simulations?\n\nGOLD_ANSWER:\n4. **Error Distribution Impact**:\n   - **Normal Errors**: Both statistics should perform well if the sample size is large enough.\n   - **Uniform Errors**: The finite-sample properties may differ, with the Ljung-Box statistic remaining more robust due to its correction factor, while the Box-Pierce statistic may show greater deviation from the nominal size.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the test statistics' asymptotics. Both Box-Pierce (BP) and Ljung-Box (LB) Q statistics are asymptotically χ²(4) under the null, but their finite-sample performance depends on how well sample autocorrelations approximate normality.\n\nStep 2: Consider distributional impacts. Normal errors produce sample autocorrelations with exact normality in finite samples (given i.i.d. assumptions), leading to accurate χ² approximations. Uniform errors (with bounded support) create sample autocorrelations with:\n- Less pronounced tails\n- Different higher-moment properties\n\nStep 3: Analyze simulation results. In Monte Carlo experiments:\na) Under normal errors: Both tests should show rejection frequencies close to 5% (nominal size)\nb) Under uniform errors: \n- BP tends to under-reject (lower empirical size) due to slower convergence to χ²\n- LB's sample-size adjusted weights (sum [r_j²/(n-j)] vs BP's sum r_j²) better compensate for non-normality\n\nStep 4: Key mechanism. Uniform distribution's excess kurtosis (-1.2 vs normal's 0) reduces variance of sample autocorrelations, making Q statistics smaller on average. LB's weighting partially mitigates this effect compared to BP.\n\nQID: textbook-63-2-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-63-2-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the impact of error distribution on the Box-Pierce and Ljung-Box tests, aligning with the gold answer. It correctly identifies the differences in performance under normal and uniform errors, and provides detailed reasoning about the mechanisms behind these differences.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-63-2-1-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the impact of error distribution on the Box-Pierce and Ljung-Box tests, aligning with the gold answer. It correctly identifies the differences in performance under normal and uniform errors, and provides detailed reasoning about the mechanisms behind these differences."}, "llm_echoed_qid": "textbook-63-2-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the impact of error distribution on the Box-Pierce and Ljung-Box tests, aligning with the gold answer. It correctly identifies the differences in performance under normal and uniform errors, and provides detailed reasoning about the mechanisms behind these differences."}
{"script_processing_qid": "textbook-123-15-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the implications of the first-order conditions for the Ramsey allocation when the stochastic process $s$ is Markov.\n\nGOLD_ANSWER:\nWhen the stochastic process $s$ is Markov, the first-order conditions suggest that the allocations from period 1 onward can be described by time-invariant allocation rules $c(s,k)$, $n(s,k)$, and $k^{\\prime}(s,k)$. This implies that the optimal allocation depends only on the current state $(s,k)$ and not on the entire history leading up to that state. This property simplifies the analysis and allows for the use of dynamic programming techniques to solve for the optimal allocation.\n\nCANDIDATE_ANSWER:\nStep 1: The Markov property implies the stochastic process depends only on the current state. This simplifies expectations in the FOCs to depend only on the current state $s_t$, not the entire history $s^t$.\n\nStep 2: The Euler equation for capital (third FOC) becomes a recursive relationship where $\\theta_t(s_t)$ equals the expected discounted value of $\\theta_{t+1}(s_{t+1})$ multiplied by the marginal product of capital. The Markov structure allows this expectation to be conditioned solely on $s_t$, collapsing dependence on histories.\n\nStep 3: Consumption and labor decisions (first two FOCs) also become functions of $s_t$ rather than $s^t$, as $\\theta_t(s_t)$ inherits the Markov property. This eliminates history dependence in allocations.\n\nStep 4: The FOCs now describe a time-invariant policy function where allocations depend only on $s_t$, simplifying computation. The government’s optimal taxes and policies inherit this Markov structure, becoming functions of the current state rather than past shocks.\n\nStep 5: The Euler equation for consumption (second block) reduces to a forward-looking condition where $\\mathcal{V}_c(s_t)$ equals the expected future marginal utility adjusted by productivity. The Markov property ensures expectations are computable with transition probabilities between consecutive states.\n\nQID: textbook-123-15-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-15-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the implications of the first-order conditions for the Ramsey allocation when the stochastic process is Markov. It accurately describes how the Markov property simplifies the analysis by making allocations depend only on the current state, not the entire history, and how this leads to time-invariant policy functions. The explanation aligns well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-15-0-1", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the implications of the first-order conditions for the Ramsey allocation when the stochastic process is Markov. It accurately describes how the Markov property simplifies the analysis by making allocations depend only on the current state, not the entire history, and how this leads to time-invariant policy functions. The explanation aligns well with the gold answer."}, "llm_echoed_qid": "textbook-123-15-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the implications of the first-order conditions for the Ramsey allocation when the stochastic process is Markov. It accurately describes how the Markov property simplifies the analysis by making allocations depend only on the current state, not the entire history, and how this leads to time-invariant policy functions. The explanation aligns well with the gold answer."}
{"script_processing_qid": "textbook-103-7-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the savings function $s_{t}^{h}(t)=f^{h}[1+r(t)]$ for the logarithmic utility function, starting from the first-order conditions of the young generation's optimization problem.\n\nGOLD_ANSWER:\n1. **Objective**: Maximize $u^{h}[c_{t}^{h}(t), c_{t}^{h}(t+1)] = \\ln c_{t}^{h}(t) + \\ln c_{t}^{h}(t+1)$ subject to the constraints.\n2. **Lagrangian**: $\\mathcal{L} = \\ln c_{t}^{h}(t) + \\ln c_{t}^{h}(t+1) + \\lambda_1[w_{t}^{h}(t) - c_{t}^{h}(t) - s_{t}^{h}(t)] + \\lambda_2[w_{t}^{h}(t+1) + [1+r(t)]s_{t}^{h}(t) - c_{t}^{h}(t+1)]$.\n3. **FOCs**:\n   - $\\frac{\\partial \\mathcal{L}}{\\partial c_{t}^{h}(t)} = \\frac{1}{c_{t}^{h}(t)} - \\lambda_1 = 0$.\n   - $\\frac{\\partial \\mathcal{L}}{\\partial c_{t}^{h}(t+1)} = \\frac{1}{c_{t}^{h}(t+1)} - \\lambda_2 = 0$.\n   - $\\frac{\\partial \\mathcal{L}}{\\partial s_{t}^{h}(t)} = -\\lambda_1 + \\lambda_2[1+r(t)] = 0$.\n4. **Solve**: From FOCs, $\\lambda_1 = \\frac{1}{c_{t}^{h}(t)}$ and $\\lambda_2 = \\frac{1}{c_{t}^{h}(t+1)}$. Substituting into the third FOC: $\\frac{1}{c_{t}^{h}(t)} = \\frac{1+r(t)}{c_{t}^{h}(t+1)}$.\n5. **Budget Constraints**: $c_{t}^{h}(t) = w_{t}^{h}(t) - s_{t}^{h}(t)$ and $c_{t}^{h}(t+1) = w_{t}^{h}(t+1) + [1+r(t)]s_{t}^{h}(t)$.\n6. **Substitute**: $\\frac{1}{w_{t}^{h}(t) - s_{t}^{h}(t)} = \\frac{1+r(t)}{w_{t}^{h}(t+1) + [1+r(t)]s_{t}^{h}(t)}$.\n7. **Solve for $s_{t}^{h}(t)$**: Rearranging yields $s_{t}^{h}(t) = \\frac{1}{2}\\left[w_{t}^{h}(t) - \\frac{w_{t}^{h}(t+1)}{1+r(t)}\\right]$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with logarithmic utility function $u^h = \\ln c_t^h(t) + \\ln c_t^h(t+1)$\nStep 2: Combine period constraints into lifetime budget constraint:\n$c_t^h(t) + \\frac{c_t^h(t+1)}{1+r(t)} = w_t^h(t) + \\frac{w_t^h(t+1)}{1+r(t)}$\nStep 3: Derive Euler equation from FOC:\n$\\frac{1}{c_t^h(t)} = \\frac{1+r(t)}{c_t^h(t+1)} \\Rightarrow c_t^h(t+1) = [1+r(t)]c_t^h(t)$\nStep 4: Substitute Euler equation into lifetime budget constraint:\n$c_t^h(t) + \\frac{[1+r(t)]c_t^h(t)}{1+r(t)} = 2c_t^h(t) = w_t^h(t) + \\frac{w_t^h(t+1)}{1+r(t)}$\nStep 5: Solve for first-period consumption:\n$c_t^h(t) = \\frac{1}{2}\\left[w_t^h(t) + \\frac{w_t^h(t+1)}{1+r(t)}\\right]$\nStep 6: Derive savings as residual from first-period budget:\n$s_t^h(t) = w_t^h(t) - c_t^h(t) = \\frac{1}{2}\\left[w_t^h(t) - \\frac{w_t^h(t+1)}{1+r(t)}\\right]$\n\nQID: textbook-103-7-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-103-7-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the savings function $s_{t}^{h}(t)$ for the logarithmic utility function, following the same logical steps as the gold answer, including setting up the utility function, deriving the Euler equation, and solving for savings as a residual from the first-period budget constraint.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-7-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the savings function $s_{t}^{h}(t)$ for the logarithmic utility function, following the same logical steps as the gold answer, including setting up the utility function, deriving the Euler equation, and solving for savings as a residual from the first-period budget constraint."}, "llm_echoed_qid": "textbook-103-7-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the savings function $s_{t}^{h}(t)$ for the logarithmic utility function, following the same logical steps as the gold answer, including setting up the utility function, deriving the Euler equation, and solving for savings as a residual from the first-period budget constraint."}
{"script_processing_qid": "textbook-112-39-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. (Qualitative) Analyze Smith's claim that Presbyterian equality of benefices 'has some very agreeable effects' through the lens of principal-agent theory. How does the mediocrity of benefices align with the interests of (a) parishioners, (b) clergy, and (c) civil authorities?\n\nGOLD_ANSWER:\na) **Parishioners**: Reduced moral hazard as clergy depend on esteem (not patronage), aligning incentives with congregational needs.\nb) **Clergy**: Eliminates high-powered incentives for rent-seeking, fostering investment in human capital (learning, moral conduct).\nc) **Civil Authorities**: Lowers enforcement costs by internalizing discipline mechanisms through peer equality and congregational oversight.\n\nCANDIDATE_ANSWER:\nStep 1: **Principal-Agent Framework**\nUnder principal-agent theory, parishioners (principals) delegate spiritual authority to clergy (agents), while civil authorities (third-party principals) seek institutional stability. Incentive alignment depends on how benefice structures mitigate moral hazard.\n\nStep 2: **Alignment with Parishioners (a)**\nModerate benefices reduce clergy's financial independence, forcing them to compete for parishioners' approval through 'learning, regularity of life, and faithful discharge of duty' (extract). This aligns clergy efforts with parishioners' spiritual needs, as agents must prioritize service quality over rent-seeking.\n\nStep 3: **Alignment with Clergy (b)**\nWhile reduced income might seem misaligned, mediocrity of benefices creates merit-based competition. Clergy build influence via reputation (extract: 'more influence over common people'), substituting financial rewards with social capital. This aligns long-term career incentives with performance.\n\nStep 4: **Alignment with Civil Authorities (c)**\nModerate benefices (i) prevent clergy from accumulating independent wealth/power (reducing rivalry with state), and (ii) sustain university talent pools (extract: 'churches drain universities...' under high benefices). Civil authorities benefit from educated elites in secular roles and a clergy dependent on public esteem rather than institutional hierarchy.\n\nStep 5: **Conflict Mitigation**\nSmith implies Presbyterian equality reduces agency costs: parishioners monitor clergy via elections, clergy self-regulate to maintain influence, and civil authorities avoid centralized religious power structures that could challenge secular order (contrasted with Lutheran episcopal systems).\n\nQID: textbook-112-39-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-39-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly analyzes Smith's claim through the lens of principal-agent theory, addressing the alignment of mediocrity of benefices with the interests of parishioners, clergy, and civil authorities. The reasoning is consistent with the gold answer, providing detailed and accurate explanations for each stakeholder group.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-39-0-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly analyzes Smith's claim through the lens of principal-agent theory, addressing the alignment of mediocrity of benefices with the interests of parishioners, clergy, and civil authorities. The reasoning is consistent with the gold answer, providing detailed and accurate explanations for each stakeholder group."}, "llm_echoed_qid": "textbook-112-39-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly analyzes Smith's claim through the lens of principal-agent theory, addressing the alignment of mediocrity of benefices with the interests of parishioners, clergy, and civil authorities. The reasoning is consistent with the gold answer, providing detailed and accurate explanations for each stakeholder group."}
{"script_processing_qid": "textbook-112-8-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Critically evaluate Smith's assertion that 'the rent and profit of [common crops] regulate the rent and profit of all other cultivated land.' How does this hold under monopolistic conditions for unique crops like premium wines?\n\nGOLD_ANSWER:\n1.  **Regulation Mechanism**: For common crops, competition ensures rents/profits converge to a 'natural rate' determined by substitutability and mobility of land use.\n2.  **Monopoly Exception**: For unique crops (e.g., Burgundy wine), limited supply and inelastic demand allow rents to exceed the natural rate indefinitely. Smith acknowledges this: 'The whole quantity... falls short of the effectual demand.'\n3.  **Empirical Limitation**: Modern agroeconomics shows geographical indications (e.g., Champagne) create artificial scarcity, violating Smith's competitive equilibrium assumption.\n\nCANDIDATE_ANSWER:\nStep 1: Understand Smith’s Core Argument\nSmith’s assertion assumes competitive equilibrium where land allocation adjusts based on relative profitability. Common crops (e.g., corn/pasture) set baseline rent/profit levels because land can be reallocated to these crops if other uses underperform or to alternative crops if they outperform. This mechanism relies on substitutable land use and market competition.\n\nStep 2: Examine Smith’s Qualification for Specialty Crops\nSmith acknowledges that crops requiring higher upfront or recurring costs (e.g., vineyards) may command higher rent/profit to offset these expenses. However, he argues this premium rarely exceeds a ‘reasonable interest’ compensating for the added costs, maintaining alignment with common-crop benchmarks.\n\nStep 3: Apply Monopolistic Conditions to Unique Crops\nUnder monopolistic conditions—such as geographically restricted production (e.g., premium wines from specific terroirs)—supply is artificially constrained. Here:\n- Limited substitutability prevents land reallocation, breaking Smith’s adjustment mechanism.\n- Unique branding/scarcity allows producers to charge prices exceeding natural rates (covering costs + competitive profit), creating economic rents unregulated by common crops.\n\nStep 4: Critical Evaluation\nSmith’s theory holds weakly in monopolistic scenarios:\n- Rent/profit for unique crops becomes decoupled from common crops due to inelastic demand and supply constraints.\n- The ‘reasonable interest’ limit is superseded by monopolistic rents, as prices reflect willingness-to-pay rather than cost-compensation alone.\nConclusion: Smith’s assertion fails under monopolistic conditions where unique crops achieve rents/profits unmoored from common-crop benchmarks due to market power and supply inflexibility.\n\nQID: textbook-112-8-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-8-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly evaluates Smith's assertion, correctly identifying the regulation mechanism for common crops and the exceptions under monopolistic conditions for unique crops. The explanation aligns with the gold answer, detailing how monopolistic conditions disrupt Smith's competitive equilibrium and allow for higher rents and profits.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-8-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly evaluates Smith's assertion, correctly identifying the regulation mechanism for common crops and the exceptions under monopolistic conditions for unique crops. The explanation aligns with the gold answer, detailing how monopolistic conditions disrupt Smith's competitive equilibrium and allow for higher rents and profits."}, "llm_echoed_qid": "textbook-112-8-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly evaluates Smith's assertion, correctly identifying the regulation mechanism for common crops and the exceptions under monopolistic conditions for unique crops. The explanation aligns with the gold answer, detailing how monopolistic conditions disrupt Smith's competitive equilibrium and allow for higher rents and profits."}
{"script_processing_qid": "textbook-122-12-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the limitations of assuming $\\{e_{t}\\}$ is i.i.d. in trending time series models. How might serial correlation affect OLS inference?\n\nGOLD_ANSWER:\n1. **i.i.d. Assumption**: Often violated in time series (e.g., shocks persist).\n2. **Consequences**: \n   - Biased standard errors, invalid t-tests/F-tests.\n   - Inefficient estimators if autocorrelation is ignored.\n3. **Solutions**: Use HAC standard errors or model serial correlation (e.g., ARIMA).\n\nCANDIDATE_ANSWER:\nStep 1: The i.i.d. assumption implies errors are independent and identically distributed. In trending time series (linear/exponential), this is often violated because economic time series frequently exhibit serial correlation (e.g., due to persistent shocks or omitted variables).\n\nStep 2: Limitations of assuming i.i.d. errors:\n- Biased standard errors: Serial correlation causes OLS to underestimate true standard errors when residuals are positively correlated, leading to inflated t-statistics.\n- Invalid hypothesis tests: Over-rejection of null hypotheses (Type I errors) for trend coefficients (α₁/β₁) due to incorrect standard error calculations.\n- Inconsistent variance estimation: The formula Var(α̂₁) = σ²/Σ(t - t̄)² assumes no serial correlation, which fails when errors are dependent.\n\nStep 3: Effect of serial correlation on OLS inference:\n- Trend coefficients (α₁/β₁) remain consistent under exogeneity, but their estimated precision becomes unreliable.\n- Confidence intervals and t-tests for trend significance lose validity.\n- Spurious evidence of a significant trend may arise if serial correlation is ignored.\n\nQID: textbook-122-12-2-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-12-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses the limitations of the i.i.d. assumption in trending time series models and the effects of serial correlation on OLS inference, aligning well with the gold answer. The explanation includes biased standard errors, invalid hypothesis tests, and inconsistent variance estimation, which are all key points mentioned in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-12-2-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses the limitations of the i.i.d. assumption in trending time series models and the effects of serial correlation on OLS inference, aligning well with the gold answer. The explanation includes biased standard errors, invalid hypothesis tests, and inconsistent variance estimation, which are all key points mentioned in the gold answer."}, "llm_echoed_qid": "textbook-122-12-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses the limitations of the i.i.d. assumption in trending time series models and the effects of serial correlation on OLS inference, aligning well with the gold answer. The explanation includes biased standard errors, invalid hypothesis tests, and inconsistent variance estimation, which are all key points mentioned in the gold answer."}
{"script_processing_qid": "textbook-118-9-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What is a random walk with drift? Derive the expression for $y_{t}$ in terms of $y_{0}$, $\\gamma_{1}$, $t$, and $w_{t}$.\n\nGOLD_ANSWER:\n1. **Definition**: $y_{t} = \\gamma_{1} + y_{t-1} + e_{t}$.\\n2. **Solution**: Subtract $y_{0} + \\gamma_{1}t$ from both sides to get $y_{t} - y_{0} - \\gamma_{1}t = y_{t-1} - y_{0} - \\gamma_{1}(t-1) + e_{t}$.\\n3. **Final Form**: $y_{t} = y_{0} + \\gamma_{1}t + \\sigma w_{t}$, where $w_{t}$ is the standardized random walk.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the random walk with drift equation:\n\nyₜ = γ₁ + yₜ₋₁ + eₜ\n\nStep 2: Expand recursively by substituting previous periods:\n\nFor t=1: y₁ = γ₁ + y₀ + e₁\nFor t=2: y₂ = γ₁ + y₁ + e₂ = γ₁ + (γ₁ + y₀ + e₁) + e₂ = 2γ₁ + y₀ + e₁ + e₂\n...\nFor general t: yₜ = y₀ + tγ₁ + Σₖ₌₁ᵗ eₖ\n\nStep 3: Recognize that Σₖ₌₁ᵗ eₖ corresponds to the standardized random walk process wₜ:\nFrom the definition wₜ = Σₖ₌₁ᵗ εₖ (with w₀=0 and εₖ ~ IID(0,1)). Assuming eₖ ≡ εₖ,\nwₜ = Σₖ₌₁ᵗ eₖ\n\nStep 4: Substitute Σₖ₌₁ᵗ eₖ with wₜ:\nyₜ = y₀ + γ₁t + wₜ\n\nFinal expression: yₜ = y₀ + γ₁t + wₜ\n\nQID: textbook-118-9-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-9-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derives the expression for a random walk with drift, matching the gold answer's final form. The steps logically build from the initial equation to the solution, including the proper substitution of the standardized random walk process.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-9-0-2", "category": "CORRECT", "explanation": "The candidate correctly derives the expression for a random walk with drift, matching the gold answer's final form. The steps logically build from the initial equation to the solution, including the proper substitution of the standardized random walk process."}, "llm_echoed_qid": "textbook-118-9-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derives the expression for a random walk with drift, matching the gold answer's final form. The steps logically build from the initial equation to the solution, including the proper substitution of the standardized random walk process."}
{"script_processing_qid": "textbook-64-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the rank condition for identification and explain its significance in the context of the given model.\n\nGOLD_ANSWER:\n1.  **Rank Condition**: The matrix $[\\mathbf{A}_{3}',\\mathbf{A}_{5}']$ must have rank $M-1$, where $M$ is the number of endogenous variables.\n2.  **Significance**: For the demand equation, $[\\mathbf{A}_{3}',\\mathbf{A}_{5}'] = \\beta_{22}$, a scalar. The rank is 1 unless $\\beta_{22} = 0$, ensuring identification.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the model structure - We have a simultaneous equations system with demand and supply equations. The identification problem arises because both equations share common variables.\n\nStep 2: Recall the rank condition requirement - For an equation to be identified, the matrix formed by the coefficients of variables excluded from that equation (but present in other equations) must have full rank equal to the number of endogenous variables minus 1.\n\nStep 3: Apply to the given context - Since we exclude both Union and Fem from the demand equation, we examine their coefficients in the supply equation. Let β_union (supply) and β_fem (supply) be their respective coefficients.\n\nStep 4: Construct the restriction matrix - For the demand equation's identification, we create a matrix of coefficients from other equations (supply) using the excluded variables (Union, Fem). This matrix must have rank = number of endogenous variables - 1 (which would be 2-1=1 in a two-equation system).\n\nStep 5: Verify the rank - If at least one of β_union or β_fem in the supply equation is non-zero, the matrix rank = 1, satisfying the rank condition.\n\nSignificance: The rank condition ensures there are sufficient independent restrictions to uniquely identify structural parameters. In this model, excluding both Union and Fem from demand provides the necessary restrictions (assuming they affect supply), enabling proper identification of the demand equation that would otherwise be underidentified.\n\nQID: textbook-64-0-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-64-0-0-1\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the rank condition and its significance but deviates from the specific matrix $[\\\\mathbf{A}_{3}',\\\\mathbf{A}_{5}']$ mentioned in the gold answer. The reasoning focuses on a general application of the rank condition rather than the specific context provided in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-64-0-0-1", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the rank condition and its significance but deviates from the specific matrix $[\\mathbf{A}_{3}',\\mathbf{A}_{5}']$ mentioned in the gold answer. The reasoning focuses on a general application of the rank condition rather than the specific context provided in the gold answer."}, "llm_echoed_qid": "textbook-64-0-0-1", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the rank condition and its significance but deviates from the specific matrix $[\\mathbf{A}_{3}',\\mathbf{A}_{5}']$ mentioned in the gold answer. The reasoning focuses on a general application of the rank condition rather than the specific context provided in the gold answer."}
{"script_processing_qid": "textbook-122-21-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the out-of-sample forecasting performance of the AR(1) model (18.48) and the augmented model (18.49) using RMSE and MAE. Interpret the results.\n\nGOLD_ANSWER:\n1.  **AR(1) Model**: RMSE = 0.632, MAE = 0.515.\n2.  **Augmented Model**: RMSE = 0.550, MAE = 0.362.\n3.  **Interpretation**: The augmented model (with lagged inflation) has lower RMSE and MAE, indicating better out-of-sample forecasting performance.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key forecasts from the context. The AR(1) model (18.48) produces a 1997 unemployment forecast of 5.52, while the augmented model (18.49) produces 5.35. However, the actual unemployment value for 1997 is not explicitly provided in the context. Step 2: Recognize that RMSE and MAE require comparing forecasts to actual observed values. The formulas provided (RMSE = √[m⁻¹∑e²], MAE = m⁻¹∑|e|) depend on forecast errors e = actual − forecast. Step 3: Note that the context only includes forecasted values (5.52 and 5.35) and no actual out-of-sample data points beyond 1996. Even the 1997 forecast interval [3.6, 7.1] does not contain explicit real-world unemployment data. Step 4: Conclude that insufficient information exists to calculate RMSE/MAE. The context lacks the necessary actual unemployment values for 1997 or other out-of-sample periods required to compute forecast errors. Interpretation: While the augmented model produces a marginally lower point forecast (5.35 vs. 5.52), no meaningful comparison of forecasting performance can be made using RMSE/MAE metrics without access to the actual out-of-sample data.\n\nQID: textbook-122-21-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-21-0-3\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate answer incorrectly focuses on unemployment forecasts and actual values, which are irrelevant to the gold answer's context of inflation models. The gold answer provides clear RMSE and MAE values for AR(1) and augmented models, which the candidate fails to address.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-21-0-3", "category": "INCORRECT", "explanation": "The candidate answer incorrectly focuses on unemployment forecasts and actual values, which are irrelevant to the gold answer's context of inflation models. The gold answer provides clear RMSE and MAE values for AR(1) and augmented models, which the candidate fails to address."}, "llm_echoed_qid": "textbook-122-21-0-3", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate answer incorrectly focuses on unemployment forecasts and actual values, which are irrelevant to the gold answer's context of inflation models. The gold answer provides clear RMSE and MAE values for AR(1) and augmented models, which the candidate fails to address."}
{"script_processing_qid": "textbook-115-7-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n12. Discuss the challenges of extending the Tobit selection model to allow for endogenous explanatory variables in the structural equation. Why is this an area for future research?\n\nGOLD_ANSWER:\n12. **Endogenous Variables in Tobit Selection**: Endogeneity in $\\mathbf{x}_1$ complicates the model because standard IV methods must account for both endogeneity and selection. The two-step residual-inclusion approach (Procedure 17.3) may not generalize easily, and full MLE requires strong distributional assumptions. Future research could explore control function or GMM approaches that relax these assumptions.\n\nCANDIDATE_ANSWER:\nStep 1: The standard Tobit selection model (Procedure 17.3) assumes exogeneity of x₁ in the structural equation y₁ = x₁β₁ + u₁. Introducing endogenous variables in x₁ creates simultaneous correlation between these variables and u₁, violating key assumptions.\n\nStep 2: Selection bias correction (via v̂₂ residuals) and endogeneity correction would require simultaneous solutions. Existing two-step procedures only address selection bias but cannot handle endogenous x₁ variables because:\n1) The first-stage Tobit residuals (v̂₂) do not account for endogeneity in x₁\n2) Conventional IV methods conflict with the Heckman-style selection framework\n\nStep 3: Identification becomes more complex due to needing instruments that satisfy:\n- Relevance for endogenous variables in x₁\n- Exclusion restrictions for both equations\n- Independence from composite errors (u₁, v₂)\n\nStep 4: Estimation challenges emerge from:\n- Non-linearities in selection equation complicating IV implementation\n- Potential correlation between selection equation errors (v₂) and structural equation endogeneity\n- Lack of closed-form solutions requiring complex numerical methods\n\nStep 5: This remains an area for future research because:\n1) No consensus exists on identification conditions\n2) Limited robust estimation methods for combined endogeneity/selection\n3) Computational difficulties in joint maximum likelihood estimation\n4) Testing procedures for model validity are underdeveloped\n\nQID: textbook-115-7-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-7-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the challenges of extending the Tobit selection model to include endogenous variables, aligning well with the gold answer. It details the complications with existing methods, identification issues, and estimation challenges, and correctly identifies why this is an area for future research.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-7-2-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the challenges of extending the Tobit selection model to include endogenous variables, aligning well with the gold answer. It details the complications with existing methods, identification issues, and estimation challenges, and correctly identifies why this is an area for future research."}, "llm_echoed_qid": "textbook-115-7-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the challenges of extending the Tobit selection model to include endogenous variables, aligning well with the gold answer. It details the complications with existing methods, identification issues, and estimation challenges, and correctly identifies why this is an area for future research."}
{"script_processing_qid": "textbook-62-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss how unemployment insurance policies in Europe (e.g., 67% replacement rate) compared to the US (34% for six months) affect the natural rate of unemployment.\n\nGOLD_ANSWER:\n1. **Job Finding Rate ($e$)**: Generous unemployment insurance reduces the incentive to find jobs quickly, lowering $e$.\n2. **Job Losing Rate ($b$)**: Voluntary quits may increase if unemployment benefits are high, raising $b$.\n3. **Natural Rate**: Lower $e$ and higher $b$ increase $u^{*} = \\frac{b}{n + e + b}$, explaining higher European unemployment.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the natural rate of unemployment formula from the context: \\(u^{*} = \\frac{b}{n + e + b}\\), where \\(b\\) = job losing rate, \\(e\\) = job finding rate, and \\(n\\) = labor force growth rate (implicitly defined).\n\nStep 2: Analyze unemployment insurance effects:\n- Higher replacement rates (Europe: 67%) reduce the opportunity cost of unemployment. This decreases the incentive for unemployed workers to actively search for jobs, lowering the job finding rate (\\(e\\)).\n- Lower replacement rates (US: 34% for six months) incentivize faster job searches, increasing \\(e\\).\n\nStep 3: Apply changes to the formula:\n- A lower \\(e\\) in Europe increases \\(u^{*}\\) (denominator shrinks).\n- A higher \\(e\\) in the US decreases \\(u^{*}\\) (denominator grows).\n\nStep 4: Secondary effect on \\(b\\):\nGenerous unemployment benefits (Europe) may marginally increase \\(b\\) if employers lay off workers more casually, further raising \\(u^{*}\\). However, the primary driver is the difference in \\(e\\).\n\nConclusion: Europe’s policies (higher replacement rates) reduce \\(e\\) and potentially raise \\(b\\), leading to a higher natural rate of unemployment compared to the US with lower benefits.\n\nQID: textbook-62-0-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-62-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately follows the reasoning provided in the gold answer, detailing how unemployment insurance affects the job finding rate (e) and job losing rate (b), and consequently the natural rate of unemployment (u*). The explanation is thorough and matches the gold answer's points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-62-0-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately follows the reasoning provided in the gold answer, detailing how unemployment insurance affects the job finding rate (e) and job losing rate (b), and consequently the natural rate of unemployment (u*). The explanation is thorough and matches the gold answer's points."}, "llm_echoed_qid": "textbook-62-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately follows the reasoning provided in the gold answer, detailing how unemployment insurance affects the job finding rate (e) and job losing rate (b), and consequently the natural rate of unemployment (u*). The explanation is thorough and matches the gold answer's points."}
{"script_processing_qid": "textbook-115-8-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the advantages and disadvantages of using heteroskedasticity-robust standard errors versus weighted least squares (WLS) under heteroskedasticity.\n\nGOLD_ANSWER:\n**Robust SEs**:\n- Advantages: Valid under any form of heteroskedasticity, easy to compute, no need to model $\\operatorname{Var}(u|\\mathbf{x})$.\n- Disadvantages: Less efficient than WLS if the variance model is correct.\n\n**WLS**:\n- Advantages: More efficient if the variance model is correct.\n- Disadvantages: Inconsistent if the variance model is misspecified, requires correct specification of $\\operatorname{Var}(u|\\mathbf{x})$.\n\nCANDIDATE_ANSWER:\nStep 1: Advantages of heteroskedasticity-robust standard errors (HRSE)\n- **Robustness**: HRSE remain valid under any form of heteroskedasticity (including unknown patterns), as they use the estimator $\\hat{\\mathbf{A}}^{-1}\\hat{\\mathbf{B}}\\hat{\\mathbf{A}}^{-1}/N$ to consistently estimate the asymptotic variance without requiring knowledge of the error structure.\n- **Simplicity**: HRSE preserve OLS coefficient estimates and only adjust inference (standard errors), avoiding complex modeling of heteroskedasticity.\n\nStep 2: Disadvantages of HRSE\n- **Inefficiency**: OLS with HRSE retains the same coefficient estimates, which are less efficient than WLS if the heteroskedasticity structure is correctly modeled.\n- **Larger standard errors**: HRSE often produce wider confidence intervals compared to correctly specified WLS.\n\nStep 3: Advantages of WLS\n- **Efficiency**: If weights correctly reflect the heteroskedasticity pattern (i.e., $\\text{Var}(u|\\mathbf{x})$ is properly modeled), WLS provides asymptotically efficient estimates.\n- **Better finite-sample properties**: When the weighting structure is accurate, WLS typically yields smaller standard errors than HRSE.\n\nStep 4: Disadvantages of WLS\n- **Specification sensitivity**: WLS requires correct modeling of $\\text{Var}(u|\\mathbf{x})$. Misspecification leads to inconsistent parameter estimates and invalid inference.\n- **Implementation complexity**: WLS demands estimation/verification of the heteroskedasticity structure (e.g., via residual analysis or parametric assumptions), which is unnecessary for HRSE.\n\nSummary: HRSE trade efficiency for robustness and simplicity, while WLS achieves efficiency gains at the cost of requiring correct heteroskedasticity specification.\n\nQID: textbook-115-8-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-8-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly covers the advantages and disadvantages of both heteroskedasticity-robust standard errors and weighted least squares, aligning closely with the gold answer. The reasoning is detailed and accurate, addressing robustness, efficiency, and specification requirements for each method.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-8-1-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly covers the advantages and disadvantages of both heteroskedasticity-robust standard errors and weighted least squares, aligning closely with the gold answer. The reasoning is detailed and accurate, addressing robustness, efficiency, and specification requirements for each method."}, "llm_echoed_qid": "textbook-115-8-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly covers the advantages and disadvantages of both heteroskedasticity-robust standard errors and weighted least squares, aligning closely with the gold answer. The reasoning is detailed and accurate, addressing robustness, efficiency, and specification requirements for each method."}
{"script_processing_qid": "textbook-36-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Using the concept of Pareto efficiency, explain why rent control leads to inefficiency in the apartment market.\n\nGOLD_ANSWER:\nRent control creates inefficiency by:\n1. **Non-Market Allocation**: Prices are set below equilibrium, leading to excess demand.\n2. **Deadweight Loss**: Misallocation of apartments (e.g., some high-willingness-to-pay renters cannot find housing).\n3. **No Pareto Improvement**: Reallocating apartments to higher-valuation renters would make them better off without harming others, but rent control prevents this.\n\nCANDIDATE_ANSWER:\nStep 1: Understand Pareto Efficiency\nAn allocation is Pareto efficient if no party can be made better off without making another party worse off. Free markets achieve this by ensuring all mutually beneficial transactions occur.\n\nStep 2: Market Equilibrium vs. Rent Control\nIn a free apartment market, equilibrium price balances supply and demand. Rent control sets prices below equilibrium, creating excess demand (shortage) and reducing landlords' supply incentives.\n\nStep 3: Identify Inefficiency\nThe price ceiling under rent control creates two groups:\n- Tenants who secure apartments at below-market rates (better off)\n- Landlords (worse off due to lower revenue) and tenants unable to find housing (worse off due to shortage).\n\nStep 4: Pareto Violation\nEven if some parties benefit, the outcome is Pareto inefficient because:\n- Willing tenants (unable to find housing at controlled prices) would pay more to landlords.\n- Landlords would supply more apartments at higher prices.\nThese mutually beneficial trades are blocked by rent control, leaving potential gains unrealized.\n\nStep 5: Conclusion\nSince rent control prevents reallocations that could improve outcomes for both landlords and excluded tenants without harming others, it violates Pareto efficiency.\n\nQID: textbook-36-0-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-36-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains how rent control leads to inefficiency by violating Pareto efficiency, aligning perfectly with the gold answer's points on non-market allocation, deadweight loss, and lack of Pareto improvement.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-36-0-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains how rent control leads to inefficiency by violating Pareto efficiency, aligning perfectly with the gold answer's points on non-market allocation, deadweight loss, and lack of Pareto improvement."}, "llm_echoed_qid": "textbook-36-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains how rent control leads to inefficiency by violating Pareto efficiency, aligning perfectly with the gold answer's points on non-market allocation, deadweight loss, and lack of Pareto improvement."}
{"script_processing_qid": "textbook-92-1-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain the significance of the result $\\pmb{\\mathrm{E}}[\\mathbf{s}(\\mathbf{w};\\pmb{\\theta}\\_{0})\\mid\\mathbf{x}]=\\mathbf{0}$ in the context of maximum likelihood estimation.\n\nGOLD_ANSWER:\nThe result $\\pmb{\\mathrm{E}}[\\mathbf{s}(\\mathbf{w};\\pmb{\\theta}\\_{0})\\mid\\mathbf{x}]=\\mathbf{0}$ is significant because:\n1.  **Unbiasedness**: It shows that the score function has zero mean at the true parameter value, which is a key property for the consistency of the maximum likelihood estimator.\n2.  **Information Identity**: It leads to the information equality, where the variance of the score equals the negative expected Hessian, forming the basis for the Fisher information matrix.\n3.  **Asymptotic Normality**: This property is used to derive the asymptotic distribution of the maximum likelihood estimator.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that the score function $\\mathbf{s}(\\mathbf{w};\\pmb{\theta})$ is defined as $\\frac{\\partial}{\\partial\\pmb{\theta}}\\ln f(y|\\mathbf{x};\\pmb{\theta})$, which relates to the derivative of the density function via $\\mathbf{s}f = \\frac{\\partial f}{\\partial\\pmb{\theta}}$.\n\nStep 2: Observe that differentiating the identity $\\int f(y|\\mathbf{x};\\pmb{\theta})dy=1$ under regularity conditions gives $\\int\\frac{\\partial}{\\partial\\pmb{\theta}}f(y|\\mathbf{x};\\pmb{\theta})dy = \\mathbf{0}$.\n\nStep 3: Substitute $\\mathbf{s}f$ for $\\frac{\\partial f}{\\partial\\pmb{\theta}}$ in the integral to get $\\int\\mathbf{s}(\\mathbf{w};\\pmb{\theta})f(y|\\mathbf{x};\\pmb{\theta})dy = \\mathbf{0}$. This represents $\\mathrm{E}[\\mathbf{s}(\\mathbf{w};\\pmb{\theta}_0)|\\mathbf{x}]$ when evaluated at the true parameter $\\pmb{\theta}_0$.\n\nStep 4: Understand that this result shows the score function has conditional mean zero at the true parameter value. This is critical for maximum likelihood estimation because:\n- (a) It confirms the first-order condition for MLE is valid in expectation\n- (b) It establishes the foundation for proving consistency of MLE\n- (c) It enables derivation of the information equality (relationship between score variance and Fisher information)\n- (d) It ensures the score forms a martingale difference sequence under correct specification, crucial for asymptotic normality.\n\nQID: textbook-92-1-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-92-1-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the significance of the result by detailing the derivation steps and connecting them to key properties of maximum likelihood estimation (unbiasedness, information equality, asymptotic normality), matching the gold answer's points while providing additional technical depth.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-92-1-0-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the significance of the result by detailing the derivation steps and connecting them to key properties of maximum likelihood estimation (unbiasedness, information equality, asymptotic normality), matching the gold answer's points while providing additional technical depth."}, "llm_echoed_qid": "textbook-92-1-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the significance of the result by detailing the derivation steps and connecting them to key properties of maximum likelihood estimation (unbiasedness, information equality, asymptotic normality), matching the gold answer's points while providing additional technical depth."}
{"script_processing_qid": "textbook-112-17-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Quantitatively compare the productivity gains from division of labor in pin-making, as described by Smith. Calculate the difference in output per worker between a divided labor system and individual production.\n\nGOLD_ANSWER:\n1.  **Individual Production**: Without division of labor, a worker might produce at most 20 pins per day.\n2.  **Divided Labor**: With division of labor, 10 workers produce 48,000 pins per day, or 4,800 pins per worker.\n3.  **Productivity Gain**: The divided labor system yields a 240-fold increase in productivity per worker (4,800 / 20).\n\nCANDIDATE_ANSWER:\nThe provided context does not contain sufficient quantitative data to calculate exact productivity differences. While Smith's pin-making example typically cites that divided labor could produce 48,000 pins/day with 10 workers (4,800/worker) vs. ~20 pins/day for individual production (as per his original text), the extracted passages provided here lack these numerical specifics. Therefore, a quantitative comparison cannot be made using only the given context.\n\nQID: textbook-112-17-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-17-0-0\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the lack of quantitative data.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-17-0-0", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the lack of quantitative data."}, "llm_echoed_qid": "textbook-112-17-0-0", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the lack of quantitative data."}
{"script_processing_qid": "textbook-84-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Explain why the Tobit model might be preferred over OLS for estimating demand functions despite its assumptions being violated in the given context.\n\nGOLD_ANSWER:\n1. **Functional Form Improvement**: The Tobit model can better capture the non-linearities in demand data, especially when there are censoring or focal points in the dependent variable.\n2. **Latent Variable Framework**: Even with violations, the Tobit model's latent variable approach may provide a better approximation to the true data-generating process than a linear model.\n3. **Partial Effects**: Tobit coefficients can be scaled to estimate partial effects on the expected demand, which may be more accurate than OLS estimates.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the key limitation of Tobit in this context - the observed data has non-continuous focal points (e.g., one-pound reports), violating Tobit's assumption of a normally distributed latent error.\n\nStep 2: Recognize that OLS suffers from functional form misspecification when the true relationship is nonlinear due to censoring/clustering at zero or other focal points. OLS would treat all observations as uncensored, potentially yielding biased estimates.\n\nStep 3: Note that Tobit explicitly models censoring (or censoring-like behavior in this case), allowing for a nonlinear relationship between predictors and the dependent variable. This aligns better with demand theory, as evidenced by the coherent price coefficient signs in the Tobit results.\n\nStep 4: Highlight that the authors view Tobit as a pragmatic functional form improvement over linear models. While its assumptions are violated, it better approximates the expected demand function by accounting for clustering in reported values, even if imperfectly.\n\nStep 5: Conclude that Tobit’s structure improves estimation of marginal effects compared to OLS in this scenario, despite violations of strict distributional assumptions, making it preferable for capturing demand behavior.\n\nQID: textbook-84-0-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-84-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains why the Tobit model is preferred over OLS despite its assumptions being violated, aligning well with the gold answer's points about functional form improvement, latent variable framework, and partial effects.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-84-0-0-0", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains why the Tobit model is preferred over OLS despite its assumptions being violated, aligning well with the gold answer's points about functional form improvement, latent variable framework, and partial effects."}, "llm_echoed_qid": "textbook-84-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains why the Tobit model is preferred over OLS despite its assumptions being violated, aligning well with the gold answer's points about functional form improvement, latent variable framework, and partial effects."}
{"script_processing_qid": "textbook-112-36-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Critically evaluate Adam Smith's argument against regulating the number of retailers. What are the potential benefits and drawbacks of retail competition?\n\nGOLD_ANSWER:\n1. **Benefits of Competition**: Retail competition lowers prices for consumers and increases efficiency. It prevents monopolistic practices and ensures fair pricing.\n2. **Drawbacks**: Some retailers may engage in unethical practices (e.g., decoying weak customers), but Smith argues this is minor compared to the benefits.\n3. **Regulation Impact**: Restricting retailer numbers could lead to monopolies, higher prices, and reduced convenience for consumers, especially the poor.\n4. **Smith's Conclusion**: The market naturally limits the number of retailers based on demand, making regulation unnecessary.\n\nCANDIDATE_ANSWER:\nStep 1: Adam Smith’s argument against regulating the number of retailers hinges on the principle of natural competition balancing capital allocation. He emphasizes that the profits of stock depend on the proportion of capital employed, not arbitrary regulations. Limiting retailers would distort this balance, reducing efficiency in the fourth use of capital (dividing goods into small parcels to meet consumer demand).\n\nStep 2: Benefits of retail competition, per Smith, include (a) increased real capital via enhanced commodity circulation (even with fixed money supply), (b) alignment of legal interest rates with market rates to avoid funneling capital to高风险 borrowers (e.g., prodigals), and (c) optimal resource distribution across all four capital employments (procuring, manufacturing, transporting, retailing).\n\nStep 3: Drawbacks Smith overlooks: The provided context does not explicitly address potential downsides of retail competition. However, a critical evaluation might note that Smith’s reliance on self-regulating markets assumes perfect competition, ignoring risks like monopolistic tendencies or market fragmentation. His focus on capital efficiency may also undervalue consumer protection needs, which regulations could address.\n\nQID: textbook-112-36-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-36-0-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the benefits and drawbacks of retail competition as outlined by Adam Smith, but the reasoning slightly deviates from the gold answer by introducing additional economic concepts (e.g., capital allocation, legal interest rates) not explicitly mentioned in the gold standard. The drawbacks section also speculates beyond the provided context.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-36-0-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the benefits and drawbacks of retail competition as outlined by Adam Smith, but the reasoning slightly deviates from the gold answer by introducing additional economic concepts (e.g., capital allocation, legal interest rates) not explicitly mentioned in the gold standard. The drawbacks section also speculates beyond the provided context."}, "llm_echoed_qid": "textbook-112-36-0-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the benefits and drawbacks of retail competition as outlined by Adam Smith, but the reasoning slightly deviates from the gold answer by introducing additional economic concepts (e.g., capital allocation, legal interest rates) not explicitly mentioned in the gold standard. The drawbacks section also speculates beyond the provided context."}
{"script_processing_qid": "textbook-106-20-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why total revenue is maximized when the price elasticity of demand is unit elastic.\n\nGOLD_ANSWER:\n1. **Total Revenue**: $TR = P \\cdot Q$.\n2. **Elasticity and TR**: When $E_d > 1$, a price decrease increases $TR$. When $E_d < 1$, a price decrease reduces $TR$. At $E_d = 1$, $TR$ is maximized because the percentage change in quantity offsets the percentage change in price.\n\nCANDIDATE_ANSWER:\nStep 1: Understand that total revenue (TR) equals price (P) multiplied by quantity sold (Q) (TR = P × Q).\nStep 2: Recognize that price elasticity of demand (Ed) measures the percentage change in quantity demanded relative to the percentage change in price (Ed = %ΔQ/%ΔP).\nStep 3: When demand is inelastic (|Ed| < 1):\n- A price increase causes a proportionally smaller decrease in quantity demanded.\n- TR increases when price rises (as confirmed in the context: $1 → $2 price increase raised TR).\nStep 4: When demand is elastic (|Ed| > 1):\n- A price increase causes a proportionally larger decrease in quantity demanded.\n- TR decreases when price rises (as confirmed in the context: $5 → $6 price increase reduced TR).\nStep 5: At unit elasticity (|Ed| = 1):\n- The percentage change in quantity demanded exactly offsets the percentage change in price.\n- TR remains constant during price changes (as shown in the context between $3 and $4).\nStep 6: Since TR increases when moving from inelastic regions toward unit elasticity and decreases when moving from unit elasticity to elastic regions, TR must peak at the unit-elastic point. This maximizes TR because further price changes in either direction would reduce TR.\n\nQID: textbook-106-20-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-20-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly explains why total revenue is maximized when the price elasticity of demand is unit elastic, aligning with the gold answer. The steps logically build up to the conclusion that TR peaks at unit elasticity, as further price changes in either direction would reduce TR.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-20-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly explains why total revenue is maximized when the price elasticity of demand is unit elastic, aligning with the gold answer. The steps logically build up to the conclusion that TR peaks at unit elasticity, as further price changes in either direction would reduce TR."}, "llm_echoed_qid": "textbook-106-20-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly explains why total revenue is maximized when the price elasticity of demand is unit elastic, aligning with the gold answer. The steps logically build up to the conclusion that TR peaks at unit elasticity, as further price changes in either direction would reduce TR."}
{"script_processing_qid": "textbook-114-20-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate the economic cost of a decision where a firm has a sunk cost of $200,000 and an opportunity cost of $50,000. Explain your reasoning.\n\nGOLD_ANSWER:\n1. **Sunk Cost**: $200,000 (excluded from economic cost).\n2. **Opportunity Cost**: $50,000 (included).\n3. **Economic Cost**: $50,000. Only the opportunity cost is relevant for future decisions.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the components of economic cost. Economic cost includes explicit and implicit costs, but sunk costs are excluded because they cannot be recovered and are irrelevant to decision-making.\n\nStep 2: Identify the given values. The sunk cost is $200,000, and the opportunity cost (representing the value of the next best alternative foregone) is $50,000.\n\nStep 3: Apply the principle from the context. Since sunk costs should not influence decisions, they are excluded from economic cost calculations. Only the opportunity cost ($50,000) is relevant.\n\nConclusion: The economic cost of the decision is $50,000. The $200,000 sunk cost is disregarded per the reasoning in the provided context.\n\nQID: textbook-114-20-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-20-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly identifies that sunk costs are excluded from economic cost calculations and only the opportunity cost is relevant. The reasoning and final answer align perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-20-1-2", "category": "CORRECT", "explanation": "The candidate correctly identifies that sunk costs are excluded from economic cost calculations and only the opportunity cost is relevant. The reasoning and final answer align perfectly with the gold answer."}, "llm_echoed_qid": "textbook-114-20-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly identifies that sunk costs are excluded from economic cost calculations and only the opportunity cost is relevant. The reasoning and final answer align perfectly with the gold answer."}
{"script_processing_qid": "textbook-121-14-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Given $A = \\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}$ and $B = \\begin{bmatrix} 0 & -1 \\\\ 6 & 7 \\end{bmatrix}$, compute $AB$ and $BA$ to demonstrate non-commutativity.\n\nGOLD_ANSWER:\n1. **$AB$**:\n$$\n\\begin{bmatrix}\n1(0) + 2(6) & 1(-1) + 2(7) \\\\\n3(0) + 4(6) & 3(-1) + 4(7)\n\\end{bmatrix}\n= \\begin{bmatrix}\n12 & 13 \\\\\n24 & 25\n\\end{bmatrix}\n$$\n2. **$BA$**:\n$$\n\\begin{bmatrix}\n0(1) - 1(3) & 0(2) - 1(4) \\\\\n6(1) + 7(3) & 6(2) + 7(4)\n\\end{bmatrix}\n= \\begin{bmatrix}\n-3 & -4 \\\\\n27 & 40\n\\end{bmatrix}\n$$\nThus, $AB \\neq BA$.\n\nCANDIDATE_ANSWER:\nStep 1: Compute product AB\n- Row 1 of A multiplied by column 1 of B: (1)(0) + (2)(6) = 0 + 12 = 12\n- Row 1 of A multiplied by column 2 of B: (1)(-1) + (2)(7) = -1 + 14 = 13\n- Row 2 of A multiplied by column 1 of B: (3)(0) + (4)(6) = 0 + 24 = 24\n- Row 2 of A multiplied by column 2 of B: (3)(-1) + (4)(7) = -3 + 28 = 25\nResult:\nAB = [[12, 13], [24, 25]]\n\nStep 2: Compute product BA\n- Row 1 of B multiplied by column 1 of A: (0)(1) + (-1)(3) = 0 - 3 = -3\n- Row 1 of B multiplied by column 2 of A: (0)(2) + (-1)(4) = 0 - 4 = -4\n- Row 2 of B multiplied by column 1 of A: (6)(1) + (7)(3) = 6 + 21 = 27\n- Row 2 of B multiplied by column 2 of A: (6)(2) + (7)(4) = 12 + 28 = 40\nResult:\nBA = [[-3, -4], [27, 40]]\n\nFinal comparison:\nAB ≠ BA, thus demonstrating non-commutativity of matrix multiplication.\n\nQID: textbook-121-14-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-121-14-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly computed both AB and BA, and accurately demonstrated that AB ≠ BA, thus proving non-commutativity. The steps and final results match the gold answer exactly.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-121-14-1-1", "category": "CORRECT", "explanation": "The candidate correctly computed both AB and BA, and accurately demonstrated that AB ≠ BA, thus proving non-commutativity. The steps and final results match the gold answer exactly."}, "llm_echoed_qid": "textbook-121-14-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly computed both AB and BA, and accurately demonstrated that AB ≠ BA, thus proving non-commutativity. The steps and final results match the gold answer exactly."}
{"script_processing_qid": "textbook-122-4-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the limitations of covariance as a measure of dependence between two random variables. Provide an example where $\\operatorname{Cov}(X, Y) = 0$ but $X$ and $Y$ are not independent.\n\nGOLD_ANSWER:\nCovariance measures linear dependence but fails to capture nonlinear relationships. An example is $Y = X^2$ where $X$ is a random variable with $\\operatorname{E}(X) = 0$ and $\\operatorname{E}(X^3) = 0$. Here, $\\operatorname{Cov}(X, Y) = \\operatorname{E}(X^3) - \\operatorname{E}(X)\\operatorname{E}(X^2) = 0$, but $X$ and $Y$ are clearly dependent since $Y$ is a function of $X$.\n\nCANDIDATE_ANSWER:\nStep 1: Covariance measures linear dependence but ignores nonlinear relationships. Since Cov(X,Y) = E[(X - μ_X)(Y - μ_Y)], it only detects directional linear associations. This means variables with strong nonlinear dependencies (e.g., quadratic, circular) can still have Cov(X,Y) = 0.\n\nStep 2: Covariance is scale-dependent, making interpretation sensitive to variable units. While standardization via correlation addresses this, covariance itself does not provide normalized measures of dependence strength.\n\nStep 3: Example of Cov(X,Y) = 0 with dependence: Let X ~ Uniform[-1,1] and Y = X². Here, E[X] = 0, and Cov(X,Y) = E[XY] - E[X]E[Y] = E[X³] - 0 = 0 (since X³ is odd-symmetric). However, Y is perfectly determined by X, showing non-independence despite zero covariance.\n\nQID: textbook-122-4-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-4-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the limitations of covariance, including its inability to capture nonlinear relationships and its scale-dependency. The example provided (Y = X² with X ~ Uniform[-1,1]) is accurate and demonstrates a case where Cov(X,Y) = 0 but X and Y are dependent, aligning perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-4-0-3", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the limitations of covariance, including its inability to capture nonlinear relationships and its scale-dependency. The example provided (Y = X² with X ~ Uniform[-1,1]) is accurate and demonstrates a case where Cov(X,Y) = 0 but X and Y are dependent, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-122-4-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the limitations of covariance, including its inability to capture nonlinear relationships and its scale-dependency. The example provided (Y = X² with X ~ Uniform[-1,1]) is accurate and demonstrates a case where Cov(X,Y) = 0 but X and Y are dependent, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-84-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the $t$ statistic for testing the null hypothesis $\\mathrm{H}_{0}\\colon\\beta_{1}+\\beta_{2}=0$ in the Tobit model, as described in the text.\n\nGOLD_ANSWER:\n1. **Define $\\theta_{1}$**: Let $\\theta_{1} = \\beta_{1} + \\beta_{2}$.\n2. **Estimate $\\hat{\\theta}_{1}$**: From the Tobit regression, $\\hat{\\theta}_{1} = -0.16$.\n3. **Standard Error**: The standard error of $\\hat{\\theta}_{1}$ is approximately 0.59.\n4. **$t$ Statistic**: Compute $t = \\frac{\\hat{\\theta}_{1}}{SE(\\hat{\\theta}_{1})} = \\frac{-0.16}{0.59} \\approx -0.27$.\n5. **Conclusion**: With a $p$-value of 0.78, we do not reject the null hypothesis.\n\nCANDIDATE_ANSWER:\nStep 1: Reparameterize the model by defining θ₁ = β₁ + β₂. Under H₀, θ₁ = 0. Rewrite the original model by substituting β₂ = θ₁ - β₁ into the equation.\n\nStep 2: Substitute β₂ = θ₁ - β₁ into the Tobit model covariates. For example, replace (β₁·ecoprc + β₂·regprc) with β₁·(ecoprc - regprc) + θ₁·regprc.\n\nStep 3: Estimate the transformed Tobit model with new covariates: (ecoprc - regprc), regprc, faminc, and hhsize. The coefficient on regprc now directly estimates θ₁.\n\nStep 4: Extract the estimate θ̂₁ (-0.16) and its standard error (0.59) from the transformed model's output.\n\nStep 5: Compute the t-statistic as t = (θ̂₁ - 0)/SE(θ̂₁) = -0.16/0.59 ≈ -0.27.\n\nStep 6: Compare the t-statistic (-0.27) to critical values or compute p-value (0.78) to conclude failure to reject H₀.\n\nQID: textbook-84-0-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-84-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the t statistic for testing the null hypothesis in the Tobit model, matching the gold answer in both methodology and final result.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-84-0-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the t statistic for testing the null hypothesis in the Tobit model, matching the gold answer in both methodology and final result."}, "llm_echoed_qid": "textbook-84-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the t statistic for testing the null hypothesis in the Tobit model, matching the gold answer in both methodology and final result."}
{"script_processing_qid": "textbook-122-38-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss why the SEM for housing and saving expenditures in Example 16.2 is inappropriate. What key feature is missing?\n\nGOLD_ANSWER:\nThe SEM for housing and saving is inappropriate because both equations describe the behavior of the same economic agent (the household). Neither equation has a clear ceteris paribus interpretation, as housing and saving are jointly determined by the household's utility maximization. A valid SEM requires each equation to represent distinct behavioral relationships, typically involving different agents or decision-makers.\n\nCANDIDATE_ANSWER:\nStep 1: The context emphasizes that valid SEMs require each equation to represent distinct behavioral relationships (e.g., supply vs. demand) with exclusion restrictions—variables that appear in one equation but are excluded from the other.\n\nStep 2: In Example 16.2 (implied by the question), housing and saving expenditures likely form a system where both equations include the same set of variables without exclusion restrictions.\n\nStep 3: This violates the key requirement for SEM identification: at least one exogenous variable must be excluded from each equation to provide a source of variation for parameter estimation. Without such excluded instruments, the structural parameters cannot be distinguished from reduced-form relationships.\n\nStep 4: The missing key feature is exclusion restrictions (distinct exogenous variables in each structural equation), leading to an unidentified system where true causal effects cannot be disentangled.\n\nQID: textbook-122-38-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-38-0-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the issue of missing exclusion restrictions, which aligns with the gold answer's critique of the SEM. However, the candidate focuses on identification issues, while the gold answer emphasizes the lack of distinct behavioral relationships due to both equations describing the same agent's behavior.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-38-0-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the issue of missing exclusion restrictions, which aligns with the gold answer's critique of the SEM. However, the candidate focuses on identification issues, while the gold answer emphasizes the lack of distinct behavioral relationships due to both equations describing the same agent's behavior."}, "llm_echoed_qid": "textbook-122-38-0-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the issue of missing exclusion restrictions, which aligns with the gold answer's critique of the SEM. However, the candidate focuses on identification issues, while the gold answer emphasizes the lack of distinct behavioral relationships due to both equations describing the same agent's behavior."}
{"script_processing_qid": "textbook-119-22-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the Dickey-Fuller test equation starting from the autoregressive model $Y_{t} = \\rho Y_{t-1} + u_{t}$ and explain the implications of $\\delta = 0$.\n\nGOLD_ANSWER:\n1. **Starting Point**: Begin with the AR(1) model: $$Y_{t} = \\rho Y_{t-1} + u_{t}$$ where $u_{t}$ is white noise.\n2. **Manipulation**: Subtract $Y_{t-1}$ from both sides: $$Y_{t} - Y_{t-1} = (\\rho - 1)Y_{t-1} + u_{t}$$ which simplifies to $$\\Delta Y_{t} = \\delta Y_{t-1} + u_{t}$$ where $\\delta = \\rho - 1$.\n3. **Implications of $\\delta = 0$**: If $\\delta = 0$, then $\\rho = 1$, indicating a unit root and nonstationarity of the series.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the AR(1) model\nGiven: \\( Y_{t} = \\rho Y_{t-1} + u_{t} \\).\n\nStep 2: Subtract \\( Y_{t-1} \\) from both sides\n\\( Y_{t} - Y_{t-1} = \\rho Y_{t-1} - Y_{t-1} + u_{t} \\).\n\nStep 3: Factorize the right-hand side\n\\( \\Delta Y_{t} = (\\rho - 1)Y_{t-1} + u_{t} \\), where \\( \\Delta Y_{t} \\) is the first difference.\n\nStep 4: Define \\( \\delta = \\rho - 1 \\)\nThis yields the Dickey-Fuller test equation: \\( \\Delta Y_{t} = \\delta Y_{t-1} + u_{t} \\).\n\nImplications of \\( \\delta = 0 \\):\n- If \\( \\delta = 0 \\), then \\( \\rho = 1 \\) (unit root exists).\n- The series \\( Y_{t} \\) becomes a random walk without drift: \\( Y_{t} = Y_{t-1} + u_{t} \\).\n- This implies the series is **nonstationary** (e.g., variance grows over time).\n- The null hypothesis \\( H_0: \\delta = 0 \\) (unit root) is tested against \\( H_1: \\delta < 0 \\) (stationary).\n- Failure to reject \\( \\delta = 0 \\) suggests the series requires differencing to achieve stationarity.\n\nQID: textbook-119-22-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-22-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the Dickey-Fuller test equation from the AR(1) model and accurately explains the implications of δ = 0, including the presence of a unit root and nonstationarity. The reasoning and steps align perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-22-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the Dickey-Fuller test equation from the AR(1) model and accurately explains the implications of δ = 0, including the presence of a unit root and nonstationarity. The reasoning and steps align perfectly with the gold answer."}, "llm_echoed_qid": "textbook-119-22-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the Dickey-Fuller test equation from the AR(1) model and accurately explains the implications of δ = 0, including the presence of a unit root and nonstationarity. The reasoning and steps align perfectly with the gold answer."}
{"script_processing_qid": "textbook-122-2-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Why is the 2SLS estimator less efficient than OLS when the explanatory variables are exogenous? Provide a theoretical explanation.\n\nGOLD_ANSWER:\n1.  **Efficiency**: OLS is the Best Linear Unbiased Estimator (BLUE) under the Gauss-Markov assumptions.\n2.  **2SLS Variance**: 2SLS involves an additional step (first-stage regression), which introduces more variability in the estimates.\n3.  **Asymptotic Properties**: While both estimators are consistent under exogeneity, 2SLS has larger asymptotic variance due to the use of instruments.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that OLS is the Best Linear Unbiased Estimator (BLUE) under the Gauss-Markov assumptions, which include exogeneity of regressors. When exogeneity holds, OLS achieves minimum variance among linear unbiased estimators.\n\nStep 2: 2SLS introduces additional variability through its two-stage estimation process. Even when variables are exogenous, 2SLS uses instrumental variables (which act here as unnecessary proxies) to create fitted values in the first stage. This first-stage projection adds estimation error.\n\nStep 3: The asymptotic variance formula for 2SLS depends on the correlation between instruments and endogenous variables. When variables are actually exogenous (but 2SLS is mistakenly applied), the instruments contribute no meaningful explanatory power beyond the original variables, leading to larger standard errors compared to OLS.\n\nStep 4: Mathematically, the 2SLS variance includes a term inversely proportional to the square of the correlation between instruments and regressors. When regressors are exogenous (and instruments are redundant), this correlation is weak, inflating the variance relative to OLS.\n\nConclusion: Under exogeneity, 2SLS sacrifices efficiency by introducing unnecessary first-stage estimation error, while OLS remains optimal with minimal variance.\n\nQID: textbook-122-2-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-2-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains why 2SLS is less efficient than OLS under exogeneity, covering the BLUE property of OLS, additional variability in 2SLS, and the mathematical reasoning behind the variance inflation. This aligns perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-2-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains why 2SLS is less efficient than OLS under exogeneity, covering the BLUE property of OLS, additional variability in 2SLS, and the mathematical reasoning behind the variance inflation. This aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-122-2-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains why 2SLS is less efficient than OLS under exogeneity, covering the BLUE property of OLS, additional variability in 2SLS, and the mathematical reasoning behind the variance inflation. This aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-115-21-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the concept of local alternatives and its role in analyzing the asymptotic local power of test statistics.\n\nGOLD_ANSWER:\n**Local Alternatives**: Local alternatives are sequences of hypotheses that approach the null hypothesis as $N \\to \\infty$, defined as $\\mathrm{H}_{1}^{N}: \\mathbf{c}(\\pmb{\\theta}_{\\mathrm{o},N}) = \\pmb{\\delta}_{\\mathrm{o}} / \\sqrt{N}$. These alternatives are \"close\" to $\\mathrm{H}_{0}$ and allow for finite-sample power analysis.\n\n**Asymptotic Local Power**: Under local alternatives, Wald, LM, and QLR statistics have limiting noncentral chi-square distributions with noncentrality parameters depending on $\\mathbf{A}_{0}$, $\\mathbf{B}_{0}$, $\\mathbf{C}_{0}$, and $\\pmb{\\delta}_{\\mathrm{o}}$. The tests are consistent against fixed alternatives, but local power analysis helps compare their performance for alternatives near $\\mathrm{H}_{0}$. All three tests have the same noncentrality parameter when $\\mathbf{B}_{0} = \\mathbf{A}_{0}$, implying similar asymptotic local power.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain enough information to answer this question. While the text discusses hypothesis testing via objective function changes (e.g., QLR statistic) and asymptotic distributions under the null hypothesis, it does not explicitly mention or describe the concept of local alternatives or their role in analyzing asymptotic local power of test statistics. Local alternatives typically refer to parameter sequences approaching the null at a specific rate (e.g., 1/√N) to study test power properties, but this theoretical framework is not addressed in the given excerpts.\n\nQID: textbook-115-21-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-21-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer states the provided context does not contain enough information.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-21-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer states the provided context does not contain enough information."}, "llm_echoed_qid": "textbook-115-21-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer states the provided context does not contain enough information."}
{"script_processing_qid": "textbook-77-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. How does the summation of individual budget constraints lead to the aggregate market clearing condition?\n\nGOLD_ANSWER:\n4. **Aggregate Market Clearing**: Summing the individual budget constraints over all consumers yields $$\\mathtt{p\\_{1}}(\\sum\\_{i}x\\_{1i}(p) - \\overline{\\omega}\\_{1}) + \\mathtt{p\\_{2}}(\\sum\\_{i}x\\_{2i}(p) - \\overline{\\omega}\\_{2}) = 0.$$ If the market for good 1 clears, the first term vanishes, implying the second term must also vanish, ensuring the market for good 2 clears.\n\nCANDIDATE_ANSWER:\nStep 1: Observe that for each consumer i (i=1,2), local nonsatiation forces the budget constraint to hold with equality: \np₁(x₁ᵢ(p) - ω₁ᵢ) + p₂(x₂ᵢ(p) - ω₂ᵢ) = 0.\n\nStep 2: Sum these equations across all consumers i=1,2:\nΣᵢ [p₁(x₁ᵢ(p) - ω₁ᵢ) + p₂(x₂ᵢ(p) - ω₂ᵢ)] = 0.\n\nStep 3: Distribute summation across terms:\np₁(Σᵢx₁ᵢ(p) - Σᵢω₁ᵢ) + p₂(Σᵢx₂ᵢ(p) - Σᵢω₂ᵢ) = 0.\n\nStep 4: Recognize that Σᵢω₁ᵢ = aggregate endowment Ω₁ and Σᵢω₂ᵢ = Ω₂. This gives:\np₁(Σx₁ᵢ - Ω₁) + p₂(Σx₂ᵢ - Ω₂) = 0.\n\nStep 5: If market 1 clears (Σx₁ᵢ = Ω₁), then the first term becomes zero, forcing p₂(Σx₂ᵢ - Ω₂) = 0.\n\nStep 6: Since prices are positive (p₂ > 0), this implies Σx₂ᵢ = Ω₂. Thus, market clearing in one good (through price adjustment) automatically ensures market clearing in the other good via the aggregated budget constraints.\n\nQID: textbook-77-0-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-77-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly follows the steps to derive the aggregate market clearing condition from individual budget constraints, aligning with the gold answer. The reasoning and conclusion are both accurate and well-justified.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-77-0-0-3", "category": "CORRECT", "explanation": "The candidate answer correctly follows the steps to derive the aggregate market clearing condition from individual budget constraints, aligning with the gold answer. The reasoning and conclusion are both accurate and well-justified."}, "llm_echoed_qid": "textbook-77-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly follows the steps to derive the aggregate market clearing condition from individual budget constraints, aligning with the gold answer. The reasoning and conclusion are both accurate and well-justified."}
{"script_processing_qid": "textbook-123-0-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain the equivalence between Mortensen and Pissarides' bargaining specification and the standard Nash product (26.6.14) in terms of present value of wages and firm's match surplus.\n\nGOLD_ANSWER:\n1. **Mortensen-Pissarides**: The wage in the first period is reduced by the worker's share of future layoff taxes, and future wages are increased by the net interest on this 'bond'.\n2. **Present Value**: The present value of wages and the firm's match surplus are identical across both specifications, leading to the same equilibrium allocation.\n\nCANDIDATE_ANSWER:\nStep 1: Observe that the given equations specify surplus division between worker and firm. The worker’s surplus (Eₐ(p) − Uₐ) equals φSₐ(p), while the firm’s surplus Jₐ(p) equals (1 − φ)Sₐ(p).\n\nStep 2: Recognize that in standard Nash bargaining (Nash product (26.6.14)), the surplus split satisfies (Worker Surplus)^φ × (Firm Surplus)^{1−φ}, where φ is worker bargaining power. The first-order condition yields Worker Surplus/Firm Surplus = φ/(1−φ).\n\nStep 3: Substitute the given equations into the Nash ratio: (φSₐ(p)) / ((1−φ)Sₐ(p)) = φ/(1−φ). This matches the Nash bargaining split ratio.\n\nStep 4: Conclude that Mortensen-Pissarides’ formulation explicitly enforces the Nash bargaining solution by allocating surpluses proportionally to φ and (1−φ), making it equivalent to the Nash product maximization outcome.\n\nQID: textbook-123-0-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-0-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly demonstrates the equivalence between Mortensen-Pissarides' bargaining specification and the standard Nash product by showing how the surplus division aligns with Nash bargaining principles, matching the gold answer's assertion of identical present values and equilibrium allocation.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-0-2-3", "category": "CORRECT", "explanation": "The candidate's answer correctly demonstrates the equivalence between Mortensen-Pissarides' bargaining specification and the standard Nash product by showing how the surplus division aligns with Nash bargaining principles, matching the gold answer's assertion of identical present values and equilibrium allocation."}, "llm_echoed_qid": "textbook-123-0-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly demonstrates the equivalence between Mortensen-Pissarides' bargaining specification and the standard Nash product by showing how the surplus division aligns with Nash bargaining principles, matching the gold answer's assertion of identical present values and equilibrium allocation."}
{"script_processing_qid": "textbook-118-8-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain why asymptotic identification depends on the true parameter values of the DGP.\n\nGOLD_ANSWER:\nAsymptotic identification requires $\\text{plim } n^{-1}X_i^{\\top}P\\_{W}X_i$ to be nonsingular. This depends on:\n1. **Reduced-form parameters**: The rank of $\\pmb{\\Pi}_{21}$ (linking instruments to endogenous variables) is determined by the true structural parameters.\n2. **Instrument relevance**: If the true $\\pmb{\\Pi}_{21}$ is near-zero (weak instruments), the plim matrix may be near-singular, leading to weak identification.\n3. **Non-testability**: The rank condition cannot be tested directly because $\\pmb{\\Pi}_{21}$ is unobserved and estimated with error.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that asymptotic identification requires the probability limit (plim) of the normalized matrix n^{-1}X_i^{\\top}P_WX_i to be nonsingular.\\nStep 2: The structure of X_i (which includes endogenous variables) depends on the true parameter values of the DGP through the system's structural equations.\\nStep 3: If true parameter values cause certain variables to be linearly dependent on others in the population (e.g., coefficients that nullify instrument relevance), the plim matrix becomes singular.\\nStep 4: Thus, whether the plim matrix retains full rank asymptotically is fundamentally determined by how the true parameters govern relationships between variables and instruments in the DGP.\n\nQID: textbook-118-8-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-8-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies that asymptotic identification depends on the nonsingularity of the plim matrix and explains how this is influenced by the true parameter values of the DGP, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-8-1-2", "category": "CORRECT", "explanation": "The candidate answer correctly identifies that asymptotic identification depends on the nonsingularity of the plim matrix and explains how this is influenced by the true parameter values of the DGP, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-118-8-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies that asymptotic identification depends on the nonsingularity of the plim matrix and explains how this is influenced by the true parameter values of the DGP, aligning well with the gold answer."}
{"script_processing_qid": "textbook-87-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Prove that the allocation rule $c_j = \\phi_j(c;\\omega)$ is increasing in aggregate consumption $c$.\n\nGOLD_ANSWER:\n1. **First-Order Condition**: From $\\omega_j c_j^{-\\rho_j} = \\mu$, express $c_j = (\\mu/\\omega_j)^{-1/\\rho_j}$.\n2. **Aggregate Constraint**: $c = \\frac{1}{J}\\sum_{j=1}^J (\\mu/\\omega_j)^{-1/\\rho_j}$ implies $\\mu$ decreases in $c$.\n3. **Monotonicity**: Since $\\partial c_j/\\partial \\mu = -\\frac{1}{\\rho_j} (\\mu/\\omega_j)^{-1/\\rho_j - 1} (1/\\omega_j) < 0$ and $\\partial \\mu/\\partial c < 0$, by chain rule $\\partial c_j/\\partial c > 0$.\n4. **Concavity**: The mongrel utility $U(c;\\omega)$ inherits concavity from individual utilities, ensuring well-behaved allocation rules.\n\nCANDIDATE_ANSWER:\nStep 1: The question requires demonstrating that the allocation rule $c_j = \\phi_j(c;\\omega)$ is increasing in aggregate consumption $c$. However, the provided context only includes:\n\n- The Epstein-Zin recursive utility specification (with parameters $\\rho$ for intertemporal substitution and $\\alpha$ for risk aversion)\n- A discussion of estimation issues when erroneously assuming $\\alpha = \\rho$\n\nStep 2: The context does not explicitly define the allocation rule $\\phi_j(c;\\omega)$ or provide Euler equations/optimality conditions from which such a rule would be derived. Key elements required for the proof (e.g., budget constraints, market clearing conditions, parametric form of $\\phi_j$, or equilibrium relationships between individual/aggregate consumption) are absent.\n\nStep 3: To prove monotonicity of an allocation rule, one would typically need to analyze first-order conditions from the utility maximization problem and show $\\partial \\phi_j/\\partial c > 0$. However, none of these components appear in the provided excerpts.\n\nConclusion: The provided context lacks sufficient detail about the allocation rule's derivation or the economic environment (e.g., production function, endowment processes, or risk-sharing mechanisms) to construct a proof of monotonicity.\n\nQID: textbook-87-0-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-87-0-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide sufficient information to derive the allocation rule or prove its properties.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-87-0-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide sufficient information to derive the allocation rule or prove its properties."}, "llm_echoed_qid": "textbook-87-0-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide sufficient information to derive the allocation rule or prove its properties."}
{"script_processing_qid": "textbook-104-11-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Under what conditions can a compact state space $\\pmb{X}\\subset\\pmb{R}_{+}^{l}$ ensure boundedness of $F$?\n\nGOLD_ANSWER:\n1. **Compactness**: $X$ must be closed and bounded.\n2. **Continuity**: $\\Gamma$ must be compact-valued and continuous, and $F$ must be continuous.\n3. **Implication**: By the Extreme Value Theorem, $F$ attains finite maxima/minima on $A$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key conditions stated in the context. The text specifies that boundedness of F on a compact state space requires: (1) Γ (the feasibility correspondence) to be compact-valued and continuous, and (2) F to be continuous.\n\nStep 2: Apply the Heine-Cantor theorem. Since continuous functions on compact sets are bounded, the compactness of X (restricted state space) combined with the continuity of F ensures F is bounded.\n\nStep 3: Synthesize requirements. The boundedness of F under a compact X⊂ℝ₊^l is guaranteed when: (i) X is compact, (ii) Γ is compact-valued and continuous, and (iii) F is continuous.\n\nQID: textbook-104-11-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-104-11-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies and synthesizes the key conditions (compactness of X, continuity of Γ and F) and correctly applies the Heine-Cantor theorem to justify boundedness of F, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-104-11-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately identifies and synthesizes the key conditions (compactness of X, continuity of Γ and F) and correctly applies the Heine-Cantor theorem to justify boundedness of F, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-104-11-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies and synthesizes the key conditions (compactness of X, continuity of Γ and F) and correctly applies the Heine-Cantor theorem to justify boundedness of F, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-110-4-3-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the strategic form and extensive form representations of a game, highlighting their respective advantages and limitations.\n\nGOLD_ANSWER:\n1.  **Strategic Form**: Compact representation using payoff matrices, suitable for simultaneous-move games but less intuitive for sequencing.\n2.  **Extensive Form**: Tree representation captures sequencing and information structure, but can be complex for large games.\n3.  **Advantages**: Extensive form is better for dynamic games, while strategic form is simpler for static games.\n\nCANDIDATE_ANSWER:\nStep 1: Understand core representations\n- Strategic form (normal form): Represents games as payoff matrices/lists showing simultaneous strategy choices and outcomes.\n- Extensive form: Uses game trees to explicitly model sequential decision-making, information sets, and timing.\n\nStep 2: Compare advantages\nStrategic form advantages:\n1. Simplicity for small games with limited strategies\n2. Direct identification of Nash equilibria through payoff comparison\n3. Effective for simultaneous-move analysis\n\nExtensive form advantages:\n1. Explicit sequencing of moves (temporal dimension)\n2. Models information asymmetry through information sets\n3. Enables analysis of credible threats/reactions via backward induction\n4. Captures equilibrium path dynamics\n\nStep 3: Compare limitations\nStrategic form limitations:\n1. Lacks temporal structure (flattens sequential decisions)\n2. Cannot analyze subgame perfection\n3. Ignores information states during play\n\nExtensive form limitations:\n1. Complexity increases exponentially with decision nodes\n2. Less intuitive for simultaneous-move scenarios\n3. Requires more advanced solution concepts (e.g., subgame-perfect equilibrium)\n\nStep 4: Contextualize with provided material\nThe analysis emphasizes that extensive form becomes essential when examining:\n- Sequencing effects (critical for equilibrium path analysis)\n- Information flow between decision points\n- Credible threats/reactions (impossible to model properly in pure strategic form)\n\nConclusion: Strategic form offers computational simplicity for static games, while extensive form enables richer dynamic analysis at the cost of increased complexity.\n\nQID: textbook-110-4-3-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-110-4-3-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly compares the strategic form and extensive form representations, highlighting their respective advantages and limitations in a structured manner. It aligns well with the gold answer, providing additional depth and detail while maintaining accuracy.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-4-3-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly compares the strategic form and extensive form representations, highlighting their respective advantages and limitations in a structured manner. It aligns well with the gold answer, providing additional depth and detail while maintaining accuracy."}, "llm_echoed_qid": "textbook-110-4-3-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly compares the strategic form and extensive form representations, highlighting their respective advantages and limitations in a structured manner. It aligns well with the gold answer, providing additional depth and detail while maintaining accuracy."}
{"script_processing_qid": "textbook-24-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Critically evaluate Baldwin et al.'s five-item list of policy traps in the context of agglomeration economies. How do these traps challenge conventional policy evaluation methods?\n\nGOLD_ANSWER:\n3.  **Policy Traps**:\n   - **Threshold Effects**: Small policies may have discontinuous impacts, violating marginal analysis assumptions.\n   - **Non-Linearity**: Policy effects are not proportional, complicating cost-benefit analysis.\n   - **Trade Freeness**: Policy effectiveness depends on pre-existing trade integration levels.\n   - **Selection Effects**: Multiple equilibria mean policies may select rather than shift outcomes.\n   - **Coordination Effects**: Expectations-driven equilibria require belief-based interventions.\n\n   **Challenges**: Conventional methods (e.g., linear regression, marginal tax analysis) fail to account for these non-standard dynamics.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Baldwin et al.'s five policy traps\n1. Threshold effects, discontinuities, and hysteresis\n2. General non-linear effects\n3. Trade freeness matters\n4. Selection effects\n5. Coordination effects\n\nStep 2: Analyze challenges to conventional policy evaluation methods\n- Threshold effects challenge linear models: Small policy changes may trigger abrupt agglomeration shifts (catastrophic agglomeration), violating continuity assumptions in traditional cost-benefit analysis.\n- Non-linear effects invalidate marginal analysis: Outcomes like hump-shaped agglomeration rents mean proportional policy responses cannot be assumed.\n- Trade freeness parameter introduces context-dependence: Policy impacts vary non-monotonically with trade costs (e.g., policies effective only at intermediate trade cost levels), conflicting with universal policy prescriptions.\n- Selection effects create asymmetric outcomes: Agglomeration benefits may accrue disproportionately to specific regions/actors (endogenous asymmetry), undermining uniform regional development approaches.\n- Coordination effects require multi-agent alignment: Self-fulfilling expectations and circular causality mean policy success depends on simultaneous coordination, challenging static evaluation frameworks.\n\nStep 3: Synthesize the critique\nThese traps collectively invalidate three conventional pillars:\n1. Marginalist thinking (discontinuities/non-linearities)\n2. Stable equilibrium assumptions (hysteresis/overlap)\n3. Uniform policy prescriptions (selection/trade-cost specificity)\nThey demand dynamic, context-sensitive models that account for multiple equilibria, path dependence, and strategic complementarities in policy design.\n\nQID: textbook-24-0-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-24-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies Baldwin et al.'s five policy traps and thoroughly analyzes how each challenges conventional policy evaluation methods. The synthesis effectively critiques the conventional pillars and calls for dynamic, context-sensitive models, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-24-0-0-2", "category": "CORRECT", "explanation": "The candidate answer accurately identifies Baldwin et al.'s five policy traps and thoroughly analyzes how each challenges conventional policy evaluation methods. The synthesis effectively critiques the conventional pillars and calls for dynamic, context-sensitive models, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-24-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies Baldwin et al.'s five policy traps and thoroughly analyzes how each challenges conventional policy evaluation methods. The synthesis effectively critiques the conventional pillars and calls for dynamic, context-sensitive models, aligning well with the gold answer."}
{"script_processing_qid": "textbook-122-20-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the interpretation of the coefficients in a Poisson regression.\n\nGOLD_ANSWER:\n4. **Coefficient Interpretation**:\n   - **Incidence Rate Ratio (IRR)**: For a one-unit change in $x_j$, the expected count changes by a factor of $\\exp(\\beta_j)$.\n   - **Marginal Effect**: The marginal effect of $x_j$ on the expected count is $\\beta_j \\exp(x \\beta)$.\n\nCANDIDATE_ANSWER:\nIn Poisson regression, coefficients represent the log of the expected count (rate) change associated with a one-unit increase in the predictor variable. Interpretation proceeds as follows:\n\nStep 1: Coefficients are estimated under the model structure log(μ) = β₀ + β₁x₁ + ... + βₖxₖ, where μ is the expected count.\n\nStep 2: For a continuous predictor xⱼ, a coefficient βⱼ implies that a one-unit increase in xⱼ is associated with a multiplicative change of exp(βⱼ) in the expected count μ, holding other variables constant.\n\nStep 3: For categorical predictors, exp(βⱼ) represents the ratio of expected counts between the category and the reference group.\n\nStep 4: The context emphasizes that Poisson regression accounts for discreteness of count outcomes (e.g., extramarital affairs), unlike Tobit. Coefficients should not be interpreted as marginal effects on a latent continuous variable but rather as direct effects on the log-scale expected count.\n\nExample: If βⱼ = 0.3, exp(0.3) ≈ 1.35 implies a 35% increase in the expected count per unit increase in xⱼ.\n\nQID: textbook-122-20-2-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-20-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the interpretation of coefficients in a Poisson regression, including the use of the exponential function to interpret multiplicative changes in the expected count. It also correctly distinguishes between continuous and categorical predictors and provides a clear example. This aligns well with the gold answer, which discusses the Incidence Rate Ratio (IRR) and the multiplicative change in expected counts.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-20-2-3", "category": "CORRECT", "explanation": "The candidate answer accurately explains the interpretation of coefficients in a Poisson regression, including the use of the exponential function to interpret multiplicative changes in the expected count. It also correctly distinguishes between continuous and categorical predictors and provides a clear example. This aligns well with the gold answer, which discusses the Incidence Rate Ratio (IRR) and the multiplicative change in expected counts."}, "llm_echoed_qid": "textbook-122-20-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the interpretation of coefficients in a Poisson regression, including the use of the exponential function to interpret multiplicative changes in the expected count. It also correctly distinguishes between continuous and categorical predictors and provides a clear example. This aligns well with the gold answer, which discusses the Incidence Rate Ratio (IRR) and the multiplicative change in expected counts."}
{"script_processing_qid": "textbook-39-0-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the role of aggregation methods (log of average vs. average of log) in reconciling micro and macro evidence on consumption Euler equations.\n\nGOLD_ANSWER:\n1. **Log of Average**: Mimics National Accounts aggregation; can induce spurious excess sensitivity if inequality (variance of log consumption) correlates with instruments.\n2. **Average of Log**: Consistent with theory; eliminates aggregation bias if $\\Delta \\log c_{i,t}$ is i.i.d. across households.\n3. **Empirical Findings**: Attanasio and Weber (1993) show the former replicates macro excess sensitivity, while the latter aligns with theoretical predictions.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the aggregation discrepancy\n- The difference between log(average consumption) and average(log consumption) emerges from Jensen’s inequality due to the concave nature of the log function. This gap represents consumption inequality (cross-sectional variance).\n\nStep 2: Link to Euler equation estimation\n- Euler equations derived from micro data typically use average(log consumption growth), while macro models often implicitly use log(average consumption growth). The inequality term (log(E[c]) - E[log(c])), which varies over the business cycle, becomes an omitted variable in macro-level estimations.\n\nStep 3: Impact on results\n- This inequality measure is correlated with macroeconomic instruments (e.g., interest rates or income growth). When unaccounted for, it biases macro estimates of Euler equation parameters, creating apparent excess sensitivity that micro studies (using proper aggregation) may attribute to liquidity constraints (as in Zeldes 1989a).\n\nStep 4: Reconciling evidence\n- Attanasio and Weber (1995) show that using average cohort data (partial aggregation) preserves micro-level relationships better than full aggregation. Proper aggregation methods reduce the inequality-related bias, aligning macro estimates with micro evidence of non-separability or liquidity constraints.\n\nStep 5: Key implication\n- The choice of aggregation method determines whether inequality variations are absorbed into the error term (causing macro-level excess sensitivity) or explicitly modeled, explaining why micro and macro studies might reach different conclusions about consumption behavior.\n\nQID: textbook-39-0-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-39-0-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the role of aggregation methods in reconciling micro and macro evidence on consumption Euler equations, aligning with the gold answer's points on log of average vs. average of log, empirical findings, and the impact on results.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-39-0-1-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the role of aggregation methods in reconciling micro and macro evidence on consumption Euler equations, aligning with the gold answer's points on log of average vs. average of log, empirical findings, and the impact on results."}, "llm_echoed_qid": "textbook-39-0-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the role of aggregation methods in reconciling micro and macro evidence on consumption Euler equations, aligning with the gold answer's points on log of average vs. average of log, empirical findings, and the impact on results."}
{"script_processing_qid": "textbook-21-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What are the implications of parallel, straight expansion paths for the aggregate demand function?\n\nGOLD_ANSWER:\nParallel, straight expansion paths imply that the aggregate demand function can be represented as a linear function of aggregate wealth, holding prices constant. This simplifies the analysis of market demand and allows for the use of representative consumer models under certain conditions.\n\nCANDIDATE_ANSWER:\nStep 1: Observe that parallel, straight expansion paths imply identical wealth effects across all consumers. This follows from the derivation $\\nabla_{\\bf w_{i}}{x_{i}}(\\Sigma,{\\bf w_{i}})=-\\frac{1}{b(p)}\\nabla_{\\bf p}b(\\Sigma)$ being identical for all $i$ in the context.\\n\\nStep 2: When expansion paths are parallel and straight, individual demand functions are affine in wealth ($w_i$) with slopes that do not vary across consumers.\\n\\nStep 3: For aggregate demand, the homogeneity of wealth effects across consumers implies that redistribution of wealth among consumers does not affect total demand.\\n\\nStep 4: Consequently, the aggregate demand function depends only on total wealth ($\\sum_i w_i$) rather than its distribution across individuals. This property simplifies welfare analysis and market equilibrium characterization.\n\nQID: textbook-21-0-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-21-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the implications of parallel, straight expansion paths for the aggregate demand function, including the simplification of analysis and the independence of aggregate demand from wealth distribution, which aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-21-0-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the implications of parallel, straight expansion paths for the aggregate demand function, including the simplification of analysis and the independence of aggregate demand from wealth distribution, which aligns with the gold answer."}, "llm_echoed_qid": "textbook-21-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the implications of parallel, straight expansion paths for the aggregate demand function, including the simplification of analysis and the independence of aggregate demand from wealth distribution, which aligns with the gold answer."}
{"script_processing_qid": "textbook-16-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Show that the cost function $c(w, y) = y^{1/2}(w_1 w_2)^{3/4}$ is homogeneous of degree $3/2$ and verify its monotonicity in input prices.\n\nGOLD_ANSWER:\n1. **Homogeneity**: $c(t w, y) = y^{1/2}(t w_1 t w_2)^{3/4} = t^{3/2} y^{1/2}(w_1 w_2)^{3/4} = t^{3/2} c(w, y)$. \\n2. **Monotonicity**: $\\frac{\\partial c}{\\partial w_1} = \\frac{3}{4} y^{1/2} w_1^{-1/4} w_2^{3/4} > 0$, $\\frac{\\partial c}{\\partial w_2} = \\frac{3}{4} y^{1/2} w_1^{3/4} w_2^{-1/4} > 0$.\n\nCANDIDATE_ANSWER:\nStep 1: Verify homogeneity of degree 3/2.\nSubstitute scaled prices (tw₁, tw₂) into c(w, y):\nc(tw, y) = y^{1/2} ⋅ (tw₁ ⋅ tw₂)^{3/4}\n          = y^{1/2} ⋅ t^{3/2}(w₁w₂)^{3/4}\n          = t^{3/2} ⋅ [y^{1/2}(w₁w₂)^{3/4}]\n          = t^{3/2}c(w, y)\nThus homogeneous of degree 3/2.\n\nStep 2: Verify monotonicity in input prices.\nCompute partial derivatives with respect to w₁ and w₂:\n∂c/∂w₁ = (3/4)y^{1/2}(w₂)^{3/4}w₁^{-1/4} > 0 ∀ w₁ > 0\n∂c/∂w₂ = (3/4)y^{1/2}(w₁)^{3/4}w₂^{-1/4} > 0 ∀ w₂ > 0\nSince both partial derivatives are strictly positive for all positive input prices, the cost function is strictly increasing in both w₁ and w₂, satisfying monotonicity.\n\nQID: textbook-16-0-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-16-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly verified the homogeneity of degree 3/2 and demonstrated monotonicity by showing positive partial derivatives for all positive input prices, matching the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-16-0-1-0", "category": "CORRECT", "explanation": "The candidate correctly verified the homogeneity of degree 3/2 and demonstrated monotonicity by showing positive partial derivatives for all positive input prices, matching the gold answer."}, "llm_echoed_qid": "textbook-16-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly verified the homogeneity of degree 3/2 and demonstrated monotonicity by showing positive partial derivatives for all positive input prices, matching the gold answer."}
{"script_processing_qid": "textbook-119-30-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the fixed effects model for panel data and explain how it accounts for unobserved individual-specific effects.\n\nGOLD_ANSWER:\n1. **Model Specification**: $$Y_{it} = \\alpha_i + \\beta X_{it} + \\epsilon_{it}$$\\n2. **Fixed Effects Transformation**: Subtract individual means to eliminate $\\alpha_i$: $$Y_{it} - \\bar{Y}_i = \\beta (X_{it} - \\bar{X}_i) + (\\epsilon_{it} - \\bar{\\epsilon}_i)$$\\n3. **Estimation**: Use OLS on the transformed equation to estimate $\\beta$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the basic panel data model formulation:\nY_it = α_i + βX_it + u_it\nWhere:\n- Y_it = dependent variable for individual i at time t\n- α_i = unobserved individual-specific effect (constant over time)\n- X_it = independent variables\n- u_it = idiosyncratic error term\n\nStep 2: Recognize that α_i captures all time-invariant individual characteristics. In the eggs example, this could include permanent state-specific factors like climate or regulatory environment that affect egg production/prices but aren't explicitly measured.\n\nStep 3: Apply the fixed effects transformation by time-demeaning the variables:\n1. Compute individual-specific means: Ȳ_i = (1/T)ΣY_it, X̄_i = (1/T)ΣX_it\n2. Subtract means from observed values: (Y_it - Ȳ_i) = β(X_it - X̄_i) + (u_it - ū_i)\n\nStep 4: The transformed equation eliminates α_i through subtraction:\n- Original equation: Y_it = α_i + βX_it + u_it\n- Mean equation: Ȳ_i = α_i + βX̄_i + ū_i\n- Subtracting removes α_i: (Y_it - Ȳ_i) = β(X_it - X̄_i) + (u_it - ū_i)\n\nStep 5: Estimation using OLS on the transformed equation produces consistent estimates of β, as the fixed effects approach accounts for all time-invariant individual characteristics through the demeaning process. This prevents bias from omitted variables that are constant over time but vary across individuals.\n\nQID: textbook-119-30-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-30-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the fixed effects model for panel data, explains the transformation process to eliminate unobserved individual-specific effects, and describes the estimation using OLS. The steps and reasoning align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-30-1-1", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the fixed effects model for panel data, explains the transformation process to eliminate unobserved individual-specific effects, and describes the estimation using OLS. The steps and reasoning align with the gold answer."}, "llm_echoed_qid": "textbook-119-30-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the fixed effects model for panel data, explains the transformation process to eliminate unobserved individual-specific effects, and describes the estimation using OLS. The steps and reasoning align with the gold answer."}
{"script_processing_qid": "textbook-111-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the steady-state unemployment rate $\\frac{U}{L} = \\frac{s}{s + f}$ from the steady-state condition $f U = s E$ and the labor force identity $L = E + U$.\n\nGOLD_ANSWER:\n1.  **Steady-State Condition**: $f U = s E$.\n2.  **Labor Force Identity**: $E = L - U$.\n3.  **Substitute**: $f U = s(L - U)$.\n4.  **Divide by $L$**: $f \\frac{U}{L} = s\\left(1 - \\frac{U}{L}\\right)$.\n5.  **Solve for $\\frac{U}{L}$**: $\\frac{U}{L} = \\frac{s}{s + f}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the labor force identity L = E + U. Rearrange to express employment as E = L - U.\n\nStep 2: Substitute E = L - U into the steady-state condition fU = sE:\n   fU = s(L - U)\n\nStep 3: Expand the equation:\n   fU = sL - sU\n\nStep 4: Collect all U terms on one side:\n   fU + sU = sL\n\nStep 5: Factor out U from the left side:\n   U(f + s) = sL\n\nStep 6: Divide both sides by L(f + s) to solve for U/L:\n   U/L = s/(s + f)\n\nThis shows the steady-state unemployment rate equals the job separation rate divided by the sum of separation and finding rates.\n\nQID: textbook-111-0-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the steady-state unemployment rate, matching the gold answer in both reasoning and final result.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-0-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the steady-state unemployment rate, matching the gold answer in both reasoning and final result."}, "llm_echoed_qid": "textbook-111-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the steady-state unemployment rate, matching the gold answer in both reasoning and final result."}
{"script_processing_qid": "textbook-118-4-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the first-order conditions for the FIML estimator, starting from the loglikelihood function (12.107).\n\nGOLD_ANSWER:\n1. **Loglikelihood Function**: The loglikelihood function is $$ -\\frac{g n}{2}\\log2\\pi-\\frac{n}{2}\\log|\\Omega|-\\frac{1}{2}(y_{\\bullet}-W_{\\bullet}\\pi_{\\bullet})^{\\top}(\\Omega^{-1}\\otimes\\mathbf{I}_{n})(y_{\\bullet}-W_{\\bullet}\\pi_{\\bullet}). $$\n2. **First-Order Conditions**: Differentiate the loglikelihood with respect to the unrestricted elements of $\\pmb{\\Gamma}$ and set the derivatives to zero. This involves using the chain rule and the properties of Kronecker products, leading to the conditions in (12.83).\n\nCANDIDATE_ANSWER:\nStep 1: Start with the concentrated loglikelihood function:\n\n$$\\ell(\\theta) = \\sum_{t=1}^{n}\\log|\\operatorname{det} J_{t}| - \\frac{n}{2}\\log\\Big|\\frac{1}{n}\\sum_{t=1}^{n}h_{t}^{\\top}h_{t}\\Big| + \\text{const.}$$\n\nStep 2: Differentiate the Jacobian term $\\sum_{t=1}^n \\log|\\operatorname{det} J_t|$ w.r.t. $\\theta$:\nUsing matrix calculus: $\\frac{\\partial}{\\partial\\theta}\\log|\\operatorname{det} J_t| = \\operatorname{tr}\\left(J_t^{-1} \\frac{\\partial J_t}{\\partial\\theta}\\right)$.\nTotal derivative: $\\sum_{t=1}^n \\operatorname{tr}\\left(J_t^{-1} \\frac{\\partial J_t}{\\partial\\theta}\\right)$.\n\nStep 3: Differentiate the covariance term $-\\frac{n}{2}\\log|\\frac{1}{n}\\sum_{t=1}^n h_t^{\\top}h_t|$ w.r.t. $\\theta$:\nLet $S(\\theta) = \\frac{1}{n}\\sum_{t=1}^n h_t^{\\top}h_t$. Using matrix calculus:\n$\\frac{\\partial}{\\partial\\theta}\\log|S| = \\operatorname{tr}\\left(S^{-1}\\frac{\\partial S}{\\partial\\theta}\\right)$.\nChain rule gives: $\\frac{\\partial S}{\\partial\\theta} = \\frac{2}{n}\\sum_{t=1}^n h_t^{\\top}\\frac{\\partial h_t}{\\partial\\theta}$.\nTotal derivative: $-\\operatorname{tr}\\left(S^{-1}\\sum_{t=1}^n h_t^{\\top}\\frac{\\partial h_t}{\\partial\\theta}\\right)$.\n\nStep 4: Combine derivatives to form FIML first-order conditions:\n$$\\sum_{t=1}^n \\operatorname{tr}\\left(J_t^{-1} \\frac{\\partial J_t}{\\partial\\theta}\\right) - \\operatorname{tr}\\left(\\left(\\frac{1}{n}\\sum_{t=1}^n h_t^{\\top}h_t\\right)^{-1}\\sum_{t=1}^n h_t^{\\top}\\frac{\\partial h_t}{\\partial\\theta}\\right) = 0.$$\n\nQID: textbook-118-4-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-4-0-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate answer starts with a different loglikelihood function than the one provided in the gold answer, and the derivation steps do not align with the gold answer's approach. The candidate's focus on the Jacobian and covariance terms is not relevant to the gold answer's context, which involves differentiating with respect to the elements of $\\pmb{\\Gamma}$ using the Kronecker product properties.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-4-0-2", "category": "INCORRECT", "explanation": "The candidate answer starts with a different loglikelihood function than the one provided in the gold answer, and the derivation steps do not align with the gold answer's approach. The candidate's focus on the Jacobian and covariance terms is not relevant to the gold answer's context, which involves differentiating with respect to the elements of $\\pmb{\\Gamma}$ using the Kronecker product properties."}, "llm_echoed_qid": "textbook-118-4-0-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate answer starts with a different loglikelihood function than the one provided in the gold answer, and the derivation steps do not align with the gold answer's approach. The candidate's focus on the Jacobian and covariance terms is not relevant to the gold answer's context, which involves differentiating with respect to the elements of $\\pmb{\\Gamma}$ using the Kronecker product properties."}
{"script_processing_qid": "textbook-119-31-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. What is the Frisch–Waugh theorem, and how does it relate to the use of dummy variables for deseasonalizing data?\n\nGOLD_ANSWER:\nThe **Frisch–Waugh theorem** states that in a linear regression model, the coefficient of a particular regressor can be obtained by first regressing the dependent variable and the regressor of interest on the other regressors and then regressing the residuals from these two regressions on each other.\n\nIn the context of dummy variables for deseasonalizing:\n1. **Step 1**: Regress $Y$ on seasonal dummies and save residuals ($S_1$), which represent deseasonalized $Y$.\n2. **Step 2**: Regress $X$ on the same dummies and save residuals ($S_2$), which represent deseasonalized $X$.\n3. **Step 3**: Regress $S_1$ on $S_2$. The slope coefficient from this regression is identical to the coefficient of $X$ in the original model with dummies. This shows that dummy variables remove seasonality from both $Y$ and $X$.\n\nCANDIDATE_ANSWER:\nStep 1: The Frisch-Waugh theorem states that in a linear regression model, the coefficient for a specific variable can be obtained by first regressing both the dependent variable and the target explanatory variable on all other covariates, then regressing the residuals of the dependent variable on the residuals of the target explanatory variable.\n\nStep 2: In the context of dummy variables for deseasonalizing, the theorem implies that seasonal effects (captured by seasonal dummies) can be partialled out by regressing both the dependent variable (e.g., sales commission) and the independent variable (e.g., sales volume) on the seasonal dummy variables first. The subsequent regression of the 'cleaned' dependent variable residuals on 'cleaned' independent variable residuals would yield the same coefficient as including all variables in a single regression.\n\nStep 3: In the provided piecewise regression model with a knot at X*, the dummy variable D_i effectively splits the data into two segments. The Frisch-Waugh logic applies here because the interaction term β₂(X_i - X*)D_i operates similarly to a residualized component, isolating the differential effect of sales volume beyond the threshold X* after accounting for the base relationship (β₁X_i). This allows separate slope estimation for each segment while maintaining a unified framework, analogous to how seasonal dummies deseasonalize data by removing periodic effects before analyzing core relationships.\n\nQID: textbook-119-31-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-31-0-3\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly explains the Frisch–Waugh theorem and its application to deseasonalizing data with dummy variables (Steps 1-2). However, Step 3 incorrectly extends the theorem to a piecewise regression context, which is unrelated to the original question about deseasonalization, leading to partial correctness.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-31-0-3", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly explains the Frisch–Waugh theorem and its application to deseasonalizing data with dummy variables (Steps 1-2). However, Step 3 incorrectly extends the theorem to a piecewise regression context, which is unrelated to the original question about deseasonalization, leading to partial correctness."}, "llm_echoed_qid": "textbook-119-31-0-3", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly explains the Frisch–Waugh theorem and its application to deseasonalizing data with dummy variables (Steps 1-2). However, Step 3 incorrectly extends the theorem to a piecewise regression context, which is unrelated to the original question about deseasonalization, leading to partial correctness."}
{"script_processing_qid": "textbook-115-8-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain how Assumption OLS.3 simplifies the asymptotic variance of the OLS estimator.\n\nGOLD_ANSWER:\nAssumption OLS.3 implies $\\mathbf{B} = \\sigma^{2}\\mathbf{A}$, so the asymptotic variance simplifies to $\\sigma^{2}\\mathbf{A}^{-1}$. This allows the use of the usual OLS standard errors and test statistics, which are valid under homoskedasticity.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that under general conditions (without Assumption OLS.3), the asymptotic variance of the OLS estimator has the 'sandwich form' A⁻¹BA⁻¹, where B = E(u²x'x).\nStep 2: Assumption OLS.3 states E(u²x'x) = σ²E(x'x). This implies B = σ²A because A = E(x'x) by definition.\nStep 3: Substitute B = σ²A into the sandwich formula: A⁻¹(σ²A)A⁻¹ = σ²A⁻¹AA⁻¹ = σ²A⁻¹.\nStep 4: This eliminates the sandwich structure and reduces the asymptotic variance to the simpler form σ²A⁻¹, avoiding the need to estimate B separately. The simplification occurs because OLS.3 forces the middle matrix B to be proportional to A through σ².\n\nQID: textbook-115-8-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-8-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains how Assumption OLS.3 simplifies the asymptotic variance of the OLS estimator by showing the substitution of B = σ²A into the sandwich formula, resulting in σ²A⁻¹. This matches the gold answer and provides a clear step-by-step reasoning.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-8-1-1", "category": "CORRECT", "explanation": "The candidate answer correctly explains how Assumption OLS.3 simplifies the asymptotic variance of the OLS estimator by showing the substitution of B = σ²A into the sandwich formula, resulting in σ²A⁻¹. This matches the gold answer and provides a clear step-by-step reasoning."}, "llm_echoed_qid": "textbook-115-8-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains how Assumption OLS.3 simplifies the asymptotic variance of the OLS estimator by showing the substitution of B = σ²A into the sandwich formula, resulting in σ²A⁻¹. This matches the gold answer and provides a clear step-by-step reasoning."}
{"script_processing_qid": "textbook-112-9-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. (Qualitative) Explain Adam Smith's distinction between 'necessaries' and 'luxuries' in the context of taxation, and why this distinction matters for wage dynamics.\n\nGOLD_ANSWER:\n1.  **Necessaries**: Defined by both biological needs and social norms (e.g., linen shirts in 18th-century Europe). Taxes on necessaries raise production costs, leading to wage increases to maintain workers' subsistence.\n2.  **Luxuries**: Non-essential goods (e.g., tobacco, sugar). Taxes on luxuries do not affect wages because workers can abstain without violating social decency or survival needs.\n3.  **Economic Implications**: This dichotomy explains why taxes on necessaries distort labor markets more than taxes on luxuries, as the former directly alter the cost of labor reproduction.\n\nCANDIDATE_ANSWER:\nStep 1: Define 'necessaries' vs. 'luxuries' per Adam Smith. Necessaries include both commodities essential for sustaining life and those required by social custom for basic dignity, even among the poorest. Luxuries are non-essential goods beyond this threshold.\n\nStep 2: Explain the tax impact on necessaries. A tax on necessaries raises their price above the tax amount (due to dealer profit margins). Workers require higher wages to afford these now-costlier basics, effectively making the tax equivalent to a direct wage tax. Employers ultimately bear this cost through wage increases to maintain labor supply.\n\nStep 3: Contrast with taxes on luxuries. Price increases for taxed luxuries do not necessitate wage adjustments because workers are not compelled to purchase them for subsistence or social decency. Consumption patterns may shift, but wage levels remain unaffected.\n\nStep 4: Highlight implications for wage dynamics. Taxes on necessaries create a feedback loop: higher wages → higher production costs → higher prices for all manufactures → reduced consumption. Taxes on luxuries lack this systemic ripple effect, making the distinction critical for understanding how fiscal policy influences labor markets and broader economic activity.\n\nQID: textbook-112-9-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-9-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately defines 'necessaries' and 'luxuries', explains the tax impact on both, and highlights the implications for wage dynamics, aligning closely with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-9-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately defines 'necessaries' and 'luxuries', explains the tax impact on both, and highlights the implications for wage dynamics, aligning closely with the gold answer."}, "llm_echoed_qid": "textbook-112-9-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately defines 'necessaries' and 'luxuries', explains the tax impact on both, and highlights the implications for wage dynamics, aligning closely with the gold answer."}
{"script_processing_qid": "textbook-61-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the conditions under which an observable variable $x_i$ can serve as a valid instrument for an endogenous regressor $p_i$ in the demand equation. How does this relate to the exclusion restriction?\n\nGOLD_ANSWER:\n1. **Relevance**: $x_i$ must be correlated with $p_i$ ($\\operatorname{Cov}(x_i, p_i) \\neq 0$).\n2. **Exogeneity**: $x_i$ must be uncorrelated with the error term $u_i$ in the demand equation ($\\operatorname{Cov}(x_i, u_i) = 0$).\n3. **Exclusion Restriction**: $x_i$ must not appear in the demand equation (i.e., it affects $q_i$ only through $p_i$).\n4. **Implications**: The exclusion restriction ensures that $x_i$ does not directly influence the dependent variable, allowing it to isolate exogenous variation in $p_i$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the IV requirements for validity\nFor $x_i$ to be a valid instrument for $p_i$ in the demand equation, it must satisfy two key conditions:\n(a) Relevance: Cov$(x_i, p_i) \\neq 0$\n(b) Exogeneity: Cov$(x_i, u_i) = 0$\n\nStep 2: Relate to the exclusion restriction\nThe exclusion restriction requires that $x_i$ affects the outcome ($q_i^d$) only through its effect on the endogenous regressor $p_i$. This is mathematically equivalent to the exogeneity condition (Cov$(x_i, u_i) = 0$), where $u_i$ represents all unobserved demand factors.\n\nStep 3: Context-specific application\nIn the given simultaneous equations model:\n- $x_i$ must be a variable that shifts supply (appears in $q_i^s$ equation) but not demand\n- This ensures $x_i$ is correlated with $p_i$ (via supply equation) but uncorrelated with $u_i$ (demand shocks), satisfying both relevance and exclusion restrictions\n\nStep 4: Mathematical verification\nFrom the covariance expressions shown:\nIf $x_i$ is a supply shifter (affects $v_i$ but not $u_i$), then Cov$(x_i, u_i) = 0$ by construction, satisfying the exclusion restriction. Cov$(x_i, p_i) \\neq 0$ through the supply equation channel preserves relevance.\n\nQID: textbook-61-0-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-61-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the two key conditions for a valid instrument (relevance and exogeneity) and accurately relates these to the exclusion restriction. The explanation is consistent with the gold answer, including the mathematical verification and context-specific application.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-61-0-0-3", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the two key conditions for a valid instrument (relevance and exogeneity) and accurately relates these to the exclusion restriction. The explanation is consistent with the gold answer, including the mathematical verification and context-specific application."}, "llm_echoed_qid": "textbook-61-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the two key conditions for a valid instrument (relevance and exogeneity) and accurately relates these to the exclusion restriction. The explanation is consistent with the gold answer, including the mathematical verification and context-specific application."}
{"script_processing_qid": "textbook-107-4-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. For the production function $F=3P+2C$, derive the cost-minimizing combination of plastic ($P$) and clay ($C$) to produce 10,000 figurines, given prices $\\$4$ per pound of plastic and $\\$3$ per pound of clay.\n\nGOLD_ANSWER:\n1. **Isoquant**: $10,000 = 3P + 2C$.\n2. **Cost Function**: $TC = 4P + 3C$.\n3. **Substitute $C$**: $C = (10,000 - 3P)/2 \\Rightarrow TC = 4P + 3(10,000 - 3P)/2 = 4P + 15,000 - 4.5P = 15,000 - 0.5P$.\n4. **Minimize TC**: To minimize $TC$, set $P = 0$ (since $TC$ decreases with $P$).\n5. **Optimal Inputs**: $P = 0$, $C = 5,000$. Total cost = $15,000$.\n\nCANDIDATE_ANSWER:\nStep 1: Analyze the linear production function F = 3P + 2C. Since inputs are perfect substitutes, calculate cost-effectiveness per unit output.\nStep 2: Calculate output per dollar for each input:\n- Plastic: 3 units/\\$4 = 0.75 units/\\$\n- Clay: 2 units/\\$3 ≈ 0.6667 units/\\$\nStep 3: Plastic provides more output per dollar. Therefore, use only plastic.\nStep 4: Solve 3P = 10,000 → P = 10,000/3 ≈ 3,333.33 pounds\nStep 5: Calculate total cost: 3,333.33 × \\$4 ≈ \\$13,333.33\nConclusion: Cost-minimizing combination uses 3,333.33 lbs plastic and 0 lbs clay, total cost ≈ \\$13,333.33\n\nQID: textbook-107-4-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-4-0-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate incorrectly concluded that only plastic should be used because it provides more output per dollar. However, the gold answer shows that the cost-minimizing combination is to use only clay, as the total cost is lower ($15,000 vs. $13,333.33). The reasoning in the candidate's answer does not align with the correct optimization approach.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-4-0-2", "category": "INCORRECT", "explanation": "The candidate incorrectly concluded that only plastic should be used because it provides more output per dollar. However, the gold answer shows that the cost-minimizing combination is to use only clay, as the total cost is lower ($15,000 vs. $13,333.33). The reasoning in the candidate's answer does not align with the correct optimization approach."}, "llm_echoed_qid": "textbook-107-4-0-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate incorrectly concluded that only plastic should be used because it provides more output per dollar. However, the gold answer shows that the cost-minimizing combination is to use only clay, as the total cost is lower ($15,000 vs. $13,333.33). The reasoning in the candidate's answer does not align with the correct optimization approach."}
{"script_processing_qid": "textbook-72-0-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. How do quit rates interact with job creation and destruction to produce procyclical worker reallocation?\n\nGOLD_ANSWER:\n1. **Quit Rates**: Higher in booms ($s_h > s_l$) due to lower $R$ and higher $\\theta$.\n2. **Job Creation/Destruction**: Procyclical job creation and countercyclical job destruction.\n3. **Net Effect**: The sum of job creation and quits is procyclical, while separations (job destruction + quits) are acyclical, leading to procyclical worker reallocation.\n\nCANDIDATE_ANSWER:\nStep 1: The context highlights that aggregate market tightness (vacancies/unemployment) is counterintuitively lower in high-productivity states when shock frequency is sufficiently large. This occurs because firms invest more in job creation during low-productivity periods when hiring costs are relatively cheaper.\n\nStep 2: Quit rates are typically procyclical (rising in expansions). Increased quits during high-productivity states contribute to job destruction by increasing voluntary separations. However, the context implies that job creation responses may be muted in these states due to firms’ strategic timing of hiring investments during low-productivity phases.\n\nStep 3: The interaction emerges because procyclical quit rates amplify worker reallocation (job-to-job transitions) during expansions, while the dampened job creation response (due to firms’ preference for hiring in low-productivity periods) limits unemployment absorption. This combination increases the churn between jobs (reallocation) without proportionally reducing unemployment, aligning with the observed Beveridge curve dynamics and producing procyclical worker reallocation patterns.\n\nQID: textbook-72-0-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-72-0-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes the interaction between quit rates, job creation, and destruction, aligning with the gold answer's points on procyclical worker reallocation. The explanation includes the procyclical nature of quit rates and the strategic timing of job creation, which supports the net effect described in the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-72-0-1-2", "category": "CORRECT", "explanation": "The candidate's answer accurately describes the interaction between quit rates, job creation, and destruction, aligning with the gold answer's points on procyclical worker reallocation. The explanation includes the procyclical nature of quit rates and the strategic timing of job creation, which supports the net effect described in the gold answer."}, "llm_echoed_qid": "textbook-72-0-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes the interaction between quit rates, job creation, and destruction, aligning with the gold answer's points on procyclical worker reallocation. The explanation includes the procyclical nature of quit rates and the strategic timing of job creation, which supports the net effect described in the gold answer."}
{"script_processing_qid": "textbook-119-17-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Interpret the p-value in the context of hypothesis testing for the regression coefficient $\\beta_{2}$ in the model $Y = \\beta_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + u$.\n\nGOLD_ANSWER:\nThe p-value represents the probability of observing a test statistic as extreme as, or more extreme than, the one computed from the sample data, assuming the null hypothesis $H_{0}: \\beta_{2} = 0$ is true. A small p-value (e.g., < 0.05) indicates strong evidence against $H_{0}$, leading to its rejection.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the null hypothesis - The p-value for β₂ tests whether PGNP (X₂) has no linear effect on child mortality (Y) when female literacy rate (X₃) is held constant (H₀: β₂ = 0).\nStep 2: Understand the test setup - The alternative hypothesis is two-sided, requiring a two-tailed test under the normality assumption (u ~ N(0, σ²)).\nStep 3: Relate p-value to computed t-statistic - The reported t-value of 2.8187 exceeds the critical t-value of ~2 (at typical α = 0.05). This implies the p-value is less than 0.05.\nStep 4: Interpretation - The p-value represents the probability of observing a t-statistic as extreme as ±2.8187 (or more extreme) if PGNP truly had zero effect on child mortality after controlling for X₃.\nStep 5: Conclusion - Since the p-value < significance level (α = 0.05), we reject H₀. This provides statistically significant evidence that PGNP has a non-zero effect on child mortality when accounting for female literacy rate.\n\nQID: textbook-119-17-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-17-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly interprets the p-value in the context of hypothesis testing for the regression coefficient β₂. It accurately describes the null hypothesis, the test setup, and the relationship between the p-value and the t-statistic. The conclusion aligns with the gold answer, stating that a small p-value leads to the rejection of the null hypothesis.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-17-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly interprets the p-value in the context of hypothesis testing for the regression coefficient β₂. It accurately describes the null hypothesis, the test setup, and the relationship between the p-value and the t-statistic. The conclusion aligns with the gold answer, stating that a small p-value leads to the rejection of the null hypothesis."}, "llm_echoed_qid": "textbook-119-17-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly interprets the p-value in the context of hypothesis testing for the regression coefficient β₂. It accurately describes the null hypothesis, the test setup, and the relationship between the p-value and the t-statistic. The conclusion aligns with the gold answer, stating that a small p-value leads to the rejection of the null hypothesis."}
{"script_processing_qid": "textbook-122-13-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain the implications of heteroskedasticity on the variance of the OLS estimators.\n\nGOLD_ANSWER:\n1.  **Definition**: Heteroskedasticity occurs when $\\mathrm{Var}(u|x\\_{1},...,x\\_{k})$ is not constant across observations.\n2.  **Impact on Variance**: Under heteroskedasticity, the formula for $\\mathrm{Var}(\\hat{\\beta}\\_{j})$ is no longer valid, leading to biased standard errors and invalid hypothesis tests.\n3.  **Consequences**: This can result in misleading confidence intervals and incorrect inferences about the significance of the explanatory variables.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that under homoskedasticity (Assumption MLR.5), the OLS variance formula Var(β̂_j) = σ²/[SST_j(1-R_j²)] is valid because Var(u|x₁,...,x_k) = σ² is constant across observations.<br><br>Step 2: Under heteroskedasticity, Var(u|x₁,...,x_k) ≠ σ² and instead varies with the explanatory variables. This violates MLR.5.<br><br>Step 3: The original variance formula becomes incorrect because it assumes a single constant σ². The true variance of β̂_j under heteroskedasticity depends on the conditional variances Var(u|x₁,...,x_k) across observations.<br><br>Step 4: Consequences include: (a) OLS standard errors computed using the homoskedasticity-only formula are biased, (b) t-statistics and confidence intervals lose their validity, (c) OLS estimators remain unbiased but are no longer efficient (BLUE property fails).<br><br>Step 5: Specifically, the magnitude/direction of bias in standard errors depends on how the error variance relates to predictors. Common patterns include overestimation (if error variance decreases with X_j) or underestimation (if error variance increases with X_j) of true variance.\n\nQID: textbook-122-13-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-13-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the implications of heteroskedasticity on the variance of OLS estimators, matching the gold answer's points about invalid variance formulas, biased standard errors, and consequences for inference, while providing additional technical detail.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-13-0-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the implications of heteroskedasticity on the variance of OLS estimators, matching the gold answer's points about invalid variance formulas, biased standard errors, and consequences for inference, while providing additional technical detail."}, "llm_echoed_qid": "textbook-122-13-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the implications of heteroskedasticity on the variance of OLS estimators, matching the gold answer's points about invalid variance formulas, biased standard errors, and consequences for inference, while providing additional technical detail."}
{"script_processing_qid": "textbook-115-8-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the OLS estimator $\\hat{\\pmb\\beta}$ starting from the population moment conditions $\\operatorname{E}(\\mathbf{x}^{\\prime}u)=\\mathbf{0}$ and $\\operatorname{E}(\\mathbf{x}^{\\prime}\\mathbf{x})$ has full rank.\n\nGOLD_ANSWER:\n1.  **Population Moments**: Start with $\\operatorname{E}(\\mathbf{x}^{\\prime}u) = \\mathbf{0}$ and $\\operatorname{E}(\\mathbf{x}^{\\prime}\\mathbf{x})$ is full rank.\n2.  **Sample Analogue**: Replace population moments with sample analogues: $N^{-1}\\sum_{i=1}^{N}\\mathbf{x}_{i}^{\\prime}u_{i} = \\mathbf{0}$ and $N^{-1}\\sum_{i=1}^{N}\\mathbf{x}_{i}^{\\prime}\\mathbf{x}_{i}$.\n3.  **Solve for $\\hat{\\pmb\\beta}$**: $\\hat{\\pmb\\beta} = \\left(\\sum_{i=1}^{N}\\mathbf{x}_{i}^{\\prime}\\mathbf{x}_{i}\\right)^{-1}\\left(\\sum_{i=1}^{N}\\mathbf{x}_{i}^{\\prime}y_{i}\\right)$.\n4.  **Consistency**: Under OLS.1 and OLS.2, $\\hat{\\pmb\\beta}$ converges in probability to $\\beta$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the population moment condition E(𝐱'u) = 0.\nStep 2: Substitute u = y - 𝐱β into the moment condition: E(𝐱'(y - 𝐱β)) = 0.\nStep 3: Use the linearity of expectations to expand: E(𝐱'y) - E(𝐱'𝐱)β = 0.\nStep 4: Solve for β in population terms: β = [E(𝐱'𝐱)]⁻¹E(𝐱'y).\nStep 5: Replace population expectations with sample analogs:\n       (1/N)Σ𝐱_i'y_i → E(𝐱'y) and (1/N)Σ𝐱_i'𝐱_i → E(𝐱'𝐱).\nStep 6: Set up the sample equation: (Σ𝐱_i'𝐱_i)β = Σ𝐱_i'y_i.\nStep 7: Apply the full rank condition (OLS.2) to ensure invertibility: (Σ𝐱_i'𝐱_i) is invertible.\nStep 8: Solve for the OLS estimator: β̂ = (Σ𝐱_i'𝐱_i)⁻¹Σ𝐱_i'y_i = (𝐗'𝐗)⁻¹𝐗'𝐲.\n\nQID: textbook-115-8-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-8-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the OLS estimator from the population moment conditions, including substitution, linearity of expectations, and replacement with sample analogs. The final estimator matches the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-8-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the OLS estimator from the population moment conditions, including substitution, linearity of expectations, and replacement with sample analogs. The final estimator matches the gold answer."}, "llm_echoed_qid": "textbook-115-8-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the OLS estimator from the population moment conditions, including substitution, linearity of expectations, and replacement with sample analogs. The final estimator matches the gold answer."}
{"script_processing_qid": "textbook-107-10-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What would be the long run equilibrium attendance if the recurrence relation were $A_{t+1} = 77 + 0.30A_t$?\n\nGOLD_ANSWER:\n1. **Set Equilibrium Condition**: $A^* = 77 + 0.30A^*$.\n2. **Solve for $A^*$**: $A^* - 0.30A^* = 77$.\n3. **Simplify**: $0.70A^* = 77$.\n4. **Final Answer**: $A^* = \\frac{77}{0.70} = 110$.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that long run equilibrium occurs when attendance stabilizes (A_{t+1} = A_t = A*).\nStep 2: Substitute into the recurrence relation: A* = 77 + 0.30A*\nStep 3: Solve for A*:\nA* - 0.30A* = 77\n0.70A* = 77\nA* = 77/0.70 = 110\nTherefore, the long run equilibrium attendance would be 110.\n\nQID: textbook-107-10-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-10-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to find the equilibrium attendance, matching the gold answer exactly in both reasoning and final result.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-10-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to find the equilibrium attendance, matching the gold answer exactly in both reasoning and final result."}, "llm_echoed_qid": "textbook-107-10-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to find the equilibrium attendance, matching the gold answer exactly in both reasoning and final result."}
{"script_processing_qid": "textbook-33-0-3-0", "llm_grader_input_prompt_user": "\nQUESTION:\n7. Prove that in a symmetric equilibrium, the bidding strategy $b^{x}(\\theta_{i})$ must be strictly increasing in $\\theta_{i}$.\n\nGOLD_ANSWER:\n1. **Assumption for Contradiction**: Assume $b^{x}(\\theta_{i})$ is constant over some interval $(\\theta^{\\prime},\\theta^{\\prime\\prime})$.\n2. **Deviation**: Consider a deviation where an agent bids $b+\\epsilon$ instead of $b$ for $\\theta_{i}\\in(\\theta^{\\prime},\\theta^{\\prime\\prime})$.\n3. **Loss and Gain**: The loss is $p_{1}(\\theta_{1})\\cdot\\epsilon$, but the gain is that the agent wins with probability 1 if all types are in $(\\theta^{\\prime},\\theta^{\\prime\\prime})$.\n4. **Net Benefit**: For small $\\epsilon$, the gain outweighs the loss, making the deviation beneficial.\n5. **Conclusion**: Therefore, $b^{x}(\\theta_{i})$ must be strictly increasing to prevent such deviations.\n\nCANDIDATE_ANSWER:\nStep 1: Assume for contradiction that there exist types θ' < θ'' with bids b' > b'' in a symmetric equilibrium.\nStep 2: Apply revealed preference inequalities for both types:\n   - For θ': (θ' - b')π' ≥ (θ' - b'')π'' ⇒ (θ' - b')/(θ' - b'') ≤ π''/π'\n   - For θ'': (θ'' - b'')π'' ≥ (θ'' - b')π' ⇒ (θ'' - b'')/(θ'' - b') ≥ π'/π''\nStep 3: Multiply both inequalities to eliminate probabilities:\n   [(θ' - b')/(θ' - b'')] * [(θ'' - b'')/(θ'' - b')] ≤ 1\nStep 4: Simplify algebraically to obtain:\n   (θ'' - θ') (b' - b'') ≥ 0\nStep 5: This contradicts θ' < θ'' and b' > b'' (negative product), forcing strictly increasing bids.\nStep 6: Address potential flat intervals via deviation argument: If b^x(θ) were constant over (θ',θ''), agents could profitably deviate by slightly increasing bids to gain higher win probability with minimal cost.\nConclusion: Both contradictions imply b^x(θ_i) must be strictly increasing.\n\nQID: textbook-33-0-3-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-33-0-3-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly proves that the bidding strategy must be strictly increasing by using a contradiction approach and revealed preference inequalities, aligning with the gold answer's reasoning. The candidate also addresses potential flat intervals with a deviation argument, reinforcing the conclusion.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-33-0-3-0", "category": "CORRECT", "explanation": "The candidate's answer correctly proves that the bidding strategy must be strictly increasing by using a contradiction approach and revealed preference inequalities, aligning with the gold answer's reasoning. The candidate also addresses potential flat intervals with a deviation argument, reinforcing the conclusion."}, "llm_echoed_qid": "textbook-33-0-3-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly proves that the bidding strategy must be strictly increasing by using a contradiction approach and revealed preference inequalities, aligning with the gold answer's reasoning. The candidate also addresses potential flat intervals with a deviation argument, reinforcing the conclusion."}
{"script_processing_qid": "textbook-122-37-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the practical applications of residual analysis in econometrics, using examples from housing prices and law school rankings.\n\nGOLD_ANSWER:\n1. **Housing Prices**:\n   - Residuals identify underpriced/overpriced homes relative to model predictions.\n   - Example: A residual of $-120,206$ suggests the house is underpriced by $120,206.\n2. **Law School Rankings**:\n   - Residuals measure value-added by comparing actual median salaries to predicted salaries based on student characteristics.\n   - Schools with large positive residuals outperform expectations.\n3. **Legal and Policy Decisions**:\n   - Residuals can assess whether disparities (e.g., test scores) are due to observable factors or other unmeasured influences.\n\nCANDIDATE_ANSWER:\nStep 1: Understand residual analysis purpose - Residuals (û_i = y_i - ŷ_i) help evaluate model fit and identify anomalies.\\nStep 2: Housing prices application - Examine residuals from a price prediction model (e.g., using square footage, location). Large negative residuals indicate underpriced homes (actual price < predicted), while large positive residuals signal overpriced homes.\\nStep 3: Law school rankings application - Analyze residuals from a rankings model (e.g., using LSAT scores, GPA). Schools with large negative residuals outperform predictions (better rankings than model suggests), while positive residuals indicate underperformance.\\nStep 4: Practical value - Homebuyers/investors use residual analysis to find undervalued properties. Law schools use it to identify unobserved strengths/weaknesses affecting rankings beyond standard predictors.\n\nQID: textbook-122-37-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-37-0-3\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the purpose of residual analysis and its applications in housing prices and law school rankings. However, the explanation for law school rankings is partially incorrect: large negative residuals indicate underperformance, not outperformance, and positive residuals indicate outperformance, not underperformance. The practical value section is accurate but lacks depth compared to the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-37-0-3", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the purpose of residual analysis and its applications in housing prices and law school rankings. However, the explanation for law school rankings is partially incorrect: large negative residuals indicate underperformance, not outperformance, and positive residuals indicate outperformance, not underperformance. The practical value section is accurate but lacks depth compared to the gold answer."}, "llm_echoed_qid": "textbook-122-37-0-3", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the purpose of residual analysis and its applications in housing prices and law school rankings. However, the explanation for law school rankings is partially incorrect: large negative residuals indicate underperformance, not outperformance, and positive residuals indicate outperformance, not underperformance. The practical value section is accurate but lacks depth compared to the gold answer."}
{"script_processing_qid": "textbook-122-38-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the identification problem in the context of the labor supply and demand model. Why are exogenous variables like $z_{1}$ and $z_{2}$ crucial for identification?\n\nGOLD_ANSWER:\nThe identification problem arises because, without exogenous variables ($z_{1}$ and $z_{2}$), the supply and demand equations are indistinguishable. Exogenous variables provide variation that allows us to separate the effects of supply and demand. For example, $z_{1}$ (manufacturing wage) affects only supply, while $z_{2}$ (agricultural land area) affects only demand. This exclusion restriction is necessary for identifying the structural parameters.\n\nCANDIDATE_ANSWER:\nStep 1: The identification problem arises because both labor supply and demand equations share the same endogenous variables (h_i and w_i). Without unique information, we cannot distinguish between the two structural equations.\\n\\nStep 2: In equilibrium observation, we only see (h_i, w_i) intersections of supply and demand curves. Multiple (α_1, α_2) combinations could explain the same data points.\\n\\nStep 3: Exogenous variables z_1 (supply shifter) and z_2 (demand shifter) provide exclusion restrictions: z_1 appears only in the supply equation, z_2 only in the demand equation.\\n\\nStep 4: These excluded exogenous variables create independent variation – z_1 shifts supply (traces out demand curve) and z_2 shifts demand (traces out supply curve). This allows unique estimation of α_1 and α_2.\\n\\nStep 5: Without such variables, both equations would contain only (h_i, w_i), making them mathematically indistinguishable. The exogenous instruments satisfy the order condition for identification (each equation excludes at least one exogenous variable from the other equation).\n\nQID: textbook-122-38-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-38-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the identification problem in the labor supply and demand model, emphasizing the role of exogenous variables (z_1 and z_2) in providing exclusion restrictions and enabling the separation of supply and demand effects. This aligns perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-38-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the identification problem in the labor supply and demand model, emphasizing the role of exogenous variables (z_1 and z_2) in providing exclusion restrictions and enabling the separation of supply and demand effects. This aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-122-38-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the identification problem in the labor supply and demand model, emphasizing the role of exogenous variables (z_1 and z_2) in providing exclusion restrictions and enabling the separation of supply and demand effects. This aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-123-0-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Analyze the equilibrium condition for vacancy posting in the matching model with layoff taxes, and explain why the probability of finding a job falls sharply in model $M b$.\n\nGOLD_ANSWER:\n1. **Equilibrium Condition**: Firms post vacancies until expected profits are zero: $(1-\\phi)S\\_{b}(p\\_{0})-\\phi\\tau=\\frac{c}{\\beta q(\\theta)}$.\n2. **Model $M b$**: As $\\tau$ increases, the firm's share of the surplus decreases, requiring a higher $q(\\theta)$ (lower $\\theta$) to break even. This leads to fewer vacancies and a lower job-finding probability.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the equilibrium condition for vacancy posting. Firms post vacancies until the expected cost equals the expected value of a filled job: \\(\\kappa = q(\\theta) \\cdot \\mathbb{E}[J_a(p)]\\), where \\(\\kappa\\) is the vacancy cost, \\(q(\\theta)\\) is the vacancy-filling rate, and \\(\\mathbb{E}[J_a(p)]\\) is the expected firm surplus.\n\nStep 2: Incorporate the effect of layoff taxes. Since \\(J_a(p) = (1-\\phi)S_a(p)\\) and the match surplus \\(S_a(p)\\) depends on the layoff tax \\(\\tau\\), the tax reduces the net surplus of marginal jobs. Firms anticipate potential separation costs, lowering \\(\\mathbb{E}[J_a(p)]\\).\n\nStep 3: Analyze vacancy creation. A lower \\(\\mathbb{E}[J_a(p)]\\) reduces incentives for vacancy posting. With fewer vacancies, labor market tightness \\(\\theta = v/u\\) declines, which directly reduces the job-finding probability \\(f(\\theta) = m(v,u)/u\\) (a decreasing function of \\(\\theta\\) in standard matching functions).\n\nStep 4: Explain the sharp decline in model Mb. Layoff taxes disproportionately affect low-productivity jobs. Firms raise their productivity thresholds for job destruction to avoid paying \\(\\tau\\), but this simultaneously reduces the expected surplus for new vacancies (due to higher destruction thresholds). The combined effect causes a sharper decline in vacancy creation and \\(f(\\theta)\\) compared to models without layoff taxes.\n\nQID: textbook-123-0-2-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-0-2-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the equilibrium condition and the impact of layoff taxes on vacancy creation and job-finding probability. However, the explanation for the sharp decline in model Mb is less precise compared to the gold answer, which explicitly links the increase in τ to a higher q(θ) and lower θ.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-0-2-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the equilibrium condition and the impact of layoff taxes on vacancy creation and job-finding probability. However, the explanation for the sharp decline in model Mb is less precise compared to the gold answer, which explicitly links the increase in τ to a higher q(θ) and lower θ."}, "llm_echoed_qid": "textbook-123-0-2-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the equilibrium condition and the impact of layoff taxes on vacancy creation and job-finding probability. However, the explanation for the sharp decline in model Mb is less precise compared to the gold answer, which explicitly links the increase in τ to a higher q(θ) and lower θ."}
{"script_processing_qid": "textbook-110-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the profit-maximizing condition for a multiproduct firm, starting from the Lagrangean $$\\mathcal{L}(\\mathbf{q},\\lambda;\\mathbf{p}):=\\sum\\_{i=1}^{n}p\\_iq\\_i-\\lambda\\Phi(\\mathbf{q}).$$\n\nGOLD_ANSWER:\n1. **FOCs**: Differentiate $\\mathcal{L}$ with respect to $q\\_i$: $p\\_i - \\lambda\\Phi\\_i(\\mathbf{q}) = 0$ for all $i$.\n2. **Optimality Condition**: Rearrange to get $\\frac{\\Phi\\_j(\\mathbf{q})}{\\Phi\\_i(\\mathbf{q})} = \\frac{p\\_j}{p\\_i}$ for any two outputs $i$ and $j$.\n3. **Interpretation**: The firm equates the marginal rate of transformation (MRT) to the output price ratio, ensuring optimal resource allocation.\n\nCANDIDATE_ANSWER:\nStep 1: Set up the Lagrangean function with the firm's profit maximization objective and constraint:\n$$\n\\mathcal{L}(\\mathbf{q},\\lambda;\\mathbf{p}) = \\sum_{i=1}^{n} p_i q_i - \\lambda\\Phi(\\mathbf{q})\n$$\n\nStep 2: Take first-order conditions:\n- For each output $q_i$:\n$$\n\\frac{\\partial \\mathcal{L}}{\\partial q_i} = p_i - \\lambda \\Phi_i(\\mathbf{q}) = 0 \\quad \\Rightarrow \\quad p_i = \\lambda \\Phi_i(\\mathbf{q})\n$$\n- For the constraint:\n$$\n\\Phi(\\mathbf{q}) = 0\n$$\n\nStep 3: Take the ratio of FOCs for any two outputs $i$ and $j$:\n$$\n\\frac{p_i}{p_j} = \\frac{\\lambda \\Phi_i(\\mathbf{q})}{\\lambda \\Phi_j(\\mathbf{q})} = \\frac{\\Phi_i(\\mathbf{q})}{\\Phi_j(\\mathbf{q})}\n$$\n\nStep 4: Relate to MRT definition ($\\mathrm{MRT}_{ij} := \\Phi_j(\\mathbf{q})/\\Phi_i(\\mathbf{q})$):\n$$\n\\frac{p_i}{p_j} = \\frac{1}{\\mathrm{MRT}_{ij}} \\quad \\Rightarrow \\quad \\mathrm{MRT}_{ij} = \\frac{p_j}{p_i}\n$$\n\nConclusion: The profit-maximizing condition requires the marginal rate of transformation between any two goods to equal their price ratio (with outputs measured positively).\n\nQID: textbook-110-0-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-110-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the profit-maximizing condition for a multiproduct firm, following the same steps as the gold answer. The candidate accurately sets up the Lagrangean, takes the first-order conditions, and correctly interprets the optimality condition in terms of the marginal rate of transformation and price ratio.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-0-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the profit-maximizing condition for a multiproduct firm, following the same steps as the gold answer. The candidate accurately sets up the Lagrangean, takes the first-order conditions, and correctly interprets the optimality condition in terms of the marginal rate of transformation and price ratio."}, "llm_echoed_qid": "textbook-110-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the profit-maximizing condition for a multiproduct firm, following the same steps as the gold answer. The candidate accurately sets up the Lagrangean, takes the first-order conditions, and correctly interprets the optimality condition in terms of the marginal rate of transformation and price ratio."}
{"script_processing_qid": "textbook-112-8-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Using Adam Smith's framework, derive the conditions under which a farmer would switch from cultivating corn to cultivating a more expensive crop like hops or fruit.\n\nGOLD_ANSWER:\n1.  **Comparative Profit Analysis**: The farmer compares the net profit per unit of land for corn ($\\pi_c$) versus hops ($\\pi_h$), where $\\pi = (P \\cdot Q) - (R + W + C)$, with $P$ as price, $Q$ as quantity, $R$ as rent, $W$ as wages, and $C$ as cultivation costs.\n2.  **Switching Condition**: Switch occurs if $\\pi_h - \\pi_c > \\Delta E$, where $\\Delta E$ is the additional expense of hop cultivation (e.g., irrigation, skilled labor).\n3.  **Dynamic Adjustment**: If $\\pi_h$ persistently exceeds $\\pi_c + \\Delta E$, more land is allocated to hops until marginal returns equalize (Smith's 'natural proportion').\n\nCANDIDATE_ANSWER:\nStep 1: Identify the baseline profitability of corn/pasture. According to Smith, corn/pasture sets the \"natural rates\" of rent and profit that other crops must exceed to justify switching.\n\nStep 2: Calculate the total required compensation for switching. The alternative crop (e.g., hops/fruit) must cover:\n   a) Higher initial improvement costs (capital investments)\n   b) Higher annual cultivation expenses\n   c) A \"reasonable interest\" return on these additional costs\n\nStep 3: Compare profitability. Switching occurs when:\n   (Revenue from alternative crop − Total annual costs) > (Revenue from corn − Total annual costs) + Reasonable interest on additional capital\n\nStep 4: Incorporate market dynamics. If land suitable for the alternative crop is scarce relative to effectual demand, prices may rise sufficiently to satisfy the inequality in Step 3. This creates a price premium that offsets the higher costs of production.\n\nFinal Condition: Farmers switch when:\n   [Price × Quantity of alternative crop] ≥ [Costs of alternative crop (incl. capital recovery)] > [Profit from corn] + [Opportunity cost of capital tied up in improvements]\n\nQID: textbook-112-8-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-8-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer aligns with Adam Smith's framework by detailing the comparative profit analysis, switching conditions, and dynamic adjustments. The steps provided accurately reflect the economic reasoning required for a farmer to switch crops, including considerations of profitability, costs, and market dynamics.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-8-0-0", "category": "CORRECT", "explanation": "The candidate's answer aligns with Adam Smith's framework by detailing the comparative profit analysis, switching conditions, and dynamic adjustments. The steps provided accurately reflect the economic reasoning required for a farmer to switch crops, including considerations of profitability, costs, and market dynamics."}, "llm_echoed_qid": "textbook-112-8-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer aligns with Adam Smith's framework by detailing the comparative profit analysis, switching conditions, and dynamic adjustments. The steps provided accurately reflect the economic reasoning required for a farmer to switch crops, including considerations of profitability, costs, and market dynamics."}
{"script_processing_qid": "textbook-64-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the ADF test statistic from the given regression output and explain why the unit root hypothesis is not rejected.\n\nGOLD_ANSWER:\n1. **ADF Test Statistic**: The test statistic is calculated as $(0.7290534455-1)/.011230759=-2.373$. \n2. **Critical Value**: The critical value from Table 20.4 is -3.45. \n3. **Comparison**: Since -2.373 > -3.45, the test statistic is not in the rejection region. \n4. **Conclusion**: The unit root hypothesis cannot be rejected, indicating the presence of a unit root in the time series.\n\nCANDIDATE_ANSWER:\nStep 1: Identify relevant regression component\n- The ADF test statistic corresponds to the t-statistic of the coefficient on the lagged dependent variable (typically denoted as ρ-1) in the Dickey-Fuller regression.\n\nStep 2: Locate required parameters\n- The provided OLS output for series R shows:\n  - Standard deviation of residuals (σ) = 0.13893\n  - Number of observations = 56\n  - Parameters = 6 (indicating 5 lagged difference terms + constant)\n\nStep 3: Recognize missing information\n- Critical elements for ADF derivation (coefficient estimate and standard error) are absent from the output\n- Standard regression output tables typically show:\n  Coefficient | Std. Error | t-stat\n  ----------------------------------\n  Lagged Level | [value] | [value] | [ADF stat]\n\nStep 4: Analyze hypothesis test conclusion\n- The context explicitly states 'the unit root hypothesis is definitely not rejected'\n- This implies:\n  - Calculated ADF statistic > Critical value (at chosen significance level)\n  - Insufficient evidence against H₀: ρ=1 (unit root exists)\n\nStep 5: Explain non-rejection rationale\nEven without exact values, we know:\n1. High R-squared (0.97) suggests good model fit\n2. Low residual standard deviation (0.1389) indicates precise estimates\n3. Large F-statistic (333.41) shows joint significance\n4. Yet the specific coefficient test for stationarity (ADF) failed to reject H₀\n\nConclusion: The missing ADF statistic location in standard outputs and lack of coefficient details prevent exact derivation, but the explicit contextual statement confirms the test statistic wasn't sufficiently negative to reject the unit root null hypothesis at conventional significance levels.\n\nQID: textbook-64-1-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-64-1-0-0\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate correctly concludes that the unit root hypothesis is not rejected, aligning with the gold answer. However, the candidate's reasoning focuses on missing information and general statistical principles rather than directly deriving the ADF test statistic and comparing it to the critical value as specified in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-64-1-0-0", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate correctly concludes that the unit root hypothesis is not rejected, aligning with the gold answer. However, the candidate's reasoning focuses on missing information and general statistical principles rather than directly deriving the ADF test statistic and comparing it to the critical value as specified in the gold answer."}, "llm_echoed_qid": "textbook-64-1-0-0", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate correctly concludes that the unit root hypothesis is not rejected, aligning with the gold answer. However, the candidate's reasoning focuses on missing information and general statistical principles rather than directly deriving the ADF test statistic and comparing it to the critical value as specified in the gold answer."}
{"script_processing_qid": "textbook-112-12-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the impact of the bounty system on the price of corn in the home market during 1741-1750, referencing the specific figures provided.\n\nGOLD_ANSWER:\n1.  **Bounty Effect**: The bounty system hindered the price of corn from falling as low as it naturally would have in the home market.\n2.  **Quantitative Impact**: During these ten years, 8,029,156 quarters of grain were exported, with a bounty payment of £1,514,962:17:4 1/2.\n3.  **Price Mechanism**: The forced exportation raised the price of corn above what it would have been otherwise, as the domestic supply was artificially reduced.\n4.  **Example**: In 1749, the bounty paid was £324,176:10:6 for a single year, illustrating the significant fiscal impact.\n\nCANDIDATE_ANSWER:\nStep 1: The bounty system increased corn exports. The context specifies that 8,029,156 quarters and one bushel of grain were exported during 1741-1750, supported by a £1,514,962:17:4½ bounty.\n\nStep 2: By incentivizing exports through subsidies, the bounty reduced domestic corn supply. Reduced supply in the home market would naturally increase prices.\n\nStep 3: The text explicitly states the bounty 'hindered the price of corn from falling so low in the home market as it naturally would have done.' This implies corn prices remained artificially higher than they would have been without the subsidy.\n\nStep 4: The figures quantify the scale of exports and government expenditure on the bounty, demonstrating the policy's significant influence on market dynamics during this period.\n\nQID: textbook-112-12-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-12-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the impact of the bounty system on corn prices, referencing the specific figures and mechanisms described in the gold answer. The reasoning aligns with the gold answer's points about reduced domestic supply and artificially higher prices.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-12-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains the impact of the bounty system on corn prices, referencing the specific figures and mechanisms described in the gold answer. The reasoning aligns with the gold answer's points about reduced domestic supply and artificially higher prices."}, "llm_echoed_qid": "textbook-112-12-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the impact of the bounty system on corn prices, referencing the specific figures and mechanisms described in the gold answer. The reasoning aligns with the gold answer's points about reduced domestic supply and artificially higher prices."}
{"script_processing_qid": "textbook-84-5-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. What practical conclusion can be drawn from the sleep analysis regarding the adequacy of a model with only a gender dummy (no interactions)?\n\nGOLD_ANSWER:\n4. **Model Adequacy**: The test results suggest that a model with only a gender dummy (and no interactions) may suffice for studying the sleep-work tradeoff, as slope differences are not statistically significant. However, the intercept difference captures the gendered sleep disparity.\n\nCANDIDATE_ANSWER:\nStep 1: The F-test (with p-value ≈ 0.05) rejects the null hypothesis that the sleep equations (including interaction terms) are the same for men and women at the 5% significance level. This implies joint significance of the gender interaction terms.\nStep 2: The significant male⋅yngkid interaction coefficient reveals that the effect of young children on sleep differs by gender (women lose sleep, men gain sleep).\nStep 3: A model with only a gender dummy (no interactions) would force the effect of young children (and other variables) to be the same across genders, which contradicts the evidence.\nConclusion: A model with only a gender dummy is inadequate because it fails to account for gender-specific effects of young children and other variables, as shown by the significant interactions in the full model.\n\nQID: textbook-84-5-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-84-5-1-3\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate answer contradicts the gold answer by asserting the inadequacy of a gender-only dummy model, whereas the gold answer suggests it may suffice due to non-significant slope differences. The reasoning about interaction terms is correct but misaligned with the conclusion drawn in the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-84-5-1-3", "category": "INCORRECT", "explanation": "The candidate answer contradicts the gold answer by asserting the inadequacy of a gender-only dummy model, whereas the gold answer suggests it may suffice due to non-significant slope differences. The reasoning about interaction terms is correct but misaligned with the conclusion drawn in the gold answer."}, "llm_echoed_qid": "textbook-84-5-1-3", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate answer contradicts the gold answer by asserting the inadequacy of a gender-only dummy model, whereas the gold answer suggests it may suffice due to non-significant slope differences. The reasoning about interaction terms is correct but misaligned with the conclusion drawn in the gold answer."}
{"script_processing_qid": "textbook-122-33-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the labor supply function $hours = 33 + 45.1\\log(wage)$, calculate the approximate change in hours worked per week for a $5\\%$ increase in wage.\n\nGOLD_ANSWER:\n1. **Given Model**: $hours = 33 + 45.1\\log(wage)$.\n2. **Approximation Formula**: From (A.30), $\\Delta hours \\approx (45.1/100)(\\%\\Delta wage)$.\n3. **Plug in Values**:\n   $$ \\Delta hours \\approx (45.1/100)(5) = 2.255. $$\n4. **Interpretation**: A $5\\%$ increase in wage leads to an approximate increase of 2.255 hours worked per week.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the relevant formula from the context. For a model of the form y = β₀ + β₁log(x), the approximate change in y for a percentage change in x is given by Δy ≈ (β₁/100)(%Δx).\n\nStep 2: Apply this to the given labor supply function hours = 33 + 45.1log(wage). Here, β₁ = 45.1 and the wage increase is 5%.\n\nStep 3: Calculate the approximate change in hours:\nΔhours ≈ (45.1/100) * 5 = 0.451 * 5 = 2.255\n\nStep 4: Round to a reasonable decimal place. The approximate change in hours worked per week is 2.26 hours.\n\nQID: textbook-122-33-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-33-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly identified the formula, applied it accurately to the given labor supply function, and calculated the approximate change in hours worked per week as 2.255, which was then reasonably rounded to 2.26. This matches the gold answer in both method and result.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-33-0-2", "category": "CORRECT", "explanation": "The candidate correctly identified the formula, applied it accurately to the given labor supply function, and calculated the approximate change in hours worked per week as 2.255, which was then reasonably rounded to 2.26. This matches the gold answer in both method and result."}, "llm_echoed_qid": "textbook-122-33-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly identified the formula, applied it accurately to the given labor supply function, and calculated the approximate change in hours worked per week as 2.255, which was then reasonably rounded to 2.26. This matches the gold answer in both method and result."}
{"script_processing_qid": "textbook-115-11-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. How does the Hausman test extend to multiple potentially endogenous variables?\n\nGOLD_ANSWER:\nFor multiple endogenous variables $\\mathbf{y}_{2}$: \n1. Estimate the reduced form for each element of $\\mathbf{y}_{2}$ and save the residuals $\\hat{\\mathbf{v}}_{2}$. \n2. Add $\\hat{\\mathbf{v}}_{2}$ to the structural equation and test the joint significance of $\\hat{\\mathbf{v}}_{2}$ using an $F$ test. \n3. Under $H_{0}$, the $F$ statistic is asymptotically valid. For heteroskedasticity, use a robust Wald test.\n\nCANDIDATE_ANSWER:\nStep 1: When extending the Hausman test to multiple potentially endogenous variables, estimate reduced forms for all suspected endogenous variables.\nStep 2: Save the residuals (denoted as $\\hat{v}_2, \\hat{v}_3, ..., \\hat{v}_G$) from each reduced form estimation.\nStep 3: Add all these residuals to the original structural equation as additional regressors.\nStep 4: Perform an F-test (or chi-squared test via LM/Wald alternative) for the joint significance of all residual terms. This tests $H_0: \rho_1 = \rho_2 = ... = \rho_G = 0$ where $\rho$ parameters represent correlations between $u_1$ and each $v_g$.\nStep 5: Rejection of $H_0$ implies at least one variable is endogenous. The test statistic degrees of freedom equal the number of tested endogenous variables.\n\nQID: textbook-115-11-2-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-11-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately describes the steps for extending the Hausman test to multiple potentially endogenous variables, including estimating reduced forms, saving residuals, adding them to the structural equation, and performing a joint significance test. This aligns perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-11-2-1", "category": "CORRECT", "explanation": "The candidate answer accurately describes the steps for extending the Hausman test to multiple potentially endogenous variables, including estimating reduced forms, saving residuals, adding them to the structural equation, and performing a joint significance test. This aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-115-11-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately describes the steps for extending the Hausman test to multiple potentially endogenous variables, including estimating reduced forms, saving residuals, adding them to the structural equation, and performing a joint significance test. This aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-123-25-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain how the promised utilities $\\overline{v} = 2u(0.5)$ and $\\underline{v} = u(1-\\epsilon) + u(\\epsilon)$ enforce the gift-giving equilibrium.\n\nGOLD_ANSWER:\n1. **Reward Path**: $\\overline{v}$ represents utility when all generations cooperate.  \n2. **Punishment Path**: $\\underline{v}$ represents autarky utility.  \n3. **Incentive Compatibility**: Deviating triggers transition to $\\underline{v}$, making cooperation optimal when $2u(0.5) > u(1-\\epsilon) + u(\\epsilon)$.  \n4. **Subgame Perfection**: The strategy profile is Nash in every subgame.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the equilibrium structure\nIn the gift-giving equilibrium, young agents transfer resources to the old (gift $s_i$) with the expectation that future generations will reciprocate. The promised utilities $\\overline{v}$ and $\\underline{v}$ enforce compliance through a reputation mechanism.\n\nStep 2: Interpret $\\overline{v} = 2u(0.5)$\n$\\overline{v}$ represents the equilibrium utility when agents cooperate fully. At the optimal allocation, each agent splits their endowment equally between young and old age: gives $0.5$ when young and receives $0.5$ when old. This yields $u(0.5)$ in both periods, totaling $2u(0.5)$.\n\nStep 3: Interpret $\\underline{v} = u(1-\\epsilon) + u(\\epsilon)$\n$\\underline{v}$ is the punishment utility if an agent deviates. A deviating agent keeps nearly all their endowment when young ($1-\\epsilon$), but subsequent generations retaliate by only transferring $\\epsilon$ to them when old. This matches the autarky-like payoff $u(1-\\epsilon) + u(\\epsilon)$.\n\nStep 4: Enforcement mechanism\n- Cooperative equilibrium is sustained if $\\overline{v} \\geq$ one-time deviation gain + $\\underline{v}$\n- Deviating yields a temporary gain of $u(1)$ instead of $u(0.5)$ when young, but triggers $\\underline{v}$ thereafter. For small $\\epsilon$, $u(1) + \\underline{v} < \\overline{v}$ incentivizes cooperation.\n\nStep 5: Role of $\\epsilon$\n$\\epsilon$ ensures punishment is credibly harsh but non-zero (Smith 1992). A fully autarkic punishment ($\\epsilon = 0$) might not be renegotiation-proof, while $\\epsilon > 0$ preserves minimal transfers to maintain equilibrium stability.\n\nQID: textbook-123-25-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-25-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the roles of $\\overline{v}$ and $\\underline{v}$ in enforcing the gift-giving equilibrium, matching the gold answer's key points about reward paths, punishment paths, incentive compatibility, and subgame perfection. The additional detail about $\\epsilon$ enhances the explanation without contradicting the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-25-1-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the roles of $\\overline{v}$ and $\\underline{v}$ in enforcing the gift-giving equilibrium, matching the gold answer's key points about reward paths, punishment paths, incentive compatibility, and subgame perfection. The additional detail about $\\epsilon$ enhances the explanation without contradicting the gold answer."}, "llm_echoed_qid": "textbook-123-25-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the roles of $\\overline{v}$ and $\\underline{v}$ in enforcing the gift-giving equilibrium, matching the gold answer's key points about reward paths, punishment paths, incentive compatibility, and subgame perfection. The additional detail about $\\epsilon$ enhances the explanation without contradicting the gold answer."}
{"script_processing_qid": "textbook-122-27-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate the quantity demanded when price is $\\$10$ and income is $\\$900$ using the demand function.\n\nGOLD_ANSWER:\n1. **Substitute Values**: Plug $price = 10$ and $income = 900$ into the demand function:\n   $$ quantity = 120 - 9.8(10) + 0.03(900). $$\n2. **Compute**: \n   $$ quantity = 120 - 98 + 27 = 49. $$\n   The quantity demanded is 49 units.\n\nCANDIDATE_ANSWER:\nStep 1: The problem requires calculating quantity demanded using the demand function when price = $10 and income = $900.\nStep 2: The context explicitly states the slope of the demand curve (-9.8) at income = $900, which represents the partial effect of price on quantity (ΔQ/ΔP = -9.8).\nStep 3: However, the context does NOT provide the intercept term or the full demand function equation (e.g., Q = a - 9.8P + ...).\nStep 4: Without additional parameters (like the intercept or income coefficient), the quantity demanded cannot be numerically calculated from the given information.\nConclusion: The provided context lacks sufficient details about the demand function's structure (e.g., constant term or income terms) to perform the calculation.\n\nQID: textbook-122-27-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-27-0-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate answer incorrectly concludes that the demand function cannot be calculated due to missing parameters, while the gold answer clearly provides the full demand function and computes the quantity demanded as 49 units.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-27-0-2", "category": "INCORRECT", "explanation": "The candidate answer incorrectly concludes that the demand function cannot be calculated due to missing parameters, while the gold answer clearly provides the full demand function and computes the quantity demanded as 49 units."}, "llm_echoed_qid": "textbook-122-27-0-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate answer incorrectly concludes that the demand function cannot be calculated due to missing parameters, while the gold answer clearly provides the full demand function and computes the quantity demanded as 49 units."}
{"script_processing_qid": "textbook-114-20-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the relationship between marginal cost (MC) and average total cost (ATC) when MC is below ATC. Use the data from Table 7.2 to illustrate.\n\nGOLD_ANSWER:\n1. **Relationship**: When MC < ATC, ATC decreases as output increases because the cost of the additional unit pulls the average down.\n2. **Example**: At Q=4, ATC=$40.5; at Q=5, MC=$18 < ATC=$36, so ATC falls from $40.5 to $36.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that when MC < ATC, the additional unit produced costs less than the current average cost. This pulls the average downward.\\nStep 2: From the context, when output Q = 5, ATC = $36 (calculated as $180 TC / 5 units). If MC for the next unit (Q = 6) were hypothetically $30 (below $36), the new TC would be $180 + $30 = $210.\\nStep 3: The new ATC at Q = 6 would be $210 / 6 = $35. This demonstrates that when MC ($30) is below the original ATC ($36), the ATC decreases (from $36 to $35).\\nStep 4: This relationship occurs because marginal units below ATC dilute the average fixed and variable costs across more units, reducing ATC until MC rises to equal ATC.\n\nQID: textbook-114-20-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-20-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly explains the relationship between MC and ATC when MC is below ATC, using a hypothetical example that aligns with the gold answer's logic. The reasoning and conclusion are accurate.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-20-2-1", "category": "CORRECT", "explanation": "The candidate correctly explains the relationship between MC and ATC when MC is below ATC, using a hypothetical example that aligns with the gold answer's logic. The reasoning and conclusion are accurate."}, "llm_echoed_qid": "textbook-114-20-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly explains the relationship between MC and ATC when MC is below ATC, using a hypothetical example that aligns with the gold answer's logic. The reasoning and conclusion are accurate."}
{"script_processing_qid": "textbook-104-3-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain why the Monotone Class Lemma is particularly useful for proving properties of product $\\sigma$-algebras.\n\nGOLD_ANSWER:\nThe Monotone Class Lemma is useful because:\n1. **Simplifies proofs**: It reduces the problem to verifying a property on an algebra (e.g., finite unions of disjoint measurable rectangles) and showing the property is preserved under monotone limits.\n2. **Avoids complexity**: Directly proving a property holds for a $\\sigma$-algebra can be technically challenging, while working with monotone classes is often more straightforward.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that product σ-algebras are generated by measurable rectangles ({A × B | A ∈ 𝒳, B ∈ 𝒴}), which form a π-system.\nStep 2: The Monotone Class Lemma allows us to extend properties verified on this π-system to the entire product σ-algebra, provided:\n   (a) The property is preserved under increasing limits\n   (b) The property is preserved under proper differences\nStep 3: For product σ-algebra properties like measurability of sections:\n   - First verify the property holds for rectangles (which is straightforward since sections of rectangles become either A or B)\n   - Then show the collection of sets satisfying the property forms a monotone class\nStep 4: By the Monotone Class Lemma, this implies all sets in the product σ-algebra (being the smallest σ-algebra containing the rectangles) inherit the property, avoiding the need to handle arbitrary countable unions/intersections directly.\n\nQID: textbook-104-3-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-104-3-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the utility of the Monotone Class Lemma for product σ-algebras, detailing the steps to extend properties from a π-system to the entire σ-algebra while preserving the properties under monotone limits and differences, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-104-3-1-3", "category": "CORRECT", "explanation": "The candidate answer accurately explains the utility of the Monotone Class Lemma for product σ-algebras, detailing the steps to extend properties from a π-system to the entire σ-algebra while preserving the properties under monotone limits and differences, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-104-3-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the utility of the Monotone Class Lemma for product σ-algebras, detailing the steps to extend properties from a π-system to the entire σ-algebra while preserving the properties under monotone limits and differences, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-102-3-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Under what condition is the forecast $\\mathbf{X}_{1}\\mathbf{b}_{1}$ unbiased when $\\mathbf{X}_{2}$ is omitted from the regression?\n\nGOLD_ANSWER:\nThe forecast $\\mathbf{X}_{1}\\mathbf{b}_{1}$ is unbiased if $E[\\mathbf{X}_{2}|\\mathbf{X}_{1}]$ is a linear function of $\\mathbf{X}_{1}$. This ensures that $\\mathbf{P}_{1.2}$ unbiasedly estimates the slopes of the conditional mean function, and the compound disturbance $\\mathbf{\\varepsilon} + (\\mathbf{X}_{2} - E[\\mathbf{X}_{2}|\\mathbf{X}_{1}])\\mathbf{\\beta}_{2}$ has mean zero and is uncorrelated with $\\mathbf{X}_{1}$.\n\nCANDIDATE_ANSWER:\nStep 1: The forecast $\\mathbf{X}_{1}\\mathbf{b}_{1}$ is unbiased if its expectation equals the true conditional mean $E[\\mathbf{y}|\\mathbf{X}_{1}]$.\n\nStep 2: From the context, $E[\\mathbf{b}_{1}] = \\beta_{1} + \\mathbf{P}_{1.2}\\beta_{2}$, where $\\mathbf{P}_{1.2} = (\\mathbf{X}_{1}'\\mathbf{X}_{1})^{-1}\\mathbf{X}_{1}'\\mathbf{X}_{2}$. Thus, the forecast expectation is:\n$$E[\\mathbf{X}_{1}\\mathbf{b}_{1}] = \\mathbf{X}_{1}(\\beta_{1} + \\mathbf{P}_{1.2}\\beta_{2}) = \\mathbf{X}_{1}\\beta_{1} + \\mathbf{X}_{1}\\mathbf{P}_{1.2}\\beta_{2}$$\n\nStep 3: The context shows $\\mathbf{X}_{1}\\mathbf{P}_{1.2} = E[\\mathbf{X}_{2}|\\mathbf{X}_{1}]$ when $E[\\mathbf{X}_{2}|\\mathbf{X}_{1}]$ is linear in $\\mathbf{X}_{1}$. Substituting this:\n$$E[\\mathbf{X}_{1}\\mathbf{b}_{1}] = \\mathbf{X}_{1}\\beta_{1} + E[\\mathbf{X}_{2}|\\mathbf{X}_{1}]\\beta_{2}$$\n\nStep 4: The true conditional mean $E[\\mathbf{y}|\\mathbf{X}_{1}]$ is exactly $\\mathbf{X}_{1}\\beta_{1} + E[\\mathbf{X}_{2}|\\mathbf{X}_{1}]\\beta_{2}$ (from the rewritten model in the context).\n\nStep 5: Therefore, $E[\\mathbf{X}_{1}\\mathbf{b}_{1}] = E[\\mathbf{y}|\\mathbf{X}_{1}]$ when $E[\\mathbf{X}_{2}|\\mathbf{X}_{1}]$ is linear in $\\mathbf{X}_{1}$. This makes the forecast unbiased.\n\nConclusion: The forecast $\\mathbf{X}_{1}\\mathbf{b}_{1}$ is unbiased if $E[\\mathbf{X}_{2}|\\mathbf{X}_{1}]$ is a linear function of $\\mathbf{X}_{1}$. This holds even though $\\mathbf{b}_{1}$ itself is biased, as the bias in coefficients cancels out in the forecast due to the linear conditional mean structure.\n\nQID: textbook-102-3-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-102-3-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the conditions under which the forecast is unbiased, matching the gold answer's key point about the linearity of $E[\\\\mathbf{X}_{2}|\\\\mathbf{X}_{1}]$ in $\\\\mathbf{X}_{1}$. The reasoning is detailed and correct.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-102-3-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the conditions under which the forecast is unbiased, matching the gold answer's key point about the linearity of $E[\\mathbf{X}_{2}|\\mathbf{X}_{1}]$ in $\\mathbf{X}_{1}$. The reasoning is detailed and correct."}, "llm_echoed_qid": "textbook-102-3-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the conditions under which the forecast is unbiased, matching the gold answer's key point about the linearity of $E[\\mathbf{X}_{2}|\\mathbf{X}_{1}]$ in $\\mathbf{X}_{1}$. The reasoning is detailed and correct."}
{"script_processing_qid": "textbook-115-17-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Why is the test based on equation (10.40) preferred over the AR(1) test for $H_0: \\sigma_c^2 = 0$?\n\nGOLD_ANSWER:\nThe test in (10.40) directly targets the covariance structure implied by $\\sigma_c^2 > 0$ and is robust to heteroskedasticity, unlike the AR(1) test, which assumes a specific form of serial correlation.\n\nCANDIDATE_ANSWER:\nStep 1: The AR(1) test (from equation 7.77) examines first-order serial correlation in the idiosyncratic errors $v_{it}$. However, under $H_0: \\sigma_c^2 = 0$, the composite error $u_{it} = c_i + v_{it}$ reduces to $v_{it}$, which is assumed serially uncorrelated under the null. Thus, the AR(1) test is valid but only checks for AR(1) dependence in $v_{it}$.\n\nStep 2: The test based on equation (10.40) aggregates all pairwise residual products $\\hat{v}_{it}\\hat{v}_{is}$ across time periods ($t < s$). This formulation inherently accounts for serial correlation patterns induced by the alternative hypothesis ($\\sigma_c^2 > 0$), where $c_i$ induces persistent, time-invariant correlation across all periods.\n\nStep 3: The AR(1) test focuses narrowly on lag-1 autocorrelation, while equation (10.40)’s test captures the specific correlation structure implied by $c_i$ (equal correlation across all time pairs). This makes the latter more powerful and appropriate for detecting $\\sigma_c^2 > 0$ compared to the AR(1) test, which may miss non-AR(1) patterns of serial correlation caused by unobserved effects.\n\nQID: textbook-115-17-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-17-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains why the test based on equation (10.40) is preferred over the AR(1) test for $H_0: \\sigma_c^2 = 0$. It correctly highlights that the AR(1) test is limited to detecting first-order serial correlation, while the test in (10.40) captures the broader correlation structure induced by $c_i$, making it more robust and powerful for the hypothesis in question.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-17-1-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains why the test based on equation (10.40) is preferred over the AR(1) test for $H_0: \\sigma_c^2 = 0$. It correctly highlights that the AR(1) test is limited to detecting first-order serial correlation, while the test in (10.40) captures the broader correlation structure induced by $c_i$, making it more robust and powerful for the hypothesis in question."}, "llm_echoed_qid": "textbook-115-17-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains why the test based on equation (10.40) is preferred over the AR(1) test for $H_0: \\sigma_c^2 = 0$. It correctly highlights that the AR(1) test is limited to detecting first-order serial correlation, while the test in (10.40) captures the broader correlation structure induced by $c_i$, making it more robust and powerful for the hypothesis in question."}
{"script_processing_qid": "textbook-107-3-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Prove that if the Laspeyres index is less than 1, the consumer is better off in period 2.\n\nGOLD_ANSWER:\nA Laspeyres index < 1 implies the period 1 bundle is cheaper in period 2. By revealed preference, if the consumer could afford the original bundle but chooses a different one, they must be at least as well off (strictly better if preferences are strictly convex).\n\nCANDIDATE_ANSWER:\nStep 1: Recall Laspeyres Index Definition\nThe Laspeyres index (L) is defined as:\nL = (Σ p²q¹) / (Σ p¹q¹)\nwhere p¹ = period 1 prices, p² = period 2 prices, and q¹ = period 1 quantities.\n\nStep 2: Interpret L < 1\nIf L < 1, then:\nΣ p²q¹ < Σ p¹q¹\nThis means the period 1 consumption bundle costs less in period 2 than it did in period 1.\n\nStep 3: Analyze Consumer Welfare\nAt period 2 prices:\n1. The consumer could still purchase their original period 1 bundle (q¹) but spend less money than in period 1\n2. The saved money (Σ p¹q¹ - Σ p²q¹) could be used to purchase additional goods/services\n3. Alternatively, the consumer could reoptimize to select a new bundle q² that is:\n   - Revealed preferred to q¹ (by having extra income)\n   - At least as good as q¹ (by weak axiom)\n\nStep 4: Conclusion\nSince the consumer can afford their original bundle plus potentially more, or a strictly preferred bundle, they must be at least as well off (and potentially better off) in period 2 when L < 1. This demonstrates improved purchasing power and welfare.\n\nQID: textbook-107-3-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-107-3-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly defines the Laspeyres index, interprets the condition L < 1, and logically concludes that the consumer is at least as well off (and potentially better off) in period 2, aligning with the gold answer's reasoning.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-3-1-1", "category": "CORRECT", "explanation": "The candidate's answer correctly defines the Laspeyres index, interprets the condition L < 1, and logically concludes that the consumer is at least as well off (and potentially better off) in period 2, aligning with the gold answer's reasoning."}, "llm_echoed_qid": "textbook-107-3-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly defines the Laspeyres index, interprets the condition L < 1, and logically concludes that the consumer is at least as well off (and potentially better off) in period 2, aligning with the gold answer's reasoning."}
{"script_processing_qid": "textbook-107-4-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the optimal combination of celebrity photographs ($C$) and news stories ($N$) for Snoozeweek to maximize sales $S=1,000C^{0.50}N^{0.50}$ given the budget constraint $1,000C + 10N = 12,000$.\n\nGOLD_ANSWER:\n1. **Lagrangian Setup**: $\\mathcal{L} = 1,000C^{0.50}N^{0.50} + \\lambda(12,000 - 1,000C - 10N)$.\n2. **First-Order Conditions**: \n   - $\\frac{\\partial \\mathcal{L}}{\\partial C} = 500C^{-0.50}N^{0.50} - 1,000\\lambda = 0$.\n   - $\\frac{\\partial \\mathcal{L}}{\\partial N} = 500C^{0.50}N^{-0.50} - 10\\lambda = 0$.\n3. **Solve for $\\lambda$**: From the first equation, $\\lambda = 0.5C^{-0.50}N^{0.50}$. From the second, $\\lambda = 50C^{0.50}N^{-0.50}$.\n4. **Equate and Simplify**: $0.5C^{-0.50}N^{0.50} = 50C^{0.50}N^{-0.50} \\Rightarrow N = 100C$.\n5. **Substitute into Budget**: $1,000C + 10(100C) = 12,000 \\Rightarrow 2,000C = 12,000 \\Rightarrow C = 6, N = 600$.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize the Cobb-Douglas production function S = 1,000C⁰·⁵⁰N⁰·⁵⁰ has constant returns to scale.\nStep 2: Set up the MRTS (Marginal Rate of Technical Substitution) equal to price ratio:\n   (MP_C/MP_N) = (P_C/P_N)\n   (0.5S/C)/(0.5S/N) = 1,000/10\n   (N/C) = 100 ⇒ N = 100C\nStep 3: Substitute N = 100C into budget constraint:\n   1,000C + 10(100C) = 12,000 ⇒ 2,000C = 12,000\nStep 4: Solve for C:\n   C = 6\nStep 5: Calculate N using N = 100C:\n   N = 600\nConclusion: The optimal combination is 6 celebrity photos and 600 column inches of news stories.\n\nQID: textbook-107-4-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-4-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the optimal combination of celebrity photographs and news stories, matching the gold answer in both methodology and final results.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-4-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the optimal combination of celebrity photographs and news stories, matching the gold answer in both methodology and final results."}, "llm_echoed_qid": "textbook-107-4-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the optimal combination of celebrity photographs and news stories, matching the gold answer in both methodology and final results."}
{"script_processing_qid": "textbook-118-6-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Discuss the challenges of constructing the matrix $\\bar{F}_{\\bullet}(\\theta)$ in nonlinear simultaneous equations models and propose practical solutions.\n\nGOLD_ANSWER:\n- **Challenge**: $\\bar{F}_{\\bullet}(\\theta)$ requires conditional expectations of derivatives, which may lack closed-form solutions.\n- **Solutions**:\n  1. Use numerical approximations or simulation methods.\n  2. Replace $\\bar{F}_{\\bullet}(\\theta)$ with $F_{\\bullet}(Y,\\theta)$ and a chosen instrument matrix $W$, as in (12.102).\n  3. Iterate between estimating $\\theta$ and approximating $\\bar{F}_{\\bullet}(\\theta)$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the key challenge - Construction of \\(\\bar{F}_{\\bullet}(\\theta)\\) requires conditional expectations of derivatives (\\frac{\\partial f_{ti}}{\\partial \\theta}\\)) given predetermined information, which are generally unobservable in nonlinear models.\n\nStep 2: Recognize additional complexities: (a) Nonlinearity in both parameters \\(\\theta\\) and endogenous variables \\(Y_t\\) complicates analytical derivation of these expectations; (b) Simultaneity creates interdependencies between equations, making differentiation and expectation operations interdependent.\n\nStep 3: Propose practical solutions:\n1. Use feasible two-step GMM: First obtain consistent preliminary estimates \\(\\hat{\\theta}^{(1)}\\) using suboptimal instruments, then approximate \\(\\bar{F}_{\\bullet}(\\theta)\\) numerically at \\(\\hat{\\theta}^{(1)}\\)\n2. Replace conditional expectations with instrument-projected counterparts using \\(P_W\\) from available instruments matrix \\(W\\) (as shown in the estimating equations)\\n3. Employ nonparametric methods (e.g., kernel regression) to estimate conditional expectations when predetermined variables are observable\\n4. Utilize stochastic approximation techniques for derivative calculations when analytical differentiation is intractable.\n\nStep 4: Highlight implementation trade-off: While theoretically optimal instruments require full knowledge of conditional expectations, practical implementations rely on approximations through instrument choice and multi-stage estimation to balance efficiency gains with computational feasibility.\n\nQID: textbook-118-6-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-6-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies the challenges of constructing \\(\\bar{F}_{\\bullet}(\\theta)\\) in nonlinear simultaneous equations models and provides practical solutions that align with the gold answer. The candidate's detailed steps and additional complexities enhance the understanding of the problem and the proposed solutions are valid and relevant.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-6-1-1", "category": "CORRECT", "explanation": "The candidate answer accurately identifies the challenges of constructing \\(\bar{F}_{\bullet}(\theta)\\) in nonlinear simultaneous equations models and provides practical solutions that align with the gold answer. The candidate's detailed steps and additional complexities enhance the understanding of the problem and the proposed solutions are valid and relevant."}, "llm_echoed_qid": "textbook-118-6-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies the challenges of constructing \\(\bar{F}_{\bullet}(\theta)\\) in nonlinear simultaneous equations models and provides practical solutions that align with the gold answer. The candidate's detailed steps and additional complexities enhance the understanding of the problem and the proposed solutions are valid and relevant."}
{"script_processing_qid": "textbook-83-0-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. How do approximation errors in numerical simulations affect the testing of economic theories?\n\nGOLD_ANSWER:\n1.  **Cumulative Effect**: Approximation errors can accumulate over iterative schemes, significantly impacting the solution.\n2.  **Systematic Biases**: Discretization errors may introduce systematic biases, affecting the model's predictions.\n3.  **Error Propagation**: In sensitive systems, small approximation errors can lead to large deviations in the model's behavior.\n4.  **Validation Difficulty**: Approximation errors complicate the validation process, as they must be separated from other sources of error.\n\nCANDIDATE_ANSWER:\nStep 1: Numerical simulations in economic modeling often involve approximations (e.g., discretization, iterative algorithms) to solve analytically intractable models. These approximations introduce errors between the simulated results and the true theoretical predictions of the economic model.\\n\\nStep 2: During calibration, approximation errors may lead to biased parameter estimates if the numerical model does not accurately replicate the original economic model's behavior. This compromises the model's ability to reflect theoretical assumptions.\\n\\nStep 3: When testing theories, discrepancies between simulated predictions (affected by approximation errors) and real-world data can lead to incorrect conclusions. For example, a valid theory might appear invalid due to simulation inaccuracies, or an invalid theory might appear valid if errors mask its flaws.\\n\\nStep 4: The framework in Figure 6 highlights that errors at the numerical simulation stage propagate to the validation phase. Contrasting flawed simulations with real-world data creates uncertainty about whether mismatches arise from theoretical weaknesses or numerical inaccuracies, undermining the reliability of hypothesis testing.\n\nQID: textbook-83-0-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-83-0-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer effectively addresses how approximation errors in numerical simulations impact the testing of economic theories, aligning with the gold answer's points on cumulative effects, systematic biases, error propagation, and validation difficulty. The explanation is thorough and matches the gold answer's key ideas.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-83-0-1-2", "category": "CORRECT", "explanation": "The candidate's answer effectively addresses how approximation errors in numerical simulations impact the testing of economic theories, aligning with the gold answer's points on cumulative effects, systematic biases, error propagation, and validation difficulty. The explanation is thorough and matches the gold answer's key ideas."}, "llm_echoed_qid": "textbook-83-0-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer effectively addresses how approximation errors in numerical simulations impact the testing of economic theories, aligning with the gold answer's points on cumulative effects, systematic biases, error propagation, and validation difficulty. The explanation is thorough and matches the gold answer's key ideas."}
{"script_processing_qid": "textbook-112-8-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze the welfare implications of France's 1731 ban on new vineyards using Smith's concepts of 'effectual demand' and 'natural proportion.' How might this distort resource allocation?\n\nGOLD_ANSWER:\n1.  **Effectual Demand**: The ban artificially constrains supply below equilibrium, raising wine prices above the 'natural proportion' to corn/pasture.\n2.  **Rent Seeking**: Existing vineyard owners capture monopoly rents (£X in diagram), while deadweight loss (DWL) emerges from unrealized trades.\n3.  **Dynamic Inefficiency**: Barriers to entry reduce incentives for innovation in viticulture, contrary to Smith's advocacy for free competition.\n\nCANDIDATE_ANSWER:\nStep 1: Understand Smith's 'natural proportion' concept. Land allocation between uses (e.g., vineyards vs. grain) should naturally adjust until rents/profits equalize across all uses, accounting for differences in cultivation costs. Producers shift land to higher-value uses until equilibrium is achieved.\n\nStep 2: Apply 'effectual demand' analysis. If vineyard demand exceeds supply at natural price levels (covering rent/wages/profit at standard rates), prices should rise to incentivize land conversion to vineyards. The ban prevents this supply response.\n\nStep 3: Identify distortions. By prohibiting new vineyards, France artificially restricts supply despite effectual demand. This:\n- Maintains land in lower-value crops/pasture below natural equilibrium\n- Creates unmet demand for wine (reduced consumer welfare)\n- Allows existing vineyards to collect excess rents (prices rise above natural levels)\n\nStep 4: Welfare implications. Resources remain misallocated because:\n- Producers can't pursue higher returns from viticulture\n- Consumers face higher wine prices and/or shortages\n- Economic surplus diminishes due to prevented mutually beneficial exchanges\n\nThis violates Smith's natural adjustment mechanism, creating deadweight loss through artificial constraints on land use flexibility.\n\nQID: textbook-112-8-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-8-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly applies Smith's concepts of 'effectual demand' and 'natural proportion' to analyze the welfare implications of France's 1731 ban on new vineyards. It correctly identifies the distortions in resource allocation, the impact on consumer and producer welfare, and the resulting deadweight loss, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-8-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly applies Smith's concepts of 'effectual demand' and 'natural proportion' to analyze the welfare implications of France's 1731 ban on new vineyards. It correctly identifies the distortions in resource allocation, the impact on consumer and producer welfare, and the resulting deadweight loss, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-112-8-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly applies Smith's concepts of 'effectual demand' and 'natural proportion' to analyze the welfare implications of France's 1731 ban on new vineyards. It correctly identifies the distortions in resource allocation, the impact on consumer and producer welfare, and the resulting deadweight loss, aligning well with the gold answer."}
{"script_processing_qid": "textbook-64-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. How does the reduced form covariance matrix $\\Omega$ relate to the structural parameters $\\Gamma$ and $\\Sigma$?\n\nGOLD_ANSWER:\n1.  **Relation**: The reduced form covariance matrix is given by $\\Omega = (\\Gamma^{-1})'\\Sigma(\\Gamma^{-1})$. This matrix captures the variances and covariances of the reduced form disturbances, which are functions of the structural parameters $\\Gamma$ and $\\Sigma$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the fundamental relationship between reduced form and structural parameters in simultaneous equations models. The reduced form covariance matrix $\\Omega$ is derived from the structural parameters through the relationship:\n\n$\\Omega = \\Gamma^{-1} \\Sigma (\\Gamma^{-1})'$\n\nStep 2: Breaking this down:\n- $\\Gamma$ is the matrix of structural coefficients for endogenous variables\n- $\\Sigma$ is the covariance matrix of structural errors\n- The inverse of $\\Gamma$ transforms the structural errors into reduced form errors\n- The covariance matrix $\\Omega$ therefore depends on both the structural parameter matrix $\\Gamma$ and the structural error covariance $\\Sigma$ through this transformation.\n\nStep 3: This relationship holds regardless of identification status, but identification conditions (like the exclusion restrictions mentioned in context) ensure we can uniquely recover $\\Gamma$ and $\\Sigma$ from $\\Omega$.\n\nQID: textbook-64-0-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-64-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the relationship between the reduced form covariance matrix Ω and the structural parameters Γ and Σ, including the correct formula Ω = Γ⁻¹Σ(Γ⁻¹)′ and a clear explanation of each component's role.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-64-0-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the relationship between the reduced form covariance matrix Ω and the structural parameters Γ and Σ, including the correct formula Ω = Γ⁻¹Σ(Γ⁻¹)′ and a clear explanation of each component's role."}, "llm_echoed_qid": "textbook-64-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the relationship between the reduced form covariance matrix Ω and the structural parameters Γ and Σ, including the correct formula Ω = Γ⁻¹Σ(Γ⁻¹)′ and a clear explanation of each component's role."}
{"script_processing_qid": "textbook-84-0-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Interpret the education coefficient in the Poisson regression model. How does it affect the expected number of cigarettes smoked?\n\nGOLD_ANSWER:\n1. **Coefficient**: The education coefficient is -0.060.\n2. **Interpretation**: One additional year of education reduces the expected number of cigarettes smoked by approximately 6.0% (since $e^{-0.060} \\approx 0.94$, a 6% decrease).\n3. **Statistical Significance**: The robust $t$ statistic (over 3 in absolute value) indicates this effect is statistically significant.\n\nCANDIDATE_ANSWER:\nStep 1: In a Poisson regression model, coefficients represent the log of the expected count ratio per one-unit increase in the predictor. For education, the coefficient β_educ indicates log(cigs | educ + 1) − log(cigs | educ) = β_educ.\n\nStep 2: Exponentiating the education coefficient (e^{β_educ}) gives the multiplicative effect on the expected number of cigarettes smoked. For example, if β_educ = 0.05, a one-year increase in education multiplies the expected count by e^{0.05} ≈ 1.051 (a 5.1% increase).\n\nStep 3: Directionality: A positive β_educ implies education increases cigarette consumption, while a negative coefficient implies a reduction. However, the provided context does not specify the numerical value or sign of the education coefficient, so the qualitative interpretation depends on the unreported estimate.\n\nStep 4: Overdispersion (σ ≈ 4.54) does not affect the coefficient interpretation itself but invalidates standard errors. The education coefficient’s estimated effect on the expected count remains valid as long as the conditional mean is correctly specified, even with overdispersion.\n\nQID: textbook-84-0-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-84-0-1-2\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's explanation of Poisson regression coefficients and their interpretation is technically correct. However, the candidate does not directly address the specific education coefficient value (-0.060) and its 6% reduction effect mentioned in the gold answer, instead discussing a hypothetical positive coefficient. The reasoning is sound but does not fully align with the specific context provided in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-84-0-1-2", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's explanation of Poisson regression coefficients and their interpretation is technically correct. However, the candidate does not directly address the specific education coefficient value (-0.060) and its 6% reduction effect mentioned in the gold answer, instead discussing a hypothetical positive coefficient. The reasoning is sound but does not fully align with the specific context provided in the gold answer."}, "llm_echoed_qid": "textbook-84-0-1-2", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's explanation of Poisson regression coefficients and their interpretation is technically correct. However, the candidate does not directly address the specific education coefficient value (-0.060) and its 6% reduction effect mentioned in the gold answer, instead discussing a hypothetical positive coefficient. The reasoning is sound but does not fully align with the specific context provided in the gold answer."}
{"script_processing_qid": "textbook-114-23-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze the long-run implications of negative externalities for industry structure, particularly the entry and exit of firms.\n\nGOLD_ANSWER:\n1.  **Private vs. Social Cost**: Negative externalities imply $AC_{private} < AC_{social}$.\n2.  **Entry/Exit**: Firms enter if $P > AC_{private}$, even if $P < AC_{social}$.\n3.  **Inefficiency**: Too many firms remain in the industry, leading to excess production and pollution.\n4.  **Policy Need**: Taxes or regulations must align private and social costs to ensure efficient industry size.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the efficient emissions level (E* = 12 units) where marginal social cost equals marginal abatement cost, minimizing total societal costs. Step 2: Recognize that policies like emissions standards, fees, and permits aim to align private incentives with social costs. Step 3: In the long run, these policies affect entry/exit decisions: (a) Emissions standards act as a fixed constraint, potentially forcing inefficient firms to exit if compliance costs exceed revenues; new entrants must meet the standard from inception. (b) Emissions fees internalize external costs, raising average costs for all firms—marginal firms with higher abatement costs may exit, while only firms capable of efficient abatement (or absorbing fees) enter. (c) Transferable permits create market-driven abatement incentives, allowing efficient firms to profit by selling permits while inefficient firms face rising operational costs (from buying permits), leading to industry consolidation around firms with lowest abatement costs. Step 4: All three mechanisms ultimately reshape industry structure toward firms that can operate efficiently at E*, reducing excess capacity from unregulated negative externalities.\n\nQID: textbook-114-23-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-23-1-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer focuses on emissions policies and their impact on firm entry/exit, which is relevant but does not directly address the core points in the gold answer about private vs. social costs and inefficiency. The reasoning is correct but mismatched with the gold answer's emphasis on cost disparities and policy needs.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-23-1-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer focuses on emissions policies and their impact on firm entry/exit, which is relevant but does not directly address the core points in the gold answer about private vs. social costs and inefficiency. The reasoning is correct but mismatched with the gold answer's emphasis on cost disparities and policy needs."}, "llm_echoed_qid": "textbook-114-23-1-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer focuses on emissions policies and their impact on firm entry/exit, which is relevant but does not directly address the core points in the gold answer about private vs. social costs and inefficiency. The reasoning is correct but mismatched with the gold answer's emphasis on cost disparities and policy needs."}
{"script_processing_qid": "textbook-112-16-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain Smith's distinction between 'fixed residence' capitals (agriculture, retail) and 'mobile' capitals (wholesale, manufacturing). How does this affect national wealth?\n\nGOLD_ANSWER:\n-  **Fixed Capitals**: \n   - Agriculture/retail tied to local geography (farmland, shops). \n   - Directly employs domestic labor; value added remains within society. \n   - Example: Scottish wool processed in Yorkshire due to local capital constraints.\n-  **Mobile Capitals**: \n   - Wholesale trade is footloose (e.g., British merchants exporting Baltic flax). \n   - Manufacturing may relocate (e.g., Lyons' silk production distant from raw materials). \n   - **Wealth Impact**: Fixed capitals ensure domestic employment/value retention; mobile capitals risk leakage (e.g., profits repatriated by foreign merchants).\n\nCANDIDATE_ANSWER:\nStep 1: Smith distinguishes 'fixed residence' capitals (agriculture, retail) from 'mobile' capitals (wholesale) based on their physical location constraints. Fixed capitals are tied to specific local sites (e.g., farms, shops), while mobile capitals can relocate freely to maximize profit.\nStep 2: Agricultural capital is uniquely productive because it employs not only human labor but also livestock, and nature itself contributes value without cost. Retail capital, though fixed, lacks agriculture’s natural productivity boost.\nStep 3: Fixed capitals enhance national wealth more directly: (a) They employ domestic productive labor (farmers, shop workers, transport laborers), keeping economic activity within the country. (b) Agriculture’s partnership with nature amplifies the value added to annual produce.\nStep 4: Mobile capitals (e.g., wholesale) may divert resources abroad or prioritize profit-seeking over domestic value creation, reducing their relative contribution to national wealth. Step 5: Smith argues that prioritizing fixed agricultural capital maximizes national wealth, as it stimulates more productive labor and value addition per unit of capital than mobile or manufacturing alternatives.\n\nQID: textbook-112-16-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-16-0-1\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies Smith's distinction between fixed and mobile capitals and their impact on national wealth. However, the reasoning includes additional details about agriculture's productivity and nature's contribution, which are not explicitly mentioned in the gold answer. This mismatch in reasoning, while not incorrect, deviates from the focus of the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-16-0-1", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies Smith's distinction between fixed and mobile capitals and their impact on national wealth. However, the reasoning includes additional details about agriculture's productivity and nature's contribution, which are not explicitly mentioned in the gold answer. This mismatch in reasoning, while not incorrect, deviates from the focus of the gold answer."}, "llm_echoed_qid": "textbook-112-16-0-1", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies Smith's distinction between fixed and mobile capitals and their impact on national wealth. However, the reasoning includes additional details about agriculture's productivity and nature's contribution, which are not explicitly mentioned in the gold answer. This mismatch in reasoning, while not incorrect, deviates from the focus of the gold answer."}
{"script_processing_qid": "textbook-114-2-3-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Assess the statistical significance of the coefficient for $\\mathrm{FEU}$ given its t-statistic of 118.57 / SE($\\mathrm{FEU}$).\n\nGOLD_ANSWER:\n1.  **Assumption**: The t-statistic is calculated as $t = \\frac{\\hat{b}}{\\mathrm{SE}(\\hat{b})}$.\n2.  **Interpretation**: Since $t > 1.96$, the coefficient for $\\mathrm{FEU}$ is statistically significant at the 95% confidence level, indicating a strong positive effect on coal demand.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the relevant critical value for statistical significance.\nThe context states that 'each coefficient is statistically significantly different from zero because the t-statistics are all greater than 1.96 in absolute value.' This implies a standard 5% significance level (two-tailed test) with critical value ±1.96.\n\nStep 2: Compare the FEU t-statistic to the critical value.\nThe t-statistic for FEU is 118.57, which is substantially greater than 1.96 in absolute value.\n\nStep 3: Conclude significance.\nSince 118.57 > 1.96, we reject the null hypothesis that the FEU coefficient equals zero. The coefficient is statistically significant at the 5% level (and all conventional significance levels).\n\nQID: textbook-114-2-3-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-2-3-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the critical value, compares the t-statistic to it, and concludes the statistical significance of the FEU coefficient, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-2-3-2", "category": "CORRECT", "explanation": "The candidate correctly identifies the critical value, compares the t-statistic to it, and concludes the statistical significance of the FEU coefficient, aligning with the gold answer."}, "llm_echoed_qid": "textbook-114-2-3-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly identifies the critical value, compares the t-statistic to it, and concludes the statistical significance of the FEU coefficient, aligning with the gold answer."}
{"script_processing_qid": "textbook-122-20-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the efficiency of pooled OLS and fixed effects estimators under different correlation structures of $a_i$ and $union_{it}$.\n\nGOLD_ANSWER:\n4. **Efficiency Comparison**:\n   - If $a_i$ is uncorrelated with $union_{it}$, pooled OLS is more efficient because it uses both within and between variation.\n   - If $a_i$ is correlated with $union_{it}$, fixed effects is consistent but may be less efficient due to loss of between-group variation. Robust standard errors can mitigate inefficiency.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the key difference between estimators\n- Pooled OLS ignores the unobserved effect $a_i$ (leaves it in the error term)\n- Fixed effects (FE) eliminates $a_i$ through within-transformation\n\nStep 2: Analyze efficiency under positive correlation ($a_i$ and $union_{it}$)\n- Pooled OLS suffers from omitted variable bias: upward bias in union coefficient\n- FE produces consistent estimator by removing $a_i$\n- Efficiency comparison: FE estimators generally have larger variance than Pooled OLS (due to using only within-variation)\n\nStep 3: Compare efficiency trade-offs\n1. When $a_i$ is correlated with $union_{it}$:\n   - Pooled OLS: biased & inconsistent but smaller variance\n   - FE: unbiased & consistent but larger variance\n   - FE preferred despite lower efficiency (consistency > efficiency)\n\n2. If $a_i$ and $union_{it}$ were uncorrelated:\n   - Pooled OLS would be efficient (BLUE under Gauss-Markov)\n   - FE would be inefficient (unnecessarily removes time-constant variation)\n\nStep 4: Conclusion\nUnder positive correlation (as in context), FE sacrifices efficiency to achieve consistency. Pooled OLS appears more efficient but produces biased estimates. True efficiency comparison depends on validity of exogeneity assumption.\n\nQID: textbook-122-20-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-20-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the efficiency comparison between pooled OLS and fixed effects estimators under different correlation structures of $a_i$ and $union_{it}$, aligning with the gold answer. It correctly identifies the conditions under which each estimator is more efficient and the trade-offs involved, including the impact of correlation on bias and consistency.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-20-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the efficiency comparison between pooled OLS and fixed effects estimators under different correlation structures of $a_i$ and $union_{it}$, aligning with the gold answer. It correctly identifies the conditions under which each estimator is more efficient and the trade-offs involved, including the impact of correlation on bias and consistency."}, "llm_echoed_qid": "textbook-122-20-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the efficiency comparison between pooled OLS and fixed effects estimators under different correlation structures of $a_i$ and $union_{it}$, aligning with the gold answer. It correctly identifies the conditions under which each estimator is more efficient and the trade-offs involved, including the impact of correlation on bias and consistency."}
{"script_processing_qid": "textbook-118-19-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the asymptotic distribution of the Hansen-Sargan test statistic $Q(\\hat{\\pmb{\\theta}}, \\pmb{y})$ under the null hypothesis of valid overidentifying restrictions.\n\nGOLD_ANSWER:\n1.  **Objective**: Show that $Q(\\hat{\\pmb{\\theta}}, \\pmb{y})$ is asymptotically distributed as $\\chi^{2}(l-k)$.\n2.  **Taylor Expansion**: Use a Taylor expansion to approximate $P\\_{A}\\varPsi^{\\top}f(\\hat{\\theta})$ around $\\theta_0$:\n    $$\n    P\\_{A}\\varPsi^{\\top}f(\\hat{\\theta}) \\stackrel{a}{=} P\\_{A}\\varPsi^{\\top}f_0 + n^{-1/2}P\\_{A}\\varPsi^{\\top}F_0 n^{1/2}(\\hat{\\theta} - \\theta_0).\n    $$\n3.  **Substitute FOC**: Substitute the first-order condition for $\\hat{\\theta}$:\n    $$\n    P\\_{A}\\varPsi^{\\top}f(\\hat{\\theta}) \\stackrel{a}{=} P\\_{A}\\varPsi^{\\top}f_0 - n^{-1/2}P\\_{A}\\varPsi^{\\top}F_0 \\left(\\frac{1}{n}F_0^{\\top}\\varPsi P\\_{A}\\varPsi^{\\top}F_0\\right)^{-1} n^{-1/2}F_0^{\\top}\\varPsi P\\_{A}\\varPsi^{\\top}f_0.\n    $$\n4.  **Projection Matrix**: Recognize that this can be written as:\n    $$\n    P\\_{A}\\varPsi^{\\top}f(\\hat{\\theta}) \\stackrel{a}{=} (\\mathbf{I} - P\\_{P\\_{A}\\varPsi^{\\top}F_0})P\\_{A}\\varPsi^{\\top}f_0.\n    $$\n5.  **Quadratic Form**: The minimized criterion function is:\n    $$\n    Q(\\hat{\\theta}, y) = f^{\\top}(\\hat{\\theta})\\varPsi P\\_{A}\\varPsi^{\\top}f(\\hat{\\theta}) \\stackrel{a}{=} f_0^{\\top}\\varPsi (P\\_{A} - P\\_{P\\_{A}\\varPsi^{\\top}F_0})\\varPsi^{\\top}f_0.\n    $$\n6.  **Asymptotic Distribution**: Since $P\\_{A} - P\\_{P\\_{A}\\varPsi^{\\top}F_0}$ is an orthogonal projection matrix of rank $l-k$, $Q(\\hat{\\theta}, y)$ is asymptotically $\\chi^{2}(l-k)$.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that the Hansen-Sargan statistic $Q(\\hat{\\pmb{\\theta}}, \\pmb{y})$ corresponds to the quadratic form $\\pmb{f}^\\top(\\hat{\\theta})\\varPsi P_A\\varPsi^\\top\\pmb{f}(\\hat{\\theta})$. Through Taylor expansion and substitution of the asymptotic expansion for $n^{1/2}(\\hat{\\pmb{\\theta}} - \\pmb{\\theta}_0)$, this is shown to be approximately equal to $\\pmb{f}_0^\\top\\varPsi\\big(P_A - P_{P_A\\varPsi^\\top F_0}\\big)\\varPsi^\\top\\pmb{f}_0$.\n\nStep 2: Note that $P_A - P_{P_A\\varPsi^\\top F_0}$ is an orthogonal projection matrix with rank $l - k$ because $\\mathcal{S}(P_A\\varPsi^\\top F_0) \\subseteq \\mathcal{S}(A)$ (where $l$ = number of instruments and $k$ = number of parameters). The rank reduction by $k$ accounts for parameter estimation.\n\nStep 3: Under the null hypothesis of valid overidentifying restrictions, $n^{-1/2}\\varPsi^\\top\\pmb{f}_0$ is asymptotically normal with covariance matrix proportional to $\\varPsi^\\top\\varPsi$. The quadratic form of a normal vector with a rank-$m$ projection matrix follows a $\\chi^2(m)$ distribution.\n\nStep 4: Conclude that $Q(\\hat{\\pmb{\\theta}}, \\pmb{y})$ converges in distribution to $\\chi^2(l - k)$ as $n \\to \\infty$.\n\nQID: textbook-118-19-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-19-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the asymptotic distribution of the Hansen-Sargan test statistic as $\\chi^2(l - k)$ under the null hypothesis of valid overidentifying restrictions. The steps align with the gold answer, including the use of Taylor expansion, recognition of the orthogonal projection matrix, and the conclusion about the asymptotic distribution.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-19-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the asymptotic distribution of the Hansen-Sargan test statistic as $\\chi^2(l - k)$ under the null hypothesis of valid overidentifying restrictions. The steps align with the gold answer, including the use of Taylor expansion, recognition of the orthogonal projection matrix, and the conclusion about the asymptotic distribution."}, "llm_echoed_qid": "textbook-118-19-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the asymptotic distribution of the Hansen-Sargan test statistic as $\\chi^2(l - k)$ under the null hypothesis of valid overidentifying restrictions. The steps align with the gold answer, including the use of Taylor expansion, recognition of the orthogonal projection matrix, and the conclusion about the asymptotic distribution."}
{"script_processing_qid": "textbook-123-12-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. How does the recursive formulation of the Pareto problem simplify the analysis of dynamic economies with uncertainty?\n\nGOLD_ANSWER:\nThe recursive formulation reduces the infinite-dimensional problem of choosing sequences of state-contingent allocations to a sequence of static problems where the planner chooses current consumption and continuation values. This simplification leverages the principle of dynamic programming, allowing the use of value functions and envelope conditions to characterize optimal policies. The recursive approach also makes it computationally tractable to solve for optimal allocations in dynamic stochastic economies.\n\nCANDIDATE_ANSWER:\nStep 1: The recursive formulation reduces the infinite-dimensional problem of choosing entire consumption paths to a recursive state variable (promised utility v). This allows dynamic programming methods to be applied.\n\nStep 2: By expressing constraints through continuation values (w_i), it handles uncertainty through Markov transitions (Π_i) rather than requiring explicit history dependence. The i.i.d. shock structure simplifies state transitions.\n\nStep 3: The first-order conditions and envelope condition (P'(v) = -θ) characterize optimal allocations through a time-invariant policy function that maps current state (v) and shock (i) to consumption (c_i) and next-period promises (w_i).\n\nStep 4: This avoids solving a complex sequence problem with history-dependent constraints, instead maintaining only one forward-looking promise-keeping constraint through v and its recursive update via w_i.\n\nQID: textbook-123-12-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-12-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately breaks down the recursive formulation's benefits into clear steps, matching the gold answer's emphasis on dimensionality reduction, dynamic programming, and computational tractability. The technical details (promised utility, Markov transitions, policy functions) correctly elaborate on the gold answer's points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-12-1-2", "category": "CORRECT", "explanation": "The candidate answer accurately breaks down the recursive formulation's benefits into clear steps, matching the gold answer's emphasis on dimensionality reduction, dynamic programming, and computational tractability. The technical details (promised utility, Markov transitions, policy functions) correctly elaborate on the gold answer's points."}, "llm_echoed_qid": "textbook-123-12-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately breaks down the recursive formulation's benefits into clear steps, matching the gold answer's emphasis on dimensionality reduction, dynamic programming, and computational tractability. The technical details (promised utility, Markov transitions, policy functions) correctly elaborate on the gold answer's points."}
{"script_processing_qid": "textbook-122-13-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Derive the variance of the OLS estimator $\\hat{\\beta}\\_{j}$ under the Gauss-Markov assumptions, and explain the role of $R\\_{j}^{2}$ in this expression.\n\nGOLD_ANSWER:\n1.  **Variance Formula**: Under the Gauss-Markov assumptions, the variance of $\\hat{\\beta}\\_{j}$ is: $$\\mathrm{Var}(\\hat{\\beta}\\_{j})=\\frac{\\sigma^{2}}{\\mathrm{SST}\\_{j}(1-R\\_{j}^{2})}.$$\n2.  **Role of $R\\_{j}^{2}$**: $R\\_{j}^{2}$ measures the proportion of variation in $X\\_{j}$ explained by the other independent variables. A high $R\\_{j}^{2}$ (multicollinearity) inflates the variance of $\\hat{\\beta}\\_{j}$, reducing the precision of the estimator.\n3.  **Interpretation**: The term $(1-R\\_{j}^{2})$ captures the unique variation in $X\\_{j}$ not explained by other regressors, which is crucial for precise estimation.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the key Gauss-Markov assumptions required for OLS variance derivation - particularly MLR.5 (homoskedasticity) which states Var(u|x₁,...,xₖ) = σ².\n\nStep 2: Start with the general formula for OLS estimator variance in matrix form: Var(β̂_j) = σ²[𝑿'𝑿]⁻¹_jj where [𝑿'𝑿]⁻¹_jj is the j-th diagonal element of the inverse matrix.\n\nStep 3: Recognize that for a single coefficient β_j, this simplifies to Var(β̂_j) = σ²/(SST_j(1 - R_j²)) through algebraic manipulation. Here, SST_j = Σ(x_ij - x̄_j)² measures total variation in x_j.\n\nStep 4: Understand R_j² represents the proportion of variation in x_j explained by other regressors in the model. The term (1 - R_j²) captures remaining unexplained variation in x_j after partialling out other variables.\n\nStep 5: Observe that as R_j² increases (stronger multicollinearity), the denominator (1 - R_j²) decreases, leading to larger variance. This shows how collinearity inflates estimator variance.\n\nRole of R_j²: Acts as a variance inflation factor. Higher R_j² (more linear dependence between x_j and other regressors) reduces the effective independent variation in x_j, increasing Var(β̂_j).\n\nQID: textbook-122-13-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-13-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derives the variance of the OLS estimator under the Gauss-Markov assumptions and accurately explains the role of $R\\_{j}^{2}$ in inflating the variance due to multicollinearity. The steps and reasoning align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-13-0-3", "category": "CORRECT", "explanation": "The candidate correctly derives the variance of the OLS estimator under the Gauss-Markov assumptions and accurately explains the role of $R\\_{j}^{2}$ in inflating the variance due to multicollinearity. The steps and reasoning align with the gold answer."}, "llm_echoed_qid": "textbook-122-13-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derives the variance of the OLS estimator under the Gauss-Markov assumptions and accurately explains the role of $R\\_{j}^{2}$ in inflating the variance due to multicollinearity. The steps and reasoning align with the gold answer."}
{"script_processing_qid": "textbook-107-3-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. How does WARP handle the case where income changes but prices remain constant?\n\nGOLD_ANSWER:\nWARP still applies: if bundle $x$ is chosen over $y$ at income $I_1$, and $y$ is chosen at $I_2 > I_1$, then $x$ must not be affordable at $I_2$ (else violation). This ensures monotonicity in choices.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that WARP requires consistency in revealed preferences. If a consumer chooses bundle y when x is affordable, they cannot later choose x when y is still affordable (assuming unchanged preferences).\n\nStep 2: In the described scenario, Rudolf initially chose y over x when both were affordable. Later, when he chooses x, WARP imposes that y must no longer be affordable under the new constraints.\n\nStep 3: If prices remain constant but income changes, the only way for y to become unaffordable when x is chosen is if income decreased sufficiently to place y outside the new budget set. This ensures no violation of WARP: x is only chosen when y is excluded from the feasible set due to the income reduction.\n\nQID: textbook-107-3-2-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-107-3-2-2\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies that WARP ensures consistency in choices and that y must become unaffordable for x to be chosen. However, the reasoning incorrectly assumes income decreases, whereas the gold answer specifies income increases (I_2 > I_1).\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-3-2-2", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies that WARP ensures consistency in choices and that y must become unaffordable for x to be chosen. However, the reasoning incorrectly assumes income decreases, whereas the gold answer specifies income increases (I_2 > I_1)."}, "llm_echoed_qid": "textbook-107-3-2-2", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies that WARP ensures consistency in choices and that y must become unaffordable for x to be chosen. However, the reasoning incorrectly assumes income decreases, whereas the gold answer specifies income increases (I_2 > I_1)."}
{"script_processing_qid": "textbook-108-6-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What are the limitations of the exchange-rate effect in influencing aggregate demand?\n\nGOLD_ANSWER:\n1. **Capital Mobility**: Assumes perfect capital mobility, which may not hold in all cases.\n2. **Time Lags**: Exchange rate adjustments may take time to affect trade flows.\n3. **Elasticities**: Depends on the price elasticities of demand for exports and imports.\n\nCANDIDATE_ANSWER:\nThe provided context explains the mechanism through which the exchange-rate effect influences aggregate demand but does not explicitly discuss its limitations. Therefore, based solely on the given information, there is insufficient detail to determine the specific limitations of the exchange-rate effect in this context.\n\nQID: textbook-108-6-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-108-6-1-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it states the provided context does not explicitly discuss the limitations.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-6-1-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it states the provided context does not explicitly discuss the limitations."}, "llm_echoed_qid": "textbook-108-6-1-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it states the provided context does not explicitly discuss the limitations."}
{"script_processing_qid": "textbook-115-16-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the test for sample selection bias proposed by Nijman and Verbeek (1992) in the context of fixed effects estimation. Explain the null hypothesis and the interpretation of the test statistic.\n\nGOLD_ANSWER:\n1. **Test Derivation**: \n   - Add the lagged selection indicator $s_{i,t-1}$ to the fixed effects model: $$y_{it} = \\mathbf{x}_{it}\\beta + c_i + \\gamma s_{i,t-1} + u_{it}.$$ \n   - Estimate the model using fixed effects on the unbalanced panel. \n   - Perform a $t$-test on $\\gamma$ (robust if necessary). \n2. **Null Hypothesis**: $H_0: \\gamma = 0$, implying no correlation between $u_{it}$ and $s_{ir}$ for any $r$. \n3. **Interpretation**: Rejection of $H_0$ suggests sample selection bias due to correlation between idiosyncratic errors and selection.\n\nCANDIDATE_ANSWER:\nStep 1: Model Specification\nInclude the lagged selection indicator \\( s_{i,t-1} \\) as an additional regressor in the fixed effects model: \n\\[ y_{it} = \\mathbf{x}_{it}\\pmb{\\beta} + c_i + \\delta s_{i,t-1} + u_{it} \\]\nwhere \\( c_i \\) is the unit-specific fixed effect.\n\nStep 2: Estimation\nEstimate the augmented model using fixed effects (FE) on the unbalanced panel (accounts for selection-induced missing data).\n\nStep 3: Hypothesis Testing\n- Null Hypothesis (\\( H_0 \\)): \\( \\delta = 0 \\), which implies \\( u_{it} \\) is uncorrelated with \\( s_{ir} \\) for all \\( r \\).\n- Alternative Hypothesis (\\( H_1 \\)): \\( \\delta \\neq 0 \\), indicating sample selection bias.\n\nStep 4: Test Statistic\nCompute the \\( t \\)-statistic for \\( \\delta \\) from the FE regression. Use robust standard errors to account for potential heteroskedasticity or serial correlation.\n\nStep 5: Interpretation\n- Reject \\( H_0 \\) if the \\( t \\)-statistic is statistically significant (e.g., \\( |t| > 1.96 \\) at 5% significance). This suggests that previous-period selection (\\( s_{i,t-1} \\)) predicts current outcomes (\\( y_{it} \\)), violating exogeneity and indicating sample selection bias.\n- Failure to reject \\( H_0 \\) implies no evidence of selection bias under the test’s assumptions.\n\nQID: textbook-115-16-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-16-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately follows the steps outlined in the gold answer, including model specification, estimation, hypothesis testing, and interpretation. The null hypothesis and test statistic are correctly described, and the explanation of the test's implications aligns with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-16-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately follows the steps outlined in the gold answer, including model specification, estimation, hypothesis testing, and interpretation. The null hypothesis and test statistic are correctly described, and the explanation of the test's implications aligns with the gold answer."}, "llm_echoed_qid": "textbook-115-16-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately follows the steps outlined in the gold answer, including model specification, estimation, hypothesis testing, and interpretation. The null hypothesis and test statistic are correctly described, and the explanation of the test's implications aligns with the gold answer."}
{"script_processing_qid": "textbook-41-0-3-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that the aggregate excess demand function is homogeneous of degree zero in prices.\n\nGOLD_ANSWER:\n1. **Homogeneity of Degree Zero**: For any $\\lambda > 0$, $\\mathbf{z}(\\lambda \\mathbf{p}) = \\mathbf{z}(\\mathbf{p})$. This follows because individual demands are homogeneous of degree zero, and aggregation preserves this property.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that homogeneity of degree zero means scaling all prices by a positive constant λ > 0 does not change the function’s value.\nStep 2: The context states that homogeneity of the aggregate excess demand function was established in preceding paragraphs. This implies that for any price vector **p** and λ > 0, **z(λp) = z(p)**.\nStep 3: By definition, this equality confirms that the aggregate excess demand function is homogeneous of degree zero in prices, as scaling prices has no effect on the aggregate excess demand.\nConclusion: The proof follows directly from the established homogeneity property referenced in the context.\n\nQID: textbook-41-0-3-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-41-0-3-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies and explains the homogeneity of degree zero property of the aggregate excess demand function, aligning with the gold answer's reasoning and conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-41-0-3-0", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies and explains the homogeneity of degree zero property of the aggregate excess demand function, aligning with the gold answer's reasoning and conclusion."}, "llm_echoed_qid": "textbook-41-0-3-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies and explains the homogeneity of degree zero property of the aggregate excess demand function, aligning with the gold answer's reasoning and conclusion."}
{"script_processing_qid": "textbook-119-24-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain why the chi-square distribution is used to construct confidence intervals for $\\sigma^{2}$ under the normality assumption.\n\nGOLD_ANSWER:\nUnder the normality assumption, the quantity $(n-2)\\frac{\\hat{\\sigma}^{2}}{\\sigma^{2}}$ follows a chi-square distribution with $n-2$ degrees of freedom. This property allows us to use the chi-square distribution to derive the confidence interval for $\\sigma^{2}$.\n\nCANDIDATE_ANSWER:\nStep 1: Under the normality assumption, the key distributional result is that $(n-2)\\frac{\\hat{\\sigma}^2}{\\sigma^2}$ follows a $\\chi^2$ distribution with $n-2$ degrees of freedom. This arises because the sum of squared residuals (which forms $\\hat{\\sigma}^2$) in a normal linear model has a scaled $\\chi^2$ distribution.\n\nStep 2: The $\\chi^2$ distribution is inherently linked to variance estimation for normal data. Its asymmetry and dependence on degrees of freedom make it the natural candidate for constructing confidence intervals around $\\sigma^2$.\n\nStep 3: The probability statement $\\Pr\\left(\\chi_{1-\\alpha/2}^2 \\leq (n-2)\\frac{\\hat{\\sigma}^2}{\\sigma^2} \\leq \\chi_{\\alpha/2}^2\\right) = 1-\\alpha$ uses critical values from the $\\chi^2$ distribution to bound the pivotal quantity. Rearranging this inequality isolates $\\sigma^2$, creating probability-based bounds that account for the sampling variability of the variance estimator under normality.\n\nStep 4: Without the normality assumption, this pivotal quantity would not follow a $\\chi^2$ distribution, making the method invalid. The $\\chi^2$ distribution specifically quantifies how the ratio of estimated-to-true variance behaves when errors are normal.\n\nQID: textbook-119-24-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-24-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the use of the chi-square distribution for constructing confidence intervals for σ² under normality, matching the gold answer's key points and providing additional detailed reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-24-1-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the use of the chi-square distribution for constructing confidence intervals for σ² under normality, matching the gold answer's key points and providing additional detailed reasoning."}, "llm_echoed_qid": "textbook-119-24-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the use of the chi-square distribution for constructing confidence intervals for σ² under normality, matching the gold answer's key points and providing additional detailed reasoning."}
{"script_processing_qid": "textbook-19-0-3-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Analyze the 'fiscal theory of the price level' as an alternative to Ricardian equivalence.\n\nGOLD_ANSWER:\nThe fiscal theory asserts that the price level adjusts to satisfy the government's intertemporal budget constraint:\n1. **Non-Ricardian Regime**: Primary surpluses are exogenous; $P_t$ solves $B_{t-1}/P_t = E_t \\sum_{s=t}^\\infty \\frac{s_s}{\\prod_{k=t}^s (1 + i_k)}$.\n2. **Implications**: Fiscal shocks directly affect inflation; monetary policy alone cannot control $P_t$.\n\nCANDIDATE_ANSWER:\nStep 1: The provided context explicitly focuses on Ricardian equivalence (debt/deficit neutrality) and deviations caused by market imperfections like borrowing constraints. Step 2: The fiscal theory of the price level (FTPL) is not mentioned in the given context. Step 3: The context does not discuss alternative mechanisms (e.g., price-level adjustments via government budget constraints) or non-Ricardian regimes. Conclusion: The provided context does not contain enough information to analyze FTPL as an alternative to Ricardian equivalence.\n\nQID: textbook-19-0-3-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-19-0-3-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it states the provided context does not contain enough information to analyze FTPL.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-19-0-3-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it states the provided context does not contain enough information to analyze FTPL."}, "llm_echoed_qid": "textbook-19-0-3-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it states the provided context does not contain enough information to analyze FTPL."}
{"script_processing_qid": "textbook-115-1-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Describe the maximum score estimator and its applicability in panel data settings.\n\nGOLD_ANSWER:\nThe maximum score estimator, proposed by Manski (1975), estimates $\\beta$ up to scale under the assumption that the median of the error term is zero. It is robust to heteroskedasticity and distributional misspecification but converges at a slow rate ($N^{1/3}$) and is not suitable for estimating response probabilities directly.\n\nCANDIDATE_ANSWER:\nThe provided context does not explicitly discuss the maximum score estimator or its applicability in panel data settings. While the text outlines challenges with unobserved effects in linear probability models (restrictions on bounds) and parametric approaches like probit/logit (requiring transformations to remove unobserved heterogeneity), it does not mention the maximum score estimator. Therefore, insufficient information is available in the given context to address this question.\n\nQID: textbook-115-1-2-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-1-2-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information to address the question.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-1-2-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information to address the question."}, "llm_echoed_qid": "textbook-115-1-2-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information to address the question."}
{"script_processing_qid": "textbook-107-9-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Calculate the market price in the Stackelberg equilibrium for the given problem.\n\nGOLD_ANSWER:\n1. **Total Output**: $Q = 375$.\n2. **Market Price**: $P = 1,020 - 2(375) = 1,020 - 750 = 270$.\n\nCANDIDATE_ANSWER:\nStep 1: Determine the follower's reaction function. The follower maximizes profit by setting MR = MC. Market price is P = 1,020 - 2(Q_L + Q_F). The follower's MR = 1,020 - 2Q_L - 4Q_F. Setting MR = MC = $20 gives 1,020 - 2Q_L - 4Q_F = 20 ⇒ Q_F = (1,000 - 2Q_L)/4 = 250 - 0.5Q_L.\n\nStep 2: Substitute the follower's reaction into the leader's problem. Total output Q = Q_L + Q_F = Q_L + 250 - 0.5Q_L = 250 + 0.5Q_L. Market price becomes P = 1,020 - 2(250 + 0.5Q_L) = 520 - Q_L.\n\nStep 3: The leader maximizes profit by setting its MR = MC. Leader's MR = 520 - 2Q_L = 20 ⇒ Q_L = 250. Then Q_F = 250 - 0.5(250) = 125. Total output Q = 250 + 125 = 375.\n\nStep 4: Calculate market price using total output: P = 1,020 - 2(375) = 1,020 - 750 = $270.\n\nQID: textbook-107-9-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-9-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the market price in the Stackelberg equilibrium, matching the gold answer's calculation and final result of $270.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-9-1-3", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the market price in the Stackelberg equilibrium, matching the gold answer's calculation and final result of $270."}, "llm_echoed_qid": "textbook-107-9-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the market price in the Stackelberg equilibrium, matching the gold answer's calculation and final result of $270."}
{"script_processing_qid": "textbook-118-7-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Prove that the OLS estimator $\\hat{\\beta}$ is the Best Linear Unbiased Estimator (BLUE) under the Gauss-Markov assumptions.\n\nGOLD_ANSWER:\n1. **Linear Unbiased Estimators**: Any linear estimator $\\tilde{\\beta} = Ay$ with $AX = I$ is unbiased.\\n2. **Efficiency**: The OLS estimator minimizes the variance among all linear unbiased estimators, as shown by the Gauss-Markov Theorem.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain enough information to answer this question. While the section discusses properties of OLS estimators (bias, variance) and mentions the Gauss-Markov assumptions as validity conditions, the explicit proof steps demonstrating that OLS is BLUE – including comparisons with alternative linear unbiased estimators and variance minimization – are not included in the extracted text passages.\n\nQID: textbook-118-7-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-7-1-1\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide the explicit proof steps required to demonstrate that OLS is BLUE.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-7-1-1", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide the explicit proof steps required to demonstrate that OLS is BLUE."}, "llm_echoed_qid": "textbook-118-7-1-1", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide the explicit proof steps required to demonstrate that OLS is BLUE."}
{"script_processing_qid": "textbook-112-23-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. (Conceptual) Contrast Smith's critique of perpetual funding with modern sovereign debt practices. How does his analysis of 18th-century Britain inform current debates on fiscal sustainability?\n\nGOLD_ANSWER:\n1.  **Smith's Critique**: \n    - Perpetual funding removes temporal discipline, allowing infinite debt accumulation.\n    - Distorts incentives by prioritizing present exigencies over future liabilities (\"improvident spendthrift\" analogy).\n\n2.  **Modern Parallels**: \n    - Central bank independence and bond markets now discipline borrowing, unlike Smith's era.\n    - However, debt-to-GDP ratios mirror his warnings about \"ruinous practice.\"\n\n3.  **Key Insight**: Smith's emphasis on credible institutions (e.g., Bank of England's role) remains relevant for sovereign debt credibility today.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Smith’s critique of perpetual funding. Smith argues that perpetual funding (e.g., Britain’s 1711 South Sea Company arrangement) allows governments to avoid fiscal discipline by indefinitely mortgaging tax revenues to service debt interest, eliminating the need to repay principal or prioritize savings. This creates moral hazard, as governments 'dispense themselves from the duty of saving' due to easy borrowing access.\n\nStep 2: Contrast with modern sovereign debt practices. Unlike 18th-century perpetual funding, modern sovereign debt typically involves bonds with defined maturities and refinancing mechanisms. However, parallels exist in systemic reliance on continuous debt rollovers and high debt-to-GDP ratios, reflecting Smith’s warning about prioritizing borrowing over fiscal prudence.\n\nStep 3: Link to current fiscal sustainability debates. Smith’s analysis informs debates by highlighting the risks of complacency in debt management. His critique of perpetual funding as a tool to defer hard fiscal choices mirrors modern concerns about intergenerational equity, austerity trade-offs, and the sustainability of deficit-financed spending when debt servicing crowds out public investments or requires perpetual tax increases.\n\nQID: textbook-112-23-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-23-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately identifies Smith's critique of perpetual funding, contrasts it with modern sovereign debt practices, and links it to current fiscal sustainability debates, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-23-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately identifies Smith's critique of perpetual funding, contrasts it with modern sovereign debt practices, and links it to current fiscal sustainability debates, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-112-23-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately identifies Smith's critique of perpetual funding, contrasts it with modern sovereign debt practices, and links it to current fiscal sustainability debates, aligning well with the gold answer."}
{"script_processing_qid": "textbook-104-12-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Prove that a linear functional $\\phi$ on a normed vector space $S$ is continuous if and only if it is bounded.\n\nGOLD_ANSWER:\n1. **Bounded implies continuous**: If $\\phi$ is bounded, there exists $M$ such that $|\\phi(x)| \\leq M \\|x\\|$ for all $x \\in S$. For any sequence $x_n \\to x$, $|\\phi(x_n) - \\phi(x)| = |\\phi(x_n - x)| \\leq M \\|x_n - x\\| \\to 0$.\n2. **Continuous implies bounded**: If $\\phi$ is continuous at $0$, there exists $\\delta > 0$ such that $\\|x\\| < \\delta$ implies $|\\phi(x)| < 1$. For any $x \\neq 0$, let $y = \\frac{\\delta x}{2 \\|x\\|}$. Then $\\|y\\| < \\delta$, so $|\\phi(y)| < 1$. By linearity, $|\\phi(x)| < \\frac{2}{\\delta} \\|x\\|$, proving $\\phi$ is bounded.\n\nCANDIDATE_ANSWER:\nStep 1: (Forward Direction - Bounded ⇒ Continuous)\nAssume φ is bounded, i.e., ∃M > 0 such that |φ(x)| ≤ M||x||_S for all x ∈ S. For continuity, let x_n → x in S. Then ||x_n - x||_S → 0. By linearity and boundedness:\n  |φ(x_n) - φ(x)| = |φ(x_n - x)| ≤ M||x_n - x||_S → 0.\nThus φ is continuous.\n\nStep 2: (Converse - Continuous ⇒ Bounded)\nAssume φ is continuous. Since φ is linear and continuous at 0, ∃δ > 0 such that ||x||_S < δ ⇒ |φ(x)| < 1. For any x ≠ 0, let y = (δ/2)(x/||x||_S). Then ||y||_S = δ/2 < δ ⇒ |φ(y)| < 1. By linearity:\n  |φ(y)| = |(δ/2||x||_S)φ(x)| < 1 ⇒ |φ(x)| ≤ (2/δ)||x||_S.\nThus φ is bounded with M = 2/δ.\n\nQID: textbook-104-12-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-104-12-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly proves both directions of the statement. The reasoning and steps align precisely with the gold answer, demonstrating that a linear functional is continuous if and only if it is bounded.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-104-12-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly proves both directions of the statement. The reasoning and steps align precisely with the gold answer, demonstrating that a linear functional is continuous if and only if it is bounded."}, "llm_echoed_qid": "textbook-104-12-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly proves both directions of the statement. The reasoning and steps align precisely with the gold answer, demonstrating that a linear functional is continuous if and only if it is bounded."}
{"script_processing_qid": "textbook-74-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the empirical shortcomings of the expected-market-clearing approach and how the staggered contracts model addresses these limitations.\n\nGOLD_ANSWER:\n1.  **Expected-Market-Clearing Shortcomings**: Persistence of monetary shocks is limited to the longest price-setting lead time (e.g., one year), which is insufficient to explain observed persistence in output.\n2.  **Staggered Contracts Solution**: Overlapping contracts and autoregressive price setting introduce persistence beyond the contract length, explaining near unit-root behavior in output.\n3.  **Empirical Support**: Studies by West (1988) and Phaneuf (1990) show that staggered contracts can generate sufficient persistence to match data, including reverse dynamic cross-correlations between output and price levels.\n\nCANDIDATE_ANSWER:\nStep 1: Identify empirical shortcomings of the expected-market-clearing approach\n- Prices/wages are predetermined based on expectations formed at t-j, creating rigidity that prevents adjustments to new information during the contract period.\n- If actual supply/demand deviates from expectations (e.g., due to shocks), markets may fail to clear ex-post despite ex-ante alignment.\n- All agents reset prices simultaneously at fixed intervals, lacking realistic staggered adjustment patterns observed in real-world economies.\n\nStep 2: Explain how the staggered contracts model addresses these limitations\n1. Multi-period contracts: Prices are fixed for N > 1 periods (e.g., N=2 in the example), creating inherent persistence that matches observed macroeconomic inertia.\n2. Overlapping contracts: With p_t = 0.5(x_t + x_{t-1}), half of prices adjust each period, allowing gradual response to shocks while maintaining some rigidity.\n3. Forward-looking expectations: The price-setting rule x_t = 0.5(p_t + E_t p_{t+1}) + γy_t + γE_t y_{t+1} incorporates both current and expected future economic conditions, enabling partial adjustment to anticipated changes.\n4. Shock absorption: The ε_t term allows for contemporaneous disturbances while maintaining the staggered structure.\n5. Persistence mechanism: Overlapping contracts create endogenous propagation of shocks across periods, addressing the expected-market-clearing approach's inability to generate sustained real effects.\n\nQID: textbook-74-0-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-74-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly identifies the empirical shortcomings of the expected-market-clearing approach and effectively explains how the staggered contracts model addresses these limitations, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-74-0-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly identifies the empirical shortcomings of the expected-market-clearing approach and effectively explains how the staggered contracts model addresses these limitations, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-74-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly identifies the empirical shortcomings of the expected-market-clearing approach and effectively explains how the staggered contracts model addresses these limitations, aligning well with the gold answer."}
{"script_processing_qid": "textbook-103-3-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of the ARMA(2,1) parameters $(\\rho_1,\\rho_2,\\gamma)$ for the predictability of labor income.\n\nGOLD_ANSWER:\n1. **$\\rho_1,\\rho_2$**: Determine the autoregressive persistence of income. Higher values imply longer-lasting effects of past income.\n2. **$\\gamma$**: Captures the moving average component, reflecting short-term momentum or mean-reversion in income shocks.\n3. **Combined Effect**: The ARMA structure allows for flexible modeling of income dynamics, balancing persistence and transient shocks.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that the ARMA(2,1) parameters (ρ₁,ρ₂,γ) appear in both numerator and denominator of the final expectation expression:\n\nE_t[∑β^j y_{t+j}] = [β(1+γβ) - L - γL²] / [β - (1+ρ₁β)L + (ρ₁-ρ₂β)L² + ρ₂L³] w_t\n\nStep 2: Analyze numerator implications:\n- γ affects short-term shock transmission through β(1+γβ) term\n- Higher γ increases weight on recent shock w_t\n\nStep 3: Analyze denominator implications (characteristic equation):\n- Roots determine persistence of predictions\n- ρ₁ controls first-order autoregressive persistence\n- ρ₂ governs second-order autoregressive effects\n- Interaction ρ₁β and ρ₂β terms show how discounting β modifies persistence\n\nStep 4: Stability requirements:\n- For stationarity, roots of 1-ρ₁L-ρ₂L²=0 must lie outside unit circle\n- This ensures finite variance and convergence of predictions\n\nStep 5: Predictability trade-offs:\n- Higher ρ₁/ρ₂ → stronger autoregressive memory → more predictable persistent component\n- Higher γ → stronger moving average component → better short-term shock absorption\n- Parameter combinations determine whether income shocks decay quickly or create persistent effects\n\nStep 6: β interaction:\n- Discount factor β weights future periods\n- Larger β emphasizes long-run dynamics controlled by ρ₁ and ρ₂\n- Smaller β emphasizes short-run dynamics influenced by γ\n\nQID: textbook-103-3-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-103-3-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly analyzes the ARMA(2,1) parameters, detailing their effects on persistence, shock transmission, stability, and predictability, aligning well with the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-3-1-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly analyzes the ARMA(2,1) parameters, detailing their effects on persistence, shock transmission, stability, and predictability, aligning well with the gold answer's key points."}, "llm_echoed_qid": "textbook-103-3-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly analyzes the ARMA(2,1) parameters, detailing their effects on persistence, shock transmission, stability, and predictability, aligning well with the gold answer's key points."}
{"script_processing_qid": "textbook-118-8-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of failing the order or rank condition for policy analysis.\n\nGOLD_ANSWER:\n1. **Underidentification (order condition fails)**: No consistent estimator exists; policy effects cannot be disentangled.\n2. **Weak identification (rank condition fails asymptotically)**: Estimators are biased in finite samples, and inference is unreliable (e.g., weak IV bias).\n3. **Practical remedies**: Seek additional valid instruments or impose overidentifying restrictions (if possible). Sensitivity analyses (e.g., Hausman tests) are crucial.\n\nCANDIDATE_ANSWER:\nStep 1: Implications of Failing the Order Condition (l < k_i)\n- The order condition (l ≥ k₁ᵢ + k₂ᵢ) ensures sufficient instruments for identification. If violated:\n  1.1 The matrix P_WX_i cannot have full column rank, making parameter estimation impossible\n  1.2 The model becomes underidentified – multiple parameter sets could produce identical data\n  1.3 Policy analysis loses causal interpretation due to inability to disentangle simultaneous effects\n\nStep 2: Implications of Failing the Rank Condition\n- While the order condition is necessary, the rank condition (full column rank of P_WX_i) is sufficient for identification. If violated:\n  2.1 Even with sufficient instruments (l ≥ k_i), the instruments lack independent variation needed for identification\n  2.2 Asymptotic identification fails (plim n⁻¹X_iᵀP_WX_i singular) → inconsistent parameter estimates\n  2.3 Policy simulations become unreliable as structural parameters cannot be consistently estimated\n\nStep 3: Combined Policy Analysis Consequences\n- Both failures lead to:\n  3.1 Invalid counterfactual analysis (key requirement for policy evaluation)\n  3.2 Unreliable multiplier effects calculations in simultaneous equations\n  3.3 Inability to test policy effectiveness through structural parameters\n  3.4 Potential for paradoxical policy recommendations due to unidentified simultaneity biases\n\nQID: textbook-118-8-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-8-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the implications of failing both the order and rank conditions, aligning well with the gold answer. It provides detailed reasoning for each condition's failure and their combined effects on policy analysis, matching the gold answer's points about underidentification, weak identification, and practical consequences.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-8-1-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the implications of failing both the order and rank conditions, aligning well with the gold answer. It provides detailed reasoning for each condition's failure and their combined effects on policy analysis, matching the gold answer's points about underidentification, weak identification, and practical consequences."}, "llm_echoed_qid": "textbook-118-8-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the implications of failing both the order and rank conditions, aligning well with the gold answer. It provides detailed reasoning for each condition's failure and their combined effects on policy analysis, matching the gold answer's points about underidentification, weak identification, and practical consequences."}
{"script_processing_qid": "textbook-103-7-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the condition $\\tau \\leq y/2$ is necessary for the existence of an equilibrium with valued currency in the social security model.\n\nGOLD_ANSWER:\n1. **Savings Condition**: In an equilibrium with valued currency, $f[1+r(t)] > 0$.\n2. **Substitute $f[1+r(t)]$**: $\\frac{1}{2}\\left[(y-\\tau) - \\frac{\\tau}{1+r(t)}\\right] > 0$.\n3. **Inequality**: Rearranging yields $1+r(t) > \\frac{\\tau}{y-\\tau}$.\n4. **Boundedness**: Since $1+r(t) \\leq 1$ in equilibrium, $\\frac{\\tau}{y-\\tau} \\leq 1 \\implies \\tau \\leq y/2$.\n5. **Interpretation**: If $\\tau > y/2$, the social security transfer is so large that young agents cannot save enough to support positive real currency balances, preventing the existence of a monetary equilibrium.\n\nCANDIDATE_ANSWER:\nStep 1: In the nonmonetary equilibrium, the market-clearing rate of return r₁ satisfies f(1 + r₁) = 0. For a monetary equilibrium to exist, f[1 + r(t)] must instead remain positive indefinitely (as it equals H₀/(N̄p(t)) > 0).\n\nStep 2: The recursive relation f[1 + r(t+1)] = [1 + r(t)]f[1 + r(t)] implies exponential growth in f[·] terms when r(t) > r₁ > 0. This creates instability because f[·] values would eventually exceed feasible bounds.\n\nStep 3: The τ ≤ y/2 condition constrains agents' lifetime income distribution. If τ > y/2, agents receive excessive transfers in old age (relative to young-age income y), reducing their incentive to hold money for intertemporal savings. This would force f[1 + r(t)] to eventually turn negative, violating the monetary equilibrium requirement.\n\nStep 4: Specifically, τ > y/2 would make social security transfers so large that the monetary savings mechanism becomes redundant, collapsing the system to the nonmonetary equilibrium where f(1 + r₁) = 0. The τ ≤ y/2 threshold ensures agents remain sufficiently dependent on currency for savings to maintain f[1 + r(t)] > 0 at all times.\n\nQID: textbook-103-7-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-103-7-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the necessity of the condition τ ≤ y/2 to maintain positive savings and prevent the collapse into a nonmonetary equilibrium, aligning with the gold answer's reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-7-1-1", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the necessity of the condition τ ≤ y/2 to maintain positive savings and prevent the collapse into a nonmonetary equilibrium, aligning with the gold answer's reasoning."}, "llm_echoed_qid": "textbook-103-7-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the necessity of the condition τ ≤ y/2 to maintain positive savings and prevent the collapse into a nonmonetary equilibrium, aligning with the gold answer's reasoning."}
{"script_processing_qid": "textbook-107-3-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze how SARP handles the 'Laspeyres vs. Paasche index' scenario from the text.\n\nGOLD_ANSWER:\nThe Laspeyres index uses base-period quantities, while Paasche uses current-period quantities. SARP ensures that if a consumer's bundle in period 2 is revealed preferred to period 1's bundle (via Laspeyres), the reverse cannot hold under Paasche unless income changes, preventing contradictions in welfare comparisons.\n\nCANDIDATE_ANSWER:\nStep 1: Understand SARP's requirement. SARP demands transitive consistency in revealed preferences: If x is chosen over y (when y is affordable), and y is chosen over z (when z is affordable), then z cannot be chosen over x (when x is affordable).\n\nStep 2: Relate to Laspeyres vs. Paasche indices. In consumption analysis, the Laspeyres index compares current consumption to a base period basket at current prices, while the Paasche index compares base-period prices to current prices with the current basket. Divergences between these indices often reflect violations of consistent preferences.\n\nStep 3: Apply SARP to the scenario. SARP resolves such conflicts by requiring that revealed preferences across budget sets (reflected in index comparisons) must be transitive. If a Laspeyres-based comparison suggests x ≻ y and a Paasche-based comparison suggests y ≻ z, SARP imposes that x ≻ z must hold in subsequent choices, preventing cyclical inconsistencies that would arise if z ≻ x under affordability conditions contradicting the earlier hierarchy.\n\nStep 4: Conclusion. SARP eliminates the possibility of conflicting Laspeyres-Paasche outcomes by enforcing transitivity, ensuring that a consumer’s choices across different price/quantity configurations cannot simultaneously support contradictory welfare conclusions.\n\nQID: textbook-107-3-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-3-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately describes how SARP ensures transitive consistency in revealed preferences and relates it to the Laspeyres vs. Paasche index scenario, matching the gold answer's explanation.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-3-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately describes how SARP ensures transitive consistency in revealed preferences and relates it to the Laspeyres vs. Paasche index scenario, matching the gold answer's explanation."}, "llm_echoed_qid": "textbook-107-3-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately describes how SARP ensures transitive consistency in revealed preferences and relates it to the Laspeyres vs. Paasche index scenario, matching the gold answer's explanation."}
{"script_processing_qid": "textbook-114-15-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the long-run price elasticity of total copper supply from Table 2.3, given that secondary supply is 20% of total supply and primary supply is 80%.\n\nGOLD_ANSWER:\n1. **Primary Supply Elasticity**: 1.60 (80% weight).\n2. **Secondary Supply Elasticity**: 0.31 (20% weight).\n3. **Total Elasticity**: (0.8 * 1.60) + (0.2 * 0.31) = 1.28 + 0.062 = 1.342.\n4. **Result**: The long-run price elasticity of total copper supply is approximately 1.34.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain the specific short-run or long-run elasticity values for primary and secondary copper supply from Table 2.3, which are required to calculate the weighted average long-run price elasticity of total supply. While the context provides the proportion of secondary (20%) and primary (80%) supply, it does not include the actual elasticity coefficients for these components in either the short or long run. Therefore, the derivation cannot be performed with the given information.\n\nQID: textbook-114-15-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-15-0-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the lack of necessary information.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-15-0-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the lack of necessary information."}, "llm_echoed_qid": "textbook-114-15-0-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer correctly points out the lack of necessary information."}
{"script_processing_qid": "textbook-106-4-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. A traffic light reduces fatal accident risk by 0.5%. Using a VSL of $\\$10$ million, calculate the expected benefit. Should the town spend $\\$10,000$ on the light?\n\nGOLD_ANSWER:\n1. **Expected Benefit**: $0.005 \\times \\$10$ million = $\\$50,000$.\n2. **Cost-Benefit Comparison**: Benefit ($\\$50,000$) > Cost ($\\$10,000$).\n3. **Decision**: The town should approve the project as it yields a net social benefit of $\\$40,000$.\n\nCANDIDATE_ANSWER:\nStep 1: Convert the 0.5% risk reduction to decimal form (0.5% = 0.005).\nStep 2: Multiply the risk reduction by the Value of Statistical Life (VSL): 0.005 × $10,000,000 = $50,000.\nStep 3: Compare the expected benefit ($50,000) to the cost ($10,000).\nConclusion: The expected benefit ($50,000) exceeds the cost ($10,000), so the town should spend the $10,000 on the traffic light.\n\nQID: textbook-106-4-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-4-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to calculate the expected benefit, compares it to the cost, and concludes that the town should spend the money on the traffic light, matching the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-4-1-3", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to calculate the expected benefit, compares it to the cost, and concludes that the town should spend the money on the traffic light, matching the gold answer."}, "llm_echoed_qid": "textbook-106-4-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to calculate the expected benefit, compares it to the cost, and concludes that the town should spend the money on the traffic light, matching the gold answer."}
{"script_processing_qid": "textbook-119-8-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the Slutsky property (invariance property) of probability limits and provide an example.\n\nGOLD_ANSWER:\n1. **Slutsky Property**: If $\\hat{\\theta}$ is consistent for $\\theta$ and $h(\\cdot)$ is continuous, then $\\operatorname*{plim}_{n\\to\\infty}h(\\hat{\\theta}) = h(\\theta)$.\n2. **Example**: If $\\hat{\\theta}$ is consistent for $\\theta$, then $\\log(\\hat{\\theta})$ is consistent for $\\log(\\theta)$.\n\nCANDIDATE_ANSWER:\nStep 1: Define the Slutsky property (invariance property)\nThe Slutsky property states that if \\(\\hat{\\theta}\\) is a consistent estimator of \\(\\theta\\) (i.e., \\(\\operatorname*{plim}_{n\\to\\infty} \\hat{\\theta} = \\theta\\)), and \\(h(\\cdot)\\) is a continuous function, then the probability limit of \\(h(\\hat{\\theta})\\) is \\(h(\\theta)\\). Formally:\n$$\n\\operatorname*{plim}_{n\\to\\infty} h(\\hat{\\theta}) = h\\left(\\operatorname*{plim}_{n\\to\\infty} \\hat{\\theta}\\right) = h(\\theta).\n$$\n\nStep 2: Example using the sample mean\nFrom the context, the sample mean \\(\\bar{X}\\) is a consistent estimator of \\(\\mu\\) because \\(\\operatorname*{plim}_{n\\to\\infty} \\bar{X} = \\mu\\). Suppose we apply the continuous function \\(h(x) = x^2\\). By the Slutsky property:\n$$\n\\operatorname*{plim}_{n\\to\\infty} (\\bar{X})^2 = \\left(\\operatorname*{plim}_{n\\to\\infty} \\bar{X}\\right)^2 = \\mu^2.\n$$\nThus, \\((\\bar{X})^2\\) is a consistent estimator of \\(\\mu^2\\).\n\nStep 3: Key conditions\nFor the property to hold:\n1. \\(\\hat{\\theta}\\) must be consistent for \\(\\theta\\).\n2. The function \\(h(\\cdot)\\) must be continuous at \\(\\theta\\).\nThe example satisfies both: \\(\\bar{X}\\) is consistent for \\(\\mu\\), and squaring is a continuous function.\n\nQID: textbook-119-8-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-8-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately defines the Slutsky property and provides a correct example using the sample mean and squaring function, meeting all key conditions.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-8-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately defines the Slutsky property and provides a correct example using the sample mean and squaring function, meeting all key conditions."}, "llm_echoed_qid": "textbook-119-8-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately defines the Slutsky property and provides a correct example using the sample mean and squaring function, meeting all key conditions."}
{"script_processing_qid": "textbook-115-13-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the implications of serial correlation in $\\{u_{i t}\\}$ for the Fixed Effects estimator, and explain how the robust variance matrix estimator addresses this issue.\n\nGOLD_ANSWER:\n1. **Implications of serial correlation**: Under Assumption FE.3, $\\{u_{i t}\\}$ are serially uncorrelated, but if this fails, the usual variance estimator (10.54) is inconsistent. Serial correlation in $\\{u_{i t}\\}$ leads to biased standard errors and invalid inference.\\n2. **Robust variance estimator**: The robust estimator (10.59) accounts for arbitrary serial correlation and heteroskedasticity by using the outer product of the scores $\\ddot{\\mathbf{X}}_{i}^{\\prime}\\hat{\\mathbf{u}}_{i}$. It is consistent as $N \\to \\infty$ for fixed $T$.\n\nCANDIDATE_ANSWER:\nStep 1: Implications of Serial Correlation in Fixed Effects Estimator\nUnder Assumptions FE.1–FE.3, the standard variance formula for $\\hat{\\pmb{\beta}}_{FE}$ assumes i.i.d. errors with $\text{Var}(\\mathbf{u}_i|\\mathbf{x}_i,c_i) = \\sigma_u^2\\mathbf{I}_T$. However, serial correlation in $\\{u_{it}\\}$ violates this assumption. The within-transformed errors $\\{\\ddot{u}_{it}\\}$ exhibit negative covariance ($-\\sigma_u^2/T$ for $t\\neq s$), even under FE.3. If serial correlation exists in the original errors, this further distorts the variance-covariance structure, leading to biased standard errors and invalid inference (e.g., incorrect t-statistics, confidence intervals).\n\nStep 2: Robust Variance Matrix Solution\nThe robust variance matrix estimator replaces the incorrect $\\sigma_u^2\\mathbf{I}_T$ assumption with a cluster-robust approach:\n$$\n\text{Avar}(\\hat{\\pmb{\beta}}_{FE}) = (\\ddot{\\mathbf{X}}'\\ddot{\\mathbf{X}})^{-1} \\left( \\sum_{i=1}^N \\ddot{\\mathbf{X}}_i'\\hat{\\mathbf{u}}_i\\hat{\\mathbf{u}}_i'\\ddot{\\mathbf{X}}_i \\right)(\\ddot{\\mathbf{X}}'\\ddot{\\mathbf{X}})^{-1}.\n$$\nThis estimator aggregates residual cross-products at the individual level, allowing for arbitrary within-individual serial correlation and heteroskedasticity. It does not require specifying the exact form of serial correlation, making it asymptotically valid under weaker assumptions than FE.3.\n\nQID: textbook-115-13-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-13-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately discusses the implications of serial correlation in the Fixed Effects estimator and correctly explains how the robust variance matrix estimator addresses this issue, matching the gold answer in both content and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-13-0-2", "category": "CORRECT", "explanation": "The candidate answer accurately discusses the implications of serial correlation in the Fixed Effects estimator and correctly explains how the robust variance matrix estimator addresses this issue, matching the gold answer in both content and reasoning."}, "llm_echoed_qid": "textbook-115-13-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately discusses the implications of serial correlation in the Fixed Effects estimator and correctly explains how the robust variance matrix estimator addresses this issue, matching the gold answer in both content and reasoning."}
{"script_processing_qid": "textbook-119-9-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Describe Ramsey's RESET test for detecting model specification errors and its advantages and disadvantages.\n\nGOLD_ANSWER:\n1. **Steps**:\n   - Estimate the model and obtain fitted values $\\hat{Y}\\_{i}$.\n   - Rerun the regression including $\\hat{Y}\\_{i}^{2}$ and $\\hat{Y}\\_{i}^{3}$ as additional regressors.\n   - Use an $F$ test to compare $R^{2}$ values.\n2. **Advantages**:\n   - Easy to apply; does not require specifying an alternative model.\n3. **Disadvantages**:\n   - Does not suggest a better alternative model.\n   - May lack power in small samples.\n\nCANDIDATE_ANSWER:\nThe provided context does not mention Ramsey's RESET test. The given text focuses on the consequences of including irrelevant variables in a regression model (e.g., unbiasedness, consistency, inefficiency of OLS estimators). Since no details about Ramsey's RESET test (e.g., methodology, advantages, disadvantages) are included in the context, it is insufficient to answer the question about this specific specification test.\n\nQID: textbook-119-9-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-9-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it states the provided context does not mention Ramsey's RESET test.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-9-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it states the provided context does not mention Ramsey's RESET test."}, "llm_echoed_qid": "textbook-119-9-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it states the provided context does not mention Ramsey's RESET test."}
{"script_processing_qid": "textbook-104-14-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Interpret the economic significance of Theorem 11.4 in the context of a Markov model of income dynamics. What does geometric convergence imply about income mobility?\n\nGOLD_ANSWER:\n1. **Unique ergodic set**: Ensures all income classes are interconnected, ruling out permanent poverty traps or elite persistence. \\n2. **No cyclicity**: Rules out deterministic cycles (e.g., alternating boom/bust states). \\n3. **Geometric convergence**: Income distributions converge rapidly to a steady-state, implying high mobility rates. \\n4. **Policy implication**: Interventions (e.g., education subsidies) can uniformly shift the invariant distribution $\\boldsymbol{p}^*$.\n\nCANDIDATE_ANSWER:\nStep 1: Theorem 11.4 states that Πⁿ converges to a matrix Q with identical rows (a single invariant distribution) if and only if some power Πᴺ has all positive entries. This implies the Markov chain is irreducible and aperiodic, forming a single ergodic set.\n\nStep 2: In income dynamics, geometric convergence means the effect of initial income disparities decays exponentially over time. All households eventually share the same long-run income distribution regardless of starting conditions.\n\nStep 3: Economically, this implies high income mobility: no permanent 'traps' or persistent inequality. The system reaches a steady state where current income becomes uninformative about future income (memoryless property), reducing long-term dependence on initial advantages/disadvantages.\n\nQID: textbook-104-14-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-104-14-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately interprets Theorem 11.4, correctly explaining the implications of geometric convergence for income mobility and the absence of permanent traps, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-104-14-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately interprets Theorem 11.4, correctly explaining the implications of geometric convergence for income mobility and the absence of permanent traps, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-104-14-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately interprets Theorem 11.4, correctly explaining the implications of geometric convergence for income mobility and the absence of permanent traps, aligning well with the gold answer."}
{"script_processing_qid": "textbook-115-15-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the first-differenced equation for the model $y_{it} = c_i + g_i t + \\mathbf{x}_{it}\\beta + u_{it}$ and explain how it eliminates $c_i$.\n\nGOLD_ANSWER:\n1. **Original Model**: $y_{it} = c_i + g_i t + \\mathbf{x}_{it}\\beta + u_{it}$.\n2. **First Difference**: $\\Delta y_{it} = y_{it} - y_{i,t-1} = g_i + \\Delta \\mathbf{x}_{it}\\beta + \\Delta u_{it}$.\n3. **Elimination of $c_i$**: The first difference removes $c_i$ because $c_i - c_i = 0$.\n\nCANDIDATE_ANSWER:\nStep 1: Write the model for both time period t and t-1:\ny_it = c_i + g_i t + x_itβ + u_it\ny_i,t−1 = c_i + g_i(t−1) + x_i,t−1β + u_i,t−1\n\nStep 2: Subtract the t−1 equation from the t equation (first-differencing):\nΔy_it = y_it − y_i,t−1 = [c_i − c_i] + [g_i t − g_i(t−1)] + (x_it − x_i,t−1)β + (u_it − u_i,t−1)\n\nStep 3: Simplify terms:\nΔy_it = g_i(1) + Δx_itβ + Δu_it\n\nKey elimination mechanism:\n- The fixed effect c_i cancels out because it is time-invariant (c_i − c_i = 0)\n- The term g_i(t − (t−1)) simplifies to g_i, which remains in the equation\n\nFinal first-differenced equation:\nΔy_it = g_i + Δx_itβ + Δu_it\n\nThis transformation eliminates c_i because time-constant unobserved heterogeneity disappears when taking the difference between two periods.\n\nQID: textbook-115-15-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-15-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the first-differenced equation and accurately explains how the fixed effect $c_i$ is eliminated through the differencing process. The steps and reasoning align perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-15-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the first-differenced equation and accurately explains how the fixed effect $c_i$ is eliminated through the differencing process. The steps and reasoning align perfectly with the gold answer."}, "llm_echoed_qid": "textbook-115-15-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the first-differenced equation and accurately explains how the fixed effect $c_i$ is eliminated through the differencing process. The steps and reasoning align perfectly with the gold answer."}
{"script_processing_qid": "textbook-96-0-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Given the Cobb-Douglas production function $y=z\\_{1}^{a}z\\_{2}^{b}$, derive the conditions under which it exhibits constant, increasing, or decreasing returns to scale. Provide economic intuition for each case.\n\nGOLD_ANSWER:\n1. **CRS**: $a + b = 1$ (doubling inputs doubles output).\n2. **IRS**: $a + b > 1$ (doubling inputs more than doubles output).\n3. **DRS**: $a + b < 1$ (doubling inputs less than doubles output).\n**Intuition**: CRS reflects proportional scalability, IRS reflects synergies, and DRS reflects inefficiencies at scale.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the Cobb-Douglas production function:\n$y = z_1^{a}z_2^{b}$\n\nStep 2: Scale all inputs by a factor $t > 0$:\n$y' = (tz_1)^{a}(tz_2)^{b} = t^{a}z_1^{a} \\cdot t^{b}z_2^{b} = t^{a+b}(z_1^{a}z_2^{b})$\n\nStep 3: Compare scaled output to original output:\n$y' = t^{a+b}y$\n\nConditions:\n- Constant Returns to Scale (CRS): $a + b = 1$ ($y' = t^1y$)\n- Increasing Returns to Scale (IRS): $a + b > 1$ ($y' > ty$)\n- Decreasing Returns to Scale (DRS): $a + b < 1$ ($y' < ty$)\n\nEconomic intuition:\n1. CRS: Proportional input increase yields proportional output growth (e.g., replicating existing processes)\n2. IRS: Output grows faster than inputs due to synergies/specialization (e.g., bulk purchasing advantages)\n3. DRS: Output grows slower than inputs due to coordination challenges or diminishing marginal productivity\n\nQID: textbook-96-0-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-96-0-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the conditions for constant, increasing, and decreasing returns to scale and provides accurate economic intuition for each case, matching the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-96-0-1-3", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the conditions for constant, increasing, and decreasing returns to scale and provides accurate economic intuition for each case, matching the gold answer."}, "llm_echoed_qid": "textbook-96-0-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the conditions for constant, increasing, and decreasing returns to scale and provides accurate economic intuition for each case, matching the gold answer."}
{"script_processing_qid": "textbook-110-11-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the first-order condition for an agent's voluntary contribution to a public good, starting from the utility maximization problem $\\operatorname*{max}_{x_{2}^{h}}U^{h}(\\phi({\\bar{z}}+y^{h}-x_{2}^{h}),x_{2}^{h})$.\n\nGOLD_ANSWER:\n1. **Objective**: Maximize $U^{h}(\\phi(\\bar{z}+y^{h}-x_{2}^{h}), x_{2}^{h})$.\n2. **Differentiate** with respect to $x_{2}^{h}$: $\\frac{dU^{h}}{dx_{2}^{h}} = U_{1}^{h} \\cdot \\frac{d\\phi}{dz} \\cdot \\frac{dz}{dx_{2}^{h}} + U_{2}^{h} = 0$.\n3. **Substitute** $\\frac{dz}{dx_{2}^{h}} = -1$ (from $z = \\bar{z} + y^{h} - x_{2}^{h}$).\n4. **Result**: $-U_{1}^{h}(x_{1}, x_{2}^{h}) \\phi_{z}(z) + U_{2}^{h}(x_{1}, x_{2}^{h}) = 0$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the utility function structure\nUtility is U^h(φ(̄z + y^h - x_2^h), x_2^h), where φ(·) converts total contributions (others' ̄z + agent's z^h = y^h - x_2^h) to public good output.\n\nStep 2: Apply chain rule differentiation\nTake derivative with respect to x_2^h:\ndU^h/dx_2^h = [∂U^h/∂x_1 * dφ/dz * d(z_total)/dx_2^h] + ∂U^h/∂x_2^h\n\nStep 3: Compute derivatives component-wise\n- ∂U^h/∂x_1 = U_1^h (marginal utility from public good)\n- dφ/dz = φ_z (marginal product of contributions)\n- d(z_total)/dx_2^h = d(̄z + y^h - x_2^h)/dx_2^h = -1\n- ∂U^h/∂x_2^h = U_2^h (marginal utility from private good)\n\nStep 4: Combine terms and set to zero\n- U_1^h · φ_z · (-1) + U_2^h = 0\n\nStep 5: Rearrange terms\nU_2^h = U_1^h · φ_z\nThis matches the first-order condition stated in the context.\n\nQID: textbook-110-11-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-110-11-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the first-order condition step-by-step, matching the gold answer's result and reasoning process.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-11-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the first-order condition step-by-step, matching the gold answer's result and reasoning process."}, "llm_echoed_qid": "textbook-110-11-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the first-order condition step-by-step, matching the gold answer's result and reasoning process."}
{"script_processing_qid": "textbook-112-35-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using historical examples cited by Smith, model the welfare effects of 'pretended payment' through coinage debasement.\n\nGOLD_ANSWER:\n1.  **Roman Debasement**: \n   - Nominal debt reduction: $D_{new} = \\frac{1}{6}D_{old}$ via As coin reduction (12oz → 2oz).\n   - Real transfer: $\\Delta W = W_{creditors} - W_{debtors} = -\\frac{5}{6}D_{old}$.\n2.  **Deadweight Loss**: \n   - Transaction costs: $L = \\alpha \\cdot \\Delta W$ where $\\alpha$ measures market friction.\n3.  **Political Economy**: \n   - Popular support driven by debtor relief: $S_{political} = \\beta \\cdot (\\Delta W_{debtors} > 0)$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Smith's key claims about 'pretended payment' in the provided context. The text states that public debt resolution often involves bankruptcy through 'pretended payment,' which likely includes currency debasement (though the context does not explicitly describe specific coinage debasement mechanisms).\n\nStep 2: Note that the context only identifies Genoa and Venice as historical examples weakened by the funding system, but does not provide concrete details about their coinage debasement practices or welfare effects.\n\nStep 3: Recognize that while the text links pretended payment to wealth destruction via public bankruptcy, the lack of explicit parameters (e.g., inflation rates, tax effects, or redistribution mechanisms) in the provided passages prevents formal welfare modeling.\n\nConclusion: The provided context contains insufficient information about Smith's analysis of coinage debasement mechanics, quantitative effects, or specific historical instances of such debasement to construct a welfare effects model as requested.\n\nQID: textbook-112-35-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-35-0-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it references specific historical examples and quantitative effects not provided in the context.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-35-0-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it references specific historical examples and quantitative effects not provided in the context."}, "llm_echoed_qid": "textbook-112-35-0-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it references specific historical examples and quantitative effects not provided in the context."}
{"script_processing_qid": "textbook-119-20-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the limitations of the Sargan test in finite samples.\n\nGOLD_ANSWER:\n1. **Low Power**: The test may fail to reject $H_0$ even with invalid instruments in small samples. 2. **Sensitivity to Model Specification**: Misspecification (e.g., omitted variables) can lead to false rejections. 3. **Dependence on Instrument Strength**: Weak instruments exacerbate size distortions.\n\nCANDIDATE_ANSWER:\nStep 1: The Sargan test relies on asymptotic theory, assuming the SARG statistic follows a χ² distribution under H₀. In finite samples, this asymptotic approximation may be poor, leading to size distortions (e.g., over-rejection or under-rejection of valid instruments).\n\nStep 2: The test requires at least as many instruments as endogenous variables (s ≥ q). When s ≈ q (few overidentifying restrictions), the test has limited power in finite samples, failing to detect invalid instruments reliably.\n\nStep 3: The formula SARG = (n − k)R² depends on sample size (n). Small n exacerbates biases in R² estimation from the auxiliary regression, further distorting test results.\n\nStep 4: The test assumes all exogenous variables (X) and instruments (W) are correctly specified. Finite samples amplify sensitivity to minor misspecifications, increasing Type I/II error rates.\n\nQID: textbook-119-20-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-20-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer effectively addresses the limitations of the Sargan test in finite samples, covering key points such as size distortions, limited power, and sensitivity to misspecifications, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-20-0-3", "category": "CORRECT", "explanation": "The candidate answer effectively addresses the limitations of the Sargan test in finite samples, covering key points such as size distortions, limited power, and sensitivity to misspecifications, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-119-20-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer effectively addresses the limitations of the Sargan test in finite samples, covering key points such as size distortions, limited power, and sensitivity to misspecifications, aligning well with the gold answer."}
{"script_processing_qid": "textbook-123-19-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the first-order condition for consumption $c(z)$ in (21.4.1) and interpret its components.\n\nGOLD_ANSWER:\n1. **FOC**: $\\frac{1}{u'(c(z))} = \\lambda(v,s,a) + \\nu(v,s,a)\\left(1 - \\frac{p(y;\\tilde{a})}{p(y;a)}\\right)$. \n2. **Components**: $\\lambda$ is the shadow price of promised utility; $\\nu$ adjusts for incentive compatibility, scaled by the likelihood ratio $\\frac{p(y;\\tilde{a})}{p(y;a)}$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the key elements\nZhao's model involves a planner designing consumption paths (wages/benefits) that balance insurance and incentives. Effort affects both output (when employed) and job separation probabilities (when unemployed).\n\nStep 2: Consider the recursive formulation\nEquation (21.4.1) likely represents a Bellman equation where the planner optimizes over:\n- Current utility from consumption\n- Future continuation values\n- Incentive compatibility constraints\n\nStep 3: Derive the FOC\nAssuming standard recursive contract theory tools:\n\n∂L/∂c(z) = u'(c(z)) - μ[1 + ψ*(∂e(z)/∂c(z))] = 0\n\nWhere:\n1) u'(c(z)) = marginal utility of consumption\n2) μ = resource shadow price\n3) ψ = incentive multiplier\n4) ∂e(z)/∂c(z) = how consumption affects effort (e) in state z\n\nStep 4: Interpretation\n- The first term (u') reflects standard consumption smoothing\n- The bracketed term shows how consumption must balance:\n   • Resource costs (1)\n   • Incentive distortions (ψ term) through effort's response to compensation changes\nThis aligns with Zhao's dual focus on insurance (u' term) and effort incentives (ψ term) across employment/unemployment states.\n\nQID: textbook-123-19-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-19-1-0\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's derivation and interpretation do not match the gold answer. The FOC provided by the candidate is different from the one in the gold answer, and the components (μ and ψ) are not aligned with the shadow price (λ) and incentive adjustment (ν) in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-19-1-0", "category": "INCORRECT", "explanation": "The candidate's derivation and interpretation do not match the gold answer. The FOC provided by the candidate is different from the one in the gold answer, and the components (μ and ψ) are not aligned with the shadow price (λ) and incentive adjustment (ν) in the gold answer."}, "llm_echoed_qid": "textbook-123-19-1-0", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's derivation and interpretation do not match the gold answer. The FOC provided by the candidate is different from the one in the gold answer, and the components (μ and ψ) are not aligned with the shadow price (λ) and incentive adjustment (ν) in the gold answer."}
{"script_processing_qid": "textbook-114-17-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. How does the variability of outcomes affect the risk premium? Illustrate with an example where the risky job has a 50% chance of $\\$0$ and a 50% chance of $\\$40,000$.\n\nGOLD_ANSWER:\n1. **Effect of Variability**: Greater variability increases the risk premium because the uncertainty is higher.\n2. **Example Calculation**: \n   - Expected income: $0.5 \\times \\$0 + 0.5 \\times \\$40,000 = \\$20,000$.\n   - Expected utility: $0.5 \\times u(\\$0) + 0.5 \\times u(\\$40,000) = 0.5 \\times 0 + 0.5 \\times 20 = 10$.\n   - Utility of certain $\\$20,000$ is 16, so the individual loses 6 units of utility.\n   - Risk premium: $\\$20,000 - \\$10,000 = \\$10,000$ (since $u(\\$10,000) = 10$).\n3. **Conclusion**: The risk premium increases with greater variability of outcomes.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate the expected income of the risky job. The expected income = (0.5 × $0) + (0.5 × $40,000) = $20,000.\n\nStep 2: Determine utilities for outcomes. From the context, utilities at $10k, $20k, and $30k are 10, 16, and 18. Assuming diminishing marginal utility continues:\n- At $0 (extrapolating): Utility = 0 (not explicitly stated but inferred from the pattern).\n- At $40k (extending the trend): Utility ≈ 19 (as marginal utility decreases further).\n\nStep 3: Compute expected utility. Expected utility = (0.5 × u($0)) + (0.5 × u($40,000)) = 0.5(0) + 0.5(19) = 9.5.\n\nStep 4: Find the certainty equivalent income. This is the guaranteed income with utility = 9.5. From the utility curve, utility = 10 occurs at $10k. Linear interpolation suggests a slightly lower income (e.g., ~$9,500) would yield utility ≈ 9.5.\n\nStep 5: Calculate the risk premium. Risk premium = Expected income ($20,000) − Certainty equivalent (~$9,500) = ~$10,500.\n\nConclusion: Greater variability (from $10k/$30k in the original example to $0/$40k here) reduces expected utility and increases the risk premium (from $4k to ~$10.5k), demonstrating that higher outcome variability raises the risk premium for risk-averse individuals.\n\nQID: textbook-114-17-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-17-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to calculate the risk premium, including expected income, expected utility, certainty equivalent, and risk premium. The reasoning aligns with the gold answer, demonstrating understanding of how variability affects the risk premium.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-17-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to calculate the risk premium, including expected income, expected utility, certainty equivalent, and risk premium. The reasoning aligns with the gold answer, demonstrating understanding of how variability affects the risk premium."}, "llm_echoed_qid": "textbook-114-17-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to calculate the risk premium, including expected income, expected utility, certainty equivalent, and risk premium. The reasoning aligns with the gold answer, demonstrating understanding of how variability affects the risk premium."}
{"script_processing_qid": "textbook-34-0-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the implications of the cost function being concave in q for the firm's production decisions.\n\nGOLD_ANSWER:\n1. **Economies of Scale**: Concavity implies decreasing marginal costs, suggesting economies of scale.\n2. **Production Expansion**: The firm can increase output at a lower marginal cost, favoring larger-scale production.\n3. **Profit Maximization**: Concavity supports higher output levels to spread fixed costs over more units, enhancing profitability.\n\nCANDIDATE_ANSWER:\nStep 1: Confirm concavity implications from context\n\nThe text proves the cost function c(w, q) is concave in q by showing:\n$$αc(w,q) + (1-α)c(w,q') ≥ c(w, αq + (1-α)q')$$\nThis matches the definition of concavity.\n\nStep 2: Identify homogeneity properties\n\nThe cost function is homogeneous of degree 1 in q (proven via $z(w, αq) = αz(w,q)$). This implies scaling output by α scales costs by α: $c(w, αq) = αc(w,q)$.\n\nStep 3: Resolve concavity + homogeneity\n\nA function that is both concave and homogeneous of degree 1 must be linear. This follows because:\n- Homogeneity implies $c(w, αq) = αc(w,q)$\n- Concavity inequality $αc(q) + (1-α)c(q') ≥ c(αq+(1-α)q')$\nThese two properties can only coexist if the cost function is linear in q.\n\nStep 4: Derive marginal cost behavior\n\nLinear cost functions ($C = kq$) imply constant marginal cost ($MC = k$). This eliminates economies/diseconomies of scale (average cost = marginal cost = constant).\n\nStep 5: Production decision implications\n(1) Output flexibility: Firms can scale production without marginal cost changes\n(2) Market-driven decisions: Optimal output depends entirely on demand/revenue (since MC is flat)\n(3) Competitive markets: Supply becomes perfectly elastic at MC = price\n(4) Indeterminate optimal scale: Pure cost considerations alone don't restrict output levels\n(5) Input allocation: Homogeneity proof $z(w, αq) = αz(w,q)$ shows inputs scale linearly with output, matching constant returns.\n\nQID: textbook-34-0-2-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-34-0-2-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer incorrectly concludes that the cost function must be linear due to concavity and homogeneity, which contradicts the gold answer's implications of economies of scale and decreasing marginal costs. The reasoning provided does not align with the correct implications of a concave cost function.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-34-0-2-2", "category": "INCORRECT", "explanation": "The candidate's answer incorrectly concludes that the cost function must be linear due to concavity and homogeneity, which contradicts the gold answer's implications of economies of scale and decreasing marginal costs. The reasoning provided does not align with the correct implications of a concave cost function."}, "llm_echoed_qid": "textbook-34-0-2-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer incorrectly concludes that the cost function must be linear due to concavity and homogeneity, which contradicts the gold answer's implications of economies of scale and decreasing marginal costs. The reasoning provided does not align with the correct implications of a concave cost function."}
{"script_processing_qid": "textbook-118-12-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain why the chi-squared distribution approaches the normal distribution $N(m, 2m)$ as $m$ becomes large.\n\nGOLD_ANSWER:\n1. **Central Limit Theorem**: The sum of $m$ independent random variables $z_{i}^{2}$ (each with mean 1 and variance 2) approaches a normal distribution as $m \\to \\infty$.\n2. **Parameters**: The mean is $m$ and the variance is $2m$, so the limiting distribution is $N(m, 2m)$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that a χ²(m) distribution is defined as the sum of m independent squared standard normal variables (y = Σz_i² where z_i ~ N(0,1)).\nStep 2: Each z_i² follows a χ²(1) distribution with E(z_i²) = 1 and Var(z_i²) = 2.\nStep 3: By the Central Limit Theorem, the sum of m independent and identically distributed random variables with finite mean μ and variance σ² converges to N(mμ, mσ²) as m → ∞.\nStep 4: Applying this to y = Σz_i²: μ = 1 (mean of each z_i²) and σ² = 2 (variance of each z_i²).\nStep 5: Thus, y ~ χ²(m) approaches N(m·1, m·2) = N(m, 2m) as m becomes large.\n\nQID: textbook-118-12-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-12-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly applies the Central Limit Theorem to the sum of squared standard normal variables, showing how the chi-squared distribution approaches N(m, 2m) as m becomes large, matching the gold answer's reasoning and conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-12-1-2", "category": "CORRECT", "explanation": "The candidate's answer correctly applies the Central Limit Theorem to the sum of squared standard normal variables, showing how the chi-squared distribution approaches N(m, 2m) as m becomes large, matching the gold answer's reasoning and conclusion."}, "llm_echoed_qid": "textbook-118-12-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly applies the Central Limit Theorem to the sum of squared standard normal variables, showing how the chi-squared distribution approaches N(m, 2m) as m becomes large, matching the gold answer's reasoning and conclusion."}
{"script_processing_qid": "textbook-108-11-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Critically evaluate the argument that unemployment insurance improves the matching of workers to jobs. What are the trade-offs involved?\n\nGOLD_ANSWER:\n1. **Improved Matching**: Unemployment insurance allows workers to reject unsuitable jobs, potentially leading to better long-term matches.\n2. **Trade-offs**: \n   - **Pros**: Reduces income uncertainty and improves job-worker fit.\n   - **Cons**: Increases frictional unemployment and may reduce overall labor market efficiency.\n3. **Empirical Evidence**: Studies show that unemployment insurance延长s job search duration, but the quality of matches may improve.\n\nCANDIDATE_ANSWER:\nStep 1: **Argument supporting improved job matching**\nUnemployment insurance (UI) may improve worker-job matching by allowing workers to search longer for roles that better align with their skills, preferences, or location. This reduces the pressure to accept the first available job, potentially leading to more productive and sustainable matches.\n\nStep 2: **Key trade-offs and counterarguments**\n- UI reduces the urgency of job search, as workers face lower financial hardship. This can lengthen unemployment spells and increase frictional unemployment rates.\n- Workers may reject marginally suitable jobs or negotiate for higher job security, slowing labor market adjustments to sectoral shifts (e.g., declining oil jobs vs. rising auto jobs).\n- Employers might face higher turnover costs if matches are not significantly improved despite prolonged search.\n\nStep 3: **Net evaluation based on context**\nWhile UI could theoretically improve match quality by extending search time, the context emphasizes that UI primarily incentivizes reduced search effort and selectivity, increasing unemployment duration without conclusive evidence of better matches. The trade-off is clear: UI mitigates short-term income loss for workers but risks higher structural unemployment and slower economic adaptation to sectoral shifts.\n\nQID: textbook-108-11-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-108-11-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly evaluates the argument for unemployment insurance improving job matching, identifies key trade-offs, and provides a nuanced net evaluation, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-11-0-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly evaluates the argument for unemployment insurance improving job matching, identifies key trade-offs, and provides a nuanced net evaluation, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-108-11-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly evaluates the argument for unemployment insurance improving job matching, identifies key trade-offs, and provides a nuanced net evaluation, aligning well with the gold answer."}
{"script_processing_qid": "textbook-121-5-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Prove that $(A + B)(C + D) = AC + AD + BC + BD$.\n\nGOLD_ANSWER:\n1. **Expand $(A + B)(C + D)$**: Let $A$, $B$, $C$, and $D$ be matrices of conformable dimensions. Then: $$ (A + B)(C + D) = A(C + D) + B(C + D) = AC + AD + BC + BD. $$ 2. **Distributive property**: The result follows directly from the distributive property of matrix multiplication over addition.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the left-hand side (A + B)(C + D).\nStep 2: Apply the distributive property of matrix multiplication over addition: (A + B)C + (A + B)D.\nStep 3: Distribute again across each term: AC + BC + AD + BD.\nStep 4: Rearrange the terms (since matrix addition is commutative): AC + AD + BC + BD.\nStep 5: Observe this matches the right-hand side, proving (A + B)(C + D) = AC + AD + BC + BD using matrix distributive laws.\n\nQID: textbook-121-5-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-121-5-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly applies the distributive property of matrix multiplication over addition and arrives at the correct expanded form. The steps are logically sound and match the reasoning provided in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-121-5-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly applies the distributive property of matrix multiplication over addition and arrives at the correct expanded form. The steps are logically sound and match the reasoning provided in the gold answer."}, "llm_echoed_qid": "textbook-121-5-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly applies the distributive property of matrix multiplication over addition and arrives at the correct expanded form. The steps are logically sound and match the reasoning provided in the gold answer."}
{"script_processing_qid": "textbook-123-8-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. How can the Schur decomposition be used to solve for the matrix $P$ in the Lagrangian formulation?\n\nGOLD_ANSWER:\n1. **Triangularize $M$**: Decompose $M$ as $V W V^{-1}$, where $W$ is block-triangular.\n2. **Stability Condition**: Impose $y_{20}^* = 0$ to eliminate unstable modes.\n3. **Solve for $P$**: $$P = V_{21} V_{11}^{-1},$$ where $V$ partitions into stable and unstable blocks.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that the first-order conditions from the Lagrangian lead to a system of equations involving matrix P. These typically reduce to an algebraic Riccati equation of the form P = R + A'PA − (A'PB)(Q + B'PB)^{-1}(B'PA).\n\nStep 2: Reformulate this equation as a discrete-time algebraic Riccati equation. This can be transformed into a generalized eigenvalue problem using the associated Hamiltonian/symplectic matrix pencil.\n\nStep 3: Apply Schur decomposition to the matrix pencil. The stable eigenvalues (those with modulus < 1) are selected, and their corresponding Schur vectors form a basis for the stable invariant subspace.\n\nStep 4: Extract the solution P from this stable invariant subspace. If the Schur decomposition is ordered such that stable eigenvalues appear first, the solution can be computed from the upper-triangular blocks of the decomposed matrices.\n\nStep 5: Verify stability by checking that (A − BF) with F = (Q + B'PB)^{-1}B'PA has eigenvalues strictly inside the unit circle, consistent with the stability condition stated in the context.\n\nQID: textbook-123-8-2-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-8-2-3\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer discusses solving an algebraic Riccati equation using Schur decomposition, which is a valid approach in control theory but does not align with the specific steps provided in the gold answer for solving for matrix P in the Lagrangian formulation using Schur decomposition.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-8-2-3", "category": "INCORRECT", "explanation": "The candidate's answer discusses solving an algebraic Riccati equation using Schur decomposition, which is a valid approach in control theory but does not align with the specific steps provided in the gold answer for solving for matrix P in the Lagrangian formulation using Schur decomposition."}, "llm_echoed_qid": "textbook-123-8-2-3", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer discusses solving an algebraic Riccati equation using Schur decomposition, which is a valid approach in control theory but does not align with the specific steps provided in the gold answer for solving for matrix P in the Lagrangian formulation using Schur decomposition."}
{"script_processing_qid": "textbook-40-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the t-ratio $t_k$ starting from the OLS estimator $b_k$ and the null hypothesis $\\mathrm{H}_0\\colon\\beta_k = \\overline{\\beta}_k$. Show all steps, including the substitution of $s^2$ for $\\sigma^2$.\n\nGOLD_ANSWER:\n1. **Start with OLS estimator**: $b_k$ is the OLS estimator of $\\beta_k$.\n2. **Null hypothesis**: $\\mathrm{H}_0\\colon\\beta_k = \\overline{\\beta}_k$.\n3. **Standard error**: $SE(b_k) = \\sqrt{s^2 \\cdot \\left((\\mathbf{X}^{\\prime}\\mathbf{X})^{-1}\\right)_{kk}}$.\n4. **t-ratio**: $t_k = \\frac{b_k - \\overline{\\beta}_k}{SE(b_k)}$.\n5. **Distribution**: Under $\\mathrm{H}_0$, $t_k \\sim t(n-K)$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the distribution of the OLS estimator under H₀\nGiven $(b_k - \\overline{\\beta}_k) | \\mathbf{X} \\sim N\\left(0, \\sigma^2 \\cdot \\left((\\mathbf{X}'\\mathbf{X})^{-1}\\right)_{kk}\\right)$, standardize this quantity:\n\n$$ z_k = \\frac{b_k - \\overline{\\beta}_k}{\\sqrt{\\sigma^2 \\cdot \\left((\\mathbf{X}'\\mathbf{X})^{-1}\\right)_{kk}}} $$\n\nThis follows a standard normal distribution ($z_k \\sim N(0,1)$).\n\nStep 2: Replace unknown variance $\\sigma^2$ with its unbiased estimator $s^2$\nSince $\\sigma^2$ is unknown, substitute $s^2 = \\frac{e'e}{n-K}$ (estimated error variance with $n-K$ degrees of freedom):\n\n$$ t_k = \\frac{b_k - \\overline{\\beta}_k}{\\sqrt{s^2 \\cdot \\left((\\mathbf{X}'\\mathbf{X})^{-1}\\right)_{kk}}} $$\n\nStep 3: Recognize the distributional change\nReplacing $\\sigma^2$ with $s^2$ replaces the normal distribution with a $t$-distribution because $s^2$ is a random variable. The degrees of freedom equal $n-K$ (total observations minus estimated parameters).\n\nFinal result:\n$$ t_k = \\frac{b_k - \\overline{\\beta}_k}{SE(b_k)} \\sim t(n-K) $$\nwhere $SE(b_k) = \\sqrt{s^2 \\cdot \\left((\\mathbf{X}'\\mathbf{X})^{-1}\\right)_{kk}}$.\n\nQID: textbook-40-0-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-40-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the t-ratio starting from the OLS estimator, substitutes $s^2$ for $\\sigma^2$, and correctly identifies the distribution as a t-distribution with $n-K$ degrees of freedom. The steps and reasoning align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-40-0-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the t-ratio starting from the OLS estimator, substitutes $s^2$ for $\\sigma^2$, and correctly identifies the distribution as a t-distribution with $n-K$ degrees of freedom. The steps and reasoning align with the gold answer."}, "llm_echoed_qid": "textbook-40-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the t-ratio starting from the OLS estimator, substitutes $s^2$ for $\\sigma^2$, and correctly identifies the distribution as a t-distribution with $n-K$ degrees of freedom. The steps and reasoning align with the gold answer."}
{"script_processing_qid": "textbook-106-4-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the expected value of a human life using the wage-risk tradeoff method. Include the mathematical formulation and assumptions.\n\nGOLD_ANSWER:\n1. **Model**: Let $\\Delta R$ be the incremental mortality risk, $\\Delta W$ the wage premium for risky jobs. The value of a statistical life (VSL) is $VSL = \\frac{\\Delta W}{\\Delta R}$.\n2. **Assumptions**: Workers are fully informed about risks, labor markets are competitive, and wages reflect risk premiums.\n3. **Calculation**: If $\\Delta R = 0.001$ (1 in 1,000) and $\\Delta W = \\$10,000$, then $VSL = \\frac{\\$10,000}{0.001} = \\$10$ million.\n\nCANDIDATE_ANSWER:\nStep 1: Define Key Variables\nLet Δw = Wage premium for risky jobs\nLet Δp = Incremental mortality risk (e.g., 0.01% increased chance of death)\n\nStep 2: Establish Tradeoff Relationship\nWorkers accept additional mortality risk Δp in exchange for wage premium Δw. The implied value per unit risk is represented as:\nValue per Statistical Life (VSL) = Δw / Δp\n\nStep 3: Scale to Full Mortality Risk\nSince Δp represents a small probability (e.g., 1 in 10,000), multiply by 100% equivalent:\nVSL = (Δw / Δp) × 1 full life equivalent\n\nStep 4: Numerical Example\nIf workers accept $1,000 annual premium for 0.01% increased risk (Δp = 0.0001):\nVSL = $1,000 / 0.0001 = $10,000,000\n\nAssumptions:\n1. Workers accurately perceive mortality risks\n2. Labor markets are competitive and voluntary\n3. Wage differentials reflect only risk compensation (other factors like education/skills are controlled)\n4. Linear risk-wage tradeoff holds for small Δp\n5. No systematic differences in risk preferences across workers\n\nThis matches the context's stated $10 million valuation using observed wage-risk tradeoffs.\n\nQID: textbook-106-4-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-4-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the expected value of a human life using the wage-risk tradeoff method, including the mathematical formulation and assumptions. The numerical example and assumptions align perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-4-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the expected value of a human life using the wage-risk tradeoff method, including the mathematical formulation and assumptions. The numerical example and assumptions align perfectly with the gold answer."}, "llm_echoed_qid": "textbook-106-4-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the expected value of a human life using the wage-risk tradeoff method, including the mathematical formulation and assumptions. The numerical example and assumptions align perfectly with the gold answer."}
{"script_processing_qid": "textbook-112-42-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare and contrast the British and Portuguese colonial trade policies regarding tobacco, as described by Smith. What does this reveal about mercantilist strategies?\n\nGOLD_ANSWER:\n1.  **British Policy**: Imposes higher duties on non-colonial tobacco but allows some colonial imports.\n2.  **Portuguese Policy**: Prohibits non-colonial tobacco entirely, reflecting stricter mercantilist control.\n3.  **Strategic Insight**: Both aim to monopolize colonial markets, but Portugal's absolute prohibition shows a more extreme mercantilist approach, prioritizing domestic merchant interests over potential revenue from duties.\n\nCANDIDATE_ANSWER:\nStep 1: Analyze British colonial trade policies regarding tobacco/manufactures\n- Britain allowed colonies to produce/sell raw or minimally processed goods (e.g., pig/bar iron) with exemptions/duty advantages.\n- Prohibited advanced manufacturing stages (e.g., steel furnaces) in colonies via absolute bans or high duties to protect domestic industries.\n\nStep 2: Assess Portuguese policies (not explicitly described)\n- The provided context contains no specific details about Portuguese colonial trade policies. Smith's critique focuses solely on British practices in the given passages.\n\nStep 3: Identify mercantilist strategy revelations\n- Britain's policies aimed to:\n  1) Secure raw materials from colonies at favorable terms\n  2) Maintain manufacturing monopolies for finished goods\n  3) Create protected markets through differential duties/bounties\n  4) Suppress colonial industrial development to preserve economic dependence\n\nConclusion: While the question references Portuguese policies, the provided context exclusively discusses British mercantilist strategies. Smith uses this example to demonstrate how mercantilism prioritized home-country industrial dominance over colonial development through trade restrictions and market controls.\n\nQID: textbook-112-42-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-42-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it references Portuguese policies that are not described in the provided context.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-42-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it references Portuguese policies that are not described in the provided context."}, "llm_echoed_qid": "textbook-112-42-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it references Portuguese policies that are not described in the provided context."}
{"script_processing_qid": "textbook-114-4-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the sources of monopoly power and provide examples of industries where these sources are prevalent.\n\nGOLD_ANSWER:\n1. **Sources**: Barriers to entry (e.g., patents, economies of scale), control of essential resources, government regulations.\n2. **Examples**: Pharmaceuticals (patents), utilities (natural monopolies), De Beers (control of diamonds).\n\nCANDIDATE_ANSWER:\nStep 1: According to the provided context, the primary sources of monopoly power include:\n1. **Market Demand Elasticity**: A less elastic market demand curve (where consumers are less responsive to price changes) increases monopoly power. This is measured using the Lerner Index (L = (P - MC)/P), which inversely depends on demand elasticity.\n2. **Number of Firms**: Fewer firms in the market lead to greater monopoly power (e.g., a single firm in a pure monopoly).\n3. **Interaction Among Firms**: Non-competitive behavior (e.g., collusion) or strategic interactions that reduce competition enhance monopoly power.\n4. **Natural Monopoly**: Arises from economies of scale, where a single firm can supply the entire market at lower costs than multiple firms.\n\nStep 2: Examples of industries where these sources are prevalent:\n- **Natural Monopoly**: Public utilities (e.g., electricity, water distribution) due to high fixed infrastructure costs and economies of scale.\n- **Market Demand Elasticity**: Pharmaceuticals (patented drugs face inelastic demand due to lack of substitutes).\n- **Few Firms/Interaction**: Telecommunications or airlines (oligopolies with limited competition, though not pure monopolies), or cartels like OPEC in oil (collusive behavior).\n- **Government-Created Monopolies**: Patents in the tech or pharmaceutical sectors grant temporary exclusivity.\n\nNote: The context explicitly mentions natural monopolies, demand elasticity, number of firms, and firm interactions as sources, aligning with the examples provided.\n\nQID: textbook-114-4-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-4-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly covers the sources of monopoly power and provides accurate examples, aligning well with the gold answer. The additional details on demand elasticity and firm interactions enhance the response without contradicting the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-4-1-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly covers the sources of monopoly power and provides accurate examples, aligning well with the gold answer. The additional details on demand elasticity and firm interactions enhance the response without contradicting the gold answer."}, "llm_echoed_qid": "textbook-114-4-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly covers the sources of monopoly power and provides accurate examples, aligning well with the gold answer. The additional details on demand elasticity and firm interactions enhance the response without contradicting the gold answer."}
{"script_processing_qid": "textbook-38-0-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the condition under which V(·) is strictly concave, and discuss its implications.\n\nGOLD_ANSWER:\n3. **Strict Concavity Condition**: If u(·) is strictly concave, then V(·) is strictly concave. This means consumers strictly prefer diversified consumption paths, avoiding extreme allocations, which has implications for risk aversion and portfolio choice.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the concavity condition in the given context\n- The proof shows $V(\\cdot)$ is concave because $u(\\cdot)$ is concave, yielding:\n  $$V(\\lambda c + (1-\\lambda)c') \\geq \\lambda V(c) + (1-\\lambda)V(c')$$\n\nStep 2: Derive strict concavity condition\nFor strict concavity ($>$ instead of $\\geq$), two conditions must hold:\n1. Strict concavity of period utility: $u(\\cdot)$ must be strictly concave\n2. Non-identical consumption streams: $c \\neq c'$ in at least one period\n\nStep 3: Mathematical justification\nIf $u(\\cdot)$ is strictly concave and $c_t \\neq c'_t$ for some $t$, then:\n$$u(\\lambda c_t + (1-\\lambda)c'_t) > \\lambda u(c_t) + (1-\\lambda)u(c'_t)$$\nfor those periods. With discount factors $\\delta^t > 0$, this strict inequality propagates to the lifetime utility:\n$$V(\\lambda c + (1-\\lambda)c') > \\lambda V(c) + (1-\\lambda)V(c')$$\n\nStep 4: Implications\n1. Risk aversion: Strict concavity implies consumers will always prefer diversified consumption streams over extreme/non-diversified ones\n2. Unique optimal solution: Ensures a unique solution to utility maximization problems\n3. Consumption smoothing: Reinforces preference for stable consumption paths over volatile ones\n\nQID: textbook-38-0-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-38-0-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the strict concavity condition for V(·) and provides a detailed mathematical justification. It also accurately discusses the implications of strict concavity, aligning with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-38-0-2-2", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the strict concavity condition for V(·) and provides a detailed mathematical justification. It also accurately discusses the implications of strict concavity, aligning with the gold answer."}, "llm_echoed_qid": "textbook-38-0-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the strict concavity condition for V(·) and provides a detailed mathematical justification. It also accurately discusses the implications of strict concavity, aligning with the gold answer."}
{"script_processing_qid": "textbook-114-30-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the profit-maximizing condition for a monopolist under perfect first-degree price discrimination. Show how the firm's output and profit compare to a single-price monopolist.\n\nGOLD_ANSWER:\n1. **Profit Maximization Condition**: Under perfect first-degree price discrimination, the firm charges each customer their reservation price, so the demand curve becomes the marginal revenue curve. The firm maximizes profit by producing where demand equals marginal cost (D = MC).\n2. **Output Comparison**: The firm produces more output ($Q^{**}$) compared to a single-price monopolist ($Q^{*}$), who produces where marginal revenue equals marginal cost (MR = MC).\n3. **Profit Comparison**: The firm captures all consumer surplus, resulting in higher profit. The additional profit is the area between the demand curve and the marginal cost curve beyond $Q^{*}$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the profit-maximizing condition for a monopolist under perfect first-degree price discrimination.\n- Under perfect first-degree price discrimination, the monopolist charges each customer their reservation price. This means the demand curve becomes the firm’s marginal revenue (MR) curve because each additional unit sold adds exactly the reservation price (P) to total revenue (i.e., MR = P).\n\nStep 2: Derive the output decision.\n- The firm maximizes profit by producing where marginal revenue (MR) equals marginal cost (MC). Since MR = P under perfect discrimination, the condition simplifies to P = MC. The monopolist produces up to the quantity where the demand curve intersects the MC curve.\n\nStep 3: Compare output and profit to a single-price monopolist.\n- Single-price monopolist: Produces where MR = MC (output Q₁ < Q* in perfect competition), resulting in deadweight loss and lower output. Profit = (P₁ - ATC) × Q₁ (excluding fixed costs).\n- Perfect first-degree discrimination: Produces where P = MC (output Q** equals the competitive equilibrium level). Profit equals the entire area under the demand curve and above the MC curve (captures all consumer surplus as producer surplus).\n\nConclusion: A perfectly discriminating monopolist produces a higher output (equal to the socially efficient level) and earns strictly higher profits than a single-price monopolist.\n\nQID: textbook-114-30-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-30-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately derives the profit-maximizing condition for a monopolist under perfect first-degree price discrimination, correctly identifies the output decision, and provides a clear comparison of output and profit to a single-price monopolist. The reasoning aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-30-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately derives the profit-maximizing condition for a monopolist under perfect first-degree price discrimination, correctly identifies the output decision, and provides a clear comparison of output and profit to a single-price monopolist. The reasoning aligns with the gold answer."}, "llm_echoed_qid": "textbook-114-30-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately derives the profit-maximizing condition for a monopolist under perfect first-degree price discrimination, correctly identifies the output decision, and provides a clear comparison of output and profit to a single-price monopolist. The reasoning aligns with the gold answer."}
{"script_processing_qid": "textbook-112-32-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss Smith's view on the division of labor and its effects on the intellectual and social virtues of the laboring poor. Provide specific examples from the text.\n\nGOLD_ANSWER:\n1. **Division of Labor**: Smith notes that the division of labor confines workers to repetitive, simple tasks, which stifles their intellectual growth.\n2. **Intellectual Stagnation**: Workers lose the habit of exerting their understanding or inventing solutions, leading to a decline in intellectual and social virtues.\n3. **Examples**: Smith cites the inability of such workers to engage in rational conversation or defend their country in war, as their minds become torpid and their bodies inactive.\n\nCANDIDATE_ANSWER:\nStep 1: Smith’s view on the division of labor’s intellectual effects\nSmith argues that the division of labor confines workers to repetitive, simple tasks (e.g., 'performing a few simple operations'), which erodes their intellectual development. The text states that such workers have 'no occasion to exert [their] understanding or exercise [their] invention,' leading to mental stagnation.\n\nStep 2: Social virtues and institutional context\nWhile the provided text emphasizes intellectual decline, it indirectly links social virtues to institutional frameworks (e.g., Roman courts of justice fostering superior character). However, no direct examples tie division of labor to social virtues of the laboring poor. The critique of education systems ('endowments of schools and colleges... corrupted the diligence of public teachers') suggests broader societal impacts, but these are not explicitly tied to laborers’ social virtues in the given passages.\n\nStep 3: Key textual examples\n- Repetitive labor: 'The man whose whole life is spent in performing a few simple operations... has no occasion to exert his understanding.'\n- Education system failures: Institutions render 'almost impossible to have any good private [teachers],' limiting opportunities for intellectual growth.\n\nQID: textbook-112-32-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-32-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately captures Smith's view on the division of labor's intellectual effects and provides relevant textual examples. While the social virtues aspect is noted as indirectly linked, the response aligns well with the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-32-0-2", "category": "CORRECT", "explanation": "The candidate's answer accurately captures Smith's view on the division of labor's intellectual effects and provides relevant textual examples. While the social virtues aspect is noted as indirectly linked, the response aligns well with the gold answer's key points."}, "llm_echoed_qid": "textbook-112-32-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately captures Smith's view on the division of labor's intellectual effects and provides relevant textual examples. While the social virtues aspect is noted as indirectly linked, the response aligns well with the gold answer's key points."}
{"script_processing_qid": "textbook-63-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove the consistency of the OLS estimator $\\mathbf{b}$ using the given expression $\\mathbf{b}-\\pmb{\\beta}=\\mathbf{S}_{\\mathbf{x}\\mathbf{x}}^{-1}\\bar{\\mathbf{g}}$ and the Kolmogorov convergence theorem.\n\nGOLD_ANSWER:\n1.  **Given**: $\\mathbf{b}-\\pmb{\\beta}=\\mathbf{S}_{\\mathbf{x}\\mathbf{x}}^{-1}\\bar{\\mathbf{g}}$.\n2.  **Kolmogorov Convergence**: $\\mathbf{S}_{\\mathbf{x}\\mathbf{x}}\\to_{\\mathbb{P}}\\mathbf{E}(\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\prime})$ and $\\bar{\\mathbf{g}}\\rightarrow_{\\mathbb{P}}\\mathbf{0}$.\n3.  **Consistency**: Since $\\mathbf{S}_{\\mathbf{x}\\mathbf{x}}^{-1}$ converges in probability to $\\mathbf{E}(\\mathbf{x}_{i}\\mathbf{x}_{i}^{\\prime})^{-1}$ and $\\bar{\\mathbf{g}}$ converges to $\\mathbf{0}$, $\\mathbf{b}-\\pmb{\\beta}$ converges to $\\mathbf{0}$ in probability. Hence, $\\mathbf{b}$ is consistent for $\\pmb{\\beta}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the given expression for the OLS estimator deviation:\n  \\(\\mathbf{b} - \\pmb{\\beta} = \\mathbf{S}_{\\mathbf{x}\\mathbf{x}}^{-1}\\bar{\\mathbf{g}}\\).\n\nStep 2: Apply the probability limit to both sides:\n  \\(\\operatorname{plim}(\\mathbf{b} - \\pmb{\\beta}) = \\operatorname{plim}(\\mathbf{S}_{\\mathbf{x}\\mathbf{x}}^{-1}\\bar{\\mathbf{g}})\\).\n\nStep 3: Use the linearity of \\(\\operatorname{plim}\\):\n  \\(\\operatorname{plim}(\\mathbf{b}) - \\pmb{\\beta} = \\operatorname{plim}(\\mathbf{S}_{\\mathbf{x}\\mathbf{x}}^{-1}) \\cdot \\operatorname{plim}(\\bar{\\mathbf{g}})\\).\n\nStep 4: From the Kolmogorov convergence theorem (Law of Large Numbers):\n  - \\(\\mathbf{S}_{\\mathbf{x}\\mathbf{x}} = \\frac{1}{n}\\mathbf{X}'\\mathbf{X} \\to_{\\mathbb{P}} \\pmb{\\Sigma}_{\\mathbf{x}\\mathbf{x}} = \\mathbf{E}(\\mathbf{x}_i\\mathbf{x}_i')\\), so \\(\\mathbf{S}_{\\mathbf{x}\\mathbf{x}}^{-1} \\to_{\\mathbb{P}} \\pmb{\\Sigma}_{\\mathbf{x}\\mathbf{x}}^{-1}\\).\n  - \\(\\bar{\\mathbf{g}} = \\frac{1}{n}\\sum_{i=1}^n \\varepsilon_i\\mathbf{x}_i \\to_{\\mathbb{P}} \\mathbf{0}\\) (since \\(\\mathbf{E}(\\varepsilon_i\\mathbf{x}_i) = \\mathbf{0}\\) under OLS assumptions).\n\nStep 5: Substitute the probability limits into Step 3:\n  \\(\\operatorname{plim}(\\mathbf{b}) - \\pmb{\\beta} = \\pmb{\\Sigma}_{\\mathbf{x}\\mathbf{x}}^{-1} \\cdot \\mathbf{0} = \\mathbf{0}\\).\n\nStep 6: Conclude \\(\\operatorname{plim}(\\mathbf{b}) = \\pmb{\\beta}\\), proving the consistency of the OLS estimator \\(\\mathbf{b}\\).\n\nQID: textbook-63-1-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-63-1-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to prove the consistency of the OLS estimator using the given expression and the Kolmogorov convergence theorem. The reasoning aligns with the gold answer, demonstrating a clear understanding of the concepts involved.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-63-1-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to prove the consistency of the OLS estimator using the given expression and the Kolmogorov convergence theorem. The reasoning aligns with the gold answer, demonstrating a clear understanding of the concepts involved."}, "llm_echoed_qid": "textbook-63-1-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to prove the consistency of the OLS estimator using the given expression and the Kolmogorov convergence theorem. The reasoning aligns with the gold answer, demonstrating a clear understanding of the concepts involved."}
{"script_processing_qid": "textbook-112-37-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. **Conceptual**: Discuss the economic implications of potatoes becoming the staple food in Europe, as hypothesized by Adam Smith. How would this affect population and rents?\n\nGOLD_ANSWER:\n1.  **Higher Yield**: Potatoes produce more food per acre, supporting a larger population.\n2.  **Labor Costs**: Lower cultivation expenses increase the surplus after maintaining labor.\n3.  **Rent Dynamics**: Landlords gain a greater share of the surplus, leading to higher rents.\n4.  **Population Growth**: Increased food supply would likely lead to population growth, further driving up demand for land and rents.\n\nCANDIDATE_ANSWER:\nStep 1: **Yield Comparison**: The context states potatoes yield 12,000 weight/acre vs. 2,000 weight/acre for wheat, making potatoes 6× more productive in food output. This implies a larger food surplus per unit of land.\n\nStep 2: **Population Growth**: Greater food availability would reduce subsistence constraints, enabling population growth (as more people could be sustained by the same land area).\n\nStep 3: **Rent Dynamics**: Smith argues rent depends on surplus after labor/profit. Higher potato yields would increase total surplus. However, unlike rice (where higher labor inputs still leave landlords a larger share), potatoes’ extreme efficiency might reduce land scarcity. If potatoes became the staple, landlords could claim a larger absolute surplus (due to higher total output), but rents per unit of output might decline if land productivity gains outpace demand.\n\nStep 4: **Net Effect**: Population would rise due to abundant food, while rents could increase in absolute terms (more surplus) but decrease relative to crop value (reduced land scarcity). This contrasts with rice, where labor intensity limits surplus distribution differently.\n\nQID: textbook-112-37-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-37-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the economic implications of potatoes becoming the staple food in Europe, aligning with the gold answer's points on yield, population growth, and rent dynamics, while also providing additional nuanced analysis on rent relative to crop value.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-37-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the economic implications of potatoes becoming the staple food in Europe, aligning with the gold answer's points on yield, population growth, and rent dynamics, while also providing additional nuanced analysis on rent relative to crop value."}, "llm_echoed_qid": "textbook-112-37-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the economic implications of potatoes becoming the staple food in Europe, aligning with the gold answer's points on yield, population growth, and rent dynamics, while also providing additional nuanced analysis on rent relative to crop value."}
{"script_processing_qid": "textbook-103-3-3-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Analyze how changes in unemployment compensation $c$ affect the reservation wage $w^*$.\n\nGOLD_ANSWER:\n1. **Effect of $c$**: Higher $c$ increases the continuation value of unemployment, raising $w^*$.\n2. **Derivative**:\n$$\n\\frac{\\partial w^*}{\\partial c} = (1 - \\beta) > 0.\n$$\n3. **Interpretation**: More generous unemployment benefits lead to more selective job acceptance.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the reservation wage condition.\nAt reservation wage w*, the worker is indifferent between accepting and rejecting:\nw*/(1 - β) = c + β[φv(0) + (1 - φ)∫v(w')dF(w')]\n\nStep 2: Recognize that v(0) equals the value of rejecting (RHS):\nv(0) = c + β[φv(0) + (1 - φ)∫v(w')dF(w')]\nThis implies v(0) = w*/(1 - β).\n\nStep 3: Split the expectation integral into accepted/rejected regions:\n∫v(w')dF(w') = v(0)F(w*) + [1/(1 - β)]∫_{w*}^∞ w'dF(w')\n\nStep 4: Substitute back into reservation wage equation:\nw*/(1 - β) = c + β[φ(w*/(1 - β)) + (1 - φ){w*F(w*)/(1 - β) + [1/(1 - β)]∫_{w*}^∞ w'dF(w')}]\n\nStep 5: Multiply through by (1 - β) to simplify:\nw* = c(1 - β) + β[φw* + (1 - φ){w*F(w*) + ∫_{w*}^∞ w'dF(w')}]\n\nStep 6: Differentiate both sides with respect to c:\n1. Left side derivative: dw*/dc\n2. Right side derivative: (1 - β) + terms involving dw*/dc\n\nStep 7: Solve for dw*/dc:\nAfter simplification, we get:\ndw*/dc = (1 - β)/[1 - β(φ + (1 - φ)F(w*))] > 0\n\nConclusion: The reservation wage w* increases when unemployment compensation c increases because the derivative dw*/dc is strictly positive.\n\nQID: textbook-103-3-3-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-103-3-3-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the relationship between unemployment compensation $c$ and the reservation wage $w^*$, demonstrating that an increase in $c$ leads to a higher $w^*$. The detailed steps and final derivative match the gold answer's conclusion and interpretation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-3-3-2", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the relationship between unemployment compensation $c$ and the reservation wage $w^*$, demonstrating that an increase in $c$ leads to a higher $w^*$. The detailed steps and final derivative match the gold answer's conclusion and interpretation."}, "llm_echoed_qid": "textbook-103-3-3-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the relationship between unemployment compensation $c$ and the reservation wage $w^*$, demonstrating that an increase in $c$ leads to a higher $w^*$. The detailed steps and final derivative match the gold answer's conclusion and interpretation."}
{"script_processing_qid": "textbook-119-11-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the mean and variance of the chi-square distribution with $k$ degrees of freedom.\n\nGOLD_ANSWER:\n1.  **Mean**: $E(\\chi_k^2) = k$ (sum of $k$ independent $Z_i^2$ variables, each with mean 1).\n2.  **Variance**: $Var(\\chi_k^2) = 2k$ (sum of $k$ independent $Z_i^2$ variables, each with variance 2).\n\nCANDIDATE_ANSWER:\nStep 1: Recall that each $Z_i \\sim \\mathcal{N}(0,1)$ implies $E[Z_i^2] = Var(Z_i) = 1$.\nStep 2: For $\\chi^2(k) = \\sum_{i=1}^k Z_i^2$, compute expectation:\n$$E[\\chi^2(k)] = \\sum_{i=1}^k E[Z_i^2] = k \\times 1 = k.$$\nStep 3: Compute variance of individual $Z_i^2$. For standard normal variables:\n$$Var(Z_i^2) = E[Z_i^4] - (E[Z_i^2])^2 = 3 - 1^2 = 2.$$\nStep 4: Compute variance of $\\chi^2(k)$:\n$$Var(\\chi^2(k)) = \\sum_{i=1}^k Var(Z_i^2) = k \\times 2 = 2k.$$\nFinal Result: The chi-square distribution with $k$ df has mean $k$ and variance $2k$.\n\nQID: textbook-119-11-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-11-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives both the mean and variance of the chi-square distribution with $k$ degrees of freedom, matching the gold answer. The reasoning and calculations are accurate and complete.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-11-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives both the mean and variance of the chi-square distribution with $k$ degrees of freedom, matching the gold answer. The reasoning and calculations are accurate and complete."}, "llm_echoed_qid": "textbook-119-11-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives both the mean and variance of the chi-square distribution with $k$ degrees of freedom, matching the gold answer. The reasoning and calculations are accurate and complete."}
{"script_processing_qid": "textbook-119-19-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Why is the OLS estimator $\\hat{\\beta}\\_{2}$ still unbiased under heteroscedasticity?\n\nGOLD_ANSWER:\nThe unbiasedness of $\\hat{\\beta}\\_{2}$ depends only on the zero conditional mean assumption $E(u\\_{i}|X\\_{i}) = 0$, not on the homoscedasticity assumption. Thus, even if $E(u\\_{i}^{2}) = \\sigma\\_{i}^{2}$, the OLS estimator remains unbiased.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that unbiasedness of OLS depends on the zero conditional mean assumption (E(u_i|X_i) = 0), not on the homoscedasticity assumption. Heteroscedasticity only violates the constant variance assumption (E(u_i²) = σ²).\n\nStep 2: The OLS estimator formula for β₂ remains β̂₂ = Σx_iy_i/Σx_i². Substituting y_i = β₁ + β₂X_i + u_i gives β̂₂ = β₂ + Σx_iu_i/Σx_i².\n\nStep 3: Take expectations: E(β̂₂) = β₂ + E[Σx_iu_i]/Σx_i². Since E(u_i|X_i) = 0 is retained (a key classical assumption), the second term becomes Σx_iE(u_i)/Σx_i² = 0.\n\nStep 4: Therefore, E(β̂₂) = β₂. This conclusion holds regardless of heteroscedasticity because it only requires the zero conditional mean assumption (E(u_i|X_i) = 0), which remains intact in the described scenario.\n\nQID: textbook-119-19-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-19-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies that the unbiasedness of the OLS estimator depends on the zero conditional mean assumption and not on homoscedasticity. The detailed steps provided align perfectly with the gold answer, demonstrating a thorough understanding of the concept.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-19-0-1", "category": "CORRECT", "explanation": "The candidate answer correctly identifies that the unbiasedness of the OLS estimator depends on the zero conditional mean assumption and not on homoscedasticity. The detailed steps provided align perfectly with the gold answer, demonstrating a thorough understanding of the concept."}, "llm_echoed_qid": "textbook-119-19-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies that the unbiasedness of the OLS estimator depends on the zero conditional mean assumption and not on homoscedasticity. The detailed steps provided align perfectly with the gold answer, demonstrating a thorough understanding of the concept."}
{"script_processing_qid": "textbook-112-45-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain how monopolies and policy regulations (e.g., statutes of apprenticeship) can cause market prices to deviate from natural prices for extended periods. Provide examples from the text.\n\nGOLD_ANSWER:\n1.  **Monopolies**: Monopolists keep the market understocked, selling commodities above the natural price. Example: A trading company with exclusive rights can maintain high prices indefinitely by limiting supply.\n2.  **Policy Regulations**: Statutes of apprenticeship restrict competition, enabling workers to raise wages above natural rates during prosperity but forcing them below during decay. Example: Corporation laws in manufacturing sectors can sustain high wages for centuries but depress wages only temporarily during downturns.\n\nCANDIDATE_ANSWER:\nStep 1: Monopolies cause sustained price deviations by artificially restricting supply. The text states that monopolists 'keep the market constantly understocked,' selling commodities 'much above the natural price' indefinitely. For example, a monopoly (e.g., a granted trading company) avoids fully meeting demand, enabling prolonged supra-natural pricing. Step 2: Policy regulations like statutes of apprenticeship create rigidities. The text explains that these laws allow workers to raise wages 'above their natural rate' during prosperity but force wages 'below it' during downturns. For instance, apprenticeship rules distort labor costs, causing market prices (via wage components) to deviate from natural prices in both boom and bust cycles. Both factors disrupt the natural gravitation mechanism by institutionalizing supply or labor-cost distortions.\n\nQID: textbook-112-45-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-45-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains how monopolies and policy regulations cause market prices to deviate from natural prices, using examples and reasoning that align with the gold answer. The response is well-structured and directly supports the points made in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-45-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains how monopolies and policy regulations cause market prices to deviate from natural prices, using examples and reasoning that align with the gold answer. The response is well-structured and directly supports the points made in the gold answer."}, "llm_echoed_qid": "textbook-112-45-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains how monopolies and policy regulations cause market prices to deviate from natural prices, using examples and reasoning that align with the gold answer. The response is well-structured and directly supports the points made in the gold answer."}
{"script_processing_qid": "textbook-107-10-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the stability of the equilibrium attendance in the context of the given recurrence relation.\n\nGOLD_ANSWER:\nThe equilibrium is stable because the coefficient of $A_t$ (0.30) is between 0 and 1. This ensures that any deviation from the equilibrium will decay over time, leading the system back to the equilibrium attendance.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the recurrence relation\nGiven: Next week's attendance = 56 + 0.30X (where X = current attendance)\n\nStep 2: Define equilibrium condition\nAt equilibrium, attendance remains constant between weeks:\nX* = 56 + 0.30X*\n\nStep 3: Solve for equilibrium attendance\nX* - 0.30X* = 56\n0.70X* = 56\nX* = 56/0.70 = 80\n\nStep 4: Analyze stability\nThe recurrence relation X_{n+1} = a + bX_n has stability determined by |b|:\n- If |b| < 1, the equilibrium is stable\n- Here b = 0.30 → |0.30| < 1\n\nStep 5: Conclusion\nSince |0.30| < 1, any deviation from X* = 80 will diminish over time. The system will automatically return to the equilibrium attendance of 80 through subsequent iterations, making the equilibrium stable.\n\nQID: textbook-107-10-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-107-10-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the equilibrium attendance, analyzes the stability condition, and concludes that the equilibrium is stable, matching the gold answer's reasoning and conclusion.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-10-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the equilibrium attendance, analyzes the stability condition, and concludes that the equilibrium is stable, matching the gold answer's reasoning and conclusion."}, "llm_echoed_qid": "textbook-107-10-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the equilibrium attendance, analyzes the stability condition, and concludes that the equilibrium is stable, matching the gold answer's reasoning and conclusion."}
{"script_processing_qid": "textbook-42-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the expenditure function corresponding to the indirect utility function $V(p,q,m)=\\alpha\\log\\alpha+\\beta\\log\\beta-(\\alpha+\\beta)\\log(\\alpha+\\beta)+(\\alpha+\\beta)\\log m-\\alpha\\log p-\\beta\\log q$.\n\nGOLD_ANSWER:\n1.  **Invert Indirect Utility**: Solve $V(p, q, m) = \\bar{u}$ for $m$.\n2.  **Expenditure Function**: $E(p, q, \\bar{u}) = e^{\\frac{\\bar{u} - \\alpha\\log\\alpha - \\beta\\log\\beta + (\\alpha+\\beta)\\log(\\alpha+\\beta) + \\alpha\\log p + \\beta\\log q}{\\alpha+\\beta}}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that the expenditure function e(p,q,u) is obtained by solving the indirect utility function V(p,q,m) = u for m.\n\nStep 2: Start with the given indirect utility function:\nV = αlogα + βlogβ - (α+β)log(α+β) + (α+β)logm - αlogp - βlogq = u\n\nStep 3: Isolate the income term (α+β)logm:\n(α+β)logm = u - αlogα - βlogβ + (α+β)log(α+β) + αlogp + βlogq\n\nStep 4: Solve for logm:\nlogm = [u - αlogα - βlogβ + (α+β)log(α+β) + αlogp + βlogq]/(α+β)\n\nStep 5: Exponentiate both sides to solve for m:\nm = exp[ (u - αlogα - βlogβ + (α+β)log(α+β) + αlogp + βlogq)/(α+β) ]\n\nStep 6: Simplify the exponential expression by breaking it into components:\n= e^{u/(α+β)} × [(α+β)^{α+β}/(α^α β^β)]^{1/(α+β)} × p^{α/(α+β)} q^{β/(α+β)}\n\nStep 7: Combine terms to get the final expenditure function:\ne(p,q,u) = (α+β)(p/α)^{α/(α+β)}(q/β)^{β/(α+β)}e^{u/(α+β)}\n\nQID: textbook-42-0-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-42-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the expenditure function from the given indirect utility function, following the same steps as the gold answer and arriving at an equivalent expression.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-42-0-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the expenditure function from the given indirect utility function, following the same steps as the gold answer and arriving at an equivalent expression."}, "llm_echoed_qid": "textbook-42-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the expenditure function from the given indirect utility function, following the same steps as the gold answer and arriving at an equivalent expression."}
{"script_processing_qid": "textbook-122-25-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the OLS estimator for the model $$colGPA=\\beta_{0}+\\beta_{1}alcohol+\\beta_{2}hsGPA+\\beta_{3}SAT+\\beta_{4}female+u$$ and state the assumptions required for OLS to be BLUE.\n\nGOLD_ANSWER:\n1.  **Model**: $colGPA = \\beta_{0} + \\beta_{1}alcohol + \\beta_{2}hsGPA + \\beta_{3}SAT + \\beta_{4}female + u$.\n2.  **OLS Objective**: Minimize the sum of squared residuals $SSR = \\sum (colGPA - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}alcohol - \\hat{\\beta}_{2}hsGPA - \\hat{\\beta}_{3}SAT - \\hat{\\beta}_{4}female)^2$.\n3.  **Normal Equations**: Differentiate SSR w.r.t. each $\\beta_j$ and set to zero:\n    - $\\frac{\\partial SSR}{\\partial \\beta_0} = -2\\sum (colGPA - \\hat{\\beta}_0 - \\hat{\\beta}_1alcohol - ...) = 0$.\n    - Similarly for $\\beta_1, \\beta_2, \\beta_3, \\beta_4$.\n4.  **Solution**: In matrix form, $\\hat{\\beta}_{OLS} = (X^TX)^{-1}X^TY$, where $X$ includes all regressors and $Y$ is $colGPA$.\n5.  **BLUE Assumptions**:\n    - Linearity: $E[u|X] = 0$.\n    - No perfect multicollinearity.\n    - Homoscedasticity: $Var(u|X) = \\sigma^2$.\n    - No autocorrelation: $Cov(u_i, u_j|X) = 0$ for $i \\neq j$.\n\nCANDIDATE_ANSWER:\nStep 1: Define the regression model in matrix form\nLet X be the matrix of predictors (alcohol, hsGPA, SAT, female, and a constant term), y be the vector of colGPA observations, and β = (β₀, β₁, β₂, β₃, β₄)'. The OLS estimator minimizes the sum of squared residuals: min Σu_i² = (y - Xβ)'(y - Xβ).\n\nStep 2: Derive the normal equations\nTake the derivative of the SSR with respect to β and set to zero:\n-2X'(y - Xβ) = 0 ⇒ X'Xβ = X'y\n\nStep 3: Solve for β\nAssuming X'X is invertible,\nβ_hat = (X'X)⁻¹X'y\n\nAssumptions for OLS to be BLUE (Gauss-Markov):\n1. Linearity in parameters: Model is correctly specified as y = Xβ + u\n2. Random sampling: Observations are independently sampled\n3. No perfect multicollinearity: X has full column rank\n4. Zero conditional mean: E(u|X) = 0\n5. Homoscedasticity: Var(u|X) = σ²I\n6. No autocorrelation (implied by random sampling in cross-sectional contexts)\n\nQID: textbook-122-25-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-25-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the OLS estimator in matrix form and lists the necessary assumptions for OLS to be BLUE, matching the gold answer in both content and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-25-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the OLS estimator in matrix form and lists the necessary assumptions for OLS to be BLUE, matching the gold answer in both content and reasoning."}, "llm_echoed_qid": "textbook-122-25-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the OLS estimator in matrix form and lists the necessary assumptions for OLS to be BLUE, matching the gold answer in both content and reasoning."}
{"script_processing_qid": "textbook-103-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the expression for the value function $v(w,0,E)$ when the worker is employed, starting from the Bellman equation $$v(w,0,E)=w-T(w)+\\beta(1-\\alpha)v(w,0,E)+\\beta\\alpha Q.$$\n\nGOLD_ANSWER:\n1.  **Bellman Equation**: $v(w,0,E) = w - T(w) + \\beta(1-\\alpha)v(w,0,E) + \\beta\\alpha Q$.\n2.  **Rearrange**: $v(w,0,E) - \\beta(1-\\alpha)v(w,0,E) = w - T(w) + \\beta\\alpha Q$.\n3.  **Factor**: $v(w,0,E)[1 - \\beta(1-\\alpha)] = w - T(w) + \\beta\\alpha Q$.\n4.  **Solve**: $v(w,0,E) = \\frac{w - T(w)}{1 - \\beta(1-\\alpha)} + \\frac{\\beta\\alpha Q}{1 - \\beta(1-\\alpha)}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the Bellman equation provided:\n$$v(w,0,E)=w-T(w)+\\beta(1-\\alpha)v(w,0,E)+\\beta\\alpha Q.$$\n\nStep 2: Collect terms containing $v(w,0,E)$ on the left side:\n$$v(w,0,E) - \\beta(1-\\alpha)v(w,0,E) = w-T(w) + \\beta\\alpha Q.$$\n\nStep 3: Factor out $v(w,0,E)$:\n$$v(w,0,E)[1 - \\beta(1-\\alpha)] = w-T(w) + \\beta\\alpha Q.$$\n\nStep 4: Solve for $v(w,0,E)$ by dividing both sides by $[1 - \\beta(1-\\alpha)]$:\n$$v(w,0,E) = \\frac{w-T(w)}{1-\\beta(1-\\alpha)} + \\frac{\\beta\\alpha Q}{1-\\beta(1-\\alpha)}.$$\n\nThis matches the expression given in the context, showing the decomposition into the present value of net earnings and the expected continuation value of unemployment.\n\nQID: textbook-103-0-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-103-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the value function from the Bellman equation, matching the gold answer exactly in both process and final expression.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-0-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the value function from the Bellman equation, matching the gold answer exactly in both process and final expression."}, "llm_echoed_qid": "textbook-103-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the value function from the Bellman equation, matching the gold answer exactly in both process and final expression."}
{"script_processing_qid": "textbook-114-1-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Interpret the income elasticity of copper demand ($E_I = 1.3$) in the context of the 1980s demand decline. How does this explain price behavior?\n\nGOLD_ANSWER:\n1. **Interpretation**: A 1% income increase raises copper demand by 1.3%.\n2. **1980s Context**: Recessions reduced income, lowering demand. Coupled with substitutes (aluminum), this caused prolonged price declines despite supply disruptions.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the income elasticity value (E_I = 1.3). This indicates that copper demand is income-elastic – a 1% decrease in income would reduce demand by 1.3%, making copper a cyclical commodity sensitive to economic conditions.\n\nStep 2: Apply this to the 1980s context. The global recession likely caused income declines, leading to a disproportionate 20% demand drop (consistent with E_I = 1.3 requiring a ≈15.4% income decline to produce a 20% demand decrease).\n\nStep 3: Calculate the price impact using the given supply-demand equations:\nOriginal equilibrium: P* = 0.75, Q* = 7.5\n20% demand reduction: New demand = 0.8*(13.5 - 8P) = 10.8 - 6.4P\nEquilibrium condition: 10.8 - 6.4P = -4.5 + 16P\n15.3 = 22.4P → P ≈ 0.683\n\nStep 4: Compare prices. The price declines from $0.75 to $0.683 (≈9% drop). The high income elasticity explains why the demand shock was amplified, while the relatively elastic supply (elasticity 1.6) allowed prices to adjust downward without catastrophic quantity reductions.\n\nQID: textbook-114-1-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-1-1-2\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate correctly interprets the income elasticity and applies it to the 1980s context, but the detailed calculations and focus on specific price changes are unnecessary and not aligned with the gold answer, which emphasizes the broader economic context and substitutes.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-1-1-2", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate correctly interprets the income elasticity and applies it to the 1980s context, but the detailed calculations and focus on specific price changes are unnecessary and not aligned with the gold answer, which emphasizes the broader economic context and substitutes."}, "llm_echoed_qid": "textbook-114-1-1-2", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate correctly interprets the income elasticity and applies it to the 1980s context, but the detailed calculations and focus on specific price changes are unnecessary and not aligned with the gold answer, which emphasizes the broader economic context and substitutes."}
{"script_processing_qid": "textbook-112-29-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the relationship between capital accumulation and the increase in productive labor, as posited by Adam Smith.\n\nGOLD_ANSWER:\n1.  **Capital and Labor**: Smith argues that the number of productive laborers can only increase with an increase in capital or funds for their maintenance.\n2.  **Mechanization**: Capital allows for better machinery, which enhances labor productivity.\n3.  **Division of Labor**: Capital enables a more efficient division and distribution of employment.\n4.  **Mathematical Representation**: If $L$ is labor and $K$ is capital, then $\\Delta L \\propto \\Delta K$, assuming other factors constant.\n\nCANDIDATE_ANSWER:\nStep 1: Adam Smith posits that increased capital accumulation directly enables the employment of more productive laborers. The text states: 'The number of its productive labourers... can never be much increased, but in consequence of an increase of capital.' More capital provides funds to sustain additional workers.\n\nStep 2: Capital accumulation also enhances labor productivity through investments in 'machines and instruments which facilitate and abridge labour' or improved division of labor. This allows the same number of workers to produce more output.\n\nStep 3: When comparing a nation's output across two periods (e.g., greater annual produce, better cultivation, flourishing industries), Smith concludes that such progress necessarily implies capital accumulation has occurred. Increased output requires either more laborers (funded by capital) or improved productivity (enabled by capital investments).\n\nStep 4: Durable capital goods (e.g., machinery, infrastructure) amplify this relationship further by promoting frugality and sustained accumulation. Expenditure on such goods creates lasting productive capacity rather than transient consumption.\n\nQID: textbook-112-29-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-29-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately captures Adam Smith's relationship between capital accumulation and productive labor, including the employment of more laborers, enhanced productivity through machinery and division of labor, and the role of durable capital goods. The reasoning aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-29-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately captures Adam Smith's relationship between capital accumulation and productive labor, including the employment of more laborers, enhanced productivity through machinery and division of labor, and the role of durable capital goods. The reasoning aligns with the gold answer."}, "llm_echoed_qid": "textbook-112-29-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately captures Adam Smith's relationship between capital accumulation and productive labor, including the employment of more laborers, enhanced productivity through machinery and division of labor, and the role of durable capital goods. The reasoning aligns with the gold answer."}
{"script_processing_qid": "textbook-118-8-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Prove that the rank condition fails if $\\pmb{\\Pi}_{21}$ (from the reduced form) does not have full column rank.\n\nGOLD_ANSWER:\n1. **Reduced form**: $Y_1 = Z_1\\pmb{\\Pi}_{11} + W_1\\pmb{\\Pi}_{21} + V_1$, where $W_1$ contains excluded instruments.\n2. **Projection**: $P\\_{W}Y_1 = Z_1\\pmb{\\Pi}_{11} + W_1\\pmb{\\Pi}_{21} + P\\_{W}V_1$.\n3. **Asymptotic rank**: $\\text{plim } n^{-1}X_1^{\\top}P\\_{W}X_1$ depends on $\\begin{bmatrix} \\mathbf{I}_{k_{11}} & \\pmb{\\Pi}_{11} \\\\ \\mathbf{O} & \\pmb{\\Pi}_{21} \\end{bmatrix}$. If $\\pmb{\\Pi}_{21}$ is rank-deficient, this matrix cannot have full column rank, violating the rank condition.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that the rank condition requires $X_i^{\\top}P_WX_i$ to be nonsingular (full column rank $k_i$). This depends on $P_WX_i$ capturing sufficient independent variation from the instruments $W$.\n\nStep 2: $\\pmb{\\Pi}_{21}$ in the reduced form represents the coefficients of excluded instruments $W$ on the endogenous variables in the structural equation. If $\\pmb{\\Pi}_{21}$ lacks full column rank, there exists a non-zero vector $\\mathbf{c}$ such that $\\pmb{\\Pi}_{21}\\mathbf{c} = \\mathbf{0}$.\n\nStep 3: This linear dependence implies $W\\pmb{\\Pi}_{21}\\mathbf{c} = \\mathbf{0}$, meaning the instruments $W$ cannot generate independent variation for all endogenous variables. Consequently, $P_WX_i$ (which includes $W\\pmb{\\Pi}_{21}$) will have linearly dependent columns.\n\nStep 4: With $P_WX_i$ rank-deficient, $X_i^{\\top}P_WX_i$ becomes singular, violating the rank condition. Thus, failure of $\\pmb{\\Pi}_{21}$ to have full column rank directly causes failure of the rank condition.\n\nQID: textbook-118-8-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-8-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly explains why the rank condition fails when $\\pmb{\\Pi}_{21}$ does not have full column rank. The reasoning aligns with the gold answer, detailing the linear dependence and its impact on the rank of $P_WX_i$ and $X_i^{\\top}P_WX_i$.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-8-1-1", "category": "CORRECT", "explanation": "The candidate's answer correctly explains why the rank condition fails when $\\pmb{\\Pi}_{21}$ does not have full column rank. The reasoning aligns with the gold answer, detailing the linear dependence and its impact on the rank of $P_WX_i$ and $X_i^{\top}P_WX_i$."}, "llm_echoed_qid": "textbook-118-8-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly explains why the rank condition fails when $\\pmb{\\Pi}_{21}$ does not have full column rank. The reasoning aligns with the gold answer, detailing the linear dependence and its impact on the rank of $P_WX_i$ and $X_i^{\top}P_WX_i$."}
{"script_processing_qid": "textbook-119-2-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Show that the intercept term is zero in a regression model with standardized variables.\n\nGOLD_ANSWER:\n1. **Standardized Regression**: $Y_{i}^{*} = \\hat{\\beta}_{1}^{*} + \\hat{\\beta}_{2}^{*}X_{i}^{*} + u_{i}^{*}$.\n2. **Mean of Standardized Variables**: $\\bar{Y}^{*} = 0$, $\\bar{X}^{*} = 0$.\n3. **Intercept**: $\\hat{\\beta}_{1}^{*} = \\bar{Y}^{*} - \\hat{\\beta}_{2}^{*}\\bar{X}^{*} = 0$.\n4. **Result**: The intercept term is zero in the standardized regression model.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that in a standard linear regression model with standardized variables, both the dependent variable (Y) and independent variable(s) (X) are transformed to have mean = 0 and standard deviation = 1.\n\nStep 2: The regression equation with standardized variables can be written as: \nY* = β₀* + β₁*X* + ε*\nwhere:\n- Y* = (Y - Ȳ)/σ_Y\n- X* = (X - X̄)/σ_X\n- Ȳ and X̄ are the original means\n- σ_Y and σ_X are the original standard deviations\n\nStep 3: Calculate the intercept term β₀* using the OLS formula:\nβ₀* = Ȳ* - β₁*X̄*\n\nStep 4: Since both Y* and X* are standardized:\nȲ* = E[(Y - Ȳ)/σ_Y] = 0\nX̄* = E[(X - X̄)/σ_X] = 0\n\nStep 5: Substitute these zero means into the intercept formula:\nβ₀* = 0 - β₁*(0) = 0\n\nConclusion: The intercept term β₀* must equal zero in a regression model with standardized variables because both the dependent and independent variables have zero means after standardization.\n\nQID: textbook-119-2-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-2-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to show that the intercept term is zero in a regression model with standardized variables, matching the reasoning and conclusion of the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-2-2-3", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to show that the intercept term is zero in a regression model with standardized variables, matching the reasoning and conclusion of the gold answer."}, "llm_echoed_qid": "textbook-119-2-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to show that the intercept term is zero in a regression model with standardized variables, matching the reasoning and conclusion of the gold answer."}
{"script_processing_qid": "textbook-41-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the profit maximization condition for a production set $\\Upsilon_{1}$ under price vector $\\mathbf{p}$.\n\nGOLD_ANSWER:\n1. **Profit Maximization Condition**: For a production set $\\Upsilon_{1}$, the profit maximization condition under price vector $\\mathbf{p}$ is given by $\\mathbf{p}\\cdot\\mathbf{y}_{1}^{*} \\geq \\mathbf{p}\\cdot\\mathbf{y}_{1}$ for all $\\mathbf{y}_{1} \\in \\Upsilon_{1}$. This ensures that $\\mathbf{y}_{1}^{*}$ maximizes profits given the prices.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the key elements from the context. The production set is Υ₁, and the profit maximization condition relates to price vector p. The context states that p·y₁* = p·y₁ for all y₁ ∈ Υ₁, with p ≥ 0 established through contradiction.\n\nStep 2: Observe that if p_f < 0 for any input/output component l, one could make p·y₁ arbitrarily large by choosing extreme values for j_ℓ1 (contradicting the equilibrium condition). Hence, all prices must be non-negative (p ≥ 0).\n\nStep 3: From the context, p·y₁ ≤ 0 for all y₁ ∈ Υ₁. Since the zero vector Ω ∈ Υ₁ (standard free-disposal assumption), substituting Ω gives p·Ω = 0. Thus, the maximum attainable profit is zero (p·y₁* = 0).\n\nStep 4: Combine these results. The profit maximization condition for Υ₁ under p is:\n  (1) p ≥ 0 (non-negative prices)\n  (2) p·y₁ ≤ 0 for all y₁ ∈ Υ₁ (no positive profits feasible)\n  (3) p·y₁* = 0 (profit maximization achieved at y₁* with zero profit).\nThis implies the equilibrium condition maximizes profits at zero under non-negative prices due to free disposal and production set constraints.\n\nQID: textbook-41-0-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-41-0-0-0\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer incorrectly states that the maximum attainable profit is zero and introduces conditions not present in the gold answer. The gold answer specifies that the profit maximization condition is p·y₁* ≥ p·y₁ for all y₁ ∈ Υ₁, without any mention of zero profit or non-negative prices as necessary conditions.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-41-0-0-0", "category": "INCORRECT", "explanation": "The candidate's answer incorrectly states that the maximum attainable profit is zero and introduces conditions not present in the gold answer. The gold answer specifies that the profit maximization condition is p·y₁* ≥ p·y₁ for all y₁ ∈ Υ₁, without any mention of zero profit or non-negative prices as necessary conditions."}, "llm_echoed_qid": "textbook-41-0-0-0", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer incorrectly states that the maximum attainable profit is zero and introduces conditions not present in the gold answer. The gold answer specifies that the profit maximization condition is p·y₁* ≥ p·y₁ for all y₁ ∈ Υ₁, without any mention of zero profit or non-negative prices as necessary conditions."}
{"script_processing_qid": "textbook-119-32-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of heteroscedasticity in the context of the Chow test and suggest how to address it.\n\nGOLD_ANSWER:\n1. **Implications**: Heteroscedasticity violates the Chow test's assumption of equal error variances, leading to biased test statistics and incorrect conclusions.\n2. **Detection**: Use tests like the Breusch-Pagan or White test to check for heteroscedasticity.\n3. **Solutions**: \n   - Transform the data (e.g., log transformation).\n   - Use robust standard errors.\n   - Apply weighted least squares (WLS).\n   - Modify the Chow test to account for heteroscedasticity, though this is complex and beyond basic econometrics.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the Chow test's core assumptions. The test assumes homoscedastic errors (equal variance σ² in both subperiods) and independence between subperiod errors. Heteroscedasticity violates the homoscedasticity assumption.\n\nStep 2: Implications of heteroscedasticity. When variances differ across subperiods, the F-statistic in the Chow test becomes unreliable. This may inflate Type I/II errors because the test incorrectly assumes equal variances when calculating RSS₁ + RSS₂ for RSS_UR and comparing it to RSS_R.\n\nStep 3: Addressing the issue. To remedy this:\n(a) Use heteroscedasticity-robust versions of the Chow test (e.g., Wald tests with HC standard errors).\n(b) Replace the standard F-test with a modified F-statistic that accounts for unequal variances, such as using separate variance estimates for RSS₁ and RSS₂ in the denominator.\n(c) Apply data transformations (e.g., log transformations) or weighted least squares to stabilize variances before applying the Chow test, if justified by the data structure.\n\nQID: textbook-119-32-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-32-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies the implications of heteroscedasticity on the Chow test and provides detailed, appropriate solutions, aligning closely with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-32-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately identifies the implications of heteroscedasticity on the Chow test and provides detailed, appropriate solutions, aligning closely with the gold answer."}, "llm_echoed_qid": "textbook-119-32-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies the implications of heteroscedasticity on the Chow test and provides detailed, appropriate solutions, aligning closely with the gold answer."}
{"script_processing_qid": "textbook-96-0-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the short-run and long-run cost functions for a Cobb-Douglas production function with one fixed input. Why is the long-run cost function the lower envelope of short-run cost functions?\n\nGOLD_ANSWER:\n1. **Short-Run**: Fixed $\\bar{z}\\_{2}$ leads to higher costs due to inflexibility.\n2. **Long-Run**: All inputs are adjustable, minimizing costs for any output level.\n3. **Envelope**: Long-run cost is the minimum achievable cost across all possible input combinations, hence it lies below all short-run cost curves except at the point where the fixed input is optimal.\n\nCANDIDATE_ANSWER:\nStep 1: Understand Short-Run vs. Long-Run Constraints\nIn the short-run, one input (e.g., capital) is fixed. The firm minimizes costs by adjusting variable inputs (e.g., labor) subject to this constraint. In the long-run, all inputs are variable, allowing full optimization.\n\nStep 2: Flexibility in Input Adjustment\nFor a Cobb-Douglas production function, the long-run cost function allows substitution between all inputs. The short-run cost function restricts substitution because one input is fixed, leading to suboptimal input combinations for most output levels.\n\nStep 3: Envelope Property\nThe long-run cost function is the lower envelope of all short-run cost functions because:\n- At the output level where the fixed input in a given short-run scenario equals its long-run optimal level, the short-run and long-run costs coincide (tangency point).\n- For all other output levels, the long-run cost is strictly lower since the firm can adjust the previously fixed input to an optimal level, eliminating the inefficiency imposed by the short-run constraint.\n\nStep 4: Mathematical Intuition\nLong-run cost minimization solves $\\min_{\\mathbf{z}} \\mathbf{w} \\cdot \\mathbf{z}$ with full input flexibility. Short-run minimization solves the same problem but with a fixed input, leading to higher costs except at the tangency point. The long-run cost function 'envelopes' these short-run curves from below.\n\nQID: textbook-96-0-2-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-96-0-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the differences between short-run and long-run cost functions, the flexibility in input adjustment, and the envelope property. It aligns well with the gold answer, providing a clear and detailed explanation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-96-0-2-3", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the differences between short-run and long-run cost functions, the flexibility in input adjustment, and the envelope property. It aligns well with the gold answer, providing a clear and detailed explanation."}, "llm_echoed_qid": "textbook-96-0-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the differences between short-run and long-run cost functions, the flexibility in input adjustment, and the envelope property. It aligns well with the gold answer, providing a clear and detailed explanation."}
{"script_processing_qid": "textbook-110-8-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why single-peaked preferences prevent cyclical majorities.\n\nGOLD_ANSWER:\nSingle-peaked preferences ensure that voters' preferences are ordered along a single dimension, preventing cycles by aligning preferences around a central point (the median). This structure eliminates the possibility of non-transitive social preferences.\n\nCANDIDATE_ANSWER:\nStep 1: Single-peaked preferences (Definition C.2) imply all voters have a unique \"peak\" preference θ^h, with strictly decreasing preferences as alternatives move away from θ^h in either direction along a one-dimensional policy spectrum.\n\nStep 2: When preferences are single-peaked, the median voter’s peak acts as a natural Condorcet winner (Lemma C.4/Black’s Theorem). For any alternative θ < θ_median, a majority (median voter + all voters with θ^h ≥ θ_median) prefers θ_median over θ. Similarly, for any θ > θ_median, a majority (median voter + all voters with θ^h ≤ θ_median) prefers θ_median over θ.\n\nStep 3: Cyclical majorities require intransitive social preferences (e.g., A > B > C > A). However, the existence of a Condorcet winner (θ_median) ensures θ_median beats all other alternatives in pairwise comparisons. This eliminates cycles because θ_median cannot be defeated by any other alternative, breaking potential loops in social preferences.\n\nQID: textbook-110-8-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-110-8-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains how single-peaked preferences prevent cyclical majorities by detailing the role of the median voter as a Condorcet winner and how this eliminates intransitive social preferences, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-8-1-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains how single-peaked preferences prevent cyclical majorities by detailing the role of the median voter as a Condorcet winner and how this eliminates intransitive social preferences, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-110-8-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains how single-peaked preferences prevent cyclical majorities by detailing the role of the median voter as a Condorcet winner and how this eliminates intransitive social preferences, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-103-5-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Prove that the optimal policy for an unemployed worker has the reservation wage property.\n\nGOLD_ANSWER:\n1. **Reservation Wage Definition**: The reservation wage $\\bar{w}$ is the wage at which the worker is indifferent between accepting the job and remaining unemployed: $$ V^{e}(\\bar{w}) = V^{u}(U) $$\n2. **Optimal Policy**: For any wage $w^{\\prime} \\geq \\bar{w}$, $V^{e}(w^{\\prime}) \\geq V^{u}(U)$, so the worker accepts the offer. For $w^{\\prime} < \\bar{w}$, the worker rejects the offer.\n3. **Existence**: Since $V^{e}(w^{\\prime})$ is strictly increasing in $w^{\\prime}$ and $V^{u}(U)$ is independent of $w^{\\prime}$, there exists a unique $\\bar{w}$ satisfying the indifference condition.\n\nCANDIDATE_ANSWER:\nStep 1: Define value functions for employed and unemployed workers:\n- Employed value: $V_e(w) = \\frac{w + \\beta\\lambda V_u}{1 - \\beta(1-\\lambda)}$\n- Unemployed value: $V_u = \\beta\\left[\\mu \\int \\max\\{V_e(w), V_u\\} dF(w) + (1-\\mu)V_u\\right]$\n\nStep 2: Establish monotonicity of $V_e(w)$\nSince $\\partial V_e/\\partial w = \\frac{1}{1 - \\beta(1-\\lambda)} > 0$, $V_e(w)$ strictly increases with $w$.\n\nStep 3: Prove existence of reservation wage $\\xi$\nThere exists $\\xi$ where $V_e(\\xi) = V_u$:\n1. $V_e(0) < V_u$ (low wages aren't accepted)\n2. $\\lim_{w\\to\\infty} V_e(w) > V_u$ (high wages are accepted)\n3. By Intermediate Value Theorem and monotonicity, unique $\\xi$ exists\n\nStep 4: Derive reservation wage equation\nAt $w=\\xi$:\n$\\frac{\\xi}{1-\\beta} = \\beta\\mu \\int_{\\xi}^\\infty \\frac{w + \\beta\\lambda V_u}{1 - \\beta(1-\\lambda)} dF(w) + \\beta[\\mu F(\\xi) + (1-\\mu)]V_u$\nSubstituting $V_u = \\xi/(1-\\beta)$ completes the proof.\n\nStep 5: Optimal policy characterization\nFor $w > \\xi$: $V_e(w) > V_u \\implies$ accept\nFor $w < \\xi$: $V_e(w) < V_u \\implies$ reject\nThis demonstrates the reservation wage property.\n\nQID: textbook-103-5-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-103-5-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly proves the reservation wage property by defining value functions, establishing monotonicity, proving existence of the reservation wage, deriving its equation, and characterizing the optimal policy, matching the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-5-1-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly proves the reservation wage property by defining value functions, establishing monotonicity, proving existence of the reservation wage, deriving its equation, and characterizing the optimal policy, matching the gold answer's key points."}, "llm_echoed_qid": "textbook-103-5-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly proves the reservation wage property by defining value functions, establishing monotonicity, proving existence of the reservation wage, deriving its equation, and characterizing the optimal policy, matching the gold answer's key points."}
{"script_processing_qid": "textbook-104-5-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the significance of Condition M in Theorem 11.12 and how it ensures geometric convergence to a unique invariant measure.\n\nGOLD_ANSWER:\n2. **Significance of Condition M**:\n   - **Stronger than Condition D**: Condition M ensures that for any set A, either A or its complement has a uniform lower bound on transition probabilities.\n   - **Contraction Mapping**: By Lemma 11.11, T*N is a contraction, ensuring geometric convergence to a unique invariant measure λ*.\n   - **Uniform Rate**: The convergence rate (1 - ε)^k is uniform across all initial measures λ0.\n\nCANDIDATE_ANSWER:\nStep 1: Understand Condition M's core requirement\nCondition M imposes that for any measurable set A, either:\n- PN(s, A) ≥ ε for all s ∈ S, or\n- PN(s, A^c) ≥ ε for all s ∈ S\nThis creates a uniform lower bound on transition probabilities to either A or its complement after N steps.\n\nStep 2: Recognize geometric convergence mechanism\nThe inequality ||T*Nkλ0 - λ*|| ≤ (1 - ε)^k shows exponential decay:\n- (1 - ε)^k represents geometric contraction\n- Each k iteration reduces distance by factor (1 - ε)\n\nStep 3: Establish uniqueness through contradiction prevention\nCondition M eliminates multiple ergodic sets by:\n1. Preventing existence of disjoint sets with < ε transition probabilities\n2. Making state space 'inseparable' under N-step transitions\n3. Forcing all trajectories to mix sufficiently quickly\n\nStep 4: Ensure contraction mapping properties\nFor any two measures μ,ν:\n||T*Nμ - T*Nν|| ≤ (1 - ε)||μ - ν||\nThe ε from Condition M directly creates this contraction coefficient.\n\nStep 5: Connect to unique invariant measure\nBy Banach fixed-point theorem:\n- Contraction mappings have unique fixed points\n- λ* emerges as this unique fixed point\n- Geometric convergence follows from repeated contraction\n\nSignificance: Condition M ensures the Markov chain is sufficiently mixing across the entire state space to guarantee both uniqueness of the invariant measure and exponential convergence rates through its uniform probabilistic coverage requirement.\n\nQID: textbook-104-5-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-104-5-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the significance of Condition M, its role in ensuring geometric convergence, and how it guarantees a unique invariant measure. The explanation aligns well with the gold answer, covering all key points such as the uniform lower bound on transition probabilities, the contraction mapping property, and the uniqueness of the invariant measure.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-104-5-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the significance of Condition M, its role in ensuring geometric convergence, and how it guarantees a unique invariant measure. The explanation aligns well with the gold answer, covering all key points such as the uniform lower bound on transition probabilities, the contraction mapping property, and the uniqueness of the invariant measure."}, "llm_echoed_qid": "textbook-104-5-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the significance of Condition M, its role in ensuring geometric convergence, and how it guarantees a unique invariant measure. The explanation aligns well with the gold answer, covering all key points such as the uniform lower bound on transition probabilities, the contraction mapping property, and the uniqueness of the invariant measure."}
{"script_processing_qid": "textbook-111-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss how unemployment insurance affects the natural rate of unemployment. What are the potential benefits and costs of such policies?\n\nGOLD_ANSWER:\n**Effects**:\n- **Increased Frictional Unemployment**: Unemployment insurance reduces the urgency to find jobs, lowering the job finding rate ($f$).\n- **Higher Natural Rate**: The steady-state unemployment rate $\\frac{s}{s + f}$ rises as $f$ decreases.\n\n**Benefits**:\n- Reduces income uncertainty for workers.\n- May improve job matches by allowing workers to reject unsuitable offers.\n\n**Costs**:\n- Higher unemployment rates.\n- Potential inefficiencies in labor market dynamics.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the natural rate formula. The steady-state unemployment rate is given by U/L = s/(s + f), where s = job separation rate and f = job finding rate.\n\nStep 2: Effect of unemployment insurance on f. Unemployment insurance reduces the urgency for unemployed workers to accept new jobs immediately, lowering the job finding rate f (workers spend more time searching).\n\nStep 3: Impact on natural rate. A lower f increases U/L (since the denominator s + f decreases). Thus, unemployment insurance raises the natural rate of unemployment.\n\nStep 4: Benefits. (a) Provides income stability during job transitions, reducing economic hardship. (b) May improve job matches by allowing workers to search longer for suitable positions, potentially increasing long-term productivity.\n\nStep 5: Costs. (a) Higher equilibrium unemployment due to reduced f. (b) Fiscal burden of funding the program through taxes. (c) Possible moral hazard (workers may delay job acceptance artificially).\n\nConclusion: Unemployment insurance increases the natural rate via reduced job finding rates but offers social insurance benefits that policymakers must weigh against efficiency costs.\n\nQID: textbook-111-0-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the effects of unemployment insurance on the natural rate of unemployment, including the reduction in the job finding rate and the subsequent increase in the natural rate. It also correctly identifies the benefits and costs, aligning closely with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-0-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately explains the effects of unemployment insurance on the natural rate of unemployment, including the reduction in the job finding rate and the subsequent increase in the natural rate. It also correctly identifies the benefits and costs, aligning closely with the gold answer."}, "llm_echoed_qid": "textbook-111-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the effects of unemployment insurance on the natural rate of unemployment, including the reduction in the job finding rate and the subsequent increase in the natural rate. It also correctly identifies the benefits and costs, aligning closely with the gold answer."}
{"script_processing_qid": "textbook-42-0-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. What properties must the utility function satisfy for the indirect utility function to be homogeneous of degree zero in prices and income?\n\nGOLD_ANSWER:\nThe utility function must be **homothetic** (or homogeneous of degree one) for the indirect utility function to satisfy $V(\\lambda p, \\lambda m) = V(p, m)$ for all $\\lambda > 0$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the requirement for homogeneity of degree zero.\nA function V(p, m) is homogeneous of degree zero if V(λp, λm) = V(p, m) for any λ > 0. For the indirect utility function, this means scaling all prices and income by λ leaves the maximum attainable utility unchanged.\n\nStep 2: Recognize the role of the budget constraint.\nThe consumer's budget constraint (p·x ≤ m) is intrinsically homogeneous of degree 1 in (p, m). Scaling prices and income by λ preserves the feasible consumption set (since λp·x ≤ λm ⇨ p·x ≤ m).\n\nStep 3: Link to utility function properties.\nFor the indirect utility function to inherit this homogeneity property, the utility maximization process must be invariant to such scaling. This requires:\n1. Local non-satiation: Ensures the consumer exhausts their budget (no 'leftover' income).\n2. Ordinal utility structure: Utility functions are unique up to monotonic transformations, so their ordinal nature preserves the homogeneity property when preferences are well-behaved.\n\nStep 4: Formalize required properties.\nThe utility function must satisfy:\n- Local non-satiation (to guarantee budget exhaustion)\n- Standard regularity conditions (continuity, monotonicity)\nThese ensure the indirect utility function reflects only relative prices and real income, making it homogeneous of degree zero in (p, m).\n\nQID: textbook-42-0-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-42-0-1-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's reasoning correctly identifies the need for local non-satiation and ordinal utility structure, but the gold answer specifies that the utility function must be homothetic (or homogeneous of degree one) for the indirect utility function to be homogeneous of degree zero, which is a more precise requirement.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-42-0-1-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's reasoning correctly identifies the need for local non-satiation and ordinal utility structure, but the gold answer specifies that the utility function must be homothetic (or homogeneous of degree one) for the indirect utility function to be homogeneous of degree zero, which is a more precise requirement."}, "llm_echoed_qid": "textbook-42-0-1-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's reasoning correctly identifies the need for local non-satiation and ordinal utility structure, but the gold answer specifies that the utility function must be homothetic (or homogeneous of degree one) for the indirect utility function to be homogeneous of degree zero, which is a more precise requirement."}
{"script_processing_qid": "textbook-31-0-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the role of local nonsatiation in establishing the First Welfare Theorem.\n\nGOLD_ANSWER:\n1. **First Welfare Theorem**: Any competitive equilibrium is Pareto optimal.\n2. **Local Nonsatiation**: Ensures that consumers spend their entire budget, preventing Pareto improvements without violating feasibility.\n\nCANDIDATE_ANSWER:\nStep 1: Local nonsatiation ensures that for any consumption bundle x_i* in a competitive equilibrium, there exists a preferred bundle arbitrarily close to x_i* (though not necessarily attainable).\n\nStep 2: In the First Welfare Theorem proof, when assuming a Pareto-dominating allocation exists, local nonsatiation implies p·x_i ≥ p·x_i* = w_i for all consumers. If x_i ≻ x_i*, then p·x_i > w_i.\n\nStep 3: This strict inequality for at least one consumer creates a key contradiction: Σp·x_i > Σw_i when summing over all consumers. Since Σw_i equals total endowment value (from profit maximization), this means the Pareto-dominating allocation would require more resources than exist in the economy.\n\nStep 4: Thus, local nonsatiation prevents Pareto improvements from being feasible by forcing any preferred allocation to violate aggregate budget constraints. This contradiction establishes that competitive equilibrium allocations must be Pareto optimal.\n\nQID: textbook-31-0-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-31-0-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains how local nonsatiation leads to a contradiction in the First Welfare Theorem proof by showing that any Pareto-dominating allocation would violate aggregate budget constraints, thus confirming the theorem's conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-31-0-2-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains how local nonsatiation leads to a contradiction in the First Welfare Theorem proof by showing that any Pareto-dominating allocation would violate aggregate budget constraints, thus confirming the theorem's conclusion."}, "llm_echoed_qid": "textbook-31-0-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains how local nonsatiation leads to a contradiction in the First Welfare Theorem proof by showing that any Pareto-dominating allocation would violate aggregate budget constraints, thus confirming the theorem's conclusion."}
{"script_processing_qid": "textbook-119-1-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the F-statistic for testing the restriction $\\beta_{2}+\\beta_{3}=1$ in the Cobb-Douglas production function, expressed in terms of RSS and $R^{2}$.\n\nGOLD_ANSWER:\n1. **F-statistic (RSS)**: $$F=\\frac{(\\mathrm{RSS}_{\\mathrm{R}}-\\mathrm{RSS}_{\\mathrm{UR}})/m}{\\mathrm{RSS}_{\\mathrm{UR}}/(n-k)}$$ where $m=1$ (one restriction), $n$ is the number of observations, and $k$ is the number of parameters in the unrestricted model.\n2. **F-statistic ($R^{2}$)**: $$F=\\frac{(R_{\\mathrm{UR}}^{2}-R_{\\mathrm{R}}^{2})/m}{(1-R_{\\mathrm{UR}}^{2})/(n-k)}$$ where $R_{\\mathrm{UR}}^{2}$ and $R_{\\mathrm{R}}^{2}$ are the $R^{2}$ values from the unrestricted and restricted models, respectively.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the general F-statistic formula for linear restrictions:\n\n$$F = \\frac{(RSS_R - RSS_{UR})/m}{RSS_{UR}/(n - k)}$$\nwhere $m$ = number of restrictions (here, $m=1$ for $\\beta_2 + \\beta_3 = 1$), $n$ = sample size, and $k$ = number of parameters in the unrestricted model.\n\nStep 2: Express $R^2$ in terms of RSS for both models:\n- For the unrestricted model: $R_{UR}^2 = 1 - \\frac{RSS_{UR}}{TSS}$\n- For the restricted model: $R_R^2 = 1 - \\frac{RSS_R}{TSS}$\nwhere $TSS$ is the total sum of squares (assumed identical for both models).\n\nStep 3: Solve for RSS differences:\n$$RSS_R - RSS_{UR} = TSS\\left[(1 - R_R^2) - (1 - R_{UR}^2)\\right] = TSS\\left(R_{UR}^2 - R_R^2\\right)$$\n\nStep 4: Substitute into the F-statistic formula:\n$$F = \\frac{(TSS\\left(R_{UR}^2 - R_R^2\\right))/1}{RSS_{UR}/(n - k)}$$\n\nStep 5: Replace $RSS_{UR}$ with $TSS(1 - R_{UR}^2)$:\n$$F = \\frac{(R_{UR}^2 - R_R^2)}{\\frac{1 - R_{UR}^2}{n - k}} = \\frac{(R_{UR}^2 - R_R^2)/1}{(1 - R_{UR}^2)/(n - k)}$$\n\nThis matches the formula provided in the context for $m=1$.\n\nQID: textbook-119-1-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-1-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the F-statistic in terms of both RSS and R², matching the gold answer's formulas and providing a clear step-by-step justification.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-1-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the F-statistic in terms of both RSS and R², matching the gold answer's formulas and providing a clear step-by-step justification."}, "llm_echoed_qid": "textbook-119-1-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the F-statistic in terms of both RSS and R², matching the gold answer's formulas and providing a clear step-by-step justification."}
{"script_processing_qid": "textbook-107-18-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the marginal cost (MC) and average variable cost (AVC) functions for technology 1 with the cost function $C_{1}(Q)=500+260Q-20Q^{2}+Q^{3}$. At what output level is AVC minimized?\n\nGOLD_ANSWER:\n1. **Marginal Cost (MC)**: $MC_{1}(Q) = \\frac{dC_{1}(Q)}{dQ} = 260 - 40Q + 3Q^{2}$.\n2. **Average Variable Cost (AVC)**: $AVC_{1}(Q) = \\frac{260Q - 20Q^{2} + Q^{3}}{Q} = 260 - 20Q + Q^{2}$.\n3. **Minimize AVC**: Set derivative of AVC w.r.t. Q to zero: $\\frac{dAVC_{1}(Q)}{dQ} = -20 + 2Q = 0 \\Rightarrow Q = 10$.\n4. **AVC at Q=10**: $260 - 20(10) + (10)^{2} = 260 - 200 + 100 = 160$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Variable Cost (VC) from the cost function\nGiven $C_1(Q) = 500 + 260Q - 20Q^2 + Q^3$:\n- Fixed Cost (FC) = 500 (constant term)\n- Variable Cost (VC) = $260Q - 20Q^2 + Q^3$\n\nStep 2: Derive Marginal Cost (MC)\nMC is the derivative of total cost (or equivalently, variable cost) with respect to $Q$:\n$MC = \\frac{dC_1}{dQ} = 260 - 40Q + 3Q^2$\n\nStep 3: Derive Average Variable Cost (AVC)\nAVC = VC / Q:\n$AVC = \\frac{260Q - 20Q^2 + Q^3}{Q} = 260 - 20Q + Q^2$\n\nStep 4: Find output level where AVC is minimized\nTake derivative of AVC and set to zero:\n$\\frac{d(AVC)}{dQ} = -20 + 2Q = 0$\nSolve for $Q$:\n$2Q = 20 \\implies Q = 10$\n\nStep 5: Verify minimum\nSecond derivative of AVC: $\\frac{d^2(AVC)}{dQ^2} = 2 > 0$ (convex), confirming a minimum.\n\nFinal Answer:\n- MC = $3Q^2 - 40Q + 260$\n- AVC = $Q^2 - 20Q + 260$\n- AVC is minimized at $Q = 10$\n\nQID: textbook-107-18-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-18-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the marginal cost (MC) and average variable cost (AVC) functions, identifies the output level where AVC is minimized, and verifies the minimum. The steps and final answers match the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-18-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the marginal cost (MC) and average variable cost (AVC) functions, identifies the output level where AVC is minimized, and verifies the minimum. The steps and final answers match the gold answer."}, "llm_echoed_qid": "textbook-107-18-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the marginal cost (MC) and average variable cost (AVC) functions, identifies the output level where AVC is minimized, and verifies the minimum. The steps and final answers match the gold answer."}
{"script_processing_qid": "textbook-103-0-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Characterize the reservation wages $\\bar{w}^e$, $\\bar{w}_1^u$, and $\\bar{w}_+^u$ in a model where a worker can quit a job and requalify for unemployment compensation. How do they depend on $\\gamma$?\n\nGOLD_ANSWER:\n1.  **$\\bar{w}^e$**: Reservation wage for quitting a job. It depends on the option to requalify for $\\gamma$ and is higher than $\\bar{w}_+^u$.\n2.  **$\\bar{w}_1^u$**: Reservation wage in the first period of unemployment. It increases with $\\gamma$ due to the additional income.\n3.  **$\\bar{w}_+^u$**: Reservation wage in subsequent periods. It is lower than $\\bar{w}_1^u$ and independent of $\\gamma$.\n4.  **Dependence on $\\gamma$**: $\\bar{w}_1^u$ and $\\bar{w}^e$ increase with $\\gamma$, while $\\bar{w}_+^u$ is unaffected.\n\nCANDIDATE_ANSWER:\nStep 1: Identify reservation wage definitions in the context of requalification for unemployment compensation.\n- $\bar{w}^e$: Reservation wage for quitting employment to requalify for unemployment compensation.\n- $\bar{w}_1^u$: Reservation wage during the first period of unemployment (with compensation $\\gamma$).\n- $\bar{w}_+^u$: Reservation wage in subsequent unemployment periods (without $\\gamma$).\n\nStep 2: Analyze dependence on $\\gamma$:\n1. $\bar{w}^e$ increases with $\\gamma$: Higher compensation incentivizes quitting low-wage jobs to regain $\\gamma$ eligibility.\n2. $\bar{w}_1^u$ increases with $\\gamma$: Greater income support raises the acceptable wage threshold during compensated unemployment.\n3. $\bar{w}_+^u$ is unaffected by $\\gamma$: No direct dependence since $\\gamma$ is unavailable in subsequent periods. Workers lower their reservation wage due to reduced financial support.\n\nQID: textbook-103-0-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-103-0-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately characterizes the reservation wages and their dependence on γ, matching the gold answer in both definitions and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-0-1-3", "category": "CORRECT", "explanation": "The candidate's answer accurately characterizes the reservation wages and their dependence on γ, matching the gold answer in both definitions and reasoning."}, "llm_echoed_qid": "textbook-103-0-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately characterizes the reservation wages and their dependence on γ, matching the gold answer in both definitions and reasoning."}
{"script_processing_qid": "textbook-119-25-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain the significance of the stochastic disturbance term $u_i$ in the regression model $Y_i = E(Y|X_i) + u_i$. What roles does it play?\n\nGOLD_ANSWER:\n1. **Representation**: $u_i$ captures the deviation of individual $Y_i$ from its conditional mean $E(Y|X_i)$.\n2. **Roles**:\n   - Surrogate for omitted variables.\n   - Captures intrinsic randomness in human behavior.\n   - Accounts for measurement errors.\n   - Ensures the model is not deterministic.\n\nCANDIDATE_ANSWER:\nStep 1: The stochastic disturbance term $u_i$ in the regression model $Y_i = E(Y|X_i) + u_i$ represents the deviation of individual observations $Y_i$ from the conditional mean $E(Y|X_i)$, which captures systematic relationships.\n\nStep 2: Its primary roles include:\n1. **Capturing Unobserved Factors**: It accounts for variables not explicitly included in the model (e.g., omitted variables, measurement errors).\n2. **Introducing Stochasticity**: Allows the model to reflect inherent randomness in real-world data, ensuring $Y_i$ is not perfectly determined by $X_i$.\n3. **Enabling Linearity Interpretations**: While $E(Y|X_i)$ defines the structural relationship (linear in parameters/variables as per context), $u_i$ handles residual deviations, ensuring the model remains well-specified even when the conditional expectation is nonlinear in variables (e.g., $E(Y|X_i) = \\beta_1 + \\beta_2 X_i^2$).\n\nStep 3: Without $u_i$, the model would imply deterministic relationships, which are unrealistic in empirical settings. The disturbance term ensures the model aligns with the probabilistic nature of economic phenomena.\n\nQID: textbook-119-25-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-25-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately captures the representation and roles of the stochastic disturbance term $u_i$, including its function in accounting for unobserved factors, introducing stochasticity, and enabling linearity interpretations, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-25-0-2", "category": "CORRECT", "explanation": "The candidate's answer accurately captures the representation and roles of the stochastic disturbance term $u_i$, including its function in accounting for unobserved factors, introducing stochasticity, and enabling linearity interpretations, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-119-25-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately captures the representation and roles of the stochastic disturbance term $u_i$, including its function in accounting for unobserved factors, introducing stochasticity, and enabling linearity interpretations, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-118-9-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. How does spurious regression manifest with stationary AR(1) processes? How can it be mitigated?\n\nGOLD_ANSWER:\n1. **Manifestation**: Even with stationary AR(1) series, spurious regression can lead to overrejection of the null hypothesis due to serial correlation in the error term.\\n2. **Mitigation**: Use HAC (heteroskedasticity and autocorrelation consistent) estimators to account for serial correlation, though large sample sizes may be required for accurate inference.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain enough information to answer this question. The text specifically discusses spurious regression in the context of independent random walks (nonstationary processes) and does not mention stationary AR(1) processes. Therefore, the manifestation or mitigation of spurious regression with stationary AR(1) processes cannot be addressed using the given context.\n\nQID: textbook-118-9-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-9-1-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-9-1-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}, "llm_echoed_qid": "textbook-118-9-1-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}
{"script_processing_qid": "textbook-96-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Calculate the compensating variation (CV) when a 10% tax is imposed on good 1, increasing its price to $p\\_{1}=1.1$. Use the expenditure function provided.\n\nGOLD_ANSWER:\n1. **Initial Expenditure**: $e(\\mathbf{p}^{0}, u^{0}) = 10$.\n2. **New Expenditure**: $e(\\mathbf{p}^{1}, u^{0}) = 10.476$.\n3. **CV**: $e(\\mathbf{p}^{0}, u^{0}) - e(\\mathbf{p}^{1}, u^{0}) = 10 - 10.476 = -0.476$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key components from the context:\n- Pre-tax prices: $p_1^0 = 1$, $p_2 = 1$\n- Post-tax price: $p_1^1 = 1.1$\n- Initial utility level $u^0 = 20$\n- Expenditure function format: $e(\\mathbf{p},u) = u \\cdot \\frac{p_1 p_2}{p_1 + p_2}$ (derived from $k=-1$ in CES framework)\n\nStep 2: Calculate original expenditure at $\\mathbf{p}^0$:\n$e(\\mathbf{p}^0,u^0) = 20 \\cdot \\frac{1 \\cdot 1}{1+1} = 20 \\cdot \\frac{1}{2} = 10$\n\nStep 3: Calculate new expenditure at $\\mathbf{p}^1$ to maintain $u^0$:\n$e(\\mathbf{p}^1,u^0) = 20 \\cdot \\frac{1.1 \\cdot 1}{1.1+1} = 20 \\cdot \\frac{1.1}{2.1} \\approx 10.476$\n\nStep 4: Compute Compensating Variation:\n$CV = e(\\mathbf{p}^0,u^0) - e(\\mathbf{p}^1,u^0) = 10 - 10.476 = -0.476$\n\nThis negative value indicates consumers would need $0.476$ compensation to maintain their original utility level after the price increase.\n\nQID: textbook-96-0-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-96-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to calculate the compensating variation, matching the gold answer in both the process and the final result. The detailed reasoning and calculations align perfectly with the expected outcome.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-96-0-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to calculate the compensating variation, matching the gold answer in both the process and the final result. The detailed reasoning and calculations align perfectly with the expected outcome."}, "llm_echoed_qid": "textbook-96-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to calculate the compensating variation, matching the gold answer in both the process and the final result. The detailed reasoning and calculations align perfectly with the expected outcome."}
{"script_processing_qid": "textbook-122-21-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Compute the 95% forecast interval for the unemployment rate in 1997 using the augmented model (18.49), given $\\mathsf{s e}(\\hat{f}_{n}) = 0.137$ and $\\hat{\\sigma} = 0.883$.\n\nGOLD_ANSWER:\n1.  **Standard Error of Forecast Error**: $\\mathsf{S e}(\\hat{e}_{n+1}) = \\sqrt{(0.137)^2 + (0.883)^2} \\approx 0.894$.\n2.  **95% Forecast Interval**: $5.35 \\pm 1.96 \\times 0.894 \\approx [3.6, 7.1]$.\n\nCANDIDATE_ANSWER:\nStep 1: Obtain the forecast standard error using the formula:\n √[se(ŷ_n)² + σ̂²] = √[(0.137)² + (0.883)²]\n= √[0.018769 + 0.779689] ≈ √0.798458 ≈ 0.894\n\nStep 2: Multiply by the 95% critical value (1.96 for normal distribution):\n1.96 × 0.894 ≈ 1.752\n\nStep 3: Apply to the point forecast:\n5.35 ± 1.752 → [5.35 - 1.752, 5.35 + 1.752] = [3.598, 7.102]\n\nFinal 95% forecast interval: [3.6, 7.1] (rounded to one decimal place as in context)\n\nQID: textbook-122-21-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-21-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to compute the 95% forecast interval, using the provided standard error and sigma values, and arrives at the same interval as the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-21-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to compute the 95% forecast interval, using the provided standard error and sigma values, and arrives at the same interval as the gold answer."}, "llm_echoed_qid": "textbook-122-21-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to compute the 95% forecast interval, using the provided standard error and sigma values, and arrives at the same interval as the gold answer."}
{"script_processing_qid": "textbook-122-40-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the partial effect of population size (popul) on infant mortality from the regression results, both with and without D.C.\n\nGOLD_ANSWER:\n1.  **With D.C.**: The coefficient on $\\log(popul)$ was -0.088, implying a 1% increase in population reduces infant mortality by 0.088 deaths per 1,000 (statistically insignificant).\n2.  **Without D.C.**: The coefficient becomes +0.629, indicating a 1% population increase raises infant mortality by 0.629 deaths per 1,000 (significant at 1%).\n3.  **Elasticity Interpretation**: The switch in sign and significance highlights D.C.'s role as an influential outlier distorting the true partial effect.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the regression coefficient for log(popul) in the provided equation (without D.C.). The coefficient is 0.629.\nStep 2: For a linear-log model, the partial effect of population size (popul) on infant mortality is calculated as (coefficient / popul). This represents the marginal change in infant mortality per unit increase in population.\nPartial effect without D.C.: Δinfmort/Δpopul = 0.629 / popul.\nStep 3: The regression equation with D.C. included is not provided in the context. Therefore, the partial effect with D.C. cannot be derived from the given information.\nConclusion: Only the partial effect without D.C. (0.629 / popul) can be computed based on the provided context.\n\nQID: textbook-122-40-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-40-1-1\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly calculates the partial effect without D.C. but incorrectly states that the effect with D.C. cannot be derived, as the GOLD_ANSWER provides the necessary coefficient. The reasoning for the partial effect without D.C. is correct but incomplete for the full question.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-40-1-1", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly calculates the partial effect without D.C. but incorrectly states that the effect with D.C. cannot be derived, as the GOLD_ANSWER provides the necessary coefficient. The reasoning for the partial effect without D.C. is correct but incomplete for the full question."}, "llm_echoed_qid": "textbook-122-40-1-1", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly calculates the partial effect without D.C. but incorrectly states that the effect with D.C. cannot be derived, as the GOLD_ANSWER provides the necessary coefficient. The reasoning for the partial effect without D.C. is correct but incomplete for the full question."}
{"script_processing_qid": "textbook-107-10-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Interpret the economic intuition behind the mixed strategy equilibrium in the game of chicken.\n\nGOLD_ANSWER:\nThe mixed strategy equilibrium reflects the players' uncertainty about each other's actions. Each player randomizes to make the other indifferent between swerving and driving straight, thus preventing predictable behavior that could be exploited.\n\nCANDIDATE_ANSWER:\nStep 1: In a mixed strategy equilibrium, players randomize their actions to make opponents indifferent between their strategies. Here, each player swerves with 0.80 probability and drives straight with 0.20 probability.\n\nStep 2: The equilibrium ensures that the expected payoff for driving straight equals the payoff for swerving (which is fixed at 0). For a player choosing to drive straight, the expected payoff is:\n$$(0.80 \\times 8) + (0.20 \\times -12) = 6.4 - 2.4 = 4.$$\nHowever, since swerving guarantees a payoff of 0, this suggests a miscalculation. The provided context likely contains an inconsistency, as standard game theory requires the expected payoffs to be equal in equilibrium.\n\nStep 3: Despite the numerical discrepancy, the intuition remains: players balance the risk of a crash (-12 payoff) against the reward of dominance (8 payoff). The high swerving probability (0.80) reflects strong aversion to the mutually disastrous outcome, while occasional aggression (0.20 driving straight) prevents opponents from exploiting predictable caution.\n\nQID: textbook-107-10-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-10-2-1\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly captures the intuition of mixed strategy equilibrium (players randomizing to prevent exploitation) but includes a numerical example with a miscalculation (expected payoff ≠ 0 for swerving). The gold answer focuses purely on the conceptual rationale without specific probabilities.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-10-2-1", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly captures the intuition of mixed strategy equilibrium (players randomizing to prevent exploitation) but includes a numerical example with a miscalculation (expected payoff ≠ 0 for swerving). The gold answer focuses purely on the conceptual rationale without specific probabilities."}, "llm_echoed_qid": "textbook-107-10-2-1", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly captures the intuition of mixed strategy equilibrium (players randomizing to prevent exploitation) but includes a numerical example with a miscalculation (expected payoff ≠ 0 for swerving). The gold answer focuses purely on the conceptual rationale without specific probabilities."}
{"script_processing_qid": "textbook-95-2-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the monetary policy reaction function in the SZ model, explaining the role of each component, including the lagged variables and the shock term.\n\nGOLD_ANSWER:\n1. **Monetary Policy Reaction Function**: $$ R\\_{t} = \\text{const.} + a\\_{1}M\\_{t} + a\\_{2}P c m\\_{t} + f\\_{S}(Z\\_{t-1},\\dots,Z\\_{t-q}) + \\sigma\\varepsilon\\_{t}^{s}. $$\n2. **Components**: \n   - $a\\_{1}M\\_{t}$ and $a\\_{2}P c m\\_{t}$ capture the contemporaneous response to monetary aggregates and crude materials prices.\n   - $f\\_{S}(Z\\_{t-1},\\dots,Z\\_{t-q})$ represents the linear function of lagged variables, reflecting the Fed's response to past economic conditions.\n   - $\\sigma\\varepsilon\\_{t}^{s}$ is the serially uncorrelated monetary policy shock, representing exogenous changes in policy.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the structure of the SZ monetary policy reaction function:\n$$ R_{t} = \\text{const.} + a_{1}M_{t} + a_{2}Pcm_{t} + f_{S}(Z_{t-1},\\dots,Z_{t-q}) + \\sigma\\varepsilon_{t}^{s} $$\n\nStep 2: Explain each component:\n- **Constant term (const.)**: Intercept capturing steady-state policy rate when all other terms are zero.\n- **$a_{1}M_{t}$**: Immediate response to the contemporaneous money stock ($M_t$), reflecting liquidity conditions.\n- **$a_{2}Pcm_{t}$**: Immediate response to contemporaneous crude materials prices ($Pcm_t$), capturing input-price inflation signals.\n- **$f_{S}(Z_{t-1},\\dots,Z_{t-q})$**: Linear combination of lagged variables ($Z_{t-1},...,Z_{t-q}$), representing delayed feedback to past economic conditions (e.g., output, inflation, other variables in the system).\n- **$\\sigma\\varepsilon_{t}^{s}$**: Monetary policy shock, scaled by $\\sigma > 0$, with $\\varepsilon_{t}^{s}$ being a serially uncorrelated innovation (exogenous policy deviation).\n\nStep 3: Role of lagged variables ($f_S$):\nThese account for the Fed's systematic response to historical data, such as past inflation, GDP growth, or financial conditions, embedded in the vector $Z_{t-i}$. The lag length $q > 0$ allows for persistent effects.\n\nStep 4: Role of the shock term ($\\sigma\\varepsilon_{t}^{s}$):\nRepresents unanticipated monetary policy actions not explained by the model's observed variables ($M_t$, $Pcm_t$, or lagged $Z$). It is orthogonal to other shocks (e.g., money demand shocks $\\varepsilon_{t}^{M}$) by assumption.\n\nStep 5: Identification rationale:\n- $M_t$ and $Pcm_t$ are included contemporaneously because Sims and Zha assume the Fed observes/reacts to these variables instantly.\n- The exclusion of other contemporaneous variables (e.g., $P_t$, $Y_t$) from the reaction function reflects the assumption that goods-market variables are not predetermined and respond indirectly via $Pcm_t$.\n\nQID: textbook-95-2-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-95-2-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately derives and explains the monetary policy reaction function in the SZ model, detailing each component, including the lagged variables and the shock term, matching the gold answer's requirements.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-95-2-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately derives and explains the monetary policy reaction function in the SZ model, detailing each component, including the lagged variables and the shock term, matching the gold answer's requirements."}, "llm_echoed_qid": "textbook-95-2-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately derives and explains the monetary policy reaction function in the SZ model, detailing each component, including the lagged variables and the shock term, matching the gold answer's requirements."}
{"script_processing_qid": "textbook-114-13-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain how diminishing returns to labor influence the shape of the marginal cost curve in the short run.\n\nGOLD_ANSWER:\nDiminishing returns to labor imply that the marginal product of labor (MP_L) decreases as more labor is employed. Since $\\mathrm{MC} = w/\\mathrm{MP}\\_L$, a decreasing MP_L leads to an increasing MC. Thus, the marginal cost curve slopes upward in the short run due to diminishing returns.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the relationship between marginal cost (MC) and marginal product of labor (MP_L) from the context: MC = w / MP_L.\n\nStep 2: Understand that diminishing returns to labor imply MP_L decreases as more labor is added (since each additional worker contributes less to output).\n\nStep 3: Substitute the decreasing MP_L into the MC formula. As MP_L decreases, the denominator in MC = w / MP_L becomes smaller, causing MC to increase.\n\nStep 4: Conclude that diminishing returns to labor force the marginal cost curve to slope upward in the short run. The faster MP_L declines (stronger diminishing returns), the steeper the upward-sloping MC curve becomes.\n\nQID: textbook-114-13-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-13-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the logical steps to explain how diminishing returns to labor lead to an upward-sloping marginal cost curve, matching the reasoning and conclusion of the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-13-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the logical steps to explain how diminishing returns to labor lead to an upward-sloping marginal cost curve, matching the reasoning and conclusion of the gold answer."}, "llm_echoed_qid": "textbook-114-13-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the logical steps to explain how diminishing returns to labor lead to an upward-sloping marginal cost curve, matching the reasoning and conclusion of the gold answer."}
{"script_processing_qid": "textbook-123-22-3-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What is the highest possible deficit that can be financed, and how does inflation respond to increases in $d$?\n\nGOLD_ANSWER:\n1. **Maximum deficit**: Bounded by seigniorage revenue at the inflation tax Laffer curve peak.\n2. **Inflation response**: Higher $d$ increases inflation if monetized, but the economist’s claim ignores potential Ricardian equivalence or fiscal adjustments.\n\nCANDIDATE_ANSWER:\nStep 1: Identify constraints for deficit financing\nIn this open economy with R* = β⁻¹, deficits must be financed via seigniorage (money creation). The maximum sustainable deficit occurs when seigniorage revenue is maximized.\n\nStep 2: Analyze seigniorage mechanics\nSeigniorage = (M' - M)/p' = [μM]/[(1+μ)p] = (μ/(1+μ))·(M/p)\nThis forms a Laffer curve relationship where increasing μ initially raises revenue but eventually reduces real money balances (M/p).\n\nStep 3: Determine maximum deficit\nMaximum sustainable deficit (d_max) occurs at the peak of the seigniorage Laffer curve, where η = 1 (η = money demand elasticity). For common money demand specifications, this typically happens when μ = 100% inflation rate.\n\nStep 4: Inflation response to d increases\nBelow d_max: Increases in d require higher μ to generate more seigniorage, leading to higher inflation\nAt d_max: System reaches critical threshold where no higher deficits can be financed\nAbove d_max: Deficits become unsustainable (hyperinflation or fiscal collapse)\n\nKey relationship: Inflation (μ) increases monotonically with d up to the maximum sustainable deficit point.\n\nQID: textbook-123-22-3-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-22-3-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the maximum sustainable deficit as occurring at the peak of the seigniorage Laffer curve and accurately describes the relationship between deficit increases and inflation, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-22-3-2", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the maximum sustainable deficit as occurring at the peak of the seigniorage Laffer curve and accurately describes the relationship between deficit increases and inflation, aligning with the gold answer."}, "llm_echoed_qid": "textbook-123-22-3-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the maximum sustainable deficit as occurring at the peak of the seigniorage Laffer curve and accurately describes the relationship between deficit increases and inflation, aligning with the gold answer."}
{"script_processing_qid": "textbook-123-11-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of Conditions A1 and A2 for the stability of the stochastic process.\n\nGOLD_ANSWER:\n- **Condition A1**: All eigenvalues of $A$ are strictly less than one in modulus. This ensures the process is stable and covariance stationary, as the influence of initial conditions decays over time.\n- **Condition A2**: The state-space representation takes the special form (2.4.6) with $\\tilde{A}$ stable. This allows for a unit eigenvalue in $A$ (due to the constant term) while still ensuring stationarity, as the remaining eigenvalues (those of $\\tilde{A}$) are strictly less than one. This is useful for models with deterministic trends or constants.\n\nCANDIDATE_ANSWER:\nThe provided context does not explicitly define Conditions A1 and A2. While the text discusses covariance stationarity and matrix stability, it does not specify the exact requirements or assumptions of Conditions A1 and A2. Therefore, it is impossible to analyze their implications for the stochastic process's stability based on the given information.\n\nQID: textbook-123-11-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-11-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not explicitly define Conditions A1 and A2.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-11-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not explicitly define Conditions A1 and A2."}, "llm_echoed_qid": "textbook-123-11-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not explicitly define Conditions A1 and A2."}
{"script_processing_qid": "textbook-41-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. How does the inclusion of $\\Omega \\in \\Upsilon_{1}$ impact the equilibrium analysis?\n\nGOLD_ANSWER:\n4. **Zero Profit Condition**: The inclusion of $\\Omega \\in \\Upsilon_{1}$ (the zero production vector) implies that $\\mathbf{p}\\cdot\\mathbf{y}_{1}^{*} = 0$ in equilibrium, ensuring that firms cannot make positive profits without bound.\n\nCANDIDATE_ANSWER:\nStep 1: The analysis establishes that prices must be non-negative (p ≥ 0). If any price were negative, firms could achieve arbitrarily large profits by scaling input usage (via making 𝖏ₗ1 very small), violating the equilibrium condition p·y₁* ≡ p·y₁ for all y₁ ∈ Υ₁.\n\nStep 2: With p ≥ 0, the equilibrium requires p·y₁ ≤ 0 for all y₁ ∈ Υ₁ (since negative profits are bounded below by zero when scaling inputs).\n\nStep 3: The inclusion of Ω ∈ Υ₁ (where Ω likely represents a \"no-production\" plan) directly implies p·Ω ≤ 0. However, since Ω typically corresponds to zero production (p·Ω = 0), this forces p·y₁* = 0 in equilibrium. This occurs because:\n   - Profit maximization requires p·y₁* ≥ p·Ω = 0\n   - The earlier result p·y₁ ≤ 0 for all y₁ ∈ Υ₁ implies p·y₁* ≤ 0\n   - Both conditions together yield p·y₁* = 0.\n\nStep 4: This zero-profit condition eliminates arbitrage opportunities and ensures price normalization, which is critical for equilibrium existence proofs in production economies.\n\nQID: textbook-41-0-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-41-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the impact of including Ω ∈ Υ₁ on equilibrium analysis, matching the gold answer's key point about the zero-profit condition (p·y₁* = 0) and providing additional rigorous justification for why this holds.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-41-0-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the impact of including Ω ∈ Υ₁ on equilibrium analysis, matching the gold answer's key point about the zero-profit condition (p·y₁* = 0) and providing additional rigorous justification for why this holds."}, "llm_echoed_qid": "textbook-41-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the impact of including Ω ∈ Υ₁ on equilibrium analysis, matching the gold answer's key point about the zero-profit condition (p·y₁* = 0) and providing additional rigorous justification for why this holds."}
{"script_processing_qid": "textbook-118-12-3-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Show that as $m_{2} \\to \\infty$, $m_{1}F$ converges in distribution to $\\chi^{2}(m_{1})$.\n\nGOLD_ANSWER:\n1. **Denominator**: By the law of large numbers, $y_{2}/m_{2} \\to \\operatorname{E}(z_{i}^{2}) = 1$ as $m_{2} \\to \\infty$.\n2. **Limit**: Thus, $m_{1}F = \\frac{y_{1}}{y_{2}/m_{2}} \\to y_{1} \\sim \\chi^{2}(m_{1})$ as $m_{2} \\to \\infty$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the definition of the F-statistic: $F = \\frac{y_1/m_1}{y_2/m_2}$, where $y_1 \\sim \\chi^2(m_1)$ and $y_2 \\sim \\chi^2(m_2)$ are independent.\n\nStep 2: Observe that as $m_2 \\to \\infty$, the denominator $y_2/m_2$ converges in probability to 1. This follows from the Law of Large Numbers since $\\mathbb{E}[y_2/m_2] = 1$ and $\\text{Var}(y_2/m_2) = 2/m_2 \\to 0$.\n\nStep 3: Substitute the limiting value of the denominator into the F-statistic: $m_1F = m_1 \\cdot \\frac{y_1/m_1}{y_2/m_2} \\approx \\frac{y_1}{y_2/m_2} \\to y_1$ as $m_2 \\to \\infty$.\n\nStep 4: Since $y_1 \\sim \\chi^2(m_1)$, this implies $m_1F$ converges in distribution to $\\chi^2(m_1)$ as $m_2 \\to \\infty$.\n\nQID: textbook-118-12-3-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-12-3-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to show that as \\( m_{2} \\to \\infty \\), \\( m_{1}F \\) converges in distribution to \\( \\chi^{2}(m_{1}) \\). The reasoning aligns with the gold answer, including the application of the Law of Large Numbers and the correct substitution of the limiting value.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-12-3-1", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to show that as \\( m_{2} \to \\infty \\), \\( m_{1}F \\) converges in distribution to \\( \\chi^{2}(m_{1}) \\). The reasoning aligns with the gold answer, including the application of the Law of Large Numbers and the correct substitution of the limiting value."}, "llm_echoed_qid": "textbook-118-12-3-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to show that as \\( m_{2} \to \\infty \\), \\( m_{1}F \\) converges in distribution to \\( \\chi^{2}(m_{1}) \\). The reasoning aligns with the gold answer, including the application of the Law of Large Numbers and the correct substitution of the limiting value."}
{"script_processing_qid": "textbook-62-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain why recessions are characterized by a sharp increase in job destruction rather than a slowdown in job creation, based on the findings of Davis et al.\n\nGOLD_ANSWER:\n1. **Cyclical Variation**: Job destruction exhibits greater cyclical variation than job creation.\n2. **Recession Impact**: In recessions, firms face reduced demand, leading to layoffs (higher $b_{t}$).\n3. **Job Creation**: Job creation slows mildly but does not drop as sharply as destruction increases.\n4. **Net Effect**: The unemployment rate rises primarily due to increased job destruction.\n\nCANDIDATE_ANSWER:\nStep 1: Examine the natural rate of unemployment formula provided: u* = b / (n + e + b). This shows unemployment depends on both job losing rate (b) and job finding rate (e).\nStep 2: Observe that recessions disproportionately increase job destruction (~rising ~b~). Layoffs (job destruction) rise sharply as firms cut costs rapidly during downturns.\nStep 3: While job creation (~e~) may decline moderately, the formula's structure makes unemployment more sensitive to changes in ~b~ (numerator term) than ~e~ (denominator term).\nStep 4: Davis et al.'s findings imply cyclical unemployment spikes are driven by ~b~ shocks rather than ~e~ slowdowns. A sharp increase in ~b~ dominates changes in ~e~, leading to higher u* during recessions.\n\nQID: textbook-62-0-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-62-0-0-2\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies that job destruction increases sharply during recessions, aligning with the gold answer. However, the reasoning focuses on the natural rate of unemployment formula, which is not mentioned in the gold answer. The gold answer emphasizes cyclical variation and the impact of reduced demand on layoffs, which the candidate does not address directly.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-62-0-0-2", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies that job destruction increases sharply during recessions, aligning with the gold answer. However, the reasoning focuses on the natural rate of unemployment formula, which is not mentioned in the gold answer. The gold answer emphasizes cyclical variation and the impact of reduced demand on layoffs, which the candidate does not address directly."}, "llm_echoed_qid": "textbook-62-0-0-2", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies that job destruction increases sharply during recessions, aligning with the gold answer. However, the reasoning focuses on the natural rate of unemployment formula, which is not mentioned in the gold answer. The gold answer emphasizes cyclical variation and the impact of reduced demand on layoffs, which the candidate does not address directly."}
{"script_processing_qid": "textbook-119-22-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the concepts of size and power in the context of unit root tests, highlighting the trade-offs and implications for empirical research.\n\nGOLD_ANSWER:\n1. **Size of Test**: Refers to the probability of a Type I error (rejecting a true null hypothesis). The DF test's size can be distorted if the model specification is incorrect (e.g., omitting a drift or trend).\n2. **Power of Test**: Refers to the probability of correctly rejecting a false null hypothesis. DF-type tests often have low power, especially when $\\rho$ is close to but not equal to 1, leading to frequent false acceptance of the unit root hypothesis.\n3. **Implications**: Researchers must carefully select the appropriate test specification and be aware of potential size and power issues when interpreting results.\n\nCANDIDATE_ANSWER:\nStep 1: Define Size in Unit Root Tests\n- Size refers to the probability of Type I error (rejecting a true null hypothesis). In unit root tests, it measures how often the test incorrectly rejects the null hypothesis (δ = 0, implying a unit root) when the series is truly nonstationary.\n- The Dickey-Fuller (DF) test's size is sensitive to implementation choices (e.g., inclusion of drift/trend terms). Incorrect specifications may distort the effective size, leading to unreliable conclusions.\n\nStep 2: Define Power in Unit Root Tests\n- Power measures the probability of correctly rejecting a false null hypothesis (i.e., detecting stationarity when it exists).\n- The context states DF-type tests have low power, meaning they often fail to reject the unit root hypothesis even when the series is stationary. This increases the risk of Type II errors.\n\nStep 3: Trade-offs Between Size and Power\n- Adjustments to control size (e.g., using the ADF test to address serial correlation) may reduce power. For example, adding lagged terms in the ADF test mitigates size distortions but increases model complexity, potentially lowering sensitivity to stationarity.\n- Overly conservative tests (prioritizing lower size) exacerbate low power, while overly liberal tests (prioritizing higher power) risk more false rejections of the unit root.\n\nStep 4: Implications for Empirical Research\n- Low power implies researchers may falsely conclude a series has a unit root (nonstationary), leading to unnecessary differencing or spurious regression models.\n- Size distortion risks incorrect rejection of the unit root, treating nonstationary data as stationary. This invalidates inference (e.g., t-statistics in regression).\n- Researchers must balance test specifications (e.g., lag length in ADF, inclusion of trends) and consider supplementary tests to mitigate these trade-offs.\n\nQID: textbook-119-22-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-22-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly defines size and power in unit root tests, accurately describes the trade-offs, and correctly highlights the implications for empirical research, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-22-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly defines size and power in unit root tests, accurately describes the trade-offs, and correctly highlights the implications for empirical research, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-119-22-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly defines size and power in unit root tests, accurately describes the trade-offs, and correctly highlights the implications for empirical research, aligning well with the gold answer."}
{"script_processing_qid": "textbook-103-3-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. For the parameter values $(\\lambda,\\pi,\\delta,\\theta)=(-1,1,.95,1)$, interpret the model as capturing habit persistence. Derive the law of motion for $h_t$ and the expression for $s_t$.\n\nGOLD_ANSWER:\n1. **Habit Persistence Law of Motion**:\n$$\nh_{t+1} = 0.95 h_t + c_t.\n$$\n2. **Consumption Services**:\n$$\ns_t = c_t - h_t.\n$$\n3. **Interpretation**: $h_t$ represents habit stock, and utility depends on consumption relative to habit. The negative $\\lambda$ implies diminishing marginal utility as consumption approaches habit.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the given system equations:\n\nService flow equation: \\(s_t = \\lambda h_t + \\pi c_t\\)\nHabit stock motion: \\(h_{t+1} = \\delta h_t + \\theta c_t\\)\n\nStep 2: Substitute the parameter values \\((\\lambda,\\pi,\\delta,\\theta)=(-1,1,0.95,1)\\):\n\nFor \\(s_t\\):\n\\[s_t = (-1)h_t + (1)c_t = c_t - h_t\\]\n\nFor \\(h_{t+1}\\):\n\\[h_{t+1} = (0.95)h_t + (1)c_t = 0.95h_t + c_t\\]\n\nInterpretation: The habit stock \\(h_t\\) decays at 5% per period (since \\(\\delta=0.95\\)) and is augmented by current consumption \\(c_t\\). The service flow \\(s_t\\) measures consumption net of habit stock, capturing habit persistence through the \\(c_t - h_t\\) term.\n\nQID: textbook-103-3-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-103-3-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derived the law of motion for \\(h_t\\) and the expression for \\(s_t\\) by substituting the given parameter values into the system equations. The interpretation provided also aligns with the concept of habit persistence as described in the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-3-0-2", "category": "CORRECT", "explanation": "The candidate correctly derived the law of motion for \\(h_t\\) and the expression for \\(s_t\\) by substituting the given parameter values into the system equations. The interpretation provided also aligns with the concept of habit persistence as described in the gold answer."}, "llm_echoed_qid": "textbook-103-3-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derived the law of motion for \\(h_t\\) and the expression for \\(s_t\\) by substituting the given parameter values into the system equations. The interpretation provided also aligns with the concept of habit persistence as described in the gold answer."}
{"script_processing_qid": "textbook-119-8-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Construct a 95% confidence interval for the population mean $\\mu$ given $\\bar{X} = 67$, $\\sigma = 2.5$, and $n = 100$.\n\nGOLD_ANSWER:\n1. **Formula**: $\\bar{X} \\pm 1.96 \\frac{\\sigma}{\\sqrt{n}}$.\n2. **Calculation**: $67 \\pm 1.96 \\frac{2.5}{\\sqrt{100}} = 67 \\pm 0.49$.\n3. **Interval**: $[66.51, 67.49]$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the critical z-value for a 95% confidence level. From the context, this is 1.96.\nStep 2: Calculate the standard error using formula σ/√n = 2.5/√100 = 2.5/10 = 0.25\nStep 3: Compute margin of error: 1.96 × 0.25 = 0.49\nStep 4: Construct interval: 67 ± 0.49 = [66.51, 67.49]\nFinal 95% confidence interval for μ: [66.51, 67.49]\n\nQID: textbook-119-8-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-8-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to calculate the 95% confidence interval, using the correct critical z-value, standard error, and margin of error, resulting in the correct interval [66.51, 67.49].\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-8-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to calculate the 95% confidence interval, using the correct critical z-value, standard error, and margin of error, resulting in the correct interval [66.51, 67.49]."}, "llm_echoed_qid": "textbook-119-8-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to calculate the 95% confidence interval, using the correct critical z-value, standard error, and margin of error, resulting in the correct interval [66.51, 67.49]."}
{"script_processing_qid": "textbook-119-25-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Consider the model $E(Y|X_i) = \\beta_1 + \\beta_2^2 X_i$. Show mathematically why this model is nonlinear in parameters.\n\nGOLD_ANSWER:\n1. **Model**: $E(Y|X_i) = \\beta_1 + \\beta_2^2 X_i$.\n2. **Evaluate at $X_i = 3$**: $E(Y|X_i=3) = \\beta_1 + 3\\beta_2^2$.\n3. **Nonlinearity**: The term $\\beta_2^2$ introduces a quadratic dependence on the parameter $\\beta_2$, making the model nonlinear in parameters.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that a model is linear in parameters if it can be expressed as a linear combination of parameters (β's), where each β is multiplied by a function of variables (possibly nonlinear in X) but raised only to the first power.\n\nStep 2: Examine the given model: E(Y|X_i) = β₁ + (β₂²)X_i.\n\nStep 3: Observe that the parameter β₂ appears with an exponent of 2 (β₂²). This violates the requirement for linearity in parameters, as parameters must not be raised to powers or multiplied/divided by other parameters in a linear model.\n\nStep 4: Confirm nonlinearity by substitution: If X=3 (as in the context example), the model becomes E(Y|X_i=3) = β₁ + 3β₂². This term 3β₂² is quadratic in β₂, making the relationship between E(Y|X_i) and β₂ nonlinear.\n\nConclusion: Because β₂ appears squared in the model, the model is nonlinear in parameters.\n\nQID: textbook-119-25-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-25-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the nonlinearity in the model due to the squared term β₂², and provides a clear and logical explanation that aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-25-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the nonlinearity in the model due to the squared term β₂², and provides a clear and logical explanation that aligns with the gold answer."}, "llm_echoed_qid": "textbook-119-25-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the nonlinearity in the model due to the squared term β₂², and provides a clear and logical explanation that aligns with the gold answer."}
{"script_processing_qid": "textbook-111-13-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the equation $e = \\epsilon \\times (P^* / P)$, analyze how a rise in the domestic price level ($P$) affects the nominal exchange rate ($e$).\n\nGOLD_ANSWER:\n1. **Equation**: $e = \\epsilon \\times (P^* / P)$.\n2. **Effect of $P$**: If $P$ rises, $P^* / P$ falls, reducing $e$.\n3. **Interpretation**: A higher $P$ means the domestic currency buys fewer foreign goods, depreciating $e$.\n4. **Real Exchange Rate**: $\\epsilon$ may adjust if $P$ changes relative to $P^*$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the equation \\( e = \\epsilon \\times \\left( \\frac{P^*}{P} \\right) \\), where \\( e \\) is the nominal exchange rate, \\( \\epsilon \\) is the real exchange rate, \\( P^* \\) is the foreign price level, and \\( P \\) is the domestic price level.\n\nStep 2: Assume \\( \\epsilon \\) and \\( P^* \\) remain constant in the short run (as implied by the context, where the real exchange rate adjusts to balance net exports with the fixed \\( S-I \\), and foreign prices are not directly affected by domestic changes).\n\nStep 3: If \\( P \\) (domestic price level) rises, the term \\( \\frac{P^*}{P} \\) decreases because the denominator increases while \\( P^* \\) is constant.\n\nStep 4: Since \\( e = \\epsilon \\times \\left( \\frac{P^*}{P} \\right) \\), a decrease in \\( \\frac{P^*}{P} \\) directly reduces \\( e \\), assuming \\( \\epsilon \\) is unchanged.\n\nStep 5: Conclusion: A rise in the domestic price level \\( P \\) causes the nominal exchange rate \\( e \\) to depreciate (i.e., \\( e \\) decreases) under the given assumptions.\n\nQID: textbook-111-13-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-13-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the equation and logically explains how a rise in the domestic price level (P) leads to a decrease in the nominal exchange rate (e), matching the gold answer's reasoning and conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-13-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the equation and logically explains how a rise in the domestic price level (P) leads to a decrease in the nominal exchange rate (e), matching the gold answer's reasoning and conclusion."}, "llm_echoed_qid": "textbook-111-13-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the equation and logically explains how a rise in the domestic price level (P) leads to a decrease in the nominal exchange rate (e), matching the gold answer's reasoning and conclusion."}
{"script_processing_qid": "textbook-94-2-0-4", "llm_grader_input_prompt_user": "\nQUESTION:\n5. Compare and contrast absolute and relative risk aversion. How do they vary with wealth?\n\nGOLD_ANSWER:\n1. **Absolute Risk Aversion ($r(w)$)**: Measures aversion to absolute gambles (e.g., fixed dollar amounts). Typically decreases with wealth (DARA).\n2. **Relative Risk Aversion ($\\rho(w)$)**: Measures aversion to relative gambles (e.g., percentages of wealth). Often assumed constant (CRRA).\n3. **Relationship**: $\\rho(w) = w \\cdot r(w)$. If $r(w)$ decreases with $w$, $\\rho(w)$ may increase, decrease, or stay constant depending on the utility function.\n\nCANDIDATE_ANSWER:\nStep 1: Define Absolute Risk Aversion (ARA)\nThe Arrow-Pratt measure of absolute risk aversion is given by:\n$$ r(w) = -\\frac{u''(w)}{u'(w)} $$\nIt measures the level of risk aversion to a fixed monetary risk amount, independent of wealth.\n\nStep 2: Define Relative Risk Aversion (RRA)\nRelative risk aversion is defined as:\n$$ \\rho(w) = -\\frac{u''(w)w}{u'(w)} = r(w) \\cdot w $$\nIt measures risk aversion to a risk proportional to wealth (i.e., percentage-based risk).\n\nStep 3: Contrast Key Differences\n- ARA focuses on absolute monetary risks (e.g., losing $100), while RRA focuses on relative/proportional risks (e.g., losing 10% of wealth).\n- ARA is scale-independent; RRA explicitly incorporates wealth level.\n\nStep 4: Analyze Wealth Variation\nExample 4.3.1 shows constant absolute risk aversion (CARA):\n$$ u(x) = -a e^{-rx} + b \\implies r(w) = r \\text{ (constant)} $$\nThis implies:\n- ARA (r(w)) remains unchanged with wealth.\n- RRA (ρ(w) = r ⋅ w) increases linearly with wealth.\n\nStep 5: General Insights\n- If ARA decreases with wealth (DARA), RRA may still increase, decrease, or remain constant depending on the utility function.\n- The provided CARA example specifically demonstrates that constant ARA implies increasing RRA with wealth, as RRA = r ⋅ w.\n\nQID: textbook-94-2-0-4\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-94-2-0-4\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately defines absolute and relative risk aversion, contrasts their key differences, and correctly analyzes how they vary with wealth, including the specific example of CARA. The explanation aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-94-2-0-4", "category": "CORRECT", "explanation": "The candidate answer accurately defines absolute and relative risk aversion, contrasts their key differences, and correctly analyzes how they vary with wealth, including the specific example of CARA. The explanation aligns with the gold answer."}, "llm_echoed_qid": "textbook-94-2-0-4", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately defines absolute and relative risk aversion, contrasts their key differences, and correctly analyzes how they vary with wealth, including the specific example of CARA. The explanation aligns with the gold answer."}
{"script_processing_qid": "textbook-35-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Using the production function $Y_t = A K_t^{\\alpha_k} H_t^{\\alpha_h}$, derive the condition under which the endogenous growth model predicts increasing income disparity over time (Figure 25).\n\nGOLD_ANSWER:\n1.  **Divergence Condition**:\n    - Assume $\\alpha_k + \\alpha_h = 1$ (constant returns to accumulable factors).\n    - Growth rate of output per capita: $g = sA - (n+\\delta)$, where $s$ is the savings rate.\n    - Countries with higher $s$ or $A$ experience persistently higher $g$, leading to divergence.\n    - The 90th/10th percentile ratio grows exponentially: $\\frac{y_{90}(t)}{y_{10}(t)} = \\frac{y_{90}(0)}{y_{10}(0)} e^{(g_{90}-g_{10})t}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that in endogenous growth models, persistent growth arises from constant/increasing returns to reproducible factors (K and H here).\nStep 2: Take logs of the production function: ln Y_t = ln A + α_k ln K_t + α_h ln H_t\nStep 3: Differentiate w.r.t time to get growth rates: g_Y = α_k g_K + α_h g_H\nStep 4: For divergence (increasing disparity), growth rates must differ systematically across countries. This occurs when:\n   - There are increasing returns to scale (α_k + α_h > 1)\n   - Capital accumulation processes (for K and H) are convex functions of existing stocks\nStep 5: The critical condition is α_k + α_h > 1. This implies countries with higher initial K/H stocks experience permanently faster growth due to:\n   - Reinforcing accumulation effects\n   - No diminishing returns to offset advantage\nStep 6: Under this condition, the model predicts the 90th/10th percentile ratio growth shown in Figure 25 because high-K/H economies pull away from low-K/H ones over time.\n\nQID: textbook-35-0-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-35-0-1-0\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer incorrectly states that the condition for divergence is α_k + α_h > 1, whereas the gold answer specifies α_k + α_h = 1. Additionally, the candidate's reasoning about increasing returns to scale and convex accumulation processes does not align with the gold answer's focus on differences in savings rates and productivity leading to divergence.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-35-0-1-0", "category": "INCORRECT", "explanation": "The candidate's answer incorrectly states that the condition for divergence is α_k + α_h > 1, whereas the gold answer specifies α_k + α_h = 1. Additionally, the candidate's reasoning about increasing returns to scale and convex accumulation processes does not align with the gold answer's focus on differences in savings rates and productivity leading to divergence."}, "llm_echoed_qid": "textbook-35-0-1-0", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer incorrectly states that the condition for divergence is α_k + α_h > 1, whereas the gold answer specifies α_k + α_h = 1. Additionally, the candidate's reasoning about increasing returns to scale and convex accumulation processes does not align with the gold answer's focus on differences in savings rates and productivity leading to divergence."}
{"script_processing_qid": "textbook-118-12-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the covariance matrix $\\operatorname{Var}(\\pmb{x})$ for a multivariate normal vector $\\pmb{x} = \\pmb{A}\\pmb{z}$, where $\\pmb{z} \\sim N(\\mathbf{0}, \\mathbf{I})$.\n\nGOLD_ANSWER:\n1. **Given**: $\\pmb{x} = \\pmb{A}\\pmb{z}$, where $\\pmb{z} \\sim N(\\mathbf{0}, \\mathbf{I})$.\n2. **Covariance Matrix**: $\\operatorname{Var}(\\pmb{x}) = \\operatorname{E}(\\pmb{x}\\pmb{x}^{\\top}) = \\operatorname{E}(\\pmb{A}\\pmb{z}\\pmb{z}^{\\top}\\pmb{A}^{\\top}) = \\pmb{A}\\operatorname{E}(\\pmb{z}\\pmb{z}^{\\top})\\pmb{A}^{\\top}$.\n3. **Since $\\pmb{z} \\sim N(\\mathbf{0}, \\mathbf{I})$**: $\\operatorname{E}(\\pmb{z}\\pmb{z}^{\\top}) = \\mathbf{I}$.\n4. **Result**: $\\operatorname{Var}(\\pmb{x}) = \\pmb{A}\\mathbf{I}\\pmb{A}^{\\top} = \\pmb{A}\\pmb{A}^{\\top}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the definition of covariance matrix for linear transformations. For any linear combination \\(\\pmb{x} = \\pmb{A}\\pmb{z}\\), the covariance matrix is given by:\n\n\\[\\operatorname{Var}(\\pmb{x}) = \\pmb{A} \\operatorname{Var}(\\pmb{z}) \\pmb{A}^{\\top}\\]\n\nStep 2: Substitute the covariance matrix of \\(\\pmb{z}\\). Since \\(\\pmb{z} \\sim N(\\mathbf{0}, \\mathbf{I})\\), its covariance matrix is the identity matrix \\(\\mathbf{I}\\):\n\n\\[\\operatorname{Var}(\\pmb{x}) = \\pmb{A} \\mathbf{I} \\pmb{A}^{\\top}\\]\n\nStep 3: Simplify the expression. Multiplying by the identity matrix leaves \\(\\pmb{A}\\pmb{A}^{\\top}\\):\n\n\\[\\operatorname{Var}(\\pmb{x}) = \\pmb{A}\\pmb{A}^{\\top}\\]\n\nFinal Result: The covariance matrix \\(\\operatorname{Var}(\\pmb{x})\\) is \\(\\pmb{A}\\pmb{A}^{\\top}\\).\n\nQID: textbook-118-12-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-12-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the covariance matrix step-by-step, matching the gold answer's reasoning and final result.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-12-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the covariance matrix step-by-step, matching the gold answer's reasoning and final result."}, "llm_echoed_qid": "textbook-118-12-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the covariance matrix step-by-step, matching the gold answer's reasoning and final result."}
{"script_processing_qid": "textbook-43-0-3-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. How do external effects break the Stolper-Samuelson-Rybczinski duality?\n\nGOLD_ANSWER:\nExternalities distort factor intensities, potentially reversing the sign of $\\frac{\\partial w_1}{\\partial p}$ and creating multiple negative roots in $J$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the Stolper-Samuelson-Rybczinski (SS-R) duality. In standard trade theory, this duality ensures that the Jacobian matrix ($J$) governing equilibrium dynamics has at least half of its roots positive under constant social returns. This guarantees determinacy (a unique equilibrium path).\n\nStep 2: Benhabib and Nishimura (1998) introduce external effects that cause private decreasing returns (despite social constant returns). These external effects disrupt the SS-R duality by altering the relationship between factor allocations and prices. Specifically, private firms no longer perfectly internalize social returns, breaking the symmetry between price and quantity adjustments implied by the duality.\n\nStep 3: The broken duality invalidates the guarantee that half of $J$'s roots are positive. With external effects, the Jacobian matrix may instead have fewer positive roots, leading to indeterminacy (multiple equilibrium paths). Thus, external effects break the SS-R duality by decoupling the mechanisms that enforce determinacy in standard models.\n\nQID: textbook-43-0-3-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-43-0-3-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains how external effects disrupt the Stolper-Samuelson-Rybczinski duality by altering factor allocations and prices, leading to potential indeterminacy, which aligns with the gold answer's focus on distortions and multiple negative roots in the Jacobian matrix.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-43-0-3-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains how external effects disrupt the Stolper-Samuelson-Rybczinski duality by altering factor allocations and prices, leading to potential indeterminacy, which aligns with the gold answer's focus on distortions and multiple negative roots in the Jacobian matrix."}, "llm_echoed_qid": "textbook-43-0-3-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains how external effects disrupt the Stolper-Samuelson-Rybczinski duality by altering factor allocations and prices, leading to potential indeterminacy, which aligns with the gold answer's focus on distortions and multiple negative roots in the Jacobian matrix."}
{"script_processing_qid": "textbook-16-0-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Verify the concavity of the cost function $c(w, y) = y(w_1 - \\sqrt{w_1 w_2} + w_2)$ in input prices.\n\nGOLD_ANSWER:\n1. **Hessian Matrix**: $H = \\begin{bmatrix} -\\frac{1}{4} y w_1^{-3/2} w_2^{1/2} & \\frac{1}{4} y w_1^{-1/2} w_2^{-1/2} \\\\ \\frac{1}{4} y w_1^{-1/2} w_2^{-1/2} & -\\frac{1}{4} y w_1^{1/2} w_2^{-3/2} \\end{bmatrix}$. \\n2. **Concavity**: $|H_1| < 0$, $|H_2| = 0$. The function is concave.\n\nCANDIDATE_ANSWER:\nStep 1: Compute the Hessian matrix of the cost function. Given $c(w, y) = y(w_1 - \\sqrt{w_1 w_2} + w_2)$, calculate second partial derivatives:\n- $\\frac{\\partial^2 c}{\\partial w_1^2} = \\frac{y\\sqrt{w_2}}{4w_1^{3/2}} > 0$\n- $\\frac{\\partial^2 c}{\\partial w_2^2} = \\frac{y\\sqrt{w_1}}{4w_2^{3/2}} > 0$\n- Cross-partials: $\\frac{\\partial^2 c}{\\partial w_1 \\partial w_2} = \\frac{-y}{4\\sqrt{w_1 w_2}}$\n\nStep 2: Check negative semi-definiteness conditions for concavity. A 2x2 Hessian $H$ requires:\n1. Diagonal elements $\\leq 0$\n2. Determinant $\\geq 0$\n\nStep 3: Observe both diagonal elements ($\\frac{\\partial^2 c}{\\partial w_1^2}$ and $\\frac{\\partial^2 c}{\\partial w_2^2}$) are strictly positive for $w_1, w_2, y > 0$. This violates the first condition for negative semi-definiteness.\n\nConclusion: The cost function is not concave in input prices because its Hessian matrix fails to satisfy necessary conditions for concavity.\n\nQID: textbook-16-0-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-16-0-1-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's conclusion that the cost function is not concave is incorrect. The Hessian matrix provided in the gold answer shows that the function is concave, as the determinant conditions are satisfied. The candidate's analysis of the diagonal elements is flawed, as they misinterpreted the signs and conditions for concavity.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-16-0-1-2", "category": "INCORRECT", "explanation": "The candidate's conclusion that the cost function is not concave is incorrect. The Hessian matrix provided in the gold answer shows that the function is concave, as the determinant conditions are satisfied. The candidate's analysis of the diagonal elements is flawed, as they misinterpreted the signs and conditions for concavity."}, "llm_echoed_qid": "textbook-16-0-1-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's conclusion that the cost function is not concave is incorrect. The Hessian matrix provided in the gold answer shows that the function is concave, as the determinant conditions are satisfied. The candidate's analysis of the diagonal elements is flawed, as they misinterpreted the signs and conditions for concavity."}
{"script_processing_qid": "textbook-107-9-3-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the social welfare outcomes of the pure and mixed strategy equilibria in the game of chicken.\n\nGOLD_ANSWER:\nPure strategy equilibria (one Swerves, one Drives Straight) are Pareto efficient, as one player gets 32 and the other gets 0. The mixed strategy equilibrium introduces a risk of collision (both Drive Straight), which is worse for both (-48). Thus, pure equilibria are socially preferable.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate social welfare in pure strategy equilibria.\nIn pure strategy Nash equilibria, one player swerves (payoff = 0) and the other drives straight (payoff = 32). Total social welfare = 0 + 32 = 32.\n\nStep 2: Calculate social welfare in mixed strategy equilibrium.\nBoth players drive straight with probability 0.4 and swerve with probability 0.6. Compute expected payoffs:\n- Probability of both swerving: 0.6 × 0.6 = 0.36 → Both get 0.\n- Probability one swerves, one drives: 2 × (0.6 × 0.4) = 0.48 → One gets 0, one gets 32. Total = 32.\n- Probability both drive straight: 0.4 × 0.4 = 0.16 → Both get -48. Total = -96.\nExpected social welfare = (0.36 × 0) + (0.48 × 32) + (0.16 × -48) = 0 + 15.36 - 7.68 = 7.68.\n\nStep 3: Compare outcomes.\nPure strategy equilibria yield total welfare = 32.\nMixed strategy equilibrium yields total welfare = 7.68.\nConclusion: Pure strategy equilibria achieve higher social welfare than the mixed strategy equilibrium.\n\nQID: textbook-107-9-3-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-107-9-3-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly calculates and compares the social welfare outcomes of pure and mixed strategy equilibria in the game of chicken, aligning with the gold answer's conclusion that pure strategy equilibria are socially preferable.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-9-3-3", "category": "CORRECT", "explanation": "The candidate's answer correctly calculates and compares the social welfare outcomes of pure and mixed strategy equilibria in the game of chicken, aligning with the gold answer's conclusion that pure strategy equilibria are socially preferable."}, "llm_echoed_qid": "textbook-107-9-3-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly calculates and compares the social welfare outcomes of pure and mixed strategy equilibria in the game of chicken, aligning with the gold answer's conclusion that pure strategy equilibria are socially preferable."}
{"script_processing_qid": "textbook-112-10-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Quantify the surplus tobacco trade described in the text. If Great Britain consumes 14,000 hogsheads annually but imports 96,000, what percentage is re-exported?\n\nGOLD_ANSWER:\n1.  **Total Imports**: 96,000 hogsheads.\n2.  **Domestic Consumption**: 14,000 hogsheads.\n3.  **Surplus**: 96,000 - 14,000 = 82,000 hogsheads.\n4.  **Percentage Re-exported**: (82,000 / 96,000) * 100 = 85.4%.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate surplus tobacco for re-export by subtracting domestic consumption from total imports\n96,000 hogsheads imported - 14,000 hogsheads consumed domestically = 82,000 hogsheads re-exported\n\nStep 2: Calculate percentage re-exported using formula:\n(Re-exported quantity / Total imports) × 100\n(82,000 / 96,000) × 100 = 85.42%\n\nFinal answer: 85.42% of imported tobacco is re-exported\n\nQID: textbook-112-10-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-10-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly calculates the surplus tobacco and the percentage re-exported, matching the gold answer's methodology and result.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-10-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly calculates the surplus tobacco and the percentage re-exported, matching the gold answer's methodology and result."}, "llm_echoed_qid": "textbook-112-10-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly calculates the surplus tobacco and the percentage re-exported, matching the gold answer's methodology and result."}
{"script_processing_qid": "textbook-119-18-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the drawbacks of ad hoc estimation of distributed-lag models and why the Koyck approach is preferred.\n\nGOLD_ANSWER:\n1. **No a priori guide for lag length**: Ad hoc estimation lacks a theoretical basis for determining the maximum lag length. 2. **Degrees of freedom**: Estimating successive lags reduces the degrees of freedom, weakening statistical inference. 3. **Multicollinearity**: High correlation between successive lags leads to imprecise estimates. 4. **Data mining**: Sequential search for lag length risks overfitting and biased results. The Koyck approach addresses these issues by imposing a geometric decay structure on the lag coefficients, reducing the number of parameters to estimate and avoiding multicollinearity.\n\nCANDIDATE_ANSWER:\nStep 1: Drawbacks of Ad Hoc Estimation\n- **Indeterminate Lag Length**: No theoretical guidance on how many lags to include, requiring arbitrary decisions.\n- **Degrees of Freedom Depletion**: Each additional lag reduces available observations, weakening statistical reliability.\n- **Multicollinearity**: High correlation between successive lagged values inflates standard errors and destabilizes coefficient estimates.\n- **Data Mining Risk**: Sequential lag-length testing increases the likelihood of overfitting or spurious results.\n\nStep 2: Advantages of the Koyck Approach\n- **Systematic Lag Structure**: Assumes geometrically declining coefficients (βₖ = β₀λᵏ), eliminating the need to arbitrarily define lag length.\n- **Parameter Efficiency**: Reduces the infinite lag model to estimating just α, β₀, and λ (decay rate), preserving degrees of freedom.\n- **Mitigates Multicollinearity**: Avoids including highly correlated sequential lags by transforming the model into an autoregressive form (e.g., Yₜ = α(1−λ) + β₀Xₜ + λYₜ₋₁ + uₜ).\n- **Interpretability**: Provides closed-form measures like median lag (−log 2 / log λ) and mean lag (λ / (1−λ)) to quantify lag dynamics.\n- **Theoretical Consistency**: Imposes a priori structure aligned with economic intuition (e.g., diminishing effects over time), reducing data mining risks.\n\nQID: textbook-119-18-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-18-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies the drawbacks of ad hoc estimation and the advantages of the Koyck approach, matching the gold answer in both content and reasoning.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-18-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately identifies the drawbacks of ad hoc estimation and the advantages of the Koyck approach, matching the gold answer in both content and reasoning."}, "llm_echoed_qid": "textbook-119-18-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies the drawbacks of ad hoc estimation and the advantages of the Koyck approach, matching the gold answer in both content and reasoning."}
{"script_processing_qid": "textbook-63-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the asymptotic variance of $\\sqrt{n}\\hat{\\boldsymbol{\\gamma}}$ in the proof of Proposition 2.10.\n\nGOLD_ANSWER:\n1. **Asymptotic equivalence**: \n   $$\\sqrt{n}\\hat{\\gamma}_j \\sim \\sqrt{n}\\tilde{\\gamma}_j - \\mu_j'\\sqrt{n}(\\mathbf{b} - \\boldsymbol{\\beta}).$$\n2. **Martingale difference property**: $\\{\\mathbf{g}_t\\}$ is a martingale difference sequence with $\\text{E}(\\mathbf{g}_t\\mathbf{g}_t') = \\begin{bmatrix} \\sigma^4\\delta_{jk} & \\sigma^2\\mu_j' \\\\ \\sigma^2\\mu_k & \\sigma^2\\Sigma_{\\mathbf{x}\\mathbf{x}} \\end{bmatrix}$.\n3. **Limit distribution**: \n   $$\\sqrt{n}\\hat{\\boldsymbol{\\gamma}} \\to_d N(\\mathbf{0}, \\sigma^4\\mathbf{I}_p - \\sigma^4\\Phi),$$\n   where $\\Phi$ is defined in Proposition 2.10.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the expression for $\\sqrt{n}(\\mathbf{b} - \\boldsymbol{\beta})$ in OLS. Under standard assumptions, \\\\n$\\sqrt{n}(\\mathbf{b} - \\boldsymbol{\beta}) = (\\frac{1}{n}\\mathbf{Y}_n)^{-1} \\frac{1}{\\sqrt{n}}\\sum_{t=1}^n \\mathbf{x}_t\\varepsilon_t$\\\\nwhere $\\mathbf{Y}_n = \\frac{1}{n}\\sum_{t=1}^n \\mathbf{x}_t\\mathbf{x}_t'$.\\\\n\\\\nStep 2: Let $\\mathbf{Q}_n = \\mathbf{Y}_n$ (the sample moment matrix) and $\\mathbf{Q} = \\text{plim } \\mathbf{Q}_n$. The term $(\\frac{1}{n}\\mathbf{Y}_n)^{-1}$ converges to $\\mathbf{Q}^{-1}$ by LLN.\\\\n\\\\nStep 3: Apply the Central Limit Theorem to $\\frac{1}{\\sqrt{n}}\\sum_{t=1}^n \\mathbf{x}_t\\varepsilon_t$. This term converges in distribution to $\\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{Q})$.\\\\n\\\\nStep 4: Combine results via Slutsky's theorem:\\\\n$\\sqrt{n}(\\mathbf{b} - \\boldsymbol{\beta}) \\to_d \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{Q}^{-1}\\mathbf{Q}\\mathbf{Q}^{-1}) = \\mathcal{N}(\\mathbf{0}, \\sigma^2\\mathbf{Q}^{-1})$\\\\n\\\\nStep 5: Identify $\\sqrt{n}\\hat{\\boldsymbol{\\gamma}}$ as $\\sqrt{n}(\\mathbf{b} - \\boldsymbol{\beta})$. The asymptotic variance is therefore $\\sigma^2\\mathbf{Q}^{-1}$.\n\nQID: textbook-63-0-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-63-0-0-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate answer incorrectly identifies $\\sqrt{n}\\hat{\\boldsymbol{\\gamma}}$ as $\\sqrt{n}(\\mathbf{b} - \\boldsymbol{\\beta})$, which is not consistent with the gold answer. The gold answer provides a different asymptotic variance expression involving $\\sigma^4\\mathbf{I}_p - \\sigma^4\\Phi$, which is not addressed in the candidate's response.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-63-0-0-2", "category": "INCORRECT", "explanation": "The candidate answer incorrectly identifies $\\sqrt{n}\\hat{\boldsymbol{\\gamma}}$ as $\\sqrt{n}(\\mathbf{b} - \boldsymbol{\beta})$, which is not consistent with the gold answer. The gold answer provides a different asymptotic variance expression involving $\\sigma^4\\mathbf{I}_p - \\sigma^4\\Phi$, which is not addressed in the candidate's response."}, "llm_echoed_qid": "textbook-63-0-0-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate answer incorrectly identifies $\\sqrt{n}\\hat{\boldsymbol{\\gamma}}$ as $\\sqrt{n}(\\mathbf{b} - \boldsymbol{\beta})$, which is not consistent with the gold answer. The gold answer provides a different asymptotic variance expression involving $\\sigma^4\\mathbf{I}_p - \\sigma^4\\Phi$, which is not addressed in the candidate's response."}
{"script_processing_qid": "textbook-115-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the maximum likelihood estimation (MLE) approach with the two-step probit method for estimating the model.\n\nGOLD_ANSWER:\n1. **MLE**: Computationally intensive but efficient and consistent. Directly maximizes the log-likelihood derived from the joint distribution.\n2. **Two-Step Probit**: Simpler but inconsistent. Involves estimating $\\Phi(\\mathbf{z}\\delta_{2})$ first and then using it in the probit for $y_{1}$, which fails to account for the nonlinearity correctly.\n\nCANDIDATE_ANSWER:\nStep 1: **MLE Approach**\n- Jointly estimates all parameters (δ₁, δ₂, α₁, ρ₁) by maximizing the likelihood derived from the joint distribution of (y₁, y₂) given 𝐙.\n- Accounts for endogeneity by explicitly modeling the correlation ρ₁ between u₁ and v₂ in the bivariate normal errors.\n- Produces consistent and efficient estimates under correct model specification but requires numerical optimization over all parameters.\n\nStep 2: **Two-Step Probit Method**\n- First estimates the reduced-form probit for y₂ (equation 2) to obtain δ₂.\n- In the second step, uses residuals from the first-stage probit (e.g., inverse Mills ratio) as a control variable in the y₁ probit to correct for endogeneity.\n- Simpler computationally but requires adjustment for generated regressors in the second step (e.g., bootstrapped standard errors).\n\nStep 3: **Key Differences**\n- Efficiency: MLE is asymptotically efficient; two-step is less efficient due to sequential estimation.\n- Implementation: MLE requires full bivariate normal distribution assumptions; two-step avoids this but introduces approximation.\n- Consistency: Both methods address endogeneity, but MLE consistency relies on strict joint normality, while two-step may be more robust to minor misspecifications.\n\nStep 4: **Practical Trade-offs**\n- MLE is theoretically preferable for precision but computationally intensive.\n- Two-step probit sacrifices efficiency for simplicity but requires careful error adjustments.\n\nQID: textbook-115-1-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-1-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes both the MLE and two-step probit methods, highlighting their computational intensity, efficiency, consistency, and practical trade-offs, which aligns well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-1-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately describes both the MLE and two-step probit methods, highlighting their computational intensity, efficiency, consistency, and practical trade-offs, which aligns well with the gold answer."}, "llm_echoed_qid": "textbook-115-1-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes both the MLE and two-step probit methods, highlighting their computational intensity, efficiency, consistency, and practical trade-offs, which aligns well with the gold answer."}
{"script_processing_qid": "textbook-115-5-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. (Quantitative) Derive the asymptotic properties of an estimator using the analogy principle, starting from the population problem $\\min_\\theta E[g(X, \\theta)]$ and its sample counterpart $\\min_\\theta \\frac{1}{n}\\sum_{i=1}^n g(X_i, \\theta)$.\n\nGOLD_ANSWER:\n1.  **Population Problem**: Define $\\theta_0$ as the solution to $E[g(X, \\theta)] = 0$.\n2.  **Sample Analogue**: The estimator $\\hat{\\theta}$ solves $\\frac{1}{n}\\sum_{i=1}^n g(X_i, \\hat{\\theta}) = 0$.\n3.  **Taylor Expansion**: Expand $\\frac{1}{n}\\sum_{i=1}^n g(X_i, \\hat{\\theta})$ around $\\theta_0$:\n    $$ 0 \\approx \\frac{1}{n}\\sum_{i=1}^n g(X_i, \\theta_0) + \\left(\\frac{1}{n}\\sum_{i=1}^n \\frac{\\partial g(X_i, \\theta_0)}{\\partial \\theta}\\right)(\\hat{\\theta} - \\theta_0). $$\n4.  **Asymptotic Distribution**: Rearrange and multiply by $\\sqrt{n}$:\n    $$ \\sqrt{n}(\\hat{\\theta} - \\theta_0) \\approx -\\left(\\frac{1}{n}\\sum_{i=1}^n \\frac{\\partial g(X_i, \\theta_0)}{\\partial \\theta}\\right)^{-1} \\cdot \\frac{1}{\\sqrt{n}}\\sum_{i=1}^n g(X_i, \\theta_0). $$\n5.  **CLT and LLN**: Under regularity conditions, $\\frac{1}{\\sqrt{n}}\\sum_{i=1}^n g(X_i, \\theta_0) \\overset{d}{\\to} N(0, V)$ and $\\frac{1}{n}\\sum_{i=1}^n \\frac{\\partial g(X_i, \\theta_0)}{\\partial \\theta} \\overset{p}{\\to} G$. Thus,\n    $$ \\sqrt{n}(\\hat{\\theta} - \\theta_0) \\overset{d}{\\to} N(0, G^{-1} V (G^{-1})^T). $$\n\nCANDIDATE_ANSWER:\nStep 1: Define the population problem as θ₀ = argmin₀ E[g(X, θ)] and the analogy principle estimator as θ̂ₙ = argmin₀ (1/n)∑ᵢg(Xᵢ, θ).\n\nStep 2: Assume regularity conditions (differentiability, identification, uniform convergence of sample objective function to population counterpart, and existence of necessary moments).\n\nStep 3: Consistency: Under uniform convergence (via LLN for ergodic processes) and identification, θ̂ₙ →ᴾ θ₀ as n → ∞.\n\nStep 4: Asymptotic normality: Take a mean-value expansion of the first-order condition (1/n)∑ᵢ∇θg(Xᵢ, θ̂ₙ) = 0 around θ₀:\n(1/n)∑ᵢ∇θg(Xᵢ, θ₀) + (1/n)∑ᵢ∇θθ'g(Xᵢ, θ̄)(θ̂ₙ − θ₀) ≈ 0\n\nStep 5: Rearrange to get √n(θ̂ₙ − θ₀) ≈ [−(1/n)∑ᵢ∇θθ'g(Xᵢ, θ₀)]⁻¹ × [√n(1/n)∑ᵢ∇θg(Xᵢ, θ₀)]\n\nStep 6: Apply CLT to show √n(1/n)∑ᵢ∇θg(Xᵢ, θ₀) →ᴰ N(0, B) where B = Var[∇θg(X, θ₀)].\n\nStep 7: Apply LLN to show (1/n)∑ᵢ∇θθ'g(Xᵢ, θ₀) →ᴾ A = E[∇θθ'g(X, θ₀)].\n\nStep 8: Combine results using Slutsky’s theorem: √n(θ̂ₙ − θ₀) →ᴰ N(0, A⁻¹BA⁻¹).\n\nQID: textbook-115-5-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-5-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly follows the steps outlined in the gold answer, including defining the population problem, applying the analogy principle, using Taylor expansion, and deriving the asymptotic distribution using CLT and LLN. The reasoning and final result align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-5-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly follows the steps outlined in the gold answer, including defining the population problem, applying the analogy principle, using Taylor expansion, and deriving the asymptotic distribution using CLT and LLN. The reasoning and final result align with the gold answer."}, "llm_echoed_qid": "textbook-115-5-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly follows the steps outlined in the gold answer, including defining the population problem, applying the analogy principle, using Taylor expansion, and deriving the asymptotic distribution using CLT and LLN. The reasoning and final result align with the gold answer."}
{"script_processing_qid": "textbook-112-15-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. **Quantitative**: Using the wage data provided for North America in 1773, calculate the real wage (in sterling) for a common labourer, ship-carpenter, and journeyman tailor, adjusting for the price of provisions. Compare these to the wages in England.\n\nGOLD_ANSWER:\n1. **Common Labourer**: 3s 6d currency = 2s sterling.\n2. **Ship-Carpenter**: 10s 6d currency + 6d (rum) = 6s 6d sterling.\n3. **Journeyman Tailor**: 5s currency ≈ 2s 10d sterling.\n\n**Comparison**: North American wages are higher than in England, and the real wage (accounting for lower provision prices) is even higher in North America.\n\nCANDIDATE_ANSWER:\nStep 1: Extract nominal wages from the text\n- Common labourer: 3s 6d colonial currency = 2s sterling\n- Ship-carpenter: 10s 6d currency + 6d rum = 6s 6d sterling\n- Journeyman tailor: 5s currency = 2s 10d sterling\n\nStep 2: Identify provision adjustments\nAll listed wages already incorporate the key provision adjustments mentioned in the text (specifically the rum valuation for ship-carpenters). No additional price index adjustments are required as the conversion to sterling already accounts for purchasing power differences.\n\nStep 3: Compare to English wages\nWhile exact English wage figures are not provided, the text explicitly states North American wages were 'much higher' than in England. For perspective:\n- A common labourer's 2s/day in NA exceeded typical 1s/day wages in England\n- Ship-carpenter's 6s 6d was 3-4x English shipwright wages (~2s/day)\n- Tailor's 2s 10d was double typical journeyman rates in England (~1s 6d)\n\nConclusion: Real wages in North America were significantly higher across all listed occupations compared to England, demonstrating Smith's observation that growing colonies offered better labor rewards than stationary European economies.\n\nQID: textbook-112-15-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-15-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly extracts and converts the nominal wages, identifies the provision adjustments, and accurately compares the North American wages to those in England, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-15-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly extracts and converts the nominal wages, identifies the provision adjustments, and accurately compares the North American wages to those in England, aligning with the gold answer."}, "llm_echoed_qid": "textbook-112-15-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly extracts and converts the nominal wages, identifies the provision adjustments, and accurately compares the North American wages to those in England, aligning with the gold answer."}
{"script_processing_qid": "textbook-102-4-3-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What is the covariance matrix of the OLS and GLS residual vectors, $E[\\mathbf{e}\\hat{\\boldsymbol{\\varepsilon}}^{\\prime}]$?\n\nGOLD_ANSWER:\nThe covariance is:\n$E[\\mathbf{e}\\hat{\\boldsymbol{\\varepsilon}}^{\\prime}] = E[\\mathbf{M}\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}^{\\prime}(\\mathbf{I} - \\mathbf{X}(\\mathbf{X}^{\\prime}\\boldsymbol{\\Omega}^{-1}\\mathbf{X})^{-1}\\mathbf{X}^{\\prime}\\boldsymbol{\\Omega}^{-1})^{\\prime}] = \\sigma^{2}\\mathbf{M}\\boldsymbol{\\Omega}(\\mathbf{I} - \\boldsymbol{\\Omega}^{-1}\\mathbf{X}(\\mathbf{X}^{\\prime}\\boldsymbol{\\Omega}^{-1}\\mathbf{X})^{-1}\\mathbf{X}^{\\prime}) = \\sigma^{2}[\\mathbf{M}\\boldsymbol{\\Omega} - \\mathbf{M}\\mathbf{X}(\\mathbf{X}^{\\prime}\\boldsymbol{\\Omega}^{-1}\\mathbf{X})^{-1}\\mathbf{X}^{\\prime}] = \\sigma^{2}\\mathbf{M}\\boldsymbol{\\Omega}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the residual definitions:\n- OLS residual: $\\mathbf{e} = \\mathbf{M}\\boldsymbol{\\varepsilon}$ where $\\mathbf{M} = \\mathbf{I} - \\mathbf{X}(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'$\n- GLS residual: $\\hat{\\boldsymbol{\\varepsilon}} = [\\mathbf{I} - \\mathbf{X}(\\mathbf{X}'\\boldsymbol{\\Omega}^{-1}\\mathbf{X})^{-1}\\mathbf{X}'\\boldsymbol{\\Omega}^{-1}]\\boldsymbol{\\varepsilon}$\n\nStep 2: Compute $E[\\mathbf{e}\\hat{\\boldsymbol{\\varepsilon}}']$:\n$E[\\mathbf{e}\\hat{\\boldsymbol{\\varepsilon}}'] = E[\\mathbf{M}\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'(\\mathbf{I} - \\boldsymbol{\\Omega}^{-1}\\mathbf{X}(\\mathbf{X}'\\boldsymbol{\\Omega}^{-1}\\mathbf{X})^{-1}\\mathbf{X}')]$\n\nStep 3: Substitute $E[\\boldsymbol{\\varepsilon}\\boldsymbol{\\varepsilon}'] = \\sigma^2\\boldsymbol{\\Omega}$:\n$= \\sigma^2\\mathbf{M}\\boldsymbol{\\Omega}(\\mathbf{I} - \\boldsymbol{\\Omega}^{-1}\\mathbf{X}(\\mathbf{X}'\\boldsymbol{\\Omega}^{-1}\\mathbf{X})^{-1}\\mathbf{X}')$\n\nStep 4: Simplify the expression:\n$= \\sigma^2[\\mathbf{M}\\boldsymbol{\\Omega} - \\mathbf{M}\\mathbf{X}(\\mathbf{X}'\\boldsymbol{\\Omega}^{-1}\\mathbf{X})^{-1}\\mathbf{X}']$\n\nStep 5: Recognize that $\\mathbf{M}\\mathbf{X} = 0$ (by definition of $\\mathbf{M}$):\n$= \\sigma^2\\mathbf{M}\\boldsymbol{\\Omega}$\n\nFinal Answer: The covariance matrix is $E[\\mathbf{e}\\hat{\\boldsymbol{\\varepsilon}}'] = \\sigma^2\\mathbf{M}\\boldsymbol{\\Omega}$.\n\nQID: textbook-102-4-3-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-102-4-3-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer matches the gold answer exactly, including the detailed steps and the final result. The reasoning is clear and correctly follows the mathematical derivations.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-102-4-3-2", "category": "CORRECT", "explanation": "The candidate's answer matches the gold answer exactly, including the detailed steps and the final result. The reasoning is clear and correctly follows the mathematical derivations."}, "llm_echoed_qid": "textbook-102-4-3-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer matches the gold answer exactly, including the detailed steps and the final result. The reasoning is clear and correctly follows the mathematical derivations."}
{"script_processing_qid": "textbook-8-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the role of labor mobility in determining the spatial distribution of industry and wages.\n\nGOLD_ANSWER:\n2. **Labor Mobility Role**:\n   - **With Mobility**: Workers migrate to regions with higher real wages, intensifying agglomeration and equalizing wages.\n   - **Without Mobility**: Wage differentials persist, and at low trade costs, firms spread to regions with cheaper labor, leading to convergence in industrial employment and wages.\n\nCANDIDATE_ANSWER:\nStep 1: Labor mobility interacts with trade costs to shape spatial patterns. At high trade costs, industries remain dispersed regardless of labor mobility to serve local demand.\n\nStep 2: At intermediate trade costs, labor mobility becomes critical. Workers migrating to regions with more firms/higher wages create a self-reinforcing cycle: (a) More firms increase job opportunities and real wages, (b) In-migration expands local consumer markets, (c) This attracts additional firms through demand linkages and economies of scale, further intensifying agglomeration.\n\nStep 3: With labor immobility, different dynamics emerge. At low trade costs: (a) Firms prioritize cost minimization over market access, (b) Wage differentials between regions persist without worker migration to equalize them, (c) Industry spreads to lower-cost locations rather than concentrating.\n\nStep 4: Wage determination follows these patterns. In mobile scenarios, agglomerated regions show higher nominal wages due to concentrated economic activity, while immobile scenarios maintain persistent wage gaps determined by local industrial concentrations.\n\nQID: textbook-8-0-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-8-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the role of labor mobility in determining the spatial distribution of industry and wages, matching the gold answer's key points about agglomeration, wage equalization, and the effects of trade costs.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-8-0-0-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the role of labor mobility in determining the spatial distribution of industry and wages, matching the gold answer's key points about agglomeration, wage equalization, and the effects of trade costs."}, "llm_echoed_qid": "textbook-8-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the role of labor mobility in determining the spatial distribution of industry and wages, matching the gold answer's key points about agglomeration, wage equalization, and the effects of trade costs."}
{"script_processing_qid": "textbook-118-18-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the asymptotic covariance matrix $\\pmb{\\cal S}$ of the vector $n^{-1/2}W^{\\top}({\\pmb y}-{\\pmb X}{\\pmb\\beta})$ and explain its role in HAC estimation.\n\nGOLD_ANSWER:\n1.  **Definition**: $\\pmb{\\cal S} \\equiv \\operatorname*{plim}_{n\\rightarrow\\infty}{\\frac{1}{n}}{\\pmb W}^{\\top}({\\pmb y}-{\\pmb X}\\beta_{0})({\\pmb y}-{\\pmb X}\\beta_{0})^{\\top}{\\pmb W} = \\operatorname*{plim}_{n\\rightarrow\\infty}{\\frac{1}{n}}{\\pmb W}^{\\top}\\pmb{\\varOmega}{\\pmb W}$.\n2.  **Role in HAC Estimation**: $\\pmb{\\cal S}$ captures the asymptotic variance of the sample moments, accounting for heteroskedasticity and autocorrelation. HAC estimators like Newey-West consistently estimate $\\pmb{\\cal S}$ under weak conditions.\n\nCANDIDATE_ANSWER:\nStep 1: Define the moment vector\nLet ${\\pmb m} = n^{-1/2}W^{\\top}({\\pmb y} - {\\pmb X}{\\pmb\\beta})$. The asymptotic covariance matrix ${\\pmb{\\cal S}}$ captures the variance of these moments under heteroskedasticity/autocorrelation.\n\nStep 2: Expand the outer product\nThe asymptotic covariance is derived from:\n$${\\pmb{\\cal S}} = \\operatorname*{plim}_{n\\rightarrow\\infty} \\frac{1}{n}W^{\\top}({\\pmb y} - {\\pmb X}\\beta_0)({\\pmb y} - {\\pmb X}\\beta_0)^{\\top}W$$\n\nStep 3: Relate to error covariance\nLet ${\\pmb\\varOmega} = \\mathbb{E}[{\\pmb\\varepsilon}{\\pmb\\varepsilon}^{\\top}]$ where ${\\pmb\\varepsilon} = {\\pmb y} - {\\pmb X}\\beta_0$. Under regularity conditions,\n$${\\pmb{\\cal S}} = \\operatorname*{plim}_{n\\rightarrow\\infty} \\frac{1}{n}W^{\\top}{\\pmb\\varOmega}W$$\n\nStep 4: Role in HAC estimation\n${\\pmb{\\cal S}}$ serves as the weighting matrix in efficient GMM estimation. HAC estimators (e.g., Newey-West, Andrews 1991) provide consistent estimates of ${\\pmb{\\cal S}}$ when ${\\pmb\\varOmega}$ has unknown heteroskedasticity/autocorrelation. This ensures valid inference in FGMM through the estimator $\\hat{\\Sigma}$ in:\n$$\\hat{\\beta}_{\\mathrm{FGMM}} = (X^{\\top}W\\hat{\\Sigma}^{-1}W^{\\top}X)^{-1}X^{\\top}W\\hat{\\Sigma}^{-1}W^{\\top}y$$\n\nQID: textbook-118-18-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-18-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the asymptotic covariance matrix $\\pmb{\\\\cal S}$ and accurately explains its role in HAC estimation, including the use of HAC estimators like Newey-West. The reasoning and details match the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-18-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the asymptotic covariance matrix $\\pmb{\\cal S}$ and accurately explains its role in HAC estimation, including the use of HAC estimators like Newey-West. The reasoning and details match the gold answer."}, "llm_echoed_qid": "textbook-118-18-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the asymptotic covariance matrix $\\pmb{\\cal S}$ and accurately explains its role in HAC estimation, including the use of HAC estimators like Newey-West. The reasoning and details match the gold answer."}
{"script_processing_qid": "textbook-115-4-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What are the steps to estimate $\\mathbf{D}_{0}$ when assumption (12.37) is violated?\n\nGOLD_ANSWER:\n1. **Estimate $\\mathbf{F}_{0}$**: Use $$\\hat{\\mathbf{F}}=N^{-1}\\sum_{i=1}^{N}\\nabla_{\\gamma}\\mathbf{s}_{i}(\\hat{\\pmb{\\theta}};\\hat{\\pmb{\\gamma}}).$$\n2. **Replace $\\mathbf{r}_{i}(\\gamma^{*})$**: Use $\\hat{\\mathbf{r}}_{i}\\equiv\\mathbf{r}_{i}(\\hat{\\boldsymbol{y}})$.\n3. **Form $\\hat{\\mathbf{g}}_{i}$**: Compute $\\hat{\\mathbf{g}}_{i}=\\hat{\\mathbf{s}}_{i}+\\hat{\\mathbf{F}}\\hat{\\mathbf{r}}_{i}$.\n4. **Estimate $\\mathbf{D}_{0}$**: Use $$\\hat{\\mathbf{D}}\\equiv N^{-1}\\sum_{i=1}^{N}\\hat{\\mathbf{g}}_{i}\\hat{\\mathbf{g}}_{i}^{\\prime}.$$\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that violation of assumption (12.37) requires accounting for the asymptotic variance of the first-stage estimator $\\hat{\\gamma}$ in the variance estimation of $\\hat{\theta}$.\n\nStep 2: Use the general asymptotic variance formula from equation (12.41), which includes adjustments for the two-step estimation process. This formula likely involves terms capturing:\n- The Hessian matrix $\\mathbf{H}_i$ (second derivatives) or score vectors $\\mathbf{s}_i$ from the second-step M-estimation\n- The influence of $\\hat{\\gamma}$ on $\\hat{\theta}$ through cross-derivatives\n- The asymptotic variance of $\\hat{\\gamma}$ itself\n\nStep 3: Compute the required components from the context:\n- $\\hat{\\mathbf{s}}_i$ (score vectors)\n- $\\hat{\\mathbf{H}}_i$ (Hessian matrices)\n- $\\hat{\\mathbf{A}}_i$ (weighting matrices)\nThese must now include derivatives with respect to both $\theta$ and $\\gamma$, unlike the simpler case where only $\theta$-derivatives were needed.\n\nStep 4: Combine these components in equation (12.41) to estimate $\\mathbf{D}_0$, incorporating the additional variability from estimating $\\gamma$. This typically involves sandwich variance estimators that account for both estimation steps and their covariance.\n\nQID: textbook-115-4-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-4-0-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate answer does not align with the gold answer. While the candidate discusses general concepts related to variance estimation in two-step processes, it does not provide the specific steps or formulas required to estimate 𝐃₀ as outlined in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-4-0-2", "category": "INCORRECT", "explanation": "The candidate answer does not align with the gold answer. While the candidate discusses general concepts related to variance estimation in two-step processes, it does not provide the specific steps or formulas required to estimate 𝐃₀ as outlined in the gold answer."}, "llm_echoed_qid": "textbook-115-4-0-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate answer does not align with the gold answer. While the candidate discusses general concepts related to variance estimation in two-step processes, it does not provide the specific steps or formulas required to estimate 𝐃₀ as outlined in the gold answer."}
{"script_processing_qid": "textbook-107-9-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What is the role of dominant strategies in determining the Nash equilibrium in this game?\n\nGOLD_ANSWER:\nBig Pig has a dominant strategy to Press because it yields a higher payoff regardless of Little Pig's choice (7 > 3 if Little Pig Waits, 7 > 0 if Little Pig Presses). Little Pig's best response to Press is Wait. Thus, the Nash equilibrium is determined by Big Pig's dominant strategy.\n\nCANDIDATE_ANSWER:\nStep 1: Identify dominant strategies:\n- Little Pig: Choosing 'Wait' yields equal or strictly better payoffs than 'Press' in all scenarios:\n  - If Big Pig presses: Little gets 1 (same as pressing)\n  - If Big Pig waits: Little gets 3 (vs 0 if pressing)\n  Thus, 'Wait' is a weakly dominant strategy for Little Pig.\n\nStep 2: Analyze Big Pig's response:\n- Given Little Pig will always play their dominant strategy (Wait), Big Pig's options are:\n  - Press: Gets 7\n  - Wait: Gets 3\n  Big Pig's optimal response is to press.\n\nStep 3: Verify Nash equilibrium:\n- Little Pig has no incentive to deviate from 'Wait' (dominant strategy)\n- Big Pig has no incentive to deviate from 'Press' (7 > 3)\n\nStep 4: Conclusion:\nThe dominant strategy for Little Pig forces the equilibrium by fixing Little's action at 'Wait', allowing Big Pig to optimally choose 'Press'. This strategic interaction directly determines the Nash equilibrium (Press, Wait) with payoffs (7, 1).\n\nQID: textbook-107-9-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-9-2-2\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate correctly identifies the Nash equilibrium (Press, Wait) but incorrectly labels 'Wait' as a weakly dominant strategy for Little Pig. The gold answer states that Big Pig has a dominant strategy (Press), which directly determines the equilibrium, while Little Pig's best response is Wait, not a dominant strategy.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-9-2-2", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate correctly identifies the Nash equilibrium (Press, Wait) but incorrectly labels 'Wait' as a weakly dominant strategy for Little Pig. The gold answer states that Big Pig has a dominant strategy (Press), which directly determines the equilibrium, while Little Pig's best response is Wait, not a dominant strategy."}, "llm_echoed_qid": "textbook-107-9-2-2", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate correctly identifies the Nash equilibrium (Press, Wait) but incorrectly labels 'Wait' as a weakly dominant strategy for Little Pig. The gold answer states that Big Pig has a dominant strategy (Press), which directly determines the equilibrium, while Little Pig's best response is Wait, not a dominant strategy."}
{"script_processing_qid": "textbook-114-18-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Calculate the exact welfare loss (deadweight loss) from the $\\$1.00$ price ceiling in 1975, assuming linear supply and demand curves. Illustrate graphically with key areas labeled.\n\nGOLD_ANSWER:\n1. **Graph Components**: \n   - Equilibrium: ($P=\\$2.00$, $Q=20$ Tcf).\n   - Price ceiling: $P_{max} = \\$1.00$ → $Q_{S}=18$ Tcf, $Q_{D}=25$ Tcf.\n2. **DWL Triangle**: Between $Q=18$ and $Q=20$ Tcf, bounded by supply and demand curves.\n3. **Calculation**: \n   - Supply slope ($\\frac{1}{2}$) and demand slope ($-\\frac{1}{5}$) from equations.\n   - Area = $\\frac{1}{2} \\times (2-1) \\times (20-18) = \\$1$ billion (simplified).\n\nCANDIDATE_ANSWER:\nStep 1: Calculate equilibrium quantity at P_G = \\$2.00 with P_O = \\$8.00\nSupply: Q = 16 + 2(2) = 20 Tcf\nDemand: Q = -5(2) + 30 = 20 Tcf\nEquilibrium quantity = 20 Tcf\n\nStep 2: Identify price-ceiling quantities\nAt \\$1.00 ceiling:\nQuantity supplied = 18 Tcf\nQuantity demanded = 25 Tcf\nActual transactions occur at Q = 18 Tcf\n\nStep 3: Determine price wedge at Q=18 Tcf\nDemand price at Q=18: P_d = (30 - 18)/5 = \\$2.40/mcf\nSupply price at Q=18: P_s = (18 - 16)/2 = \\$1.00/mcf\nPrice difference = 2.40 - 1.00 = \\$1.40/mcf\n\nStep 4: Calculate deadweight loss\nDWL = 0.5 × (Q_equilibrium - Q_ceiling) × Price difference\n     = 0.5 × (20 - 18) × 1.40\n     = 0.5 × 2 × 1.40 = \\$1.40 billion\n\nGraphical illustration shows:\n- Vertical axis: Price (\\$/mcf), Horizontal axis: Quantity (Tcf)\n- DWL triangle between Q=18-20, bounded by demand curve (\\$2.40 at Q=18) and supply curve (\\$1.00 at Q=18)\n- Area labeled as \"Deadweight Loss\" (\\$1.4 billion)\n\nQID: textbook-114-18-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-18-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly calculates the deadweight loss as \\$1.40 billion, matches the graphical illustration, and aligns with the gold answer's methodology despite a slight difference in the final value due to more detailed intermediate steps.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-18-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly calculates the deadweight loss as \\$1.40 billion, matches the graphical illustration, and aligns with the gold answer's methodology despite a slight difference in the final value due to more detailed intermediate steps."}, "llm_echoed_qid": "textbook-114-18-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly calculates the deadweight loss as \\$1.40 billion, matches the graphical illustration, and aligns with the gold answer's methodology despite a slight difference in the final value due to more detailed intermediate steps."}
{"script_processing_qid": "textbook-103-8-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Formulate the Bellman equation for the social planner's problem, explaining how it differs from the Nash equilibrium formulation.\n\nGOLD_ANSWER:\nThe social planner's Bellman equation maximizes a weighted sum of utilities:\n   $$ Q(\\lambda) = \\operatorname*{max}_{c_1, c_2} \\left\\{ \\int \\operatorname*{max}_{\\text{accept,reject}} \\left[ \\lambda \\left( \\frac{\\theta}{1-\\beta} - c_1 \\right) + (1-\\lambda) \\left( \\frac{\\theta}{1-\\beta} - c_2 \\right), -\\lambda c_1 - (1-\\lambda)c_2 + \\beta Q(\\lambda) \\right] d(F^n)(\\theta) \\right\\}, $$\n   subject to $n = f(c_1 + c_2)$. Unlike the Nash equilibrium, the planner internalizes the externality of $c_1$ and $c_2$ on total search efficiency.\n\nCANDIDATE_ANSWER:\nStep 1: Understand key components for Bellman equations\n- Match success probability depends on total search effort c₁ + c₂ through n = f(c₁ + c₂)\n- Social planner considers joint utilities: λU₁ + (1-λ)U₂\n- Must account for combined costs (c₁ + c₂) and matching benefits\n\nStep 2: Formulate social planner's Bellman equation\nV = max_{c₁,c₂} [λ(βE[θ|accept] - c₁) + (1-λ)(βE[θ|accept] - c₂) \n          + β(1-P(match))V]\nWhere:\n- P(match) depends on f(c₁ + c₂)\n- β is discount factor\n- Costs are internalized jointly (c₁ + c₂)\n\nStep 3: Compare with Nash equilibrium formulation\nNash Bellman equations (for party i):\nQ_i = max_{c_i} [βE[θ|accept] - c_i \n          + β(1-P(match))Q_i]\n\nKey differences:\n1. Social planner jointly optimizes (c₁,c₂) while Nash players optimize separately\n2. Planner's equation includes cross-effects: ∂f/∂c₁ affects both parties' utilities\n3. Nash formulation ignores marginal benefits to other party:\n   - Private MPB < Social MPB for search efforts\n\nStep 4: Explain inefficiency in Nash equilibrium\nIn Nash equilibrium:\n- Each party equates marginal private cost to marginal private benefit\n- Ignores positive externality: Increased c_i raises match probability for both\n- Results in c₁^{Nash} + c₂^{Nash} < c₁^{SP} + c₂^{SP}\n- Underinvestment in search efforts compared to social optimum\n\nQID: textbook-103-8-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-103-8-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly formulates the Bellman equation for the social planner's problem, including the weighted sum of utilities and the internalization of externalities. It also accurately contrasts this with the Nash equilibrium formulation, highlighting key differences such as joint optimization and the consideration of cross-effects.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-8-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly formulates the Bellman equation for the social planner's problem, including the weighted sum of utilities and the internalization of externalities. It also accurately contrasts this with the Nash equilibrium formulation, highlighting key differences such as joint optimization and the consideration of cross-effects."}, "llm_echoed_qid": "textbook-103-8-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly formulates the Bellman equation for the social planner's problem, including the weighted sum of utilities and the internalization of externalities. It also accurately contrasts this with the Nash equilibrium formulation, highlighting key differences such as joint optimization and the consideration of cross-effects."}
{"script_processing_qid": "textbook-111-17-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Using the concept of endogenous and exogenous variables, explain how the unemployment rate is determined in an economic model.\n\nGOLD_ANSWER:\n4. In an economic model:\n   - **Endogenous variables**: Unemployment rate (determined within the model).\n   - **Exogenous variables**: Factors like labor market policies, technological changes, and aggregate demand (taken as given).\n   The model explains how exogenous variables (e.g., a recession) affect the endogenous variable (unemployment rate) through mechanisms like reduced demand for labor.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the distinction between endogenous and exogenous variables. \nEndogenous variables are determined within the model (e.g., influenced by other variables in the system), while exogenous variables are determined outside the model (e.g., external inputs or policy parameters).\n\nStep 2: Apply this framework to the context provided. \nThe section highlights three key macroeconomic variables: real GDP, inflation rate, and unemployment rate. These variables are interdependent in most macroeconomic models.\n\nStep 3: Explain unemployment rate determination. \nIn a typical macroeconomic model, the unemployment rate is an endogenous variable. It is influenced by other endogenous variables like real GDP (via Okun’s Law, which links output growth to unemployment changes) and inflation (via the Phillips Curve, which relates unemployment to wage/price dynamics). Exogenous variables such as fiscal/monetary policy, technological shocks, or labor market regulations could indirectly affect unemployment by altering real GDP or inflation.\n\nStep 4: Relate to the provided context. \nWhile the context does not explicitly describe a full model, it establishes that unemployment fluctuates alongside real GDP and inflation (Figures 1-1 to 1-3). Thus, unemployment is determined endogenously by these interrelated variables, while exogenous factors like policy changes or external shocks (not explicitly mentioned in the text) would shift the model’s equilibrium.\n\nQID: textbook-111-17-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-17-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately distinguishes between endogenous and exogenous variables and correctly identifies the unemployment rate as endogenous, influenced by other variables like real GDP and inflation. It also mentions exogenous factors like policy changes, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-17-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately distinguishes between endogenous and exogenous variables and correctly identifies the unemployment rate as endogenous, influenced by other variables like real GDP and inflation. It also mentions exogenous factors like policy changes, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-111-17-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately distinguishes between endogenous and exogenous variables and correctly identifies the unemployment rate as endogenous, influenced by other variables like real GDP and inflation. It also mentions exogenous factors like policy changes, aligning well with the gold answer."}
{"script_processing_qid": "textbook-118-9-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Define the same-order notation $O(n^{p})$ and $O_{p}(n^{p})$. How are they used in asymptotic analysis?\n\nGOLD_ANSWER:\n1. **Deterministic Case**: $f(n) = O(n^{p})$ if $|f(n)/n^{p}| < K$ for all $n > N$.\\n2. **Stochastic Case**: $a_{n} = O_{p}(n^{p})$ if $\\Pr(|a_{n}/n^{p}| > K) < \\varepsilon$ for all $n > N$.\\n3. **Usage**: These notations simplify asymptotic analysis by focusing on the rate of growth or decay of functions or random variables as $n \\to \\infty$.\n\nCANDIDATE_ANSWER:\nStep 1: Definition of $O(n^p)$ (deterministic case)\nA deterministic function $f(n)$ satisfies $f(n) = O(n^p)$ if there exist constants $K > 0$ and $N$ such that $\\left|\\frac{f(n)}{n^p}\\right| < K$ for all $n > N$. This means $f(n)$ grows asymptotically at the same rate as $n^p$, bounded by a constant multiple.\n\nStep 2: Definition of $O_p(n^p)$ (stochastic case)\nA stochastic sequence $a_n$ satisfies $a_n = O_p(n^p)$ if for every $\\varepsilon > 0$, there exist constants $K > 0$ and $N$ such that $\\Pr\\left(\\left|\\frac{a_n}{n^p}\\right| > K\\right) < \\varepsilon$ for all $n > N$. This means $a_n$ is stochastically bounded at the rate $n^p$ asymptotically.\n\nStep 3: Usage in asymptotic analysis\n- $O(n^p)$ characterizes deterministic growth rates (e.g., to analyze regression coefficient convergence rates).\n- $O_p(n^p)$ characterizes probabilistic boundedness for stochastic sequences (e.g., to study the behavior of test statistics like the divergent $t$-statistic in spurious regressions).\nBoth notations formalize how variables behave as $n \\to \\infty$, enabling rigorous comparisons of asymptotic properties in nonstationary settings.\n\nQID: textbook-118-9-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-9-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately defines both $O(n^p)$ and $O_p(n^p)$ and correctly explains their usage in asymptotic analysis. The response aligns with the gold answer in both content and reasoning.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-9-1-0", "category": "CORRECT", "explanation": "The candidate's answer accurately defines both $O(n^p)$ and $O_p(n^p)$ and correctly explains their usage in asymptotic analysis. The response aligns with the gold answer in both content and reasoning."}, "llm_echoed_qid": "textbook-118-9-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately defines both $O(n^p)$ and $O_p(n^p)$ and correctly explains their usage in asymptotic analysis. The response aligns with the gold answer in both content and reasoning."}
{"script_processing_qid": "textbook-115-23-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the NTW statistic and explain its asymptotic distribution under the null hypothesis.\n\nGOLD_ANSWER:\n1. **Derivation**: The NTW statistic is derived by testing how far the sample average of $\\mathbf{g}(\\mathbf{w}_{i},\\hat{\\pmb{\\theta}})$ is from zero. The statistic is given by $$ N T W=\\left[\\sum_{i=1}^{N}\\mathbf{g}_{i}(\\hat{\\pmb{\\theta}})\\right]^{\\prime}\\left[\\sum_{i=1}^{N}(\\hat{\\mathbf{g}}_{i}-\\hat{\\mathbf{s}}_{i}\\hat{\\Pi})(\\hat{\\mathbf{g}}_{i}-\\hat{\\mathbf{s}}_{i}\\hat{\\Pi})^{\\prime}\\right]^{-1}\\left[\\sum_{i=1}^{N}\\mathbf{g}_{i}(\\hat{\\pmb{\\theta}})\\right] $$ \n2. **Asymptotic Distribution**: Under the null hypothesis, NTW is asymptotically distributed as $\\chi_{Q}^{2}$, where $Q$ is the number of nonredundant moment conditions.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the moment condition framework\nUnder H₀, we have Q moment conditions E[g(w_i,θ₀)] = 0. Let G_N(θ̂) = ∑g_i(θ̂) be the sample moments evaluated at MLE θ̂.\n\nStep 2: Account for parameter estimation\nThe key adjustment comes from θ̂ being estimated. Through a mean-value expansion:\nG_N(θ̂) ≈ G_N(θ₀) + ∇θG_N(θ̄)(θ̂ - θ₀)\n\nStep 3: Use MLE properties\nUnder MLE, √N(θ̂ - θ₀) ≈ [E(s_is_i')]⁻¹(1/√N)∑s_i\nwhere s_i = ∇θlogf(y_i|x_i;θ₀) are scores.\n\nStep 4: Develop covariance matrix\nThe asymptotic variance of G_N(θ̂) requires:\nVar[g_i - E(∇θg_i)s_i(E(s_is_i'))⁻¹] = E[(g_i - s_iΠ)(g_i - s_iΠ)']\nwhere Π = E(s_is_i')⁻¹E(s_ig_i')\n\nStep 5: Construct quadratic form\nThe NTW statistic becomes:\nNTW = G_N(θ̂)[Var(G_N(θ̂))]⁻¹G_N(θ̂)\n= [∑g_i(θ̂)]'[∑(ĝ_i - ŝ_iΠ̂)(ĝ_i - ŝ_iΠ̂)']⁻¹[∑g_i(θ̂)]\n\nStep 6: Asymptotic distribution\nUnder H₀ and regularity conditions:\nNTW →ᴰ χ²_Q\nThis follows because (1/√N)G_N(θ̂) →ᴰ Normal(0,V)\nand NTW is a Wald statistic where V⁻¹ is estimated.\n\nQID: textbook-115-23-2-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-23-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the NTW statistic and explains its asymptotic distribution under the null hypothesis. The steps align with the gold answer, including the use of moment conditions, parameter estimation adjustments, and the final quadratic form. The asymptotic distribution is also correctly identified as χ²_Q.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-23-2-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the NTW statistic and explains its asymptotic distribution under the null hypothesis. The steps align with the gold answer, including the use of moment conditions, parameter estimation adjustments, and the final quadratic form. The asymptotic distribution is also correctly identified as χ²_Q."}, "llm_echoed_qid": "textbook-115-23-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the NTW statistic and explains its asymptotic distribution under the null hypothesis. The steps align with the gold answer, including the use of moment conditions, parameter estimation adjustments, and the final quadratic form. The asymptotic distribution is also correctly identified as χ²_Q."}
{"script_processing_qid": "textbook-115-17-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Interpret a rejection of $H_0: \\sigma_c^2 = 0$ using the test statistic in equation (10.40). Does it imply the RE structure is correct?\n\nGOLD_ANSWER:\nNo. Rejection indicates serial correlation in $v_{it}$ but does not confirm the RE structure. Other forms of serial correlation could also lead to rejection.\n\nCANDIDATE_ANSWER:\nStep 1: Rejection of H₀: σ_c² = 0 implies we have statistically significant evidence against the absence of unobserved individual effects.\nStep 2: The test statistic in equation (10.40) evaluates serial correlation patterns in residuals under H₀. A rejection indicates dependence in errors across time for individuals.\nStep 3: This dependence reflects the presence of unobserved time-constant heterogeneity (σ_c² > 0), which is the core requirement for using random effects (RE) models.\nStep 4: However, rejection does NOT confirm the full RE structure's correctness. The RE model also requires strict exogeneity of regressors and that unobserved effects are uncorrelated with observed variables. The test only addresses σ_c² = 0, not these additional assumptions.\nConclusion: While rejecting H₀ supports the need to account for unobserved effects, it does not validate the stronger RE model assumptions about independence between unobserved effects and regressors.\n\nQID: textbook-115-17-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-17-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly interprets the rejection of H₀, explaining that it indicates the presence of unobserved individual effects but does not confirm the full RE structure. This aligns with the gold answer, which also states that rejection indicates serial correlation but does not confirm the RE structure.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-17-1-2", "category": "CORRECT", "explanation": "The candidate answer correctly interprets the rejection of H₀, explaining that it indicates the presence of unobserved individual effects but does not confirm the full RE structure. This aligns with the gold answer, which also states that rejection indicates serial correlation but does not confirm the RE structure."}, "llm_echoed_qid": "textbook-115-17-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly interprets the rejection of H₀, explaining that it indicates the presence of unobserved individual effects but does not confirm the full RE structure. This aligns with the gold answer, which also states that rejection indicates serial correlation but does not confirm the RE structure."}
{"script_processing_qid": "textbook-123-10-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What are the first-order conditions for the Ramsey problem, and how do they lead to the Friedman rule?\n\nGOLD_ANSWER:\nThe first-order conditions for consumption, leisure, and real money balances are derived from the Lagrangian. The condition for real money balances, $H_{\\hat{m}}(t) = 0$, implies satiation, which is achieved by setting the nominal interest rate to zero, consistent with the Friedman rule.\n\nCANDIDATE_ANSWER:\nStep 1: The Ramsey problem maximizes social welfare (24.2.2) subject to the government budget constraint (24.5.12) and a feasibility constraint combining production, consumption, and transaction costs.\n\nStep 2: The transaction technology (24.2.4) exhibits homogeneity, implying proportional scaling of transaction costs with money/consumption. This allows simplification of first-order conditions.\n\nStep 3: First-order conditions equate marginal utilities of consumption to marginal disutilities of labor (adjusted for taxes) and relate money's marginal transaction cost reduction to its opportunity cost (nominal interest rate).\n\nStep 4: The homogeneity condition eliminates cross-derivatives in the transaction technology, forcing the multiplier on the government budget constraint to match the marginal benefit of money in reducing transaction costs.\n\nStep 5: This alignment implies the optimal nominal interest rate (opportunity cost of money) must equal zero to avoid distorting money holdings - the Friedman rule.\n\nQID: textbook-123-10-2-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-10-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly outlines the steps leading to the Friedman rule, including the maximization of social welfare, the role of transaction technology, and the alignment of the nominal interest rate with the marginal benefit of money, culminating in the optimal nominal interest rate being zero.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-10-2-2", "category": "CORRECT", "explanation": "The candidate answer correctly outlines the steps leading to the Friedman rule, including the maximization of social welfare, the role of transaction technology, and the alignment of the nominal interest rate with the marginal benefit of money, culminating in the optimal nominal interest rate being zero."}, "llm_echoed_qid": "textbook-123-10-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly outlines the steps leading to the Friedman rule, including the maximization of social welfare, the role of transaction technology, and the alignment of the nominal interest rate with the marginal benefit of money, culminating in the optimal nominal interest rate being zero."}
{"script_processing_qid": "textbook-31-0-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the conditions under which a price equilibrium with transfers is Pareto optimal.\n\nGOLD_ANSWER:\n1. **Feasibility**: $\\sum_i x_i \\leq \\omega + \\sum_j y_j$.\n2. **Budget Constraints**: $p \\cdot x_i \\leq w_i$ for all $i$.\n3. **Pareto Optimality**: No feasible allocation $(x',y')$ exists that Pareto dominates $(x^*,y^*)$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that a price equilibrium with transfers requires (x*, y*) to be feasible (∑x_i* = ∑w_i + ∑y_j*) and consumers maximize utility under budget constraints p·x_i* ≤ w_i + θ_i·p·y_j*.\\n\\nStep 2: Assume allocation (x', y') Pareto dominates (x*, y*). For Pareto optimality, we must show (x', y') cannot be feasible.\\n\\nStep 3: Use local nonsatiation (condition (a) in context):\\n- If x_i* is a global satiation point, uniqueness implies x_i' = x_i* ⇒ p·x_i' = w_i.\\n- If x_i* is not a satiation point, local nonsatiation implies p·x_i' ≥ p·x_i* = w_i.\\n\\nStep 4: For at least one agent i with x_i' ≻_i x_i*, strong monotonicity implies strict inequality: p·x_i' > w_i.\\n\\nStep 5: Summing across all agents: ∑p·x_i' > ∑w_i. This violates feasibility since aggregate expenditure exceeds total wealth (∑w_i = ∑p·x_i* by equilibrium).\\n\\nConclusion: The equilibrium allocation is Pareto optimal if:\\n1. Preferences are locally nonsatiated at equilibrium allocations (to enforce p·x_i' ≥ w_i)\\n2. At least one agent experiences strict preference improvement under Pareto domination (to create ∑p·x_i' > ∑w_i)\\n3. Strong monotonicity ensures non-trivial preference comparisons align with price valuations.\n\nQID: textbook-31-0-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-31-0-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly derives the conditions for Pareto optimality in a price equilibrium with transfers, aligning with the gold answer's key points (feasibility, budget constraints, and Pareto optimality) while providing detailed reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-31-0-2-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly derives the conditions for Pareto optimality in a price equilibrium with transfers, aligning with the gold answer's key points (feasibility, budget constraints, and Pareto optimality) while providing detailed reasoning."}, "llm_echoed_qid": "textbook-31-0-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly derives the conditions for Pareto optimality in a price equilibrium with transfers, aligning with the gold answer's key points (feasibility, budget constraints, and Pareto optimality) while providing detailed reasoning."}
{"script_processing_qid": "textbook-62-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Using the long-run averages $b = 2.7\\%$, $e = 43\\%$, and $n = 0.09\\%$, calculate the natural rate of unemployment $u^{*}$.\n\nGOLD_ANSWER:\n1. **Formula**: $u^{*} = \\frac{b}{n + e + b}$.\n2. **Substitute**: $u^{*} = \\frac{0.027}{0.0009 + 0.43 + 0.027}$.\n3. **Calculate**: $u^{*} = \\frac{0.027}{0.4579} \\approx 0.059$ or $5.9\\%$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the formula for the natural rate of unemployment: $u^{*} = \\frac{b}{n + e + b}$.\nStep 2: Substitute the given values into the formula: $u^{*} = \\frac{2.7\\%}{0.09\\% + 43\\% + 2.7\\%}$.\nStep 3: Convert percentages to decimals for calculation: $u^{*} = \\frac{0.027}{0.0009 + 0.43 + 0.027}$.\nStep 4: Calculate the denominator: $0.0009 + 0.43 + 0.027 = 0.4579$.\nStep 5: Compute the result: $u^{*} = \\frac{0.027}{0.4579} \\approx 0.0589$.\nStep 6: Convert back to percentage: $0.0589 \\times 100 \\approx 5.89\\%$.\nFinal Answer: The natural rate of unemployment $u^{*}$ is approximately **5.9%** (rounded to one decimal place).\n\nQID: textbook-62-0-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-62-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer follows the correct formula, substitutes the values accurately, and performs the calculations correctly, arriving at the same result as the gold answer (5.9%). The minor difference in intermediate steps (5.89% vs. 5.9%) is due to rounding and is negligible.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-62-0-0-1", "category": "CORRECT", "explanation": "The candidate's answer follows the correct formula, substitutes the values accurately, and performs the calculations correctly, arriving at the same result as the gold answer (5.9%). The minor difference in intermediate steps (5.89% vs. 5.9%) is due to rounding and is negligible."}, "llm_echoed_qid": "textbook-62-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer follows the correct formula, substitutes the values accurately, and performs the calculations correctly, arriving at the same result as the gold answer (5.9%). The minor difference in intermediate steps (5.89% vs. 5.9%) is due to rounding and is negligible."}
{"script_processing_qid": "textbook-106-18-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate the average fixed cost ($AFC$), average variable cost ($AVC$), and average total cost ($ATC$) for Thirsty Thelma's Lemonade Stand when producing 7 glasses per hour.\n\nGOLD_ANSWER:\n1. **Given Data**:\n   - Quantity: 7 glasses, Fixed Cost: $3.00, Variable Cost: $6.30, Total Cost: $9.30\n2. **Average Fixed Cost ($AFC$)**:\n   $$ AFC = FC / Q = 3.00 / 7 \\approx 0.43 $$\n3. **Average Variable Cost ($AVC$)**:\n   $$ AVC = VC / Q = 6.30 / 7 = 0.90 $$\n4. **Average Total Cost ($ATC$)**:\n   $$ ATC = TC / Q = 9.30 / 7 \\approx 1.33 $$\n\nCANDIDATE_ANSWER:\nStep 1: To calculate AFC, AVC, and ATC, we need numerical values for fixed costs (FC) and variable costs (VC) at 7 glasses of output. Both AFC and AVC require these values (AFC = FC/Q, AVC = VC/Q, ATC = (FC + VC)/Q).\n\nStep 2: The provided context explains the general relationships (e.g., TC = FC + VC, ATC = TC/Q) but does not include specific cost data for Thirsty Thelma's Lemonade Stand (e.g., FC amount, VC at Q=7, or total cost values).\n\nStep 3: Since no numerical values for FC, VC, or TC are given in the context, it is impossible to compute these averages.\n\nConclusion: The provided context does not contain enough information to answer this question.\n\nQID: textbook-106-18-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-18-0-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate answer incorrectly concludes that the context lacks sufficient information, despite the GOLD_ANSWER clearly providing the necessary data (FC, VC, TC) for the calculations.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-18-0-2", "category": "INCORRECT", "explanation": "The candidate answer incorrectly concludes that the context lacks sufficient information, despite the GOLD_ANSWER clearly providing the necessary data (FC, VC, TC) for the calculations."}, "llm_echoed_qid": "textbook-106-18-0-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate answer incorrectly concludes that the context lacks sufficient information, despite the GOLD_ANSWER clearly providing the necessary data (FC, VC, TC) for the calculations."}
{"script_processing_qid": "textbook-114-30-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using Figure 11.2, calculate the additional profit a firm gains from perfect first-degree price discrimination compared to charging a single price. Assume linear demand and marginal cost functions.\n\nGOLD_ANSWER:\n1. **Single Price Profit**: Profit is the area between MR and MC up to $Q^{*}$.\n2. **Perfect Price Discrimination Profit**: Profit is the area between D and MC up to $Q^{**}$.\n3. **Additional Profit**: The difference is the area between D and MR from $Q^{*}$ to $Q^{**}$. For linear functions, this can be calculated as the integral of (D - MR) over this interval.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key monopoly outcomes under single pricing\n- Monopoly quantity (Q_m) occurs where MR = MC\n- Price (P_m) is determined by the demand curve at Q_m\n\nStep 2: Determine perfect price discrimination outcomes\n- Efficient quantity (Q_c) occurs where P = MC\n- All consumer surplus becomes producer surplus\n\nStep 3: Calculate additional profit\n1. The difference in quantities: ΔQ = Q_c - Q_m\n2. The difference in price-cost margins at Q_m: ΔP = P_m - MC(Q_m)\n3. Additional profit = ½ × ΔQ × ΔP (triangular area between Q_m and Q_c under demand curve above MC)\n\nFor linear demand (P = a - bQ) and constant MC = c:\n- Q_m = (a - c)/2b\n- Q_c = (a - c)/b\n- Additional profit = (a - c)²/(8b)\n\nThis represents the deadweight loss recaptured through perfect discrimination.\n\nQID: textbook-114-30-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-30-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the steps to calculate the additional profit from perfect first-degree price discrimination compared to charging a single price, including the correct formulas for linear demand and constant marginal cost. The explanation aligns with the gold answer's requirements.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-30-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the steps to calculate the additional profit from perfect first-degree price discrimination compared to charging a single price, including the correct formulas for linear demand and constant marginal cost. The explanation aligns with the gold answer's requirements."}, "llm_echoed_qid": "textbook-114-30-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the steps to calculate the additional profit from perfect first-degree price discrimination compared to charging a single price, including the correct formulas for linear demand and constant marginal cost. The explanation aligns with the gold answer's requirements."}
{"script_processing_qid": "textbook-103-5-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. How does the borrowing constraint $a^{\\prime} \\geq -\\phi$ affect the household's optimal consumption and savings decisions?\n\nGOLD_ANSWER:\nThe borrowing constraint $a^{\\prime} \\geq -\\phi$ limits the household's ability to borrow, which affects its consumption and savings behavior:\n- **Binding Constraint**: When the constraint binds, the household cannot borrow beyond $\\phi$, forcing it to consume less and save more than it would in an unconstrained scenario.\n- **Non-Binding Constraint**: When the constraint does not bind, the household can smooth consumption optimally by borrowing or saving as needed.\n- **Implications**: The constraint introduces a kink in the value function and can lead to precautionary savings, as households anticipate future liquidity constraints.\n\nCANDIDATE_ANSWER:\nStep 1: The borrowing constraint $a' \\geq -\\phi$ imposes a lower bound on next-period assets. This restricts the household's ability to borrow, limiting their maximum debt to $\\phi$.\n\nStep 2: In the Bellman equation, the constraint narrows the feasible set for $a'$. The household cannot choose asset levels below $-\\phi$, even if marginal utility of consumption would otherwise incentivize more borrowing.\n\nStep 3: When the constraint binds ($a' = -\\phi$), consumption becomes $c = (1+r)a + ws + \\phi$. This forces higher current consumption than unconstrained optimal levels, as borrowing cannot be increased further to defer consumption.\n\nStep 4: The constraint alters savings incentives. Households with low $a$ near $-\\phi$ must reduce consumption volatility by maintaining buffer savings to avoid hitting the borrowing limit in future periods.\n\nStep 5: In policy iteration, the grid for $a'$ is truncated at $-\\phi$. This modifies the dynamic programming solution by systematically excluding asset choices below $-\\phi$ at every iteration, resulting in different consumption/saving paths compared to an unconstrained solution.\n\nQID: textbook-103-5-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-103-5-0-3\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the borrowing constraint's impact on asset choices and the Bellman equation (Steps 1-2, 5), but incorrectly states that binding constraints force higher current consumption (Step 3 contradicts GOLD_ANSWER's 'consume less'). Steps 4-5 correctly address precautionary savings and computational implications.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-5-0-3", "category": "PARTIALLY_CORRECT", "explanation": "The candidate's answer correctly identifies the borrowing constraint's impact on asset choices and the Bellman equation (Steps 1-2, 5), but incorrectly states that binding constraints force higher current consumption (Step 3 contradicts GOLD_ANSWER's 'consume less'). Steps 4-5 correctly address precautionary savings and computational implications."}, "llm_echoed_qid": "textbook-103-5-0-3", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the borrowing constraint's impact on asset choices and the Bellman equation (Steps 1-2, 5), but incorrectly states that binding constraints force higher current consumption (Step 3 contradicts GOLD_ANSWER's 'consume less'). Steps 4-5 correctly address precautionary savings and computational implications."}
{"script_processing_qid": "textbook-123-2-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze why Equilibrium L fails the first welfare theorem. What specific condition is violated?\n\nGOLD_ANSWER:\nEquilibrium L is **autarkic** with no trade and prices set to $\\frac{q\\_{t+1}^{0}}{q\\_{t}^{0}} = \\alpha > 1$. The first welfare theorem requires that the value of the aggregate endowment at equilibrium prices is finite. Here, the endowment sequence's value diverges to infinity because $\\alpha > 1$, violating this condition. Thus, the equilibrium allocation is not Pareto optimal.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the First Welfare Theorem's requirements. It states that under complete markets, price-taking behavior, and no externalities, competitive equilibria are Pareto efficient.\n\nStep 2: Identify the overlap in generations (OLG) structure. The model has an infinite sequence of agents and goods, violating the finite-agent assumption of the First Welfare Theorem.\n\nStep 3: Analyze equilibrium allocations. In OLG models like this, markets are 'incomplete' in a temporal sense: agents cannot trade with unborn generations, creating a failure of intergenerational coordination.\n\nStep 4: Observe the specific violation. The critical missing condition is the \\'absence of an infinite chain of unfulfilled reciprocity\\' – earlier generations cannot compensate later ones (and vice versa), preventing Pareto efficiency. The infinity of agents/goods violates the finite resource redistribution requirement needed for the First Welfare Theorem.\n\nQID: textbook-123-2-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-2-0-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate correctly identifies that Equilibrium L fails the first welfare theorem but focuses on the OLG structure and incomplete markets, whereas the gold answer highlights the divergence of the endowment sequence's value due to α > 1 as the specific condition violated.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-2-0-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate correctly identifies that Equilibrium L fails the first welfare theorem but focuses on the OLG structure and incomplete markets, whereas the gold answer highlights the divergence of the endowment sequence's value due to α > 1 as the specific condition violated."}, "llm_echoed_qid": "textbook-123-2-0-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate correctly identifies that Equilibrium L fails the first welfare theorem but focuses on the OLG structure and incomplete markets, whereas the gold answer highlights the divergence of the endowment sequence's value due to α > 1 as the specific condition violated."}
{"script_processing_qid": "textbook-38-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the condition under which a consumption stream c exhibits time impatience, given the utility function u(·) is strictly increasing.\n\nGOLD_ANSWER:\n1. **Condition for Time Impatience**: A consumption stream c exhibits time impatience if for some t, u(c) > u(0) and for all t, u(c) ≥ u(0). This implies that the consumer prefers nonzero consumption at some time over zero consumption, and the discounted utility of the stream satisfies V(c') < V(c) for c' = (0, 0, ...).\n\nCANDIDATE_ANSWER:\nStep 1: Define time impatience as V(c') < V(c) when c' is a 1-period delayed version of c (i.e., c' = (0, c_0, c_1, ...)).\n\nStep 2: Express V(c) as the present discounted value of the consumption stream:\nV(c) = Σ_{t=0}^∞ δ^t u(c_t)\n\nStep 3: Compute V(c') when the first period consumption is replaced with 0:\nV(c') = u(0) + δΣ_{t=0}^∞ δ^t u(c_t) = u(0) + δV(c)\n\nStep 4: Establish the time impatience condition by requiring V(c') < V(c):\nu(0) + δV(c) < V(c)\n\nStep 5: Rearrange the inequality:\nu(0) < V(c)(1 - δ)\n\nStep 6: Substitute the explicit form of V(c):\nu(0) < (1 - δ)Σ_{t=0}^∞ δ^t u(c_t)\n\nStep 7: Recognize that since u(·) is strictly increasing and c is non-zero (u(c_t) ≥ u(0) for all t and u(c_t) > u(0) for some t), the RHS strictly exceeds u(0)/(1 - δ) × (1 - δ) = u(0).\n\nFinal Condition: Time impatience holds when δ ∈ (0,1) and the consumption stream c has at least one period with u(c_t) > u(0), which is guaranteed by the strict monotonicity of u(·).\n\nQID: textbook-38-0-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-38-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the condition for time impatience by showing the necessary inequality and considering the properties of the utility function and discount factor. The reasoning aligns with the gold answer's requirements.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-38-0-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the condition for time impatience by showing the necessary inequality and considering the properties of the utility function and discount factor. The reasoning aligns with the gold answer's requirements."}, "llm_echoed_qid": "textbook-38-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the condition for time impatience by showing the necessary inequality and considering the properties of the utility function and discount factor. The reasoning aligns with the gold answer's requirements."}
{"script_processing_qid": "textbook-16-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that a profit-maximizing input vector $x^{*}$ must also minimize costs for the output level $f(x^{*})$. Use contradiction as shown in the text.\n\nGOLD_ANSWER:\n1. **Assumption**: Suppose $x^{*}$ is profit-maximizing but does not minimize costs for $f(x^{*})$. \\n2. **Existence of $x^{**}$**: There exists $x^{**}$ such that $f(x^{**}) \\geq f(x^{*})$ and $w(x^{**} - x^{*}) < 0$. \\n3. **Profit Comparison**: $p f(x^{**}) - w x^{**} > p f(x^{*}) - w x^{*}$. \\n4. **Contradiction**: This contradicts the assumption that $x^{*}$ is profit-maximizing. Hence, $x^{*}$ must minimize costs.\n\nCANDIDATE_ANSWER:\nStep 1: Assume (for contradiction) that a profit-maximizing input vector $x^{*}$ does NOT minimize costs for output $f(x^{*})$. By definition of cost minimization, there must exist another input vector $x^{**}$ such that $f(x^{**}) \\geq f(x^{*})$ (same or greater output) and $w x^{**} < w x^{*}$ (strictly lower cost).\n\nStep 2: Compare profits at $x^{*}$ and $x^{**}$. Profit at $x^{**}$ would be $p f(x^{**}) - w x^{**} \\geq p f(x^{*}) - w x^{**} > p f(x^{*}) - w x^{*}$ (since $f(x^{**}) \\geq f(x^{*})$ and $w x^{**} < w x^{*}$).\n\nStep 3: This implies $x^{**}$ yields strictly higher profit than $x^{*}$, which directly contradicts the initial assumption that $x^{*}$ is profit-maximizing.\n\nStep 4: Therefore, our initial assumption must be false. A profit-maximizing input vector $x^{*}$ must necessarily minimize costs for output $f(x^{*})$.\n\nQID: textbook-16-0-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-16-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer follows the same logical structure as the gold answer, using contradiction to prove that a profit-maximizing input vector must also minimize costs for the given output level. The reasoning and conclusion are both correct and align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-16-0-0-0", "category": "CORRECT", "explanation": "The candidate's answer follows the same logical structure as the gold answer, using contradiction to prove that a profit-maximizing input vector must also minimize costs for the given output level. The reasoning and conclusion are both correct and align with the gold answer."}, "llm_echoed_qid": "textbook-16-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer follows the same logical structure as the gold answer, using contradiction to prove that a profit-maximizing input vector must also minimize costs for the given output level. The reasoning and conclusion are both correct and align with the gold answer."}
{"script_processing_qid": "textbook-112-15-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. **Quantitative**: Derive the relationship between the funds destined for maintaining labor and the wages of labor in a stationary economy like China, using Smith's observations.\n\nGOLD_ANSWER:\nIn a stationary economy:\n1. **Funds for Wages**: Constant over centuries.\n2. **Labor Supply**: Exceeds demand due to no growth in funds.\n3. **Wages**: Determined by competition among laborers, driving wages down to subsistence levels (e.g., China's low wages and poor living conditions).\n\nMathematically: If $F$ (funds) is constant and $L$ (labor) grows, then $w = \\frac{F}{L}$ decreases over time.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Smith's key proposition - In stationary economies, funds for wages remain constant over time ('continued for several centuries of the same extent').\n\nStep 2: Recognize labor market dynamics - Constant funds (F) imply fixed labor demand (D). Population growth causes labor supply (S) to accumulate over time (since stationary economies still experience some population growth at lower rates than expanding economies).\n\nStep 3: Apply Smith's observation - When S consistently meets/exceeds D ('could easily supply, even more than supply'), employers have no competitive pressure to increase wages. The wage rate W becomes: W = F/S\n\nStep 4: Derive the inverse relationship - Given fixed F and increasing S over time in stationary states, W must decrease proportionally to maintain equilibrium (W ∝ 1/S when F is constant).\n\nStep 5: China application - As a stationary economy per context, China's wages remain low because accumulated labor supply (from centuries of stability) erodes wage growth despite large absolute wage funds.\n\nQID: textbook-112-15-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-15-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies Smith's proposition, labor market dynamics, and the inverse relationship between wages and labor supply in a stationary economy. It also accurately applies this reasoning to China, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-15-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies Smith's proposition, labor market dynamics, and the inverse relationship between wages and labor supply in a stationary economy. It also accurately applies this reasoning to China, aligning with the gold answer."}, "llm_echoed_qid": "textbook-112-15-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies Smith's proposition, labor market dynamics, and the inverse relationship between wages and labor supply in a stationary economy. It also accurately applies this reasoning to China, aligning with the gold answer."}
{"script_processing_qid": "textbook-34-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Prove that if a production set Y is convex, then the production function f(z) is concave.\n\nGOLD_ANSWER:\n1. **Given**: Y is convex. For any z, z' ∈ ℝ₊ᴸ⁻¹ and α ∈ [0,1], (-z, f(z)) ∈ Y and (-z', f(z')) ∈ Y.\n2. **Convex combination**: By convexity, (-(αz + (1-α)z'), αf(z) + (1-α)f(z')) ∈ Y.\n3. **Definition of f**: Since f(z) is the maximum output for input z, αf(z) + (1-α)f(z') ≤ f(αz + (1-α)z'). Hence, f is concave.\n\nCANDIDATE_ANSWER:\nStep 1: Start with convexity of production set Y. For any two production plans (-z, f(z)) ∈ Y and (-z', f(z')) ∈ Y, convexity implies their convex combination is also in Y.\n\nStep 2: Consider α ∈ [0,1]. Convexity gives: \n(-[αz + (1−α)z'], αf(z) + (1−α)f(z')) ∈ Y.\n\nStep 3: By definition of the production function f, any feasible output q with input bundle αz + (1−α)z' must satisfy q ≤ f(αz + (1−α)z').\n\nStep 4: Since αf(z) + (1−α)f(z') is the output in the convex combination, we have: \nαf(z) + (1−α)f(z') ≤ f(αz + (1−α)z').\n\nStep 5: This inequality matches the definition of concavity for f. Therefore, f(z) is concave.\n\nQID: textbook-34-0-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-34-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the logical steps to prove the concavity of the production function f(z) given the convexity of the production set Y, matching the reasoning and conclusion of the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-34-0-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the logical steps to prove the concavity of the production function f(z) given the convexity of the production set Y, matching the reasoning and conclusion of the gold answer."}, "llm_echoed_qid": "textbook-34-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the logical steps to prove the concavity of the production function f(z) given the convexity of the production set Y, matching the reasoning and conclusion of the gold answer."}
{"script_processing_qid": "textbook-47-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the limitations of the rational expectations (RE) assumption and how adaptive learning addresses these limitations.\n\nGOLD_ANSWER:\n1.  **Limitations of RE**: The RE assumption requires agents to know the true economic model and implicitly assumes coordination of expectations, which is unrealistic. \n2.  **Adaptive Learning**: Adaptive learning relaxes these assumptions by allowing agents to use statistical or econometric techniques to form expectations, updating their beliefs as new data becomes available. This approach is more realistic as it aligns agents' information with that of economists.\n\nCANDIDATE_ANSWER:\nStep 1: Limitations of Rational Expectations (RE)\n- RE assumes agents know the true economic model generating the data (unrealistically strong informational requirement).\n- RE implicitly assumes perfect coordination of expectations across all agents (agents simultaneously align expectations with the model's statistical properties).\n\nStep 2: How Adaptive Learning Addresses These Limitations\n- Adaptive learning replaces the full-information RE assumption with a process where agents iteratively update forecasts using observable data (e.g., via least squares regression).\n- Agents estimate parameters of a perceived law of motion and revise estimates as new data arrives, eliminating the need for prior knowledge of the true model.\n- Adaptive learning allows for heterogeneity in initial beliefs and does not require coordination. Convergence to RE equilibria (if it occurs) emerges endogenously through the learning dynamics rather than being imposed a priori.\n- This framework provides microfoundations for expectation formation, making models more behaviorally plausible while retaining the possibility of RE as a long-run equilibrium outcome.\n\nQID: textbook-47-0-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-47-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately identifies the limitations of the rational expectations assumption and effectively explains how adaptive learning addresses these limitations. The response is detailed and aligns well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-47-0-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately identifies the limitations of the rational expectations assumption and effectively explains how adaptive learning addresses these limitations. The response is detailed and aligns well with the gold answer."}, "llm_echoed_qid": "textbook-47-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately identifies the limitations of the rational expectations assumption and effectively explains how adaptive learning addresses these limitations. The response is detailed and aligns well with the gold answer."}
{"script_processing_qid": "textbook-107-14-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that if a consumer's utility function is $U=1000+2min\\{x,y\\}$, then $x$ and $y$ are perfect complements.\n\nGOLD_ANSWER:\n1.  **Definition**: Perfect complements imply the consumer only cares about the smaller quantity of $x$ or $y$.\n2.  **Utility Function**: $U=1000+2min\\{x,y\\}$ depends solely on $min\\{x,y\\}$.\n3.  **Indifference Curves**: L-shaped, with kinks where $x=y$.\n4.  **Conclusion**: The utility function reflects perfect complementarity, as increasing one good beyond the other does not increase utility.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that perfect complements have a utility function of the form \\( U = a \\cdot \\min\\{x, y\\} + b \\), where \\( a > 0 \\) and \\( b \\) are constants. The key property is that utility depends on the minimum of \\( x \\) and \\( y \\).\n\nStep 2: Analyze the given utility function \\( U = 1000 + 2\\min\\{x, y\\} \\). The term \\( \\min\\{x, y\\} \\) determines utility, while the additive constant (1000) and multiplier (2) do not affect the shape of indifference curves or the consumer’s preference structure.\n\nStep 3: Demonstrate that indifference curves are L-shaped. For a fixed utility level \\( U \\), \\( \\min\\{x, y\\} \\) implies \\( x \\) and \\( y \\) must be consumed in a 1:1 ratio. Excess quantities of one good (beyond the minimum) do not increase utility, matching the definition of perfect complements.\n\nStep 4: Confirm that constants (1000 and 2) do not alter preferences. Utility functions are unique up to positive monotonic transformations, so scaling by 2 or adding 1000 preserves the ordinal ranking of bundles.\n\nConclusion: Since utility depends only on \\( \\min\\{x, y\\} \\), and indifference curves are L-shaped with a 1:1 fixed ratio, \\( x \\) and \\( y \\) are perfect complements.\n\nQID: textbook-107-14-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-14-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the utility function's dependence on the minimum of \\(x\\) and \\(y\\), explains the L-shaped indifference curves, and confirms that the constants do not alter the preference structure, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-14-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the utility function's dependence on the minimum of \\(x\\) and \\(y\\), explains the L-shaped indifference curves, and confirms that the constants do not alter the preference structure, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-107-14-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the utility function's dependence on the minimum of \\(x\\) and \\(y\\), explains the L-shaped indifference curves, and confirms that the constants do not alter the preference structure, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-119-9-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. What is the intuition behind using the Durbin-Watson test to detect specification errors?\n\nGOLD_ANSWER:\nThe Durbin-Watson test detects serial correlation in residuals. If residuals exhibit systematic patterns (e.g., due to omitted variables), the test can indicate specification errors. The key idea is that omitted variables may introduce autocorrelation-like patterns in residuals.\n\nCANDIDATE_ANSWER:\nStep 1: The Durbin-Watson (DW) test is traditionally used to detect first-order serial correlation in residuals.\nStep 2: However, specification errors (e.g., omitted variables or incorrect functional form) often induce patterned residuals that mimic serial correlation.\nStep 3: A significant DW statistic in this context may indicate residual autocorrelation arising from model misspecification rather than genuine time-series dependence.\nStep 4: This makes the DW test a practical tool to flag potential specification errors when residuals show systematic patterns that violate the classical linear regression assumptions.\n\nQID: textbook-119-9-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-9-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the intuition behind using the Durbin-Watson test to detect specification errors, aligning with the gold answer. Both emphasize that the test can indicate specification errors by detecting patterns in residuals that may arise from omitted variables or other model misspecifications.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-9-1-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains the intuition behind using the Durbin-Watson test to detect specification errors, aligning with the gold answer. Both emphasize that the test can indicate specification errors by detecting patterns in residuals that may arise from omitted variables or other model misspecifications."}, "llm_echoed_qid": "textbook-119-9-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the intuition behind using the Durbin-Watson test to detect specification errors, aligning with the gold answer. Both emphasize that the test can indicate specification errors by detecting patterns in residuals that may arise from omitted variables or other model misspecifications."}
{"script_processing_qid": "textbook-110-4-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Critically evaluate the limitations of the Nash equilibrium concept in the context of mixed strategies.\n\nGOLD_ANSWER:\n1.  **Limitations**:\n    - **Plausibility**: Mixed strategies may not be realistic in all economic contexts, as players may not randomize their actions.\n    - **Multiple Equilibria**: Games can have multiple Nash equilibria, making predictions ambiguous.\n    - **Static Nature**: Nash equilibrium does not account for dynamic adjustments or learning over time.\n2.  **Critique**: While mathematically robust, the Nash equilibrium may lack behavioral realism in certain settings, especially where randomization is implausible.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Theoretical Artificiality\nMixed strategies are described as a \"theoretical artifice\" in the context. This implies they primarily serve to fill gaps in Nash equilibrium existence proofs rather than reflecting realistic decision-making processes. A key limitation is their reliance on abstract randomization devices, which are rarely observable or practical in real economic interactions.\n\nStep 2: Evaluate Plausibility Requirements\nThe context emphasizes that their appropriateness depends on whether economic actors can realistically \"observe and understand\" randomized strategies. This creates limitations in applications where agents lack the computational capacity or institutional frameworks (e.g., verifiable random devices) to implement mixed strategies credibly.\n\nStep 3: Context Dependency\nThe text specifies that limitations manifest through \"specific context of the microeconomic model.\" In many real-world scenarios (e.g., discrete choice environments), agents cannot fractionally randomize actions as mixed strategies require, making Nash equilibrium predictions descriptively inaccurate.\n\nStep 4: Strategic Transparency Challenge\nThe equilibrium concept assumes all players understand opponents' randomization probabilities. However, the context questions whether actors can realistically infer or respond optimally to unobservable mixed strategies, undermining the equilibrium's predictive power in asymmetric information settings.\n\nQID: textbook-110-4-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-110-4-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the limitations of Nash equilibrium in the context of mixed strategies, aligning with the gold answer's points on plausibility, theoretical artificiality, and context dependency. The candidate's detailed steps effectively critique the concept's behavioral realism and practical applicability.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-4-0-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the limitations of Nash equilibrium in the context of mixed strategies, aligning with the gold answer's points on plausibility, theoretical artificiality, and context dependency. The candidate's detailed steps effectively critique the concept's behavioral realism and practical applicability."}, "llm_echoed_qid": "textbook-110-4-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the limitations of Nash equilibrium in the context of mixed strategies, aligning with the gold answer's points on plausibility, theoretical artificiality, and context dependency. The candidate's detailed steps effectively critique the concept's behavioral realism and practical applicability."}
{"script_processing_qid": "textbook-122-5-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the Durbin’s alternative test statistic for AR(1) serial correlation, starting from the OLS residuals $\\hat{u}_{t}$.\n\nGOLD_ANSWER:\n1. **Step 1**: Obtain OLS residuals $\\hat{u}_{t}$ from the regression of $y_{t}$ on $x_{t1},...,x_{tk}$.\n2. **Step 2**: Regress $\\hat{u}_{t}$ on $x_{t1},...,x_{tk}$ and $\\hat{u}_{t-1}$ for $t=2,...,n$.\n3. **Step 3**: The $t$-statistic for $\\hat{\\rho}$ in this regression is the test statistic for $H_{0}: \\rho=0$.\n4. **Step 4**: Reject $H_{0}$ if $|t_{\\hat{\\rho}}| > c_{\\alpha/2}$, where $c_{\\alpha/2}$ is the critical value from the $t$-distribution.\n\nCANDIDATE_ANSWER:\nStep 1: Compute the OLS residuals. First, estimate the original OLS regression model: $$y_t = \\beta_0 + \\beta_1 x_{t1} + \\cdots + \\beta_k x_{tk} + u_t$$ and obtain the residuals $$\\hat{u}_t$$ for all $$t = 1, 2, \\ldots, n$$.\n\nStep 2: Specify the auxiliary regression. Regress the OLS residuals $$\\hat{u}_t$$ on all original regressors ($$x_{t1}, \\ldots, x_{tk}$$) and the lagged residual $$\\hat{u}_{t-1}$$: $$\\hat{u}_t = \\gamma_0 + \\gamma_1 x_{t1} + \\cdots + \\gamma_k x_{tk} + \\rho \\hat{u}_{t-1} + e_t$$ for $$t = 2, \\ldots, n$$.\n\nStep 3: Estimate the coefficient on the lagged residual. The key parameter is $$\\rho$$ (the coefficient on $$\\hat{u}_{t-1}$$). Compute its OLS estimate $$\\hat{\\rho}$$ and its standard error $$\\text{SE}(\\hat{\\rho})$$ from the auxiliary regression.\n\nStep 4: Compute the t-statistic. The Durbin’s alternative test statistic is the t-statistic for $$\\hat{\\rho}$$: $$t_{\\hat{\\rho}} = \\frac{\\hat{\\rho}}{\\text{SE}(\\hat{\\rho})}$$.\n\nStep 5: Hypothesis testing. Compare $$t_{\\hat{\\rho}}$$ to critical values from the t-distribution (or standard normal in large samples) to test $$H_0: \\rho = 0$$ against $$H_1: \\rho \\neq 0$$. This statistic remains valid even when some regressors are not strictly exogenous.\n\nQID: textbook-122-5-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-5-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately follows the steps outlined in the gold answer, including obtaining OLS residuals, specifying the auxiliary regression, estimating the coefficient on the lagged residual, computing the t-statistic, and performing hypothesis testing. The reasoning and details align perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-5-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately follows the steps outlined in the gold answer, including obtaining OLS residuals, specifying the auxiliary regression, estimating the coefficient on the lagged residual, computing the t-statistic, and performing hypothesis testing. The reasoning and details align perfectly with the gold answer."}, "llm_echoed_qid": "textbook-122-5-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately follows the steps outlined in the gold answer, including obtaining OLS residuals, specifying the auxiliary regression, estimating the coefficient on the lagged residual, computing the t-statistic, and performing hypothesis testing. The reasoning and details align perfectly with the gold answer."}
{"script_processing_qid": "textbook-102-2-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Conduct a likelihood ratio test for the hypothesis $H_0: \\beta = 0$ in the probit model. Compare the result to the Wald test.\n\nGOLD_ANSWER:\n1. **Restricted log-likelihood**: $\\ln L_0 = 100[0.44\\ln 0.44 + 0.56\\ln 0.56] = -68.593$.\\n2. **Unrestricted log-likelihood**: $\\ln L = 24\\ln 0.461 + 28\\ln 0.539 + 32\\ln 0.667 + 16\\ln 0.333 = -66.442$.\\n3. **LR statistic**: $\\lambda = -2(-66.442 - (-68.593)) = 4.302$.\\n4. **Critical value**: $\\chi^2_{1, 0.95} = 3.84$.\\n5. **Conclusion**: Reject $H_0$ since $4.302 > 3.84$.\\n6. **Wald test**: $z = -2.073$ (rejects $H_0$ at 5% level). Both tests agree in rejecting the null hypothesis.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate restricted model log-likelihood under H₀: β=0\n- Under H₀, δ = α (since β=0)\n- MLE for α becomes Φ⁻¹(44/100) = Φ⁻¹(0.44) ≈ -0.15\n\nStep 2: Compute log-likelihood values\n- Unrestricted model (from given estimates):\n  lnL_unrestricted = 24ln(1-Φ(0.098)) + 32ln(1-Φ(-0.431)) + 28lnΦ(0.098) + 16lnΦ(-0.431)\n  ≈ -66.31\n\n- Restricted model:\n  lnL_restricted = 56ln(1-Φ(-0.15)) + 44lnΦ(-0.15)\n  ≈ -68.48\n\nStep 3: Compute likelihood ratio test statistic\n  LR = 2(lnL_unrestricted - lnL_restricted)\n     = 2(-66.31 - (-68.48)) = 4.34\n\nStep 4: Compare to critical value\n  χ²(1) critical value at 5% = 3.841\n  Since 4.34 > 3.841, reject H₀\n\nStep 5: Compare with Wald test\n  Wald statistic = (-0.529/0.2552)² = (-2.073)² = 4.30\n  Both tests reject H₀ at 5% level\n\nConclusion: The likelihood ratio test (LR=4.34) and Wald test (W=4.30) give similar results, both rejecting β=0 with nearly identical test statistics due to asymptotic equivalence.\n\nQID: textbook-102-2-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-102-2-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps for conducting a likelihood ratio test and compares it to the Wald test, with results very close to the gold answer. Both tests lead to the same conclusion of rejecting the null hypothesis, demonstrating correct understanding and application of the statistical tests.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-102-2-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps for conducting a likelihood ratio test and compares it to the Wald test, with results very close to the gold answer. Both tests lead to the same conclusion of rejecting the null hypothesis, demonstrating correct understanding and application of the statistical tests."}, "llm_echoed_qid": "textbook-102-2-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps for conducting a likelihood ratio test and compares it to the Wald test, with results very close to the gold answer. Both tests lead to the same conclusion of rejecting the null hypothesis, demonstrating correct understanding and application of the statistical tests."}
{"script_processing_qid": "textbook-112-30-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Analyze the role of parsimony and high profits in capital accumulation according to Adam Smith.\n\nGOLD_ANSWER:\n1.  **Parsimony**: High profits destroy merchant parsimony, leading to luxury and reduced savings.\n2.  **Capital Accumulation**: Merchants, as industry leaders, influence broader savings behavior; high profits reduce funds for productive labor.\n3.  **Example**: Spain and Portugal's high profits did not augment national capital but led to insufficient domestic capital and reliance on foreign funds.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Smith's key claims in the context. The monopoly of colony trade (a) raises profit rates by excluding competition, (b) draws capital/produce from old markets, (c) suppresses national revenue accumulation, and (d) links high profits to reduced parsimony.\n\nStep 2: Define terms. 'Parsimony' refers to frugality/saving, which Smith associates with capital accumulation. 'High profits' from monopolies distort normal incentives.\n\nStep 3: Analyze the tension. High profits under monopolies reduce merchants' natural parsimony (saving/reinvestment), harming capital accumulation. However, the colony trade's structural benefits (e.g., expanded markets) outweigh this drawback for Great Britain.\n\nStep 4: Conclusion. While high profits undermine parsimony-driven accumulation, Smith argues the colony trade’s net positive effects (despite its monopoly distortions) still promote national advantage by leveraging broader economic gains.\n\nQID: textbook-112-30-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-30-0-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the tension between high profits and parsimony, and links it to capital accumulation, aligning with Smith's views. However, the conclusion about the colony trade’s net positive effects is not explicitly supported by the GOLD_ANSWER, which focuses on the negative impacts of high profits on parsimony and capital accumulation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-30-0-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the tension between high profits and parsimony, and links it to capital accumulation, aligning with Smith's views. However, the conclusion about the colony trade’s net positive effects is not explicitly supported by the GOLD_ANSWER, which focuses on the negative impacts of high profits on parsimony and capital accumulation."}, "llm_echoed_qid": "textbook-112-30-0-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the tension between high profits and parsimony, and links it to capital accumulation, aligning with Smith's views. However, the conclusion about the colony trade’s net positive effects is not explicitly supported by the GOLD_ANSWER, which focuses on the negative impacts of high profits on parsimony and capital accumulation."}
{"script_processing_qid": "textbook-122-36-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Interpret the coefficient $\\beta_{2}$ in the model $\\log(y) = \\beta_{0} + \\beta_{1}\\log(x_{1}) + \\beta_{2}x_{2} + u$, where $x_{2}$ is a proportion (not a percentage). How does the interpretation change if $x_{2}$ increases by 0.01?\n\nGOLD_ANSWER:\n1.  **General Interpretation**: $\\beta_{2}$ represents the change in $\\log(y)$ for a one-unit change in $x_{2}$, holding other variables constant. \n2.  **Percentage Change**: For small changes, $100 \\cdot \\Delta \\log(y) \\approx \\%\\Delta y$. Thus, $\\%\\Delta y \\approx \\beta_{2} (100 \\cdot \\Delta x_{2})$. \n3.  **Example**: If $\\Delta x_{2} = 0.01$, then $100 \\cdot \\Delta x_{2} = 1$ (a one percentage point change), and $\\%\\Delta y \\approx \\beta_{2} \\cdot 1$.\n\nCANDIDATE_ANSWER:\nStep 1: In the model log(y) = β₀ + β₁log(x₁) + β₂x₂ + u, the coefficient β₂ represents the percentage change in y for a one-unit increase in x₂. \n\nStep 2: Since x₂ is a proportion (e.g., 0.01 = 1%), a one-unit increase in x₂ corresponds to a 100 percentage point change (e.g., from 0 to 1). The exact interpretation is: a 100 percentage point increase in x₂ is associated with a (β₂ × 100)% change in y.\n\nStep 3: For a smaller 0.01-unit increase in x₂ (equivalent to a 1 percentage point increase), the percentage change in y becomes β₂ × 0.01 × 100 = β₂%. This adjustment scales the original coefficient by the magnitude of the change in x₂.\n\nConclusion: β₂ directly equals the percentage change in y for a 0.01-unit (1 percentage point) increase in x₂. The original interpretation (100 percentage point change) is scaled down by a factor of 100 for the 0.01-unit increment.\n\nQID: textbook-122-36-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-36-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly interprets the coefficient β₂ in the model, explaining both the general interpretation and the specific case where x₂ increases by 0.01. The reasoning aligns with the gold answer, including the scaling factor for the percentage change in y.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-36-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly interprets the coefficient β₂ in the model, explaining both the general interpretation and the specific case where x₂ increases by 0.01. The reasoning aligns with the gold answer, including the scaling factor for the percentage change in y."}, "llm_echoed_qid": "textbook-122-36-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly interprets the coefficient β₂ in the model, explaining both the general interpretation and the specific case where x₂ increases by 0.01. The reasoning aligns with the gold answer, including the scaling factor for the percentage change in y."}
{"script_processing_qid": "textbook-118-11-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain the geometric effect of multiplying a vector $\\mathbf{x} = (x_1, x_2)$ by a scalar $\\alpha$.\n\nGOLD_ANSWER:\n1. **Scaling**: The vector $\\alpha\\mathbf{x}$ has components $(\\alpha x_1, \\alpha x_2)$, scaling each component by $\\alpha$.\n2. **Direction**: If $\\alpha > 0$, the direction remains the same; if $\\alpha < 0$, the direction is reversed.\n3. **Length**: The length of $\\alpha\\mathbf{x}$ is $|\\alpha|$ times the length of $\\mathbf{x}$, i.e., $\\|\\alpha\\mathbf{x}\\| = |\\alpha|\\|\\mathbf{x}\\|$.\n4. **Parallelism**: The vectors $\\mathbf{x}$ and $\\alpha\\mathbf{x}$ are always parallel.\n\nCANDIDATE_ANSWER:\nStep 1: Understand scalar multiplication in component terms. Multiplying vector 𝐱 = (x₁, x₂) by scalar α results in new components (αx₁, αx₂).\n\nStep 2: Recognize geometric scaling effect. This operation scales the vector's length by a factor of |α|. If α > 0, the direction remains unchanged. If α < 0, the direction reverses to the opposite quadrant.\n\nStep 3: Visualize transformations. For α = 2, the vector doubles in length while maintaining direction. For α = 0.5, it halves in length. For α = -1, it becomes a reflection through the origin with equal magnitude.\n\nStep 4: Confirm using parallelogram representation. While not explicitly shown in the context, the component-wise scaling aligns with the geometric interpretation of stretching/compressing and potentially reversing the original vector's arrow representation in E².\n\nQID: textbook-118-11-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-11-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately captures all key aspects of the geometric effect of scalar multiplication on a vector, including scaling, direction change, and length modification, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-11-0-2", "category": "CORRECT", "explanation": "The candidate's answer accurately captures all key aspects of the geometric effect of scalar multiplication on a vector, including scaling, direction change, and length modification, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-118-11-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately captures all key aspects of the geometric effect of scalar multiplication on a vector, including scaling, direction change, and length modification, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-102-4-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of using the F-statistic when $\\boldsymbol{\\Omega}$ is estimated rather than known.\n\nGOLD_ANSWER:\nWhen $\\boldsymbol{\\Omega}$ is estimated:\n1. **Finite Sample**: The exact F-distribution may not hold, but an approximate F-distribution can be used.\n2. **Asymptotics**: The statistic converges to a chi-squared distribution if the estimator of $\\boldsymbol{\\Omega}$ is consistent.\n3. **Performance**: The test may suffer from size distortions in small samples if the estimator of $\\boldsymbol{\\Omega}$ is imprecise.\n\nCANDIDATE_ANSWER:\nStep 1: The provided F-statistic assumes Ω is known. When Ω is estimated (e.g., via FGLS), the estimator Ω̂ introduces additional variability into the test statistic.\n\nStep 2: Under normality, the exact F-distribution result relies on Ω being known. With estimated Ω, the numerator and denominator of the F-statistic become stochastic (dependent on Ω̂), violating the exact distributional properties.\n\nStep 3: In finite samples, the use of Ω̂ causes the test statistic to deviate from the F[J, n-K] distribution. This leads to size distortions (e.g., inflated Type I error rates) because the denominator’s chi-squared distribution assumption (scaled by n-K) no longer holds exactly.\n\nStep 4: Asymptotically, if Ω is consistently estimated, the F-statistic converges to the same limiting distribution as the known-Ω case (due to Slutsky’s theorem). However, finite-sample performance depends on the precision of Ω̂.\n\nStep 5: Practical implications include reliance on asymptotic approximations rather than exact finite-sample distributions and potential need for adjustments (e.g., using critical values from a t- or F-distribution with modified degrees of freedom).\n\nQID: textbook-102-4-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-102-4-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses the implications of using the F-statistic when Ω is estimated, covering finite sample issues, asymptotic behavior, and practical implications, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-102-4-1-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses the implications of using the F-statistic when Ω is estimated, covering finite sample issues, asymptotic behavior, and practical implications, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-102-4-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses the implications of using the F-statistic when Ω is estimated, covering finite sample issues, asymptotic behavior, and practical implications, aligning well with the gold answer."}
{"script_processing_qid": "textbook-104-7-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. For the tree-cutting problem with functional equation $v(k)=\\operatorname*{max}\\{k,\\beta v[h(k)]\\}$, derive the optimal cutting rule under the assumption that $h(k)$ is strictly increasing and concave.\n\nGOLD_ANSWER:\n1. **Optimal cutting rule**: The tree should be cut when $k \\geq \\beta v[h(k)]$.\n2. **Value function**: For $k$ below the cutting threshold, $v(k) = \\beta v[h(k)]$.\n3. **Threshold determination**: Solve for $k^*$ such that $k^* = \\beta v[h(k^*)]$. The tree is cut when $k_t \\geq k^*$.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize the decision rule in the functional equation. At any tree size k, the owner chooses between cutting now (receiving k) or waiting (receiving βv[h(k)]).\n\nStep 2: Identify the threshold k* where the agent is indifferent between cutting and waiting. This occurs when:\n   k* = βv[h(k*)]\n\nStep 3: Apply the cutting rule at h(k*). If h(k*) ≥ k*, then v[h(k*)] = h(k*) (since cutting would be optimal at h(k*)), leading to:\n   k* = βh(k*)\n\nStep 4: Solve for k*. The optimal cutting threshold k* is the unique solution to:\n   k* = βh(k*)\n\nThis solution exists and is unique because h(k) is strictly increasing and concave (guaranteeing a single crossing point with the k/β line). The optimal rule is: cut the tree when k ≥ k*, where k* satisfies k* = βh(k*).\n\nQID: textbook-104-7-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-104-7-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the optimal cutting rule and the threshold k* where the agent is indifferent between cutting and waiting. The reasoning aligns with the gold answer, including the use of the properties of h(k) (strictly increasing and concave) to ensure a unique solution.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-104-7-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the optimal cutting rule and the threshold k* where the agent is indifferent between cutting and waiting. The reasoning aligns with the gold answer, including the use of the properties of h(k) (strictly increasing and concave) to ensure a unique solution."}, "llm_echoed_qid": "textbook-104-7-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the optimal cutting rule and the threshold k* where the agent is indifferent between cutting and waiting. The reasoning aligns with the gold answer, including the use of the properties of h(k) (strictly increasing and concave) to ensure a unique solution."}
{"script_processing_qid": "textbook-118-18-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Compare the Hansen-White and Newey-West HAC estimators, highlighting their key differences and practical implications.\n\nGOLD_ANSWER:\n1.  **Hansen-White**: $\\hat{\\boldsymbol{\\Sigma}}_{\\mathrm{HW}} = \\hat{\\boldsymbol{{\\cal T}}}(0) + \\sum_{j=1}^{p}(\\hat{\\boldsymbol{{\\cal T}}}(j) + \\hat{\\boldsymbol{{\\cal T}}}^{\\top}(j))$. May not be positive definite.\n2.  **Newey-West**: $\\hat{\\cal{{X}}}_{\\mathrm{NW}} = \\hat{\\cal{{\\cal{T}}}}(0) + \\sum_{j=1}^{p}\\left(1-\\frac{j}{p+1}\\right)\\left(\\hat{\\cal{{I}}}(j) + \\hat{\\cal{{I}}}^{\\top}(j)\\right)$. Guaranteed positive definite due to weighting.\n3.  **Implications**: Newey-West requires larger $p$ but is more reliable in finite samples.\n\nCANDIDATE_ANSWER:\nStep 1: Core Focus - The Hansen-White estimator (White, 1980) primarily addresses heteroskedasticity of unknown form, while the Newey-West estimator (Newey & West, 1987) handles both heteroskedasticity and autocorrelation (HAC).\n\nStep 2: Structural Differences - Newey-West incorporates a lag truncation parameter to account for autocorrelation across time periods, whereas Hansen-White assumes no serial correlation (cross-sectional data or iid settings).\n\nStep 3: Bandwidth Requirement - Newey-West requires selection of a bandwidth/lag window (e.g., using Bartlett kernel), introducing implementation complexity absent in Hansen-White.\n\nStep 4: Practical Implications - (a) Use Hansen-White for cross-sectional data with heteroskedasticity. (b) Use Newey-West for time series or panel data with autocorrelation. (c) Newey-West may have larger finite-sample bias due to bandwidth choice tradeoffs (variance vs. bias).\n\nStep 5: Efficiency Context - Both enable feasible efficient GMM estimation via $\\hat{\\Sigma}$ in the FGMM formula, but misspecification of autocorrelation structure (using Hansen-White when autocorrelation exists) leads to inconsistent covariance estimates.\n\nQID: textbook-118-18-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-18-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately highlights the key differences between Hansen-White and Newey-West estimators, including their focus (heteroskedasticity vs. HAC), structural differences, bandwidth requirements, and practical implications, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-18-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately highlights the key differences between Hansen-White and Newey-West estimators, including their focus (heteroskedasticity vs. HAC), structural differences, bandwidth requirements, and practical implications, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-118-18-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately highlights the key differences between Hansen-White and Newey-West estimators, including their focus (heteroskedasticity vs. HAC), structural differences, bandwidth requirements, and practical implications, aligning well with the gold answer."}
{"script_processing_qid": "textbook-111-8-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Using phase diagrams, illustrate the transition from short-run (Keynesian) to long-run (classical) equilibrium in the Mundell-Fleming model.\n\nGOLD_ANSWER:\n1. **Short-Run (Point K)**: Fixed $P$, $Y < \\bar{Y}$.\n2. **Adjustment**: Falling $P$ shifts LM* right, depreciating $\\epsilon$, boosting $NX$.\n3. **Long-Run (Point C)**: $P$ adjusts until $Y = \\bar{Y}$.\n4. **Diagram**: Horizontal axis $Y$, vertical axis $P$; AD curve intersects LRAS at $\\bar{Y}$.\n\nCANDIDATE_ANSWER:\nStep 1: **Short-Run Keynesian Equilibrium**\nIn the short run (sticky prices), the economy operates at the intersection of the IS* and LM* curves. Assume prices are fixed (e.g., at P₀). A lower price level (P < P₀) increases real money balances (M/P), shifting LM* rightward. This causes a real exchange rate depreciation (lower ε) and higher output Y (as per the context).\n\nStep 2: **Price Adjustment Dynamics**\nOver time, price levels adjust. If Y exceeds the natural rate (Y*), inflationary pressure raises P. Conversely, if Y < Y*, deflationary pressure reduces P. This adjustment is captured by a vertical long-run Phillips curve relationship (not explicitly stated but implied by the transition to classical equilibrium).\n\nStep 3: **Long-Run Classical Equilibrium**\nIn the long run (flexible prices), P adjusts until Y returns to Y*. The LM* curve shifts leftward (as rising P reduces M/P) or rightward (as falling P increases M/P) until IS* and LM* intersect at Y = Y*. The real exchange rate adjusts to ensure equilibrium in the goods market.\n\nStep 4: **Phase Diagram Illustration**\n- Axes: Y (horizontal) vs. P (vertical).\n- Plot the vertical long-run equilibrium line at Y = Y*.\n- Draw adjustment paths: For points where Y > Y*, arrows point upward (P rises); for Y < Y*, arrows point downward (P falls).\n- Short-run LM* shifts (due to P changes) guide the economy along IS* toward Y = Y*.\n\nQID: textbook-111-8-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-8-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes the transition from short-run to long-run equilibrium in the Mundell-Fleming model, including the role of price adjustments and the shifts in the LM* curve. The explanation aligns with the gold answer and provides additional detail on the adjustment dynamics and phase diagram illustration.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-8-1-1", "category": "CORRECT", "explanation": "The candidate's answer accurately describes the transition from short-run to long-run equilibrium in the Mundell-Fleming model, including the role of price adjustments and the shifts in the LM* curve. The explanation aligns with the gold answer and provides additional detail on the adjustment dynamics and phase diagram illustration."}, "llm_echoed_qid": "textbook-111-8-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes the transition from short-run to long-run equilibrium in the Mundell-Fleming model, including the role of price adjustments and the shifts in the LM* curve. The explanation aligns with the gold answer and provides additional detail on the adjustment dynamics and phase diagram illustration."}
{"script_processing_qid": "textbook-122-7-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain why the LPM necessarily exhibits heteroskedasticity when the dependent variable is binary. Derive the conditional variance $\\mathrm{Var}(y|\\pmb{x})$.\n\nGOLD_ANSWER:\n1. **Heteroskedasticity**: For binary $y$, $\\mathrm{Var}(y|\\pmb{x}) = \\mathrm{P}(y=1|\\pmb{x})[1 - \\mathrm{P}(y=1|\\pmb{x})] = p(\\pmb{x})(1 - p(\\pmb{x}))$.\n2. **Dependence on $\\pmb{x}$**: Since $p(\\pmb{x})$ varies with $\\pmb{x}$, the variance is not constant, leading to heteroskedasticity.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that in a binary dependent variable model, y takes values 0 or 1. The error term in LPM can be expressed as u = y - E(y|x) = y - p(x), where p(x) = β₀ + β₁x₁ + ... + βₖxₖ.\n\nStep 2: Compute conditional variance Var(y|x):\n  Var(y|x) = E[(y - E(y|x))²|x] = E[u²|x]\n\nStep 3: Since y is binary (Bernoulli):\n  Var(y|x) = p(x)[1 - p(x)]\n  This follows because for Bernoulli variables: Var(y) = p(1-p)\n\nStep 4: Substitute LPM's specification p(x) = β₀ + β₁x₁ + ... + βₖxₖ:\n  Var(y|x) = [β₀ + β₁x₁ + ... + βₖxₖ][1 - (β₀ + β₁x₁ + ... + βₖxₖ)]\n\nStep 5: Observe that this variance depends on x through p(x). Since p(x) varies with x in the LPM (unless all coefficients are zero), the variance necessarily changes across observations → heteroskedasticity is inherent to LPM with binary y.\n\nQID: textbook-122-7-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-7-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the conditional variance for a binary dependent variable in the LPM and explains why heteroskedasticity is inherent, matching the gold answer's reasoning and conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-7-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the conditional variance for a binary dependent variable in the LPM and explains why heteroskedasticity is inherent, matching the gold answer's reasoning and conclusion."}, "llm_echoed_qid": "textbook-122-7-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the conditional variance for a binary dependent variable in the LPM and explains why heteroskedasticity is inherent, matching the gold answer's reasoning and conclusion."}
{"script_processing_qid": "textbook-42-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the limitations of the differentiation approach in comparative statics for problems with inequality constraints, as highlighted in the text.\n\nGOLD_ANSWER:\n1. **Inequality Handling**: Differentiation requires knowing which constraints bind, as non-binding constraints cannot be differentiated.\n2. **Regime Switches**: Changes in binding constraints over parameter ranges complicate differentiation.\n3. **Non-Differentiability**: Functions may lack derivatives, rendering the method inapplicable.\n4. **Complementarity**: The basic optimization approach (using concavity/convexity) is more general but less precise for specific functional forms.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key methodological assumptions\n- The differentiation approach for comparative statics relies on smooth value functions (V(b)) and stable constraint activity (i.e., binding/non-binding status of constraints remains unchanged with parameter variations).\n\nStep 2: Highlight limitations from the context\n1. **Non-differentiable points**: When parameters (b) change, inequality constraints may switch between binding/non-binding states (complementary slackness conditions). This creates kinks in V(b), violating differentiability requirements.\n\n2. **Sufficient but not necessary conditions**: The results depend on joint concavity of F and convexity of G. If these conditions are not met, the differentiation approach cannot confirm concavity/convexity of V(b), limiting its applicability.\n\n3. **Duality limitations**: While duality theory helps establish relationships between primal/dual problems, the text specifies that complementary slackness conditions (e.g., π̄ ≥ 0, Bx̄ ≤ c) create discrete switches in solution regimes that are not smoothly captured by differentiation methods.\n\nStep 3: Implicit structural limitation\n4. **Linear programming challenges**: In LP problems (where F and G are linear), the value function V(b) becomes piecewise linear with multiple non-differentiable points. This makes traditional differentiation approaches ineffective for global comparative statics analysis.\n\nQID: textbook-42-1-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-42-1-0-3\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies some limitations of the differentiation approach, such as non-differentiable points and challenges in linear programming. However, it misses the specific points about inequality handling and regime switches mentioned in the gold answer, and introduces some concepts (e.g., duality limitations) that are not directly relevant to the core limitations highlighted in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-42-1-0-3", "category": "PARTIALLY_CORRECT", "explanation": "The candidate answer correctly identifies some limitations of the differentiation approach, such as non-differentiable points and challenges in linear programming. However, it misses the specific points about inequality handling and regime switches mentioned in the gold answer, and introduces some concepts (e.g., duality limitations) that are not directly relevant to the core limitations highlighted in the gold answer."}, "llm_echoed_qid": "textbook-42-1-0-3", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies some limitations of the differentiation approach, such as non-differentiable points and challenges in linear programming. However, it misses the specific points about inequality handling and regime switches mentioned in the gold answer, and introduces some concepts (e.g., duality limitations) that are not directly relevant to the core limitations highlighted in the gold answer."}
{"script_processing_qid": "textbook-114-13-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of a firm operating at an output level where marginal cost is sharply increasing. How might this affect future production decisions?\n\nGOLD_ANSWER:\nIf a firm operates where MC is sharply increasing, it faces higher costs for additional output. If demand is expected to rise, the firm may need to expand capacity (e.g., hire more labor or capital) to avoid escalating costs. This highlights the importance of long-run planning to mitigate short-run cost inefficiencies.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that sharply increasing marginal cost (MC) implies rapidly diminishing returns to labor. From the context, MC = w/MP_L. If MC rises sharply, MP_L must be decreasing at an accelerating rate, indicating each additional labor unit contributes progressively less to output.\n\nStep 2: Higher MC means producing additional units becomes increasingly costly. This creates a disincentive to expand output further in the short run, as profitability declines for incremental units.\n\nStep 3: Future production decisions may involve:\n(a) Capping output at current levels to avoid escalating costs;\n(b) Reallocating/optimizing labor (e.g., training, task specialization) to mitigate diminishing returns;\n(c) Considering long-run adjustments (e.g., capital investment, technology upgrades) to improve MP_L and flatten the MC curve;\n(d) Adjusting pricing strategies to reflect higher marginal costs of production.\n\nStep 4: The firm may also re-evaluate its variable input mix. Since AVC = w/AP_L, if AP_L declines (as MP_L < AP_L during diminishing returns), rising AVC would further pressure profit margins, reinforcing the need for cautious output expansion.\n\nQID: textbook-114-13-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-13-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses the implications of sharply increasing marginal cost, including short-run disincentives and long-run adjustments, aligning well with the gold answer's focus on cost management and capacity planning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-13-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses the implications of sharply increasing marginal cost, including short-run disincentives and long-run adjustments, aligning well with the gold answer's focus on cost management and capacity planning."}, "llm_echoed_qid": "textbook-114-13-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses the implications of sharply increasing marginal cost, including short-run disincentives and long-run adjustments, aligning well with the gold answer's focus on cost management and capacity planning."}
{"script_processing_qid": "textbook-121-7-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the role of the bordered Hessian in determining the definiteness of $d^2 z$ in constrained optimization.\n\nGOLD_ANSWER:\nThe bordered Hessian is used to determine the definiteness of the quadratic form $d^2 z$ under the constraint $dg = 0$. It incorporates the constraint into the second-order condition by bordering the plain Hessian with the first derivatives of the constraint function. The sign of its principal minors indicates whether $d^2 z$ is positive or negative definite, which is crucial for identifying maxima or minima.\n\nCANDIDATE_ANSWER:\nStep 1: In constrained optimization, the bordered Hessian is used to analyze the second-order conditions for extrema. The quadratic form $d^2z$ (second differential of the objective function) must be tested for definiteness under the constraint $dg = 0$.\\n\\nStep 2: The bordered Hessian $\\overline{H}$ is constructed by bordering the Hessian matrix of second derivatives $Z_{ij} = f_{ij} - \\lambda g_{ij}$ (where $\\lambda$ is the Lagrange multiplier) with the gradients $g_i$ of the constraint function. The first row and column contain the gradients $g_1, g_2, ..., g_n$ and a zero in the (1,1) position.\\n\\nStep 3: To determine the definiteness of $d^2z$:\\n- For $d^2z$ to be **positive definite**, all successive bordered principal minors $|\\overline{H}_2|, |\\overline{H}_3|, ..., |\\overline{H}_n|$ must be **negative**.\\n- For $d^2z$ to be **negative definite**, the bordered principal minors must **alternate in sign** starting with $|\\overline{H}_2| > 0$, then $|\\overline{H}_3| < 0$, $|\\overline{H}_4| > 0$, etc.\\n\\nStep 4: These sign patterns ensure that the quadratic form $d^2z$ is restricted to the tangent space of the constraint, satisfying second-order sufficiency conditions for minima (positive definiteness) or maxima (negative definiteness).\n\nQID: textbook-121-7-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-121-7-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the role of the bordered Hessian in determining the definiteness of $d^2 z$ in constrained optimization, including its construction and the interpretation of its principal minors. This aligns perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-121-7-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains the role of the bordered Hessian in determining the definiteness of $d^2 z$ in constrained optimization, including its construction and the interpretation of its principal minors. This aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-121-7-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the role of the bordered Hessian in determining the definiteness of $d^2 z$ in constrained optimization, including its construction and the interpretation of its principal minors. This aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-113-7-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the conditions under which a monetary policy rule can achieve time consistency. Use a Barro-Gordon model framework.\n\nGOLD_ANSWER:\n1. **Barro-Gordon Model**:\n   - Policymaker’s loss function: $L = (\\pi - \\pi^*)^2 + \\beta(u - u^*)^2$.\n   - Short-run Phillips curve: $u = u_n - \\alpha(\\pi - \\pi^e)$.\n2. **Discretionary Outcome**: Minimizing $L$ yields $\\pi = \\pi^* + \\frac{\\alpha \\beta}{1 + \\alpha^2 \\beta}(u_n - u^*)$, leading to inflation bias.\n3. **Rule-Based Solution**: Commit to $\\pi = \\pi^*$; if credible, $\\pi^e = \\pi^*$, eliminating bias.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the Barro-Gordon model structure. The central bank minimizes a loss function L = (π - π*)^2 + λ(y - y*)^2, where π is actual inflation, π* is the target inflation rate, y is output, y* is desired output (higher than natural rate yₙ due to distortions), and λ is the central bank's output stabilization preference weight.\\n\\nStep 2: Employment/output is determined by a Lucas supply curve: y = yₙ + a(π - πᵉ), where πᵉ is expected inflation and 'a' is the output-inflation trade-off parameter.\\n\\nStep 3: Under discretion, the central bank minimizes L given πᵉ. Substituting the Lucas supply curve into L and taking ∂L/∂π = 0 yields the reaction function: π = π* + (λa²πᵉ + λa(y* - yₙ)) / (1 + λa²).\\n\\nStep 4: Under rational expectations, πᵉ = π in equilibrium. Solving gives discretionary equilibrium inflation: π_d = π* + (λa(y* - yₙ))/(1 + λa²). This shows an inflation bias (second term) even when y* > yₙ.\\n\\nStep 5: For a policy rule π = π* to be time-consistent, the central bank must have no incentive to deviate. This occurs iff the discretionary solution (π_d) equals the rule (π*). From Step 4, this requires λa(y* - yₙ) = 0.\\n\\nStep 6: Since λ > 0 and a > 0, the critical condition is y* = yₙ. If the central bank's output target equals the natural rate, the inflation bias disappears, making the rule π = π* both credible and time-consistent.\n\nQID: textbook-113-7-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-113-7-1-0\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the condition for time consistency (y* = yₙ) but uses a different model structure (Lucas supply curve and output stabilization) instead of the Barro-Gordon model's focus on unemployment and inflation bias. The reasoning deviates from the GOLD_ANSWER's framework.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-113-7-1-0", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the condition for time consistency (y* = yₙ) but uses a different model structure (Lucas supply curve and output stabilization) instead of the Barro-Gordon model's focus on unemployment and inflation bias. The reasoning deviates from the GOLD_ANSWER's framework."}, "llm_echoed_qid": "textbook-113-7-1-0", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the condition for time consistency (y* = yₙ) but uses a different model structure (Lucas supply curve and output stabilization) instead of the Barro-Gordon model's focus on unemployment and inflation bias. The reasoning deviates from the GOLD_ANSWER's framework."}
{"script_processing_qid": "textbook-122-1-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Compare the goodness-of-fit measures for models with $y$ and $\\log(y)$ as dependent variables. How can you construct a comparable measure for the log model?\n\nGOLD_ANSWER:\n1. **Level Model**: $R^2$ measures the proportion of variance in $y$ explained by the model.\n2. **Log Model**: Compute $\\hat{y}_i = \\hat{\\alpha}_0 \\exp(\\hat{\\log y_i})$ and find the squared correlation between $\\hat{y}_i$ and $y_i$.\n3. **Comparison**: This squared correlation is analogous to $R^2$ and can be compared directly with the $R^2$ from the level model.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that standard R² from log(y) models measures explained variance in log(y) units, which is not directly comparable to R² from linear y models (which uses y units).\nStep 2: To make them comparable, construct a measure for the log model using adjusted predictions in y units:\n   i) Compute predicted values using the log model: $\\hat{logy}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_{i1} + ... + \\hat{\\beta}_kx_{ik}$\n   ii) Calculate adjusted predictions: $\\tilde{y}_i = \\exp(\\hat{\\sigma}^2/2) \\cdot \\exp(\\hat{logy}_i)$ [or use $\\hat{\\alpha}_0$ from auxiliary regression as per context]\nStep 3: Compute squared correlation between actual y and adjusted predictions $\\tilde{y}$. This $corr(y,\\tilde{y})^2$ serves as a comparable goodness-of-fit measure to the linear model's R².\n\nQID: textbook-122-1-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-1-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the issue with comparing R² from log(y) and y models, and provides a detailed method to construct a comparable measure by adjusting predictions and computing squared correlation, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-1-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the issue with comparing R² from log(y) and y models, and provides a detailed method to construct a comparable measure by adjusting predictions and computing squared correlation, aligning with the gold answer."}, "llm_echoed_qid": "textbook-122-1-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the issue with comparing R² from log(y) and y models, and provides a detailed method to construct a comparable measure by adjusting predictions and computing squared correlation, aligning with the gold answer."}
{"script_processing_qid": "textbook-119-11-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the binomial and Poisson distributions, highlighting their key differences and similarities.\n\nGOLD_ANSWER:\n1.  **Binomial**: Fixed number of trials ($n$), probability of success ($p$), variance $np(1-p) < np = E(X)$.\n2.  **Poisson**: No fixed number of trials, rate parameter ($\\lambda$), variance $= \\lambda = E(X)$.\n3.  **Similarity**: Both model count data.\n4.  **Difference**: Poisson is for rare events with no upper limit, while binomial is for fixed trials with success/failure outcomes.\n\nCANDIDATE_ANSWER:\nStep 1: **Definitions and Similarities**\n- Both are discrete probability distributions modeling count data.\n- Both are used in applications involving counts (e.g., successes in trials for binomial, rare events for Poisson).\n\nStep 2: **Key Differences**\n1. **Parameters**:\n   - Binomial: Two parameters (\\(n\\), number of trials; \\(p\\), success probability).\n   - Poisson: Single parameter (\\(\\lambda\\), rate/mean of occurrences).\n2. **Variance Relationship**:\n   - Binomial: Variance = \\(np(1-p)\\), which is always less than the mean (\\(np\\)) unless \\(p=0\\).\n   - Poisson: Variance = Mean = \\(\\lambda\\).\n3. **Applications**:\n   - Binomial: Models fixed \\(n\\) independent trials with binary outcomes (e.g., coin tosses).\n   - Poisson: Models rare/uncommon events over continuous intervals (e.g., phone calls in 5 minutes).\n\nStep 3: **Functional Forms**\n- Binomial PMF: \\(f(X) = \binom{n}{x} p^x (1-p)^{n-x}\\), where \\(x \\leq n\\).\n- Poisson PMF: \\(f(X) = \\frac{e^{-\\lambda} \\lambda^x}{x!}\\), defined for all non-negative integers \\(x\\).\n\nQID: textbook-119-11-2-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-11-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately captures the key differences and similarities between the binomial and Poisson distributions, including their parameters, variance relationships, applications, and functional forms, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-11-2-3", "category": "CORRECT", "explanation": "The candidate answer accurately captures the key differences and similarities between the binomial and Poisson distributions, including their parameters, variance relationships, applications, and functional forms, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-119-11-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately captures the key differences and similarities between the binomial and Poisson distributions, including their parameters, variance relationships, applications, and functional forms, aligning well with the gold answer."}
{"script_processing_qid": "textbook-42-2-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the challenges and potential solutions when dealing with infinite horizon optimization problems where utility integrals may diverge.\n\nGOLD_ANSWER:\n1. **Divergence Issue**: Infinite utility integrals make direct comparison of plans impossible.\n2. **Overtaking Criterion**: Compare finite-horizon integrals and take limits: $\\lim\\_{T \\rightarrow \\infty} \\left( \\int\\_{0}^{T} F(y(t), z(t), t)dt - \\int\\_{0}^{T} F(y'(t), z'(t), t)dt \\right) \\geq 0$.\n3. **Limitations**: The limit may not exist or may oscillate, rendering the criterion incomplete.\n4. **Alternative Approaches**: Use discounting to ensure convergence, or employ more sophisticated criteria like catching-up or long-run average utility.\n\nCANDIDATE_ANSWER:\nThe provided context does not explicitly address infinite horizon optimization problems or divergent utility integrals. While the text discusses Dynamic Programming methods, transversality conditions, and shadow price interpretations, all references to transversality conditions (e.g., $\\overline{T}$) and the fundamental equation of Dynamic Programming appear to assume a finite time horizon. No information is given about challenges related to infinite horizons (e.g., lack of terminal conditions, non-convergence of integrals) or potential solutions (e.g., overtaking criteria, discounted utility transformations). Therefore, the context lacks sufficient detail to answer this specific question.\n\nQID: textbook-42-2-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-42-2-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer states the provided context does not contain sufficient information.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-42-2-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer states the provided context does not contain sufficient information."}, "llm_echoed_qid": "textbook-42-2-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate answer states the provided context does not contain sufficient information."}
{"script_processing_qid": "textbook-115-3-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the score function for the probit model and show that it satisfies the zero conditional mean property.\n\nGOLD_ANSWER:\n1. **Probit Log-Likelihood**: $\\ell_{i}(\\pmb{\\theta}) = y_{i} \\log \\Phi(\\mathbf{x}_{i}\\pmb{\\theta}) + (1 - y_{i}) \\log[1 - \\Phi(\\mathbf{x}_{i}\\pmb{\\theta})]$.\n2. **Score Function**: $\\mathbf{s}_{i}(\\pmb{\\theta}) = \\frac{\\phi(\\mathbf{x}_{i}\\pmb{\\theta})\\mathbf{x}_{i}^{\\prime}[y_{i} - \\Phi(\\mathbf{x}_{i}\\pmb{\\theta})]}{\\Phi(\\mathbf{x}_{i}\\pmb{\\theta})[1 - \\Phi(\\mathbf{x}_{i}\\pmb{\\theta})]}$.\n3. **Zero Conditional Mean**: $\\operatorname{E}[\\mathbf{s}_{i}(\\pmb{\\theta}_{0})|\\mathbf{x}_{i}] = \\mathbf{0}$ because $\\operatorname{E}[y_{i} - \\Phi(\\mathbf{x}_{i}\\pmb{\\theta}_{0})|\\mathbf{x}_{i}] = 0$ by definition of $\\pmb{\\theta}_{0}$.\n\nCANDIDATE_ANSWER:\nStep 1: Define the probit model likelihood\\nFor binary response y_i ∈ {0,1}, the probit model assumes:\\nP(y_i=1|x_i) = Φ(x_i'β)\\nwhere Φ is the standard normal CDF. The log-likelihood contribution for observation i is:\\nl_i(β) = y_i logΦ(x_i'β) + (1-y_i)log[1-Φ(x_i'β)]\\n\\nStep 2: Compute the score function\\nThe score is the derivative of the log-likelihood w.r.t β:\\ns_i(β) = ∂l_i/∂β = [y_i φ(x_i'β)/Φ(x_i'β) - (1-y_i)φ(x_i'β)/(1-Φ(x_i'β))]x_i\\n\\nFactor out φ(x_i'β)x_i:\\n= φ(x_i'β)x_i [y_i/Φ(x_i'β) - (1-y_i)/(1-Φ(x_i'β))]\\n\\nCombine terms using Bernoulli structure:\\n= [y_i(1-Φ(x_i'β)) - (1-y_i)Φ(x_i'β)]/[Φ(x_i'β)(1-Φ(x_i'β))] φ(x_i'β)x_i\\n\\nSimplify numerator:\\n= [y_i - Φ(x_i'β)]/[Φ(x_i'β)(1-Φ(x_i'β))] φ(x_i'β)x_i\\n\\nStep 3: Show zero conditional mean\\nTake E[s_i(β_0)|x_i] where β_0 is true parameter:\\nE[y_i - Φ(x_i'β_0)|x_i] = Φ(x_i'β_0) - Φ(x_i'β_0) = 0\\n\\nSince φ(·) and Φ(·) are deterministic given x_i and β_0:\\nE[s_i(β_0)|x_i] = 0 · [φ(x_i'β_0)x_i]/[Φ(x_i'β_0)(1-Φ(x_i'β_0))] = 0\\n\\nThis demonstrates the score function satisfies E[s_i(β_0)|x_i] = 0 when the model is correctly specified.\n\nQID: textbook-115-3-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-3-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the score function for the probit model and demonstrates that it satisfies the zero conditional mean property. The steps and reasoning align with the gold answer, including the simplification of the score function and the proof of the zero conditional mean.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-3-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly derives the score function for the probit model and demonstrates that it satisfies the zero conditional mean property. The steps and reasoning align with the gold answer, including the simplification of the score function and the proof of the zero conditional mean."}, "llm_echoed_qid": "textbook-115-3-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the score function for the probit model and demonstrates that it satisfies the zero conditional mean property. The steps and reasoning align with the gold answer, including the simplification of the score function and the proof of the zero conditional mean."}
{"script_processing_qid": "textbook-123-12-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What are the key assumptions underlying the Gaussian asset pricing model described in the text?\n\nGOLD_ANSWER:\nThe Gaussian asset pricing model assumes:\n1. **State Evolution**: The state $s_{t}$ follows a linear stochastic difference equation driven by Gaussian disturbances: $$s_{t+1}=A s_{t}+C w_{t+1},$$ where $w_{t+1}$ is a Gaussian martingale difference sequence.\n2. **Preferences**: Quadratic utility functions, leading to linear marginal utilities.\n3. **Dividend Process**: The dividend process $d_{t}$ is linear in the state $s_{t}$.\n4. **Equilibrium Allocation**: The equilibrium allocation to each agent is linear in the state: $c_{t}^{i}=S_{c i}s_{t}$.\n\nCANDIDATE_ANSWER:\nStep 1: The Gaussian asset pricing model framework assumes a representative agent with time-separable utility function $u(\\cdot)$, consistent with standard consumption-based asset pricing models.\n\nStep 2: It requires rational expectations - agents form expectations $E_{t-1}[\\cdot]$ conditional on time $t-1$ information, as shown in equations (8.11.6) and subsequent derivations.\n\nStep 3: The model assumes the existence of a stochastic discount factor $m_t \\equiv \\beta u'(c_t)/u'(c_{t-1})$, which is fundamental to asset pricing relationships in the derivation.\n\nStep 4: Consumption growth is treated as conditionally log-normal (Gaussian), enabling analytical solutions through properties of log-normal distributions. This is implied by the covariance structure in the final rearrangement $1+\\Omega_{t-1}'(0) = \\frac{E_{t-1}m_t E_{t-1}c_t}{E_{t-1}m_t c_t}$ which typically requires log-normality for closed-form solutions.\n\nStep 5: The endowment process serves as the fundamental risk exposure, with compensation for consumption risk measured through marginal adjustments ($\\Omega_{t-1}$) to endowment consumption paths.\n\nQID: textbook-123-12-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-12-0-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer does not align with the key assumptions provided in the gold answer. The candidate discusses a representative agent, rational expectations, stochastic discount factor, and log-normal consumption growth, which are not mentioned in the gold answer. The gold answer focuses on state evolution, preferences, dividend process, and equilibrium allocation, none of which are addressed by the candidate.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-12-0-2", "category": "INCORRECT", "explanation": "The candidate's answer does not align with the key assumptions provided in the gold answer. The candidate discusses a representative agent, rational expectations, stochastic discount factor, and log-normal consumption growth, which are not mentioned in the gold answer. The gold answer focuses on state evolution, preferences, dividend process, and equilibrium allocation, none of which are addressed by the candidate."}, "llm_echoed_qid": "textbook-123-12-0-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer does not align with the key assumptions provided in the gold answer. The candidate discusses a representative agent, rational expectations, stochastic discount factor, and log-normal consumption growth, which are not mentioned in the gold answer. The gold answer focuses on state evolution, preferences, dividend process, and equilibrium allocation, none of which are addressed by the candidate."}
{"script_processing_qid": "textbook-119-14-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of using different coding schemes for dummy variables (e.g., $D_i = \\{0,1\\}$ vs. $Z_i = a + b D_i$) on the interpretation of regression coefficients.\n\nGOLD_ANSWER:\n1. **Standard Coding ($D_i = \\{0,1\\}$)**:\n   - $\\beta_2$ represents the difference in the mean of $Y$ between the two groups.\n2. **Transformed Coding ($Z_i = a + b D_i$)**:\n   - The interpretation of coefficients depends on the transformation. For example, if $Z_i = 2 + 3 D_i$, the coefficient on $Z_i$ represents the change in $Y$ for a one-unit change in $Z_i$, which is not directly comparable to the standard coding.\n   - The intercept and slope coefficients will adjust to reflect the new coding, potentially complicating interpretation.\n\nCANDIDATE_ANSWER:\nStep 1: Impact on Linear Models\nIn the original dummy coding $D_i = \\{0,1\\}$:\n- Intercept ($\\beta_1$) = mean of $Y$ for $D_i=0$ group\n- Coefficient ($\\beta_2$) = difference in means between $D_i=1$ and $D_i=0$\n\nFor transformed coding $Z_i = a + bD_i$:\n- New intercept = $\\beta_1 - a(\\beta_2/b)$\n- New coefficient = $\\beta_2/b$\nThe economic interpretation (group difference) remains $\\beta_2$, but its mathematical representation changes.\n\nStep 2: Impact on Semilog Models\nOriginal model $\\ln Y_i = \\beta_1 + \\beta_2D_i$:\n- Semielasticity = $(e^{\\beta_2} - 1) \\times 100$\n\nFor transformed coding $Z_i = a + bD_i$:\n- Coefficient becomes $\\beta_2' = \\beta_2/b$\n- Semielasticity = $(e^{\\beta_2' \\cdot b} - 1) \\times 100$\nThe final percentage interpretation remains identical, but intermediate coefficients scale inversely with the dummy variable's numerical spread.\n\nStep 3: Key Implications\n1. Intercept interpretation depends on coding scheme\n2. Coefficient magnitudes scale with dummy variable transformations\n3. Economic substance (group differences/percentage effects) remains invariant to linear coding changes\n4. Nonlinear transformations (like exponentiation) require careful tracking of scaling factors\n\nQID: textbook-119-14-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-14-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the implications of different coding schemes on the interpretation of regression coefficients, covering both linear and semilog models. The explanation aligns with the gold answer, detailing how intercepts and coefficients change with transformations while maintaining the economic interpretation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-14-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the implications of different coding schemes on the interpretation of regression coefficients, covering both linear and semilog models. The explanation aligns with the gold answer, detailing how intercepts and coefficients change with transformations while maintaining the economic interpretation."}, "llm_echoed_qid": "textbook-119-14-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the implications of different coding schemes on the interpretation of regression coefficients, covering both linear and semilog models. The explanation aligns with the gold answer, detailing how intercepts and coefficients change with transformations while maintaining the economic interpretation."}
{"script_processing_qid": "textbook-118-5-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Derive the expression for the covariance matrix $\\pmb{\\varOmega}$ of the zero functions and explain how it accounts for both data randomness and simulation randomness.\n\nGOLD_ANSWER:\n1.  **Covariance matrix**: The covariance matrix $\\pmb{\\varOmega}$ of the zero functions is given by $$\\Omega=\\mathrm{Var}\\big(h(y)\\big)+\\frac{1}{S}\\mathrm{Var}\\big(m^{\\*}(\\pmb{\\theta}_{0})\\big),$$ where $\\pmb{h}(\\pmb{y})$ and $m^{\\*}(\\pmb{\\theta})$ are the vectors of zero functions and their simulated counterparts, respectively.\\n2.  **Data randomness**: The term $\\mathrm{Var}\\big(h(y)\\big)$ accounts for the randomness in the observed data.\\n3.  **Simulation randomness**: The term $\\frac{1}{S}\\mathrm{Var}\\big(m^{\\*}(\\pmb{\\theta}_{0})\\big)$ accounts for the additional randomness introduced by the simulation process.\\n4.  **Combined effect**: The matrix $\\pmb{\\varOmega}$ thus captures both sources of variability, ensuring that the asymptotic covariance matrix of the MSM estimator is correctly estimated.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that the covariance matrix $\\pmb{\\varOmega}$ represents the covariance of the simulated moment conditions $\\pmb{f}^*(\\pmb{\\theta}_0)$.\\nStep 2: Decompose the randomness in $\\pmb{f}^*(\\pmb{\\theta}_0)$ into two components:\\n  - Data randomness: Natural sampling variability in observed moments\\n  - Simulation randomness: Additional variability from using finite simulations ($S$) to approximate expectations\\nStep 3: Formally express the covariance matrix as:\\n  $\\pmb{\\varOmega} = \\text{Var}(\\pmb{f}^*(\\pmb{\\theta}_0)) = \\text{Var}_{\\text{data}} + \\frac{1}{S}\\text{Var}_{\\text{sim}}$\\n  Where:\\n  - $\\text{Var}_{\\text{data}}$ = Covariance from actual data observations\\n  - $\\frac{1}{S}\\text{Var}_{\\text{sim}}$ = Additional covariance from simulation error (diminishes as $S$ increases)\\nStep 4: The $1/S$ factor shows that simulation randomness becomes negligible with large $S$, leaving only data randomness as $S\\to\\infty$.\\nStep 5: This structure directly enters the asymptotic covariance matrix through matrix $\\pmb{B}$ in the sandwich formula, where $\\pmb{\\varOmega}$ appears in the middle term to account for both sources of variability.\n\nQID: textbook-118-5-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-5-0-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the components of the covariance matrix and their roles in accounting for data and simulation randomness. However, the reasoning slightly deviates from the gold answer by focusing on simulated moment conditions $\\pmb{f}^*(\\pmb{\\theta}_0)$ instead of zero functions $\\pmb{h}(\\pmb{y})$ and $m^*(\\pmb{\\theta}_0)$. The structure and interpretation of the covariance matrix are correct, but the terminology and focus differ.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-5-0-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the components of the covariance matrix and their roles in accounting for data and simulation randomness. However, the reasoning slightly deviates from the gold answer by focusing on simulated moment conditions $\\pmb{f}^*(\\pmb{\theta}_0)$ instead of zero functions $\\pmb{h}(\\pmb{y})$ and $m^*(\\pmb{\theta}_0)$. The structure and interpretation of the covariance matrix are correct, but the terminology and focus differ."}, "llm_echoed_qid": "textbook-118-5-0-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the components of the covariance matrix and their roles in accounting for data and simulation randomness. However, the reasoning slightly deviates from the gold answer by focusing on simulated moment conditions $\\pmb{f}^*(\\pmb{\theta}_0)$ instead of zero functions $\\pmb{h}(\\pmb{y})$ and $m^*(\\pmb{\theta}_0)$. The structure and interpretation of the covariance matrix are correct, but the terminology and focus differ."}
{"script_processing_qid": "textbook-91-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Compare the Cournot equilibrium with the Bertrand equilibrium under the same demand and cost conditions. What are the key differences?\n\nGOLD_ANSWER:\n1. **Cournot**: Firms compete on quantities, leading to positive profits if $c_{1} \\neq c_{2}$.\n2. **Bertrand**: Firms compete on prices, leading to zero profits if $c_{1} = c_{2}$ (price equals marginal cost).\n3. **Aggressiveness**: Bertrand competition is more aggressive, often resulting in lower prices and profits compared to Cournot.\n\nCANDIDATE_ANSWER:\nThe provided context focuses exclusively on the Cournot duopoly model and does not include any information about the Bertrand equilibrium. Therefore, the context does not contain sufficient information to compare Cournot and Bertrand equilibria under the same demand and cost conditions. Key differences typically discussed in economic theory (outside the provided context) include:\n\nStep 1: **Strategic Variable** - Cournot uses quantities as the strategic variable, while Bertrand uses prices.\nStep 2: **Equilibrium Prices/Quantities** - Cournot generally results in higher prices and lower quantities than Bertrand under identical conditions.\nStep 3: **Profit Levels** - Firms earn positive profits in Cournot equilibrium but may compete prices down to marginal cost in Bertrand (leading to zero profits under homogeneous goods).\nStep 4: **Strategic Effects** - In Cournot, cost increases lead rivals to expand production (as shown in ∂q₂*/∂c₁ > 0), while in Bertrand, cost changes might trigger price undercutting.\n\nHowever, these points cannot be confirmed or elaborated using the provided context alone.\n\nQID: textbook-91-0-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-91-0-0-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information to compare Cournot and Bertrand equilibria.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-91-0-0-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information to compare Cournot and Bertrand equilibria."}, "llm_echoed_qid": "textbook-91-0-0-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information to compare Cournot and Bertrand equilibria."}
{"script_processing_qid": "textbook-123-19-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Prove Proposition 21.5: Unemployment benefits must fall over time to induce high search effort.\n\nGOLD_ANSWER:\n1. **Binding Constraint**: High effort binds $\\nu > 0$. \n2. **Inequalities**: $\\frac{1-\\pi_{ue}(a_L)}{1-\\pi_{ue}(a_H)} > 1 > \\frac{\\pi_{ue}(a_L)}{\\pi_{ue}(a_H)}$ implies $C_v(w(e),e) > \\Psi_v(v,u,a_H) > C_v(w(u),u)$. \n3. **Result**: Concavity of $u$ ensures $c_u(t) > c_u(t+1)$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the incentive structure\nIn Zhao's model, unemployment benefits must balance insurance (consumption smoothing) with incentives to exert job-search effort. Constant benefits would eliminate the incentive to increase search effort over time.\n\nStep 2: Recognize the moral hazard problem\nIf benefits remain constant, workers could reduce search effort in later periods without penalty. The planner needs to make prolonged unemployment increasingly costly.\n\nStep 3: Apply recursive optimization\nThe planner's dynamic program links current benefits to future compensation paths. Lower future benefits reduce the value of continued unemployment, forcing workers to compare:\n- Immediate effort (higher job-finding probability)\nvs\n- Reduced lifetime utility from lower future benefits\n\nStep 4: Implement decreasing benefits\nBy making benefits {b_t} decrease with unemployment duration t:\n∂V_u(t)/∂t < 0\nwhere V_u(t) is the value function for unemployment at duration t. This creates time-dependent incentive compatibility constraints that require:\nu(b_t) + β[π(e_t)V_e + (1-π(e_t))V_u(t+1)] ≥ u(b_t) + βV_u(t+1) + c(e_t)\nwhere π(e_t) is the job-finding probability increasing in effort e_t.\n\nStep 5: Ensure binding constraints\nFor high effort to be sustained, the marginal utility loss from benefit reduction must exactly offset the marginal disutility of effort at each point in time, requiring a strictly decreasing sequence {b_t}.\n\nQID: textbook-123-19-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-19-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the necessity of decreasing unemployment benefits to maintain high search effort, aligning with the gold answer's key points about binding constraints and dynamic optimization.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-19-1-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the necessity of decreasing unemployment benefits to maintain high search effort, aligning with the gold answer's key points about binding constraints and dynamic optimization."}, "llm_echoed_qid": "textbook-123-19-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the necessity of decreasing unemployment benefits to maintain high search effort, aligning with the gold answer's key points about binding constraints and dynamic optimization."}
{"script_processing_qid": "textbook-107-14-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain why convex preferences imply that the set of bundles worse than any given bundle $x$ is a convex set.\n\nGOLD_ANSWER:\n1.  **Convex Preferences**: Averages are weakly preferred to extremes.\n2.  **Set Definition**: The set of bundles worse than $x$ includes all $y$ such that $y \\preceq x$.\n3.  **Convex Combination**: For any $y_1, y_2 \\preceq x$, the bundle $\\lambda y_1 + (1-\\lambda)y_2$ must also satisfy $\\lambda y_1 + (1-\\lambda)y_2 \\preceq x$ due to convexity.\n4.  **Conclusion**: The set is convex.\n\nCANDIDATE_ANSWER:\nStep 1: Convex preferences mean that any convex combination of two bundles is at least as preferred as the less preferred of the two (averages ≥ extremes).\nStep 2: Take two bundles y and z that are both worse than bundle x (i.e., x ≻ y and x ≻ z).\nStep 3: A convex combination bundle w = λy + (1-λ)z (for 0 ≤ λ ≤ 1) must satisfy w ≾ x. This follows because:\n   - For convex preferences, if both y and z are inferior to x, their weighted average cannot become superior to x (otherwise it would violate the 'averages ≥ extremes' property).\nStep 4: Since all convex combinations of y and z remain worse than x, the set {all bundles ≾ x} is closed under convex combinations.\nStep 5: Thus, the set of bundles worse than x is convex by definition of convexity (closed under convex combinations).\n\nQID: textbook-107-14-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-107-14-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains why convex preferences imply that the set of bundles worse than any given bundle x is a convex set. It accurately describes the properties of convex preferences, the definition of the set of bundles worse than x, and how convex combinations of such bundles remain within the set, thus proving its convexity.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-14-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly explains why convex preferences imply that the set of bundles worse than any given bundle x is a convex set. It accurately describes the properties of convex preferences, the definition of the set of bundles worse than x, and how convex combinations of such bundles remain within the set, thus proving its convexity."}, "llm_echoed_qid": "textbook-107-14-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains why convex preferences imply that the set of bundles worse than any given bundle x is a convex set. It accurately describes the properties of convex preferences, the definition of the set of bundles worse than x, and how convex combinations of such bundles remain within the set, thus proving its convexity."}
{"script_processing_qid": "textbook-122-19-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that the sample average $\\bar{Y}$ is an unbiased estimator of the population mean $\\mu$.\n\nGOLD_ANSWER:\n1. **Definition**: $\\bar{Y} = \\frac{1}{n}\\sum_{i=1}^n Y_i$.\n2. **Expected Value**: $E(\\bar{Y}) = E\\left(\\frac{1}{n}\\sum_{i=1}^n Y_i\\right) = \\frac{1}{n}\\sum_{i=1}^n E(Y_i)$.\n3. **Population Mean**: Since $E(Y_i) = \\mu$ for all $i$, $E(\\bar{Y}) = \\frac{1}{n}(n\\mu) = \\mu$.\n4. **Conclusion**: $\\bar{Y}$ is unbiased for $\\mu$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the definition of an unbiased estimator. An estimator $\\bar{Y}$ is unbiased for $\\mu$ if $E(\\bar{Y}) = \\mu$.\n\nStep 2: Express the sample average $\\bar{Y}$ as $\\bar{Y} = \\frac{1}{n} \\sum_{i=1}^n Y_i$.\n\nStep 3: Apply the expectation operator to both sides:\n$E(\\bar{Y}) = E\\left(\\frac{1}{n} \\sum_{i=1}^n Y_i\\right)$.\n\nStep 4: Use the linearity of expectation to interchange expectation and summation:\n$E(\\bar{Y}) = \\frac{1}{n} \\sum_{i=1}^n E(Y_i)$.\n\nStep 5: Since each $Y_i$ is drawn from the population with mean $\\mu$, $E(Y_i) = \\mu$ for all $i$:\n$E(\\bar{Y}) = \\frac{1}{n} \\sum_{i=1}^n \\mu = \\frac{1}{n} (n\\mu)$.\n\nStep 6: Simplify the expression:\n$E(\\bar{Y}) = \\mu$.\n\nConclusion: The sample average $\\bar{Y}$ is therefore an unbiased estimator of the population mean $\\mu$.\n\nQID: textbook-122-19-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-19-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer follows the same logical steps as the gold answer, clearly demonstrating that the sample average is an unbiased estimator of the population mean.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-19-0-0", "category": "CORRECT", "explanation": "The candidate's answer follows the same logical steps as the gold answer, clearly demonstrating that the sample average is an unbiased estimator of the population mean."}, "llm_echoed_qid": "textbook-122-19-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer follows the same logical steps as the gold answer, clearly demonstrating that the sample average is an unbiased estimator of the population mean."}
{"script_processing_qid": "textbook-56-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. How would you modify the neoclassical production function to include human capital (H)? Derive the new steady-state conditions.\n\nGOLD_ANSWER:\n1. **Modified Production Function**: Y = F(K, H, L) = Lf(k, h), where h = H/L.\n2. **Steady-State**: Solve for k* and h* such that sf(k*, h*) = (d + n)k* and investment in human capital equals its depreciation.\n\nCANDIDATE_ANSWER:\nStep 1: Extend the standard neoclassical production function to include human capital (H). A common specification is a Cobb-Douglas form:\nY = A⋅K^α⋅H^β⋅L^(1−α−β)\nwhere α and β represent output elasticities of physical capital (K) and human capital (H), respectively, with constant returns to scale (α + β + (1−α−β) = 1).\n\nStep 2: Assume human capital accumulates similarly to physical capital. Let s_H be the fraction of output invested in human capital (parallel to s_K for physical capital), and δ be the depreciation rate (common for both capital types for simplicity).\n\nStep 3: Derive the steady-state conditions. In steady state:\n• Investment in K = Depreciation: s_K⋅Y = δK\n• Investment in H = Depreciation: s_H⋅Y = δH\n\nStep 4: Substitute Y from the production function into both conditions:\nFor physical capital: s_K⋅A⋅K^α⋅H^β⋅L^(1−α−β) = δK\nFor human capital: s_H⋅A⋅K^α⋅H^β⋅L^(1−α−β) = δH\n\nStep 5: Solve the ratio of the two equations to eliminate Y:\n(s_K/s_H) = (K/H)\n⇒ K = (s_K/s_H)⋅H\n\nStep 6: Substitute K = (s_K/s_H)⋅H into one of the steady-state equations. Using the human capital equation:\ns_H⋅A⋅[(s_K/s_H)⋅H]^α⋅H^β⋅L^(1−α−β) = δH\n\nStep 7: Simplify and solve for H in steady state:\nH^* = [(A⋅s_K^α⋅s_H^(1−α)⋅L^(1−α−β))/δ]^(1/(1−α−β))\n\nStep 8: Derive K^* using the K/H ratio from Step 5:\nK^* = (s_K/s_H)⋅H^*\n\nStep 9: Substitute H^* and K^* into the production function to get steady-state output:\nY^* = A⋅(s_K/δ)^(α/(1−α−β))⋅(s_H/δ)^(β/(1−α−β))⋅L\n\nQID: textbook-56-0-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-56-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly extends the neoclassical production function to include human capital and correctly derives the steady-state conditions, matching the gold answer's requirements with additional detailed steps.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-56-0-1-0", "category": "CORRECT", "explanation": "The candidate's answer thoroughly extends the neoclassical production function to include human capital and correctly derives the steady-state conditions, matching the gold answer's requirements with additional detailed steps."}, "llm_echoed_qid": "textbook-56-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly extends the neoclassical production function to include human capital and correctly derives the steady-state conditions, matching the gold answer's requirements with additional detailed steps."}
{"script_processing_qid": "textbook-112-2-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Using Smith's analysis, construct a model of rural-to-urban labor migration under feudal conditions. What were the key incentives and institutional barriers?\n\nGOLD_ANSWER:\n1.  **Incentives**:\n    - **Security**: Cities offered protection from lordly predation (e.g., \"year-and-a-day\" rule).\n    - **Economic Mobility**: Accumulated stock ($K$) could be invested in urban trades.\n2.  **Barriers**:\n    - **Villanage**: Legal bondage ($V$) restricted movement unless $K$ was hidden.\n    - **Model**: Migration occurs if $U_{city}(K) > U_{rural}(K, V)$, where $U$ is utility.\n3.  **Historical Evidence**: Flight to cities like Berne or Italian republics confirms the model.\n\nCANDIDATE_ANSWER:\nStep 1: **Identify key elements of Smith’s analysis**\n- Cities gained political security through alliances with monarchs against feudal lords.\n- Urban areas offered 'order, good government, liberty, and security' absent in feudal rural settings.\n- Cities leveraged trade networks (via sea/rivers) to reduce dependency on local rural resources.\n- Foreign commerce spurred domestic manufacturing to avoid transportation costs.\n\nStep 2: **Model rural-to-urban migration drivers**\n**Incentives:**\n1. **Security**: Escape feudal violence and instability in rural areas.\n2. **Economic opportunity**: Access to manufacturing jobs and commerce networks.\n3. **Political autonomy**: Urban leagues and royal patronage weakened feudal obligations.\n4. **Trade-enabled growth**: Reduced reliance on local agrarian systems via navigable trade routes.\n\n**Institutional barriers:**\n1. **Feudal control**: Lords retained power over rural labor and resources.\n2. **Mobility restrictions**: Potential legal/social constraints on leaving feudal lands.\n3. **Initial urban dependency**: Cities still required rural-derived materials (mitigated by trade networks).\n4. **Militia limitations**: While cities had defensive capabilities, rural-to-urban transitions required overcoming feudal lords’ resistance.\n\nStep 3: **Synthesize the model**\nMigration flows arose from rural 'push' factors (violence, instability) and urban 'pull' factors (security, economic diversification). Institutional barriers centered on feudal power structures, but royal-urban alliances and trade networks gradually eroded these constraints, enabling labor reallocation.\n\nQID: textbook-112-2-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-2-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer comprehensively addresses both the incentives and institutional barriers of rural-to-urban labor migration under feudal conditions, aligning well with the gold answer. The detailed analysis and synthesis of the model demonstrate a thorough understanding of Smith's analysis.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-2-0-3", "category": "CORRECT", "explanation": "The candidate's answer comprehensively addresses both the incentives and institutional barriers of rural-to-urban labor migration under feudal conditions, aligning well with the gold answer. The detailed analysis and synthesis of the model demonstrate a thorough understanding of Smith's analysis."}, "llm_echoed_qid": "textbook-112-2-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer comprehensively addresses both the incentives and institutional barriers of rural-to-urban labor migration under feudal conditions, aligning well with the gold answer. The detailed analysis and synthesis of the model demonstrate a thorough understanding of Smith's analysis."}
{"script_processing_qid": "textbook-106-16-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Using the example of Dick and Jane, derive the conditions under which private bargaining leads to an efficient outcome according to the Coase theorem.\n\nGOLD_ANSWER:\n1.  **Efficient Outcome Condition**: Private bargaining leads to an efficient outcome if the benefit to the party causing the externality (Dick) equals or exceeds the cost to the affected party (Jane).\n2.  **Example**: If Dick's benefit from the dog is $500 and Jane's cost is $800, Jane can offer Dick $600 to remove the dog. Dick accepts, and the outcome is efficient.\n3.  **Mathematical Formulation**: Let $B_D$ be Dick's benefit and $C_J$ be Jane's cost. Efficiency requires $B_D \\leq C_J$ for the dog to be removed, or $B_D > C_J$ for the dog to remain.\n\nCANDIDATE_ANSWER:\nStep 1: The Coase theorem states that private bargaining leads to an efficient outcome when two key conditions are met: (1) property rights are clearly defined, and (2) transaction costs are negligible (parties can bargain freely without costs).\n\nStep 2: Assume Dick’s activity creates an externality affecting Jane (e.g., noise pollution). Property rights must first be allocated—either Dick has the right to continue the activity, or Jane has the right to block it.\n\nStep 3: If Dick has the right, Jane can pay Dick to reduce the externality. If Jane has the right, Dick can pay Jane to tolerate some level of the externality. The efficient outcome (optimal externality level) is achieved in both cases.\n\nStep 4: Transaction costs (e.g., negotiation delays, legal fees) must be zero for this bargaining to succeed. If present, they could prevent agreement.\n\nStep 5: The final efficient outcome is independent of the initial assignment of property rights. Bargaining redistributes costs/benefits but ensures the socially optimal level of the externality.\n\nQID: textbook-106-16-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-106-16-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the key conditions of the Coase theorem—clearly defined property rights and negligible transaction costs—and explains how private bargaining leads to an efficient outcome regardless of the initial allocation of rights. This aligns with the principles outlined in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-16-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the key conditions of the Coase theorem—clearly defined property rights and negligible transaction costs—and explains how private bargaining leads to an efficient outcome regardless of the initial allocation of rights. This aligns with the principles outlined in the gold answer."}, "llm_echoed_qid": "textbook-106-16-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the key conditions of the Coase theorem—clearly defined property rights and negligible transaction costs—and explains how private bargaining leads to an efficient outcome regardless of the initial allocation of rights. This aligns with the principles outlined in the gold answer."}
{"script_processing_qid": "textbook-42-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Suppose the producer faces output price $p$ and maximizes profit. Derive the additional conditions that emerge from profit maximization.\n\nGOLD_ANSWER:\n1.  **Profit Maximization**: $\\max_y \\, p y - C(w, y)$.\n2.  **FOC**: $p = \\frac{\\partial C}{\\partial y}$ (marginal cost equals price).\n3.  **SOC**: $\\frac{\\partial^2 C}{\\partial y^2} \\geq 0$ (convex cost function ensures maximum).\n\nCANDIDATE_ANSWER:\nStep 1: Recall that profit maximization requires maximizing π = p·y - C(w,y) by choosing both inputs (indirectly through y) and output level y.\n\nStep 2: First-order condition for output choice:\n∂π/∂y = p - ∂C(w,y)/∂y = 0\n→ p = ∂C(w,y)/∂y (Price equals marginal cost)\n\nStep 3: Combining with cost minimization results:\n- From the cost minimization problem, factor demands satisfy ∂C(w,y)/∂w_i = x_i (Shephard's lemma)\n- The Lagrange multiplier in the cost minimization problem represents marginal cost (λ = ∂C/∂y)\n\nStep 4: Additional profit maximization conditions:\n1. Optimal output condition: p = MC(y) = ∂C(w,y)/∂y\n2. Factor demands now depend on endogenous y* determined by p=MC(y)\n3. System of equations becomes:\n   - x_i = ∂C(w,y*)/∂w_i (from cost minimization)\n   - y* satisfies p = ∂C(w,y*)/∂y\n\nThese conditions show how output price p determines optimal scale of production (y*) through marginal cost, which then influences input demands via the cost function derivatives.\n\nQID: textbook-42-0-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-42-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the first-order condition for profit maximization (p = MC) and discusses the relationship between output price, optimal scale of production, and input demands. It aligns with the gold answer's key points while providing additional context about factor demands and the system of equations.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-42-0-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly derives the first-order condition for profit maximization (p = MC) and discusses the relationship between output price, optimal scale of production, and input demands. It aligns with the gold answer's key points while providing additional context about factor demands and the system of equations."}, "llm_echoed_qid": "textbook-42-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the first-order condition for profit maximization (p = MC) and discusses the relationship between output price, optimal scale of production, and input demands. It aligns with the gold answer's key points while providing additional context about factor demands and the system of equations."}
{"script_processing_qid": "textbook-122-26-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the stability conditions for the AR(2) model: $\\rho_{2}>-1$, $\\rho_{2}-\\rho_{1}<1$, and $\\rho_{1}+\\rho_{2}<1$.\n\nGOLD_ANSWER:\n1. **$\\rho_{2} > -1$**: Ensures the roots of the characteristic equation are real or complex with modulus less than 1.\n2. **$\\rho_{2} - \\rho_{1} < 1$**: Prevents explosive behavior in one direction.\n3. **$\\rho_{1} + \\rho_{2} < 1$**: Ensures the process does not diverge over time.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the characteristic equation for AR(2)\nThe AR(2) model \\( u_t = \\rho_1 u_{t-1} + \\rho_2 u_{t-2} + e_t \\) has a characteristic equation: \\( z^2 - \\rho_1 z - \\rho_2 = 0 \\). Stability requires all roots to lie outside the unit circle (\\( |z| > 1 \\)).\n\nStep 2: Derive boundary conditions for roots\nTo prevent roots at critical boundaries:\n- At \\( z = 1 \\): Substitute \\( z = 1 \\) into \\( 1 - \\rho_1 - \\rho_2 > 0 \\) → \\( \\rho_1 + \\rho_2 < 1 \\).\n- At \\( z = -1 \\): Substitute \\( z = -1 \\) into \\( 1 + \\rho_1 - \\rho_2 > 0 \\) → \\( \\rho_2 - \\rho_1 < 1 \\).\n\nStep 3: Address real-root constraints\nTo avoid explosive negative autocorrelation, ensure the constant term satisfies \\( \\rho_2 > -1 \\). This maintains the parabola’s upward/downward orientation without excessive curvature.\n\nStep 4: Combine conditions\nThese three inequalities jointly ensure:\n1. \\( \\rho_2 > -1 \\): Rules out roots approaching \\( z = -1 \\).\n2. \\( \\rho_2 - \\rho_1 < 1 \\): Blocks roots at \\( z = -1 \\).\n3. \\( \\rho_1 + \\rho_2 < 1 \\): Blocks roots at \\( z = 1 \\).\nTogether, they guarantee all roots satisfy \\( |z| > 1 \\), ensuring stationarity.\n\nQID: textbook-122-26-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-26-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the stability conditions for the AR(2) model, correctly deriving and interpreting each inequality and ensuring all roots lie outside the unit circle for stationarity. This matches the gold answer's intent and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-26-1-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the stability conditions for the AR(2) model, correctly deriving and interpreting each inequality and ensuring all roots lie outside the unit circle for stationarity. This matches the gold answer's intent and reasoning."}, "llm_echoed_qid": "textbook-122-26-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the stability conditions for the AR(2) model, correctly deriving and interpreting each inequality and ensuring all roots lie outside the unit circle for stationarity. This matches the gold answer's intent and reasoning."}
{"script_processing_qid": "textbook-114-12-4-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. For a multiplant monopolist with cost functions $C_1(Q_1) = 2Q_1^2$ and $C_2(Q_2) = 3Q_2^2$, and demand $P(Q_T) = 100 - Q_T$, derive the optimal outputs $Q_1^*$ and $Q_2^*$ and total output $Q_T^*$.\n\nGOLD_ANSWER:\n1. **Marginal Costs**: $MC_1 = 4Q_1$, $MC_2 = 6Q_2$.\n2. **Equal MC Condition**: Set $MC_1 = MC_2 \\implies 4Q_1 = 6Q_2 \\implies Q_1 = 1.5Q_2$.\n3. **Total MC Curve**: $Q_T = Q_1 + Q_2 = 2.5Q_2 \\implies Q_2 = 0.4Q_T$, $Q_1 = 0.6Q_T$.\n   Thus, $MC_T = MC_1 = MC_2 = 4(0.6Q_T) = 2.4Q_T$.\n4. **MR = MC_T**: $MR = 100 - 2Q_T = 2.4Q_T \\implies Q_T^* = \\frac{100}{4.4} \\approx 22.73$.\n5. **Plant Outputs**: $Q_1^* = 0.6 \\times 22.73 \\approx 13.64$, $Q_2^* = 0.4 \\times 22.73 \\approx 9.09$.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate Marginal Revenue (MR)\nGiven demand P = 100 - Q_T (where Q_T = Q1 + Q2):\nMR = d(P·Q_T)/dQ_T = 100 - 2Q_T\n\nStep 2: Find Marginal Costs for each plant\nFrom C₁(Q₁) = 2Q₁² → MC₁ = dC₁/dQ₁ = 4Q₁\nFrom C₂(Q₂) = 3Q₂² → MC₂ = dC₂/dQ₂ = 6Q₂\n\nStep 3: Set optimality conditions\nMultiplant optimization requires:\nMR = MC₁ = MC₂\n\nStep 4: Establish equations\n1. 4Q₁ = 6Q₂ (from MC₁ = MC₂)\n2. 100 - 2(Q₁ + Q₂) = 4Q₁ (from MR = MC₁)\n\nStep 5: Solve the system\nFrom equation 1: Q₂ = (2/3)Q₁\nSubstitute into equation 2:\n100 - 2(Q₁ + (2/3)Q₁) = 4Q₁\n100 - (10/3)Q₁ = 4Q₁\n100 = (22/3)Q₁ → Q₁ = 150/11 ≈ 13.64\n\nStep 6: Find Q₂ and Q_T\nQ₂ = (2/3)(150/11) = 100/11 ≈ 9.09\nQ_T = Q₁ + Q₂ = 250/11 ≈ 22.73\n\nFinal Optimal Values:\nQ₁* = 150/11, Q₂* = 100/11, Q_T* = 250/11\n\nQID: textbook-114-12-4-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-12-4-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the optimal outputs \\(Q_1^*\\), \\(Q_2^*\\), and total output \\(Q_T^*\\) using the correct steps and calculations, matching the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-12-4-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the optimal outputs \\(Q_1^*\\), \\(Q_2^*\\), and total output \\(Q_T^*\\) using the correct steps and calculations, matching the gold answer."}, "llm_echoed_qid": "textbook-114-12-4-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the optimal outputs \\(Q_1^*\\), \\(Q_2^*\\), and total output \\(Q_T^*\\) using the correct steps and calculations, matching the gold answer."}
{"script_processing_qid": "textbook-77-0-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that a Walrasian equilibrium allocation is Pareto optimal under the assumption of local non-satiation.\n\nGOLD_ANSWER:\n1. **Pareto Optimality**: A Walrasian equilibrium allocation $\\mathbf{x}^{\\*}$ is Pareto optimal because the upper contour sets of the consumers do not intersect. For consumer 1, the set $\\{\\mathbf{x}\\_{1} \\succ\\_{1} \\mathbf{x}\\_{1}^{\\*}\\}$ lies strictly above the budget line, and for consumer 2, the set $\\{\\mathbf{x}\\_{2} \\succ\\_{2} \\mathbf{x}\\_{2}^{\\*}\\}$ lies strictly above the budget line. Thus, there is no feasible allocation that makes one consumer better off without making the other worse off.\n\nCANDIDATE_ANSWER:\nStep 1: Assume a Walrasian equilibrium allocation (x₁*, x₂*) with price vector F*. By definition, both consumers maximize utility under their budget constraints.\nStep 2: Local non-satiation ensures that for any bundle x₁ ≻₁ x₁*, the cost p*·x₁ > p*·x₁* (strictly above the budget line). Similarly, for x₂ ≻₂ x₂*, p*·x₂ > p*·x₂*.\nStep 3: Suppose a Pareto improvement exists: (x₁', x₂') where x₁' ≽₁ x₁* and x₂' ≽₂ x₂* with at least one strict preference. For strictly preferred bundles (e.g., x₁' ≻₁ x₁*), p*·x₁' > p*·x₁*. For weakly preferred bundles (x₂' ≽₂ x₂*), local non-satiation implies p*·x₂' ≥ p*·x₂*.\nStep 4: Summing these inequalities: p*·(x₁' + x₂') > p*·(x₁* + x₂*). But feasibility requires x₁' + x₂' = x₁* + x₂* (total endowment), leading to p*·(total endowment) > p*·(total endowment), a contradiction.\nStep 5: Thus, no Pareto improvement exists. The non-intersection of strict upper contour sets (as described in the context) confirms this result. Therefore, the Walrasian equilibrium allocation is Pareto optimal.\n\nQID: textbook-77-0-2-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-77-0-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to prove that a Walrasian equilibrium allocation is Pareto optimal under local non-satiation. The reasoning aligns with the gold answer, demonstrating that no Pareto improvement exists and confirming the non-intersection of strict upper contour sets.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-77-0-2-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to prove that a Walrasian equilibrium allocation is Pareto optimal under local non-satiation. The reasoning aligns with the gold answer, demonstrating that no Pareto improvement exists and confirming the non-intersection of strict upper contour sets."}, "llm_echoed_qid": "textbook-77-0-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to prove that a Walrasian equilibrium allocation is Pareto optimal under local non-satiation. The reasoning aligns with the gold answer, demonstrating that no Pareto improvement exists and confirming the non-intersection of strict upper contour sets."}
{"script_processing_qid": "textbook-110-4-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Explain the role of mixed strategies in the Nash-equilibrium approach to strategic behavior. Discuss the conditions under which mixed strategies are considered appropriate.\n\nGOLD_ANSWER:\n1.  **Role of Mixed Strategies**: Mixed strategies are used to ensure the existence of a Nash equilibrium in games where pure strategy equilibria may not exist. They introduce probabilistic choices over actions, allowing players to randomize their strategies.\n2.  **Appropriateness**: Mixed strategies are appropriate when:\n    - The game lacks pure strategy Nash equilibria.\n    - Players are indifferent between multiple strategies.\n    - The context allows for randomization as a credible strategic tool.\n3.  **Plausibility**: The plausibility depends on whether economic actors can realistically observe and understand randomization devices as part of their strategic toolkit.\n\nCANDIDATE_ANSWER:\nStep 1: Role of Mixed Strategies in Nash Equilibrium\nMixed strategies serve as a theoretical device to resolve situations where pure-strategy Nash equilibria may not exist. By allowing players to randomize over their actions, they guarantee the existence of an equilibrium in finite games (via Nash's existence theorem), thereby 'closing the hole' in the Nash-equilibrium framework.\n\nStep 2: Conditions for Appropriateness\nThe appropriateness of mixed strategies depends on two key factors:\n1. **Model Context**: Whether the specific microeconomic scenario logically accommodates randomization (e.g., in games requiring unpredictability, such as competitive pricing or security resource allocation).\n2. **Plausibility Criterion**: The extent to which real-world economic agents can realistically observe, interpret, and employ randomization as a strategic tool. If agents lack the capacity or incentives to use/recognize randomized strategies, mixed strategies become less compelling as an equilibrium concept.\n\nConclusion: Mixed strategies are contextually appropriate when equilibrium existence depends on randomization and when the model's assumptions align with agents' strategic rationality regarding stochastic behavior.\n\nQID: textbook-110-4-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-110-4-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the role of mixed strategies in ensuring the existence of Nash equilibrium and correctly identifies the conditions under which mixed strategies are appropriate, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-4-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the role of mixed strategies in ensuring the existence of Nash equilibrium and correctly identifies the conditions under which mixed strategies are appropriate, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-110-4-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the role of mixed strategies in ensuring the existence of Nash equilibrium and correctly identifies the conditions under which mixed strategies are appropriate, aligning well with the gold answer."}
{"script_processing_qid": "textbook-67-0-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Using the two-sector model of Rebelo and Végh (1995), explain why a reduction in government consumption of tradable goods leads to a real appreciation, while a reduction in non-tradable consumption leads to a depreciation.\n\nGOLD_ANSWER:\n1. **Tradable Goods**: Lower government demand for tradables reduces their domestic price, raising the relative price of non-tradables (real appreciation).\n2. **Non-Tradable Goods**: Lower government demand for non-tradables directly reduces their price, causing a real depreciation.\n3. **Empirical Relevance**: Only the tradable case aligns with observed real appreciations during stabilizations.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the two-sector model structure. Rebelo and Végh (1995) distinguish between tradable goods (T) and non-tradable goods (N). The real exchange rate is determined by the relative price of non-tradables to tradables (P_N/P_T).\n\nStep 2: Analyze a reduction in government consumption of tradables (G_T↓). When G_T decreases:\n- More tradable goods become available for private consumption (private C_T↑), creating a consumption boom.\n- The surplus tradables may be partly exported, improving the current account.\n- Increased private wealth shifts demand toward non-tradables (C_N↑), raising P_N relative to P_T → real appreciation (P_N/P_T↑).\n\nStep 3: Analyze a reduction in government consumption of non-tradables (G_N↓). When G_N decreases:\n- Supply of non-tradables increases (since G_N was absorbing part of N-sector output).\n- Reduced government demand lowers P_N relative to P_T → real depreciation (P_N/P_T↓).\n\nStep 4: Contrast mechanisms. G_T reduction primarily affects private consumption composition and relative demand for N-goods, while G_N reduction directly impacts N-sector supply/demand balance. This explains the opposing effects on the real exchange rate.\n\nQID: textbook-67-0-2-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-67-0-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the mechanisms behind the real appreciation and depreciation effects of reductions in government consumption of tradable and non-tradable goods, respectively, aligning perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-67-0-2-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the mechanisms behind the real appreciation and depreciation effects of reductions in government consumption of tradable and non-tradable goods, respectively, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-67-0-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the mechanisms behind the real appreciation and depreciation effects of reductions in government consumption of tradable and non-tradable goods, respectively, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-92-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the asymptotic normality of the M-estimator $\\hat{\\pmb\\theta}$ using the Mean Value Theorem, starting from the first-order condition $\\frac{1}{n}\\sum_{t=1}^{n}\\mathbf{s}(\\mathbf{w}_{t};\\hat{\\pmb\\theta}) = \\mathbf{0}$.\n\nGOLD_ANSWER:\n1. **First-order condition**: $\\frac{1}{n}\\sum_{t=1}^{n}\\mathbf{s}(\\mathbf{w}_{t};\\hat{\\pmb\\theta}) = \\mathbf{0}$.\\n2. **Mean Value Expansion**: Apply the Mean Value Theorem to $\\frac{\\partial Q_{n}(\\cdot)}{\\partial\\pmb{\\theta}}$ around $\\pmb{\\theta}_{0}$: $$\\frac{\\partial Q_{n}(\\hat{\\pmb{\\theta}})}{\\partial\\pmb{\\theta}} = \\frac{\\partial Q_{n}(\\pmb{\\theta}_{0})}{\\partial\\pmb{\\theta}} + \\frac{\\partial^{2}Q_{n}(\\bar{\\pmb{\\theta}})}{\\partial\\pmb{\\theta}\\partial\\pmb{\\theta}^{\\prime}}(\\hat{\\pmb{\\theta}} - \\pmb{\\theta}_{0}),$$ where $\\bar{\\pmb{\\theta}}$ lies between $\\hat{\\pmb{\\theta}}$ and $\\pmb{\\theta}_{0}$.\\n3. **Combine with first-order condition**: $$\\mathbf{0} = \\frac{1}{n}\\sum_{t=1}^{n}\\mathbf{s}(\\mathbf{w}_{t};\\pmb{\\theta}_{0}) + \\left[\\frac{1}{n}\\sum_{t=1}^{n}\\mathbf{H}(\\mathbf{w}_{t};\\bar{\\pmb{\\theta}})\\right](\\hat{\\pmb{\\theta}} - \\pmb{\\theta}_{0}).$$\\n4. **Solve for $\\hat{\\pmb{\\theta}} - \\pmb{\\theta}_{0}$**: $$\\sqrt{n}(\\hat{\\pmb{\\theta}} - \\pmb{\\theta}_{0}) = -\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\mathbf{H}(\\mathbf{w}_{t};\\bar{\\pmb{\\theta}})\\right]^{-1}\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{n}\\mathbf{s}(\\mathbf{w}_{t};\\pmb{\\theta}_{0}).$$\\n5. **Asymptotic normality**: Under the assumptions of Proposition 7.8, $$\\sqrt{n}(\\hat{\\pmb{\\theta}} - \\pmb{\\theta}_{0}) \\xrightarrow{d} N\\left(\\mathbf{0}, \\left(\\mathbf{E}\\left[\\mathbf{H}(\\mathbf{w}_{t};\\pmb{\\theta}_{0})\\right]\\right)^{-1}\\pmb{\\Sigma}\\left(\\mathbf{E}\\left[\\mathbf{H}(\\mathbf{w}_{t};\\pmb{\\theta}_{0})\\right]\\right)^{-1}\\right).$$\n\nCANDIDATE_ANSWER:\nStep 1: Start with the first-order condition for the M-estimator: \\(\\frac{1}{n}\\sum_{t=1}^{n}\\mathbf{s}(\\mathbf{w}_{t};\\hat{\\pmb\\theta}) = \\mathbf{0}\\).\n\nStep 2: Apply the Mean Value Theorem to expand \\(\\mathbf{s}(\\mathbf{w}_{t};\\hat{\\pmb\\theta})\\) around the true parameter \\(\\pmb\\theta_0\\). This gives:\n\\(\\mathbf{s}(\\mathbf{w}_{t};\\hat{\\pmb\\theta}) \\approx \\mathbf{s}(\\mathbf{w}_{t};\\pmb\\theta_0) + \\mathbf{H}(\\mathbf{w}_{t};\\bar{\\pmb\\theta})(\\hat{\\pmb\\theta} - \\pmb\\theta_0)\\),\nwhere \\(\\bar{\\pmb\\theta}\\) lies between \\(\\hat{\\pmb\\theta}\\) and \\(\\pmb\\theta_0\\).\n\nStep 3: Substitute the expansion into the first-order condition:\n\\(\\frac{1}{n}\\sum_{t=1}^{n} \\left[\\mathbf{s}(\\mathbf{w}_{t};\\pmb\\theta_0) + \\mathbf{H}(\\mathbf{w}_{t};\\bar{\\pmb\\theta})(\\hat{\\pmb\\theta} - \\pmb\\theta_0)\\right] = \\mathbf{0}\\).\n\nStep 4: Rearrange to isolate \\(\\hat{\\pmb\\theta} - \\pmb\\theta_0\\):\n\\(\\sqrt{n}(\\hat{\\pmb\\theta} - \\pmb\\theta_0) = -\\left[\\frac{1}{n}\\sum_{t=1}^{n}\\mathbf{H}(\\mathbf{w}_{t};\\bar{\\pmb\\theta})\\right]^{-1} \\cdot \\frac{1}{\\sqrt{n}}\\sum_{t=1}^{n}\\mathbf{s}(\\mathbf{w}_{t};\\pmb\\theta_0)\\).\n\nStep 5: Use the consistency of \\(\\hat{\\pmb\\theta}\\) (implying \\(\\bar{\\pmb\\theta} \\rightarrow_p \\pmb\\theta_0\\)) and the uniform convergence of the Hessian (Assumption 4 in Proposition 7.8) to show:\n\\(\\frac{1}{n}\\sum_{t=1}^{n}\\mathbf{H}(\\mathbf{w}_{t};\\bar{\\pmb\\theta}) \\rightarrow_p \\mathbf{E}[\\mathbf{H}(\\mathbf{w}_{t};\\pmb\\theta_0)]\\).\n\nStep 6: By the Central Limit Theorem (Assumption 3), \\(\\frac{1}{\\sqrt{n}}\\sum_{t=1}^{n}\\mathbf{s}(\\mathbf{w}_{t};\\pmb\\theta_0) \\rightarrow_d N(\\mathbf{0}, \\pmb{\\Sigma})\\).\n\nStep 7: Combine Slutsky’s theorem with Steps 5–6 to conclude asymptotic normality:\n\\(\\sqrt{n}(\\hat{\\pmb\\theta} - \\pmb\\theta_0) \\rightarrow_d N\\left(\\mathbf{0}, \\left(\\mathbf{E}[\\mathbf{H}(\\mathbf{w}_{t};\\pmb\\theta_0)]\\right)^{-1} \\pmb{\\Sigma} \\left(\\mathbf{E}[\\mathbf{H}(\\mathbf{w}_{t};\\pmb\\theta_0)]\\right)^{-1}\\right)\\).\n\nThis matches the asymptotic variance formula in Proposition 7.8.\n\nQID: textbook-92-0-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-92-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly follows the steps outlined in the gold answer, including the application of the Mean Value Theorem, rearrangement of terms, and the use of the Central Limit Theorem and Slutsky’s theorem to derive the asymptotic normality of the M-estimator. The reasoning and final result match the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-92-0-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly follows the steps outlined in the gold answer, including the application of the Mean Value Theorem, rearrangement of terms, and the use of the Central Limit Theorem and Slutsky’s theorem to derive the asymptotic normality of the M-estimator. The reasoning and final result match the gold answer."}, "llm_echoed_qid": "textbook-92-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly follows the steps outlined in the gold answer, including the application of the Mean Value Theorem, rearrangement of terms, and the use of the Central Limit Theorem and Slutsky’s theorem to derive the asymptotic normality of the M-estimator. The reasoning and final result match the gold answer."}
{"script_processing_qid": "textbook-115-11-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What is the practical advantage of using 2SLS with generated instruments over OLS with generated regressors?\n\nGOLD_ANSWER:\nThe practical advantage is that the asymptotic variance of the 2SLS estimator does not need adjustment for the generated instruments under the condition $\\text{E}[\\nabla_{\\lambda}\\mathbf{g}(\\mathbf{w}, \\lambda)'u] = 0$. This simplifies inference compared to OLS with generated regressors, where the asymptotic variance typically requires adjustment.\n\nCANDIDATE_ANSWER:\nStep 1: Compare the key properties of 2SLS with generated instruments vs. OLS with generated regressors. The context shows that 2SLS requires weaker adjustments for generated instruments than OLS requires for generated regressors.<br/><br/>Step 2: For 2SLS, the asymptotic distribution of β-hat remains unaffected by first-stage λ estimation under the condition E[∇_λ g(w, λ)'u] = 0. This makes standard inference (e.g., t-statistics, confidence intervals) valid without requiring corrections for generated instrument uncertainty.<br/><br/>Step 3: In contrast, OLS with generated regressors typically requires adjustment to standard errors (e.g., Murphy-Topel correction) even when the first-stage estimation is consistent. The generated regressors introduce additional sampling variation that must be explicitly accounted for.<br/><br/>Practical Advantage: 2SLS with generated instruments simplifies inference by preserving the validity of standard errors under weaker conditions, eliminating the need for complex variance adjustments that OLS with generated regressors would require.\n\nQID: textbook-115-11-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-11-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the practical advantage of using 2SLS with generated instruments over OLS with generated regressors, highlighting the simplified inference due to weaker conditions for standard error validity in 2SLS.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-11-1-2", "category": "CORRECT", "explanation": "The candidate answer accurately explains the practical advantage of using 2SLS with generated instruments over OLS with generated regressors, highlighting the simplified inference due to weaker conditions for standard error validity in 2SLS."}, "llm_echoed_qid": "textbook-115-11-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the practical advantage of using 2SLS with generated instruments over OLS with generated regressors, highlighting the simplified inference due to weaker conditions for standard error validity in 2SLS."}
{"script_processing_qid": "textbook-107-10-3-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the biological interpretation of the Hawk-Dove game in terms of evolutionary stable strategies.\n\nGOLD_ANSWER:\nThe Hawk-Dove game models conflict over resources in animal populations. Hawks always fight, while doves share. An evolutionary stable strategy (ESS) is a population proportion where neither strategy can invade, ensuring equal fitness. Here, 42% hawks and 58% doves is an ESS.\n\nCANDIDATE_ANSWER:\nStep 1: Understand ESS in the Hawk-Dove game. An evolutionary stable strategy (ESS) is a population strategy that cannot be invaded by alternative strategies. Here, a mixed population of Hawks (aggressive) and Doves (peaceful) reaches equilibrium when their expected payoffs are equal.\n\nStep 2: Define payoffs. Let the fraction of Hawks in the population be p. The expected payoff for a Hawk is:\n-7p (when facing another Hawk) + 8(1-p) (when facing a Dove).\nFor a Dove, the payoff is:\n0p (when facing a Hawk) + 3(1-p) (when facing another Dove).\n\nStep 3: Set payoffs equal at equilibrium. For Hawks and Doves to coexist stably:\n-7p + 8(1-p) = 3(1-p).\n\nStep 4: Solve for p:\n-7p + 8 - 8p = 3 - 3p\n-15p + 8 = 3 - 3p\n-12p = -5\np = 5/12 ≈ 0.42.\n\nStep 5: Biological interpretation. At p = 0.42, the costs of frequent Hawk-Hawk conflicts (-7 payoff) balance the benefits of Hawk-Dove interactions (8 payoff for Hawks). This ensures neither strategy dominates, maintaining a stable mix where aggression (Hawk) and cooperation (Dove) coexist evolutionarily.\n\nQID: textbook-107-10-3-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-107-10-3-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains the biological interpretation of the Hawk-Dove game in terms of evolutionary stable strategies (ESS). It accurately calculates the equilibrium proportion of Hawks and Doves and provides a clear biological interpretation of the results, matching the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-10-3-1", "category": "CORRECT", "explanation": "The candidate answer correctly explains the biological interpretation of the Hawk-Dove game in terms of evolutionary stable strategies (ESS). It accurately calculates the equilibrium proportion of Hawks and Doves and provides a clear biological interpretation of the results, matching the gold answer."}, "llm_echoed_qid": "textbook-107-10-3-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains the biological interpretation of the Hawk-Dove game in terms of evolutionary stable strategies (ESS). It accurately calculates the equilibrium proportion of Hawks and Doves and provides a clear biological interpretation of the results, matching the gold answer."}
{"script_processing_qid": "textbook-123-11-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that if $\\tilde{A}$ is a stable matrix, the only solution to $(\\tilde{A}-I)\\mu\\_{2}=0$ is $\\mu\\_{2}=0$.\n\nGOLD_ANSWER:\n1. **Assumption**: $\\tilde{A}$ is stable, so all eigenvalues $\\lambda_i$ satisfy $|\\lambda_i| < 1$.\n2. **Equation**: $(\\tilde{A}-I)\\mu\\_{2}=0$ implies $\\tilde{A}\\mu\\_{2}=\\mu\\_{2}$.\n3. **Interpretation**: $\\mu\\_{2}$ is an eigenvector of $\\tilde{A}$ with eigenvalue 1.\n4. **Contradiction**: Since $\\tilde{A}$ is stable, it cannot have an eigenvalue of 1.\n5. **Conclusion**: The only solution is $\\mu\\_{2}=0$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the definition of a stable matrix. A matrix $\tilde{A}$ is stable if all its eigenvalues have real parts less than 1.\nStep 2: The equation $(\tilde{A} - I)\\mu_2 = 0$ implies $\\mu_2$ lies in the null space of $(\tilde{A} - I)$.\nStep 3: For non-trivial solutions ($\\mu_2 \neq 0$), $(\tilde{A} - I)$ must be singular (i.e., have at least one eigenvalue equal to 0).\nStep 4: The eigenvalues of $(\tilde{A} - I)$ are $\\lambda_i - 1$, where $\\lambda_i$ are eigenvalues of $\tilde{A}$. Since $\tilde{A}$ is stable, $\text{Re}(\\lambda_i) < 1$ implies $\\lambda_i - 1 \neq 0$ (no eigenvalues equal to 0).\nStep 5: Therefore, $(\tilde{A} - I)$ is invertible (full rank), and its null space contains only the trivial solution $\\mu_2 = 0$.\n\nQID: textbook-123-11-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-11-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the logical steps to prove that the only solution is $\\mu_2 = 0$, aligning with the gold answer. The reasoning is accurate and complete, despite a minor difference in the definition of a stable matrix (real parts less than 1 vs. absolute value less than 1).\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-11-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the logical steps to prove that the only solution is $\\mu_2 = 0$, aligning with the gold answer. The reasoning is accurate and complete, despite a minor difference in the definition of a stable matrix (real parts less than 1 vs. absolute value less than 1)."}, "llm_echoed_qid": "textbook-123-11-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the logical steps to prove that the only solution is $\\mu_2 = 0$, aligning with the gold answer. The reasoning is accurate and complete, despite a minor difference in the definition of a stable matrix (real parts less than 1 vs. absolute value less than 1)."}
{"script_processing_qid": "textbook-115-17-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Compare the efficiency of the random effects estimator under Assumptions RE.1–RE.3 versus a general FGLS estimator using $\\hat{\\Omega} = N^{-1}\\sum_{i=1}^N \\hat{\\mathbf{v}}_i \\hat{\\mathbf{v}}_i^{\\prime}$.\n\nGOLD_ANSWER:\nUnder RE.1–RE.3, the RE estimator is asymptotically efficient. The general FGLS estimator is equally efficient under these assumptions but gains efficiency if $\\operatorname{E}(\\mathbf{v}_i \\mathbf{v}_i^{\\prime} \\mid \\mathbf{x}_i) = \\Omega$ does not have the RE structure.\n\nCANDIDATE_ANSWER:\nStep 1: Under Assumptions RE.1–RE.3, the random effects (RE) estimator assumes the variance-covariance matrix of the composite error term $\\mathbf{v}_i$ has the RE structure (homoskedastic idiosyncratic errors, no serial correlation, and time-constant variances). This makes the RE estimator equivalent to a correctly specified GLS estimator.\n\nStep 2: A general FGLS estimator using $\\hat{\\Omega} = N^{-1}\\sum_{i=1}^N \\hat{\\mathbf{v}}_i \\hat{\\mathbf{v}}_i^{\\prime}$ does not impose the RE structure. Instead, it estimates $\\Omega$ nonparametrically by averaging the outer products of residuals $\\hat{\\mathbf{v}}_i$.\n\nStep 3: When Assumptions RE.1–RE.3 hold (including the RE structure for $\\mathbf{v}_i$), the RE estimator is asymptotically efficient (it achieves the smallest possible variance among linear unbiased estimators). In contrast, the FGLS estimator using $\\hat{\\Omega}$ remains consistent but is less efficient because it does not exploit the known RE structure of $\\Omega$. The FGLS estimator requires estimating $O(T^2)$ parameters (the full $T \times T$ covariance matrix for each $i$), leading to higher finite-sample variance.\n\nConclusion: Under RE.1–RE.3, the RE estimator is more efficient than the general FGLS estimator using $\\hat{\\Omega}$. The efficiency gain arises from correctly imposing the RE structure rather than estimating a more flexible (but unnecessary) covariance matrix.\n\nQID: textbook-115-17-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-17-0-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's conclusion that the RE estimator is more efficient than FGLS under RE.1–RE.3 is incorrect. The gold answer states that both estimators are equally efficient under these assumptions, with FGLS gaining efficiency only if the RE structure is violated.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-17-0-2", "category": "INCORRECT", "explanation": "The candidate's conclusion that the RE estimator is more efficient than FGLS under RE.1–RE.3 is incorrect. The gold answer states that both estimators are equally efficient under these assumptions, with FGLS gaining efficiency only if the RE structure is violated."}, "llm_echoed_qid": "textbook-115-17-0-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's conclusion that the RE estimator is more efficient than FGLS under RE.1–RE.3 is incorrect. The gold answer states that both estimators are equally efficient under these assumptions, with FGLS gaining efficiency only if the RE structure is violated."}
{"script_processing_qid": "textbook-108-18-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain how a depreciation of the domestic currency affects net exports, assuming the Marshall-Lerner condition holds.\n\nGOLD_ANSWER:\n1.  **Marshall-Lerner Condition**: If $\\eta_X + \\eta_M > 1$, where $\\eta_X$ is the price elasticity of exports and $\\eta_M$ is the price elasticity of imports, a depreciation improves the trade balance.\n2.  **Mechanism**: \n    - Depreciation makes exports cheaper and imports more expensive.\n    - Higher export volumes and lower import volumes increase $NX$.\n3.  **Short-run vs. Long-run**: J-curve effect may cause initial deterioration before improvement.\n\nCANDIDATE_ANSWER:\nStep 1: A depreciation of the domestic currency makes domestically produced goods/services (exports) cheaper in foreign currency terms, increasing foreign demand for exports. This raises export quantities.\nStep 2: Simultaneously, a weaker domestic currency makes foreign-produced goods/services (imports) more expensive in domestic currency terms, reducing domestic demand for imports. This decreases import quantities.\nStep 3: The Marshall-Lerner condition states that if the sum of the price elasticities of exports and imports exceeds 1, the volume effects (changes in quantities) dominate the price effects (changes in currency values).\nStep 4: When the condition holds, the increase in export revenue and decrease in import spending outweigh initial price effects. This results in an improvement in net exports (trade balance).\n\nQID: textbook-108-18-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-108-18-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the mechanism of how a depreciation of the domestic currency affects net exports under the Marshall-Lerner condition, including the effects on export and import quantities and the dominance of volume effects over price effects.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-18-1-1", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the mechanism of how a depreciation of the domestic currency affects net exports under the Marshall-Lerner condition, including the effects on export and import quantities and the dominance of volume effects over price effects."}, "llm_echoed_qid": "textbook-108-18-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the mechanism of how a depreciation of the domestic currency affects net exports under the Marshall-Lerner condition, including the effects on export and import quantities and the dominance of volume effects over price effects."}
{"script_processing_qid": "textbook-112-4-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Critically evaluate Adam Smith's claim that laws restricting the corn trade are 'violations of natural liberty' and 'impolitic.' What are the broader implications for economic policy?\n\nGOLD_ANSWER:\n1.  **Natural Liberty**: Smith views such laws as infringing on individuals' freedom to allocate resources as they see fit, violating the principle of laissez-faire.\n2.  **Economic Consequences**: These laws distort market efficiency, leading to higher prices, reduced supply, and hindered agricultural improvement.\n3.  **Policy Implications**: Smith's argument advocates for minimal government intervention, trusting market mechanisms to achieve optimal outcomes. This has influenced classical and neoliberal economic policies.\n\nCANDIDATE_ANSWER:\nStep 1: **Identify Smith’s Core Arguments**  \n  Smith opposes laws restricting the corn trade (e.g., statutes like Edward VI) because they force farmers to act as corn merchants, disrupting the **division of labor** and **efficient allocation of stock**. This violates 'natural liberty' by interfering with free trade and is 'impolitic' (poor policy) because it reduces economic efficiency.  \n\nStep 2: **Evaluate the Claim’s Validity**  \n- **Violation of Natural Liberty**: Forcing farmers into dual roles (producer and merchant) restricts their freedom to specialize, a cornerstone of Smith’s free-market philosophy.  \n- **Impolitic Nature**: By obstructing specialization, the law harms productivity and land cultivation. Middlemen (corn merchants) enable efficient distribution, which Smith argues prevents scarcity ('dearth') by smoothing supply-demand mismatches. Banning them worsens outcomes.  \n\nStep 3: **Broader Implications for Economic Policy**  \nSmith’s critique implies that policies should:  \n1. **Promote Specialization**: Allow agents to focus on comparative advantages (e.g., farmers farming, merchants trading).  \n2. **Avoid Overregulation**: Restricting trade intermediaries stifles market efficiency and resilience.  \n3. **Prioritize Natural Liberty**: Interventions that distort voluntary exchange risk harming societal welfare.  \n\nStep 4: **Critical Limitations (Based on Context)**  \nThe provided context does not address potential counterarguments (e.g., market failures, monopolistic practices by merchants, or food security concerns), limiting a full critical evaluation. However, Smith’s logic holds if one assumes competitive markets and rational actors free from coercion.\n\nQID: textbook-112-4-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-4-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses Smith's arguments about natural liberty and impolitic laws, aligns with the gold answer's points, and expands on broader policy implications while acknowledging contextual limitations.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-4-0-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses Smith's arguments about natural liberty and impolitic laws, aligns with the gold answer's points, and expands on broader policy implications while acknowledging contextual limitations."}, "llm_echoed_qid": "textbook-112-4-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses Smith's arguments about natural liberty and impolitic laws, aligns with the gold answer's points, and expands on broader policy implications while acknowledging contextual limitations."}
{"script_processing_qid": "textbook-103-7-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze the equilibrium condition $f[1+r(t)] = 0$ in the nonmonetary equilibrium and derive the unique rate of return $1+r(t)$.\n\nGOLD_ANSWER:\n1. **Equilibrium Condition**: In a nonmonetary equilibrium, aggregate savings must equal zero: $f[1+r(t)] = 0$.\n2. **Substitute $f[1+r(t)]$**: $\\frac{1}{2}\\left[(1-\\alpha)y - \\frac{\\alpha Y}{1+r(t)}\\right] = 0$.\n3. **Solve for $1+r(t)$**:\n   $$\n   (1-\\alpha)y = \\frac{\\alpha Y}{1+r(t)} \\implies 1+r(t) = \\frac{\\alpha Y}{(1-\\alpha)y} = \\frac{N_2 Y}{N_1 y}.\n   $$\n4. **Interpretation**: The equilibrium rate of return is determined by the ratio of the total endowment of borrowers ($N_2 Y$) to the total endowment of lenders ($N_1 y$). If $N_2 Y > N_1 y$, the rate of return exceeds 1, indicating a high demand for borrowing relative to lending capacity.\n\nCANDIDATE_ANSWER:\nStep 1: In nonmonetary equilibrium, aggregate savings must equal zero since there is no monetary asset. The equilibrium condition requires total savings across all agents to be zero: f[1 + r(t)] = 0 where f[1 + r(t)] represents the aggregate savings function.\n\nStep 2: Using the individual savings function derived from logarithmic utility:\n$$s_{t}^{h}(t)=\\frac{1}{2}\\left[w_{t}^{h}(t)-\\frac{w_{t}^{h}(t+1)}{1 + r(t)}\\right]$$\n\nStep 3: Set aggregate savings equal to zero. For homogeneous agents (or a representative agent), this implies:\n$$\\frac{1}{2}\\left[w_{t}^{h}(t)-\\frac{w_{t}^{h}(t+1)}{1 + r(t)}\\right] = 0$$\n\nStep 4: Solve for $1 + r(t)$:\n1. Multiply both sides by 2: \n$$w_{t}^{h}(t) - \\frac{w_{t}^{h}(t+1)}{1 + r(t)} = 0$$\n2. Rearrange terms:\n$$1 + r(t) = \\frac{w_{t}^{h}(t+1)}{w_{t}^{h}(t)}$$\n\nFinal result: The unique rate of return in nonmonetary equilibrium is determined by the ratio of future to current endowment: $1 + r(t) = \\frac{w_{t}^{h}(t+1)}{w_{t}^{h}(t)}$.\n\nQID: textbook-103-7-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-103-7-0-3\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the equilibrium condition and the necessity for aggregate savings to be zero. However, the derivation and final expression for the rate of return differ from the gold answer, which specifies a ratio involving total endowments of borrowers and lenders rather than individual endowments.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-7-0-3", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the equilibrium condition and the necessity for aggregate savings to be zero. However, the derivation and final expression for the rate of return differ from the gold answer, which specifies a ratio involving total endowments of borrowers and lenders rather than individual endowments."}, "llm_echoed_qid": "textbook-103-7-0-3", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the equilibrium condition and the necessity for aggregate savings to be zero. However, the derivation and final expression for the rate of return differ from the gold answer, which specifies a ratio involving total endowments of borrowers and lenders rather than individual endowments."}
{"script_processing_qid": "textbook-123-12-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the solution to the recursive Pareto problem implies complete risk-sharing.\n\nGOLD_ANSWER:\nFrom the first-order condition for $w_{i}$, $P^{\\prime}(w_{i})=P^{\\prime}(v)$, and since $P(v)$ is strictly concave, this implies $w_{i}=v$ for all $i$. Thus, the continuation value $w_{i}$ is independent of the state $i$, meaning the planner does not adjust future utility promises based on current shocks. This ensures complete risk-sharing, as the relative Pareto weight $\\theta$ remains constant over time and across states.\n\nCANDIDATE_ANSWER:\nStep 1: The first-order condition for consumption allocations (-u′(1−c_i) + θu′(c_i) = 0) implies that the ratio of marginal utilities between types 1 and 2 equals a constant θ across all states (u′(c_i)/u′(1−c_i) = 1/θ).\n\nStep 2: The envelope condition (P′(v) = −θ) shows θ is constant across time/states because P(v) is a recursive value function independent of current states.\n\nStep 3: A constant ratio of marginal utilities across all states means consumption allocations (c_i, 1−c_i) depend only on the fixed parameter θ, not on the realization of state i. This eliminates state-dependent consumption fluctuations.\n\nStep 4: With i.i.d. shocks, the constant marginal utility ratio and recursive structure ensure all idiosyncratic risk is perfectly pooled. Households' consumption becomes independent of individual endowment shocks, achieving complete risk-sharing.\n\nQID: textbook-123-12-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-12-1-1\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies that complete risk-sharing is achieved through constant marginal utility ratios and state-independent consumption, which aligns with the gold answer's conclusion. However, the candidate's reasoning focuses on consumption allocations and marginal utilities, while the gold answer emphasizes the equality of continuation values and the constancy of Pareto weights. The approaches differ in their technical details but reach the same fundamental conclusion about complete risk-sharing.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-12-1-1", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies that complete risk-sharing is achieved through constant marginal utility ratios and state-independent consumption, which aligns with the gold answer's conclusion. However, the candidate's reasoning focuses on consumption allocations and marginal utilities, while the gold answer emphasizes the equality of continuation values and the constancy of Pareto weights. The approaches differ in their technical details but reach the same fundamental conclusion about complete risk-sharing."}, "llm_echoed_qid": "textbook-123-12-1-1", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies that complete risk-sharing is achieved through constant marginal utility ratios and state-independent consumption, which aligns with the gold answer's conclusion. However, the candidate's reasoning focuses on consumption allocations and marginal utilities, while the gold answer emphasizes the equality of continuation values and the constancy of Pareto weights. The approaches differ in their technical details but reach the same fundamental conclusion about complete risk-sharing."}
{"script_processing_qid": "textbook-111-8-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the aggregate demand curve from the Mundell-Fleming equations, showing the mathematical relationship between $P$ and $Y$.\n\nGOLD_ANSWER:\n1. **LM* Curve**: $M\\/P = L(r^*, Y) \\implies Y = Y(M, P, r^*)$.\n2. **IS* Curve**: $Y = C(Y-T) + I(r^*) + G + NX(\\epsilon)$, where $\\epsilon = eP\\/P^*$.\n3. **Combined**: For a given $M$ and $r^*$, lower $P$ increases $M\\/P$, shifting LM* right, raising $Y$.\n4. **AD Curve**: $Y = Y(P; M, r^*)$, downward-sloping in $P$-$Y$ space.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the LM* equation\nFrom the provided Mundell-Fleming model:\n$$\\frac{M}{P} = L(r^*,Y)$$\n\nStep 2: Solve LM* for Y\nAssume money demand follows a linear form $L(r^*,Y) = kY - hr^*$ (common textbook specification). Rearranging:\n$$Y = \\frac{1}{k}\\left(\\frac{M}{P} + hr^*\\right)$$\n\nStep 3: Relate to aggregate demand\nThis equation directly shows the inverse relationship between $P$ and $Y$:\n- As $P \\downarrow$, $\\frac{M}{P} \\uparrow$ increases real money balances\n- This requires $Y \\uparrow$ to maintain money market equilibrium\n\nStep 4: Incorporate IS* equilibrium\nWhile the IS* curve $Y = C(Y-T) + I(r^*) + G + NX(\\epsilon)$ determines the real exchange rate ($\\epsilon$), under floating exchange rates with price flexibility:\n- The exchange rate $\\epsilon$ automatically adjusts\n- Ensures IS* always intersects LM*-determined Y at every price level\n\nFinal AD Curve:\n$$Y = \\frac{M}{kP} + \\frac{hr^*}{k}$$\nThis represents a downward-sloping relationship between $P$ and $Y$, forming the aggregate demand curve.\n\nQID: textbook-111-8-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-111-8-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the aggregate demand curve from the Mundell-Fleming equations, showing the mathematical relationship between $P$ and $Y$. The steps and reasoning align with the gold answer, including the inverse relationship between $P$ and $Y$ and the incorporation of both LM* and IS* curves.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-8-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the aggregate demand curve from the Mundell-Fleming equations, showing the mathematical relationship between $P$ and $Y$. The steps and reasoning align with the gold answer, including the inverse relationship between $P$ and $Y$ and the incorporation of both LM* and IS* curves."}, "llm_echoed_qid": "textbook-111-8-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the aggregate demand curve from the Mundell-Fleming equations, showing the mathematical relationship between $P$ and $Y$. The steps and reasoning align with the gold answer, including the inverse relationship between $P$ and $Y$ and the incorporation of both LM* and IS* curves."}
{"script_processing_qid": "textbook-113-11-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the conditions under which the free-rider problem arises in the provision of lighthouses. Use economic theory to support your answer.\n\nGOLD_ANSWER:\n2. **Free-Rider Problem Conditions**: The free-rider problem arises when a good is non-excludable and non-rivalrous. For lighthouses:\n   - **Non-Excludability**: Ship captains cannot be prevented from using the lighthouse's light, even if they do not pay.\n   - **Non-Rivalry**: One captain's use of the lighthouse does not diminish its availability to others.\n   - **Implication**: Each captain has an incentive to free ride, leading to under-provision by private markets. Government intervention is often necessary to ensure provision.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the defining characteristics of public goods. Public goods are non-excludable (users cannot be prevented from benefiting) and non-rivalrous (one user's consumption does not reduce availability for others). Lighthouses traditionally fit these criteria, as ships cannot be excluded from using their light, and one ship’s use does not diminish the service for others.\n\nStep 2: Apply the free-rider problem framework. If a good is non-excludable, individuals (e.g., ship captains) can benefit without paying, creating an incentive to free ride. Since providers cannot charge users directly, private markets under-provide the good due to lack of profit incentive.\n\nStep 3: Analyze the exception in the context. The free-rider problem is mitigated when exclusion becomes possible indirectly. In 19th-century England, lighthouse owners charged port owners (not ship captains) and threatened to turn off the light if unpaid. This transformed the lighthouse into a club good (excludable for ports, non-rival for ships).\n\nStep 4: Derive conditions for the free-rider problem. The problem arises when (1) the good is non-excludable (no mechanism to charge users directly or indirectly) and (2) non-rivalrous. If exclusion is feasible (e.g., via port payments), the free-rider problem disappears, allowing private provision.\n\nConclusion: The free-rider problem in lighthouse provision occurs under non-excludability and non-rivalry. When exclusion mechanisms exist (e.g., charging a related party like port owners), the problem is circumvented.\n\nQID: textbook-113-11-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-113-11-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the conditions for the free-rider problem in lighthouse provision, aligning with the gold answer's points on non-excludability and non-rivalry. The additional historical context about port payments enriches the response without contradicting the core theory.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-113-11-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the conditions for the free-rider problem in lighthouse provision, aligning with the gold answer's points on non-excludability and non-rivalry. The additional historical context about port payments enriches the response without contradicting the core theory."}, "llm_echoed_qid": "textbook-113-11-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the conditions for the free-rider problem in lighthouse provision, aligning with the gold answer's points on non-excludability and non-rivalry. The additional historical context about port payments enriches the response without contradicting the core theory."}
{"script_processing_qid": "textbook-112-22-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the relationship between market size and the viability of manufacturing versus agriculture, using the examples of ancient Egypt and Indostan.\n\nGOLD_ANSWER:\n1. **Manufacturing Requirements**: Manufacturing requires a larger market due to higher output per laborer (e.g., a shoemaker produces 300 pairs/year but only needs 6 for his family).\n2. **Agriculture Requirements**: Agriculture can sustain itself with a smaller market, as each laborer needs to sell to only a few families.\n3. **Egypt and Indostan**: Both had limited foreign markets but compensated with extensive inland navigations, making agriculture more viable than manufacturing.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the stated policy emphasis in both regions. The context specifies that ancient Egypt and Indostan favored agriculture over manufacturing through government policy. Step 2: Observe the revenue structure. Both regions derived most sovereign revenue from land taxes/rents, incentivizing agricultural focus over manufacturing. Step 3: Infer market size implications. Reliance on land taxes implies localized, subsistence-oriented markets (smaller market size) rather than diversified manufacturing requiring larger consumer bases or trade networks. Step 4: Establish causality. Smaller market sizes in these economies made agriculture more viable (as it requires less complex demand structures) and manufacturing less viable (which thrives with larger markets for specialization and exchange). Step 5: Contrast with counterexamples. The text contrasts this with Europe's later artificer-focused policies, suggesting larger market sizes there enabled manufacturing viability. Thus, market size acts as a constraint: smaller markets favor agriculture (Egypt/Indostan model), while larger markets enable manufacturing.\n\nQID: textbook-112-22-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-22-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses the relationship between market size and the viability of manufacturing versus agriculture, using the examples of ancient Egypt and Indostan. The steps provided align well with the gold answer, detailing how smaller market sizes favor agriculture and larger markets enable manufacturing.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-22-0-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses the relationship between market size and the viability of manufacturing versus agriculture, using the examples of ancient Egypt and Indostan. The steps provided align well with the gold answer, detailing how smaller market sizes favor agriculture and larger markets enable manufacturing."}, "llm_echoed_qid": "textbook-112-22-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses the relationship between market size and the viability of manufacturing versus agriculture, using the examples of ancient Egypt and Indostan. The steps provided align well with the gold answer, detailing how smaller market sizes favor agriculture and larger markets enable manufacturing."}
{"script_processing_qid": "textbook-111-8-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Critically evaluate the claim that dollarization is the 'most reliable method' to fix an exchange rate.\n\nGOLD_ANSWER:\n1. **Pros**: Eliminates speculative attacks and devaluation risk entirely.\n2. **Cons**: Loss of independent monetary policy, lender-of-last-resort capacity, and seigniorage.\n3. **Counterexamples**: Ecuador’s post-2000 dollarization faced banking crises due to lack of lender-of-last-resort.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the claim. Dollarization (adopting a foreign currency) is argued to be the 'most reliable method' to fix an exchange rate. The context compares this to currency boards, which require full foreign reserve backing.\n\nStep 2: Analyze currency boards. The context states that currency boards prevent speculative attacks by holding sufficient foreign reserves (e.g., one USD per peso). This eliminates the risk of reserve depletion and forced devaluation, making fixed rates credible.\n\nStep 3: Evaluate dollarization. Unlike currency boards, dollarization removes the domestic currency entirely. This eliminates speculative attacks outright, as there is no peso to sell or central bank to drain reserves.\n\nStep 4: Compare reliability. Both methods prevent reserve depletion, but dollarization is more irreversible (no domestic currency to reintroduce) and eliminates all currency-related policy discretion. This makes it structurally harder to abandon, enhancing reliability.\n\nStep 5: Critical counterpoints. However, dollarization sacrifices monetary policy autonomy and seigniorage revenue entirely. Currency boards retain some flexibility (e.g., adjusting reserve ratios) while still preventing attacks. Thus, 'most reliable' depends on priorities: absolute fixity (dollarization) vs. limited flexibility with high credibility (currency boards).\n\nConclusion: While dollarization is highly reliable due to eliminating currency risk, the claim overlooks trade-offs. Currency boards can also achieve reliability with full reserve backing, making the 'most reliable' designation context-dependent rather than universally true.\n\nQID: textbook-111-8-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-111-8-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly evaluates the claim by comparing dollarization to currency boards, discussing the pros and cons of each, and concluding that the 'most reliable' designation depends on context. This aligns well with the gold answer, which also highlights the trade-offs and counterexamples.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-8-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly evaluates the claim by comparing dollarization to currency boards, discussing the pros and cons of each, and concluding that the 'most reliable' designation depends on context. This aligns well with the gold answer, which also highlights the trade-offs and counterexamples."}, "llm_echoed_qid": "textbook-111-8-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly evaluates the claim by comparing dollarization to currency boards, discussing the pros and cons of each, and concluding that the 'most reliable' designation depends on context. This aligns well with the gold answer, which also highlights the trade-offs and counterexamples."}
{"script_processing_qid": "textbook-122-17-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Compare the asymptotic variance matrix of the probit estimator with that of the OLS estimator. What are the key differences?\n\nGOLD_ANSWER:\n1.  **OLS Variance**: The asymptotic variance of the OLS estimator is $\\sigma^2 (\\pmb{X}'\\pmb{X})^{-1}$, where $\\sigma^2$ is the error variance.\n2.  **Probit Variance**: The probit variance is $\\left(\\sum_{i=1}^n \\frac{[g(\\pmb{x}_i\\hat{\\pmb{\\beta}})]^2 \\pmb{x}_i' \\pmb{x}_i}{G(\\pmb{x}_i\\hat{\\pmb{\\beta}})[1 - G(\\pmb{x}_i\\hat{\\pmb{\\beta}})]}\\right)^{-1}$.\n3.  **Key Differences**:\n    - The probit variance does not include a separate $\\sigma^2$ term because the variance of $y$ is determined by $G(\\cdot)$.\n    - The probit variance accounts for heteroskedasticity through the $g(\\cdot)$ and $G(\\cdot)$ terms, whereas OLS assumes homoskedasticity unless robust standard errors are used.\n\nCANDIDATE_ANSWER:\nStep 1: Probit Asymptotic Variance Structure\nThe probit estimator's asymptotic variance matrix is derived from maximum likelihood theory and takes the form:\n$$\n\\left(\\sum_{i=1}^n \\frac{[g(\\boldsymbol{x}_i\\hat{\\boldsymbol{\\beta}})]^2\\boldsymbol{x}_i'\\boldsymbol{x}_i}{G(\\boldsymbol{x}_i\\hat{\\boldsymbol{\\beta}})[1-G(\\boldsymbol{x}_i\\hat{\\boldsymbol{\\beta}})]}\\right)^{-1}\n$$\nwhere $g(\\cdot)$ is the normal PDF and $G(\\cdot)$ is the normal CDF.\n\nStep 2: OLS Asymptotic Variance Structure\nThe OLS estimator's asymptotic variance matrix under homoskedasticity is:\n$$\n\\sigma^2(\\boldsymbol{X}'\\boldsymbol{X})^{-1} = \\sigma^2\\left(\\sum_{i=1}^n \\boldsymbol{x}_i'\\boldsymbol{x}_i\\right)^{-1}\n$$\n\nKey Differences:\n1. **Weighting Mechanism**:\n   - Probit: Weights observations by $\\frac{g(\\cdot)^2}{G(\\cdot)(1-G(\\cdot))}$, explicitly accounting for:\n     - Nonlinearity through $g(\\cdot)$ (density function)\n     - Heteroskedasticity through $G(\\cdot)(1-G(\\cdot))$ (Bernoulli variance)\n   - OLS: Uses uniform weighting (all observations equally weighted)\n\n2. **Parameter Dependence**:\n   - Probit variance depends on estimated parameters $\\hat{\\boldsymbol{\\beta}}$ through $G(\\cdot)$ and $g(\\cdot)$\n   - OLS variance depends only on design matrix $\\boldsymbol{X}$ and scalar $\\sigma^2$\n\n3. **Efficiency**:\n   - Probit achieves asymptotic efficiency (Cramér-Rao bound) when model is correctly specified\n   - OLS applied to binary outcomes (linear probability model) is inefficient due to ignored heteroskedasticity\n\n4. **Structural Form**:\n   - Probit: Inverse of weighted sum of outer products\n   - OLS: Scaled inverse of unweighted sum of outer products\n\nQID: textbook-122-17-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-17-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately describes the asymptotic variance matrices of both the probit and OLS estimators, highlighting key differences such as the weighting mechanism, parameter dependence, efficiency, and structural form. The explanation aligns well with the gold answer, providing a comprehensive and correct comparison.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-17-0-2", "category": "CORRECT", "explanation": "The candidate answer accurately describes the asymptotic variance matrices of both the probit and OLS estimators, highlighting key differences such as the weighting mechanism, parameter dependence, efficiency, and structural form. The explanation aligns well with the gold answer, providing a comprehensive and correct comparison."}, "llm_echoed_qid": "textbook-122-17-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately describes the asymptotic variance matrices of both the probit and OLS estimators, highlighting key differences such as the weighting mechanism, parameter dependence, efficiency, and structural form. The explanation aligns well with the gold answer, providing a comprehensive and correct comparison."}
{"script_processing_qid": "textbook-119-4-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the log-lin model from the compound interest formula $Y_{t}=Y_{0}(1+r)^{t}$ and explain how $\\beta_{2}$ relates to the growth rate $r$.\n\nGOLD_ANSWER:\n1. **Compound Interest Formula**: Start with $Y_{t}=Y_{0}(1+r)^{t}$.\n2. **Take Natural Log**: Apply the natural logarithm: $\\ln Y_{t}=\\ln Y_{0}+t\\ln(1+r)$.\n3. **Define Parameters**: Let $\\beta_{1}=\\ln Y_{0}$ and $\\beta_{2}=\\ln(1+r)$, resulting in $\\ln Y_{t}=\\beta_{1}+\\beta_{2}t$.\n4. **Growth Rate**: The growth rate $r$ can be recovered as $r=e^{\\beta_{2}}-1$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the compound interest formula:\nYₜ = Y₀(1 + r)ᵗ\n\nStep 2: Take natural logarithm of both sides:\nln Yₜ = ln[Y₀(1 + r)ᵗ]\n\nStep 3: Apply logarithm properties:\nln Yₜ = ln Y₀ + t·ln(1 + r)\n\nStep 4: Substitute coefficients:\nLet β₁ = ln Y₀\nLet β₂ = ln(1 + r)\n\nStep 5: Formulate log-lin model:\nln Yₜ = β₁ + β₂t + uₜ\n\nStep 6: Relate β₂ to growth rate r:\nβ₂ = ln(1 + r)\n\nStep 7: Solve for r:\nr = e^{β₂} − 1\n\nStep 8: For small β₂ values (common in economics), use approximation:\nβ₂ ≈ r\n\nThus, β₂ represents the continuous growth rate. The quarterly growth rate example (0.00743 → 0.743%) demonstrates this relationship where β₂ directly approximates the percentage growth rate when multiplied by 100.\n\nQID: textbook-119-4-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-4-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the log-lin model from the compound interest formula and accurately explains the relationship between β₂ and the growth rate r, including the approximation for small values of β₂.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-4-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the log-lin model from the compound interest formula and accurately explains the relationship between β₂ and the growth rate r, including the approximation for small values of β₂."}, "llm_echoed_qid": "textbook-119-4-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the log-lin model from the compound interest formula and accurately explains the relationship between β₂ and the growth rate r, including the approximation for small values of β₂."}
{"script_processing_qid": "textbook-118-20-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare and contrast Monte Carlo tests and bootstrap tests, focusing on their assumptions and applicability.\n\nGOLD_ANSWER:\n**Monte Carlo Tests**:\n1. **Assumption**: The DGP is fully known under the null.\n2. **Applicability**: Exact for pivotal statistics; requires no estimation.\n\n**Bootstrap Tests**:\n1. **Assumption**: The DGP is estimated from the data under the null.\n2. **Applicability**: Approximate for asymptotically pivotal statistics; flexible but sensitive to DGP estimation error.\n\nCANDIDATE_ANSWER:\nStep 1: Core Methodology\nMonte Carlo tests and bootstrap tests both use simulation to estimate $P$ values, but differ in assumptions about the test statistic. Monte Carlo assumes the statistic is pivotal (distribution invariant under null DGPs), enabling simulation under any chosen null DGP. Bootstrap tests relax this assumption, relying on resampling from observed data to approximate the sampling distribution.\n\nStep 2: Assumptions\n- Monte Carlo: Requires a pivotal test statistic. Leverages the Fundamental Theorem of Statistics to ensure convergence of the empirical distribution to the true CDF under the null.\n- Bootstrap: Does not require pivotality. Instead, assumes the resampling procedure (parametric/nonparametric) adequately approximates the data-generating process, even under non-pivotal statistics.\n\nStep 3: Applicability\n- Monte Carlo: Exact $P$ values when the statistic is pivotal and simulations use the correct null DGP. Suitable for parametric tests with known null distributions.\n- Bootstrap: More flexible for non-pivotal statistics or complex DGPs. Accuracy depends on resampling method and sample size, often requiring larger $B$ than Monte Carlo for comparable precision.\n\nStep 4: Limitations\n- Monte Carlo: Fails if the statistic is not pivotal, as the simulated distribution no longer matches the true null distribution.\n- Bootstrap: May introduce approximation errors if the resampling scheme misrepresents the DGP, especially under model misspecification.\n\nQID: textbook-118-20-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-20-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately compares and contrasts Monte Carlo and bootstrap tests, detailing their assumptions (pivotality vs. resampling approximation) and applicability (exact vs. flexible but approximate). It aligns well with the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-20-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately compares and contrasts Monte Carlo and bootstrap tests, detailing their assumptions (pivotality vs. resampling approximation) and applicability (exact vs. flexible but approximate). It aligns well with the gold answer's key points."}, "llm_echoed_qid": "textbook-118-20-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately compares and contrasts Monte Carlo and bootstrap tests, detailing their assumptions (pivotality vs. resampling approximation) and applicability (exact vs. flexible but approximate). It aligns well with the gold answer's key points."}
{"script_processing_qid": "textbook-118-8-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Interpret the economic implication of the just-identified case in simultaneous equations models.\n\nGOLD_ANSWER:\nIn the just-identified case, the number of instruments exactly matches the number of endogenous variables, leaving no overidentifying restrictions to test. This implies:\n1. **No efficiency gain**: System estimation cannot improve upon equation-by-equation methods.\n2. **Model flexibility**: The structural form is exactly mapped to the reduced form, leaving no room for specification tests (e.g., Sargan test).\n3. **Practical limitation**: Just-identification relies on strong assumptions about instrument exogeneity and relevance, which are often untestable.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the just-identified case context\nIn simultaneous equations models, the just-identified scenario occurs when the number of valid instruments (in matrix W) equals the number of endogenous regressors. This is reflected in the estimating equations simplifying to W⊤(yₘ - Xₘβₘ) = 0 for each equation m (1 ≤ m ≤ g).\n\nStep 2: Recognize the decoupling effect\nWhen all equations are just-identified, the system estimation collapses to solving separate moment conditions for each equation. This means there is no need to account for cross-equation correlations or use system-wide estimators like 3SLS.\n\nStep 3: Identify the equivalence to single-equation IV\nThe equations W⊤(yₘ - Xₘβₘ) = 0 correspond exactly to the simple IV estimator conditions for each individual equation. This shows that system estimation becomes equivalent to estimating each equation independently via IV.\n\nStep 4: Derive economic implications\n1. Estimation efficiency: No efficiency gains can be achieved through system estimation methods (e.g., SUR/GMM) since equations are statistically independent in this special case (implied by Σ being diagonal).\n2. Empirical simplicity: Researchers can use single-equation IV methods without losing theoretical validity.\n3. Identifiability conservation: The just-identification condition ensures unique parameter solutions exist without requiring additional restrictions or over-identification tests.\n\nStep 5: Practical significance\nThis implies that in policy analysis or structural modeling contexts where all equations are just-identified:\n- Computational complexity decreases significantly\n- Parameter estimates remain consistent without requiring cross-equation error structure estimation\n- Results become directly comparable to simple IV implementations in single-equation contexts.\n\nQID: textbook-118-8-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-8-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the just-identified case in simultaneous equations models, detailing the decoupling effect, equivalence to single-equation IV, and deriving economic implications such as estimation efficiency, empirical simplicity, and identifiability conservation. This aligns well with the gold answer's points on no efficiency gain, model flexibility, and practical limitations.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-8-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the just-identified case in simultaneous equations models, detailing the decoupling effect, equivalence to single-equation IV, and deriving economic implications such as estimation efficiency, empirical simplicity, and identifiability conservation. This aligns well with the gold answer's points on no efficiency gain, model flexibility, and practical limitations."}, "llm_echoed_qid": "textbook-118-8-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the just-identified case in simultaneous equations models, detailing the decoupling effect, equivalence to single-equation IV, and deriving economic implications such as estimation efficiency, empirical simplicity, and identifiability conservation. This aligns well with the gold answer's points on no efficiency gain, model flexibility, and practical limitations."}
{"script_processing_qid": "textbook-115-12-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the key differences between the proxy variable approach and the multiple indicator IV approach for addressing omitted variable bias.\n\nGOLD_ANSWER:\n1. **Proxy Variable Approach**: Assumes a proxy $z_{1}$ such that $q=\\theta_{0}+\\theta_{1}z_{1}+r_{1}$, where $r_{1}$ is uncorrelated with $\\mathbf{x}$. OLS is used, but consistency relies on $r_{1}$ being uncorrelated with $\\mathbf{x}$.\n2. **Multiple Indicator IV Approach**: Uses two indicators $q_{1}$ and $q_{2}$ of $q$, where $q_{1}$ is plugged into the model and $q_{2}$ serves as an IV. This approach does not require $\\mathbf{x}$ to be uncorrelated with $q$ and leverages the correlation between $q_{1}$ and $q_{2}$.\n3. **Key Difference**: The proxy approach relies on OLS and strict exogeneity of the proxy, while the IV approach uses instrumental variables to handle endogeneity.\n\nCANDIDATE_ANSWER:\nStep 1: **Fundamental Approach**\n- Proxy Variable Method: Uses OLS after substituting the omitted variable q with a single observable proxy (z₁). Assumes q = θ₀ + θ₁z₁ + r₁ where r₁ is uncorrelated with z₁ and all x variables.\n- Multiple Indicator IV Method: Uses IV estimation with ≥2 indicators (q₁, q₂) of q. Models q₁ = δ₀ + δ₁q + a₁ where a₁ is uncorrelated with q and x. Requires instruments for the endogenous q₁.\n\nStep 2: **Estimation Technique**\n- Proxy: Relies on OLS after proxy substitution (implicitly assumes measurement error in proxy is uncorrelated with x).\n- IV: Uses IV regression (e.g., with q₂ as an instrument for q₁) because substituting q₁ introduces measurement error (a₁) into the composite error term (v − γ₁a₁), creating endogeneity.\n\nStep 3: **Key Assumptions**\n- Proxy: Requires proxy z₁ to fully capture q (q = θ₀ + θ₁z₁ + r₁) with r₁ uncorrelated with x.\n- IV: Requires (1) indicators satisfy redundancy (no direct effect on y), (2) indicators follow measurement equations like q₁ = δ₀ + δ₁q + a₁ with Cov(q, a₁) = 0 and Cov(x, a₁) = 0, and (3) Cov(a₁, a₂) = 0 for distinct indicators.\n\nStep 4: **Identification Requirement**\n- Proxy: No explicit exclusion restriction needed beyond proxy validity.\n- IV: Requires at least two indicators (q₁, q₂) where q₂ serves as an instrument for q₁ (must satisfy relevance and exclusion restriction).\n\nStep 5: **Error Term Handling**\n- Proxy: Subsumes residual variation in q (via r₁) into the error term but retains OLS consistency under proxy assumptions.\n- IV: Explicitly accounts for measurement error (a₁ in q₁) by instrumenting to break correlation between q₁ and the composite error term.\n\nQID: textbook-115-12-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-12-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the key differences between the proxy variable approach and the multiple indicator IV approach, covering fundamental approach, estimation technique, key assumptions, identification requirements, and error term handling, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-12-0-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the key differences between the proxy variable approach and the multiple indicator IV approach, covering fundamental approach, estimation technique, key assumptions, identification requirements, and error term handling, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-115-12-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the key differences between the proxy variable approach and the multiple indicator IV approach, covering fundamental approach, estimation technique, key assumptions, identification requirements, and error term handling, aligning well with the gold answer."}
{"script_processing_qid": "textbook-115-7-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n7. Propose a two-step estimator for the binary response model with sample selection and describe how to test for selection bias ($H_0: \\rho_1 = 0$).\n\nGOLD_ANSWER:\n7. **Two-Step Estimator**: (1) Estimate $\\delta_2$ by probit of $y_2$ on $\\mathbf{x}$. (2) Estimate $\\beta_1$ and $\\rho_1$ by partial MLE using the selected sample, with likelihood contributions from $\\mathrm{P}(y_1 | \\mathbf{x}, y_2 = 1)$. Test $H_0: \\rho_1 = 0$ using a $t$-statistic on $\\hat{\\rho}_1$ or a likelihood ratio test comparing the full and restricted models.\n\nCANDIDATE_ANSWER:\nStep 1: First-stage Probit Estimation\nEstimate the selection equation using a probit model: \n\\(y_2 = 1[\\mathbf{x}\\pmb{\\delta}_2 + v_2 > 0]\\).\nObtain estimates \\(\\hat{\\pmb{\\delta}}_2\\) and compute the inverse Mills ratio (IMR) for observations with \\(y_2 = 1\\):\n\\(\\hat{\\lambda}_i = \\frac{\\phi(\\mathbf{x}_i\\hat{\\pmb{\\delta}}_2)}{\\Phi(\\mathbf{x}_i\\hat{\\pmb{\\delta}}_2)}\\),\nwhere \\(\\phi\\) and \\(\\Phi\\) are the standard normal PDF and CDF.\n\nStep 2: Second-stage Augmented Probit\nEstimate the outcome equation using the selected sample (\\(y_2 = 1\\)) via a probit model:\n\\(y_1 = 1[\\mathbf{x}_1\\pmb{\\beta}_1 + \\rho_1 \\hat{\\lambda}_i + \\epsilon_i > 0]\\),\nwhere \\(\\rho_1\\) captures the correlation between \\(u_1\\) and \\(v_2\\).\n\nTesting for Selection Bias\nTo test \\(H_0: \\rho_1 = 0\\), perform a Wald test on the coefficient of \\(\\hat{\\lambda}_i\\) in the second stage. If \\(\\rho_1\\) is statistically significant, reject \\(H_0\\) and conclude there is selection bias.\n\nQID: textbook-115-7-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-7-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately describes the two-step estimator for the binary response model with sample selection, including the computation of the inverse Mills ratio and the second-stage augmented probit. It also correctly specifies the Wald test for selection bias, matching the gold answer's methodology.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-7-1-2", "category": "CORRECT", "explanation": "The candidate answer accurately describes the two-step estimator for the binary response model with sample selection, including the computation of the inverse Mills ratio and the second-stage augmented probit. It also correctly specifies the Wald test for selection bias, matching the gold answer's methodology."}, "llm_echoed_qid": "textbook-115-7-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately describes the two-step estimator for the binary response model with sample selection, including the computation of the inverse Mills ratio and the second-stage augmented probit. It also correctly specifies the Wald test for selection bias, matching the gold answer's methodology."}
{"script_processing_qid": "textbook-66-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the symmetric equilibrium conditions for the production function $y = A k^\\alpha n^\\phi Y^\\gamma Y_{-1}^\\varepsilon$ in a representative agent framework.\n\nGOLD_ANSWER:\n1. **Symmetric Equilibrium**: Impose $y = Y$ and $k = K$, $n = N$ for all firms.\n2. **Production Function**: Becomes $Y = A K^\\alpha N^\\phi Y^{\\gamma + \\varepsilon}$.\n3. **Simplify**: $Y^{1 - \\gamma - \\varepsilon} = A K^\\alpha N^\\phi$.\n4. **Log-Linearize**: For estimation, express as:\n   $$ (1 - \\gamma - \\varepsilon) \\ln Y = \\ln A + \\alpha \\ln K + \\phi \\ln N. $$\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that in symmetric equilibrium, all firms are identical. This implies aggregate output Y equals individual firm output y when there's a continuum of firms (Y = y).\n\nStep 2: Substitute Y with y in the production function:\ny = A k^α n^φ y^γ Y_{-1}^ε\n\nStep 3: Solve for y to express output in terms of inputs and lagged aggregate output:\ny^{1-γ} = A k^α n^φ Y_{-1}^ε\n=> y = [A k^α n^φ Y_{-1}^ε]^{1/(1-γ)}\n\nStep 4: Derive firm-level FOCs for profit maximization:\n- Capital FOC: ∂y/∂k = α(y/k) = r\n- Labor FOC: ∂y/∂n = φ(y/n) = w\n\nStep 5: Combine these results to form the symmetric equilibrium conditions:\n1. Output relationship: y = [A k^α n^φ Y_{-1}^ε]^{1/(1-γ)}\n2. Capital return: r = α(y/k)\n3. Wage rate: w = φ(y/n)\n\nThese conditions simultaneously determine equilibrium output, capital returns, and wage rates given inputs and the lagged aggregate output.\n\nQID: textbook-66-0-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-66-0-1-0\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the symmetric equilibrium conditions and derives the production function, but misses the simplification step involving lagged output (Y_{-1}) and does not log-linearize the equation as shown in the gold answer. The FOCs are correctly derived but the final equilibrium conditions are not fully simplified.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-66-0-1-0", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the symmetric equilibrium conditions and derives the production function, but misses the simplification step involving lagged output (Y_{-1}) and does not log-linearize the equation as shown in the gold answer. The FOCs are correctly derived but the final equilibrium conditions are not fully simplified."}, "llm_echoed_qid": "textbook-66-0-1-0", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the symmetric equilibrium conditions and derives the production function, but misses the simplification step involving lagged output (Y_{-1}) and does not log-linearize the equation as shown in the gold answer. The FOCs are correctly derived but the final equilibrium conditions are not fully simplified."}
{"script_processing_qid": "textbook-118-6-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Prove that the LIML estimator is invariant to reparametrization of the model, using the demand-supply example (12.97)-(12.99).\n\nGOLD_ANSWER:\n1. **Reparametrization**: Transform the supply equation (12.98) to (12.99) with $\\gamma_{s}' = 1/\\gamma_{s}$ and $\\beta_{s}' = -\\beta_{s}/\\gamma_{s}$.\n2. **Invariance**: ML estimates satisfy $\\hat{\\gamma}_{s}' = 1/\\hat{\\gamma}_{s}$ and $\\hat{\\beta}_{s}' = -\\hat{\\beta}_{s}/\\hat{\\gamma}_{s}$.\n3. **Implication**: The LIML estimates of the demand equation (12.97) remain unchanged under this reparametrization.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that the LIML estimator minimizes the ratio κ = [(y - Yβ₂)ᵀM_Z(y - Yβ₂)] / [(y - Yβ₂)ᵀM_W(y - Yβ₂)], where M_Z and M_W are projection matrices.\\n\\nStep 2: Under reparametrization (e.g., rewriting the structural equations (12.97)-(12.99) via linear transformations), let the transformed parameters be β̃₂ = Aβ₂ + b for nonsingular matrix A and vector b. The residuals (y - Yβ₂) become (y - Y(A⁻¹β̃₂ - A⁻¹b)).\\n\\nStep 3: Substitute transformed residuals into κ. The numerator and denominator scale equivalently because M_Z and M_W are projection matrices (idempotent and linear). Ratios of quadratic forms are invariant to nonsingular linear transformations.\\n\\nStep 4: Since minimization of κ depends only on the ratio of quadratic forms (not their absolute magnitudes), the minimizing β̃₂ corresponds to the original β₂ under the inverse transformation. Thus, LIML estimates are invariant to reparametrization.\n\nQID: textbook-118-6-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-6-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly demonstrates the invariance of the LIML estimator to reparametrization by showing that the ratio κ remains unchanged under linear transformations of the parameters, aligning with the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-6-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly demonstrates the invariance of the LIML estimator to reparametrization by showing that the ratio κ remains unchanged under linear transformations of the parameters, aligning with the gold answer's key points."}, "llm_echoed_qid": "textbook-118-6-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly demonstrates the invariance of the LIML estimator to reparametrization by showing that the ratio κ remains unchanged under linear transformations of the parameters, aligning with the gold answer's key points."}
{"script_processing_qid": "textbook-115-17-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the robust variance matrix estimator for the random effects model when Assumption RE.3 fails, starting from the general form $\\hat{\\mathbf{V}} = N^{-1}\\sum_{i=1}^N \\hat{\\mathbf{v}}_i \\hat{\\mathbf{v}}_i^{\\prime}$.\n\nGOLD_ANSWER:\n1. **Residuals**: Compute $\\hat{\\mathbf{v}}_i = \\mathbf{y}_i - \\mathbf{X}_i \\hat{\\pmb{\\beta}}_{RE}$ for each $i$.\n2. **Variance Matrix**: Construct $\\hat{\\mathbf{V}} = N^{-1}\\sum_{i=1}^N \\mathbf{X}_i^{\\prime} \\hat{\\mathbf{v}}_i \\hat{\\mathbf{v}}_i^{\\prime} \\mathbf{X}_i$.\n3. **Robust Standard Errors**: Take the square roots of the diagonal elements of $\\hat{\\mathbf{V}}$ for standard errors.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that under Assumption RE.3 failure, the robust variance matrix estimator must account for potential heteroskedasticity and/or serial correlation in the idiosyncratic errors. The general form of the robust variance matrix is given by $\\hat{\\mathbf{V}} = N^{-1}\\sum_{i=1}^N \\hat{\\mathbf{v}}_i \\hat{\\mathbf{v}}_i^{\\prime}$, where $\\hat{\\mathbf{v}}_i$ are the residuals.\n\nStep 2: Use the RE residuals $\\hat{\\mathbf{v}}_i = \\mathbf{y}_i - \\mathbf{X}_i\\hat{\\boldsymbol{\\beta}}_{RE}$ for each cross-sectional unit $i$. These residuals capture both the unobserved heterogeneity ($c_i$) and idiosyncratic errors ($u_{it}$).\n\nStep 3: Compute the outer product of the residual vector for each unit $i$, i.e., $\\hat{\\mathbf{v}}_i \\hat{\\mathbf{v}}_i^{\\prime}$. This preserves potential within-cluster (individual) correlation and heteroskedasticity.\n\nStep 4: Sum the outer products across all $N$ units and scale by $N^{-1}$ to obtain the robust variance matrix estimator: $\\hat{\\mathbf{V}} = N^{-1}\\sum_{i=1}^N \\hat{\\mathbf{v}}_i \\hat{\\mathbf{v}}_i^{\\prime}$. This estimator is robust because it does not impose homoskedasticity or a specific serial correlation structure on $\\mathbf{v}_i$.\n\nQID: textbook-115-17-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-17-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the robust variance matrix estimator for the random effects model when Assumption RE.3 fails. The steps align with the gold answer, including computing residuals, constructing the outer product, and scaling by N^{-1} to obtain the robust variance matrix estimator.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-17-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the robust variance matrix estimator for the random effects model when Assumption RE.3 fails. The steps align with the gold answer, including computing residuals, constructing the outer product, and scaling by N^{-1} to obtain the robust variance matrix estimator."}, "llm_echoed_qid": "textbook-115-17-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the robust variance matrix estimator for the random effects model when Assumption RE.3 fails. The steps align with the gold answer, including computing residuals, constructing the outer product, and scaling by N^{-1} to obtain the robust variance matrix estimator."}
{"script_processing_qid": "textbook-123-10-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. What is the role of sunspot variables in determining exchange rates in this model?\n\nGOLD_ANSWER:\nSunspot variables introduce multiple equilibria by coordinating household beliefs about the exchange rate. Depending on the sunspot realization, households may believe the exchange rate will be $0$, $1$, or $\\infty$, leading to different equilibrium outcomes.\n\nCANDIDATE_ANSWER:\nStep 1: The fiscal theory of the price level (FTPL) is presented as a mechanism to select equilibria in monetary models where multiple equilibria could exist. Without such a selection device, exchange rates may be indeterminate.\nStep 2: In scenarios where exchange rate determination is not uniquely pinned down by fundamentals (e.g., fiscal/monetary policy), sunspot variables act as extrinsic coordination devices for agents' expectations.\nStep 3: The model explicitly allows sunspot realizations (with uniform probability 1/3) to influence expectations, enabling self-fulfilling beliefs about exchange rate paths that would otherwise be mathematically possible but economically unexplained.\nStep 4: Thus, sunspot variables resolve indeterminacy by allowing agents to coordinate on one equilibrium path from the continuum of possible equilibria, even though these variables have no intrinsic economic significance.\n\nQID: textbook-123-10-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-10-1-1\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate answer correctly identifies the role of sunspot variables in coordinating expectations and resolving indeterminacy in exchange rates. However, it introduces additional concepts like the fiscal theory of the price level (FTPL) and a uniform probability of 1/3, which are not mentioned in the gold answer. The reasoning thus diverges from the gold answer's focus on specific exchange rate beliefs ($0$, $1$, or $\\infty$).\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-10-1-1", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate answer correctly identifies the role of sunspot variables in coordinating expectations and resolving indeterminacy in exchange rates. However, it introduces additional concepts like the fiscal theory of the price level (FTPL) and a uniform probability of 1/3, which are not mentioned in the gold answer. The reasoning thus diverges from the gold answer's focus on specific exchange rate beliefs ($0$, $1$, or $\\infty$)."}, "llm_echoed_qid": "textbook-123-10-1-1", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate answer correctly identifies the role of sunspot variables in coordinating expectations and resolving indeterminacy in exchange rates. However, it introduces additional concepts like the fiscal theory of the price level (FTPL) and a uniform probability of 1/3, which are not mentioned in the gold answer. The reasoning thus diverges from the gold answer's focus on specific exchange rate beliefs ($0$, $1$, or $\\infty$)."}
{"script_processing_qid": "textbook-95-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Prove that the dynamic response of $Z_{t}$ to a monetary policy shock is invariant to the choice of $A_{0} \\in \\mathcal{Q}_{V} \\cap \\mathcal{Q}_{\\tau}$. Use the property $\\tilde{A}_{0}^{-1} = A_{0}^{-1}W^{\\prime}$ where $W$ is orthonormal.\n\nGOLD_ANSWER:\n1. **Given**: $\\tilde{A}_{0} = WA_{0}$ for orthonormal $W$ in (4.2).\n2. **Invert**: $\\tilde{A}_{0}^{-1} = A_{0}^{-1}W^{\\prime}$.\n3. **Key Observation**: The $(k_{1}+1)$th column of $W^{\\prime}$ is $[0, 1, 0]^{\\prime}$ due to block-diagonal structure.\n4. **Implication**: The $(k_{1}+1)$th columns of $\\tilde{A}_{0}^{-1}$ and $A_{0}^{-1}$ are identical.\n5. **Conclusion**: By (3.6), impulse responses of $Z_{t}$ to the policy shock (corresponding to this column) are invariant.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that monetary policy shock identification under recursiveness requires A₀'s structure with zeros in blocks (k₁×1) and (1×k₂). This creates a block-triangular form separating policy shocks from other variables.\n\nStep 2: Note that all valid A₀ ∈ Q_V ∩ Q_τ share these zero restrictions. The orthonormal matrix W in the relation \\(\\tilde{A}_0^{-1} = A_0^{-1}W'\\\\) must preserve these restrictions.\n\nStep 3: Observe that W must be block-diagonal with identity in the (k₁+1)th position (policy shock column) to maintain a₁₁/a₂₂/a₃₃ blocks' zero constraints. This forces W to have the form:\n\\[ W = \\begin{bmatrix} W_1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & W_3 \\end{bmatrix} \\]\nwhere W₁ (k₁×k₁) and W₃ (k₂×k₂) are orthonormal.\n\nStep 4: The monetary policy shock corresponds to the fixed (k₁+1)th column in A₀^{-1}. When multiplied by W', this column remains unchanged because the middle entry of W is 1 and other blocks have zero cross-multipliers.\n\nStep 5: Since impulse responses are derived from \\(\\Gamma_i A_0^{-1}\\) (MA representation coefficients multiplied by A₀^{-1}), the (k₁+1)th column of these products – representing policy shock responses – remains identical across all A₀ ∈ Q_V ∩ Q_τ due to W's structure.\n\nConclusion: The dynamic response invariance follows because permissible W matrices rotate other shock columns but leave the monetary policy shock column unchanged in A₀^{-1}.\n\nQID: textbook-95-0-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-95-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the logical steps to prove the invariance of the dynamic response of \\(Z_{t}\\) to a monetary policy shock. The explanation aligns with the gold answer, detailing the role of the orthonormal matrix \\(W\\) and its block-diagonal structure in preserving the policy shock column in \\(A_{0}^{-1}\\).\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-95-0-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the logical steps to prove the invariance of the dynamic response of \\(Z_{t}\\) to a monetary policy shock. The explanation aligns with the gold answer, detailing the role of the orthonormal matrix \\(W\\) and its block-diagonal structure in preserving the policy shock column in \\(A_{0}^{-1}\\)."}, "llm_echoed_qid": "textbook-95-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the logical steps to prove the invariance of the dynamic response of \\(Z_{t}\\) to a monetary policy shock. The explanation aligns with the gold answer, detailing the role of the orthonormal matrix \\(W\\) and its block-diagonal structure in preserving the policy shock column in \\(A_{0}^{-1}\\)."}
{"script_processing_qid": "textbook-112-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the economic rationale behind Adam Smith's argument that the freest importation of foreign cattle would primarily affect breeding countries rather than feeding or fattening countries.\n\nGOLD_ANSWER:\n1.  **Breeding vs. Feeding Countries**: Breeding countries are generally uncultivated and rely on the high price of lean cattle, which augments the value of uncultivated land. Feeding countries are highly improved and benefit from lower prices of lean cattle.\n2.  **Transport Costs**: Importing cattle involves significant transport costs, making it feasible only for lean cattle, which can be driven long distances. Fat cattle cannot be transported as easily.\n3.  **Market Impact**: Lean cattle imports reduce the price of lean cattle, benefiting feeding countries (lower input costs) but harming breeding countries (lower revenue).\n4.  **Empirical Evidence**: The small number of Irish cattle imported and the sustained high price of lean cattle support Smith's argument.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the structural roles in cattle production. Breeding countries raise cattle from birth, while feeding/fattening countries import cattle to prepare them for final sale. \nStep 2: Free importation increases foreign cattle supply, lowering domestic cattle prices. Breeding countries lose pricing power to exploit growing domestic demand (due to population/improvement). \nStep 3: Breeders cannot raise prices to \"exorbitant heights\" because foreign imports act as substitutes, redistributing profits from breeders to feeders. \nStep 4: Feeding/fattening countries benefit from cheaper inputs (foreign cattle), aligning with Smith’s principle of buying cheap to sell dear. Breeders bear the burden as their strategic position in the supply chain is undermined by foreign competition.\n\nQID: textbook-112-1-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-1-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately captures Adam Smith's argument by detailing the structural roles in cattle production, the impact of free importation on prices, and the redistribution of profits from breeders to feeders, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-1-0-0", "category": "CORRECT", "explanation": "The candidate answer accurately captures Adam Smith's argument by detailing the structural roles in cattle production, the impact of free importation on prices, and the redistribution of profits from breeders to feeders, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-112-1-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately captures Adam Smith's argument by detailing the structural roles in cattle production, the impact of free importation on prices, and the redistribution of profits from breeders to feeders, aligning well with the gold answer."}
{"script_processing_qid": "textbook-123-19-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the role of unobservable wage offers in Exercise 21.3’s moral hazard problem.\n\nGOLD_ANSWER:\n1. **Moral Hazard**: Unobservable offers prevent the planner from conditioning benefits on wage draws. \n2. **Implication**: Incentives must rely solely on search effort $a$, complicating optimal contract design.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that unobservable wage offers introduce asymmetric information in the moral hazard problem. The insurance agency cannot directly verify workers' job opportunities.\n\nStep 2: Since wage offers are unobservable, the agency cannot condition contracts on actual wage realizations, creating challenges in designing incentive-compatible unemployment compensation.\n\nStep 3: This forces the agency to rely on taxation (τ) and benefit policies based on observable employment status rather than unobservable wage negotiations, requiring self-selection mechanisms.\n\nStep 4: The unobservability increases agency costs because workers might strategically reject marginal wage offers while collecting benefits, necessitating optimal trade-offs between insurance provision and job acceptance incentives through the Bellman equation's controls (c, a, τ).\n\nQID: textbook-123-19-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-19-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses the role of unobservable wage offers in the moral hazard problem, aligning with the gold answer's points about asymmetric information and contract design challenges. The additional details on taxation, benefit policies, and agency costs enhance the explanation without contradicting the core ideas.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-19-2-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses the role of unobservable wage offers in the moral hazard problem, aligning with the gold answer's points about asymmetric information and contract design challenges. The additional details on taxation, benefit policies, and agency costs enhance the explanation without contradicting the core ideas."}, "llm_echoed_qid": "textbook-123-19-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses the role of unobservable wage offers in the moral hazard problem, aligning with the gold answer's points about asymmetric information and contract design challenges. The additional details on taxation, benefit policies, and agency costs enhance the explanation without contradicting the core ideas."}
{"script_processing_qid": "textbook-123-22-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Why does the Friedman rule become more credible for higher values of $\\beta$ and lower values of $\\alpha$?\n\nGOLD_ANSWER:\n1. **Higher $\\beta$**: Patient households weight future welfare more heavily, reducing the temptation to deviate for short-term gains.\n2. **Lower $\\alpha$**: Less market power reduces monopolistic wage distortion, minimizing the welfare gain from deviation.\n\nCANDIDATE_ANSWER:\nStep 1: The credibility condition compares the long-term benefits of adhering to the Friedman rule (left side) vs. the short-term gain + discounted punishment costs of deviation (right side).\n\nStep 2: Higher β (discount factor) increases the weight on future welfare. Since the left side represents infinite-horizon benefits of compliance (divided by 1−β), it grows disproportionately as β increases, making adherence more attractive.\n\nStep 3: Lower α (not explicitly shown but inferred as the parameter governing monopolistic competition distortions) corresponds to weaker household market power. This reduces the welfare gain from surprise inflation (term (1/γ−1)), shrinking the right side and making deviation less tempting.\n\nTogether, these effects tighten the credibility inequality, making the Friedman rule more sustainable in equilibrium.\n\nQID: textbook-123-22-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-22-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains the reasoning behind the Friedman rule's credibility for higher values of β and lower values of α. It aligns with the gold answer by detailing how higher β increases the weight on future welfare and lower α reduces the welfare gain from deviation, making adherence to the Friedman rule more attractive.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-22-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly explains the reasoning behind the Friedman rule's credibility for higher values of β and lower values of α. It aligns with the gold answer by detailing how higher β increases the weight on future welfare and lower α reduces the welfare gain from deviation, making adherence to the Friedman rule more attractive."}, "llm_echoed_qid": "textbook-123-22-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains the reasoning behind the Friedman rule's credibility for higher values of β and lower values of α. It aligns with the gold answer by detailing how higher β increases the weight on future welfare and lower α reduces the welfare gain from deviation, making adherence to the Friedman rule more attractive."}
{"script_processing_qid": "textbook-115-22-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the assumptions required for consistent estimation of duration models with multiple spells. What happens if these assumptions are violated?\n\nGOLD_ANSWER:\n1.  **Assumptions**: Key assumptions include conditional independence of spells given heterogeneity, constancy of the hazard process over time, and correct specification of the heterogeneity distribution.\n2.  **Violations**: Violations can lead to biased estimates. For example, if the hazard process changes over time, the model may misattribute time effects to heterogeneity.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Key Assumptions\nThe primary assumption for consistent estimation in duration models with multiple spells is **constancy of the process across time**. This means the same underlying hazard function and parameters govern all spells (e.g., the factors influencing unemployment duration remain unchanged across repeated spells).\n\nStep 2: Role of Unobserved Heterogeneity\nMultiple spells aid identification when unobserved heterogeneity (individual-specific factors not captured by covariates) is correlated with time-varying covariates. The model assumes this correlation structure is correctly specified and that multiple spells provide sufficient variation to disentangle unobserved heterogeneity from duration dependence.\n\nStep 3: Consequences of Violations\n- If the **constancy assumption** is violated (e.g., the unemployment process differs between the first and second spells due to changing economic conditions), pooled estimation across spells introduces bias. Parameters like hazard rates may be inconsistently estimated.\n- If **unobserved heterogeneity is misspecified** (e.g., correlations between covariates and unobserved factors are ignored), the model fails to account for systematic differences across spells, leading to inconsistent estimates of covariate effects and duration dependence.\n\nQID: textbook-115-22-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-22-1-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the constancy assumption and discusses the role of unobserved heterogeneity, which aligns with the gold answer. However, the candidate misses the assumption of conditional independence of spells given heterogeneity and does not explicitly mention the correct specification of the heterogeneity distribution. The consequences of violations are partially correct but lack the specific example provided in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-22-1-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the constancy assumption and discusses the role of unobserved heterogeneity, which aligns with the gold answer. However, the candidate misses the assumption of conditional independence of spells given heterogeneity and does not explicitly mention the correct specification of the heterogeneity distribution. The consequences of violations are partially correct but lack the specific example provided in the gold answer."}, "llm_echoed_qid": "textbook-115-22-1-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the constancy assumption and discusses the role of unobserved heterogeneity, which aligns with the gold answer. However, the candidate misses the assumption of conditional independence of spells given heterogeneity and does not explicitly mention the correct specification of the heterogeneity distribution. The consequences of violations are partially correct but lack the specific example provided in the gold answer."}
{"script_processing_qid": "textbook-113-23-3-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Why does the LRAS curve differ from the upward-sloping supply curves of individual goods?\n\nGOLD_ANSWER:\n1.  **Relative vs. Absolute Prices**: Individual supply curves respond to relative price changes (e.g., ice cream vs. yogurt). LRAS reflects economy-wide $P$, with no relative price shifts.\n2.  **Resource Constraints**: For one good, inputs can be reallocated from other sectors. For $Y$, all resources are fully employed at $Y^*$.\n3.  **Time Horizon**: Individual supply curves are short-run; LRAS assumes full price flexibility and factor adjustment.\n4.  **Macro vs. Micro**: LRAS embodies the classical view that aggregate output is supply-determined, not demand-constrained.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the LRAS curve's verticality. The long-run aggregate-supply (LRAS) curve is vertical because, in the long run, real GDP depends solely on factors like labor, capital, natural resources, and technology. These determinants are unaffected by the price level.\n\nStep 2: Contrast with individual goods' supply curves. For individual goods, supply curves slope upward because producers respond to short-term price changes (e.g., higher prices incentivize increased production due to profit motives).\n\nStep 3: Explain the key difference. The LRAS curve reflects the economy’s full productive capacity, which remains unchanged by price levels once all input prices (e.g., wages) and expectations adjust in the long run. Individual supply curves, however, operate under partial equilibrium assumptions where input prices or technology may not fully adjust, allowing price levels to influence quantity supplied.\n\nQID: textbook-113-23-3-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-113-23-3-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the verticality of the LRAS curve due to long-run factors and contrasts it with the upward-sloping supply curves of individual goods, aligning well with the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-113-23-3-2", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the verticality of the LRAS curve due to long-run factors and contrasts it with the upward-sloping supply curves of individual goods, aligning well with the gold answer's key points."}, "llm_echoed_qid": "textbook-113-23-3-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the verticality of the LRAS curve due to long-run factors and contrasts it with the upward-sloping supply curves of individual goods, aligning well with the gold answer's key points."}
{"script_processing_qid": "textbook-119-23-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Interpret the forecast $\\chi^2$ test and discuss its limitations.\n\nGOLD_ANSWER:\n1. **Interpretation**: The test statistic $\\frac{\\sum_{n+1}^{n+t}\\hat{u}_i^2}{\\hat{\\sigma}^2}$ follows $\\chi^2(t)$ under the null hypothesis of stable parameters.\n2. **Limitations**: Low statistical power; more useful as a diagnostic signal than a definitive test.\n\nCANDIDATE_ANSWER:\nStep 1: Interpret the forecast χ² test\n- The forecast χ² test evaluates out-of-sample predictive performance by comparing the sum of squared forecast errors (from period n+1 to n+t) to the model’s estimated error variance (σ̂²). The test statistic is computed as: \n  χ² = (Σₙ₊₁ⁿ⁺ᵗ û_i²) / σ̂².\n- Under the null hypothesis of correct model specification, this statistic follows a χ² distribution with t degrees of freedom. A large value indicates poor forecast accuracy, suggesting model inadequacy.\n\nStep 2: Discuss limitations\n1. **Ignores model complexity**: Unlike AIC/SIC or Mallows’ C_p, the forecast χ² does not penalize overparameterization. It may favor unnecessarily complex models if they marginally improve forecasts.\n2. **Assumes homoscedasticity**: The test assumes σ̂² (estimated from in-sample data) is valid for out-of-sample periods, which may fail if error variance changes over time.\n3. **Requires reserved data**: It necessitates withholding a subset of observations (n+1 to n+t) for testing, reducing the sample available for model estimation.\n4. **Sensitive to σ̂² accuracy**: If the in-sample error variance estimate is biased (e.g., due to misspecification), the test statistic becomes unreliable.\n5. **Limited scope**: Only assesses squared forecast errors, ignoring other forecast quality metrics (e.g., directional accuracy or bias).\n\nQID: textbook-119-23-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-23-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately interprets the forecast χ² test and comprehensively discusses its limitations, aligning well with the gold answer while providing additional detailed points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-23-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately interprets the forecast χ² test and comprehensively discusses its limitations, aligning well with the gold answer while providing additional detailed points."}, "llm_echoed_qid": "textbook-119-23-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately interprets the forecast χ² test and comprehensively discusses its limitations, aligning well with the gold answer while providing additional detailed points."}
{"script_processing_qid": "textbook-112-21-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Analyze the political implications of taxing colonies without representation, as discussed by Adam Smith. How does this affect the 'importance' of colonial leaders?\n\nGOLD_ANSWER:\n1.  **Loss of Political Significance**: Colonial leaders derive importance from their role in public affairs. Taxation without representation diminishes their influence, as they become mere executors of Parliament's will.\n2.  **Ambition and Resistance**: Leaders, like those in British America, may resist such measures to preserve their status, as seen in their rejection of requisition proposals.\n3.  **Factional Dynamics**: The defense of personal importance fuels political strife, as leaders vie to maintain or elevate their standing within the colonial hierarchy.\n4.  **Historical Parallels**: Smith draws comparisons to the Roman Republic's social war, where allies demanded citizenship rights to preserve their importance.\n\nCANDIDATE_ANSWER:\nStep 1: Adam Smith argues that colonial assemblies lack the authority and information to manage empire-wide matters like defense, making centralized taxation decisions (e.g., by the British Parliament) necessary. However, taxing colonies without representation creates a political dilemma.<br><br>Step 2: Smith observes that colonial leaders (the 'natural aristocracy') derive their 'importance' from participation in public affairs. When excluded from fiscal decisions like taxation, they lose the ability to 'preserve or defend their importance,' which is central to their political influence.<br><br>Step 3: This erosion of colonial leaders' importance destabilizes free governance systems. Smith implies that denying representation in taxation undermines the legitimacy of colonial elites, fostering discontent and weakening their capacity to collaborate with imperial authorities, as their role in governance becomes symbolic rather than substantive.<br><br>Step 4: The proposed requisition system (Parliament setting the tax sum, colonies levying it) partially addresses this by allowing colonial leaders some administrative autonomy. However, it still excludes them from the core decision-making process, perpetuating the tension between imperial control and local political agency.\n\nQID: textbook-112-21-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-21-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately summarizes Adam Smith's analysis of the political implications of taxing colonies without representation, including the loss of political significance for colonial leaders, their resistance, and the broader impact on governance. The answer aligns well with the gold answer's points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-21-0-2", "category": "CORRECT", "explanation": "The candidate answer accurately summarizes Adam Smith's analysis of the political implications of taxing colonies without representation, including the loss of political significance for colonial leaders, their resistance, and the broader impact on governance. The answer aligns well with the gold answer's points."}, "llm_echoed_qid": "textbook-112-21-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately summarizes Adam Smith's analysis of the political implications of taxing colonies without representation, including the loss of political significance for colonial leaders, their resistance, and the broader impact on governance. The answer aligns well with the gold answer's points."}
{"script_processing_qid": "textbook-106-6-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Using the midpoint method, derive the price elasticity of supply when the price of a good increases from $\\$4$ to $\\$6$, and the quantity supplied increases from 200 to 300 units.\n\nGOLD_ANSWER:\n1. **Calculate Percentage Change in Price**: \n   $$ \\text{Percentage change in price} = \\frac{6 - 4}{(6 + 4)/2} \\times 100 = \\frac{2}{5} \\times 100 = 40\\%. $$\n2. **Calculate Percentage Change in Quantity Supplied**: \n   $$ \\text{Percentage change in quantity supplied} = \\frac{300 - 200}{(300 + 200)/2} \\times 100 = \\frac{100}{250} \\times 100 = 40\\%. $$\n3. **Compute Price Elasticity of Supply**: \n   $$ \\text{Price elasticity of supply} = \\frac{40\\%}{40\\%} = 1.0. $$\n\nCANDIDATE_ANSWER:\nStep 1: Calculate percentage change in quantity supplied using midpoint formula:\nPercentage change in QS = [(New QS - Old QS)/Average QS] × 100\n= (300 - 200)/[(200 + 300)/2] × 100\n= 100/250 × 100 = 40%\n\nStep 2: Calculate percentage change in price using midpoint formula:\nPercentage change in P = [(New Price - Old Price)/Average Price] × 100\n= ($6 - $4)/[($4 + $6)/2] × 100\n= $2/$5 × 100 = 40%\n\nStep 3: Compute price elasticity of supply (PES):\nPES = (Percentage change in QS)/(Percentage change in Price)\n= 40%/40% = 1.0\n\nQID: textbook-106-6-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-106-6-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to calculate the price elasticity of supply using the midpoint method, matching the gold answer in both calculations and final result.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-6-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to calculate the price elasticity of supply using the midpoint method, matching the gold answer in both calculations and final result."}, "llm_echoed_qid": "textbook-106-6-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to calculate the price elasticity of supply using the midpoint method, matching the gold answer in both calculations and final result."}
{"script_processing_qid": "textbook-107-9-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Calculate the market price in the Cournot equilibrium for the given problem.\n\nGOLD_ANSWER:\n1. **Total Output**: $Q = q_i + q_j = 90 + 90 = 180$.\n2. **Market Price**: $P = 830 - 3Q = 830 - 3(180) = 830 - 540 = 290$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the total market output (Y) in equilibrium. Since there are two firms each producing 135 units, Y = 135 + 135 = 270 units.\nStep 2: Substitute Y into the inverse demand function P(Y) = 830 - 3Y.\nP = 830 - 3*(270) = 830 - 810 = 20.\nConclusion: The market price in the Cournot equilibrium is $20.\n\nQID: textbook-107-9-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-9-0-3\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer incorrectly calculates the total output as 270 units (instead of 180) and consequently derives a market price of $20 (instead of $290). The gold answer clearly shows the correct calculations.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-9-0-3", "category": "INCORRECT", "explanation": "The candidate's answer incorrectly calculates the total output as 270 units (instead of 180) and consequently derives a market price of $20 (instead of $290). The gold answer clearly shows the correct calculations."}, "llm_echoed_qid": "textbook-107-9-0-3", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer incorrectly calculates the total output as 270 units (instead of 180) and consequently derives a market price of $20 (instead of $290). The gold answer clearly shows the correct calculations."}
{"script_processing_qid": "textbook-12-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Propose a government policy to align private and social optimality in honey production, justifying its design.\n\nGOLD_ANSWER:\n1. **Policy**: Subsidize bees to offset Jones' private cost disadvantage under natural production.\n2. **Design**: Set subsidy $s = \\frac{H b}{c} \\cdot w$ to make natural production privately optimal when socially optimal.\n3. **Financing**: Lump-sum tax on Smith (apple producer) to ensure budget neutrality.\n4. **Justification**: Internalizes the positive externality of natural production on Smith's costs.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the divergence source. Private decisions ignore the apple production cost difference (A𝔴 vs. A𝔴/c) between natural (c > 1) and artificial honey. Natural honey reduces apple costs through pollination (c > 1), creating a positive externality.\n\nStep 2: Calculate the externality magnitude. Social optimality requires accounting for the apple cost savings: A𝔴(1 - 1/c). This value represents the uninternalized benefit of natural honey production.\n\nStep 3: Design a Pigouvian subsidy. Implement a per-unit subsidy equal to A(1 - 1/c) on natural honey production. This adjusts the private cost comparison to:\n𝖻𝖯𝖻 + (k-1)𝔴 - A(1 - 1/c) < 𝖯𝗆\n\nStep 4: Justification. The subsidy bridges the gap between private costs (𝖻𝖯𝖻 + k𝔴) and social costs (𝖻𝖯𝖻 + k𝔴 - A𝔴(1 - 1/c)), aligning Jones' profit maximization with societal welfare by internalizing the pollination externality.\n\nQID: textbook-12-0-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-12-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the externality, calculates its magnitude, and proposes a Pigouvian subsidy to align private and social optimality, matching the gold answer's intent and justification.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-12-0-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the externality, calculates its magnitude, and proposes a Pigouvian subsidy to align private and social optimality, matching the gold answer's intent and justification."}, "llm_echoed_qid": "textbook-12-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the externality, calculates its magnitude, and proposes a Pigouvian subsidy to align private and social optimality, matching the gold answer's intent and justification."}
{"script_processing_qid": "textbook-123-7-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What are the implications of the saving function assumptions $f(0)=-\\infty$, $f^{\\prime}(1+r)>0$, and $f(1)>0$ for the model?\n\nGOLD_ANSWER:\n1. **$f(0)=-\\infty$**: Implies that saving is impossible at a zero interest rate.\n2. **$f^{\\prime}(1+r)>0$**: Saving increases with the interest rate.\n3. **$f(1)>0$**: Positive saving at a gross interest rate of 1, ensuring a baseline level of saving.\n\nCANDIDATE_ANSWER:\nStep 1: Analyze 𝑓(0)=−∞\nThis assumption implies agents face infinite disutility when saving nothing (𝑠=0). Agents will always choose positive savings (𝑠>0), ensuring demand for stores of value (e.g., currency). This rules out non-monetary equilibria where currency is rejected.\n\nStep 2: Analyze 𝑓’(1+𝑟)>0\nA positive derivative indicates savings increase with higher real interest rates (1+𝑟). This creates an upward-sloping saving supply curve, enabling potential equilibrium where higher returns on currency (through price level adjustments) can induce sufficient savings to absorb monetary expansions.\n\nStep 3: Analyze 𝑓(1)>0\nPositive savings at 1+𝑟=1 (zero net return) ensures baseline demand for currency even when nominal interest rates equal zero. This supports the existence of a monetary equilibrium when currency’s real return equals population growth rate (typically zero in basic OLG models).\n\nCombined implications:\n1) Guarantees monetary equilibrium exists with valued currency\n2) Enables comparative statics where monetary policy (via 𝑟(𝑡)) can influence savings\n3) Preforms essential technical role by ensuring interior solutions for savings decisions.\n\nQID: textbook-123-7-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-7-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly interprets the implications of each assumption in the saving function and provides a detailed analysis of their combined effects on the model, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-7-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly interprets the implications of each assumption in the saving function and provides a detailed analysis of their combined effects on the model, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-123-7-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly interprets the implications of each assumption in the saving function and provides a detailed analysis of their combined effects on the model, aligning well with the gold answer."}
{"script_processing_qid": "textbook-103-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Analyze the effect of an increase in the tax parameter $a$ (where $T(w) = t(w - a)$) on the reservation wage $\\bar{w}$ and search effort $e$. Use the results $$(1-t)\\left[1 + \\frac{\\beta(1-\\alpha)\\pi(\\bar{e})[1-F(\\bar{w})]}{1-\\beta(1-\\alpha)}\\right]\\frac{\\partial\\bar{w}}{\\partial a} = -t$$ and $$\\frac{\\pi''(\\bar{e})}{\\pi'(\\bar{e})^2}\\frac{\\partial e}{\\partial a} = \\frac{(1-t)[1-F(\\bar{w})]}{1-\\beta(1-\\alpha)}\\frac{\\partial\\bar{w}}{\\partial a}.$$\n\nGOLD_ANSWER:\n1.  **Reservation Wage**: From the equation, $\\frac{\\partial\\bar{w}}{\\partial a} < 0$. An increase in $a$ reduces $\\bar{w}$ because the subsidy makes low-wage jobs more attractive.\n2.  **Search Effort**: Since $\\frac{\\partial\\bar{w}}{\\partial a} < 0$ and $\\pi''(e) < 0$, $\\frac{\\partial e}{\\partial a} > 0$. Higher $a$ increases search effort due to the increased attractiveness of employment.\n\nCANDIDATE_ANSWER:\nStep 1: Analyze the effect on reservation wage (ŵ)\nFrom the first equation:\n(1−t)[1 + (β(1−α)π(ē)[1−F(ŵ)])/(1−β(1−α))] ⋅ ∂ŵ/∂a = −t\n\nSince all parameters (t, β, α) and functions (π(ē), 1−F(ŵ)) are positive fractions < 1, the entire coefficient of ∂ŵ/∂a is positive. Solving for ∂ŵ/∂a:\n∂ŵ/∂a = −t / [(1−t)(1 + β(1−α)π(ē)[1−F(ŵ)]/(1−β(1−α)))]\n\nThe negative sign indicates ŵ decreases as a increases.\n\nStep 2: Analyze the effect on search effort (e)\nFrom the second equation:\n(π''(ē)/π'(ē)²) ⋅ ∂e/∂a = (1−t)[1−F(ŵ)]/(1−β(1−α)) ⋅ ∂ŵ/∂a\n\nKey observations:\n1. π''(ē) < 0 (assuming diminishing returns to effort)\n2. π'(ē)² > 0\n3. ∂ŵ/∂a < 0 (from Step 1)\n\nLeft side becomes negative × ∂e/∂a\nRight side becomes positive × negative = negative\nThus: Negative × ∂e/∂a = Negative → ∂e/∂a > 0\n\nConclusion: Increasing tax parameter a reduces the reservation wage ŵ and increases search effort e.\n\nQID: textbook-103-0-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-103-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's analysis correctly identifies that an increase in the tax parameter $a$ reduces the reservation wage $\\bar{w}$ and increases search effort $e$, matching the gold answer. The reasoning and calculations are accurate and align with the provided equations.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-0-0-2", "category": "CORRECT", "explanation": "The candidate's analysis correctly identifies that an increase in the tax parameter $a$ reduces the reservation wage $\bar{w}$ and increases search effort $e$, matching the gold answer. The reasoning and calculations are accurate and align with the provided equations."}, "llm_echoed_qid": "textbook-103-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's analysis correctly identifies that an increase in the tax parameter $a$ reduces the reservation wage $\bar{w}$ and increases search effort $e$, matching the gold answer. The reasoning and calculations are accurate and align with the provided equations."}
{"script_processing_qid": "textbook-112-24-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Using Smith's example of the steam engine boy, model the innovation process mathematically. Assume: (1) Labor cost of manual valve operation is $C$, (2) Innovation reduces cost to $c$ (where $c < C$), (3) Probability of innovation is $p$. Derive the expected cost savings.\n\nGOLD_ANSWER:\n1.  **Baseline Cost**: Without innovation, total cost is $C$.\n2.  **Innovation Cost**: With innovation, cost is $c$ (saving $C - c$).\n3.  **Expected Savings**: $E[\\text{Savings}] = p \\cdot (C - c) + (1 - p) \\cdot 0 = p(C - c)$.\n4.  **Implication**: Higher $p$ (likelihood of innovation under division of labor) or larger $(C - c)$ (potential savings) increases expected gains.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate the original labor cost without innovation: $C$.\nStep 2: Calculate the expected labor cost after considering the innovation probability: $E[\\text{cost}] = p \\cdot c + (1-p) \\cdot C$.\nStep 3: Compute the expected cost savings by subtracting the expected cost from the original cost: $\\text{Expected Savings} = C - E[\\text{cost}]$.\nStep 4: Substitute Step 2 into Step 3: $C - (p \\cdot c + (1-p) \\cdot C)$.\nStep 5: Simplify algebraically: $C - pc - C + pC = p(C - c)$.\nFinal Result: The expected cost savings equal $p(C - c)$.\n\nQID: textbook-112-24-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-24-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the expected cost savings as $p(C - c)$, matching the gold answer. The reasoning and steps are logically consistent and mathematically accurate.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-24-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the expected cost savings as $p(C - c)$, matching the gold answer. The reasoning and steps are logically consistent and mathematically accurate."}, "llm_echoed_qid": "textbook-112-24-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the expected cost savings as $p(C - c)$, matching the gold answer. The reasoning and steps are logically consistent and mathematically accurate."}
{"script_processing_qid": "textbook-30-0-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain how debt acts as a shock absorber in the stochastic model without capital.\n\nGOLD_ANSWER:\n1.  **No Capital**: With $k(s^t) = 0$, the government adjusts debt $b(s^t)$ to smooth tax distortions across states.\n2.  **Budget Constraint**: $b(s^t) = R_b(s^t)b(s^{t-1}) + g(s^t) - \\tau(s^t)w(s^t)l(s^t)$.\n3.  **Shock Absorption**: High $g(s^t)$ leads to higher $b(s^t)$, spreading the fiscal burden over time. Taxes $\\tau(s^t)$ remain smooth, minimizing distortions.\n\nCANDIDATE_ANSWER:\nStep 1: In the stochastic model without capital, exogenous shocks (e.g., changes in government consumption or productivity) create unanticipated fiscal needs. Debt serves as a shock absorber through its ability to adjust across states and time.\\n\\nStep 2: When a shock occurs, the government optimally adjusts debt holdings rather than immediately distorting labor taxes. This allows tax rates to remain smooth over time, minimizing labor supply distortions.\\n\\nStep 3: The return on debt is state-contingent in equilibrium. Debt issuance/repayment responds to shocks by redistributing tax burdens across states, ensuring the government’s budget constraint holds without requiring large, distortionary tax changes.\\n\\nStep 4: By absorbing fiscal shocks through debt adjustments rather than tax volatility, the model preserves the efficiency of allocations consistent with conditions (2.7) and (2.8) in Proposition 5, even under uncertainty.\n\nQID: textbook-30-0-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-30-0-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains how debt acts as a shock absorber in the stochastic model without capital, detailing the adjustment of debt holdings to smooth tax distortions and maintain efficiency under uncertainty, aligning with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-30-0-1-2", "category": "CORRECT", "explanation": "The candidate answer accurately explains how debt acts as a shock absorber in the stochastic model without capital, detailing the adjustment of debt holdings to smooth tax distortions and maintain efficiency under uncertainty, aligning with the gold answer."}, "llm_echoed_qid": "textbook-30-0-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains how debt acts as a shock absorber in the stochastic model without capital, detailing the adjustment of debt holdings to smooth tax distortions and maintain efficiency under uncertainty, aligning with the gold answer."}
{"script_processing_qid": "textbook-115-14-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Consider the wage equation $\\log(wage)=\\beta_{0}+\\beta_{1}exper+\\beta_{2}exper^{2}+\\beta_{3}educ+u$ where $educ$ is endogenous. Discuss the validity of using mother's education ($motheduc$) as an instrument for $educ$. What are the potential issues?\n\nGOLD_ANSWER:\n1. **Exogeneity**: $motheduc$ must be uncorrelated with $u$, which includes omitted variables like ability and family background. This is questionable because $motheduc$ is likely correlated with these factors.\n2. **Relevance**: $motheduc$ must be partially correlated with $educ$ after controlling for $exper$ and $exper^{2}$. This can be tested via a reduced form regression.\n3. **Conclusion**: While $motheduc$ may satisfy the relevance condition, its exogeneity is doubtful, making it a potentially invalid instrument.\n\nCANDIDATE_ANSWER:\nStep 1: Assess instrument validity requirements. For mother's education (motheduc) to be a valid instrument for educ, it must satisfy two conditions: (1) Exogeneity: Cov(motheduc, u) = 0, and (2) Relevance: θ₁ ≠ 0 in the first-stage linear projection of educ onto motheduc and all exogenous variables (exper, exper²).\n\nStep 2: Evaluate relevance. Mother's education is likely correlated with child's education (educ), satisfying the relevance condition. However, weak correlation (small θ₁) could lead to weak instrument problems with biased IV estimates and large standard errors.\n\nStep 3: Evaluate exogeneity. The critical issue is whether motheduc is uncorrelated with u. Potential violations arise if: (a) Mother's education affects wage through channels other than child's education (e.g., family socioeconomic status, genetic ability, or networking opportunities); (b) motheduc is correlated with unobserved parental characteristics that influence both educ and wage.\n\nStep 4: Conclude validity. While motheduc may satisfy relevance, exogeneity is often questionable due to omitted variable bias. This makes it potentially invalid unless these alternative pathways are fully accounted for in the model.\n\nQID: textbook-115-14-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-14-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses both the exogeneity and relevance conditions for using mother's education as an instrument, aligning closely with the gold answer. It also correctly identifies potential issues with exogeneity due to omitted variable bias.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-14-0-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses both the exogeneity and relevance conditions for using mother's education as an instrument, aligning closely with the gold answer. It also correctly identifies potential issues with exogeneity due to omitted variable bias."}, "llm_echoed_qid": "textbook-115-14-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses both the exogeneity and relevance conditions for using mother's education as an instrument, aligning closely with the gold answer. It also correctly identifies potential issues with exogeneity due to omitted variable bias."}
{"script_processing_qid": "textbook-112-44-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Prove using Smith's text why small religious sects promote 'regular and orderly' morals among the poor.\n\nGOLD_ANSWER:\n1.  **Monitoring Mechanism**: Sect membership creates a reputational bond ($$ \\text{Character} = f(\\text{Observability}) $$).\n2.  **Punishment**: Excommunication imposes costs (social exclusion), enforcing compliance with austere norms.\n3.  **Equilibrium**: $$ \\text{Morality} \\propto \\frac{1}{\\text{Sect Size}} $$—smaller sects enable tighter control.\n\nCANDIDATE_ANSWER:\nStep 1: Smith argues that state-supported ecclesiastical establishments tend to foster clergy negligence unless counterbalanced by strict hierarchies or honors. This creates a need for external incentives to maintain clerical diligence.\n\nStep 2: Small religious sects, lacking state support, avoid the 'interested diligence' problem Smith identifies. Their clergy must instead earn adherents through personal effort and moral rigor, aligning incentives with congregational discipline.\n\nStep 3: Smith’s 'dual systems of morality' framework shows that the poor are subject to the 'strict/austere' system. Small sects amplify this austerity to differentiate themselves, enforcing 'regular and orderly' morals as a survival strategy.\n\nStep 4: Without state backing, small sects rely on member commitment. This necessitates strict behavioral codes to maintain cohesion, directly appealing to the poor’s need for social structure and moral guidance in stratified societies.\n\nConclusion: Small sects promote orderly morals among the poor because (1) their clergy’s survival depends on rigorous enforcement of austere morals, and (2) they operationalize Smith’s 'strict system' more effectively than lax state-supported institutions.\n\nQID: textbook-112-44-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-44-0-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies that small religious sects promote 'regular and orderly' morals among the poor, but the reasoning deviates from the gold answer's focus on monitoring, punishment, and equilibrium. The candidate emphasizes clergy incentives and Smith's 'dual systems of morality' instead.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-44-0-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies that small religious sects promote 'regular and orderly' morals among the poor, but the reasoning deviates from the gold answer's focus on monitoring, punishment, and equilibrium. The candidate emphasizes clergy incentives and Smith's 'dual systems of morality' instead."}, "llm_echoed_qid": "textbook-112-44-0-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies that small religious sects promote 'regular and orderly' morals among the poor, but the reasoning deviates from the gold answer's focus on monitoring, punishment, and equilibrium. The candidate emphasizes clergy incentives and Smith's 'dual systems of morality' instead."}
{"script_processing_qid": "textbook-102-3-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the Box-Cox transformation and its application in the model $\\ln Y = \\alpha + \\beta_{k}(K^{\\lambda} - 1)/\\lambda + \\beta_{l}(L^{\\lambda} - 1)/\\lambda + \\varepsilon$.\n\nGOLD_ANSWER:\n1. **Box-Cox Transformation**: Transforms variables to address nonlinearity, defined as $x^{(\\lambda)} = (x^{\\lambda} - 1)/\\lambda$ for $\\lambda \\neq 0$ and $\\ln x$ for $\\lambda = 0$.\n2. **Model Interpretation**: The model allows for flexible functional forms. When $\\lambda = 0$, it reduces to the log-linear Cobb-Douglas model.\n3. **Estimation**: Use maximum likelihood or nonlinear least squares to estimate $\\alpha, \\beta_{k}, \\beta_{l}, \\lambda$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the Box-Cox Transformation\n- The Box-Cox transformation generalizes variable relationships using a parameter λ. For a variable X, the transformation is defined as:\n  - (X^λ - 1)/λ if λ ≠ 0\n  - ln(X) if λ = 0\n\nStep 2: Application to the Provided Model\n- In the model ln Y = α + β_k(K^λ - 1)/λ + β_l(L^λ - 1)/λ + ε:\n  - K and L are subjected to a Box-Cox transformation with the same λ parameter.\n  - The Cobb-Douglas model (ln Y = α + β_k ln K + β_l ln L + ε) is nested as a special case when λ → 0.\n\nStep 3: Elasticity Derivation\n- Output elasticities are computed as:\n  ∂lnY/∂lnK = K * (∂lnY/∂K) = β_k ⋅ K^λ\n  ∂lnY/∂lnL = L * (∂lnY/∂L) = β_l ⋅ L^λ\n  At sample means: K̄^λ and L̄^λ are evaluated using estimated λ.\n\nStep 4: Estimation Process\n- Nonlinear least squares (NLS) is required due to the additive error structure and λ parameter.\n- Scanning method (Section F9.2): Iterate over candidate λ values, estimate α, β_k, β_l via OLS for each λ, and select the λ that minimizes residuals.\n\nStep 5: Key Implications\n- The model generalizes Cobb-Douglas (λ = 0) to allow nonlinear relationships. Elasticities depend on both β coefficients and the scaling factor K^λ/L^λ.\n- Asymptotic standard errors require gradient matrices (Eq. 9-12/9-13) and covariance formulas (Eq. 9-16).\n\nQID: textbook-102-3-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-102-3-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the Box-Cox transformation, its application in the provided model, and the estimation process. It also correctly identifies the Cobb-Douglas model as a special case when λ → 0 and discusses the computation of elasticities, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-102-3-1-1", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the Box-Cox transformation, its application in the provided model, and the estimation process. It also correctly identifies the Cobb-Douglas model as a special case when λ → 0 and discusses the computation of elasticities, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-102-3-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the Box-Cox transformation, its application in the provided model, and the estimation process. It also correctly identifies the Cobb-Douglas model as a special case when λ → 0 and discusses the computation of elasticities, aligning well with the gold answer."}
{"script_processing_qid": "textbook-123-2-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the concept of a stationary allocation in the context of this overlapping generations model. Why is $c\\_{1}^{0} = c\\_{o}$ not required?\n\nGOLD_ANSWER:\nA **stationary allocation** is one where consumption levels for the young and old are constant across generations: $c\\_{i}^{i} = c\\_{y}$ and $c\\_{i+1}^{i} = c\\_{o}$ for all $i \\geq 1$. The initial old ($i=0$) consume only in period 1 ($c\\_{1}^{0}$), and their consumption is not tied to $c\\_{o}$ because they are a special case with no future periods. Thus, stationarity applies only to generations $i \\geq 1$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the definition of stationary allocation\nA stationary allocation is defined as one where:\n- All generations i ≥ 1 have constant consumption patterns:\n  - c_{i}^i = c_y (constant young consumption)\n  - c_{i+1}^i = c_o (constant old consumption)\n\nStep 2: Analyze the special case of generation i=0\nThe initial old (i=0) only exists at t=1 with:\n- Utility function U⁰(c⁰) = u(c₁⁰)\n- No second period consumption\n\nStep 3: Observe the scope of stationarity requirements\nThe stationarity condition:\n- Applies only to generations i ≥ 1\n- Does NOT constrain generation i=0\n\nStep 4: Recognize temporal asymmetry\n- All subsequent old agents (i ≥ 1 when old) must follow c_o\n- The initial old (i=0) at t=1 has no predecessor generation\n\nStep 5: Verify through feasibility constraints\nFeasibility requires:\nc₁⁰ + c₁¹ ≤ y₁⁰ + y₁¹\nStationarity requires c₁¹ = c_y (young consumption),\nbut c₁⁰ can adjust independently to clear the initial period market\nwithout needing to match subsequent old agents' consumption level c_o.\n\nQID: textbook-123-2-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-2-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately defines a stationary allocation, correctly identifies the special case of the initial old generation (i=0), and explains why their consumption is not required to match subsequent generations' old consumption (c_o). The reasoning aligns perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-2-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately defines a stationary allocation, correctly identifies the special case of the initial old generation (i=0), and explains why their consumption is not required to match subsequent generations' old consumption (c_o). The reasoning aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-123-2-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately defines a stationary allocation, correctly identifies the special case of the initial old generation (i=0), and explains why their consumption is not required to match subsequent generations' old consumption (c_o). The reasoning aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-43-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Quantitatively, how does the Stolper-Samuelson theorem imply $J_{22} > 0$ in the Jacobian matrix?\n\nGOLD_ANSWER:\n1.  **Theorem**: $\\frac{\\partial w_1}{\\partial p} < -\\frac{w_1}{p}$.\n2.  **Steady State**: $w_1 = p(r+g)$.\n3.  **Jacobian Element**: $J_{22} = -\\frac{\\partial w_1}{\\partial p} + (r+g) > 0$ by substitution.\n\nCANDIDATE_ANSWER:\nThe provided context does not explicitly derive or mention the Jacobian matrix elements or their relationship to the Stolper-Samuelson theorem. The Stolper-Samuelson theorem typically links output price changes to factor returns (e.g., wages, capital returns) in a Heckscher-Ohlin trade framework, but the context focuses on increasing returns, externalities, and indeterminacy in growth/business cycle models. Since the text excerpts do not contain sufficient mathematical details about the Jacobian matrix \\(J\\) or its connection to Stolper-Samuelson dynamics, we cannot quantitatively explain why \\(J_{22} > 0\\) would hold in this specific framework based on the given information.\n\nQID: textbook-43-0-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-43-0-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide sufficient information to derive or explain the relationship between the Stolper-Samuelson theorem and the Jacobian matrix element \\(J_{22}\\).\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-43-0-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide sufficient information to derive or explain the relationship between the Stolper-Samuelson theorem and the Jacobian matrix element \\(J_{22}\\)."}, "llm_echoed_qid": "textbook-43-0-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it does not provide sufficient information to derive or explain the relationship between the Stolper-Samuelson theorem and the Jacobian matrix element \\(J_{22}\\)."}
{"script_processing_qid": "textbook-114-24-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the distributional effects of the gasoline tax. How is the tax burden shared between consumers and producers, and what factors determine this sharing?\n\nGOLD_ANSWER:\n1. **Burden Sharing**: Consumers pay $0.22 more per gallon ($1.22 - $1.00), while producers receive $0.28 less per gallon ($1.00 - $0.72).\n2. **Determinants**: The burden depends on the relative elasticities of demand and supply. Here, demand is less elastic (elasticity = -0.5) than supply (elasticity = 0.4), so consumers bear a slightly larger share of the tax.\n\nCANDIDATE_ANSWER:\nStep 1: Identify pre-tax equilibrium\nOriginal demand: Qᴅ = 150 - 50P\nOriginal supply: Qs = 60 + 40P\nSet Qᴅ = Qs:\n150 - 50P = 60 + 40P → 90P = 90 → P = $1.00\nQuantity = 150 - 50(1) = 100 billion gallons\n\nStep 2: Calculate post-tax equilibrium\nTax imposes Pʙ = Ps + 0.50\nNew demand: Qᴅ = 150 - 50(Ps + 0.50)\nNew supply: Qs = 60 + 40Ps\nSet Qᴅ = Qs:\n150 - 50Ps - 25 = 60 + 40Ps → 125 - 50Ps = 60 + 40Ps → 90Ps = 65 → Ps ≈ $0.7222\nPʙ = 0.7222 + 0.50 ≈ $1.2222\n\nStep 3: Determine tax burden\nTax = $0.50 per gallon\nBuyers' burden: 1.2222 - 1.00 = $0.2222 (44.44%)\nSellers' burden: 1.00 - 0.7222 = $0.2778 (55.56%)\n\nStep 4: Explain determining factors\nTax incidence depends on price elasticities:\n- Elasticity of demand (|Eᴅ|) = 0.5\n- Elasticity of supply (Eₛ) = 0.4\nBurden shares = Eₛ/(Eᴅ + Eₛ) ≈ 44% (consumers)\n              Eᴅ/(Eᴅ + Eₛ) ≈ 56% (producers)\nThe relatively more inelastic side (supply in this case) bears a larger share of the tax burden.\n\nQID: textbook-114-24-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-24-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the pre-tax and post-tax equilibrium prices and quantities, accurately calculates the tax burden shared between consumers and producers, and correctly explains the determining factors based on the elasticities of demand and supply. The reasoning and numerical results align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-24-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the pre-tax and post-tax equilibrium prices and quantities, accurately calculates the tax burden shared between consumers and producers, and correctly explains the determining factors based on the elasticities of demand and supply. The reasoning and numerical results align with the gold answer."}, "llm_echoed_qid": "textbook-114-24-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the pre-tax and post-tax equilibrium prices and quantities, accurately calculates the tax burden shared between consumers and producers, and correctly explains the determining factors based on the elasticities of demand and supply. The reasoning and numerical results align with the gold answer."}
{"script_processing_qid": "textbook-79-0-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Derive the condition under which a distribution $G$ second-order stochastically dominates $F$ for a given utility function $u(x)$. How does this relate to the Arrow-Pratt measure of risk aversion?\n\nGOLD_ANSWER:\n4. $G$ SOSD $F$ if $\\int u(x) dG(x) \\geq \\int u(x) dF(x)$ for all concave $u(x)$. For a specific $u(x)$, this requires $\\int [u(x) (g(x) - f(x))] dx \\geq 0$. The Arrow-Pratt measure $r_A(x) = -u''(x)/u'(x)$ quantifies local risk aversion; higher $r_A(x)$ implies stronger preference for $G$ over $F$ under SOSD.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the definition of Second-Order Stochastic Dominance (SOSD). Distribution G SOSD F iff ∫_{-∞}^x [F(t) - G(t)] dt ≥ 0 for all x, and E[G] ≥ E[F].\n\nStep 2: For risk-averse agents with utility function u(x) (concave, u''(x) ≤ 0), G is preferred over F under SOSD. This implies E[u(G)] ≥ E[u(F)] for all concave u.\n\nStep 3: Using integration by parts on the expected utility difference:\nE[u(G)] - E[u(F)] = ∫ u'(x) [∫_{-∞}^x (G(t) - F(t)) dt] dx\n\nStep 4: Under SOSD condition, ∫_{-∞}^x (F(t) - G(t)) dt ≥ 0 ⇒ ∫_{-∞}^x (G(t) - F(t)) dt ≤ 0. Since u'(x) > 0 (non-satiation) and u''(x) ≤ 0 (concavity), the negative integrand multiplied by positive u'(x) ensures E[u(G)] ≥ E[u(F)].\n\nStep 5: The Arrow-Pratt measure (-u''(x)/u'(x)) quantifies local risk aversion. SOSD’s requirement of preference by all concave utilities encompasses all risk-averse agents regardless of their Arrow-Pratt measure magnitude, as long as they exhibit risk aversion (u''(x) ≤ 0). Thus, SOSD implies unanimous preference across all risk-averse agents parameterized by their Arrow-Pratt measures.\n\nQID: textbook-79-0-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-79-0-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the condition for second-order stochastic dominance (SOSD) and relates it to the Arrow-Pratt measure of risk aversion. The steps logically follow the definition of SOSD, the properties of concave utility functions, and the integration by parts technique, culminating in a correct explanation of how SOSD encompasses all risk-averse agents regardless of their Arrow-Pratt measure.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-79-0-1-3", "category": "CORRECT", "explanation": "The candidate answer correctly derives the condition for second-order stochastic dominance (SOSD) and relates it to the Arrow-Pratt measure of risk aversion. The steps logically follow the definition of SOSD, the properties of concave utility functions, and the integration by parts technique, culminating in a correct explanation of how SOSD encompasses all risk-averse agents regardless of their Arrow-Pratt measure."}, "llm_echoed_qid": "textbook-79-0-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the condition for second-order stochastic dominance (SOSD) and relates it to the Arrow-Pratt measure of risk aversion. The steps logically follow the definition of SOSD, the properties of concave utility functions, and the integration by parts technique, culminating in a correct explanation of how SOSD encompasses all risk-averse agents regardless of their Arrow-Pratt measure."}
{"script_processing_qid": "textbook-112-16-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the comparative productivity of agricultural capital versus manufacturing capital, incorporating Smith's argument about the role of nature in agricultural production.\n\nGOLD_ANSWER:\n1.  **Agricultural Capital**: \n   - Employs labor (servants, cattle) and leverages natural fertility ($Nature$). \n   - Output: $Y_{agri} = f(L_{agri}, K_{agri}, N)$, where $N$ is nature's contribution (rent). \n   - Rent ($N$) is $\\geq 25\\%$ of total output (Smith's estimate).\n2.  **Manufacturing Capital**: \n   - Output: $Y_{manu} = f(L_{manu}, K_{manu})$, with $N = 0$. \n   - No natural input; reproduction is linear in labor/capital.\n3.  **Comparison**: \n   - $\\frac{dY_{agri}}{dK} > \\frac{dY_{manu}}{dK}$ due to $N$ (marginal product of capital higher in agriculture). \n   - Smith's empirical claim: $Y_{agri}/K > Y_{manu}/K$ holds even after accounting for wages/profits.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Smith's key arguments from the context. Agricultural capital (1) employs more productive labor (including laboring cattle), (2) benefits from nature's 'free' contribution, and (3) generates proportionally greater value-added to annual produce compared to manufacturing capital.\n\nStep 2: Establish the productivity components. Agricultural productivity = (Human labor + Animal labor + Nature's contribution) per unit capital. Manufacturing productivity = (Human labor only) per unit capital.\n\nStep 3: Factor in nature's role. Smith explicitly states that 'Nature labours along with man' in agriculture at no cost, effectively increasing output without requiring capital expenditure. This creates a productivity multiplier absent in manufacturing.\n\nStep 4: Compare value creation. The text states agricultural capital 'adds a much greater value to the annual produce... in proportion to the quantity of productive labour employed.' Even with equal labor quantities, agricultural output benefits from natural processes (e.g., crop growth, soil fertility) that enhance yield.\n\nDerivation Result: Agricultural capital is more productive than manufacturing capital because:\n1. It utilizes three productive factors (humans, animals, nature) vs. one (humans)\n2. Nature's zero-cost contribution creates surplus value\n3. This combination enables greater output per capital unit invested, as reflected in higher annual produce value.\n\nQID: textbook-112-16-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-16-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately captures Smith's arguments about the comparative productivity of agricultural and manufacturing capital, including the role of nature, and aligns with the gold answer's key points and conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-16-0-0", "category": "CORRECT", "explanation": "The candidate answer accurately captures Smith's arguments about the comparative productivity of agricultural and manufacturing capital, including the role of nature, and aligns with the gold answer's key points and conclusion."}, "llm_echoed_qid": "textbook-112-16-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately captures Smith's arguments about the comparative productivity of agricultural and manufacturing capital, including the role of nature, and aligns with the gold answer's key points and conclusion."}
{"script_processing_qid": "textbook-114-32-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Define economic rent and explain how it differs from accounting profit in the long run. Provide a mathematical expression for economic rent.\n\nGOLD_ANSWER:\n1. **Definition**: Economic rent is the difference between what firms are willing to pay for an input and the minimum amount necessary to obtain it. \n2. **Difference from Accounting Profit**: Accounting profit includes all revenues minus explicit costs, while economic profit subtracts both explicit and implicit costs (including opportunity costs). In the long run, economic profit is zero due to competition, but economic rent can persist.\n3. **Mathematical Expression**: Economic Rent = Payment for Input - Minimum Supply Price of Input.\n\nCANDIDATE_ANSWER:\nStep 1: Define Economic Rent\nEconomic rent is the difference between the amount firms are willing to pay for a scarce input and the minimum payment required to secure that input. It arises due to the scarcity of certain production factors (e.g., a prime river location for a firm).\n\nStep 2: Differentiate from Accounting Profit in the Long Run\n- Accounting profit = Total revenue - Explicit costs (e.g., wages, materials).\n- In competitive markets, long-run economic profit = Total revenue - (Explicit + Implicit costs) = 0 due to free entry/exit.\n- Economic rent is not subtracted in accounting profit calculations. Firms can earn accounting profit equal to economic rent in the long run, but economic profit remains zero because economic rent represents returns to scarce inputs (not excess profits).\n\nStep 3: Mathematical Expression\nEconomic Rent = Payment made for the input − Minimum payment required to retain the input\nFormally: \\( ER = P_{\\text{paid}} - P_{\\text{min}} \\)\nExample: If a firm pays $10,000 annually for a uniquely productive resource that would stay for $7,000, the economic rent is $3,000.\n\nKey Insight: Economic rent persists in the long run as part of producer surplus (from scarce inputs), while economic profit is driven to zero by competition. Accounting profit includes economic rent but excludes implicit costs, unlike economic profit.\n\nQID: textbook-114-32-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-32-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately defines economic rent, clearly differentiates it from accounting profit in the long run, and provides a correct mathematical expression. The explanation aligns with the gold answer, including the persistence of economic rent and the zeroing of economic profit in competitive markets.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-32-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately defines economic rent, clearly differentiates it from accounting profit in the long run, and provides a correct mathematical expression. The explanation aligns with the gold answer, including the persistence of economic rent and the zeroing of economic profit in competitive markets."}, "llm_echoed_qid": "textbook-114-32-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately defines economic rent, clearly differentiates it from accounting profit in the long run, and provides a correct mathematical expression. The explanation aligns with the gold answer, including the persistence of economic rent and the zeroing of economic profit in competitive markets."}
{"script_processing_qid": "textbook-118-9-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the variance of the standardized random walk process $w_{t}$ and explain why it is nonstationary.\n\nGOLD_ANSWER:\n1. **Definition**: $w_{t} = w_{t-1} + \\varepsilon_{t}$, $\\varepsilon_{t} \\sim \\operatorname{IID}(0,1)$.\\n2. **Recursive Solution**: $w_{t} = \\sum_{s=1}^{t} \\varepsilon_{s}$.\\n3. **Variance Calculation**: $\\mathrm{Var}(w_{t}) = \\mathrm{Var}\\left(\\sum_{s=1}^{t} \\varepsilon_{s}\\right) = \\sum_{s=1}^{t} \\mathrm{Var}(\\varepsilon_{s}) = t$.\\n4. **Nonstationarity**: The variance depends on $t$ and tends to infinity as $t \\to \\infty$, violating stationarity conditions.\n\nCANDIDATE_ANSWER:\nStep 1: Expand the random walk process recursively:\nw_t = w_{t-1} + ε_t\n     = w_{t-2} + ε_{t-1} + ε_t\n     = ...\n     = w_0 + ε_1 + ε_2 + ... + ε_t\nSince w_0 = 0, we have w_t = Σ_{i=1}^t ε_i\n\nStep 2: Compute variance using IID property of ε_t:\nVar(w_t) = Var(Σ_{i=1}^t ε_i) = Σ_{i=1}^t Var(ε_i) (due to independence)\nGiven Var(ε_i) = 1 for all i, this becomes Var(w_t) = t\n\nStep 3: Analyze stationarity:\nFor a process to be stationary, its variance must be constant over time. Since Var(w_t) = t grows without bound as t→∞, the variance depends critically on time. This violates the stationarity requirement of constant variance, making the random walk process nonstationary.\n\nQID: textbook-118-9-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-9-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the variance of the standardized random walk process as t and explains its nonstationarity due to the variance's dependence on time, matching the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-9-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the variance of the standardized random walk process as t and explains its nonstationarity due to the variance's dependence on time, matching the gold answer."}, "llm_echoed_qid": "textbook-118-9-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the variance of the standardized random walk process as t and explains its nonstationarity due to the variance's dependence on time, matching the gold answer."}
{"script_processing_qid": "textbook-104-6-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. (Qualitative) Contrast the informational assumptions in Propositions 6 and 8. Why does Proposition 8 require Assumption 17.4 despite its stronger restrictions?\n\nGOLD_ANSWER:\n- **Proposition 6**: Relies on the Contraction Mapping Theorem (Assumption 17.2) to guarantee uniqueness. The informational structure allows shocks to be partially observed via prices.  \n- **Proposition 8**: Ensures monotonicity of the price function $\\rho(z)$, which is necessary for equilibrium interpretation. Assumption 17.4 ($\\phi(y) = yV^{\\prime}(y)$ being concave) guarantees that agents' inferences about $x/\\theta$ from prices are well-behaved.  \n- **Trade-off**: Without Assumption 17.4, solutions to (3) may exist (per Proposition 7) but lack economic meaning as equilibria due to non-monotonic prices.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key assumptions in Propositions 6 and 8.\n- Proposition 6 requires Assumptions 17.1 and 17.2 (likely continuity/compactness for Contraction Mapping Theorem).\n- Proposition 8 requires Assumptions 17.1 and 17.4 (monotonicity-related).\n\nStep 2: Contrast informational implications:\n- Proposition 6 guarantees uniqueness under general conditions but does not require monotonicity of equilibrium objects. Its assumptions allow equilibrium existence without restricting how agents process nominal/real shock signals (θ and x).\n- Proposition 8 establishes strict monotonicity of ρ(z) = z/η(z), which depends crucially on how η(z) responds to z. This requires Assumption 17.4 to ensure the fixed point η(z) inherits monotonicity from operators involving G(dθ|z) and θ′/θ̃ terms.\n\nStep 3: Explain why Proposition 8 needs Assumption 17.4 despite stronger restrictions:\nMonotonicity of ρ(z) requires coordinated responses to z across both Θ integrals in operator T. Assumption 17.4 likely imposes distributional regularity (e.g., affiliation/stochastic monotonicity of G(θ|z)) to ensure θ′/θ̃ and ζ^{-1}(f(z′)) terms preserve order. Without this, even with uniqueness (stronger restriction in Proposition 6), the equilibrium η(z) might not be monotonic due to ambiguous informational feedback between z and θ.\n\nQID: textbook-104-6-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-104-6-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately contrasts the informational assumptions in Propositions 6 and 8, identifies the key assumptions, and explains why Proposition 8 requires Assumption 17.4 despite its stronger restrictions. The reasoning aligns with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-104-6-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately contrasts the informational assumptions in Propositions 6 and 8, identifies the key assumptions, and explains why Proposition 8 requires Assumption 17.4 despite its stronger restrictions. The reasoning aligns with the gold answer."}, "llm_echoed_qid": "textbook-104-6-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately contrasts the informational assumptions in Propositions 6 and 8, identifies the key assumptions, and explains why Proposition 8 requires Assumption 17.4 despite its stronger restrictions. The reasoning aligns with the gold answer."}
{"script_processing_qid": "textbook-112-23-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. (Conceptual) Analyze Smith's linkage between commercial confidence and state borrowing capacity. How do his conditions for flourishing commerce (e.g., rule of law) create systemic risks when violated?\n\nGOLD_ANSWER:\n1.  **Confidence Mechanism**: \n    - Merchants lend when property rights are secure (\"faith of contracts supported by law\").\n    - Reciprocal trust enables liquidity (e.g., transferable government securities).\n\n2.  **Systemic Risks**: \n    - Erosion of legal safeguards → capital flight (hoarding behavior in \"rude states\").\n    - Loss of market confidence → higher risk premiums (60-70% discounts on French billets d’etat).\n\n3.  **Modern Implications**: Sovereign debt crises often stem from institutional decay, validating Smith's framework.\n\nCANDIDATE_ANSWER:\nStep 1: Identify Smith's core linkage. Smith argues that commercial confidence (rooted in institutions like the rule of law) enables governments to borrow freely, creating a false sense of security. States become dependent on perpetual debt instruments (e.g., perpetual funding) rather than fiscal discipline.\n\nStep 2: Analyze systemic risk creation. Violations of commercial trust conditions (e.g., weak rule of law, fiscal irresponsibility) expose two risks: (1) Creditors lose confidence in revenue streams tied to mortgaged public funds (e.g., the South Sea Company's £9M advance), destabilizing debt markets. (2) Reliance on 'anticipation' borrowing (short-term debt backed by future tax receipts) becomes unsustainable when revenue predictability breaks down, as seen in fluctuating Bank of England interest rates (8% to 3%).\n\nStep 3: Connect to systemic collapse. These mechanisms create a feedback loop: Debt accumulation erodes the fiscal transparency and institutional reliability that initially enabled borrowing, leading to credit contraction or sovereign default risks. Perpetual funding transforms temporary deficits into structural liabilities, magnifying vulnerability to external shocks.\n\nQID: textbook-112-23-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-23-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies Smith's linkage between commercial confidence and state borrowing capacity, analyzes systemic risks arising from violations of trust conditions, and connects these mechanisms to potential systemic collapse. The response aligns well with the gold answer, covering confidence mechanisms, systemic risks, and modern implications.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-23-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately identifies Smith's linkage between commercial confidence and state borrowing capacity, analyzes systemic risks arising from violations of trust conditions, and connects these mechanisms to potential systemic collapse. The response aligns well with the gold answer, covering confidence mechanisms, systemic risks, and modern implications."}, "llm_echoed_qid": "textbook-112-23-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies Smith's linkage between commercial confidence and state borrowing capacity, analyzes systemic risks arising from violations of trust conditions, and connects these mechanisms to potential systemic collapse. The response aligns well with the gold answer, covering confidence mechanisms, systemic risks, and modern implications."}
{"script_processing_qid": "textbook-25-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the asymptotic variance of the OLS estimator $\\widehat{\\pmb{\\delta}}_{\\mathrm{OLS}}$ when $\\mathbf{x}_{t}=\\mathbf{z}_{t}$ and explain why GLS may not be consistent in this context.\n\nGOLD_ANSWER:\n1.  **Asymptotic Variance of OLS**: Given $\\mathbf{x}_{t} = \\mathbf{z}_{t}$, the OLS estimator is $\\widehat{\\pmb{\\delta}}_{\\mathrm{OLS}} = (\\mathbf{Z}^{\\prime}\\mathbf{Z})^{-1}\\mathbf{Z}^{\\prime}\\mathbf{y}$. Its asymptotic variance is:\n   $$\\widehat{\\bf A\\mathrm{var}(\\widehat{\\pmb{\\delta}}_{\\mathrm{OLS}})} = n \\cdot (\\mathbf{Z}^{\\prime}\\mathbf{Z})^{-1}(\\mathbf{Z}^{\\prime}\\widehat{\\pmb{\\Omega}}\\mathbf{Z})(\\mathbf{Z}^{\\prime}\\mathbf{Z})^{-1}.$$\n2.  **GLS Inconsistency**: GLS requires strict exogeneity of regressors for consistency. If regressors are merely predetermined (not strictly exogenous), $\\operatorname{E}(\\mathbf{z}_{s}\\cdot\\varepsilon_{v}) \\neq 0$ for $s \\neq v$, leading to inconsistent GLS estimates.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the OLS estimator in the context where 𝐱_t = 𝐳_t.\nWhen regressors equal instruments (𝐗 = 𝐙), the OLS estimator becomes:\n\n$$\\widehat{\\pmb{\\delta}}_{\\mathrm{OLS}} = (\\mathbf{Z'Z})^{-1}\\mathbf{Z'y}$$\n\nUnder GMM framework, this is equivalent to the efficient GMM estimator.\n\nStep 2: Derive asymptotic variance using GMM formula\nFrom Eq (6.7.10) in the context, substitute 𝐗 = 𝐙:\n\n$$\n\\widehat{\\mathbf{Avar}}(\\widehat{\\pmb{\\delta}}_{\\mathrm{OLS}}) = n \\cdot (\\mathbf{Z'Z})^{-1}(\\mathbf{Z'}\\widehat{\\pmb{\\Omega}}\\mathbf{Z})(\\mathbf{Z'Z})^{-1}\n$$\n\nwhere $\\widehat{\\pmb{\\Omega}}$ is a HAC estimator accounting for serial correlation.\n\nStep 3: Explain GLS inconsistency\nGLS requires $\\mathbf{\\Omega}$ (error covariance matrix) to be known or parameterized correctly. In this context:\n\n- Serial correlation under market efficiency vanishes after finite lags (implied by Gordin’s condition).\n- GLS assumes a specific parametric structure for $\\mathbf{\\Omega}$, which may mismatch the true autocorrelation pattern.\n- If the assumed structure is incorrect, the GLS transformation fails to orthogonalize errors, leading to biased estimates. OLS+GMM remains consistent because it uses a non-parametric HAC estimator ($\\widehat{\\pmb{\\Omega}}$) robust to general forms of serial correlation.\n\nQID: textbook-25-0-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-25-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derived the asymptotic variance of the OLS estimator and provided a valid explanation for why GLS may not be consistent in this context. The reasoning aligns with the gold answer, including the use of the GMM framework and the discussion on the requirements and potential failures of GLS.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-25-0-0-0", "category": "CORRECT", "explanation": "The candidate correctly derived the asymptotic variance of the OLS estimator and provided a valid explanation for why GLS may not be consistent in this context. The reasoning aligns with the gold answer, including the use of the GMM framework and the discussion on the requirements and potential failures of GLS."}, "llm_echoed_qid": "textbook-25-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derived the asymptotic variance of the OLS estimator and provided a valid explanation for why GLS may not be consistent in this context. The reasoning aligns with the gold answer, including the use of the GMM framework and the discussion on the requirements and potential failures of GLS."}
{"script_processing_qid": "textbook-114-3-0-5", "llm_grader_input_prompt_user": "\nQUESTION:\n6. Analyze the 'Battle of the Sexes' game (Table 13.7). Identify the pure and mixed strategy Nash equilibria and discuss their realism.\n\nGOLD_ANSWER:\n1. **Pure Strategy Nash Equilibria**: (Wrestling, Wrestling) and (Opera, Opera). Both are stable as neither player can improve their payoff by deviating unilaterally.\n2. **Mixed Strategy Nash Equilibrium**: Jim chooses Wrestling with probability 2/3 and Opera with 1/3; Joan chooses Wrestling with 1/3 and Opera with 2/3. Expected payoff is 2/3 for both.\n3. **Realism**: Pure strategies are more realistic as they guarantee a payoff of at least 1, whereas mixed strategies yield lower expected payoffs and may not reflect real behavior.\n\nCANDIDATE_ANSWER:\nStep 1: Pure Strategy Nash Equilibria\nIn the Battle of the Sexes game, there are two pure strategy Nash equilibria. These occur when both players coordinate on the same outcome. For example, if Player 1 chooses Football (F) and Player 2 chooses Football (F), neither has an incentive to unilaterally deviate (Player 1 receives their highest payoff for F, and Player 2 prefers coordinating over being alone). Similarly, if both choose Ballet (B), this is also a Nash equilibrium.\n\nStep 2: Mixed Strategy Nash Equilibrium\nTo compute the mixed strategy equilibrium, assume Player 1 chooses F with probability \\( p \\) and B with \\( 1-p \\). Player 2 chooses F with probability \\( q \\) and B with \\( 1-q \\). For Player 2 to be indifferent between F and B: \\( p \\cdot 1 + (1-p) \\cdot 0 = p \\cdot 0 + (1-p) \\cdot 2 \\), solving gives \\( p = \\frac{2}{3} \\). For Player 1 to be indifferent: \\( q \\cdot 2 + (1-q) \\cdot 0 = q \\cdot 0 + (1-q) \\cdot 1 \\), solving gives \\( q = \\frac{1}{3} \\). Thus, the mixed strategy equilibrium is \\( \\left(p = \\frac{2}{3}, q = \\frac{1}{3}\right) \\).\n\nStep 3: Realism Discussion\nPure strategy equilibria require coordination, which may be unrealistic without communication. Mixed strategies involve randomizing actions, which is theoretically stable but often impractical in real-world scenarios where players may not randomize optimally or consistently.\n\nQID: textbook-114-3-0-5\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-3-0-5\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the pure and mixed strategy Nash equilibria but mislabels the actions as 'Football' and 'Ballet' instead of 'Wrestling' and 'Opera'. The reasoning for realism is correct but slightly less detailed than the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-3-0-5", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the pure and mixed strategy Nash equilibria but mislabels the actions as 'Football' and 'Ballet' instead of 'Wrestling' and 'Opera'. The reasoning for realism is correct but slightly less detailed than the gold answer."}, "llm_echoed_qid": "textbook-114-3-0-5", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the pure and mixed strategy Nash equilibria but mislabels the actions as 'Football' and 'Ballet' instead of 'Wrestling' and 'Opera'. The reasoning for realism is correct but slightly less detailed than the gold answer."}
{"script_processing_qid": "textbook-82-2-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Rotemberg and Woodford (1996b) analyze oil price shocks. Using Equations (2.30) and (2.31), explain why a decline in real wages ($w/\\tilde{P}$) following an oil shock implies countercyclical markups, even though oil prices are not technological shocks.\n\nGOLD_ANSWER:\n1.  **Equation (2.30)**: $\\mu = \\frac{\\tilde{P} V_H(K, z H)}{W}$, where $\\tilde{P}$ is the adjusted price index (Equation 2.31).\n2.  **Oil shock effect**: If labor demand contracts ($H$ decreases), $V_H$ should increase. If $W/\\tilde{P}$ does not rise (or falls), $\\mu$ must increase to satisfy Equation (2.30).\n3.  **Key distinction**: Unlike technological shocks, oil shocks do not shift $V_H$ directly. Thus, the decline in $w/\\tilde{P}$ cannot be attributed to a technology shift and must reflect markup increases.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the markup formula from the context: \\(\\mu = \\frac{P z F_H(u_K K, z H)}{W}\\). This implies \\(\\mu = \\frac{z F_H}{W/P}\\), where \\(W/P\\) is the real wage.\n\nStep 2: Under diminishing marginal product of labor (MPL), a decline in output (e.g., from an oil shock) typically reduces labor input \\(H\\). This would raise \\(F_H\\) (MPL) due to diminishing returns, leading to higher real wages \\(W/P\\) if markups are constant.\n\nStep 3: However, Rotemberg and Woodford observe that real wages \\(W/P\\) *decline* after oil shocks. For this to occur despite falling \\(H\\) (which should raise \\(F_H\\)), the markup \\(\\mu\\) must rise to offset the MPL increase.\n\nStep 4: Since output is falling (countercyclical) and markups \\(\\mu\\) are rising, this confirms countercyclical markups. The oil shock reduces output but increases markups, as firms pass cost shocks to prices while suppressing real wages.\n\nQID: textbook-82-2-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-82-2-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately follows the reasoning in the gold answer, explaining how a decline in real wages implies countercyclical markups after an oil shock, despite the absence of technological shifts. The steps logically connect the decline in labor input, the rise in marginal product of labor, and the necessary markup adjustment to observe falling real wages.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-82-2-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately follows the reasoning in the gold answer, explaining how a decline in real wages implies countercyclical markups after an oil shock, despite the absence of technological shifts. The steps logically connect the decline in labor input, the rise in marginal product of labor, and the necessary markup adjustment to observe falling real wages."}, "llm_echoed_qid": "textbook-82-2-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately follows the reasoning in the gold answer, explaining how a decline in real wages implies countercyclical markups after an oil shock, despite the absence of technological shifts. The steps logically connect the decline in labor input, the rise in marginal product of labor, and the necessary markup adjustment to observe falling real wages."}
{"script_processing_qid": "textbook-112-31-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the data from the herring fishery bounty table, calculate the average bounty per fully packed barrel for the years 1771–1778. Show your steps.\n\nGOLD_ANSWER:\n1.  **Total Bounty Paid (1771–1778)**: £165,463 14s 0d (from the table).\n2.  **Total Barrels Fully Packed**: 252,231 (after deducting 1/3 of sea-sticks).\n3.  **Average Bounty per Barrel**:\n   $$ \\frac{£165,463.7}{252,231} \\approx £0.656 \\text{ per barrel} $$\n   (Note: Detailed currency conversion of shillings and pence is required for precise calculation.)\n\nCANDIDATE_ANSWER:\nStep 1: The question requires calculating the average bounty per fully packed barrel for 1771–1778 using data from a herring fishery bounty table. However, the provided context exclusively contains theoretical arguments from Adam Smith about agricultural/commercial systems and natural liberty, with no numerical data, tables, or bounty figures. Step 2: Since no actual bounty values, barrel quantities, or year-specific data are included in the passages, it is impossible to compute the requested average. Step 3: The absence of the critical dataset (the bounty table) means the question cannot be answered with the given context.\n\nQID: textbook-112-31-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-31-0-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the absence of necessary data.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-31-0-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the absence of necessary data."}, "llm_echoed_qid": "textbook-112-31-0-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the absence of necessary data."}
{"script_processing_qid": "textbook-106-10-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the deadweight loss of a tax on land is zero, using the concepts of elasticity and market distortion.\n\nGOLD_ANSWER:\n1.  **Zero Elasticity of Supply**: Land supply is perfectly inelastic ($\\eta_s = 0$), so quantity does not respond to price changes.\n2.  **No Behavioral Change**: Tax does not alter landowners' behavior; they cannot reduce supply.\n3.  **No Market Shrinkage**: Equilibrium quantity remains unchanged, so no deadweight loss triangle forms.\n4.  **Full Tax Incidence**: Tax burden falls entirely on landowners, with no efficiency loss.\n\nCANDIDATE_ANSWER:\nStep 1: Deadweight loss (DWL) occurs when a tax distorts market behavior, reducing the quantity traded below the efficient equilibrium. This distortion depends on the price elasticities of supply and demand.\nStep 2: Land has a perfectly inelastic supply (elasticity = 0), meaning the quantity supplied cannot change regardless of price. A tax on land does not alter the equilibrium quantity because suppliers cannot reduce production (e.g., landowners cannot 'produce' less land).\nStep 3: Since the quantity traded remains unchanged, there is no market distortion (no reduction in mutually beneficial transactions). The tax burden falls entirely on landowners (suppliers) via lower net prices, but buyers pay the same market price as before the tax.\nStep 4: DWL is calculated as ½ × tax × ΔQ. With ΔQ = 0 due to perfectly inelastic supply, the DWL formula yields zero. Thus, no efficiency loss occurs.\n\nQID: textbook-106-10-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-106-10-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly explains why the deadweight loss of a tax on land is zero by detailing the perfectly inelastic supply of land, the unchanged equilibrium quantity, and the absence of market distortion. The reasoning aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-10-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly explains why the deadweight loss of a tax on land is zero by detailing the perfectly inelastic supply of land, the unchanged equilibrium quantity, and the absence of market distortion. The reasoning aligns with the gold answer."}, "llm_echoed_qid": "textbook-106-10-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly explains why the deadweight loss of a tax on land is zero by detailing the perfectly inelastic supply of land, the unchanged equilibrium quantity, and the absence of market distortion. The reasoning aligns with the gold answer."}
{"script_processing_qid": "textbook-107-9-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the equilibrium outputs for the leader and follower in a Stackelberg duopoly given the demand function $P=1,020-2Q$ and constant marginal cost $MC=20$.\n\nGOLD_ANSWER:\n1. **Follower's Reaction Function**: The follower maximizes $\\pi_f = (1,020 - 2(q_l + q_f))q_f - 20q_f$. FOC: $1,020 - 2q_l - 4q_f - 20 = 0 \\Rightarrow q_f = 250 - 0.5q_l$.\n2. **Leader's Profit Maximization**: The leader incorporates the follower's reaction: $\\pi_l = (1,020 - 2(q_l + 250 - 0.5q_l))q_l - 20q_l = (1,020 - 2q_l - 500 + q_l)q_l - 20q_l = (520 - q_l)q_l - 20q_l$.\n3. **Leader's FOC**: $520 - 2q_l - 20 = 0 \\Rightarrow q_l = 250$.\n4. **Follower's Output**: $q_f = 250 - 0.5(250) = 125$.\n5. **Total Output**: $Q = q_l + q_f = 250 + 125 = 375$.\n\nCANDIDATE_ANSWER:\nStep 1: Define total output Q = Qₗ + Qᴵ (leader + follower)\nStep 2: Follower's reaction function derived from profit maximization:\n       πᴵ = [1020 - 2(Qₗ + Qᴵ) - 20]Qᴵ\n       First-order condition yields: Qᴵ = (1000 - 2Qₗ)/4 = 250 - 0.5Qₗ\nStep 3: Leader incorporates reaction function into their profit maximization:\n       πₗ = [1020 - 2(Qₗ + (250 - 0.5Qₗ)) - 20]Qₗ\n       Simplify to: πₗ = (500 - Qₗ)Qₗ\nStep 4: Maximize leader's profit by taking derivative:\n       dπₗ/dQₗ = 500 - 2Qₗ = 0 ⇒ Qₗ* = 250\nStep 5: Calculate follower's equilibrium output:\n       Qᴵ* = 250 - 0.5(250) = 125\nEquilibrium outputs: Leader = 250 units, Follower = 125 units. Total output = 375 units.\n\nQID: textbook-107-9-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-9-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows all steps of deriving the equilibrium outputs for both the leader and follower in the Stackelberg duopoly, matching the gold answer exactly in methodology and results.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-9-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows all steps of deriving the equilibrium outputs for both the leader and follower in the Stackelberg duopoly, matching the gold answer exactly in methodology and results."}, "llm_echoed_qid": "textbook-107-9-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows all steps of deriving the equilibrium outputs for both the leader and follower in the Stackelberg duopoly, matching the gold answer exactly in methodology and results."}
{"script_processing_qid": "textbook-103-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain why the reservation wage $\\bar{w}(\\epsilon)$ increases with nonhuman wealth $\\epsilon$. Use the equation $$\\frac{\\bar{w}(\\epsilon)}{1-\\beta} = c + (1-\\phi)\\epsilon + \\beta Q - \\frac{\\beta}{1-\\beta}\\phi E(\\epsilon).$$\n\nGOLD_ANSWER:\n1.  **Equation**: $\\frac{\\bar{w}(\\epsilon)}{1-\\beta} = c + (1-\\phi)\\epsilon + \\beta Q - \\frac{\\beta}{1-\\beta}\\phi E(\\epsilon)$.\n2.  **Dependence on $\\epsilon$**: Since $0 < \\phi < 1$, the term $(1-\\phi)\\epsilon$ is positive and increasing in $\\epsilon$.\n3.  **Conclusion**: $\\bar{w}(\\epsilon)$ increases with $\\epsilon$ because higher nonhuman wealth reduces the urgency to accept lower-wage jobs.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the reservation wage equation:\n$$\n\\frac{\\bar{w}(\\epsilon)}{1-\\beta} = c + (1-\\phi)\\epsilon + \\beta Q - \\frac{\\beta}{1-\\beta}\\phi E(\\epsilon)\n$$\n\nStep 2: Multiply both sides by (1-β) to isolate $\\bar{w}(\\epsilon)$:\n$$\n\\bar{w}(\\epsilon) = (1-\\beta)c + (1-\\beta)(1-\\phi)\\epsilon + \\beta(1-\\beta)Q - \\beta\\phi E(\\epsilon)\n$$\n\nStep 3: Focus on the terms containing $\\epsilon$:\nThe direct effect from nonhuman wealth appears in two components:\n1. Positive term: $(1-\\beta)(1-\\phi)\\epsilon$\n2. Negative term: $-\\beta\\phi E(\\epsilon)$ (where $E(\\epsilon)$ represents expected future wealth)\n\nStep 4: Assume persistence in wealth ($E(\\epsilon)$ ≈ $\\epsilon$ for simplicity):\n$$\n\\bar{w}(\\epsilon) = \\text{constants} + [(1-\\beta)(1-\\phi) - \\beta\\phi]\\epsilon\n$$\n\nStep 5: Analyze the net coefficient:\n$$\n(1-\\beta)(1-\\phi) - \\beta\\phi = (1-\\phi) - \\beta(1-\\phi + \\phi) = 1-\\phi -\\beta\n$$\n\nStep 6: The reservation wage increases with $\\epsilon$ if:\n$$\n1-\\phi > \\beta\n$$\nThis condition typically holds because:\n- $\\beta$ (discount factor) is < 1\n- $\\phi$ (tax/decay rate on wealth) is small\n\nStep 7: Economic intuition:\nHigher nonhuman wealth $\\epsilon$:\n1. Increases consumption capacity ($(1-\\phi)\\epsilon$ term)\n2. Reduces urgency to accept low wages\n3. Offsets only partially by future wealth taxation ($\\beta\\phi E(\\epsilon)$ term)\n\nThus, $\\bar{w}(\\epsilon)$ increases with $\\epsilon$ because the positive direct effect dominates the negative expectation term under standard parameter assumptions.\n\nQID: textbook-103-0-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-103-0-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the mathematical derivation and economic intuition behind why the reservation wage increases with nonhuman wealth, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-0-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the mathematical derivation and economic intuition behind why the reservation wage increases with nonhuman wealth, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-103-0-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the mathematical derivation and economic intuition behind why the reservation wage increases with nonhuman wealth, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-115-23-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Discuss the computational form of the NTW statistic using auxiliary regressions.\n\nGOLD_ANSWER:\n1. **Computational Form**: The NTW statistic can be computed as $N-\\mathrm{SSR}_{0}=N R_{0}^{2}$ from the regression $$ 1 \\text{ on } \\hat{\\mathbf{s}}_{i}^{\\prime}, \\hat{\\mathbf{g}}_{i}^{\\prime}, \\qquad i=1,2,\\ldots,N $$ where $\\mathrm{SSR}_{0}$ is the usual sum of squared residuals.\n\nCANDIDATE_ANSWER:\nStep 1: The NTW statistic tests moment conditions derived from the conditional density specification. The core idea involves auxiliary regressions to adjust for parameter estimation effects (via the scores).\n\nStep 2: Let $\\hat{\\mathbf{g}}_i = \\mathbf{g}(\\mathbf{w}_i, \\hat{\\pmb{\\theta}})$ (moment conditions) and $\\hat{\\mathbf{s}}_i = \\nabla_{\\pmb{\\theta}} \\log f(\\mathbf{y}_i|\\mathbf{x}_i; \\hat{\\pmb{\\theta}})$ (score vector).\n\nStep 3: Perform auxiliary regressions of each element of $\\hat{\\mathbf{g}}_i$ on $\\hat{\\mathbf{s}}_i$ to obtain projection coefficients $\\hat{\\Pi}$. Specifically, regress $\\hat{\\mathbf{g}}_i$ (dependent variable) on $\\hat{\\mathbf{s}}_i$ (independent variables) to estimate $\\hat{\\Pi}$ via OLS.\n\nStep 4: Compute residuals $\\hat{\\mathbf{g}}_i - \\hat{\\mathbf{s}}_i\\hat{\\Pi}$. These residuals account for the fact that $\\pmb{\\theta}$ is estimated rather than known.\n\nStep 5: Construct the robust covariance matrix estimator as $\\sum_{i=1}^N (\\hat{\\mathbf{g}}_i - \\hat{\\mathbf{s}}_i\\hat{\\Pi})(\\hat{\\mathbf{g}}_i - \\hat{\\mathbf{s}}_i\\hat{\\Pi})'$. This adjusts for the influence of $\\hat{\\pmb{\\theta}}$ estimation.\n\nStep 6: Form the quadratic NTW statistic using the summed moment conditions and the inverse of the covariance matrix. The auxiliary regressions ensure the statistic accounts for model parameter uncertainty while testing the $Q$ moment conditions.\n\nQID: textbook-115-23-2-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-23-2-1\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer provides a detailed explanation of the NTW statistic's computational process, including auxiliary regressions and adjustments for parameter estimation. However, it does not directly match the specific computational form given in the gold answer, which focuses on the regression of 1 on $\\hat{\\mathbf{s}}_{i}^{\\prime}, \\hat{\\mathbf{g}}_{i}^{\\prime}$ and the resulting $N R_{0}^{2}$ statistic. The reasoning is correct but does not align precisely with the gold answer's focus.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-23-2-1", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer provides a detailed explanation of the NTW statistic's computational process, including auxiliary regressions and adjustments for parameter estimation. However, it does not directly match the specific computational form given in the gold answer, which focuses on the regression of 1 on $\\hat{\\mathbf{s}}_{i}^{\\prime}, \\hat{\\mathbf{g}}_{i}^{\\prime}$ and the resulting $N R_{0}^{2}$ statistic. The reasoning is correct but does not align precisely with the gold answer's focus."}, "llm_echoed_qid": "textbook-115-23-2-1", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer provides a detailed explanation of the NTW statistic's computational process, including auxiliary regressions and adjustments for parameter estimation. However, it does not directly match the specific computational form given in the gold answer, which focuses on the regression of 1 on $\\hat{\\mathbf{s}}_{i}^{\\prime}, \\hat{\\mathbf{g}}_{i}^{\\prime}$ and the resulting $N R_{0}^{2}$ statistic. The reasoning is correct but does not align precisely with the gold answer's focus."}
{"script_processing_qid": "textbook-115-6-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. For the Poisson random effects model (19.60)–(19.62), derive the conditional variance $\\operatorname{Var}(y_{i t}|\\mathbf{x}_{i})$ and show how it implies overdispersion. How does this relate to the robust variance estimator $\\hat{\\mathbf{A}}^{-1}\\hat{\\mathbf{B}}\\hat{\\mathbf{A}}^{-1}/N$?\n\nGOLD_ANSWER:\n1.  **Conditional variance**: $\\operatorname{Var}(y_{i t}|\\mathbf{x}_{i}) = \\operatorname{E}[\\operatorname{Var}(y_{i t}|\\mathbf{x}_{i},c_{i})|\\mathbf{x}_{i}] + \\operatorname{Var}[\\operatorname{E}(y_{i t}|\\mathbf{x}_{i},c_{i})|\\mathbf{x}_{i}] = m_{i t}(\\pmb{\\beta}_{\\mathrm{o}}) + \\eta_{\\mathrm{o}}^{2}m_{i t}^{2}(\\pmb{\\beta}_{\\mathrm{o}})$.\n2.  **Overdispersion**: The term $\\eta_{\\mathrm{o}}^{2}m_{i t}^{2}(\\pmb{\\beta}_{\\mathrm{o}})$ implies variance exceeds mean unless $\\eta_{\\mathrm{o}}^{2} = 0$.\n3.  **Robust estimator**: $\\hat{\\mathbf{A}}^{-1}\\hat{\\mathbf{B}}\\hat{\\mathbf{A}}^{-1}/N$ accounts for this overdispersion and serial correlation via $\\hat{\\mathbf{B}} = N^{-1}\\sum_{i=1}^{N}\\mathbf{s}_{i}(\\hat{\\pmb{\\beta}})\\mathbf{s}_{i}(\\hat{\\pmb{\\beta}})'$, where $\\mathbf{s}_{i}(\\pmb{\\beta}) = \\sum_{t=1}^{T}\\mathbf{s}_{i t}(\\pmb{\\beta})$.\n\nCANDIDATE_ANSWER:\nStep 1: Apply the law of total variance to decompose Var(y_{it}|x_i)\nVar(y_{it}|x_i) = E[Var(y_{it}|x_i,c_i)|x_i] + Var[E(y_{it}|x_i,c_i)|x_i]\n\nStep 2: Compute first term (Poisson variance given c_i)\nVar(y_{it}|x_i,c_i) = c_i m(x_{it},β_o) (property of Poisson)\nE[c_i m(x_{it},β_o)|x_i] = m(x_{it},β_o) (since E(c_i)=1)\n\nStep 3: Compute second term (variance of conditional mean)\nE(y_{it}|x_i,c_i) = c_i m(x_{it},β_o)\nVar(c_i m(x_{it},β_o)|x_i) = [m(x_{it},β_o)]^2 Var(c_i) = [m(x_{it},β_o)]^2/δ_o\n(From Gamma(δ_o,δ_o) distribution: Var(c_i) = 1/δ_o)\n\nStep 4: Combine results\nVar(y_{it}|x_i) = m(x_{it},β_o) + [m(x_{it},β_o)]^2/δ_o > m(x_{it},β_o)\nThis shows overdispersion (variance > mean).\n\nStep 5: Relate to robust variance estimator\nOverdispersion violates Poisson variance assumption. The robust sandwich estimator Â⁻¹B̂Â⁻¹/N accounts for this by:\n- Â = -E[Hessian] captures curvature (local variance)\n- B̂ = E[outer product of scores] captures true variance structure\n- The [m(x_{it},β_o)]^2/δ_o term in Var(y_{it}|x_i) contributes to B̂, making the robust SEs valid despite overdispersion.\n\nQID: textbook-115-6-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-6-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the conditional variance, demonstrates overdispersion, and accurately relates it to the robust variance estimator. The steps and reasoning align perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-6-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the conditional variance, demonstrates overdispersion, and accurately relates it to the robust variance estimator. The steps and reasoning align perfectly with the gold answer."}, "llm_echoed_qid": "textbook-115-6-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the conditional variance, demonstrates overdispersion, and accurately relates it to the robust variance estimator. The steps and reasoning align perfectly with the gold answer."}
{"script_processing_qid": "textbook-94-7-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the concept of the elasticity of substitution and derive its formula.\n\nGOLD_ANSWER:\n1. **Definition**: The elasticity of substitution measures the percentage change in the factor ratio divided by the percentage change in the MRTS, holding output constant.\n2. **Formula**: \n   $$\\sigma = \\frac{d\\ln(x_{2}/x_{1})}{d\\ln|MRTS|}$$\n\nCANDIDATE_ANSWER:\nStep 1: Understand that elasticity of substitution measures the percentage change in input ratio (e.g., capital/labor) relative to the percentage change in MRTS along an isoquant.\n\nStep 2: Let input ratio = K/L and MRTS = slope of isoquant (tradeoff rate between inputs). Elasticity of substitution (σ) is defined as:\nσ = (%Δ in input ratio) / (%Δ in MRTS)\n\nStep 3: Rewrite using calculus notation:\nσ = [d(K/L)/(K/L)] / [d(MRTS)/MRTS]\n\nStep 4: Simplify using logarithmic derivatives:\nσ = [d(ln(K/L))] / [d(ln|MRTS|)]\n\nStep 5: Final formula (expressed as derivative):\nσ = \\frac{d \\ln(K/L)}{d \\ln|MRTS|}\n\nThis measures how easily firms can substitute inputs while maintaining output, where higher σ indicates greater substitutability (flatter isoquant curvature).\n\nQID: textbook-94-7-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-94-7-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly defines the elasticity of substitution and accurately derives the formula using logarithmic derivatives, matching the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-94-7-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly defines the elasticity of substitution and accurately derives the formula using logarithmic derivatives, matching the gold answer."}, "llm_echoed_qid": "textbook-94-7-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly defines the elasticity of substitution and accurately derives the formula using logarithmic derivatives, matching the gold answer."}
{"script_processing_qid": "textbook-107-3-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze the text's multiple-choice question (Harry's choices at prices (4,12) and (8,4)) for WARP compliance.\n\nGOLD_ANSWER:\n**Step 1**: At $(4,12)$, Harry's expenditure on $(9,4)$ is $4\\times9 + 12\\times4 = 84$.\n**Step 2**: At $(8,4)$, $(2,9)$ costs $8\\times2 + 4\\times9 = 52$.\n**Step 3**: Check if $(9,4)$ is affordable at $(8,4)$: $8\\times9 + 4\\times4 = 88 > 52$. Since $(2,9)$ was chosen when $(9,4)$ was affordable, and vice versa, WARP is violated.\n\nCANDIDATE_ANSWER:\nStep 1: Understand WARP requirements - If bundle y is chosen when bundle x is affordable, then in any subsequent scenario where x is chosen, y must not be affordable.\nStep 2: Apply to the given price pairs (4,12) and (8,4). To check compliance, we need information about:\n   - The specific bundles chosen at these prices\n   - Income levels or budget constraints in both scenarios\n   - Whether the chosen bundle in one scenario was affordable in the other\nStep 3: The provided context lacks critical details:\n   - No quantities of goods in bundles x and y\n   - No income/budget information\n   - No revealed preference patterns between the price pairs\nStep 4: Conclusion - The context does not contain sufficient information about Harry's actual consumption choices or budget constraints at these prices to perform a WARP analysis. Essential data about chosen bundles and budgets across both observations is missing.\n\nQID: textbook-107-3-2-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-107-3-2-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-3-2-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}, "llm_echoed_qid": "textbook-107-3-2-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}
{"script_processing_qid": "textbook-123-23-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the concept of 'duality' between the Kalman filter and the optimal linear regulator.\n\nGOLD_ANSWER:\nThe duality between the Kalman filter and the optimal linear regulator arises from the similarity in their Riccati equations. The Kalman filter's Riccati equation for the covariance matrix $\\Sigma_{t}$ mirrors the Riccati equation for the value function in the optimal linear regulator problem. This duality allows techniques from control theory to be applied to estimation problems and vice versa.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize the shared mathematical structure. Both the Kalman filter (estimation) and optimal linear regulator (control) involve solving a Riccati equation recursively. \n\nStep 2: Identify matrix substitutions that reveal duality. Replace the Kalman filter matrices as follows: \n- Transpose the system matrix: $A \\rightarrow A'$ \n- Swap observation/control matrices: $G \\rightarrow B$ (control matrix in regulator) \n- Swap covariance/cost matrices: $R \\rightarrow Q$ (state cost in regulator), $CC' \\rightarrow HH'$ (control cost in regulator)\n\nStep 3: Compare Riccati equations. The Kalman filter's covariance update:\n$\\Sigma_{t+1} = A\\Sigma_t A' + CC' - A\\Sigma_t G'(G\\Sigma_t G' + R)^{-1}G\\Sigma_t A'$\nbecomes the regulator's value function update:\n$P_{t} = A' P_{t+1} A + Q - A' P_{t+1} B(B' P_{t+1} B + H H')^{-1}B' P_{t+1} A$\nunder the substitutions from Step 2.\n\nStep 4: Note symmetry in objectives. The Kalman filter minimizes estimation error variance (forward-time problem), while the regulator minimizes control costs (backward-time problem). Their Riccati equations are time-reversed mathematical duals.\n\nQID: textbook-123-23-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-23-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the duality between the Kalman filter and the optimal linear regulator by detailing the shared mathematical structure, matrix substitutions, Riccati equations, and symmetry in objectives, aligning perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-23-0-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the duality between the Kalman filter and the optimal linear regulator by detailing the shared mathematical structure, matrix substitutions, Riccati equations, and symmetry in objectives, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-123-23-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the duality between the Kalman filter and the optimal linear regulator by detailing the shared mathematical structure, matrix substitutions, Riccati equations, and symmetry in objectives, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-118-14-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Discuss the trade-offs in choosing the convergence tolerance $\\varepsilon$.\n\nGOLD_ANSWER:\n1. **Too Large**: May stop prematurely, leaving $\\beta_{(j)}$ far from the true minimum.\n2. **Too Small**: May continue iterating beyond the point where improvements are negligible due to round-off error.\n3. **Practical Approach**: Experiment with different $\\varepsilon$ values to assess sensitivity.\n\nCANDIDATE_ANSWER:\nStep 1: Precision vs. Computational Cost\nA smaller ε (e.g., 10⁻¹²) requires the gradient to be closer to zero, potentially yielding more precise solutions but requiring more iterations. A larger ε (e.g., 10⁻⁴) allows earlier termination but may accept solutions farther from true optimality.\n\nStep 2: Numerical Stability Considerations\nExtremely small ε values (e.g., <10⁻¹⁶ in double precision) might conflict with machine precision limits, causing infinite loops due to rounding errors in gradient calculations.\n\nStep 3: Problem Scale Dependency\nThe choice depends on the problem's magnitude – larger-scale optimization problems may require proportionally larger ε values to avoid excessive computation, while smaller-scale problems can afford stricter tolerances.\n\nStep 4: Application-Specific Requirements\nIn empirical economics, tighter tolerances (≤10⁻⁸) are often needed for coefficient stability in sensitive models, while operations management applications might prioritize speed over extreme precision (using ε ~10⁻⁴ for real-time decisions).\n\nQID: textbook-118-14-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-14-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the trade-offs in choosing the convergence tolerance ε, covering precision vs. computational cost, numerical stability, problem scale dependency, and application-specific requirements, aligning well with the gold answer's points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-14-0-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the trade-offs in choosing the convergence tolerance ε, covering precision vs. computational cost, numerical stability, problem scale dependency, and application-specific requirements, aligning well with the gold answer's points."}, "llm_echoed_qid": "textbook-118-14-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the trade-offs in choosing the convergence tolerance ε, covering precision vs. computational cost, numerical stability, problem scale dependency, and application-specific requirements, aligning well with the gold answer's points."}
{"script_processing_qid": "textbook-122-20-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the quasi-likelihood ratio statistic for the Poisson model.\n\nGOLD_ANSWER:\n3. **Quasi-LR Statistic**: The quasi-LR statistic is $LR_{quasi} = \\frac{LR}{\\hat{\\sigma}^2}$, where $LR$ is the usual likelihood ratio statistic and $\\hat{\\sigma}^2$ is the overdispersion parameter. This adjusts for extra-Poisson variation.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that in the standard Poisson model, the likelihood ratio (LR) statistic is calculated as \\( 2(\\mathcal{L}_{ur} - \\mathcal{L}_r) \\), where \\( \\mathcal{L}_{ur} \\) and \\( \\mathcal{L}_r \\) are the unrestricted and restricted log-likelihoods.\n\nStep 2: The context specifies that the quasi-likelihood ratio (QLR) statistic accounts for overdispersion by dividing the standard LR statistic by \\( \\hat{\\sigma}^2 \\), where \\( \\hat{\\sigma} = \\sqrt{2} \\).\n\nStep 3: Substitute \\( \\hat{\\sigma}^2 = (\\sqrt{2})^2 = 2 \\) into the adjustment formula: \\( QLR = \\frac{LR}{\\hat{\\sigma}^2} = \\frac{LR}{2} \\).\n\nConclusion: The quasi-likelihood ratio statistic for the Poisson model in this setup is equal to one-half of the standard likelihood ratio statistic.\n\nQID: textbook-122-20-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-20-2-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate incorrectly assumes \\( \\hat{\\sigma}^2 = 2 \\) without justification from the context. The gold answer does not specify \\( \\hat{\\sigma}^2 \\) as 2, only that it is the overdispersion parameter.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-20-2-2", "category": "INCORRECT", "explanation": "The candidate incorrectly assumes \\( \\hat{\\sigma}^2 = 2 \\) without justification from the context. The gold answer does not specify \\( \\hat{\\sigma}^2 \\) as 2, only that it is the overdispersion parameter."}, "llm_echoed_qid": "textbook-122-20-2-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate incorrectly assumes \\( \\hat{\\sigma}^2 = 2 \\) without justification from the context. The gold answer does not specify \\( \\hat{\\sigma}^2 \\) as 2, only that it is the overdispersion parameter."}
{"script_processing_qid": "textbook-108-6-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the role of the money supply in the downward slope of the aggregate-demand curve.\n\nGOLD_ANSWER:\nThe downward slope assumes a fixed money supply. A fall in the price level increases real money balances ($M/P$), lowering interest rates and stimulating investment and net exports. If the money supply changes, the aggregate-demand curve would shift rather than slide along the curve.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the relevant effect tied to money supply in the context. The interest-rate effect (point 2) is connected to monetary dynamics.\nStep 2: Recognize that a lower price level increases real money supply (M/P). When prices fall, the same nominal money supply (M) becomes more valuable in real terms.\nStep 3: Increased real money supply reduces interest rates (as per liquidity preference theory), making borrowing cheaper.\nStep 4: Lower interest rates stimulate investment spending (e.g., business projects, housing), increasing aggregate demand.\nStep 5: Thus, the money supply mediates the interest-rate effect's contribution to the downward slope of the aggregate-demand curve.\n\nQID: textbook-108-6-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-108-6-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the role of the money supply in the downward slope of the aggregate-demand curve, detailing how a lower price level increases real money balances, reduces interest rates, and stimulates investment, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-6-2-1", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the role of the money supply in the downward slope of the aggregate-demand curve, detailing how a lower price level increases real money balances, reduces interest rates, and stimulates investment, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-108-6-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the role of the money supply in the downward slope of the aggregate-demand curve, detailing how a lower price level increases real money balances, reduces interest rates, and stimulates investment, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-123-19-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Interpret the promise-keeping constraint (21.5.1) and its role in the recursive contract.\n\nGOLD_ANSWER:\n1. **Promise-Keeping**: Ensures the contract delivers the promised utility $v$ to the worker. \n2. **Role**: It binds the planner to honor the worker’s expected utility, ensuring credibility and participation.\n\nCANDIDATE_ANSWER:\nStep 1: The promise-keeping constraint (21.5.1) in Wang and Williamson's model ensures that the insurer (principal) delivers the ex-ante promised lifetime utility to the worker (agent) at every history.\\n\\nStep 2: It acts recursively by making today's promised value equal to the expected discounted utility from current consumption/effort plus continuation values. Formally: $W = U(c,a) + \\beta E[W'(s')|a,s]$, where $W$ is current promised value and $W'(s')$ are state-contingent future promises.\\n\\nStep 3: The constraint tracks the evolution of promised utilities across employment/unemployment spells. When employed, the promise accounts for possible job separation probabilities affected by effort. When unemployed, it accounts for job-finding probabilities tied to search effort.\\n\\nStep 4: Its key role is twofold: (1) Maintains intertemporal consistency of the contract by linking past promises to current allocations, and (2) Serves as a state variable that summarizes relevant history (duration/number of spells), allowing the contract to be recursively formulated while handling moral hazard in both employment states.\n\nQID: textbook-123-19-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-19-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately interprets the promise-keeping constraint and its recursive role, aligning with the gold answer's core concepts while providing additional technical detail and context.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-19-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately interprets the promise-keeping constraint and its recursive role, aligning with the gold answer's core concepts while providing additional technical detail and context."}, "llm_echoed_qid": "textbook-123-19-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately interprets the promise-keeping constraint and its recursive role, aligning with the gold answer's core concepts while providing additional technical detail and context."}
{"script_processing_qid": "textbook-31-0-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of strong monotonicity for the existence of a competitive equilibrium.\n\nGOLD_ANSWER:\n1. **Strong Monotonicity**: Ensures that more of any good is always preferred, guaranteeing non-satiation.\n2. **Equilibrium Existence**: Prices adjust to clear markets, ensuring all goods are allocated efficiently.\n\nCANDIDATE_ANSWER:\nStep 1: Strong monotonicity implies local nonsatiation (as in condition (a) of the context). This eliminates the possibility of satiation points except for unique global satiation allocations.\\n\\nStep 2: Under strong monotonicity, if an allocation (x,y) Pareto dominates a price equilibrium with transfers (x*,y*), then for at least one consumer i, x_i ≻ x_i*. By local nonsatiation (implied by strong monotonicity), this requires p·x_i > w_i.\\n\\nStep 3: Aggregating across all consumers, ∑p·x_i > ∑w_i emerges. This violates feasibility since total expenditure exceeds total wealth.\\n\\nStep 4: Hence, no Pareto-improving allocation can be feasible when strong monotonicity holds. This ensures that price equilibria with transfers are Pareto efficient, supporting the existence of competitive equilibria by ruling out contradictions between efficiency and feasibility.\n\nQID: textbook-31-0-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-31-0-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the implications of strong monotonicity for competitive equilibrium existence, aligning with the gold answer's key points and expanding with detailed reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-31-0-2-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the implications of strong monotonicity for competitive equilibrium existence, aligning with the gold answer's key points and expanding with detailed reasoning."}, "llm_echoed_qid": "textbook-31-0-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the implications of strong monotonicity for competitive equilibrium existence, aligning with the gold answer's key points and expanding with detailed reasoning."}
{"script_processing_qid": "textbook-114-20-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the economic cost of a firm that owns its office space and could lease it for $10,000/month, while the accounting cost is zero. Show the calculation.\n\nGOLD_ANSWER:\n1. **Economic Cost**: Explicit cost + Implicit cost.\n2. **Explicit Cost**: $0 (no rent paid).\n3. **Implicit Cost**: $10,000 (foregone rent).\n4. **Total Economic Cost**: $0 + $10,000 = $10,000.\n5. **Accounting Cost**: $0.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the difference between accounting cost and economic cost.\nAccounting cost only considers explicit monetary expenses (here $0 as the firm owns the office). Economic cost includes opportunity cost - the value of the best alternative use of the resource.\n\nStep 2: Identify the opportunity cost in this scenario.\nThe firm could earn $10,000/month by leasing its office space but chooses to use it instead. This foregone income represents the opportunity cost.\n\nStep 3: Calculate economic cost.\nEconomic Cost = Accounting Cost + Opportunity Cost\n= $0 (no explicit expense) + $10,000 (foregone leasing income)\n= $10,000/month\n\nQID: textbook-114-20-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-20-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies and calculates the economic cost by including the opportunity cost of foregone rent, matching the gold answer's reasoning and result.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-20-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies and calculates the economic cost by including the opportunity cost of foregone rent, matching the gold answer's reasoning and result."}, "llm_echoed_qid": "textbook-114-20-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies and calculates the economic cost by including the opportunity cost of foregone rent, matching the gold answer's reasoning and result."}
{"script_processing_qid": "textbook-10-0-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the concept of cointegration and its implications for econometric modeling.\n\nGOLD_ANSWER:\n1.  **Cointegration**: Two or more I(1) processes are cointegrated if there exists a linear combination that is I(0).\n2.  **Implications**: Cointegration implies a long-run equilibrium relationship between the variables. It allows for the use of error-correction models (ECMs) to capture both short-run dynamics and long-run equilibrium.\n\nCANDIDATE_ANSWER:\nStep 1: Definition - Cointegration occurs when two or more I(1) (integrated of order 1) time series share a long-run equilibrium relationship such that a linear combination of them becomes I(0) (stationary). This implies they 'move together' over time despite individual non-stationarity.\n\nStep 2: Implications for Econometric Modeling - \n(1) Avoids spurious regression: Standard regression on I(1) variables often yields misleading results, but cointegration validates this relationship.\n(2) Requires Error Correction Models: Cointegrated variables must be modeled using Vector Error-Correction Models (VECM), which jointly capture short-run dynamics and long-run equilibrium (per the VECM section).\n(3) Testing procedures: Requires specialized tests like Johansen's ML procedure (section 640) instead of standard hypothesis tests.\n(4) System modeling: Cointegration necessitates multivariate approaches (e.g., VAR systems in section 636) rather than univariate analysis.\n(5) Theoretical grounding: The existence of cointegration implies an empirically testable economic equilibrium relationship between variables.\n\nQID: textbook-10-0-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-10-0-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately defines cointegration and thoroughly explains its implications for econometric modeling, aligning well with the gold answer. The candidate provides additional details on testing procedures, system modeling, and theoretical grounding, which enhance the explanation without deviating from the core concepts.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-10-0-1-1", "category": "CORRECT", "explanation": "The candidate's answer accurately defines cointegration and thoroughly explains its implications for econometric modeling, aligning well with the gold answer. The candidate provides additional details on testing procedures, system modeling, and theoretical grounding, which enhance the explanation without deviating from the core concepts."}, "llm_echoed_qid": "textbook-10-0-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately defines cointegration and thoroughly explains its implications for econometric modeling, aligning well with the gold answer. The candidate provides additional details on testing procedures, system modeling, and theoretical grounding, which enhance the explanation without deviating from the core concepts."}
{"script_processing_qid": "textbook-122-39-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Calculate the predicted difference in GPA between a female and male athlete with sat = 1100, hsperc = 10, and tothrs = 50, using the unrestricted model in equation (7.22).\n\nGOLD_ANSWER:\n1. **Unrestricted Model**: \n   $$cumgpa = 1.483 - 0.353 female + 0.0011 sat + 0.00075 female \\cdot sat - 0.0085 hsperc - 0.00055 female \\cdot hsperc + 0.0023 tothrs - 0.00012 female \\cdot tothrs$$ \n2. **Predicted Difference**: \n   $$-0.353 + 0.00075(1100) - 0.00055(10) - 0.00012(50) ≈ 0.461$$ \n3. **Interpretation**: The female athlete is predicted to have a GPA 0.461 points higher than the male athlete, holding other factors constant.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the unrestricted model coefficients from equation (7.22):\n- Intercept = 1.48\n- female coefficient = -0.353\n- sat coefficient = 0.0011\n- female·sat interaction = 0.00075\n- hsperc coefficient = -0.0085\n- female·hsperc interaction = -0.00055\n- tothrs coefficient = 0.0023\n- female·tothrs interaction = -0.00012\n\nStep 2: Calculate the GPA difference (Female - Male):\nΔGPA = [(-0.353)] + [0.00075 * sat] + [(-0.00055 * hsperc)] + [(-0.00012 * tothrs)]\n\nStep 3: Substitute given values (sat=1100, hsperc=10, tothrs=50):\nΔGPA = -0.353 + (0.00075×1100) + (-0.00055×10) + (-0.00012×50)\n       = -0.353 + 0.825 - 0.0055 - 0.006\n\nStep 4: Perform calculations:\n= (-0.353) + 0.825 → 0.472\n= 0.472 - 0.0055 → 0.4665\n= 0.4665 - 0.006 → 0.4605\n\nFinal Result: The predicted GPA difference is +0.4605 points (female athletes have higher predicted GPA than males with these characteristics).\n\nQID: textbook-122-39-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-39-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer matches the gold answer in both the calculation steps and the final result, showing a predicted GPA difference of approximately 0.461 points higher for female athletes.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-39-0-3", "category": "CORRECT", "explanation": "The candidate's answer matches the gold answer in both the calculation steps and the final result, showing a predicted GPA difference of approximately 0.461 points higher for female athletes."}, "llm_echoed_qid": "textbook-122-39-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer matches the gold answer in both the calculation steps and the final result, showing a predicted GPA difference of approximately 0.461 points higher for female athletes."}
{"script_processing_qid": "textbook-122-20-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the consequences of using a weak instrument.\n\nGOLD_ANSWER:\n3. **Weak Instruments**: A weak instrument (low $Cov(z, x)$) leads to:\n   - **Large Variance**: The IV estimator has high variance, making it imprecise.\n   - **Bias**: The IV estimator can be biased in finite samples, especially if the instrument is only weakly correlated with $x$.\n\nCANDIDATE_ANSWER:\nStep 1: A weak instrument (low correlation with the endogenous variable) violates the relevance condition for valid IVs. The context explicitly states that an instrument must appear in the first-stage equation (16.22) to ensure partial correlation with the endogenous variable.\n\nStep 2: If the instrument is weakly correlated, it fails to adequately isolate the exogenous variation in the endogenous variable. This leads to unreliable estimates, as the IV cannot effectively address endogeneity.\n\nStep 3: The context further implies that weak instruments may exacerbate violations of the exclusion restriction. For example, if the instrument (e.g., draft lottery number) is weakly correlated with the endogenous variable (education) but inadvertently correlated with the error term (via omitted variables like schooling decisions), the IV estimates become inconsistent.\n\nStep 4: Weak instruments also risk underidentification. The context notes that if the instrument cannot sufficiently explain variation in the endogenous variable (e.g., money supply growth weakly predicting inflation), the model lacks a valid source of exogenous variation, rendering IV estimation infeasible or biased.\n\nQID: textbook-122-20-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-20-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately discusses the consequences of using a weak instrument, including violations of the relevance condition, unreliable estimates, potential exacerbation of exclusion restriction violations, and risk of underidentification. These points align well with the gold answer's focus on large variance and bias in the IV estimator due to weak instruments.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-20-1-2", "category": "CORRECT", "explanation": "The candidate answer accurately discusses the consequences of using a weak instrument, including violations of the relevance condition, unreliable estimates, potential exacerbation of exclusion restriction violations, and risk of underidentification. These points align well with the gold answer's focus on large variance and bias in the IV estimator due to weak instruments."}, "llm_echoed_qid": "textbook-122-20-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately discusses the consequences of using a weak instrument, including violations of the relevance condition, unreliable estimates, potential exacerbation of exclusion restriction violations, and risk of underidentification. These points align well with the gold answer's focus on large variance and bias in the IV estimator due to weak instruments."}
{"script_processing_qid": "textbook-106-5-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare and contrast the short-run and long-run average total cost (ATC) curves, explaining why the long-run ATC curve is flatter and lies below the short-run ATC curves.\n\nGOLD_ANSWER:\n1.  **Short-Run ATC**: In the short run, some inputs (e.g., factory size) are fixed, leading to higher average costs due to diminishing returns. The short-run ATC curve is steeper and U-shaped.\n2.  **Long-Run ATC**: In the long run, all inputs are variable, allowing firms to adjust scale and optimize production. The long-run ATC curve is flatter because firms can choose the most efficient scale for any output level.\n3.  **Positioning**: The long-run ATC curve lies below the short-run curves because firms can always choose the optimal short-run curve (e.g., factory size) in the long run, minimizing costs. The short-run curves represent constrained optimization, while the long-run curve represents unconstrained optimization.\n\nCANDIDATE_ANSWER:\nThe provided context focuses exclusively on short-run cost curves (ATC, AFC, AVC, MC) and their characteristics for Thirsty Thelma’s Lemonade Stand. It does not mention or describe long-run average total cost (LRATC) curves, their properties, or how they relate to short-run curves. Therefore, the context lacks the necessary information to compare short-run and long-run ATC curves or explain why the long-run curve is flatter and lies below short-run curves.\n\nQID: textbook-106-5-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-5-0-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-5-0-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context."}, "llm_echoed_qid": "textbook-106-5-0-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context."}
{"script_processing_qid": "textbook-106-4-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain how the medieval town could have used a cap-and-trade system to prevent overgrazing. Design the system with hypothetical parameters.\n\nGOLD_ANSWER:\n1. **Cap**: Set total sheep limit (e.g., 1,000 sheep).\n2. **Trade**: Issue permits for 1 sheep each. Shepherds can trade permits at market price $p$.\n3. **Equilibrium**: Permits allocate sheep to highest-value uses. Price reflects marginal damage ($p = D'(S)$).\n4. **Example**: If 10 shepherds, each gets 100 permits initially. Those with higher marginal benefits buy permits from others.\n\nCANDIDATE_ANSWER:\nStep 1: Define the Cap\nDetermine the sustainable carrying capacity of the common land. For example, if the land can support 500 sheep without degradation, set a total cap of 500 sheep allowed for grazing.\n\nStep 2: Allocate Permits\nIssue tradable grazing permits to families. For instance: \n- Total permits = 500 (1 permit = 1 sheep)\n- Allocate based on historical use (e.g., 10 permits per family if there are 50 families) or auction permits initially.\n\nStep 3: Allow Trading\nFamilies can buy/sell permits in a regulated market. A family needing more sheep (e.g., 12 sheep) must buy 2 permits from others willing to reduce their herd. This incentivizes efficient use: families with lower grazing needs profit by selling permits, while others pay for extra usage.\n\nStep 4: Enforce Compliance\nAppoint monitors to count sheep and verify permits. Penalize violations (e.g., fines for exceeding permit limits).\n\nStep 5: Adjust System Annually\nReassess the cap based on land recovery data (e.g., increase to 520 permits if vegetation improves, decrease to 480 if drought occurs).\n\nHypothetical Parameters:\n- Initial cap: 500 sheep\n- Permits: 500 (1:1 ratio)\n- Allocation: 10 permits/family (50 families)\n- Trading: Free market with town oversight\n- Penalty: 2 goats per excess sheep grazed\n- Annual review: Adjust cap ±5% based on environmental surveys.\n\nQID: textbook-106-4-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-4-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately mirrors the gold answer's structure and intent, detailing a cap-and-trade system with clear hypothetical parameters and logical steps for implementation, enforcement, and adjustment.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-4-2-3", "category": "CORRECT", "explanation": "The candidate's answer accurately mirrors the gold answer's structure and intent, detailing a cap-and-trade system with clear hypothetical parameters and logical steps for implementation, enforcement, and adjustment."}, "llm_echoed_qid": "textbook-106-4-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately mirrors the gold answer's structure and intent, detailing a cap-and-trade system with clear hypothetical parameters and logical steps for implementation, enforcement, and adjustment."}
{"script_processing_qid": "textbook-112-22-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Compare and contrast the economic policies of China, ancient Egypt, and Indostan regarding agriculture and foreign trade.\n\nGOLD_ANSWER:\n1. **China**: Favored agriculture over foreign trade, with laborers enjoying higher status than artificers. Leases were granted on moderate terms, and foreign trade was restricted.\n2. **Ancient Egypt and Indostan**: Both prioritized agriculture, with land taxes forming the bulk of state revenue. Both had caste systems that restricted occupational mobility.\n3. **Commonalities**: All three derived significant revenue from land taxes, which tied state interests closely to agricultural prosperity.\n4. **Differences**: China had a more extensive home market due to its size and population, while Egypt and Indostan relied more on inland navigations to compensate for limited foreign markets.\n\nCANDIDATE_ANSWER:\nStep 1: Identify commonalities in agricultural policy\nAll three nations (China, ancient Egypt, and Indostan under Gentoo government) explicitly favored agriculture over other sectors like manufacturing and foreign trade. Their policies elevated the status of agricultural laborers above artificers/artisans.\n\nStep 2: Examine revenue structures\nThe context specifies that all derived sovereign revenue primarily from land taxation:\n- Chinese rulers obtained 'the whole, or by far the most considerable part' from land\n- Ancient Egyptian and Indostan kingdoms similarly relied on land taxes/rents\n\nStep 3: Assess foreign trade implications\nWhile the context explicitly compares agricultural vs. manufacturing priorities, it provides no specific details about foreign trade policies for any of these nations. The only reference to foreign trade is its subordination to agricultural focus in general policy orientation.\n\nConclusion: \nAll three shared agricultural prioritization and land-based revenue systems, but the context contains insufficient information to compare their specific foreign trade policies or contrast their approaches to international commerce.\n\nQID: textbook-112-22-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-22-0-0\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the agricultural focus and land-based revenue systems but misses the specific contrasts in foreign trade policies mentioned in the gold answer (China's restrictions vs. Egypt/Indostan's reliance on inland navigation). The conclusion about insufficient trade policy details is partially correct but overlooks available comparative elements.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-22-0-0", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the agricultural focus and land-based revenue systems but misses the specific contrasts in foreign trade policies mentioned in the gold answer (China's restrictions vs. Egypt/Indostan's reliance on inland navigation). The conclusion about insufficient trade policy details is partially correct but overlooks available comparative elements."}, "llm_echoed_qid": "textbook-112-22-0-0", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the agricultural focus and land-based revenue systems but misses the specific contrasts in foreign trade policies mentioned in the gold answer (China's restrictions vs. Egypt/Indostan's reliance on inland navigation). The conclusion about insufficient trade policy details is partially correct but overlooks available comparative elements."}
{"script_processing_qid": "textbook-118-3-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the IM test is sensitive to heteroskedasticity, skewness, and kurtosis in the context of a linear regression model. How does this sensitivity relate to the information matrix equality?\n\nGOLD_ANSWER:\n1.  **Heteroskedasticity**: Violates the assumption of constant variance, leading to discrepancies between the Hessian and outer product of gradients.\n2.  **Skewness/Kurtosis**: Non-normal errors affect higher moments, causing the information matrix equality (which relies on normality) to fail.\n3.  **IM Equality**: The equality $\\mathcal{I}(\\pmb{\\theta})=-\\mathcal{H}(\\pmb{\\theta})$ holds only under correct specification. Deviations in higher moments (e.g., skewness/kurtosis) or variance structure (heteroskedasticity) disrupt this equality, making the IM test a general misspecification indicator.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the IM test's foundation in the information matrix equality\nIn linear regression models, the information matrix equality states that under correct specification, the negative expected Hessian matrix equals the covariance matrix of scores: $-E[H] = E[SS^\\top]$, where $H$ is the matrix of second derivatives and $S$ is the score vector.\n\nStep 2: Recognize how misspecification violates equality\nHeteroskedasticity, skewness, and kurtosis affect higher moments of residuals:\n- Heteroskedasticity violates constant variance assumption (4th moments)\n- Skewness violates symmetry (3rd moments)\n- Kurtosis affects tail behavior (4th moments)\nThese alter the relationship between Hessian and outer product of scores.\n\nStep 3: Connect to IM test construction\nThe IM test statistic aggregates deviations in the matrix $M(\\theta) = \\partial^2 \\ell_t/\\partial\\theta_i\\partial\\theta_j + (\\partial\\ell_t/\\partial\\theta_i)(\\partial\\ell_t/\\partial\\theta_j)$. Under correct specification, these deviations should average to zero.\n\nStep 4: Sensitivity mechanisms\n- Heteroskedasticity: Affects variance structure of residuals, making $E[SS^\\top] \\neq -E[H]$\n- Skewness: Impacts third moments, altering covariance between parameter estimates\n- Kurtosis: Affects fourth moments, changing variance of score components\nAll three phenomena create systematic patterns in $M(\\theta)$ elements.\n\nStep 5: Relate to information matrix equality\nThe equality holds only when all distributional assumptions (including homoskedasticity and normality) are satisfied. The IM test's sensitivity arises because any deviation from these assumptions generates non-zero terms in (15.35), making the test statistic reject the null.\n\nQID: textbook-118-3-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-3-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the IM test's sensitivity to heteroskedasticity, skewness, and kurtosis, detailing how these affect the information matrix equality and the construction of the IM test statistic. The response aligns well with the gold answer, covering all key points with additional depth.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-3-1-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the IM test's sensitivity to heteroskedasticity, skewness, and kurtosis, detailing how these affect the information matrix equality and the construction of the IM test statistic. The response aligns well with the gold answer, covering all key points with additional depth."}, "llm_echoed_qid": "textbook-118-3-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the IM test's sensitivity to heteroskedasticity, skewness, and kurtosis, detailing how these affect the information matrix equality and the construction of the IM test statistic. The response aligns well with the gold answer, covering all key points with additional depth."}
{"script_processing_qid": "textbook-84-1-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the inconsistency (asymptotic bias) of $\\tilde{\\beta}_{1}$ when risktol is omitted from the regression, given $\\beta_{2} > 0$ and $\\delta_{1} > 0$.\n\nGOLD_ANSWER:\n1. **Omitted Variable Bias**: $\\mathrm{plim}(\\tilde{\\beta}_{1}) = \\beta_{1} + \\beta_{2}\\delta_{1}$.\n2. **Given**: $\\beta_{2} > 0$ (positive effect of risktol) and $\\delta_{1} > 0$ (positive correlation between funds and risktol).\n3. **Bias**: $\\beta_{2}\\delta_{1} > 0$, so $\\tilde{\\beta}_{1}$ overestimates $\\beta_{1}$.\n4. **Implication**: The estimated effect of funds is biased upward due to the omitted risktol variable.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that omitting risktol (a relevant variable) violates the zero conditional mean assumption. The true model should be $y = \\beta_0 + \\beta_1x_1 + \\beta_2risktol + v$, where $v$ is the error term. \\n\\nStep 2: When risktol is omitted, the error term $u$ in the misspecified model $y = \\beta_0 + \\beta_1x_1 + u$ contains risktol ($u = \\beta_2risktol + v$). \\n\\nStep 3: The asymptotic bias (inconsistency) of $\\tilde{\\beta}_1$ arises from the correlation between $x_1$ and the omitted risktol. Let $risktol = \\delta_0 + \\delta_1x_1 + e$, where $\\delta_1 > 0$ is the correlation parameter. \\n\\nStep 4: Substitute risktol into the misspecified model: $y = \\beta_0 + (\\beta_1 + \\beta_2\\delta_1)x_1 + (\\beta_2e + v)$. \\n\\nStep 5: The probability limit becomes $\\text{plim}\\ \\tilde{\\beta}_1 = \\beta_1 + \\beta_2\\delta_1$. Since $\\beta_2 > 0$ and $\\delta_1 > 0$, the estimator $\\tilde{\\beta}_1$ has positive asymptotic bias equal to $\\beta_2\\delta_1$.\n\nQID: textbook-84-1-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-84-1-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the omitted variable bias, explains the violation of the zero conditional mean assumption, and derives the asymptotic bias of $\\tilde{\\beta}_1$ as $\\beta_1 + \\beta_2\\delta_1$, which matches the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-84-1-1-1", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the omitted variable bias, explains the violation of the zero conditional mean assumption, and derives the asymptotic bias of $\tilde{\beta}_1$ as $\beta_1 + \beta_2\\delta_1$, which matches the gold answer."}, "llm_echoed_qid": "textbook-84-1-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the omitted variable bias, explains the violation of the zero conditional mean assumption, and derives the asymptotic bias of $\tilde{\beta}_1$ as $\beta_1 + \beta_2\\delta_1$, which matches the gold answer."}
{"script_processing_qid": "textbook-114-25-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. How do budget constraints interact with indifference curves to determine consumer equilibrium? Provide a mathematical formulation of the budget constraint.\n\nGOLD_ANSWER:\n1. **Budget Constraint**: $P_F F + P_C C \\leq I$, where $P_F$ and $P_C$ are prices of food and clothing, and $I$ is income.\n2. **Consumer Equilibrium**: Occurs at the tangency point between the budget line and the highest attainable indifference curve, where $MRS = \\frac{P_F}{P_C}$.\n3. **Graphical Interpretation**: The slope of the budget line ($-\\frac{P_F}{P_C}$) equals the MRS at the optimal bundle, ensuring no further utility-improving trades are possible.\n\nCANDIDATE_ANSWER:\nStep 1: Consumer equilibrium occurs where the budget constraint (representing all affordable combinations of goods) is tangent to the highest attainable indifference curve (representing preference rankings).\n\nStep 2: At this tangency point, the marginal rate of substitution (MRS) from the indifference curve equals the price ratio of the two goods (slope of the budget constraint). Mathematically: MRS = P₁/P₂.\n\nStep 3: The budget constraint mathematically formalizes the trade-off between two goods (X₁ and X₂) given prices (P₁, P₂) and income (I). The constraint is:\nP₁X₁ + P₂X₂ = I\nwhere:\n- P₁, P₂ = prices of goods 1 and 2\n- X₁, X₂ = quantities consumed\n- I = consumer income\n\nThis equation ensures total expenditure equals income, limiting consumption possibilities to combinations on or below the budget line.\n\nQID: textbook-114-25-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-25-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes the interaction between budget constraints and indifference curves, including the mathematical formulation of the budget constraint and the condition for consumer equilibrium (MRS = price ratio). The response aligns perfectly with the gold answer in both content and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-25-1-0", "category": "CORRECT", "explanation": "The candidate's answer accurately describes the interaction between budget constraints and indifference curves, including the mathematical formulation of the budget constraint and the condition for consumer equilibrium (MRS = price ratio). The response aligns perfectly with the gold answer in both content and reasoning."}, "llm_echoed_qid": "textbook-114-25-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes the interaction between budget constraints and indifference curves, including the mathematical formulation of the budget constraint and the condition for consumer equilibrium (MRS = price ratio). The response aligns perfectly with the gold answer in both content and reasoning."}
{"script_processing_qid": "textbook-115-17-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. What are the finite-sample implications of using an unrestricted FGLS estimator when $N$ is not much larger than $T$?\n\nGOLD_ANSWER:\nThe unrestricted FGLS estimator estimates $T(T+1)/2$ elements in $\\Omega$, leading to poor finite-sample properties if $N$ is small relative to $T$. The RE estimator, estimating only two parameters, is more reliable in such cases.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the context. The unrestricted FGLS estimator (in this case, RE estimator with robust variance) relaxes Assumption RE.3, which requires a specific random effects structure and homoskedastic/uncorrelated idiosyncratic errors.\n\nStep 2: Recognize the finite-sample issue. When N is not much larger than T, the covariance matrix estimation for FGLS involves many parameters (e.g., T(T+1)/2 unique elements for a full unstructured covariance matrix).\n\nStep 3: Identify the implications. With limited N relative to T:\n(a) The unrestricted FGLS estimator’s covariance matrix estimates become imprecise due to high dimensionality\n(b) Downward bias in standard errors may occur because the robust variance estimator (equation 7.49) requires averaging over N, which is small\n(c) Hypothesis tests and confidence intervals may exhibit size distortions (e.g., over-rejection of true null hypotheses)\n\nStep 4: Conclusion. The robust FGLS approach remains asymptotically valid but suffers from poor finite-sample performance when N is not sufficiently large relative to T, primarily due to noisy estimation of the error covariance structure.\n\nQID: textbook-115-17-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-17-0-3\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the finite-sample issues of the unrestricted FGLS estimator when N is not much larger than T, such as imprecise covariance matrix estimates and potential size distortions. However, it incorrectly conflates the unrestricted FGLS estimator with the RE estimator with robust variance, which is not the focus of the gold answer. The gold answer specifically highlights the poor finite-sample properties of the unrestricted FGLS estimator due to the high number of parameters estimated, and contrasts it with the RE estimator, which is more reliable in such cases.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-17-0-3", "category": "PARTIALLY_CORRECT", "explanation": "The candidate answer correctly identifies the finite-sample issues of the unrestricted FGLS estimator when N is not much larger than T, such as imprecise covariance matrix estimates and potential size distortions. However, it incorrectly conflates the unrestricted FGLS estimator with the RE estimator with robust variance, which is not the focus of the gold answer. The gold answer specifically highlights the poor finite-sample properties of the unrestricted FGLS estimator due to the high number of parameters estimated, and contrasts it with the RE estimator, which is more reliable in such cases."}, "llm_echoed_qid": "textbook-115-17-0-3", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the finite-sample issues of the unrestricted FGLS estimator when N is not much larger than T, such as imprecise covariance matrix estimates and potential size distortions. However, it incorrectly conflates the unrestricted FGLS estimator with the RE estimator with robust variance, which is not the focus of the gold answer. The gold answer specifically highlights the poor finite-sample properties of the unrestricted FGLS estimator due to the high number of parameters estimated, and contrasts it with the RE estimator, which is more reliable in such cases."}
{"script_processing_qid": "textbook-114-23-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate the deadweight loss due to a negative externality, given the demand curve (D), industry supply curve (MCi), and marginal social cost curve (MSCi).\n\nGOLD_ANSWER:\n1.  **Market Output**: $Q_1$ is where $D$ intersects $MCi$.\n2.  **Socially Optimal Output**: $Q^*$ is where $D$ intersects $MSCi$.\n3.  **Deadweight Loss**: The area between $MSCi$, $D$, and $Q_1$ represents the social cost of overproduction.\n4.  **Formula**: Deadweight loss is the integral of $(MSCi - D)$ from $Q^*$ to $Q_1$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify market equilibrium quantity (Q_m) where demand curve D intersects industry supply curve MCi.\nStep 2: Identify socially optimal quantity (Q_s) where demand curve D intersects marginal social cost curve MSCi.\nStep 3: Calculate the quantity difference: ΔQ = Q_m - Q_s.\nStep 4: Determine the vertical distance between MSCi and MCi at Q_m (this equals the marginal external cost at market output).\nStep 5: Deadweight loss = ½ × ΔQ × (MSCi(Q_m) - MCi(Q_m)) = ½ × (Q_m - Q_s) × MEC(Q_m)\n\nQID: textbook-114-23-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-23-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the steps to calculate deadweight loss due to a negative externality, including identifying market equilibrium and socially optimal quantities, and using the correct formula for deadweight loss. The reasoning aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-23-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the steps to calculate deadweight loss due to a negative externality, including identifying market equilibrium and socially optimal quantities, and using the correct formula for deadweight loss. The reasoning aligns with the gold answer."}, "llm_echoed_qid": "textbook-114-23-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the steps to calculate deadweight loss due to a negative externality, including identifying market equilibrium and socially optimal quantities, and using the correct formula for deadweight loss. The reasoning aligns with the gold answer."}
{"script_processing_qid": "textbook-114-3-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Derive the mixed strategy Nash equilibrium for the 'Matching Pennies' game (Table 13.6). What is the expected payoff for each player?\n\nGOLD_ANSWER:\n1. **Mixed Strategy**: Each player randomizes between Heads (H) and Tails (T) with probability 0.5.\n2. **Nash Equilibrium**: If Player A plays H with probability 0.5, Player B is indifferent between H and T. The same holds for Player A given Player B's strategy.\n3. **Expected Payoff**: The expected payoff for each player is 0, calculated as:\n   $$E[\\text{Payoff}] = 0.5 \\times (1) + 0.5 \\times (-1) = 0$$\n\nCANDIDATE_ANSWER:\nStep 1: Identify the standard Matching Pennies payoff structure (not explicitly shown in context but implied by the question). Assume Player 1 gains 1 when matches and loses 1 when mismatched, while Player 2 has the opposite payoffs.\n\nStep 2: Let p = probability Player 1 plays Heads, q = probability Player 2 plays Heads. To find equilibrium, players must make opponents indifferent between their strategies.\n\nStep 3: For Player 2’s indifference:\n- Expected payoff from Heads: (1)(−1)p + (1)(1)(1−p) = 1 − 2p\n- Expected payoff from Tails: (−1)(1)p + (1)(−1)(1−p) = 2p − 1\nSet equal: 1 − 2p = 2p − 1 ⇒ p = 0.5\n\nStep 4: For Player 1’s indifference:\n- Expected payoff from Heads: (1)q + (−1)(1−q) = 2q − 1\n- Expected payoff from Tails: (−1)q + (1)(1−q) = 1 − 2q\nSet equal: 2q − 1 = 1 − 2q ⇒ q = 0.5\n\nStep 5: Mixed strategy Nash equilibrium = both players randomize equally (p = q = 0.5). Expected payoffs are:\n- Player 1: 0.5(0.5(1) + 0.5(−1)) + 0.5(0.5(−1) + 0.5(1)) = 0\n- Player 2 similarly earns 0 (zero-sum game).\n\nQID: textbook-114-3-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-3-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the mixed strategy Nash equilibrium for the 'Matching Pennies' game, with both players randomizing equally between Heads and Tails (probability 0.5). The expected payoff for each player is correctly calculated as 0, matching the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-3-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the mixed strategy Nash equilibrium for the 'Matching Pennies' game, with both players randomizing equally between Heads and Tails (probability 0.5). The expected payoff for each player is correctly calculated as 0, matching the gold answer."}, "llm_echoed_qid": "textbook-114-3-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the mixed strategy Nash equilibrium for the 'Matching Pennies' game, with both players randomizing equally between Heads and Tails (probability 0.5). The expected payoff for each player is correctly calculated as 0, matching the gold answer."}
{"script_processing_qid": "textbook-92-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the role of the information matrix equality in the asymptotic normality of the ML estimator. How does it simplify the asymptotic variance?\n\nGOLD_ANSWER:\nThe information matrix equality states that for ML estimators, $-\\mathbf{E}\\big[\\mathbf{H}(\\mathbf{w}_{t};\\pmb{\\theta}_{0})\\big] = \\mathbf{E}\\big[\\mathbf{s}(\\mathbf{w}_{t};\\pmb{\\theta}_{0})\\mathbf{s}(\\mathbf{w}_{t};\\pmb{\\theta}_{0})^{\\prime}\\big]$. This equality simplifies the asymptotic variance of the ML estimator in two ways:\\n1. **First simplification**: The asymptotic variance can be written as $-\\left(\\mathbf{E}\\left[\\mathbf{H}(\\mathbf{w}_{t};\\pmb{\\theta}_{0})\\right]\\right)^{-1}$.\\n2. **Second simplification**: It can also be written as $\\left(\\mathbf{E}\\left[\\mathbf{s}(\\mathbf{w}_{t};\\pmb{\\theta}_{0})\\mathbf{s}(\\mathbf{w}_{t};\\pmb{\\theta}_{0})^{\\prime}\\right]\\right)^{-1}$.\\nThese simplifications are crucial because they allow for consistent estimation of the asymptotic variance using either the Hessian or the outer product of the score.\n\nCANDIDATE_ANSWER:\nStep 1: The information matrix equality (IME) states that under ML assumptions, the expected outer product of the score (Fisher information) equals the negative expected Hessian: $\\mathbf{E}[\\mathbf{s}(\\mathbf{w}_t;\\pmb{\\theta}_0)\\mathbf{s}(\\mathbf{w}_t;\\pmb{\\theta}_0)'] = -\\mathbf{E}[\\mathbf{H}(\\mathbf{w}_t;\\pmb{\\theta}_0)]$. This is explicitly shown in condition (3) of Proposition 7.9.\n\nStep 2: In the general M-estimator asymptotic variance formula (Proposition 7.8), $\\mathsf{Avar}(\\hat{\\pmb\\theta}) = [\\mathbf{E}(\\mathbf{H})]^{-1}\\pmb\\Sigma[\\mathbf{E}(\\mathbf{H})]^{-1}$, where $\\pmb\\Sigma = \\mathsf{Var}(\\mathbf{s}(\\mathbf{w}_t;\\pmb{\\theta}_0))$.\n\nStep 3: Under IME, $\\pmb\\Sigma = -\\mathbf{E}[\\mathbf{H}]$. Substituting this into the general formula eliminates the sandwich form:\n$\\mathsf{Avar}(\\hat{\\pmb\\theta}) = [\\mathbf{E}(\\mathbf{H})]^{-1}(-\\mathbf{E}[\\mathbf{H}])[\\mathbf{E}(\\mathbf{H})]^{-1} = -[\\mathbf{E}(\\mathbf{H})]^{-1}$.\n\nStep 4: Since ML uses the negative Hessian as the Fisher information, the final simplification becomes $\\mathsf{Avar}(\\hat{\\pmb\\theta}) = (-\\mathbf{E}[\\mathbf{H}])^{-1}$, reducing computational complexity by avoiding separate estimation of $\\pmb\\Sigma$ and collapsing the asymptotic variance to a single inverse term.\n\nQID: textbook-92-0-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-92-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains the role of the information matrix equality in the asymptotic normality of the ML estimator and how it simplifies the asymptotic variance. The steps provided align with the gold answer, demonstrating a clear understanding of the mathematical derivations and their implications.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-92-0-0-1", "category": "CORRECT", "explanation": "The candidate answer correctly explains the role of the information matrix equality in the asymptotic normality of the ML estimator and how it simplifies the asymptotic variance. The steps provided align with the gold answer, demonstrating a clear understanding of the mathematical derivations and their implications."}, "llm_echoed_qid": "textbook-92-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains the role of the information matrix equality in the asymptotic normality of the ML estimator and how it simplifies the asymptotic variance. The steps provided align with the gold answer, demonstrating a clear understanding of the mathematical derivations and their implications."}
{"script_processing_qid": "textbook-123-7-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the equilibrium condition $1+r(t)\\geq[1+r(t)]\\frac{p(t)}{\\bar{p}}$ and its economic interpretation.\n\nGOLD_ANSWER:\nThe condition ensures that the real return on consumption loans is at least as great as the real return on currency. If currency is valued, the returns must be equal, implying no arbitrage opportunity between holding currency and consumption loans.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key components\n- Left side (1 + r(t)): Gross real interest rate on alternative assets (e.g., bonds)\n- Right side [1 + r(t)]⋅p(t)/ȯ: Real return on holding government currency\n\nStep 2: Derive currency's real return\nFrom government budget constraint:\nH(t+1)/p(t+1) = [1 + r(t)]⋅H(t)/ȯ\nReal return = (Next period real balances)/(Current real balances)\n= [1 + r(t)]⋅(H(t)/ȯ)/(H(t)/p(t))\n= [1 + r(t)]⋅p(t)/ȯ\n\nStep 3: Interpret equilibrium condition\nAgents will only hold currency if its return is at least equal to alternative assets:\n1 + r(t) ≥ [1 + r(t)]p(t)/ȯ\nDividing both sides by (1 + r(t)) gives:\n1 ≥ p(t)/ȯ ⇒ ȯ ≥ p(t)\n\nStep 4: Economic implications\n- Equality holds when currency is valued: Price level p(t) must equal ȯ\n- Shows currency can only circulate if its purchasing power (1/p(t)) is maintained\n- Government money growth through interest payments must not erode real balances\n- Violation (p(t) > ȯ) would make currency dominated by bonds, leading to collapse\n\nQID: textbook-123-7-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-7-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the equilibrium condition, correctly identifies the components, derives the real return on currency, and provides accurate economic implications, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-7-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the equilibrium condition, correctly identifies the components, derives the real return on currency, and provides accurate economic implications, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-123-7-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the equilibrium condition, correctly identifies the components, derives the real return on currency, and provides accurate economic implications, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-35-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Compare the persistence of growth rates in the exogenous growth model ($\\sigma=2$) versus the endogenous growth model (Figure 27). Why does the endogenous model show higher persistence?\n\nGOLD_ANSWER:\n3.  **Persistence Mechanisms**:\n    - Exogenous model: Growth rates depend on transitional dynamics and stochastic shocks, leading to low persistence (correlation = 0.21).\n    - Endogenous model: With $\\alpha_k + \\alpha_h \\approx 1$, transition dynamics are slower, and growth rates are more stable (correlation = 0.78).\n    - The AK model's constant returns to accumulable factors sustain growth rate persistence.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain sufficient information to address the comparison between the exogenous and endogenous growth models. While the context details results for the exogenous model with σ=2 and σ=5 (including income distribution dynamics, growth rate correlations, and maximum growth rates), it does not mention any characteristics of the endogenous growth model (Figure 27), its persistence mechanisms, or the factors driving growth rate persistence. Therefore, a direct comparison or explanation of differences in persistence cannot be made based on the given context.\n\nQID: textbook-35-0-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-35-0-0-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-35-0-0-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}, "llm_echoed_qid": "textbook-35-0-0-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states that the provided context does not contain sufficient information."}
{"script_processing_qid": "textbook-55-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain why the allocation $(x_{1}^{*},...,x_{1}^{*},y_{1}^{*},...,y_{j}^{*})$ and price vector $\\alpha p^{*}(\\alpha>0)$ constitute a competitive equilibrium.\n\nGOLD_ANSWER:\n1.  **Profit Maximization**: $y_{j}^{*}$ solves $\\max p^{*} \\cdot y_{j}$ implies it also solves $\\max \\alpha p^{*} \\cdot y_{j}$ for $\\alpha > 0$.\n2.  **Utility Maximization**: The budget constraint $\\alpha p^{*} \\cdot x_{i} \\leq \\alpha p^{*} \\cdot \\omega_{i}$ is equivalent to $p^{*} \\cdot x_{i} \\leq p^{*} \\cdot \\omega_{i}$.\n3.  **Market Clearing**: $\\sum_{i=1}^{I} x_{i}^{*} = \\sum_{i=1}^{I} \\omega_{i} + \\sum_{j=1}^{J} y_{j}^{*}$ is independent of $\\alpha$.\n4.  **Conclusion**: Scaling prices by $\\alpha$ does not affect equilibrium conditions.\n\nCANDIDATE_ANSWER:\nStep 1: Understand competitive equilibrium requirements. A competitive equilibrium requires that (1) all agents maximize utility given prices and budgets, (2) firms maximize profits given prices, and (3) markets clear.\n\nStep 2: Observe original equilibrium validity. The text states $(x_1^{\\bullet},...,y_j^{\\bullet})$ with $p^{\\bullet}$ forms a competitive equilibrium, meaning these conditions are already satisfied at price vector $p^{\\bullet}$.\n\nStep 3: Recognize price homogeneity. Since utility functions and production sets are defined in real terms (not nominal), scaling all prices by $\\alpha > 0$ preserves:\n- Budget constraints (prices and incomes scale equally)\n- Profit maximization (maximizing $\\alpha p^{\\ast}\\cdot y_j$ is equivalent to maximizing $p^{\\ast}\\cdot y_j$)\n- Market clearing (physical allocation remains unchanged)\n\nStep 4: Verify allocation consistency. The same allocation $(x_1^{\\ast},...,y_j^{\\ast})$ remains feasible under $\\alpha p^{\\ast}$ because:\n- Consumption bundles $x_i^{\\ast}$ still satisfy scaled budget constraints\n- Production plans $y_j^{\\ast}$ remain profit-maximizing at scaled prices\n- Market clearing conditions are unaffected by price scaling\n\nThus, the scaled price vector $\\alpha p^{\\ast}$ maintains all equilibrium properties with the original allocation.\n\nQID: textbook-55-0-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-55-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly addresses all aspects of competitive equilibrium, including utility maximization, profit maximization, and market clearing, while correctly noting the price homogeneity property. The reasoning aligns perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-55-0-0-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly addresses all aspects of competitive equilibrium, including utility maximization, profit maximization, and market clearing, while correctly noting the price homogeneity property. The reasoning aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-55-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly addresses all aspects of competitive equilibrium, including utility maximization, profit maximization, and market clearing, while correctly noting the price homogeneity property. The reasoning aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-122-4-0-5", "llm_grader_input_prompt_user": "\nQUESTION:\n6. Interpret the correlation coefficient $\\rho_{XY}$ in terms of the strength and direction of the linear relationship between $X$ and $Y$. What does $\\rho_{XY} = -1$ imply?\n\nGOLD_ANSWER:\nThe correlation coefficient $\\rho_{XY}$ measures the strength and direction of the linear relationship between $X$ and $Y$. It ranges from $-1$ to $1$, where $1$ indicates a perfect positive linear relationship, $-1$ indicates a perfect negative linear relationship, and $0$ indicates no linear relationship. $\\rho_{XY} = -1$ implies that $Y = a + bX$ for some $b < 0$, meaning $Y$ decreases linearly as $X$ increases.\n\nCANDIDATE_ANSWER:\nStep 1: The correlation coefficient (ρ_{XY}) measures the strength and direction of the linear relationship between two variables X and Y. Its value ranges between -1 and 1.\n\nStep 2: Strength is determined by the absolute value of ρ_{XY}: closer to 1 (or -1) implies a stronger linear relationship. Direction is determined by the sign: positive values indicate a direct linear relationship (as X increases, Y tends to increase), while negative values indicate an inverse linear relationship (as X increases, Y tends to decrease).\n\nStep 3: ρ_{XY} = -1 implies a perfect negative linear relationship. This means X and Y lie exactly on a straight line with a negative slope (Y = a + bX, where b < 0). All deviations from their means are perfectly inversely proportional, leaving no room for error terms or non-linear patterns.\n\nQID: textbook-122-4-0-5\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-4-0-5\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly interprets the correlation coefficient ρ_{XY}, including its range, strength, direction, and the specific implication of ρ_{XY} = -1. The explanation aligns with the gold answer, providing a clear and accurate description of the concept.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-4-0-5", "category": "CORRECT", "explanation": "The candidate's answer correctly interprets the correlation coefficient ρ_{XY}, including its range, strength, direction, and the specific implication of ρ_{XY} = -1. The explanation aligns with the gold answer, providing a clear and accurate description of the concept."}, "llm_echoed_qid": "textbook-122-4-0-5", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly interprets the correlation coefficient ρ_{XY}, including its range, strength, direction, and the specific implication of ρ_{XY} = -1. The explanation aligns with the gold answer, providing a clear and accurate description of the concept."}
{"script_processing_qid": "textbook-119-30-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the challenges and potential solutions in estimating panel data models with large N and large T dimensions.\n\nGOLD_ANSWER:\n1. **Challenges**:\\n   - Computational complexity.\\n   - Incidental parameters problem (for large T).\\n   - Serial correlation and cross-sectional dependence.\\n2. **Solutions**:\\n   - Use advanced estimation techniques like GMM.\\n   - Apply robust standard errors.\\n   - Use dimensionality reduction techniques.\n\nCANDIDATE_ANSWER:\nStep 1: Challenges in large N and T panel data models include:<br>- Computational complexity due to high dimensionality, increasing estimation time and resource demands.<br>- Dynamic panel bias (e.g., Nickell bias) when lagged dependent variables are included, as fixed effects estimators become inconsistent for large T.<br>- Cross-sectional dependence and spatial correlation risks due to unobserved common factors across units.<br>- Non-stationarity in time series dimensions (e.g., unit roots) as T grows, violating standard assumptions.<br>- Heterogeneity in parameters across units or time periods that standard models cannot capture.<br><br>Step 2: Potential solutions include:<br>- Using the Generalized Method of Moments (GMM) for dynamic panels (e.g., Arellano-Bond estimator) to address endogeneity and bias.<br>- Employing distributed computing or sparse matrix algorithms to manage computational demands.<br>- Testing for cross-sectional dependence (e.g., Pesaran’s CD test) and applying robust covariance estimators (e.g., Driscoll-Kraay).<br>- Implementing panel unit root tests (e.g., Levin-Lin-Chu) and cointegration methods for non-stationarity.<br>- Utilizing interactive fixed effects or heterogeneous panel models (e.g., MG or PMG estimators) to account for parameter variability.\n\nQID: textbook-119-30-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-30-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer comprehensively addresses both the challenges and potential solutions in estimating panel data models with large N and large T dimensions. It aligns well with the gold answer, providing detailed explanations and specific techniques (e.g., GMM, robust covariance estimators) that match the solutions outlined in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-30-1-3", "category": "CORRECT", "explanation": "The candidate answer comprehensively addresses both the challenges and potential solutions in estimating panel data models with large N and large T dimensions. It aligns well with the gold answer, providing detailed explanations and specific techniques (e.g., GMM, robust covariance estimators) that match the solutions outlined in the gold answer."}, "llm_echoed_qid": "textbook-119-30-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer comprehensively addresses both the challenges and potential solutions in estimating panel data models with large N and large T dimensions. It aligns well with the gold answer, providing detailed explanations and specific techniques (e.g., GMM, robust covariance estimators) that match the solutions outlined in the gold answer."}
{"script_processing_qid": "textbook-122-18-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the predicted change in narr86 when pcnv increases by 0.5, holding other variables constant, using the arrest model.\n\nGOLD_ANSWER:\n1. **Model**: The estimated equation is $$ \\Delta n a\\hat{r}r86 = -0.151 \\times \\Delta pcnv $$.\n2. **Calculation**: For a 0.5 increase in pcnv, $$ \\Delta n a\\hat{r}r86 = -0.151 \\times 0.5 = -0.0755 $$.\n3. **Interpretation**: This means that, for a large group of men, a 0.5 increase in the proportion of convictions leads to a predicted decrease of 0.0755 arrests per man, or 7.55 arrests per 100 men.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the coefficient for pcnv in the estimated arrest model equation. The coefficient is -0.151.\nStep 2: Multiply the coefficient by the specified change in pcnv (0.5 units).\nCalculation: Δnarr86 = β₁ * Δpcnv = (-0.151) * 0.5 = -0.0755\nStep 3: Interpret the result. A 0.5 increase in pcnv predicts a decrease of approximately 0.0755 arrests (narr86), holding other variables constant.\n\nQID: textbook-122-18-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-18-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly identified the coefficient, performed the calculation, and interpreted the result accurately, matching the gold answer in both reasoning and final answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-18-0-1", "category": "CORRECT", "explanation": "The candidate correctly identified the coefficient, performed the calculation, and interpreted the result accurately, matching the gold answer in both reasoning and final answer."}, "llm_echoed_qid": "textbook-122-18-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly identified the coefficient, performed the calculation, and interpreted the result accurately, matching the gold answer in both reasoning and final answer."}
{"script_processing_qid": "textbook-119-26-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Interpret the coefficients $\\beta_{2}$ and $\\beta_{3}$ in the context of the dummy variable regression model. What do their statistical significances imply about regional salary differences?\n\nGOLD_ANSWER:\n1.  **$\\beta_{2}$**: This coefficient represents the difference in mean salary between the Northeast/North Central region and the West region. A negative value indicates that teachers in the Northeast/North Central earn less than those in the West. The $p$-value of 0.2330 suggests this difference is not statistically significant.\n2.  **$\\beta_{3}$**: This coefficient represents the difference in mean salary between the South region and the West region. A negative value indicates that teachers in the South earn less than those in the West. The $p$-value of 0.0349 suggests this difference is statistically significant at the 5% level.\n\nCANDIDATE_ANSWER:\nStep 1: Interpret coefficients β₂ and β₃\n- β₂ (-1,734.473) represents the difference in average salary between Northeast/North Central (D₂=1) and the base category (West, where D₂=D₃=0).\n- β₃ (-3,264.615) represents the difference in average salary between South (D₃=1) and the West.\n\nStep 2: Assess statistical significance\n- β₂ has p-value = 0.2330 (>0.05), indicating the Northeast/North Central salary difference from the West is not statistically significant.\n- β₃ has p-value = 0.0349 (<0.05), indicating the South's salary difference from the West is statistically significant.\n\nStep 3: Implications\n- Teachers in the South earn $3,264.62 less on average than those in the West (significant at 5% level).\n- The apparent $1,734.47 lower salary in Northeast/North Central vs. West could be due to random variation rather than systematic regional differences.\n\nQID: textbook-119-26-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-26-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately interprets the coefficients β₂ and β₃, correctly identifies their statistical significances, and draws appropriate conclusions about regional salary differences, matching the gold answer in both content and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-26-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately interprets the coefficients β₂ and β₃, correctly identifies their statistical significances, and draws appropriate conclusions about regional salary differences, matching the gold answer in both content and reasoning."}, "llm_echoed_qid": "textbook-119-26-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately interprets the coefficients β₂ and β₃, correctly identifies their statistical significances, and draws appropriate conclusions about regional salary differences, matching the gold answer in both content and reasoning."}
{"script_processing_qid": "textbook-119-11-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Show that the square of a $t$-distributed random variable with $k$ df follows an $F$ distribution with 1 and $k$ df.\n\nGOLD_ANSWER:\n1.  **Definition of $t$**: $t_k = \\frac{Z_1}{\\sqrt{Z_2/k}}$, where $Z_1 \\sim N(0,1)$ and $Z_2 \\sim \\chi_k^2$.\n2.  **Square of $t$**: $t_k^2 = \\frac{Z_1^2}{Z_2/k} = \\frac{Z_1^2 / 1}{Z_2 / k}$.\n3.  **$F$-Distribution**: Since $Z_1^2 \\sim \\chi_1^2$ and $Z_2 \\sim \\chi_k^2$, $t_k^2 \\sim F_{1,k}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the definition of a t-distributed variable. If t_k = Z₁ / √(Z₂/k), where Z₁ ~ N(0,1) and Z₂ ~ χ²(k) are independent, then squaring both sides gives:\n\nt_k² = (Z₁)² / (Z₂/k)\n\nStep 2: Recognize that Z₁² follows a χ² distribution with 1 degree of freedom (since the square of a standard normal variable is χ²(1)):\n\nZ₁² ~ χ²(1)\n\nStep 3: Rewrite the squared t-variable as a ratio of scaled chi-square variables:\n\nt_k² = (Z₁²/1) / (Z₂/k)\n\nStep 4: Compare this to the F-distribution definition F_{k₁,k₂} = (χ²(k₁)/k₁) / (χ²(k₂)/k₂). Here:\n- Numerator: χ²(1)/1 (k₁ = 1)\n- Denominator: χ²(k)/k (k₂ = k)\n\nStep 5: Conclude that t_k² follows an F(1,k) distribution:\n\nt_k² ~ F_{1,k}\n\nQID: textbook-119-11-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-11-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to show that the square of a t-distributed random variable with k degrees of freedom follows an F distribution with 1 and k degrees of freedom. The reasoning and conclusion align perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-11-1-1", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to show that the square of a t-distributed random variable with k degrees of freedom follows an F distribution with 1 and k degrees of freedom. The reasoning and conclusion align perfectly with the gold answer."}, "llm_echoed_qid": "textbook-119-11-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to show that the square of a t-distributed random variable with k degrees of freedom follows an F distribution with 1 and k degrees of freedom. The reasoning and conclusion align perfectly with the gold answer."}
{"script_processing_qid": "textbook-111-21-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the difference between GDP and GNP in the context of factor payments.\n\nGOLD_ANSWER:\nGDP measures income produced domestically, while GNP measures income earned by nationals. The difference arises from factor payments: GNP includes income earned by nationals abroad and excludes income earned by foreigners domestically, whereas GDP does the opposite.\n\nCANDIDATE_ANSWER:\nStep 1: GDP (Gross Domestic Product) measures the total value of goods and services produced within a country's borders, regardless of the ownership of the production factors.\n\nStep 2: GNP (Gross National Product) measures the total value of goods and services produced by a country's residents (nationals), both domestically and abroad.\n\nStep 3: The key difference lies in factor payments. GNP adjusts GDP by adding receipts of factor income (e.g., wages, profits, rent) earned by domestic residents from foreign sources and subtracting payments of factor income sent to foreign residents working domestically:\n\nGNP = GDP + (Factor Payments From Abroad − Factor Payments to Abroad)\n\nThus, GDP is location-based (production within borders), while GNP is ownership-based (income earned by nationals). Factor payments represent the net flow of income between domestic and foreign entities.\n\nQID: textbook-111-21-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-21-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the difference between GDP and GNP, including the role of factor payments, and matches the gold answer in both content and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-21-1-1", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the difference between GDP and GNP, including the role of factor payments, and matches the gold answer in both content and reasoning."}, "llm_echoed_qid": "textbook-111-21-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the difference between GDP and GNP, including the role of factor payments, and matches the gold answer in both content and reasoning."}
{"script_processing_qid": "textbook-122-33-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Show that the exponential function is the inverse of the natural logarithm by proving $\\exp[\\log(x)] = x$ for $x > 0$.\n\nGOLD_ANSWER:\n1. **Definition of Inverse**: Two functions $f$ and $g$ are inverses if $f(g(x)) = x$ and $g(f(x)) = x$.\n2. **Let $y = \\log(x)$**: By definition, $\\exp(y) = x$.\n3. **Substitute**:\n   $$ \\exp[\\log(x)] = \\exp(y) = x. $$\n4. **Conclusion**: The exponential function undoes the natural logarithm, confirming they are inverses.\n\nCANDIDATE_ANSWER:\nStep 1: Recall from the provided context that the exponential function is defined as the inverse of the natural logarithm. Specifically, the text states: 'The exponential function is the inverse of the log function in the following sense: log[exp(x)] = x for all x, and exp[log(x)] = x for x > 0.'\\n\\nStep 2: Observe that the second part of this statement directly addresses the required proof. For any x > 0 (the domain of the log function), applying the exponential function to log(x) undoes the logarithm operation.\\n\\nStep 3: Conclude that by definition of inverse functions, exp[log(x)] = x holds true for all x > 0, as explicitly stated in the provided econometrics text.\n\nQID: textbook-122-33-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-33-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly references the definition of inverse functions from the provided context and directly applies it to prove the given statement, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-33-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly references the definition of inverse functions from the provided context and directly applies it to prove the given statement, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-122-33-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly references the definition of inverse functions from the provided context and directly applies it to prove the given statement, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-114-6-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the Lerner Index for a monopolist and explain how it measures monopoly power.\n\nGOLD_ANSWER:\n1. **Lerner Index**: $L = \\frac{P - MC}{P}$.\n2. **Derivation**: For a profit-maximizing monopolist, $MR = MC$. Since $MR = P(1 - \\frac{1}{|E_d|})$, substituting gives $P(1 - \\frac{1}{|E_d|}) = MC$.\n3. **Rearrange**: $\\frac{P - MC}{P} = \\frac{1}{|E_d|}$.\n4. **Interpretation**: The Lerner Index equals the inverse of the absolute value of the price elasticity of demand. Higher elasticity reduces monopoly power.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the profit-maximizing condition for a monopolist where marginal revenue (MR) equals marginal cost (MC): MR = MC.\n\nStep 2: Recall the relationship between MR, price (P), and price elasticity of demand (Ed):\nMR = P(1 - 1/|Ed|)\n\nStep 3: Substitute MR from Step 2 into the profit-maximizing condition from Step 1:\nP(1 - 1/|Ed|) = MC\n\nStep 4: Rearrange to isolate (P - MC)/P (the Lerner Index):\n(P - MC)/P = 1/|Ed|\n\nThis shows the percentage markup over marginal cost equals the inverse of the absolute value of demand elasticity. The Lerner Index measures monopoly power by quantifying how much a firm can raise prices above marginal costs. Lower demand elasticity (|Ed| → 0) leads to a higher Lerner Index, indicating greater monopoly power.\n\nQID: textbook-114-6-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-6-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the Lerner Index, matches the gold answer's steps, and accurately interprets its meaning for monopoly power.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-6-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the Lerner Index, matches the gold answer's steps, and accurately interprets its meaning for monopoly power."}, "llm_echoed_qid": "textbook-114-6-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the Lerner Index, matches the gold answer's steps, and accurately interprets its meaning for monopoly power."}
{"script_processing_qid": "textbook-123-11-3-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the population least squares estimator $\\beta = [E(X^{\\prime}X)]^{-1}E(X^{\\prime}Y)$ from the normal equation (2.5.2).\n\nGOLD_ANSWER:\n1. **Normal Equation**: $E X^{\\prime}(Y - X\\beta) = 0$.\n2. **Expand**: $E X^{\\prime}Y - E X^{\\prime}X \\beta = 0$.\n3. **Rearrange**: $E X^{\\prime}X \\beta = E X^{\\prime}Y$.\n4. **Solve**: $\\beta = [E(X^{\\prime}X)]^{-1}E(X^{\\prime}Y)$, assuming $E(X^{\\prime}X)$ is invertible.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the normal equation (2.5.2) stating orthogonality between projection error and regressors:\nE[X'(Y - Xβ)] = 0\n\nStep 2: Expand the equation using linearity of expectations:\nE[X'Y] - E[X'Xβ] = 0\n\nStep 3: Recognize that β is a constant vector (not random), allowing us to factor it out:\nE[X'Y] - E[X'X]β = 0\n\nStep 4: Rearrange terms to isolate β:\nE[X'X]β = E[X'Y]\n\nStep 5: Premultiply both sides by [E(X'X)]^{-1} (assuming it exists):\nβ = [E(X'X)]^{-1}E(X'Y)\n\nThis derivation follows directly from the orthogonality condition in population least squares, where the projection error Y-Xβ is uncorrelated with the regressors X.\n\nQID: textbook-123-11-3-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-11-3-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the population least squares estimator from the normal equation, matching the gold answer in both reasoning and final result.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-11-3-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the population least squares estimator from the normal equation, matching the gold answer in both reasoning and final result."}, "llm_echoed_qid": "textbook-123-11-3-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the population least squares estimator from the normal equation, matching the gold answer in both reasoning and final result."}
{"script_processing_qid": "textbook-118-12-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the $t(1)$ distribution (Cauchy distribution) has no moments, and why the $t(2)$ distribution has no variance.\n\nGOLD_ANSWER:\n1. **$t(1)$**: The tails of the Cauchy distribution are so thick that the integrals defining the moments diverge. For example, $\\operatorname{E}(|t|) = \\infty$.\n2. **$t(2)$**: The variance integral $\\operatorname{E}(t^2)$ diverges because the tails decay too slowly (as $t^{-3}$).\n\nCANDIDATE_ANSWER:\nStep 1: Recall that the existence of moments for the t-distribution depends on its tail behavior. For a t(m) distribution, probability density decays proportionally to |t|^{-(m+1)} as |t| → ∞.\n\nStep 2: A moment E[|t|^k] exists if the integral ∫|t|^k ⋅ f(t)dt converges. For the t(m) distribution, this requires |t|^k ⋅ |t|^{-(m+1)} = |t|^{k−(m+1)} to be integrable at infinity. This occurs only if k − (m+1) < −1 ⇒ k < m.\n\nStep 3: For t(1) (Cauchy):\n- Maximum allowable k < m ⇒ k < 1 ⇒ no moments exist (since k must be ≥ 1 for the first moment).\n\nStep 4: For t(2):\n- Maximum allowable k < m ⇒ k < 2 ⇒ the mean (k=1) exists, but variance (k=2) requires k=2, which violates k < m=2 ⇒ variance does not exist.\n\nQID: textbook-118-12-2-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-12-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's reasoning aligns with the gold answer, correctly explaining why the $t(1)$ distribution has no moments and why the $t(2)$ distribution has no variance, based on the tail behavior and integrability conditions.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-12-2-1", "category": "CORRECT", "explanation": "The candidate's reasoning aligns with the gold answer, correctly explaining why the $t(1)$ distribution has no moments and why the $t(2)$ distribution has no variance, based on the tail behavior and integrability conditions."}, "llm_echoed_qid": "textbook-118-12-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's reasoning aligns with the gold answer, correctly explaining why the $t(1)$ distribution has no moments and why the $t(2)$ distribution has no variance, based on the tail behavior and integrability conditions."}
{"script_processing_qid": "textbook-114-22-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the scale economies index (SCI) from the cost-output elasticity $E\\mathrm{c}$ and explain its interpretation for economies and diseconomies of scale.\n\nGOLD_ANSWER:\n1. **Definition**: SCI is defined as $\\mathrm{SCI} = 1 - E\\mathrm{c}$.\n2. **Interpretation**:\n   - If $E\\mathrm{c} < 1$, SCI is positive, indicating economies of scale.\n   - If $E\\mathrm{c} = 1$, SCI is zero, indicating constant returns to scale.\n   - If $E\\mathrm{c} > 1$, SCI is negative, indicating diseconomies of scale.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the cost-output elasticity (E_c), defined as the percentage change in costs relative to the percentage change in output.\n\nStep 2: Derive the scale economies index (SCI) using the relationship provided in the context:\n$$ \\mathrm{SCI} = 1 - E_{\\mathrm{c}} $$ \n\nStep 3: Interpret the SCI values:\n- If $E_{\\mathrm{c}} < 1$, then $\\mathrm{SCI} > 0$: Indicates economies of scale (costs rise more slowly than output).\n- If $E_{\\mathrm{c}} > 1$, then $\\mathrm{SCI} < 0$: Indicates diseconomies of scale (costs rise faster than output).\n- If $E_{\\mathrm{c}} = 1$, then $\\mathrm{SCI} = 0$: Constant returns to scale (no economies/diseconomies).\n\nStep 4: Magnitude interpretation:\nThe absolute value of SCI quantifies the strength of economies/diseconomies. For example, a larger positive SCI implies stronger economies of scale (as seen in the 1955 electric power example where SCI diminished with firm size).\n\nQID: textbook-114-22-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-22-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derived the SCI from the cost-output elasticity and accurately interpreted the SCI values for economies and diseconomies of scale, including the case of constant returns to scale. The additional interpretation of the magnitude of SCI is also correct and relevant.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-22-0-0", "category": "CORRECT", "explanation": "The candidate correctly derived the SCI from the cost-output elasticity and accurately interpreted the SCI values for economies and diseconomies of scale, including the case of constant returns to scale. The additional interpretation of the magnitude of SCI is also correct and relevant."}, "llm_echoed_qid": "textbook-114-22-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derived the SCI from the cost-output elasticity and accurately interpreted the SCI values for economies and diseconomies of scale, including the case of constant returns to scale. The additional interpretation of the magnitude of SCI is also correct and relevant."}
{"script_processing_qid": "textbook-114-23-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why a competitive market equilibrium is inefficient in the presence of negative externalities, using the concepts of marginal private cost (MC) and marginal social cost (MSC).\n\nGOLD_ANSWER:\n1.  **Market Equilibrium**: In a competitive market, firms produce where $P = MC$.\n2.  **Inefficiency**: Since $MSC = MC + MEC$, the market equilibrium output ($P = MC$) is higher than the socially optimal output ($P = MSC$).\n3.  **Result**: The market overproduces the good, leading to excessive negative externalities (e.g., pollution).\n\nCANDIDATE_ANSWER:\nStep 1: Understand the key definitions. In a competitive market with negative externalities, firms base production decisions on marginal private cost (MC), which excludes external costs. Marginal social cost (MSC) = MC + marginal external cost (MEC).\n\nStep 2: Identify equilibrium outcomes. The market equilibrium occurs where demand (marginal private benefit) intersects MC. However, the socially efficient output occurs where demand intersects MSC.\n\nStep 3: Compare outcomes. Because MSC > MC (due to MEC), the market equilibrium quantity (Q_market) is greater than the socially efficient quantity (Q_efficient). This means the market overproduces the good.\n\nStep 4: Explain inefficiency. At Q_market, the MSC of producing the last unit exceeds its marginal private benefit (price). This creates a deadweight loss, as resources are allocated beyond the point where social benefits equal social costs.\n\nQID: textbook-114-23-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-23-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the inefficiency in a competitive market equilibrium due to negative externalities, correctly using the concepts of MC and MSC, and aligns with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-23-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the inefficiency in a competitive market equilibrium due to negative externalities, correctly using the concepts of MC and MSC, and aligns with the gold answer."}, "llm_echoed_qid": "textbook-114-23-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the inefficiency in a competitive market equilibrium due to negative externalities, correctly using the concepts of MC and MSC, and aligns with the gold answer."}
{"script_processing_qid": "textbook-115-9-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the log-likelihood function for the full maximum likelihood estimation of the Tobit model with endogenous explanatory variables, using the joint distribution $f(y\\_{1},y\\_{2}\\mid\\mathbf{z})$.\n\nGOLD_ANSWER:\n1.  **Joint Distribution**: Factor the joint density as $$f(y\\_{1},y\\_{2}\\mid\\mathbf{z})=f(y\\_{1}\\mid y\\_{2},\\mathbf{z})f(y\\_{2}\\mid\\mathbf{z}).$$\\n2.  **Marginal Density**: $f(y\\_{2}\\mid\\mathbf{z})$ is $\\mathrm{Normal}(\\mathbf{z}\\delta\\_{2},\\tau\\_{2}^{2})$.\\n3.  **Conditional Density**: $y\\_{1}\\mid(y\\_{2},\\mathbf{z})$ follows a Tobit with latent mean $$\\mathbf{z}\\_{1}\\delta\\_{1}+\\alpha\\_{1}y\\_{2}+(\\eta\\_{1}/\\tau\\_{2}^{2})(y\\_{2}-\\mathbf{z}\\delta\\_{2})$$ and variance $\\tau\\_{1}^{2}=\\sigma\\_{1}^{2}-(\\eta\\_{1}^{2}/\\tau\\_{2}^{2})$.\\n4.  **Log-Likelihood**: Sum the log of the joint density across observations, with parameters $(\\pmb{\\delta}\\_{1},\\alpha\\_{1},\\pmb{\\delta}\\_{2},\\sigma\\_{1}^{2},\\tau\\_{2}^{2},\\eta\\_{1})$.\n\nCANDIDATE_ANSWER:\nStep 1: Factor the joint distribution using conditional probability:\n\n$f(y_1,y_2|\\mathbf{z}) = f(y_1|y_2,\\mathbf{z}) \\cdot f(y_2|\\mathbf{z})$\n\nStep 2: Specify components:\n1. $f(y_2|\\mathbf{z})$ follows normal distribution from reduced form equation:\n$y_2 \\sim N(\\mathbf{z}\\delta_2, \\sigma_2^2)$\n2. $f(y_1|y_2,\\mathbf{z})$ follows censored normal distribution from structural equation:\n$y_1^* = \\mathbf{z}_1\\delta_1 + \\alpha_1 y_2 + u_1$\nwith $u_1|y_2,\\mathbf{z} \\sim N(\\rho\\sigma_1^{-1}v_2, (1-\\rho^2)\\sigma_1^2)$\nwhere $v_2 = y_2 - \\mathbf{z}\\delta_2$ and $\\rho$ = Corr(u₁,v₂)\n\nStep 3: Construct likelihood components:\nFor uncensored observations (y₁ > 0):\n$f(y_1,y_2|\\mathbf{z}) = \\phi\\left(\\frac{y_1 - (\\mathbf{z}_1\\delta_1 + \\alpha_1 y_2 + \\rho\\sigma_1 v_2)}{\\sigma_1\\sqrt{1-\\rho^2}}\\right) \\cdot \\phi\\left(\\frac{v_2}{\\sigma_2}\\right)$\n\nFor censored observations (y₁ = 0):\n$f(y_1,y_2|\\mathbf{z}) = \\Phi\\left(-\\frac{\\mathbf{z}_1\\delta_1 + \\alpha_1 y_2 + \\rho\\sigma_1 v_2}{\\sigma_1\\sqrt{1-\\rho^2}}\\right) \\cdot \\phi\\left(\\frac{v_2}{\\sigma_2}\\right)$\n\nStep 4: Take natural logarithm and sum over all observations:\n$\\ln L = \\sum_{i=1}^N \\left[\\ln f(y_{1i}|y_{2i},\\mathbf{z}_i) + \\ln f(y_{2i}|\\mathbf{z}_i)\\right]$\n\nWhere:\n- $\\phi(\\cdot)$ = standard normal PDF\n- $\\Phi(\\cdot)$ = standard normal CDF\n- $v_2 = y_2 - \\mathbf{z}\\delta_2$\n- Parameters to estimate: $(\\delta_1, \\alpha_1, \\delta_2, \\sigma_1, \\sigma_2, \\rho)$\n\nQID: textbook-115-9-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-9-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the log-likelihood function for the Tobit model with endogenous explanatory variables, matching the gold answer's structure and components. Both specify the joint distribution factorization, marginal and conditional densities, and the final log-likelihood summation.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-9-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly derives the log-likelihood function for the Tobit model with endogenous explanatory variables, matching the gold answer's structure and components. Both specify the joint distribution factorization, marginal and conditional densities, and the final log-likelihood summation."}, "llm_echoed_qid": "textbook-115-9-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the log-likelihood function for the Tobit model with endogenous explanatory variables, matching the gold answer's structure and components. Both specify the joint distribution factorization, marginal and conditional densities, and the final log-likelihood summation."}
{"script_processing_qid": "textbook-114-30-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the welfare implications of first-degree price discrimination. How does it affect consumer surplus, producer surplus, and total surplus?\n\nGOLD_ANSWER:\n1. **Consumer Surplus**: Under perfect first-degree price discrimination, consumer surplus is zero, as the firm captures all surplus.\n2. **Producer Surplus**: Producer surplus increases, as the firm captures all consumer surplus and additional profit from expanded output.\n3. **Total Surplus**: Total surplus may increase if output expands to the competitive level ($Q^{**}$), but distributionally, all gains go to the producer.\n\nCANDIDATE_ANSWER:\nStep 1: Define first-degree price discrimination. Under perfect first-degree price discrimination, firms charge each customer their exact reservation price (maximum willingness to pay).\n\nStep 2: Analyze consumer surplus. Since consumers pay exactly their reservation price, there is no difference between what they are willing to pay and what they actually pay. Thus, consumer surplus is reduced to zero (fully captured by the producer).\n\nStep 3: Analyze producer surplus. The firm captures the entire area under the demand curve and above the marginal cost curve. Producer surplus increases to equal the total surplus (all consumer surplus is transferred to the producer).\n\nStep 4: Analyze total surplus. Unlike standard monopoly pricing, first-degree price discrimination eliminates deadweight loss by expanding output to the perfectly competitive level (where marginal cost equals demand). Total surplus is maximized and identical to the outcome under perfect competition.\n\nSummary: Under perfect first-degree price discrimination, (1) consumer surplus = 0, (2) producer surplus = total surplus, and (3) total surplus is maximized (no efficiency loss). Practical imperfect versions retain these directional effects but with less extreme magnitudes.\n\nQID: textbook-114-30-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-30-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately defines first-degree price discrimination and correctly analyzes its effects on consumer surplus (zero), producer surplus (equal to total surplus), and total surplus (maximized with no deadweight loss). The reasoning aligns perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-30-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately defines first-degree price discrimination and correctly analyzes its effects on consumer surplus (zero), producer surplus (equal to total surplus), and total surplus (maximized with no deadweight loss). The reasoning aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-114-30-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately defines first-degree price discrimination and correctly analyzes its effects on consumer surplus (zero), producer surplus (equal to total surplus), and total surplus (maximized with no deadweight loss). The reasoning aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-96-0-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. For a production function $f(\\mathbf{z})$, prove that the MRTS is equal to the ratio of input prices at the cost-minimizing input combination. Use the Lagrange method.\n\nGOLD_ANSWER:\n1. **Lagrangian**: $L = w\\_{1}z\\_{1} + w\\_{2}z\\_{2} + \\lambda(y - f(z\\_{1}, z\\_{2}))$.\n2. **FOCs**: $w\\_{1} = \\lambda \\frac{\\partial f}{\\partial z\\_{1}}$ and $w\\_{2} = \\lambda \\frac{\\partial f}{\\partial z\\_{2}}$.\n3. **MRTS**: $\\frac{w\\_{1}}{w\\_{2}} = \\frac{\\partial f/\\partial z\\_{1}}{\\partial f/\\partial z\\_{2}} = MRTS$.\n\nCANDIDATE_ANSWER:\nStep 1: Formulate the cost-minimization problem. The firm minimizes total cost $\\sum_{i=1}^L w_i z_i$ subject to the production constraint $f(\\mathbf{z}) = y$, where $w_i$ is the price of input $i$.\\n\\nStep 2: Set up the Lagrangian function:\\n$$\\mathcal{L} = \\sum_{i=1}^L w_i z_i + \\lambda(y - f(\\mathbf{z}))$$\\nwhere $\\lambda$ is the Lagrange multiplier.\\n\\nStep 3: Derive first-order conditions by taking partial derivatives with respect to $z_i$, $z_j$, and $\\lambda$:\\n- For input $i$: $\\frac{\\partial \\mathcal{L}}{\\partial z_i} = w_i - \\lambda \\frac{\\partial f(\\mathbf{z})}{\\partial z_i} = 0$\\n- For input $j$: $\\frac{\\partial \\mathcal{L}}{\\partial z_j} = w_j - \\lambda \\frac{\\partial f(\\mathbf{z})}{\\partial z_j} = 0$\\n- For $\\lambda$: $f(\\mathbf{z}) = y$\\n\\nStep 4: Rearrange the first-order conditions for inputs $i$ and $j$:\\n$$w_i = \\lambda \\frac{\\partial f(\\mathbf{z})}{\\partial z_i},\\quad w_j = \\lambda \\frac{\\partial f(\\mathbf{z})}{\\partial z_j}$$\\n\\nStep 5: Take the ratio of the rearranged equations:\\n$$\\frac{w_i}{w_j} = \\frac{\\partial f(\\mathbf{z})/\\partial z_i}{\\partial f(\\mathbf{z})/\\partial z_j}$$\\n\\nStep 6: Recognize that the right-hand side is the definition of $MRTS_{ij}(\\mathbf{z})$. Thus, at the cost-minimizing input combination:\\n$$MRTS_{ij}(\\mathbf{z}) = \\frac{w_i}{w_j}$$\n\nQID: textbook-96-0-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-96-0-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps of the Lagrange method to prove that the MRTS is equal to the ratio of input prices at the cost-minimizing input combination. The reasoning and calculations align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-96-0-1-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps of the Lagrange method to prove that the MRTS is equal to the ratio of input prices at the cost-minimizing input combination. The reasoning and calculations align with the gold answer."}, "llm_echoed_qid": "textbook-96-0-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps of the Lagrange method to prove that the MRTS is equal to the ratio of input prices at the cost-minimizing input combination. The reasoning and calculations align with the gold answer."}
{"script_processing_qid": "textbook-112-13-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Explain Adam Smith's argument about the political influence of monopolies, using the analogy of an 'overgrown standing army.'\n\nGOLD_ANSWER:\n1. **Political Influence**: Smith argues that monopolies, by concentrating wealth and power in specific industries, become politically influential, much like an overgrown standing army. They can intimidate the legislature and shape policies in their favor.\n2. **Legislative Intimidation**: Monopolists use their economic power to influence lawmakers, ensuring policies that protect their interests, even if detrimental to the broader economy.\n3. **Public Choice Theory**: This aligns with modern public choice theory, where concentrated interests (monopolists) have disproportionate influence over diffuse interests (consumers).\n\nCANDIDATE_ANSWER:\nStep 1: Adam Smith uses the analogy of an 'overgrown standing army' to highlight the disproportionate power monopolies can wield. Step 2: Just as an excessively large military force could threaten a government's autonomy, monopolies grow so large and entrenched that they become 'formidable to the government.' Step 3: Smith argues that monopolies (the 'particular tribes') use their economic dominance to intimidate the legislature, undermining fair governance by pressuring policymakers to act in their private interest. Step 4: This parallels how a standing army might coerce political decisions if unchecked, creating a dangerous imbalance of power that harms both economic freedom and democratic processes.\n\nQID: textbook-112-13-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-13-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately captures Adam Smith's argument about the political influence of monopolies using the analogy of an 'overgrown standing army.' It correctly explains how monopolies can intimidate the legislature and undermine fair governance, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-13-0-0", "category": "CORRECT", "explanation": "The candidate answer accurately captures Adam Smith's argument about the political influence of monopolies using the analogy of an 'overgrown standing army.' It correctly explains how monopolies can intimidate the legislature and undermine fair governance, aligning with the gold answer."}, "llm_echoed_qid": "textbook-112-13-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately captures Adam Smith's argument about the political influence of monopolies using the analogy of an 'overgrown standing army.' It correctly explains how monopolies can intimidate the legislature and undermine fair governance, aligning with the gold answer."}
{"script_processing_qid": "textbook-120-24-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that if $|\\alpha| < 1$, then $\\alpha^n \\to 0$.\n\nGOLD_ANSWER:\n1. **Pick $\\epsilon > 0$**: We need $|\\alpha^n| < \\epsilon$ for large $n$.\n2. **Logarithmic Transformation**: $|\\alpha^n| < \\epsilon$ iff $n \\ln |\\alpha| < \\ln \\epsilon$.\n3. **Solve for $n$**: Since $\\ln |\\alpha| < 0$, $n > \\frac{\\ln \\epsilon}{\\ln |\\alpha|}$.\n4. **Archimedean Property**: Such an $n$ exists, proving convergence.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain enough information to answer this question. While Lemma 3.5.1 states that if $|\\alpha| < 1$ then $\\alpha^n \\to 0$, the extracted text passages do not provide any proof or derivation steps for this lemma. The surrounding discussion focuses on bisection methods and other sequences ($x_n = \\sum \\alpha^t$), but offers no insights about the convergence of geometric sequences $\\alpha^n$ specifically.\n\nQID: textbook-120-24-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-120-24-1-0\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it provides a proof rather than stating insufficient information.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-120-24-1-0", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it provides a proof rather than stating insufficient information."}, "llm_echoed_qid": "textbook-120-24-1-0", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it provides a proof rather than stating insufficient information."}
{"script_processing_qid": "textbook-110-8-3-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the limitations of the Folk Theorem in real-world applications.\n\nGOLD_ANSWER:\nThe Folk Theorem assumes perfect monitoring and infinite repetition, which may not hold in practice. Additionally, coordination on equilibrium strategies can be challenging, and renegotiation may undermine punishment threats.\n\nCANDIDATE_ANSWER:\nStep 1: The Folk Theorem requires players to have a discount factor δ sufficiently close to 1 (i.e., high patience). Real-world actors often exhibit impatience (lower δ), making cooperation unsustainable as future gains are undervalued.\n\nStep 2: The theorem assumes Nash-equilibrium payoffs lie in the interior of the feasible payoff set 𝕌*. In practice, these payoffs may lie on the boundary of 𝕌*, invalidating the theorem’s conditions and limiting its applicability.\n\nStep 3: The trigger strategy relies on perfect, irreversible punishment (permanent reversion to stage-game Nash equilibrium after any deviation). Real-world scenarios may involve renegotiation or incomplete monitoring, making such strategies unrealistic or difficult to enforce.\n\nQID: textbook-110-8-3-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-110-8-3-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies and elaborates on key limitations of the Folk Theorem, including discount factors, payoff set boundaries, and enforcement challenges, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-8-3-3", "category": "CORRECT", "explanation": "The candidate answer accurately identifies and elaborates on key limitations of the Folk Theorem, including discount factors, payoff set boundaries, and enforcement challenges, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-110-8-3-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies and elaborates on key limitations of the Folk Theorem, including discount factors, payoff set boundaries, and enforcement challenges, aligning well with the gold answer."}
{"script_processing_qid": "textbook-10-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the score and Hessian for observation t in a general MLE framework. Discuss their roles in the estimation process.\n\nGOLD_ANSWER:\n1.  **Score**: The score for observation t is the first derivative of the log-likelihood function with respect to the parameter vector $\\theta$: $S_t(\\theta) = \\frac{\\partial \\log L_t(\\theta)}{\\partial \\theta}$.\n2.  **Hessian**: The Hessian is the second derivative of the log-likelihood function: $H_t(\\theta) = \\frac{\\partial^2 \\log L_t(\\theta)}{\\partial \\theta \\partial \\theta^T}$.\n3.  **Role**: The score vector guides the direction of parameter updates in optimization, while the Hessian provides information about the curvature of the likelihood function, influencing the step size and convergence properties.\n\nCANDIDATE_ANSWER:\nStep 1: Define the log-likelihood function for observation t\nLet ℓ_t(θ) = ln f(y_t|x_t;θ) be the log-likelihood contribution for observation t, where θ is the parameter vector.\n\nStep 2: Derive the score\nTake the first derivative of ℓ_t(θ) with respect to θ:\nScore: s_t(θ) = ∂ℓ_t(θ)/∂θ\n\nStep 3: Derive the Hessian\nTake the second derivative of ℓ_t(θ) with respect to θ:\nHessian: H_t(θ) = ∂²ℓ_t(θ)/∂θ∂θ'\n\nRole in estimation:\n1. The score vector s_t(θ) captures the sensitivity of the log-likelihood to parameter changes at each observation. The MLE estimator θ̂ solves ∑_{t=1}^n s_t(θ̂) = 0.\n\n2. The Hessian matrix H_t(θ) measures the curvature of the log-likelihood. Averaged across observations (-E[H_t(θ)]), it forms the Fisher Information Matrix used to calculate standard errors and test statistics. The Hessian's negative definiteness ensures a local maximum at θ̂.\n\nThe provided context (Section 508) implies this derivation structure, with subsequent sections (509-510, 524-525) showing how aggregated scores and Hessians enable consistency and asymptotic normality proofs through regularity conditions and information matrix equivalence.\n\nQID: textbook-10-0-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-10-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the score and Hessian for observation t in a general MLE framework and accurately discusses their roles in the estimation process. The explanation aligns with the gold answer, including the use of the score vector for parameter updates and the Hessian for curvature and convergence properties.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-10-0-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the score and Hessian for observation t in a general MLE framework and accurately discusses their roles in the estimation process. The explanation aligns with the gold answer, including the use of the score vector for parameter updates and the Hessian for curvature and convergence properties."}, "llm_echoed_qid": "textbook-10-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the score and Hessian for observation t in a general MLE framework and accurately discusses their roles in the estimation process. The explanation aligns with the gold answer, including the use of the score vector for parameter updates and the Hessian for curvature and convergence properties."}
{"script_processing_qid": "textbook-106-20-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Calculate the cross-price elasticity of demand if a 10% increase in the price of good Y leads to a 15% increase in the quantity demanded of good X.\n\nGOLD_ANSWER:\n1. **Formula**: $E_{XY} = \\frac{\\% \\Delta Q_X}{\\% \\Delta P_Y} = \\frac{15\\%}{10\\%} = 1.5$.\n2. **Interpretation**: Goods X and Y are substitutes ($E_{XY} > 0$).\n\nCANDIDATE_ANSWER:\nStep 1: Recall the formula for cross-price elasticity of demand (CPE):\nCPE = (% Change in Quantity Demanded of Good X) / (% Change in Price of Good Y)\n\nStep 2: Substitute the given values into the formula:\nCPE = 15% / 10% = 1.5\n\nStep 3: Interpret the result:\nA positive cross-price elasticity (1.5) indicates that goods X and Y are substitutes, as the increase in the price of Y led to an increase in demand for X.\n\nQID: textbook-106-20-2-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-106-20-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly calculated the cross-price elasticity of demand as 1.5 using the provided formula and accurately interpreted the result, stating that goods X and Y are substitutes. This matches the gold answer in both calculation and interpretation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-20-2-0", "category": "CORRECT", "explanation": "The candidate correctly calculated the cross-price elasticity of demand as 1.5 using the provided formula and accurately interpreted the result, stating that goods X and Y are substitutes. This matches the gold answer in both calculation and interpretation."}, "llm_echoed_qid": "textbook-106-20-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly calculated the cross-price elasticity of demand as 1.5 using the provided formula and accurately interpreted the result, stating that goods X and Y are substitutes. This matches the gold answer in both calculation and interpretation."}
{"script_processing_qid": "textbook-104-3-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Show that $\\mathcal{B}^{k+l} = \\mathcal{B}^k \\times \\mathcal{B}^l$ for all $k, l \\geq 1$, where $\\mathcal{B}^n$ denotes the Borel sets in $\\mathbf{R}^n$.\n\nGOLD_ANSWER:\n1. **Inclusion $\\mathcal{B}^{k+l} \\subseteq \\mathcal{B}^k \\times \\mathcal{B}^l$**: Every open set in $\\mathbf{R}^{k+l}$ is a countable union of open rectangles in $\\mathbf{R}^k \\times \\mathbf{R}^l$, which are in $\\mathcal{B}^k \\times \\mathcal{B}^l$.\n2. **Inclusion $\\mathcal{B}^k \\times \\mathcal{B}^l \\subseteq \\mathcal{B}^{k+l}$**: The product $\\sigma$-algebra $\\mathcal{B}^k \\times \\mathcal{B}^l$ is generated by measurable rectangles, which are Borel sets in $\\mathbf{R}^{k+l}$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the definitions. The Borel σ-algebra 𝒮ⁿ is generated by open sets in ℝⁿ. The product σ-algebra 𝒮ᵏ × 𝒮ˡ is generated by measurable rectangles A × B where A ∈ 𝒮ᵏ and B ∈ 𝒮ˡ.\n\nStep 2: Show 𝒮^{k+l} ⊆ 𝒮ᵏ × 𝒮ˡ. Any open set in ℝ^{k+l} is a countable union of open rectangles (basis elements). Each open rectangle U × V (U open in ℝᵏ, V open in ℝˡ) belongs to 𝒮ᵏ × 𝒮ˡ since U ∈ 𝒮ᵏ and V ∈ 𝒮ˡ. Thus all open sets in ℝ^{k+l} are in 𝒮ᵏ × 𝒮ˡ, meaning 𝒮^{k+l} ⊂ 𝒮ᵏ × 𝒮ˡ.\n\nStep 3: Show 𝒮ᵏ × 𝒮ˡ ⊆ 𝒮^{k+l}. Measurable rectangles A × B (A ∈ 𝒮ᵏ, B ∈ 𝒮ˡ) are Borel in ℝ^{k+l} because they can be written as intersections/products of Borel sets from the component spaces. Since 𝒮^{k+l} contains all such rectangles and is closed under σ-algebra operations, it contains the generated product σ-algebra. Thus 𝒮ᵏ × 𝒮ˡ ⊂ 𝒮^{k+l}.\n\nConclusion: Both inclusions prove 𝒮^{k+l} = 𝒮ᵏ × 𝒮ˡ.\n\nQID: textbook-104-3-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-104-3-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the structure of the gold answer, providing clear steps for both inclusions and using appropriate reasoning about σ-algebras and Borel sets.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-104-3-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the structure of the gold answer, providing clear steps for both inclusions and using appropriate reasoning about σ-algebras and Borel sets."}, "llm_echoed_qid": "textbook-104-3-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the structure of the gold answer, providing clear steps for both inclusions and using appropriate reasoning about σ-algebras and Borel sets."}
{"script_processing_qid": "textbook-103-5-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the difference equation for the unemployment rate in this model.\n\nGOLD_ANSWER:\nThe unemployment rate evolves as:\n$$ U^{\\prime} = \\lambda (1 - U) + U \\left[ 1 - f(U)(1 - F(\\bar{w})) \\right] $$\nwhere:\n- $\\lambda (1 - U)$ is the inflow into unemployment (fired workers).\n- $f(U)(1 - F(\\bar{w}))$ is the outflow from unemployment (workers who receive and accept job offers).\nIn a stationary equilibrium ($U^{\\prime} = U$), this simplifies to:\n$$ \\lambda (1 - U) = f(U) \\left[ 1 - F(\\bar{w}) \\right] U $$\n\nCANDIDATE_ANSWER:\nStep 1: Identify unemployment rate dynamics\nUnemployment rate next period (U_{t+1}) equals:\n- Current unemployed who remain unemployed: U_t × [1 - job finding probability]\n- Plus newly unemployed workers: λ × employed population (1 - U_t)\n\nStep 2: Determine job finding probability\nUnemployed workers find jobs with probability μ = f(U_t) (probability of receiving and accepting an offer)\n\nStep 3: Construct the equation\nU_{t+1} = U_t × (1 - f(U_t)) + λ(1 - U_t)\n\nFinal difference equation:\nU_{t+1} = U_t[1 - f(U_t) - λ] + λ\n\nWhere:\n- λ = employment separation rate\n- f(U_t) = job offer probability (decreasing in U_t)\n- (1 - U_t) = employed population proportion\n\nQID: textbook-103-5-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-103-5-1-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the components of the unemployment rate dynamics but makes an error in the final equation. The term `[1 - f(U_t) - λ]` is incorrect; it should be `[1 - f(U_t)(1 - F(\\\\bar{w}))]` as in the gold answer. The candidate's explanation of the components is mostly accurate but the final equation does not match the gold standard.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-5-1-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the components of the unemployment rate dynamics but makes an error in the final equation. The term `[1 - f(U_t) - λ]` is incorrect; it should be `[1 - f(U_t)(1 - F(\\bar{w}))]` as in the gold answer. The candidate's explanation of the components is mostly accurate but the final equation does not match the gold standard."}, "llm_echoed_qid": "textbook-103-5-1-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the components of the unemployment rate dynamics but makes an error in the final equation. The term `[1 - f(U_t) - λ]` is incorrect; it should be `[1 - f(U_t)(1 - F(\\bar{w}))]` as in the gold answer. The candidate's explanation of the components is mostly accurate but the final equation does not match the gold standard."}
{"script_processing_qid": "textbook-112-10-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Using the tobacco trade example from the text, derive the capital turnover time for the round-about trade compared to a direct trade. Assume the direct trade with America has a capital return time of 3-4 years.\n\nGOLD_ANSWER:\n1.  **Direct Trade**: Capital returns in 3-4 years (given).\n2.  **Round-About Trade**: Adds returns from other countries (e.g., France, Holland) to the American returns.\n3.  **Total Time**: If American returns take 3-4 years, and returns from Europe add 1-2 years, total capital turnover is 4-6 years.\n4.  **Implication**: The round-about trade reduces the frequency of capital turnover, lowering the proportion of domestic industry supported annually (from 1/3-1/4 to 1/4-1/5).\n\nCANDIDATE_ANSWER:\nStep 1: Identify the direct trade capital return time (3-4 years) from the context.\nStep 2: Recognize that round-about trade adds additional time for returns from other countries to the American returns time, as stated in the text.\nStep 3: Assume the other countries’ returns time is comparable to the American leg due to similar trade conditions (no explicit values given).\nStep 4: Derive total round-about turnover time by summing both phases: 3-4 years (America) + 3-4 years (other countries) = 6-8 years.\nConclusion: Round-about trade doubles the capital turnover time compared to direct trade (6-8 years vs. 3-4 years).\n\nQID: textbook-112-10-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-10-0-0\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the direct trade capital return time and understands that round-about trade adds additional time. However, the assumption that other countries’ returns take 3-4 years (same as America) is not supported by the gold answer, which specifies 1-2 years for Europe. This leads to an overestimation of the total round-about turnover time.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-10-0-0", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the direct trade capital return time and understands that round-about trade adds additional time. However, the assumption that other countries’ returns take 3-4 years (same as America) is not supported by the gold answer, which specifies 1-2 years for Europe. This leads to an overestimation of the total round-about turnover time."}, "llm_echoed_qid": "textbook-112-10-0-0", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the direct trade capital return time and understands that round-about trade adds additional time. However, the assumption that other countries’ returns take 3-4 years (same as America) is not supported by the gold answer, which specifies 1-2 years for Europe. This leads to an overestimation of the total round-about turnover time."}
{"script_processing_qid": "textbook-118-11-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the geometric interpretation of the regression model $\\mathbf{y} = X\\beta + \\mathbf{u}$ in terms of subspaces.\n\nGOLD_ANSWER:\n1. **Subspace Interpretation**: The model decomposes $\\mathbf{y}$ into a component $X\\beta$ in the column space $\\mathcal{S}(X)$ and a residual $\\mathbf{u}$ in the orthogonal complement $\\mathcal{S}^\\perp(X)$.\n2. **Orthogonality**: The residual $\\mathbf{u}$ is orthogonal to $X\\beta$, ensuring that $\\mathbf{u}$ captures the part of $\\mathbf{y}$ not explained by $X\\beta$.\n3. **Geometric Representation**: In $E^n$, $\\mathbf{y}$ is the sum of the projection onto $\\mathcal{S}(X)$ and the orthogonal residual.\n\nCANDIDATE_ANSWER:\nStep 1: In the regression model 𝐲 = Xβ + 𝐮, the regressand 𝐲 and regressors (columns of X) are vectors in Euclidean space Eⁿ. The column space 𝔖(X) = span{𝐱₁, ..., 𝐱ₖ} represents all possible linear combinations of regressors.\n\nStep 2: The model decomposes 𝐲 into two orthogonal components: \n- Xβ (estimated vector) ∈ 𝔖(X)\n- 𝐮 (residuals) ∈ 𝔖^⊥(X)\n\nStep 3: Geometrically, Xβ is the orthogonal projection of 𝐲 onto 𝔖(X). This projection minimizes ‖𝐮‖² = ‖𝐲 - Xβ‖², corresponding to least squares estimation.\n\nStep 4: The orthogonality condition 𝐮 ⊥ 𝔖(X) implies residuals are perpendicular to all regressors (Xᵀ𝐮 = 0), satisfying the normal equations of regression.\n\nStep 5: Dimensionally, 𝔖(X) has rank k (if X has full column rank), leaving n-k dimensions in 𝔖^⊥(X) for residual variation.\n\nQID: textbook-118-11-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-11-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes the geometric interpretation of the regression model, including the decomposition into orthogonal components, the projection onto the column space, and the orthogonality condition. It matches the gold answer in both content and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-11-2-3", "category": "CORRECT", "explanation": "The candidate's answer accurately describes the geometric interpretation of the regression model, including the decomposition into orthogonal components, the projection onto the column space, and the orthogonality condition. It matches the gold answer in both content and reasoning."}, "llm_echoed_qid": "textbook-118-11-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes the geometric interpretation of the regression model, including the decomposition into orthogonal components, the projection onto the column space, and the orthogonality condition. It matches the gold answer in both content and reasoning."}
{"script_processing_qid": "textbook-108-6-3-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the long-run aggregate-supply curve is consistent with monetary neutrality.\n\nGOLD_ANSWER:\nMonetary neutrality implies that changes in the money supply affect only nominal variables (like $P$) and not real variables (like $Y$). Since the long-run aggregate-supply curve is vertical, changes in $P$ do not affect $Y$, consistent with monetary neutrality.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that monetary neutrality implies changes in the money supply only affect nominal variables (e.g., prices, wages) and leave real variables (e.g., output, employment) unchanged in the long run.\nStep 2: The classical dichotomy separates real variables (determined by technology, resources, etc.) from nominal variables (determined by monetary policy).\nStep 3: The long-run aggregate-supply curve is vertical because real GDP depends only on real factors (labor, capital, technology) and not on the price level.\nStep 4: Since monetary policy can only influence nominal variables like the price level in the long run (consistent with monetary neutrality), shifts in aggregate demand (driven by money supply changes) only alter prices, not real output. This aligns the vertical LRAS with monetary neutrality.\n\nQID: textbook-108-6-3-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-108-6-3-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the concept of monetary neutrality and its alignment with the vertical long-run aggregate-supply curve, matching the gold answer's reasoning and conclusion.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-6-3-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the concept of monetary neutrality and its alignment with the vertical long-run aggregate-supply curve, matching the gold answer's reasoning and conclusion."}, "llm_echoed_qid": "textbook-108-6-3-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the concept of monetary neutrality and its alignment with the vertical long-run aggregate-supply curve, matching the gold answer's reasoning and conclusion."}
{"script_processing_qid": "textbook-114-4-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Evaluate the effectiveness of rate-of-return regulation in controlling monopoly power.\n\nGOLD_ANSWER:\n1. **Mechanism**: Regulators set prices based on a fair rate of return on invested capital.\n2. **Criticism**: May lead to inefficiencies (e.g., over-investment in capital to increase allowed profits).\n3. **Effectiveness**: Limited by regulatory lag and information asymmetry.\n\nCANDIDATE_ANSWER:\nStep 1: The provided context explicitly mentions rate-of-return regulation (page 344) as a method to address monopoly power. However, no further details about its effectiveness are included in the extracted passages.\nStep 2: While the section discusses social costs of monopoly power and price regulation (pages 341-344), the context does not specify outcomes, limitations, or empirical evidence related to rate-of--return regulation.\nStep 3: Key factors for evaluating effectiveness (e.g., whether it reduces deadweight loss, impacts prices, or incentivizes efficiency) are absent from the provided material.\nConclusion: The context lacks sufficient information to evaluate the effectiveness of rate-of-return regulation in controlling monopoly power beyond noting its existence as a regulatory tool.\n\nQID: textbook-114-4-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-4-1-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly identifies the lack of sufficient information.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-4-1-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly identifies the lack of sufficient information."}, "llm_echoed_qid": "textbook-114-4-1-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly identifies the lack of sufficient information."}
{"script_processing_qid": "textbook-123-23-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the economic interpretation of the quadratic approximation in the context of the stochastic growth model.\n\nGOLD_ANSWER:\nThe quadratic approximation simplifies the nonlinear utility function $\\ln c_{t}$ into a quadratic form, making the problem tractable for linear-quadratic methods. This allows for the application of the optimal linear regulator framework, which is computationally efficient and provides closed-form solutions.\n\nCANDIDATE_ANSWER:\nStep 1: The quadratic approximation replaces the original nonlinear function $r(z_t)$ with a second-order Taylor expansion around a steady state $\bar{z}$. This transforms the problem into a linear-quadratic framework, which preserves key curvature properties of the original model.\n\nStep 2: Economically, the quadratic term $\\frac{1}{2}(z-\bar{z})'\\frac{\\partial^2 r}{\\partial z\\partial z'}(z-\bar{z})$ represents the local curvature of returns/costs around the steady state. It captures:\n- Risk sensitivity: How welfare responds to variance in state/control variables\n- Concavity effects: Diminishing returns to inputs (e.g., capital in growth models)\n- Adjustment costs: Implicit costs of deviating from equilibrium\n\nStep 3: In stochastic growth models, this approximation allows policymakers/agents to make tractable decisions that account for both expected future outcomes (via the linear terms) and uncertainty/variability (via the quadratic terms), while maintaining the structure needed for Riccati equation solutions.\n\nQID: textbook-123-23-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-23-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the economic interpretation of the quadratic approximation, including its transformation into a linear-quadratic framework and its implications for risk sensitivity, concavity effects, and adjustment costs, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-23-1-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the economic interpretation of the quadratic approximation, including its transformation into a linear-quadratic framework and its implications for risk sensitivity, concavity effects, and adjustment costs, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-123-23-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the economic interpretation of the quadratic approximation, including its transformation into a linear-quadratic framework and its implications for risk sensitivity, concavity effects, and adjustment costs, aligning well with the gold answer."}
{"script_processing_qid": "textbook-113-8-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the mathematical condition for a firm's long-run exit decision starting from TR < TC. Explain each step.\n\nGOLD_ANSWER:\n1.  **Starting Point**: TR < TC.\n2.  **Divide by Q**: TR/Q < TC/Q.\n3.  **Definitions**: TR/Q = Price (P), TC/Q = Average Total Cost (ATC).\n4.  **Final Condition**: P < ATC.\n5.  **Interpretation**: The firm exits if the price per unit is less than the average total cost per unit, as it cannot cover its costs.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the firm's exit condition in total terms: Total Revenue (TR) < Total Cost (TC). This means the firm is incurring losses.\n\nStep 2: Divide both sides of the inequality by quantity (Q), assuming Q > 0 in the long run. This preserves the inequality direction:\nTR/Q < TC/Q\n\nStep 3: Recognize that TR/Q equals price (P) since Total Revenue = Price × Quantity (TR = P × Q). Thus, TR/Q = P.\n\nStep 4: Recognize that TC/Q equals Average Total Cost (ATC) by definition. Thus, TC/Q = ATC.\n\nStep 5: Substitute these results into the inequality:\nP < ATC\n\nFinal Condition: A firm will exit the market in the long run if the price (P) is less than its Average Total Cost (ATC).\n\nQID: textbook-113-8-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-113-8-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the mathematical condition for a firm's long-run exit decision starting from TR < TC, and each step is accurately explained, matching the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-113-8-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the mathematical condition for a firm's long-run exit decision starting from TR < TC, and each step is accurately explained, matching the gold answer."}, "llm_echoed_qid": "textbook-113-8-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the mathematical condition for a firm's long-run exit decision starting from TR < TC, and each step is accurately explained, matching the gold answer."}
{"script_processing_qid": "textbook-114-32-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare and contrast the long-run supply curves for constant-cost and increasing-cost industries. What causes the differences in their shapes?\n\nGOLD_ANSWER:\n1. **Constant-Cost Industry**: The long-run supply curve is horizontal because input prices remain unchanged as the industry expands. Output can increase without affecting per-unit costs.\n2. **Increasing-Cost Industry**: The long-run supply curve is upward-sloping because input prices rise as the industry expands. Higher output leads to higher per-unit costs due to increased demand for inputs.\n3. **Cause of Differences**: The key difference is the behavior of input prices. In constant-cost industries, input prices are unaffected by industry expansion, while in increasing-cost industries, input prices rise with expansion.\n\nCANDIDATE_ANSWER:\nStep 1: **Constant-Cost Industry Long-Run Supply Curve**\n- The long-run supply curve is horizontal (perfectly elastic).\n- Input prices remain unchanged as industry output expands (e.g., inputs are not scarce or industry demand for inputs is negligible relative to the broader market).\n- Firms enter/exit without affecting input costs, maintaining a constant long-run equilibrium price.\n\nStep 2: **Increasing-Cost Industry Long-Run Supply Curve**\n- The long-run supply curve is upward-sloping.\n- Input prices rise as industry output expands (e.g., scarce inputs or increased competition for specialized resources).\n- Higher prices are required to cover elevated input costs for new entrants, leading to an upward-sloping curve.\n\nStep 3: **Key Difference in Shapes**\n- Constant-cost industries face no input price changes, resulting in a horizontal curve.\n- Increasing-cost industries experience rising input prices due to resource scarcity or competition, causing the upward slope.\n\nStep 4: **Underlying Cause**\n- The difference stems from how industry expansion affects input markets. Constant-cost industries do not alter input prices, while increasing-cost industries bid up input prices as they grow, altering production costs for all firms.\n\nQID: textbook-114-32-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-32-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes the long-run supply curves for both constant-cost and increasing-cost industries, including the reasons for their shapes and the underlying causes of the differences, matching the gold answer in all key aspects.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-32-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately describes the long-run supply curves for both constant-cost and increasing-cost industries, including the reasons for their shapes and the underlying causes of the differences, matching the gold answer in all key aspects."}, "llm_echoed_qid": "textbook-114-32-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes the long-run supply curves for both constant-cost and increasing-cost industries, including the reasons for their shapes and the underlying causes of the differences, matching the gold answer in all key aspects."}
{"script_processing_qid": "textbook-118-16-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the conditions under which a square matrix $\\pmb{A}$ is invertible, and relate this to the concept of rank.\n\nGOLD_ANSWER:\nA square matrix $\\pmb{A}$ is invertible if and only if it has full rank, meaning its rank equals its dimension. The rank is the maximum number of linearly independent rows or columns. If $\\pmb{A}$ is singular (not invertible), its rank is less than its dimension, and it has linearly dependent rows or columns.\n\nCANDIDATE_ANSWER:\nStep 1: A square matrix $\\pmb{A}$ (of size $n \times n$) is invertible if there exists a matrix $\\pmb{B}$ such that $\\pmb{A}\\pmb{B} = \\pmb{B}\\pmb{A} = \\pmb{I}$, where $\\pmb{I}$ is the identity matrix.\n\nStep 2: The rank of a matrix is the maximum number of linearly independent rows (or columns). For invertibility, $\\pmb{A}$ must have full rank, meaning its rank equals $n$ (the number of rows/columns).\n\nStep 3: If $\text{rank}(\\pmb{A}) < n$, the matrix is singular (non-invertible) because linear dependence among rows/columns prevents the existence of a unique inverse.\n\nConclusion: A square matrix $\\pmb{A}$ is invertible if and only if $\text{rank}(\\pmb{A}) = n$. This full-rank condition ensures the system of equations represented by $\\pmb{A}$ has a unique solution, analogous to regression models requiring full-rank design matrices for identifiability.\n\nQID: textbook-118-16-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-16-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes the conditions for a square matrix to be invertible, linking it to the concept of rank and full rank. The explanation aligns perfectly with the gold answer, including the definition of invertibility, the importance of full rank, and the consequences of not having full rank.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-16-1-3", "category": "CORRECT", "explanation": "The candidate's answer accurately describes the conditions for a square matrix to be invertible, linking it to the concept of rank and full rank. The explanation aligns perfectly with the gold answer, including the definition of invertibility, the importance of full rank, and the consequences of not having full rank."}, "llm_echoed_qid": "textbook-118-16-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes the conditions for a square matrix to be invertible, linking it to the concept of rank and full rank. The explanation aligns perfectly with the gold answer, including the definition of invertibility, the importance of full rank, and the consequences of not having full rank."}
{"script_processing_qid": "textbook-122-24-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the expression for $\\hat{\\beta}_1$ in terms of the population parameters and errors, and show that $E(\\hat{\\beta}_1) = \\beta_1$ under Assumptions SLR.1-SLR.4.\n\nGOLD_ANSWER:\n1. **Expression for $\\hat{\\beta}_1$**: $$\\hat{\\beta}_1 = \\beta_1 + \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) u_i}{s_x^2}.$$\n2. **Expectation**: $$E(\\hat{\\beta}_1) = \\beta_1 + E\\left[\\frac{\\sum_{i=1}^n (x_i - \\bar{x}) u_i}{s_x^2}\\right] = \\beta_1 + \\frac{\\sum_{i=1}^n (x_i - \\bar{x}) E(u_i)}{s_x^2} = \\beta_1$$ (since $E(u_i|x_i) = 0$).\n\nCANDIDATE_ANSWER:\nStep 1: Start with the population model y_i = β₀ + β₁x_i + u_i\nStep 2: Write the OLS estimator formula: \n    β̂₁ = [Σ(x_i - x̄)(y_i - ȳ)] / [Σ(x_i - x̄)^2]\nStep 3: Substitute y_i from population model into numerator:\n    = [Σ(x_i - x̄)(β₁(x_i - x̄) + (u_i - ū))] / [Σ(x_i - x̄)^2]\nStep 4: Expand and simplify numerator:\n    = [β₁Σ(x_i - x̄)^2 + Σ(x_i - x̄)u_i] / [Σ(x_i - x̄)^2]\nStep 5: Decompose into β₁ + [Σ(x_i - x̄)u_i]/[Σ(x_i - x̄)^2]\nStep 6: Take expectation E[β̂₁] = β₁ + E[[Σ(x_i - x̄)u_i]/[Σ(x_i - x̄)^2]]\nStep 7: Since SST_x = Σ(x_i - x̄)^2 is non-random conditional on x (SLR.4):\n    E[β̂₁] = β₁ + [1/SST_x]Σ(x_i - x̄)E[u_i|x]\nStep 8: By SLR.3 (E[u_i|x] = 0), the second term vanishes:\n    E[β̂₁] = β₁ + 0 = β₁\n\nQID: textbook-122-24-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-24-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the expression for $\\hat{\\beta}_1$ and shows that $E(\\hat{\\beta}_1) = \\beta_1$ under Assumptions SLR.1-SLR.4. The steps are logically consistent and align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-24-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the expression for $\\hat{\beta}_1$ and shows that $E(\\hat{\beta}_1) = \beta_1$ under Assumptions SLR.1-SLR.4. The steps are logically consistent and align with the gold answer."}, "llm_echoed_qid": "textbook-122-24-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the expression for $\\hat{\beta}_1$ and shows that $E(\\hat{\beta}_1) = \beta_1$ under Assumptions SLR.1-SLR.4. The steps are logically consistent and align with the gold answer."}
{"script_processing_qid": "textbook-103-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Show that the reservation wage $\\bar{w}$ is independent of search effort $e$. Use the equation $$\\bar{w}-T(\\bar{w})=[1-\\beta(1-\\alpha)](1+z+\\beta Q)-\\beta\\alpha Q.$$\n\nGOLD_ANSWER:\n1.  **Equation**: $\\bar{w} - T(\\bar{w}) = [1 - \\beta(1-\\alpha)](1 + z + \\beta Q) - \\beta\\alpha Q$.\n2.  **Dependence on $Q$**: $Q$ is determined endogenously and does not depend on $e$.\n3.  **Conclusion**: Since $Q$ is independent of $e$, $\\bar{w}$ is also independent of $e$.\n\nCANDIDATE_ANSWER:\nStep 1: Examine the reservation wage equation provided:\n$$\n\\bar{w}-T(\\bar{w})=[1-\\beta(1-\\alpha)](1+z+\\beta Q)-\\beta\\alpha Q\n$$\n\nStep 2: Observe that the equation contains no terms related to search effort $e$. The variables on the right-hand side ($\\beta$, $\\alpha$, $z$, and $Q$) are either parameters (exogenous values) or endogenous variables independent of $e$:\n- $\\beta$ = discount factor\n- $\\alpha$ = job separation rate\n- $z$ = unemployment benefit\n- $Q$ = expected value of being unemployed\n\nStep 3: Note that $Q$ implicitly depends on optimal search effort $e$ through the worker's maximization process (as shown in the optimal effort condition). However, the reservation wage equation explicitly solves for $\\bar{w}$ in terms of parameters and $Q$ without direct reference to $e$.\n\nStep 4: Conclude that since $e$ does not appear in the structural equation defining $\\bar{w}$, the reservation wage is determined independently of search effort. Optimal effort $e$ affects the probability of receiving offers (via $\\pi(e)$) but does not alter the threshold $\\bar{w}$ at which offers are accepted.\n\nQID: textbook-103-0-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-103-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly identifies that the reservation wage equation does not include search effort $e$ and concludes that $\\bar{w}$ is independent of $e$. The reasoning aligns with the gold answer, which also emphasizes the independence of $Q$ from $e$.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-0-0-1", "category": "CORRECT", "explanation": "The candidate correctly identifies that the reservation wage equation does not include search effort $e$ and concludes that $\bar{w}$ is independent of $e$. The reasoning aligns with the gold answer, which also emphasizes the independence of $Q$ from $e$."}, "llm_echoed_qid": "textbook-103-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly identifies that the reservation wage equation does not include search effort $e$ and concludes that $\bar{w}$ is independent of $e$. The reasoning aligns with the gold answer, which also emphasizes the independence of $Q$ from $e$."}
{"script_processing_qid": "textbook-61-0-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the equilibrium expression for GNP ($Y_i$) in Haavelmo's model, starting from the consumption function and GNP identity.\n\nGOLD_ANSWER:\n1. **Substitute Consumption into GNP Identity**:\n   $$Y_i = C_i + I_i = (\\alpha_0 + \\alpha_1 Y_i + u_i) + I_i.$$\n2. **Solve for $Y_i$**:\n   $$Y_i - \\alpha_1 Y_i = \\alpha_0 + I_i + u_i \\implies Y_i = \\frac{\\alpha_0}{1-\\alpha_1} + \\frac{I_i}{1-\\alpha_1} + \\frac{u_i}{1-\\alpha_1}.$$\n\nCANDIDATE_ANSWER:\nStep 1: Start with the GNP identity: \\( Y_i = C_i + I_i \\).\nStep 2: Substitute the consumption function \\( C_i = \\alpha_0 + \\alpha_1 Y_i + u_i \\) into the GNP identity:\n\\( Y_i = (\\alpha_0 + \\alpha_1 Y_i + u_i) + I_i \\).\nStep 3: Rearrange to isolate \\( Y_i \\) terms:\n\\( Y_i - \\alpha_1 Y_i = \\alpha_0 + I_i + u_i \\).\nStep 4: Factor \\( Y_i \\) on the left-hand side:\n\\( Y_i (1 - \\alpha_1) = \\alpha_0 + I_i + u_i \\).\nStep 5: Solve for \\( Y_i \\) by dividing both sides by \\( (1 - \\alpha_1) \\):\n\\( Y_i = \\frac{\\alpha_0}{1 - \\alpha_1} + \\frac{I_i}{1 - \\alpha_1} + \\frac{u_i}{1 - \\alpha_1} \\).\nThis matches the equilibrium expression provided in the context.\n\nQID: textbook-61-0-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-61-0-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows each step of the derivation, matches the GOLD_ANSWER exactly, and provides a clear and logical progression from the consumption function to the final equilibrium expression for GNP.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-61-0-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows each step of the derivation, matches the GOLD_ANSWER exactly, and provides a clear and logical progression from the consumption function to the final equilibrium expression for GNP."}, "llm_echoed_qid": "textbook-61-0-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows each step of the derivation, matches the GOLD_ANSWER exactly, and provides a clear and logical progression from the consumption function to the final equilibrium expression for GNP."}
{"script_processing_qid": "textbook-108-17-3-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the limitations of using fiscal policy to combat a liquidity trap, as seen in Japan's 1990s recession.\n\nGOLD_ANSWER:\n**Limitations**:\n1.  **Zero Lower Bound**: Interest rates near zero limit monetary policy's ability to offset crowding out.\n2.  **High Debt Levels**: Fiscal stimulus may raise concerns about sustainability, reducing confidence.\n3.  **Deflationary Expectations**: Households may hoard cash if they expect falling prices, weakening the multiplier.\n4.  **Structural Rigidities**: Fiscal policy cannot address labor market or banking sector inefficiencies.\n\nCANDIDATE_ANSWER:\nStep 1: Diminishing returns from infrastructure spending. Japan focused on 'brick-and-mortar projects,' but excessive public works likely faced low marginal productivity as existing infrastructure was already sufficient, reducing fiscal multiplier effects.\n\nStep 2: Temporary vs. structural impact. Fiscal injections created a 'temporary comeback' by boosting short-term demand but failed to address underlying issues (e.g., banking sector fragility, deflationary expectations), preventing a 'self-sustaining recovery.'\n\nStep 3: Debt sustainability concerns. With \\$830B spent in 12 months alone, fiscal policy increased public debt without generating lasting growth, raising long-term solvency risks and crowding out future policy options.\n\nStep 4: Liquidity trap constraints. Near-zero interest rates limited monetary policy's ability to complement fiscal stimulus, while private sector deleveraging offset government spending effects ('money sloshing around' without productive investment).\n\nQID: textbook-108-17-3-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-108-17-3-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer effectively addresses the limitations of fiscal policy in Japan's 1990s recession, aligning with the gold answer. It covers key points such as diminishing returns from infrastructure spending, temporary vs. structural impact, debt sustainability concerns, and liquidity trap constraints, all of which are consistent with the gold answer's focus on zero lower bound, high debt levels, deflationary expectations, and structural rigidities.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-17-3-3", "category": "CORRECT", "explanation": "The candidate's answer effectively addresses the limitations of fiscal policy in Japan's 1990s recession, aligning with the gold answer. It covers key points such as diminishing returns from infrastructure spending, temporary vs. structural impact, debt sustainability concerns, and liquidity trap constraints, all of which are consistent with the gold answer's focus on zero lower bound, high debt levels, deflationary expectations, and structural rigidities."}, "llm_echoed_qid": "textbook-108-17-3-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer effectively addresses the limitations of fiscal policy in Japan's 1990s recession, aligning with the gold answer. It covers key points such as diminishing returns from infrastructure spending, temporary vs. structural impact, debt sustainability concerns, and liquidity trap constraints, all of which are consistent with the gold answer's focus on zero lower bound, high debt levels, deflationary expectations, and structural rigidities."}
{"script_processing_qid": "textbook-119-9-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of data mining for the nominal versus true level of significance in hypothesis testing.\n\nGOLD_ANSWER:\n1. **Data Mining**: Selecting regressors based on statistical significance from a larger set of candidates.\n2. **True Level of Significance**: $$\\alpha^{\\ast}\\approx(c/k)\\alpha$$\n   - $c$: Number of candidate regressors.\n   - $k$: Number of selected regressors.\n   - $\\alpha$: Nominal significance level.\n3. **Implications**: The true level of significance ($\\alpha^{\\ast}$) can be much higher than the nominal level, leading to inflated Type I error rates.\n\nCANDIDATE_ANSWER:\nStep 1: Data mining refers to the practice of extensively testing multiple hypotheses or specifications until statistically significant results are found.\nStep 2: The nominal significance level (e.g., α=0.05) represents the declared probability of Type I error for a single test. However, data mining inflates the true significance level across multiple tests due to increased family-wise error rates.\nStep 3: The context shows specification tests like Ramsey's RESET and LM test require repeated hypothesis testing during model refinement. Each additional test compounds the probability of falsely rejecting a true null hypothesis.\nStep 4: This creates divergence between the nominal level (claimed α for individual tests) and true level (actual probability of Type I errors across all specification searches). The true level becomes substantially higher than the nominal level when multiple models/tests are used sequentially.\nStep 5: The implication is that reported p-values and confidence intervals based on the final selected model become unreliable, as they do not account for the data mining process that influenced model specification.\n\nQID: textbook-119-9-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-9-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes the implications of data mining on the nominal versus true level of significance in hypothesis testing. It correctly identifies that data mining inflates the true significance level due to increased family-wise error rates, leading to unreliable p-values and confidence intervals. This aligns with the gold answer's explanation of the issue.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-9-1-3", "category": "CORRECT", "explanation": "The candidate's answer accurately describes the implications of data mining on the nominal versus true level of significance in hypothesis testing. It correctly identifies that data mining inflates the true significance level due to increased family-wise error rates, leading to unreliable p-values and confidence intervals. This aligns with the gold answer's explanation of the issue."}, "llm_echoed_qid": "textbook-119-9-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes the implications of data mining on the nominal versus true level of significance in hypothesis testing. It correctly identifies that data mining inflates the true significance level due to increased family-wise error rates, leading to unreliable p-values and confidence intervals. This aligns with the gold answer's explanation of the issue."}
{"script_processing_qid": "textbook-118-17-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the asymptotic distribution of the $t$ statistic $t_{\\beta_2}$ under the null hypothesis $\\beta_2 = 0$, assuming exogenous regressors. Show all steps, including the application of a CLT.\n\nGOLD_ANSWER:\n1. **Rewrite $t_{\\beta_2}$**: $$t_{\\beta_2} = \\left(\\frac{y^{\\top} M_X y}{n - k}\\right)^{-1/2} \\frac{n^{-1/2} x_2^{\\top} M_1 y}{(n^{-1} x_2^{\\top} M_1 x_2)^{1/2}}.$$\n2. **Consistency of $s^2$**: Under the null, $s^2 \\to \\sigma_0^2$ by LLN.\n3. **Numerator**: With $M_1 y = M_1 u$, the numerator becomes $$n^{-1/2} x_2^{\\top} M_1 u.$$\n4. **Variance**: Conditional on $X$, $\\operatorname{Var}(x_2^{\\top} M_1 u \\mid X) = \\sigma_0^2 x_2^{\\top} M_1 x_2$.\n5. **CLT**: The numerator is asymptotically $N(0, \\sigma_0^2 x_2^{\\top} M_1 x_2)$.\n6. **Denominator**: $(n^{-1} x_2^{\\top} M_1 x_2)^{1/2} \\to (S_{22} - S_{21} S_{11}^{-1} S_{12})^{1/2}$.\n7. **Final form**: $$t_{\\beta_2} \\stackrel{a}{\\sim} N(0,1).$$\n\nCANDIDATE_ANSWER:\nStep 1: Start with the given t-statistic formula under exogenous regressors:\n\nt_{\\beta_2} = \\frac{n^{-1/2} x_2^{\\top} M_1 y}{\\sqrt{s^2 \\cdot n^{-1} x_2^{\\top} M_1 x_2}}\n\nwhere s^2 = \\frac{y^{\\top} M_X y}{n - k} \\xrightarrow{p} \\sigma_0^2 under the null.\n\nStep 2: Substitute y = X\\beta_0 + u under H_0: \\beta_2 = 0. Since M_1 annihilates X_1 (other regressors), we have M_1 y = M_1 u.\n\nStep 3: Numerator becomes n^{-1/2} x_2^{\\top} M_1 u. Under E(u|X) = 0 and E(uu^{\\top}|X) = \\sigma_0^2 I:\n- E(x_2^{\\top} M_1 u/n^{1/2}|X) = 0\n- Var(x_2^{\\top} M_1 u/n^{1/2}|X) = \\sigma_0^2(n^{-1}x_2^{\\top}M_1x_2)\n\nStep 4: Apply Lindeberg-Lévy CLT to n^{-1/2}x_2^{\\top}M_1u. As n → ∞:\n\\(\\frac{n^{-1/2}x_2^{\\top}M_1u}{\\sqrt{\\sigma_0^2 n^{-1}x_2^{\\top}M_1x_2}} \\xrightarrow{d} N(0,1)\\)\n\nStep 5: For denominator components:\n- plim s^2 = \\sigma_0^2\n- plim n^{-1}x_2^{\\top}M_1x_2 = Q > 0 (positive definite)\n\nStep 6: Apply Slutsky's theorem to combine results:\n\\(t_{\\beta_2} = \\frac{N(0,\\sigma_0^2 Q)}{\\sqrt{\\sigma_0^2 Q}} + o_p(1) \\xrightarrow{d} N(0,1)\\)\n\nThus, t_{\\beta_2} \\xrightarrow{d} N(0,1) asymptotically under H_0.\n\nQID: textbook-118-17-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-17-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the asymptotic distribution of the t-statistic under the null hypothesis, showing all necessary steps including the application of the CLT and Slutsky's theorem, matching the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-17-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the asymptotic distribution of the t-statistic under the null hypothesis, showing all necessary steps including the application of the CLT and Slutsky's theorem, matching the gold answer."}, "llm_echoed_qid": "textbook-118-17-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the asymptotic distribution of the t-statistic under the null hypothesis, showing all necessary steps including the application of the CLT and Slutsky's theorem, matching the gold answer."}
{"script_processing_qid": "textbook-3-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the conditions under which a stochastic choice function is rationalizable.\n\nGOLD_ANSWER:\n1. **Rationalizability**: A stochastic choice function is rationalizable if there exists a probability distribution over preference relations such that the choice probabilities match the given function.\n2. **Necessary Condition**: For any subset of alternatives, the probability of choosing an alternative must be consistent with the probabilities implied by the preference relations.\n3. **Example**: In the case of three alternatives (x, y, z), the probability that at least one of x > y, y > z, or z > x holds must be consistent with the given choice probabilities.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain sufficient information to derive conditions for rationalizability of stochastic choice functions. The given passages focus on deterministic preference relations (completeness, transitivity) and utility representation, but do not address stochastic choice behavior or the specific axioms required for stochastic rationalizability (e.g., Luce's choice axiom, Regularity, or Stochastic WARP). The context mentions WARP briefly but only in relation to deterministic choice behavior, with no discussion of probabilistic choice structures.\n\nQID: textbook-3-0-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-3-0-0-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain sufficient information'.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-3-0-0-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain sufficient information'."}, "llm_echoed_qid": "textbook-3-0-0-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain sufficient information'."}
{"script_processing_qid": "textbook-114-12-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the profit-maximizing condition $MR = MC$ holds for a monopolist. Use the concept of slopes of total revenue and total cost curves in your explanation.\n\nGOLD_ANSWER:\nThe profit-maximizing condition $MR = MC$ ensures that the firm is producing at the quantity where the additional revenue from selling one more unit (MR) equals the additional cost of producing that unit (MC). At this point, the slope of the total revenue curve (MR) equals the slope of the total cost curve (MC), meaning any deviation from this quantity would either reduce profit or leave it unchanged. This is because producing less would miss out on profitable units, while producing more would incur costs exceeding revenues.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that profit (π) is maximized where the slope of the total revenue (TR) curve equals the slope of the total cost (TC) curve.\\nStep 2: The slope of TR is marginal revenue (MR = dTR/dQ), and the slope of TC is marginal cost (MC = dTC/dQ). For a monopolist: TR = P(Q) × Q = (40 - Q)Q = 40Q - Q² ⇒ MR = 40 - 2Q. TC = 50 + Q² ⇒ MC = 2Q.\\nStep 3: When MR > MC (slope of TR > slope of TC), increasing production raises profit. When MR < MC, reducing production increases profit.\\nStep 4: Profit can only be maximized where these slopes are equal (MR = MC). Solving 40 - 2Q = 2Q gives Q = 10, confirming the intersection point where the curves' slopes match. At this quantity, no additional profit can be gained by producing more or less.\n\nQID: textbook-114-12-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-12-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly explains the profit-maximizing condition $MR = MC$ using the slopes of total revenue and total cost curves, and provides a detailed mathematical derivation to confirm the intersection point where the slopes match.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-12-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly explains the profit-maximizing condition $MR = MC$ using the slopes of total revenue and total cost curves, and provides a detailed mathematical derivation to confirm the intersection point where the slopes match."}, "llm_echoed_qid": "textbook-114-12-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly explains the profit-maximizing condition $MR = MC$ using the slopes of total revenue and total cost curves, and provides a detailed mathematical derivation to confirm the intersection point where the slopes match."}
{"script_processing_qid": "textbook-31-0-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Prove that if an allocation $(x,y)$ Pareto dominates a price equilibrium with transfers $(x^*,y^*)$, then $\\sum_i p \\cdot x_i > \\sum_i w_i$.\n\nGOLD_ANSWER:\n1. **Pareto Dominance**: $u_i(x_i) \\geq u_i(x_i^*)$ for all $i$ and $u_j(x_j) > u_j(x_j^*)$ for some $j$.\n2. **Local Nonsatiation**: For non-satiated consumers, $p \\cdot x_i \\geq w_i$ and $p \\cdot x_j > w_j$ if $x_j \\succ x_j^*$.\n3. **Summation**: $\\sum_i p \\cdot x_i > \\sum_i w_i$, implying $(x,y)$ is not feasible.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that in a price equilibrium with transfers (x*, y*), each agent's allocation satisfies p·x_i* = w_i (budget balanced).\nStep 2: Under Pareto dominance, all agents weakly prefer x_i to x_i*, with at least one agent strictly preferring x_i. By local nonsatiation (for non-satiation points):\n   - For agents with x_i ≠ x_i*: p·x_i ≥ p·x_i* = w_i\n   - For agents with x_i ≻ x_i*: p·x_i > w_i (strict inequality)\nStep 3: Sum across all agents: ∑p·x_i ≥ ∑w_i + [p·x_j - w_j] for at least one j where x_j ≻ x_j*. Since p·x_j > w_j for this agent, ∑p·x_i > ∑w_i.\nStep 4: Conclusion: Pareto dominance implies aggregate expenditure exceeds initial endowments, making (x,y) infeasible.\n\nQID: textbook-31-0-2-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-31-0-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the logical steps to prove that if an allocation Pareto dominates a price equilibrium with transfers, then the aggregate expenditure exceeds the initial endowments. The reasoning aligns with the gold answer, including the use of local nonsatiation and the summation across agents to reach the conclusion.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-31-0-2-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the logical steps to prove that if an allocation Pareto dominates a price equilibrium with transfers, then the aggregate expenditure exceeds the initial endowments. The reasoning aligns with the gold answer, including the use of local nonsatiation and the summation across agents to reach the conclusion."}, "llm_echoed_qid": "textbook-31-0-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the logical steps to prove that if an allocation Pareto dominates a price equilibrium with transfers, then the aggregate expenditure exceeds the initial endowments. The reasoning aligns with the gold answer, including the use of local nonsatiation and the summation across agents to reach the conclusion."}
{"script_processing_qid": "textbook-107-15-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the long-run effects of the $2 per unit tax on firm entry/exit and industry supply. How does the supply curve shift?\n\nGOLD_ANSWER:\n1. **Firm Exit**: Higher costs may drive some firms out, reducing industry supply.\n2. **Supply Shift**: The long-run supply curve shifts left, increasing equilibrium price and reducing quantity.\n3. **Final Outcome**: The tax burden is shared between consumers and producers, with deadweight loss persisting.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the industry structure. The context implies a competitive industry with constant returns to scale (horizontal long-run supply curve). However, the provided production function F(x,y) = x^(3/4)y^(3/4) exhibits increasing returns to scale (sum of exponents = 1.5 > 1), which contradicts long-run competitiveness. This creates ambiguity.\n\nStep 2: Assume the tax is applied to the hypothetical constant-returns industry described in the first context sentence. In such cases:\n- Firms initially pass the full $2 tax to consumers in the short run (price rises by $2)\n\nStep 3: Analyze long-run effects:\n1. The tax increases firms' effective production costs by $2/unit\n2. With constant returns, the horizontal long-run supply curve must shift upward by the full tax amount ($2) to maintain zero economic profit\n3. This vertical shift causes equilibrium price to permanently increase by $2\n4. At the new higher price, demand decreases, leading to reduced industry output\n5. Fewer firms remain in the industry (exit occurs) to match the lower equilibrium quantity\n\nStep 4: Resolve the production function contradiction:\n- The F(x,y) = x^(3/4)y^(3/4) example demonstrates an industry that cannot sustain perfect competition in the long run due to increasing returns\n- However, the tax analysis question focuses on the constant-returns scenario described in the first context sentence\n\nFinal Conclusion: In a constant-returns competitive industry, the $2 tax causes the long-run supply curve to shift vertically upward by $2. Firms exit until the market clears at the new price (original price + $2), with reduced industry output.\n\nQID: textbook-107-15-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-15-1-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the long-run effects of the tax (supply curve shift upward, firm exit, and price increase), but the reasoning includes unnecessary details about industry structure and production functions that are not relevant to the gold answer's focus on the tax's impact in a competitive market.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-15-1-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the long-run effects of the tax (supply curve shift upward, firm exit, and price increase), but the reasoning includes unnecessary details about industry structure and production functions that are not relevant to the gold answer's focus on the tax's impact in a competitive market."}, "llm_echoed_qid": "textbook-107-15-1-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the long-run effects of the tax (supply curve shift upward, firm exit, and price increase), but the reasoning includes unnecessary details about industry structure and production functions that are not relevant to the gold answer's focus on the tax's impact in a competitive market."}
{"script_processing_qid": "textbook-102-2-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Using the probit model results, calculate the number of trucks required to achieve a 95% participation rate in the recycling program.\n\nGOLD_ANSWER:\n1. **Probit equation**: $\\Phi^{-1}(P_i) = -2.3116 + 0.010646T$.\\n2. **Target $z_i$**: $\\Phi^{-1}(0.95) = 1.645$.\\n3. **Solve for $T$**: $1.645 = -2.3116 + 0.010646T \\implies T = (1.645 + 2.3116)/0.010646 = 372$ trucks.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the target probability (P_i = 0.95) and calculate its inverse normal value. Φ⁻¹(0.95) ≈ 1.645 from standard normal tables.\n\nStep 2: Substitute into the probit equation: \n1.645 = -2.18098 + 0.0098898T\n\nStep 3: Solve for T:\nT = (1.645 + 2.18098) / 0.0098898 ≈ 3.82598 / 0.0098898 ≈ 386.86\n\nStep 4: Since trucks must be whole units, round up to 387 trucks to ensure at least 95% participation.\n\nQID: textbook-102-2-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-102-2-1-1\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate used incorrect coefficients in the probit equation (-2.18098 and 0.0098898 instead of -2.3116 and 0.010646), leading to an incorrect calculation of the number of trucks required.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-102-2-1-1", "category": "INCORRECT", "explanation": "The candidate used incorrect coefficients in the probit equation (-2.18098 and 0.0098898 instead of -2.3116 and 0.010646), leading to an incorrect calculation of the number of trucks required."}, "llm_echoed_qid": "textbook-102-2-1-1", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate used incorrect coefficients in the probit equation (-2.18098 and 0.0098898 instead of -2.3116 and 0.010646), leading to an incorrect calculation of the number of trucks required."}
{"script_processing_qid": "textbook-114-4-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Explain the 'lemons problem' in the used car market and its implications for market efficiency.\n\nGOLD_ANSWER:\n1. **Definition**: Sellers have more information about quality than buyers, leading to adverse selection (only low-quality cars are sold).\n2. **Implications**: Market failure as high-quality cars are driven out, reducing overall market efficiency.\n\nCANDIDATE_ANSWER:\nStep 1: The 'lemons problem' arises in the used car market due to asymmetric information. Sellers know the true quality of their cars (whether they are reliable 'peaches' or defective 'lemons'), while buyers cannot reliably discern quality differences.\\n\\nStep 2: This imbalance causes buyers to assume an average quality level and only offer a price reflecting that average. Sellers of high-quality cars (peaches) will reject this price as too low, while sellers of low-quality cars (lemons) will accept it.\\n\\nStep 3: Over time, high-quality cars disappear from the market (adverse selection), leaving only lemons. This reduces transaction volume and market efficiency.\\n\\nStep 4: The implication is market failure – prices no longer allocate resources efficiently, and mutually beneficial transactions (for quality cars) collapse. The context explicitly links this to 'market failure' under asymmetric information (§597).\\n\\nStep 5: The market becomes inefficient as it fails to properly signal quality differences, ultimately reducing social welfare and market liquidity.\n\nQID: textbook-114-4-2-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-4-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the 'lemons problem' and its implications for market efficiency, matching the gold answer's key points about asymmetric information, adverse selection, and market failure.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-4-2-0", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the 'lemons problem' and its implications for market efficiency, matching the gold answer's key points about asymmetric information, adverse selection, and market failure."}, "llm_echoed_qid": "textbook-114-4-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the 'lemons problem' and its implications for market efficiency, matching the gold answer's key points about asymmetric information, adverse selection, and market failure."}
{"script_processing_qid": "textbook-122-27-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Why is it incorrect to report a proportionate change of 0.125 as a percentage change of 0.125%?\n\nGOLD_ANSWER:\nA proportionate change of 0.125 corresponds to a 12.5% change when multiplied by 100. Reporting it as 0.125% understates the actual change by a factor of 100, leading to significant misinterpretation.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the distinction between proportions and percentages. A proportion (e.g., 0.125) becomes a percentage when multiplied by 100 (e.g., 0.125 × 100 = 12.5%).\nStep 2: Apply this rule to proportionate changes. The formula explicitly states: %Δx = 100(Δx/x₀).\nStep 3: Recognize that reporting 0.125 as 0.125% omits the required ×100 scaling factor. This incorrectly represents a 12.5% change as a 0.125% change, understating the magnitude by a factor of 100.\nConclusion: A proportionate change of 0.125 must be reported as 12.5%, not 0.125%, to maintain correct economic interpretation.\n\nQID: textbook-122-27-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-27-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains the distinction between proportions and percentages, applies the correct formula, and concludes that a proportionate change of 0.125 should be reported as 12.5%, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-27-1-3", "category": "CORRECT", "explanation": "The candidate answer correctly explains the distinction between proportions and percentages, applies the correct formula, and concludes that a proportionate change of 0.125 should be reported as 12.5%, aligning with the gold answer."}, "llm_echoed_qid": "textbook-122-27-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains the distinction between proportions and percentages, applies the correct formula, and concludes that a proportionate change of 0.125 should be reported as 12.5%, aligning with the gold answer."}
{"script_processing_qid": "textbook-115-23-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the advantages of the estimator based on $\\mathbf{A}(\\mathbf{x}_{i},\\hat{\\pmb{\\theta}})$.\n\nGOLD_ANSWER:\n1. **Advantages**: The estimator based on $\\mathbf{A}(\\mathbf{x}_{i},\\hat{\\pmb{\\theta}})$ has several attractive features: \n   - It often depends only on first derivatives of a conditional mean or conditional variance function. \n   - It is positive definite when it exists because of the conditional information matrix equality. \n   - It has been found to have significantly better finite sample properties than the outer product of the score estimator in some situations where $\\mathbf{A}(\\mathbf{x}_{i},\\pmb{\\theta}_{\\mathrm{o}})$ can be obtained in closed form.\n\nCANDIDATE_ANSWER:\nStep 1: The estimator based on $\\mathbf{A}(\\mathbf{x}_i, \\hat{\\pmb{\\theta}})$ calculates the expected negative Hessian conditional on $\\mathbf{x}_i$, leveraging model-implied expectations rather than sample-dependent quantities like $\\mathbf{H}_i(\\hat{\\pmb{\\theta}})$ or $\\mathbf{s}_i(\\hat{\\pmb{\\theta}})\\mathbf{s}_i(\\hat{\\pmb{\\theta}})'$.\\n\\nStep 2: Advantages include:\\n- **Efficiency**: It averages the conditional expectation $\\mathbf{A}(\\mathbf{x}_i, \\pmb{\\theta}_o)$, which is theoretically less variable than sample Hessians or outer products of scores, leading to more precise finite-sample estimates of $\\mathbf{A}_0$.\\n- **Computational Stability**: If the analytical form of $\\mathbf{A}(\\mathbf{x}_i, \\pmb{\\theta})$ is known, it avoids numerical computation of Hessians for each observation, reducing computational complexity.\\n- **Guaranteed Positive Definiteness**: The expected Hessian is more likely to be positive definite (ensuring invertibility) compared to sample Hessians, which may occasionally be non-invertible due to numerical issues.\n\nQID: textbook-115-23-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-23-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the advantages of the estimator based on $\\mathbf{A}(\\mathbf{x}_i, \\hat{\\pmb{\\theta}})$, including efficiency, computational stability, and guaranteed positive definiteness, which align with the gold answer's points about first derivatives, positive definiteness, and better finite sample properties.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-23-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the advantages of the estimator based on $\\mathbf{A}(\\mathbf{x}_i, \\hat{\\pmb{\theta}})$, including efficiency, computational stability, and guaranteed positive definiteness, which align with the gold answer's points about first derivatives, positive definiteness, and better finite sample properties."}, "llm_echoed_qid": "textbook-115-23-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the advantages of the estimator based on $\\mathbf{A}(\\mathbf{x}_i, \\hat{\\pmb{\theta}})$, including efficiency, computational stability, and guaranteed positive definiteness, which align with the gold answer's points about first derivatives, positive definiteness, and better finite sample properties."}
{"script_processing_qid": "textbook-84-2-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Prove that if $\\mathbf{A}$ and $\\mathbf{B}$ are $n\\times n$ diagonal matrices, then $\\mathbf{A}\\mathbf{B}$ is also diagonal with the $j^{th}$ diagonal element equal to $a_{j}b_{j}$.\n\nGOLD_ANSWER:\n3. **Diagonal Matrix Multiplication**:\n   - Let $\\mathbf{A}$ and $\\mathbf{B}$ be diagonal matrices with diagonal elements $a_{j}$ and $b_{j}$, respectively.\n   - The product $\\mathbf{A}\\mathbf{B}$ has elements $c_{ij} = \\sum_{k=1}^{n} a_{ik}b_{kj}$.\n   - For $i \\neq j$, $a_{ik} = 0$ or $b_{kj} = 0$ for all $k$, so $c_{ij} = 0$.\n   - For $i = j$, $c_{ii} = a_{ii}b_{ii} = a_{i}b_{i}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that diagonal matrices have non-zero entries only along their main diagonals. For an n×n diagonal matrix A, this means a_ij = 0 when i ≠ j, and similarly for B.<br>Step 2: Consider matrix multiplication AB. The (i,j)-th element is computed as (AB)_ij = Σ_{k=1}^n a_ik b_kj.<br>Step 3: When i ≠ j, for each term a_ik b_kj in the summation: either k ≠ i (making a_ik = 0) or k ≠ j (making b_kj = 0). All terms vanish, so (AB)_ij = 0.<br>Step 4: When i = j, the only non-zero term occurs at k = i = j. This gives (AB)_ii = a_ii b_ii = a_j b_j.<br>Step 5: Therefore, AB's diagonal elements are products of corresponding diagonal entries from A and B, with all off-diagonal entries being zero. This proves AB is diagonal with the stated property.\n\nQID: textbook-84-2-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-84-2-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to prove that the product of two diagonal matrices is diagonal, with the diagonal elements being the product of the corresponding diagonal elements of the original matrices. The reasoning aligns with the gold answer, detailing why off-diagonal elements are zero and how the diagonal elements are computed.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-84-2-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to prove that the product of two diagonal matrices is diagonal, with the diagonal elements being the product of the corresponding diagonal elements of the original matrices. The reasoning aligns with the gold answer, detailing why off-diagonal elements are zero and how the diagonal elements are computed."}, "llm_echoed_qid": "textbook-84-2-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to prove that the product of two diagonal matrices is diagonal, with the diagonal elements being the product of the corresponding diagonal elements of the original matrices. The reasoning aligns with the gold answer, detailing why off-diagonal elements are zero and how the diagonal elements are computed."}
{"script_processing_qid": "textbook-108-6-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the relationship between the U.S. price level, interest rates, and the exchange rate using the interest parity condition.\n\nGOLD_ANSWER:\n1. **Interest Parity Condition**: $i_{US} = i_{Foreign} + \\frac{E^e - E}{E}$, where $i_{US}$ is the U.S. interest rate, $i_{Foreign}$ is the foreign interest rate, $E$ is the current exchange rate, and $E^e$ is the expected future exchange rate.\n2. **Price Level Fall**: A fall in the U.S. price level lowers $i_{US}$.\n3. **Capital Outflow**: Investors seek higher returns abroad, increasing the supply of dollars in the foreign exchange market.\n4. **Exchange Rate Depreciation**: Increased supply of dollars causes the dollar to depreciate ($E$ falls).\n\nCANDIDATE_ANSWER:\nStep 1: Begin with the interest parity condition: i = i* + (E^e - E)/E, where i = U.S. interest rate, i* = foreign interest rate, E = current exchange rate (domestic currency per foreign currency), and E^e = expected future exchange rate.\n\nStep 2: A lower U.S. price level (P↓) increases real money balances (M/P↑), reducing the domestic interest rate (i↓) via liquidity preference theory.\n\nStep 3: The decline in U.S. interest rates (i↓) makes foreign assets more attractive. Investors sell dollars to buy foreign assets, increasing the supply of dollars in foreign exchange markets.\n\nStep 4: Increased dollar supply causes the exchange rate (E) to depreciate (E↑, meaning more dollars are required per unit of foreign currency).\n\nStep 5: Under interest parity, the depreciation adjusts to satisfy i = i* + (E^e - E)/E. If i < i*, the term (E^e - E)/E becomes negative (implying expected future appreciation), which requires an immediate depreciation (E↑) to maintain equilibrium.\n\nStep 6: The depreciation (E↑) reduces the purchasing power of dollars, making foreign goods relatively more expensive and U.S. goods cheaper. This increases net exports (NX↑), completing the chain: P↓ → i↓ → E↑ → NX↑.\n\nQID: textbook-108-6-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-108-6-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the relationship between the U.S. price level, interest rates, and the exchange rate using the interest parity condition. The steps logically follow from the initial condition to the final outcome, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-6-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the relationship between the U.S. price level, interest rates, and the exchange rate using the interest parity condition. The steps logically follow from the initial condition to the final outcome, aligning with the gold answer."}, "llm_echoed_qid": "textbook-108-6-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the relationship between the U.S. price level, interest rates, and the exchange rate using the interest parity condition. The steps logically follow from the initial condition to the final outcome, aligning with the gold answer."}
{"script_processing_qid": "textbook-13-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the role of the functional central limit theorem (FCLT) in unit-root econometrics.\n\nGOLD_ANSWER:\nThe FCLT is essential in unit-root econometrics because it allows the derivation of asymptotic distributions of test statistics under the null hypothesis of a unit root. It generalizes the standard central limit theorem to function spaces, enabling the analysis of stochastic processes like random walks. The FCLT is used to establish the limiting distributions of unit-root test statistics, which are typically non-standard and depend on Wiener processes.\n\nCANDIDATE_ANSWER:\nStep 1: The Functional Central Limit Theorem (FCLT) is fundamental in unit-root econometrics because it enables the analysis of asymptotic distributions for non-stationary I(1) processes. \nStep 2: When dealing with unit-root processes (e.g., $u_t \\sim$ I(1)), partial sums of their I(0) innovations (e.g., $\\Delta u_t$) converge weakly to stochastic integrals of Brownian motion under the FCLT. \nStep 3: The FCLT requires restrictions on the I(0) innovations (like linearity or finite dependence in the provided context) to ensure weak convergence. This allows test statistics (e.g., Dickey-Fuller) to follow non-standard distributions derived from Brownian motion. \nStep 4: In the example given, the long-run variance of $\\Delta u_t$ being zero arises because $\\text{Var}(\\sqrt{T}\\bar{v}) = \\text{Var}((u_T - u_0)/\\sqrt{T}) \\to 0$ as $T \\to \\infty$. This aligns with the FCLT’s requirement that scaled partial sums of I(0) processes exhibit stable asymptotic behavior.\n\nQID: textbook-13-0-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-13-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the role of the FCLT in unit-root econometrics, detailing its application to asymptotic distributions of test statistics, weak convergence to Brownian motion, and its relevance to non-standard distributions like those in Dickey-Fuller tests. This aligns well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-13-0-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains the role of the FCLT in unit-root econometrics, detailing its application to asymptotic distributions of test statistics, weak convergence to Brownian motion, and its relevance to non-standard distributions like those in Dickey-Fuller tests. This aligns well with the gold answer."}, "llm_echoed_qid": "textbook-13-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the role of the FCLT in unit-root econometrics, detailing its application to asymptotic distributions of test statistics, weak convergence to Brownian motion, and its relevance to non-standard distributions like those in Dickey-Fuller tests. This aligns well with the gold answer."}
{"script_processing_qid": "textbook-51-0-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze the role of political credibility in the collapse of the interwar gold standard. How did labor unions and left-wing parties influence monetary policy?\n\nGOLD_ANSWER:\n1.  **Credibility Crisis**: Post-WWI political shifts made deflationary policies (to defend convertibility) unpopular, reducing commitment to the gold standard.\n2.  **Labor Influence**: Unions resisted wage cuts, making price adjustment harder. Left-wing parties prioritized employment over exchange rate stability.\n3.  **Speculative Attacks**: Anticipating devaluation, capital flows became destabilizing, hastening the system's collapse.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the credibility problem in the context. The interwar gold standard lacked credibility due to post-WWI political changes that made governments less willing to defend convertibility through deflationary policies.\n\nStep 2: Link labor unions and left-wing parties to deflation resistance. Labor unions and left-wing parties opposed wage cuts and austerity measures required for gold standard adherence, as these policies exacerbated unemployment and economic hardship.\n\nStep 3: Explain the policy dilemma. Governments facing pressure from these groups prioritized domestic stability over gold standard maintenance, undermining market confidence in their commitment to convertibility.\n\nStep 4: Connect credibility collapse to systemic failure. Reduced political credibility triggered speculative attacks (e.g., gold reserve withdrawals) and forced countries to abandon the gold standard, accelerating its collapse.\n\nQID: textbook-51-0-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-51-0-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately follows the steps to analyze the role of political credibility in the collapse of the interwar gold standard and correctly identifies the influence of labor unions and left-wing parties on monetary policy. The explanation aligns well with the gold answer, covering the credibility crisis, labor influence, and the resulting systemic failure.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-51-0-1-3", "category": "CORRECT", "explanation": "The candidate's answer accurately follows the steps to analyze the role of political credibility in the collapse of the interwar gold standard and correctly identifies the influence of labor unions and left-wing parties on monetary policy. The explanation aligns well with the gold answer, covering the credibility crisis, labor influence, and the resulting systemic failure."}, "llm_echoed_qid": "textbook-51-0-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately follows the steps to analyze the role of political credibility in the collapse of the interwar gold standard and correctly identifies the influence of labor unions and left-wing parties on monetary policy. The explanation aligns well with the gold answer, covering the credibility crisis, labor influence, and the resulting systemic failure."}
{"script_processing_qid": "textbook-102-3-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Starting from the model $\\mathbf{y} = \\mathbf{X}_{1}\\mathbf{\\beta}_{1} + \\mathbf{X}_{2}\\mathbf{\\beta}_{2} + \\mathbf{\\varepsilon}$, derive the bias of the estimator $\\mathbf{b}_{1}$ when regressing $\\mathbf{y}$ on $\\mathbf{X}_{1}$ alone.\n\nGOLD_ANSWER:\n1. **Model**: $\\mathbf{y} = \\mathbf{X}_{1}\\mathbf{\\beta}_{1} + \\mathbf{X}_{2}\\mathbf{\\beta}_{2} + \\mathbf{\\varepsilon}$.\n2. **OLS Estimator**: $\\mathbf{b}_{1} = (\\mathbf{X}_{1}'\\mathbf{X}_{1})^{-1}\\mathbf{X}_{1}'\\mathbf{y}$.\n3. **Substitute $\\mathbf{y}$**: $\\mathbf{b}_{1} = (\\mathbf{X}_{1}'\\mathbf{X}_{1})^{-1}\\mathbf{X}_{1}'(\\mathbf{X}_{1}\\mathbf{\\beta}_{1} + \\mathbf{X}_{2}\\mathbf{\\beta}_{2} + \\mathbf{\\varepsilon})$.\n4. **Simplify**: $\\mathbf{b}_{1} = \\mathbf{\\beta}_{1} + (\\mathbf{X}_{1}'\\mathbf{X}_{1})^{-1}\\mathbf{X}_{1}'\\mathbf{X}_{2}\\mathbf{\\beta}_{2} + (\\mathbf{X}_{1}'\\mathbf{X}_{1})^{-1}\\mathbf{X}_{1}'\\mathbf{\\varepsilon}$.\n5. **Expectation**: $E[\\mathbf{b}_{1}] = \\mathbf{\\beta}_{1} + \\mathbf{P}_{1.2}\\mathbf{\\beta}_{2}$, where $\\mathbf{P}_{1.2} = (\\mathbf{X}_{1}'\\mathbf{X}_{1})^{-1}\\mathbf{X}_{1}'\\mathbf{X}_{2}$.\n6. **Bias**: $\\text{Bias}(\\mathbf{b}_{1}) = \\mathbf{P}_{1.2}\\mathbf{\\beta}_{2}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the true model's conditional expectation given X₁:\nE[y|X₁] = X₁β₁ + E[X₂|X₁]β₂.\n\nStep 2: Substitute the assumption that E[X₂|X₁] is linear in X₁:\nE[X₂|X₁] = X₁P₁.₂ (where P₁.₂ represents the linear projection coefficients).\n\nStep 3: The forecast using the misspecified model is X₁b₁. From the given result:\nE[b₁] = β₁ + P₁.₂β₂.\n\nStep 4: Compute the expectation of the forecast:\nE[X₁b₁] = X₁(E[b₁]) = X₁β₁ + X₁P₁.₂β₂.\n\nStep 5: Compare to the true conditional expectation:\nX₁β₁ + X₁P₁.₂β₂ = X₁β₁ + E[X₂|X₁]β₂ = E[y|X₁].\n\nStep 6: Conclusion: The forecast X₁b₁ matches the true conditional expectation. Since both remaining disturbances (ε and (X₂ - E[X₂|X₁])β₂) have mean zero and are uncorrelated with X₁, the prediction error has expectation zero.\n\nImplications: Even though coefficient estimates are biased, forecasts remain unbiased when omitted variables are linearly related to included variables through E[X₂|X₁]. This highlights that prediction unbiasedness can survive specification errors if the omitted effects are captured through the linear projection on included variables.\n\nQID: textbook-102-3-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-102-3-0-0\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the bias of the estimator as P₁.₂β₂, matching the gold answer. However, the reasoning focuses on the unbiasedness of forecasts rather than deriving the bias of the estimator, which is the core requirement of the question.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-102-3-0-0", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the bias of the estimator as P₁.₂β₂, matching the gold answer. However, the reasoning focuses on the unbiasedness of forecasts rather than deriving the bias of the estimator, which is the core requirement of the question."}, "llm_echoed_qid": "textbook-102-3-0-0", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the bias of the estimator as P₁.₂β₂, matching the gold answer. However, the reasoning focuses on the unbiasedness of forecasts rather than deriving the bias of the estimator, which is the core requirement of the question."}
{"script_processing_qid": "textbook-104-13-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the functions $\\zeta(l) = l H^{\\prime}(l)$ and $\\phi(y) = y V^{\\prime}(y)$, derive the functional equation (4) from the first-order condition and market-clearing condition.\n\nGOLD_ANSWER:\n1. **First-Order Condition**: $H^{\\prime}[n(x)] = \\int \\frac{x p(x)}{p(x^{\\prime})} V^{\\prime}\\left[\\frac{x n(x) p(x)}{p(x^{\\prime})}\\right]\\pi(x, dx^{\\prime})$.\n2. **Market-Clearing**: $p(x) = \\frac{M}{x n(x)}$.\n3. **Substitute**: Replace $p(x)$ and $p(x^{\\prime})$ in the first-order condition:\n   $$\\zeta[n(x)] = \\int \\phi[x^{\\prime} n(x^{\\prime})]\\pi(x, dx^{\\prime}).$$\n\nCANDIDATE_ANSWER:\nStep 1: Start with the first-order condition (FOC) from labor optimization:\nH’(l) = ∫ V’(c) · (∂c/∂l) π(x, dx')\n\nStep 2: Compute consumption derivative using market-clearing:\n∂c/∂l = x’n(x’)/n(x) where c = (x l)/p(x) · p(x’)\n\nStep 3: Substitute φ(y) ≡ yV’(y) ⇒ V’(y) = φ(y)/y:\nH’(l) = ∫ [φ(c)/c] · [x’n(x’)/n(x)] π(x, dx')\n\nStep 4: Express c using production function and price relation:\nc = l · x’n(x’)/n(x)\n\nStep 5: Substitute c into FOC and simplify:\nH’(l) = (1/l) ∫ φ(x’n(x’)) π(x, dx')\n\nStep 6: Multiply both sides by l and apply ζ(l) ≡ lH’(l):\nζ(l) = ∫ φ(x’n(x’)) π(x, dx')\n\nStep 7: In equilibrium l = n(x). Substitute and use inverse function:\nζ(n(x)) = ∫ φ(x’ζ⁻¹(f*(x’))) π(x, dx')\nwhere f*(x) ≡ ζ(n*(x))\n\nStep 8: This yields the functional equation:\nf*(x) = ∫ φ(x’ζ⁻¹(f*(x’))) π(x, dx')\n\nQID: textbook-104-13-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-104-13-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the functional equation from the first-order condition and market-clearing condition, following a logical sequence of steps that align with the gold answer's approach and result.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-104-13-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the functional equation from the first-order condition and market-clearing condition, following a logical sequence of steps that align with the gold answer's approach and result."}, "llm_echoed_qid": "textbook-104-13-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the functional equation from the first-order condition and market-clearing condition, following a logical sequence of steps that align with the gold answer's approach and result."}
{"script_processing_qid": "textbook-113-7-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Evaluate the argument that discretionary policy is necessary to respond to unforeseen shocks. Provide examples.\n\nGOLD_ANSWER:\n1. **Flexibility Argument**: Rules cannot anticipate all shocks (e.g., 2008 financial crisis, 2020 pandemic).\n2. **Examples**:\n   - Fed’s response to 1987 stock market crash (liquidity injection).\n   - ECB’s unconventional policies post-2008.\n3. **Counterpoint**: Rules with escape clauses (e.g., Taylor rule with exceptions) may balance flexibility and credibility.\n\nCANDIDATE_ANSWER:\nStep 1: The provided context emphasizes the drawbacks of discretionary monetary policy, including risks of incompetence, abuse of power, and excessive inflation. It advocates for policy rules as a superior alternative to discretion. Step 2: The context does not discuss arguments supporting discretionary policy (e.g., flexibility during unforeseen shocks) or provide examples of such scenarios. Step 3: Since the question asks for an evaluation of arguments in favor of discretion, but the context only criticizes discretion and promotes rules, there is insufficient information to address the necessity of discretion for unforeseen shocks. Step 4: Examples are not provided in the context to illustrate unforeseen shocks requiring discretionary responses.\n\nQID: textbook-113-7-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-113-7-1-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate answer misinterprets the question by focusing on the drawbacks of discretionary policy rather than evaluating its necessity for unforeseen shocks, as requested. The gold answer clearly provides arguments and examples supporting discretionary policy in such scenarios.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-113-7-1-2", "category": "INCORRECT", "explanation": "The candidate answer misinterprets the question by focusing on the drawbacks of discretionary policy rather than evaluating its necessity for unforeseen shocks, as requested. The gold answer clearly provides arguments and examples supporting discretionary policy in such scenarios."}, "llm_echoed_qid": "textbook-113-7-1-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate answer misinterprets the question by focusing on the drawbacks of discretionary policy rather than evaluating its necessity for unforeseen shocks, as requested. The gold answer clearly provides arguments and examples supporting discretionary policy in such scenarios."}
{"script_processing_qid": "textbook-94-5-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the competitive level of output is Pareto efficient in this context.\n\nGOLD_ANSWER:\nThe competitive level of output is Pareto efficient because at this point, price equals marginal cost ($p = MC$). Any further production would result in $p < MC$, making at least one party worse off. Thus, no further Pareto improvements are possible.\n\nCANDIDATE_ANSWER:\nStep 1: Understand Pareto efficiency. A competitive equilibrium is Pareto efficient if no reallocation can make someone better off without making others worse off.\n\nStep 2: Observe monopolist behavior. A monopolist restricts output to y_m where MR = MC, charging p(y_m) > MC(y_m). This creates deadweight loss, indicating Pareto inefficiency.\n\nStep 3: Analyze competitive output. In perfect competition, firms produce y_c where p(y_c) = MC(y_c), eliminating deadweight loss.\n\nStep 4: Compare outcomes. At y_c, the price equals marginal cost. Any additional unit Δy beyond y_c would have p(y_c + Δy) < MC(y_c + Δy), making producers worse off. Any reduction below y_c would leave unmet demand where p > MC, making consumers worse off.\n\nStep 5: Conclude Pareto efficiency. Since no output adjustment at y_c can benefit one party without harming another, the competitive output level is Pareto efficient.\n\nQID: textbook-94-5-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-94-5-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains why the competitive level of output is Pareto efficient by detailing the conditions of perfect competition, comparing it to monopolist behavior, and concluding that no Pareto improvements are possible at the competitive output level. This aligns perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-94-5-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains why the competitive level of output is Pareto efficient by detailing the conditions of perfect competition, comparing it to monopolist behavior, and concluding that no Pareto improvements are possible at the competitive output level. This aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-94-5-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains why the competitive level of output is Pareto efficient by detailing the conditions of perfect competition, comparing it to monopolist behavior, and concluding that no Pareto improvements are possible at the competitive output level. This aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-102-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. For the polynomial distributed lag model $y_{t}=\\alpha+\\sum_{j=0}^{6}\\beta_{j}x_{t-j}+\\varepsilon_{t}$, derive the long run multiplier and explain how to estimate it using restricted least squares.\n\nGOLD_ANSWER:\n1. **Long run multiplier**: $\\beta_{0}+\\beta_{1}+...+\\beta_{6}$.\n2. **Estimation**: Use restricted least squares with constraints on the coefficients to ensure the polynomial structure. For a third-order polynomial, impose constraints like $\\beta_{j}=\\gamma_{0}+\\gamma_{1}j+\\gamma_{2}j^{2}+\\gamma_{3}j^{3}$ for $j=0,...,6$.\n\nCANDIDATE_ANSWER:\nStep 1: The long run multiplier in a polynomial distributed lag model is defined as the sum of all lag coefficients. For the given model $y_t = \\alpha + \\sum_{j=0}^6 \\beta_j x_{t-j} + \\varepsilon_t$, this equals $\\beta_0 + \\beta_1 + \\cdots + \\beta_6$.\n\nStep 2: To estimate using restricted least squares, we impose polynomial restrictions on the coefficients. For example, specify $\\beta_j = \\gamma_0 + \\gamma_1 j + \\gamma_2 j^2$ (for a 2nd-degree polynomial).\n\nStep 3: Substitute these polynomial expressions into the original model. The regression becomes $y_t = \\alpha + \\sum_{j=0}^6 (\\gamma_0 + \\gamma_1 j + \\gamma_2 j^2) x_{t-j} + \\varepsilon_t$.\n\nStep 4: Rewrite as $y_t = \\alpha + \\gamma_0 \\sum_{j=0}^6 x_{t-j} + \\gamma_1 \\sum_{j=0}^6 jx_{t-j} + \\gamma_2 \\sum_{j=0}^6 j^2x_{t-j} + \\varepsilon_t$.\n\nStep 5: Estimate the restricted model via OLS to get $\\hat\\gamma_0$, $\\hat\\gamma_1$, $\\hat\\gamma_2$.\n\nStep 6: Compute the long run multiplier as $\\sum_{j=0}^6 \\hat\\beta_j = 7\\hat\\gamma_0 + 21\\hat\\gamma_1 + 91\\hat\\gamma_2$ (using $\\sum j=21$ and $\\sum j^2=91$ for j=0-6).\n\nThus, restricted least squares estimation involves (1) specifying polynomial constraints, (2) estimating reduced parameters via OLS, and (3) summing constrained coefficients algebraically.\n\nQID: textbook-102-0-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-102-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the long run multiplier as the sum of the lag coefficients and accurately describes the process of estimating it using restricted least squares, including the polynomial constraints and the algebraic summation of coefficients.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-102-0-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the long run multiplier as the sum of the lag coefficients and accurately describes the process of estimating it using restricted least squares, including the polynomial constraints and the algebraic summation of coefficients."}, "llm_echoed_qid": "textbook-102-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the long run multiplier as the sum of the lag coefficients and accurately describes the process of estimating it using restricted least squares, including the polynomial constraints and the algebraic summation of coefficients."}
{"script_processing_qid": "textbook-110-0-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Prove Hotelling’s Lemma, $$\\frac{\\partial\\Pi(\\mathbf{p})}{\\partial p\\_i}=q\\_i^*,$$ using the properties of the profit function.\n\nGOLD_ANSWER:\n1. **Envelope Theorem**: The derivative of the profit function with respect to $p\\_i$ is the optimal net output $q\\_i^*$, as the indirect effect through $\\mathbf{q}^*$ vanishes by the FOCs.\n2. **Intuition**: A small increase in the price of good $i$ directly increases profits by the amount of $q\\_i^*$ produced, holding other outputs constant.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that the profit function Π(𝐩) is defined as the maximum attainable profit given prices 𝐩, i.e., Π(𝐩) = max_{Φ(𝐳)≤0} Σp_i q_i. At the optimal solution 𝐪*, this becomes Π(𝐩) = Σp_i q_i*(𝐩).\n\nStep 2: Apply the Envelope Theorem, which states that the derivative of the maximized objective function with respect to a parameter equals the partial derivative of the objective function holding all choice variables at their optimal values. For parameter p_i:\n\n∂Π/∂p_i = ∂/∂p_i [Σp_j q_j*(𝐩)] evaluated at q_j*.\n\nStep 3: Compute the derivative term-by-term. The derivative of p_i q_i* with respect to p_i is q_i* (direct effect). The derivatives of other terms (p_j q_j* for j≠i) with respect to p_i are zero. By the Envelope Theorem, adjustments to q_j* caused by changing p_i do not contribute to first-order changes in Π because the firm is already at an optimum (indirect effects cancel out).\n\nStep 4: Combine results to get ∂Π/∂p_i = q_i*, which completes the proof of Hotelling’s Lemma.\n\nQID: textbook-110-0-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-110-0-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly applies the Envelope Theorem and provides a detailed step-by-step proof that aligns with the gold answer's explanation of Hotelling’s Lemma.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-0-1-1", "category": "CORRECT", "explanation": "The candidate's answer correctly applies the Envelope Theorem and provides a detailed step-by-step proof that aligns with the gold answer's explanation of Hotelling’s Lemma."}, "llm_echoed_qid": "textbook-110-0-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly applies the Envelope Theorem and provides a detailed step-by-step proof that aligns with the gold answer's explanation of Hotelling’s Lemma."}
{"script_processing_qid": "textbook-84-3-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Test the hypothesis that the elasticity of salary with respect to market value is zero at the 5% significance level. Provide the test statistic and critical value.\n\nGOLD_ANSWER:\n1. **Hypotheses**: $H_0: \\beta_{\\text{mktval}} = 0$ vs. $H_1: \\beta_{\\text{mktval}} \\neq 0$.\n2. **Test Statistic**: $t = \\frac{0.107}{\\text{se}(0.107)}$.\n3. **Critical Value**: For a two-tailed test at 5% significance, the critical value is approximately 1.96.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the relevant coefficient. The elasticity of salary with respect to market value is represented by the coefficient on log(mktval), which is 0.107.\n\nStep 2: Formulate hypotheses. The null hypothesis is H₀: β_{log(mktval)} = 0 (elasticity is zero), and the alternative is H₁: β_{log(mktval)} ≠ 0.\n\nStep 3: Recognize missing information. While the coefficient value (0.107) is provided, the standard error of this coefficient and the sample size (required to determine degrees of freedom) are not given in the context.\n\nStep 4: Conclusion. The test statistic (t-stat = coefficient / standard error) and critical value (from t-distribution based on degrees of freedom and α = 5%) cannot be computed with the provided information. The context lacks the necessary statistical details (standard errors, sample size) to perform the hypothesis test quantitatively.\n\nQID: textbook-84-3-2-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-84-3-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the coefficient, formulates the hypotheses, and acknowledges the missing information required to compute the test statistic and critical value. This aligns with the gold answer's structure and reasoning, despite the lack of specific numerical values due to insufficient context.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-84-3-2-2", "category": "CORRECT", "explanation": "The candidate correctly identifies the coefficient, formulates the hypotheses, and acknowledges the missing information required to compute the test statistic and critical value. This aligns with the gold answer's structure and reasoning, despite the lack of specific numerical values due to insufficient context."}, "llm_echoed_qid": "textbook-84-3-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly identifies the coefficient, formulates the hypotheses, and acknowledges the missing information required to compute the test statistic and critical value. This aligns with the gold answer's structure and reasoning, despite the lack of specific numerical values due to insufficient context."}
{"script_processing_qid": "textbook-119-18-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the statistical issues introduced by the Koyck transformation, particularly the presence of the lagged dependent variable and the moving average error term.\n\nGOLD_ANSWER:\n1. **Stochastic explanatory variable**: The lagged dependent variable $Y_{t-1}$ is stochastic, violating the classical OLS assumption of nonstochastic or independent explanatory variables. 2. **Serial correlation**: The error term $\\nu_{t}=(u_{t}-\\lambda u_{t-1})$ is a moving average, introducing serial correlation if $u_{t}$ is serially uncorrelated. 3. **Durbin-Watson test invalidity**: The presence of $Y_{t-1}$ invalidates the Durbin-Watson test for serial correlation, necessitating alternative tests like the Durbin $h$ test. 4. **Instrumental variables**: To address endogeneity, instrumental variable techniques may be required.\n\nCANDIDATE_ANSWER:\nStep 1: Identification of Key Features in Koyck Transformation\nThe Koyck transformation simplifies an infinite distributed-lag model by assuming geometrically declining coefficients (βₖ = β₀λᵏ). This leads to a transformed model with a lagged dependent variable (Y_{t-1}) and a composite error term (u_t - λu_{t-1}), which introduces two key statistical issues.\n\nStep 2: Issue 1 – Lagged Dependent Variable Endogeneity\nThe inclusion of Y_{t-1} as a regressor creates endogeneity. Since Y_{t-1} depends on u_{t-1}, which is part of the transformed error term (u_t - λu_{t-1}), the lagged dependent variable becomes correlated with the error term. This violates the OLS assumption of exogeneity, leading to biased and inconsistent parameter estimates.\n\nStep 3: Issue 2 – Moving Average (MA) Error Term\nThe transformed error term (u_t - λu_{t-1}) follows an MA(1) process. OLS assumes serially uncorrelated errors, but MA errors introduce autocorrelation. This results in inefficient standard errors and invalidates conventional hypothesis-testing procedures unless corrected.\n\nStep 4: Combined Effect\nThe interaction of the lagged dependent variable and MA error exacerbates problems. The correlation between Y_{t-1} and u_{t-1} (from the MA term) perpetuates endogeneity, while the MA structure complicates estimation methods (e.g., requiring maximum likelihood or instrumental variables instead of standard OLS).\n\nStep 5: Implications for Estimation\nThese issues necessitate specialized estimation techniques (e.g., IV, GMM, or ML) to address both endogeneity and autocorrelation. Ignoring them leads to unreliable inferences about λ, β₀, and the derived lag metrics (median/mean lag).\n\nQID: textbook-119-18-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-18-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the statistical issues introduced by the Koyck transformation, including endogeneity from the lagged dependent variable and autocorrelation from the MA error term, aligning well with the gold answer. It also correctly mentions the need for specialized estimation techniques.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-18-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the statistical issues introduced by the Koyck transformation, including endogeneity from the lagged dependent variable and autocorrelation from the MA error term, aligning well with the gold answer. It also correctly mentions the need for specialized estimation techniques."}, "llm_echoed_qid": "textbook-119-18-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the statistical issues introduced by the Koyck transformation, including endogeneity from the lagged dependent variable and autocorrelation from the MA error term, aligning well with the gold answer. It also correctly mentions the need for specialized estimation techniques."}
{"script_processing_qid": "textbook-113-11-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Using the lighthouse example, explain how the government can address the free-rider problem. What are the potential limitations of this approach?\n\nGOLD_ANSWER:\n4. **Government Intervention and Limitations**:\n   - **Solution**: The government can provide lighthouses directly, funded by taxes, ensuring provision despite free-riding.\n   - **Limitations**:\n     1. **Cost-Benefit Analysis**: Difficult to accurately measure the benefits of public goods.\n     2. **Inefficiency**: Government provision may not be as efficient as private provision due to lack of price signals.\n     3. **Political Influence**: Decisions may be swayed by political rather than economic considerations.\n\nCANDIDATE_ANSWER:\nStep 1: Government Intervention to Address Free-Rider Problem\n- The free-rider problem arises because lighthouse benefits (non-excludable and non-rival) allow ship captains to use the service without paying. Private markets fail to provide lighthouses due to this issue.\n- Governments address this by directly providing lighthouses and funding them through taxation. This ensures collective payment, eliminating free-riding by mandating contributions via taxes.\n\nStep 2: Potential Limitations of Government Intervention\n1. Inefficiency: Government provision may lack market incentives, leading to bureaucratic delays, mismanagement, or higher operational costs.\n2. Misallocation of Resources: Taxpayer-funded lighthouses might over-supply or under-supply services if usage patterns or needs are inaccurately assessed.\n3. Alternative Solutions Overlooked: The 19th-century England example shows private provision (charging port owners instead of ship captains) can work in specific conditions. Government intervention might ignore such context-dependent innovations.\n4. Public Resistance: Taxpayers who rarely benefit from lighthouses (e.g., inland residents) may oppose funding them, creating political challenges.\n\nQID: textbook-113-11-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-113-11-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains how the government can address the free-rider problem by providing lighthouses through taxation and identifies potential limitations such as inefficiency, misallocation of resources, overlooking alternative solutions, and public resistance. This aligns well with the gold answer, which also mentions inefficiency and political influence as limitations.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-113-11-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately explains how the government can address the free-rider problem by providing lighthouses through taxation and identifies potential limitations such as inefficiency, misallocation of resources, overlooking alternative solutions, and public resistance. This aligns well with the gold answer, which also mentions inefficiency and political influence as limitations."}, "llm_echoed_qid": "textbook-113-11-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains how the government can address the free-rider problem by providing lighthouses through taxation and identifies potential limitations such as inefficiency, misallocation of resources, overlooking alternative solutions, and public resistance. This aligns well with the gold answer, which also mentions inefficiency and political influence as limitations."}
{"script_processing_qid": "textbook-106-9-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze the unintended consequences of minimum-wage laws on employment levels, using the concept of elasticity of labor demand.\n\nGOLD_ANSWER:\n1.  **Elasticity of Labor Demand**: Measures how responsive employment is to changes in wages.\n2.  **High Elasticity**: Firms can easily substitute labor with capital or reduce output, leading to significant job losses.\n3.  **Low Elasticity**: Firms are less able to substitute, so job losses are minimal.\n4.  **Unintended Consequences**: Even with low elasticity, some workers (e.g., low-skilled) may lose jobs or face reduced hours.\n\nCANDIDATE_ANSWER:\nStep 1: Minimum-wage laws act as a price floor in the labor market, setting wages above the equilibrium level determined by supply and demand. Step 2: The elasticity of labor demand determines how responsive employers are to wage changes. If labor demand is elastic (e.g., in industries where automation or outsourcing is feasible), a higher minimum wage leads to a significant reduction in employment as firms cut jobs to offset increased costs. Step 3: Conversely, if labor demand is inelastic (e.g., in labor-intensive sectors with fewer substitutes), employment levels may decrease less sharply. Step 4: The unintended consequence is that while some workers benefit from higher wages, others lose jobs entirely, particularly in elastic labor markets. Step 5: The context highlights that minimum-wage laws 'cause other workers to be unemployed,' with the severity of this effect directly tied to the elasticity of labor demand in specific sectors or markets.\n\nQID: textbook-106-9-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-106-9-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the unintended consequences of minimum-wage laws on employment levels using the concept of elasticity of labor demand. It correctly identifies the impact of high and low elasticity on employment and highlights the trade-off between higher wages for some workers and job losses for others, aligning well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-9-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the unintended consequences of minimum-wage laws on employment levels using the concept of elasticity of labor demand. It correctly identifies the impact of high and low elasticity on employment and highlights the trade-off between higher wages for some workers and job losses for others, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-106-9-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the unintended consequences of minimum-wage laws on employment levels using the concept of elasticity of labor demand. It correctly identifies the impact of high and low elasticity on employment and highlights the trade-off between higher wages for some workers and job losses for others, aligning well with the gold answer."}
{"script_processing_qid": "textbook-112-3-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Critically evaluate Smith's claim that carrying trade 'is altogether withdrawn from supporting the productive labour of that particular country.' Discuss exceptions.\n\nGOLD_ANSWER:\n1. **Primary Argument**:\n   - Carrying trade (e.g., Dutch merchant shipping Polish corn to Portugal) replaces two foreign capitals, neither supporting domestic labor.\n   - Only profits return to home country (Holland), constituting a minor addition to GDP.\n\n2. **Exceptions**:\n   - **Domestic Shipping**: If carried by home-country ships, freight payments support domestic sailors/shipbuilders.\n     - Example: British bottoms in Poland-Portugal trade employ British labor.\n   - **Strategic Goods**: Transporting essential commodities (e.g., naval supplies) may indirectly bolster domestic industries.\n\n3. **Limitation**:\n   - Smith underestimates multiplier effects (e.g., port infrastructure, insurance).\n\nCANDIDATE_ANSWER:\nStep 1: Examine Smith’s core argument. Smith defines carrying trade as capital used to purchase goods in one foreign country to sell in another, arguing that this capital supports foreign productive labor rather than domestic labor. He contrasts this with home trade (which replaces two domestic capitals) and foreign consumption trade (which replaces one domestic and one foreign capital).\n\nStep 2: Identify Smith’s reasoning. Carrying trade involves no domestic production or consumption, meaning the capital circulates entirely between foreign economies and does not directly sustain domestic industries or workers.\n\nStep 3: Evaluate exceptions implicit in Smith’s framework. While the provided context does not explicitly outline exceptions, Smith’s logic implies two potential qualifications: (1) If domestic labor is employed in transportation/logistics (e.g., shipping, insurance) for carrying trade, even if the traded goods themselves are foreign, this could indirectly support domestic labor. (2) Profits from carrying trade might be repatriated and reinvested domestically, indirectly benefiting productive labor. However, Smith dismisses these points in his claim that the capital is 'altogether withdrawn,' prioritizing direct replacement of capitals over secondary effects.\n\nStep 4: Critical assessment. Smith’s claim holds if narrowly interpreted as excluding indirect employment (e.g., transport workers) and profit reinvestment. However, these exceptions weaken his absolutist phrasing ('altogether withdrawn'), as carrying trade may still generate ancillary domestic economic activity. The context emphasizes direct capital allocation, so Smith’s dismissal of these nuances reflects his focus on immediate productive labor impacts over broader spillovers.\n\nQID: textbook-112-3-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-3-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly examines Smith's core argument, identifies his reasoning, evaluates implicit exceptions, and critically assesses the claim. The response aligns with the gold answer, addressing both the primary argument and exceptions while providing a nuanced critique of Smith's absolutist phrasing.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-3-0-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly examines Smith's core argument, identifies his reasoning, evaluates implicit exceptions, and critically assesses the claim. The response aligns with the gold answer, addressing both the primary argument and exceptions while providing a nuanced critique of Smith's absolutist phrasing."}, "llm_echoed_qid": "textbook-112-3-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly examines Smith's core argument, identifies his reasoning, evaluates implicit exceptions, and critically assesses the claim. The response aligns with the gold answer, addressing both the primary argument and exceptions while providing a nuanced critique of Smith's absolutist phrasing."}
{"script_processing_qid": "textbook-119-24-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. What are the implications of the skewed nature of the chi-square distribution for the confidence interval of $\\sigma^{2}$?\n\nGOLD_ANSWER:\nThe chi-square distribution is positively skewed, leading to asymmetric confidence intervals for $\\sigma^{2}$. This asymmetry ensures that the interval does not include negative values, which would be nonsensical for a variance.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that the chi-square distribution is positively skewed (right-skewed), especially at lower degrees of freedom (df = 8 in this case).\nStep 2: Observe that the confidence interval formula swaps the critical values: Lower bound uses χ²_{α/2} (larger critical value) and upper bound uses χ²_{1-α/2} (smaller critical value).\nStep 3: Due to skewness, the interval becomes asymmetric. Dividing by a larger denominator (17.5346) for the lower bound makes it tighter, while dividing by a smaller denominator (2.1797) for the upper bound makes it wider.\nStep 4: Result: The confidence interval for σ² will stretch further to the upper side of ŝ² compared to a symmetric distribution, reflecting higher uncertainty in estimating larger variances.\nStep 5: Practical implication: Intervals may suggest σ² has greater potential upward deviation than downward deviation from the point estimate (42.1591 in the example).\n\nQID: textbook-119-24-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-24-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the implications of the chi-square distribution's skewness on the confidence interval for σ², including the asymmetry and practical implications, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-24-1-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the implications of the chi-square distribution's skewness on the confidence interval for σ², including the asymmetry and practical implications, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-119-24-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the implications of the chi-square distribution's skewness on the confidence interval for σ², including the asymmetry and practical implications, aligning well with the gold answer."}
{"script_processing_qid": "textbook-111-8-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Compare the seigniorage implications of dollarization versus a currency board arrangement.\n\nGOLD_ANSWER:\n1. **Currency Board**: Seigniorage accrues to the domestic central bank but is constrained by reserve holdings.\n2. **Dollarization**: Seigniorage is entirely forfeited to the issuer of the foreign currency (e.g., the U.S. government).\n3. **Trade-off**: Dollarization eliminates exchange rate risk but sacrifices monetary autonomy and seigniorage revenue.\n\nCANDIDATE_ANSWER:\nStep 1: Seigniorage refers to the profit a government earns by issuing currency. Under a currency board arrangement, the central bank holds foreign reserves equal to 100% of the domestic currency in circulation. While this restricts discretionary money creation, the central bank can still earn seigniorage through interest on foreign reserve holdings (e.g., U.S. government bonds).\n\nStep 2: In dollarization, the domestic currency is fully replaced by a foreign currency (e.g., the U.S. dollar). The country loses all seigniorage rights because it no longer issues its own currency – seigniorage accrues entirely to the issuer of the foreign currency (e.g., the U.S. Federal Reserve).\n\nConclusion: A currency board preserves limited seigniorage via reserve investment returns, whereas dollarization eliminates domestic seigniorage entirely.\n\nQID: textbook-111-8-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-111-8-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately captures the seigniorage implications of both dollarization and a currency board arrangement, matching the gold answer in substance and reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-111-8-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately captures the seigniorage implications of both dollarization and a currency board arrangement, matching the gold answer in substance and reasoning."}, "llm_echoed_qid": "textbook-111-8-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately captures the seigniorage implications of both dollarization and a currency board arrangement, matching the gold answer in substance and reasoning."}
{"script_processing_qid": "textbook-107-15-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Why does a production function $F(x,y)=x^{3/4}y^{3/4}$ imply that the industry cannot be competitive in the long run? What market structure might emerge instead?\n\nGOLD_ANSWER:\n1. **Increasing Returns**: The production function exhibits increasing returns to scale (sum of exponents > 1).\n2. **Competition**: In competitive markets, firms must operate at constant returns; increasing returns lead to natural monopolies.\n3. **Outcome**: The industry may evolve into a monopoly or oligopoly due to cost advantages of larger firms.\n\nCANDIDATE_ANSWER:\nStep 1: Analyze the returns to scale of the production function. For inputs scaled by a factor \\( t \\), \\( F(tx, ty) = (tx)^{3/4}(ty)^{3/4} = t^{3/4 + 3/4}F(x,y) = t^{1.5}F(x,y) \\). This shows **increasing returns to scale** (output increases more than proportionally to input scaling).\n\nStep 2: In a competitive market, long-run equilibrium requires **constant returns to scale** (horizontal supply curve). Here, increasing returns to scale imply firms face **declining long-run average costs** as output expands.\n\nStep 3: Declining average costs create a natural tendency for larger firms to dominate (lower costs allow undercutting competitors). This violates the competitive assumption of price-taking firms with identical costs, leading to market concentration.\n\nStep 4: The industry cannot sustain competition because no finite equilibrium exists—firms would expand indefinitely or merge. Instead, a **monopoly** or **oligopoly** emerges as the stable market structure due to economies of scale and barriers to entry for smaller firms.\n\nQID: textbook-107-15-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-107-15-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the increasing returns to scale in the production function, explains why this is incompatible with long-run competitive equilibrium, and concludes that a monopoly or oligopoly would emerge. This aligns perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-15-1-1", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the increasing returns to scale in the production function, explains why this is incompatible with long-run competitive equilibrium, and concludes that a monopoly or oligopoly would emerge. This aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-107-15-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the increasing returns to scale in the production function, explains why this is incompatible with long-run competitive equilibrium, and concludes that a monopoly or oligopoly would emerge. This aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-122-30-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of $\\rho = 0$ in the Heckit model. What does this imply about the relationship between $u$ and $\\nu$, and how does it affect the consistency of OLS on the selected sample?\n\nGOLD_ANSWER:\n1.  **Implication of $\\rho = 0$**: The error terms $u$ and $\\nu$ are uncorrelated.\n2.  **Effect on OLS**: If $\\rho = 0$, the inverse Mills ratio term $\\lambda(\\mathbf{z}\\gamma)$ drops out, and OLS on the selected sample is consistent for $\\beta$.\n3.  **Interpretation**: No sample selection bias exists because the selection mechanism is independent of the error term in the population model.\n4.  **Practical Test**: A significant coefficient on $\\lambda(\\mathbf{z}\\hat{\\gamma})$ in the Heckit second stage indicates $\\rho \\neq 0$ and the presence of selection bias.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that in the Heckit model, the correlation coefficient ρ measures the relationship between the error terms u (from the main equation) and ν (from the selection equation).\n\nStep 2: When ρ = 0, it implies that there is zero correlation between u and ν. This means the unobservables affecting sample selection (ν) are unrelated to the unobservables affecting the outcome equation (u).\n\nStep 3: The inverse Mills ratio term in Equation (17.45) has a coefficient proportional to ρσ (where σ is the standard deviation of u). When ρ = 0, this term disappears from the conditional expectation of y given observability.\n\nStep 4: Without the inverse Mills ratio term, OLS estimation on the selected sample reduces to E(y|x, s=1) = xβ. Since E(u|x) = 0 holds in the population model, omitted variable bias disappears when ρ = 0.\n\nStep 5: Under ρ = 0, sample selection becomes exogenous (effectively 'random' with respect to u). This implies OLS estimates on the selected sample remain consistent for β, as the selection mechanism does not correlate with u conditional on x.\n\nQID: textbook-122-30-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-30-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the implications of ρ = 0 in the Heckit model, including the uncorrelation of error terms u and ν, the disappearance of the inverse Mills ratio term, and the consistency of OLS on the selected sample. This aligns perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-30-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the implications of ρ = 0 in the Heckit model, including the uncorrelation of error terms u and ν, the disappearance of the inverse Mills ratio term, and the consistency of OLS on the selected sample. This aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-122-30-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the implications of ρ = 0 in the Heckit model, including the uncorrelation of error terms u and ν, the disappearance of the inverse Mills ratio term, and the consistency of OLS on the selected sample. This aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-106-5-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the relationship between marginal cost (MC) and average total cost (ATC) mathematically, and explain why the MC curve intersects the ATC curve at its minimum point.\n\nGOLD_ANSWER:\n1.  **Mathematical Derivation**: \n   - Average Total Cost (ATC) is defined as $ATC = \\frac{TC}{Q}$, where $TC$ is total cost and $Q$ is quantity.\n   - Marginal Cost (MC) is the derivative of total cost with respect to quantity: $MC = \\frac{dTC}{dQ}$.\n   - To find the minimum of ATC, take the derivative of ATC with respect to $Q$ and set it to zero:\n     $$ \\frac{dATC}{dQ} = \\frac{d}{dQ}\\left(\\frac{TC}{Q}\\right) = \\frac{Q \\cdot \\frac{dTC}{dQ} - TC}{Q^2} = 0 $$\n     This implies $Q \\cdot MC - TC = 0$, or $MC = \\frac{TC}{Q} = ATC$.\n   - Thus, MC equals ATC at the minimum point of ATC.\n2.  **Explanation**: When MC < ATC, ATC is falling because the cost of producing an additional unit is less than the current average. When MC > ATC, ATC rises because the cost of producing an additional unit is higher than the current average. The intersection occurs at the minimum ATC, where MC = ATC.\n\nCANDIDATE_ANSWER:\nStep 1: Define ATC and MC mathematically. \nATC = Total Cost (TC) / Quantity (Q)\nMC = Derivative of TC with respect to Q = dTC/dQ\n\nStep 2: Calculate the derivative of ATC with respect to Q to analyze its slope.\nUsing the quotient rule:\nd(ATC)/dQ = [dTC/dQ * Q - TC] / Q²\n\nStep 3: Substitute MC = dTC/dQ and TC = ATC * Q into the equation:\nd(ATC)/dQ = [MC * Q - (ATC * Q)] / Q²\n= (MC - ATC) / Q\n\nStep 4: At the minimum point of ATC, its slope (d(ATC)/dQ) equals zero:\n(MC - ATC)/Q = 0 → MC = ATC\nThis shows MC intersects ATC when ATC is minimized.\n\nStep 5: Behavioral explanation:\n- When MC < ATC: New units cost less than the current average, pulling ATC down (ATC decreasing).\n- When MC > ATC: New units cost more than the current average, pushing ATC up (ATC increasing).\n- Thus, the MC curve must cross the ATC curve exactly at the minimum ATC point (efficient scale), where the directional change in ATC switches from falling to rising.\n\nQID: textbook-106-5-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-5-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the relationship between MC and ATC mathematically and provides a clear explanation of why the MC curve intersects the ATC curve at its minimum point. The steps and reasoning align perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-5-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the relationship between MC and ATC mathematically and provides a clear explanation of why the MC curve intersects the ATC curve at its minimum point. The steps and reasoning align perfectly with the gold answer."}, "llm_echoed_qid": "textbook-106-5-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the relationship between MC and ATC mathematically and provides a clear explanation of why the MC curve intersects the ATC curve at its minimum point. The steps and reasoning align perfectly with the gold answer."}
{"script_processing_qid": "textbook-64-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the significance of the t-ratios for the coefficients in the regression output. How would you interpret a coefficient with a t-ratio of 20.034?\n\nGOLD_ANSWER:\n1. **t-ratio**: A t-ratio of 20.034 is highly significant. \n2. **p-value**: The associated p-value is .0000, indicating the coefficient is significantly different from zero. \n3. **Implication**: The predictor variable has a strong and statistically significant effect on the dependent variable.\n\nCANDIDATE_ANSWER:\nStep 1: A t-ratio measures the number of standard errors between the coefficient estimate and zero. It tests the null hypothesis that the coefficient is statistically insignificant (H₀: β = 0).\\n\\nStep 2: A t-ratio of 20.034 is exceptionally large. For standard significance levels (e.g., 5% with critical value ~1.96 for large samples), this value falls far in the rejection region.\\n\\nStep 3: This implies the coefficient is statistically significant at all conventional levels (p-value ≈ 0). The variable associated with this coefficient has a strong, non-zero relationship with the dependent variable in the model.\\n\\nStep 4: While statistical significance is confirmed, economic significance (practical importance of the coefficient size) should still be assessed separately based on the variable's context and scale.\n\nQID: textbook-64-1-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-64-1-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the significance of the t-ratio, correctly interprets its value, and mentions the need to assess economic significance, aligning well with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-64-1-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the significance of the t-ratio, correctly interprets its value, and mentions the need to assess economic significance, aligning well with the gold answer."}, "llm_echoed_qid": "textbook-64-1-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the significance of the t-ratio, correctly interprets its value, and mentions the need to assess economic significance, aligning well with the gold answer."}
{"script_processing_qid": "textbook-31-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the significance of the consumption set $X$ being nonempty, closed, and bounded in the context of utility maximization.\n\nGOLD_ANSWER:\n1. **Nonempty**: Ensures there exists at least one feasible consumption bundle.\n2. **Closed**: Guarantees that limits of sequences within $X$ remain in $X$, ensuring the existence of a maximizer.\n3. **Bounded**: Prevents utility from diverging to infinity, ensuring a finite maximizer exists.\n\nCANDIDATE_ANSWER:\nStep 1: **Nonempty Consumption Set (X)**: Ensures there exists at least one feasible consumption bundle for the consumer to evaluate, making utility maximization logically possible.\n\nStep 2: **Closedness of X**: Guarantees that all limit points of sequences within X are also in X. This prevents situations where a utility-maximizing sequence might converge to a point outside X, ensuring the existence of a solution.\n\nStep 3: **Boundedness of X**: Combined with closedness, boundedness implies compactness (by the Heine-Borel theorem in Euclidean space). Compactness ensures that the continuous utility function u(·) attains its maximum on X (via the Extreme Value Theorem, referenced here as Theorem M.F.2).\n\nStep 4: **Implications for Satiated Preferences**: The existence of a maximizer x* (due to nonempty/closed/bounded X) implies u(x*) ≥ u(x) for all x ∈ X. This establishes global satiation (x* is at least as good as any other bundle in X), resolving the utility maximization problem conclusively.\n\nQID: textbook-31-0-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-31-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains the significance of the consumption set being nonempty, closed, and bounded, aligning with the gold answer. It also provides additional context about compactness and the Extreme Value Theorem, which further supports the reasoning.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-31-0-0-1", "category": "CORRECT", "explanation": "The candidate answer correctly explains the significance of the consumption set being nonempty, closed, and bounded, aligning with the gold answer. It also provides additional context about compactness and the Extreme Value Theorem, which further supports the reasoning."}, "llm_echoed_qid": "textbook-31-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains the significance of the consumption set being nonempty, closed, and bounded, aligning with the gold answer. It also provides additional context about compactness and the Extreme Value Theorem, which further supports the reasoning."}
{"script_processing_qid": "textbook-108-10-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the opportunity cost of computers in terms of cars from the PPF in Figure 2-2 when moving from point A (700 cars, 2,000 computers) to point C (600 cars, 2,200 computers). Interpret the result.\n\nGOLD_ANSWER:\n1.  **Calculation**: Opportunity cost = Δcars / Δcomputers = (700 - 600) / (2,200 - 2,000) = 100 cars / 200 computers = 0.5 cars per computer.\n2.  **Interpretation**: Producing 200 additional computers requires sacrificing 100 cars, implying each computer \"costs\" 0.5 cars at this production level.\n3.  **PPF Shape**: The bowed-out shape indicates increasing opportunity costs (slope changes along the curve).\n\nCANDIDATE_ANSWER:\nStep 1: Identify the changes in production when moving from point A to C.\n- Cars decrease from 700 to 600: ΔCars = 600 - 700 = -100\n- Computers increase from 2,000 to 2,200: ΔComputers = 2,200 - 2,000 = +200\n\nStep 2: Calculate the opportunity cost of computers in terms of cars.\nOpportunity cost = |ΔCars / ΔComputers| = |(-100)/200| = 0.5 cars per computer.\n\nInterpretation: For every additional computer produced, the economy must sacrifice 0.5 cars. This reflects the trade-off dictated by the PPF’s slope, showing the rate at which resources must be reallocated between the two goods.\n\nQID: textbook-108-10-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-108-10-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly calculated the opportunity cost as 0.5 cars per computer and provided a clear interpretation of the trade-off, matching the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-10-1-0", "category": "CORRECT", "explanation": "The candidate correctly calculated the opportunity cost as 0.5 cars per computer and provided a clear interpretation of the trade-off, matching the gold answer."}, "llm_echoed_qid": "textbook-108-10-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly calculated the opportunity cost as 0.5 cars per computer and provided a clear interpretation of the trade-off, matching the gold answer."}
{"script_processing_qid": "textbook-112-3-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the comparative impact on domestic productive labor of allocating capital to home trade versus foreign trade of consumption, based on Smith's analysis.\n\nGOLD_ANSWER:\n1. **Home Trade Impact**: \n   - Replaces two domestic capitals per operation.\n   - Directly supports domestic agriculture/manufacturing twice per cycle.\n   - Higher frequency of operations (up to 12x/year) multiplies labor support.\n   - Formula: $\\text{Labor Support}_{\\text{home}} = 2 \\times \\text{Capital} \\times \\text{Operations/Year}$.\n\n2. **Foreign Trade of Consumption Impact**:\n   - Replaces one domestic and one foreign capital per operation.\n   - Only half the capital supports domestic labor.\n   - Slower returns (1-3 years) reduce operational frequency.\n   - Formula: $\\text{Labor Support}_{\\text{foreign}} = 1 \\times \\text{Capital} \\times \\frac{1}{\\text{Return Time}}$.\n\n3. **Comparative Ratio**: \n   - If home trade operates 12x/year and foreign trade 1x/2 years:\n   $$\\frac{\\text{Labor Support}_{\\text{home}}}{\\text{Labor Support}_{\\text{foreign}}} = \\frac{2 \\times 12}{1 \\times 0.5} = 48:1$$\n\nCANDIDATE_ANSWER:\nStep 1: Analyze Smith's claims about capital allocation outcomes:\n- Home trade replaces *two* domestic capitals supporting domestic productive labor (agriculture/manufactures).\n- Foreign consumption trade replaces *two* capitals, but only *one* is domestic (the other supports foreign industry).\n\nStep 2: Compare multiplier effects:\nHome trade impact = 2 domestic capitals maintained per operation\nForeign consumption trade impact = 1 domestic capital maintained per operation\n\nStep 3: Establish comparative ratio:\nCapital allocated to home trade supports *twice as much domestic productive labor* as an equal capital allocated to foreign consumption trade. This is derived from 2:1 replacement ratio of domestic capitals in the respective trades.\n\nStep 4: Confirm Smith's conclusion:\nThe text explicitly states home trade 'will generally give encouragement and support to a greater quantity of productive labour [...] more than an equal capital employed in the foreign trade of consumption' due to this differential replacement mechanism.\n\nQID: textbook-112-3-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-3-0-0\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the differential impact of capital allocation on domestic labor (2:1 ratio) but misses the operational frequency multiplier (48:1 ratio) and quantitative formulas provided in the gold answer. The reasoning is directionally accurate but lacks full precision.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-3-0-0", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the differential impact of capital allocation on domestic labor (2:1 ratio) but misses the operational frequency multiplier (48:1 ratio) and quantitative formulas provided in the gold answer. The reasoning is directionally accurate but lacks full precision."}, "llm_echoed_qid": "textbook-112-3-0-0", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the differential impact of capital allocation on domestic labor (2:1 ratio) but misses the operational frequency multiplier (48:1 ratio) and quantitative formulas provided in the gold answer. The reasoning is directionally accurate but lacks full precision."}
{"script_processing_qid": "textbook-114-4-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the formula for arc elasticity of demand and explain its significance in measuring elasticity over a range of prices.\n\nGOLD_ANSWER:\n1. **Formula**: Arc elasticity of demand is given by $$E_d = \\frac{\\frac{Q_2 - Q_1}{(Q_2 + Q_1)/2}}{\\frac{P_2 - P_1}{(P_2 + P_1)/2}}$$ where $Q_1$ and $Q_2$ are quantities demanded at prices $P_1$ and $P_2$, respectively.\n2. **Significance**: Arc elasticity measures the average elasticity over a range of prices, providing a more accurate measure when price changes are large.\n\nCANDIDATE_ANSWER:\nStep 1: Define arc elasticity as a measure of elasticity between two distinct points on a demand curve. Unlike point elasticity, it calculates elasticity over a price/quantity range rather than at a single point.\n\nStep 2: Start with the basic elasticity formula: %ΔQ_d / %ΔP. For arc elasticity, percentage changes are calculated using the midpoint method to avoid base-value bias:\n   - %ΔQ_d = [(Q₂ - Q₁) / ((Q₁ + Q₂)/2)] × 100%\n   - %ΔP = [(P₂ - P₁) / ((P₁ + P₂)/2)] × 100%\n\nStep 3: Combine these components to derive the arc elasticity formula:\n   E_arc = [(Q₂ - Q₁)/(P₂ - P₁)] × [(P₁ + P₂)/(Q₁ + Q₂)]\n\nStep 4: Explain significance:\n   (1) Midpoint averaging ensures symmetry: Elasticity is the same whether moving from P₁→P₂ or P₂→P₁.\n   (2) Better suited for measuring elasticity over discrete price intervals compared to point elasticity.\n   (3) Avoids ambiguity in selecting base values for percentage calculations.\n   (4) Particularly useful for policy analysis involving non-marginal price changes.\n\nQID: textbook-114-4-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-4-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the arc elasticity formula using the midpoint method and thoroughly explains its significance, aligning perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-4-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the arc elasticity formula using the midpoint method and thoroughly explains its significance, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-114-4-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the arc elasticity formula using the midpoint method and thoroughly explains its significance, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-16-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain why the cost function $c(y) = y - 1/2$ for $y > 1$ in the example is incorrect and derive the correct piecewise cost function.\n\nGOLD_ANSWER:\n1. **Error**: The proposed cost function ignores the constraint $y_2 \\geq 0$. \\n2. **Correct Approach**: For $y < 1$, produce all output in plant 1: $c(y) = y^2 / 2$. \\n3. For $y > 1$, produce 1 unit in plant 1 and the rest in plant 2: $c(y) = 1/2 + (y - 1)$. \\n4. **Piecewise Function**: $c(y) = \\begin{cases} y^2 / 2 & \\text{if } y \\leq 1 \\\\ y - 1/2 & \\text{if } y > 1 \\end{cases}$.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the contradiction in the given cost function\n- The original cost function c(y) = y - 1/2 for y > 1 implies negative fixed costs (-0.5) when extended to y=0, which is economically impossible as costs cannot be negative.\n\nStep 2: Analyze cost minimization requirements\n- Profit maximization requires cost minimization at the optimal output level (from the provided context). Any valid cost function must represent minimal costs.\n\nStep 3: Derive proper piecewise structure\nFor y ≤ 1:\n- Assume simple production with constant marginal cost = 1\n- Cost function: c(y) = y\n\nFor y > 1:\n- Production requires additional fixed cost + increased marginal cost\n- Let fixed cost = 0.5 (incurred when y > 1)\n- Marginal cost = 1.5 (reflecting constrained production)\n- Cost function: c(y) = 0.5 + 1.5(y - 1)\n\nStep 4: Simplify and verify\n- For y > 1: c(y) = 1.5y - 1\n- This ensures:\n  1) Continuity at y=1: 1.5(1) - 1 = 0.5 matches y=1 cost\n  2) Positive fixed costs\n  3) Increasing marginal cost consistent with production constraints\n\nFinal piecewise cost function:\nc(y) = { y for y ≤ 1;\n         1.5y - 1 for y > 1 }\n\nQID: textbook-16-0-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-16-0-0-3\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer incorrectly derives the cost function. The candidate assumes a constant marginal cost and introduces an incorrect fixed cost and marginal cost for y > 1. The gold answer correctly identifies the error in the original cost function and provides the correct piecewise cost function based on the given constraints.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-16-0-0-3", "category": "INCORRECT", "explanation": "The candidate's answer incorrectly derives the cost function. The candidate assumes a constant marginal cost and introduces an incorrect fixed cost and marginal cost for y > 1. The gold answer correctly identifies the error in the original cost function and provides the correct piecewise cost function based on the given constraints."}, "llm_echoed_qid": "textbook-16-0-0-3", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer incorrectly derives the cost function. The candidate assumes a constant marginal cost and introduces an incorrect fixed cost and marginal cost for y > 1. The gold answer correctly identifies the error in the original cost function and provides the correct piecewise cost function based on the given constraints."}
{"script_processing_qid": "textbook-123-3-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the role of the Markov property in simplifying the likelihood function.\n\nGOLD_ANSWER:\nThe Markov property implies that the future state $x_{t+1}$ depends only on the current state $x_{t}$ and not on the entire history of states. This simplifies the likelihood function to a product of conditional densities $f(x_{t+1}|x_{t})$, reducing the computational complexity of the estimation.\n\nCANDIDATE_ANSWER:\nStep 1: The Markov property states that the conditional distribution of future states depends only on the current state, not on the entire history. Formally, this gives: f(x_{t+1}|x_t, x_{t−1}, ..., x_0) = f(x_{t+1}|x_t).\n\nStep 2: The likelihood function is the joint distribution f(x_T, x_{T−1}, ..., x_0). Using the chain rule of probability, this can be decomposed as f(x_0) ∏_{t=0}^{T−1} f(x_{t+1}|x_t, x_{t−1}, ..., x_0).\n\nStep 3: The Markov property simplifies each conditional term in the product to f(x_{t+1}|x_t). Thus, the likelihood becomes f(x_0) ∏_{t=0}^{T−1} f(x_{t+1}|x_t), which is computationally tractable.\n\nStep 4: With Gaussian shocks (w_t), each conditional distribution f(x_{t+1}|x_t) is explicitly characterized as N(A_o x_t, CC'), allowing closed-form evaluation of the likelihood function.\n\nQID: textbook-123-3-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-3-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains the role of the Markov property in simplifying the likelihood function by reducing it to a product of conditional densities, which aligns with the gold answer. The additional detail about Gaussian shocks further enriches the explanation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-3-0-1", "category": "CORRECT", "explanation": "The candidate answer correctly explains the role of the Markov property in simplifying the likelihood function by reducing it to a product of conditional densities, which aligns with the gold answer. The additional detail about Gaussian shocks further enriches the explanation."}, "llm_echoed_qid": "textbook-123-3-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains the role of the Markov property in simplifying the likelihood function by reducing it to a product of conditional densities, which aligns with the gold answer. The additional detail about Gaussian shocks further enriches the explanation."}
{"script_processing_qid": "textbook-115-12-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Show that the OLS estimator of $\\gamma_{1}$ in the equation $$ y=\\alpha_{0}+\\mathbf{x}\\beta+\\gamma_{1}q_{1}+(v-\\gamma_{1}a_{1}) $$ is inconsistent if $q_{1}$ is correlated with $a_{1}$.\n\nGOLD_ANSWER:\n1. **OLS Estimator**: The OLS estimator for $\\gamma_{1}$ is $$ \\hat{\\gamma}_{1}^{OLS} = \\frac{\\operatorname{Cov}(q_{1}, y)}{\\operatorname{Var}(q_{1})} $$.\n2. **Substitute $y$**: Using the model, $$ \\hat{\\gamma}_{1}^{OLS} = \\gamma_{1} + \\frac{\\operatorname{Cov}(q_{1}, v - \\gamma_{1}a_{1})}{\\operatorname{Var}(q_{1})} $$.\n3. **Bias Term**: Since $q_{1}=\\delta_{0}+\\delta_{1}q+a_{1}$ and $\\operatorname{Cov}(q_{1}, a_{1}) = \\operatorname{Var}(a_{1}) \\neq 0$, the OLS estimator is biased and inconsistent.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the error term in the specified equation. The composite error term is (v - γ₁a₁).\n\nStep 2: Recognize that OLS inconsistency arises when regressors are correlated with the error term. The key issue is whether q₁ is correlated with (v - γ₁a₁).\n\nStep 3: Break down q₁'s composition from the indicator equation: q₁ = δ₀ + δ₁q + a₁. Since a₁ appears in both q₁ and the error term, q₁ inherently contains a component correlated with -γ₁a₁ in the error term.\n\nStep 4: Compute Cov(q₁, error term):\nCov(q₁, v - γ₁a₁) = Cov(δ₁q + a₁, v) - γ₁Cov(δ₁q + a₁, a₁)\n\nStep 5: Use contextual assumptions:\n- Cov(q, v) = 0 (from E(v|x,q) = 0)\n- Cov(a₁, v) = 0 (implied by indicator redundancy)\n- Cov(q, a₁) = 0 (given)\n- Cov(a₁, a₁) = Var(a₁)\n\nThis simplifies to:\nCov(q₁, error) = -γ₁Var(a₁)\n\nStep 6: Since Var(a₁) > 0 and γ₁ ≠ 0, this covariance is non-zero. The OLS estimator plim(γ̂₁_OLS) = γ₁ + [Cov(q₁, error)/Var(q₁)] ≠ γ₁, proving inconsistency.\n\nQID: textbook-115-12-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-12-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the source of inconsistency in the OLS estimator by showing the correlation between q₁ and the error term, and derives the bias term explicitly, matching the gold answer's reasoning and conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-12-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the source of inconsistency in the OLS estimator by showing the correlation between q₁ and the error term, and derives the bias term explicitly, matching the gold answer's reasoning and conclusion."}, "llm_echoed_qid": "textbook-115-12-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the source of inconsistency in the OLS estimator by showing the correlation between q₁ and the error term, and derives the bias term explicitly, matching the gold answer's reasoning and conclusion."}
{"script_processing_qid": "textbook-104-10-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. (Qualitative) Discuss the economic interpretation of the continuous linear functional $\\phi$ in the context of the Second Welfare Theorem. Why might it not always represent a traditional price system?\n\nGOLD_ANSWER:\nThe functional $\\phi$ assigns values to bundles but may lack an inner product representation (e.g., $\\phi(x) = p \\cdot x$). In infinite-dimensional spaces (e.g., $l_{\\infty}$), $\\phi$ may not correspond to a sequence of prices. This occurs when the functional 'puts weight at infinity,' making it economically unintuitive. Additional assumptions (e.g., integrability or discounting) are needed to ensure $\\phi$ behaves like traditional prices.\n\nCANDIDATE_ANSWER:\nStep 1: In the Second Welfare Theorem, the continuous linear functional ϕ acts as a generalized price system that supports the Pareto-optimal allocation by separating preferred consumption/production bundles from equilibrium allocations (via conditions (3)-(4) in Theorem 15.4).\n\nStep 2: However, ϕ operates on an arbitrary normed vector space S (commodity space), which may be infinite-dimensional (e.g., function spaces in dynamic/uncertainty models). Traditional price systems typically assume finite-dimensional commodity spaces where prices correspond to vectors in ℝⁿ.\n\nStep 3: In infinite-dimensional spaces, continuous linear functionals may not have a representation as \"simple\" price vectors. For example, they might involve integrals (e.g., pricing continuous-time consumption streams) or other non-atomic measures, which lack the intuitive interpretation of per-unit prices.\n\nStep 4: Additionally, the Hahn-Banach Theorem guarantees the existence of ϕ but does not ensure strict positivity or local convexity properties required for traditional price systems. For instance, ϕ might assign zero value to economically relevant commodities in some settings.\n\nStep 5: Thus, while ϕ serves a pricing-like role, its mathematical generality in infinite-dimensional spaces means it may not align with standard notions of price systems that assume countable goods and explicit per-unit valuation.\n\nQID: textbook-104-10-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-104-10-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the economic interpretation of the continuous linear functional ϕ in the context of the Second Welfare Theorem, including its role in supporting Pareto-optimal allocations and the challenges it faces in infinite-dimensional spaces. The explanation aligns well with the gold answer, detailing why ϕ might not always represent a traditional price system due to its mathematical generality and lack of intuitive interpretation in certain settings.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-104-10-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the economic interpretation of the continuous linear functional ϕ in the context of the Second Welfare Theorem, including its role in supporting Pareto-optimal allocations and the challenges it faces in infinite-dimensional spaces. The explanation aligns well with the gold answer, detailing why ϕ might not always represent a traditional price system due to its mathematical generality and lack of intuitive interpretation in certain settings."}, "llm_echoed_qid": "textbook-104-10-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the economic interpretation of the continuous linear functional ϕ in the context of the Second Welfare Theorem, including its role in supporting Pareto-optimal allocations and the challenges it faces in infinite-dimensional spaces. The explanation aligns well with the gold answer, detailing why ϕ might not always represent a traditional price system due to its mathematical generality and lack of intuitive interpretation in certain settings."}
{"script_processing_qid": "textbook-112-37-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. **Conceptual**: Explain Adam Smith's argument on why rice lands cannot regulate the rent of other cultivated lands, even in rice-producing countries.\n\nGOLD_ANSWER:\n1.  **Specialization**: Rice fields are bogs, unsuitable for other crops like corn or pasture.\n2.  **Limited Adaptability**: Lands fit for other crops cannot be converted to rice production.\n3.  **Regulation**: Rent of other lands is determined by their own produce (e.g., corn), not rice, due to this incompatibility.\n\nCANDIDATE_ANSWER:\nStep 1: Adam Smith argues that rice lands inherently produce a much greater surplus due to higher yields (two annual crops of 30-60 bushels/acre each) compared to corn or wheat. This surplus allows landlords to claim a larger share of the output as rent. Step 2: However, rice cannot regulate rent for other crops because its unique cultivation requirements (e.g., labor intensity, water management) and role as the staple food in rice-producing countries create a self-contained economic system. Step 3: Non-rice lands grow crops with distinct demand dynamics, labor costs, and yield structures (e.g., wheat, potatoes, or tobacco), meaning their rent is determined by their own profitability thresholds rather than rice's surplus. Step 4: Smith emphasizes that only human food production 'always and necessarily' generates rent, but each food type operates in its own economic context. Thus, rice’s surplus does not serve as a universal benchmark for other land uses, even within the same country.\n\nQID: textbook-112-37-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-37-0-1\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies that rice cannot regulate rent for other crops due to distinct cultivation requirements and economic contexts, but it introduces additional details (e.g., labor intensity, staple food role) not emphasized in the gold answer, which focuses on land specialization and incompatibility.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-37-0-1", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies that rice cannot regulate rent for other crops due to distinct cultivation requirements and economic contexts, but it introduces additional details (e.g., labor intensity, staple food role) not emphasized in the gold answer, which focuses on land specialization and incompatibility."}, "llm_echoed_qid": "textbook-112-37-0-1", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies that rice cannot regulate rent for other crops due to distinct cultivation requirements and economic contexts, but it introduces additional details (e.g., labor intensity, staple food role) not emphasized in the gold answer, which focuses on land specialization and incompatibility."}
{"script_processing_qid": "textbook-110-8-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the implications of Arrow's Theorem for democratic decision-making.\n\nGOLD_ANSWER:\nArrow's Theorem shows that no social welfare function can simultaneously satisfy unrestricted domain, Pareto efficiency, independence of irrelevant alternatives, and non-dictatorship. This implies that democratic systems must compromise on at least one of these criteria, often leading to trade-offs between fairness and decisiveness.\n\nCANDIDATE_ANSWER:\nStep 1: Arrow's Theorem demonstrates that no social welfare function can universally satisfy all desirable fairness criteria (e.g., non-dictatorship, Pareto efficiency, independence of irrelevant alternatives). This implies inherent limitations in democratic systems.\\n\\nStep 2: The concept of a decisive subset \\( K \\) in the provided context shows that if a group is decisive for one pair of alternatives, Lemma C.1 implies they are decisive for all pairs (via equivalence across preference profiles). This aligns with Arrow's conclusion that decisiveness concentrates decisiveness power in a single entity (a dictator) or small group.\\n\\nStep 3: For democratic decision-making, this implies that any rule satisfying basic rationality conditions must either (a) prioritize a subset of individuals (undermining equality) or (b) fail to consistently aggregate preferences (leading to cycles or paradoxes). Democratic systems face a fundamental trade-off between procedural fairness and coherent collective choice.\n\nQID: textbook-110-8-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-110-8-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately summarizes Arrow's Theorem and its implications for democratic decision-making, aligning with the gold answer. It correctly identifies the trade-offs and limitations in democratic systems as outlined by the theorem.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-8-0-3", "category": "CORRECT", "explanation": "The candidate answer accurately summarizes Arrow's Theorem and its implications for democratic decision-making, aligning with the gold answer. It correctly identifies the trade-offs and limitations in democratic systems as outlined by the theorem."}, "llm_echoed_qid": "textbook-110-8-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately summarizes Arrow's Theorem and its implications for democratic decision-making, aligning with the gold answer. It correctly identifies the trade-offs and limitations in democratic systems as outlined by the theorem."}
{"script_processing_qid": "textbook-34-0-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain how the homogeneity of the cost function in input prices w relates to the homogeneity of the input demand correspondence.\n\nGOLD_ANSWER:\n1. **Homogeneity**: The cost function c(w,q) is homogeneous of degree one in w, meaning c(αw,q) = αc(w,q) for α > 0.\n2. **Input Demand**: The input demand z(w,q) is homogeneous of degree zero in w, as z(αw,q) = z(w,q).\n3. **Relation**: The homogeneity of c in w ensures that scaling input prices scales costs proportionally, while input demands remain unchanged due to relative price invariance.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain sufficient information to directly address the homogeneity of the cost function in input prices (w) or its relationship to the homogeneity of the input demand correspondence. The extracted text focuses on homogeneity in output quantity (q), demonstrating that the input demand correspondence z(w, q) is homogeneous of degree 1 in q (i.e., z(w, αq) = αz(w, q)) and that the cost function c(w, q) inherits this homogeneity in q. However, no explicit analysis or results regarding homogeneity in input prices (w) are presented in the given context.\n\nQID: textbook-34-0-2-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-34-0-2-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain sufficient information'.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-34-0-2-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain sufficient information'."}, "llm_echoed_qid": "textbook-34-0-2-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as it explicitly states 'The provided context does not contain sufficient information'."}
{"script_processing_qid": "textbook-106-4-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Critically assess the limitations of using wage-risk tradeoffs to value human life. What biases or confounding factors might arise?\n\nGOLD_ANSWER:\n1. **Selection Bias**: Workers in risky jobs may have higher risk tolerance, skewing VSL estimates.\n2. **Information Asymmetry**: Workers may not fully understand risks, leading to undervaluation.\n3. **Market Imperfections**: Non-competitive labor markets may distort wage premiums.\n4. **Heterogeneity**: VSL varies by income, age, and other factors, complicating aggregate estimates.\n\nCANDIDATE_ANSWER:\nStep 1: **Selection Bias** - Workers in high-risk jobs may self-select based on unobservable traits (e.g., risk tolerance, financial desperation), skewing wage-risk tradeoffs. Their compensation may reflect these traits rather than a pure valuation of life.\n\nStep 2: **Confounding Variables** - Factors like job-specific benefits (health insurance, skill premiums) or non-monetary compensation (flexibility, prestige) may co-vary with risk but are rarely fully controlled for, distorting risk-premium estimates.\n\nStep 3: **Imperfect Risk Perception** - Workers might systematically misperceive actual mortality risks (e.g., underestimating construction risks or overestimating office-job safety), leading to biased wage adjustments.\n\nStep 4: **Labor Market Rigidities** - Limited job mobility, unequal bargaining power, or lack of alternative employment options may force workers to accept risky jobs without adequate compensation, violating the 'voluntary tradeoff' assumption.\n\nStep 5: **Heterogeneity in Risk Valuation** - The methodology assumes uniform preferences, but valuations likely differ across demographics (e.g., age, income, dependents). Aggregating into a single $10M figure ignores this variation.\n\nStep 6: **Ethical Limitations** - Reduces human life to a market transaction, excluding non-instrumental values (e.g., societal grief, moral worth) and potentially undervaluing vulnerable populations with fewer labor-market choices.\n\nQID: textbook-106-4-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-4-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly addresses the limitations and biases of wage-risk tradeoffs, aligning closely with the gold answer while providing additional depth and detail.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-4-1-1", "category": "CORRECT", "explanation": "The candidate answer thoroughly addresses the limitations and biases of wage-risk tradeoffs, aligning closely with the gold answer while providing additional depth and detail."}, "llm_echoed_qid": "textbook-106-4-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly addresses the limitations and biases of wage-risk tradeoffs, aligning closely with the gold answer while providing additional depth and detail."}
{"script_processing_qid": "textbook-114-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the conditions under which private bargaining can lead to an efficient outcome according to the Coase Theorem. What are the key assumptions and why might they fail in practice?\n\nGOLD_ANSWER:\n1. **Coase Theorem Conditions**: \n   - Clearly defined property rights.\n   - Zero transaction costs.\n   - No strategic behavior.\n2. **Efficiency**: Under these conditions, parties will negotiate to an efficient outcome regardless of the initial allocation of property rights.\n3. **Failure in Practice**: \n   - Transaction costs are often high.\n   - Strategic behavior (e.g., holdout problems) can prevent agreement.\n   - Property rights may be unclear or unenforceable.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the Coase Theorem's core conditions. According to the provided context, private bargaining achieves efficiency under three key assumptions: (1) property rights are clearly specified, (2) transaction costs are zero, and (3) no strategic behavior occurs.\\n\\nStep 2: Explain the role of each condition:\\n- Clearly specified property rights ensure all parties know who must compensate whom for externalities (e.g., whether the polluter or affected party holds the right).\\n- Zero transaction costs allow costless negotiation and enforcement of agreements.\\n- No strategic behavior means parties act cooperatively to maximize joint gains rather than withholding information or bargaining unfairly.\\n\\nStep 3: Derive why these conditions enable efficiency. With these assumptions, affected parties will negotiate until the externality’s marginal social cost equals its marginal benefit (e.g., agreeing to reduce pollution to the socially optimal level).\\n\\nStep 4: Analyze why assumptions fail in practice:\\n- Transaction costs are rarely zero (e.g., legal fees, time delays).\\n- Strategic behavior is common (e.g., parties misrepresent costs/benefits to gain bargaining advantage).\\n- Property rights may be unclear or costly to enforce (e.g., disputes over liability for pollution).\\n\\nConclusion: While the Coase Theorem provides a theoretical benchmark, its assumptions are typically violated in real-world scenarios, leading to inefficient bargaining outcomes.\n\nQID: textbook-114-0-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately identifies and explains the core conditions of the Coase Theorem, including clearly defined property rights, zero transaction costs, and no strategic behavior. It also correctly derives why these conditions enable efficiency and thoroughly analyzes why these assumptions often fail in practice, aligning perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-0-0-0", "category": "CORRECT", "explanation": "The candidate answer accurately identifies and explains the core conditions of the Coase Theorem, including clearly defined property rights, zero transaction costs, and no strategic behavior. It also correctly derives why these conditions enable efficiency and thoroughly analyzes why these assumptions often fail in practice, aligning perfectly with the gold answer."}, "llm_echoed_qid": "textbook-114-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately identifies and explains the core conditions of the Coase Theorem, including clearly defined property rights, zero transaction costs, and no strategic behavior. It also correctly derives why these conditions enable efficiency and thoroughly analyzes why these assumptions often fail in practice, aligning perfectly with the gold answer."}
{"script_processing_qid": "textbook-122-24-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain the concept of unbiasedness of OLS estimators $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ under Assumptions SLR.1-SLR.4.\n\nGOLD_ANSWER:\n1. **Assumptions**:\n   - SLR.1: Linear in parameters.\n   - SLR.2: Random sampling.\n   - SLR.3: Zero conditional mean ($E(u|x) = 0$).\n   - SLR.4: Sample variation in $x$.\n2. **Unbiasedness**: $E(\\hat{\\beta}_1) = \\beta_1$ and $E(\\hat{\\beta}_0) = \\beta_0$.\n3. **Proof**:\n   - For $\\hat{\\beta}_1$: $E(\\hat{\\beta}_1) = \\beta_1 + E\\left[\\frac{\\sum_{i=1}^n (x_i - \\bar{x}) u_i}{s_x^2}\\right] = \\beta_1$ (since $E(u_i|x_i) = 0$).\n   - For $\\hat{\\beta}_0$: $E(\\hat{\\beta}_0) = \\beta_0 + E[(\\beta_1 - \\hat{\\beta}_1)\\bar{x}] + E(\\bar{u}) = \\beta_0$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the key assumptions required for OLS unbiasedness. Assumptions SLR.1-SLR.4 include:\n- SLR.1 (Linear parameters): The population model is linear in parameters (as shown in the constant elasticity model).\n- SLR.2 (Random sampling): The data is a random sample from the population.\n- SLR.3 (Zero conditional mean): E(u|x) = 0, meaning no omitted variables correlate with x.\n- SLR.4 (Variation in x): The sample outcomes of x are not all the same value.\n\nStep 2: Recognize that under these assumptions, the OLS estimators β̂₀ and β̂₁ are unbiased. This means E[β̂₀] = β₀ and E[β̂₁] = β₁ in repeated sampling.\n\nStep 3: Apply to the context. The log-log model log(salary) = β₀ + β₁ log(sales) + u satisfies SLR.1 through its linear parameter structure. The estimation yields β̂₁ = 0.257, which would equal the true population elasticity β₁ on average if SLR.1-SLR.4 hold.\n\nStep 4: Note that SLR.3 (zero conditional mean) is critical for unbiasedness. In this example, it requires that firm sales (log(sales)) are unrelated to other factors affecting CEO salary contained in u. If this assumption fails, β̂₁ would be biased.\n\nQID: textbook-122-24-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-24-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies and explains the assumptions SLR.1-SLR.4 and the concept of unbiasedness for OLS estimators. The explanation aligns with the gold answer, including the critical role of SLR.3 in ensuring unbiasedness.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-24-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies and explains the assumptions SLR.1-SLR.4 and the concept of unbiasedness for OLS estimators. The explanation aligns with the gold answer, including the critical role of SLR.3 in ensuring unbiasedness."}, "llm_echoed_qid": "textbook-122-24-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies and explains the assumptions SLR.1-SLR.4 and the concept of unbiasedness for OLS estimators. The explanation aligns with the gold answer, including the critical role of SLR.3 in ensuring unbiasedness."}
{"script_processing_qid": "textbook-107-3-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Using the text's example, show how a price change affecting inferior goods complicates welfare analysis.\n\nGOLD_ANSWER:\nFor inferior goods, a price decrease may reduce consumption (Giffen effect). The text's condition—where potatoes (a Giffen good) are consumed more despite a price drop—violates standard income/substitution effects, requiring SARP to ensure consistent preferences.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the Laspeyres and Paasche indices. The Laspeyres index uses period 1 quantities (base period) to compare costs, while the Paasche index uses period 2 quantities (current period).\nStep 2: Inferior goods exhibit an income effect where demand increases when purchasing power decreases. If the price of an inferior good rises, consumers may paradoxically buy more of it due to reduced real income.\nStep 3: Welfare analysis via Laspeyres assumes consumption patterns remain fixed at period 1 levels. For an inferior good, actual period 2 consumption might increase (contrary to substitution effects), making the Laspeyres index overstate the required compensation (since it ignores increased consumption of the now-costlier inferior good).\nStep 4: Conversely, the Paasche index uses period 2 quantities, which now include higher consumption of the inferior good. This could understate welfare loss because the index incorporates behavioral adjustments that worsen the consumer’s position (spending more on a good with negative welfare implications).\nStep 5: The divergence between Laspeyres (overestimation of compensation needs) and Paasche (underestimation due to adverse adjustments) complicates welfare analysis, as neither index accurately reflects the true welfare change for inferior goods.\n\nQID: textbook-107-3-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-3-1-3\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate answer provides a detailed analysis of welfare indices and inferior goods but deviates from the GOLD_ANSWER's focus on the Giffen effect and SARP. While relevant, it does not directly address the text's specific example or the violation of standard effects mentioned in the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-3-1-3", "category": "PARTIALLY_CORRECT", "explanation": "The candidate answer provides a detailed analysis of welfare indices and inferior goods but deviates from the GOLD_ANSWER's focus on the Giffen effect and SARP. While relevant, it does not directly address the text's specific example or the violation of standard effects mentioned in the gold answer."}, "llm_echoed_qid": "textbook-107-3-1-3", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate answer provides a detailed analysis of welfare indices and inferior goods but deviates from the GOLD_ANSWER's focus on the Giffen effect and SARP. While relevant, it does not directly address the text's specific example or the violation of standard effects mentioned in the gold answer."}
{"script_processing_qid": "textbook-38-0-3-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the significance of the boundedness assumption on sequences in M for the existence of V(·).\n\nGOLD_ANSWER:\n4. **Boundedness Significance**: Boundedness ensures that the limit defining V(c) exists and is finite. Without boundedness, the utility sums or limits might diverge, making V(·) ill-defined.\n\nCANDIDATE_ANSWER:\nThe boundedness assumption on sequences in M is crucial for ensuring the existence of V(·) through the following steps:\n\nStep 1: **Convergence in Extension to M**: When extending V(·) to the entire space M, the proof constructs a sequence {V(hₜ(c))} where hₜ(c) truncates c after t periods. Boundedness of c ensures supₜ cₜ exists, allowing the creation of a stationary sequence (c*, c*, ...) ∈ M₀ that upper-bounds hₜ(c). This guarantees {V(hₜ(c))} is non-decreasing and bounded above, ensuring convergence.\n\nStep 2: **Domain Validity for f(V)**: The function f(V) = V - δV^α used to define V(·) on M₀ requires V ∈ [σ^{1/(1−α)}, ∞) for invertibility. Boundedness of sequences ensures V values remain in this domain during recursive aggregation via G(·), preserving strict monotonicity and surjectivity of f(·).\n\nStep 3: **Monotonicity Preservation**: Boundedness ensures coordinatewise comparisons (c ≥ c') remain well-defined, allowing monotonicity of u(·) and V(·) on M₀ to extend to all of M. Without boundedness, utilities could diverge, breaking monotonicity.\n\nThus, the boundedness assumption enables convergence, domain consistency, and monotonicity preservation, all essential for constructing V(·) recursively across M.\n\nQID: textbook-38-0-3-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-38-0-3-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the significance of boundedness in ensuring the existence of V(·), covering convergence, domain validity, and monotonicity preservation, which aligns with the gold answer's focus on preventing divergence and ensuring well-defined limits.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-38-0-3-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the significance of boundedness in ensuring the existence of V(·), covering convergence, domain validity, and monotonicity preservation, which aligns with the gold answer's focus on preventing divergence and ensuring well-defined limits."}, "llm_echoed_qid": "textbook-38-0-3-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the significance of boundedness in ensuring the existence of V(·), covering convergence, domain validity, and monotonicity preservation, which aligns with the gold answer's focus on preventing divergence and ensuring well-defined limits."}
{"script_processing_qid": "textbook-122-5-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the quasi-differenced transformation for the AR(1) model $u_{t}=\\rho u_{t-1}+e_{t}$ and explain how it eliminates serial correlation.\n\nGOLD_ANSWER:\n1. **Quasi-differencing**: For $t\\geq2$, subtract $\\rho y_{t-1}$ from $y_{t}$ to obtain $\\tilde{y}_{t}=y_{t}-\\rho y_{t-1}$ and $\\tilde{x}_{t}=x_{t}-\\rho x_{t-1}$.\n2. **Transformed equation**: $\\tilde{y}_{t}=(1-\\rho)\\beta_{0}+\\beta_{1}\\tilde{x}_{t}+e_{t}$, where $e_{t}=u_{t}-\\rho u_{t-1}$ is serially uncorrelated.\n3. **First observation**: For $t=1$, use $\\tilde{y}_{1}=(1-\\rho^{2})^{1/2}y_{1}$ and $\\tilde{x}_{1}=(1-\\rho^{2})^{1/2}x_{1}$ to ensure homoskedasticity.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the original AR(1) error model:\n\nyₜ = β₀ + β₁xₜ₁ + ... + βₖxₜₖ + uₜ\n\nuₜ = ρuₜ₋₁ + eₜ (where eₜ is white noise)\n\nStep 2: For t ≥ 2, multiply the t-1 period equation by ρ:\nρyₜ₋₁ = ρβ₀ + ρβ₁xₜ₋₁₁ + ... + ρβₖxₜ₋₁ₖ + ρuₜ₋₁\n\nStep 3: Subtract this from the original equation:\nyₜ - ρyₜ₋₁ = β₀(1 − ρ) + β₁(xₜ₁ − ρxₜ₋₁₁) + ... + βₖ(xₜₖ − ρxₜ₋₁ₖ) + (uₜ − ρuₜ₋₁)\n\nStep 4: Recognize that the error term becomes eₜ by the AR(1) structure:\nuₜₚ − ρuₜ₋₁ = eₜ\n\nThis gives the transformed equation for t ≥ 2:\nỹₜ = β₀(1 − ρ) + β₁x̃ₜ₁ + ... + βₖx̃ₜₖ + eₜ\nwhere ỹₜ = yₜ − ρyₜ₋₁ and x̃ₜⱼ = xₜⱼ − ρxₜ₋₁ⱼ\n\nStep 5: For t=1, use a different transformation to preserve homoskedasticity:\nỹ₁ = √(1 − ρ²)y₁\nx̃₁ⱼ = √(1 − ρ²)x₁ⱼ\n\nKey Elimination of Serial Correlation:\nThe transformed error term eₜ is now white noise (i.i.d. with zero autocorrelation). By explicitly removing the ρuₜ₋₁ component through quasi-differencing, we eliminate the AR(1) dependence in errors. This makes OLS on the transformed equation (GLS) efficient and valid for inference.\n\nQID: textbook-122-5-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-5-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the quasi-differenced transformation for the AR(1) model, including the special case for the first observation, and accurately explains how it eliminates serial correlation by transforming the error term into white noise.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-5-1-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the quasi-differenced transformation for the AR(1) model, including the special case for the first observation, and accurately explains how it eliminates serial correlation by transforming the error term into white noise."}, "llm_echoed_qid": "textbook-122-5-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the quasi-differenced transformation for the AR(1) model, including the special case for the first observation, and accurately explains how it eliminates serial correlation by transforming the error term into white noise."}
{"script_processing_qid": "textbook-106-8-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain the self-correcting mechanism of the market when there is a surplus, as described in Figure 4-9.\n\nGOLD_ANSWER:\n1.  **Surplus condition**: Quantity supplied > Quantity demanded at prices above equilibrium.\n2.  **Seller response**: Sellers lower prices to reduce excess inventory.\n3.  **Adjustment**: Prices fall until equilibrium is restored, eliminating the surplus.\n\nCANDIDATE_ANSWER:\nStep 1: Identify surplus occurrence - A surplus occurs when the market price is above the equilibrium price ($2.00 in the ice-cream example), leading to quantity supplied exceeding quantity demanded.\\nStep 2: Seller response to surplus - Suppliers unable to sell all goods at the current price will lower prices to reduce unsold inventory.\\nStep 3: Price adjustment effects - Lower prices simultaneously (a) decrease quantity supplied (as producers become less willing to sell at lower prices) and (b) increase quantity demanded (as buyers become more willing to purchase at lower prices).\\nStep 4: Equilibrium restoration - This price adjustment continues until the market reaches equilibrium where quantity supplied = quantity demanded ($2.00 price and 7-unit quantity in the example).\\nStep 5: Market balance - The equilibrium price naturally eliminates the surplus by coordinating buyer and seller behavior through price signals.\n\nQID: textbook-106-8-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-8-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately describes the self-correcting mechanism of the market when there is a surplus, including the identification of surplus, seller response, price adjustment effects, and equilibrium restoration, matching the gold answer's key points.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-8-1-2", "category": "CORRECT", "explanation": "The candidate answer accurately describes the self-correcting mechanism of the market when there is a surplus, including the identification of surplus, seller response, price adjustment effects, and equilibrium restoration, matching the gold answer's key points."}, "llm_echoed_qid": "textbook-106-8-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately describes the self-correcting mechanism of the market when there is a surplus, including the identification of surplus, seller response, price adjustment effects, and equilibrium restoration, matching the gold answer's key points."}
{"script_processing_qid": "textbook-62-4-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the growth rates and inflation rates from the old and new methods in Table 6 and interpret the differences.\n\nGOLD_ANSWER:\n1. **Observation**: Old method shows higher real GDP growth (9.0% vs. 7.5% in 1993) and lower inflation rates (-9.6% vs. -8.4%).\n2. **Reason**: Old method overweights computers, exaggerating quantity growth and understating price declines.\n3. **Implication**: Chain-weighting provides a more balanced measure of economic activity and price changes.\n\nCANDIDATE_ANSWER:\nThe provided context does not contain the actual data from Table 6 showing growth rates and inflation rates calculated using old (fixed-base-year) and new (chain-weighted) methods. Without access to the numerical values in Table 6, it is impossible to perform a comparison or interpret specific differences between the two methods' results. The context only provides the theoretical framework for chain-weighted GDP calculation (using Laspeyres and Paasche indices) but does not include empirical results or data tables required for this comparison.\n\nQID: textbook-62-4-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-62-4-1-3\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the absence of necessary data in Table 6.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-62-4-1-3", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the absence of necessary data in Table 6."}, "llm_echoed_qid": "textbook-62-4-1-3", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, as the candidate correctly points out the absence of necessary data in Table 6."}
{"script_processing_qid": "textbook-112-30-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Evaluate Adam Smith's argument on the monopoly's effect on land improvement and rent.\n\nGOLD_ANSWER:\n1.  **Profit Rate Impact**: Higher mercantile profit rates discourage land improvement by making it less attractive relative to trade.\n2.  **Rent Dynamics**: Reduced improvement retards rent increases and land price appreciation, hurting landlords.\n3.  **Interest Rates**: Monopoly raises interest rates, further depressing land prices relative to rent.\n\nCANDIDATE_ANSWER:\nStep 1: Identify relevant arguments in the provided context. The extracted passages focus on how monopolies in colony trade (a) raise profit rates by excluding competition, (b) divert capital and produce from existing markets, (c) suppress national revenue growth, and (d) weaken capital accumulation by undermining merchant parsimony.\n\nStep 2: Assess applicability to land improvement and rent. The context does not explicitly mention land improvement, agricultural productivity, or rent dynamics. Smith’s arguments here are limited to general effects on capital allocation, profit rates, and national revenue – not land-specific impacts.\n\nStep 3: Conclusion. The provided context lacks direct evidence of Smith’s arguments about monopolies’ effects on land improvement or rent. The analysis exclusively concerns trade monopolies’ broader economic consequences rather than agrarian economics.\n\nQID: textbook-112-30-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-30-0-3\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate answer incorrectly concludes that the context lacks evidence of Smith's arguments on land improvement and rent, despite the GOLD_ANSWER clearly outlining specific points on this topic. The candidate's analysis misses the direct connections made in the GOLD_ANSWER.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-30-0-3", "category": "INCORRECT", "explanation": "The candidate answer incorrectly concludes that the context lacks evidence of Smith's arguments on land improvement and rent, despite the GOLD_ANSWER clearly outlining specific points on this topic. The candidate's analysis misses the direct connections made in the GOLD_ANSWER."}, "llm_echoed_qid": "textbook-112-30-0-3", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate answer incorrectly concludes that the context lacks evidence of Smith's arguments on land improvement and rent, despite the GOLD_ANSWER clearly outlining specific points on this topic. The candidate's analysis misses the direct connections made in the GOLD_ANSWER."}
{"script_processing_qid": "textbook-122-10-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the one-sided p-value for testing $\\mathrm{H}_{0}: \\beta_{j} = 0$ against $\\mathrm{H}_{1}: \\beta_{j} > 0$ given a two-sided p-value of 0.086.\n\nGOLD_ANSWER:\n1. **Derivation**: For a one-sided alternative, the p-value is half of the two-sided p-value: $$ p\\mathrm{-value}_{\\text{one-sided}} = \\frac{0.086}{2} = 0.043. $$\n2. **Interpretation**: At the 5% significance level, we reject $\\mathrm{H}_{0}$ in favor of $\\mathrm{H}_{1}: \\beta_{j} > 0$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall from the context that for one-sided alternatives with a positive t-statistic (β̂ⱼ > 0), the one-sided p-value equals half of the two-sided p-value.\nStep 2: Given the two-sided p-value is 0.086, divide this value by 2.\nStep 3: One-sided p-value = 0.086 / 2 = 0.043.\nConclusion: The one-sided p-value for testing H₀: βⱼ = 0 against H₁: βⱼ > 0 is 0.043.\n\nQID: textbook-122-10-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-10-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the one-sided p-value as half of the two-sided p-value, resulting in 0.043, which matches the gold answer. The reasoning and steps are also accurate and well-explained.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-10-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the one-sided p-value as half of the two-sided p-value, resulting in 0.043, which matches the gold answer. The reasoning and steps are also accurate and well-explained."}, "llm_echoed_qid": "textbook-122-10-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the one-sided p-value as half of the two-sided p-value, resulting in 0.043, which matches the gold answer. The reasoning and steps are also accurate and well-explained."}
{"script_processing_qid": "textbook-114-3-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain the concept of a maximin strategy using the example from Table 13.4. Why might a player choose this strategy?\n\nGOLD_ANSWER:\n1. **Maximin Strategy**: A player chooses the strategy that maximizes their minimum possible payoff. In Table 13.4, Player 1's maximin strategy is 'Top' because it guarantees a minimum payoff of 1, avoiding the risk of losing 1000.\n2. **Rationale**: Players may choose maximin strategies due to uncertainty about opponents' rationality or information, prioritizing risk aversion over potential higher payoffs.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the maximin strategy definition. A maximin strategy involves selecting the option that maximizes the player’s minimum possible payoff (i.e., it focuses on avoiding the worst-case scenario rather than optimizing potential gains).\n\nStep 2: Hypothesize Table 13.4’s structure. While the table is not provided, standard maximin examples often involve a payoff matrix where players face asymmetric risks. Assume Player A has two strategies (e.g., Strategy 1 and Strategy 2) with payoffs dependent on Player B’s actions.\n\nStep 3: Apply maximin logic. For each of Player A’s strategies, identify the worst-case payoff:\n- Strategy 1: Minimum payoff = $2\n- Strategy 2: Minimum payoff = $1\n\nStep 4: Choose the strategy with the highest minimum payoff. Here, Player A would select Strategy 1 because $2 > $1.\n\nWhy choose this strategy?\n- Risk aversion: Players prioritize avoiding catastrophic losses over maximizing potential gains.\n- Uncertainty: When players lack information about opponents’ choices, maximin safeguards against exploitation.\n- Stability: Ensures a guaranteed minimum payoff, aligning with the conservative nature described in the context.\n\nQID: textbook-114-3-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-3-0-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly explains the concept of a maximin strategy and provides a plausible example, but the example does not match the specific context of Table 13.4 mentioned in the gold answer. The rationale for choosing a maximin strategy is well-articulated and aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-3-0-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly explains the concept of a maximin strategy and provides a plausible example, but the example does not match the specific context of Table 13.4 mentioned in the gold answer. The rationale for choosing a maximin strategy is well-articulated and aligns with the gold answer."}, "llm_echoed_qid": "textbook-114-3-0-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly explains the concept of a maximin strategy and provides a plausible example, but the example does not match the specific context of Table 13.4 mentioned in the gold answer. The rationale for choosing a maximin strategy is well-articulated and aligns with the gold answer."}
{"script_processing_qid": "textbook-118-7-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Prove that adding an additional regressor to a linear regression model will always increase the centered $R^2$ ($R_c^2$).\n\nGOLD_ANSWER:\n1. **ESS and TSS**: The centered $R^2$ is defined as $R_c^2 = 1 - \\frac{SSR}{TSS}$.\\n2. **Adding Regressors**: Adding a regressor reduces $SSR$ (or leaves it unchanged) while $TSS$ remains the same. Thus, $R_c^2$ increases (or stays the same).\n\nCANDIDATE_ANSWER:\nStep 1: Recall the formula for centered R² (R_c²). It is defined as R_c² = 1 - (SSR/SST), where SSR is the sum of squared residuals and SST is the total sum of squares (centered around the mean of y).\n\nStep 2: When adding a new regressor, the OLS estimator minimizes SSR. The expanded model (with the additional regressor) will have an SSR that is less than or equal to the original model's SSR. This is because the OLS solution for the expanded model includes the possibility of setting the new coefficient to zero (yielding the original SSR), but generally achieves a lower SSR unless the new regressor is perfectly collinear with existing ones.\n\nStep 3: SST remains unchanged because it depends only on the variance of the dependent variable y, which is unaffected by regressor additions.\n\nStep 4: Since R_c² = 1 - (SSR/SST), and SSR decreases (or stays the same) while SST is constant, R_c² must increase (or stay the same). However, if the new regressor is not perfectly collinear with existing variables (a standard assumption in regression), SSR strictly decreases, ensuring R_c² strictly increases.\n\nConclusion: Adding an additional regressor always increases R_c² because it cannot raise SSR, and the non-collinear case guarantees a strict SSR reduction.\n\nQID: textbook-118-7-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-7-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly explains the reasoning behind why adding an additional regressor to a linear regression model will always increase the centered $R^2$. The steps provided align with the gold answer, detailing how SSR decreases (or stays the same) while SST remains unchanged, leading to an increase in $R_c^2$.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-7-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly explains the reasoning behind why adding an additional regressor to a linear regression model will always increase the centered $R^2$. The steps provided align with the gold answer, detailing how SSR decreases (or stays the same) while SST remains unchanged, leading to an increase in $R_c^2$."}, "llm_echoed_qid": "textbook-118-7-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly explains the reasoning behind why adding an additional regressor to a linear regression model will always increase the centered $R^2$. The steps provided align with the gold answer, detailing how SSR decreases (or stays the same) while SST remains unchanged, leading to an increase in $R_c^2$."}
{"script_processing_qid": "textbook-118-8-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. How does the URF facilitate consistent estimation when instruments are weak?\n\nGOLD_ANSWER:\n1. **Direct estimation**: The URF can be estimated via OLS or NLS, avoiding weak-instrument bias in the first stage.\n2. **No identification requirement**: The URF does not require $\\pmb{\\Pi}_{21}$ to have full rank, making it robust to weak instruments.\n3. **Limitation**: URF estimates lack structural interpretation, complicating policy analysis.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the key difference between RRF and URF\n- RRF (Y = WBΓ⁻¹ + V) inherits all exclusion restrictions from the structural form\n- URF (Y = WΠ + V) ignores these restrictions, estimating Π without constraints\n\nStep 2: Recognize weak instrument implications\n- Weak instruments lead to poor identification of structural parameters (Γ,B)\n- RRF estimation relies on correctly specified exclusion restrictions\n\nStep 3: Explain URF's advantage under weak instruments\n- URF avoids imposing potentially invalid/misspecified structural restrictions\n- Directly estimates reduced-form parameters Π through standard methods (e.g., OLS)\n- Requires only instrument exogeneity (not instrument relevance) for consistency\n\nStep 4: Conclusion\nBy removing dependence on structural restrictions (which become problematic when identification is weak through WBΓ⁻¹ terms), the URF maintains consistent estimation of reduced-form relationships regardless of instrument strength.\n\nQID: textbook-118-8-2-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-8-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains how the URF facilitates consistent estimation under weak instruments by avoiding structural restrictions and directly estimating reduced-form parameters. This aligns with the gold answer's points about direct estimation and no identification requirement.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-8-2-3", "category": "CORRECT", "explanation": "The candidate answer correctly explains how the URF facilitates consistent estimation under weak instruments by avoiding structural restrictions and directly estimating reduced-form parameters. This aligns with the gold answer's points about direct estimation and no identification requirement."}, "llm_echoed_qid": "textbook-118-8-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains how the URF facilitates consistent estimation under weak instruments by avoiding structural restrictions and directly estimating reduced-form parameters. This aligns with the gold answer's points about direct estimation and no identification requirement."}
{"script_processing_qid": "textbook-107-5-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Draw indifference curves for Marvin and Mavis, indicating the direction of greater preference.\n\nGOLD_ANSWER:\n1. **Marvin**: Indifference curves slope upward and to the right. The arrow points downward and to the left.\n2. **Mavis**: Indifference curves are horizontal lines. The arrow points downward.\n\nCANDIDATE_ANSWER:\nStep 1: Analyze Marvin's preferences. He loves pepperoni (good) and hates anchovy (bad). In a pepperoni-anchovy graph:\n- Indifference curves slope upward because increased anchovy consumption requires additional pepperoni to maintain indifference.\n- Direction of greater preference: Southeast (more pepperoni/less anchovy).\n\nStep 2: Analyze Mavis' preferences. She is indifferent to pepperoni (neutral) and hates anchovy (bad):\n- Indifference curves are horizontal lines (pepperoni quantity doesn't affect utility).\n- Direction of greater preference: Downward along anchovy axis (less anchovy = better).\n\nQID: textbook-107-5-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-5-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately describes the indifference curves and direction of greater preference for both Marvin and Mavis, matching the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-5-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately describes the indifference curves and direction of greater preference for both Marvin and Mavis, matching the gold answer."}, "llm_echoed_qid": "textbook-107-5-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately describes the indifference curves and direction of greater preference for both Marvin and Mavis, matching the gold answer."}
{"script_processing_qid": "textbook-122-7-0-5", "llm_grader_input_prompt_user": "\nQUESTION:\n6. Using the labor force participation LPM, derive the marginal effect of experience ($exper$) on the probability of participation, given the quadratic term $exper^2$.\n\nGOLD_ANSWER:\n1. **Marginal Effect**: The model includes $exper$ and $exper^2$. The marginal effect is: $$\\frac{\\partial \\mathrm{P}(inlf=1|\\pmb{x})}{\\partial exper} = 0.039 - 2(0.0006)exper = 0.039 - 0.0012 \\cdot exper.$$\n2. **Interpretation**: The effect diminishes as $exper$ increases (e.g., at $exper = 10$, the marginal effect is $0.039 - 0.012 = 0.027$).\n\nCANDIDATE_ANSWER:\nStep 1: Identify the LPM equation components related to experience\nFrom the estimated equation:\ninlf = ... + 0.039 exper - 0.0012 exper² + ...\n\nStep 2: Compute the marginal effect using calculus\nIn models with quadratic terms, the marginal effect is the first derivative of the probability equation with respect to 'exper':\n∂P(y=1|x)/∂exper = β_exper + 2β_exper² * exper\n\nStep 3: Substitute coefficients from the context\nβ_exper = 0.039\nβ_exper² = -0.0012\n\nStep 4: Calculate the marginal effect formula\nMarginal Effect = 0.039 + 2*(-0.0012)*exper\n= 0.039 - 0.0024*exper\n\nThis shows the marginal effect of experience depends on its own level - each additional year of experience reduces its own marginal impact by 0.0024 percentage points due to the quadratic term.\n\nQID: textbook-122-7-0-5\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-7-0-5\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's final marginal effect formula (0.039 - 0.0024*exper) differs from the gold answer (0.039 - 0.0012*exper) due to a calculation error in Step 4. However, the reasoning process and understanding of how to derive the marginal effect in a model with quadratic terms is correct.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-7-0-5", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's final marginal effect formula (0.039 - 0.0024*exper) differs from the gold answer (0.039 - 0.0012*exper) due to a calculation error in Step 4. However, the reasoning process and understanding of how to derive the marginal effect in a model with quadratic terms is correct."}, "llm_echoed_qid": "textbook-122-7-0-5", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's final marginal effect formula (0.039 - 0.0024*exper) differs from the gold answer (0.039 - 0.0012*exper) due to a calculation error in Step 4. However, the reasoning process and understanding of how to derive the marginal effect in a model with quadratic terms is correct."}
{"script_processing_qid": "textbook-118-5-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Derive the asymptotic covariance matrix of the MSM estimator for the lognormal distribution parameters $\\mu$ and $\\sigma^{2}$, accounting for simulation randomness.\n\nGOLD_ANSWER:\n1.  **Covariance matrix**: The asymptotic covariance matrix of the MSM estimator is given by $$\\widehat{\\operatorname{Var}}\\left[\\begin{array}{l}{\\hat{\\mu}}\\\\{\\hat{\\sigma}^{2}}\\end{array}\\right]=(\\boldsymbol{W}^{\\top}\\hat{\\boldsymbol{F}})^{-1}\\boldsymbol{W}^{\\top}\\hat{\\boldsymbol{\\Omega}}\\boldsymbol{W}(\\hat{\\boldsymbol{F}}^{\\top}\\boldsymbol{W})^{-1},$$ where $\\boldsymbol{W}$ is the instrument matrix, $\\hat{\\boldsymbol{F}}$ is the matrix of partial derivatives, and $\\hat{\\boldsymbol{\\Omega}}$ is the estimated covariance matrix of the zero functions.\\n2.  **Simulation adjustment**: The covariance matrix must be multiplied by $1+1/S$ to account for the simulation randomness.\\n3.  **Estimation**: The matrix $\\hat{\\boldsymbol{\\Omega}}$ is estimated using sample analogs of $\\sigma_{z}^{2}$, $\\sigma_{y}^{2}$, and $\\sigma_{z y}$, ensuring that both data and simulation randomness are accounted for.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the moment conditions and parameter mapping. The estimators are defined as μ_hat = z̄ (sample mean of logs) and σ²_hat = 2(logȳ - z̄). This implies two moment conditions: (1) E[z] = μ and (2) E[logȳ] = μ + σ²/2.\n\nStep 2: Compute covariance matrix of moments. Let m = [z̄, logȳ]'. The asymptotic covariance matrix Σ_m has:\n- Var(z̄) = σ²/n\n- Var(logȳ) = (e^{σ²}-1)/n (via delta method)\n- Cov(z̄, logȳ) = σ²/n\n\nStep 3: Apply delta method. Parameters θ = (μ, σ²)' relate to moments via:\nθ = g(m) = [m₁, 2(m₂-m₁)]'\nJacobian matrix G = ∂g/∂m = [[1,0],[-2,2]]\n\nStep 4: Compute asymptotic covariance without simulation:\nCov(θ) = GΣ_mG' = (1/n)[[σ²,0],[0,4(e^{σ²}-σ²-1)]]\n\nStep 5: Account for simulation randomness. Multiply by (1+1/S) factor where S = number of simulations:\nFinal Cov(θ) = (1+1/S)(1/n)[[σ²,0],[0,4(e^{σ²}-σ²-1)]]\n\nQID: textbook-118-5-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-5-1-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's final answer for the asymptotic covariance matrix is correct, including the simulation adjustment factor. However, the derivation process differs significantly from the gold answer's approach, which uses an instrument matrix and zero functions rather than moment conditions and the delta method.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-5-1-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's final answer for the asymptotic covariance matrix is correct, including the simulation adjustment factor. However, the derivation process differs significantly from the gold answer's approach, which uses an instrument matrix and zero functions rather than moment conditions and the delta method."}, "llm_echoed_qid": "textbook-118-5-1-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's final answer for the asymptotic covariance matrix is correct, including the simulation adjustment factor. However, the derivation process differs significantly from the gold answer's approach, which uses an instrument matrix and zero functions rather than moment conditions and the delta method."}
{"script_processing_qid": "textbook-115-9-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the implications of heteroskedasticity and nonnormality in the latent variable model for the consistency of the Tobit estimator and the interpretation of partial effects.\n\nGOLD_ANSWER:\n1.  **Inconsistency**: The Tobit estimator $\\hat{\\pmb\\beta}$ is inconsistent for $\\beta$ under heteroskedasticity or nonnormality because the derived density of $y$ given $\\mathbf{X}$ relies on $y^{\\*}\\left|\\mathbf{x}\\sim\\mathrm{Normal}(\\mathbf{x}{\\pmb{\\beta}},\\sigma^{2})\\right.$.\\n2.  **Partial Effects**: Heteroskedasticity or nonnormality changes the functional forms for $\\mathrm{E}(y|{\\bf x},y>0)$ and $\\operatorname{E}(y\\mid\\mathbf{x})$, making the Tobit partial effects invalid.\\n3.  **Robustness**: Without censoring, $\\beta$ could be estimated consistently under $\\operatorname{E}(u\\mid\\mathbf{x})=0$, but censoring complicates this.\n\nCANDIDATE_ANSWER:\nStep 1: **Consistency of the Tobit Estimator**\n- The standard Tobit MLE requires normality and homoskedasticity of the latent error term \\( u_1 \\). Under these assumptions, the estimator is consistent.\n- Heteroskedasticity or nonnormality in \\( u_1 \\) violates these assumptions, leading to inconsistent parameter estimates. The likelihood function is misspecified, biasing estimates of \\( \\delta_1 \\), \\( \\alpha_1 \\), and \\( \tau_1^2 \\).\n\nStep 2: **Partial Effects Interpretation**\n- Partial effects in Tobit models depend on the conditional expectation \\( E(y_1^* \\mid \\mathbf{z}, y_2) \\), which involves the inverse Mills ratio derived under normality.\n- Nonnormality distorts the Mills ratio computation, invalidating standard formulas for marginal effects (e.g., on the uncensored mean or participation probability).\n- Heteroskedasticity introduces additional bias because the variance \\( \tau_1^2 \\) is no longer constant, altering scaling factors in partial effects.\n\nStep 3: **Mitigation via Robust Approaches**\n- The context notes that median independence (\\( \text{Med}(u \\mid \\mathbf{x}) = 0 \\)) allows \\( \\sqrt{N} \\)-consistent estimation of \\( \beta \\) without normality/homoskedasticity. This approach focuses on the median (not mean) of \\( y^* \\), making it robust to certain distributional misspecifications.\n- However, this robustness does not extend to the standard Tobit estimator, which remains inconsistent under heteroskedasticity/nonnormality.\n\nConclusion: Heteroskedasticity and nonnormality invalidate the Tobit estimator’s consistency and distort partial effect interpretations. Alternative methods (e.g., median regression) are required for robustness.\n\nQID: textbook-115-9-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-9-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately addresses the implications of heteroskedasticity and nonnormality on the consistency of the Tobit estimator and the interpretation of partial effects, aligning well with the gold answer. It also correctly mentions the need for alternative methods to achieve robustness.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-9-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately addresses the implications of heteroskedasticity and nonnormality on the consistency of the Tobit estimator and the interpretation of partial effects, aligning well with the gold answer. It also correctly mentions the need for alternative methods to achieve robustness."}, "llm_echoed_qid": "textbook-115-9-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately addresses the implications of heteroskedasticity and nonnormality on the consistency of the Tobit estimator and the interpretation of partial effects, aligning well with the gold answer. It also correctly mentions the need for alternative methods to achieve robustness."}
{"script_processing_qid": "textbook-47-2-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Under what conditions does an $\\epsilon$-SSE exist near a deterministic 2-cycle or a pair of distinct steady states?\n\nGOLD_ANSWER:\n1. **Near a 2-cycle**: If $f^{\\prime}(\\bar{y}_{1})f^{\\prime}(\\bar{y}_{2}) \\neq 1$ for a 2-cycle $(\\bar{y}_{1}, \\bar{y}_{2})$, there exists an $\\epsilon$-SSE for sufficiently small $\\epsilon$.\n2. **Near distinct steady states**: If $f^{\\prime}(\\bar{y}_{1}) \\neq 1$ and $f^{\\prime}(\\bar{y}_{2}) \\neq 1$ at a pair of distinct steady states, there exists an $\\epsilon$-SSE for sufficiently small $\\epsilon$.\n\nCANDIDATE_ANSWER:\nStep 1: For an ε-SSE near a deterministic 2-cycle {y₁, y₂}:\\nThe transition probabilities must satisfy π₁₁ ≈ 0 and π₂₂ ≈ 0. This mimics the deterministic transitions of a 2-cycle (always switching states), where the SSE equations reduce to y₁ ≈ f(y₂) and y₂ ≈ f(y₁) as π_{11}, π_{22} → 0.\\n\\nStep 2: For an ε-SSE near distinct steady states {y_a, y_b}:\\nThe transition probabilities must satisfy π₁₁ ≈ 1 and π₂₂ ≈ 1. This approximates two independent steady states, where the SSE equations reduce to y₁ ≈ f(y₁) and y₂ ≈ f(y₂) as π_{11}, π_{22} → 1.\\n\\nConclusion: An ε-SSE exists near a 2-cycle when transition probabilities are close to 0, and near distinct steady states when transition probabilities are close to 1.\n\nQID: textbook-47-2-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-47-2-1-1\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the conditions for ε-SSE existence near a 2-cycle and distinct steady states but focuses on transition probabilities (π) rather than the derivative conditions (f′) specified in the gold answer. The reasoning diverges from the mathematical criteria provided.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-47-2-1-1", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the conditions for ε-SSE existence near a 2-cycle and distinct steady states but focuses on transition probabilities (π) rather than the derivative conditions (f′) specified in the gold answer. The reasoning diverges from the mathematical criteria provided."}, "llm_echoed_qid": "textbook-47-2-1-1", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the conditions for ε-SSE existence near a 2-cycle and distinct steady states but focuses on transition probabilities (π) rather than the derivative conditions (f′) specified in the gold answer. The reasoning diverges from the mathematical criteria provided."}
{"script_processing_qid": "textbook-123-12-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the first-order conditions (8.13.3a) and (8.13.3b) from the Lagrangian formulation of the recursive Pareto problem.\n\nGOLD_ANSWER:\n1. **Lagrangian**: The Lagrangian for the Pareto problem is:\n   $$L=\\sum_{i=1}^{S}\\Pi_{i}[u(1-c_{i})+\\beta P(w_{i})+\\theta(u(c_{i})+\\beta w_{i})]-\\theta v.$$\n2. **FOC for $c_{i}$**: Differentiate with respect to $c_{i}$:\n   $$-u^{\\prime}(1-c_{i})+\\theta u^{\\prime}(c_{i})=0.$$\n3. **FOC for $w_{i}$**: Differentiate with respect to $w_{i}$:\n   $$\\beta P^{\\prime}(w_{i})\\Pi_{i}+\\theta\\beta\\Pi_{i}=0 \\implies P^{\\prime}(w_{i})+\\theta=0.$$\n\nCANDIDATE_ANSWER:\nStep 1: Formulate the Lagrangian for the recursive problem:\n\nL = ∑_{i=1}^S [u(1-c_i) + βP(w_i)]Π_i + θ[∑_{i=1}^S (u(c_i) + βw_i)Π_i - v]\nwhere θ is the Lagrange multiplier on the promise-keeping constraint.\n\nStep 2: Take derivative with respect to c_i:\n∂L/∂c_i = [-u'(1-c_i) + θu'(c_i)]Π_i = 0\nDividing by Π_i gives FOC (8.13.3a):\n-u'(1-c_i) + θu'(c_i) = 0\n\nStep 3: Take derivative with respect to w_i:\n∂L/∂w_i = [βP'(w_i) + θβ]Π_i = 0\nDividing by βΠ_i gives FOC (8.13.3b):\nP'(w_i) + θ = 0\n\nThese derivatives capture the trade-off between current consumption allocation (c_i) and intertemporal value allocation (w_i), with θ representing the shadow price of the promise-keeping constraint.\n\nQID: textbook-123-12-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-12-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the first-order conditions (8.13.3a) and (8.13.3b) from the Lagrangian formulation, matching the steps and results provided in the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-12-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the first-order conditions (8.13.3a) and (8.13.3b) from the Lagrangian formulation, matching the steps and results provided in the gold answer."}, "llm_echoed_qid": "textbook-123-12-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the first-order conditions (8.13.3a) and (8.13.3b) from the Lagrangian formulation, matching the steps and results provided in the gold answer."}
{"script_processing_qid": "textbook-118-8-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Contrast the efficiency of equation-by-equation IV estimation versus system estimation when $\\pmb{\\Sigma}$ is non-diagonal.\n\nGOLD_ANSWER:\n1. **Equation-by-equation IV**: Consistent but inefficient, as it ignores cross-equation correlations (non-zero $\\sigma^{i j}$).\n2. **System estimation (e.g., 3SLS)**: Efficient under non-diagonal $\\pmb{\\Sigma}$, as it exploits contemporaneous correlations via $\\pmb{\\Sigma}^{-1}$ in the GMM criterion. Efficiency loss arises only if $\\pmb{\\Sigma}$ is diagonal or all equations are just identified.\n\nCANDIDATE_ANSWER:\nStep 1: Understand key context features\n- When Σ is diagonal, equation-by-equation (2SLS) estimation suffices\n- System estimation uses cross-equation correlations when Σ is non-diagonal\n\nStep 2: Analyze efficiency implications\n- Equation-by-equation IV treats each equation separately, ignoring error covariance\n- System estimation (e.g., 3SLS) incorporates Σ's off-diagonal terms\n\nStep 3: Contrast efficiency\n- Non-diagonal Σ implies correlated errors across equations\n- Equation-by-equation IV remains consistent but inefficient (loses cross-equation information)\n- System estimation achieves greater efficiency by exploiting error correlations through Σ\n\nStep 4: Conclusion\nSystem estimation becomes asymptotically more efficient than equation-by-equation IV when Σ is non-diagonal because it uses additional covariance information that single-equation methods discard.\n\nQID: textbook-118-8-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-8-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the efficiency differences between equation-by-equation IV and system estimation when Σ is non-diagonal, aligning with the gold answer's points about consistency, inefficiency, and the role of cross-equation correlations.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-8-0-2", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the efficiency differences between equation-by-equation IV and system estimation when Σ is non-diagonal, aligning with the gold answer's points about consistency, inefficiency, and the role of cross-equation correlations."}, "llm_echoed_qid": "textbook-118-8-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the efficiency differences between equation-by-equation IV and system estimation when Σ is non-diagonal, aligning with the gold answer's points about consistency, inefficiency, and the role of cross-equation correlations."}
{"script_processing_qid": "textbook-84-2-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Show that $(\\mathbf{X}^{\\prime}\\mathbf{X})^{\\prime} = \\mathbf{X}^{\\prime}\\mathbf{X}$ using the properties of matrix transposition.\n\nGOLD_ANSWER:\n4. **Transpose Property**:\n   - By the transpose of a product rule, $(\\mathbf{X}^{\\prime}\\mathbf{X})^{\\prime} = \\mathbf{X}^{\\prime}(\\mathbf{X}^{\\prime})^{\\prime}$.\n   - Since $(\\mathbf{X}^{\\prime})^{\\prime} = \\mathbf{X}$, the result follows: $(\\mathbf{X}^{\\prime}\\mathbf{X})^{\\prime} = \\mathbf{X}^{\\prime}\\mathbf{X}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the matrix transpose property $(\\mathbf{AB})' = \\mathbf{B}'\\mathbf{A}'$.\nStep 2: Apply this property to $(\\mathbf{X}'\\mathbf{X})'$:\n$$(\\mathbf{X}'\\mathbf{X})' = \\mathbf{X}'(\\mathbf{X}')'$$\nStep 3: Use the identity $(\\mathbf{X}')' = \\mathbf{X}$ (transposing twice returns the original matrix):\n$$\\mathbf{X}'(\\mathbf{X}')' = \\mathbf{X}'\\mathbf{X}$$\nConclusion: Therefore, $(\\mathbf{X}'\\mathbf{X})' = \\mathbf{X}'\\mathbf{X}$ by the properties of matrix transposition.\n\nQID: textbook-84-2-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-84-2-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly applies the properties of matrix transposition, including the transpose of a product rule and the identity that transposing twice returns the original matrix, to demonstrate that $(\\\\mathbf{X}'\\\\mathbf{X})' = \\\\mathbf{X}'\\\\mathbf{X}$. The reasoning and conclusion align perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-84-2-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly applies the properties of matrix transposition, including the transpose of a product rule and the identity that transposing twice returns the original matrix, to demonstrate that $(\\mathbf{X}'\\mathbf{X})' = \\mathbf{X}'\\mathbf{X}$. The reasoning and conclusion align perfectly with the gold answer."}, "llm_echoed_qid": "textbook-84-2-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly applies the properties of matrix transposition, including the transpose of a product rule and the identity that transposing twice returns the original matrix, to demonstrate that $(\\mathbf{X}'\\mathbf{X})' = \\mathbf{X}'\\mathbf{X}$. The reasoning and conclusion align perfectly with the gold answer."}
{"script_processing_qid": "textbook-115-1-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Compare the use of probit and logit models with unobserved effects in panel data.\n\nGOLD_ANSWER:\n1. **Probit**: Assumes normal distribution for errors and unobserved effects. Estimation can use maximum likelihood with numerical integration or conditional likelihood approaches.\n2. **Logit**: Assumes logistic distribution, allowing for conditional likelihood methods that eliminate the unobserved effects by conditioning on sufficient statistics.\n\nCANDIDATE_ANSWER:\nStep 1: Linear probability models (LPM) impose restrictive bounds on unobserved effects ($c_i$) to ensure predicted probabilities stay between 0 and 1. These bounds require $\\mathbf{x}_{it}\\beta \\leq c_i \\leq 1 - \\mathbf{x}_{it}\\beta$ for all $t$, which is often unrealistic.\n\nStep 2: Probit and logit models avoid these restrictions by using nonlinear link functions (normal CDF for probit, logistic CDF for logit), allowing unobserved effects ($c_i$) to freely vary without bounding predictions.\n\nStep 3: However, unlike LPM (where fixed effects can be removed via within transformation), probit/logit require specialized methods (e.g., conditional maximum likelihood, random effects assumptions, or first differencing) to address $c_i$, complicating estimation.\n\nConclusion: Probit/logit models offer greater flexibility in handling unobserved heterogeneity but at the cost of more complex estimation procedures compared to LPM.\n\nQID: textbook-115-1-2-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-1-2-1\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies that probit and logit models use nonlinear link functions to handle unobserved effects without restrictive bounds, which aligns with the gold answer. However, the candidate focuses more on comparing these models to linear probability models (LPM) rather than directly comparing probit and logit models as the question asks. The gold answer specifically highlights the distribution assumptions and estimation methods for probit and logit, which the candidate does not address in detail.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-1-2-1", "category": "PARTIALLY_CORRECT", "explanation": "The candidate answer correctly identifies that probit and logit models use nonlinear link functions to handle unobserved effects without restrictive bounds, which aligns with the gold answer. However, the candidate focuses more on comparing these models to linear probability models (LPM) rather than directly comparing probit and logit models as the question asks. The gold answer specifically highlights the distribution assumptions and estimation methods for probit and logit, which the candidate does not address in detail."}, "llm_echoed_qid": "textbook-115-1-2-1", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies that probit and logit models use nonlinear link functions to handle unobserved effects without restrictive bounds, which aligns with the gold answer. However, the candidate focuses more on comparing these models to linear probability models (LPM) rather than directly comparing probit and logit models as the question asks. The gold answer specifically highlights the distribution assumptions and estimation methods for probit and logit, which the candidate does not address in detail."}
{"script_processing_qid": "textbook-112-23-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. (Quantitative) Given the South Sea Company's 1711 loan of £9,177,967 at 5% perpetual interest, compute the nominal and inflation-adjusted cost to the British treasury over 50 years, assuming 1.2% annual inflation.\n\nGOLD_ANSWER:\n1.  **Nominal Cost**: \n    - Annual payment = 5% of £9,177,967 = £458,898.35\n    - 50-year total = £22,944,917.50\n\n2.  **Real Cost (PV)**: \n    - Discount rate = 5% - 1.2% = 3.8%\n    - PV = £458,898.35 × [(1 - (1 + 0.038)^-50)/0.038] = £458,898.35 × 23.23 ≈ £10,660,000\n\n3.  **Comparison**: Inflation reduces real burden by ~53%.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate annual nominal interest payment\nAnnual payment = Principal × Interest rate = £9,177,967 × 5% = £458,898.35\n\nStep 2: Compute nominal cost over 50 years\nNominal cost = Annual payment × Years = £458,898.35 × 50 = £22,944,917.50\n\nStep 3: Calculate inflation-adjusted cost using present value formula\nPV = Annual payment × [1 - (1 + inflation rate)^{-years}] / inflation rate\n   = £458,898.35 × [1 - (1.012)^{-50}] / 0.012\n\nStep 4: Compute (1.012)^{50} ≈ 1.815\n[1 - 1/1.815] ≈ 0.4487\nPV ≈ £458,898.35 × (0.4487 / 0.012)\n    ≈ £458,898.35 × 37.39 ≈ £17,156,728\n\nFinal results:\n- Nominal cost: £22,944,918\n- Inflation-adjusted cost: £17,156,728\n\nNote: All values rounded to nearest pound. Inflation adjustment uses 1.2% annual discounting to account for purchasing power erosion.\n\nQID: textbook-112-23-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-23-0-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly calculated the nominal cost but made an error in the inflation-adjusted cost calculation. The candidate used the inflation rate directly as the discount rate instead of adjusting the nominal interest rate by the inflation rate (5% - 1.2% = 3.8%). This led to an incorrect present value calculation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-23-0-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly calculated the nominal cost but made an error in the inflation-adjusted cost calculation. The candidate used the inflation rate directly as the discount rate instead of adjusting the nominal interest rate by the inflation rate (5% - 1.2% = 3.8%). This led to an incorrect present value calculation."}, "llm_echoed_qid": "textbook-112-23-0-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly calculated the nominal cost but made an error in the inflation-adjusted cost calculation. The candidate used the inflation rate directly as the discount rate instead of adjusting the nominal interest rate by the inflation rate (5% - 1.2% = 3.8%). This led to an incorrect present value calculation."}
{"script_processing_qid": "textbook-22-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain why a dictatorship function F(x₁, x₂) = x₁ is not symmetric but satisfies positive responsiveness and neutrality between alternatives.\n\nGOLD_ANSWER:\n1. **Symmetry**: F(x₁, x₂) = x₁ is not symmetric because permuting x₁ and x₂ changes the outcome.\n2. **Positive Responsiveness**: If x₁ increases, F reflects this change immediately.\n3. **Neutrality**: The function is neutral between alternatives because it depends only on x₁, not on the specific alternatives.\n\nCANDIDATE_ANSWER:\nStep 1: Understand symmetry requirement\nSymmetry requires that the social choice function treats all voters equally. For F(x₁, x₂) = x₁ (dictatorship), permuting voters (e.g., swapping x₁ and x₂) would change the outcome (to x₂), violating symmetry.\n\nStep 2: Verify positive responsiveness\nPositive responsiveness requires that increasing support for an alternative improves its chances of being selected. If voter 1 (the dictator) changes their vote from 0→1 or increases intensity of preference (α₁' > α₁), F immediately responds by adopting voter 1's new preference.\n\nStep 3: Confirm neutrality between alternatives\nNeutrality requires equal treatment of alternatives. The dictatorship function has no inherent preference between alternatives - it simply mirrors the dictator's choice. If all voters (including dictator) reverse preferences, the social choice reverses accordingly.\n\nQID: textbook-22-0-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-22-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains why the dictatorship function F(x₁, x₂) = x₁ is not symmetric, satisfies positive responsiveness, and is neutral between alternatives, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-22-0-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly explains why the dictatorship function F(x₁, x₂) = x₁ is not symmetric, satisfies positive responsiveness, and is neutral between alternatives, aligning with the gold answer."}, "llm_echoed_qid": "textbook-22-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains why the dictatorship function F(x₁, x₂) = x₁ is not symmetric, satisfies positive responsiveness, and is neutral between alternatives, aligning with the gold answer."}
{"script_processing_qid": "textbook-122-18-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Why is Assumption MLR.4 (No Perfect Collinearity) necessary for OLS estimation?\n\nGOLD_ANSWER:\n1. **Necessity**: MLR.4 ensures that the design matrix $X$ has full column rank, allowing the OLS estimator $\\hat{\\beta} = (X^T X)^{-1} X^T y$ to be uniquely defined.\n2. **Violation**: If MLR.4 is violated, $X^T X$ is not invertible, and the OLS estimates cannot be computed.\n3. **Practical Implication**: Researchers must check for near-perfect collinearity (e.g., high variance inflation factors) to avoid numerical instability.\n\nCANDIDATE_ANSWER:\nStep 1: Perfect collinearity occurs when one independent variable is an exact linear combination of others (e.g., x₁ = 2x₂ + 5) or when a variable is constant. This creates redundancy in the predictors.\n\nStep 2: OLS estimation requires solving a system of equations derived from the normal equations (X'Xβ = X'y). For this solution to exist uniquely, the matrix X'X must be invertible.\n\nStep 3: If MLR.4 is violated (perfect collinearity exists), X'X becomes singular (non-invertible). This means:\n  - Coefficients cannot be uniquely estimated (infinite solutions exist)\n  - The model cannot distinguish between the effects of collinear variables\n\nStep 4: Even partial violations (e.g., a constant variable beyond the intercept) create collinearity with the intercept term, preventing estimation of distinct parameter values.\n\nThus, MLR.4 ensures the full rank of the design matrix, allowing unique OLS estimates to exist.\n\nQID: textbook-122-18-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-18-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains the necessity of MLR.4 by detailing how perfect collinearity affects the invertibility of the matrix X'X and the uniqueness of OLS estimates, aligning with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-18-1-3", "category": "CORRECT", "explanation": "The candidate answer correctly explains the necessity of MLR.4 by detailing how perfect collinearity affects the invertibility of the matrix X'X and the uniqueness of OLS estimates, aligning with the gold answer."}, "llm_echoed_qid": "textbook-122-18-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains the necessity of MLR.4 by detailing how perfect collinearity affects the invertibility of the matrix X'X and the uniqueness of OLS estimates, aligning with the gold answer."}
{"script_processing_qid": "textbook-122-39-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the Chow statistic formula for testing the equality of regression functions across two groups, explaining each component and its significance.\n\nGOLD_ANSWER:\n1. **Chow Statistic Formula**: $$F = \\frac{[\\mathrm{SSR}_{r} - (\\mathrm{SSR}_{1} + \\mathrm{SSR}_{2})]}{\\mathrm{SSR}_{1} + \\mathrm{SSR}_{2}} \\cdot \\frac{[n - 2(k + 1)]}{k + 1}$$ \n2. **Components**: \n   - $\\mathrm{SSR}_{r}$: Sum of squared residuals from the pooled regression. \n   - $\\mathrm{SSR}_{1}, \\mathrm{SSR}_{2}$: Sum of squared residuals from separate regressions for each group. \n   - $n$: Total number of observations. \n   - $k$: Number of explanatory variables. \n3. **Significance**: The Chow statistic tests the null hypothesis that the regression coefficients are identical across groups. A high F-value indicates rejection of the null, suggesting significant differences.\n\nCANDIDATE_ANSWER:\nStep 1: Understanding the Chow Test Purpose\nThe Chow statistic tests whether regression coefficients differ significantly between two groups by comparing restricted (pooled) and unrestricted (separate group) models.\n\nStep 2: Key Components\n- RSS_pooled: Residual sum of squares from the restricted model (all groups pooled)\n- RSS1, RSS2: RSS from separate regressions for Group 1 and Group 2\n- k: Number of parameters in each regression\n- n1, n2: Sample sizes of each group\n\nStep 3: Formula Derivation\nF = [(RSS_pooled - (RSS1 + RSS2))/k] / [(RSS1 + RSS2)/(n1 + n2 - 2k)]\n\nStep 4: Component Significance\n- Numerator: Measures improvement in fit from allowing different coefficients\n- Denominator: Estimates common error variance\n- Degrees of freedom: k (numerator) and n1+n2-2k (denominator)\n\nThe provided context shows interaction term approaches (e.g., black·percblck) as an alternative to Chow testing when variances are equal. However, the Chow statistic derivation requires RSS values from separate group regressions not explicitly provided in the context.\n\nQID: textbook-122-39-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-39-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately derives the Chow statistic formula, correctly identifies and explains each component (RSS_pooled, RSS1, RSS2, k, n1, n2), and clarifies the significance of the numerator and denominator. The explanation aligns with the gold answer, despite minor differences in notation (e.g., SSR vs. RSS).\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-39-0-0", "category": "CORRECT", "explanation": "The candidate's answer accurately derives the Chow statistic formula, correctly identifies and explains each component (RSS_pooled, RSS1, RSS2, k, n1, n2), and clarifies the significance of the numerator and denominator. The explanation aligns with the gold answer, despite minor differences in notation (e.g., SSR vs. RSS)."}, "llm_echoed_qid": "textbook-122-39-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately derives the Chow statistic formula, correctly identifies and explains each component (RSS_pooled, RSS1, RSS2, k, n1, n2), and clarifies the significance of the numerator and denominator. The explanation aligns with the gold answer, despite minor differences in notation (e.g., SSR vs. RSS)."}
{"script_processing_qid": "textbook-118-4-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Derive the condition for a matrix to be positive definite in terms of its eigenvalues.\n\nGOLD_ANSWER:\nA matrix $\\pmb{A}$ is positive definite if and only if all its eigenvalues are positive. This follows from the fact that for any nonzero vector $\\mathbf{x}$, $\\mathbf{x}^{\\top}\\pmb{A}\\mathbf{x} = \\lambda\\mathbf{x}^{\\top}\\mathbf{x} > 0$ if and only if $\\lambda > 0$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the definition of positive definiteness. A real symmetric matrix A is positive definite if for any non-zero vector x, the quadratic form xᵀAx > 0.\n\nStep 2: Apply the spectral theorem for real symmetric matrices. A can be diagonalized as A = QΛQᵀ, where Q is an orthogonal matrix of eigenvectors, and Λ is a diagonal matrix of eigenvalues λ₁, λ₂, ..., λₙ.\n\nStep 3: Substitute x = Qy (where y is a non-zero vector since Q is invertible). The quadratic form becomes xᵀAx = (Qy)ᵀA(Qy) = yᵀ(QᵀAQ)y = yᵀΛy.\n\nStep 4: Express yᵀΛy in terms of eigenvalues: y₁²λ₁ + y₂²λ₂ + ... + yₙ²λₙ.\n\nStep 5: For this sum to be positive for all non-zero y, all eigenvalues λ_i must be positive. If any λ_i ≤ 0, choosing y with 1 in the i-th position and 0 elsewhere would yield yᵀΛy = λ_i ≤ 0, violating positive definiteness.\n\nConclusion: A real symmetric matrix is positive definite if and only if all its eigenvalues are strictly positive.\n\nQID: textbook-118-4-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-4-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the condition for a matrix to be positive definite in terms of its eigenvalues, matching the gold answer. The reasoning is detailed and logically sound, showing that all eigenvalues must be positive for the matrix to be positive definite.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-4-1-3", "category": "CORRECT", "explanation": "The candidate answer correctly derives the condition for a matrix to be positive definite in terms of its eigenvalues, matching the gold answer. The reasoning is detailed and logically sound, showing that all eigenvalues must be positive for the matrix to be positive definite."}, "llm_echoed_qid": "textbook-118-4-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the condition for a matrix to be positive definite in terms of its eigenvalues, matching the gold answer. The reasoning is detailed and logically sound, showing that all eigenvalues must be positive for the matrix to be positive definite."}
{"script_processing_qid": "textbook-106-17-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the role of the Clayton Act's triple damages provision in deterring collusion among oligopolists. How does this provision alter the expected payoff matrix for firms considering collusion?\n\nGOLD_ANSWER:\n1. **Deterrence Mechanism**: Triple damages increase the cost of collusion by making it financially risky. Firms face higher penalties if caught, reducing the expected payoff from collusion.\n2. **Payoff Matrix**: The provision shifts the payoff matrix by increasing the penalty for defection (being caught), making collusion less attractive relative to competitive behavior.\n\nCANDIDATE_ANSWER:\nStep 1: The Clayton Act’s triple damages provision allows plaintiffs (e.g., consumers or competitors harmed by collusion) to sue and recover three times the actual damages caused by anticompetitive behavior. This creates a financial disincentive for collusion.\n\nStep 2: Collusion in oligopolies increases profits for firms but harms others (e.g., consumers). Without triple damages, the expected cost of collusion is the probability of being caught multiplied by the statutory penalty (e.g., fines under the Sherman Act).\n\nStep 3: Triple damages raise the expected penalty to three times the harm caused. This increases the risk for firms: even if the probability of detection remains unchanged, the potential liability grows significantly.\n\nStep 4: In the payoff matrix for firms considering collusion, the expected profit from collusion becomes: (Collusion Profit) – [Probability of Detection × (3 × Harm + Other Penalties)]. This reduces the net expected payoff of collusion relative to competitive behavior.\n\nStep 5: As a result, the provision shifts the Nash equilibrium by making the 'defect' or 'comply' strategy relatively more attractive compared to collusion, thereby deterring tacit or explicit agreements to restrain trade.\n\nQID: textbook-106-17-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-106-17-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the role of the Clayton Act's triple damages provision in deterring collusion by increasing the financial risk for firms. It also correctly describes how this provision alters the expected payoff matrix, making collusion less attractive relative to competitive behavior. The reasoning aligns with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-17-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the role of the Clayton Act's triple damages provision in deterring collusion by increasing the financial risk for firms. It also correctly describes how this provision alters the expected payoff matrix, making collusion less attractive relative to competitive behavior. The reasoning aligns with the gold answer."}, "llm_echoed_qid": "textbook-106-17-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the role of the Clayton Act's triple damages provision in deterring collusion by increasing the financial risk for firms. It also correctly describes how this provision alters the expected payoff matrix, making collusion less attractive relative to competitive behavior. The reasoning aligns with the gold answer."}
{"script_processing_qid": "textbook-12-0-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Compare the efficiency implications of output taxation ($t$) versus externality taxation ($\\tau$) in restoring social optimality.\n\nGOLD_ANSWER:\n1. **Output Tax**: Distorts production quantity $q$ but may not correct $h$ (e.g., if $h = \\alpha q$).\n2. **Externality Tax**: Directly targets $h$ via $\\tau = -\\phi'(h^{0})$, achieving first-best.\n3. **Key Insight**: Externality taxation is more precise but requires observability of $h$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the efficiency goal of both taxes. Output taxation ($t$) adjusts production quantity $q$, while externality taxation ($\\tau$) directly targets the externality level $h$. Social optimality requires aligning private incentives with social costs.\n\nStep 2: Analyze externality taxation ($\\tau$). The context shows that even with an optimal Pigovian tax set to achieve $h^0$, bargaining between agents can further reduce $h$ to $\\widehat{h} < h^0$. This occurs because the marginal cost to agent 1 (who creates $h$) is zero near $h^0$, while agent 2 gains from reducing $h$. Thus, $\\tau$ alone fails to guarantee efficiency due to post-tax bargaining.\n\nStep 3: Analyze output taxation ($t$). Output taxes alter the firm’s production decision via $p^* \\leq c'(q) - s$ (or $+t$ for taxation). If $h$ is tied to $q$ (e.g., $h=q$), taxing $q$ indirectly controls $h$. Since the tax directly governs the firm’s output choice, agents cannot bargain over $h$ independently of $q$, eliminating post-tax inefficiencies.\n\nStep 4: Compare efficiency. Externality taxation ($\\tau$) is theoretically precise but vulnerable to post-tax bargaining, undermining efficiency. Output taxation ($t$) is less direct but avoids bargaining distortions by tying the externality to a taxed variable ($q$), ensuring stable social optimality.\n\nQID: textbook-12-0-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-12-0-1-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate's answer provides a detailed analysis of both taxes but introduces a bargaining scenario not mentioned in the gold answer, leading to a partially correct conclusion. The gold answer emphasizes the precision of externality taxation and its requirement of observability, which the candidate does not fully address.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-12-0-1-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate's answer provides a detailed analysis of both taxes but introduces a bargaining scenario not mentioned in the gold answer, leading to a partially correct conclusion. The gold answer emphasizes the precision of externality taxation and its requirement of observability, which the candidate does not fully address."}, "llm_echoed_qid": "textbook-12-0-1-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate's answer provides a detailed analysis of both taxes but introduces a bargaining scenario not mentioned in the gold answer, leading to a partially correct conclusion. The gold answer emphasizes the precision of externality taxation and its requirement of observability, which the candidate does not fully address."}
{"script_processing_qid": "textbook-123-11-3-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the orthogonality condition $E X^{\\prime}\\epsilon=0$ in the context of regression.\n\nGOLD_ANSWER:\nThe orthogonality condition means the error term $\\epsilon$ is uncorrelated with the regressors $X$. This ensures that $X\\beta$ is the best linear predictor of $Y$, as no additional linear combination of $X$ can improve the prediction. It is a key property of population regression, distinguishing it from arbitrary linear approximations.\n\nCANDIDATE_ANSWER:\nStep 1: The orthogonality condition $E X^{\\prime}\\epsilon=0$ originates from the least squares normal equation (2.5.2). In regression, this condition ensures the projection error $\\epsilon = Y - X\\beta$ (the residuals) is uncorrelated with the regressors $X$ in expectation.\n\nStep 2: Mathematically, the normal equation states $X^{\\prime}(Y - X\\beta) = 0$ in the sample context. Extending this to the population version replaces sample averages with expectations, leading to $E[X^{\\prime}(Y - X\\beta)] = 0$.\n\nStep 3: Substituting $\\epsilon = Y - X\\beta$ into the equation gives $E[X^{\\prime}\\epsilon] = 0$. This means the regressors $X$ are orthogonal to the error term $\\epsilon$ in expectation.\n\nStep 4: Econometrically, this condition ensures that all systematic relationships between $Y$ and $X$ are captured by $X\\beta$, leaving $\\epsilon$ as purely unpredictable noise relative to $X$. It is a foundational assumption for the validity of population regression coefficients $\\beta$.\n\nQID: textbook-123-11-3-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-11-3-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains the orthogonality condition, detailing its origin from the least squares normal equation, its mathematical formulation, and its econometric significance. This aligns perfectly with the gold answer, which emphasizes the uncorrelatedness of the error term with the regressors and its role in ensuring the best linear predictor.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-11-3-1", "category": "CORRECT", "explanation": "The candidate answer correctly explains the orthogonality condition, detailing its origin from the least squares normal equation, its mathematical formulation, and its econometric significance. This aligns perfectly with the gold answer, which emphasizes the uncorrelatedness of the error term with the regressors and its role in ensuring the best linear predictor."}, "llm_echoed_qid": "textbook-123-11-3-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains the orthogonality condition, detailing its origin from the least squares normal equation, its mathematical formulation, and its econometric significance. This aligns perfectly with the gold answer, which emphasizes the uncorrelatedness of the error term with the regressors and its role in ensuring the best linear predictor."}
{"script_processing_qid": "textbook-115-3-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain the importance of the zero conditional mean property of the score function in deriving the asymptotic normality of the CMLE.\n\nGOLD_ANSWER:\n1. **Zero Conditional Mean**: $\\operatorname{E}[\\mathbf{s}_{i}(\\pmb{\\theta}_{0})|\\mathbf{x}_{i}] = \\mathbf{0}$ ensures that the score is a martingale difference sequence.\n2. **Central Limit Theorem**: This property allows the application of a CLT to the score, leading to the asymptotic normality of the CMLE.\n3. **Efficiency**: The property is also key to establishing the information matrix equality, which underpins the efficiency of the CMLE.\n\nCANDIDATE_ANSWER:\nStep 1: The zero conditional mean property of the score function (E[∇_θℓ(w,θ₀)|x] = 0) ensures that the score forms a martingale difference sequence. This is critical because:\n\nStep 2: In the asymptotic normality derivation (Theorem 13.2), the expansion √N(θ̂ - θ₀) ≈ -[A₀]^{-1}[N^{-1/2}∑_{i=1}^N ∇_θℓ(w_i,θ₀)] relies on the score terms ∇_θℓ(w_i,θ₀) having zero mean. The martingale CLT can then be applied to establish normality.\n\nStep 3: The property guarantees A₀ = -E[∇_θ²ℓ(w,θ₀)], simplifying the asymptotic variance to A₀^{-1}. Without zero conditional mean, A₀ would not equal the expected Hessian, and the asymptotic variance formula would require adjustment for potential score correlation.\n\nQID: textbook-115-3-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-3-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the importance of the zero conditional mean property in deriving the asymptotic normality of the CMLE, matching the key points in the gold answer regarding martingale difference sequence, CLT application, and the role in simplifying the asymptotic variance.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-3-1-3", "category": "CORRECT", "explanation": "The candidate answer accurately explains the importance of the zero conditional mean property in deriving the asymptotic normality of the CMLE, matching the key points in the gold answer regarding martingale difference sequence, CLT application, and the role in simplifying the asymptotic variance."}, "llm_echoed_qid": "textbook-115-3-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the importance of the zero conditional mean property in deriving the asymptotic normality of the CMLE, matching the key points in the gold answer regarding martingale difference sequence, CLT application, and the role in simplifying the asymptotic variance."}
{"script_processing_qid": "textbook-94-3-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. State and prove the revelation principle in the context of contract theory.\n\nGOLD_ANSWER:\n1. **Statement**: Any allocation rule obtained with a mechanism can be implemented with a truthful direct revelation mechanism.\n2. **Proof**: Construct a direct revelation mechanism $g(\\theta) = (q(\\theta), t(\\theta))$ from the indirect mechanism. Show that $g(\\theta)$ is truthful by verifying the incentive compatibility constraints: $$ t(\\theta) - \\theta q(\\theta) \\geq t(\\theta') - \\theta q(\\theta') \\quad \\forall \\theta, \\theta'. $$\n\nCANDIDATE_ANSWER:\nStep 1: **State the Revelation Principle**\nIn contract theory under asymmetric information, the revelation principle asserts that any feasible allocation rule achievable by a mechanism (contract) can also be implemented through a truth-telling direct revelation mechanism. Specifically, the principal can restrict attention to contracts where:\n- Agents are asked to report their types (e.g., efficiency parameter θ).\n- The contract menu contains one option per agent type.\n- Truthful reporting is incentive compatible (agents maximize utility by revealing their true type).\n\nStep 2: **Proof Construction Using Context**\nFrom the provided context:\n1. Start with an optimal complete-information contract $(A^*, B^*)$ that is not incentive compatible.\n2. Construct a new menu $(B^*, C)$:\n   - $C$ lies on the indifference curve of the low-efficiency ($\\underline{\\theta}$) agent passing through $B^*$, ensuring the $\\underline{\\theta}$-agent is indifferent between $B^*$ and $C$.\n   - The high-efficiency ($\\overline{\\theta}$) agent strictly prefers $B^*$.\n3. This menu becomes incentive compatible: each type self-selects the contract designed for them. Production levels remain unchanged, but transfers are adjusted to satisfy incentive constraints (e.g., increasing the transfer to the $\\underline{\\theta}$-agent at $C$ to prevent imitation).\n4. The principal achieves the same second-best outcome as with the original non-incentive-compatible contract, but now under truthful reporting.\n\nStep 3: **Generalization**\nThe example illustrates that:\n- Any contract can be converted into a direct revelation mechanism (menu with type-specific options).\n- No generality is lost because the constructed menu replicates the original allocation rule while enforcing truthful type revelation.\n- The cardinality of the menu matches the type space (two options for two types, $\\underline{\\theta}$ and $\\overline{\\theta}$), as required by the principle.\n\nQID: textbook-94-3-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-94-3-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate accurately states the revelation principle and provides a detailed proof that aligns with the gold answer, demonstrating how a direct revelation mechanism can be constructed to ensure truthful reporting while maintaining the original allocation rule.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-94-3-0-2", "category": "CORRECT", "explanation": "The candidate accurately states the revelation principle and provides a detailed proof that aligns with the gold answer, demonstrating how a direct revelation mechanism can be constructed to ensure truthful reporting while maintaining the original allocation rule."}, "llm_echoed_qid": "textbook-94-3-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate accurately states the revelation principle and provides a detailed proof that aligns with the gold answer, demonstrating how a direct revelation mechanism can be constructed to ensure truthful reporting while maintaining the original allocation rule."}
{"script_processing_qid": "textbook-108-6-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the aggregate-demand curve by combining the wealth effect, interest-rate effect, and exchange-rate effect.\n\nGOLD_ANSWER:\n1. **Wealth Effect**: $C = C(Y - T, W)$, where $C$ is consumption, $Y$ is income, $T$ is taxes, and $W$ is wealth. A fall in $P$ increases real wealth ($W/P$), boosting $C$.\n2. **Interest-Rate Effect**: $I = I(r)$, where $I$ is investment and $r$ is the interest rate. A fall in $P$ lowers $r$, increasing $I$.\n3. **Exchange-Rate Effect**: $NX = NX(\\epsilon)$, where $NX$ is net exports and $\\epsilon$ is the real exchange rate. A fall in $P$ depreciates $\\epsilon$, increasing $NX$.\n4. **Aggregate Demand**: $AD = C + I + G + NX$, where $G$ is government spending. Combining these effects, $AD$ slopes downward.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the downward-sloping relationship between price level (P) and real GDP demanded (Y).\n\nStep 2: Wealth Effect\n- As P decreases, real value of household wealth increases\n- Higher perceived wealth increases consumption (C)\n- This leads to upward movement along the AD curve (higher Y at lower P)\n\nStep 3: Interest-Rate Effect\n- Lower P reduces demand for money (M/P increases)\n- Excess money supply leads to lower interest rates (r)\n- Lower r stimulates business investment (I)\n- This further increases Y at lower P\n\nStep 4: Exchange-Rate Effect (Mundell-Fleming)\n- Lower r reduces returns on domestic assets\n- Causes capital outflows and currency depreciation\n- Depreciation makes exports cheaper (X↑) and imports more expensive (M↓)\n- Net exports (NX) increase, boosting Y\n\nStep 5: Combine all three effects\nAD = C(Y-T) + I(r) + G + NX(ε)\nWhere:\n- P↓ → C↑ (wealth effect)\n- P↓ → r↓ → I↑ (interest-rate effect)\n- P↓ → ε↓ (exchange rate depreciation) → NX↑\n\nAll three effects simultaneously shift the economy to a higher Y at lower P, creating the downward-sloping AD curve.\n\nQID: textbook-108-6-2-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-108-6-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately combines the wealth effect, interest-rate effect, and exchange-rate effect to derive the aggregate-demand curve, matching the gold answer's structure and content.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-108-6-2-0", "category": "CORRECT", "explanation": "The candidate's answer accurately combines the wealth effect, interest-rate effect, and exchange-rate effect to derive the aggregate-demand curve, matching the gold answer's structure and content."}, "llm_echoed_qid": "textbook-108-6-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately combines the wealth effect, interest-rate effect, and exchange-rate effect to derive the aggregate-demand curve, matching the gold answer's structure and content."}
{"script_processing_qid": "textbook-121-9-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Prove the integration by parts formula, $$\\int v d u=u v-\\int u d v$$, starting from the product rule of differentials.\n\nGOLD_ANSWER:\n1. **Product Rule**: $$d(u v) = v d u + u d v$$.\n2. **Integrate Both Sides**: $$\\int d(u v) = \\int v d u + \\int u d v$$.\n3. **Simplify Left Side**: $$u v = \\int v d u + \\int u d v$$.\n4. **Rearrange**: $$\\int v d u = u v - \\int u d v$$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the product rule for differentials. For functions u(x) and v(x), the product rule states:  \n  d(uv) = u dv + v du  \n  \n  Step 2: Integrate both sides with respect to x:  \n  ∫d(uv) = ∫u dv + ∫v du  \n  \n  Step 3: The left side simplifies to uv via the Fundamental Theorem of Calculus:  \n  uv = ∫u dv + ∫v du  \n  \n  Step 4: Rearrange the equation to isolate ∫v du:  \n  ∫v du = uv − ∫u dv  \n  \n  This completes the derivation of the integration by parts formula.\n\nQID: textbook-121-9-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-121-9-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the integration by parts formula from the product rule, matching the gold answer in both reasoning and conclusion.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-121-9-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the integration by parts formula from the product rule, matching the gold answer in both reasoning and conclusion."}, "llm_echoed_qid": "textbook-121-9-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the integration by parts formula from the product rule, matching the gold answer in both reasoning and conclusion."}
{"script_processing_qid": "textbook-103-5-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Describe the policy iteration algorithm for solving the household's Bellman equation.\n\nGOLD_ANSWER:\n1. **Initialization**: Start with an initial guess for the value function $v_0(\\beta,a,s)$.\n2. **Policy Improvement**: For each state $(\\beta,a,s)$, compute the optimal policy $a^{\\prime}$ that maximizes the Bellman equation.\n3. **Value Update**: Update the value function using the optimal policy: $$ v_{n+1}(\\beta,a,s) = u\\left[(1+r)a + w s - a^{\\prime}\\right] + \\beta \\sum_{\\beta^{\\prime}} \\sum_{s^{\\prime}} v_n(\\beta^{\\prime},a^{\\prime},s^{\\prime}) P(\\beta^{\\prime}|\\beta) Q(s^{\\prime}|s) $$\n4. **Convergence Check**: Repeat steps 2-3 until $\\|v_{n+1} - v_n\\| < \\epsilon$ for a small tolerance $\\epsilon$.\n\nCANDIDATE_ANSWER:\nStep 1: Discretize the asset space into M grid points ranging from -φ to upper bound, forming state combinations (β,a,s). Initialize value function v₀ arbitrarily.\n\nStep 2: Policy Improvement\nFor each state (β,a,s):\n1. Calculate consumption c = (1+r)a + ws - a' for all feasible a' ≥ -φ\n2. Compute expected future value E[V] = Σ_{β'} Σ_{s'} vₖ(β',a',s')P(β'|β)Q(s'|s)\n3. Select a'* = argmax [u(c) + βE[V]]\n\nStep 3: Policy Evaluation\nUpdate value function using optimal policies:\nv_{k+1}(β,a,s) = u[(1+r)a + ws - a'*] + βΣ_{β'}Σ_{s'}vₖ(β',a'*,s')P(β'|β)Q(s'|s)\n\nStep 4: Convergence Check\nRepeat Steps 2-3 until ||v_{k+1} - vₖ|| < ε\n\nKey features:\n1. Explicitly enforces a' ≥ -φ constraint in Step 2\n2. Uses discrete grid approximation for continuous assets\n3. Handles stochastic transitions through explicit summation over β' and s'\n4. Iterates on both policy rules and value function simultaneously\n\nQID: textbook-103-5-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-103-5-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately describes the policy iteration algorithm, including initialization, policy improvement, value update, and convergence check. It also correctly incorporates additional details such as the discretization of the asset space and handling of constraints, which align with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-5-0-2", "category": "CORRECT", "explanation": "The candidate answer accurately describes the policy iteration algorithm, including initialization, policy improvement, value update, and convergence check. It also correctly incorporates additional details such as the discretization of the asset space and handling of constraints, which align with the gold answer."}, "llm_echoed_qid": "textbook-103-5-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately describes the policy iteration algorithm, including initialization, policy improvement, value update, and convergence check. It also correctly incorporates additional details such as the discretization of the asset space and handling of constraints, which align with the gold answer."}
{"script_processing_qid": "textbook-114-20-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the short-run cost function TC(Q) = FC + VC(Q) from the data in Table 7.2 and explain its components.\n\nGOLD_ANSWER:\n1. **FC**: $50 (constant for all Q).\n2. **VC(Q)**: Increases with Q (e.g., VC=150 at Q=6).\n3. **TC(Q)**: $50 + VC(Q). For Q=6, TC=$50 + $150 = $200.\n\nCANDIDATE_ANSWER:\nStep 1: The context explicitly defines Total Cost (TC) as the sum of Fixed Cost (FC) and Variable Cost (VC(Q)), where FC is constant regardless of output Q, and VC(Q) depends on Q. This gives the basic structure: TC(Q) = FC + VC(Q).\n\nStep 2: Table 7.2 (not provided in the context) would typically list FC (constant for all Q), VC values for different Q, and possibly TC values. To derive TC(Q), one would directly use the additive relationship TC = FC + VC at each output level Q.\n\nStep 3: The components are:\n- FC: Fixed costs (e.g., leases, salaries) that do not vary with Q.\n- VC(Q): Variable costs (e.g., raw materials, labor for production) that increase with Q.\n\nStep 4: Without specific numerical data from Table 7.2, the exact values of FC and the functional form of VC(Q) (e.g., linear, quadratic) cannot be determined. However, the general relationship TC(Q) = FC + VC(Q) holds based on the definitions provided in the context.\n\nConclusion: The short-run cost function structure is inherently defined by the separation of fixed and variable costs, but deriving the numerical equation requires data from Table 7.2, which is not included in the provided context.\n\nQID: textbook-114-20-2-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-20-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the structure of the short-run cost function TC(Q) = FC + VC(Q) and explains its components accurately. They also correctly note that specific numerical values require data from Table 7.2, which aligns with the gold answer's approach.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-20-2-2", "category": "CORRECT", "explanation": "The candidate correctly identifies the structure of the short-run cost function TC(Q) = FC + VC(Q) and explains its components accurately. They also correctly note that specific numerical values require data from Table 7.2, which aligns with the gold answer's approach."}, "llm_echoed_qid": "textbook-114-20-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly identifies the structure of the short-run cost function TC(Q) = FC + VC(Q) and explains its components accurately. They also correctly note that specific numerical values require data from Table 7.2, which aligns with the gold answer's approach."}
{"script_processing_qid": "textbook-119-10-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. For a regression with $R^2 = 0.25$, $k=5$, and $n=50$, compute the $F$ statistic and test the null hypothesis $H_0: R^2 = 0$ at $\\alpha = 0.01$.\n\nGOLD_ANSWER:\n1. **Compute $F$**: $F = \\frac{0.25/4}{0.75/45} = 3.75$.\n2. **Critical Value**: $F_{0.01}(4, 45) \\approx 3.77$.\n3. **Decision**: Since $3.75 < 3.77$, fail to reject $H_0$.\n4. **Conclusion**: The $R^2$ is not statistically significant at the 1% level.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the F-statistic formula in terms of R²:\nF = (R²/(k−1)) / ((1−R²)/(n−k))\n\nStep 2: Substitute the given values (R² = 0.25, k = 5, n = 50):\nF = (0.25/(5−1)) / ((1−0.25)/(50−5))\n   = (0.25/4) / (0.75/45)\n\nStep 3: Simplify numerator and denominator:\nNumerator = 0.0625\nDenominator = 0.0166667\n\nStep 4: Compute F-statistic:\nF = 0.0625 / 0.0166667 ≈ 3.75\n\nStep 5: Determine critical F-value for α = 0.01 with df1 = k−1 = 4 and df2 = n−k = 45. Critical F-value ≈ 3.77 (from F-table).\n\nStep 6: Compare computed F (3.75) to critical value (3.77). Since 3.75 < 3.77, fail to reject H₀ at α = 0.01.\n\nConclusion: The F-statistic is approximately 3.75, and we do not reject the null hypothesis that R² = 0 at the 1% significance level.\n\nQID: textbook-119-10-1-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-10-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly computes the F-statistic as 3.75, identifies the critical F-value as 3.77, and concludes to fail to reject the null hypothesis at the 1% significance level, matching the gold answer in both calculation and reasoning.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-10-1-1", "category": "CORRECT", "explanation": "The candidate's answer correctly computes the F-statistic as 3.75, identifies the critical F-value as 3.77, and concludes to fail to reject the null hypothesis at the 1% significance level, matching the gold answer in both calculation and reasoning."}, "llm_echoed_qid": "textbook-119-10-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly computes the F-statistic as 3.75, identifies the critical F-value as 3.77, and concludes to fail to reject the null hypothesis at the 1% significance level, matching the gold answer in both calculation and reasoning."}
{"script_processing_qid": "textbook-36-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. **Qualitative**: Discuss the social welfare implications of educational signaling in labor markets. Why might such signaling be considered 'socially wasteful'?\n\nGOLD_ANSWER:\nEducational signaling is socially wasteful because the investment in education (e.g., time, resources) does not enhance productivity ($MP$ remains unchanged). Its sole purpose is to distinguish worker types, leading to deadweight loss without societal benefit. This inefficiency arises from the competitive pursuit of signals rather than productive skill accumulation.\n\nCANDIDATE_ANSWER:\nStep 1: **Identify the signaling mechanism** - Education acts as a costly signal for able workers to distinguish themselves from unable workers. The education itself does not enhance productivity (MP remains unchanged).\n\nStep 2: **Resource allocation inefficiency** - Resources (time, effort, costs) are spent on acquiring education purely for signaling purposes rather than skill development. This represents a deadweight loss to society.\n\nStep 3: **Duplicative investments** - If all able workers must acquire education to prove their type, the aggregate costs incurred by workers to ‘prove’ their innate ability exceed the social benefit, as no productivity gains occur.\n\nStep 4: **Zero-sum redistribution** - Educational signaling merely redistributes wages between worker types (able workers gain higher wages, unable workers lose) without increasing total economic output. The net welfare effect is negative due to the costs of signaling.\n\nStep 5: **Alternative scenario comparison** - If education genuinely improved productivity, its costs would be socially justified. Purely wasteful signaling occurs when costs are incurred solely to overcome information asymmetry, not to create value.\n\nQID: textbook-36-1-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-36-1-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly explains the social welfare implications of educational signaling, aligning with the gold answer. It identifies the signaling mechanism, resource allocation inefficiency, duplicative investments, zero-sum redistribution, and compares it to an alternative scenario, all of which are consistent with the concept of educational signaling being socially wasteful.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-36-1-0-3", "category": "CORRECT", "explanation": "The candidate answer thoroughly explains the social welfare implications of educational signaling, aligning with the gold answer. It identifies the signaling mechanism, resource allocation inefficiency, duplicative investments, zero-sum redistribution, and compares it to an alternative scenario, all of which are consistent with the concept of educational signaling being socially wasteful."}, "llm_echoed_qid": "textbook-36-1-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly explains the social welfare implications of educational signaling, aligning with the gold answer. It identifies the signaling mechanism, resource allocation inefficiency, duplicative investments, zero-sum redistribution, and compares it to an alternative scenario, all of which are consistent with the concept of educational signaling being socially wasteful."}
{"script_processing_qid": "textbook-115-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Compare the implications of measurement error being uncorrelated with the observed measure ($\\mathrm{Cov}(x_{K}, e_{K}) = 0$) versus the CEV assumption ($\\mathrm{Cov}(x_{K}^{*}, e_{K}) = 0$).\n\nGOLD_ANSWER:\n1. **Uncorrelated with Observed Measure**: OLS remains consistent because $\\mathrm{Cov}(x_{K}, v - \\beta_{K}e_{K}) = 0$.\n2. **CEV Assumption**: OLS is inconsistent due to non-zero covariance between $x_{K}$ and the composite error $v - \\beta_{K}e_{K}$, leading to attenuation bias.\n\nCANDIDATE_ANSWER:\nStep 1: Understand key assumptions\n- Case 1: Cov(x_K, e_K) = 0 (Measurement error uncorrelated with observed measure)\n- Case 2: Cov(x_K*, e_K) = 0 (CEV assumption)\n\nStep 2: Analyze Case 1 implications\nWhen Cov(x_K, e_K) = 0:\n1. From e_K = x_K - x_K*, Cov(x_K, e_K) = Var(x_K) - Cov(x_K, x_K*) = 0\n2. This implies Cov(x_K*, e_K) = -Var(e_K)\n3. The measurement error becomes correlated with the true x_K*\n4. OLS estimates become inconsistent for:\n   - β_K due to omitted variable bias\n   - Other β_j coefficients due to e_K correlation with x_K* (which may be correlated with other regressors)\n\nStep 3: Analyze CEV assumption implications\nWhen Cov(x_K*, e_K) = 0:\n1. This implies measurement error is classical (independent of true value)\n2. OLS estimator for β_K has:\n   plim(β̂_K) = β_K [σ_{r_K*}^2 / (σ_{r_K*}^2 + σ_{e_K}^2)]\n   where σ_{r_K*}^2 = variance of x_K* after partialling out other regressors\n3. Results in attenuation bias (β_K underestimated)\n4. Other coefficients may also be biased due to:\n   - Correlation between x_K and other regressors\n   - Endogeneity from measurement error propagation\n\nStep 4: Key differences\n1. Direction of correlation:\n   - Case 1: Negative correlation between e_K and x_K*\n   - CEV: Zero correlation between e_K and x_K*\n2. Bias pattern:\n   - Case 1: All coefficients potentially biased\n   - CEV: Clear attenuation bias formula for β_K\n3. Error structure:\n   - Case 1 creates complex confounding through x_K* correlations\n   - CEV isolates measurement error effects in x_K\n4. Identification:\n   - CEV allows quantifiable bias correction\n   - Case 1 requires stronger assumptions for identification\n\nQID: textbook-115-0-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-115-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer thoroughly analyzes both cases, correctly identifying the implications of each assumption on OLS consistency and bias patterns, matching the gold answer's key points while providing additional detailed reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-0-0-2", "category": "CORRECT", "explanation": "The candidate answer thoroughly analyzes both cases, correctly identifying the implications of each assumption on OLS consistency and bias patterns, matching the gold answer's key points while providing additional detailed reasoning."}, "llm_echoed_qid": "textbook-115-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer thoroughly analyzes both cases, correctly identifying the implications of each assumption on OLS consistency and bias patterns, matching the gold answer's key points while providing additional detailed reasoning."}
{"script_processing_qid": "textbook-84-2-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Verify that $\\mathbf{C} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}$ is indeed the inverse of $\\mathbf{A}\\mathbf{B}$ by showing that $\\mathbf{C}(\\mathbf{A}\\mathbf{B}) = \\mathbf{I}_{n}$ and $(\\mathbf{A}\\mathbf{B})\\mathbf{C} = \\mathbf{I}_{n}$.\n\nGOLD_ANSWER:\n1. **Inverse Verification**:\n   - **First Equality**: $(\\mathbf{B}^{-1}\\mathbf{A}^{-1})(\\mathbf{A}\\mathbf{B}) = \\mathbf{B}^{-1}(\\mathbf{A}^{-1}\\mathbf{A})\\mathbf{B} = \\mathbf{B}^{-1}\\mathbf{I}_{n}\\mathbf{B} = \\mathbf{B}^{-1}\\mathbf{B} = \\mathbf{I}_{n}$.\n   - **Second Equality**: $(\\mathbf{A}\\mathbf{B})(\\mathbf{B}^{-1}\\mathbf{A}^{-1}) = \\mathbf{A}(\\mathbf{B}\\mathbf{B}^{-1})\\mathbf{A}^{-1} = \\mathbf{A}\\mathbf{I}_{n}\\mathbf{A}^{-1} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}_{n}$.\n\nCANDIDATE_ANSWER:\nStep 1: Verify $\\mathbf{C}(\\mathbf{AB}) = \\mathbf{I}_n$.\n$\\mathbf{C}(\\mathbf{AB}) = (\\mathbf{B}^{-1}\\mathbf{A}^{-1})(\\mathbf{A}\\mathbf{B}) = \\mathbf{B}^{-1}(\\mathbf{A}^{-1}\\mathbf{A})\\mathbf{B} = \\mathbf{B}^{-1}\\mathbf{I}_n\\mathbf{B} = \\mathbf{B}^{-1}\\mathbf{B} = \\mathbf{I}_n$.\n\nStep 2: Verify $(\\mathbf{AB})\\mathbf{C} = \\mathbf{I}_n$.\n$(\\mathbf{AB})\\mathbf{C} = (\\mathbf{A}\\mathbf{B})(\\mathbf{B}^{-1}\\mathbf{A}^{-1}) = \\mathbf{A}(\\mathbf{B}\\mathbf{B}^{-1})\\mathbf{A}^{-1} = \\mathbf{A}\\mathbf{I}_n\\mathbf{A}^{-1} = \\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}_n$.\n\nBoth products equal $\\mathbf{I}_n$, confirming $\\mathbf{C} = \\mathbf{B}^{-1}\\mathbf{A}^{-1}$ is the inverse of $\\mathbf{AB}$.\n\nQID: textbook-84-2-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-84-2-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly verifies both equalities step-by-step, matching the gold answer exactly in reasoning and conclusion.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-84-2-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly verifies both equalities step-by-step, matching the gold answer exactly in reasoning and conclusion."}, "llm_echoed_qid": "textbook-84-2-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly verifies both equalities step-by-step, matching the gold answer exactly in reasoning and conclusion."}
{"script_processing_qid": "textbook-114-14-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Derive the conditions under which a warranty can serve as a credible signal of product quality. Use a simple model with two types of producers (high-quality and low-quality).\n\nGOLD_ANSWER:\n1. **Model Setup**: Let $c_H$ and $c_L$ be the expected warranty servicing costs for high-quality and low-quality producers, respectively, with $c_L > c_H$.\n2. **Profit Functions**: High-quality producer's profit: $\\pi_H = p - c_H$, low-quality producer's profit: $\\pi_L = p - c_L$.\n3. **Credibility Condition**: For the warranty to be a credible signal, it must be that $\\pi_L < 0$ if the low-quality producer mimics the high-quality producer by offering the same warranty. Thus, $p < c_L$ ensures low-quality producers cannot profitably offer the warranty.\n\nCANDIDATE_ANSWER:\nStep 1: Establish Cost Asymmetry\nAssume:\n- High-quality producers incur cost $C_H$ to offer warranty\n- Low-quality producers incur cost $C_L$ to offer warranty\nSince low-quality products fail more often, $C_L > C_H$\n\nStep 2: Define Consumer Valuation\nConsumers value:\n- High-quality product at $V_H$\n- Low-quality product at $V_L$\nwhere $V_H > V_L$\n\nStep 3: Signaling Equilibrium Conditions\nFor warranties to act as credible signals:\n1. High-quality producers must prefer offering warranties:\n$V_H - C_H > V_L$ → $C_H < V_H - V_L$\n2. Low-quality producers must prefer NOT offering warranties:\n$V_H - C_L < V_L$ → $C_L > V_H - V_L$\n\nStep 4: Combine Conditions\nThe separating equilibrium exists iff:\n$C_H < V_H - V_L < C_L$\n\nThis ensures:\n- High-quality producers can profitably signal\n- Low-quality producers cannot profitably mimic\nWarranty becomes credible because mimicking is too costly for low-quality producers.\n\nQID: textbook-114-14-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-14-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the conditions for a warranty to serve as a credible signal of product quality, including cost asymmetry and the equilibrium conditions that prevent low-quality producers from mimicking high-quality ones. The reasoning aligns with the gold answer, though it expands on consumer valuation which is implicitly considered in the gold answer's profit functions.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-14-0-1", "category": "CORRECT", "explanation": "The candidate answer correctly derives the conditions for a warranty to serve as a credible signal of product quality, including cost asymmetry and the equilibrium conditions that prevent low-quality producers from mimicking high-quality ones. The reasoning aligns with the gold answer, though it expands on consumer valuation which is implicitly considered in the gold answer's profit functions."}, "llm_echoed_qid": "textbook-114-14-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the conditions for a warranty to serve as a credible signal of product quality, including cost asymmetry and the equilibrium conditions that prevent low-quality producers from mimicking high-quality ones. The reasoning aligns with the gold answer, though it expands on consumer valuation which is implicitly considered in the gold answer's profit functions."}
{"script_processing_qid": "textbook-63-2-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the results of the Fama regression under conditional homoskedasticity (from part g) with those obtained using robust standard errors (from parts e and f). What differences do you observe and why?\n\nGOLD_ANSWER:\n4. **Comparison of Results**:\n   - **Conditional Homoskedasticity**: Assumes constant error variance, leading to potentially inefficient standard errors if heteroskedasticity is present.\n   - **Robust Standard Errors**: Adjust for heteroskedasticity, providing more reliable inference. Differences in standard errors and t-statistics between the two methods indicate the presence of heteroskedasticity in the data.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the key methodological difference - Conditional homoskedasticity (Part g) assumes constant error variance, while robust standard errors (Parts e/f) account for heteroskedasticity and autocorrelation.\n\nStep 2: Observe the Breusch-Godfrey test result (nR² ≈ 27.0 with p=12) indicates significant residual serial correlation in the Part g regression. This violates the OLS assumption of no autocorrelation under conditional homoskedasticity.\n\nStep 3: Compare standard errors - The robust standard errors (Parts e/f) will generally be larger than those from Part g because they adjust for the serial correlation detected in the residuals. This makes t-statistics smaller and confidence intervals wider in Parts e/f compared to Part g.\n\nStep 4: Note that coefficient estimates (β) remain identical across specifications - only their standard errors and related test statistics differ.\n\nStep 5: Conclude that the main difference is greater statistical significance (lower p-values) shown in Part g results compared to Parts e/f, due to underestimated standard errors in Part g that fail to account for residual autocorrelation.\n\nQID: textbook-63-2-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-63-2-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately identifies the key differences between conditional homoskedasticity and robust standard errors, including the impact on standard errors, t-statistics, and the presence of residual autocorrelation. The explanation aligns well with the gold answer, providing a detailed and correct analysis.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-63-2-0-3", "category": "CORRECT", "explanation": "The candidate's answer accurately identifies the key differences between conditional homoskedasticity and robust standard errors, including the impact on standard errors, t-statistics, and the presence of residual autocorrelation. The explanation aligns well with the gold answer, providing a detailed and correct analysis."}, "llm_echoed_qid": "textbook-63-2-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately identifies the key differences between conditional homoskedasticity and robust standard errors, including the impact on standard errors, t-statistics, and the presence of residual autocorrelation. The explanation aligns well with the gold answer, providing a detailed and correct analysis."}
{"script_processing_qid": "textbook-103-7-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the economic interpretation of the savings function $f^{h}[1+r(t)]$ in the context of the overlapping generations model.\n\nGOLD_ANSWER:\nThe savings function $f^{h}[1+r(t)]$ represents the optimal savings of a young agent in generation $t$ as a function of the gross rate of return $1+r(t)$. It captures the trade-off between current and future consumption:\n- **First Term ($w_{t}^{h}(t)$)**: The agent's endowment when young, which is the maximum possible savings if no consumption occurs in the first period.\n- **Second Term ($\\frac{w_{t}^{h}(t+1)}{1+r(t)}$)**: The present value of the agent's endowment when old, discounted by the rate of return. This term reflects the agent's ability to borrow against future income.\n- **Net Savings**: The difference between these terms, scaled by $\\frac{1}{2}$, indicates the agent's optimal savings, balancing current and future consumption under logarithmic utility.\n\nCANDIDATE_ANSWER:\nStep 1: The savings function $f^h[1+r(t)] = \\frac{1}{2}\\left[w_t^h(t) - \\frac{w_t^h(t+1)}{1+r(t)}\\right]$ reflects intertemporal consumption smoothing in the OLG framework.\\n\\nStep 2: The term $w_t^h(t)$ represents current wage income, while $\\frac{w_t^h(t+1)}{1+r(t)}$ is the present value of next period's wage income using the gross return rate $1+r(t)$.\\n\\nStep 3: The difference between current income and discounted future income ($w_t^h(t) - \\frac{w_t^h(t+1)}{1+r(t)}$) represents net lifetime wealth available for allocation between periods.\\n\\nStep 4: The $\\frac{1}{2}$ coefficient originates from the logarithmic utility specification, which splits consumption equally between periods when marginal utilities are balanced.\\n\\nStep 5: Thus, $f^h[1+r(t)]$ represents optimal savings as half of: current resources minus the amount needed to finance future wage income through current saving (i.e., maintaining consumption smoothing given relative wages and interest rates).\n\nQID: textbook-103-7-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-103-7-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately explains the economic interpretation of the savings function in the overlapping generations model, detailing the components and their significance, including the role of logarithmic utility in determining optimal savings.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-7-0-1", "category": "CORRECT", "explanation": "The candidate's answer accurately explains the economic interpretation of the savings function in the overlapping generations model, detailing the components and their significance, including the role of logarithmic utility in determining optimal savings."}, "llm_echoed_qid": "textbook-103-7-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately explains the economic interpretation of the savings function in the overlapping generations model, detailing the components and their significance, including the role of logarithmic utility in determining optimal savings."}
{"script_processing_qid": "textbook-106-10-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the deadweight loss (DWL) of a tax as a function of the tax rate (t), price elasticity of demand (η_d), and price elasticity of supply (η_s). Assume linear supply and demand curves.\n\nGOLD_ANSWER:\n1.  **Define Initial Equilibrium**: Let the initial equilibrium price and quantity be $P_0$ and $Q_0$.\n2.  **Tax Impact**: A tax $t$ shifts the supply curve upward by $t$, leading to new equilibrium price $P_d = P_0 + \\Delta P$ and quantity $Q_1$.\n3.  **Elasticities**: $\\eta_d = \\frac{\\Delta Q}{Q_0} / \\frac{\\Delta P_d}{P_0}$, $\\eta_s = \\frac{\\Delta Q}{Q_0} / \\frac{\\Delta P_s}{P_0}$.\n4.  **DWL Formula**: $DWL = \\frac{1}{2} \\cdot t \\cdot \\Delta Q = \\frac{1}{2} \\cdot t^2 \\cdot \\left(\\frac{\\eta_d \\eta_s}{\\eta_d + \\eta_s}\\right) \\cdot \\frac{Q_0}{P_0}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the standard deadweight loss formula for linear supply/demand curves:\nDWL = ½ * t * ΔQ\n\nStep 2: Calculate the quantity reduction (ΔQ) caused by tax t using elasticities. From elasticity definitions:\nη_d = |(ΔQ/Q)/(ΔP_d/P)| and η_s = (ΔQ/Q)/(ΔP_s/P)\n\nStep 3: With tax t splitting into price changes:\nΔP_d + ΔP_s = t\nUsing elasticity relationships:\nΔQ/Q = η_d*(ΔP_d/P) = η_s*(ΔP_s/P)\n\nStep 4: Solve for price changes:\nΔP_d = (η_s/(η_d + η_s)) * t\nΔP_s = (η_d/(η_d + η_s)) * t\n\nStep 5: Substitute price changes into quantity reduction:\nΔQ = Q * η_d * (ΔP_d/P) = Q * η_d * (η_s t)/(η_d + η_s)P\n\nStep 6: Substitute ΔQ into DWL formula:\nDWL = ½ * t * [Q * η_d η_s t]/[(η_d + η_s)P]\n\nStep 7: Simplify using equilibrium relationships (Q/P is constant for linear curves):\nDWL = (η_d η_s t²)/[2(η_d + η_s)] * (Q/P)\n\nFinal Simplified Form Assuming Constant Q/P:\nDWL = (η_d η_s t²)/(2(η_d + η_s))\n\nQID: textbook-106-10-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-10-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the deadweight loss (DWL) formula step-by-step, matching the gold answer's final expression. The reasoning aligns perfectly with the provided steps and elasticities.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-10-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the deadweight loss (DWL) formula step-by-step, matching the gold answer's final expression. The reasoning aligns perfectly with the provided steps and elasticities."}, "llm_echoed_qid": "textbook-106-10-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the deadweight loss (DWL) formula step-by-step, matching the gold answer's final expression. The reasoning aligns perfectly with the provided steps and elasticities."}
{"script_processing_qid": "textbook-104-8-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Explain Blackwell's example where $v^{\\ast}$ fails to be measurable.\n\nGOLD_ANSWER:\nBlackwell's example uses a non-measurable projection of a Borel set $E \\subseteq [0,1]^2$. The return function $F(x, y, z) = 1$ if $(y, z) \\in E$, and $0$ otherwise, leads to a non-measurable $v^{\\ast}$ because $\\text{Proj}\\_{z}E$ is not Borel-measurable.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the theorem's requirements.\nThe theorem states that if correspondence G (defined by (6)) is nonempty and permits a measurable selection, then the measurable solution v to (5) coincides with v* (the supremum function defined by (2)). Measurability of v* is guaranteed under these conditions.\n\nStep 2: Identify the failure case.\nBlackwell's example constructs a scenario where G does NOT permit a measurable selection, even though G is nonempty. This violates the theorem's sufficient condition.\n\nStep 3: Consequences of the failure.\nWhen no measurable selection exists for G, the supremum in (2) defining v* may require integrating over non-measurable policies. This causes v* to fail measurability despite being well-defined as a value function.\n\nStep 4: Key mechanism.\nThe example leverages the fact that the product σ-algebra (𝓧 × 𝓧) on X × Z may not contain certain sets required to make v* measurable, even if projections of optimal policies exist. This occurs when the exogenous shock space Z and transition function Q create complex interdependencies that evade measurability under the given σ-algebra.\n\nQID: textbook-104-8-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-104-8-1-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate provides a detailed explanation of Blackwell's theorem and conditions for measurability, but it does not directly address the specific example in the gold answer involving the non-measurable projection of a Borel set. The reasoning is correct but does not fully match the gold answer's focus.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-104-8-1-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate provides a detailed explanation of Blackwell's theorem and conditions for measurability, but it does not directly address the specific example in the gold answer involving the non-measurable projection of a Borel set. The reasoning is correct but does not fully match the gold answer's focus."}, "llm_echoed_qid": "textbook-104-8-1-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate provides a detailed explanation of Blackwell's theorem and conditions for measurability, but it does not directly address the specific example in the gold answer involving the non-measurable projection of a Borel set. The reasoning is correct but does not fully match the gold answer's focus."}
{"script_processing_qid": "textbook-118-3-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the OPG regression form of the information matrix test starting from the loglikelihood function, showing how the typical element (15.35) is incorporated into the regression framework.\n\nGOLD_ANSWER:\n1.  **Loglikelihood Setup**: Start with the loglikelihood function $\\ell(\\pmb{y},\\pmb{\\theta})=\\sum\\_{t=1}^{n}\\ell\\_{t}(\\pmb{y}^{t},\\pmb{\\theta})$.\n2.  **Moment Conditions**: The IM test null hypothesis involves the moment conditions: $$\\operatorname{E}\\left[\\frac{\\partial^{2}\\ell\\_{t}(\\pmb\\theta)}{\\partial\\theta\\_{i}\\partial\\theta\\_{j}}+\\frac{\\partial\\ell\\_{t}(\\pmb\\theta)}{\\partial\\theta\\_{i}}\\frac{\\partial\\ell\\_{t}(\\pmb\\theta)}{\\partial\\theta\\_{j}}\\right]=0.$$\n3.  **OPG Regression**: Construct the matrix $M(\\theta)$ with elements as in (15.35) and regress a vector of ones on the combined matrix $[G(\\theta)~M(\\theta)]$, where $G(\\theta)$ contains the gradient terms $\\partial\\ell\\_{t}/\\partial\\theta\\_{i}$.\n4.  **Test Statistic**: The explained sum of squares from this regression is asymptotically $\\chi^{2}$ distributed with $\\frac{1}{2}k(k+1)$ degrees of freedom under the null.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the loglikelihood function ℓ_t(θ) for each observation t. Compute first derivatives g_{ti} = ∂ℓ_t/∂θ_i and second derivatives H_{tij} = ∂²ℓ_t/(∂θ_i∂θ_j).\n\nStep 2: Form the key quantity for the information matrix test: m_{tij} = H_{tij} + g_{ti}g_{tj} (Equation 15.35). These terms measure deviations from the information matrix equality under H0.\n\nStep 3: Construct matrix M with dimensions n × ½k(k+1), where each column corresponds to a unique (i,j) pair (i ≤ j) containing m_{tij} for all t.\n\nStep 4: Build the OPG regression framework:\n   - Regress a vector of ones on [G(θ) M(θ)], where G(θ) is the n × k matrix of first derivatives g_{ti}\n   - This forms the augmented regression 1 = G(θ)α + M(θ)β + ε\n\nStep 5: The test statistic is computed as n - SSR, where SSR is the sum of squared residuals from this regression. Under H0, this follows a χ²(½k(k+1)) distribution asymptotically when [G(θ̂) M(θ̂)] has full rank.\n\nStep 6: The inclusion of M(θ) in the OPG regression directly operationalizes the IM test by testing β = 0 through the explained sum of squares, implementing the information matrix equality verification.\n\nQID: textbook-118-3-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-3-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately follows the steps outlined in the gold answer, correctly incorporating the typical element (15.35) into the OPG regression framework and detailing the construction of the test statistic.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-3-1-0", "category": "CORRECT", "explanation": "The candidate answer accurately follows the steps outlined in the gold answer, correctly incorporating the typical element (15.35) into the OPG regression framework and detailing the construction of the test statistic."}, "llm_echoed_qid": "textbook-118-3-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately follows the steps outlined in the gold answer, correctly incorporating the typical element (15.35) into the OPG regression framework and detailing the construction of the test statistic."}
{"script_processing_qid": "textbook-119-10-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Under what conditions will the incremental $F$ test and the $t$ test for the new variable yield identical results? Prove this mathematically.\n\nGOLD_ANSWER:\n1. **Single Variable Addition**: When adding one variable, $F = t^2$.\n2. **Proof**: For $H_0: \\beta_{\\text{new}} = 0$, the $t$ statistic is $t = \\hat{\\beta}_{\\text{new}}/\\text{se}(\\hat{\\beta}_{\\text{new}})$.\n3. **F Statistic**: $F = \\frac{(\\text{ESS}_{\\text{new}} - \\text{ESS}_{\\text{old}})/1}{\\text{RSS}_{\\text{new}}/(n - k_{\\text{new}})}$, which simplifies to $t^2$.\n4. **Implication**: The $F$ test and two-tailed $t$ test are equivalent for single-variable addition.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that the incremental F-test and t-test will yield identical results when testing a single new variable. This occurs because:\nStep 2: The F-statistic for testing one new regressor (k = 1) is defined as:\n$$F = \\frac{(\\mathrm{ESS}_{\\mathrm{new}} - \\mathrm{ESS}_{\\mathrm{old}})/1}{\\mathrm{RSS}_{\\mathrm{new}}/(n - p_{\\mathrm{new}})}$$\nwhere $p_{\\mathrm{new}}$ is the number of parameters in the new model.\nStep 3: The t-statistic for the coefficient of the new variable $X_3$ is:\n$$t = \\frac{\\hat{\\beta}_3}{\\mathrm{se}(\\hat{\\beta}_3)}$$\nStep 4: In linear regression, $\\mathrm{ESS}_{\\mathrm{new}} - \\mathrm{ESS}_{\\mathrm{old}} = \\hat{\\beta}_3^2 \\cdot \\mathrm{Var}(X_3|X_2)$ (extra explained variance from adding $X_3$).\nStep 5: The standard error $\\mathrm{se}(\\hat{\\beta}_3)$ equals $\\sqrt{\\mathrm{RSS}_{\\mathrm{new}}/((n - p_{\\mathrm{new}}) \\cdot \\mathrm{Var}(X_3|X_2))}$.\nStep 6: Squaring the t-statistic gives:\n$$t^2 = \\frac{\\hat{\\beta}_3^2}{\\mathrm{RSS}_{\\mathrm{new}}/((n - p_{\\mathrm{new}}) \\cdot \\mathrm{Var}(X_3|X_2))} = F$$\nStep 7: Thus, $F = t^2$ when testing a single new variable, meaning their p-values and conclusions will be identical when:\n- Only one new regressor is added\n- Both tests use the same significance level\n- The tests are two-tailed for t (matching the F-test's directional nature).\n\nQID: textbook-119-10-2-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-10-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies the conditions under which the incremental F-test and t-test yield identical results (single variable addition) and provides a detailed mathematical proof showing that F = t². This aligns perfectly with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-10-2-3", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies the conditions under which the incremental F-test and t-test yield identical results (single variable addition) and provides a detailed mathematical proof showing that F = t². This aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-119-10-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies the conditions under which the incremental F-test and t-test yield identical results (single variable addition) and provides a detailed mathematical proof showing that F = t². This aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-96-0-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the Marshallian demand functions for goods 1 and 2 given the CES utility function with $r=0.5$, prices $p\\_{1}=1$, $p\\_{2}=1$, and wealth $w=10$.\n\nGOLD_ANSWER:\n1. **Utility Function**: $U(x\\_{1}, x\\_{2}) = (x\\_{1}^{0.5} + x\\_{2}^{0.5})^{1/0.5}$.\n2. **Budget Constraint**: $p\\_{1}x\\_{1} + p\\_{2}x\\_{2} = w$.\n3. **FOC**: $\\frac{\\partial U/\\partial x\\_{1}}{\\partial U/\\partial x\\_{2}} = \\frac{p\\_{1}}{p\\_{2}}$ leads to $x\\_{1} = x\\_{2}$.\n4. **Substitute**: $x\\_{1} + x\\_{2} = 10 \\implies x\\_{1} = x\\_{2} = 5$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with CES utility function parameterized by r=0.5:\nU = (x₁^0.5 + x₂^0.5)²\n\nStep 2: Calculate marginal utilities:\nMU₁ = ∂U/∂x₁ = 2(x₁^0.5 + x₂^0.5)(0.5x₁^{-0.5}) = (x₁^0.5 + x₂^0.5)/x₁^{0.5}\nMU₂ = ∂U/∂x₂ = (x₁^0.5 + x₂^0.5)/x₂^{0.5}\n\nStep 3: Set MRS equal to price ratio:\nMRS = MU₁/MU₂ = (x₂/x₁)^{0.5} = p₁/p₂\nGiven p₁=p₂=1 → (x₂/x₁)^{0.5} = 1 → x₁ = x₂\n\nStep 4: Substitute into budget constraint:\nx₁ + x₂ = 10 → 2x₁ = 10 → x₁ = x₂ = 5\n\nFinal Marshallian demands:\nx₁ = 5, x₂ = 5\n\nQID: textbook-96-0-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-96-0-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's derivation matches the gold answer exactly, including the correct utility function, marginal utilities, MRS condition, and budget constraint substitution, leading to the same demand functions.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-96-0-0-0", "category": "CORRECT", "explanation": "The candidate's derivation matches the gold answer exactly, including the correct utility function, marginal utilities, MRS condition, and budget constraint substitution, leading to the same demand functions."}, "llm_echoed_qid": "textbook-96-0-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's derivation matches the gold answer exactly, including the correct utility function, marginal utilities, MRS condition, and budget constraint substitution, leading to the same demand functions."}
{"script_processing_qid": "textbook-84-0-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Compare the Tobit and OLS estimates in terms of their magnitudes and statistical significance. Why might the OLS estimates be smaller?\n\nGOLD_ANSWER:\n1. **Magnitude**: Tobit estimates are generally larger than OLS estimates (e.g., -5.82 vs. -2.90 for ecoprc).\n2. **Statistical Significance**: Both models show significant price effects at the 1% level.\n3. **Scaling Factor**: Tobit coefficients must be scaled by a factor between 0 and 1 (from equation 17.27) to estimate partial effects, which often results in larger raw coefficients compared to OLS.\n\nCANDIDATE_ANSWER:\nStep 1: Compare magnitudes and statistical significance\n- The context reveals that the Tobit coefficient estimate for the combined price effect (θ₁ = β₁ + β₂) is -0.16 with a t-statistic ≈ -0.27 (p-value = 0.78). While OLS estimates are not explicitly provided, the text implies Tobit and OLS estimates generally share the same sign direction (negative own-price effect, positive cross-price effect) but differ in magnitude.\n\nStep 2: Explain why OLS estimates might be smaller\nOLS estimates might be smaller due to:\n1. Censoring bias: If the dependent variable (ecolbs) has excess zeros or rounding clusters (focal points), OLS averages these non-responsive observations with continuous responses, attenuating coefficient magnitudes.\n2. Functional form limitations: OLS assumes a linear relationship across the entire distribution, while Tobit accounts for latent demand thresholds. Even with imperfect Tobit assumptions (non-normal errors from rounding), its censoring adjustment partially corrects OLS attenuation.\n3. Boundary clustering: Focal points (e.g., 1 lb reports) create a quasi-censored structure. OLS treats these as exact values, while Tobit better handles their non-linear influence on estimates.\n\nQID: textbook-84-0-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-84-0-0-3\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies reasons why OLS estimates might be smaller, such as censoring bias and functional form limitations, which aligns with the gold answer's implication. However, the candidate does not directly compare the magnitudes and statistical significance of Tobit and OLS estimates as explicitly stated in the gold answer, leading to a partial match.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-84-0-0-3", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies reasons why OLS estimates might be smaller, such as censoring bias and functional form limitations, which aligns with the gold answer's implication. However, the candidate does not directly compare the magnitudes and statistical significance of Tobit and OLS estimates as explicitly stated in the gold answer, leading to a partial match."}, "llm_echoed_qid": "textbook-84-0-0-3", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies reasons why OLS estimates might be smaller, such as censoring bias and functional form limitations, which aligns with the gold answer's implication. However, the candidate does not directly compare the magnitudes and statistical significance of Tobit and OLS estimates as explicitly stated in the gold answer, leading to a partial match."}
{"script_processing_qid": "textbook-102-4-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. What are the implications of Kruskal's Theorem for model specification and efficiency?\n\nGOLD_ANSWER:\nKruskal's Theorem implies that:\n1. **Efficiency**: OLS is as efficient as GLS when $\\mathbf{X}$ spans the characteristic vectors of $\\boldsymbol{\\Omega}$.\n2. **Specification**: Correctly specifying the structure of $\\boldsymbol{\\Omega}$ is unnecessary in this special case, simplifying estimation.\n3. **Robustness**: OLS remains robust to certain forms of heteroscedasticity or autocorrelation if the regressors align with $\\boldsymbol{\\Omega}$'s characteristic vectors.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the key result of Kruskal's Theorem. When the columns of the design matrix $\\mathbf{X}$ are eigenvectors of the error covariance matrix $\\boldsymbol{\\Omega}$, OLS and GLS estimators coincide. This occurs because $\\boldsymbol{\\Omega}$'s action on $\\mathbf{X}$ reduces to scaling by eigenvalues ($\\boldsymbol{\\Omega}\\mathbf{X}_k = \\lambda_k\\mathbf{X}_k$), simplifying GLS to OLS.\n\nStep 2: Implications for model specification. If researchers intentionally structure $\\mathbf{X}$ using $\\boldsymbol{\\Omega}$'s eigenvectors (or linear combinations spanning the same space), they can use OLS without efficiency loss. This avoids the need to estimate $\\boldsymbol{\\Omega}$ for GLS, which is often impractical.\n\nStep 3: Efficiency implications. Since OLS = GLS in this case, OLS achieves the Gauss-Markov efficiency bound despite non-spherical errors. This eliminates the usual efficiency advantage of GLS over OLS when $\\mathbf{X}$ aligns with $\\boldsymbol{\\Omega}$'s eigenvectors.\n\nStep 4: Variance structure simplification. The diagonal variance matrix $\\sigma^2\\boldsymbol{\\Lambda}_K$ implies uncorrelated coefficient estimates, simplifying hypothesis testing and confidence intervals.\n\nSummary: Kruskal's Theorem implies that strategic specification of $\\mathbf{X}$ using $\\boldsymbol{\\Omega}$'s eigenvectors allows OLS to retain full efficiency while avoiding GLS's computational and estimation challenges. This highlights the importance of covariate selection/transformation aligned with error structure in certain applications.\n\nQID: textbook-102-4-2-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-102-4-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the implications of Kruskal's Theorem for model specification and efficiency, aligning well with the gold answer. It correctly identifies the conditions under which OLS and GLS coincide, the efficiency of OLS, and the simplification in model specification and variance structure.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-102-4-2-2", "category": "CORRECT", "explanation": "The candidate answer accurately explains the implications of Kruskal's Theorem for model specification and efficiency, aligning well with the gold answer. It correctly identifies the conditions under which OLS and GLS coincide, the efficiency of OLS, and the simplification in model specification and variance structure."}, "llm_echoed_qid": "textbook-102-4-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the implications of Kruskal's Theorem for model specification and efficiency, aligning well with the gold answer. It correctly identifies the conditions under which OLS and GLS coincide, the efficiency of OLS, and the simplification in model specification and variance structure."}
{"script_processing_qid": "textbook-119-4-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. What are the assumptions required for the OLS estimators of the log-linear model to be BLUE?\n\nGOLD_ANSWER:\nThe assumptions are:\n1. **Linearity**: The model is linear in parameters.\n2. **Random Sampling**: The sample is randomly drawn from the population.\n3. **No Perfect Multicollinearity**: The regressors are not perfectly correlated.\n4. **Zero Conditional Mean**: $E[u_{i}|X_{i}]=0$.\n5. **Homoscedasticity**: $Var(u_{i}|X_{i})=\\sigma^{2}$.\n6. **No Autocorrelation**: $Cov(u_{i},u_{j}|X_{i},X_{j})=0$ for $i \\neq j$.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that the log-linear model (ln Y_i = α + β₂ ln X_i + u_i) is linear in parameters after transformation.\nStep 2: Recall the Gauss-Markov assumptions required for OLS to be BLUE:\n   a. Linearity in parameters\n   b. Random sampling\n   c. No perfect multicollinearity\n   d. Zero conditional mean (E[u_i|ln X_i] = 0)\n   e. Homoscedasticity (Var[u_i|ln X_i] constant)\n   f. No autocorrelation\nStep 3: Verify these assumptions apply to the transformed equation:\n   - The logged model already satisfies linearity in parameters\n   - Explanatory variable ln X_i must not be constant (no perfect multicollinearity)\n   - u_i must have zero mean conditional on ln X_i\n   - Errors must have constant variance conditional on ln X_i\n   - Errors must be uncorrelated with each other\nStep 4: Conclusion: The standard Gauss-Markov assumptions must hold for the transformed log-linear equation to ensure OLS estimators are BLUE.\n\nQID: textbook-119-4-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-4-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly lists and explains all the Gauss-Markov assumptions required for OLS estimators of the log-linear model to be BLUE, matching the gold answer. The step-by-step reasoning aligns perfectly with the assumptions provided.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-4-0-3", "category": "CORRECT", "explanation": "The candidate answer correctly lists and explains all the Gauss-Markov assumptions required for OLS estimators of the log-linear model to be BLUE, matching the gold answer. The step-by-step reasoning aligns perfectly with the assumptions provided."}, "llm_echoed_qid": "textbook-119-4-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly lists and explains all the Gauss-Markov assumptions required for OLS estimators of the log-linear model to be BLUE, matching the gold answer. The step-by-step reasoning aligns perfectly with the assumptions provided."}
{"script_processing_qid": "textbook-42-0-2-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. If $\\gamma<1$, derive the profit function $\\pi(p, w)$ for the Cobb-Douglas production function.\n\nGOLD_ANSWER:\n1.  **Profit Maximization**: $\\max_y \\, p y - C(w, y)$.\n2.  **FOC**: $p = \\frac{\\partial C}{\\partial y} = \\frac{1}{\\gamma} \\gamma (\\dots) y^{1/\\gamma - 1}$.\n3.  **Solve for $y$**: $y^*(p, w) = \\left( \\frac{p}{\\text{constant}} \\right)^{\\gamma/(1-\\gamma)}$.\n4.  **Profit Function**: $\\pi(p, w) = p y^*(p, w) - C(w, y^*(p, w))$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the given Cobb-Douglas cost function:\nC(w, y) = γ (γ/A)^{1/γ} Π_{j=1}^n (w_j/α_j)^{α_j/γ} y^{1/γ}\n\nStep 2: Profit function is π = py - C(w, y). Substitute C(w, y):\nπ = py - γ (γ/A)^{1/γ} [Π_{j=1}^n (w_j/α_j)^{α_j/γ}] y^{1/γ}\n\nStep 3: Maximize profit with respect to y. Take derivative and set to zero:\np = (1/γ) γ (γ/A)^{1/γ} [Π_{j=1}^n (w_j/α_j)^{α_j/γ}] y^{(1/γ)-1}\n\nStep 4: Solve for optimal y*:\ny* = [pA^{1/γ} Π_{j=1}^n (α_j/w_j)^{α_j/γ}]^{γ/(1-γ)}\n\nStep 5: Substitute y* back into profit function and simplify:\nπ(p, w) = (1-γ) [pA^{1/γ} Π_{j=1}^n (α_j/w_j)^{α_j/γ}]^{1/(1-γ)}\n\nFinal form:\nπ(p, w) = (1-γ) (A/γ)^{1/(1-γ)} p^{1/(1-γ)} Π_{j=1}^n (α_j/w_j)^{α_j/(1-γ)}\n\nKey observation: The (1-γ) coefficient confirms requirements for γ < 1 (decreasing returns). If γ=1 (CRS), the exponent 1/(1-γ) becomes undefined and profit maximization fails as firms cannot determine optimal scale.\n\nQID: textbook-42-0-2-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-42-0-2-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the profit function for the Cobb-Douglas production function when γ < 1, following the steps of profit maximization, solving for optimal output, and substituting back into the profit function. The final form and key observation are also accurate.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-42-0-2-1", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the profit function for the Cobb-Douglas production function when γ < 1, following the steps of profit maximization, solving for optimal output, and substituting back into the profit function. The final form and key observation are also accurate."}, "llm_echoed_qid": "textbook-42-0-2-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the profit function for the Cobb-Douglas production function when γ < 1, following the steps of profit maximization, solving for optimal output, and substituting back into the profit function. The final form and key observation are also accurate."}
{"script_processing_qid": "textbook-112-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Compare and contrast the economic implications of 'wear and tear' for free servants versus slaves, according to Adam Smith.\n\nGOLD_ANSWER:\n1. **Free Servants**: Managed by the servants themselves, leading to lower costs and better health outcomes.\n2. **Slaves**: Managed by negligent masters, resulting in higher costs and poorer health.\n3. **Economic Efficiency**: Free labor is more efficient and cost-effective in the long run.\n4. **Societal Impact**: Free labor promotes better overall economic health and productivity.\n\nCANDIDATE_ANSWER:\nStep 1: Adam Smith argues that the 'wear and tear' (maintenance costs) of free servants are borne by their masters through wages. For free servants, wages must be sufficient to sustain their livelihoods and population growth in proportion to labor demand. This creates a self-regulating system where wages adjust to match societal needs (rising with increasing demand or falling with diminished demand).\n\nStep 2: Slaves, though not explicitly mentioned in the provided context, are implicitly contrasted. For slaves, masters directly bear the full cost of maintenance (food, shelter, etc.) regardless of economic conditions, as slaves lack wage autonomy. This creates rigid labor costs unresponsive to market demand.\n\nStep 3: Economic implications: Free labor systems align maintenance costs with productivity and demand, incentivizing population growth when economically beneficial. Slave systems incur fixed maintenance costs irrespective of demand, leading to inefficiencies and stagnation. Smith views free labor’s wage mechanism as a driver of public prosperity, while slavery’s static costs hinder adaptive economic growth.\n\nQID: textbook-112-0-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately reflects Adam Smith's arguments regarding the economic implications of 'wear and tear' for free servants versus slaves. It correctly contrasts the self-regulating wage system of free labor with the fixed costs of slavery, aligning with the gold answer's emphasis on economic efficiency and societal impact.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-0-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately reflects Adam Smith's arguments regarding the economic implications of 'wear and tear' for free servants versus slaves. It correctly contrasts the self-regulating wage system of free labor with the fixed costs of slavery, aligning with the gold answer's emphasis on economic efficiency and societal impact."}, "llm_echoed_qid": "textbook-112-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately reflects Adam Smith's arguments regarding the economic implications of 'wear and tear' for free servants versus slaves. It correctly contrasts the self-regulating wage system of free labor with the fixed costs of slavery, aligning with the gold answer's emphasis on economic efficiency and societal impact."}
{"script_processing_qid": "textbook-114-3-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the Nash equilibrium for the 'Product Choice Problem' (Table 13.3) and explain why the strategies are stable.\n\nGOLD_ANSWER:\n1. **Payoff Matrix Analysis**: The payoff matrix for the 'Product Choice Problem' is:\n   $$\\begin{array}{|c|c|c|}\n   \\hline\n   & \\text{Crispy} & \\text{Sweet} \\\\\n   \\hline\n   \\text{Crispy} & (-5, -5) & (10, 10) \\\\\n   \\hline\n   \\text{Sweet} & (10, 10) & (-5, -5) \\\\\n   \\hline\n   \\end{array}$$\n2. **Nash Equilibrium**: The Nash equilibria are (Crispy, Sweet) and (Sweet, Crispy). In each case, neither firm can improve its payoff by unilaterally changing its strategy given the other firm's choice.\n3. **Stability**: The strategies are stable because any deviation by one firm, given the other's strategy, results in a lower payoff (-5).\n\nCANDIDATE_ANSWER:\nStep 1: Assume the 'Product Choice Problem' involves two firms (e.g., Firm 1 and Firm 2) choosing between producing a High-quality (H) or Low-quality (L) product. The Nash equilibrium requires both firms to choose strategies that are mutual best responses.\n\nStep 2: Analyze payoff matrix entries (hypothesizing a standard setup since Table 13.3 is unavailable):\n- If Firm 1 chooses H, Firm 2’s best response is L (to avoid profit erosion from direct competition).\n- If Firm 1 chooses L, Firm 2’s best response is H (to capture premium market share).\nSymmetrically, the same logic applies to Firm 1’s responses.\n\nStep 3: Identify pure-strategy Nash equilibria as (H, L) and (L, H). These are stable because:\n- In (H, L), Firm 1 cannot increase profits by switching to L (given Firm 2 stays at L), and Firm 2 cannot gain by switching to H (given Firm 1 stays at H).\n- Similarly, in (L, H), neither firm benefits from unilateral deviation.\n\nStep 4: Stability arises because no player has an incentive to deviate unilaterally under Nash equilibrium strategies, as required by the definition in the context.\n\nQID: textbook-114-3-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-3-0-0\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer incorrectly assumes a different payoff matrix (High-quality vs. Low-quality) and derives equilibria (H, L) and (L, H), which do not match the gold answer's equilibria (Crispy, Sweet) and (Sweet, Crispy). The reasoning and conclusions are based on a misinterpretation of the problem setup.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-3-0-0", "category": "INCORRECT", "explanation": "The candidate's answer incorrectly assumes a different payoff matrix (High-quality vs. Low-quality) and derives equilibria (H, L) and (L, H), which do not match the gold answer's equilibria (Crispy, Sweet) and (Sweet, Crispy). The reasoning and conclusions are based on a misinterpretation of the problem setup."}, "llm_echoed_qid": "textbook-114-3-0-0", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer incorrectly assumes a different payoff matrix (High-quality vs. Low-quality) and derives equilibria (H, L) and (L, H), which do not match the gold answer's equilibria (Crispy, Sweet) and (Sweet, Crispy). The reasoning and conclusions are based on a misinterpretation of the problem setup."}
{"script_processing_qid": "textbook-123-10-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the conditions under which the fiscal theory of the price level selects a unique equilibrium exchange rate.\n\nGOLD_ANSWER:\n1. **Government Budget Constraint**: The government sets a real level of seigniorage $x_{1} = x_{2} = 0.5 \\cdot f\\left(\\frac{1}{1+\\epsilon}\\right)\\left[1-\\frac{1}{1+\\epsilon}\\right]$.\n2. **Equilibrium Condition**: The governments can satisfy their budget constraints only if the exchange rate $e = 1$. This condition eliminates other equilibria, selecting a unique rational expectations equilibrium.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that the fiscal theory of the price level (FTPL) resolves equilibrium indeterminacy by using fiscal policy constraints. In monetary models with nominal rigidities, exchange rates often exhibit multiple equilibria influenced by sunspots.\n\nStep 2: Observe that the context specifies sunspot variables with three realizations (probability 1/3 each), which typically enable multiple equilibria. The FTPL eliminates these by imposing fiscal discipline.\n\nStep 3: Identify the key condition: Fiscal policy must satisfy a transversality/govt budget constraint where the present value of future primary surpluses equals current nominal liabilities. This pins down the exchange rate uniquely (case ii in the context).\n\nStep 4: Conclude that the required condition is a fiscal policy rule ensuring the government’s intertemporal budget constraint holds with equality at exactly one exchange rate. This rules out sunspot-driven equilibria by making fiscal sustainability contingent on a unique price level/exchange rate path.\n\nQID: textbook-123-10-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-10-1-0\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the fiscal theory of the price level's role in selecting a unique equilibrium exchange rate through fiscal policy constraints. However, the reasoning does not directly align with the specific conditions (government budget constraint and equilibrium condition) provided in the gold answer, leading to a mismatch in the detailed justification.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-10-1-0", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the fiscal theory of the price level's role in selecting a unique equilibrium exchange rate through fiscal policy constraints. However, the reasoning does not directly align with the specific conditions (government budget constraint and equilibrium condition) provided in the gold answer, leading to a mismatch in the detailed justification."}, "llm_echoed_qid": "textbook-123-10-1-0", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the fiscal theory of the price level's role in selecting a unique equilibrium exchange rate through fiscal policy constraints. However, the reasoning does not directly align with the specific conditions (government budget constraint and equilibrium condition) provided in the gold answer, leading to a mismatch in the detailed justification."}
{"script_processing_qid": "textbook-123-11-3-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. How does the state-space system (2.4.3) restrict the form of the regression equation $Y = X\\beta + \\epsilon$?\n\nGOLD_ANSWER:\nThe state-space system implies specific dynamics for $X$ and $Y$ (e.g., autocovariance structure via (2.4.11)-(2.4.12)). This restricts the moments $E(X^{\\prime}X)$ and $E(X^{\\prime}Y)$, thereby determining $\\beta$. For example, if $X$ and $Y$ are selected from $y\\_{t}$, their autocovariances depend on $A\\_{o}, C, G$.\n\nCANDIDATE_ANSWER:\nStep 1: The state-space system (2.4.3) implicitly enforces the orthogonality condition described in (2.5.2). This condition requires the projection error $Y - X\\beta$ to be orthogonal to the regressors $X$.<br><br>Step 2: Orthogonality implies $E[X^{\\prime}(Y - X\\beta)] = 0$, which simplifies to $E[X^{\\prime}Y] = E[X^{\\prime}X]\\beta$. This is equivalent to the normal equation defining population least squares.<br><br>Step 3: Consequently, the regression equation $Y = X\\beta + \\epsilon$ must satisfy $E[X^{\\prime}\\epsilon] = 0$. This restricts the error term $\\epsilon$ to be uncorrelated with $X$ in expectation, distinguishing it from general regression models where such orthogonality might not hold.<br><br>Step 4: The restriction ensures $\\beta$ represents the population least squares regression vector, making the model a best linear predictor under the state-space system's assumptions.\n\nQID: textbook-123-11-3-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-11-3-2\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate answer correctly identifies the orthogonality condition and its implications for the regression equation, but it does not explicitly connect these points to the state-space system's dynamics or autocovariance structure as described in the gold answer. The reasoning focuses on general regression properties rather than the specific restrictions imposed by the state-space system.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-11-3-2", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate answer correctly identifies the orthogonality condition and its implications for the regression equation, but it does not explicitly connect these points to the state-space system's dynamics or autocovariance structure as described in the gold answer. The reasoning focuses on general regression properties rather than the specific restrictions imposed by the state-space system."}, "llm_echoed_qid": "textbook-123-11-3-2", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate answer correctly identifies the orthogonality condition and its implications for the regression equation, but it does not explicitly connect these points to the state-space system's dynamics or autocovariance structure as described in the gold answer. The reasoning focuses on general regression properties rather than the specific restrictions imposed by the state-space system."}
{"script_processing_qid": "textbook-115-1-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Provide an example where heteroskedasticity in the latent variable model reverses the sign of the partial effect compared to the coefficient.\n\nGOLD_ANSWER:\nConsider the latent variable model:\n$$\ny^{*}=\\beta_{0}+\\beta_{1}x_{1}+e, \\quad e\\mid x_{1}\\sim\\mathrm{Normal}(0,x_{1}^{2})\n$$\nThe response probability is:\n$$\n\\mathrm{P}(y=1\\mid x_{1})=\\Phi(\\beta_{0}/x_{1}+\\beta_{1})\n$$\nThe partial effect:\n$$\n\\frac{\\partial\\mathrm{P}(y=1\\mid x_{1})}{\\partial x_{1}}=-(\\beta_{0}/x_{1}^{2})\\phi(\\beta_{0}/x_{1}+\\beta_{1})\n$$\nIf $\\beta_{0}>0$ and $\\beta_{1}>0$, the partial effect and $\\beta_{1}$ have opposite signs.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that in a latent variable model $y^* = \\mathbf{x}\\beta + e$ with heteroskedasticity in $\\mathrm{Var}(e|\\mathbf{x})$, the probability $\\mathbf{P}(y=1|\\mathbf{x})$ becomes $\\Phi\\left(\\frac{\\mathbf{x}\\beta}{\\sigma(\\mathbf{x})}\\right)$ where $\\sigma(\\mathbf{x})$ is the standard deviation function.\n\nStep 2: Assume heteroskedasticity takes the form $\\mathrm{Var}(e|x) = \\exp(\\gamma x)^2$ for a single regressor $x$. Then $\\sigma(x) = \\exp(\\gamma x)$, and the probability simplifies to $\\Phi\\left(x\\beta / \\exp(\\gamma x)\\right)$.\n\nStep 3: Compute the partial effect of $x$ using the chain rule:\n\\[\n\\frac{\\partial \\mathbf{P}(y=1|x)}{\\partial x} = \\phi\\left(\\frac{x\\beta}{\\exp(\\gamma x)}\\right) \\cdot \\frac{\\beta \\exp(\\gamma x) - x\\beta\\gamma \\exp(\\gamma x)}{\\exp(2\\gamma x)}\n\\]\n\nStep 4: Simplify the numerator to $\\beta\\exp(\\gamma x) - x\\beta\\gamma\\exp(\\gamma x) = \\beta\\exp(\\gamma x)(1 - \\gamma x)$.\nThe partial effect becomes:\n\\[\n\\phi\\left(\\frac{x\\beta}{\\exp(\\gamma x)}\\right) \\cdot \\frac{\\beta(1 - \\gamma x)}{\\exp(\\gamma x)}\n\\]\n\nStep 5: Set $\\beta > 0$ (original positive coefficient). If $\\gamma x > 1$, the term $(1 - \\gamma x)$ becomes negative. Despite $\\beta > 0$, the partial effect reverses sign.\n\nExample: Let $\\beta=1$, $\\gamma=2$, and $x=1$:\n\\[\n\\text{Partial effect} = \\phi\\left(\\frac{1\\cdot 1}{\\exp(2\\cdot 1)}\\right) \\cdot \\frac{1(1 - 2\\cdot 1)}{\\exp(2)} < 0\n\\]\nThis shows heteroskedasticity reverses the sign even though $\\beta > 0$.\n\nQID: textbook-115-1-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-1-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly demonstrates a scenario where heteroskedasticity reverses the sign of the partial effect compared to the coefficient, using a different but valid model and reasoning. The example provided aligns with the concept described in the gold answer, even though the specific model and calculations differ.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-1-1-0", "category": "CORRECT", "explanation": "The candidate answer correctly demonstrates a scenario where heteroskedasticity reverses the sign of the partial effect compared to the coefficient, using a different but valid model and reasoning. The example provided aligns with the concept described in the gold answer, even though the specific model and calculations differ."}, "llm_echoed_qid": "textbook-115-1-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly demonstrates a scenario where heteroskedasticity reverses the sign of the partial effect compared to the coefficient, using a different but valid model and reasoning. The example provided aligns with the concept described in the gold answer, even though the specific model and calculations differ."}
{"script_processing_qid": "textbook-114-12-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the expression for marginal revenue $MR = P + P(1/E_d)$ starting from the definition $MR = \\frac{\\Delta(PQ)}{\\Delta Q}$. Show all steps.\n\nGOLD_ANSWER:\n1. **Expand $\\Delta(PQ)$**: $\\Delta(PQ) = P\\Delta Q + Q\\Delta P + \\Delta P \\Delta Q$ (for small $\\Delta Q$, $\\Delta P \\Delta Q \\approx 0$).\n2. **Marginal Revenue**: $MR = \\frac{\\Delta(PQ)}{\\Delta Q} = P + Q\\frac{\\Delta P}{\\Delta Q}$.\n3. **Elasticity Substitution**: Recall $E_d = \\frac{P}{Q}\\frac{\\Delta Q}{\\Delta P} \\implies \\frac{Q}{P}\\frac{\\Delta P}{\\Delta Q} = \\frac{1}{E_d}$.\n4. **Final Expression**: Substitute into MR: $MR = P + P\\left(\\frac{1}{E_d}\\right)$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the definition of marginal revenue:\nMR = Δ(PQ)/ΔQ\n\nStep 2: Apply the product rule to expand the derivative of PQ with respect to Q:\nMR = P(ΔQ/ΔQ) + Q(ΔP/ΔQ)\nThis accounts for both direct revenue from additional units (P) and the price reduction effect on all units (Q·ΔP/ΔQ)\n\nStep 3: Simplify ΔQ/ΔQ to 1:\nMR = P + Q·(ΔP/ΔQ)\n\nStep 4: Multiply and divide the second term by P to prepare for elasticity substitution:\nMR = P + P·(Q/P)·(ΔP/ΔQ)\n\nStep 5: Recognize that (Q/P)·(ΔP/ΔQ) is the reciprocal of demand elasticity (1/E_d), where E_d = (P/Q)·(ΔQ/ΔP):\nMR = P + P·(1/E_d)\n\nFinal expression: MR = P + P(1/E_d) as required.\n\nQID: textbook-114-12-1-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-114-12-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows all steps to derive the marginal revenue expression, including proper application of the product rule, simplification, and substitution of elasticity. The reasoning and final result match the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-12-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows all steps to derive the marginal revenue expression, including proper application of the product rule, simplification, and substitution of elasticity. The reasoning and final result match the gold answer."}, "llm_echoed_qid": "textbook-114-12-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows all steps to derive the marginal revenue expression, including proper application of the product rule, simplification, and substitution of elasticity. The reasoning and final result match the gold answer."}
{"script_processing_qid": "textbook-107-7-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Interpret the Slutsky decomposition for Marmaduke's claret demand when the price rises from $\\$30$ to $\\$60$, given his demand function $q = 0.02m - 2p$.\n\nGOLD_ANSWER:\n1. **Initial Demand**: $q_1 = 0.02 \\times 6500 - 2 \\times 30 = 130 - 60 = 70$.\n2. **Final Demand**: $q_2 = 0.02 \\times 6500 - 2 \\times 60 = 130 - 120 = 10$.\n3. **Substitution Effect**: Hold utility constant by compensating income to afford the original bundle at new prices:\n   - Original expenditure: $30 \\times 70 = \\$2100$.\n   - Compensated income: $m' = m + (p'_1 - p_1)q_1 = 6500 + (60 - 30)70 = 6500 + 2100 = \\$8600$.\n   - Compensated demand: $q' = 0.02 \\times 8600 - 2 \\times 60 = 172 - 120 = 52$.\n   - Substitution effect: $52 - 70 = -18$.\n4. **Income Effect**: $10 - 52 = -42$.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate original quantity demanded at p = $30\nq₁ = 0.02(6500) - 2(30) = 130 - 60 = 70 units\n\nStep 2: Calculate new quantity demanded at p = $60\nq₂ = 0.02(6500) - 2(60) = 130 - 120 = 10 units\n\nStep 3: Determine total effect (TE)\nTE = q₂ - q₁ = 10 - 70 = -60 units\n\nStep 4: Use given substitution effect (SE)\nSE = -40 units (price rise makes claret relatively more expensive)\n\nStep 5: Calculate income effect (IE)\nIE = TE - SE = (-60) - (-40) = -20 units\n\nInterpretation:\n- Substitution effect (-40): Reduced consumption due to claret becoming relatively more expensive compared to substitutes\n- Income effect (-20): Reduced consumption from equivalent income loss due to price increase\nBoth effects work in the same direction, confirming claret is a normal good. The Slutsky decomposition shows 40 units reduction from substitution and 20 units reduction from income effects when price doubles.\n\nQID: textbook-107-7-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-107-7-0-2\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly calculated the initial and final demands and the total effect. However, the substitution effect was incorrectly stated as -40 instead of -18, leading to an incorrect income effect calculation. The interpretation of the effects is conceptually correct but based on incorrect values.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-7-0-2", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly calculated the initial and final demands and the total effect. However, the substitution effect was incorrectly stated as -40 instead of -18, leading to an incorrect income effect calculation. The interpretation of the effects is conceptually correct but based on incorrect values."}, "llm_echoed_qid": "textbook-107-7-0-2", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly calculated the initial and final demands and the total effect. However, the substitution effect was incorrectly stated as -40 instead of -18, leading to an incorrect income effect calculation. The interpretation of the effects is conceptually correct but based on incorrect values."}
{"script_processing_qid": "textbook-106-11-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Calculate the net benefit per resident of Smalltown if the government provides the fireworks display, given that each resident values the display at $10 and the cost is $1,000.\n\nGOLD_ANSWER:\n1. **Total Benefit**: 500 residents * $10 = $5,000.\n2. **Total Cost**: $1,000.\n3. **Net Benefit**: $5,000 - $1,000 = $4,000.\n4. **Net Benefit per Resident**: $4,000 / 500 = $8.\n\nCANDIDATE_ANSWER:\nStep 1: Identify missing information - The context does not specify the number of residents in Smalltown, which is required to calculate total benefits.\nStep 2: Note that net benefit per resident = (Total benefits - Total cost) ÷ Number of residents\nStep 3: Total benefits = Number of residents × $10 valuation\nStep 4: Without the population size, we cannot compute total benefits or distribute the $1,000 cost across residents\nConclusion: The provided context does not contain enough information to answer this question because the population size of Smalltown is not specified.\n\nQID: textbook-106-11-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-11-0-2\",\n  \"category\": \"INVALID_QUESTION\",\n  \"explanation\": \"The gold answer itself indicates the question is unanswerable or flawed due to missing context, specifically the number of residents in Smalltown.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-11-0-2", "category": "INVALID_QUESTION", "explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, specifically the number of residents in Smalltown."}, "llm_echoed_qid": "textbook-106-11-0-2", "llm_grader_category": "INVALID_QUESTION", "llm_grader_explanation": "The gold answer itself indicates the question is unanswerable or flawed due to missing context, specifically the number of residents in Smalltown."}
{"script_processing_qid": "textbook-119-15-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain why the supply function in the model (19.2.28) and (19.2.22) is overidentified, while the same supply function in the model (19.2.12) and (19.2.22) is just identified.\n\nGOLD_ANSWER:\n1.  **Model (19.2.12) and (19.2.22)**: The supply function excludes only the income variable ($I_t$), which satisfies the order condition $K - k = m - 1$ (just identified).\n2.  **Model (19.2.28) and (19.2.22)**: The supply function excludes both income ($I_t$) and wealth ($R_t$), leading to $K - k > m - 1$ (overidentified). The oversufficiency of restrictions (excluding more variables than necessary) causes overidentification.\n\nCANDIDATE_ANSWER:\nStep 1: Understand identification criteria\nIn simultaneous equations, an equation is:\n- Just identified if the number of excluded predetermined variables (K - k) exactly equals the number of included endogenous variables minus 1 (m - 1)\n- Overidentified if (K - k) > (m - 1)\n\nStep 2: Analyze Model (19.2.12) + (19.2.22)\nSupply function variables: P_t (endogenous), P_{t-1} (predetermined)\nOriginal demand function (19.2.12) variables: P_t (endogenous), I_t (predetermined)\nTotal system's predetermined variables (K): I_t, P_{t-1}\nPredetermined variables excluded from supply (K - k): I_t (1 exclusion)\nm (endogenous variables in supply): Q_t, P_t ⇒ m - 1 = 1\nSince 1 exclusion = 1, supply is just identified\n\nStep 3: Analyze Model (19.2.28) + (19.2.22)\nModified demand adds R_t (predetermined)\nTotal system's predetermined variables (K): I_t, R_t, P_{t-1}\nPredetermined variables excluded from supply (K - k): I_t, R_t (2 exclusions)\nm remains 2 ⇒ m - 1 = 1\nSince 2 exclusions > 1, supply is overidentified\n\nConclusion: Adding R_t in demand increases excluded predetermined variables in supply from 1 to 2, changing identification status from just- to over-identified.\n\nQID: textbook-119-15-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-15-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly explains the identification criteria and applies them accurately to both models, matching the gold answer's reasoning and conclusions.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-15-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly explains the identification criteria and applies them accurately to both models, matching the gold answer's reasoning and conclusions."}, "llm_echoed_qid": "textbook-119-15-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly explains the identification criteria and applies them accurately to both models, matching the gold answer's reasoning and conclusions."}
{"script_processing_qid": "textbook-119-30-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Given the data in Table 15.21, estimate the probability of admission to the graduate program using the logit model with GRE quantitative and verbal scores as explanatory variables.\n\nGOLD_ANSWER:\n1. **Model Specification**: $$P(Y=1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 Q + \\beta_2 V)}}$$\\n2. **Estimate Parameters**: Use MLE to estimate $\\beta_0$, $\\beta_1$, and $\\beta_2$.\\n3. **Predict Probability**: For a given student, plug in their Q and V scores into the estimated model to get the probability of admission.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that the logit model estimates the probability of admission using the logistic function: \\( P(Y=1) = \\frac{e^{(\\beta_1 + \\beta_2 X_{\\text{quant}} + \\beta_3 X_{\\text{verbal}})}}{1 + e^{(\\beta_1 + \\beta_2 X_{\\text{quant}} + \\beta_3 X_{\\text{verbal}})}} \\), where \\( X_{\\text{quant}} \\) and \\( X_{\\text{verbal}} \\) are GRE quantitative and verbal scores.\n\nStep 2: Note that the provided context includes the log-likelihood function for the logit model but does not specify the estimated coefficients (\\( \\beta_1, \\beta_2, \\beta_3 \\)) or numerical values for GRE scores from Table 15.21.\n\nStep 3: Conclude that without the coefficient estimates (from MLE optimization) or the specific GRE score values in Table 15.21, the probability of admission cannot be numerically calculated using the given context alone.\n\nQID: textbook-119-30-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-119-30-0-2\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate correctly identifies the need for coefficient estimates and specific GRE scores to calculate the probability, but their model specification slightly differs from the gold answer (intercept term notation). The core reasoning about missing information aligns with the gold answer's implied requirements.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-30-0-2", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate correctly identifies the need for coefficient estimates and specific GRE scores to calculate the probability, but their model specification slightly differs from the gold answer (intercept term notation). The core reasoning about missing information aligns with the gold answer's implied requirements."}, "llm_echoed_qid": "textbook-119-30-0-2", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate correctly identifies the need for coefficient estimates and specific GRE scores to calculate the probability, but their model specification slightly differs from the gold answer (intercept term notation). The core reasoning about missing information aligns with the gold answer's implied requirements."}
{"script_processing_qid": "textbook-115-7-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n9. Derive the conditional expectation $\\operatorname{E}(y_1 | \\mathbf{x}, y_2 > 0)$ for the Tobit selection model under Assumption 17.3. Show how this leads to Procedure 17.3.\n\nGOLD_ANSWER:\n9. **Conditional Expectation**: Under Assumption 17.3d, $\\operatorname{E}(y_1 | \\mathbf{x}, v_2) = \\mathbf{x}_1\\beta_1 + \\gamma_1 v_2$. For $y_2 > 0$, $v_2 = y_2 - \\mathbf{x}\\delta_2$ is observed. Replacing $\\delta_2$ with its Tobit estimate $\\hat{\\delta}_2$ yields $\\hat{v}_2$, leading to the regression in Procedure 17.3. The error term $e_1 = u_1 - \\gamma_1 v_2$ is orthogonal to $\\mathbf{x}_1$ and $\\hat{v}_2$ by construction.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the Tobit selection model structure:\n\ny₁ = x₁β₁ + u₁ (observed only when y₂ > 0)\ny₂ = max(0, xδ₂ + v₂)\n\nStep 2: Recognize we need E(y₁|x, y₂ > 0) = x₁β₁ + E(u₁|x, xδ₂ + v₂ > 0)\n\nStep 3: Under standard Tobit assumptions (joint normality of u₁, v₂), decompose:\nE(u₁|x, v₂ > -xδ₂) = (σ₁₂/σ₂²)E(v₂|v₂ > -xδ₂)\nwhere σ₁₂ = Cov(u₁,v₂), σ₂² = Var(v₂)\n\nStep 4: For Tobit models, E(v₂|v₂ > -xδ₂) = v₂ = y₂ - xδ₂ when y₂ > 0. Thus:\nE(u₁|x, y₂ > 0) = (σ₁₂/σ₂²)(y₂ - xδ₂)\n\nStep 5: Substitute into original equation:\nE(y₁|x, y₂ > 0) = x₁β₁ + (σ₁₂/σ₂²)(y₂ - xδ₂)\n\nStep 6: Recognize this implies a linear relationship with the residual v₂ = y₂ - xδ₂. Procedure 17.3 replaces δ₂ with its Tobit estimate δ̂₂ from Step (a), then includes v̂_i2 = y_i2 - x_iδ̂₂ as a regressor in Step (b).\n\nStep 7: The coefficient on v̂_i2 estimates (σ₁₂/σ₂²). Testing its significance (t-stat) determines if σ₁₂ ≠ 0 (selection bias exists).\n\nQID: textbook-115-7-2-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-7-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the conditional expectation for the Tobit selection model and shows how it leads to Procedure 17.3, matching the gold answer's explanation and steps.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-7-2-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the conditional expectation for the Tobit selection model and shows how it leads to Procedure 17.3, matching the gold answer's explanation and steps."}, "llm_echoed_qid": "textbook-115-7-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the conditional expectation for the Tobit selection model and shows how it leads to Procedure 17.3, matching the gold answer's explanation and steps."}
{"script_processing_qid": "textbook-118-3-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the test statistic $\\tau\\_{3}$ for skewness starting from the OPG regression framework, showing all intermediate steps and justifying the asymptotic normality under the null hypothesis.\n\nGOLD_ANSWER:\n1.  **OPG Regression Setup**: Start with the OPG regression for the linear regression model: $$1=\\frac{1}{\\hat{\\sigma}^{2}}u\\_{t}(\\hat{\\beta})X\\_{t}b+b\\_{\\sigma}\\frac{u\\_{t}^{2}(\\hat{\\beta})-\\hat{\\sigma}^{2}}{\\hat{\\sigma}^{3}}+c u\\_{t}^{3}(\\hat{\\beta})+\\mathrm{residual}.$$\n2.  **Orthogonality Argument**: Show that the regressor $u\\_{t}^{3}(\\hat{\\beta})$ is asymptotically orthogonal to the other regressors under the null, allowing simplification to: $$1=c u\\_{t}^{3}(\\hat{\\beta})+\\mathrm{residual}.$$\n3.  **Normalized Residuals**: Express the statistic in terms of normalized residuals $e\\_{t}\\equiv\\hat{u}\\_{t}/\\hat{\\sigma}$: $$\\tau\\_{3}=(6n)^{-1/2}\\sum\\_{t=1}^{n}e\\_{t}^{3}.$$\n4.  **Asymptotic Normality**: Use the fact that $\\operatorname{E}(e\\_{t}^{3})=0$ and $\\operatorname{Var}(e\\_{t}^{3})=6$ under normality to justify the asymptotic standard normal distribution.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the OPG testing regression under H₀ (normality):\n1 = \\frac{1}{\\hat{\\sigma}^2}u_t(\\hat{\\beta})X_tb + b_\\sigma\\frac{u_t^2(\\hat{\\beta})-\\hat{\\sigma}^2}{\\hat{\\sigma}^3} + c u_t^3(\\hat{\\beta}) + \\text{residual}.\n\nStep 2: Note that normalized residuals are defined as e_t ≡ û_t/\\hat{\\sigma}. Substitute û_t = e_t\\hat{\\sigma} into the test regressor:\nu_t^3(\\hat{\\beta}) = (e_t\\hat{\\sigma})^3 = e_t^3\\hat{\\sigma}^3.\n\nStep 3: Since û_t and \\hat{\\sigma} are OLS estimates, the terms involving X_t and u_t^2 are asymptotically orthogonal to u_t^3 under H₀ (normality). Thus, the estimator of c in the OPG regression simplifies to:\n\\hat{c} ≈ \\frac{1}{n}\\sum_{t=1}^n e_t^3 + o_p(1).\n\nStep 4: Compute the asymptotic variance of \\sum e_t^3 under H₀. For normal variables, E[e_t^6] = 15 and E[e_t^3] = 0, giving Var(\\sum e_t^3) = n(E[e_t^6] - (E[e_t^3])^2) = 15n - 0 = 15n. However, corrections from parameter estimation (β and σ) reduce this to 6n (due to orthogonality between scores and skewness under H₀).\n\nStep 5: Standardize the sum to obtain the test statistic:\nτ_3 = \\frac{\\sum_{t=1}^n e_t^3}{\\sqrt{6n}} = (6n)^{-1/2}\\sum_{t=1}^n e_t^3.\n\nStep 6: Apply the Central Limit Theorem: Under H₀, τ_3 → N(0,1) asymptotically because (1/\\sqrt{n})\\sum e_t^3 → N(0,6) and scaling by (6n)^{-1/2} yields unit variance.\n\nQID: textbook-118-3-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-3-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps outlined in the gold answer, including the OPG regression setup, orthogonality argument, normalization of residuals, and justification of asymptotic normality. The reasoning and final test statistic match the gold answer precisely.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-3-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps outlined in the gold answer, including the OPG regression setup, orthogonality argument, normalization of residuals, and justification of asymptotic normality. The reasoning and final test statistic match the gold answer precisely."}, "llm_echoed_qid": "textbook-118-3-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps outlined in the gold answer, including the OPG regression setup, orthogonality argument, normalization of residuals, and justification of asymptotic normality. The reasoning and final test statistic match the gold answer precisely."}
{"script_processing_qid": "textbook-104-8-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. How can a second-order Markov process be represented as a first-order process?\n\nGOLD_ANSWER:\nA second-order process can be written as a first-order process by expanding the state space to $(W \\times W, \\mathcal{W} \\times \\mathcal{W})$. The transition function $Q[(w, w'), A \\times B] = P(w, w', B)$ if $w' \\in A$, and $0$ otherwise, where $P$ is the second-order transition kernel.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that a second-order Markov process has dependence on the two most recent states (z_t and z_{t-1}).\nStep 2: Redefine the state space to include pairs of consecutive states. Let the new state variable be s_t = (z_t, z_{t-1}) ∈ Z × Z.\nStep 3: Rewrite the transition dynamics as s_{t+1} = (g(s_t, w_t), z_t), where g now operates on the expanded state s_t = (z_t, z_{t-1}) and shock w_t.\nStep 4: Apply Theorem 8.9 to this new state space S = Z × Z. The measurable function becomes G(s, w) = (g(z_t, z_{t-1}, w), z_t), and the transition function Q(s, A) is defined through the probability measure μ over shocks.\nStep 5: This reformulation makes the process first-order Markov over the expanded state space S, as the next state s_{t+1} now depends only on the current state s_t (and the i.i.d. shock w_t).\n\nQID: textbook-104-8-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-104-8-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly explains how to represent a second-order Markov process as a first-order process by expanding the state space and redefining the transition dynamics, matching the gold answer's approach.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-104-8-0-3", "category": "CORRECT", "explanation": "The candidate answer correctly explains how to represent a second-order Markov process as a first-order process by expanding the state space and redefining the transition dynamics, matching the gold answer's approach."}, "llm_echoed_qid": "textbook-104-8-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly explains how to represent a second-order Markov process as a first-order process by expanding the state space and redefining the transition dynamics, matching the gold answer's approach."}
{"script_processing_qid": "textbook-119-19-1-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the GLS estimator $\\hat{\\beta}\\_{2}^{\\*}$ for the transformed model $Y\\_{i}^{\\*}=\\beta\\_{1}^{\\*}X\\_{0i}^{\\*}+\\beta\\_{2}^{\\*}X\\_{i}^{\\*}+u\\_{i}^{\\*}$.\n\nGOLD_ANSWER:\n1. **Transformed Model**: $Y\\_{i}^{\\*} = \\beta\\_{1}^{\\*}X\\_{0i}^{\\*} + \\beta\\_{2}^{\\*}X\\_{i}^{\\*} + u\\_{i}^{\\*}$.\\n2. **Minimization**: Minimize $\\sum (Y\\_{i}^{\\*} - \\hat{\\beta}\\_{1}^{\\*}X\\_{0i}^{\\*} - \\hat{\\beta}\\_{2}^{\\*}X\\_{i}^{\\*})^{2}$.\\n3. **Normal Equations**: Solve $\\frac{\\partial}{\\partial \\hat{\\beta}\\_{1}^{\\*}} = 0$ and $\\frac{\\partial}{\\partial \\hat{\\beta}\\_{2}^{\\*}} = 0$ to obtain the GLS estimator: $$ \\hat{\\beta}\\_{2}^{\\*} = \\frac{(\\sum w\\_{i})(\\sum w\\_{i}X\\_{i}Y\\_{i}) - (\\sum w\\_{i}X\\_{i})(\\sum w\\_{i}Y\\_{i})}{(\\sum w\\_{i})(\\sum w\\_{i}X\\_{i}^{2}) - (\\sum w\\_{i}X\\_{i})^{2}} $$ where $w\\_{i} = 1/\\sigma\\_{i}^{2}$.\n\nCANDIDATE_ANSWER:\nStep 1: Set up the transformed model using weighted variables:\nY_i* = Y_i/σ_i\nX_{0i}* = 1/σ_i (intercept term)\nX_i* = X_i/σ_i\n\nStep 2: Write the normal equations for GLS by minimizing ∑[(Y_i/σ_i) - β₁*(1/σ_i) - β₂*(X_i/σ_i)]²:\n(1) ∑(Y_i/σ_i²) = β₁*∑(1/σ_i²) + β₂*∑(X_i/σ_i²)\n(2) ∑(X_iY_i/σ_i²) = β₁*∑(X_i/σ_i²) + β₂*∑(X_i²/σ_i²)\n\nStep 3: Solve equations (1) and (2) simultaneously for β₂*:\nβ₂* = [∑(1/σ_i²)∑(X_iY_i/σ_i²) - ∑(X_i/σ_i²)∑(Y_i/σ_i²)] \n       / [∑(1/σ_i²)∑(X_i²/σ_i²) - (∑X_i/σ_i²)²]\n\nThis matches the weighted least squares formula where weights w_i = 1/σ_i².\n\nQID: textbook-119-19-1-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-19-1-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the GLS estimator for the transformed model, matching the gold answer in both the setup and the final formula. The steps and reasoning are consistent with the expected derivation process.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-19-1-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the GLS estimator for the transformed model, matching the gold answer in both the setup and the final formula. The steps and reasoning are consistent with the expected derivation process."}, "llm_echoed_qid": "textbook-119-19-1-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the GLS estimator for the transformed model, matching the gold answer in both the setup and the final formula. The steps and reasoning are consistent with the expected derivation process."}
{"script_processing_qid": "textbook-110-2-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Derive the optimal bid function for a second-price auction with $N$ bidders and uniform valuations on $[\\underline{\\tau}, \\overline{\\tau}]$.\n\nGOLD_ANSWER:\n1. **Optimal Bid**: In a second-price auction, the dominant strategy is to bid one's true valuation: $\\beta(\\tau) = \\tau$.\n2. **Proof**: Bidding higher or lower than $\\tau$ cannot increase the bidder's expected utility but may decrease it.\n3. **Revenue**: The expected price paid by the winner is the second-highest order statistic, $\\frac{N-1}{N+1}\\overline{\\tau}$.\n\nCANDIDATE_ANSWER:\nStep 1: In a first-price auction with uniform valuations on [0,1], assume all bidders use symmetric strategy β(τ). A bidder with value τ chooses bid β(τ̂) to maximize expected payoff: (τ - β(τ̂)) * [Probability of winning].\n\nStep 2: The probability of winning equals the probability that all other N-1 bidders have τ_j < τ̂. For uniform distributions, this is (τ̂)^{N-1}.\n\nStep 3: Expected payoff becomes (τ - β(τ̂)) * (τ̂)^{N-1}. Take first-order condition by differentiating with respect to τ̂ and set to zero:\n-dβ/dτ̂ * τ̂^{N-1} + (τ - β)(N-1)τ̂^{N-2} = 0.\n\nStep 4: In equilibrium (τ̂ = τ), substitute and simplify:\nβ'(τ)τ + (N-1)(τ - β(τ)) = 0.\nThis yields differential equation: β'(τ) = (N-1)(τ - β(τ))/τ.\n\nStep 5: Solve using integrating factors. The solution satisfying β(0) = 0 is:\nβ(τ) = (N-1)τ/N.\n\nStep 6: Verify expected price calculation: The maximum value among N bidders has expectation N/(N+1). Multiply by bidding strategy (N-1)/N to get (N-1)/(N+1).\n\nQID: textbook-110-2-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-110-2-1-3\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer incorrectly derives the optimal bid function for a first-price auction instead of a second-price auction. The gold answer clearly states that in a second-price auction, the dominant strategy is to bid one's true valuation, which the candidate does not address.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-2-1-3", "category": "INCORRECT", "explanation": "The candidate's answer incorrectly derives the optimal bid function for a first-price auction instead of a second-price auction. The gold answer clearly states that in a second-price auction, the dominant strategy is to bid one's true valuation, which the candidate does not address."}, "llm_echoed_qid": "textbook-110-2-1-3", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer incorrectly derives the optimal bid function for a first-price auction instead of a second-price auction. The gold answer clearly states that in a second-price auction, the dominant strategy is to bid one's true valuation, which the candidate does not address."}
{"script_processing_qid": "textbook-114-22-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Using the quadratic long-run average cost function $\\mathrm{LAC}=2.38 - 0.6153Q + 0.0536Q^2$, derive the output level at which average cost is minimized.\n\nGOLD_ANSWER:\n1. **Differentiate LAC**: $\\frac{d\\mathrm{LAC}}{dQ} = -0.6153 + 0.1072Q$.\n2. **Set to zero**: $-0.6153 + 0.1072Q = 0$.\n3. **Solve for Q**: $Q = \\frac{0.6153}{0.1072} \\approx 5.74$ (in hundreds of millions of dollars).\n4. **Interpretation**: The minimum average cost occurs at $\\$574$ million in assets.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the quadratic long-run average cost (LAC) function: LAC = 2.38 - 0.6153Q + 0.0536Q².\nStep 2: To find the output level (Q) that minimizes average cost, take the first derivative of LAC with respect to Q: d(LAC)/dQ = -0.6153 + 2(0.0536)Q = -0.6153 + 0.1072Q.\nStep 3: Set the first derivative equal to zero and solve for Q: -0.6153 + 0.1072Q = 0 ⇒ Q = 0.6153 / 0.1072 ≈ 5.74.\nStep 4: Interpret the result using the context provided. The derived Q ≈ 5.74 corresponds to \\$574 million in total assets, as stated in the passage (note the implicit unit scaling: Q = 5.74 represents 574 million due to units used in the study).\n\nQID: textbook-114-22-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-114-22-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the output level at which average cost is minimized, matching the gold answer in both the mathematical derivation and the interpretation of the result.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-114-22-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the output level at which average cost is minimized, matching the gold answer in both the mathematical derivation and the interpretation of the result."}, "llm_echoed_qid": "textbook-114-22-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the output level at which average cost is minimized, matching the gold answer in both the mathematical derivation and the interpretation of the result."}
{"script_processing_qid": "textbook-110-8-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the conditions under which a Nash equilibrium exists in infinite strategy sets.\n\nGOLD_ANSWER:\n1. **Strategy Sets**: $S^{h}$ is non-empty, compact, and convex.\n2. **Payoff Functions**: $v^{h}$ is continuous in $[s]$ and quasiconcave in $s^{h}$.\n3. **Best-Response**: $\\beta^{h}$ is non-empty and convex-valued.\n4. **Fixed Point**: $\\beta$ has a fixed point $[s^{\\*}]$ such that $s^{\\*h} \\in \\beta^{h}([s^{\\*}]^{-h})$ for all $h$.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that Theorem 10.2 establishes two key properties:\n- Convexity of best-response sets β^h([s]^−h) due to quasiconcave payoff functions v^h and convex strategy sets S^h\n- Upper-hemicontinuity of the product correspondence β = β^1 × β^2 × ...\n\nStep 2: Apply Kakutani's Fixed-Point Theorem, which requires:\n1. A non-empty, compact, convex strategy space S\n2. A non-empty-valued correspondence β\n3. Convex-valuedness of β (established by Theorem 10.2)\n4. Upper-hemicontinuity of β (directly stated in the context)\n\nStep 3: Combine these requirements with the given conditions from the context:\n- Convexity of S^h (satisfies convex strategy space)\n- Quasiconcavity of v^h (ensures convex best-response sets)\n- Upper-hemicontinuity of β (directly given)\n\nStep 4: Conclude that a Nash equilibrium exists when:\n1. All strategy sets S^h are convex\n2. All payoff functions v^h are quasiconcave in their own strategy\n3. The strategy space S is non-empty and compact (implicit requirement)\n4. Best-response correspondence β is upper-hemicontinuous (as established)\n\nQID: textbook-110-8-2-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-110-8-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the conditions for the existence of a Nash equilibrium in infinite strategy sets, aligning with the gold answer. The candidate's reasoning, which includes the application of Kakutani's Fixed-Point Theorem and the properties of strategy sets and payoff functions, is accurate and comprehensive.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-8-2-2", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the conditions for the existence of a Nash equilibrium in infinite strategy sets, aligning with the gold answer. The candidate's reasoning, which includes the application of Kakutani's Fixed-Point Theorem and the properties of strategy sets and payoff functions, is accurate and comprehensive."}, "llm_echoed_qid": "textbook-110-8-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the conditions for the existence of a Nash equilibrium in infinite strategy sets, aligning with the gold answer. The candidate's reasoning, which includes the application of Kakutani's Fixed-Point Theorem and the properties of strategy sets and payoff functions, is accurate and comprehensive."}
{"script_processing_qid": "textbook-119-8-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Define and differentiate between Type I and Type II errors in the context of hypothesis testing.\n\nGOLD_ANSWER:\n1. **Type I Error**: Rejecting $H_0$ when it is true. Probability denoted by $\\alpha$.\n2. **Type II Error**: Not rejecting $H_0$ when it is false. Probability denoted by $\\beta$.\n3. **Trade-off**: For a fixed sample size, decreasing $\\alpha$ increases $\\beta$ and vice versa.\n\nCANDIDATE_ANSWER:\nStep 1: In hypothesis testing, a Type I error occurs when the null hypothesis (H₀: θ = θ*) is incorrectly rejected even though it is true. This corresponds to the probability α (e.g., 5% in the 95% confidence interval example), which represents the significance level. Step 2: A Type II error occurs when the null hypothesis is not rejected even though it is false (i.e., the alternative hypothesis H₁ is true). Step 3: The provided context explicitly mentions the critical region (outside the confidence interval) and acceptance region (within the confidence interval) but does not formally define Type I/II errors. However, the 95% probability statement implies that the Type I error rate is 5% (the complement of the confidence level). The context does not discuss the Type II error probability (β) or its dependence on effect size and sample size.\n\nQID: textbook-119-8-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-119-8-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly defines Type I and Type II errors, aligning with the gold answer. It also mentions the significance level (α) and the lack of discussion on Type II error probability (β), which is consistent with the provided context.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-119-8-1-2", "category": "CORRECT", "explanation": "The candidate answer correctly defines Type I and Type II errors, aligning with the gold answer. It also mentions the significance level (α) and the lack of discussion on Type II error probability (β), which is consistent with the provided context."}, "llm_echoed_qid": "textbook-119-8-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly defines Type I and Type II errors, aligning with the gold answer. It also mentions the significance level (α) and the lack of discussion on Type II error probability (β), which is consistent with the provided context."}
{"script_processing_qid": "textbook-8-0-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Analyze the role of trade costs in determining the equilibrium distribution of industrial firms.\n\nGOLD_ANSWER:\n4. **Trade Costs and Firm Distribution**:\n   - High trade costs favor local production to meet demand.\n   - Intermediate trade costs encourage agglomeration due to linkages.\n   - Low trade costs allow firms to exploit wage differentials, leading to re-dispersion without labor mobility.\n\nCANDIDATE_ANSWER:\nStep 1: Identify key features of the industrial sector. The industrial sector operates under increasing returns to scale with firm-level fixed costs (α) and variable costs (βx). Firms face a cost function C(h) = q_i^μ w_i^(1-μ)(α + βx(h)), where q_i (land costs) and w_i (labor costs) are region-specific inputs.\n\nStep 2: Understand the role of trade costs. While trade costs are not explicitly modeled in the provided equations, they implicitly affect equilibrium firm distribution by influencing (1) market access for industrial goods and (2) input prices (q_i, w_i) through labor-land allocation between sectors.\n\nStep 3: Analyze spatial equilibrium mechanisms. Firms cluster where they can minimize total costs (q_i^μ w_i^(1-μ)) while benefiting from scale economies (α + βx). High trade costs amplify the importance of local market size (linked to agricultural sector productivity via y_i = g(L_i^A, K_i)), favoring concentration in regions with larger agricultural bases that provide local demand and input supplies.\n\nStep 4: Link to sectoral interdependence. Trade costs affect labor allocation (L_i^A vs. industrial labor) by altering the relative profitability of industrial production. Lower trade costs reduce spatial advantages, allowing firms to disperse while remaining competitive in multiple markets.\n\nStep 5: Equilibrium outcome. The equilibrium distribution N_i* balances (i) scale economies requiring concentrated production and (ii) trade costs that determine the feasibility of serving dispersed markets. Higher trade costs strengthen agglomeration forces, while lower costs permit dispersion.\n\nQID: textbook-8-0-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-8-0-1-3\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly identifies the role of trade costs in determining firm distribution but focuses on cost minimization and scale economies rather than explicitly addressing the three scenarios (high, intermediate, low trade costs) outlined in the gold answer. The reasoning is detailed but does not fully align with the gold answer's structure.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-8-0-1-3", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly identifies the role of trade costs in determining firm distribution but focuses on cost minimization and scale economies rather than explicitly addressing the three scenarios (high, intermediate, low trade costs) outlined in the gold answer. The reasoning is detailed but does not fully align with the gold answer's structure."}, "llm_echoed_qid": "textbook-8-0-1-3", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly identifies the role of trade costs in determining firm distribution but focuses on cost minimization and scale economies rather than explicitly addressing the three scenarios (high, intermediate, low trade costs) outlined in the gold answer. The reasoning is detailed but does not fully align with the gold answer's structure."}
{"script_processing_qid": "textbook-110-5-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Using the Lagrangean framework, derive the condition under which a firm operates in a technically efficient manner ($\\Phi^{f}(\\mathbf{q}^{f}) = 0$).\n\nGOLD_ANSWER:\nFrom the FOC for production: $-\\mu_{f}\\Phi_{i}^{f}(\\mathbf{q}^{f}) + \\kappa_{i} = 0$. If $\\kappa_{i} > 0$ (good $i$ is scarce) and $q_{i}^{f} \\neq 0$, then $\\mu_{f} > 0$ and $\\Phi^{f}(\\mathbf{q}^{f}) = 0$. This implies the firm operates on its production possibility frontier, leaving no resources idle.\n\nCANDIDATE_ANSWER:\nStep 1: Technical efficiency (Φ^f(q^f) = 0) implies the firm operates on its production frontier. Using the Lagrangian framework, consider the firm's optimization problem subject to its technology constraint.\n\nStep 2: Formulate the Lagrangian with Φ^f(q^f) = 0 as the constraint. Let L = Objective Function + λ·Φ^f(q^f), where λ is the Lagrange multiplier.\n\nStep 3: Take partial derivatives of L with respect to all inputs/outputs in q^f and set them to zero. For each component q_i^f: ∂L/∂q_i^f = ∂(Objective)/∂q_i^f + λ·∂Φ^f/∂q_i^f = 0.\n\nStep 4: For technical efficiency (Φ^f(q^f) = 0 binding), the Lagrange multiplier λ must be non-zero. This implies proportionality between marginal effects in the objective and the gradient of Φ^f.\n\nStep 5: From Theorem 9.3, efficiency requires MRT_{ij}^f = κ_j/κ_i for all firms. This corresponds to ∂Φ^f/∂q_i^f divided by ∂Φ^f/∂q_j^f equaling κ_j/κ_i, derived from the first-order conditions.\n\nQID: textbook-110-5-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-110-5-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the condition for technical efficiency using the Lagrangian framework, aligning with the gold answer's requirement that the firm operates on its production possibility frontier with no resources idle. The steps logically lead to the conclusion that the Lagrange multiplier must be non-zero for efficiency, which is consistent with the gold answer's implication of μ_f > 0.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-110-5-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly derives the condition for technical efficiency using the Lagrangian framework, aligning with the gold answer's requirement that the firm operates on its production possibility frontier with no resources idle. The steps logically lead to the conclusion that the Lagrange multiplier must be non-zero for efficiency, which is consistent with the gold answer's implication of μ_f > 0."}, "llm_echoed_qid": "textbook-110-5-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the condition for technical efficiency using the Lagrangian framework, aligning with the gold answer's requirement that the firm operates on its production possibility frontier with no resources idle. The steps logically lead to the conclusion that the Lagrange multiplier must be non-zero for efficiency, which is consistent with the gold answer's implication of μ_f > 0."}
{"script_processing_qid": "textbook-82-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Using the Cobb-Douglas production function $Y=g(K)(z H)^{\\alpha}$, show that the markup is inversely proportional to the labor share $s_{H}$.\n\nGOLD_ANSWER:\n1. **Marginal Product of Labor**: $F_{H} = \\alpha g(K)(z H)^{\\alpha-1} z = \\alpha \\frac{Y}{H}$.\n2. **Markup Formula**: $\\mu = \\frac{P z F_{H}}{W} = \\frac{P z (\\alpha Y / H)}{W} = \\alpha \\frac{P Y}{W H} = \\alpha s_{H}^{-1}$.\n3. **Interpretation**: The markup $\\mu$ is inversely related to the labor share $s_{H} = \\frac{W H}{P Y}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the Cobb-Douglas production function:\nY = g(K)(zH)^α\n\nStep 2: Compute the marginal product of labor (F_H):\nF_H = ∂Y/∂H = αg(K)z(zH)^{α-1} = αg(K)z^αH^{α-1}\n\nStep 3: Substitute F_H into the markup formula:\nμ = (P·z·F_H)/W = (P·z·αg(K)z^αH^{α-1})/W\n\nStep 4: Simplify using the production function Y = g(K)(zH)^α:\nμ = (P·α·g(K)z^{α+1}H^{α-1})/W = (αPY)/(WH)  (since z^{α+1}H^{α-1} = z(zH)^α/H and Y = g(K)(zH)^α)\n\nStep 5: Relate to labor share s_H = (WH)/(PY):\nμ = α(PY)/(WH) = α/s_H\n\nThis demonstrates μ ∝ 1/s_H, establishing the inverse proportionality between markup and labor share.\n\nQID: textbook-82-0-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-82-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the inverse proportionality between the markup and the labor share using the Cobb-Douglas production function. The reasoning and calculations align with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-82-0-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the inverse proportionality between the markup and the labor share using the Cobb-Douglas production function. The reasoning and calculations align with the gold answer."}, "llm_echoed_qid": "textbook-82-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the inverse proportionality between the markup and the labor share using the Cobb-Douglas production function. The reasoning and calculations align with the gold answer."}
{"script_processing_qid": "textbook-106-9-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Calculate the deadweight loss from a $\\$0.50$ tax on ice cream, assuming linear supply and demand curves with slopes of 0.1 and -0.1, respectively.\n\nGOLD_ANSWER:\n1.  **Slopes**: $\\frac{dP}{dQ}_S = 0.1$, $\\frac{dP}{dQ}_D = -0.1$.\n2.  **Quantity Change**: $\\Delta Q = 10$ cones (from 100 to 90).\n3.  **Deadweight Loss**:\n    - $$ DWL = \\frac{1}{2} \\times t \\times \\Delta Q = \\frac{1}{2} \\times 0.5 \\times 10 = \\$2.50 $$\n\nCANDIDATE_ANSWER:\nStep 1: Identify slopes as price-to-quantity relationships (dP/dQ = -0.1 for demand, 0.1 for supply).\nStep 2: Calculate original equilibrium quantity Q₀ = (a - b)/0.2 using supply/demand intercepts.\nStep 3: Post-tax demand becomes P = a - 0.1Q - 0.50. Set equal to supply: a - 0.1Q - 0.50 = b + 0.1Q\nStep 4: Solve for new equilibrium quantity: Q₁ = (a - b - 0.50)/0.2\nStep 5: Determine quantity reduction: ΔQ = Q₀ - Q₁ = 0.50/0.2 = 2.5 units\nStep 6: Calculate DWL = ½ × tax × ΔQ = 0.5 × 0.50 × 2.5 = \\$0.625\n\nQID: textbook-106-9-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-9-1-3\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's calculation of the quantity change (ΔQ = 2.5 units) and deadweight loss (DWL = \\$0.625) is incorrect. The gold answer shows ΔQ = 10 cones and DWL = \\$2.50, which aligns with the given slopes and tax.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-9-1-3", "category": "INCORRECT", "explanation": "The candidate's calculation of the quantity change (ΔQ = 2.5 units) and deadweight loss (DWL = \\$0.625) is incorrect. The gold answer shows ΔQ = 10 cones and DWL = \\$2.50, which aligns with the given slopes and tax."}, "llm_echoed_qid": "textbook-106-9-1-3", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's calculation of the quantity change (ΔQ = 2.5 units) and deadweight loss (DWL = \\$0.625) is incorrect. The gold answer shows ΔQ = 10 cones and DWL = \\$2.50, which aligns with the given slopes and tax."}
{"script_processing_qid": "textbook-43-0-1-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Why does indeterminacy require the labor demand curve to slope upward more steeply than the supply curve?\n\nGOLD_ANSWER:\nThis ensures that conjectured price changes shift equilibrium employment in the 'wrong' direction (e.g., higher $p$ reduces $L$), creating self-fulfilling expectations.\n\nCANDIDATE_ANSWER:\nStep 1: In standard models, labor demand slopes downward (firms hire less as wages rise) and labor supply slopes upward (workers supply more as wages rise). Stable equilibrium occurs where they intersect.\n\nStep 2: For indeterminacy, Benhabib-Farmer's model requires upward-sloping labor demand (due to increasing returns from externalities/monopolistic competition). This inversion creates a non-standard labor market structure.\n\nStep 3: When labor demand slopes upward more steeply than supply, small deviations from equilibrium are amplified. A rise in employment increases labor demand more than labor supply (due to steeper demand slope), creating positive feedback.\n\nStep 4: This amplifies expectations-driven shifts: Optimistic expectations → increased hiring → higher equilibrium employment (validating expectations). Multiple self-fulfilling equilibria emerge, causing indeterminacy.\n\nQID: textbook-43-0-1-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-43-0-1-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the mechanism by which a steeper upward-sloping labor demand curve leads to indeterminacy, aligning with the gold answer's focus on self-fulfilling expectations and equilibrium shifts.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-43-0-1-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the mechanism by which a steeper upward-sloping labor demand curve leads to indeterminacy, aligning with the gold answer's focus on self-fulfilling expectations and equilibrium shifts."}, "llm_echoed_qid": "textbook-43-0-1-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the mechanism by which a steeper upward-sloping labor demand curve leads to indeterminacy, aligning with the gold answer's focus on self-fulfilling expectations and equilibrium shifts."}
{"script_processing_qid": "textbook-123-8-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Show that the matrix $M$ in the Lagrangian formulation is symplectic and explain its implications for eigenvalues.\n\nGOLD_ANSWER:\n1. **Symplectic Condition**: $M J M^{\\prime} = J$, where $J = \\begin{pmatrix} 0 & -I \\\\ I & 0 \\end{pmatrix}$.\n2. **Eigenvalues**: For symplectic $M$, eigenvalues occur in reciprocal pairs ($\\lambda$, $\\lambda^{-1}$). This structure is useful for solving the system.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the structure of matrix M from the first-order conditions. The Hamiltonian system derived from the Lagrangian yields state equations:\n\n$x_{t+1} = A x_t - B Q^{-1} B' \\mu_{t+1}$\n$\\mu_t = R x_t + A' \\mu_{t+1}$\n\nStep 2: Express in matrix form. After rearrangement, these equations define:\n\n$M = \\begin{bmatrix}\nA + B Q^{-1} B' A^{-\\top} R & -B Q^{-1} B' A^{-\\top} \\\\\n-A^{-\\top} R & A^{-\\top}\n\\end{bmatrix}$\n\nStep 3: Verify symplecticity. A symplectic matrix satisfies $M' \\Omega M = \\Omega$ where $\\Omega = \\begin{bmatrix} 0 & I \\\\ -I & 0 \\end{bmatrix}$. Direct computation confirms this property using blocks from M.\n\nStep 4: Eigenvalue implications. Symplectic matrices have eigenvalues in reciprocal pairs ($\\lambda, 1/\\lambda$). Stability requires eigenvalues of $A - BF$ (submatrix of M) to lie inside the unit circle. The symplectic structure ensures unstable eigenvalues would require reciprocals outside the unit circle, but optimality via the Riccati equation suppresses this by selecting only stable eigenvalues.\n\nQID: textbook-123-8-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-8-2-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the symplectic condition and verifies it for matrix M. It also accurately explains the implications for eigenvalues, including the reciprocal pairs and stability considerations, matching the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-8-2-2", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the symplectic condition and verifies it for matrix M. It also accurately explains the implications for eigenvalues, including the reciprocal pairs and stability considerations, matching the gold answer."}, "llm_echoed_qid": "textbook-123-8-2-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the symplectic condition and verifies it for matrix M. It also accurately explains the implications for eigenvalues, including the reciprocal pairs and stability considerations, matching the gold answer."}
{"script_processing_qid": "textbook-122-6-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the asymptotic distribution of $\\sqrt{n}(\\hat{\\beta}_{1}-\\beta_{1})$ step-by-step, starting from the expression involving $s_{x}^{2}$ and $n^{-1/2}\\sum_{i=1}^{n}(x_{i}-\\bar{x})u_{i}$.\n\nGOLD_ANSWER:\n1. **Decomposition**: Express $n^{-1/2}\\sum_{i=1}^{n}(x_{i}-\\bar{x})u_{i}$ as $n^{-1/2}\\sum_{i=1}^{n}(x_{i}-\\mu)u_{i} + (\\mu-\\bar{x})[n^{-1/2}\\sum_{i=1}^{n}u_{i}]$.\n2. **CLT Application**: Under MLR.3 and MLR.5, $n^{-1/2}\\sum_{i=1}^{n}(x_{i}-\\mu)u_{i}$ converges to $N(0,\\sigma^{2}\\sigma_{x}^{2})$.\n3. **Probability Limit**: Show $\\mathrm{plim}(\\mu-\\bar{x})=0$ and $\\mathrm{plim}((\\mu-\\bar{x})[n^{-1/2}\\sum_{i=1}^{n}u_{i}])=0$.\n4. **Final Form**: Combine results to obtain $\\sqrt{n}(\\hat{\\beta}_{1}-\\beta_{1}) \\xrightarrow{d} N(0,\\sigma^{2}/\\sigma_{x}^{2})$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the given expression: \n$$\\sqrt{n}(\\hat{\\beta}_{1}-\\beta_{1}) = \\frac{1}{s_{x}^{2}} \\left[ n^{-1/2} \\sum_{i=1}^{n} (x_{i} - \\bar{x})u_{i} \\right]$$\n\nStep 2: Note that by the law of large numbers (LLN), the sample variance $s_{x}^{2} \\xrightarrow{p} \\sigma_{x}^{2}$, where $\\sigma_{x}^{2} = \\text{Var}(x)$.\n\nStep 3: For the numerator $n^{-1/2} \\sum_{i=1}^{n} (x_{i} - \\bar{x})u_{i}$, observe that replacing $\\bar{x}$ with $\\mu$ (the population mean of $x_i$) introduces an asymptotically negligible error. Thus, $n^{-1/2} \\sum_{i=1}^{n} (x_{i} - \\bar{x})u_{i}$ has the same asymptotic distribution as $n^{-1/2} \\sum_{i=1}^{n} (x_{i} - \\mu)u_{i}$, which is Normal$(0, \\sigma^{2}\\sigma_{x}^{2})$ under MLR.3 and MLR.5.\n\nStep 4: Apply Slutsky’s theorem: Since $\\frac{1}{s_{x}^{2}} \\xrightarrow{p} \\frac{1}{\\sigma_{x}^{2}}$ and $n^{-1/2} \\sum_{i=1}^{n} (x_{i} - \\bar{x})u_{i} \\xrightarrow{d} \\text{Normal}(0, \\sigma^{2}\\sigma_{x}^{2})$, their product converges in distribution to:\n$$\\frac{1}{\\sigma_{x}^{2}} \\cdot \\text{Normal}(0, \\sigma^{2}\\sigma_{x}^{2}) = \\text{Normal}\\left(0, \\frac{\\sigma^{2}}{\\sigma_{x}^{2}}\\right).$$\n\nConclusion: The asymptotic distribution of $\\sqrt{n}(\\hat{\\beta}_{1} - \\beta_{1})$ is Normal$(0, \\sigma^{2}/\\sigma_{x}^{2})$.\n\nQID: textbook-122-6-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-6-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to derive the asymptotic distribution, including the application of the LLN, CLT, and Slutsky's theorem, and arrives at the correct final distribution.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-6-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to derive the asymptotic distribution, including the application of the LLN, CLT, and Slutsky's theorem, and arrives at the correct final distribution."}, "llm_echoed_qid": "textbook-122-6-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to derive the asymptotic distribution, including the application of the LLN, CLT, and Slutsky's theorem, and arrives at the correct final distribution."}
{"script_processing_qid": "textbook-123-7-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the government’s budget constraint $H(t+1)-H(t)=\\left\\{[1+r(t)]\\frac{p(t+1)}{\\bar{p}}-1\\right\\}H(t)$ from the given model assumptions.\n\nGOLD_ANSWER:\n1. **Interest Payments**: The government pays interest on currency at a nominal rate of $[1+r(t)]\\frac{p(t+1)}{\\bar{p}}$.\n2. **Financing**: The government prints new money to finance these payments, so the change in money supply $H(t+1)-H(t)$ equals the interest payments minus the principal: $\\left\\{[1+r(t)]\\frac{p(t+1)}{\\bar{p}}-1\\right\\}H(t)$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the government's obligation to finance nominal interest payments on currency. The nominal interest due at t+1 is r(t)H(t).\nStep 2: Recognize this must be financed through money creation: H(t+1) - H(t) = [1 + r(t)] ⋅ (value adjustment) ⋅ H(t) - H(t)\nStep 3: Incorporate price level changes using the equilibrium relationship. The real value adjustment requires multiplying by p(t+1)/p̄ to maintain purchasing power.\nStep 4: Substitute the equilibrium condition [1 + r(t)] = [1 + r(t)]p(t)/p̄ from the model's arbitrage equation between currency and bonds.\nStep 5: Combine these elements algebraically to arrive at the budget constraint:\nH(t+1) - H(t) = {[1 + r(t)]p(t+1)/p̄ - 1}H(t)\n\nQID: textbook-123-7-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-7-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the government's budget constraint by logically incorporating interest payments, money creation, and price level adjustments, aligning with the gold answer's reasoning.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-7-0-0", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the government's budget constraint by logically incorporating interest payments, money creation, and price level adjustments, aligning with the gold answer's reasoning."}, "llm_echoed_qid": "textbook-123-7-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the government's budget constraint by logically incorporating interest payments, money creation, and price level adjustments, aligning with the gold answer's reasoning."}
{"script_processing_qid": "textbook-122-32-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Discuss the conditions under which missing data in a regression analysis does not introduce bias in OLS estimators.\n\nGOLD_ANSWER:\n1. **Missing at Random (MAR)**: Data are missing randomly conditional on observed variables. This preserves the exogeneity of the error term.\n2. **Ignorable Missingness**: The missingness mechanism is independent of the unobserved data and the model parameters.\n3. **Consequence**: Under MAR, OLS remains unbiased and consistent, though less efficient due to reduced sample size.\n\nCANDIDATE_ANSWER:\nStep 1: Identify the key assumption in the Classical Errors-in-Variables (CEV) framework. The CEV model requires measurement error (e₁) to be uncorrelated with both (1) the true unobserved variable (e.g., faminc* or smoked*) and (2) the regression error term (u).\n\nStep 2: Recognize that violating CEV assumptions introduces bias. In the marijuana example, measurement error (e₁) correlates with smoked*, violating CEV and causing biased OLS estimators.\n\nStep 3: State the conditions for no bias. Missing data (or measurement error) does not bias OLS estimators when:\n   - (Condition 1) Measurement error is uncorrelated with the true variable (Cov(e₁, x*) = 0).\n   - (Condition 2) Measurement error is uncorrelated with the regression error term (Cov(e₁, u) = 0).\n\nStep 4: Note a critical implication. Even under CEV assumptions, the coefficient on the mismeasured variable (e.g., β₁ for faminc) is biased toward zero. However, coefficients on other variables (e.g., hsGPA, SAT) remain unbiased if the mismeasured variable is uncorrelated with them.\n\nStep 5: Highlight the exception. If the true causal effect of the mismeasured variable is zero (β₁ = 0, as hypothesized in the family income example), CEV-compliant measurement error does not introduce bias because attenuation bias reduces an already zero effect.\n\nQID: textbook-122-32-0-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-122-32-0-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer focuses on measurement error and the CEV framework, which is unrelated to the question about missing data in regression analysis. The gold answer correctly identifies conditions like MAR and ignorable missingness, which are relevant to missing data, not measurement error.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-32-0-2", "category": "INCORRECT", "explanation": "The candidate's answer focuses on measurement error and the CEV framework, which is unrelated to the question about missing data in regression analysis. The gold answer correctly identifies conditions like MAR and ignorable missingness, which are relevant to missing data, not measurement error."}, "llm_echoed_qid": "textbook-122-32-0-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer focuses on measurement error and the CEV framework, which is unrelated to the question about missing data in regression analysis. The gold answer correctly identifies conditions like MAR and ignorable missingness, which are relevant to missing data, not measurement error."}
{"script_processing_qid": "textbook-123-15-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Under what circumstances is the Lagrange multiplier $\\Phi$ equal to zero, and what does this imply for optimal tax policies?\n\nGOLD_ANSWER:\nThe Lagrange multiplier $\\Phi$ is equal to zero when the government's initial claims $-b\\_{0}$ against the private sector equal the present value of all future government expenditures. This implies that the household's present-value budget does not exert any additional constraining effect on welfare maximization beyond what is already present in the economy's technology. In this case, the government does not need to resort to any distortionary taxation, as the optimal labor tax rate $\\tau\\_{t}^{n}(s^{t})=0$.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the role of Φ\nΦ is the Lagrange multiplier associated with the implementability constraint derived from the household's budget constraint and government policies. It reflects the marginal welfare cost of raising revenue through distortionary taxes.\n\nStep 2: Identify when Φ = 0\nΦ equals zero when the implementability constraint does not bind. This occurs if the government can achieve its fiscal requirements (financing expenditures and initial debt b₀) without needing to distort economic decisions through taxes. Specifically, this happens when:\n1. The government has sufficient initial assets (negative b₀) to cover all future expenditures, or\n2. The Ramsey allocation coincides with the first-best allocation (where lump-sum taxes are available)\n\nStep 3: Implications for optimal tax policies\nWhen Φ = 0, the first-order conditions simplify to:\n- For consumption: u_c(s^t) = θ_t(s^t)\n- For labor: -u_ℓ(s^t) = -θ_t(s^t)\nThis implies u_ℓ(s^t)/u_c(s^t) = 1, meaning no tax distortion (labor taxes τₙ = 0). Thus, Φ = 0 indicates optimal tax policies require zero labor income taxation, as the government can finance expenditures without distortionary taxes.\n\nQID: textbook-123-15-1-2\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-15-1-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly identifies that the Lagrange multiplier Φ equals zero when the implementability constraint does not bind, and accurately describes the implications for optimal tax policies, including the absence of distortionary labor taxes. This aligns with the gold answer, which states that Φ is zero when the government's initial claims equal the present value of future expenditures, leading to no need for distortionary taxation.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-15-1-2", "category": "CORRECT", "explanation": "The candidate's answer correctly identifies that the Lagrange multiplier Φ equals zero when the implementability constraint does not bind, and accurately describes the implications for optimal tax policies, including the absence of distortionary labor taxes. This aligns with the gold answer, which states that Φ is zero when the government's initial claims equal the present value of future expenditures, leading to no need for distortionary taxation."}, "llm_echoed_qid": "textbook-123-15-1-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly identifies that the Lagrange multiplier Φ equals zero when the implementability constraint does not bind, and accurately describes the implications for optimal tax policies, including the absence of distortionary labor taxes. This aligns with the gold answer, which states that Φ is zero when the government's initial claims equal the present value of future expenditures, leading to no need for distortionary taxation."}
{"script_processing_qid": "textbook-112-41-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. **Quantitative**: Using Smith's framework, model the wealth transfer between town and country as a terms-of-trade equation where $P_t$ is the price index of urban goods and $P_c$ is the price index of rural goods. Show how urban regulations that raise $P_t/P_c$ redistribute real income.\n\nGOLD_ANSWER:\n1. **Model Setup**: Let $Y_t$ be urban output and $Y_c$ rural output. Town's real income: $RI_t = \\frac{Y_t}{P_c}$; Country's real income: $RI_c = \\frac{Y_c}{P_t}$.\n2. **Regulation Effect**: Urban policies artificially inflate $P_t$ via monopolistic pricing (e.g., corporation laws). This reduces $RI_c$ since $\\frac{\\partial RI_c}{\\partial P_t} = -\\frac{Y_c}{P_t^2} < 0$.\n3. **Net Transfer**: The town gains $\\Delta RI_t = Y_t(\\frac{1}{P_c} - \\frac{1}{P_c^0})$ where $P_c^0$ is the counterfactual rural price without distortions.\n4. **Inequality**: The ratio $\\frac{RI_t}{RI_c} = \\frac{Y_t P_t}{Y_c P_c}$ grows with $P_t/P_c$, capturing Smith's 'unnatural' redistribution.\n\nCANDIDATE_ANSWER:\nStep 1: Define terms of trade as the ratio Pt/Pc. Urban goods (manufactures) are exported by the town, and rural goods (provisions/materials) are imported.\\n\\nStep 2: Urban regulations (e.g., corporate monopolies) raise Pt by limiting competition (as stated: 'a bye-law... limits competition more effectually'), artificially inflating urban goods' prices relative to rural goods.\\n\\nStep 3: Town's real income = (Value of urban exports)/Pc. With higher Pt/Pc, each unit of urban goods sold buys more rural goods (as per Smith: 'The dearer the latter are sold, the cheaper the former are bought'), increasing the town's purchasing power.\\n\\nStep 4: Country's real income = (Value of rural exports)/Pt. Higher Pt/Pc reduces rural income’s purchasing power for urban goods (they export more rural goods per unit of urban goods acquired).\\n\\nStep 5: Net effect: Wealth transfers from country to town via (Pt/Pc)↑ redistributing real income toward urban dwellers, consistent with Smith’s critique that regulations 'give a greater share to the town... and a less to the country.'\n\nQID: textbook-112-41-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-112-41-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately models the wealth transfer between town and country using Smith's framework, correctly identifying the effects of urban regulations on the terms of trade (Pt/Pc) and the redistribution of real income. The explanation aligns with the gold answer in terms of both the mechanism and the outcome of the wealth transfer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-41-0-0", "category": "CORRECT", "explanation": "The candidate answer accurately models the wealth transfer between town and country using Smith's framework, correctly identifying the effects of urban regulations on the terms of trade (Pt/Pc) and the redistribution of real income. The explanation aligns with the gold answer in terms of both the mechanism and the outcome of the wealth transfer."}, "llm_echoed_qid": "textbook-112-41-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately models the wealth transfer between town and country using Smith's framework, correctly identifying the effects of urban regulations on the terms of trade (Pt/Pc) and the redistribution of real income. The explanation aligns with the gold answer in terms of both the mechanism and the outcome of the wealth transfer."}
{"script_processing_qid": "textbook-112-3-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Using Smith's framework, model the round-about foreign trade where British manufactures → Virginia tobacco → Riga hemp/flax. Calculate the effective capital required versus direct trade.\n\nGOLD_ANSWER:\n1. **Direct Trade (British manufactures ↔ Riga hemp/flax)**:\n   - Capital required: $C$.\n   - Single operation replaces two capitals (1 domestic).\n\n2. **Round-About Trade**:\n   - Phase 1: $C$ buys Virginia tobacco with British manufactures (replaces 1 domestic capital).\n   - Phase 2: Tobacco buys Riga hemp/flax (replaces 0 domestic capitals).\n   - Total capital employed: $3C$ (3x direct trade) due to sequential dependencies.\n   - **Effective Capital**: $\\text{Effective Capital}_{\\text{round-about}} = 3 \\times \\text{Capital}_{\\text{direct}}$.\n   - Time delay: Returns depend on two trade cycles, doubling liquidity risk.\n\nCANDIDATE_ANSWER:\nStep 1: Identify trade types in Smith's framework\n- British manufactures → Virginia tobacco = Foreign trade of consumption (replaces 1 domestic capital + 1 foreign capital)\n- Virginia tobacco → Riga hemp = Carrying trade (withdrawn from domestic support entirely)\n\nStep 2: Calculate capital requirements\nRound-about trade:\n- First transaction: 1 unit capital supporting domestic (British manufactures)\n- Second transaction: 1 unit capital supporting foreign (no domestic impact)\nTotal effective domestic capital = 1 unit\n\nDirect trade (hypothetical British manufactures → Riga hemp):\n- Foreign trade of consumption: 1 unit capital supporting domestic\nTotal effective domestic capital = 1 unit\n\nStep 3: Compare effective capital\n- Round-about trade requires 2 units total capital (1 domestic-effective + 1 foreign) to achieve same final good (hemp)\n- Direct trade requires only 1 unit domestic-effective capital\n\nConclusion: The round-about trade requires twice the effective capital compared to direct trade under Smith's framework, as half of its total capital fails to support domestic productive labor.\n\nQID: textbook-112-3-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-112-3-0-1\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the trade types and compares capital requirements, but their calculation of effective capital (2x vs. direct trade) differs from the gold answer's 3x multiplier. The reasoning partially aligns but underestimates the round-about trade's capital intensity due to sequential dependencies.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-112-3-0-1", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the trade types and compares capital requirements, but their calculation of effective capital (2x vs. direct trade) differs from the gold answer's 3x multiplier. The reasoning partially aligns but underestimates the round-about trade's capital intensity due to sequential dependencies."}, "llm_echoed_qid": "textbook-112-3-0-1", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the trade types and compares capital requirements, but their calculation of effective capital (2x vs. direct trade) differs from the gold answer's 3x multiplier. The reasoning partially aligns but underestimates the round-about trade's capital intensity due to sequential dependencies."}
{"script_processing_qid": "textbook-122-40-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Propose a formal hypothesis test to determine whether D.C. should be classified as an influential observation, detailing the test statistic and decision rule.\n\nGOLD_ANSWER:\n1.  **Test Statistic**: Use Cook's Distance ($D_i$) to measure D.C.'s influence: $$D_i = \\frac{(\\hat{\\beta} - \\hat{\\beta}_{(-i)})^T (X^TX) (\\hat{\\beta} - \\hat{\\beta}_{(-i)})}{k \\cdot MSE},$$ where $\\hat{\\beta}_{(-i)}$ is the estimate without D.C.\n2.  **Decision Rule**: If $D_i > F_{0.5, k, n-k}$ (the median of the F-distribution), D.C. is influential. Here, the large shift in coefficients and $R^2$ when dropping D.C. would likely exceed this threshold.\n3.  **Alternative**: Use DFBETAS to assess D.C.'s impact on each coefficient, with $|DFBETAS| > 2/\\sqrt{n}$ indicating influence.\n\nCANDIDATE_ANSWER:\nStep 1: Specify the hypothesis test framework.\nH₀: D.C. is not an influential observation (coefficients remain unchanged when D.C. is excluded)\nH₁: D.C. is influential (exclusion significantly changes coefficients)\n\nStep 2: Use DFBETAS test statistic:\nFor each coefficient β_j:\nDFBETAS_j = (β_j^(full) - β_j^(-D.C.)) / SE(β_j^(-D.C.))\nWhere β_j^(full) = coefficient from model with D.C.\nβ_j^(-D.C.) = coefficient from model without D.C. (shown in context)\n\nStep 3: Compute critical value threshold:\nThreshold = 2/√n\nAssuming n = 51 (50 states + D.C.), threshold ≈ 0.28\n\nStep 4: Decision rule:\nReject H₀ if any |DFBETAS_j| > 0.28\nConclusion: D.C. is influential if any coefficient's DFBETAS exceeds this threshold\n\nQID: textbook-122-40-1-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-40-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly proposes a hypothesis test using DFBETAS, details the test statistic, and provides a clear decision rule. This aligns with the alternative method mentioned in the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-40-1-3", "category": "CORRECT", "explanation": "The candidate's answer correctly proposes a hypothesis test using DFBETAS, details the test statistic, and provides a clear decision rule. This aligns with the alternative method mentioned in the gold answer."}, "llm_echoed_qid": "textbook-122-40-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly proposes a hypothesis test using DFBETAS, details the test statistic, and provides a clear decision rule. This aligns with the alternative method mentioned in the gold answer."}
{"script_processing_qid": "textbook-123-23-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the Kalman filter update equations for $\\hat{x}_{t+1}$ and $\\Sigma_{t+1}$ starting from the state-space model $x_{t+1}=A x_{t}+C w_{t+1}$ and $y_{t}=G x_{t}+v_{t}$.\n\nGOLD_ANSWER:\n1. **State Prediction**: $\\hat{x}_{t+1|t} = A \\hat{x}_{t|t}$.\n2. **Covariance Prediction**: $\\Sigma_{t+1|t} = A \\Sigma_{t|t} A^{\\prime} + C C^{\\prime}$.\n3. **Kalman Gain**: $K_{t} = \\Sigma_{t|t-1} G^{\\prime} (G \\Sigma_{t|t-1} G^{\\prime} + R)^{-1}$.\n4. **State Update**: $\\hat{x}_{t+1|t+1} = \\hat{x}_{t+1|t} + K_{t} (y_{t} - G \\hat{x}_{t+1|t})$.\n5. **Covariance Update**: $\\Sigma_{t+1|t+1} = (I - K_{t} G) \\Sigma_{t+1|t}$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with prior estimate \\(\\hat{x}_t\\) and covariance \\(\\Sigma_t\\).\nStep 2: Compute Kalman gain \\(K_t = A \\Sigma_t G' (G \\Sigma_t G' + R)^{-1}\\).\nStep 3: Update state estimate using measurement \\(y_t\\):\n\\(\\hat{x}_{t+1} = (A - K_t G)\\hat{x}_t + K_t y_t\\).\nStep 4: Update covariance matrix via:\n\\(\\Sigma_{t+1} = A \\Sigma_t A' + CC' - A \\Sigma_t G'(G \\Sigma_t G' + R)^{-1}G \\Sigma_t A'\\).\nThis combines prediction (\\(A\\hat{x}_t\\)) with measurement correction (\\(K_t(y_t - G\\hat{x}_t)\\)), and adjusts covariance for process noise (\\(CC'\\)) while reducing uncertainty through the Kalman gain term.\n\nQID: textbook-123-23-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-123-23-0-0\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's Kalman gain and state update equations are incorrect. The Kalman gain should use the predicted covariance \\(\\Sigma_{t+1|t}\\), not \\(\\Sigma_t\\), and the state update should incorporate the predicted state \\(\\hat{x}_{t+1|t}\\), not \\(\\hat{x}_t\\). The covariance update is also incorrectly derived.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-23-0-0", "category": "INCORRECT", "explanation": "The candidate's Kalman gain and state update equations are incorrect. The Kalman gain should use the predicted covariance \\(\\Sigma_{t+1|t}\\), not \\(\\Sigma_t\\), and the state update should incorporate the predicted state \\(\\hat{x}_{t+1|t}\\), not \\(\\hat{x}_t\\). The covariance update is also incorrectly derived."}, "llm_echoed_qid": "textbook-123-23-0-0", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's Kalman gain and state update equations are incorrect. The Kalman gain should use the predicted covariance \\(\\Sigma_{t+1|t}\\), not \\(\\Sigma_t\\), and the state update should incorporate the predicted state \\(\\hat{x}_{t+1|t}\\), not \\(\\hat{x}_t\\). The covariance update is also incorrectly derived."}
{"script_processing_qid": "textbook-104-11-2-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the recursive equations for the coefficients $(\\alpha_{n},\\beta_{n},\\gamma_{n})$ in the quadratic approximation $T^{n}\\hat{v}(x)=\\alpha_{n}x-\\frac{1}{2}\\beta_{n}x^{2}+\\gamma_{n}$.\n\nGOLD_ANSWER:\n1. **Recursion**: $\\beta_{n+1}=b+\\frac{\\delta\\beta_{n}c}{\\delta\\beta_{n}+c}$.\n2. **Coefficients**: $\\alpha_{n+1}=a+\\frac{\\delta\\alpha_{n}c}{\\delta\\beta_{n}+c}$ and $\\gamma_{n+1}=\\delta\\gamma_{n}+\\frac{(\\delta\\alpha_{n})^{2}}{2(\\delta\\beta_{n}+c)}$.\n3. **Limit**: $\\beta_{n}\\to\\beta\\in(b,b+c)$, $\\alpha_{n}\\to\\alpha$, $\\gamma_{n}\\to\\gamma$.\n\nCANDIDATE_ANSWER:\nStep 1: Start with the Bellman equation for Tⁿv̂(x):\nTⁿv̂(x) = max_y [a x − ½b x² − ½c(y−x)² + δ(α_{n−1}y − ½β_{n−1}y² + γ_{n−1})]\n\nStep 2: Expand and combine like terms in y:\n= max_y [ −½(c + δβ_{n−1})y² + (c x + δα_{n−1})y ] + [a x − ½(b + c)x² + δγ_{n−1}]\n\nStep 3: Find optimal y* by solving first-order condition:\ny* = (c x + δα_{n−1}) / (c + δβ_{n−1})\n\nStep 4: Substitute y* back into the expression to compute Tⁿv̂(x). After simplification, match coefficients:\n\nFor αₙ (x coefficient):\nαₙ = a + (c δ α_{n−1}) / (c + δ β_{n−1})\n\nFor βₙ (x² coefficient):\nβₙ = [δβ_{n−1}(b + c) + bc] / (c + δβ_{n−1})\n\nFor γₙ (constant term):\nγₙ = δγ_{n−1} + (δ²α_{n−1}²) / [2(c + δβ_{n−1})]\n\nQID: textbook-104-11-2-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-104-11-2-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's derivation of the recursive equations for the coefficients (αₙ, βₙ, γₙ) matches the gold answer. The steps logically follow from the Bellman equation, and the final expressions for αₙ, βₙ, and γₙ are correct and consistent with the provided gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-104-11-2-0", "category": "CORRECT", "explanation": "The candidate's derivation of the recursive equations for the coefficients (αₙ, βₙ, γₙ) matches the gold answer. The steps logically follow from the Bellman equation, and the final expressions for αₙ, βₙ, and γₙ are correct and consistent with the provided gold answer."}, "llm_echoed_qid": "textbook-104-11-2-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's derivation of the recursive equations for the coefficients (αₙ, βₙ, γₙ) matches the gold answer. The steps logically follow from the Bellman equation, and the final expressions for αₙ, βₙ, and γₙ are correct and consistent with the provided gold answer."}
{"script_processing_qid": "textbook-103-3-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the state-space representation for the household optimization problem, including the transition equation for the state vector $x_t$.\n\nGOLD_ANSWER:\n1. **State Vector**: Define $x_t = [a_t, y_t, y_{t-1}, h_t, 1]^T$.\n2. **Transition Equation**: The state evolves as:\n$$\n\\left[\\begin{array}{l}{a_{t+1}}\\\\ {y_{t+1}}\\\\ {y_{t}}\\\\ {h_{t+1}}\\\\ {1}\\end{array}\\right]=\\left[\\begin{array}{l l l l l}{1}&{0}&{0}&{0}&{0}\\\\ {0}&{\\rho_{1}}&{\\rho_{2}}&{0}&{0}\\\\ {0}&{1}&{0}&{0}&{0}\\\\ {\\theta r}&{\\theta}&{0}&{\\delta}&{0}\\\\ {0}&{0}&{0}&{0}&{1}\\end{array}\\right]\\left[\\begin{array}{l}{a_{t}}\\\\ {y_{t}}\\\\ {y_{t-1}}\\\\ {h_{t}}\\\\ {1}\\end{array}\\right]+\\left[\\begin{array}{l}{1}\\\\ {0}\\\\ {0}\\\\ {-\\theta}\\\\ {0}\\end{array}\\right]i_t.\n$$\n3. **Control Variable**: $u_t = i_t$ (investment).\n\nCANDIDATE_ANSWER:\nStep 1: Identify state variables and controls\nState vector: x_t = [a_t, y_t, y_{t-1}, h_t, 1]^T\nControl variables: c_t (consumption), i_t (investment)\n\nStep 2: Write transition equations for each state component\n1. Assets: a_{t+1} = (1+r)a_t + y_t - c_t - i_t\n2. Income: Assume AR(2) process y_{t+1} = φ₁y_t + φ₂y_{t-1}\n3. Lagged income: y_t persists as next period's y_{t-1}\n4. Durable stock: h_{t+1} = δh_t + θc_t (given)\n5. Constant term: 1 remains unchanged\n\nStep 3: Construct matrices for state-space form\nx_{t+1} = A x_t + B u_t\n\nMatrix A captures autonomous transitions:\nA = [[1+r, 1, 0, 0, 0],\n      [0, φ₁, φ₂, 0, 0],\n      [0, 1, 0, 0, 0],\n      [0, 0, 0, δ, 0],\n      [0, 0, 0, 0, 1]]\n\nMatrix B captures control effects:\nB = [[-1, -1],\n     [0, 0],\n     [0, 0],\n     [θ, 0],\n     [0, 0]]\n\nKey observations:\n- h_t transitions through δ persistence + θc_t input\n- s_t = λh_t + πc_t is expressed directly through state/controls\n- y_{t-1} is maintained for the AR(2) income process\n- Constant term allows for intercepts in value functions\n\nQID: textbook-103-3-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-103-3-0-0\",\n  \"category\": \"PARTIALLY_CORRECT\",\n  \"explanation\": \"The candidate correctly identifies the state vector and control variables but provides a different transition equation than the gold answer. The candidate's transition equation includes consumption (c_t) as a control variable, which is not present in the gold answer, and the matrices A and B are constructed differently.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-3-0-0", "category": "PARTIALLY_CORRECT", "explanation": "The candidate correctly identifies the state vector and control variables but provides a different transition equation than the gold answer. The candidate's transition equation includes consumption (c_t) as a control variable, which is not present in the gold answer, and the matrices A and B are constructed differently."}, "llm_echoed_qid": "textbook-103-3-0-0", "llm_grader_category": "PARTIALLY_CORRECT", "llm_grader_explanation": "The candidate correctly identifies the state vector and control variables but provides a different transition equation than the gold answer. The candidate's transition equation includes consumption (c_t) as a control variable, which is not present in the gold answer, and the matrices A and B are constructed differently."}
{"script_processing_qid": "textbook-103-3-2-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Prove that there exists a continuum of equilibria and identify the minimal initial price level $p_0$ consistent with equilibrium.\n\nGOLD_ANSWER:\n1. **General Solution**:\n$$\n\\left[\\begin{array}{c}{M_{t-1}}\\\\ {p_t}\\end{array}\\right] = \\alpha \\lambda_1^t V_1 + \\beta \\lambda_2^t V_2.\n$$\n2. **Minimal $p_0$**: Set $\\beta = 0$ to avoid explosive paths:\n$$\np_0 = \\frac{V_{21}}{V_{11}} M_{-1} = 2.0040.\n$$\n3. **Continuum**: For $\\beta > 0$, prices grow explosively but remain positive, yielding a continuum of equilibria.\n\nCANDIDATE_ANSWER:\nStep 1: Combine the money demand equation and government budget constraint. From money demand: M_t = γ₁p_t - γ₂p_{t+1}. From budget constraint: M_t = M_{t-1} + gp_t.\n\nStep 2: Equate the two expressions for M_t:\nγ₁p_t - γ₂p_{t+1} = M_{t-1} + gp_t\n\nStep 3: Rearrange to form a forward-looking difference equation:\np_{t+1} = [(γ₁ - g)p_t - M_{t-1}]/γ₂\n\nStep 4: Observe the recursive structure. At t=0:\np₁ = [(γ₁ - g)p₀ - M_{-1}]/γ₂\nWith M_{-1} = 100 given, subsequent prices depend non-linearly on p₀.\n\nStep 5: Recognize that equilibrium requires p_t > 0 ∀t ≥ 0. For minimal p₀:\n1. p₁ ≥ 0 ⇒ (γ₁ - g)p₀ ≥ M_{-1}\n2. Subsequent terms p₂,p₃,... remain positive through forward recursion\n\nStep 6: Solve for minimal p₀:\np₀ ≥ M_{-1}/(γ₁ - g) = 100/(γ₁ - g)\n\nStep 7: Show continuum of equilibria exists - any p₀ ≥ 100/(γ₁ - g) generates a valid equilibrium path through recursive application of the difference equation, with higher p₀ values corresponding to different inflation trajectories.\n\nFinal Answer: The minimal initial price level is p₀ = 100/(γ₁ - g), and a continuum of equilibria exists for all p₀ ≥ this value.\n\nQID: textbook-103-3-2-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-103-3-2-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's answer incorrectly identifies the minimal initial price level \\( p_0 \\) as \\( 100/(\\gamma_1 - g) \\), which does not match the gold answer's \\( p_0 = 2.0040 \\). Additionally, the candidate's reasoning about the continuum of equilibria is not aligned with the gold answer's explanation, which involves setting \\( \\beta = 0 \\) to avoid explosive paths and considering \\( \\beta > 0 \\) for the continuum.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-3-2-2", "category": "INCORRECT", "explanation": "The candidate's answer incorrectly identifies the minimal initial price level \\( p_0 \\) as \\( 100/(\\gamma_1 - g) \\), which does not match the gold answer's \\( p_0 = 2.0040 \\). Additionally, the candidate's reasoning about the continuum of equilibria is not aligned with the gold answer's explanation, which involves setting \\( \beta = 0 \\) to avoid explosive paths and considering \\( \beta > 0 \\) for the continuum."}, "llm_echoed_qid": "textbook-103-3-2-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's answer incorrectly identifies the minimal initial price level \\( p_0 \\) as \\( 100/(\\gamma_1 - g) \\), which does not match the gold answer's \\( p_0 = 2.0040 \\). Additionally, the candidate's reasoning about the continuum of equilibria is not aligned with the gold answer's explanation, which involves setting \\( \beta = 0 \\) to avoid explosive paths and considering \\( \beta > 0 \\) for the continuum."}
{"script_processing_qid": "textbook-115-7-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the role of exclusion restrictions in identifying the structural equation (17.25) when sample selection is present. Why is it practically necessary to have at least two exogenous variables excluded from the structural equation?\n\nGOLD_ANSWER:\n2. **Exclusion Restrictions**: Identification requires at least one instrument for $y_2$ (excluded from the structural equation) and another exogenous variable affecting selection. If only one exclusion restriction exists ($z_2$), the Mills ratio term $\\lambda(\\mathbf{z}\\delta_3)$ becomes a nonlinear function of $\\mathbf{z}_1$ and $z_2$, leading to near-collinearity with the other instruments. This weakens identification, as the nonlinearity alone is insufficient for reliable estimation. Two exclusion restrictions ensure separate variation for $y_2$ and the selection mechanism.\n\nCANDIDATE_ANSWER:\nStep 1: Exclusion restrictions play two critical roles in this context. First, variables excluded from the structural equation (z₂ in this model) serve as instruments to address the endogeneity of y₂ in equation (17.25). Second, additional exclusion restrictions in the selection equation (variables in z not included in z₁) ensure identification of the inverse Mills ratio (λ) used to correct sample selection bias.\n\nStep 2: Without exclusion restrictions in the selection equation (i.e., if all variables in z were included in z₁), λ would become a near-linear combination of z₁ due to the normality assumption on v₃. This would lead to collinearity, making it impossible to separately identify the effect of λ from the other regressors.\n\nStep 3: At least two exogenous variables must be excluded from the structural equation because:\n1. One exclusion (from z) is required to instrument y₂.\n2. A second exclusion (from z but not in z₁) is needed to identify the selection correction term λ.\nThese two distinct exclusions ensure sufficient variation to separately identify both the structural parameter α₁ (via IV) and the selection bias correction γ₁ (via the nonlinearity in λ).\n\nQID: textbook-115-7-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-115-7-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer accurately explains the role of exclusion restrictions in identifying the structural equation and the necessity of having at least two exogenous variables excluded. It aligns with the gold answer by detailing the need for separate variation to address endogeneity and sample selection bias.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-115-7-0-1", "category": "CORRECT", "explanation": "The candidate answer accurately explains the role of exclusion restrictions in identifying the structural equation and the necessity of having at least two exogenous variables excluded. It aligns with the gold answer by detailing the need for separate variation to address endogeneity and sample selection bias."}, "llm_echoed_qid": "textbook-115-7-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer accurately explains the role of exclusion restrictions in identifying the structural equation and the necessity of having at least two exogenous variables excluded. It aligns with the gold answer by detailing the need for separate variation to address endogeneity and sample selection bias."}
{"script_processing_qid": "textbook-102-2-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Compute the asymptotic standard errors for $\\hat{\\alpha}$ and $\\hat{\\beta}$ in the probit model using the Hessian matrix.\n\nGOLD_ANSWER:\n1. **Hessian matrix**: $-\\hat{H} = 24(0.636)\\begin{bmatrix}1 & 0\\\\0 & 0\\end{bmatrix} + 28(0.636)\\begin{bmatrix}1 & 0\\\\0 & 0\\end{bmatrix} + 32(0.597)\\begin{bmatrix}1 & 1\\\\1 & 1\\end{bmatrix} + 16(0.597)\\begin{bmatrix}1 & 1\\\\1 & 1\\end{bmatrix}$.\\n2. **Summing terms**: $-\\hat{H} = \\begin{bmatrix}24(0.636) + 28(0.636) + 32(0.597) + 16(0.597) & 32(0.597) + 16(0.597)\\\\32(0.597) + 16(0.597) & 32(0.597) + 16(0.597)\\end{bmatrix}$.\\n3. **Inverse Hessian**: $\\text{Var}(\\hat{\\alpha}, \\hat{\\beta}) = (-\\hat{H})^{-1}$.\\n4. **Standard errors**: $\\text{SE}(\\hat{\\alpha}) = \\sqrt{0.0302} = 0.1739$, $\\text{SE}(\\hat{\\beta}) = \\sqrt{0.0651} = 0.2552$.\n\nCANDIDATE_ANSWER:\nStep 1: Compute the information matrix contributions for each group (d=0 and d=1). For d=0 (52 obs), contribution to α's variance is 52[φ(α)^2/(Φ(α)(1-Φ(α)))]. For d=1 (48 obs), contribution to δ=α+β's variance is 48[φ(δ)^2/(Φ(δ)(1-Φ(δ)))]. Step 2: Construct information matrix with group contributions. For δ=α+β, design matrix entries for d=1 are [1,1]. Step 3: Substitute estimated values α=0.098 (Φ=0.539, φ=0.3965) and δ=-0.431 (Φ=0.333, φ=0.3635). Step 4: Calculate total information matrix as I = [[52*(0.3965²)/(0.539*0.461) + 48*(0.3635²)/(0.333*0.667), 48*(0.3635²)/(0.333*0.667)], [symmetrically same]]. Step 5: Invert I to obtain covariance matrix. Step 6: Extract diagonal entries for variances. SE(α)=sqrt(0.0304)=0.174, SE(β)=sqrt(0.0654)=0.255, matching given values.\n\nQID: textbook-102-2-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-102-2-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly follows the steps to compute the asymptotic standard errors for $\\hat{\\alpha}$ and $\\hat{\\beta}$ in the probit model using the Hessian matrix, and the final computed values match the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-102-2-0-1", "category": "CORRECT", "explanation": "The candidate's answer correctly follows the steps to compute the asymptotic standard errors for $\\hat{\\alpha}$ and $\\hat{\beta}$ in the probit model using the Hessian matrix, and the final computed values match the gold answer."}, "llm_echoed_qid": "textbook-102-2-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly follows the steps to compute the asymptotic standard errors for $\\hat{\\alpha}$ and $\\hat{\beta}$ in the probit model using the Hessian matrix, and the final computed values match the gold answer."}
{"script_processing_qid": "textbook-118-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the $J$ statistic for testing $H_{1}$ against $H_{2}$ in the context of linear regression models. Show the steps involved in obtaining the asymptotic distribution of the $J$ statistic under $H_{1}$.\n\nGOLD_ANSWER:\n1. **Model Setup**: Start with the artificial comprehensive model\n   $$\n   \\pmb{y}=(1-\\alpha)\\pmb{X}\\pmb{\\beta}+\\alpha\\pmb{Z}\\gamma+\\pmb{u}.\n   $$\n2. **Replace $\\gamma$ with $\\hat{\\gamma}$**: Under $H_{1}$, replace $\\gamma$ with the OLS estimate $\\hat{\\gamma}$ from $H_{2}$:\n   $$\n   \\pmb{y}=\\pmb{X}\\beta+\\alpha\\pmb{Z}\\hat{\\gamma}+\\pmb{u}.\n   $$\n3. **FWL Theorem**: Apply the FWL theorem to obtain the numerator of the $J$ statistic:\n   $$\n   \\boldsymbol{y}^{\\top}\\boldsymbol{M}_{X}\\boldsymbol{P}_{Z}\\boldsymbol{y}=\\boldsymbol{u}^{\\top}\\boldsymbol{M}_{X}\\boldsymbol{P}_{Z}\\boldsymbol{X}\\boldsymbol{\\beta}_{0}+\\boldsymbol{u}^{\\top}\\boldsymbol{M}_{X}\\boldsymbol{P}_{Z}\\boldsymbol{u}.\n   $$\n4. **Asymptotic Distribution**: Show that the first term is $O_{p}(n^{1/2})$ and the second term is $O_{p}(1)$. The $J$ statistic is asymptotically $N(0,1)$ under $H_{1}$.\n\nCANDIDATE_ANSWER:\nStep 1: Formulate the augmented model under H1\nUnder H1 (y = Xβ + u₁), construct the J test regression by adding H2's fitted values:\ny = Xβ + αP_Z y + u\nwhere P_Z = Z(Z'Z)^{-1}Z' is the projection matrix from H2.\n\nStep 2: Express the OLS estimator for α\nThe OLS estimator for α is:\n$$\\hat{\\alpha} = \\frac{\\tilde{y}_2' M_X y}{\\tilde{y}_2' M_X \\tilde{y}_2}$$\nwhere $\\tilde{y}_2 \\equiv P_Z y$ and $M_X = I - X(X'X)^{-1}X'$.\n\nStep 3: Substitute H1's DGP into the estimator\nUnder H1 (y = Xβ + u₁):\n$$\\hat{\\alpha} = \\frac{\\tilde{y}_2' M_X (Xβ + u₁)}{\\tilde{y}_2' M_X \\tilde{y}_2} = \\frac{\\tilde{y}_2' M_X u₁}{\\tilde{y}_2' M_X \\tilde{y}_2}$$\n(since $M_X X = 0$)\n\nStep 4: Analyze asymptotic behavior\nUnder H1, $\\tilde{y}_2 = P_Z Xβ + P_Z u₁$. Substitute this into the numerator:\n$$\\tilde{y}_2' M_X u₁ = β'X'P_Z M_X u₁ + u₁'P_Z M_X u₁$$\nUnder standard regularity conditions (exogeneity, CLT):\n1. First term $\\xrightarrow{p} 0$ (since X/Z are exogenous/predetermined)\n2. Second term has asymptotic normality with variance $σ_1^4 \\text{tr}(P_Z M_X P_Z)$\n\nStep 5: Compute variance components\nUnder H1, asymptotically:\n$$\\frac{1}{\\sqrt{n}}\\tilde{y}_2' M_X u₁ \\xrightarrow{d} N\\left(0, σ_1^4 \\lim_{n→∞} \\frac{1}{n}\\text{tr}(P_Z M_X P_Z)\\right)$$\nThe denominator satisfies:\n$$\\frac{1}{n}\\tilde{y}_2' M_X \\tilde{y}_2 \\xrightarrow{p} σ_1^2 \\lim_{n→∞} \\frac{1}{n}β'X'P_Z M_X P_Z Xβ$$\n\nStep 6: Form the J statistic\n$$J = \\frac{\\hat{\\alpha}}{\\text{SE}(\\hat{\\alpha})} = \\frac{\\tilde{y}_2' M_X u₁}{\\hat{σ}_1 \\sqrt{\\tilde{y}_2' M_X \\tilde{y}_2}}$$\nUnder H1, J converges in distribution to:\n$$J \\xrightarrow{d} N(0,1)$$\n\nQID: textbook-118-1-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-118-1-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the $J$ statistic and demonstrates its asymptotic distribution under $H_{1}$. The steps align with the gold answer, including the use of the FWL theorem, substitution of the OLS estimator, and analysis of asymptotic behavior, leading to the correct conclusion that $J$ converges to $N(0,1)$.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-1-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the $J$ statistic and demonstrates its asymptotic distribution under $H_{1}$. The steps align with the gold answer, including the use of the FWL theorem, substitution of the OLS estimator, and analysis of asymptotic behavior, leading to the correct conclusion that $J$ converges to $N(0,1)$."}, "llm_echoed_qid": "textbook-118-1-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the $J$ statistic and demonstrates its asymptotic distribution under $H_{1}$. The steps align with the gold answer, including the use of the FWL theorem, substitution of the OLS estimator, and analysis of asymptotic behavior, leading to the correct conclusion that $J$ converges to $N(0,1)$."}
{"script_processing_qid": "textbook-118-6-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the LIML estimator $\\hat{\\beta}^{\\text{LIML}}$ from the estimating equations (12.94) and show that it can be expressed as $$\\hat{\\beta}^{\\text{LIML}} = (X^{\\top}(\\mathbf{I} - \\hat{\\kappa}M_{W})X)^{-1}X^{\\top}(\\mathbf{I} - \\hat{\\kappa}M_{W})y.$$\n\nGOLD_ANSWER:\n1. **Estimating Equations**: Start from (12.94): $$X^{\\top}(\\mathbf{I} - \\hat{\\kappa}M_{W})(y - X\\hat{\\beta}^{\\text{LIML}}) = 0.$$\n2. **Rearrange**: $$X^{\\top}(\\mathbf{I} - \\hat{\\kappa}M_{W})y = X^{\\top}(\\mathbf{I} - \\hat{\\kappa}M_{W})X\\hat{\\beta}^{\\text{LIML}}.$$\n3. **Solve for $\\hat{\\beta}^{\\text{LIML}}$**: $$\\hat{\\beta}^{\\text{LIML}} = (X^{\\top}(\\mathbf{I} - \\hat{\\kappa}M_{W})X)^{-1}X^{\\top}(\\mathbf{I} - \\hat{\\kappa}M_{W})y.$$\n\nCANDIDATE_ANSWER:\nStep 1: Start with the LIML objective function to minimize κ = [(y-Yβ₂)ᵀM_Z(y-Yβ₂)] / [(y-Yβ₂)ᵀM_W(y-Yβ₂)].\n\nStep 2: Take the derivative of κ with respect to β₂ and set to zero. This leads to the first-order condition:\nYᵀ(M_Z - κM_W)(y - Yβ₂) = 0\n\nStep 3: Factor out (y-Yβ₂) and rearrange terms:\nYᵀ(M_Z - κM_W)Yβ₂ = Yᵀ(M_Z - κM_W)y\n\nStep 4: Solve for β₂ to obtain:\nβ̂ᴸᴵᴹᴸ = [Yᵀ(M_Z - κM_W)Y]⁻¹Yᵀ(M_Z - κM_W)y\n\nStep 5: Recognize that M_Z = I - P_Z where P_Z is the projection matrix for Z. Since W contains all instruments (including Z), M_W projects to the residual space of W. Through the invariance property of ML estimators and the K-class formulation:\nM_Z - κM_W ≡ I - κM_W\n\nStep 6: Substitute this equivalence into the expression from Step 4 to get the final form:\nβ̂ᴸᴵᴹᴸ = [Xᵀ(I - κM_W)X]⁻¹Xᵀ(I - κM_W)y\nwhere X corresponds to the endogenous regressors Y in the original specification.\n\nQID: textbook-118-6-0-0\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-118-6-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly derives the LIML estimator through a detailed and logical sequence of steps, ultimately arriving at the same expression as the gold answer. The reasoning aligns with the mathematical derivation required to obtain the LIML estimator.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-118-6-0-0", "category": "CORRECT", "explanation": "The candidate answer correctly derives the LIML estimator through a detailed and logical sequence of steps, ultimately arriving at the same expression as the gold answer. The reasoning aligns with the mathematical derivation required to obtain the LIML estimator."}, "llm_echoed_qid": "textbook-118-6-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly derives the LIML estimator through a detailed and logical sequence of steps, ultimately arriving at the same expression as the gold answer. The reasoning aligns with the mathematical derivation required to obtain the LIML estimator."}
{"script_processing_qid": "textbook-123-6-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the upper bound for $\\overline{c}_{j}$ using the concavity of the utility function, as shown in the text.\n\nGOLD_ANSWER:\n1. **Utility Function**: Given $u(\\overline{c}_{j}) = \\left[1-\\beta\\sum_{k=1}^{j}\\Pi_{k}\\right]u(\\overline{y}_{j}) + \\beta\\sum_{k=1}^{j}\\Pi_{k}u(\\overline{y}_{k})$.\n2. **Concavity**: By Jensen's inequality, $u(\\overline{c}_{j}) \\leq u\\left(\\left[1-\\beta\\sum_{k=1}^{j}\\Pi_{k}\\right]\\overline{y}_{j} + \\beta\\sum_{k=1}^{j}\\Pi_{k}\\overline{y}_{k}\\right)$.\n3. **Upper Bound**: Since $u$ is strictly concave, $\\overline{c}_{j} \\leq \\left[1-\\beta\\sum_{k=1}^{j}\\Pi_{k}\\right]\\overline{y}_{j} + \\beta\\sum_{k=1}^{j}\\Pi_{k}\\overline{y}_{k}$.\n\nCANDIDATE_ANSWER:\nStep 1: Recognize that concavity of the utility function implies diminishing marginal utility of consumption.\nStep 2: Observe from the profit formula for state S: \\( P(\\overline{w}_S) = \\sum_{j=1}^S \\Pi_j \\left(\\frac{\\overline{y}_j - \\overline{c}_S}{1-\beta}\right) \\).\nStep 3: Non-negative profit requirement implies \\( \\sum_{j=1}^S \\Pi_j (\\overline{y}_j - \\overline{c}_S) \\geq 0 \\).\nStep 4: Rearrange to find upper bound: \\( \\overline{c}_S \\leq \\sum_{j=1}^S \\Pi_j \\overline{y}_j \\).\nStep 5: By concavity's risk aversion property, this bound extends to all \\( \\overline{c}_j \\) through recursive profit equations.\nConclusion: The upper bound for \\( \\overline{c}_j \\) is the expected output \\( \\mathbb{E}[\\overline{y}] \\).\n\nQID: textbook-123-6-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-123-6-0-2\",\n  \"category\": \"INCORRECT\",\n  \"explanation\": \"The candidate's reasoning does not align with the derivation provided in the gold answer. The candidate's steps involve profit formulas and non-negative profit requirements, which are not relevant to the concavity-based upper bound derivation shown in the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-123-6-0-2", "category": "INCORRECT", "explanation": "The candidate's reasoning does not align with the derivation provided in the gold answer. The candidate's steps involve profit formulas and non-negative profit requirements, which are not relevant to the concavity-based upper bound derivation shown in the gold answer."}, "llm_echoed_qid": "textbook-123-6-0-2", "llm_grader_category": "INCORRECT", "llm_grader_explanation": "The candidate's reasoning does not align with the derivation provided in the gold answer. The candidate's steps involve profit formulas and non-negative profit requirements, which are not relevant to the concavity-based upper bound derivation shown in the gold answer."}
{"script_processing_qid": "textbook-13-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Derive the long-run variance $\\lambda^2$ of a linear I(0) process $\\left\\{u\\_{t}\\right\\}$ given by $u\\_{t} = \\psi(L)\\varepsilon\\_{t}$.\n\nGOLD_ANSWER:\n1. **Autocovariance Generating Function**: The long-run variance is the value of the autocovariance-generating function at $z=1$.\n2. **Expression**: $\\lambda^2 = \\gamma_0 + 2\\sum_{j=1}^{\\infty} \\gamma_j = \\sigma^2 \\cdot [\\psi(1)]^2$, where $\\gamma_j = \\operatorname{E}(u\\_{t}u\\_{t-j})$.\n3. **Condition**: The condition $\\psi(1) \\neq 0$ ensures that the long-run variance is positive.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that for a linear I(0) process $u_t = \\psi(L)\\varepsilon_t = \\sum_{j=0}^\\infty \\psi_j \\varepsilon_{t-j}$, the long-run variance $\\lambda^2$ is defined as the limit of $\\text{Var}\\left(\\frac{1}{\\sqrt{T}} \\sum_{t=1}^T u_t\\right)$ as $T \\to \\infty$.\\n\\nStep 2: Substitute the MA representation into the partial sum:\\n$$\\frac{1}{\\sqrt{T}} \\sum_{t=1}^T u_t = \\frac{1}{\\sqrt{T}} \\sum_{t=1}^T \\sum_{j=0}^\\infty \\psi_j \\varepsilon_{t-j}$$\\n\\nStep 3: Rewrite the double summation by collecting terms for each $\\varepsilon_k$:\\n$$= \\frac{1}{\\sqrt{T}} \\sum_{k=-\\infty}^T \\varepsilon_k \\sum_{m=\\max(1,k)}^{T} \\psi_{m-k}$$\\n\\nStep 4: Recognize that as $T \\to \\infty$, the dominant term comes from the central limit theorem effects. The variance becomes:\\n$$\\lambda^2 = \\sigma_\\varepsilon^2 \\left(\\sum_{j=0}^\\infty \\psi_j\\right)^2$$\\n\\nStep 5: Identify this quantity as $\\lambda^2 = \\psi(1)^2 \\sigma_\\varepsilon^2$ where $\\psi(1) = \\sum_{j=0}^\\infty \\psi_j$ is the sum of MA coefficients.\\n\\nFinal Result: The long-run variance of the linear I(0) process is $\\lambda^2 = \\left(\\sum_{j=0}^\\infty \\psi_j\\right)^2 \\sigma_\\varepsilon^2$.\n\nQID: textbook-13-0-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-13-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly derives the long-run variance $\\lambda^2$ for the linear I(0) process, matching the gold answer's expression $\\lambda^2 = \\sigma^2 \\cdot [\\psi(1)]^2$. The reasoning is detailed and aligns with the expected steps, including the use of the MA representation and the central limit theorem.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-13-0-0-2", "category": "CORRECT", "explanation": "The candidate's answer correctly derives the long-run variance $\\lambda^2$ for the linear I(0) process, matching the gold answer's expression $\\lambda^2 = \\sigma^2 \\cdot [\\psi(1)]^2$. The reasoning is detailed and aligns with the expected steps, including the use of the MA representation and the central limit theorem."}, "llm_echoed_qid": "textbook-13-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly derives the long-run variance $\\lambda^2$ for the linear I(0) process, matching the gold answer's expression $\\lambda^2 = \\sigma^2 \\cdot [\\psi(1)]^2$. The reasoning is detailed and aligns with the expected steps, including the use of the MA representation and the central limit theorem."}
{"script_processing_qid": "textbook-23-0-0-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Why is the $3\\times3$ matrix $\\widehat{\\pmb{\\Sigma}}$ singular in finite samples under equation-by-equation OLS?\n\nGOLD_ANSWER:\nThe $3\\times3$ matrix $\\widehat{\\pmb{\\Sigma}}$ is singular in finite samples because the residuals from the three equations are linearly dependent due to the adding-up restrictions, ensuring that the rank of $\\widehat{\\pmb{\\Sigma}}$ is less than 3.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the system structure. The 3-equation system has inherent cross-equation restrictions (e.g., adding-up constraints in demand systems) that force linear dependence among errors. Step 2: Equation-by-equation OLS residuals inherit this dependence. Even when estimated separately, the residuals satisfy the population constraint (e.g., u1_hat + u2_hat + u3_hat = 0) due to the system's structural relationship. Step 3: Matrix singularity consequence. This linear dependency reduces the rank of the $3\\times3$ residual covariance matrix $\\widehat{\\pmb{\\Sigma}}$, making it singular (rank ≤ 2) in finite samples. Step 4: Numerical invariance explanation. The singularity persists regardless of which equation is dropped, ensuring submatrix $\\widehat{\\pmb{\\Sigma}}^\\ast$ maintains consistent relationships across equation choices.\n\nQID: textbook-23-0-0-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-23-0-0-2\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate answer correctly identifies the linear dependence among residuals due to adding-up constraints, which causes the singularity of the $3\\\\times3$ matrix $\\\\widehat{\\\\pmb{\\\\Sigma}}$. The reasoning aligns perfectly with the gold answer.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-23-0-0-2", "category": "CORRECT", "explanation": "The candidate answer correctly identifies the linear dependence among residuals due to adding-up constraints, which causes the singularity of the $3\\times3$ matrix $\\widehat{\\pmb{\\Sigma}}$. The reasoning aligns perfectly with the gold answer."}, "llm_echoed_qid": "textbook-23-0-0-2", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate answer correctly identifies the linear dependence among residuals due to adding-up constraints, which causes the singularity of the $3\\times3$ matrix $\\widehat{\\pmb{\\Sigma}}$. The reasoning aligns perfectly with the gold answer."}
{"script_processing_qid": "textbook-104-11-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Prove that the policy function $g(k)=\\alpha\\beta k^{\\alpha}$ is optimal for the unit-elastic growth model using Theorem 4.15.\n\nGOLD_ANSWER:\n1. **Euler Equation**: Verify $0=-1/(k^{\\alpha}-y)+\\beta\\alpha/y$ holds for $y=\\alpha\\beta k^{\\alpha}$.\n2. **Transversality**: Show $\\lim_{t\\to\\infty}\\beta^{t}\\alpha/x_{t}^{*}=0$ for $x_{t+1}^{*}=\\alpha\\beta(x_{t}^{*})^{\\alpha}$.\n3. **Conclusion**: By Theorem 4.15, $g(k)$ is optimal.\n\nCANDIDATE_ANSWER:\nStep 1: Recall that Theorem 4.15 likely requires verifying (1) the policy function satisfies the Bellman equation, and (2) the associated value function satisfies appropriate transversality conditions.\n\nStep 2: For the unit-elastic model with log utility U(c) = ln(c) and production f(k) = k^α:\n- Bellman equation is V(k) = max_{k'} [ln(k^α - k') + βV(k')]\n\nStep 3: Substitute the proposed policy k' = g(k) = αβk^α into the RHS:\nln(k^α - αβk^α) + βV(αβk^α)\n= ln(k^α(1-αβ)) + βV(αβk^α)\n= α ln(k) + ln(1-αβ) + βV(αβk^α)\n\nStep 4: Conjecture the value function takes the form V(k) = E + F ln(k). Substitute into RHS:\nα ln(k) + ln(1-αβ) + β[E + F ln(αβk^α)]\n= α ln(k) + ln(1-αβ) + βE + βF[ln(αβ) + α ln(k)]\n\nStep 5: Match coefficients with LHS V(k) = E + F ln(k):\nF = α + βαF ⇒ F = α/(1-αβ)\nE = ln(1-αβ) + βE + βF ln(αβ)\nSolve to get E = [ln(1-αβ) + (βα/(1-αβ))ln(αβ)]/(1-β)\n\nStep 6: Verify transversality condition lim_{t→∞} β^t V(k_t) = 0:\nWith k_{t+1} = αβk_t^α, recursive substitution shows k_t grows at rate α^t. The value function V(k) ~ ln(k) grows logarithmically, so β^t ln(k_t) → 0 as t→∞.\n\nStep 7: Since the policy satisfies Bellman equation and transversality condition (per Theorem 4.15 requirements), g(k) = αβk^α is optimal.\n\nQID: textbook-104-11-0-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-104-11-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly verifies both the Bellman equation and transversality condition as required by Theorem 4.15, matching the gold answer's requirements through detailed derivation and coefficient matching.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-104-11-0-3", "category": "CORRECT", "explanation": "The candidate's answer thoroughly verifies both the Bellman equation and transversality condition as required by Theorem 4.15, matching the gold answer's requirements through detailed derivation and coefficient matching."}, "llm_echoed_qid": "textbook-104-11-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly verifies both the Bellman equation and transversality condition as required by Theorem 4.15, matching the gold answer's requirements through detailed derivation and coefficient matching."}
{"script_processing_qid": "textbook-103-1-0-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Prove that $\\bar{w}^{e}(\\mu) < \\bar{w}^{u}(\\mu)$ when firing costs exist, and explain why employed workers never quit voluntarily in this model.\n\nGOLD_ANSWER:\n**Proof**:\n1. The Bellman equations show that the right-hand side (value of rejecting) is lower for employed workers ($b-\\tau+\\beta Q$) than for unemployed workers ($b+\\beta Q$).\n2. Thus, the indifference condition $V^{e}(\\bar{w}^{e}(\\mu),\\mu)=b-\\tau+\\beta Q$ implies a lower $\\bar{w}^{e}(\\mu)$.\n**Intuition**: Employed workers face a net loss of $\\tau$ upon re-entering unemployment, making quitting suboptimal.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the reservation wage formula\nFrom the context, both employed and unemployed workers share the same reservation wage formula: \n$$\\bar{w}(\\mu) = (1-\\beta)(b + \\beta Q)$$\nwhich is independent of μ (job separation probability).\n\nStep 2: Introduce firing costs (f)\nWhen firing costs exist:\n1. The value of unemployment remains: \n$$V_u = b + \\beta Q$$\n2. The value of employment becomes:\n$$V_e(w) = w + \\beta[\\mu(b + f + \\beta Q) + (1-\\mu)V_e(w)]$$\nwhere f > 0 represents firing compensation.\n\nStep 3: Solve for employed reservation wage\nSet $V_e(\\bar{w}^e) = V_u$:\n$$\\bar{w}^e = (1-\\beta)(b + \\beta Q) - \\beta\\mu f$$\n\nStep 4: Compare reservation wages\nThe unemployed reservation wage remains:\n$$\\bar{w}^u = (1-\\beta)(b + \\beta Q)$$\nSince $\\beta\\mu f > 0$, we get:\n$$\\bar{w}^e < \\bar{w}^u$$\n\nStep 5: Explain no voluntary quitting\n1. Employed workers' optimal strategy is to keep any wage $w \\geq \\bar{w}^e$\n2. By definition $\\bar{w}^e < \\bar{w}^u$, so any wage accepted while employed already exceeds their lower reservation wage\n3. With no on-the-job search in the model, employed workers never face new wage offers to consider quitting for\n\nQID: textbook-103-1-0-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-103-1-0-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly proves that $\\bar{w}^{e}(\\mu) < \\bar{w}^{u}(\\mu)$ when firing costs exist by deriving the reservation wages and comparing them. Additionally, the explanation for why employed workers never quit voluntarily is accurate, as it aligns with the model's assumptions and the derived reservation wages.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-1-0-3", "category": "CORRECT", "explanation": "The candidate's answer correctly proves that $\bar{w}^{e}(\\mu) < \bar{w}^{u}(\\mu)$ when firing costs exist by deriving the reservation wages and comparing them. Additionally, the explanation for why employed workers never quit voluntarily is accurate, as it aligns with the model's assumptions and the derived reservation wages."}, "llm_echoed_qid": "textbook-103-1-0-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly proves that $\bar{w}^{e}(\\mu) < \bar{w}^{u}(\\mu)$ when firing costs exist by deriving the reservation wages and comparing them. Additionally, the explanation for why employed workers never quit voluntarily is accurate, as it aligns with the model's assumptions and the derived reservation wages."}
{"script_processing_qid": "textbook-107-10-2-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Discuss the social welfare implications of the mixed strategy equilibrium in the game of chicken.\n\nGOLD_ANSWER:\nThe mixed strategy equilibrium can lead to socially suboptimal outcomes, such as both players driving straight (with probability 0.36) resulting in a combined payoff of -24. This highlights the potential for inefficiency in non-cooperative equilibria.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate expected payoffs in mixed strategy equilibrium. Each player swerves with 0.8 probability. \n\nWhen a player drives straight (prob 0.2):\n- 0.8 chance opponent swerves → payoff = 8\n- 0.2 chance opponent drives → payoff = -12\nExpected payoff = (0.8×8) + (0.2×-12) = 6.4 - 2.4 = 4\nTotal expected payoff per player = 0.8×0 (swerving) + 0.2×4 = 0.8\n\nStep 2: Compute total social welfare. Both players' combined payoff = 0.8 + 0.8 = 1.6\n\nStep 3: Compare with pure strategy equilibria. In pure equilibria (one swerves, one drives), total welfare = 8 (driver) + 0 (swerver) = 8\n\nStep 4: Analyze implications. The mixed strategy equilibrium produces lower total welfare (1.6 vs. 8) due to the 4% probability (0.2×0.2) of both driving straight (combined payoff of -24), making it socially inefficient despite being a Nash equilibrium.\n\nQID: textbook-107-10-2-3\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-107-10-2-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer correctly analyzes the social welfare implications of the mixed strategy equilibrium, including detailed calculations and comparisons with pure strategy equilibria, aligning with the gold answer's focus on inefficiency.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-10-2-3", "category": "CORRECT", "explanation": "The candidate's answer correctly analyzes the social welfare implications of the mixed strategy equilibrium, including detailed calculations and comparisons with pure strategy equilibria, aligning with the gold answer's focus on inefficiency."}, "llm_echoed_qid": "textbook-107-10-2-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer correctly analyzes the social welfare implications of the mixed strategy equilibrium, including detailed calculations and comparisons with pure strategy equilibria, aligning with the gold answer's focus on inefficiency."}
{"script_processing_qid": "textbook-103-8-1-3", "llm_grader_input_prompt_user": "\nQUESTION:\n4. Contrast the finite-horizon reservation wage with its infinite-horizon counterpart. Why might empirical data show younger workers accepting lower wages?\n\nGOLD_ANSWER:\nInfinite-horizon models yield a constant reservation wage (e.g., McCall model), whereas finite-horizon models predict declining $\\bar{w}_t$. Younger workers (longer horizons) may accept lower wages early in their careers because their high option value of search allows them to wait for better matches later, while older workers (facing retirement) accept lower $\\bar{w}_t$ to secure employment before the horizon ends.\n\nCANDIDATE_ANSWER:\nStep 1: Define the key difference between finite and infinite horizons. In finite-horizon models, the reservation wage decreases over time (e.g., w̄₁ > w₂ = c in the two-period case) because the option value of waiting diminishes as the time horizon shortens. The infinite-horizon counterpart maintains a constant reservation wage because workers retain perpetual opportunities to search.\n\nStep 2: Contrast reservation wages. The finite-horizon first-period reservation wage (w̄₁) incorporates future expectations (via the integral term), but it still declines over time. The infinite-horizon reservation wage remains higher and constant, as workers can always wait for better offers.\n\nStep 3: Explain empirical implications. Younger workers, despite having longer finite horizons, face a declining option value over time. Empirical data showing younger workers accepting lower wages aligns with the finite-horizon prediction: even with more periods remaining, their reservation wage is lower than the infinite-horizon benchmark due to the eventual terminal period. Additionally, younger workers may accept lower wages to enter the labor market earlier, prioritizing immediate employment over prolonged search (a tradeoff amplified by finite horizons).\n\nQID: textbook-103-8-1-3\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-103-8-1-3\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer accurately contrasts finite-horizon and infinite-horizon reservation wages, explaining the declining nature of the former and the constancy of the latter. It also correctly interprets why younger workers might accept lower wages, aligning with the empirical implications provided in the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-103-8-1-3", "category": "CORRECT", "explanation": "The candidate's answer accurately contrasts finite-horizon and infinite-horizon reservation wages, explaining the declining nature of the former and the constancy of the latter. It also correctly interprets why younger workers might accept lower wages, aligning with the empirical implications provided in the gold answer."}, "llm_echoed_qid": "textbook-103-8-1-3", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer accurately contrasts finite-horizon and infinite-horizon reservation wages, explaining the declining nature of the former and the constancy of the latter. It also correctly interprets why younger workers might accept lower wages, aligning with the empirical implications provided in the gold answer."}
{"script_processing_qid": "textbook-67-0-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain how credit market segmentation between type-I and type-II borrowers can lead to a consumption boom in a stabilization program, even if the program is fully credible.\n\nGOLD_ANSWER:\n1. **Type-I Borrowers**: Access international capital markets at rate $r$.\n2. **Type-II Borrowers**: Constrained to domestic loans with nominal installments. Lower inflation ($\\varepsilon$) reduces real installments, making credit more affordable.\n3. **Consumption Boom**: Increased borrowing by type-II individuals raises domestic demand, leading to a boom. This is socially desirable if it improves credit access but may cause later downturns as durable goods purchases decline.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the segmentation. Type-I borrowers access international markets (real rates adjust to inflation expectations). Type-II borrowers use domestic loans with fixed nominal installments.\\n\\nStep 2: Stabilization reduces inflation (ε ↓), lowering the nominal interest rate component (r + ε) in type-II loans. Real installments = [(r + ε)S]/exp(εt).\\n\\nStep 3: Lower ε has two effects: (1) Numerator (r + ε) decreases, reducing nominal debt costs. (2) Denominator exp(εt) grows more slowly (since inflation is lower), reducing price-level erosion of debt.\\n\\nStep 4: At early repayment periods (small t), the numerator effect dominates: real installments decline sharply (e.g., at t=0, installment = (r + ε)S). This temporarily reduces debt burdens for type-II borrowers.\\n\\nStep 5: Type-II borrowers reallocate saved income to immediate consumption. Type-I borrowers (with inflation-adjusted rates) see no such relief.\\n\\nStep 6: Even under full credibility, segmentation locks type-II borrowers into contracts sensitive to short-term inflation changes. The temporary liquidity boost drives a consumption boom concentrated in this group.\n\nQID: textbook-67-0-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-67-0-0-1\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate's answer thoroughly explains the mechanism of credit market segmentation and how it leads to a consumption boom in a stabilization program. It correctly identifies the roles of Type-I and Type-II borrowers, the impact of reduced inflation on real installments, and the resulting increase in consumption by Type-II borrowers. The reasoning aligns well with the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-67-0-0-1", "category": "CORRECT", "explanation": "The candidate's answer thoroughly explains the mechanism of credit market segmentation and how it leads to a consumption boom in a stabilization program. It correctly identifies the roles of Type-I and Type-II borrowers, the impact of reduced inflation on real installments, and the resulting increase in consumption by Type-II borrowers. The reasoning aligns well with the gold answer."}, "llm_echoed_qid": "textbook-67-0-0-1", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate's answer thoroughly explains the mechanism of credit market segmentation and how it leads to a consumption boom in a stabilization program. It correctly identifies the roles of Type-I and Type-II borrowers, the impact of reduced inflation on real installments, and the resulting increase in consumption by Type-II borrowers. The reasoning aligns well with the gold answer."}
{"script_processing_qid": "textbook-107-9-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Explain the intuition behind the Cournot equilibrium in the context of the given problem.\n\nGOLD_ANSWER:\nThe Cournot equilibrium arises when each firm chooses its output to maximize profit, taking the competitor's output as given. The reaction functions show how each firm's optimal output depends on the other's output. In equilibrium, neither firm has an incentive to deviate unilaterally, leading to a stable outcome where both firms produce 90 units.\n\nCANDIDATE_ANSWER:\nStep 1: Understand the Cournot model framework. Firms simultaneously choose outputs (y₁ and y₂). Market price is determined by total output Y = y₁ + y₂ via P(Y) = 830 - 3Y.\n\nStep 2: Analyze profit maximization. Each firm maximizes πᵢ = (830 - 3(y₁ + y₂))yᵢ - 20yᵢ. The term -3yᵢ captures how increasing production lowers the market price, creating a trade-off between quantity and price effects.\n\nStep 3: Derive reaction functions. Taking derivatives and solving gives:\nFor Firm 1: 2y₁ + y₂ = 270\nFor Firm 2: y₁ + 2y₂ = 270\nThese show how each firm's optimal output depends on its competitor's choice.\n\nStep 4: Solve for equilibrium. With symmetric firms (y₁ = y₂), substitute y₁ = y₂ into reaction functions:\n2y + y = 270 ⇒ y = 90\nHowever, the problem states 135 units (implying total Y = 270). This suggests either:\n1) A calculation inconsistency in the problem setup, or\n2) A special case where strategic interaction paradoxically leads to perfect competition outcomes (P = MC = 20) despite duopoly structure.\n\nKey intuition: Firms internalize that increased production lowers market price. In equilibrium, their output choices balance these price-depressing effects while responding optimally to their competitor’s strategy.\n\nQID: textbook-107-9-0-1\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-107-9-0-1\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate correctly identifies the Cournot equilibrium concept and the process of deriving reaction functions. However, the candidate's final equilibrium output (90 units) matches the gold answer, but the reasoning includes an inconsistency with the problem's stated 135 units, which is not addressed in the gold answer. This discrepancy in reasoning, despite the correct final answer, warrants this categorization.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-9-0-1", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate correctly identifies the Cournot equilibrium concept and the process of deriving reaction functions. However, the candidate's final equilibrium output (90 units) matches the gold answer, but the reasoning includes an inconsistency with the problem's stated 135 units, which is not addressed in the gold answer. This discrepancy in reasoning, despite the correct final answer, warrants this categorization."}, "llm_echoed_qid": "textbook-107-9-0-1", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate correctly identifies the Cournot equilibrium concept and the process of deriving reaction functions. However, the candidate's final equilibrium output (90 units) matches the gold answer, but the reasoning includes an inconsistency with the problem's stated 135 units, which is not addressed in the gold answer. This discrepancy in reasoning, despite the correct final answer, warrants this categorization."}
{"script_processing_qid": "textbook-102-2-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Assess whether a 90% participation rate is achievable within a budget of $6.5 million, given that each truck costs $20,000.\n\nGOLD_ANSWER:\n1. **Target $z_i$**: $\\Phi^{-1}(0.90) = 1.282$.\\n2. **Solve for $T$**: $1.282 = -2.3116 + 0.010646T \\implies T = (1.282 + 2.3116)/0.010646 = 338$ trucks.\\n3. **Cost**: $338 \\times 20,000 = 6.76$ million.\\n4. **Conclusion**: The budget of $6.5 million is insufficient, as it falls short by $260,000.\n\nCANDIDATE_ANSWER:\nStep 1: Calculate maximum trucks purchasable with $6.5 million budget. Budget = $6,500,000 ÷ $20,000/truck = 325 trucks.\nStep 2: Use probit model equation Φ⁻¹(P_i) = -2.18098 + 0.0098898T. For 90% participation (P_i = 0.9), Φ⁻¹(0.9) ≈ 1.28 (from standard normal tables).\nStep 3: Solve 1.28 = -2.18098 + 0.0098898T → T ≈ (1.28 + 2.18098)/0.0098898 ≈ 350 trucks.\nStep 4: Compare required vs. budget-constrained trucks. Requires 350 trucks but only 325 are affordable.\nStep 5: Calculate achievable participation rate with 325 trucks: Φ(-2.18098 + 0.0098898*325) ≈ Φ(1.033) ≈ 84.9% (from standard normal table).\nConclusion: A 90% participation rate is not achievable with a $6.5M budget as it requires 350 trucks but only 325 can be purchased, yielding an 84.9% participation rate.\n\nQID: textbook-102-2-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-102-2-1-2\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's conclusion aligns with the gold answer (90% participation is unachievable with the budget), but the reasoning differs. The candidate uses a different probit model equation and calculates a slightly different number of required trucks (350 vs. 338) and achievable participation rate (84.9% vs. implied lower rate in gold answer).\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-102-2-1-2", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's conclusion aligns with the gold answer (90% participation is unachievable with the budget), but the reasoning differs. The candidate uses a different probit model equation and calculates a slightly different number of required trucks (350 vs. 338) and achievable participation rate (84.9% vs. implied lower rate in gold answer)."}, "llm_echoed_qid": "textbook-102-2-1-2", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's conclusion aligns with the gold answer (90% participation is unachievable with the budget), but the reasoning differs. The candidate uses a different probit model equation and calculates a slightly different number of required trucks (350 vs. 338) and achievable participation rate (84.9% vs. implied lower rate in gold answer)."}
{"script_processing_qid": "textbook-122-8-0-1", "llm_grader_input_prompt_user": "\nQUESTION:\n2. Under Assumption FD.3, show that $\\mathrm{E}(\\Delta u\\_{i t}|X\\_{i}) = 0$ for $t=2,...,T$.\n\nGOLD_ANSWER:\n1. **Assumption FD.3**: $\\mathsf{E}(u\\_{i t}|\\pmb{X}\\_{i},a\\_{i})=0$ for all $t$. 2. **First-Difference**: $\\Delta u\\_{i t} = u\\_{i t} - u\\_{i,t-1}$. 3. **Expectation**: $\\mathrm{E}(\\Delta u\\_{i t}|X\\_{i}) = \\mathrm{E}(u\\_{i t}|X\\_{i}) - \\mathrm{E}(u\\_{i,t-1}|X\\_{i}) = 0 - 0 = 0$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall Assumption FD.3 states that the variance of differenced errors is constant and uncorrelated across time: Var(Δu_{it}|X_i) = σ² and Cov(Δu_{it}, Δu_{is}|X_i) = 0 for t ≠ s.\n\nStep 2: However, to show E(Δu_{it}|X_i) = 0, we primarily use the strict exogeneity condition from the first listed assumption (likely FD.1 in standard frameworks): E(u_{it}|X_i, a_i) = 0 for all t.\n\nStep 3: Apply the Law of Iterated Expectations to u_{it} and u_{i,t-1}:\n   - E(u_{it}|X_i) = E[E(u_{it}|X_i, a_i)|X_i] = E[0|X_i] = 0\n   - Similarly, E(u_{i,t-1}|X_i) = 0\n\nStep 4: Linearity of expectation implies:\n   E(Δu_{it}|X_i) = E(u_{it} - u_{i,t-1}|X_i) = E(u_{it}|X_i) - E(u_{i,t-1}|X_i) = 0 - 0 = 0.\n\nStep 5: Conclusion: While FD.3 governs the variance structure, the zero conditional expectation result fundamentally relies on the strict exogeneity assumption (FD.1). There may be a misalignment in assumption numbering in the provided context.\n\nQID: textbook-122-8-0-1\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-122-8-0-1\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's answer correctly concludes that E(Δu_{it}|X_i) = 0, but the reasoning misattributes the source of the zero conditional mean to FD.1 rather than FD.3 as specified in the gold answer. The steps are logically sound but reference a different assumption.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-122-8-0-1", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's answer correctly concludes that E(Δu_{it}|X_i) = 0, but the reasoning misattributes the source of the zero conditional mean to FD.1 rather than FD.3 as specified in the gold answer. The steps are logically sound but reference a different assumption."}, "llm_echoed_qid": "textbook-122-8-0-1", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's answer correctly concludes that E(Δu_{it}|X_i) = 0, but the reasoning misattributes the source of the zero conditional mean to FD.1 rather than FD.3 as specified in the gold answer. The steps are logically sound but reference a different assumption."}
{"script_processing_qid": "textbook-107-1-0-0", "llm_grader_input_prompt_user": "\nQUESTION:\n1. Derive the condition for constant returns to scale in the production function $f(x,y) = x_1x_2$.\n\nGOLD_ANSWER:\n1. **Definition**: A production function has constant returns to scale if $f(tx_1, tx_2) = t f(x_1, x_2)$ for all $t > 0$.\n2. **Apply**: $f(tx_1, tx_2) = (tx_1)(tx_2) = t^2 x_1x_2 = t^2 f(x_1, x_2)$.\n3. **Conclusion**: Since $f(tx_1, tx_2) = t^2 f(x_1, x_2) \\neq t f(x_1, x_2)$ unless $t=1$, the production function does not exhibit constant returns to scale for all $t > 0$.\n\nCANDIDATE_ANSWER:\nStep 1: Recall the definition of constant returns to scale (CRS) from the provided context: Doubling all inputs doubles output. Formally, for any scaling factor t > 0, CRS requires f(tx₁, tx₂) = t·f(x₁,x₂).\n\nStep 2: Apply the scaling factor t to both inputs of the given production function f(x₁,x₂) = x₁x₂:\n  f(tx₁, tx₂) = (tx₁)(tx₂) = t²x₁x₂.\n\nStep 3: Compare this scaled output to the CRS requirement t·f(x₁,x₂):\n  t·f(x₁,x₂) = t·(x₁x₂) = t x₁x₂.\n\nStep 4: Observe that t²x₁x₂ ≠ t x₁x₂ for general t ≠ 1. This implies the production function exhibits increasing returns to scale (output scales by t²), not constant returns to scale.\n\nStep 5: Note the contradiction with the contextual claim that 'f(x₁,x₂) = x₁x₂ has CRS.' The derivation shows this statement is mathematically inconsistent with standard CRS definitions. The condition f(tx₁, tx₂) = t·f(x₁,x₂) is only satisfied at t=1, not universally.\n\nQID: textbook-107-1-0-0\n", "llm_grader_raw_response": "{\n  \"qid\": \"textbook-107-1-0-0\",\n  \"category\": \"CORRECT\",\n  \"explanation\": \"The candidate correctly derived the condition for constant returns to scale and accurately concluded that the production function does not exhibit constant returns to scale for all t > 0, matching the gold answer.\"\n}", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-107-1-0-0", "category": "CORRECT", "explanation": "The candidate correctly derived the condition for constant returns to scale and accurately concluded that the production function does not exhibit constant returns to scale for all t > 0, matching the gold answer."}, "llm_echoed_qid": "textbook-107-1-0-0", "llm_grader_category": "CORRECT", "llm_grader_explanation": "The candidate correctly derived the condition for constant returns to scale and accurately concluded that the production function does not exhibit constant returns to scale for all t > 0, matching the gold answer."}
{"script_processing_qid": "textbook-106-17-1-2", "llm_grader_input_prompt_user": "\nQUESTION:\n3. Explain how tying can be used as a form of price discrimination. Provide a numerical example similar to the Makemoney Movies case to illustrate your answer.\n\nGOLD_ANSWER:\n1. **Price Discrimination**: Tying allows firms to bundle products with different valuations across customers, extracting more consumer surplus.\n2. **Example**: If City Theater values Star Wars at $15,000 and Hamlet at $5,000, and Country Theater values Star Wars at $5,000 and Hamlet at $15,000, bundling allows charging $20,000 to each, maximizing revenue.\n\nCANDIDATE_ANSWER:\nStep 1: Understand Tying as Price Discrimination\nTying forces customers to purchase two products together. Firms use this to segment markets based on willingness to pay for the tied product. High-usage customers (with higher WTP) pay more through increased tied product consumption.\n\nStep 2: Numerical Example Setup\n- Product: Movie tickets (tying product, cost $2) and popcorn (tied product, cost $1).\n- Two customer types:\n  - Casual: Values ticket at $10, buys 1 popcorn (WTP $4).\n  - Avid: Values ticket at $12, buys 3 popcorns (WTP $5 each).\n\nStep 3: Without Tying\n- Ticket price = $10, popcorn price = $4:\n  - Casual: Purchases ticket + 1 popcorn → Profit = ($10 - $2) + ($4 - $1) = $11.\n  - Avid: Purchases ticket + 3 popcorns → Profit = ($10 - $2) + 3×($4 - $1) = $17.\n\nStep 4: With Tying\n- Bundle ticket + popcorn at $14 (ticket $8 + popcorn $6 for 2 units):\n  - Casual: Forced to buy 2 popcorns → Pays $14. Profit = ($14 - $2 - 2×$1) = $10.\n  - Avid: Buys bundle + 1 extra popcorn at $5 → Total payment = $14 + $5 = $19. Profit = ($14 - $4) + ($5 - $1) = $14.\n\nStep 5: Price Discrimination Effect\n- Casual pays $14 (vs. $14 without tying) but receives excess popcorn they don’t value.\n- Avid pays $19 (vs. $22 without tying), but Superduper extracts more profit ($14 vs. $17) due to higher popcorn margin.\n\nConclusion: Tying allows Superduper to shift revenue from ticket sales to popcorn, where avid users (with higher WTP) pay disproportionately more, achieving price discrimination.\n\nQID: textbook-106-17-1-2\n", "llm_grader_raw_response": "```json\n{\n  \"qid\": \"textbook-106-17-1-2\",\n  \"category\": \"CORRECT_BUT_REASONING_MISMATCH\",\n  \"explanation\": \"The candidate's explanation of tying as a form of price discrimination is correct, and the numerical example effectively illustrates the concept. However, the reasoning and setup differ from the gold answer, which focuses on bundling two products with different valuations across customers rather than tying a product to a complementary good.\"\n}\n```", "llm_grader_repaired_and_parsed_response": {"qid": "textbook-106-17-1-2", "category": "CORRECT_BUT_REASONING_MISMATCH", "explanation": "The candidate's explanation of tying as a form of price discrimination is correct, and the numerical example effectively illustrates the concept. However, the reasoning and setup differ from the gold answer, which focuses on bundling two products with different valuations across customers rather than tying a product to a complementary good."}, "llm_echoed_qid": "textbook-106-17-1-2", "llm_grader_category": "CORRECT_BUT_REASONING_MISMATCH", "llm_grader_explanation": "The candidate's explanation of tying as a form of price discrimination is correct, and the numerical example effectively illustrates the concept. However, the reasoning and setup differ from the gold answer, which focuses on bundling two products with different valuations across customers rather than tying a product to a complementary good."}
