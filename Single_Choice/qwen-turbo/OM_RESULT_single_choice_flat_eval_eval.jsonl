{"qid": "OM-multiple-choice-4151", "question": "In the context of the Prisoner's Dilemma with strategies implemented by finite automata of sizes m₁ and m₂, what condition ensures that the strategy pair (σ^ω, τ^ω) is an equilibrium of G^T(m₁, m₂)?", "options": ["A) The complexity of any deviating strategy for Player 1 is at least T.", "B) The payoff vector x is within ε of the friendly payoff (3, 3).", "C) The play ω is periodic with period d.", "D) The automata sizes m₁ and m₂ are both at least T."], "gold_answer": "A", "source_files": ["Finitely Repeated Games with Finite Automata.md"], "question_context": "The proof outlines that for the Prisoner’s Dilemma, a pure strategy equilibrium (σ, τ) of G^T(m₁, m₂) results in a payoff of 3 for each player in each stage when 2 ≤ m₁ ≤ m₂ < T. The strategies σ^ω and τ^ω follow the play ω as long as the other player follows it and switch to punishing forever upon deviation. The play ω = T*(0,0) is such that any play with average payoff greater than 3 to Player 2 compatible with σ^ω requires a strategy with complexity at least T. Thus, σ^ω ∈ Σ¹(2) ⊂ Σ¹(m₁) and τ^ω ∈ Σ²(m₂), making (σ^ω, τ^ω) an equilibrium of G^T(m₁, m₂). The condition that any deviating strategy for Player 1 must have complexity at least T is crucial for ensuring equilibrium.", "question_context_used": "The proof outlines that for the Prisoner’s Dilemma, a pure strategy equilibrium (σ, τ) of G^T(m₁, m₂) results in a payoff of 3 for each player in each stage when 2 ≤ m₁ ≤ m₂ < T. The strategies σ^ω and τ^ω follow the play ω as long as the other player follows it and switch to punishing forever upon deviation. The play ω = T*(0,0) is such that any play with average payoff greater than 3 to Player 2 compatible with σ^ω requires a strategy with complexity at least T. Thus, σ^ω ∈ Σ¹(2) ⊂ Σ¹(m₁) and τ^ω ∈ Σ²(m₂), making (σ^ω, τ^ω) an equilibrium of G^T(m₁, m₂). The condition that any deviating strategy for Player 1 must have complexity at least T is crucial for ensuring equilibrium.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-1939", "question": "In the context of the NYCDOS marine transport system, which of the following complexities necessitated the development of the BOSS2 model to replace BOSS1?", "options": ["A) The introduction of a multi-commodity network due to recyclable waste transport", "B) The need to model a single sink network exclusively for solid municipal waste", "C) The reduction in the number of Marine Transfer Stations (MTSs) from eight to five", "D) The elimination of environmental regulations requiring barge netting"], "gold_answer": "A", "source_files": ["Modeling the Increased Complexity of New York City.md"], "question_context": "The transportation of recyclable waste within the marine transport system transforms the system from a single commodity network, transporting exclusively solid municipal waste, into a multi-commodity problem. The network modeled in BOSS1 had a single sink, the landfill. BOSS2 needs to model one additional sink in the form of an incinerator with variable capacity. BOSS2 will also be required to model a two-depot dispatch system. In BOSS1 all tugs could be assumed to start their tours for the delivery and pickup of barges at the landfill. This is no longer the case. The proximity of three MTSs located along the Brooklyn waterfront, coupled with the possible construction of incineration facilities in the same area, may result in a 'local area network' in which a set of tug(s) would operate.", "question_context_used": "The transportation of recyclable waste within the marine transport system transforms the system from a single commodity network, transporting exclusively solid municipal waste, into a multi-commodity problem. The network modeled in BOSS1 had a single sink, the landfill. BOSS2 needs to model one additional sink in the form of an incinerator with variable capacity. BOSS2 will also be required to model a two-depot dispatch system. In BOSS1 all tugs could be assumed to start their tours for the delivery and pickup of barges at the landfill. This is no longer the case. The proximity of three MTSs located along the Brooklyn waterfront, coupled with the possible construction of incineration facilities in the same area, may result in a 'local area network' in which a set of tug(s) would operate.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-938", "question": "Under what condition does a 'fair' tax incentive scheme enable a firm to benefit from decentralization according to the given model?", "options": ["A) If the variances and covariances of the error terms are sufficiently large", "B) If the centralized solution's profit is maximized", "C) If the tax vector is set to zero", "D) If the units are of the same magnitude"], "gold_answer": "A", "source_files": ["An Optimal Tax Incentive Scheme for a Decentralize.md"], "question_context": "The text discusses the conditions under which a 'fair' tax incentive scheme can enable a firm to benefit from decentralization. It states that if the variances of the $\\epsilon_{t}^{\\mathrm{~\\tiny~s~}}$ are large enough, such a scheme will exist. The central planner chooses the tax $\\mathbf{x}^{*}$ to maximize expected profit, with the condition $E[\\pi_{d}(\\mathbf{x}^{*})]>E[\\pi_{c}]$ if the elements of $\\Sigma_{\\epsilon}$ are large enough. For example, if $\\Sigma_{\\epsilon}=\\sigma^{2}I$, then $E[\\pi_{d}(\\mathbf{x}^{*})]>E[\\pi_{c}]$ if $\\sigma^{2}>(E[\\pi_{c}]-{\\bf K}_{1})/t r[F^{-1}(F-A)F^{-1}]$. The intuitive explanation is that large variances make the error terms significant enough to offset losses from not achieving the centralized solution.", "question_context_used": "The text discusses the conditions under which a 'fair' tax incentive scheme can enable a firm to benefit from decentralization. It states that if the variances of the $\\epsilon_{t}^{\\mathrm{~\\tiny~s~}}$ are large enough, such a scheme will exist. The central planner chooses the tax $\\mathbf{x}^{*}$ to maximize expected profit, with the condition $E[\\pi_{d}(\\mathbf{x}^{*})]>E[\\pi_{c}]$ if the elements of $\\Sigma_{\\epsilon}$ are large enough. For example, if $\\Sigma_{\\epsilon}=\\sigma^{2}I$, then $E[\\pi_{d}(\\mathbf{x}^{*})]>E[\\pi_{c}]$ if $\\sigma^{2}>(E[\\pi_{c}]-{\\bf K}_{1})/t r[F^{-1}(F-A)F^{-1}]$. The intuitive explanation is that large variances make the error terms significant enough to offset losses from not achieving the centralized solution.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-3393", "question": "In the context of the marginal cost model for fleet maintenance, what was the primary challenge identified in the data normalization process?", "options": ["A) Adjusting for inflation across different operating environments", "B) Reconciling differences between historic accounting data and model requirements", "C) Incorporating fleetwide average costs into the regression model", "D) Quantifying the impact of different design series on maintenance costs"], "gold_answer": "B", "source_files": ["Buttercup's Advice_ A Tale of Data Definition.md"], "question_context": "The challenge in this modeling situation turned out to be one of data normalization. Once we understood the specific differences between the historic data available from general accounting records and that data required to construct the marginal cost model, model construction was simple. We finished this project with a choice between two multiple linear regression models with marginal cost coefficients in the tradition of a classic classroom-problem solution [Levin and Kirkpatrick. l978]. But that's the end of the story; it is the tale of getting there that we are unfolding. The primary issue was aligning the existing accounting data with the requirements for a marginal cost model that could distinguish between expenditures at each operating location.", "question_context_used": "The challenge in this modeling situation turned out to be one of data normalization. Once we understood the specific differences between the historic data available from general accounting records and that data required to construct the marginal cost model, model construction was simple. We finished this project with a choice between two multiple linear regression models with marginal cost coefficients in the tradition of a classic classroom-problem solution [Levin and Kirkpatrick. l978]. But that's the end of the story; it is the tale of getting there that we are unfolding. The primary issue was aligning the existing accounting data with the requirements for a marginal cost model that could distinguish between expenditures at each operating location.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-4580", "question": "In the context of asymptotic probabilistic analysis of routing problems, which of the following is a key consideration when evaluating the performance of routing algorithms?", "options": ["A) The deterministic shortest path under worst-case conditions", "B) The convergence behavior of the algorithm as problem size increases", "C) The exact solution for small-scale instances", "D) The computational complexity under fixed input sizes"], "gold_answer": "B", "source_files": ["An Asymptotic, Probabilistic Analysis of a Routing.md"], "question_context": "The article 'An Asymptotic, Probabilistic Analysis of a Routing Problem' by David M. Stein focuses on the behavior of routing algorithms as the problem size grows. The analysis is probabilistic, meaning it considers the likelihood of certain outcomes rather than deterministic guarantees. A key aspect of such analysis is understanding how the algorithm's performance converges as the number of nodes or routes increases, which is central to evaluating scalability and efficiency in large-scale routing problems. The mathematical framework often involves limits and probability distributions to model this asymptotic behavior.", "question_context_used": "The article 'An Asymptotic, Probabilistic Analysis of a Routing Problem' by David M. Stein focuses on the behavior of routing algorithms as the problem size grows. The analysis is probabilistic, meaning it considers the likelihood of certain outcomes rather than deterministic guarantees. A key aspect of such analysis is understanding how the algorithm's performance converges as the number of nodes or routes increases, which is central to evaluating scalability and efficiency in large-scale routing problems. The mathematical framework often involves limits and probability distributions to model this asymptotic behavior.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-4612", "question": "According to the text, what is a critical requirement for effectively mitigating cost impacts in engineering and construction projects?", "options": ["A) Conducting retrospective analysis of past projects", "B) Anticipating the timing and causes of future impacts", "C) Focusing exclusively on construction changes", "D) Prioritizing schedule over cost considerations"], "gold_answer": "B", "source_files": ["System Dynamics Transforms Fluor Project and Chang.md"], "question_context": "The text highlights the challenges in conventional industry practices, emphasizing the need for proactive identification and avoidance of cost impacts. A fundamental challenge is the temporal and spatial separation of secondary impacts from originating change events, making impact quantification and causal attribution difficult. The text underscores the importance of foreseeing not only the amounts of impact but also their future timing to initiate effective preemptive action. Understanding the major causes of future productivity impacts is crucial for identifying advance actions to reduce impacts. For example, curtailing sustained overtime or expediting resolution of design issues can mitigate productivity losses. The text also critiques the traditional focus on construction changes, advocating for a projectwide examination of impacts rooted in engineering, which has highly leveraged effects. Additionally, it notes the phenomenon of 'death by a thousand cuts,' where cumulative changes create disproportionately more impact, and the importance of analyzing changes in isolation and together at different project times.", "question_context_used": "The text highlights the challenges in conventional industry practices, emphasizing the need for proactive identification and avoidance of cost impacts. A fundamental challenge is the temporal and spatial separation of secondary impacts from originating change events, making impact quantification and causal attribution difficult. The text underscores the importance of foreseeing not only the amounts of impact but also their future timing to initiate effective preemptive action. Understanding the major causes of future productivity impacts is crucial for identifying advance actions to reduce impacts. For example, curtailing sustained overtime or expediting resolution of design issues can mitigate productivity losses. The text also critiques the traditional focus on construction changes, advocating for a projectwide examination of impacts rooted in engineering, which has highly leveraged effects. Additionally, it notes the phenomenon of 'death by a thousand cuts,' where cumulative changes create disproportionately more impact, and the importance of analyzing changes in isolation and together at different project times.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-782", "question": "According to Churchman's argument in 'The Reliability of Models in the Social Sciences', what is the foundational basis of rationality?", "options": ["A) Empirical evidence and logical consistency", "B) Faith and emotional belief", "C) Scientific methodology and peer review", "D) Mathematical proofs and axiomatic systems"], "gold_answer": "B", "source_files": ["Information Systems in Management Science—Pragmati.md"], "question_context": "Churchman's opening gambit in 'The Reliability of Models in the Social Sciences' is: 'One of man's great intellectual dreams is to think rationally about the world he inhabits.' He observes that rationality is a dream, something in which we believe, something in which we have faith. Thus, the very foundation of rationality is irrational and emotional. This perspective challenges the conventional view that rationality is purely based on logical and empirical grounds, suggesting instead that it is underpinned by emotional and belief-based commitments.", "question_context_used": "Churchman's opening gambit in 'The Reliability of Models in the Social Sciences' is: 'One of man's great intellectual dreams is to think rationally about the world he inhabits.' He observes that rationality is a dream, something in which we believe, something in which we have faith. Thus, the very foundation of rationality is irrational and emotional. This perspective challenges the conventional view that rationality is purely based on logical and empirical grounds, suggesting instead that it is underpinned by emotional and belief-based commitments.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-4075", "question": "In the context of radial-arc routing traffic dynamics, what is the relationship between the functions F(t) and G(s) as described in the model?", "options": ["A) F(t) and G(s) are linear transformations of each other", "B) F(t) and G(s) are inverses of each other", "C) F(t) is the integral of G(s) with respect to time", "D) G(s) is the derivative of F(t) with respect to distance"], "gold_answer": "B", "source_files": ["Time Dependence in Commuter Traffic Models.md"], "question_context": "The text discusses a traffic model based on radial-arc routing, where distances and times are expressed in terms of integrals involving speeds. The functions F(t) and G(s) are introduced to represent arc distance traversed by time t and the time by which an arc distance s is traversed, respectively. The relationship between these functions is given by the equation: \n\n$$\nF(t)=s\\Leftrightarrow G(s)=t\n$$\n\nThis indicates that F and G are inverses of each other. The dynamics of the system are further elaborated with differential equations governing these functions, emphasizing their role in determining travel times and distances in the traffic model.", "question_context_used": "The text discusses a traffic model based on radial-arc routing, where distances and times are expressed in terms of integrals involving speeds. The functions F(t) and G(s) are introduced to represent arc distance traversed by time t and the time by which an arc distance s is traversed, respectively. The relationship between these functions is given by the equation: \n\n$$\nF(t)=s\\Leftrightarrow G(s)=t\n$$\n\nThis indicates that F and G are inverses of each other. The dynamics of the system are further elaborated with differential equations governing these functions, emphasizing their role in determining travel times and distances in the traffic model.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-4630", "question": "In the integrated optimization model for production and distribution planning proposed by Fumero and Vercellis, which of the following is a key advantage of the synchronized approach over the decoupled decision process?", "options": ["A) Reduction in the number of vehicles required for distribution", "B) Substantial cost savings through integrated logistic decision-making", "C) Elimination of the need for inventory management", "D) Simplification of the production scheduling process"], "gold_answer": "B", "source_files": ["Synchronized Development of Production, Inventory,.md"], "question_context": "The paper by Fumero and Vercellis proposes an integrated optimization model for production and distribution planning, aiming to optimally coordinate logistic decisions such as capacity management, inventory allocation, and vehicle routing. The model is solved via Lagrangean relaxation, providing both lower bounds and heuristic feasible solutions. Computational results indicate a substantial advantage of the synchronized approach over the decoupled decision process, where production and distribution are planned separately. The synchronized approach leads to relevant improvements in service level and substantial savings in total costs, including inventory, work in progress, setups, and transportation. The model considers a multiperiod horizon and multiproduct environment, with constraints on production capacity and vehicle fleet.", "question_context_used": "The paper by Fumero and Vercellis proposes an integrated optimization model for production and distribution planning, aiming to optimally coordinate logistic decisions such as capacity management, inventory allocation, and vehicle routing. The model is solved via Lagrangean relaxation, providing both lower bounds and heuristic feasible solutions. Computational results indicate a substantial advantage of the synchronized approach over the decoupled decision process, where production and distribution are planned separately. The synchronized approach leads to relevant improvements in service level and substantial savings in total costs, including inventory, work in progress, setups, and transportation. The model considers a multiperiod horizon and multiproduct environment, with constraints on production capacity and vehicle fleet.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-1145", "question": "For which type of matrix does Bard's original algorithm guarantee convergence without cycling?", "options": ["A) Any matrix with non-negative entries", "B) P-matrices", "C) Symmetric positive definite matrices", "D) Matrices with a Petrie path"], "gold_answer": "B", "source_files": ["Digraph Models of Bard-Type Algorithms for the Lin.md"], "question_context": "Bard’s original proposal was to interchange the basic status of $w_{k}$ and $z_{k}$ if $q_{k}=\\operatorname*{min}q_{i}<0$. This scheme can certainly cycle for a $Q$-matrix $M$ which is not a $P$-matrix. However, Murty's algorithm, a Bard-type algorithm, was proven to converge for $P$-matrices. The convergence of Murty's algorithm for $P$-matrices is transparent from the $P$-digraph $G(M/q)$. The algorithm starts out and remains on the face $F(M,q,e_{n})$ as long as $q_{i}<0$ for some $i<n$. $F(M,q,e_{n})$ is a $P$ digraph, so by induction the algorithm reaches the (unique) sink of $F(M,q,e_{n})$.", "question_context_used": "Bard’s original proposal was to interchange the basic status of $w_{k}$ and $z_{k}$ if $q_{k}=\\operatorname*{min}q_{i}<0$. This scheme can certainly cycle for a $Q$-matrix $M$ which is not a $P$-matrix. However, Murty's algorithm, a Bard-type algorithm, was proven to converge for $P$-matrices. The convergence of Murty's algorithm for $P$-matrices is transparent from the $P$-digraph $G(M/q)$. The algorithm starts out and remains on the face $F(M,q,e_{n})$ as long as $q_{i}<0$ for some $i<n$. $F(M,q,e_{n})$ is a $P$ digraph, so by induction the algorithm reaches the (unique) sink of $F(M,q,e_{n})$.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-2838", "question": "According to Lemma 2, what is the upper bound for the value function \\( u(x, \\pi, t) \\) in the given stochastic control problem?", "options": ["A) \\( u(x, \\pi, t) \\leq x e^{(a - r)(T - t)} \\)", "B) \\( u(x, \\pi, t) \\leq x e^{(a - r)^{+}(T - t)} \\)", "C) \\( u(x, \\pi, t) \\leq x e^{(r - a)^{+}(T - t)} \\)", "D) \\( u(x, \\pi, t) \\leq x e^{(a + r)(T - t)} \\)"], "gold_answer": "B", "source_files": ["Optimal Redeeming Strategy of Stock Loans Under Dr.md"], "question_context": "Lemma 2 provides an upper bound for the value function \\( u(x, \\pi, t) \\), ensuring the problem is well-posed. The lemma states: \n\n$$ u(x, \\pi, t) \\leq x e^{(a - r)^{+}(T - t)}, \\quad (x, \\pi, t) \\in \\mathcal{A}. $$\n\nHere, \\( (a - r)^{+} \\) denotes the positive part of \\( a - r \\), i.e., \\( \\max\\{a - r, 0\\} \\). This bound is derived using Itô's lemma and properties of the stochastic processes involved. The result is crucial for establishing the finiteness of the optimal value in the stochastic control problem.", "question_context_used": "Lemma 2 provides an upper bound for the value function \\( u(x, \\pi, t) \\), ensuring the problem is well-posed. The lemma states: \n\n$$ u(x, \\pi, t) \\leq x e^{(a - r)^{+}(T - t)}, \\quad (x, \\pi, t) \\in \\mathcal{A}. $$\n\nHere, \\( (a - r)^{+} \\) denotes the positive part of \\( a - r \\), i.e., \\( \\max\\{a - r, 0\\} \\). This bound is derived using Itô's lemma and properties of the stochastic processes involved. The result is crucial for establishing the finiteness of the optimal value in the stochastic control problem.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-2704", "question": "According to the consumer surplus model for Extended Area Service (EAS), what is the mathematical condition under which a customer will purchase EAS?", "options": ["A) $\\Delta CS > \\pi$", "B) $\\Delta CS + \\gamma > \\pi$", "C) $TS + V_s > \\pi$", "D) $\\gamma > \\pi$"], "gold_answer": "B", "source_files": ["Modeling Telecommunications Demand Analysis.md"], "question_context": "The customer's decision to purchase EAS can be modeled using a utility maximization framework. A customer will purchase EAS if the benefits associated with purchase exceed the costs, or mathematically, when $$\\Delta C S+\\gamma>\\pi$$ where $$\\Delta C S=T S+V_{s}$$ $\\gamma$ captures any nonfinancial benefits associated with EAS purchase, and $\\pi$ is the EAS subscription price. The model explicitly acknowledges nonfinancial benefits and includes the benefits of increased calling in the benefit measure.", "question_context_used": "The customer's decision to purchase EAS can be modeled using a utility maximization framework. A customer will purchase EAS if the benefits associated with purchase exceed the costs, or mathematically, when $$\\Delta C S+\\gamma>\\pi$$ where $$\\Delta C S=T S+V_{s}$$ $\\gamma$ captures any nonfinancial benefits associated with EAS purchase, and $\\pi$ is the EAS subscription price. The model explicitly acknowledges nonfinancial benefits and includes the benefits of increased calling in the benefit measure.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-678", "question": "In the context of the bullwhip effect and coordinated demand forecasting, which of the following strategies is most effective in reducing the wholesaler's inventory levels in a two-firm supply chain?", "options": ["A) Implementing order batching to consolidate shipments", "B) Sharing consumer demand information between the retailer and wholesaler", "C) Increasing the lead time for order fulfillment", "D) Allowing the wholesaler to set the retailer's selling price"], "gold_answer": "B", "source_files": ["Teaching the Costs of Uncoordinated Supply Chains.md"], "question_context": "The bullwhip effect describes the phenomenon where order sizes, inventory, and back orders oscillate widely in a supply chain, particularly upstream. Lee et al. (1997a, b) suggest sharing information about consumer demand with all supply-chain members or having one member perform forecasting for all members. In a simplified beer game simulation with coordinated forecasting, the wholesaler’s total on-hand inventory over 20 periods was 42% smaller when demand information was shared compared to uncoordinated forecasting. The simulation uses an order-up-to policy where the order size equals next period’s forecast minus the inventory position. The formula for the retailer's ending inventory is: \\(\\text{MAX}(D5 + G5 - E5 - B6, 0)\\), and the wholesaler’s forecast in the coordinated case equals the retailer’s forecast, which equals the consumer orders in that period.", "question_context_used": "The bullwhip effect describes the phenomenon where order sizes, inventory, and back orders oscillate widely in a supply chain, particularly upstream. Lee et al. (1997a, b) suggest sharing information about consumer demand with all supply-chain members or having one member perform forecasting for all members. In a simplified beer game simulation with coordinated forecasting, the wholesaler’s total on-hand inventory over 20 periods was 42% smaller when demand information was shared compared to uncoordinated forecasting. The simulation uses an order-up-to policy where the order size equals next period’s forecast minus the inventory position. The formula for the retailer's ending inventory is: \\(\\text{MAX}(D5 + G5 - E5 - B6, 0)\\), and the wholesaler’s forecast in the coordinated case equals the retailer’s forecast, which equals the consumer orders in that period.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-976", "question": "In the context of stochastic user equilibrium (SUE) sensitivity analysis, what is the primary advantage of adopting Dial's algorithm for stochastic traffic assignment?", "options": ["A) It simplifies the computation of derivatives of link costs and flows with respect to uncertainty parameters.", "B) It eliminates the need for origin-destination demand variables in the model.", "C) It reduces the dispersion parameter θ to zero for deterministic equilibrium.", "D) It allows for exact computation of Wardropian equilibrium without approximation."], "gold_answer": "A", "source_files": ["Sensitivity Analysis for Stochastic User Equilibri.md"], "question_context": "The paper presents a method for sensitivity analysis for network flows at stochastic user equilibrium, developed from a dual formulation of the stochastic user equilibrium analysis. By adopting Dial’s algorithm for stochastic traffic assignment, the authors formulate a computationally efficient link-based algorithm for the sensitivity analysis. The primary problem addressed is the computation of the derivatives of link costs and flows with respect to some uncertainty parameters in link cost functions and in O-D demands. The method leverages Dial’s algorithm to achieve computational efficiency, as highlighted in the text: 'By adopting Dial’s algorithm (Dial 1971) for stochastic traffic assignment, we are able to formulate an efficient link-based computational method for the sensitivity analysis.'", "question_context_used": "The paper presents a method for sensitivity analysis for network flows at stochastic user equilibrium, developed from a dual formulation of the stochastic user equilibrium analysis. By adopting Dial’s algorithm for stochastic traffic assignment, the authors formulate a computationally efficient link-based algorithm for the sensitivity analysis. The primary problem addressed is the computation of the derivatives of link costs and flows with respect to some uncertainty parameters in link cost functions and in O-D demands. The method leverages Dial’s algorithm to achieve computational efficiency, as highlighted in the text: 'By adopting Dial’s algorithm (Dial 1971) for stochastic traffic assignment, we are able to formulate an efficient link-based computational method for the sensitivity analysis.'", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-1854", "question": "In the bottom-up approach for structuring manufacturing strategy alternatives, which of the following is a key advantage of using a relative scoring system with zero points assigned to the worst alternative and 100 points to the best for each attribute?", "options": ["A) It ensures that all attributes are weighted equally regardless of their impact on decision-making.", "B) It guarantees maximum numerical separation between alternatives by dropping attributes with no discriminatory power.", "C) It simplifies the analysis by eliminating the need for hierarchical structuring of value differences.", "D) It allows for the direct comparison of absolute costs across different manufacturing regions."], "gold_answer": "B", "source_files": ["Structuring Value Attributes.md"], "question_context": "The bottom-up approach was used to structure the values of a commercial client considering the construction of a new factory. A relative scoring system was used in which zero points of value were assigned to the worst alternative and 100 points to the best, for each attribute. Then, for each attribute, value weights were assigned that reflected the change in value to the decision maker between the best and worst alternatives for each attribute. These value weights are also relative in that they form a quantitative description of the swing in importance from the worst to best level of one attribute compared to the others. One advantage of the bottom-up approach in this situation is its automatic stopping rule: attributes (and even objectives) that do not discriminate between the alternatives are not considered; that is, an attribute for which the best and worst alternatives are of equal value is given a zero value weight and is dropped from the analysis. As a result, the bottom-up approach guarantees maximum numerical separation between the alternatives because there are no attributes on which all of the alternatives are equal.", "question_context_used": "The bottom-up approach was used to structure the values of a commercial client considering the construction of a new factory. A relative scoring system was used in which zero points of value were assigned to the worst alternative and 100 points to the best, for each attribute. Then, for each attribute, value weights were assigned that reflected the change in value to the decision maker between the best and worst alternatives for each attribute. These value weights are also relative in that they form a quantitative description of the swing in importance from the worst to best level of one attribute compared to the others. One advantage of the bottom-up approach in this situation is its automatic stopping rule: attributes (and even objectives) that do not discriminate between the alternatives are not considered; that is, an attribute for which the best and worst alternatives are of equal value is given a zero value weight and is dropped from the analysis. As a result, the bottom-up approach guarantees maximum numerical separation between the alternatives because there are no attributes on which all of the alternatives are equal.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-119", "question": "In an off-airport terminal system with two optimally located terminals, what is the optimal location of the second terminal (X'*) relative to the distance between the city boundary and the airport (L) and the network speed relative to the link speed (R)?", "options": ["A) X'* = L / (3 + R)", "B) X'* = 4L / (5 + R)", "C) X'* = 2L / (3 + R)", "D) X'* = L / (2 + R)"], "gold_answer": "B", "source_files": ["The Application of Location Models to Off-Airport.md"], "question_context": "The optimal location of the second terminal in a two-terminal system is derived from the equation: $$X^{\\prime*}=4L/(5+R).$$ This result is obtained by combining the optimal location of the first terminal, $$X^{*}=X^{\\prime}/2,$$ with the time savings formula. The analysis shows that the best location for a single off-airport terminal is far from either of the desired locations in a two-terminal system, highlighting the trade-offs in planning such systems.", "question_context_used": "The optimal location of the second terminal in a two-terminal system is derived from the equation: $$X^{\\prime*}=4L/(5+R).$$ This result is obtained by combining the optimal location of the first terminal, $$X^{*}=X^{\\prime}/2,$$ with the time savings formula. The analysis shows that the best location for a single off-airport terminal is far from either of the desired locations in a two-terminal system, highlighting the trade-offs in planning such systems.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-1772", "question": "In the Bass model for new product sales, which of the following correctly represents the epidemic equation for initial purchases when considering both innovators and imitators?", "options": ["A) V = γ₁[Q_M - Q] + γ₂[Q_M - Q]Q", "B) V = γ₁Q + γ₂[Q_M - Q]Q", "C) V = γ₁[Q_M - Q] + γ₂Q²", "D) V = γ₁Q_M + γ₂[Q_M - Q]"], "gold_answer": "A", "source_files": ["Dynamic Price Models for New-Product Planning.md"], "question_context": "The Bass model assumes that there are essentially two basic kinds of purchasers. The first group, innovators, make an independent decision to buy or not to buy. The volume of sales to this group is simply proportional to the number of potential customers who do not already own the product, Qm -- Q. The second group consists of people who are sensitive to the actions of their peers. Sales to the second group is again proportional to the number of people who do not have the product, Qm - Q; but it is also proportional to the number of people who do have the product, Q. Summing these two terms, we obtain the epidemic equation for new product sales: $$V=\\gamma_{1}[Q_{M}-Q]+\\gamma_{2}[Q_{M}-Q]Q$$ where γ₁ and γ₂ are constants of proportionality. This equation captures the combined effect of innovators and imitators on the sales volume of a new product.", "question_context_used": "The Bass model assumes that there are essentially two basic kinds of purchasers. The first group, innovators, make an independent decision to buy or not to buy. The volume of sales to this group is simply proportional to the number of potential customers who do not already own the product, Qm -- Q. The second group consists of people who are sensitive to the actions of their peers. Sales to the second group is again proportional to the number of people who do not have the product, Qm - Q; but it is also proportional to the number of people who do have the product, Q. Summing these two terms, we obtain the epidemic equation for new product sales: $$V=\\gamma_{1}[Q_{M}-Q]+\\gamma_{2}[Q_{M}-Q]Q$$ where γ₁ and γ₂ are constants of proportionality. This equation captures the combined effect of innovators and imitators on the sales volume of a new product.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-2846", "question": "According to the analysis of Operations Research job ads, which technical skill is most frequently required by employers?", "options": ["A) Operations management skills", "B) Modeling requirements", "C) Statistics skills", "D) Programming skills"], "gold_answer": "A", "source_files": ["ASP, The Art and Science of Practice_ Skills Emplo.md"], "question_context": "The text analyzes the proportion of job ads requiring specific skills in Operations Research. Technical skills are divided into operations management, modeling, statistics, programming, spreadsheet and database, and basic IT skills. Operations management skills are the most frequently required, appearing in 53 percent of all employer ads. These include process-related skills (21 percent), performance measurement (15 percent), and supply chain management (22 percent). Modeling requirements appear in 52 percent of the ads, statistics skills in 48 percent, and programming skills in 47 percent. The data clearly indicate that operations management skills are the most demanded by employers in this field.", "question_context_used": "The text analyzes the proportion of job ads requiring specific skills in Operations Research. Technical skills are divided into operations management, modeling, statistics, programming, spreadsheet and database, and basic IT skills. Operations management skills are the most frequently required, appearing in 53 percent of all employer ads. These include process-related skills (21 percent), performance measurement (15 percent), and supply chain management (22 percent). Modeling requirements appear in 52 percent of the ads, statistics skills in 48 percent, and programming skills in 47 percent. The data clearly indicate that operations management skills are the most demanded by employers in this field.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-104", "question": "In Algorithm A.1, what is the primary purpose of the variable $f_{m t}$ in the context of the simulation model for empty hopper car arrivals?", "options": ["A) To store the total number of hopper cars arriving across all replications for each day $t$.", "B) To specify the probability of $m$ cars arriving for each day $t$ in the planning horizon.", "C) To track the cumulative inventory levels for each replication $r$ and day $t$.", "D) To determine the optimal number of replications $R$ required for accurate simulation results."], "gold_answer": "B", "source_files": ["A Decision Support System for Empty Hopper Car Man.md"], "question_context": "Algorithm A.1 provides pseudocode for the simulation model that generates the probability distribution for the number of cars arriving throughout the planning horizon. The algorithm returns a variable, $f_{m t},$ which specifies the probability of $m$ cars arriving for each day $t$ in the planning horizon $0,\\ldots,T$. The variable $f_{m t}$ is constructed by counting the number of cars arriving on each day $t$ in each replication $r$ and then translating these counts into relative frequencies by dividing by the number of replications $R$. This process results in $f_{m t}$ capturing the probability distribution of arrivals for each day in the planning horizon.", "question_context_used": "Algorithm A.1 provides pseudocode for the simulation model that generates the probability distribution for the number of cars arriving throughout the planning horizon. The algorithm returns a variable, $f_{m t},$ which specifies the probability of $m$ cars arriving for each day $t$ in the planning horizon $0,\\ldots,T$. The variable $f_{m t}$ is constructed by counting the number of cars arriving on each day $t$ in each replication $r$ and then translating these counts into relative frequencies by dividing by the number of replications $R$. This process results in $f_{m t}$ capturing the probability distribution of arrivals for each day in the planning horizon.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-1896", "question": "According to the study by Zadorojniy et al., which analytical method was used to model the wastewater treatment process behavior and estimate a transition probability matrix?", "options": ["A) Descriptive analytics using historical sensor data", "B) Predictive analytics with a transition probability matrix", "C) Prescriptive analytics via Markov decision processes", "D) Multivariate adaptive regression splines (MARS)"], "gold_answer": "B", "source_files": ["Unleashing Analytics to Reduce Costs and Improve Q.md"], "question_context": "The study developed an innovative operational control process applying descriptive, predictive, and prescriptive analytics. The predictive analytics model the wastewater treatment process behavior using a transition probability matrix, which was estimated. The prescriptive Markov decision process analytics offer recommendations for improved operations. The system was deployed at a plant in Lleida, Spain, showing significant reductions in electricity consumption, chemical usage, and sludge production.", "question_context_used": "The study developed an innovative operational control process applying descriptive, predictive, and prescriptive analytics. The predictive analytics model the wastewater treatment process behavior using a transition probability matrix, which was estimated. The prescriptive Markov decision process analytics offer recommendations for improved operations. The system was deployed at a plant in Lleida, Spain, showing significant reductions in electricity consumption, chemical usage, and sludge production.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-316", "question": "In the context of Minimum-Aggregate-Concave-Cost Multicommodity Flows in Strong-Series-Parallel Networks, what is the primary mathematical structure that facilitates the optimization problem?", "options": ["A) Strong-Series-Parallel Networks due to their recursive decomposition properties", "B) Complete Graphs due to their high connectivity", "C) Random Graphs due to their probabilistic edge distributions", "D) Tree Networks due to their acyclic nature"], "gold_answer": "A", "source_files": ["Minimum-Aggregate-Concave-Cost Multicommodity Flow.md"], "question_context": "The article discusses the optimization problem of Minimum-Aggregate-Concave-Cost Multicommodity Flows, specifically within Strong-Series-Parallel Networks. These networks are chosen for their unique properties that allow for efficient decomposition and solution of the multicommodity flow problem. The recursive structure of Strong-Series-Parallel Networks enables the application of dynamic programming techniques to find the minimum aggregate concave cost. The mathematical formulation leverages the network's properties to ensure tractability and optimality of the solution.", "question_context_used": "The article discusses the optimization problem of Minimum-Aggregate-Concave-Cost Multicommodity Flows, specifically within Strong-Series-Parallel Networks. These networks are chosen for their unique properties that allow for efficient decomposition and solution of the multicommodity flow problem. The recursive structure of Strong-Series-Parallel Networks enables the application of dynamic programming techniques to find the minimum aggregate concave cost. The mathematical formulation leverages the network's properties to ensure tractability and optimality of the solution.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-3030", "question": "In the construction of the probability law $\\pmb{P_{R}}$ on $(\\overrightarrow{10,\\infty})\\times\\overrightarrow{5})^{\\infty}$, what is the role of the transition probability mapping $z\\rightarrow Q_{R}^{(n)}(z)$?", "options": ["A) It defines the probability distribution of the initial state of the system.", "B) It specifies the joint probability of the next jump epoch and the state of the system after the jump.", "C) It determines the stationary distribution of the Markov process.", "D) It calculates the expected number of jumps within a finite time interval."], "gold_answer": "B", "source_files": ["Discretization and Weak Convergence in Markov Deci.md"], "question_context": "The inductive procedure for the construction of a probability law $\\pmb{P_{R}}$ on $(\\overrightarrow{10,\\infty})\\times\\overrightarrow{5})^{\\infty}$ involves constructing a probability law $Q_{R}^{(n)}(z)$ on $\\widetilde{[0,\\infty)\\times\\bar{s}}$ for the next jump epoch and state of the system after the jump. The mapping $z\\rightarrow Q_{R}^{(n)}(z)$ is a transition probability from $(\\overbrace{\\mathrm{f}\\boldsymbol{0},\\infty})\\times\\overbrace{\\boldsymbol{s}})^{n}$ to $\\widetilde{[0,\\infty)\\times S}$. This construction is essential for defining the probability measures $\\pmb{P_{R}^{(n)}}$ and $\\pmb{P_{R}}$ on the respective spaces.", "question_context_used": "The inductive procedure for the construction of a probability law $\\pmb{P_{R}}$ on $(\\overrightarrow{10,\\infty})\\times\\overrightarrow{5})^{\\infty}$ involves constructing a probability law $Q_{R}^{(n)}(z)$ on $\\widetilde{[0,\\infty)\\times\\bar{s}}$ for the next jump epoch and state of the system after the jump. The mapping $z\\rightarrow Q_{R}^{(n)}(z)$ is a transition probability from $(\\overbrace{\\mathrm{f}\\boldsymbol{0},\\infty})\\times\\overbrace{\\boldsymbol{s}})^{n}$ to $\\widetilde{[0,\\infty)\\times S}$. This construction is essential for defining the probability measures $\\pmb{P_{R}^{(n)}}$ and $\\pmb{P_{R}}$ on the respective spaces.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-944", "question": "In the context of integrating Data Envelopment Analysis (DEA) with Activity-Based Costing (ABC) in a large Mideast bank, which of the following best describes the primary advantage of this combined approach?", "options": ["A) It reduces the overall implementation cost of ABC systems.", "B) It provides detailed quantitative performance benchmarks for specific business activities.", "C) It eliminates the need for traditional cost accounting methods.", "D) It automates the entire process of financial reporting."], "gold_answer": "B", "source_files": ["Measuring Efficiency by Product Group_ Integrating.md"], "question_context": "The article discusses a method for measuring product-specific inefficiency in bank branches by integrating activity-based cost accounting (ABC) and data envelopment analysis (DEA). The authors highlight that while ABC systems are expensive to implement, combining them with DEA provides managers with detailed quantitative performance benchmarks for specific business activities. This integrated approach allows for precise measurement of waste and identification of its causes, facilitating decision-making in areas such as cost reduction, product mix changes, and performance-linked incentives. The method was applied to a large 250-branch Mideast bank, measuring inefficiency separately for customer services and transactions.", "question_context_used": "The article discusses a method for measuring product-specific inefficiency in bank branches by integrating activity-based cost accounting (ABC) and data envelopment analysis (DEA). The authors highlight that while ABC systems are expensive to implement, combining them with DEA provides managers with detailed quantitative performance benchmarks for specific business activities. This integrated approach allows for precise measurement of waste and identification of its causes, facilitating decision-making in areas such as cost reduction, product mix changes, and performance-linked incentives. The method was applied to a large 250-branch Mideast bank, measuring inefficiency separately for customer services and transactions.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-1546", "question": "In the context of the QED regime for the G/GI/N + GI queue with customer abandonment, what is the primary role of the mapping ϕ as defined in the text?", "options": ["A) To model the arrival process of customers who abandon the queue upon arrival.", "B) To provide a generalized version of Reed's operator φ, incorporating the effect of customer abandonment through the parameter θ.", "C) To calculate the exact waiting time for each customer in the system.", "D) To determine the service completion rate for the infinite-server process."], "gold_answer": "B", "source_files": ["Queues with Many Servers and Impatient Customers.md"], "question_context": "The mapping ϕ plays a central role in the analysis of the G/GI/N + GI queue in the QED regime, particularly in modeling systems with customer abandonment. It is a generalized version of Reed's operator φ, applicable to systems where customers may abandon the queue. The key difference lies in ϕ's incorporation of the parameter θ, which quantifies the rate of customer abandonment. Specifically, ϕ is defined such that for each x ∈ D[0,∞), ϕ(x) is the unique solution y to the equation: y(t) = x(t) + ∫₀ᵗ y⁺(t-s)dF(s) - (θ/μ)∫₀ᵗ y⁺(t-s)dF*(s), t ≥ 0. This equation accounts for the negative feedback effect of abandonment on the system's dynamics, where higher queue lengths lead to increased abandonment rates, which in turn reduce the effective arrival rate of customers who eventually receive service.", "question_context_used": "The mapping ϕ plays a central role in the analysis of the G/GI/N + GI queue in the QED regime, particularly in modeling systems with customer abandonment. It is a generalized version of Reed's operator φ, applicable to systems where customers may abandon the queue. The key difference lies in ϕ's incorporation of the parameter θ, which quantifies the rate of customer abandonment. Specifically, ϕ is defined such that for each x ∈ D[0,∞), ϕ(x) is the unique solution y to the equation: y(t) = x(t) + ∫₀ᵗ y⁺(t-s)dF(s) - (θ/μ)∫₀ᵗ y⁺(t-s)dF*(s), t ≥ 0. This equation accounts for the negative feedback effect of abandonment on the system's dynamics, where higher queue lengths lead to increased abandonment rates, which in turn reduce the effective arrival rate of customers who eventually receive service.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3431", "question": "According to the hierarchical planning system described, why is it critical for the planning horizon at a given level to equal the planning period at the level above?", "options": ["A) To minimize work-in-process inventory and lead times", "B) To ensure continuity and stability in the planning process", "C) To reduce the number of planning levels required", "D) To increase flexibility in rescheduling orders"], "gold_answer": "B", "source_files": ["Production Scheduling Theory_ Just Where Is It App.md"], "question_context": "The text emphasizes the importance of hierarchical planning systems, where planning levels must preserve continuity. Table 1 outlines recommended planning parameters, stating that 'for continuity and stability it is vital that the planning horizon at a planning level equals the planning period at the planning level one level above.' This ensures that plans at each organizational level are detailed versions of the top-level plan and that feedback aggregates properly from lower levels [Beischel and Smith 1991]. The steel company example illustrates a three-level hierarchy, where misalignment in capacities and planning periods leads to operational inefficiencies.", "question_context_used": "The text emphasizes the importance of hierarchical planning systems, where planning levels must preserve continuity. Table 1 outlines recommended planning parameters, stating that 'for continuity and stability it is vital that the planning horizon at a planning level equals the planning period at the planning level one level above.' This ensures that plans at each organizational level are detailed versions of the top-level plan and that feedback aggregates properly from lower levels [Beischel and Smith 1991]. The steel company example illustrates a three-level hierarchy, where misalignment in capacities and planning periods leads to operational inefficiencies.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3700", "question": "According to Wagner's article on Decision Support Systems (DSS), which of the following is identified as the 'touchstone and hallmark' of a DSS?", "options": ["A) Speed and ease of development and modification", "B) Communication with the manager in familiar terms", "C) Providing 'Executive Mind Support'", "D) Use of Monte Carlo (probabilistic) solutions"], "gold_answer": "C", "source_files": ["Decision Support Systems_ The Real Substance.md"], "question_context": "The article discusses the emergence of Decision Support Systems (DSS) as a new school of thought that enhances managers' thinking processes. A key finding from a questionnaire survey among users of an advanced planning or modeling language is reported, indicating that the applications of this language largely coincide with certain norms of the DSS concept. The touchstone and hallmark of a DSS is identified as its ability to provide 'Executive Mind Support.' This is made possible by software specifically created for this purpose, such as newer planning languages that enable managerial personnel to create and exercise simulation models. These systems are characterized by their interactive support for the thought processes of executives, speed and ease of development, and communication in terms familiar to the manager.", "question_context_used": "The article discusses the emergence of Decision Support Systems (DSS) as a new school of thought that enhances managers' thinking processes. A key finding from a questionnaire survey among users of an advanced planning or modeling language is reported, indicating that the applications of this language largely coincide with certain norms of the DSS concept. The touchstone and hallmark of a DSS is identified as its ability to provide 'Executive Mind Support.' This is made possible by software specifically created for this purpose, such as newer planning languages that enable managerial personnel to create and exercise simulation models. These systems are characterized by their interactive support for the thought processes of executives, speed and ease of development, and communication in terms familiar to the manager.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-1773", "question": "Given the dynamic pricing model discussed, which pricing strategy yields the highest discounted accumulated profit after 5 years according to Table 1?", "options": ["A) Marginal pricing", "B) Optimum constant-return-on-sales (26%)", "C) Optimum constant price (P'=3.25)", "D) The optimum price scenario"], "gold_answer": "D", "source_files": ["Dynamic Price Models for New-Product Planning.md"], "question_context": "The text discusses various pricing strategies, including marginal pricing, constant-return-on-sales, constant price, and an optimum price scenario. The evolution of the business under these strategies is analyzed, with a focus on discounted accumulated profit ($\\pi_{D}$). The key equations include:\n\n1. Constant-return-on-sales pricing: $P=C/[1-\\mu]$\n2. Constant price strategy: $P=P^{\\prime}=\\mathrm{constant}$\n\nTable 1 shows the performance of each strategy over 5 years, with future profits discounted at 40% per year. The optimum price scenario outperforms other strategies in terms of discounted accumulated profit.", "question_context_used": "The text discusses various pricing strategies, including marginal pricing, constant-return-on-sales, constant price, and an optimum price scenario. The evolution of the business under these strategies is analyzed, with a focus on discounted accumulated profit ($\\pi_{D}$). The key equations include:\n\n1. Constant-return-on-sales pricing: $P=C/[1-\\mu]$\n2. Constant price strategy: $P=P^{\\prime}=\\mathrm{constant}$\n\nTable 1 shows the performance of each strategy over 5 years, with future profits discounted at 40% per year. The optimum price scenario outperforms other strategies in terms of discounted accumulated profit.", "gold_answer_parsed": "D", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-3429", "question": "In the context of the steel company's aggregate capacity-planning procedure (ACPP), what is the primary objective of the heuristic scheduling algorithm mentioned?", "options": ["A) To maximize the utilization of all resources", "B) To minimize mean flow time while avoiding changes in the closest weeks", "C) To ensure all operations exceed one shift in length", "D) To reduce carbon emissions in the production process"], "gold_answer": "B", "source_files": ["Production Scheduling Theory_ Just Where Is It App.md"], "question_context": "At the steel company, we made recommendations for alleviating the problems, suggesting that the company use an aggregate capacity-planning procedure (ACPP) to link corporate planning and scheduling and to determine order acceptance. The ACPP has the following features: (1) All processing times are expressed in days with a rolling plan developed weekly for a planning horizon of eight weeks. (2) The technological order is expressed as a linear sequence of operations, each a facility’s processing of an order. A special heuristic scheduling algorithm minimizes mean flow time, using the rolling plan corrected by feedback as an initial solution and developing it while trying to avoid changes in the closest weeks.", "question_context_used": "At the steel company, we made recommendations for alleviating the problems, suggesting that the company use an aggregate capacity-planning procedure (ACPP) to link corporate planning and scheduling and to determine order acceptance. The ACPP has the following features: (1) All processing times are expressed in days with a rolling plan developed weekly for a planning horizon of eight weeks. (2) The technological order is expressed as a linear sequence of operations, each a facility’s processing of an order. A special heuristic scheduling algorithm minimizes mean flow time, using the rolling plan corrected by feedback as an initial solution and developing it while trying to avoid changes in the closest weeks.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-2584", "question": "Given the inequality for a random variable X with tail probability bound $P(|X|\\geqslant t)\\leqslant C\\exp(-t^{2}/H^{2})$ for $C\\geqslant1$, what is the resulting bound on $E|X|$ when $\\theta=H(\\log C)^{1/2}$ is chosen?", "options": ["A) $E|X|\\leqslant H(1 + (\\log C)^{1/2})$", "B) $E|X|\\leqslant H(\\log C)^{1/2}$", "C) $E|X|\\leqslant C H(1 + (\\log C)^{1/2})$", "D) $E|X|\\leqslant H(1 + C^{1/2})$"], "gold_answer": "A", "source_files": ["Stochastic Analysis of the Quadratic Assignment Pr.md"], "question_context": "For a random variable $X$, we have, for $\\theta\\geqslant0$, the inequality: $$ E|X|=\\int_{0}^{\\infty}P{\\big(}|X|\\geqslant t{\\big)}d t\\leqslant\\theta+\\int_{\\theta}^{\\infty}P{\\big(}|X|\\geqslant t{\\big)}d t. $$ If $P(|X|\\geqslant t)\\leqslant C\\exp(-t^{2}/H^{2})$ for $C\\geqslant1$, by taking $\\pmb{\\theta}=H(\\log C)^{1/2}$, we see by simple computation that $E|X|\\leqslant H(1+(\\log C)^{1/2})$. This result is derived by integrating the tail probability bound and optimizing the choice of $\\theta$ to minimize the upper bound on the expectation.", "question_context_used": "For a random variable $X$, we have, for $\\theta\\geqslant0$, the inequality: $$ E|X|=\\int_{0}^{\\infty}P{\\big(}|X|\\geqslant t{\\big)}d t\\leqslant\\theta+\\int_{\\theta}^{\\infty}P{\\big(}|X|\\geqslant t{\\big)}d t. $$ If $P(|X|\\geqslant t)\\leqslant C\\exp(-t^{2}/H^{2})$ for $C\\geqslant1$, by taking $\\pmb{\\theta}=H(\\log C)^{1/2}$, we see by simple computation that $E|X|\\leqslant H(1+(\\log C)^{1/2})$. This result is derived by integrating the tail probability bound and optimizing the choice of $\\theta$ to minimize the upper bound on the expectation.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-296", "question": "According to the survey of forecasting experts in a pharmaceutical company, which of the following is a significant conclusion about experts who rely on model-based forecasts?", "options": ["A) They believe the model-based forecast captures recent country-specific events accurately.", "B) They prefer to make large adjustments to the model-based forecasts.", "C) They do not believe that the model typically signifies an incorrect trend.", "D) They consider the model-based forecast unimportant in their forecasting decisions."], "gold_answer": "C", "source_files": ["Experts' Stated Behavior.md"], "question_context": "The survey involved forecasting experts in local sales offices of a multinational pharmaceutical company. The company uses an automated statistical package to generate sales forecasts, which are then adjusted by local experts. The program updates model parameters monthly, using sales data from previous months, and selects the best model based on past performance. Among the 42 experts surveyed, 20 relied on model-based forecasts. Key findings about these experts include: (1) They believe the model-based forecast is important in their decisions (Q1.1). (2) They prefer small adjustments (Q1.7). (3) They do not believe the model typically signifies an incorrect trend (Q1.10). (4) They are convinced the model does not capture recent country-specific events (Q1.12).", "question_context_used": "The survey involved forecasting experts in local sales offices of a multinational pharmaceutical company. The company uses an automated statistical package to generate sales forecasts, which are then adjusted by local experts. The program updates model parameters monthly, using sales data from previous months, and selects the best model based on past performance. Among the 42 experts surveyed, 20 relied on model-based forecasts. Key findings about these experts include: (1) They believe the model-based forecast is important in their decisions (Q1.1). (2) They prefer small adjustments (Q1.7). (3) They do not believe the model typically signifies an incorrect trend (Q1.10). (4) They are convinced the model does not capture recent country-specific events (Q1.12).", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-2984", "question": "In the NMBR mechanism, what is the probability bound for the event $\\mathcal{G}_{s}$ where the sample-average budget $\tilde{B}_{s}$ differs from its expected value $B_{s}$ by more than $T\\overline{{c}}_{s}\\delta_{s}/2$?", "options": ["A) $\\mathbb{P}[\\mathcal{G}_{s}] \\le 2\\exp\\left(-\\frac{\\delta_{s}^{2}R_{s}m}{27}\\right)$", "B) $\\mathbb{P}[\\mathcal{G}_{s}] \\le \\exp\\left(-\\frac{\\delta_{s}^{2}R_{s}m}{12}\\right)$", "C) $\\mathbb{P}[\\mathcal{G}_{s}] \\le 2\\exp\\left(-\\frac{\\delta_{s}^{2}R_{s}m}{3}\\right)$", "D) $\\mathbb{P}[\\mathcal{G}_{s}] \\le \\exp\\left(-\\frac{\\delta_{s}^{2}R_{s}m}{27}\\right)$"], "gold_answer": "A", "source_files": ["From Monetary to Nonmonetary Mechanism Design via.md"], "question_context": "The NMBR mechanism involves analyzing the sample-average budget $\tilde{B}_{s}$ and its deviation from the expected value $B_{s}$. The event $\\mathcal{G}_{s}$ is defined as $\\{\\left|\\tilde{B}_{s}-B_{s}\\right|\\geq T\\overline{{c}}_{s}\\delta_{s}/2\\}$. The probability bound for this event is derived using standard Chernoff bounds, resulting in $\\mathbb{P}[\\mathcal{G}_{s}]\\le2\\exp\\left(-\\frac{\\delta_{s}^{2}R_{s}m}{27}\\right)$. This bound is crucial for understanding the concentration properties of the budget estimates in the NMBR mechanism.", "question_context_used": "The NMBR mechanism involves analyzing the sample-average budget $\tilde{B}_{s}$ and its deviation from the expected value $B_{s}$. The event $\\mathcal{G}_{s}$ is defined as $\\{\\left|\\tilde{B}_{s}-B_{s}\\right|\\geq T\\overline{{c}}_{s}\\delta_{s}/2\\}$. The probability bound for this event is derived using standard Chernoff bounds, resulting in $\\mathbb{P}[\\mathcal{G}_{s}]\\le2\\exp\\left(-\\frac{\\delta_{s}^{2}R_{s}m}{27}\\right)$. This bound is crucial for understanding the concentration properties of the budget estimates in the NMBR mechanism.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-567", "question": "In the context of jump-diffusion processes with double exponential jumps, what is the primary mathematical challenge addressed by Cai, Chen, and Wan (2010) in their work on occupation times and option pricing?", "options": ["A) Deriving closed-form solutions for the characteristic function of occupation times", "B) Estimating the volatility parameter in the presence of jumps", "C) Calibrating the risk-neutral measure for double exponential jumps", "D) Solving the partial integro-differential equation for option prices"], "gold_answer": "A", "source_files": ["Occupation Times of Jump-Diffusion Processes with.md"], "question_context": "The article by Cai, Chen, and Wan (2010) focuses on the occupation times of jump-diffusion processes with double exponential jumps and their application to option pricing. The authors address the mathematical challenge of deriving closed-form solutions for the characteristic function of occupation times, which is crucial for pricing path-dependent options. Their work extends the existing theory by incorporating double exponential jumps, which are known for their ability to capture the leptokurtic features of asset returns. The mathematical framework involves solving complex integro-differential equations, with the characteristic function playing a central role in the analysis. The results have significant implications for the pricing of exotic options and other financial derivatives.", "question_context_used": "The article by Cai, Chen, and Wan (2010) focuses on the occupation times of jump-diffusion processes with double exponential jumps and their application to option pricing. The authors address the mathematical challenge of deriving closed-form solutions for the characteristic function of occupation times, which is crucial for pricing path-dependent options. Their work extends the existing theory by incorporating double exponential jumps, which are known for their ability to capture the leptokurtic features of asset returns. The mathematical framework involves solving complex integro-differential equations, with the characteristic function playing a central role in the analysis. The results have significant implications for the pricing of exotic options and other financial derivatives.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-157", "question": "In the context of the sequential pricing problem with patience constraints, what is the primary role of the attenuation function $a(e)$ in Algorithm 2.1?", "options": ["A) To ensure that edges with less contention are penalized more heavily", "B) To guarantee that all edges are matched with equal probability", "C) To maximize the number of edges probed regardless of their contention", "D) To enforce a strict upper bound on the number of edges that can be matched"], "gold_answer": "A", "source_files": ["Improved Online Contention Resolution for Matching.md"], "question_context": "The attenuation function $a(e)$ plays a crucial role in Algorithm 2.1 by adjusting the probability with which an edge $e$ is matched based on the level of contention for its endpoints. The key observation is that edges with less contention can be attenuated more while still improving upon previous bounds. This approach allows the algorithm to capitalize on scenarios where either the edge itself or its incident edges have high contention, thereby breaking the previous state-of-the-art balance ratio of $1/2(1-\\mathsf{e}^{2})$. The function $a(e)$ is designed to penalize edges with less contention, ensuring a more efficient matching process.", "question_context_used": "The attenuation function $a(e)$ plays a crucial role in Algorithm 2.1 by adjusting the probability with which an edge $e$ is matched based on the level of contention for its endpoints. The key observation is that edges with less contention can be attenuated more while still improving upon previous bounds. This approach allows the algorithm to capitalize on scenarios where either the edge itself or its incident edges have high contention, thereby breaking the previous state-of-the-art balance ratio of $1/2(1-\\mathsf{e}^{2})$. The function $a(e)$ is designed to penalize edges with less contention, ensuring a more efficient matching process.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-589", "question": "According to the text, what is the most effective way for decision scientists to apply their expertise to management decisions?", "options": ["A) Develop complex econometric models for business simulation", "B) Align their mindset and objectives with those of management", "C) Focus on publishing papers for professional meetings", "D) Avoid direct interaction with line management to maintain objectivity"], "gold_answer": "B", "source_files": ["Fantasies, Fundamentals, and a Framework in Corpor.md"], "question_context": "The text emphasizes the importance of decision scientists adopting a management perspective to effectively contribute to organizational decisions. It states, 'The way to apply decision science to management decisions is to become management. The successful practitioner must think like management, talk like management, and have the same concerns as management.' This alignment fosters mutual respect and practical problem-solving, as opposed to focusing solely on academic or theoretical pursuits. The text further elaborates on the necessity of developing rapport with management and accepting responsibility for outcomes, highlighting the transition from being perceived as 'scientists' to being integral parts of the management team.", "question_context_used": "The text emphasizes the importance of decision scientists adopting a management perspective to effectively contribute to organizational decisions. It states, 'The way to apply decision science to management decisions is to become management. The successful practitioner must think like management, talk like management, and have the same concerns as management.' This alignment fosters mutual respect and practical problem-solving, as opposed to focusing solely on academic or theoretical pursuits. The text further elaborates on the necessity of developing rapport with management and accepting responsibility for outcomes, highlighting the transition from being perceived as 'scientists' to being integral parts of the management team.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-803", "question": "Under the assumptions of the STEM model, which property does the Jacobian matrix of the inverse demand function satisfy?", "options": ["A) Symmetric and positive definite", "B) Symmetric and negative definite", "C) Asymmetric and positive definite", "D) Asymmetric and negative definite"], "gold_answer": "B", "source_files": ["A Combined Trip Generation, Trip Distribution, Mod.md"], "question_context": "The text establishes several properties of the demand model, including the invertibility of the demand function and the properties of its Jacobian matrix. Specifically, Proposition 2.2 states that the Jacobian matrix of the inverse demand function is symmetric, and Proposition 2.3 states that it is negative definite under certain conditions. The demand function components are given by:\n\n$$ T_{\\upsilon}(u^{\\iota})=(\\alpha S_{\\iota}+E_{\\iota})\\frac{\\exp(-\\theta u_{\\iota\\j}+A_{\\j})}{\\sum_{k\\in{\\cal D}_{\\iota}}\\exp(-\\theta u_{\\iota k}+A_{k})} $$\n\nand the components of the inverse demand function are:\n\n$$ u_{i j}(t^{i}) =\\frac{1}{\\theta}\\Bigg[A_{j}-\\ln t_{i j}+\\ln\\sum_{k\\in D_{i}}t_{i k} -\\frac{1}{\\alpha}\\sum_{k\\in D_{i}}\\left(t_{i k}-E_{i}\\right)\\Bigg]. $$", "question_context_used": "The text establishes several properties of the demand model, including the invertibility of the demand function and the properties of its Jacobian matrix. Specifically, Proposition 2.2 states that the Jacobian matrix of the inverse demand function is symmetric, and Proposition 2.3 states that it is negative definite under certain conditions. The demand function components are given by:\n\n$$ T_{\\upsilon}(u^{\\iota})=(\\alpha S_{\\iota}+E_{\\iota})\\frac{\\exp(-\\theta u_{\\iota\\j}+A_{\\j})}{\\sum_{k\\in{\\cal D}_{\\iota}}\\exp(-\\theta u_{\\iota k}+A_{k})} $$\n\nand the components of the inverse demand function are:\n\n$$ u_{i j}(t^{i}) =\\frac{1}{\\theta}\\Bigg[A_{j}-\\ln t_{i j}+\\ln\\sum_{k\\in D_{i}}t_{i k} -\\frac{1}{\\alpha}\\sum_{k\\in D_{i}}\\left(t_{i k}-E_{i}\\right)\\Bigg]. $$", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3681", "question": "In the context of geometric programming for sales maximization, which condition must the sales function satisfy to ensure global optimality?", "options": ["A) The sales function must have all positive regression coefficients.", "B) The sales function must have at most one term with a positive linear coefficient and all other terms with zero or negative coefficients.", "C) The sales function must be a signomial with both positive and negative terms.", "D) The sales function must have a constant term with a negative coefficient."], "gold_answer": "B", "source_files": ["Solving the “Marketing Mix” Problem using Geometri.md"], "question_context": "The estimated functional $\\pmb{Q}$ and the constraints under which an optimization by G-P. must be posynomials for a maximization problem. However, since we are maximizing $\\pmb{Q}$ in the context of sales, this requires the functional to carry negative coefficients in each term without constraining the constant term. To satisfy the G-P. requirements, the sales function $\\alpha$ to be maximized in terms of the regressors must satisfy the following requirements: a) a constant term (positive or negative); b) at most, one positive regression coefficient; c) all other terms have either zero or negative regression coefficients. (Note: The exponents of the terms can be positive or negative.) Since the basic four-step procedure used in the beer sales example satisfied the requirements of case (B), we are ensured of a regression function of beer sales with, at most, one term having a positive linear coefficient. This will enable us to guarantee global optimality as we have avoided signomials or approximate solutions.", "question_context_used": "The estimated functional $\\pmb{Q}$ and the constraints under which an optimization by G-P. must be posynomials for a maximization problem. However, since we are maximizing $\\pmb{Q}$ in the context of sales, this requires the functional to carry negative coefficients in each term without constraining the constant term. To satisfy the G-P. requirements, the sales function $\\alpha$ to be maximized in terms of the regressors must satisfy the following requirements: a) a constant term (positive or negative); b) at most, one positive regression coefficient; c) all other terms have either zero or negative regression coefficients. (Note: The exponents of the terms can be positive or negative.) Since the basic four-step procedure used in the beer sales example satisfied the requirements of case (B), we are ensured of a regression function of beer sales with, at most, one term having a positive linear coefficient. This will enable us to guarantee global optimality as we have avoided signomials or approximate solutions.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3562", "question": "In the context of the medical student scheduling problem at Ghent University, which of the following is identified as a primary challenge in developing an automated scheduling module?", "options": ["A) Ensuring the algorithm can handle synthetic instances for theoretical validation", "B) Integrating the module into the existing administrative system with technical constraints", "C) Maximizing the number of elective disciplines assigned to students", "D) Reducing the computational time for small-scale problem instances"], "gold_answer": "B", "source_files": ["The Scheduling of Medical Students at Ghent Univer.md"], "question_context": "The internship scheduling problem at Ghent University is complex due to conflicting requirements from multiple stakeholders, including educational requirements, hospital capacity limits, and student preferences. The development of an automated scheduling module must address several challenges, including the need for seamless integration into the university's existing administrative system, Sparta, which operates under specific technical constraints related to CPU and RAM. The module must also produce high-quality solutions within a limited timeframe to accommodate the dynamic nature of input parameters and stakeholder feedback.", "question_context_used": "The internship scheduling problem at Ghent University is complex due to conflicting requirements from multiple stakeholders, including educational requirements, hospital capacity limits, and student preferences. The development of an automated scheduling module must address several challenges, including the need for seamless integration into the university's existing administrative system, Sparta, which operates under specific technical constraints related to CPU and RAM. The module must also produce high-quality solutions within a limited timeframe to accommodate the dynamic nature of input parameters and stakeholder feedback.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-2549", "question": "According to Harrison and Kreps [1978], why does a rational investor's reservation price for an asset exceed the present value of the uncertain income stream from holding it indefinitely?", "options": ["A) Due to the investor's risk aversion and preference for immediate liquidity", "B) Because of the possibility that future buyers may have more optimistic expectations about the asset's value", "C) To account for the potential depreciation of the asset over time", "D) As a result of the investor's need to hedge against inflation"], "gold_answer": "B", "source_files": ["Uncertainty and the Model Builder.md"], "question_context": "Harrison and Kreps [1978] noted that in a world with heterogeneous expectations, a rational investor will have a reservation price for an asset in excess of 'what it's worth to him to keep it indefinitely' because of the finite probability that someone will offer more than that amount. This reflects the investor's awareness of others who may be more sanguine about the asset's future, either now or later.", "question_context_used": "Harrison and Kreps [1978] noted that in a world with heterogeneous expectations, a rational investor will have a reservation price for an asset in excess of 'what it's worth to him to keep it indefinitely' because of the finite probability that someone will offer more than that amount. This reflects the investor's awareness of others who may be more sanguine about the asset's future, either now or later.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3164", "question": "In the context of forecast aggregation in partial evidence environments, what is the crucial property of the evidence matrix A that determines whether optimal learning is possible for an ignorant aggregator?", "options": ["A) The existence of a balancing vector of the evidence matrix A", "B) The number of experts providing forecasts", "C) The conditional independence of the signals", "D) The prior awareness of the aggregator"], "gold_answer": "A", "source_files": ["Learning Optimal Forecast Aggregation in Partial E.md"], "question_context": "The paper addresses the question of whether a non-Bayesian aggregator can learn to aggregate forecasts of Bayesian experts optimally in a situation where the aggregator is ignorant of the information (i.e., signals) that led each expert to his subjective forecast. We show that a crucial property that determines whether learning is possible or impossible for the ignorant aggregator is the existence of a balancing vector of the evidence matrix A; that is, the existence of a linear combination of A’s rows that is equal to (1,…,1). Informally speaking, if such a balancing vector exists, the aggregator should linearly aggregate the log-likelihoods of expert’s forecasts according to the weights of such a balancing vector. Conversely, if such a balancing vector does not exist, aggregation is impossible even in the simplest constant environment.", "question_context_used": "The paper addresses the question of whether a non-Bayesian aggregator can learn to aggregate forecasts of Bayesian experts optimally in a situation where the aggregator is ignorant of the information (i.e., signals) that led each expert to his subjective forecast. We show that a crucial property that determines whether learning is possible or impossible for the ignorant aggregator is the existence of a balancing vector of the evidence matrix A; that is, the existence of a linear combination of A’s rows that is equal to (1,…,1). Informally speaking, if such a balancing vector exists, the aggregator should linearly aggregate the log-likelihoods of expert’s forecasts according to the weights of such a balancing vector. Conversely, if such a balancing vector does not exist, aggregation is impossible even in the simplest constant environment.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-2438", "question": "In the context of the PCG method applied to problem (16), which of the following statements is correct regarding the equivalence to the CG method?", "options": ["A) The PCG method is equivalent to the CG method applied to the original problem without any modifications.", "B) The PCG method is equivalent to the CG method applied to the problem min $q(x_{J_{0}^{\\mathrm{c}}},0)$, where $J_{0}^{\\mathrm{c}}$ is the complement of $J_{0}$ in $\\{1,\\ldots,n\\}$.", "C) The PCG method is equivalent to the CG method only when $p^{k}\\neq0$ and $(d^{k})^{T}A d^{k}=0$.", "D) The PCG method is equivalent to the CG method when the objective function is bounded below."], "gold_answer": "B", "source_files": ["Generalized Conjugate Gradient Methods forℓ1Regula.md"], "question_context": "The PCG method applied to problem (16) is equivalent to the CG method applied to the problem min $q(x_{J_{0}^{\\mathrm{c}}},0)$, where $J_{0}^{\\mathrm{c}}$ is the complement of $J_{0}$ in $\\{1,\\ldots,n\\}$. This equivalence is crucial for understanding the convergence properties of the PCG method in constrained optimization problems.", "question_context_used": "The PCG method applied to problem (16) is equivalent to the CG method applied to the problem min $q(x_{J_{0}^{\\mathrm{c}}},0)$, where $J_{0}^{\\mathrm{c}}$ is the complement of $J_{0}$ in $\\{1,\\ldots,n\\}$. This equivalence is crucial for understanding the convergence properties of the PCG method in constrained optimization problems.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-2287", "question": "In the context of the SAFER system implemented at the port of Singapore, which of the following is a direct benefit of the machine learning algorithm's error detection module?", "options": ["A) Reduction in illegal bunkering activities", "B) Correction of infeasible vessel behaviors", "C) Improvement in billing system accuracy", "D) Enhanced detection of infringement policies"], "gold_answer": "B", "source_files": ["Introduction—2018 Caterpillar and INFORMS Analytic.md"], "question_context": "The port of Singapore, one the busiest in the world, serves more than 1,000 vessels every day. To ensure a smooth, safe, secure, and sustainable operation, a state-of-the-art analytics-based system was developed. The system detects vessel behavior that differs from expectations, allowing quick intervention by port authorities. At the core of the system is a machine learning algorithm that predicts vessel direction and speed. The algorithm includes an error detection module that corrects inaccuracies, such as infeasible behaviors, in the system. SAFER’s benefits go beyond controlling bad behavior; the application also improves billing systems, reduces and helps to settle disputes, improves vessel arrival predictions and consequently port infrastructure utilization, allows better detection of infringement policies, and helps to detect illegal bunkering (i.e., ship-to-ship fuel transfer).", "question_context_used": "The port of Singapore, one the busiest in the world, serves more than 1,000 vessels every day. To ensure a smooth, safe, secure, and sustainable operation, a state-of-the-art analytics-based system was developed. The system detects vessel behavior that differs from expectations, allowing quick intervention by port authorities. At the core of the system is a machine learning algorithm that predicts vessel direction and speed. The algorithm includes an error detection module that corrects inaccuracies, such as infeasible behaviors, in the system. SAFER’s benefits go beyond controlling bad behavior; the application also improves billing systems, reduces and helps to settle disputes, improves vessel arrival predictions and consequently port infrastructure utilization, allows better detection of infringement policies, and helps to detect illegal bunkering (i.e., ship-to-ship fuel transfer).", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-2785", "question": "According to the critique of Lodish's model, under what conditions would Examples 1 and 2 NOT produce identical sales responses despite identical exposure probabilities $v_{s}(t_{j})'$?", "options": ["A) If the insertions attract the same audience with each successive insertion", "B) If the insertions correspond to identical advertising options and are scheduled at the same times", "C) If the insertions attract different audiences due to differing advertising options or insertion times", "D) If the model neglects to consider diminishing returns at the individual level"], "gold_answer": "C", "source_files": ["Examining the Pattern of Response Behavior in a Me.md"], "question_context": "The text critiques Lodish's assertion that Examples 1 and 2 would yield identical sales responses if the $v_{s}(t_{j})'$s are identical. It argues that differences in audience attraction due to varying advertising options (appeal or copy differences) or insertion times would lead to non-identical sales responses. Specifically, differences in response parameters $\\pmb{\\alpha}_{j}$ and $\\pmb{b_{j}}$ or decay effects from timing discrepancies would cause these dissimilarities. The critique emphasizes that even with identical exposure probabilities, the sales responses would differ if the insertions attract widely different audiences.", "question_context_used": "The text critiques Lodish's assertion that Examples 1 and 2 would yield identical sales responses if the $v_{s}(t_{j})'$s are identical. It argues that differences in audience attraction due to varying advertising options (appeal or copy differences) or insertion times would lead to non-identical sales responses. Specifically, differences in response parameters $\\pmb{\\alpha}_{j}$ and $\\pmb{b_{j}}$ or decay effects from timing discrepancies would cause these dissimilarities. The critique emphasizes that even with identical exposure probabilities, the sales responses would differ if the insertions attract widely different audiences.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-2975", "question": "What is the primary purpose of INFORMS as described in the provided context?", "options": ["A) To publish academic journals on operations research", "B) To provide networking and learning opportunities for O.R. and analytics professionals", "C) To organize international conferences exclusively for students", "D) To develop automated transportation systems"], "gold_answer": "B", "source_files": ["Subject Index—Articles and Letters.md"], "question_context": "With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes.", "question_context_used": "With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-4832", "question": "In the study of scheduling a Flexible Manufacturing System (FMS) with tooling constraints, what was the primary reason the goal of generating slack production time for additional parts was unachievable?", "options": ["A) The FMS was operating near or over capacity when tooling flow was considered", "B) The heuristic rules used were too simplistic to handle the complexity", "C) The AGVs were insufficient in number to handle material and tool transport", "D) The processing plans for part types were not optimized properly"], "gold_answer": "A", "source_files": ["Scheduling a Flexible Manufacturing System with To.md"], "question_context": "Material handling and the provision of supporting resources, particularly tools, in flexible manufacturing systems (FMSs) can constrain production. Detailed simulations of the FMS showed that a simple set of heuristic rules could produce several alternative schedules of nearly optimum performance. They also showed that when the flow of tooling was considered the FMS was operating near or over capacity. The goal of the study—to generate slack production time for the manufacture of additional parts—was unachievable. Clearly companies must consider the constraints arising from material handling and the flow of supporting resources in scheduling, yet these constraints have seldom been considered in simulations of FMSs.", "question_context_used": "Material handling and the provision of supporting resources, particularly tools, in flexible manufacturing systems (FMSs) can constrain production. Detailed simulations of the FMS showed that a simple set of heuristic rules could produce several alternative schedules of nearly optimum performance. They also showed that when the flow of tooling was considered the FMS was operating near or over capacity. The goal of the study—to generate slack production time for the manufacture of additional parts—was unachievable. Clearly companies must consider the constraints arising from material handling and the flow of supporting resources in scheduling, yet these constraints have seldom been considered in simulations of FMSs.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-4708", "question": "In the context of Generalized Linear Contextual Bandits (GLCBs) with delayed rewards, which condition ensures that there is enough diversity in the sample contexts?", "options": ["A) The minimum singular value of the expected feature covariance matrix is bounded below by a positive constant.", "B) The link function g is twice differentiable with bounded derivatives.", "C) The sub-Gaussian parameter of the noise terms is strictly positive.", "D) The delays in observing rewards are independent and identically distributed."], "gold_answer": "A", "source_files": ["Delay-Adaptive Learning in Generalized Linear Cont.md"], "question_context": "The GLCB satisfies the following regularity conditions: \n\n1. $\\lambda_{\\operatorname*{min}}\\biggl(\\mathbb{E}\\left[\\frac{1}{K}\\sum_{a\\in[K]}x_{t,a}x_{t,a}^{\\prime}\\right]\\biggr)\\geq\\sigma_{0}^{2}$ for all $t\\in[T],$ ; \n2. $\\kappa:=\\operatorname*{inf}_{\\{\\|x\\|\\leq1,\\|\\theta-\\theta*\\|\\leq1\\}}\\dot{g}(x^{\\prime}\\theta)>0;$ \n3. $g$ is twice differentiable, and $\\dot{g}$ and $\\ddot{g}$ are upper bounded by $L_{g}$ and $M_{g\\prime}$ respectively. \n\nThe first condition ensures that there is enough diversity in the sample contexts. All three conditions are satisfied by linear contextual bandits and logistics contextual bandits, two notable special cases of GLCBs.", "question_context_used": "The GLCB satisfies the following regularity conditions: \n\n1. $\\lambda_{\\operatorname*{min}}\\biggl(\\mathbb{E}\\left[\\frac{1}{K}\\sum_{a\\in[K]}x_{t,a}x_{t,a}^{\\prime}\\right]\\biggr)\\geq\\sigma_{0}^{2}$ for all $t\\in[T],$ ; \n2. $\\kappa:=\\operatorname*{inf}_{\\{\\|x\\|\\leq1,\\|\\theta-\\theta*\\|\\leq1\\}}\\dot{g}(x^{\\prime}\\theta)>0;$ \n3. $g$ is twice differentiable, and $\\dot{g}$ and $\\ddot{g}$ are upper bounded by $L_{g}$ and $M_{g\\prime}$ respectively. \n\nThe first condition ensures that there is enough diversity in the sample contexts. All three conditions are satisfied by linear contextual bandits and logistics contextual bandits, two notable special cases of GLCBs.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-988", "question": "In the context of games on a random environment as described by Conforti et al., what is the necessary condition for a probability measure ν* to be a Nash equilibrium when considering the first-order optimality condition?", "options": ["A) The gradient of the linear functional derivative with respect to x must be zero for all x and m-almost every y.", "B) The potential function must be linear in the players' strategies.", "C) The relative entropy term must dominate the potential function.", "D) The marginal law of ν* on the environment Y must be uniform."], "gold_answer": "A", "source_files": ["Game on Random Environment, Mean-Field Langevin Sy.md"], "question_context": "The paper studies games regularized by relative entropy where players' strategies are coupled through a random environment. A key result is the first-order optimality condition for a Nash equilibrium, which states that if ν* minimizes the regularized objective function, then ∇ₓ(δF/δν)(ν*,x,y) + (σ²/2)∇ₓlnν*(x|y) = 0 for all x and m-almost every y. This condition links the Nash equilibrium to the invariant measure of a mean-field Langevin system. The framework generalizes classical game theory by allowing nonlinear potential functions and coupling through the environment, with applications in neural networks and dynamic games.", "question_context_used": "The paper studies games regularized by relative entropy where players' strategies are coupled through a random environment. A key result is the first-order optimality condition for a Nash equilibrium, which states that if ν* minimizes the regularized objective function, then ∇ₓ(δF/δν)(ν*,x,y) + (σ²/2)∇ₓlnν*(x|y) = 0 for all x and m-almost every y. This condition links the Nash equilibrium to the invariant measure of a mean-field Langevin system. The framework generalizes classical game theory by allowing nonlinear potential functions and coupling through the environment, with applications in neural networks and dynamic games.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-1882", "question": "According to the analysis of the F-35 pilot transition program, what is the primary effect of increasing the transition phase by one year over the base case?", "options": ["A) It reduces the total number of F-35 pilot years by eight.", "B) It increases the ratio of converted to new F-35 pilots.", "C) It decreases the PTC's capacity by 11 percent.", "D) It delays full operational capability by one year."], "gold_answer": "A", "source_files": ["From F-16 to F-35_ Optimizing the Training of Pilo.md"], "question_context": "In the base case, we set the MLOR in the transition phase to Level 3. By adding one year to the transition phase, compared to the base case, we found that we needed eight fewer F-35 pilot years during the transition phase. Adding one additional year to the transition phase did not affect the number of F-35 pilot years (i.e., we still needed eight fewer F-35 pilot years during the planning horizon). The total number of F35 pilot years decreases when the transition phase is longer because waiting longer before training pilots is possible. We achieve this result when we increase the transition phase by one year; however, increasing the length of the transition phase by two or more years provides no additional benefit.", "question_context_used": "In the base case, we set the MLOR in the transition phase to Level 3. By adding one year to the transition phase, compared to the base case, we found that we needed eight fewer F-35 pilot years during the transition phase. Adding one additional year to the transition phase did not affect the number of F-35 pilot years (i.e., we still needed eight fewer F-35 pilot years during the planning horizon). The total number of F35 pilot years decreases when the transition phase is longer because waiting longer before training pilots is possible. We achieve this result when we increase the transition phase by one year; however, increasing the length of the transition phase by two or more years provides no additional benefit.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-3299", "question": "In the context of quadratic programming as discussed by B. Curtis Eaves, which condition ensures that a quadratic program has an odd number of stationary points?", "options": ["A) The program has an optimum and satisfies a nondegeneracy condition", "B) The matrix R is positive semidefinite", "C) The objective function is bounded below on a polyhedral convex set", "D) The Kuhn-Tucker conditions are satisfied with linearly independent columns"], "gold_answer": "A", "source_files": ["On Quadratic Programming.md"], "question_context": "B. Curtis Eaves discusses quadratic programming and the conditions under which a quadratic program has an odd number of stationary points. Specifically, it is demonstrated that if a general quadratic program has an optimum and satisfies a nondegeneracy condition, then there are an odd number of stationary points. The paper also explores the Kuhn-Tucker conditions and the linear complementarity problem in the context of quadratic programming. The quadratic program is defined as minimizing $\\Theta(w)=w^{\\tau}R w+w^{\\tau}c$ subject to $A w\\leq b$ and $w\\geq0$. The notion of reduced and nondegenerate stationary points is introduced, where a point is nondegenerate if it has a Kuhn-Tucker solution using $n+m$ linearly independent columns.", "question_context_used": "B. Curtis Eaves discusses quadratic programming and the conditions under which a quadratic program has an odd number of stationary points. Specifically, it is demonstrated that if a general quadratic program has an optimum and satisfies a nondegeneracy condition, then there are an odd number of stationary points. The paper also explores the Kuhn-Tucker conditions and the linear complementarity problem in the context of quadratic programming. The quadratic program is defined as minimizing $\\Theta(w)=w^{\\tau}R w+w^{\\tau}c$ subject to $A w\\leq b$ and $w\\geq0$. The notion of reduced and nondegenerate stationary points is introduced, where a point is nondegenerate if it has a Kuhn-Tucker solution using $n+m$ linearly independent columns.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-1222", "question": "In the context of American Trim's production optimization, what was the primary trade-off considered when increasing batch sizes for commonly ordered parts?", "options": ["A) Reduced overtime costs vs. increased product damage risk", "B) Reduced setup costs vs. increased inventory holding costs", "C) Improved customer delivery service vs. increased managerial oversight costs", "D) Reduced obsolescence risk vs. increased storage space requirements"], "gold_answer": "B", "source_files": ["ASP, The Art and Science of Practice_ Tales from t.md"], "question_context": "American Trim (AT) faced regular break-in orders, leading to changeovers and setups on its production lines, particularly at the bottleneck 'A' line decorating step. Using optimization, AT evaluated trade-offs between overtime and product-holding costs. By combining small, frequent orders into larger batches for commonly ordered parts, AT reduced setups but incurred inventory costs. This intentional desynchronization minimized obsolescence risk for high-volume parts while increasing bottleneck throughput and reducing overtime. The policy also improved customer delivery service and reduced break-in orders, which disrupted scheduling.", "question_context_used": "American Trim (AT) faced regular break-in orders, leading to changeovers and setups on its production lines, particularly at the bottleneck 'A' line decorating step. Using optimization, AT evaluated trade-offs between overtime and product-holding costs. By combining small, frequent orders into larger batches for commonly ordered parts, AT reduced setups but incurred inventory costs. This intentional desynchronization minimized obsolescence risk for high-volume parts while increasing bottleneck throughput and reducing overtime. The policy also improved customer delivery service and reduced break-in orders, which disrupted scheduling.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-480", "question": "According to the paper, what is a primary differentiating factor between Operations Research (OR) and mathematics?", "options": ["A) OR focuses exclusively on theoretical mathematical formulations without real-world applications.", "B) OR incorporates a 'science' orientation, emphasizing validation and adaptation to real-world systems.", "C) OR avoids the use of probabilistic models and dynamic programming.", "D) OR prioritizes abstract mathematical problems over industry-specific solutions."], "gold_answer": "B", "source_files": ["The Role of Operations Research in Systems Analysi.md"], "question_context": "Operations Research can be viewed as either an extension of mathematics to the study of certain types of systems or as a 'science' attempting to explain the behavior of various systems of interest... What differentiates an Operations Researcher from a mathematician is the 'science’ orientation of Operations Research. This science orientation implies that one examines existing systems and determines techniques for analyzing them... Greater emphasis on this validation process, as well as greater interest in 'solutions' to systems that do not conform to the assumptions in solved problems, would lead to generating information needed to adapt the existing solutions to the commercial environment.", "question_context_used": "Operations Research can be viewed as either an extension of mathematics to the study of certain types of systems or as a 'science' attempting to explain the behavior of various systems of interest... What differentiates an Operations Researcher from a mathematician is the 'science’ orientation of Operations Research. This science orientation implies that one examines existing systems and determines techniques for analyzing them... Greater emphasis on this validation process, as well as greater interest in 'solutions' to systems that do not conform to the assumptions in solved problems, would lead to generating information needed to adapt the existing solutions to the commercial environment.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3798", "question": "According to the robust FAM model results, which station purity scenario provides the best trade-off between computational efficiency and FAM profit loss?", "options": ["A) Adding family purity", "B) Adding type purity", "C) Adding crew base purity", "D) No fleet purity"], "gold_answer": "C", "source_files": ["Integrated Airline Fleet and Crew Robust Planning.md"], "question_context": "The text discusses the impact of different station purity scenarios on the robust FAM model. It is shown that adding crew base purity can reduce the problem size and CPU time significantly (by 10∼100 times) while only decreasing FAM profit by 0.6% on average. In contrast, family purity and type purity lead to higher computational times (300x and 900x increases, respectively) and greater profit losses (2.9% and 3.6%). The most notable benefits of crew base purity are improved computational efficiency and minimal profit loss, making it the optimal trade-off.", "question_context_used": "The text discusses the impact of different station purity scenarios on the robust FAM model. It is shown that adding crew base purity can reduce the problem size and CPU time significantly (by 10∼100 times) while only decreasing FAM profit by 0.6% on average. In contrast, family purity and type purity lead to higher computational times (300x and 900x increases, respectively) and greater profit losses (2.9% and 3.6%). The most notable benefits of crew base purity are improved computational efficiency and minimal profit loss, making it the optimal trade-off.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-1818", "question": "In the multiperiod linear programming model for academic planning described by Walters et al., what is the primary advantage of using goal programming over traditional linear programming?", "options": ["A) It allows for the incorporation of multiple conflicting goals and their trade-offs.", "B) It simplifies the computational complexity of the model.", "C) It eliminates the need for post-optimality analysis.", "D) It reduces the number of decision variables required."], "gold_answer": "A", "source_files": ["A Comprehensive Planning Model for Long-Range Acad.md"], "question_context": "The model described is a multiperiod linear programming model designed to aid the administrator of a school in planning and decision-making for a five-year horizon. To simulate this activity with an analytical model, we have employed the technique of 'goal programming' (see Charnes and Cooper [7]). This technique is particularly suited to this model because it allows one to make evaluations of marginal tradeoffs between possible courses of action and the opportunity costs of the various goals which are considered as constraints. Thus the use of dual evaluators, along with a detailed post optimality analysis of the model, enables an administrator to evaluate the 'cost' of various alternatives which are available to him. The knowledge of the cost of such tradeoffs is, of course, vital in determining the ultimate optimal course of action which should be taken by the decision-maker.", "question_context_used": "The model described is a multiperiod linear programming model designed to aid the administrator of a school in planning and decision-making for a five-year horizon. To simulate this activity with an analytical model, we have employed the technique of 'goal programming' (see Charnes and Cooper [7]). This technique is particularly suited to this model because it allows one to make evaluations of marginal tradeoffs between possible courses of action and the opportunity costs of the various goals which are considered as constraints. Thus the use of dual evaluators, along with a detailed post optimality analysis of the model, enables an administrator to evaluate the 'cost' of various alternatives which are available to him. The knowledge of the cost of such tradeoffs is, of course, vital in determining the ultimate optimal course of action which should be taken by the decision-maker.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-1992", "question": "In the context of hierarchical classification for Product Service Codes, which approach demonstrated superior performance when explicitly modeling the hierarchical structure through top-down strategies?", "options": ["A) Bottom-up approach with Bayesian classifiers", "B) Top-down approach with character-level CNNs", "C) Big-bang approach with recurrent neural networks", "D) Local classifier per level with word-level Naive Bayes"], "gold_answer": "B", "source_files": ["Using Machine Learning to Improve Public Reporting.md"], "question_context": "The paper explores machine learning approaches for classifying U.S. government contracts into a hierarchical Product Service Code taxonomy. A key finding is that the top-down classification approach, which directly models the hierarchical structure of the taxonomy, offers superior performance to bottom-up classification. Specifically, the character-level convolutional neural network (CNN) achieved an accuracy of 0.485 at the leaf node level under the top-down approach, compared to 0.467 with the bottom-up approach. The formula for calculating leaf node probability in the top-down approach is given by: $$\\operatorname{Prob}(c)=\\operatorname{Prob}(c\\mid g)\\times\\operatorname{Prob}(g),$$ where $\\mathrm{Prob}(g)$ is estimated by the root node classifier. This demonstrates the effectiveness of explicitly modeling hierarchical relationships in complex classification tasks.", "question_context_used": "The paper explores machine learning approaches for classifying U.S. government contracts into a hierarchical Product Service Code taxonomy. A key finding is that the top-down classification approach, which directly models the hierarchical structure of the taxonomy, offers superior performance to bottom-up classification. Specifically, the character-level convolutional neural network (CNN) achieved an accuracy of 0.485 at the leaf node level under the top-down approach, compared to 0.467 with the bottom-up approach. The formula for calculating leaf node probability in the top-down approach is given by: $$\\operatorname{Prob}(c)=\\operatorname{Prob}(c\\mid g)\\times\\operatorname{Prob}(g),$$ where $\\mathrm{Prob}(g)$ is estimated by the root node classifier. This demonstrates the effectiveness of explicitly modeling hierarchical relationships in complex classification tasks.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-858", "question": "In the context of the mixed-integer programming model developed for Elkem, which of the following constraints is NOT explicitly mentioned as part of the model?", "options": ["A) Constraints on the initial operating status of existing and potential plants", "B) Constraints ensuring that sales do not exceed production or demand", "C) Constraints on the maximum number of employees per plant", "D) Constraints governing the operation and conversion of furnaces"], "gold_answer": "C", "source_files": ["Elkem Uses Optimization in Redesigning Its Supply.md"], "question_context": "The mixed-integer programming model incorporates various constraints in several categories. First are those related to facility-level decisions. One constraint set identifies the initial operating status of existing and potential plants. Another states that operating a plant during a time period requires that it be open prior to the beginning of the period or that the firm invest in a new plant at the beginning of the period. Constraints governing the operation and conversion of furnaces specify that a facility must be open for any furnace in it to operate. Additional constraints model the generation of byproducts, such as microsilica, during the production process. Finally, market constraints ensure that sales cannot exceed production or demand and that the plant satisfies contractual agreements with certain customers.", "question_context_used": "The mixed-integer programming model incorporates various constraints in several categories. First are those related to facility-level decisions. One constraint set identifies the initial operating status of existing and potential plants. Another states that operating a plant during a time period requires that it be open prior to the beginning of the period or that the firm invest in a new plant at the beginning of the period. Constraints governing the operation and conversion of furnaces specify that a facility must be open for any furnace in it to operate. Additional constraints model the generation of byproducts, such as microsilica, during the production process. Finally, market constraints ensure that sales cannot exceed production or demand and that the plant satisfies contractual agreements with certain customers.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-3112", "question": "In the context of a two-brand market where Brand 1 is established and Brand 2 enters, what is the optimal cooperative strategy for pricing the two brands to maximize joint sales?", "options": ["A) Price both brands equally at the monopoly optimal price", "B) Set Brand 1's price at μ and Brand 2's price at μ + α", "C) Set Brand 1's price at μ + α/2 and Brand 2's price at μ - α/2", "D) Set Brand 1's price at μ + α and Brand 2's price at μ - α"], "gold_answer": "B", "source_files": ["A Quasi-Game Theory Approach to Pricing.md"], "question_context": "The text discusses a two-brand market scenario where Brand 1 is established and Brand 2 enters. Under cooperative behavior (type-of-behavior combination (3, 3)), the optimal strategy is to price the brands such that they are equidistant from μ + α/2 and separated by a distance α. This results in the optimal prices: p₁* = μ and p₂* = μ + α. This strategy ensures that the brands are noncompetitive and joint sales are maximized. The mathematical derivation involves minimizing the Lagrangian and solving the resulting equations to find these optimal prices.", "question_context_used": "The text discusses a two-brand market scenario where Brand 1 is established and Brand 2 enters. Under cooperative behavior (type-of-behavior combination (3, 3)), the optimal strategy is to price the brands such that they are equidistant from μ + α/2 and separated by a distance α. This results in the optimal prices: p₁* = μ and p₂* = μ + α. This strategy ensures that the brands are noncompetitive and joint sales are maximized. The mathematical derivation involves minimizing the Lagrangian and solving the resulting equations to find these optimal prices.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-1237", "question": "Under the assumptions of Lemma 2, what property ensures that the value functions \\(V^{a}\\) and \\(V^{b}\\) are \\(C\\)-close to \\(x\\)?", "options": ["A) The admissible barrier \\(v\\) is measurable and \\(C\\)-close to \\(x\\)", "B) The functions \\(g^{a/b}/c\\) are \\(C\\)-close to \\(x\\)", "C) The continuous controls \\(p^{a}\\) and \\(p^{b}\\) satisfy the 1-shift property", "D) The Wronskian \\(W\\) is strictly positive and independent of \\(x\\)"], "gold_answer": "B", "source_files": ["Control-Stopping Games for Market Microstructure a.md"], "question_context": "Lemma 2 states that if \\(v\\) is an admissible barrier and \\(p^{a} \\in \\mathcal{A}^{a}\\), \\(p^{b} \\in \\mathcal{A}^{b}\\) are such that \\(g^{a/b}/c\\) are \\(C\\)-close to \\(x\\), then \\(V^{a}(\\cdot,p^{a},p^{b},v^{b})\\) and \\(V^{b}(\\cdot,p^{a},p^{b},v^{a})\\) are \\(C\\)-close to \\(x\\). The \\(C\\)-closeness property is defined as \\(||f(x) - x||_{\\infty} \\leq C\\) for all \\(x \\in \\mathbb{R}\\). The proof relies on the representation of the value functions and the boundedness of \\(g^{a/b}/c - x\\).", "question_context_used": "Lemma 2 states that if \\(v\\) is an admissible barrier and \\(p^{a} \\in \\mathcal{A}^{a}\\), \\(p^{b} \\in \\mathcal{A}^{b}\\) are such that \\(g^{a/b}/c\\) are \\(C\\)-close to \\(x\\), then \\(V^{a}(\\cdot,p^{a},p^{b},v^{b})\\) and \\(V^{b}(\\cdot,p^{a},p^{b},v^{a})\\) are \\(C\\)-close to \\(x\\). The \\(C\\)-closeness property is defined as \\(||f(x) - x||_{\\infty} \\leq C\\) for all \\(x \\in \\mathbb{R}\\). The proof relies on the representation of the value functions and the boundedness of \\(g^{a/b}/c - x\\).", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-768", "question": "Based on the analysis of obsolescence reduction methods in Table 6, which statement best describes the principal advantage of the DEA approach over the Wisconsin method?", "options": ["A) The DEA approach uses a frontier of best practice as a standard for comparison, relating each railroad to a practically attainable performance level.", "B) The DEA approach reduces aggregate obsolescence by a larger margin than the Wisconsin method for all railroads.", "C) The DEA approach exclusively compares railroads to other railroads, ignoring competition from other transportation modes.", "D) The DEA approach does not require any inputs to evaluate performance levels, focusing solely on outcomes achieved."], "gold_answer": "A", "source_files": ["Railroad Property Valuation Using Data Envelopment.md"], "question_context": "The text discusses the shortcomings and advantages of the Wisconsin and DEA methods for measuring obsolescence in railroads. A key advantage of the DEA approach is that it uses a frontier of best practice as a standard for comparison and relates each railroad to a practically attainable performance level. The Wisconsin approach, on the other hand, compares each railroad with a standard that cannot be shown to be attainable in practice. Additionally, the DEA approach allows the analyst to include multiple inputs and to evaluate performance levels considering the required resources as well as outcomes achieved. Table 6 shows the dollar amount (in millions) of obsolescence reduction under alternative methods, highlighting the differences between the Wisconsin and DEA-derived results.", "question_context_used": "The text discusses the shortcomings and advantages of the Wisconsin and DEA methods for measuring obsolescence in railroads. A key advantage of the DEA approach is that it uses a frontier of best practice as a standard for comparison and relates each railroad to a practically attainable performance level. The Wisconsin approach, on the other hand, compares each railroad with a standard that cannot be shown to be attainable in practice. Additionally, the DEA approach allows the analyst to include multiple inputs and to evaluate performance levels considering the required resources as well as outcomes achieved. Table 6 shows the dollar amount (in millions) of obsolescence reduction under alternative methods, highlighting the differences between the Wisconsin and DEA-derived results.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-4125", "question": "What is the time complexity of the exact algorithm for Problem 3 when using Squire's enumeration algorithm for ideals of a poset?", "options": ["A) O(|P|^2 + |P|⋅|D(P)|)", "B) O(log|P|⋅i)", "C) O(i⋅poly(|P|))", "D) O(|P|⋅T_A(i))"], "gold_answer": "B", "source_files": ["On Randomized Approximation for Finding a Level Id.md"], "question_context": "The exact algorithm for Problem 3 runs in time O(i⋅poly(|P|)) and is based on the enumeration of the ideals of a poset. Squire [20] provided a faster enumeration algorithm running in O(log|P|⋅|D(P)|) time. Consequently, the algorithm for Problem 3 runs in time O(log|P|⋅i) when using Squire's method, implying polynomial time complexity for input sizes where i is poly(|P|). The time complexity is derived as O(|P|⋅T_A(i)), where T_A(i) is the computation time for the enumeration algorithm to output up to the ith ideal.", "question_context_used": "The exact algorithm for Problem 3 runs in time O(i⋅poly(|P|)) and is based on the enumeration of the ideals of a poset. Squire [20] provided a faster enumeration algorithm running in O(log|P|⋅|D(P)|) time. Consequently, the algorithm for Problem 3 runs in time O(log|P|⋅i) when using Squire's method, implying polynomial time complexity for input sizes where i is poly(|P|). The time complexity is derived as O(|P|⋅T_A(i)), where T_A(i) is the computation time for the enumeration algorithm to output up to the ith ideal.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-247", "question": "In the context of the finite horizon optimal stopping problem, what is the relationship between the optimal stopping boundary $b_{1}(t)$ and the remaining time $T-t$ when $n=1$?", "options": ["A) $b_{1}(t)$ is decreasing as $T-t$ decreases, indicating a less aggressive strategy.", "B) $b_{1}(t)$ is increasing as $T-t$ decreases, indicating a more aggressive strategy.", "C) $b_{1}(t)$ remains constant regardless of $T-t$, indicating a neutral strategy.", "D) $b_{1}(t)$ oscillates as $T-t$ decreases, indicating an unpredictable strategy."], "gold_answer": "B", "source_files": ["Optimal Liquidation of Child Limit Orders.md"], "question_context": "The finite horizon optimal stopping problem involves determining the optimal time to replace a limit order with a market order based on the short-term momentum indicator $u_t$. The optimal stopping boundary $b_{1}(t)$ is derived for the case $n=1$, where it is shown that $b_{1}(t)$ is an increasing function in $t$. This implies that as the remaining time $T-t$ decreases, the optimal strategy becomes more aggressive, i.e., more likely to replace the limit order with a market order. The mathematical justification is provided by Proposition 5, which establishes the monotonicity of $b_{1}(t)$ and its implications for the continuation region $\\mathcal{C}_{1}$.", "question_context_used": "The finite horizon optimal stopping problem involves determining the optimal time to replace a limit order with a market order based on the short-term momentum indicator $u_t$. The optimal stopping boundary $b_{1}(t)$ is derived for the case $n=1$, where it is shown that $b_{1}(t)$ is an increasing function in $t$. This implies that as the remaining time $T-t$ decreases, the optimal strategy becomes more aggressive, i.e., more likely to replace the limit order with a market order. The mathematical justification is provided by Proposition 5, which establishes the monotonicity of $b_{1}(t)$ and its implications for the continuation region $\\mathcal{C}_{1}$.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-105", "question": "In the Bayesian sequential multi-decision problem described, what is the optimal policy structure for data collection when the process parameters are initially unknown?", "options": ["A) Start with less expensive data collection and switch to more expensive methods as parameters become known", "B) Start with more expensive data collection and switch to less expensive methods as parameters become known", "C) Use the same data collection method throughout all periods", "D) Avoid data collection in initial periods and rely solely on prior distributions"], "gold_answer": "B", "source_files": ["A Bayesian Sequential Multi-Decision Problem.md"], "question_context": "The paper presents a Bayesian framework for a multi-decision problem where data can be collected periodically with various choices available each period. The model involves estimating a data item $\\pmb{X}$ periodically, with potential contamination in reported values $\\pmb{z}$ and $\\pmb{Y}$. A cost is attached to each observation, and a loss is incurred depending on the true data value and the estimate made. The optimal policy structure for the finite horizon calls for observations on $z$ and $\\boldsymbol{x}$ to be made in the initial periods, switching to less expensive forms of data collection as the process parameters become better understood. The loss function is quadratic, and all random variables are normally distributed.", "question_context_used": "The paper presents a Bayesian framework for a multi-decision problem where data can be collected periodically with various choices available each period. The model involves estimating a data item $\\pmb{X}$ periodically, with potential contamination in reported values $\\pmb{z}$ and $\\pmb{Y}$. A cost is attached to each observation, and a loss is incurred depending on the true data value and the estimate made. The optimal policy structure for the finite horizon calls for observations on $z$ and $\\boldsymbol{x}$ to be made in the initial periods, switching to less expensive forms of data collection as the process parameters become better understood. The loss function is quadratic, and all random variables are normally distributed.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-835", "question": "What is the primary purpose of INFORMS as an international association?", "options": ["A) To provide certification programs for operations research professionals", "B) To offer networking and learning opportunities for O.R. and analytics professionals", "C) To develop proprietary analytics software for member organizations", "D) To regulate the standards of operations research globally"], "gold_answer": "B", "source_files": ["Acknowledgment.md"], "question_context": "INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students, with 12,500 members from nearly 90 countries. It provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes.", "question_context_used": "INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students, with 12,500 members from nearly 90 countries. It provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3947", "question": "In the SPRINT system for Hera's customer relations management, what is the primary advantage of the two-phase optimization procedure based on adaptive staffing?", "options": ["A) It reduces the number of customer contact desks (CCDs) required.", "B) It enhances the trade-off between service level efficacy and operational efficiency.", "C) It eliminates the need for back office tasks by front office staff.", "D) It decentralizes the decision-making process to individual desk managers."], "gold_answer": "B", "source_files": ["SPRINT_ Optimization of Staff Management for Desk.md"], "question_context": "The SPRINT system implements a novel two-phase optimization procedure based on adaptive staffing, which proved superior to other state-of-the-art approaches. The challenge desk managers face is finding the best trade-off between the efficacy required to achieve appropriate service levels (e.g., waiting times) and the efficiency required to control operational costs. The system's adaptive staffing technique allows for effective balancing of these competing objectives, leading to significant improvements in service levels and staff productivity.", "question_context_used": "The SPRINT system implements a novel two-phase optimization procedure based on adaptive staffing, which proved superior to other state-of-the-art approaches. The challenge desk managers face is finding the best trade-off between the efficacy required to achieve appropriate service levels (e.g., waiting times) and the efficiency required to control operational costs. The system's adaptive staffing technique allows for effective balancing of these competing objectives, leading to significant improvements in service levels and staff productivity.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-2727", "question": "In McCormick's Variable Reduction Method for Nonlinear Programming, what is the primary condition required for the algorithm to achieve superlinear convergence?", "options": ["A) The Kuhn-Tucker point satisfies the first-order necessary conditions", "B) The Kuhn-Tucker point satisfies the second-order sufficiency conditions", "C) The objective function is twice continuously differentiable", "D) The constraints are strictly linear and non-redundant"], "gold_answer": "B", "source_files": ["The Variable Reduction Method for Nonlinear Progra.md"], "question_context": "The paper presents an algorithm for solving nonlinear programming problems with linear constraints. A key result is that if the Kuhn-Tucker point satisfies the second-order sufficiency conditions, the rate of convergence is superlinear. The algorithm involves sequences of iterations with piece-wise linear segments, and the proof of convergence relies on these conditions. The mathematical formulation includes constraints $A x - b \\geq 0$ and a once continuously differentiable function $f(x)$ to be minimized.", "question_context_used": "The paper presents an algorithm for solving nonlinear programming problems with linear constraints. A key result is that if the Kuhn-Tucker point satisfies the second-order sufficiency conditions, the rate of convergence is superlinear. The algorithm involves sequences of iterations with piece-wise linear segments, and the proof of convergence relies on these conditions. The mathematical formulation includes constraints $A x - b \\geq 0$ and a once continuously differentiable function $f(x)$ to be minimized.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-4713", "question": "In the context of nondegenerate Leontief substitution systems, which property characterizes the dual simplicial complex Σ*(L)?", "options": ["A) It is always a balanced complex with vertices labeled uniformly across facets.", "B) It comprises the independent sets of a matroid.", "C) It forms a complete graph where every pair of vertices is connected by an edge.", "D) It is isomorphic to the collection of faces of any arbitrary polyhedron."], "gold_answer": "B", "source_files": ["Leontief Substitution Systems and Matroid Complexe.md"], "question_context": "A (pre-) Leontief substitution system is a linear system $A x=b,\\ x\\geq0$ where $b$ is a nonnegative vector and $A$ is a matrix containing at most one positive element in each column. The dual simplicial complex Σ*(L) of a nondegenerate Leontief substitution system L is investigated, and it is shown that such a collection comprises the independent sets of a matroid. Conversely, a characterization is given for the class of matroids whose independent sets form the dual complex to a nondegenerate linear system, and it is shown that this system can always be chosen to be a (not necessarily bounded) Leontief substitution system.", "question_context_used": "A (pre-) Leontief substitution system is a linear system $A x=b,\\ x\\geq0$ where $b$ is a nonnegative vector and $A$ is a matrix containing at most one positive element in each column. The dual simplicial complex Σ*(L) of a nondegenerate Leontief substitution system L is investigated, and it is shown that such a collection comprises the independent sets of a matroid. Conversely, a characterization is given for the class of matroids whose independent sets form the dual complex to a nondegenerate linear system, and it is shown that this system can always be chosen to be a (not necessarily bounded) Leontief substitution system.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3779", "question": "In the context of the objective function for two-stage processing centers, what condition ensures that the term d(xi, xj) is not required in the summand associated with a source-destination pair (s, δ)?", "options": ["A) w^t(s, δ) ≥ min{w(s, δ), w*(s, δ)}", "B) w^t(s, δ) ≤ min{w(s, δ), w*(s, δ)}", "C) w^t(s, δ) = max{w(s, δ), w*(s, δ)}", "D) w^t(s, δ) ≠ min{w(s, δ), w*(s, δ)}"], "gold_answer": "A", "source_files": ["Optimal Locations for Centers in a Network.md"], "question_context": "The text discusses a two-stage processing model where material undergoes initial and final processing, potentially at different centers. The objective function involves minimizing a sum of weighted distances, with terms like d(xi, xj) representing distances between centers. A key condition is presented: if w^t(s, δ) ≥ min{w(s, δ), w*(s, δ)}, then the term d(xi, xj) is not required in the summand for the source-destination pair (s, δ). This condition avoids contradictions with the triangle inequality and ensures concavity of the objective function. The mathematical formulation is given by:\n\n$$w^{t}(s,\\delta)\\geq\\operatorname*{min}\\left\\{w(s,\\delta),w^{*}(s,\\delta)\\right\\}.$$\n\nThis condition is critical for maintaining the concavity of the function F, which is essential for applying Hakimi's results in this context.", "question_context_used": "The text discusses a two-stage processing model where material undergoes initial and final processing, potentially at different centers. The objective function involves minimizing a sum of weighted distances, with terms like d(xi, xj) representing distances between centers. A key condition is presented: if w^t(s, δ) ≥ min{w(s, δ), w*(s, δ)}, then the term d(xi, xj) is not required in the summand for the source-destination pair (s, δ). This condition avoids contradictions with the triangle inequality and ensures concavity of the objective function. The mathematical formulation is given by:\n\n$$w^{t}(s,\\delta)\\geq\\operatorname*{min}\\left\\{w(s,\\delta),w^{*}(s,\\delta)\\right\\}.$$\n\nThis condition is critical for maintaining the concavity of the function F, which is essential for applying Hakimi's results in this context.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-1464", "question": "Under the assumptions of Lemma 3.1, what is the necessary condition for the optimal solution \\(a^{*}\\) to satisfy the equation \\(C_{k}^{\\prime}(a_{k}^{*}) = \\mathbb{E}\\left[R^{\\prime}\\left(\\sum_{k=0}^{N-1}a_{k}^{*}\\right)\\middle|\\mathcal{F}_{k}^{Y}\\right]\\)?", "options": ["A) \\(R(a, y)\\) is strictly convex and \\(C_k\\) is linear", "B) \\(a \\mapsto R(a, y)\\) is continuously differentiable and \\(0 < a_k^* < \\infty\\) a.s.", "C) \\(C_k\\) is concave and \\(R(a, y)\\) is piecewise linear", "D) \\(a_k^*\\) is independent of \\(Y\\) and \\(R(a, y)\\) is constant"], "gold_answer": "B", "source_files": ["Optimal Relative Performance Criteria in Mean-Fiel.md"], "question_context": "Lemma 3.1 states that if \\(V < \\infty\\) and there exists an optimal solution \\(a^{*} = (a_{0}^{*}, \\ldots, a_{N-1}^{*}) \\in \\mathcal{A}\\) for \\(V\\) satisfying \\(0 < a_{k}^{*} < \\infty\\) a.s. for \\(k = 0, \\ldots, N-1\\), and if \\(a \\mapsto R(a, y)\\) is continuously differentiable, then for \\(k = 0, \\ldots, N-1\\), the condition \\(C_{k}^{\\prime}(a_{k}^{*}) = \\mathbb{E}\\left[R^{\\prime}\\left(\\sum_{k=0}^{N-1}a_{k}^{*}\\right)\\middle|\\mathcal{F}_{k}^{Y}\\right]\\) holds almost surely. This result is derived from the maximum principle under the given assumptions.", "question_context_used": "Lemma 3.1 states that if \\(V < \\infty\\) and there exists an optimal solution \\(a^{*} = (a_{0}^{*}, \\ldots, a_{N-1}^{*}) \\in \\mathcal{A}\\) for \\(V\\) satisfying \\(0 < a_{k}^{*} < \\infty\\) a.s. for \\(k = 0, \\ldots, N-1\\), and if \\(a \\mapsto R(a, y)\\) is continuously differentiable, then for \\(k = 0, \\ldots, N-1\\), the condition \\(C_{k}^{\\prime}(a_{k}^{*}) = \\mathbb{E}\\left[R^{\\prime}\\left(\\sum_{k=0}^{N-1}a_{k}^{*}\\right)\\middle|\\mathcal{F}_{k}^{Y}\\right]\\) holds almost surely. This result is derived from the maximum principle under the given assumptions.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-898", "question": "Under what condition can the joint probability distribution function \\( H(t) \\) be guaranteed to be concave in the context of chance constrained programming with independent random variables?", "options": ["A) When the random variables \\( \\beta_i \\) are normally distributed with \\( \\alpha \\geq 0.5 \\)", "B) When the distribution functions \\( F_i \\) have strongly unimodal density and \\( \\alpha \\geq \\alpha_0 \\)", "C) When the logarithmic transformation \\( \\sum_{i \\in I} \\ln F_i(t_i) \\geq \\ln \\alpha \\) is applied", "D) When the random variables \\( \\beta_i \\) are exponentially distributed"], "gold_answer": "B", "source_files": ["On Chance Constrained Programming Problems with Jo.md"], "question_context": "The paper investigates chance constrained programming problems with joint constraints, focusing on conditions under which the joint probability distribution function \\( H(t) \\) is concave. For independent random variables \\( \\beta_i \\), it is shown that for a large class of distribution functions with strongly unimodal density functions, there exists a lower bound \\( \\alpha_0 \\) such that for \\( \\alpha \\geq \\alpha_0 \\), \\( H(t) \\) is concave. This lower bound depends on the form of the distribution but is independent of \\( p \\). For example, for the normal distribution, \\( \\alpha_0 \\approx 0.5 \\). This finding allows the use of existing nonlinear programming algorithms to find a global optimum without resorting to logarithmic transformations.", "question_context_used": "The paper investigates chance constrained programming problems with joint constraints, focusing on conditions under which the joint probability distribution function \\( H(t) \\) is concave. For independent random variables \\( \\beta_i \\), it is shown that for a large class of distribution functions with strongly unimodal density functions, there exists a lower bound \\( \\alpha_0 \\) such that for \\( \\alpha \\geq \\alpha_0 \\), \\( H(t) \\) is concave. This lower bound depends on the form of the distribution but is independent of \\( p \\). For example, for the normal distribution, \\( \\alpha_0 \\approx 0.5 \\). This finding allows the use of existing nonlinear programming algorithms to find a global optimum without resorting to logarithmic transformations.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-162", "question": "In the model discussed by Rhys and Murchland, what is the primary objective when making an optimal selection?", "options": ["A) Maximize the sum of profits from selected subsets while minimizing the sum of costs of chosen points", "B) Minimize the total number of points selected from the set S", "C) Maximize the number of subsets selected from the collection ∑", "D) Minimize the cost of constructing all points in the set S"], "gold_answer": "A", "source_files": ["Notes—On a Selection Problem.md"], "question_context": "The model involves a finite set of points S (\"stations\") with costs ${\\pmb{c}}_{\\pmb{\\imath}}>{\\pmb{0}}$ for choosing any point s of S. A finite collection ∑ of subsets of points from S is specified with profits ${\\pmb p}_{\\pmb{\\sigma}}$ for choosing any subset σ. A selection is defined as a collection of subsets from ∑ together with all points of S belonging to this collection. The value of the selection is the sum of the profits of the subsets minus the sum of the costs of the points in the selection. The problem is to find a selection of maximum value.", "question_context_used": "The model involves a finite set of points S (\"stations\") with costs ${\\pmb{c}}_{\\pmb{\\imath}}>{\\pmb{0}}$ for choosing any point s of S. A finite collection ∑ of subsets of points from S is specified with profits ${\\pmb p}_{\\pmb{\\sigma}}$ for choosing any subset σ. A selection is defined as a collection of subsets from ∑ together with all points of S belonging to this collection. The value of the selection is the sum of the profits of the subsets minus the sum of the costs of the points in the selection. The problem is to find a selection of maximum value.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-120", "question": "According to the linear model analysis of the TACV system, what is the optimal number of terminals for maximizing time savings while minimizing costs?", "options": ["A) One terminal, achieving 29% time savings at half the cost", "B) Two terminals, achieving 24% time savings with balanced cost and coverage", "C) Three terminals, achieving 27% time savings despite higher costs", "D) Four terminals, maximizing coverage but with diminishing returns on time savings"], "gold_answer": "A", "source_files": ["The Application of Location Models to Off-Airport.md"], "question_context": "The linear model suggests that with R = 0.67 the total travel time that can be saved with one terminal is 18 per cent of the total time required by passengers using the existing system (the San Diego Freeway), with two, it is 24 per cent and with three, 27 per cent. If R = 0.5 it is possible to obtain a time saving of 29 per cent with just one station. A system with perhaps two-thirds of the track mileage and only one station could probably be constructed for half the cost of the proposed system. The addition of a third station, even if it is well located, adds little to the savings possible with a two-terminal system. The most cost-effective design is probably a TACV system extending between LAX and single terminal at Sepulveda Boulevard.", "question_context_used": "The linear model suggests that with R = 0.67 the total travel time that can be saved with one terminal is 18 per cent of the total time required by passengers using the existing system (the San Diego Freeway), with two, it is 24 per cent and with three, 27 per cent. If R = 0.5 it is possible to obtain a time saving of 29 per cent with just one station. A system with perhaps two-thirds of the track mileage and only one station could probably be constructed for half the cost of the proposed system. The addition of a third station, even if it is well located, adds little to the savings possible with a two-terminal system. The most cost-effective design is probably a TACV system extending between LAX and single terminal at Sepulveda Boulevard.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-4184", "question": "In the context of the NWBPMJSS problem, what is the primary reason for the efficiency improvement observed when using the NWBPMJSS-Liu-Kozan-BIH heuristic algorithm compared to the initial NWBPMJSS-Liu-Kozan algorithm?", "options": ["A) Reduction in computational resources required for scheduling", "B) Optimization of the order of trains to minimize makespan", "C) Elimination of all blocking times in the schedule", "D) Increased capacity of single-track sections"], "gold_answer": "B", "source_files": ["Scheduling Trains with Priorities_ A No-Wait Block.md"], "question_context": "The NWBPMJSS-Liu-Kozan algorithm is a constructive algorithm that constructs a feasible NWBPMJSS schedule based on a given order of trains. The solution quality depends on this order. The NWBPMJSS-Liu-Kozan-BIH heuristic algorithm improves the solution quality by finding a preferable order of trains. For the 10-train 19-section NWBPMJSS instance, the makespan improved from 132.35 to 118.64, a 10.36% efficiency improvement. The CPU time for the heuristic algorithm was 374 milliseconds, indicating additional computational effort to optimize the train order.", "question_context_used": "The NWBPMJSS-Liu-Kozan algorithm is a constructive algorithm that constructs a feasible NWBPMJSS schedule based on a given order of trains. The solution quality depends on this order. The NWBPMJSS-Liu-Kozan-BIH heuristic algorithm improves the solution quality by finding a preferable order of trains. For the 10-train 19-section NWBPMJSS instance, the makespan improved from 132.35 to 118.64, a 10.36% efficiency improvement. The CPU time for the heuristic algorithm was 374 milliseconds, indicating additional computational effort to optimize the train order.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3971", "question": "What was the primary methodological innovation in the Grady Health System project that contributed to its recognition as a Wagner Prize finalist?", "options": ["A) Implementation of a decision support system combining machine learning, simulation, and optimization", "B) Extensive data collection before and after system implementation", "C) Process consolidation and activity tracking initiatives", "D) Addition of a clinical decision unit and staffing revision"], "gold_answer": "A", "source_files": ["Special Issue Editors' Note_ Wagner Prize Second-P.md"], "question_context": "The Grady Health System project involved customizing and implementing a decision support system that incorporates an innovative combination of machine learning, simulation, and optimization. This approach, applied at Grady Memorial Hospital, led to significant gains in effectiveness (e.g., reduced average length of stay, lower readmissions) and efficiency (e.g., process consolidation, activity tracking). The Wagner Prize criteria emphasize innovative methods, which this project demonstrated through its novel methodological integration. The same approach has been successfully adapted and implemented at other hospitals.", "question_context_used": "The Grady Health System project involved customizing and implementing a decision support system that incorporates an innovative combination of machine learning, simulation, and optimization. This approach, applied at Grady Memorial Hospital, led to significant gains in effectiveness (e.g., reduced average length of stay, lower readmissions) and efficiency (e.g., process consolidation, activity tracking). The Wagner Prize criteria emphasize innovative methods, which this project demonstrated through its novel methodological integration. The same approach has been successfully adapted and implemented at other hospitals.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-68", "question": "In the case study by J. McDonald, which analytical technique was used to determine the optimal implementation method for charging for directory assistance service?", "options": ["A) Development of quantifiable objectives", "B) Construction of an econometric demand model", "C) Simulation of the economic impact", "D) Analysis of potential customer reaction"], "gold_answer": "C", "source_files": ["The Use of Management Science in Making a Corporat.md"], "question_context": "The paper outlines the structure of the telecommunications industry, describes how directory assistance service is provided and indicates how customer use of the service has grown. Then, the analytical techniques that were used to assist in making the decision are described. These include: development of quantifiable objectives, determination of how customers currently use the service, construction of an econometric demand model, selection of optimal implementation methods, simulation of the economic impact, and analysis of potential customer reaction. This study led to the conclusion that charging would be beneficial and pointed out the particular charging plan that would be best.", "question_context_used": "The paper outlines the structure of the telecommunications industry, describes how directory assistance service is provided and indicates how customer use of the service has grown. Then, the analytical techniques that were used to assist in making the decision are described. These include: development of quantifiable objectives, determination of how customers currently use the service, construction of an econometric demand model, selection of optimal implementation methods, simulation of the economic impact, and analysis of potential customer reaction. This study led to the conclusion that charging would be beneficial and pointed out the particular charging plan that would be best.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3719", "question": "What was the primary reason for the development of the Intermediate Future Forecasting System (IFFS) as described in the text?", "options": ["A) To replace the outdated PIES model with a more politically neutral name", "B) To handle transition issues in energy markets with a smaller, more efficient model", "C) To comply with the Reagan administration's directive to eliminate long-term forecasting", "D) To integrate natural gas market analysis with the Gas Analysis Modeling System (GAMS)"], "gold_answer": "B", "source_files": ["The Evolution of Energy Modeling at the Federal En.md"], "question_context": "The text describes the development of the Intermediate Future Forecasting System (IFFS) as a response to the need for a smaller, more efficient model to handle transition issues in energy markets. The model was based on an ad hoc natural gas market model that could solve for annual equilibria quickly. The name IFFS was chosen partly as a play on 'if and only if,' reflecting the conditional nature of the projections. The reorganization of EIA in 1981 aimed to bring analysts closer to data collection, and IFFS fit this new structure with its modular design. The model was developed with fewer resources than PIES, and its run times were significantly faster, making it more practical for the downsized EIA.", "question_context_used": "The text describes the development of the Intermediate Future Forecasting System (IFFS) as a response to the need for a smaller, more efficient model to handle transition issues in energy markets. The model was based on an ad hoc natural gas market model that could solve for annual equilibria quickly. The name IFFS was chosen partly as a play on 'if and only if,' reflecting the conditional nature of the projections. The reorganization of EIA in 1981 aimed to bring analysts closer to data collection, and IFFS fit this new structure with its modular design. The model was developed with fewer resources than PIES, and its run times were significantly faster, making it more practical for the downsized EIA.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-499", "question": "In the context of the truck assignment problem, what is the primary reason for setting the cost to $\\$9,999$ when a vehicle cannot arrive in time to meet the demand?", "options": ["A) To maximize the profit margin by excluding high-cost assignments", "B) To ensure the solver does not select infeasible assignments", "C) To account for potential penalties from late deliveries", "D) To simplify the cost calculation by using a fixed value"], "gold_answer": "B", "source_files": ["Implementing Large-Scale Optimization Models in Ex.md"], "question_context": "In a truck assignment problem, a scheduler must assign some or all of the available vehicles to a set of customer demands. The analyst initially uses a static model for the case of a single type of truck. If the scheduler wants to expand the model to include a dynamic aspect with specified times for truck availabilities and requirements, the analyst can calculate the cost of assigning each available vehicle to a demand site with worksheet functions using a travel cost table, assuming that the vehicle can arrive in time to meet the demand. If the vehicle cannot arrive in time, the cost is set to $\\$9,999$ to preclude the combination (Figure 6). Enhancing the model in this way is within the ability of most skilled developers.", "question_context_used": "In a truck assignment problem, a scheduler must assign some or all of the available vehicles to a set of customer demands. The analyst initially uses a static model for the case of a single type of truck. If the scheduler wants to expand the model to include a dynamic aspect with specified times for truck availabilities and requirements, the analyst can calculate the cost of assigning each available vehicle to a demand site with worksheet functions using a travel cost table, assuming that the vehicle can arrive in time to meet the demand. If the vehicle cannot arrive in time, the cost is set to $\\$9,999$ to preclude the combination (Figure 6). Enhancing the model in this way is within the ability of most skilled developers.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-309", "question": "According to the detailed study recommendations for the Narmada project, what is the primary methodological approach suggested for establishing the actual magnitude of the CAP problem?", "options": ["A) Rely solely on Narmada Nigam's existing land acquisition data", "B) Conduct a stratified sample study of individual villages across different districts", "C) Use only the Save the Narmada Campaign's sample study data", "D) Apply a uniform compensation package without further investigation"], "gold_answer": "B", "source_files": ["OR Helps the Poor in a Controversial Irrigation Pr.md"], "question_context": "The report emphasizes the need for a detailed study to establish the actual magnitude of the CAP problem, stating that Narmada Nigam's existing data is insufficient as it only includes information for acquired survey numbers. The study should be augmented by a stratified sample study of individual villages in different districts affected by the conveyance and distributary systems. Complete khatedar profiles should be built using forms 7/12 and 8a, along with other relevant forms held by talatis, and local information to augment the database. This approach was demonstrated in the study of Gadkoi. The end goal is to delineate all major categories of CAP uniformly, provide accurate numbers for each category, and quantify all outlined problems.", "question_context_used": "The report emphasizes the need for a detailed study to establish the actual magnitude of the CAP problem, stating that Narmada Nigam's existing data is insufficient as it only includes information for acquired survey numbers. The study should be augmented by a stratified sample study of individual villages in different districts affected by the conveyance and distributary systems. Complete khatedar profiles should be built using forms 7/12 and 8a, along with other relevant forms held by talatis, and local information to augment the database. This approach was demonstrated in the study of Gadkoi. The end goal is to delineate all major categories of CAP uniformly, provide accurate numbers for each category, and quantify all outlined problems.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3993", "question": "Under what condition does the termwise relaxation Ψ coincide with the convex hull conv(ϕ^Q) of a bilinear function ϕ defined on a graph G?", "options": ["A) The graph G is acyclic", "B) The graph G is balanced (every cycle has an even number of negative edges)", "C) The graph G is complete", "D) The graph G has no negative edges"], "gold_answer": "B", "source_files": ["Tractable Relaxations of Composite Functions.md"], "question_context": "The termwise relaxation Ψ of a bilinear function ϕ defined on a graph G is given by: \n\n$$\n\\Psi:=\\biggl\\{(s,\\phi)\\left|\\sum_{e\\in E}\\mathrm{conv}_{Q}(\\tilde{\\phi}_{e})(s)\\leq\\phi\\leq\\sum_{e\\in E}\\mathrm{conc}_{Q}(\\tilde{\\phi}_{e})(s),s\\in Q\\right.\\biggr\\}.\n$$\n\nTheorem 3 states that Ψ coincides with conv(ϕ^Q) if and only if the graph G is balanced, i.e., every cycle in G has an even number of negative edges. This result is derived from the properties of supermodular functions and the structure of the graph's edge signs.", "question_context_used": "The termwise relaxation Ψ of a bilinear function ϕ defined on a graph G is given by: \n\n$$\n\\Psi:=\\biggl\\{(s,\\phi)\\left|\\sum_{e\\in E}\\mathrm{conv}_{Q}(\\tilde{\\phi}_{e})(s)\\leq\\phi\\leq\\sum_{e\\in E}\\mathrm{conc}_{Q}(\\tilde{\\phi}_{e})(s),s\\in Q\\right.\\biggr\\}.\n$$\n\nTheorem 3 states that Ψ coincides with conv(ϕ^Q) if and only if the graph G is balanced, i.e., every cycle in G has an even number of negative edges. This result is derived from the properties of supermodular functions and the structure of the graph's edge signs.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-2764", "question": "According to the Edelman Award competition guidelines, which of the following is a crucial factor in the verification process for semifinalists?", "options": ["A) The number of citations the work has received in academic journals", "B) The direct communication with the entrant’s team, users of the work, and client management", "C) The total budget allocated for the implementation of the work", "D) The geographical diversity of the team members"], "gold_answer": "B", "source_files": ["Introduction_ 2018 Franz Edelman Award for Achieve.md"], "question_context": "The selection committee reviews all entrants and selects a set of semifinalists. Verifiers then work behind the scenes to validate the claims made by each semifinalist, and to convey this information to the rest of the selection committee. Verification is performed by a mix of practice-oriented academics and full-time practitioners. The verifiers communicate directly with the entrant’s team, the users of the work, and client management. Support from client executives is important. Verification is a crucial element of the competition because it ensures that only the highest-achieving OR/MS and advanced analytics work makes it to the Edelman Award finals. All verifiers are provided with written guidelines and sample verification reports, and novice verifiers are paired with experienced verifiers.", "question_context_used": "The selection committee reviews all entrants and selects a set of semifinalists. Verifiers then work behind the scenes to validate the claims made by each semifinalist, and to convey this information to the rest of the selection committee. Verification is performed by a mix of practice-oriented academics and full-time practitioners. The verifiers communicate directly with the entrant’s team, the users of the work, and client management. Support from client executives is important. Verification is a crucial element of the competition because it ensures that only the highest-achieving OR/MS and advanced analytics work makes it to the Edelman Award finals. All verifiers are provided with written guidelines and sample verification reports, and novice verifiers are paired with experienced verifiers.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-2155", "question": "In the context of LSH Count, what is the primary purpose of the random variable $W$ as defined in the text?", "options": ["A) To measure the total number of elements in the dataset", "B) To represent the sum of elements of interest weighted by their collision probabilities, ensuring unbiased estimation", "C) To calculate the exact hamming distance between query and samples", "D) To determine the optimal number of hash tables required for sampling"], "gold_answer": "B", "source_files": ["Local Density Estimation in High Dimensions.md"], "question_context": "The random variable $W$ is introduced as an intermediate variable in LSH Count, defined as: $$W=\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{\\substack{x\\in\\mathcal{A}_{q}}}\\frac{\\mathbb{1}(x\\in\\mathcal{B}_{q}^{k}(\\mathcal{T}))}{p(x)},$$ where $p(x)=\\mathbb{P}(d_{q x}\\in\\mathcal{T}|\\theta_{q x})$. $W$ represents the sum of the elements of interest $|\\mathcal{A}_{q}|$ that are hashed to the buckets of sampling focus $B_{q}^{k}(\\mathcal{T})$, weighted by their probabilities $p(x)$. It is shown that $\\mathbb{E}(W)=|\\mathcal{A}_{q}|$, ensuring that the estimator $Z$ is unbiased when conditioned on $W$. The variance of $W$ is analyzed to determine the number of hash tables $K$ needed to ensure $W$ concentrates around its mean.", "question_context_used": "The random variable $W$ is introduced as an intermediate variable in LSH Count, defined as: $$W=\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{\\substack{x\\in\\mathcal{A}_{q}}}\\frac{\\mathbb{1}(x\\in\\mathcal{B}_{q}^{k}(\\mathcal{T}))}{p(x)},$$ where $p(x)=\\mathbb{P}(d_{q x}\\in\\mathcal{T}|\\theta_{q x})$. $W$ represents the sum of the elements of interest $|\\mathcal{A}_{q}|$ that are hashed to the buckets of sampling focus $B_{q}^{k}(\\mathcal{T})$, weighted by their probabilities $p(x)$. It is shown that $\\mathbb{E}(W)=|\\mathcal{A}_{q}|$, ensuring that the estimator $Z$ is unbiased when conditioned on $W$. The variance of $W$ is analyzed to determine the number of hash tables $K$ needed to ensure $W$ concentrates around its mean.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3428", "question": "In the context of meta-heuristics as discussed in the book 'Meta-Heuristics: Advances and Trends in Local Search Paradigms for Optimization', which of the following best describes the primary reason for applying meta-heuristic techniques?", "options": ["A) To provide exact solutions to optimization problems in polynomial time", "B) To handle problems that are too large and complex for exact solutions in practically acceptable time", "C) To replace traditional optimization methods entirely due to their superior efficiency", "D) To simplify the mathematical formulation of optimization problems"], "gold_answer": "B", "source_files": ["Book Reviews.md"], "question_context": "Meta-heuristics (MH) aim at finding good quality solutions efficiently. Since the main emphasis of the book is on combinatorial—but also on continuous—global optimization models, and many techniques discussed are of global scope, the word local could perhaps have been omitted from the title. Meta-heuristics—as excerpted from the definition given by the editors of this volume—is an iterative master process that guides the operations of (typically model-specific) subalgorithms. The reason for applying such techniques is that many real-world design and operational decision problems are too large and complex to obtain exact solutions in practically acceptable time.", "question_context_used": "Meta-heuristics (MH) aim at finding good quality solutions efficiently. Since the main emphasis of the book is on combinatorial—but also on continuous—global optimization models, and many techniques discussed are of global scope, the word local could perhaps have been omitted from the title. Meta-heuristics—as excerpted from the definition given by the editors of this volume—is an iterative master process that guides the operations of (typically model-specific) subalgorithms. The reason for applying such techniques is that many real-world design and operational decision problems are too large and complex to obtain exact solutions in practically acceptable time.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-2235", "question": "In the context of the damped Newton's method for nonsmooth equations via path search, what is a key requirement for ensuring global convergence?", "options": ["A) The function must be twice continuously differentiable", "B) The path search must satisfy the Armijo condition", "C) The Hessian matrix must be positive definite at all points", "D) The initial guess must be within a small neighborhood of the solution"], "gold_answer": "B", "source_files": ["Global Convergence of Damped Newton's Method for N.md"], "question_context": "The article discusses the global convergence of the damped Newton's method for nonsmooth equations via the path search. A critical aspect of this method is the use of a path search strategy that ensures sufficient decrease in the merit function, often formalized through conditions like the Armijo rule. The method does not require the function to be twice continuously differentiable or the Hessian to be positive definite everywhere, distinguishing it from classical Newton's method approaches. The global convergence is achieved under mild conditions on the nonsmooth equations, leveraging the path search mechanism to guide the iterations.", "question_context_used": "The article discusses the global convergence of the damped Newton's method for nonsmooth equations via the path search. A critical aspect of this method is the use of a path search strategy that ensures sufficient decrease in the merit function, often formalized through conditions like the Armijo rule. The method does not require the function to be twice continuously differentiable or the Hessian to be positive definite everywhere, distinguishing it from classical Newton's method approaches. The global convergence is achieved under mild conditions on the nonsmooth equations, leveraging the path search mechanism to guide the iterations.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-614", "question": "Based on the experiments described, what is the primary factor that determines the optimum red-time for minimizing delay and mean queue length between two intersections?", "options": ["A) The flow rate of vehicles", "B) The mean velocity of vehicles", "C) The distance between intersections", "D) The diffusion constant α"], "gold_answer": "B", "source_files": ["Some Aspects of a Switching Policy for Commuter Fl.md"], "question_context": "The paper discusses the relationship between flow rate, mean velocity, and optimum red-time for traffic control between intersections. Figure 8 plots the relation between flow rate and optimum red-time for a mean velocity of 30 mph and a distance of 1/4 mi between intersections. The experiments reveal that for a spacing of 1/4 mi, minimum values of delay and mean queue length occur at a mean velocity of around 30 mph. The study emphasizes that the optimum red-time is a function of the separation between intersections and that a complete study would require consideration of multiple mean velocities. The conclusion highlights that for separations of about 1/4 mi, minimum delay and queue length are achieved at mean velocities of approximately 30 mph.", "question_context_used": "The paper discusses the relationship between flow rate, mean velocity, and optimum red-time for traffic control between intersections. Figure 8 plots the relation between flow rate and optimum red-time for a mean velocity of 30 mph and a distance of 1/4 mi between intersections. The experiments reveal that for a spacing of 1/4 mi, minimum values of delay and mean queue length occur at a mean velocity of around 30 mph. The study emphasizes that the optimum red-time is a function of the separation between intersections and that a complete study would require consideration of multiple mean velocities. The conclusion highlights that for separations of about 1/4 mi, minimum delay and queue length are achieved at mean velocities of approximately 30 mph.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-2446", "question": "According to Lemma 5, in an optimal policy for the MIMV problem, what must be true about the shipment quantity in period t' when considering an arc (t', t'') in the network?", "options": ["A) X*(t') mod C = D(t', t'') mod C", "B) X*(t') must be a full vehicle shipment (integer multiple of C)", "C) X*(t') must equal the demand in period t' (D(t', t'+1))", "D) X*(t') must be zero if D(t', t'') mod C = 0"], "gold_answer": "A", "source_files": ["Shipping Multiple Items by Capacitated Vehicles_ A.md"], "question_context": "Lemma 5 specifies properties of an optimal policy X* for the MIMV problem when considering arcs in the network. For an arc (t', t'') with I(t'-1) = I(t''-1) = 0 and I(τ) > 0 for t'-1 < τ < t''-1, the lemma states that: (a) X*(t') mod C = D(t', t'') mod C, and (b) D(t', t'+1) ≤ X*(t') ≤ ⌈D(t', t'+1)/C⌉C. Part (a) follows from the zero initial and final inventory conditions and the fact that shipments in intermediate periods are full vehicles. The bounds in part (b) ensure feasibility while respecting vehicle capacity constraints. These properties are crucial for determining admissible arcs in the network and computing optimal holding costs.", "question_context_used": "Lemma 5 specifies properties of an optimal policy X* for the MIMV problem when considering arcs in the network. For an arc (t', t'') with I(t'-1) = I(t''-1) = 0 and I(τ) > 0 for t'-1 < τ < t''-1, the lemma states that: (a) X*(t') mod C = D(t', t'') mod C, and (b) D(t', t'+1) ≤ X*(t') ≤ ⌈D(t', t'+1)/C⌉C. Part (a) follows from the zero initial and final inventory conditions and the fact that shipments in intermediate periods are full vehicles. The bounds in part (b) ensure feasibility while respecting vehicle capacity constraints. These properties are crucial for determining admissible arcs in the network and computing optimal holding costs.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-1009", "question": "In the 3TA model, what is the key factor influencing the maximum delay for a driver to exit the accident at instant $t_i$?", "options": ["A) The volume of vehicles queuing at the beginning of the previous interval", "B) The percentage of people informed of the accident at each discrete time", "C) The free travel time on the accident link", "D) The exit flow downstream from the accident"], "gold_answer": "A", "source_files": ["Performability of a Congested Urban Transportation.md"], "question_context": "The maximum delay for a driver to exit the accident, evaluated at instant $t_i$, is given by $\\mathbf{D}(t_i) = \\frac{v_{\\mathrm{q}}(t_i)}{d_{\\mathrm{s}}(i)}$. Here, $v_{\\mathrm{q}}(t_i)$ represents the volume of vehicles queuing at the beginning of an iteration interval, which equals the number of vehicles queuing at the beginning of the previous interval plus the number of arrivals during that interval, minus the number of departures. The exit flows $d_{\\mathrm{s}}(i)$ remain constant at $d_{\\mathrm{s}}$ as long as the accident is not removed. The free travel time on the accident link, $\\xi$, is calculated by solving M1 with reduced capacity on link $\\ell^{*}$ and is referred to as $\\boldsymbol{\\underline{{T}}}_{\\ell^{*}}$.", "question_context_used": "The maximum delay for a driver to exit the accident, evaluated at instant $t_i$, is given by $\\mathbf{D}(t_i) = \\frac{v_{\\mathrm{q}}(t_i)}{d_{\\mathrm{s}}(i)}$. Here, $v_{\\mathrm{q}}(t_i)$ represents the volume of vehicles queuing at the beginning of an iteration interval, which equals the number of vehicles queuing at the beginning of the previous interval plus the number of arrivals during that interval, minus the number of departures. The exit flows $d_{\\mathrm{s}}(i)$ remain constant at $d_{\\mathrm{s}}$ as long as the accident is not removed. The free travel time on the accident link, $\\xi$, is calculated by solving M1 with reduced capacity on link $\\ell^{*}$ and is referred to as $\\boldsymbol{\\underline{{T}}}_{\\ell^{*}}$.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-3941", "question": "In the context of the Constraint-Dissolving Function (CDF) approach for solving Optimization Problems with Constraints (OCP), which of the following solvers is NOT available in a Riemannian version within Python packages, according to the provided text?", "options": ["A) CG (Conjugate Gradient)", "B) BFGS (Broyden-Fletcher-Goldfarb-Shanno)", "C) Trust-ncg (Newton Conjugate Gradient Trust-Region)", "D) Trust-krylov (Newton Generalized Lanczos Trust-Region)"], "gold_answer": "B", "source_files": ["Dissolving Constraints for Riemannian Optimization.md"], "question_context": "The text discusses the application of various unconstrained optimization solvers to solve OCP via the CDF approach. It highlights that while some solvers like CG and Trust-ncg have Riemannian versions available in Python packages, others such as BFGS do not. The generalized Stiefel manifold, the hyperbolic manifold, and the symplectic Stiefel manifold are mentioned as examples where Riemannian solvers are not readily available. The CDF approach allows embedding these problems into existing unconstrained solvers, bypassing the need for specialized Riemannian implementations. The inequality \n\n$$ h(y)\\geq h(\\mathcal{A}^{\\infty}(y))+\\frac{\\beta}{16}\\|c(y)\\|^{2}\\geq h(\\tilde{x}) $$ \n\nis provided as part of the theoretical foundation for the CDF method.", "question_context_used": "The text discusses the application of various unconstrained optimization solvers to solve OCP via the CDF approach. It highlights that while some solvers like CG and Trust-ncg have Riemannian versions available in Python packages, others such as BFGS do not. The generalized Stiefel manifold, the hyperbolic manifold, and the symplectic Stiefel manifold are mentioned as examples where Riemannian solvers are not readily available. The CDF approach allows embedding these problems into existing unconstrained solvers, bypassing the need for specialized Riemannian implementations. The inequality \n\n$$ h(y)\\geq h(\\mathcal{A}^{\\infty}(y))+\\frac{\\beta}{16}\\|c(y)\\|^{2}\\geq h(\\tilde{x}) $$ \n\nis provided as part of the theoretical foundation for the CDF method.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-1105", "question": "According to the author's analysis of cost-benefit tradeoffs in EDP consulting, why might analysts overlook the 'Happy Hunting Ground' region of solutions?", "options": ["A) Analysts assume higher investment always yields proportionally higher returns, per Grosch's law", "B) Hardware-software imbalances obscure the potential of lower-cost solutions", "C) Clients explicitly prohibit exploration of solutions below a certain budget threshold", "D) Regulatory constraints prevent implementation of unconventional system designs"], "gold_answer": "B", "source_files": ["Letters to the Editor.md"], "question_context": "The author describes a phenomenon where optimal EDP solutions are missed during analysis due to cognitive biases in cost-benefit tradeoffs. Analysts often confine searches to higher investment ranges, assuming 'pay more to get more' (invoking Grosch's law for medium/large computers: performance ∝ investment²). However, hardware-software imbalances (e.g., fast processors with slow software) can obscure better solutions in lower-cost regions termed the 'Happy Hunting Ground'. The author illustrates this with a case where an ideal MIS design was rejected because it implicitly criticized the existing system by delivering more for less.", "question_context_used": "The author describes a phenomenon where optimal EDP solutions are missed during analysis due to cognitive biases in cost-benefit tradeoffs. Analysts often confine searches to higher investment ranges, assuming 'pay more to get more' (invoking Grosch's law for medium/large computers: performance ∝ investment²). However, hardware-software imbalances (e.g., fast processors with slow software) can obscure better solutions in lower-cost regions termed the 'Happy Hunting Ground'. The author illustrates this with a case where an ideal MIS design was rejected because it implicitly criticized the existing system by delivering more for less.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-722", "question": "Given a fixed integer matrix $A$ of size $m$ by $n$ and the body $K_{b} = \\{x \\colon A x \\leqslant b\\}$, what is the primary factor that bounds the number of shortest nonzero integral vectors with respect to the norm $\\|\\cdot\\|_{b}$?", "options": ["A) The bit size of the matrix $A$", "B) The dimension $n$ of the matrix $A$", "C) The largest absolute value of an $n$ by $n$ subdeterminant of $A$, $\\Delta_{n}(A)$", "D) The number of rows $m$ in the matrix $A$"], "gold_answer": "A", "source_files": ["Shortest Integer Vectors.md"], "question_context": "The paper examines the set of shortest nonzero integral vectors with respect to the family of norms whose unit balls are given by $(K_{b} - K_{b})$, where $K_{b} = \\{x \\colon A x \\leqslant b\\}$. It is shown that the number of such shortest vectors is polynomial in the bit size of $A$ for fixed $n$. The dependence on the bit size of $A$ is through the log of $\\Delta_{n}(A)$, defined as the largest absolute value of an $n$ by $n$ subdeterminant of $A$. The argument follows the outlines of the polynomial bound on the number of vertices of the integral hull of a polyhedron. The paper also demonstrates the existence of a family of matrices for which the number of shortest vectors has a lower bound polynomial in the bit size of the matrix.", "question_context_used": "The paper examines the set of shortest nonzero integral vectors with respect to the family of norms whose unit balls are given by $(K_{b} - K_{b})$, where $K_{b} = \\{x \\colon A x \\leqslant b\\}$. It is shown that the number of such shortest vectors is polynomial in the bit size of $A$ for fixed $n$. The dependence on the bit size of $A$ is through the log of $\\Delta_{n}(A)$, defined as the largest absolute value of an $n$ by $n$ subdeterminant of $A$. The argument follows the outlines of the polynomial bound on the number of vertices of the integral hull of a polyhedron. The paper also demonstrates the existence of a family of matrices for which the number of shortest vectors has a lower bound polynomial in the bit size of the matrix.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-1048", "question": "According to the analysis of the DSS implementation at Omya Hustadmarmor, which factor contributed the most to the total yearly savings in production costs?", "options": ["A) Reduced number of changeovers in production", "B) Increased use of production capacity by four percent", "C) Improved reliability of the distribution plan", "D) Reduction in raw materials and energy consumption"], "gold_answer": "A", "source_files": ["Omya Hustadmarmor Optimizes Its Supply Chain for D.md"], "question_context": "The DSS implementation at Omya Hustadmarmor led to considerable savings, largely due to increased predictability throughout the supply chain. The production department benefited from reliable shipping plans and four-week planning horizons. The savings in production costs from reduced waste, raw materials, and energy consumption resulted from the more reliable distribution plan, which stabilized production and reduced changeovers (Figure 6). Product-quality problems that require reprocessing have almost disappeared since the DSS was implemented because the number of changeovers has dropped by 40 percent. The total yearly savings are significant, with production cost savings amounting to 4,310 (units not specified) and a present value of 46,970.", "question_context_used": "The DSS implementation at Omya Hustadmarmor led to considerable savings, largely due to increased predictability throughout the supply chain. The production department benefited from reliable shipping plans and four-week planning horizons. The savings in production costs from reduced waste, raw materials, and energy consumption resulted from the more reliable distribution plan, which stabilized production and reduced changeovers (Figure 6). Product-quality problems that require reprocessing have almost disappeared since the DSS was implemented because the number of changeovers has dropped by 40 percent. The total yearly savings are significant, with production cost savings amounting to 4,310 (units not specified) and a present value of 46,970.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-1361", "question": "Based on the survey results of OR method usage in commercial banks, which OR methodology has the highest frequency of use when combined with other methods, as indicated by the percentage of total frequency?", "options": ["A) Probabilistic Programming", "B) Nonlinear Programming", "C) Simulation", "D) Linear Programming"], "gold_answer": "A", "source_files": ["The Application of Operations Research in Commerci.md"], "question_context": "The data suggest that the propensity of the model builder to combine the use of various methods is greatly influenced by his sophistication. For example, nonlinear programming and probabilistic programming, relatively sophisticated methods, are used with other methods in 74% and 89% of their applications, respectively. Simulation, an approach which can be used by groups with less sophistication and resources, is used with the other selected OR methods in only 30% of its applications. However, those banks which used nonlinear programming for one or more of the problem areas used simulation in combination with the other OR Methods in 64% of its applications.", "question_context_used": "The data suggest that the propensity of the model builder to combine the use of various methods is greatly influenced by his sophistication. For example, nonlinear programming and probabilistic programming, relatively sophisticated methods, are used with other methods in 74% and 89% of their applications, respectively. Simulation, an approach which can be used by groups with less sophistication and resources, is used with the other selected OR methods in only 30% of its applications. However, those banks which used nonlinear programming for one or more of the problem areas used simulation in combination with the other OR Methods in 64% of its applications.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-1154", "question": "According to the development of BRANDFRAME for HFMC's brand managers, which of the following was a primary motivation for creating a customized decision-support system?", "options": ["A) To replace the brand managers with an automated system", "B) To integrate academic and practical knowledge into a single interactive system", "C) To reduce the number of brands managed by each brand manager", "D) To eliminate the need for market research data"], "gold_answer": "B", "source_files": ["Developing a Customized Decision-Support System fo.md"], "question_context": "Working with brand managers at Holland Fast Moving Company (HFMC), we designed a marketing-management-support system, BRANDFRAME. BRANDFRAME contains a combination of academic and practical knowledge and integrates all the information needed by the brand manager in one system. The system does not replace the brand manager but, in an interactive way, acts as a sparring partner. The system was very well received by the managers who worked with it and has shown that customizing decision-support systems for specific marketing jobs (in this case, the brand manager) may well be an important factor for the success of marketing-management-support systems. Brand managers at HFMC were not very satisfied with the existing systems, particularly the ERP system, citing issues with user friendliness, reliability, and adaptability to their needs.", "question_context_used": "Working with brand managers at Holland Fast Moving Company (HFMC), we designed a marketing-management-support system, BRANDFRAME. BRANDFRAME contains a combination of academic and practical knowledge and integrates all the information needed by the brand manager in one system. The system does not replace the brand manager but, in an interactive way, acts as a sparring partner. The system was very well received by the managers who worked with it and has shown that customizing decision-support systems for specific marketing jobs (in this case, the brand manager) may well be an important factor for the success of marketing-management-support systems. Brand managers at HFMC were not very satisfied with the existing systems, particularly the ERP system, citing issues with user friendliness, reliability, and adaptability to their needs.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-712", "question": "In the context of multivariate transient price impact models, what is the primary mathematical tool used by Alfonsi, Klock, and Schied to analyze the problem?", "options": ["A) Matrix-valued positive definite functions", "B) Stochastic differential equations", "C) Fourier transform techniques", "D) Game-theoretic equilibrium analysis"], "gold_answer": "A", "source_files": ["Multivariate Transient Price Impact and Matrix-Val.md"], "question_context": "The article by Alfonsi, Klock, and Schied (2016) focuses on multivariate transient price impact and utilizes matrix-valued positive definite functions as a key mathematical framework. The authors analyze the transient price impact in a multivariate setting, extending previous univariate models. The use of matrix-valued positive definite functions allows for a more general and flexible approach to modeling the cross-impact of different assets in a portfolio. The mathematical formulation is given by: $\\Lambda(t) = \\int_0^t G(s)G(s)^T ds$, where $G$ is a matrix-valued function and $\\Lambda$ is the resulting positive definite kernel.", "question_context_used": "The article by Alfonsi, Klock, and Schied (2016) focuses on multivariate transient price impact and utilizes matrix-valued positive definite functions as a key mathematical framework. The authors analyze the transient price impact in a multivariate setting, extending previous univariate models. The use of matrix-valued positive definite functions allows for a more general and flexible approach to modeling the cross-impact of different assets in a portfolio. The mathematical formulation is given by: $\\Lambda(t) = \\int_0^t G(s)G(s)^T ds$, where $G$ is a matrix-valued function and $\\Lambda$ is the resulting positive definite kernel.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-4056", "question": "In the integer program $(\\mathcal{M}_{k})$ for playoff elimination, which constraint ensures that team $k$ is eliminated from the playoffs if $n$ or more teams have better records?", "options": ["A) Constraint (A.2d): $\\sum_{t\\in\\mathcal{C}_{k}}\\omega_{t k}\\leq n-1$", "B) Constraint (A.3d): $W_{t}-W_{t^{\\prime}}\\le M\\cdot(1-Z_{t t^{\\prime}})$", "C) Constraint (A.4c): $M\\cdot\\gamma_{t}\\geq\\left(n-\\frac{1}{2}\\right)-\\sum_{t^{\\prime}\\in\\mathcal{C}_{t}}\\beta_{t^{\\prime}t}$", "D) Constraint (A.13b): $\\sum_{t\\in\\mathcal{C}_{k}}\\beta_{k t}\\geq(|\\mathcal{C}_{k}|+1-n)\\cdot(1-\\lambda)$"], "gold_answer": "A", "source_files": ["Improving Sports Media’s Crystal Ball for National.md"], "question_context": "The integer program $(\\mathcal{M}_{k})$ determines the number of wins needed for team $k$ to avoid playoff elimination. Constraint (A.2d) limits the number of teams with more wins than team $k$ to be one fewer than the number of teams that make the playoffs ($n$). If $n$ or more teams have better records, team $k$ is eliminated. The model uses big-M constructs and binary variables to handle tie-breaking scenarios and win comparisons. The objective function minimizes $W_k$, the wins needed by team $k$ to avoid elimination.", "question_context_used": "The integer program $(\\mathcal{M}_{k})$ determines the number of wins needed for team $k$ to avoid playoff elimination. Constraint (A.2d) limits the number of teams with more wins than team $k$ to be one fewer than the number of teams that make the playoffs ($n$). If $n$ or more teams have better records, team $k$ is eliminated. The model uses big-M constructs and binary variables to handle tie-breaking scenarios and win comparisons. The objective function minimizes $W_k$, the wins needed by team $k$ to avoid elimination.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-1819", "question": "In the context of faculty staffing optimization, what does the variable $f_{2j}(t)$ represent in the given mathematical model?", "options": ["A) The number of associate professors filling positions for missing assistant professors in field $j$ at time $t$", "B) The number of assistant professors filling positions for missing associate professors in field $j$ at time $t$", "C) The number of full professors filling positions for missing associate professors in field $j$ at time $t$", "D) The number of assistant professors filling positions for missing full professors in field $j$ at time $t$"], "gold_answer": "B", "source_files": ["A Comprehensive Planning Model for Long-Range Acad.md"], "question_context": "The faculty staffing model includes definitional constraints and variables to account for deviations from staffing goals and fill-ins. The variable $f_{2j}(t)$ is explicitly defined as the number of assistant professors filling positions for missing associate professors in field $j$ at time $t$. The model also specifies other types of fill-ins, such as associate professors filling in for missing assistant professors ($f_{1j}(t)$) or full professors ($f_{3j}(t)$). The objective function minimizes a weighted sum of deviations and fill-ins, where the weights represent the school's preferences.", "question_context_used": "The faculty staffing model includes definitional constraints and variables to account for deviations from staffing goals and fill-ins. The variable $f_{2j}(t)$ is explicitly defined as the number of assistant professors filling positions for missing associate professors in field $j$ at time $t$. The model also specifies other types of fill-ins, such as associate professors filling in for missing assistant professors ($f_{1j}(t)$) or full professors ($f_{3j}(t)$). The objective function minimizes a weighted sum of deviations and fill-ins, where the weights represent the school's preferences.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-425", "question": "In the context of the estimated linear specific cross-sectional model for travel by United States citizens to Canada, what is the primary reason for the insignificance of the income coefficient in all cases?", "options": ["A) Travel to Canada is truly independent of per capita income", "B) Per capita income is the wrong measure and a minimum level of income should be used instead", "C) The model suffers from multicollinearity between income and automobile ownership", "D) The data on income was not accurately collected or reported"], "gold_answer": "B", "source_files": ["Demand for Travel to Canada by American Citizens.md"], "question_context": "The paper estimates a linear specific cross-sectional model for travel by United States citizens to Canada, where the number of visitors from state i to province α is a function of per capita income, registered automobiles, distance, and visits to other provinces. The results indicate that the income coefficient is insignificant in all cases. Two possible explanations are provided: first, that travel to Canada is truly independent of per capita income, and second, that per capita income is the wrong measure and instead some measure of minimum level of income should be used. The coefficient of the registered automobiles variable also shows inconsistencies, attributed to the colinearity of automobile ownership and income.", "question_context_used": "The paper estimates a linear specific cross-sectional model for travel by United States citizens to Canada, where the number of visitors from state i to province α is a function of per capita income, registered automobiles, distance, and visits to other provinces. The results indicate that the income coefficient is insignificant in all cases. Two possible explanations are provided: first, that travel to Canada is truly independent of per capita income, and second, that per capita income is the wrong measure and instead some measure of minimum level of income should be used. The coefficient of the registered automobiles variable also shows inconsistencies, attributed to the colinearity of automobile ownership and income.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-366", "question": "What was the primary driver for the increased corporate interest in Investment Recovery programs during the 1960s and 1970s?", "options": ["A) Changes in internal revenue laws that eliminated tax benefits from donating assets", "B) The introduction of new accounting principles requiring reporting of asset disposition gains/losses", "C) The rising cost of raw materials and supply chain disruptions", "D) Increased shareholder demand for corporate social responsibility initiatives"], "gold_answer": "B", "source_files": ["Investment Recovery_ An Emerging Management Strate.md"], "question_context": "In the early sixties, plant expansions and modernizations were commonplace, and techniques for disposing of old plants, equipment, and inventories were much in the business news. By 1968, Union Carbide's Investment Recovery program had recovered $10 million from capital equipment. Changes in the internal revenue laws in 1961 and 1969 made tax benefits from donating assets virtually nonexistent. In 1966, the Accounting Principles Board decided that losses or gains from disposition of assets should be reported to shareholders as a factor in current earnings [Bolger, 1969]. This led to increased corporate focus on asset recovery techniques, which would today be considered facets of OR/MS.", "question_context_used": "In the early sixties, plant expansions and modernizations were commonplace, and techniques for disposing of old plants, equipment, and inventories were much in the business news. By 1968, Union Carbide's Investment Recovery program had recovered $10 million from capital equipment. Changes in the internal revenue laws in 1961 and 1969 made tax benefits from donating assets virtually nonexistent. In 1966, the Accounting Principles Board decided that losses or gains from disposition of assets should be reported to shareholders as a factor in current earnings [Bolger, 1969]. This led to increased corporate focus on asset recovery techniques, which would today be considered facets of OR/MS.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-883", "question": "Based on the analysis of coefficient values in Table 2, which of the following statements is most strongly supported by the data regarding the relationship between endogenous and exogenous variables?", "options": ["A) Endogenous variables consistently exhibit higher explanatory power than exogenous variables across all relationships.", "B) Exogenous variables, such as income and education, dominate the predictive power in brand comprehension equations.", "C) The coefficient of sex in the confidence equations indicates females feel better informed about the product than males.", "D) The constants being nonzero suggest that the minimum value of the scale is always the actual bottom for new brands."], "gold_answer": "A", "source_files": ["A “Working” System Model of Car Buyer Behavior.md"], "question_context": "The analysis of average coefficient values reveals several key insights. First, constants are nonzero, suggesting that the actual bottom of the scales is not the minimum value, even for new brands. Second, the signs of coefficients among all endogenous variables are positive and significantly different from zero, indicating their strong explanatory power. Third, the coefficients of income and education suggest the new brand appeals to a low to moderate income segment, but exogenous variables generally have less explanatory power than endogenous variables. The coefficient of sex in the confidence equations confirms that males feel better informed about the product. The formula for the relationship can be represented as: Y = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ, where endogenous variables (X₁, X₂, ...) show stronger coefficients than exogenous ones.", "question_context_used": "The analysis of average coefficient values reveals several key insights. First, constants are nonzero, suggesting that the actual bottom of the scales is not the minimum value, even for new brands. Second, the signs of coefficients among all endogenous variables are positive and significantly different from zero, indicating their strong explanatory power. Third, the coefficients of income and education suggest the new brand appeals to a low to moderate income segment, but exogenous variables generally have less explanatory power than endogenous variables. The coefficient of sex in the confidence equations confirms that males feel better informed about the product. The formula for the relationship can be represented as: Y = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ, where endogenous variables (X₁, X₂, ...) show stronger coefficients than exogenous ones.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-286", "question": "What is the primary purpose of INFORMS as an international association?", "options": ["A) To publish academic journals exclusively for operations research", "B) To provide networking and learning opportunities for O.R. and analytics professionals", "C) To regulate the certification of operations research professionals globally", "D) To develop proprietary analytics software for its members"], "gold_answer": "B", "source_files": ["Introduction to the Special Issue on Military Appl.md"], "question_context": "INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students, with 12,500 members from nearly 90 countries. It provides unique networking and learning opportunities for individual professionals and organizations to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes.", "question_context_used": "INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students, with 12,500 members from nearly 90 countries. It provides unique networking and learning opportunities for individual professionals and organizations to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-59", "question": "In the context of the single-track railroad segment delay model, what is the expected delay for a southbound train arriving during a northbound busy period?", "options": ["A) $P_{\\mathrm{N}}\\left[\\frac{1}{\\lambda_{\\mathrm{N}}}e^{\\lambda_{\\mathrm{N}}T}-\\frac{T e^{\\lambda_{\\mathrm{N}}T}}{e^{\\lambda_{\\mathrm{N}}T}-1}+l\\right]$", "B) $P_{\\mathrm{S}}\\left[\\frac{1}{\\lambda_{\\mathrm{S}}}e^{\\lambda_{\\mathrm{S}}T}-\\frac{T e^{\\lambda_{\\mathrm{S}}T}}{e^{\\lambda_{\\mathrm{S}}T}-1}+l\\right]$", "C) $\\frac{{\\bf E}[B_{\\bf N}^{e}]}{2{\\bf E}[B_{\\bf N}]} + l$", "D) $\\frac{\\lambda_{\\mathrm{N}}}{\\lambda_{\\mathrm{N}}+\\lambda_{\\mathrm{S}}}\\left(e^{(\\lambda_{\\mathrm{N}}+\\lambda_{\\mathrm{S}})T}-1\\right)$"], "gold_answer": "A", "source_files": ["Predicting Dispatching Delays on a Low Speed, Sing.md"], "question_context": "The expected delay for southbound trains arriving during a northbound busy period is derived from the equilibrium distribution of the remaining busy period duration. The formula incorporates the proportion of time the track is occupied by northbound trains ($P_{\\mathrm{N}}$), the arrival rate of northbound trains ($\\lambda_{\\mathrm{N}}$), the running time ($T$), and the running time losses ($l$). The correct expression is given by: $$\\mathbf{E}[D_{\\mathrm{s}}]=P_{\\mathrm{N}}\\biggl[\\frac{1}{\\lambda_{\\mathrm{N}}}e^{\\lambda_{\\mathrm{N}}T}-\\frac{T e^{\\lambda_{\\mathrm{N}}T}}{e^{\\lambda_{\\mathrm{N}}T}-1}+l\\biggr].$$ This accounts for the expected remaining busy period duration and the additional delay due to running time losses.", "question_context_used": "The expected delay for southbound trains arriving during a northbound busy period is derived from the equilibrium distribution of the remaining busy period duration. The formula incorporates the proportion of time the track is occupied by northbound trains ($P_{\\mathrm{N}}$), the arrival rate of northbound trains ($\\lambda_{\\mathrm{N}}$), the running time ($T$), and the running time losses ($l$). The correct expression is given by: $$\\mathbf{E}[D_{\\mathrm{s}}]=P_{\\mathrm{N}}\\biggl[\\frac{1}{\\lambda_{\\mathrm{N}}}e^{\\lambda_{\\mathrm{N}}T}-\\frac{T e^{\\lambda_{\\mathrm{N}}T}}{e^{\\lambda_{\\mathrm{N}}T}-1}+l\\biggr].$$ This accounts for the expected remaining busy period duration and the additional delay due to running time losses.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-4391", "question": "In the context of the multilevel optimization model for network protection, what is the primary objective of hardening network components against worst-case disruptions?", "options": ["A) To minimize the initial shortest path length without any disruptions", "B) To maximize the number of protected components regardless of their criticality", "C) To minimize the length of the shortest path after a worst-case disruption of unprotected components", "D) To ensure all network components have equal protection levels"], "gold_answer": "C", "source_files": ["Optimal Allocation of Protective Resources in Shor.md"], "question_context": "The paper introduces a game-theoretic approach for allocating protection resources among the components of a network to maximize its robustness to external disruptions. Specifically, it considers shortest-path networks where disruptions may result in traffic flow delays or complete loss of some elements. A multilevel program is proposed to identify the set of components to harden so as to minimize the length of the shortest path between a supply node and a demand node after a worst-case disruption of some unprotected components. The approach is streamlined by solving the lower-level interdiction problem heuristically at each node of an enumeration tree and by using some variable fixing rules to reduce the dimension of the lower-level problems. The primary objective is to minimize the post-disruption shortest path length, ensuring network robustness against strategic attacks.", "question_context_used": "The paper introduces a game-theoretic approach for allocating protection resources among the components of a network to maximize its robustness to external disruptions. Specifically, it considers shortest-path networks where disruptions may result in traffic flow delays or complete loss of some elements. A multilevel program is proposed to identify the set of components to harden so as to minimize the length of the shortest path between a supply node and a demand node after a worst-case disruption of some unprotected components. The approach is streamlined by solving the lower-level interdiction problem heuristically at each node of an enumeration tree and by using some variable fixing rules to reduce the dimension of the lower-level problems. The primary objective is to minimize the post-disruption shortest path length, ensuring network robustness against strategic attacks.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-2415", "question": "In the dynamic routing strategy for priority shipments in LTL networks, what is the primary factor contributing to the reduction in total travel time?", "options": ["A) Decreased road travel time between terminals", "B) Reduced variance in the time spent at terminals", "C) Minimized loading time at terminals", "D) Increased number of direct routes between terminals"], "gold_answer": "C", "source_files": ["Dynamic Routing for Priority Shipments in LTL Serv.md"], "question_context": "The travel time for a shipment from its origin terminal to its destination terminal consists of the time spent on the road $(T_{\\mathbf{r}})$ and the time spent at terminals $(T_{\\mathrm{t}})$. Although $T_{\\mathbf{r}}$ and $T_{\\mathrm{t}}$ are both random variables, the variance of $T_{\\mathbf{r}}$ is considerably smaller than that of $T_{\\mathrm{t}},$ where $T_{\\mathrm{t}}$ depends on a wide range of factors, such as day of the week, closing times at terminals, service deadlines of the shipments, sizes of the shipments, and driver availability. Because the physical mileage between the terminals is fixed, substantial decrease in $T_{\\mathbf{r}}$ is unlikely. Thus, our primary focus is to cut down $T_{\\mathrm{t}}.$ We have observed from real data that the loading time, which ranges from 1 to 48 hours, contributes the most to the waiting time at terminals in most cases. We thus argue that reducing it should receive a higher priority.", "question_context_used": "The travel time for a shipment from its origin terminal to its destination terminal consists of the time spent on the road $(T_{\\mathbf{r}})$ and the time spent at terminals $(T_{\\mathrm{t}})$. Although $T_{\\mathbf{r}}$ and $T_{\\mathrm{t}}$ are both random variables, the variance of $T_{\\mathbf{r}}$ is considerably smaller than that of $T_{\\mathrm{t}},$ where $T_{\\mathrm{t}}$ depends on a wide range of factors, such as day of the week, closing times at terminals, service deadlines of the shipments, sizes of the shipments, and driver availability. Because the physical mileage between the terminals is fixed, substantial decrease in $T_{\\mathbf{r}}$ is unlikely. Thus, our primary focus is to cut down $T_{\\mathrm{t}}.$ We have observed from real data that the loading time, which ranges from 1 to 48 hours, contributes the most to the waiting time at terminals in most cases. We thus argue that reducing it should receive a higher priority.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-2312", "question": "According to the analysis of initial attack systems for forest fires in Ontario, which factor is most critical for the selective cost-effectiveness of air tankers?", "options": ["A) The ability of air tankers to extinguish fires completely without ground crew support", "B) The linear decrease in fire-line construction rate by ground crews as fire intensity increases", "C) The capability of air tankers to suppress fires at intensities exceeding 1000 Btu/ft-sec", "D) The lower operational cost of air tankers compared to ground crews"], "gold_answer": "C", "source_files": ["An Evaluation of Forest Fire Initial Attack Resour.md"], "question_context": "The rate of line construction of fire fighters equipped with a power pumping unit decreases linearly from a maximum which depends on crew size and the type of forest in which the fire is burning, to zero feet per hour when fire intensity exceeds 1000 Btu/ft-sec. Although air tanker line is more expensive than that produced by ground crews and ground crews are ultimately required to extinguish all fires, air tankers can usually begin suppression action sooner than ground crews, and some aircraft produce fire line at intensities in excess of 1000 Btu/ft-sec. Since the intensity of most initial attack fires in Ontario is considerably less than 1000 Btu/ft-sec, ground crew alone can handle most fires. Very intense fires can be controlled by neither crews nor air tankers. Only a small percentage of intermediate intensity fires behave in such a way that they cannot readily be controlled by ground crews alone but require air tankers as well.", "question_context_used": "The rate of line construction of fire fighters equipped with a power pumping unit decreases linearly from a maximum which depends on crew size and the type of forest in which the fire is burning, to zero feet per hour when fire intensity exceeds 1000 Btu/ft-sec. Although air tanker line is more expensive than that produced by ground crews and ground crews are ultimately required to extinguish all fires, air tankers can usually begin suppression action sooner than ground crews, and some aircraft produce fire line at intensities in excess of 1000 Btu/ft-sec. Since the intensity of most initial attack fires in Ontario is considerably less than 1000 Btu/ft-sec, ground crew alone can handle most fires. Very intense fires can be controlled by neither crews nor air tankers. Only a small percentage of intermediate intensity fires behave in such a way that they cannot readily be controlled by ground crews alone but require air tankers as well.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-2244", "question": "In the context of market clearing problems with non-unique solutions, what is the implication when the supply and demand curves overlap horizontally in a two-asset setting?", "options": ["A) The market equilibrium price becomes indeterminate", "B) The consumer surplus is guaranteed to converge to zero", "C) The uniqueness of the solution affects the paper's conclusions", "D) The double auction equilibrium must always be zero"], "gold_answer": "B", "source_files": ["Efficient Allocations in Double Auction Markets.md"], "question_context": "The solution to the market clearing problem need not be unique, and there may be other solutions besides zero at a double auction equilibrium $x^{0}$. In the two-asset setting of Section 2, this would mean that horizontal parts of the supply and demand curves $s$ and $d$ overlap. The uniqueness or the lack of it has no effect on the conclusions of this paper. The author is grateful to professor Sjur D. Flåm for proposing the growth condition in Assumption 5 to guarantee that the consumer surplus converges to zero.", "question_context_used": "The solution to the market clearing problem need not be unique, and there may be other solutions besides zero at a double auction equilibrium $x^{0}$. In the two-asset setting of Section 2, this would mean that horizontal parts of the supply and demand curves $s$ and $d$ overlap. The uniqueness or the lack of it has no effect on the conclusions of this paper. The author is grateful to professor Sjur D. Flåm for proposing the growth condition in Assumption 5 to guarantee that the consumer surplus converges to zero.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-2408", "question": "According to the study on microcomputer labs in universities, which software category has the highest percentage of labs planning to purchase more in the next one or two years?", "options": ["A) Statistical programs", "B) Data base software", "C) Management science software", "D) Word processing software"], "gold_answer": "A", "source_files": ["Note_ A Microcomputer Revolution in the School of.md"], "question_context": "The study examined the level of use of various software packages in microcomputer labs. It found that 64 percent of all labs used data base software, with d-Base II being the strong leader. A smaller (37 percent) but faster growing number of labs have statistical and management science packages available. The big packages, however, are generic spreadsheets and word processors. Plans for purchasing new software are strong, with 51.4 percent of all schools planning to buy more spreadsheet software in the next one or two years; 61.4 percent will buy data base software; 67.6 percent, accounting packages; 56.2 percent, management science software; 69.0 percent, statistical programs; and 51.4 percent, word processing software. Only 2.9 percent of the micro labs do not intend to purchase more software during the next two years.", "question_context_used": "The study examined the level of use of various software packages in microcomputer labs. It found that 64 percent of all labs used data base software, with d-Base II being the strong leader. A smaller (37 percent) but faster growing number of labs have statistical and management science packages available. The big packages, however, are generic spreadsheets and word processors. Plans for purchasing new software are strong, with 51.4 percent of all schools planning to buy more spreadsheet software in the next one or two years; 61.4 percent will buy data base software; 67.6 percent, accounting packages; 56.2 percent, management science software; 69.0 percent, statistical programs; and 51.4 percent, word processing software. Only 2.9 percent of the micro labs do not intend to purchase more software during the next two years.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-2292", "question": "In the classroom assignment system described, what is the primary reason for assigning examinations with the largest number of students to the largest classrooms first?", "options": ["A) To minimize the number of professors required for supervision", "B) To ensure that common subjects are scheduled without conflicts", "C) To reduce the overhead costs associated with classroom maintenance", "D) To optimize the use of available classroom capacities efficiently"], "gold_answer": "A", "source_files": ["Examination Scheduling in a Large Engineering Scho.md"], "question_context": "The classrooms are assigned in the same order as the dates are scheduled. It is convenient to assign the examinations to be taken by the largest number of students to the largest classrooms, and these are the common subjects which are scheduled first. An important aspect of this priority is that more professors would be required to supervise a given examination if it were held in more than one room; hence the faculty would find it undesirable to have several small classrooms assigned to a single examination. The capacity of the classrooms is given in Table 1, and the assignment process is outlined in Figure 3.", "question_context_used": "The classrooms are assigned in the same order as the dates are scheduled. It is convenient to assign the examinations to be taken by the largest number of students to the largest classrooms, and these are the common subjects which are scheduled first. An important aspect of this priority is that more professors would be required to supervise a given examination if it were held in more than one room; hence the faculty would find it undesirable to have several small classrooms assigned to a single examination. The capacity of the classrooms is given in Table 1, and the assignment process is outlined in Figure 3.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-2227", "question": "In the context of large crowding games, what condition ensures that the function $\\hat{f}:\\Delta(\\mathcal{T})\\to\\mathbf{R}$ is continuous at $\\mu\\circ g^{-1}$?", "options": ["A) The game $g$ has a strict equilibrium", "B) The type distribution $\\mu\\circ g^{-1}$ is nonatomic", "C) The payo3 functions $g_{ij}$ are strictly decreasing", "D) The set of representative players $I$ is finite"], "gold_answer": "A", "source_files": ["Generic Uniqueness of Equilibrium in Large Crowdin.md"], "question_context": "The function $\\hat{f}:\\Delta(\\mathcal{T})\\to\\mathbf{R}$ is defined such that $f(g) = \\hat{f}(\\mu\\circ g^{-1})$ for every $g\\in\\mathcal{G}$. The function $\\hat{f}$ is upper semicontinuous, and is continuous at $\\mu\\circ g^{-1}$ if and only if $f$ is continuous at $g$. By Lemma 4.2 and the fact that the Lebesgue measure is nonatomic, $f$ is continuous at a large crowding game $g$ if and only if that game has a strict equilibrium. It therefore follows from Lemma 5.1 that $\\hat{f}$ is continuous at $\\mu\\circ g^{-1}$ if and only if that type distribution has a strict equilibrium distribution.", "question_context_used": "The function $\\hat{f}:\\Delta(\\mathcal{T})\\to\\mathbf{R}$ is defined such that $f(g) = \\hat{f}(\\mu\\circ g^{-1})$ for every $g\\in\\mathcal{G}$. The function $\\hat{f}$ is upper semicontinuous, and is continuous at $\\mu\\circ g^{-1}$ if and only if $f$ is continuous at $g$. By Lemma 4.2 and the fact that the Lebesgue measure is nonatomic, $f$ is continuous at a large crowding game $g$ if and only if that game has a strict equilibrium. It therefore follows from Lemma 5.1 that $\\hat{f}$ is continuous at $\\mu\\circ g^{-1}$ if and only if that type distribution has a strict equilibrium distribution.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-2391", "question": "In the context of low-rank matrix recovery using the matrix exponentiated gradient (MEG) method, what condition ensures the correct convergence of the low-rank variant when the rank of the ground-truth matrix is known?", "options": ["A) The Bregman distance between iterates of the low-rank and standard MEG methods decays quickly", "B) The strict complementarity condition holds and is dimension independent", "C) The convergence certification condition holds from the early stages of the run", "D) The initialization matrix is set using the known rank parameter"], "gold_answer": "C", "source_files": ["On the Efficient Implementation of the Matrix Expo.md"], "question_context": "In our first line of experiments, we assume that rank(M) is known, and we set the SVD rank parameter in Algorithm 2 to $r={\\mathrm{rank}}(\\mathbf{M})$. This parameter is also used to set the initialization matrix $\\mathbf{X}_{0}$. Importantly, Table 1 indicates that, (i) for our recovery, setting the strict complementarity condition (Assumption 1) indeed holds and seems dimension independent, and (ii) in all cases, the convergence certification condition $$\\log\\left(\\frac{(n-r)\\lambda_{r+1}(\\mathbf{Y}_{t})}{\\varepsilon_{t}b_{t}}\\right)\\leq2\\varepsilon_{t},\\quad b_{t}=\\sum_{i=1}^{n}\\lambda_{i}(\\mathbf{Y}_{t})$$ holds from the early stages of the run (right from the first iteration in most cases) and throughout all following iterations, implying the correct convergence of our low-rank MEG method up to negligible error.", "question_context_used": "In our first line of experiments, we assume that rank(M) is known, and we set the SVD rank parameter in Algorithm 2 to $r={\\mathrm{rank}}(\\mathbf{M})$. This parameter is also used to set the initialization matrix $\\mathbf{X}_{0}$. Importantly, Table 1 indicates that, (i) for our recovery, setting the strict complementarity condition (Assumption 1) indeed holds and seems dimension independent, and (ii) in all cases, the convergence certification condition $$\\log\\left(\\frac{(n-r)\\lambda_{r+1}(\\mathbf{Y}_{t})}{\\varepsilon_{t}b_{t}}\\right)\\leq2\\varepsilon_{t},\\quad b_{t}=\\sum_{i=1}^{n}\\lambda_{i}(\\mathbf{Y}_{t})$$ holds from the early stages of the run (right from the first iteration in most cases) and throughout all following iterations, implying the correct convergence of our low-rank MEG method up to negligible error.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-1803", "question": "In the context of determining the feasibility of a period as the end of a regeneration interval, what condition must be satisfied to ensure that all trains are full in period τ?", "options": ["A) The sum of demands up to period τ-1 must be greater than the balance of period τ's demand.", "B) The sum of demands up to period τ-1 must be less than the balance of period τ's demand.", "C) The sum of demands up to period τ must be exactly divisible by the train capacity C.", "D) The sum of demands up to period τ must be less than the train capacity C."], "gold_answer": "A", "source_files": ["Scheduling Trains and Containers with Due Dates an.md"], "question_context": "The feasibility of a period τ as the end of a regeneration interval is determined by the condition that all trains must be full in period τ. The optimal shipment quantity in period τ is given by: $$x_{\\tau}^{*}(\\tau)=\\left\\lfloor\\frac{\\sum_{u\\leq\\tau}D_{u}(\\tau)}{C}\\right\\rfloor*C.$$ The balance of period τ's demand is: $$\\sum_{u\\leq\\tau}D_{u}(\\tau)-\\left\\lfloor\\frac{\\sum_{u\\leq\\tau}D_{u}(\\tau)}{C}\\right\\rfloor*C=\\sum_{u\\leq\\tau}D_{u}(\\tau)\\mathrm{~mod~}C.$$ If $$\\sum_{u=1}^{\\tau-1}D_{u}(\\tau)<\\sum_{u=1}^{\\tau}D_{u}(\\tau)-\\lfloor\\frac{\\sum_{u=1}^{\\tau}D_{u}(\\tau)}{C}\\rfloor*C,$$ then τ cannot be the last period in any regeneration interval, because we cannot send all trains full in period τ.", "question_context_used": "The feasibility of a period τ as the end of a regeneration interval is determined by the condition that all trains must be full in period τ. The optimal shipment quantity in period τ is given by: $$x_{\\tau}^{*}(\\tau)=\\left\\lfloor\\frac{\\sum_{u\\leq\\tau}D_{u}(\\tau)}{C}\\right\\rfloor*C.$$ The balance of period τ's demand is: $$\\sum_{u\\leq\\tau}D_{u}(\\tau)-\\left\\lfloor\\frac{\\sum_{u\\leq\\tau}D_{u}(\\tau)}{C}\\right\\rfloor*C=\\sum_{u\\leq\\tau}D_{u}(\\tau)\\mathrm{~mod~}C.$$ If $$\\sum_{u=1}^{\\tau-1}D_{u}(\\tau)<\\sum_{u=1}^{\\tau}D_{u}(\\tau)-\\lfloor\\frac{\\sum_{u=1}^{\\tau}D_{u}(\\tau)}{C}\\rfloor*C,$$ then τ cannot be the last period in any regeneration interval, because we cannot send all trains full in period τ.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-1995", "question": "In the context of the class-term weighting algorithm, what is the purpose of the transformation step where $\\mathbf{A}_{i j} = \\mathbf{A}_{i j} / \\tilde{m} * \\log(\\sum_{i=1}^{\\tilde{m}} \\mathbf{A}_{i j})$?", "options": ["A) To normalize the class-term weights by the number of distinct classes and scale them logarithmically", "B) To apply a logarithmic discounting factor to the term frequencies", "C) To ensure the weights are non-negative and sum to one", "D) To reduce the impact of high-frequency terms in the classification process"], "gold_answer": "A", "source_files": ["Using Machine Learning to Improve Public Reporting.md"], "question_context": "The class-term weighting algorithm involves several steps to compute the weights of terms within distinct classes. One critical step is the transformation of the raw term counts into weighted values. The formula $\\mathbf{A}_{i j} = \\mathbf{A}_{i j} / \\tilde{m} * \\log(\\sum_{i=1}^{\\tilde{m}} \\mathbf{A}_{i j})$ is applied to each term weight $\\mathbf{A}_{i j}$, where $\\tilde{m}$ is the number of distinct classes. This step normalizes the weights by the number of classes and scales them logarithmically to account for the distribution of terms across classes. The logarithmic scaling helps mitigate the dominance of very high-frequency terms while preserving the discriminative power of less frequent terms.", "question_context_used": "The class-term weighting algorithm involves several steps to compute the weights of terms within distinct classes. One critical step is the transformation of the raw term counts into weighted values. The formula $\\mathbf{A}_{i j} = \\mathbf{A}_{i j} / \\tilde{m} * \\log(\\sum_{i=1}^{\\tilde{m}} \\mathbf{A}_{i j})$ is applied to each term weight $\\mathbf{A}_{i j}$, where $\\tilde{m}$ is the number of distinct classes. This step normalizes the weights by the number of classes and scales them logarithmically to account for the distribution of terms across classes. The logarithmic scaling helps mitigate the dominance of very high-frequency terms while preserving the discriminative power of less frequent terms.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-2006", "question": "In the context of the OFFIS system described in the Applications Reviews, what is the primary function of the OFFIS Analyzer?", "options": ["A) To facilitate the construction and maintenance of a data base containing the specification", "B) To generate analysis reports on demand", "C) To design office systems for aircraft sales and lease-back companies", "D) To implement a general office information system model"], "gold_answer": "B", "source_files": ["Applications Reviews.md"], "question_context": "The OFFIS system includes the Office Information Specifier (OFFIS) and the OFFIS Analyzer, designed and implemented at the University of Arizona. A language has been implemented and an analyzer developed to facilitate the construction and maintenance of a data base containing the specification. The analyzer also generates the analysis reports on demand. The OFFIS system was used in the design of a system for an aircraft sales and lease-back company.", "question_context_used": "The OFFIS system includes the Office Information Specifier (OFFIS) and the OFFIS Analyzer, designed and implemented at the University of Arizona. A language has been implemented and an analyzer developed to facilitate the construction and maintenance of a data base containing the specification. The analyzer also generates the analysis reports on demand. The OFFIS system was used in the design of a system for an aircraft sales and lease-back company.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-1691", "question": "Which of the following properties must a linear operator φ:Q→FA satisfy to be classified as a semivalue on pM∞?", "options": ["A) Symmetric, positive, and a projection", "B) Symmetric, efficient, and a projection", "C) Positive, efficient, and continuous", "D) Symmetric, efficient, and continuous"], "gold_answer": "A", "source_files": ["The Banzhaf Value and General Semivalues for Diffe.md"], "question_context": "Given a symmetric subspace Q of BV, a linear operator φ:Q→FA is called a semivalue on pM∞ if it satisfies three properties: (i) symmetric if θφ(v)=φ(θv) for every v∈Q and θ∈Θ, (ii) positive if φ(v) is nonnegative whenever v is monotonic, and (iii) a projection if the restriction φ|FA∩Q is an identity function on FA∩Q. A linear projection that is symmetric and positive is called a semivalue on Q. Our main interest is in semivalues on pM∞. Any positive linear projection operator φ on pM∞ is continuous of ‖⋅‖∞-norm 1. Moreover, φ(v)∈M for every v∈pM∞, because for every positive μ∈M such that −μ≤v≤μ, the positivity and projection properties of φ yield −μ≤φ(v)≤μ, implying that φ(v) is countably additive.", "question_context_used": "Given a symmetric subspace Q of BV, a linear operator φ:Q→FA is called a semivalue on pM∞ if it satisfies three properties: (i) symmetric if θφ(v)=φ(θv) for every v∈Q and θ∈Θ, (ii) positive if φ(v) is nonnegative whenever v is monotonic, and (iii) a projection if the restriction φ|FA∩Q is an identity function on FA∩Q. A linear projection that is symmetric and positive is called a semivalue on Q. Our main interest is in semivalues on pM∞. Any positive linear projection operator φ on pM∞ is continuous of ‖⋅‖∞-norm 1. Moreover, φ(v)∈M for every v∈pM∞, because for every positive μ∈M such that −μ≤v≤μ, the positivity and projection properties of φ yield −μ≤φ(v)≤μ, implying that φ(v) is countably additive.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-2434", "question": "In the context of the Generalized Conjugate Gradient (GCG) method for solving problem (2), what is the condition under which the method terminates at an optimal solution when the input parameter ε is set to 0?", "options": ["A) When the norm of the minimum-norm subgradient v(x^k) is less than or equal to ε", "B) When the projected minimum-norm subgradient v^P(x^k) equals zero", "C) When the number of iterations exceeds L(n) + 1", "D) When the exact line search along -v^P(x^k) yields no improvement"], "gold_answer": "A", "source_files": ["Generalized Conjugate Gradient Methods forℓ1Regula.md"], "question_context": "The GCG method proposed terminates at an optimal solution of problem (2) in a finite number of iterations when its input parameter ε is set to 0. The method checks whether the norm of the minimum-norm subgradient v(x^k) is less than or equal to ε. If yes, then x^k is an optimal solution of (2). The method is designed to ensure finite convergence under Assumption 1, with the number of iterations bounded by L(n) + 1, where L(n) is a defined function of the problem dimension n. The method's convergence and optimality are formally stated in Theorem 3.3.", "question_context_used": "The GCG method proposed terminates at an optimal solution of problem (2) in a finite number of iterations when its input parameter ε is set to 0. The method checks whether the norm of the minimum-norm subgradient v(x^k) is less than or equal to ε. If yes, then x^k is an optimal solution of (2). The method is designed to ensure finite convergence under Assumption 1, with the number of iterations bounded by L(n) + 1, where L(n) is a defined function of the problem dimension n. The method's convergence and optimality are formally stated in Theorem 3.3.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-2486", "question": "In the context of the Markov quality control process described by White (1977), what condition ensures that an optimal policy can be characterized by two numbers \\(x_1, x_2 \\in X\\) such that \\(\\phi(x) = 0\\) for \\(x \\in [0, x_1)\\), \\(\\phi(x) = 1\\) for \\(x \\in [x_1, x_2)\\), and \\(\\phi(x) = 2\\) for \\(x \\in [x_2, 1]\\)?", "options": ["A) \\(q_0 = 0.5\\) and \\(1 \\leq (1-q_0)\\pi / ((1-\\pi)(2q_0 - 1))\\)", "B) \\(q_0 \neq 0.5\\) and \\(1 \\leq (1-q_0)\\pi / ((1-\\pi)(2q_0 - 1))\\)", "C) \\(q_0 = 0.5\\) or \\(q_0 \neq 0.5\\) and \\(1 \\leq (1-q_0)\\pi / ((1-\\pi)(2q_0 - 1))\\)", "D) \\(q_0 < 0.5\\) and \\(1 \\leq (1-q_0)\\pi / ((1-\\pi)(2q_0 - 1))\\)"], "gold_answer": "C", "source_files": ["A Markov Quality Control Process Subject to Partia.md"], "question_context": "The paper studies a discrete-time production process with a countable state space, subject to three control settings: produce, inspect while producing, or repair. The inspector receives imperfect on-line observations of the production process. Theorem 6.2 states that under certain conditions, an optimal policy can be characterized by two numbers \\(x_1, x_2 \\in X\\). The conditions are: either \\(q_0 \neq 0.5\\) and \\(1 \\leq (1-q_0)\\pi / ((1-\\pi)(2q_0 - 1))\\) or \\(q_0 = 0.5\\). The inequality \\((1-\beta)(R+\beta\\Gamma(0))/C \\leq \\left[(R-I)/(\beta(1-\\pi)(\\Gamma(1)-\\Gamma(0))) - \\pi/(1-\\pi)\right]\\) must also hold.", "question_context_used": "The paper studies a discrete-time production process with a countable state space, subject to three control settings: produce, inspect while producing, or repair. The inspector receives imperfect on-line observations of the production process. Theorem 6.2 states that under certain conditions, an optimal policy can be characterized by two numbers \\(x_1, x_2 \\in X\\). The conditions are: either \\(q_0 \neq 0.5\\) and \\(1 \\leq (1-q_0)\\pi / ((1-\\pi)(2q_0 - 1))\\) or \\(q_0 = 0.5\\). The inequality \\((1-\beta)(R+\beta\\Gamma(0))/C \\leq \\left[(R-I)/(\beta(1-\\pi)(\\Gamma(1)-\\Gamma(0))) - \\pi/(1-\\pi)\right]\\) must also hold.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-1631", "question": "In the goal programming formulation for allocating a research budget with risk constraints, what is the correct expression for the risk goal when management aims to allocate 30% of the budget to low-risk projects?", "options": ["A) $X_{1} + X_{2} + d^{-} - d^{+} = 0.3 \\sum_{i=1}^{6} X_{i}$", "B) $X_{1} + X_{2} + d^{-} + d^{+} = 0.3 \\sum_{i=1}^{6} X_{i}$", "C) $X_{1} + X_{2} - d^{-} + d^{+} = 0.3 \\sum_{i=1}^{6} X_{i}$", "D) $X_{1} + X_{2} + d^{-} = 0.3 \\sum_{i=1}^{6} X_{i}$"], "gold_answer": "A", "source_files": ["Ballistic Missile Defense Technology Management wi.md"], "question_context": "The text discusses a goal programming formulation for allocating a research budget with risk constraints. Projects 1 and 2 are low-risk, while the others are moderate to high-risk. Management aims to allocate 30% of the total budget to low-risk projects. The formulation involves setting the sum of allocation variables $(X_{i},j)$ for projects 1 and 2, plus an underachievement ($d^{-}$) and an overachievement ($d^{+}$) deviational variable, equal to 0.3 times the sum of all allocation variables. The objective is to minimize underachievement of this goal.", "question_context_used": "The text discusses a goal programming formulation for allocating a research budget with risk constraints. Projects 1 and 2 are low-risk, while the others are moderate to high-risk. Management aims to allocate 30% of the total budget to low-risk projects. The formulation involves setting the sum of allocation variables $(X_{i},j)$ for projects 1 and 2, plus an underachievement ($d^{-}$) and an overachievement ($d^{+}$) deviational variable, equal to 0.3 times the sum of all allocation variables. The objective is to minimize underachievement of this goal.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-456", "question": "In the context of the Reny-Wooders extension of Shapley's generalization of the KKM Theorem, what property must the collection of sets in the intersection have to ensure the existence of a strictly balanced and minimally partnered point?", "options": ["A) The collection must be countable", "B) The collection must be zero-dimensional", "C) The collection must be convex-valued", "D) The collection must be finite"], "gold_answer": "B", "source_files": ["A Further Extension of the KKMS Theorem.md"], "question_context": "Reny and Wooders (1998) extend Shapley’s (1973) generalization of the Knaster-Kuratowski-Mazurkiwicz Theorem by showing that there is some point in the intersection (whose nonemptiness is assured by the theorem) with the property that the supporting collection for that point is partnered as well as balanced. Assuming that the closure of the set of partnered and balanced points is zero-dimensional, we obtain a stronger result on minimal partnership than Reny and Wooders (1998): There is at least one point in the intersection of a strictly balanced and partnered collection of sets that is minimally partnered. Our proof involves the Eilenberg-Montgomery Fixed Point Theorem for set-valued mappings and degree theory for correspondences.", "question_context_used": "Reny and Wooders (1998) extend Shapley’s (1973) generalization of the Knaster-Kuratowski-Mazurkiwicz Theorem by showing that there is some point in the intersection (whose nonemptiness is assured by the theorem) with the property that the supporting collection for that point is partnered as well as balanced. Assuming that the closure of the set of partnered and balanced points is zero-dimensional, we obtain a stronger result on minimal partnership than Reny and Wooders (1998): There is at least one point in the intersection of a strictly balanced and partnered collection of sets that is minimally partnered. Our proof involves the Eilenberg-Montgomery Fixed Point Theorem for set-valued mappings and degree theory for correspondences.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-1954", "question": "In the context of RP-ADMM for solving linear systems, what is the relationship between the matrices L_σ and R_σ as defined in the update equations?", "options": ["A) L_σ = R_σ + A^T A", "B) L_σ = R_σ - A^T A", "C) L_σ = R_σ + I", "D) L_σ = R_σ - I"], "gold_answer": "A", "source_files": ["On the Efficiency of Random Permutation for ADMM a.md"], "question_context": "The update equations of Algorithm 1 for solving (7) can be written in the matrix form as (when the permutation is σ and β=1) y^{k+1}=L̄_σ^{-1}R̄_σy^{k}+L̄_σ^{-1}b̄, where L̄_σ and R̄_σ are defined by L̄_σ≜[L_σ 0; A I_{N×N}], R̄_σ≜[R_σ A^T; 0 I_{N×N}], b̄=[A^T b; b], in which L_σ∈ℝ^{N×N} has n×n blocks and the (i,j)-th block is defined as L_σ[i,j]≜{A_i^T A_j if σ^{-1}(j)≤σ^{-1}(i), 0 otherwise}, and R_σ is defined as R_σ≜L_σ−A^T A. Another expression of L_σ, equivalent to (19), is the following: L_σ[σ(i),σ(j)]≜{A_σ(i)^T A_σ(j) if j≤i, 0 if j>i}.", "question_context_used": "The update equations of Algorithm 1 for solving (7) can be written in the matrix form as (when the permutation is σ and β=1) y^{k+1}=L̄_σ^{-1}R̄_σy^{k}+L̄_σ^{-1}b̄, where L̄_σ and R̄_σ are defined by L̄_σ≜[L_σ 0; A I_{N×N}], R̄_σ≜[R_σ A^T; 0 I_{N×N}], b̄=[A^T b; b], in which L_σ∈ℝ^{N×N} has n×n blocks and the (i,j)-th block is defined as L_σ[i,j]≜{A_i^T A_j if σ^{-1}(j)≤σ^{-1}(i), 0 otherwise}, and R_σ is defined as R_σ≜L_σ−A^T A. Another expression of L_σ, equivalent to (19), is the following: L_σ[σ(i),σ(j)]≜{A_σ(i)^T A_σ(j) if j≤i, 0 if j>i}.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-1713", "question": "Based on the optimization results for BCC5, what was the net reduction in its bill after accounting for credits received from other BCCs?", "options": ["A) $1.39 million", "B) $6.3 million", "C) $19.2 million", "D) $20.6 million"], "gold_answer": "A", "source_files": ["Improving the Utility of the Bellcore Consortium.md"], "question_context": "BCC5 had participated in 79 (out of 143) 1991 projects for a total net bill of $20.6 million. It assigned a positive utility index to 29 of the remaining 64 projects. Through the optimization, BCC5 bought into all of these 29 projects. This in turn generated credit, which the other BCCs used (along with other credits and cash) to buy into BCC5's projects. As a result, BCC5 received $1.39 million in credit, reducing its net bill to $19.2 million. If BCC5 had wanted to buy into the same 29 projects without any other BCC buy-ins, it would have had to pay an extra $6.3 million.", "question_context_used": "BCC5 had participated in 79 (out of 143) 1991 projects for a total net bill of $20.6 million. It assigned a positive utility index to 29 of the remaining 64 projects. Through the optimization, BCC5 bought into all of these 29 projects. This in turn generated credit, which the other BCCs used (along with other credits and cash) to buy into BCC5's projects. As a result, BCC5 received $1.39 million in credit, reducing its net bill to $19.2 million. If BCC5 had wanted to buy into the same 29 projects without any other BCC buy-ins, it would have had to pay an extra $6.3 million.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-1749", "question": "According to the article 'Quickening the Queue in Grocery Stores', what is the primary focus of the study?", "options": ["A) Analyzing the impact of queue management on customer satisfaction", "B) Optimizing the number of cashiers to reduce waiting times", "C) Evaluating the economic benefits of self-checkout systems", "D) Developing a mathematical model for queue dynamics in retail environments"], "gold_answer": "A", "source_files": ["Quickening the Queue in Grocery Stores.md"], "question_context": "The article 'Quickening the Queue in Grocery Stores' by Michael T. Jones, Arlene M. O'Berski, and Gail Tom, published in Interfaces (1980), discusses strategies to improve queue management in grocery stores. The study emphasizes the importance of reducing waiting times to enhance customer satisfaction. The authors explore various operational techniques and their effects on queue dynamics, providing insights into efficient service delivery in retail environments. The mathematical model used in the study is given by: $\\lambda = \\mu - \\rho$, where $\\lambda$ is the arrival rate, $\\mu$ is the service rate, and $\\rho$ is the traffic intensity.", "question_context_used": "The article 'Quickening the Queue in Grocery Stores' by Michael T. Jones, Arlene M. O'Berski, and Gail Tom, published in Interfaces (1980), discusses strategies to improve queue management in grocery stores. The study emphasizes the importance of reducing waiting times to enhance customer satisfaction. The authors explore various operational techniques and their effects on queue dynamics, providing insights into efficient service delivery in retail environments. The mathematical model used in the study is given by: $\\lambda = \\mu - \\rho$, where $\\lambda$ is the arrival rate, $\\mu$ is the service rate, and $\\rho$ is the traffic intensity.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-2120", "question": "In the context of monotone networks with bounded flow size densities, what does Lemma 3 establish about the relationship between the solutions of the fluid-model equations and the fluid-limit equations?", "options": ["A) The solution of the fluid-model equations is always greater than or equal to the solution of the fluid-limit equations.", "B) The solution of the fluid-limit equations is always greater than or equal to the solution of the fluid-model equations.", "C) The solutions of the fluid-model and fluid-limit equations are always equal.", "D) The solutions of the fluid-model and fluid-limit equations are independent of each other."], "gold_answer": "A", "source_files": ["Fluid Limits for Bandwidth-Sharing Networks in Ove.md"], "question_context": "LEMMA 3.Consider a monotone network. Assume the flow size distributions have bounded densities. Let $z(t),\\tau(t)\\in\\mathcal{M}(C)$ a.e., satisfy Equations (29)-(30) and let $z^{\\varepsilon}(t)$ be a solution of the fluid-model Equations (9). Then, for all $t\\geq0$, $z(t)\\leq z^{\\varepsilon}(t)$. PROOF. Let us construct a sequence of functions $z^{\\varepsilon,n}(\\cdot)$, $n>0$, in the following manner. Let $z^{\\varepsilon,0}(t)=z(t)$ be a solution of Equation (29). Lemma 1 implies that there exists at least one such solution. Let $C$ be the space of continuous real-valued functions on $[0,\\infty)$. For a fixed $t\\geq0$, introduce the function $\\Psi^{\\varepsilon}(\\cdot,t)$: $C\\to[\\varepsilon,\\infty)$, $\\Psi_{i}^{\\varepsilon}(y,t)=\\varepsilon_{i}+\\lambda_{i}\\int_{0}^{t}\\mathbf{P}\\bigg(B_{i}>\\int_{s}^{t}\\xi_{i}(y(u))d u\\bigg)d s$. Define $z^{\\varepsilon,n}(t)=\\Psi^{\\varepsilon}(z^{\\varepsilon,n-1},t),\\quad n\\geq1$. We show that the sequence is nondecreasing by induction. Clearly, $z_{i}^{\\varepsilon,1}(t)=\\Psi_{i}^{\\varepsilon}(z,t)=\\varepsilon_{i}+z_{i}(t)\\geq z_{i}^{\\varepsilon,0}(t)$. Suppose $z^{\\varepsilon,n}(t)\\geq z^{\\varepsilon,n-1}(t)$. Since the network is monotone, this implies $\\xi(z^{\\varepsilon,n}(t))\\leq\\xi(z^{\\varepsilon,n-1}(t))$. Hence, $z_{i}^{\\varepsilon,n+1}(t)=\\varepsilon_{i}+\\lambda_{i}\\int_{0}^{t}\\mathbf{P}\\bigg(B_{i}>\\int_{s}^{t}\\xi_{i}(z^{\\varepsilon,n}(u))d u\\bigg)d s\\geq\\varepsilon_{i}+\\lambda_{i}\\int_{0}^{t}\\mathbf{P}\\bigg(B_{i}>\\int_{s}^{t}\\xi_{i}(z^{\\varepsilon,n-1}(u))d u\\bigg)d s=z^{\\varepsilon,n}(t)$. Since $z^{\\varepsilon,n}(t)$ is nondecreasing in $n$ and bounded from above on any finite time interval, there exists a function $z^{\\varepsilon,*}(t)$ such that $\\operatorname*{lim}_{n\\to\\infty}z^{\\varepsilon,n}(t)=z^{\\varepsilon,\\ast}(t)$. Let us now show that the function $z^{\\varepsilon,*}(t)$ is continuous in $t$. Fix $h>0$. Then we have $|z^{\\varepsilon,*}(t+h)-z^{\\varepsilon,*}(t)|=\\operatorname*{lim}_{n\\to\\infty}|z^{\\varepsilon,n}(t+h)-z^{\\varepsilon,n}(t)|\\leq\\lambda_{i}\\int_{t}^{t+h}\\mathbf{P}(B_{i}>S_{i}(s,t+h))d s+\\lambda_{i}\\int_{0}^{t}\\mathbf{P}(S_{i}(s,t)<B_{i}<S_{i}(s,t+h))d s$. The first term is bounded from above by $\\lambda_{i}h$. Consider now the second term. Since the flow size distribution has a bounded density, there exists an $M\\in(0,\\infty)$ such that $f_{B_{i}}(u)\\leq M$ for all $u\\geq0$, $i=1,\\ldots,I$. Hence, $\\int_{0}^{t}\\mathbf{P}(S_{i}(s,t)<B_{i}<S_{i}(s,t+h))d s=\\int_{0}^{t}\\int_{S_{i}(s,t)}^{S_{i}(s,t+h)}f_{B_{i}}(u)d u d s\\leq M\\int_{0}^{t}\\left(S_{i}(s,t+h)-S_{i}(s,t)\\right)d s=M S_{i}(t,t+h)t$. From $z_{i}^{\\varepsilon,*}(t)\\geq\\varepsilon_{i}$, by monotonicity we derive $\\xi_{i}(z^{\\varepsilon,*}(t))\\leq C_{i}^{\\operatorname*{min}}/\\varepsilon_{i},C_{i}^{\\operatorname*{min}}=\\operatorname*{min}\\{C_{j}\\colon A_{j i}>0\\}$. Consequently, $\\int_{0}^{t}\\mathbf{P}(S_{i}(s,t)<B_{i}<S_{i}(s,t+h))\\mathop{d s}\\leq\\frac{M C_{i}^{\\operatorname*{min}}}{\\varepsilon_{i}}h t$. Thus, as $h\\rightarrow0$, the right-hand side of (32) tends to zero, yielding continuity of the function $z^{\\varepsilon,*}(t)$. We now show that $z^{\\varepsilon,*}(t)$ satisfies Equations (9). Since the sequence $z^{\\varepsilon,n}(t)$ is bounded away from zero and is nondecreasing in $n$, $\\xi(z^{\\varepsilon,n}(t))$ is continuous in $t$ and nonincreasing in $n$. Then, $S_{i}^{\\varepsilon,n}(s,t)\\to S_{i}^{\\varepsilon,*}(s,t)$ by monotone convergence. Hence, $\\Psi^{\\varepsilon}(z^{\\varepsilon,n},t)\\to\\Psi^{\\varepsilon}(z^{\\varepsilon,*},t)$, implying $z^{\\varepsilon,*}(t)=\\Psi^{\\varepsilon}(z^{\\varepsilon,*},t)$. Now since the function $z^{\\varepsilon,*}(t)$ is continuous and satisfies the fluid-model Equation (9), Theorem 2 yields $z^{\\varepsilon,*}(t)\\equiv z^{\\varepsilon}(t)$, a unique solution of Equation (9). Since the sequence $z^{\\varepsilon,n}(t)$ is nondecreasing and for all $n$ $z^{\\varepsilon,n}(t)\\geq z(t)$, we deduce that for all $t>0$ $z(t)\\leq z^{\\varepsilon}(t)$.", "question_context_used": "LEMMA 3.Consider a monotone network. Assume the flow size distributions have bounded densities. Let $z(t),\\tau(t)\\in\\mathcal{M}(C)$ a.e., satisfy Equations (29)-(30) and let $z^{\\varepsilon}(t)$ be a solution of the fluid-model Equations (9). Then, for all $t\\geq0$, $z(t)\\leq z^{\\varepsilon}(t)$. PROOF. Let us construct a sequence of functions $z^{\\varepsilon,n}(\\cdot)$, $n>0$, in the following manner. Let $z^{\\varepsilon,0}(t)=z(t)$ be a solution of Equation (29). Lemma 1 implies that there exists at least one such solution. Let $C$ be the space of continuous real-valued functions on $[0,\\infty)$. For a fixed $t\\geq0$, introduce the function $\\Psi^{\\varepsilon}(\\cdot,t)$: $C\\to[\\varepsilon,\\infty)$, $\\Psi_{i}^{\\varepsilon}(y,t)=\\varepsilon_{i}+\\lambda_{i}\\int_{0}^{t}\\mathbf{P}\\bigg(B_{i}>\\int_{s}^{t}\\xi_{i}(y(u))d u\\bigg)d s$. Define $z^{\\varepsilon,n}(t)=\\Psi^{\\varepsilon}(z^{\\varepsilon,n-1},t),\\quad n\\geq1$. We show that the sequence is nondecreasing by induction. Clearly, $z_{i}^{\\varepsilon,1}(t)=\\Psi_{i}^{\\varepsilon}(z,t)=\\varepsilon_{i}+z_{i}(t)\\geq z_{i}^{\\varepsilon,0}(t)$. Suppose $z^{\\varepsilon,n}(t)\\geq z^{\\varepsilon,n-1}(t)$. Since the network is monotone, this implies $\\xi(z^{\\varepsilon,n}(t))\\leq\\xi(z^{\\varepsilon,n-1}(t))$. Hence, $z_{i}^{\\varepsilon,n+1}(t)=\\varepsilon_{i}+\\lambda_{i}\\int_{0}^{t}\\mathbf{P}\\bigg(B_{i}>\\int_{s}^{t}\\xi_{i}(z^{\\varepsilon,n}(u))d u\\bigg)d s\\geq\\varepsilon_{i}+\\lambda_{i}\\int_{0}^{t}\\mathbf{P}\\bigg(B_{i}>\\int_{s}^{t}\\xi_{i}(z^{\\varepsilon,n-1}(u))d u\\bigg)d s=z^{\\varepsilon,n}(t)$. Since $z^{\\varepsilon,n}(t)$ is nondecreasing in $n$ and bounded from above on any finite time interval, there exists a function $z^{\\varepsilon,*}(t)$ such that $\\operatorname*{lim}_{n\\to\\infty}z^{\\varepsilon,n}(t)=z^{\\varepsilon,\\ast}(t)$. Let us now show that the function $z^{\\varepsilon,*}(t)$ is continuous in $t$. Fix $h>0$. Then we have $|z^{\\varepsilon,*}(t+h)-z^{\\varepsilon,*}(t)|=\\operatorname*{lim}_{n\\to\\infty}|z^{\\varepsilon,n}(t+h)-z^{\\varepsilon,n}(t)|\\leq\\lambda_{i}\\int_{t}^{t+h}\\mathbf{P}(B_{i}>S_{i}(s,t+h))d s+\\lambda_{i}\\int_{0}^{t}\\mathbf{P}(S_{i}(s,t)<B_{i}<S_{i}(s,t+h))d s$. The first term is bounded from above by $\\lambda_{i}h$. Consider now the second term. Since the flow size distribution has a bounded density, there exists an $M\\in(0,\\infty)$ such that $f_{B_{i}}(u)\\leq M$ for all $u\\geq0$, $i=1,\\ldots,I$. Hence, $\\int_{0}^{t}\\mathbf{P}(S_{i}(s,t)<B_{i}<S_{i}(s,t+h))d s=\\int_{0}^{t}\\int_{S_{i}(s,t)}^{S_{i}(s,t+h)}f_{B_{i}}(u)d u d s\\leq M\\int_{0}^{t}\\left(S_{i}(s,t+h)-S_{i}(s,t)\\right)d s=M S_{i}(t,t+h)t$. From $z_{i}^{\\varepsilon,*}(t)\\geq\\varepsilon_{i}$, by monotonicity we derive $\\xi_{i}(z^{\\varepsilon,*}(t))\\leq C_{i}^{\\operatorname*{min}}/\\varepsilon_{i},C_{i}^{\\operatorname*{min}}=\\operatorname*{min}\\{C_{j}\\colon A_{j i}>0\\}$. Consequently, $\\int_{0}^{t}\\mathbf{P}(S_{i}(s,t)<B_{i}<S_{i}(s,t+h))\\mathop{d s}\\leq\\frac{M C_{i}^{\\operatorname*{min}}}{\\varepsilon_{i}}h t$. Thus, as $h\\rightarrow0$, the right-hand side of (32) tends to zero, yielding continuity of the function $z^{\\varepsilon,*}(t)$. We now show that $z^{\\varepsilon,*}(t)$ satisfies Equations (9). Since the sequence $z^{\\varepsilon,n}(t)$ is bounded away from zero and is nondecreasing in $n$, $\\xi(z^{\\varepsilon,n}(t))$ is continuous in $t$ and nonincreasing in $n$. Then, $S_{i}^{\\varepsilon,n}(s,t)\\to S_{i}^{\\varepsilon,*}(s,t)$ by monotone convergence. Hence, $\\Psi^{\\varepsilon}(z^{\\varepsilon,n},t)\\to\\Psi^{\\varepsilon}(z^{\\varepsilon,*},t)$, implying $z^{\\varepsilon,*}(t)=\\Psi^{\\varepsilon}(z^{\\varepsilon,*},t)$. Now since the function $z^{\\varepsilon,*}(t)$ is continuous and satisfies the fluid-model Equation (9), Theorem 2 yields $z^{\\varepsilon,*}(t)\\equiv z^{\\varepsilon}(t)$, a unique solution of Equation (9). Since the sequence $z^{\\varepsilon,n}(t)$ is nondecreasing and for all $n$ $z^{\\varepsilon,n}(t)\\geq z(t)$, we deduce that for all $t>0$ $z(t)\\leq z^{\\varepsilon}(t)$.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-146", "question": "In the context of the Court Scheduling Problem (CSP), which of the following constraints is NOT typically shared with round-robin tournament scheduling?", "options": ["A) Location constraints for participants", "B) Avoid-date constraints for scheduling events", "C) Requirement for each judge to sit with every other judge at least once in a three-judge panel", "D) One-to-one correspondence between teams and home locations"], "gold_answer": "D", "source_files": ["The Court of Appeals of Virginia Uses Integer Prog.md"], "question_context": "The CSP shares similarities with round-robin tournament sports scheduling, including location constraints and avoid-date constraints. However, it differs in several key aspects: (1) court sessions include either three or at least eight judges, whereas tournament games consist of two teams; (2) multiple judges can be from the same home district, whereas tournaments assume a one-to-one correspondence between teams and home locations; and (3) a session may occur in a district even if no judge from that district is participating, whereas a tournament usually specifies a home team and a visiting team for each match.", "question_context_used": "The CSP shares similarities with round-robin tournament sports scheduling, including location constraints and avoid-date constraints. However, it differs in several key aspects: (1) court sessions include either three or at least eight judges, whereas tournament games consist of two teams; (2) multiple judges can be from the same home district, whereas tournaments assume a one-to-one correspondence between teams and home locations; and (3) a session may occur in a district even if no judge from that district is participating, whereas a tournament usually specifies a home team and a visiting team for each match.", "gold_answer_parsed": "D", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-1032", "question": "In the context of the nonlinear generalized assignment problem (NGAP) with chance constraints, what is the condition for a solution to be feasible?", "options": ["A) The sum of all capacity slacks is non-negative", "B) The sum of all capacity slacks is zero", "C) The sum of all capacity slacks is positive", "D) The sum of all capacity slacks is minimized"], "gold_answer": "A", "source_files": ["Territory Planning and Vehicle Dispatching with Dr.md"], "question_context": "The capacity slack of core area $k$ is defined as $s_{k}=Q_{k}-\\sum_{i}\\rho_{i k}T_{i}\\mu_{i}I_{i k}-\\tau\\sqrt{\\sum_{i}(\\rho_{i k}T_{i}\\sigma_{i}I_{i k})^{2}}-\\sum_{i}\\rho_{i k}C_{i k}I_{i k}$. A solution is feasible only if all $s_{k}\\geq0$. The measure of infeasibility $v(x)$ is defined as the absolute value of the sum of all negative capacity slacks, i.e., $v(x)=|\\sum_{k=1}^{m}\\min(s_{k},0)|$. Therefore, solution $x$ is feasible only if $v(x)=0$.", "question_context_used": "The capacity slack of core area $k$ is defined as $s_{k}=Q_{k}-\\sum_{i}\\rho_{i k}T_{i}\\mu_{i}I_{i k}-\\tau\\sqrt{\\sum_{i}(\\rho_{i k}T_{i}\\sigma_{i}I_{i k})^{2}}-\\sum_{i}\\rho_{i k}C_{i k}I_{i k}$. A solution is feasible only if all $s_{k}\\geq0$. The measure of infeasibility $v(x)$ is defined as the absolute value of the sum of all negative capacity slacks, i.e., $v(x)=|\\sum_{k=1}^{m}\\min(s_{k},0)|$. Therefore, solution $x$ is feasible only if $v(x)=0$.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-545", "question": "According to Theorem 11 and the surrounding discussion, what is the implication of a bin packing instance $(s,b)$ having a large vertex distance $\\mathrm{Dist}(b)$?", "options": ["A) The instance will always satisfy the integer roundup property (IRUP).", "B) The instance will have many subinstances where the IRUP does not hold.", "C) The instance will have a unique optimal solution with small multiplicities.", "D) The instance will violate the modified integer roundup property (MIRUP)."], "gold_answer": "B", "source_files": ["About the Structure of the Integer Cone and Its Ap.md"], "question_context": "The text discusses the connection between the vertex distance $\\mathrm{Dist}(b)$ and the integer roundup property (IRUP) in bin packing problems. Specifically, it states that a bin packing instance $(s,b)$ with a large vertex distance implies the existence of many subinstances where the IRUP does not hold. Theorem 11 formalizes this by showing that for every $q\\in(\\mathcal{P}\\cap\\mathbb{Z}^{d})\\setminus V_{I}$ with $\\lambda_{q}=d+Z$ for some $Z\\in\\mathbb{Z}_{\\geq0}$, there exist at least $Z$ instances where the IRUP does not hold. The conclusion is that instances with large vertex distances lead to solutions with large multiplicities, resulting in many subinstances violating IRUP.", "question_context_used": "The text discusses the connection between the vertex distance $\\mathrm{Dist}(b)$ and the integer roundup property (IRUP) in bin packing problems. Specifically, it states that a bin packing instance $(s,b)$ with a large vertex distance implies the existence of many subinstances where the IRUP does not hold. Theorem 11 formalizes this by showing that for every $q\\in(\\mathcal{P}\\cap\\mathbb{Z}^{d})\\setminus V_{I}$ with $\\lambda_{q}=d+Z$ for some $Z\\in\\mathbb{Z}_{\\geq0}$, there exist at least $Z$ instances where the IRUP does not hold. The conclusion is that instances with large vertex distances lead to solutions with large multiplicities, resulting in many subinstances violating IRUP.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-2936", "question": "According to the principles for successful LP modeling in civil engineering applications, which of the following is NOT a recommended practice?", "options": ["A) Making the model as simple as possible to ensure managerial understanding and adaptability", "B) Quantifying savings against a benchmark figure in the first application to demonstrate utility", "C) Using the initial optimal solution as the definitive end result without further managerial input", "D) Incorporating the model from the preliminary estimate stage and modifying it in subsequent planning stages"], "gold_answer": "C", "source_files": ["From the Shadows_ Earthmoving on Construction Proj.md"], "question_context": "The text outlines key rules for successful LP modeling in civil engineering: 1) The model must be simple for managerial understanding and adaptability. 2) Savings must be quantified against a benchmark in the first application. 3) The initial optimal solution should not be the end result; shadow prices should supplement managerial intuition. 4) The model should be set up at the preliminary estimate stage and modified in subsequent stages. The incorrect option (C) contradicts rule 3, which emphasizes that the model's output should assist, not replace, the manager's final decision.", "question_context_used": "The text outlines key rules for successful LP modeling in civil engineering: 1) The model must be simple for managerial understanding and adaptability. 2) Savings must be quantified against a benchmark in the first application. 3) The initial optimal solution should not be the end result; shadow prices should supplement managerial intuition. 4) The model should be set up at the preliminary estimate stage and modified in subsequent stages. The incorrect option (C) contradicts rule 3, which emphasizes that the model's output should assist, not replace, the manager's final decision.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-4306", "question": "In the context of deterministic equivalents for chance constraints, what is the correct interpretation of the term $\\Phi^{-1}(\\beta_{r s})$ in the deterministic equivalent constraint $\\sum_{j\\epsilon D_{r}s}k_{j}x_{j}\\ge\\mu_{r s}+\\sigma_{r s}\\Phi^{-1}(\\beta_{r s})$?", "options": ["A) The inverse cumulative distribution function of the standard normal distribution evaluated at the probability level $\\beta_{r s}$", "B) The mean of the demand distribution for link $(r,s)$", "C) The variance of the demand distribution for link $(r,s)$", "D) The capacity of the unit train on descriptive route $j$"], "gold_answer": "A", "source_files": ["Regulatory Models for Pricing and Evaluation of Tr.md"], "question_context": "The deterministic equivalent for a chance constraint is derived under the assumption that the demand $d_{r s}$ has a normal distribution. The constraint ensures that the demand is met with a probability of at least $\\beta_{r s}$. The term $\\Phi^{-1}(\\beta_{r s})$ represents the fractile function for the standardized normal distribution, where $\\mu_{r s}$ is the mean and $\\sigma_{r s}^{2}$ is the variance of the demand distribution. The standard deviation $\\sigma_{r s}$ is multiplied by this fractile and added to the mean to determine the required capacity.", "question_context_used": "The deterministic equivalent for a chance constraint is derived under the assumption that the demand $d_{r s}$ has a normal distribution. The constraint ensures that the demand is met with a probability of at least $\\beta_{r s}$. The term $\\Phi^{-1}(\\beta_{r s})$ represents the fractile function for the standardized normal distribution, where $\\mu_{r s}$ is the mean and $\\sigma_{r s}^{2}$ is the variance of the demand distribution. The standard deviation $\\sigma_{r s}$ is multiplied by this fractile and added to the mean to determine the required capacity.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-536", "question": "Based on the cost-benefit analysis of HIV screening policies, which policy is economically justified for high-risk women when considering only direct costs and savings in medical care for HIV-infected infants?", "options": ["A) One-time screen of all women (Policy 1)", "B) One-time screen of high-risk women (Policy 2)", "C) Ongoing screening of all pregnant women (Policy 3)", "D) Ongoing screening of high-risk pregnant women (Policy 4)"], "gold_answer": "B", "source_files": ["A Policy Model of Human Immunodeficiency Virus Scr.md"], "question_context": "The text evaluates four HIV screening policies using a cost-benefit framework. The direct cost of screening tests is estimated at $20 per test, and the savings in care for HIV-infected newborns is estimated at $90,000 per case. Policies targeting high-risk women (Policies 2 and 4) are economically justified, as the savings in medical care offset the screening costs, even with small behavioral changes. In contrast, Policies 1 and 3, which screen all women, are not economically viable due to low infection prevalence in the general population. The analysis focuses on direct costs and savings, excluding indirect costs like counseling or productivity loss.", "question_context_used": "The text evaluates four HIV screening policies using a cost-benefit framework. The direct cost of screening tests is estimated at $20 per test, and the savings in care for HIV-infected newborns is estimated at $90,000 per case. Policies targeting high-risk women (Policies 2 and 4) are economically justified, as the savings in medical care offset the screening costs, even with small behavioral changes. In contrast, Policies 1 and 3, which screen all women, are not economically viable due to low infection prevalence in the general population. The analysis focuses on direct costs and savings, excluding indirect costs like counseling or productivity loss.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-680", "question": "According to the text, what is the primary purpose of INFORMS as an organization?", "options": ["A) To enforce copyright laws on academic publications", "B) To provide networking and learning opportunities for OR and analytics professionals", "C) To develop psychosociological techniques for mass transit", "D) To guarantee the accuracy and completeness of published articles"], "gold_answer": "B", "source_files": ["Psychosociological Techniques for Mass Transit Dev.md"], "question_context": "With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes.", "question_context_used": "With 12,500 members from nearly 90 countries, INFORMS is the largest international association of operations research (O.R.) and analytics professionals and students. INFORMS provides unique networking and learning opportunities for individual professionals, and organizations of all types and sizes, to better understand and use O.R. and analytics tools and methods to transform strategic visions and achieve better outcomes.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-2424", "question": "Given the definitions of outer and inner ergodic separation of transition matrices, which of the following statements is true?", "options": ["A) The inner separation of two matrices is always greater than their outer separation.", "B) The inner separation of a matrix with itself is 0 if the matrix is unichain, and 1.0 otherwise.", "C) The outer separation of two matrices is defined as the inner separation of their associated stationary matrices.", "D) The inner and outer separations of two stationary matrices are always different."], "gold_answer": "B", "source_files": ["Discounting, Ergodicity and Convergence for Markov.md"], "question_context": "The inner separation is always less than or equal to the outer separation, and the inner and outer separations of two stationary matrices are equal. The inner separation of a matrix with itself is 0 if the matrix is unichain, 1.0 otherwise. The outer ergodic separation of two transition matrices $\\boldsymbol{P}_{1}$ and ${\\pmb P}_{2}$ is defined by $d^{U}(P_{1},P_{2})=1-\\sum_{j=1}^{N}\\operatorname*{min}_{i_{1},i_{2}}\\{P_{1}(i_{1},j),P_{2}(i_{2},j)\\}$, where $P(i,j)$ represents the transition probability from state $i$ to state $j$. The inner ergodic separation of two transition matrices is defined as the outer ergodic separation of their associated stationary matrices; that is, $d^{L}(P_{1},P_{2})=d^{U}(S_{1},S_{2})$, where $S_{i}\\equiv\\operatorname*{lim}_{n\\to\\infty}1/n{\\sum_{k=0}^{n}}(P_{i})^{k}$.", "question_context_used": "The inner separation is always less than or equal to the outer separation, and the inner and outer separations of two stationary matrices are equal. The inner separation of a matrix with itself is 0 if the matrix is unichain, 1.0 otherwise. The outer ergodic separation of two transition matrices $\\boldsymbol{P}_{1}$ and ${\\pmb P}_{2}$ is defined by $d^{U}(P_{1},P_{2})=1-\\sum_{j=1}^{N}\\operatorname*{min}_{i_{1},i_{2}}\\{P_{1}(i_{1},j),P_{2}(i_{2},j)\\}$, where $P(i,j)$ represents the transition probability from state $i$ to state $j$. The inner ergodic separation of two transition matrices is defined as the outer ergodic separation of their associated stationary matrices; that is, $d^{L}(P_{1},P_{2})=d^{U}(S_{1},S_{2})$, where $S_{i}\\equiv\\operatorname*{lim}_{n\\to\\infty}1/n{\\sum_{k=0}^{n}}(P_{i})^{k}$.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-2682", "question": "Which of the following roles did Robert Herman NOT hold in the field of Transportation Science?", "options": ["A) Founder of the Transportation Science section", "B) First Chairman of the Transportation Science section", "C) Editor-in-Chief of Transportation Science journal", "D) Past president of the RSA"], "gold_answer": "C", "source_files": ["Foreword to the Focused Issue on Traffic Theory (i.md"], "question_context": "Robert Herman (1914-1997) was a past president of RSA, the founder and past editor of Transportation Science, and the founder and the first Chairman of the Transportation Science section. A tribute to him appeared in Transportation Science (Vol. 31, No. 2, May 1997) shortly after his passing. This focused issue commemorates his life with contributions from friends, colleagues, and his graduate student, SIAMAK A. ARDEKANI, the Guest Editor from The University of Texas at Arlington.", "question_context_used": "Robert Herman (1914-1997) was a past president of RSA, the founder and past editor of Transportation Science, and the founder and the first Chairman of the Transportation Science section. A tribute to him appeared in Transportation Science (Vol. 31, No. 2, May 1997) shortly after his passing. This focused issue commemorates his life with contributions from friends, colleagues, and his graduate student, SIAMAK A. ARDEKANI, the Guest Editor from The University of Texas at Arlington.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-850", "question": "Given the described process of handling RFQs (Request for Quotations) in the engineering section, what is the primary reason for the delay in processing quotes?", "options": ["A) The distance between the sales and engineering sections", "B) The timing of the company mail service pickups", "C) The need for manual coding and ledger recording by a secretary", "D) The use of a typing pool for outgoing letters"], "gold_answer": "B", "source_files": ["The Fifth Column—On Doing Operations Research in t.md"], "question_context": "The text describes a process where RFQs are sent from sales to engineering via the company mail service, which picks up mail at 11:00 AM and 3:00 PM. A quote from the morning meeting is not picked up until the afternoon mail, arriving in engineering after 4:00 PM, likely not being looked at until the next day. The same delay occurs on the return trip. Additionally, there are 'coordination' steps where quotes are coded and recorded in a ledger before being handed to a meeting, further delaying the process. The probability that a meeting will see all quotes that come in on a given morning is given as $\\ensuremath{\\boldsymbol{p}}=0$. The text also mentions the inefficiency of using a typing pool for outgoing letters.", "question_context_used": "The text describes a process where RFQs are sent from sales to engineering via the company mail service, which picks up mail at 11:00 AM and 3:00 PM. A quote from the morning meeting is not picked up until the afternoon mail, arriving in engineering after 4:00 PM, likely not being looked at until the next day. The same delay occurs on the return trip. Additionally, there are 'coordination' steps where quotes are coded and recorded in a ledger before being handed to a meeting, further delaying the process. The probability that a meeting will see all quotes that come in on a given morning is given as $\\ensuremath{\\boldsymbol{p}}=0$. The text also mentions the inefficiency of using a typing pool for outgoing letters.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-3341", "question": "In the context of Algorithm Construct-Schedule, what is the primary condition that ensures the migrated load is bounded by $|\\Delta M| \\cdot (2 + \\rho + \\varepsilon) \\cdot A$?", "options": ["A) The stable average $A' \\leq 2A$ and $(1+\\varepsilon)^u \\leq \\rho \\cdot A$ for some constant $\\rho$.", "B) The number of machines $|\\Delta M|$ is minimized by permuting machines in $M \\setminus M_H$.", "C) The algorithm enters either step (vi) or step (vii), but not both.", "D) The solution $S'$ $I$-follows $S_{N'}$ and no $I$-huge job is migrated."], "gold_answer": "A", "source_files": ["Robust Polynomial-Time Approximation Schemes for P.md"], "question_context": "The migrated load in Algorithm Construct-Schedule is bounded under specific conditions. Lemma 3.14 states that for a valid input where $A' \\leq 2A$ and $(1+\\varepsilon)^u \\leq \\rho \\cdot A$ for some constant $\\rho$, the total processing time of jobs migrated is at most $|\\Delta M| \\cdot (2 + \\rho + \\varepsilon) \\cdot A$. This bound is derived by ensuring no $I$-huge jobs are migrated and leveraging the properties of locally optimal solutions. The inequality $$\\ell_i(S') \\leq \\ell_{\\min}(S_{N'}) + p_{\\min} + (1+\\varepsilon)^\\ell \\leq A' + \\rho A + \\varepsilon A \\leq (2 + \\rho + \\varepsilon)A$$ plays a key role in this analysis.", "question_context_used": "The migrated load in Algorithm Construct-Schedule is bounded under specific conditions. Lemma 3.14 states that for a valid input where $A' \\leq 2A$ and $(1+\\varepsilon)^u \\leq \\rho \\cdot A$ for some constant $\\rho$, the total processing time of jobs migrated is at most $|\\Delta M| \\cdot (2 + \\rho + \\varepsilon) \\cdot A$. This bound is derived by ensuring no $I$-huge jobs are migrated and leveraging the properties of locally optimal solutions. The inequality $$\\ell_i(S') \\leq \\ell_{\\min}(S_{N'}) + p_{\\min} + (1+\\varepsilon)^\\ell \\leq A' + \\rho A + \\varepsilon A \\leq (2 + \\rho + \\varepsilon)A$$ plays a key role in this analysis.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-2901", "question": "In the context of the dynamic programming model for bond underwriting, what was the key finding when comparing its solution to the optimal solution obtained via integer programming for the second test problem?", "options": ["A) The dynamic programming solution was identical to the integer programming solution.", "B) The dynamic programming solution was inferior by \\$2.50 out of \\$1.5 million.", "C) The dynamic programming solution was superior by \\$71,500.", "D) The dynamic programming solution could not be compared due to missing data."], "gold_answer": "B", "source_files": ["Municipal Bond Coupon Schedules with Limitations o.md"], "question_context": "The dynamic programming model was tested against an integer programming solution for bond underwriting. In the second test problem, the integer programming solution differed from the dynamic programming solution in three coupons and was inferior by \\$2.504 out of \\$1.5 million, corresponding to a net interest cost difference of 6.532437% vs. 6.532441%. This small difference could translate to a potential \\$71,500 for the winning syndicate, based on the actual numbers used by the underwriters. The integer programming code could only handle two of the original test problems, and the dynamic programming model matched the integer programming solution in the first test case but showed a slight deviation in the second.", "question_context_used": "The dynamic programming model was tested against an integer programming solution for bond underwriting. In the second test problem, the integer programming solution differed from the dynamic programming solution in three coupons and was inferior by \\$2.504 out of \\$1.5 million, corresponding to a net interest cost difference of 6.532437% vs. 6.532441%. This small difference could translate to a potential \\$71,500 for the winning syndicate, based on the actual numbers used by the underwriters. The integer programming code could only handle two of the original test problems, and the dynamic programming model matched the integer programming solution in the first test case but showed a slight deviation in the second.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-715", "question": "In the context of Lemma 1, what is the expected value of the sum involving the martingale property of $S^{0}$ and the requirement $\\sum_{k=1}^{N}\\xi_{k}=-X_{0}$?", "options": ["A) $-X_{0}^{\\top}S_{0}^{0}$", "B) $\\mathbb{E}\\bigg[\\sum_{k=1}^{N}\\xi_{k}^{\\top}S_{t_{k}}^{0}\\bigg]$", "C) $\\mathbb{E}\\bigg[\\sum_{k=1}^{N}\\xi_{k}^{\\top}S_{T}^{0}\\bigg]$", "D) $0$"], "gold_answer": "A", "source_files": ["Multivariate Transient Price Impact and Matrix-Val.md"], "question_context": "From the martingale property of $S^{0}$ and the requirement that $\\sum_{k=1}^{N}\\xi_{k}=-X_{0}$ we obtain that $$\\mathbb{E}\\bigg[\\sum_{k=1}^{N}\\xi_{k}^{\\top}S_{t_{k}}^{0}\\bigg]=\\mathbb{E}\\bigg[\\sum_{k=1}^{N}\\xi_{k}^{\\top}S_{T}^{0}\\bigg]=-X_{0}^{\\top}S_{0}^{0}.$$ Furthermore, $$\\begin{array}{r l r}{\\lefteqn{\\frac{1}{2}\\sum_{k=1}^{N}\\xi_{k}^{\\top}G(0)\\xi_{k}+\\sum_{k=1}^{N}\\sum_{\\ell=1}^{k-1}\\xi_{k}^{\\top}G(t_{k}-t_{\\ell})\\xi_{\\ell}}}\\\\ &{}&{=\\frac{1}{2}\\sum_{k=1}^{N}\\xi_{k}^{\\top}\\tilde{G}(0)\\xi_{k}+\\frac{1}{2}\\sum_{k=1}^{N}\\sum_{\\ell=1}^{k-1}\\xi_{k}^{\\top}\\tilde{G}(t_{k}-t_{\\ell})\\xi_{\\ell}+\\frac{1}{2}\\sum_{k=1}^{N}\\sum_{\\ell=1}^{k-1}\\xi_{\\ell}^{\\top}\\tilde{G}(t_{\\ell}-t_{k})\\xi_{k}=C_{\\mathbb{T}}(\\xi).}\\end{array}$$ This proves (4).", "question_context_used": "From the martingale property of $S^{0}$ and the requirement that $\\sum_{k=1}^{N}\\xi_{k}=-X_{0}$ we obtain that $$\\mathbb{E}\\bigg[\\sum_{k=1}^{N}\\xi_{k}^{\\top}S_{t_{k}}^{0}\\bigg]=\\mathbb{E}\\bigg[\\sum_{k=1}^{N}\\xi_{k}^{\\top}S_{T}^{0}\\bigg]=-X_{0}^{\\top}S_{0}^{0}.$$ Furthermore, $$\\begin{array}{r l r}{\\lefteqn{\\frac{1}{2}\\sum_{k=1}^{N}\\xi_{k}^{\\top}G(0)\\xi_{k}+\\sum_{k=1}^{N}\\sum_{\\ell=1}^{k-1}\\xi_{k}^{\\top}G(t_{k}-t_{\\ell})\\xi_{\\ell}}}\\\\ &{}&{=\\frac{1}{2}\\sum_{k=1}^{N}\\xi_{k}^{\\top}\\tilde{G}(0)\\xi_{k}+\\frac{1}{2}\\sum_{k=1}^{N}\\sum_{\\ell=1}^{k-1}\\xi_{k}^{\\top}\\tilde{G}(t_{k}-t_{\\ell})\\xi_{\\ell}+\\frac{1}{2}\\sum_{k=1}^{N}\\sum_{\\ell=1}^{k-1}\\xi_{\\ell}^{\\top}\\tilde{G}(t_{\\ell}-t_{k})\\xi_{k}=C_{\\mathbb{T}}(\\xi).}\\end{array}$$ This proves (4).", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-2510", "question": "In the context of the MEUE algorithm, what is the primary motivation for choosing a topological order that minimizes the size of the set D?", "options": ["A) To reduce the computational effort by minimizing the number of global representatives", "B) To ensure the set D is a compact representation of the structure of alternatives in R^0", "C) To maximize the number of linearly independent vectors in D", "D) To simplify the Gauss-Jordan elimination process for identifying core DSPRs"], "gold_answer": "B", "source_files": ["Primal Method for Determining the Most Likely Rout.md"], "question_context": "The computational effort of the proposed MEUE algorithm presented in §5 depends mainly on the number of global representatives, |D|/2, and the number of basic core DSPRs, |W^0|=rank(W)=|D^+|−rank(D^+)=|D|/2−rank(D), where rank(D) is the maximal number of linearly independent vectors in D (or in D^+). As shown there, rank(D) depends on R^0, but it is independent of the choice of T^0; hence, minimizing |W^0| is equivalent to minimizing |D| as well. Another motivation to choose T^0 that minimizes |D| is that the set D may be considered as a compact representation of the structure of alternatives in R^0 in the following sense. If R^1⊆R^0 contains at least one route for each OD pair and at least one route pair [r,r′] such that K_r−K_r′=d for every d∈D, then any completely consistent set of routes that contains R^1 must contain the original set of routes R^0 as well.", "question_context_used": "The computational effort of the proposed MEUE algorithm presented in §5 depends mainly on the number of global representatives, |D|/2, and the number of basic core DSPRs, |W^0|=rank(W)=|D^+|−rank(D^+)=|D|/2−rank(D), where rank(D) is the maximal number of linearly independent vectors in D (or in D^+). As shown there, rank(D) depends on R^0, but it is independent of the choice of T^0; hence, minimizing |W^0| is equivalent to minimizing |D| as well. Another motivation to choose T^0 that minimizes |D| is that the set D may be considered as a compact representation of the structure of alternatives in R^0 in the following sense. If R^1⊆R^0 contains at least one route for each OD pair and at least one route pair [r,r′] such that K_r−K_r′=d for every d∈D, then any completely consistent set of routes that contains R^1 must contain the original set of routes R^0 as well.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-1779", "question": "In the single-buyer case described in Theorem 1, what is the necessary and sufficient condition for a mechanism to be incentive-compatible, satisfy agent sovereignty, and local side-flatness?", "options": ["A) The mechanism must have a fixed price for all objects regardless of the buyer's valuation.", "B) The mechanism must use threshold functions that depend on the buyer's bids for other objects.", "C) The mechanism must have threshold functions independent of the buyer's bids for other objects and charge the sum of thresholds for allocated objects.", "D) The mechanism must allow the buyer to influence the price of one object by changing her bid for another object."], "gold_answer": "C", "source_files": ["Strategy-Proof Multidimensional Mechanism Design.md"], "question_context": "Theorem 1 characterizes the class of incentive-compatible mechanisms that satisfy agent sovereignty and local side-flatness in the single-buyer case. It states that a mechanism is incentive-compatible if and only if for any valuation profile, there exist positive real numbers {T^k} such that the allocation rule is threshold-based and the payment is the sum of thresholds for allocated objects. The proof shows that the threshold functions must be independent of the buyer's bids for other objects to prevent manipulation of prices. The mechanism must also satisfy local side-flatness, ensuring that small changes in bids do not affect the allocation of other objects.", "question_context_used": "Theorem 1 characterizes the class of incentive-compatible mechanisms that satisfy agent sovereignty and local side-flatness in the single-buyer case. It states that a mechanism is incentive-compatible if and only if for any valuation profile, there exist positive real numbers {T^k} such that the allocation rule is threshold-based and the payment is the sum of thresholds for allocated objects. The proof shows that the threshold functions must be independent of the buyer's bids for other objects to prevent manipulation of prices. The mechanism must also satisfy local side-flatness, ensuring that small changes in bids do not affect the allocation of other objects.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-3146", "question": "In the context of refinement derivatives for transferable utility (TU) games, what is the condition for the existence of the outer derivative $D^{+}\\nu(A)(\\cdot)$ at a set $A$?", "options": ["A) The limit of the sums $\\sum_{i=1}^{n}[\\nu(A\\cup H_{i})-\\nu(A)]$ exists for every partition $\\{H_{i}\\}_{i=1}^{n}$ of $H$ disjoint from $A$ as the partitions get finer.", "B) The set function $\\nu$ must be convex and defined on a finite algebra.", "C) The measure $D^{+}\\nu(A)$ must be nonatomic and countably additive.", "D) The game $\\nu$ must have a finite carrier and be supermodular."], "gold_answer": "A", "source_files": ["Refinement Derivatives and Values of Games.md"], "question_context": "The outer derivative $D^{+}\\nu(A)(\\cdot)$ of a set function $\\nu$ at a set $A$ is defined through a refinement process of partitions. Formally, fix a set $A$ in $\\mathcal{A}$ and consider a generic finite partition $\\{H_{i}\\}_{i=1}^{n}$ of a set $H$ disjoint from $A$. The behavior of the sums $\\sum_{i=1}^{n}[\\nu(A\\cup H_{i})-\\nu(A)]$ is analyzed as the partitions get finer. If this refinement process has a limit for every $H$, such a limit is an additive set function $D^{+}\\nu(A)(\\cdot)$ defined over the subsets $H$ of $A^{c}$, which is regarded as the outer derivative of $\\nu$ at $A$. This construction is closely related to the definitions proposed by Rosenmuller, Epstein, and Epstein and Marinacci, but without the constraint of nonatomicity.", "question_context_used": "The outer derivative $D^{+}\\nu(A)(\\cdot)$ of a set function $\\nu$ at a set $A$ is defined through a refinement process of partitions. Formally, fix a set $A$ in $\\mathcal{A}$ and consider a generic finite partition $\\{H_{i}\\}_{i=1}^{n}$ of a set $H$ disjoint from $A$. The behavior of the sums $\\sum_{i=1}^{n}[\\nu(A\\cup H_{i})-\\nu(A)]$ is analyzed as the partitions get finer. If this refinement process has a limit for every $H$, such a limit is an additive set function $D^{+}\\nu(A)(\\cdot)$ defined over the subsets $H$ of $A^{c}$, which is regarded as the outer derivative of $\\nu$ at $A$. This construction is closely related to the definitions proposed by Rosenmuller, Epstein, and Epstein and Marinacci, but without the constraint of nonatomicity.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-2705", "question": "What was the primary challenge faced by SWBT when transitioning from simple to complex Maximum Likelihood Estimation (MLE) models in their modeling environment?", "options": ["A) The need to integrate models from different paradigms and handle intricate ad hoc programming modifications", "B) The requirement for analysts to develop explicit objective functions and work closely with minimization algorithms", "C) The exponential increase in data series size from hundreds to tens of thousands", "D) The necessity to incorporate intelligent systems for recommending econometric practices"], "gold_answer": "B", "source_files": ["Modeling Telecommunications Demand Analysis.md"], "question_context": "The transition from simple to complex MLE models had two significant ramifications for the modeling environment. First, from a system architecture perspective, it forced direct involvement with model integration issues, requiring intricate ad hoc programming modifications. The second ramification was that the user was no longer sheltered from the details of the optimization model. The user had to develop the objective function explicitly and work more closely with the related minimization algorithm to arrive at a meaningful solution. Optimization presents practical difficulties such as determining feasible initial values, providing functional forms for the likelihood function and its first derivatives, and troubleshooting pathological models. The net effect was that the analyst needed a broader base of expertise than typically expected in the past.", "question_context_used": "The transition from simple to complex MLE models had two significant ramifications for the modeling environment. First, from a system architecture perspective, it forced direct involvement with model integration issues, requiring intricate ad hoc programming modifications. The second ramification was that the user was no longer sheltered from the details of the optimization model. The user had to develop the objective function explicitly and work more closely with the related minimization algorithm to arrive at a meaningful solution. Optimization presents practical difficulties such as determining feasible initial values, providing functional forms for the likelihood function and its first derivatives, and troubleshooting pathological models. The net effect was that the analyst needed a broader base of expertise than typically expected in the past.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-815", "question": "In the context of the maximin formulation of the planar location problem, what is the gradient of the function \\( f_{2}(x,y) \\) when \\( x < a_{t} \\) and \\( y \\geq b_{t} \\)?", "options": ["A) \\((-v_{t}, v_{t})\\)", "B) \\((v_{t}, -v_{t})\\)", "C) \\((v_{t}, v_{t})\\)", "D) \\((-v_{t}, -v_{t})\\)"], "gold_answer": "A", "source_files": ["A Single Facility Rectilinear Location Problem wit.md"], "question_context": "The function \\( f_{2}(x,y) \\) is defined in the context of the maximin formulation of the planar location problem. The gradient of \\( f_{2}(x,y) \\) is given by:\n\n$$\\nabla f_{2}(x,y)=\\left\\{\\begin{array}{l l l}{(-v_{t},-v_{t})}&{x<a_{t}}&{\\mathrm{and}\\quad y<b_{t}}\\\\ {(v_{t},-v_{t})}&{x\\geq a_{t}}&{\\mathrm{and}\\quad y<b_{t}}\\\\ {(v_{t},v_{t})}&{x\\geq a_{t}}&{\\mathrm{and}\\quad y\\geq b_{t}}\\\\ {(-v_{t},v_{t})}&{x<a_{t}}&{\\mathrm{and}\\quad y\\geq b_{t}}\\end{array}\\right.$$\n\nThis gradient is derived from the active distance function \\( g(x,y) = U(|x-a| + |y-b|) \\), where \\( (a_t, b_t) \\) is the limiting (minimax) facility at point \\( (x,y) \\). The function \\( f_{2}(x,y) \\) is piecewise linear, and its gradient changes depending on the relative position of \\( (x,y) \\) with respect to \\( (a_t, b_t) \\).", "question_context_used": "The function \\( f_{2}(x,y) \\) is defined in the context of the maximin formulation of the planar location problem. The gradient of \\( f_{2}(x,y) \\) is given by:\n\n$$\\nabla f_{2}(x,y)=\\left\\{\\begin{array}{l l l}{(-v_{t},-v_{t})}&{x<a_{t}}&{\\mathrm{and}\\quad y<b_{t}}\\\\ {(v_{t},-v_{t})}&{x\\geq a_{t}}&{\\mathrm{and}\\quad y<b_{t}}\\\\ {(v_{t},v_{t})}&{x\\geq a_{t}}&{\\mathrm{and}\\quad y\\geq b_{t}}\\\\ {(-v_{t},v_{t})}&{x<a_{t}}&{\\mathrm{and}\\quad y\\geq b_{t}}\\end{array}\\right.$$\n\nThis gradient is derived from the active distance function \\( g(x,y) = U(|x-a| + |y-b|) \\), where \\( (a_t, b_t) \\) is the limiting (minimax) facility at point \\( (x,y) \\). The function \\( f_{2}(x,y) \\) is piecewise linear, and its gradient changes depending on the relative position of \\( (x,y) \\) with respect to \\( (a_t, b_t) \\).", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-2988", "question": "In the given linear programming problem, what is the optimal value of the objective function z after adding the constraint $X^{+}+X^{-}+X_{0}=M$?", "options": ["A) M", "B) 4", "C) 2", "D) M/2"], "gold_answer": "B", "source_files": ["Comment on a Précis by Shanno and Weil.md"], "question_context": "The transformed problem corresponding to (2) is: \n\n$$(2^{\\prime})\\cdot{\\mathrm{~Max~}}z=X^{+}+X^{-},\\qquad-X^{+}+X^{-}\\leq4,\\qquad X^{+}-X^{-}\\leq2,\\qquad X^{+},X^{-}\\geq0.$$\n\nThe use of the simplex method to the above leads to an unbounded solution indication. However, if the constraint $X^{+}+X^{-}+X_{0}=M$ is added, the tableaux corresponding to the alternate optimal basic solutions are shown. The corresponding values of $\\pmb{z}$ become 4 and 2 respectively indicating that the new first tableau corresponds to the optimal solution to $(2^{\\prime})$. Hence $X^{-}=4$, $X^{+}=\\mathbf{0}$, and the optimal solution to $(1^{\\prime})$ is $X=-4$, $z=4$.", "question_context_used": "The transformed problem corresponding to (2) is: \n\n$$(2^{\\prime})\\cdot{\\mathrm{~Max~}}z=X^{+}+X^{-},\\qquad-X^{+}+X^{-}\\leq4,\\qquad X^{+}-X^{-}\\leq2,\\qquad X^{+},X^{-}\\geq0.$$\n\nThe use of the simplex method to the above leads to an unbounded solution indication. However, if the constraint $X^{+}+X^{-}+X_{0}=M$ is added, the tableaux corresponding to the alternate optimal basic solutions are shown. The corresponding values of $\\pmb{z}$ become 4 and 2 respectively indicating that the new first tableau corresponds to the optimal solution to $(2^{\\prime})$. Hence $X^{-}=4$, $X^{+}=\\mathbf{0}$, and the optimal solution to $(1^{\\prime})$ is $X=-4$, $z=4$.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-2843", "question": "Under the conditions of Theorem 2.1, which of the following statements is equivalent to the asymptotic irrelevance of initial conditions for the Skorohod reflection mapping?", "options": ["A) The spectral radius of the routing matrix P is less than one.", "B) The limit of L_i(a+X)(t) as t approaches infinity is infinite for all a ≥ 0.", "C) The process X has stationary increments.", "D) The reflection matrix R is time-dependent."], "gold_answer": "B", "source_files": ["Asymptotic Irrelevance of Initial Conditions for S.md"], "question_context": "The paper investigates the asymptotic behavior of the Skorohod reflection mapping on the nonnegative orthant. A key result is Theorem 2.1, which establishes equivalent conditions for the asymptotic irrelevance of initial conditions. Specifically, for any given cadlag X and every i, the following conditions are equivalent: (i) lim_{t→∞}[W_i(a+X)(t) - W_i(X)(t)] = 0 for all a ≥ 0; (ii) lim_{t→∞} L_i(a+X)(t) = ∞ for some a ≥ 0; (iii) lim_{t→∞} L_i(a+X)(t) = ∞ for all a ≥ 0. The reflection mapping is defined by W(X)(t) = X(t) + R L(X)(t), where R = I - P^t and P is a nonnegative matrix with spectral radius less than one. The conditions imply that the difference between the regulated functions converges to zero as time diverges, ensuring the asymptotic irrelevance of initial conditions.", "question_context_used": "The paper investigates the asymptotic behavior of the Skorohod reflection mapping on the nonnegative orthant. A key result is Theorem 2.1, which establishes equivalent conditions for the asymptotic irrelevance of initial conditions. Specifically, for any given cadlag X and every i, the following conditions are equivalent: (i) lim_{t→∞}[W_i(a+X)(t) - W_i(X)(t)] = 0 for all a ≥ 0; (ii) lim_{t→∞} L_i(a+X)(t) = ∞ for some a ≥ 0; (iii) lim_{t→∞} L_i(a+X)(t) = ∞ for all a ≥ 0. The reflection mapping is defined by W(X)(t) = X(t) + R L(X)(t), where R = I - P^t and P is a nonnegative matrix with spectral radius less than one. The conditions imply that the difference between the regulated functions converges to zero as time diverges, ensuring the asymptotic irrelevance of initial conditions.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-1211", "question": "In the context of the Reflecting Stochastic Differential Game (RSDG) and the Multi-Server Differential Game (MSDG), what is the primary role of the function γ as defined in the text?", "options": ["A) To project the workload from the RSDG to the MSDG by filling cheaper buffers first", "B) To minimize the holding costs in the MSDG by optimizing buffer states", "C) To ensure continuity of the workload process in the RSDG", "D) To provide a mapping between the standard basis of ℝ^I and the workload levels"], "gold_answer": "B", "source_files": ["Brownian Control Problems for a Multiclass M_M_1 Q.md"], "question_context": "The function γ is defined to send any workload value x₀ to the cheapest state of the MSDG (from the holding cost perspective) among all the states whose workload levels are x₀. This is mathematically expressed as: \n\n$$ \n\\gamma(x) \\in \\arg \\min \\{\\hat{h} \\cdot \\hat{\\xi} : \\hat{\\xi} \\in \\mathcal{X}, \\theta \\cdot \\hat{\\xi} = x\\}. \n$$ \n\nBy filling up the cheaper buffers first (w.r.t. the holding costs), γ ensures that the holding costs are minimized. The curve γ(x), x ∈ [0, b], is continuous and located on the edges of 𝒳. This function is crucial for constructing optimal strategies in both the RSDG and MSDG, as it directly influences the cost structure and dynamics of the games.", "question_context_used": "The function γ is defined to send any workload value x₀ to the cheapest state of the MSDG (from the holding cost perspective) among all the states whose workload levels are x₀. This is mathematically expressed as: \n\n$$ \n\\gamma(x) \\in \\arg \\min \\{\\hat{h} \\cdot \\hat{\\xi} : \\hat{\\xi} \\in \\mathcal{X}, \\theta \\cdot \\hat{\\xi} = x\\}. \n$$ \n\nBy filling up the cheaper buffers first (w.r.t. the holding costs), γ ensures that the holding costs are minimized. The curve γ(x), x ∈ [0, b], is continuous and located on the edges of 𝒳. This function is crucial for constructing optimal strategies in both the RSDG and MSDG, as it directly influences the cost structure and dynamics of the games.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-4206", "question": "In the context of the US Open's 7-point rating system for line umpires, which of the following assignments would be considered feasible given the desirable ratings for each line umpire location?", "options": ["A) An umpire with a rating of 3 assigned to the serve line", "B) An umpire with a rating of 5 assigned to the baseline", "C) An umpire with a rating of 6 assigned to the centerline", "D) An umpire with a rating of 7 assigned to the far line"], "gold_answer": "C", "source_files": ["Scheduling Umpire Crews for Professional Tennis To.md"], "question_context": "The US Open uses a 7-point rating system for line umpires (Table 1). The skill levels required run from highest to lowest for the following positions: serve line, baseline, far line, centerline, and near line. Overlaps in skill levels between court locations provide flexibility for assigning umpires, allowing slightly under- or overqualified umpires to call matches. Table 1 shows the desirable ratings for each line umpire location: Serve line (1,2), Baseline (3,4), Far line (4,5), Centerline (5,6), Near line (6,7).", "question_context_used": "The US Open uses a 7-point rating system for line umpires (Table 1). The skill levels required run from highest to lowest for the following positions: serve line, baseline, far line, centerline, and near line. Overlaps in skill levels between court locations provide flexibility for assigning umpires, allowing slightly under- or overqualified umpires to call matches. Table 1 shows the desirable ratings for each line umpire location: Serve line (1,2), Baseline (3,4), Far line (4,5), Centerline (5,6), Near line (6,7).", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3739", "question": "In the context of Solomon's test set for the Vehicle Routing Problem with Time Windows (VRPTW), which problem type is characterized by clustered customer locations and a narrow time window at the central depot?", "options": ["A) Type R1", "B) Type C1", "C) Type RC1", "D) Type R2"], "gold_answer": "B", "source_files": ["A Tabu Search Heuristic for the Vehicle Routing Pr.md"], "question_context": "Six different sets of problems are defined in Solomon's test set, namely C1, C2, R1, R2, RC1, and RC2. The customers are uniformly distributed in the problems of type R and clustered in groups in the problems of type C. These characteristics are mixed in the problems of type RC. Furthermore, the time window is narrow at the central depot for the problems of type 1, so that only a few customers can be serviced on each route. Conversely, this time window is large for the problems of type 2, so that many customers can be serviced on the same route. Finally, a fixed service time is found at each customer location (i.e., time for unloading the goods). This service time is set at 10 time units per customer for the problems of type R and RC, and 90 time units per customer for the problems of type C.", "question_context_used": "Six different sets of problems are defined in Solomon's test set, namely C1, C2, R1, R2, RC1, and RC2. The customers are uniformly distributed in the problems of type R and clustered in groups in the problems of type C. These characteristics are mixed in the problems of type RC. Furthermore, the time window is narrow at the central depot for the problems of type 1, so that only a few customers can be serviced on each route. Conversely, this time window is large for the problems of type 2, so that many customers can be serviced on the same route. Finally, a fixed service time is found at each customer location (i.e., time for unloading the goods). This service time is set at 10 time units per customer for the problems of type R and RC, and 90 time units per customer for the problems of type C.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3986", "question": "Under the conditions of first-degree stochastic dominance (FSD), if an investor prefers asset A over asset B in an either-or choice situation, what can be inferred about the composition of their optimal portfolio when both assets are available?", "options": ["A) The investor will always hold more of asset A than asset B in the optimal portfolio.", "B) The investor may hold any combination of assets A and B, as FSD does not impose restrictions on the portfolio composition.", "C) The investor will hold only asset A in the optimal portfolio, completely excluding asset B.", "D) The investor will hold assets A and B in equal proportions in the optimal portfolio."], "gold_answer": "B", "source_files": ["Note—A Note on First-Degree Stochastic Dominance a.md"], "question_context": "The concept of preference over marginal distributions has been studied by various authors; a convenient reference is Hadar and Russell [2]. In particular, if we compare two probability density functions, $f(z)$ and $g(z)$, defined on the same random variable $z$ over a closed bounded interval, $g(z)$ is said to exhibit first-degree stochastic dominance (FSD) over $f(z)$ if the cumulative distribution function of $g(z)$ is less than or equal to the cumulative distribution function of $f(z)$ for every point $z$ in the domain. Hadar and Russell show in their paper that FSD is equivalent to preference in an either-or choice situation for all investors possessing bounded and strictly increasing utility functions with a continuous first derivative on the domain of $z$. In this note, we study special cases of FSD to see whether preference over the marginal distributions of risky assets can yield qualitative restrictions on the portfolios investors will hold when both assets are available.", "question_context_used": "The concept of preference over marginal distributions has been studied by various authors; a convenient reference is Hadar and Russell [2]. In particular, if we compare two probability density functions, $f(z)$ and $g(z)$, defined on the same random variable $z$ over a closed bounded interval, $g(z)$ is said to exhibit first-degree stochastic dominance (FSD) over $f(z)$ if the cumulative distribution function of $g(z)$ is less than or equal to the cumulative distribution function of $f(z)$ for every point $z$ in the domain. Hadar and Russell show in their paper that FSD is equivalent to preference in an either-or choice situation for all investors possessing bounded and strictly increasing utility functions with a continuous first derivative on the domain of $z$. In this note, we study special cases of FSD to see whether preference over the marginal distributions of risky assets can yield qualitative restrictions on the portfolios investors will hold when both assets are available.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-3942", "question": "In the numerical comparison of constraint-dissolving approaches and Riemannian optimization solvers for the nearest low-rank correlation matrix problem, which solver demonstrates superior performance when the column size \\( s \\) is relatively large?", "options": ["A) CG solver from the SciPy package", "B) RCG solver from the PyManopt package", "C) Trust-ncg solver", "D) L-BFGS-B solver"], "gold_answer": "C", "source_files": ["Dissolving Constraints for Riemannian Optimization.md"], "question_context": "The study compares the numerical performance of constraint-dissolving approaches with state-of-the-art Riemannian optimization solvers for the nearest low-rank correlation matrix problem. The problem is formulated as an optimization problem over the oblique manifold: \n\n$$\n\\begin{array}{r l}{\\underset{X\\in\\mathbb{R}^{m\\times s}}{\\mathrm{min}}}&{\\quad f(X)=\\frac{1}{2}\\|H\\circ(X X^{\\top}-G)\\|_{F}^{2}}\\\\ {\\mathrm{s.t.}}&{\\quad\\mathrm{Diag}(X X^{\\top})=I_{m}.}\\end{array}\n$$\n\nKey findings indicate that the Trust-ncg solver shows superior performance over the RTR solver when the column size \\( s \\) is relatively large. Additionally, the L-BFGS-B solver, benefiting from its FORTRAN implementation, provides significant advantages in most test instances.", "question_context_used": "The study compares the numerical performance of constraint-dissolving approaches with state-of-the-art Riemannian optimization solvers for the nearest low-rank correlation matrix problem. The problem is formulated as an optimization problem over the oblique manifold: \n\n$$\n\\begin{array}{r l}{\\underset{X\\in\\mathbb{R}^{m\\times s}}{\\mathrm{min}}}&{\\quad f(X)=\\frac{1}{2}\\|H\\circ(X X^{\\top}-G)\\|_{F}^{2}}\\\\ {\\mathrm{s.t.}}&{\\quad\\mathrm{Diag}(X X^{\\top})=I_{m}.}\\end{array}\n$$\n\nKey findings indicate that the Trust-ncg solver shows superior performance over the RTR solver when the column size \\( s \\) is relatively large. Additionally, the L-BFGS-B solver, benefiting from its FORTRAN implementation, provides significant advantages in most test instances.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-2494", "question": "In the context of the Convex Hull Algorithm for the Traveling Salesman Problem (TSP), which step ensures the minimal insertion cost for a node not yet in the subtour?", "options": ["A) Step 1: Form the convex hull to establish the initial subtour.", "B) Step 2: For each node not in the subtour, find nodes on the subtour that minimize the insertion cost.", "C) Step 3: Among all possible insertions, select the one with the minimal ratio of insertion cost to existing edge cost.", "D) Step 4: Insert the selected node between the nodes identified in Step 3."], "gold_answer": "C", "source_files": ["The Efficient Drilling of Printed Circuit Boards.md"], "question_context": "The Convex Hull Heuristic for the TSP involves several steps. Step 1 forms the convex hull to establish the initial subtour. Step 2 identifies nodes not in the subtour and calculates the minimal insertion cost for each. Step 3 selects the insertion with the minimal ratio of insertion cost to existing edge cost, ensuring optimality. Step 4 performs the insertion, and Step 5 repeats the process until all nodes are included. The algorithm leverages geometric properties to optimize the tour.", "question_context_used": "The Convex Hull Heuristic for the TSP involves several steps. Step 1 forms the convex hull to establish the initial subtour. Step 2 identifies nodes not in the subtour and calculates the minimal insertion cost for each. Step 3 selects the insertion with the minimal ratio of insertion cost to existing edge cost, ensuring optimality. Step 4 performs the insertion, and Step 5 repeats the process until all nodes are included. The algorithm leverages geometric properties to optimize the tour.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-4638", "question": "According to Richard B. Chase's '10 Commandments of Service System Management', what is the primary advantage of services over manufacturing in the context of productivity improvement?", "options": ["A) The ability to draw prestige from historical or religious references", "B) The inherent flexibility and adaptability of service systems", "C) The rapid technological advancements in service delivery", "D) The higher profit margins associated with service industries"], "gold_answer": "A", "source_files": ["The 10 Commandments of Service System Management.md"], "question_context": "Service junkies like myself are Truly in an uphill battle; 'Gainst the drama in the factories, Service sounds like so much prattle. Can your burger shop compete with Spaces stuffed with tech and glamour, Manufacturing that is flexible, Robots wise with chips and hammer. Perhaps my best advantage lies in Drawing prestige from the Bible, Venturing forth with ten commandments. Holy Moses, is this libel? Service yields a mighty matrix. Indulge us, factory-bound elite. After the day of song and kan ban Hokkaidans, too, go out to eat. What is needed, in my opinion, is a better understanding of the very nature of services themselves, and more specifically, some practical philosophy for designing the service delivery process.", "question_context_used": "Service junkies like myself are Truly in an uphill battle; 'Gainst the drama in the factories, Service sounds like so much prattle. Can your burger shop compete with Spaces stuffed with tech and glamour, Manufacturing that is flexible, Robots wise with chips and hammer. Perhaps my best advantage lies in Drawing prestige from the Bible, Venturing forth with ten commandments. Holy Moses, is this libel? Service yields a mighty matrix. Indulge us, factory-bound elite. After the day of song and kan ban Hokkaidans, too, go out to eat. What is needed, in my opinion, is a better understanding of the very nature of services themselves, and more specifically, some practical philosophy for designing the service delivery process.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-4223", "question": "In the context of utility theory and the given proofs, what condition must be satisfied for $\\pmb{\\chi}$ to be utility independent (UI) of $\\pmb{Y}$?", "options": ["A) $f^{\\prime\\prime}(y)g^{\\prime}(y) - g^{\\prime\\prime}(y)f^{\\prime}(y) = 0$", "B) $\\frac{f^{\\prime\\prime}(y) + g^{\\prime\\prime}(y)h(x)}{f^{\\prime}(y) + g^{\\prime}(y)h(x)} = \\alpha(x)\\frac{f^{\\prime\\prime}(y)}{f^{\\prime}(y)} + (1-\\alpha(x))\\frac{f^{\\prime\\prime}(y) + g^{\\prime\\prime}(y)}{f^{\\prime}(y) + g^{\\prime}(y)}$", "C) $u(x,y) = f(y) + g(y)h(x)$ for some functions $f, g, h$", "D) $r(y|x) = r(y|x^{0})$ for all $x$"], "gold_answer": "A", "source_files": ["Double-Exponential Utility Functions.md"], "question_context": "The proof of Lemma 1 establishes that if $\\pmb{\\chi}$ is utility independent (UI) of $\\pmb{Y}$, then the utility function can be expressed as $u(x,y) = f(y) + g(y)h(x)$ for some functions $f, g, h$, where $h(x) = u(x|y^{0})$. The condition $f^{\\prime\\prime}(y)g^{\\prime}(y) - g^{\\prime\\prime}(y)f^{\\prime}(y) = 0$ is derived as necessary for $\\pmb{Y}$ to be UI of $\\pmb{\\chi}$. This condition implies that $r(y|x^{0}) = r(y|x^{*})$, which is a key requirement for utility independence. The proof further explores the implications of this condition and its role in establishing multiplicative utility functions.", "question_context_used": "The proof of Lemma 1 establishes that if $\\pmb{\\chi}$ is utility independent (UI) of $\\pmb{Y}$, then the utility function can be expressed as $u(x,y) = f(y) + g(y)h(x)$ for some functions $f, g, h$, where $h(x) = u(x|y^{0})$. The condition $f^{\\prime\\prime}(y)g^{\\prime}(y) - g^{\\prime\\prime}(y)f^{\\prime}(y) = 0$ is derived as necessary for $\\pmb{Y}$ to be UI of $\\pmb{\\chi}$. This condition implies that $r(y|x^{0}) = r(y|x^{*})$, which is a key requirement for utility independence. The proof further explores the implications of this condition and its role in establishing multiplicative utility functions.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-3867", "question": "Based on the simulation results, which policy maintains the highest aircraft availability during the combat phase while also minimizing the proportion of unresponded mission requests?", "options": ["A) Policy 3, which partially suspends periodic maintenance", "B) Policy 4, which fully utilizes maintenance resources for periodic maintenance", "C) Policy 1, which suspends some periodic maintenance", "D) Policy 2, which suspends some periodic maintenance"], "gold_answer": "C", "source_files": ["Improving Maintenance Decision Making in the Finni.md"], "question_context": "The most critical phase of the conflict is the actual combat phase, during which the availability decreases rapidly. If policy 4 is employed, the periodic maintenance will use up the maintenance resources and delay the battle-damage repairs. The availability consequently drops to as low as 0.4; this means that it is very unlikely that the air commands could meet all their operational goals. Policies 1 and 2 produce the highest availability. The proportion of mission requests to which there was no response during the busiest phase of the conflict was approximately 10 percent with policies 1 and 2. The ratios were 25 percent and 40 percent, respectively, for policies 3 and 4.", "question_context_used": "The most critical phase of the conflict is the actual combat phase, during which the availability decreases rapidly. If policy 4 is employed, the periodic maintenance will use up the maintenance resources and delay the battle-damage repairs. The availability consequently drops to as low as 0.4; this means that it is very unlikely that the air commands could meet all their operational goals. Policies 1 and 2 produce the highest availability. The proportion of mission requests to which there was no response during the busiest phase of the conflict was approximately 10 percent with policies 1 and 2. The ratios were 25 percent and 40 percent, respectively, for policies 3 and 4.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-3753", "question": "Under the regularity conditions for maximum likelihood estimation, which condition ensures that the maximum likelihood estimates of the parameters are asymptotically normally distributed?", "options": ["A) Condition 1: The quantities are contained in compact subsets of Euclidean spaces.", "B) Condition 3: The functions are three times continuously differentiable with respect to the parameters.", "C) Condition 7: The matrices of second derivatives are positive definite.", "D) Condition 8: Models P and Q are non-nested."], "gold_answer": "B", "source_files": ["Statistical Comparison of Non-Nested Probabilistic.md"], "question_context": "Condition 3. For each $j\\in C$ and $\\underline{{\\boldsymbol{X}}}\\in\\boldsymbol{S}$ and for every $\\pmb{\\alpha}$ and $\\beta$ interior to $A$ and $B,~f(j,~\\underline{{X}}|\\mathop{\\alpha})$ and $g(j,\\underline{{X}}|\\beta)$ are three times continuously differentiable with respect to the components of $\\pmb{\\alpha}$ and $\\beta_{:}$ respectively. Conditions 3, 5, 6 and 7 make possible certain Taylor series expansions that are used in deriving the asymptotic distributions of the Cox separate families test statistic and the likelihood ratio index. These conditions also ensure that the maximum likelihood estimates of $\\pmb{\\alpha}$ and $\\beta$ are asymptotically normally distributed.", "question_context_used": "Condition 3. For each $j\\in C$ and $\\underline{{\\boldsymbol{X}}}\\in\\boldsymbol{S}$ and for every $\\pmb{\\alpha}$ and $\\beta$ interior to $A$ and $B,~f(j,~\\underline{{X}}|\\mathop{\\alpha})$ and $g(j,\\underline{{X}}|\\beta)$ are three times continuously differentiable with respect to the components of $\\pmb{\\alpha}$ and $\\beta_{:}$ respectively. Conditions 3, 5, 6 and 7 make possible certain Taylor series expansions that are used in deriving the asymptotic distributions of the Cox separate families test statistic and the likelihood ratio index. These conditions also ensure that the maximum likelihood estimates of $\\pmb{\\alpha}$ and $\\beta$ are asymptotically normally distributed.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-4605", "question": "In the context of the machine-shop case study, which factor is identified as the most significant contributor to the variability in set-up and processing times?", "options": ["A) The sensitivity of parts to ambient temperature shifts", "B) The practice of preempting jobs for political sensitivity", "C) The shortage of skilled manpower for final work stages", "D) The variability in alloys used and difficulties in preparing raw materials"], "gold_answer": "A", "source_files": ["Job-Shop Scheduling Theory_ What Is Relevant_.md"], "question_context": "The shop described is a large machine shop with close-tolerance machining of alloy castings, featuring approximately 80 operations per part, over 300 work centers, and an average of 5,000 open work orders. Set-up and processing times are extremely variable, with the same part on the same machine with the same operator varying from two days to six weeks in set-up time. Parts in a batch can vary in processing times by 100 percent. A two-degree ambient temperature shift in the shop can cause the close tolerance parts to go in and out of specification while waiting for assembly. The machines themselves are very sensitive to temperature and humidity, which affects their yield as well as their failure rate.", "question_context_used": "The shop described is a large machine shop with close-tolerance machining of alloy castings, featuring approximately 80 operations per part, over 300 work centers, and an average of 5,000 open work orders. Set-up and processing times are extremely variable, with the same part on the same machine with the same operator varying from two days to six weeks in set-up time. Parts in a batch can vary in processing times by 100 percent. A two-degree ambient temperature shift in the shop can cause the close tolerance parts to go in and out of specification while waiting for assembly. The machines themselves are very sensitive to temperature and humidity, which affects their yield as well as their failure rate.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-4799", "question": "According to the text, which industry is most likely to experience the largest increase in computer power usage due to societal trends toward a 'checkless, cashless society'?", "options": ["A) Manufacturing industries", "B) Banking or its successor industry", "C) Retail and marketing", "D) Government and public sector"], "gold_answer": "B", "source_files": ["Information Systems in Management Science—Predicti.md"], "question_context": "Banking seems to be a good candidate for the most growth as the country moves toward greater use of checking accounts, credit cards, etc. If the country continues this move to a 'checkless, cashless society,' then banking (or the successor industry) will need greatly expanded computer power. Manufacturing industries will most likely expand their computing capacity, particularly in the areas of inventory control and its integration into manufacturing and raw materials purchasing. Numerical control of tools and process control should be greatly expanded in the next ten years, particularly if the United States is to compete in world markets.", "question_context_used": "Banking seems to be a good candidate for the most growth as the country moves toward greater use of checking accounts, credit cards, etc. If the country continues this move to a 'checkless, cashless society,' then banking (or the successor industry) will need greatly expanded computer power. Manufacturing industries will most likely expand their computing capacity, particularly in the areas of inventory control and its integration into manufacturing and raw materials purchasing. Numerical control of tools and process control should be greatly expanded in the next ten years, particularly if the United States is to compete in world markets.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-4705", "question": "According to the analysis of PEPCO's fuel strategies, under which conditions does the oil-only strategy become less expensive than the coal conversion strategies?", "options": ["A) High peak demand growth and RPA II fuel price forecast", "B) Low peak demand growth and Chase fuel price forecast", "C) High peak demand growth and Chase fuel price forecast", "D) Low peak demand growth and RPA I fuel price forecast"], "gold_answer": "A", "source_files": ["Developing Generation Construction Plans for the P.md"], "question_context": "In the case of the high peak demand growth (2.5 percent per year), the oil/coal strategy was still the preferred strategy, followed closely by the coal-only strategy under the Chase fuel price forecast. For RPA I fuel price forecast, the revenue requirements of the three strategies were virtually equal. For the RPA II fuel price forecast, PEPCO's oil-only strategy was less expensive. (Figure 7 is a strategy comparison table, which summarizes the results discussed above.) The present value of revenue requirements for the three strategies are compared under different possible fuel price forecasts and load growth rates. All assume a conversion cost of $578/123, an 11.3% discount rate, and a 9.5% rate of return. The least-cost strategy for each scenario combination is shaded; negative values indicate savings.", "question_context_used": "In the case of the high peak demand growth (2.5 percent per year), the oil/coal strategy was still the preferred strategy, followed closely by the coal-only strategy under the Chase fuel price forecast. For RPA I fuel price forecast, the revenue requirements of the three strategies were virtually equal. For the RPA II fuel price forecast, PEPCO's oil-only strategy was less expensive. (Figure 7 is a strategy comparison table, which summarizes the results discussed above.) The present value of revenue requirements for the three strategies are compared under different possible fuel price forecasts and load growth rates. All assume a conversion cost of $578/123, an 11.3% discount rate, and a 9.5% rate of return. The least-cost strategy for each scenario combination is shaded; negative values indicate savings.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-3258", "question": "In the context of constraint qualifications for the Kuhn-Tucker conditions, which of the following statements is true regarding Type I and Type II constraint qualifications?", "options": ["A) Type I C.Q.'s are stronger and do not make use of gradient vectors, while Type II C.Q.'s are weaker and rely on gradient vectors of binding constraints.", "B) Type I C.Q.'s are weaker and make use of gradient vectors of binding constraints, while Type II C.Q.'s are stronger and do not necessarily rely on gradient vectors.", "C) Both Type I and Type II C.Q.'s are equally strong and rely on gradient vectors of binding constraints.", "D) Type I C.Q.'s are weaker and do not make use of gradient vectors, while Type II C.Q.'s are stronger and rely on gradient vectors of binding constraints."], "gold_answer": "B", "source_files": ["Constraint Qualifications Revisited.md"], "question_context": "The study investigates the constraint qualifications for the Kuhn-Tucker conditions to hold for an inequality-constrained nonlinear programming problem. It presents the qualifications in a consistent manner, highlighting the interrelationships between them. This gives rise naturally to two types of constraint qualifications: Type I and Type II. Type I C.Q.'s can be expressed in the form C C S where C and S are sets in E\" defined such that int(C) CS C C. These qualifications are 'close' to each other and make use of the gradients of the binding constraints. Type II C.Q.'s are stronger and do not make use of the gradient vectors (except for the nondegeneracy C.Q.). Some of these qualifications assume a special form of the binding constraints, e.g., pseudoconvexity.", "question_context_used": "The study investigates the constraint qualifications for the Kuhn-Tucker conditions to hold for an inequality-constrained nonlinear programming problem. It presents the qualifications in a consistent manner, highlighting the interrelationships between them. This gives rise naturally to two types of constraint qualifications: Type I and Type II. Type I C.Q.'s can be expressed in the form C C S where C and S are sets in E\" defined such that int(C) CS C C. These qualifications are 'close' to each other and make use of the gradients of the binding constraints. Type II C.Q.'s are stronger and do not make use of the gradient vectors (except for the nondegeneracy C.Q.). Some of these qualifications assume a special form of the binding constraints, e.g., pseudoconvexity.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-4768", "question": "In the logistics model case study for Coraopolis supply alternatives, what was the primary reason the model suggested supplying Coraopolis directly out of Wood River despite its capacity constraints?", "options": ["A) To minimize transshipment charges at Wood River by rerouting volumes directly to East Chicago", "B) To reduce the overall transportation costs by optimizing the entire system rather than a single terminal", "C) To eliminate the need for capital expenditures at East Chicago by shifting supply to Detroit", "D) To increase the utilization of the Buckeye Pipe Line for cost efficiency"], "gold_answer": "A", "source_files": ["Practical Applications of Linear Programming to Sh.md"], "question_context": "The logistics model suggested that Coraopolis be supplied directly out of Wood River, despite Wood River being at capacity. The model readjusted the flows by sending the product Norco was barging to Coraopolis to Betendorf, thus backing up that volume into Wood River. The volume being barged by Houston to Coraopolis was sent all the way to East Chicago (via Explorer) rather than to Wood River. This was more economical due to the transshipment charge at Wood River. The cost of supplying Coraopolis was decreased by about $15\\% per year by using the new route, though most savings were offset by increased costs elsewhere in the system.", "question_context_used": "The logistics model suggested that Coraopolis be supplied directly out of Wood River, despite Wood River being at capacity. The model readjusted the flows by sending the product Norco was barging to Coraopolis to Betendorf, thus backing up that volume into Wood River. The volume being barged by Houston to Coraopolis was sent all the way to East Chicago (via Explorer) rather than to Wood River. This was more economical due to the transshipment charge at Wood River. The cost of supplying Coraopolis was decreased by about $15\\% per year by using the new route, though most savings were offset by increased costs elsewhere in the system.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-4670", "question": "Given the inequality in Lemma 7.1, what is the upper bound for the supremum of the expected value of the square of the supremum of the absolute value of the process W^ε(s) over s ≤ t?", "options": ["A) C + C t", "B) C t^2", "C) C (1 + t)^2", "D) C e^t"], "gold_answer": "A", "source_files": ["Double Band Policy for Stochastic Manufacturing Sy.md"], "question_context": "LEMMA 7.1. There exists a constant $C$ such that \n$$\n\\operatorname*{sup}_{\\epsilon}\\left(E{\\Big(}\\operatorname*{sup}_{s\\leqslant t}{\\big|}W^{\\epsilon}(s){\\big|}{\\Big)}^{2}\\right)^{1/2}\\leqslant C+C t.\n$$\nThis lemma provides an upper bound for the expected supremum of the squared absolute value of the process W^ε(s) over the interval [0, t]. The bound is linear in t, indicating that the process does not grow too rapidly. The constant C captures the initial variability of the process, while the term C t accounts for the accumulation of variability over time.", "question_context_used": "LEMMA 7.1. There exists a constant $C$ such that \n$$\n\\operatorname*{sup}_{\\epsilon}\\left(E{\\Big(}\\operatorname*{sup}_{s\\leqslant t}{\\big|}W^{\\epsilon}(s){\\big|}{\\Big)}^{2}\\right)^{1/2}\\leqslant C+C t.\n$$\nThis lemma provides an upper bound for the expected supremum of the squared absolute value of the process W^ε(s) over the interval [0, t]. The bound is linear in t, indicating that the process does not grow too rapidly. The constant C captures the initial variability of the process, while the term C t accounts for the accumulation of variability over time.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-3217", "question": "In the described single station service system, which of the following statements about the service types is correct?", "options": ["A) Service type 1 is faster but less efficient than service type 2.", "B) Service type 2 is faster and more efficient than service type 1.", "C) Service type 1 is both faster and more efficient than service type 2.", "D) Service type 2 is faster but less efficient than service type 1."], "gold_answer": "B", "source_files": ["The Optimality of Connected Policies for Markovian.md"], "question_context": "The text describes a single station service system with two service types. The type of service with higher $\\mu_{j}$ is termed faster, and the type with lower $C_{j}/\\mu_{j}$ is termed more efficient. Without loss of generality, it is assumed that $\\mu_{2}>\\mu_{1}$. The expected service time is given by $1/\\mu_{j}$, and the expected cost per customer is $C_{j}/\\mu_{j}$ for service type $j$. The transition probabilities and immediate costs under different policies are also detailed, with the system operating in discrete time periods and decisions made based on the number of customers observed at the beginning of each period.", "question_context_used": "The text describes a single station service system with two service types. The type of service with higher $\\mu_{j}$ is termed faster, and the type with lower $C_{j}/\\mu_{j}$ is termed more efficient. Without loss of generality, it is assumed that $\\mu_{2}>\\mu_{1}$. The expected service time is given by $1/\\mu_{j}$, and the expected cost per customer is $C_{j}/\\mu_{j}$ for service type $j$. The transition probabilities and immediate costs under different policies are also detailed, with the system operating in discrete time periods and decisions made based on the number of customers observed at the beginning of each period.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-4760", "question": "According to Theorem 1(a), which of the following conditions must a semivalue on the space of finite games G satisfy?", "options": ["A) It must be linear, symmetric, monotonic, and efficient", "B) It must be linear, symmetric, monotonic, and satisfy the projection axiom", "C) It must be efficient, symmetric, monotonic, and satisfy the dummy axiom", "D) It must be linear, efficient, monotonic, and satisfy the projection axiom"], "gold_answer": "B", "source_files": ["Value Theory Without Efficiency.md"], "question_context": "A semivalue on G is a function ψ:G→AG such that: (1) ψ is linear; (2) ψΘ∗=Θ∗ψ, for each permutation Θ of U (symmetry); (3) if v is monotonic, then ψv is monotonic (monotonicity); (4) if v∈AG then ψv=v (projection axiom). These are the linearity, symmetry, monotonicity and projection axioms. The projection axiom is an easy consequence of the more familiar dummy axiom, which says that if i is a dummy player in v then (ψv)(i)=v(i). Theorem 1(a) states that for each probability measure ξ on [0,1], ψξ is a semivalue, and every semivalue on G is of this form.", "question_context_used": "A semivalue on G is a function ψ:G→AG such that: (1) ψ is linear; (2) ψΘ∗=Θ∗ψ, for each permutation Θ of U (symmetry); (3) if v is monotonic, then ψv is monotonic (monotonicity); (4) if v∈AG then ψv=v (projection axiom). These are the linearity, symmetry, monotonicity and projection axioms. The projection axiom is an easy consequence of the more familiar dummy axiom, which says that if i is a dummy player in v then (ψv)(i)=v(i). Theorem 1(a) states that for each probability measure ξ on [0,1], ψξ is a semivalue, and every semivalue on G is of this form.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-3370", "question": "Based on the data in Table III, which problem instance demonstrates the greatest percentage reduction in total distance traveled (BC) compared to the original total distance?", "options": ["A) 50a;8", "B) 50c;8", "C) 75a;9", "D) 100a;10"], "gold_answer": "A", "source_files": ["A Heuristic for the Periodic Vehicle Routing Probl.md"], "question_context": "Table III presents a comparison of standard test problems in terms of total distance traveled and the best cost (BC) achieved. The table includes data for different problem instances with varying numbers of customers and planning horizons. For instance, problem 50a;8 has a total distance of 601.6 and a BC of 558.4, indicating a reduction of 43.2 units. The percentage reduction can be calculated as (601.6 - 558.4) / 601.6 ≈ 7.18%. Similarly, problem 50c;8 shows a reduction of (625.5 - 547.5) / 625.5 ≈ 12.47%, problem 75a;9 shows (949.4 - 855.4) / 949.4 ≈ 9.90%, and problem 100a;10 shows (902.2 - 839.2) / 902.2 ≈ 6.98%. The greatest percentage reduction is observed in problem 50a;8.", "question_context_used": "Table III presents a comparison of standard test problems in terms of total distance traveled and the best cost (BC) achieved. The table includes data for different problem instances with varying numbers of customers and planning horizons. For instance, problem 50a;8 has a total distance of 601.6 and a BC of 558.4, indicating a reduction of 43.2 units. The percentage reduction can be calculated as (601.6 - 558.4) / 601.6 ≈ 7.18%. Similarly, problem 50c;8 shows a reduction of (625.5 - 547.5) / 625.5 ≈ 12.47%, problem 75a;9 shows (949.4 - 855.4) / 949.4 ≈ 9.90%, and problem 100a;10 shows (902.2 - 839.2) / 902.2 ≈ 6.98%. The greatest percentage reduction is observed in problem 50a;8.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-3934", "question": "In the context of cargo overbooking simulations, which of the following statements is true regarding the comparison between the normal and discrete show-up-rate estimators?", "options": ["A) The normal estimator results in higher mean spoilage but lower frequency compared to the discrete estimator.", "B) The discrete estimator has a higher mean absolute error compared to the real capacity at departure than the normal estimator.", "C) The discrete estimator results in statistically equal off-loads when untruncating demand with a high mean and variance.", "D) The normal estimator yields a 25% higher mean absolute error when assuming perfect capacity forecast."], "gold_answer": "D", "source_files": ["Estimating Air-Cargo Overbooking Based on a Discre.md"], "question_context": "The study compares normal and discrete show-up-rate estimators in cargo overbooking simulations. Key findings include: (1) The mean absolute error of $T_{normal}$ (compared to the real capacity at departure) was approximately four percent higher than for $T_{discrete}$; (2) When assuming perfect forecast of cargo capacity, the mean absolute error is on average 25% higher for the normal estimator; (3) For real-world truncated demand, the discrete estimator resulted in off-loads five percent lower in mean when the added normal variable had low mean and variance. The formula for tendered cargo is given by: $$T_{normal\\ RD} = CB_{normal\\ RD} * SR_{actual\\ RD}$$ and similarly for $T_{discrete}$.", "question_context_used": "The study compares normal and discrete show-up-rate estimators in cargo overbooking simulations. Key findings include: (1) The mean absolute error of $T_{normal}$ (compared to the real capacity at departure) was approximately four percent higher than for $T_{discrete}$; (2) When assuming perfect forecast of cargo capacity, the mean absolute error is on average 25% higher for the normal estimator; (3) For real-world truncated demand, the discrete estimator resulted in off-loads five percent lower in mean when the added normal variable had low mean and variance. The formula for tendered cargo is given by: $$T_{normal\\ RD} = CB_{normal\\ RD} * SR_{actual\\ RD}$$ and similarly for $T_{discrete}$.", "gold_answer_parsed": "D", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-3775", "question": "In the context of Tanner's scheduling model, what is the primary reason for the trade-off between workload shortfall and finished goods inventory?", "options": ["A) The inability to accurately quantify inventory and shortfall costs in dollars", "B) The variability in demand and the timing of job releases", "C) The limitations of the mixed-integer programming model", "D) The weekly departmental production capacity constraints"], "gold_answer": "B", "source_files": ["Lower In-Process Inventories and Better On-Time Pe.md"], "question_context": "There is an important trade-off between workload shortfall and finished goods inventory; reducing one tends to increase the other. Finished goods inventory could be minimized by releasing jobs just prior to their due dates, but the variation in demand would at times create significant workload shortfalls that would cause employee cutbacks or layoffs in areas in which skilled labor is not readily available. Alternately, Tanner could maintain a constant workload by smoothing demand variation through the early release of jobs. This, however, tends to increase finished goods inventories. Quantifying the trade-off between these two objectives posed a significant problem. Inventory costs and shortfall costs are considered to be noncommensurate since neither can be fully quantified in dollars.", "question_context_used": "There is an important trade-off between workload shortfall and finished goods inventory; reducing one tends to increase the other. Finished goods inventory could be minimized by releasing jobs just prior to their due dates, but the variation in demand would at times create significant workload shortfalls that would cause employee cutbacks or layoffs in areas in which skilled labor is not readily available. Alternately, Tanner could maintain a constant workload by smoothing demand variation through the early release of jobs. This, however, tends to increase finished goods inventories. Quantifying the trade-off between these two objectives posed a significant problem. Inventory costs and shortfall costs are considered to be noncommensurate since neither can be fully quantified in dollars.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-3071", "question": "In the proof of Lemma 5, what is the role of the transition function θₘ(qᵐ; F, s_{m+1})?", "options": ["A) It defines the conditional law of q_{m+1} given qᵐ and s_{m+1}.", "B) It integrates the probability measure Q over the set F to determine the probability of s_{m+1}.", "C) It serves as a regularization term to ensure the convergence of the sequence μₘ.", "D) It acts as a test function to verify the induction hypothesis."], "gold_answer": "B", "source_files": ["Markov Games with Frequent Actions and Incomplete.md"], "question_context": "The transition function θₘ from Δ(S)ᵐ × S to Δ(S) × S is defined by θₘ(qᵐ; F, s_{m+1}) = ∫_F q_{m+1}(s_{m+1}) Q(qᵐ; dq_{m+1}). Intuitively, θₘ(qᵐ; q_{m+1}, s_{m+1}) is the probability obtained when first choosing q_{m+1} according to its (conditional) law Q(qᵐ; q_{m+1}), then picking s_{m+1} according to q_{m+1}. The sequence μ = (μₘ) is defined by μₘ(qᵐ, s_{m+1}; F) := θₘ(qᵐ; F, s_{m+1}) / θₘ(qᵐ; s_{m+1}), so that μₘ(qᵐ, s_{m+1}; q_{m+1}) is the conditional law of q_{m+1} given qᵐ and s_{m+1}.", "question_context_used": "The transition function θₘ from Δ(S)ᵐ × S to Δ(S) × S is defined by θₘ(qᵐ; F, s_{m+1}) = ∫_F q_{m+1}(s_{m+1}) Q(qᵐ; dq_{m+1}). Intuitively, θₘ(qᵐ; q_{m+1}, s_{m+1}) is the probability obtained when first choosing q_{m+1} according to its (conditional) law Q(qᵐ; q_{m+1}), then picking s_{m+1} according to q_{m+1}. The sequence μ = (μₘ) is defined by μₘ(qᵐ, s_{m+1}; F) := θₘ(qᵐ; F, s_{m+1}) / θₘ(qᵐ; s_{m+1}), so that μₘ(qᵐ, s_{m+1}; q_{m+1}) is the conditional law of q_{m+1} given qᵐ and s_{m+1}.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-3031", "question": "Given the definition of $Q_{R}^{(n)}(z)$ in Lemma 3.6, which condition ensures that $Q_{R}^{(n)}(z)$ can be uniquely extended to a probability measure on the Borel field of $\\widetilde{[0,\\infty)\\times S}$?", "options": ["A) $\\tilde{P}$ is $\\sigma$-additive and $\\tilde{P}((0,\\infty)\\times S)=1$", "B) $\\mathcal{S}$ is a semialgebra generating the Borel field", "C) $\\tilde{P}$ is continuous and $\\mathcal{S}$ is compact", "D) $\\tilde{P}$ is monotone and $\\mathcal{S}$ is a $\\sigma$-algebra"], "gold_answer": "B", "source_files": ["Discretization and Weak Convergence in Markov Deci.md"], "question_context": "Lemma 3.6 states that any $\\sigma$-additive set function $\\tilde{P}$ mapping $\\mathcal{S}$ into [0, 1], with $\\tilde{P}((0,\\infty)\\times S)=1$, can be uniquely extended to a probability measure on the Borel field of $\\widetilde{[0,\\infty)\\times S}$. The proof hinges on the fact that $\\mathcal{S}$ is a semialgebra generating the Borel field, referencing Proposition 1.6.1 of Neveu (1965). The lemma underscores the importance of the structural properties of $\\mathcal{S}$ in ensuring the extension's uniqueness and validity.", "question_context_used": "Lemma 3.6 states that any $\\sigma$-additive set function $\\tilde{P}$ mapping $\\mathcal{S}$ into [0, 1], with $\\tilde{P}((0,\\infty)\\times S)=1$, can be uniquely extended to a probability measure on the Borel field of $\\widetilde{[0,\\infty)\\times S}$. The proof hinges on the fact that $\\mathcal{S}$ is a semialgebra generating the Borel field, referencing Proposition 1.6.1 of Neveu (1965). The lemma underscores the importance of the structural properties of $\\mathcal{S}$ in ensuring the extension's uniqueness and validity.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-3041", "question": "According to Bartholomew-Biggs, what is a key challenge when solving non-quadratic optimization problems in financial portfolio management?", "options": ["A) The necessity to use iterative methods like the secant and Newton's methods", "B) The requirement to compute both first and second derivatives of the objective functions", "C) The potential for inaccurate results due to poor choice of differencing step size", "D) The transformation of the problem into a minimize-risk or maximize-expected-return framework"], "gold_answer": "A", "source_files": ["Book Reviews.md"], "question_context": "Bartholomew-Biggs introduces pertinent notions such as return and risk in financial portfolios, emphasizing that not all problems in finance can be posed as quadratic optimization problems. For non-quadratic problems, iterative methods like the secant method and Newton's method are often necessary, incorporating gradient techniques that involve computing first and second derivatives. The author also notes that a poor choice of differencing step size can lead to inaccurate results. The book further discusses transforming problems to handle constraints like non-negativity in investments, highlighting the increased difficulty when dealing with non-quadratic functions.", "question_context_used": "Bartholomew-Biggs introduces pertinent notions such as return and risk in financial portfolios, emphasizing that not all problems in finance can be posed as quadratic optimization problems. For non-quadratic problems, iterative methods like the secant method and Newton's method are often necessary, incorporating gradient techniques that involve computing first and second derivatives. The author also notes that a poor choice of differencing step size can lead to inaccurate results. The book further discusses transforming problems to handle constraints like non-negativity in investments, highlighting the increased difficulty when dealing with non-quadratic functions.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3148", "question": "In the context of differentiable games on a standard Borel space, what is the derivative of the product of two differentiable set functions ν and λ at a coalition A?", "options": ["A) λ(A)D⁻ν(A) + ν(A)D⁻λ(A) - [D⁻ν(A)⊗D⁻λ(A)]_Δ", "B) λ(A)D⁺ν(A) + ν(A)D⁺λ(A) + [D⁺ν(A)⊗D⁺λ(A)]_Δ", "C) λ(A)D⁻ν(A) + ν(A)D⁺λ(A) - [D⁻ν(A)⊗D⁺λ(A)]_Δ", "D) λ(A)D⁺ν(A) + ν(A)D⁻λ(A) + [D⁺ν(A)⊗D⁻λ(A)]_Δ"], "gold_answer": "B", "source_files": ["Refinement Derivatives and Values of Games.md"], "question_context": "The text discusses the differentiability of games on a standard Borel space (Ω,Σ). Theorem 5.2 states that the product of differentiable set functions ν and λ is differentiable, and its derivative can be evaluated. For outer differentiable games, the derivative is given by: D⁺λν(A) = λ(A)D⁺ν(A) + ν(A)D⁺λ(A) + [D⁺ν(A)⊗D⁺λ(A)]_Δ. A similar result holds for inner differentiable games: D⁻λν(A) = λ(A)D⁻ν(A) + ν(A)D⁻λ(A) - [D⁻ν(A)⊗D⁻λ(A)]_Δ. The diagonal measures in these formulas exhibit a special structure, as detailed in Proposition 5.5.", "question_context_used": "The text discusses the differentiability of games on a standard Borel space (Ω,Σ). Theorem 5.2 states that the product of differentiable set functions ν and λ is differentiable, and its derivative can be evaluated. For outer differentiable games, the derivative is given by: D⁺λν(A) = λ(A)D⁺ν(A) + ν(A)D⁺λ(A) + [D⁺ν(A)⊗D⁺λ(A)]_Δ. A similar result holds for inner differentiable games: D⁻λν(A) = λ(A)D⁻ν(A) + ν(A)D⁻λ(A) - [D⁻ν(A)⊗D⁻λ(A)]_Δ. The diagonal measures in these formulas exhibit a special structure, as detailed in Proposition 5.5.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-3228", "question": "According to Proposition 7, what condition ensures that the convergence ratio at the beginning of Dinkelbach's algorithm is less than 1/2?", "options": ["A) $D(x_1) < 2D(x^+)$", "B) $1 - D(x_0)/D(x_1) < \\frac{1}{2}$", "C) $K = D(x_1)/D(x_0) - 1$", "D) $q_1 < q^+$"], "gold_answer": "B", "source_files": ["Fractional Programming. II, On Dinkelbach's Algori.md"], "question_context": "The text discusses error estimates in numerical methods, particularly focusing on Dinkelbach's algorithm. Proposition 7 states that if $q_1 < q^+$ is obtained such that $D(x_1) < 2D(x^+)$, then for any $K \\geqslant D(x_1)/D(x^+) - 1, K < 1$, the error estimate $q^+ - Q(x_i) \\leqslant (q_2 - q_1)K^i/(1 - K)$ holds. The condition $1 - D(x_0)/D(x_1) < \\frac{1}{2}$ ensures that $K < 1$ and implies that the starting value in Dinkelbach's algorithm is chosen such that the convergence ratio at the beginning $(q^+ - q_2)/(q^+ - q_1)$ is less than $\\frac{1}{2}$.", "question_context_used": "The text discusses error estimates in numerical methods, particularly focusing on Dinkelbach's algorithm. Proposition 7 states that if $q_1 < q^+$ is obtained such that $D(x_1) < 2D(x^+)$, then for any $K \\geqslant D(x_1)/D(x^+) - 1, K < 1$, the error estimate $q^+ - Q(x_i) \\leqslant (q_2 - q_1)K^i/(1 - K)$ holds. The condition $1 - D(x_0)/D(x_1) < \\frac{1}{2}$ ensures that $K < 1$ and implies that the starting value in Dinkelbach's algorithm is chosen such that the convergence ratio at the beginning $(q^+ - q_2)/(q^+ - q_1)$ is less than $\\frac{1}{2}$.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3185", "question": "Given the total cost minimization problem described, what is the optimal average inventory level (in units) that minimizes total costs, based on the derivative of the setup cost function and the holding cost slope?", "options": ["A) 5,000", "B) 6,000", "C) 7,000", "D) 8,000"], "gold_answer": "B", "source_files": ["Minimizing Inventory Cost.md"], "question_context": "The total cost curve is plotted by summing inventory holding cost and setup cost. The holding cost for an average inventory of 5,000 units is \\$5,000, calculated as [material cost × (safety stock + production run)] × 10%. The setup cost is \\$750 per setup, leading to a total setup charge of \\$9,000 for 12 setups. The derivative of the setup cost function is given by \\frac{-.5(18,000,000)}{x^{2}} = -0.15, where K is \\$18,000,000 (24,000 × \\$750). Solving this equation yields the optimal average inventory level, which aligns with the data in Figure 1.", "question_context_used": "The total cost curve is plotted by summing inventory holding cost and setup cost. The holding cost for an average inventory of 5,000 units is \\$5,000, calculated as [material cost × (safety stock + production run)] × 10%. The setup cost is \\$750 per setup, leading to a total setup charge of \\$9,000 for 12 setups. The derivative of the setup cost function is given by \\frac{-.5(18,000,000)}{x^{2}} = -0.15, where K is \\$18,000,000 (24,000 × \\$750). Solving this equation yields the optimal average inventory level, which aligns with the data in Figure 1.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-1376", "question": "Given the quadratic cost function minimization problem in sequencing, what is a necessary condition for optimality when two requests $J_k$ and $J_l$ satisfy $P_k \\leqslant P_l$ and $COST(J_k J_l) < COST(J_l J_k)$?", "options": ["A) $J_k$ must precede $J_l$ in the optimal sequence", "B) $J_l$ must precede $J_k$ in the optimal sequence", "C) The separation between $J_k$ and $J_l$ must satisfy $\\sum_{i=l+1}^{k-1} P_{[i]} \\leqslant (k-l-1)P_{[l]}$", "D) The requests $J_k$ and $J_l$ must be adjacent in the optimal sequence"], "gold_answer": "C", "source_files": ["A Problem in Single-Machine Sequencing with Nonlin.md"], "question_context": "The text discusses the minimization of quadratic cost functions in sequencing problems. Specifically, it presents LEMMA 13, which states that for an optimal sequence, if there exist two requests $J_{[k]}$ and $J_{[l]}$ such that (a) $l<k$, (b) $P_{[k]} \\leqslant P_{[l]}$, and (c) $COST(J_{[k]} J_{[l]}) \\leqslant COST(J_{[l]} J_{[k]})$, then the separation between $J_{[k]}$ and $J_{[l]}$ must satisfy $\\sum_{i=l+1}^{k-1} P_{[i]} \\leqslant (k-l-1)P_{[l]}$. This condition is derived from comparing the costs of different sequences and solving the resulting inequalities.", "question_context_used": "The text discusses the minimization of quadratic cost functions in sequencing problems. Specifically, it presents LEMMA 13, which states that for an optimal sequence, if there exist two requests $J_{[k]}$ and $J_{[l]}$ such that (a) $l<k$, (b) $P_{[k]} \\leqslant P_{[l]}$, and (c) $COST(J_{[k]} J_{[l]}) \\leqslant COST(J_{[l]} J_{[k]})$, then the separation between $J_{[k]}$ and $J_{[l]}$ must satisfy $\\sum_{i=l+1}^{k-1} P_{[i]} \\leqslant (k-l-1)P_{[l]}$. This condition is derived from comparing the costs of different sequences and solving the resulting inequalities.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-1533", "question": "Based on the quantitative impact analysis of OR-based scheduling in Chilean soccer, which component contributed the most to the total direct economic impact from 2005 to 2015?", "options": ["A) Impact on ticket revenue", "B) Impact on the soccer pay-TV channel", "C) Team travel savings", "D) Reduction in stadium maintenance costs"], "gold_answer": "A", "source_files": ["Operations Research Transforms the Scheduling of C.md"], "question_context": "The total direct economic impact of applying O.R. to Chilean soccer scheduling from 2005 to 2015 amounts to about $59 million. The impact on ticket revenue was estimated at $23.9 million, driven by increased attendance and revenue from classic matches and summer games in tourist areas. The pay-TV channel saw cost reductions of $2.1 million and an estimated economic impact of $27.8 million from increased subscribers. Team travel savings amounted to $5.2 million, with the First Division, Second Division, and Youth Division saving $1.6 million, $0.6 million, and $3.0 million, respectively. The global quantitative impact from all sources is currently about $6.4 million per year.", "question_context_used": "The total direct economic impact of applying O.R. to Chilean soccer scheduling from 2005 to 2015 amounts to about $59 million. The impact on ticket revenue was estimated at $23.9 million, driven by increased attendance and revenue from classic matches and summer games in tourist areas. The pay-TV channel saw cost reductions of $2.1 million and an estimated economic impact of $27.8 million from increased subscribers. Team travel savings amounted to $5.2 million, with the First Division, Second Division, and Youth Division saving $1.6 million, $0.6 million, and $3.0 million, respectively. The global quantitative impact from all sources is currently about $6.4 million per year.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-1404", "question": "Given the Bregman distance $D_{\\varphi}(x,y) = \\varphi(x) - \\varphi(y) - \\nabla\\varphi(y)^{T}(x-y)$, which of the following conditions ensures that the partial level sets $\\Gamma(x,\\delta) = \\{y \\in \\mathbb{R}_{++}^{n} : D_{\\varphi}(x,y) \\leq \\delta\\}$ are bounded for all $x \\in \\mathbb{R}_{+}^{n}$?", "options": ["A) $\\varphi$ is strictly convex and differentiable", "B) $\\varphi$ satisfies Condition (B1): partial level sets are bounded", "C) $\\varphi$ is boundary coercive", "D) $\\varphi$ has a closed-form conjugate $\\varphi^{*}$"], "gold_answer": "B", "source_files": ["On Dual Convergence of the Generalized Proximal Po.md"], "question_context": "The Bregman distance $D_{\\varphi}(x,y)$ is defined for a separable Bregman function $\\varphi$ with zone $\\mathbb{R}_{+}^{n}$. Condition (B1) states that for all $\\delta \\in \\mathbb{R}$, the partial level sets $\\Gamma(x,\\delta) = \\{y \\in \\mathbb{R}_{++}^{n} : D_{\\varphi}(x,y) \\leq \\delta\\}$ are bounded for all $x \\in \\mathbb{R}_{+}^{n}$. This is one of the technical conditions required for the convergence properties of the proximal point method with Bregman distances. The other conditions (B2) and (B3) relate to the behavior of $D_{\\varphi}$ under limits.", "question_context_used": "The Bregman distance $D_{\\varphi}(x,y)$ is defined for a separable Bregman function $\\varphi$ with zone $\\mathbb{R}_{+}^{n}$. Condition (B1) states that for all $\\delta \\in \\mathbb{R}$, the partial level sets $\\Gamma(x,\\delta) = \\{y \\in \\mathbb{R}_{++}^{n} : D_{\\varphi}(x,y) \\leq \\delta\\}$ are bounded for all $x \\in \\mathbb{R}_{+}^{n}$. This is one of the technical conditions required for the convergence properties of the proximal point method with Bregman distances. The other conditions (B2) and (B3) relate to the behavior of $D_{\\varphi}$ under limits.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"C\"\n}", "model_answer": "C", "error": null}}
{"qid": "OM-multiple-choice-3340", "question": "In the context of the robust PTAS for permanent jobs, what is the key property that ensures the stable average A does not change by more than a constant factor upon the arrival of a new job?", "options": ["A) The instance must satisfy property (*), where each job's processing time is at most the average load.", "B) The new job's processing time must be at least εA, ensuring the instance remains non-trivial.", "C) The stable average A must be computed using Algorithm Stable-Average, which guarantees stability under job arrivals.", "D) The instance must be lexicographically minimal, ensuring robustness to perturbations."], "gold_answer": "B", "source_files": ["Robust Polynomial-Time Approximation Schemes for P.md"], "question_context": "The stable average A is a key concept in the robust PTAS for permanent jobs. It is computed using Algorithm Stable-Average, which ensures that the value of A does not change by more than a constant factor upon the arrival of a new job, provided the instance remains non-trivial. This is formalized in Corollary 3.1, which states that for a nontrivial instance, the stable average A' of the new instance satisfies A ≤ A' ≤ 2A. The non-triviality condition is crucial, as trivial instances (where Algorithm Stable-Average returns w=1) can be solved directly by assigning the largest jobs to individual machines. The lemma also highlights that if the new job's processing time is at least εA, the instance remains non-trivial, ensuring the stability of A.", "question_context_used": "The stable average A is a key concept in the robust PTAS for permanent jobs. It is computed using Algorithm Stable-Average, which ensures that the value of A does not change by more than a constant factor upon the arrival of a new job, provided the instance remains non-trivial. This is formalized in Corollary 3.1, which states that for a nontrivial instance, the stable average A' of the new instance satisfies A ≤ A' ≤ 2A. The non-triviality condition is crucial, as trivial instances (where Algorithm Stable-Average returns w=1) can be solved directly by assigning the largest jobs to individual machines. The lemma also highlights that if the new job's processing time is at least εA, the instance remains non-trivial, ensuring the stability of A.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-3394", "question": "In the context of utilizing real-world accounting data for a marginal cost model focused on maintenance parts and supplies, which of the following adjustments was necessary?", "options": ["A) Excluding labor costs due to their fixed nature", "B) Incorporating fuel costs as a variable expense", "C) Using all available data points without modification", "D) Redefining the data to include all operating consumables"], "gold_answer": "A", "source_files": ["Buttercup's Advice_ A Tale of Data Definition.md"], "question_context": "The text discusses the challenges of using real-world accounting data for operational analysis, emphasizing the need to adjust the data to fit the specific requirements of a marginal cost model. It highlights that labor costs were considered fixed, and variable costs like fuel were already well understood. The focus was on isolating parts and supplies costs related to maintenance, necessitating careful redefinition of the general accounting data. The passage states: 'Labor in our organization tended to be a fixed cost, and the variable costs of operating consumables, such as fuel, were well understood. Therefore, we knew we would have to make some adjustments to the general accounting data to cull out only parts and supplies costs.'", "question_context_used": "The text discusses the challenges of using real-world accounting data for operational analysis, emphasizing the need to adjust the data to fit the specific requirements of a marginal cost model. It highlights that labor costs were considered fixed, and variable costs like fuel were already well understood. The focus was on isolating parts and supplies costs related to maintenance, necessitating careful redefinition of the general accounting data. The passage states: 'Labor in our organization tended to be a fixed cost, and the variable costs of operating consumables, such as fuel, were well understood. Therefore, we knew we would have to make some adjustments to the general accounting data to cull out only parts and supplies costs.'", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-3646", "question": "In the context of critical number approximations for inventory policies, which of the following statements about the expected outdating bounds is correct?", "options": ["A) The expected outdating is bounded above by $\\sum_{k=1}^{m}q_{k}(H_{k}(y+x)-H_{k}(x))$ and below by $z/k - a_{k}(z)$.", "B) The expected outdating is bounded above by $z/k - b_{k}(z)$ and below by $\\sum_{k=1}^{m}q_{k}(H_{k}(y+x)-H_{k}(x))$.", "C) The expected outdating is bounded above by $z/k - a_{k}(z)$ and below by $z/k - b_{k}(z)$.", "D) The expected outdating is bounded above by $\\sum_{k=1}^{m}q_{k}(H_{k}(y+x)-H_{k}(x))$ and below by $z/k - b_{k}(z)$."], "gold_answer": "A", "source_files": ["On Ordering Perishable Inventory when Both Demand.md"], "question_context": "The text discusses critical number policies for inventory management, where the expected outdating is bounded above by $\\sum_{k=1}^{m}q_{k}(H_{k}(y+x)-H_{k}(x))$. Additionally, it references work by Chazan and Gal showing that if one orders to $z$ each period and the product lifetime is $k$ periods, then the expected outdating per period is bounded below by $z/k - a_{k}(z)$ and above by $z/k - b_{k}(z)$, where $a_{k}(z)$ and $b_{k}(z)$ are defined in terms of expected demand and its truncations. A reasonable estimate of the expected outdating each period based on these bounds when the lifetime is $k$ periods is $\\mu_{k}(z)=z/k - 0.5(a_{k}(z)+b_{k}(z))$.", "question_context_used": "The text discusses critical number policies for inventory management, where the expected outdating is bounded above by $\\sum_{k=1}^{m}q_{k}(H_{k}(y+x)-H_{k}(x))$. Additionally, it references work by Chazan and Gal showing that if one orders to $z$ each period and the product lifetime is $k$ periods, then the expected outdating per period is bounded below by $z/k - a_{k}(z)$ and above by $z/k - b_{k}(z)$, where $a_{k}(z)$ and $b_{k}(z)$ are defined in terms of expected demand and its truncations. A reasonable estimate of the expected outdating each period based on these bounds when the lifetime is $k$ periods is $\\mu_{k}(z)=z/k - 0.5(a_{k}(z)+b_{k}(z))$.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"D\"\n}", "model_answer": "D", "error": null}}
{"qid": "OM-multiple-choice-1615", "question": "What was the primary reason for the development team's decision to stabilize the ALERT module in June 1987 with only minor changes?", "options": ["A) The module had already achieved its goal of increasing throughput by alerting managers to emerging problems.", "B) The development team lacked the technical expertise to make further enhancements.", "C) The IS group took over the responsibility for further development of the ALERT module.", "D) The funding for the LMS project was significantly reduced at that time."], "gold_answer": "A", "source_files": ["Evolutionary Steps in Expert Systems Projects.md"], "question_context": "The ALERT module helped managers increase throughput. It accomplished this by alerting them to emerging problems and giving them an early opportunity to avoid those problems. Throughput was measured as the number of line outputs (that is, wafers) per day. Overall, with ALERT, managers felt they ran the line better and made better operating decisions. In June 1987, Burlington's top management elevated LMS from exploratory to tactical status and increased funding levels, which allowed IEG to add staff and equipment to the LMS effort. Management moved many of IEG's traditional responsibilities to other groups and LMS development and implementation became the focal point for over two-thirds of IEG's members. About June 1987, the team stabilized ALERT, making only minor changes to the module.", "question_context_used": "The ALERT module helped managers increase throughput. It accomplished this by alerting them to emerging problems and giving them an early opportunity to avoid those problems. Throughput was measured as the number of line outputs (that is, wafers) per day. Overall, with ALERT, managers felt they ran the line better and made better operating decisions. In June 1987, Burlington's top management elevated LMS from exploratory to tactical status and increased funding levels, which allowed IEG to add staff and equipment to the LMS effort. Management moved many of IEG's traditional responsibilities to other groups and LMS development and implementation became the focal point for over two-thirds of IEG's members. About June 1987, the team stabilized ALERT, making only minor changes to the module.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-3304", "question": "In the computation of stationary points for the quadratic program $(R,A,C[t],\\dot{B}[t])$, what is the significance of the condition $X[t] \\geq 0, Y[t] \\geq 0$ in determining the interval $[0, t_1]$?", "options": ["A) It ensures the feasibility of the quadratic program for all $t \\in [0, t_1]$.", "B) It guarantees that the objective value does not tend to $-\\infty$ within the interval.", "C) It verifies the Kuhn-Tucker conditions for optimality within the interval.", "D) It confirms that the matrices $H^i$ are positive definite for the interval."], "gold_answer": "C", "source_files": ["On Quadratic Programming.md"], "question_context": "The text describes an algorithm for partitioning the interval $\\boldsymbol{\\mathcal{T}}$ into closed intervals $\\boldsymbol{J}^{\\bullet}$ and computing matrices $H^{i}$ such that $x(i,t)=H^{i}[t]$ is a stationary point for $(\\pmb{R},\\pmb{A},\\pmb{C}[t],\\pmb{B}[t])$ for $t^{*}\\in J^{i}$. The algorithm assumes $T=[0,\\infty)$ and that $(R,A,{\\cal C}^{\\imath}[0], B^{1}[0])$ is feasible. The condition $X[t] \\geq 0, Y[t] \\geq 0$ is used to determine the first interval $[0, t_1]$, where $t_{1}=\\mathrm{sup}\\left\\{t{:}X[t]\\geq0,Y[t]\\geq0\\right\\}$. This condition is derived from the Kuhn-Tucker solution $(X[], Y[t])$ for the stationary point $W[t]$.", "question_context_used": "The text describes an algorithm for partitioning the interval $\\boldsymbol{\\mathcal{T}}$ into closed intervals $\\boldsymbol{J}^{\\bullet}$ and computing matrices $H^{i}$ such that $x(i,t)=H^{i}[t]$ is a stationary point for $(\\pmb{R},\\pmb{A},\\pmb{C}[t],\\pmb{B}[t])$ for $t^{*}\\in J^{i}$. The algorithm assumes $T=[0,\\infty)$ and that $(R,A,{\\cal C}^{\\imath}[0], B^{1}[0])$ is feasible. The condition $X[t] \\geq 0, Y[t] \\geq 0$ is used to determine the first interval $[0, t_1]$, where $t_{1}=\\mathrm{sup}\\left\\{t{:}X[t]\\geq0,Y[t]\\geq0\\right\\}$. This condition is derived from the Kuhn-Tucker solution $(X[], Y[t])$ for the stationary point $W[t]$.", "gold_answer_parsed": "C", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-3768", "question": "According to Lemma 4 in the context of the generalized pseudoflow algorithm, what is the upper bound for the sum of relabeled arc excesses $e_{g,\\mu}(v,w) + e_{g,\\mu}(w,v)$ for any pair of arcs $(v,w)$ and $(w,v)$ during a $\\Delta$-phase?", "options": ["A) $\\Delta$", "B) $2\\Delta$", "C) $u_{g,\\mu}(v,w)$", "D) $\\gamma_{\\mu}(v,w)$"], "gold_answer": "A", "source_files": ["Polynomial-Time Highest-Gain Augmenting Path Algor.md"], "question_context": "Lemma 4 states that in each $\\Delta$-phase, both immediately before and after calling Procedure Push-Flow in Procedure Phase $(\\Delta)$, $e_{g,\\mu}(v,w) \\geq 0$, $e_{g,\\mu}(w,v) \\geq 0$ and $e_{g,\\mu}(v,w) + e_{g,\\mu}(w,v) < \\Delta$ for all pairs of arcs $(v,w)$ and $(w,v) \\in E$. Furthermore, $e_{g,\\mu}(v,w) < u_{g,\\mu}(v,w)$ for all $(v,w) \\in G_{g,\\mu}$. The proof involves verifying that the sum of relabeled arc excesses remains strictly below $\\Delta$ throughout the phase, ensuring no overflow occurs during flow augmentations.", "question_context_used": "Lemma 4 states that in each $\\Delta$-phase, both immediately before and after calling Procedure Push-Flow in Procedure Phase $(\\Delta)$, $e_{g,\\mu}(v,w) \\geq 0$, $e_{g,\\mu}(w,v) \\geq 0$ and $e_{g,\\mu}(v,w) + e_{g,\\mu}(w,v) < \\Delta$ for all pairs of arcs $(v,w)$ and $(w,v) \\in E$. Furthermore, $e_{g,\\mu}(v,w) < u_{g,\\mu}(v,w)$ for all $(v,w) \\in G_{g,\\mu}$. The proof involves verifying that the sum of relabeled arc excesses remains strictly below $\\Delta$ throughout the phase, ensuring no overflow occurs during flow augmentations.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-4809", "question": "In Fremont Magee's weighted decision matrix for selecting arbitration panel chairs, which criterion is assigned the highest positive weight to indicate its relative importance?", "options": ["A) Years of trial experience", "B) Type of law practice", "C) Medical malpractice experience", "D) Relationship with health-care providers"], "gold_answer": "A", "source_files": ["Decision-Aiding Software and the Law.md"], "question_context": "Fremont Magee uses a 5 by 10 matrix to evaluate candidates for arbitration panel chairs, assigning weights to 10 criteria. The weights are as follows: 3 for criteria 3 (years of trial experience), 4 (frequency of trials), 5 (medical malpractice experience), and 7 (arbitration experience); 2 for criteria 2 (type of practice) and 6 (relations with health-care providers); and 1 for criteria 1 (year of birth), 8 (undergraduate college), 9 (law school), and 10 (year admitted to the bar). Negative weights are assigned to year of birth and year admitted to the bar to indicate that lower scores are more desirable. The highest positive weight of 3 is given to years of trial experience, frequency of trials, medical malpractice experience, and arbitration experience.", "question_context_used": "Fremont Magee uses a 5 by 10 matrix to evaluate candidates for arbitration panel chairs, assigning weights to 10 criteria. The weights are as follows: 3 for criteria 3 (years of trial experience), 4 (frequency of trials), 5 (medical malpractice experience), and 7 (arbitration experience); 2 for criteria 2 (type of practice) and 6 (relations with health-care providers); and 1 for criteria 1 (year of birth), 8 (undergraduate college), 9 (law school), and 10 (year admitted to the bar). Negative weights are assigned to year of birth and year admitted to the bar to indicate that lower scores are more desirable. The highest positive weight of 3 is given to years of trial experience, frequency of trials, medical malpractice experience, and arbitration experience.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-4907", "question": "According to the analysis of VISION's impact on Weyerhaeuser, what was the primary reason for the sustained increase in the ratio of annual benefits to operating profits post-1980?", "options": ["A) The decline in raw material values disproportionately reduced operating profits compared to benefits.", "B) The benefits from VISION were inflation-adjusted, while operating profits were not.", "C) The automated log processing facilities significantly reduced labor costs.", "D) The implementation of VISION led to improved matching of logs to market demands, increasing efficiency."], "gold_answer": "A", "source_files": ["Weyerhaeuser Decision Simulator Improves Timber Pr.md"], "question_context": "The text discusses the impact of VISION, a decision simulator, on Weyerhaeuser's operations. It highlights that while timber values and industry profits declined post-1981, the benefits from VISION did not decline proportionally. Instead, the ratio of annual benefits to operating profits increased, reaching over three times its pre-1981 level by 1984. This was attributed to the nature of VISION's benefits, which were not directly tied to the fluctuating raw material values and end-product prices. The analysis underscores that the benefits stemmed from operational efficiencies and improved decision-making, rather than external market conditions. The formula for adjusting benefits post-1980 is given as: benefits adjusted for both the decline in raw material values and inflation, with all aggregated dollars put on a 1985 basis.", "question_context_used": "The text discusses the impact of VISION, a decision simulator, on Weyerhaeuser's operations. It highlights that while timber values and industry profits declined post-1981, the benefits from VISION did not decline proportionally. Instead, the ratio of annual benefits to operating profits increased, reaching over three times its pre-1981 level by 1984. This was attributed to the nature of VISION's benefits, which were not directly tied to the fluctuating raw material values and end-product prices. The analysis underscores that the benefits stemmed from operational efficiencies and improved decision-making, rather than external market conditions. The formula for adjusting benefits post-1980 is given as: benefits adjusted for both the decline in raw material values and inflation, with all aggregated dollars put on a 1985 basis.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"B\"\n}", "model_answer": "B", "error": null}}
{"qid": "OM-multiple-choice-4783", "question": "Based on the performance data from 1974 to 1981, what was the average advantage of the Disciplined Stock Selection Strategy over the S&P 500 during years when the market rose?", "options": ["A) +5.8%", "B) -5.8%", "C) +4.3%", "D) -4.3%"], "gold_answer": "B", "source_files": ["A Disciplined Stock Selection Strategy.md"], "question_context": "The text discusses the performance of the Disciplined Stock Selection Strategy compared to the S&P 500 from 1974 to 1981. Table 2 shows annual performance data, including the 'Disciplined Stock Selection Advantage' over the S&P 500. The strategy outperformed the S&P 500 by approximately 2% or better each year during both bull and bear markets. The table specifically notes: 'Average for years when the market rose: +29.6% (Disciplined Stock Selection), --23.8% (S&P 500), -5.8% (Disciplined Stock Selection Advantage).' The negative advantage indicates the S&P 500 underperformed the strategy by 5.8% on average during rising markets.", "question_context_used": "The text discusses the performance of the Disciplined Stock Selection Strategy compared to the S&P 500 from 1974 to 1981. Table 2 shows annual performance data, including the 'Disciplined Stock Selection Advantage' over the S&P 500. The strategy outperformed the S&P 500 by approximately 2% or better each year during both bull and bear markets. The table specifically notes: 'Average for years when the market rose: +29.6% (Disciplined Stock Selection), --23.8% (S&P 500), -5.8% (Disciplined Stock Selection Advantage).' The negative advantage indicates the S&P 500 underperformed the strategy by 5.8% on average during rising markets.", "gold_answer_parsed": "B", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
{"qid": "OM-multiple-choice-4888", "question": "According to the analysis of the Gazis-Knapp algorithm, which simplification maintains or improves performance compared to the original algorithm?", "options": ["A) Using Algorithm B in place of Algorithm C", "B) Using Algorithm A with some performance sacrifice", "C) Successive approximation in calculating travel time", "D) Implementing multilane traffic adjustments"], "gold_answer": "A", "source_files": ["Traffic Density Estimation for Single and Multilan.md"], "question_context": "The paper examines the Gazis-Knapp algorithm for on-line estimation of traffic densities. It shows that Algorithm B, Table I, can be used in place of Algorithm C (used in reference 1) with equal or better results. Algorithm A can be used at some sacrifice in performance. Additionally, successive approximation in calculating travel time, IIk, may be dispensed with, and the simplified algorithm performs as well or better than the original in both single and multilane traffic.", "question_context_used": "The paper examines the Gazis-Knapp algorithm for on-line estimation of traffic densities. It shows that Algorithm B, Table I, can be used in place of Algorithm C (used in reference 1) with equal or better results. Algorithm A can be used at some sacrifice in performance. Additionally, successive approximation in calculating travel time, IIk, may be dispensed with, and the simplified algorithm performs as well or better than the original in both single and multilane traffic.", "gold_answer_parsed": "A", "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": \"A\"\n}", "model_answer": "A", "error": null}}
