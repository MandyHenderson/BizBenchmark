{"gold_answer": "Analyzing the options based on the background text:\n\n- **Option A** is correct because the equations (4·2) and (3·6) are indeed linear in $\\tilde{\\gamma}_{j}$ and $\\tilde{\\eta}$ and can be solved by a determinant method.\n- **Option B** is correct because the text explicitly states that using (3.8) ensures the $\\theta_{j}$ sum to unity.\n- **Option C** is incorrect because the $\\tilde{\\gamma}_{j}$ are approximate solutions, not exact, to the equations (3·10).\n- **Option D** is correct as the $\\tilde{\\gamma}_{j}$ are described as weighted averages involving $g_{j}$, $\\tilde{\\gamma}_{j-1}$, $\\tilde{\\gamma}_{j+1}$, and $\\mu_{j}$.\n\n**Correct Choices**: A, B, D\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["A_Bayesian_method_for_histograms.md"], "background_text": "Equations (4·2) and (3·6) provide a set of linear equations in the $\\tilde{\\gamma}_{j}$ and $\\tilde{\\eta}$ which can be solved by a determinant method. The estimates $\\tilde{\\gamma}_{j}$ are weighted averages of the maximum likelihood estimate $g_{j}$, the form $\\frac{1}{2}(\\tilde{\\gamma}_{j-1}+\\tilde{\\gamma}_{j+1})-\\frac{1}{2}(\\mu_{j-1}+\\mu_{j+1})+\\mu_{j}$, and $\\mu_{j}+\\gamma$. The weights are $x_{j}$, $2\\tau\\rho$, and $\\tau(1-\\rho)^{2}$, respectively.", "folder": "", "qid": "statistic-mcqm-3487", "question": "In the context of the approximate equations (4·2) and (3·6), which of the following statements are true about the estimates $\\tilde{\\gamma}_{j}$?", "options": ["A) They are derived from linear equations in $\\tilde{\\gamma}_{j}$ and $\\tilde{\\eta}$", "B) They ensure that the $\\theta_{j}$ sum to unity when obtained from (3.8)", "C) They are exact solutions to the equations (3·10)", "D) They are weighted averages involving $g_{j}$, $\\tilde{\\gamma}_{j-1}$, $\\tilde{\\gamma}_{j+1}$, and $\\mu_{j}$"], "category": "PARTIALLY_CORRECT", "question_context_used": "Equations (4·2) and (3·6) provide a set of linear equations in the $\\tilde{\\gamma}_{j}$ and $\\tilde{\\eta}$ which can be solved by a determinant method. The estimates $\\tilde{\\gamma}_{j}$ are weighted averages of the maximum likelihood estimate $g_{j}$, the form $\\frac{1}{2}(\\tilde{\\gamma}_{j-1}+\\tilde{\\gamma}_{j+1})-\\frac{1}{2}(\\mu_{j-1}+\\mu_{j+1})+\\mu_{j}$, and $\\mu_{j}+\\gamma$. The weights are $x_{j}$, $2\\tau\\rho$, and $\\tau(1-\\rho)^{2}$, respectively.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A) $\\mathbf{C}_{2}\\mathbf{A}_{1}^{\\mathrm{T}}\\mathbf{C}_{1}^{-1}\\mathbf{y}$**: Correct, as it is a component in the expression for $\\mathbf{\\Theta}_{1}^{*}$.\n- **B) $\\mathbf{A}_{2}\\mathbf{\\Theta}_{2}^{*}$**: Correct, as it is another component in the expression for $\\mathbf{\\Theta}_{1}^{*}$.\n- **C) $\\mathbf{M}^{-1}$**: Correct, because it is the matrix that multiplies the sum of the other two components to give $\\mathbf{\\Theta}_{1}^{*}$.\n- **D) $\\mathbf{D}_{2}^{*}$**: Incorrect, as it is not directly involved in the calculation of $\\mathbf{\\Theta}_{1}^{*}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["A Bayesian Analysis of Some Pharmacological Data U.md"], "background_text": "The posterior mean of the stage 1 parameters, $\\mathbf{\\Theta}_{1}^{*}$, is given by $\\mathbf{M}^{-1}(\\mathbf{C}_{2}\\mathbf{A}_{1}^{\\mathrm{T}}\\mathbf{C}_{1}^{-1}\\mathbf{y} + \\mathbf{A}_{2}\\mathbf{\\Theta}_{2}^{*})$.", "folder": "", "qid": "statistic-mcqm-6013", "question": "In the posterior distribution of the stage 1 parameters, which components contribute to the mean $\\mathbf{\\Theta}_{1}^{*}$?", "options": ["A) $\\mathbf{C}_{2}\\mathbf{A}_{1}^{\\mathrm{T}}\\mathbf{C}_{1}^{-1}\\mathbf{y}$", "B) $\\mathbf{A}_{2}\\mathbf{\\Theta}_{2}^{*}$", "C) $\\mathbf{M}^{-1}$", "D) $\\mathbf{D}_{2}^{*}$"], "category": "CORRECT", "question_context_used": "The posterior mean of the stage 1 parameters, $\\mathbf{\\Theta}_{1}^{*}$, is given by $\\mathbf{M}^{-1}(\\mathbf{C}_{2}\\mathbf{A}_{1}^{\\mathrm{T}}\\mathbf{C}_{1}^{-1}\\mathbf{y} + \\mathbf{A}_{2}\\mathbf{\\Theta}_{2}^{*})$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A)** Correct as per Theorem 1, which states that the Context algorithm produces context trees whose truncated versions consistently estimate the underlying truncated context tree.\n- **B)** Correct as per Theorem 2, which asserts that the transition probabilities, given finite contexts, are consistently estimated.\n- **C)** Incorrect because Theorem 1 explicitly states that the Context algorithm's asymptotic selection of correct finite dimensional model components cannot be achieved by traditional criteria like AIC or BIC.\n- **D)** Incorrect as the convergence in Kullback–Leibler distance is mentioned in the context of other algorithms (Learn-PSA and Context Tree Weighting algorithm), not the Context algorithm under Theorems 1 and 2.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Scandinavian J Statistics - 2003 - Ferrari - Estim.md"], "background_text": "Under Assumptions 1 and 2, Theorems 1 and 2 guarantee the consistency of the Context algorithm in producing context trees and estimating transition probabilities. The algorithm's power lies in its ability to select the correct finite dimensional model components asymptotically, a feat not achievable by traditional criteria like AIC or BIC due to the large number of possible sub-models.", "folder": "", "qid": "statistic-mcqm-3031", "question": "Under Assumptions 1 and 2, which of the following outcomes are guaranteed by Theorems 1 and 2?", "options": ["A) The Context algorithm produces context trees whose truncated versions consistently estimate the underlying truncated context tree.", "B) The transition probabilities, given finite contexts, are consistently estimated.", "C) The Context algorithm selects asymptotically the correct finite dimensional model components, which can also be achieved by AIC or BIC.", "D) The estimated probability distribution converges to the true underlying probability distribution in Kullback–Leibler distance for fixed finite-order VLMCs."], "category": "PARTIALLY_CORRECT", "question_context_used": "Under Assumptions 1 and 2, Theorems 1 and 2 guarantee the consistency of the Context algorithm in producing context trees and estimating transition probabilities. The algorithm's power lies in its ability to select the correct finite dimensional model components asymptotically, a feat not achievable by traditional criteria like AIC or BIC due to the large number of possible sub-models.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C\n\n**Explanation**: \n- **A) Incorrect**: The text states that $\\sigma$ and $\\overline{\\sigma}$ are random themselves, making their estimation non-classical due to this randomness.\n- **B) Correct**: The text mentions that estimators $\\hat{\\sigma}_{n}^{2}$ and $\\hat{\\overline{\\sigma}}_{n}^{2}$ are derived to separate the parts of the random increments linked to each Brownian integral.\n- **C) Correct**: The text explicitly states that the trajectories $t\\sim(\\sigma^{2}(\\omega)_{t},\\overline{\\sigma}^{2}(\\omega)_{t})$ are estimated pointwise as realizations of the volatility process.\n- **D) Incorrect**: The text indicates that the model cannot be identified for $d=1$, but this is not a requirement for estimating $\\sigma$ and $\\overline{\\sigma}$; estimation is possible for $d \\geq 2$.\n\n**Final Answer**: The final answer is $\\boxed{{B, C}}$", "source": ["Scandinavian J Statistics - 2019 - Féron - Efficie.md"], "background_text": "The text discusses the nonparametric estimation of the volatility components $\\sigma$ and $\\overline{\\sigma}$. It highlights that these components are random, and their estimation involves pointwise estimation of their trajectories. The estimators are designed to separate the contributions from each Brownian motion. The model's identification requires $d \\geq 2$.", "folder": "", "qid": "statistic-mcqm-5780", "question": "Which of the following statements about the estimation of nonparametric components $\\sigma$ and $\\overline{\\sigma}$ in the multidimensional diffusion process are true?", "options": ["A) The nonparametric estimation of $\\sigma$ and $\\overline{\\sigma}$ is classical and does not involve any randomness.", "B) The estimators $\\hat{\\sigma}_{n}^{2}$ and $\\hat{\\overline{\\sigma}}_{n}^{2}$ are derived to separate the parts of the random increments linked to each Brownian integral.", "C) The trajectories $t\\sim(\\sigma^{2}(\\omega)_{t},\\overline{\\sigma}^{2}(\\omega)_{t})$ are estimated pointwise as realizations of the volatility process.", "D) The estimation of $\\sigma$ and $\\overline{\\sigma}$ requires the model to be identified for $d=1$."], "category": "CORRECT", "question_context_used": "The text discusses the nonparametric estimation of the volatility components $\\sigma$ and $\\overline{\\sigma}$. It highlights that these components are random, and their estimation involves pointwise estimation of their trajectories. The estimators are designed to separate the contributions from each Brownian motion. The model's identification requires $d \\geq 2$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**: \n- **A)** Correct. This is the formula provided for the smallest contamination fraction under which the estimator breaks down.\n- **B)** Correct. For the replacement BP, at least $\\lceil n M_{r,n}(\\hat{\\theta}_{r,n})/2\\rceil$ sinusoids need to be replaced.\n- **C)** Incorrect. The breakdown point is dependent on the sample size $n$, as seen in the formulas.\n- **D)** Correct. The breakdown point is determined by the number of sinusoids in the Hough domain, as they influence $M_{r,n}(\\hat{\\theta}_{r,n})$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Statistical-properties-of-the-Hough-transform-esti.md"], "background_text": "The breakdown point of the HT estimator is discussed in terms of the smallest contamination fraction that can cause the estimator to break down, and it is related to the number of sinusoids in the Hough domain.", "folder": "", "qid": "statistic-mcqm-3399", "question": "Regarding the breakdown point of the HT estimator, which of the following statements are correct?", "options": ["A) The smallest contamination fraction under which the estimator breaks down is $(n M_{r,n}(\\hat{\\theta}_{r,n})+1)/(n+n M_{r,n}(\\hat{\\theta}_{r,n})+1)$.", "B) The breakdown point for the replacement BP is at least $\\lceil n M_{r,n}(\\hat{\\theta}_{r,n})/2\\rceil$.", "C) The estimator's breakdown point is independent of the sample size $n$.", "D) The breakdown point is determined by the number of sinusoids in the Hough domain."], "category": "PARTIALLY_CORRECT", "question_context_used": "The breakdown point of the HT estimator is discussed in terms of the smallest contamination fraction that can cause the estimator to break down, and it is related to the number of sinusoids in the Hough domain.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\"answer\": [\"D\"]}", "model_answer": ["D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**:\n- **A)** Correct. Bayesian estimate and finite sample inference of $\\pmb{A}_{0}$ are indeed straightforward through MCMC simulation.\n- **B)** Correct. The objective is to develop a Bayesian framework for finite sample inference and testing of competing hypotheses.\n- **C)** Correct. A conjugate prior is proposed for normalized identified VARs.\n- **D)** Correct. While the Bayes factor is generally challenging, the prior setting allows for a closed form solution.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["A-Bayesian-analysis-of-normalized-VAR-mod_2014_Jou.md"], "background_text": "Bayesian estimate and finite sample inference of $\\pmb{A}_{0}$ are often straightforward to obtain through Markov chain Monte Carlo (MCMC) simulation of the posterior. The objective of the present study is to develop a Bayesian framework for finite sample inference of identified VAR parameters $(\\pmb{A}_{0},\\pmb{\\Phi})$ and testing of competing hypotheses. A conjugate prior is proposed that makes use of the special features of $A_{0}$ in normalized identified VARs. The Bayesian hypothesis requires computation of the Bayes factor, which under the prior setting admits a closed form solution.", "folder": "", "qid": "statistic-mcqm-1782", "question": "Which of the following are true about the Bayesian framework for finite sample inference of identified VAR parameters $(\\pmb{A}_{0},\\pmb{\\Phi})$?", "options": ["A) Bayesian estimate and finite sample inference of $\\pmb{A}_{0}$ are often straightforward to obtain through Markov chain Monte Carlo (MCMC) simulation of the posterior.", "B) The objective of the present study is to develop a Bayesian framework for finite sample inference of identified VAR parameters $(\\pmb{A}_{0},\\pmb{\\Phi})$ and testing of competing hypotheses.", "C) A conjugate prior is proposed that makes use of the special features of $A_{0}$ in normalized identified VARs.", "D) The Bayesian hypothesis requires computation of the Bayes factor, which is computationally challenging for high-dimensional VARs."], "category": "PARTIALLY_CORRECT", "question_context_used": "Bayesian estimate and finite sample inference of $\\pmb{A}_{0}$ are often straightforward to obtain through Markov chain Monte Carlo (MCMC) simulation of the posterior. The objective of the present study is to develop a Bayesian framework for finite sample inference of identified VAR parameters $(\\pmb{A}_{0},\\pmb{\\Phi})$ and testing of competing hypotheses. A conjugate prior is proposed that makes use of the special features of $A_{0}$ in normalized identified VARs. The Bayesian hypothesis requires computation of the Bayes factor, which under the prior setting admits a closed form solution.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: C\n\n- **A) $\\mathbf{A}_{1}$ must be of full column rank**: Incorrect, because the text states that $\\mathbf{M}$ is non-singular even if $\\mathbf{A}_{1}$ is of less than full column rank.\n- **B) $\\mathbf{A}_{2}$ must be of full column rank**: Incorrect, because the condition specified is on $\\mathbf{A}_{1}\\mathbf{A}_{2}$, not individually on $\\mathbf{A}_{2}$.\n- **C) $\\mathbf{A}_{1}\\mathbf{A}_{2}$ must be of full column rank**: Correct, as the text explicitly states this condition for the inverse to exist.\n- **D) $\\mathbf{C}_{2}$ must be non-singular**: Incorrect, because the text mentions that $\\mathbf{M}$ is non-singular even if $\\mathbf{C}_{2}$ is singular.\n\n**Final Answer**: The final answer is $\\boxed{{C}}$", "source": ["A Bayesian Analysis of Some Pharmacological Data U.md"], "background_text": "The text specifies that the inverse required for the calculation of $\\mathbf{D}_{2}^{*}$ exists provided $\\mathbf{A}_{1}\\mathbf{A}_{2}$ is of full column rank. It also mentions that $\\mathbf{M}$ is non-singular even if $\\mathbf{C}_{2}$ is singular and $\\mathbf{A}_{1}$ is of less than full column rank.", "folder": "", "qid": "statistic-mcqm-6018", "question": "Which of the following conditions are required for the inverse in the calculation of $\\mathbf{D}_{2}^{*}$ to exist?", "options": ["A) $\\mathbf{A}_{1}$ must be of full column rank", "B) $\\mathbf{A}_{2}$ must be of full column rank", "C) $\\mathbf{A}_{1}\\mathbf{A}_{2}$ must be of full column rank", "D) $\\mathbf{C}_{2}$ must be non-singular"], "category": "CORRECT", "question_context_used": "The text specifies that the inverse required for the calculation of $\\mathbf{D}_{2}^{*}$ exists provided $\\mathbf{A}_{1}\\mathbf{A}_{2}$ is of full column rank. It also mentions that $\\mathbf{M}$ is non-singular even if $\\mathbf{C}_{2}$ is singular and $\\mathbf{A}_{1}$ is of less than full column rank.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"C\"]\n}", "model_answer": ["C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A) Student’s $t$-distribution with 5 degrees of freedom**: Correct, $\\kappa_{4}$ is mentioned to be the biggest.\n- **B) $\\chi^{2}$ distribution with 3 degrees of freedom**: Correct, it is mentioned in the context of $\\kappa_{4}$ values.\n- **D) $\\chi^{2}$ distribution with 8 degrees of freedom**: Correct, it is mentioned in the context of $\\kappa_{4}$ values.\n- **C) Symmetric uniform distribution $U(-5,5)$**: Incorrect, $\\kappa_{4}$ is not mentioned to be the biggest or have a specific value.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Asymptotic-expansion-of-the-null-distribution-of-t.md"], "background_text": "For (iii) Student’s $t$-distribution we choose the one with 5 degrees of freedom, since $\\kappa_{4}$ is biggest of all. For (iv) uniform distribution we use $U(-5,5)$ with width 10, but our results are not much effected by width. In (v) and (vi) we choose $\\chi^{2}$ distributions with difference of degrees of freedom which are nonsymmetric.", "folder": "", "qid": "statistic-mcqm-1003", "question": "For which distributions is $\\kappa_{4}$ explicitly mentioned to be the biggest or have a specific value?", "options": ["A) Student’s $t$-distribution with 5 degrees of freedom", "B) $\\chi^{2}$ distribution with 3 degrees of freedom", "C) Symmetric uniform distribution $U(-5,5)$", "D) $\\chi^{2}$ distribution with 8 degrees of freedom"], "category": "PARTIALLY_CORRECT", "question_context_used": "For (iii) Student’s $t$-distribution we choose the one with 5 degrees of freedom, since $\\kappa_{4}$ is biggest of all. For (iv) uniform distribution we use $U(-5,5)$ with width 10, but our results are not much effected by width. In (v) and (vi) we choose $\\chi^{2}$ distributions with difference of degrees of freedom which are nonsymmetric.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "Substituting the given values into the formula: $A = 4(100) - 3(20) + 2 - 10 = 400 - 60 + 2 - 10 = 332$. However, none of the options match this calculation, indicating a possible error in the options provided.\n\n**Correct Choices**: None of the options are correct based on the given formula and values.\n\n**Final Answer**: The final answer is $\\boxed{{}}$", "source": ["SPREAD_OF_DISEASES_IN_A_RECTANGULAR_PLANTATION_WITH_VACANCIES.md"], "background_text": "In the queen case, the total number of joins of existent points on the lattice is calculated using the formula $A = 4b - 3a + 2 - V$, where $b$ is the total number of existent points, $a$ is the sum of the lattice dimensions, and $V$ is a correction for vacancies.", "folder": "", "qid": "statistic-mcqm-3464", "question": "In the queen case, the total number of joins of existent points on the lattice is given by $A = 4b - 3a + 2 - V$. If $b = 100$, $a = 20$, and $V = 10$, what is the value of $A$?", "options": ["A) $A = 342$", "B) $A = 352$", "C) $A = 362$", "D) $A = 372$"], "category": "INCORRECT", "question_context_used": "In the queen case, the total number of joins of existent points on the lattice is calculated using the formula $A = 4b - 3a + 2 - V$, where $b$ is the total number of existent points, $a$ is the sum of the lattice dimensions, and $V$ is a correction for vacancies.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"C\"]\n}", "model_answer": ["C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: The equation $z_{j}(\\tilde{\\Upsilon},\\tilde{\\eta})=n e^{\\tilde{\\gamma}_{j}}-x_{j}+\\tau b_{j}(\\tilde{\\Upsilon},\\tilde{\\eta},\\rho)=0$ is directly stated in the context for $j=1,...,s$.\n- **B) Correct**: The partial derivative $\\frac{\\partial z_{j}(\\tilde{\\gamma},\\tilde{\\eta})}{\\partial\\tilde{\\gamma}_{k}}$ is indeed $n e^{\\tilde{\\gamma}_{j}}+\\tau(1+\\rho^{2})$ when $k=j$, as specified in the context.\n- **C) Correct**: For $j=1$ or $s$, the context states that the first derivatives are obtained by replacing $\\tau(1+\\rho^{2})$ by $\\tau$ in the partial derivative expression.\n- **D) Incorrect**: The context explicitly mentions that because of (3·6), iteration on $\\tilde{\\eta}$ is necessary, making this statement false.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["A_Bayesian_method_for_histograms.md"], "background_text": "The context describes an iterative procedure for solving equations (3:10) and (3·6), which are equivalent to $z_{j}(\\tilde{\\Upsilon},\\tilde{\\eta})=n e^{\\tilde{\\gamma}_{j}}-x_{j}+\\tau b_{j}(\\tilde{\\Upsilon},\\tilde{\\eta},\\rho)=0$ for $j=1,...,s$. The partial derivatives of $z_{j}$ with respect to $\\tilde{\\gamma}_{k}$ are given for $j=2,...,s-1$ and for $j=1$ or $s$, with the latter case involving a replacement of $\\tau(1+\\rho^{2})$ by $\\tau$. The solution involves iterating on both $\\tilde{\\Upsilon}$ and $\\tilde{\\eta}$.", "folder": "", "qid": "statistic-mcqm-3478", "question": "Given the iterative procedure for exact modes described, which of the following statements are correct about the equations $z_{j}(\\tilde{\\Upsilon},\\tilde{\\eta})$?", "options": ["A) The equations $z_{j}(\\tilde{\\Upsilon},\\tilde{\\eta})$ are equivalent to $n e^{\\tilde{\\gamma}_{j}}-x_{j}+\\tau b_{j}(\\tilde{\\Upsilon},\\tilde{\\eta},\\rho)=0$ for $j=1,...,s$.", "B) The partial derivative $\\frac{\\partial z_{j}(\\tilde{\\gamma},\\tilde{\\eta})}{\\partial\\tilde{\\gamma}_{k}}$ is $n e^{\\tilde{\\gamma}_{j}}+\\tau(1+\\rho^{2})$ when $k=j$.", "C) For $j=1$ or $s$, the first derivatives are obtained by replacing $\\tau(1+\\rho^{2})$ in the partial derivative expression by $\\tau$.", "D) The equations $z_{j}(\\tilde{\\Upsilon},\\tilde{\\eta})$ can be solved without iterating on $\\tilde{\\eta}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The context describes an iterative procedure for solving equations (3:10) and (3·6), which are equivalent to $z_{j}(\\tilde{\\Upsilon},\\tilde{\\eta})=n e^{\\tilde{\\gamma}_{j}}-x_{j}+\\tau b_{j}(\\tilde{\\Upsilon},\\tilde{\\eta},\\rho)=0$ for $j=1,...,s$. The partial derivatives of $z_{j}$ with respect to $\\tilde{\\gamma}_{k}$ are given for $j=2,...,s-1$ and for $j=1$ or $s$, with the latter case involving a replacement of $\\tau(1+\\rho^{2})$ by $\\tau$. The solution involves iterating on both $\\tilde{\\Upsilon}$ and $\\tilde{\\eta}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct, the knot sequence must be non-decreasing.\n- **B)** Correct, to ensure that the intervals are non-trivial and the B-splines are well-defined.\n- **C)** Correct, $K$ is defined by the sum of the multiplicities $\\nu_{i}$ of the knots.\n- **D)** Correct, each knot $t_{i}$ must appear exactly $\\nu_{i}$ times in the sequence to define the spline space properly.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Universally-Consistent-Regression-Function-Estimat.md"], "background_text": "The knot sequence $\\mathbf{u}$ for the space $S_{\\mathbf{u},M}$ must satisfy: non-decreasing order, $u_{i} < u_{i+M+1}$ to ensure non-trivial intervals, $K$ defined by the sum of multiplicities $\\nu_{i}$, and each knot $t_{i}$ appears exactly $\\nu_{i}$ times in the sequence.", "folder": "", "qid": "statistic-mcqm-4515", "question": "For the space $S_{\\mathbf{u},M}$ of spline functions, which conditions must the knot sequence $\\mathbf{u}$ satisfy?", "options": ["A) $u_{-M} \\leqslant u_{-M+1} \\leqslant \\cdots \\leqslant u_{K+M}$", "B) $u_{i} < u_{i+M+1}$ for all $-M \\leqslant i < K-1$", "C) $K = 1 + \\sum_{i=1}^{\\tilde{K}-1} \\nu_{i}$", "D) Each $t_{i}$ is contained exactly $\\nu_{i}$ times among $u_{1},...,u_{K-1}$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The knot sequence $\\mathbf{u}$ for the space $S_{\\mathbf{u},M}$ must satisfy: non-decreasing order, $u_{i} < u_{i+M+1}$ to ensure non-trivial intervals, $K$ defined by the sum of multiplicities $\\nu_{i}$, and each knot $t_{i}$ appears exactly $\\nu_{i}$ times in the sequence.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**:\n- **Option A** is correct because the trace of the product $\\varphi(a,b,c)\\varphi(x,y,z)$ is indeed $2a x + 2b y + 2c z$ as stated in the context.\n- **Option B** is incorrect because the context does not provide information about the bijectivity of $\\varphi$.\n- **Option C** is correct because the cone $C$ is parameterized by the mapping $\\varphi$ as described in the context.\n- **Option D** is correct because the image of $\\mu$ by $\\varphi$ is explicitly stated to be $m(1,2,2)$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["The-Laplace-transform--dets--pexptr-s-1w--and-the-.md"], "background_text": "The cone $C$ is defined as the set of points $(x,y,z)$ in $\\mathbb{R}^{3}$ such that $x\\geq\\sqrt{y^{2}+z^{2}}$. The mapping $\\varphi$ from $C$ to $\\overline{{\\mathcal{P}_{2}}}$ is defined by $(x,y,z)\\mapsto\\left[\\begin{array}{ll}{x+y}&{z}\\\\ {z}&{x-y}\\end{array}\\right]$. The trace of the product $\\varphi(a,b,c)\\varphi(x,y,z)$ is given by $2a x + 2b y + 2c z$. The image of the measure $\\mu$ by $\\varphi$ is $m(1,2,2)$.", "folder": "", "qid": "statistic-mcqm-2372", "question": "Given the cone $C=\\left\\{(x,y,z)\\in\\mathbb{R}^{3}:x\\geq\\sqrt{y^{2}+z^{2}}\\right\\}$ and the mapping $\\varphi$ from $C$ to $\\overline{{\\mathcal{P}_{2}}}$ defined by $(x,y,z)\\mapsto\\varphi(x,y,z)=\\left[\\begin{array}{ll}{x+y}&{z}\\\\ {z}&{x-y}\\end{array}\\right]$, which of the following statements are correct?", "options": ["A) The trace of the product $\\varphi(a,b,c)\\varphi(x,y,z)$ is $2a x + 2b y + 2c z$.", "B) The mapping $\\varphi$ is bijective.", "C) The cone $C$ is parameterized by the mapping $\\varphi$.", "D) The image of $\\mu$ by $\\varphi$ is $m(1,2,2)$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The cone $C$ is defined as the set of points $(x,y,z)$ in $\\mathbb{R}^{3}$ such that $x\\geq\\sqrt{y^{2}+z^{2}}$. The mapping $\\varphi$ from $C$ to $\\overline{{\\mathcal{P}_{2}}}$ is defined by $(x,y,z)\\mapsto\\left[\\begin{array}{ll}{x+y}&{z}\\\\ {z}&{x-y}\\end{array}\\right]$. The trace of the product $\\varphi(a,b,c)\\varphi(x,y,z)$ is given by $2a x + 2b y + 2c z$. The image of the measure $\\mu$ by $\\varphi$ is $m(1,2,2)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct: $H_{0\\{123\\}}^{*}$ is rejected if the smallest $p$-value $p_{[1]}$ is less than the smallest critical value $c_{1}$.\n- **B)** Correct: $H_{0\\{123\\}}^{*}$ is also rejected if the second smallest $p$-value $p_{[2]}$ is less than the second critical value $c_{2}$.\n- **C)** Correct: $H_{0\\{123\\}}^{*}$ is also rejected if the largest $p$-value $p_{[3]}$ is less than the largest critical value $c_{3}$.\n- **D)** Incorrect: This condition would lead to the acceptance of $H_{0\\{123\\}}^{*}$, not rejection.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Hochberg's_Step-Up_Method_Cutting_Corners_Off_Holm's_Step-Down_Method.md"], "background_text": "The partition test rejects $H_{0\\{123\\}}^{*}$ if any of the ordered $p$-values is less than its corresponding critical value, i.e., $p_{[1]} < c_{1}$ or $p_{[2]} < c_{2}$ or $p_{[3]} < c_{3}$.", "folder": "", "qid": "statistic-mcqm-5962", "question": "Given a partition test with $k=3$ and ordered $p$-values $p_{[1]}, p_{[2]}, p_{[3]}$ and critical values $c_{1} \\leqslant c_{2} \\leqslant c_{3}$, which of the following conditions would lead to the rejection of $H_{0\\{123\\}}^{*}$?", "options": ["A) $p_{[1]} < c_{1}$", "B) $p_{[2]} < c_{2}$", "C) $p_{[3]} < c_{3}$", "D) $p_{[1]} > c_{1}$ and $p_{[2]} > c_{2}$ and $p_{[3]} > c_{3}$"], "category": "CORRECT", "question_context_used": "The partition test rejects $H_{0\\{123\\}}^{*}$ if any of the ordered $p$-values is less than its corresponding critical value, i.e., $p_{[1]} < c_{1}$ or $p_{[2]} < c_{2}$ or $p_{[3]} < c_{3}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**: \n- **A) Correct**: The curse of dimensionality refers to the sparsity of data in high-dimensional spaces, making nonparametric estimation challenging.\n- **B) Incorrect**: The variance of the error term $\\varepsilon$ is constant and does not depend on the dimension of the predictor.\n- **C) Correct**: The Sacks–Ylvisaker condition is a requirement on the design points for consistent estimation in nonparametric regression.\n- **D) Incorrect**: The regression function $g$ does not necessarily become linear in higher dimensions; the challenge is due to the curse of dimensionality.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Analysis-of-Variance-in-Nonparametric-Regress_2001.md"], "background_text": "The nonparametric regression model involves estimating a function $g$ without assuming a parametric form. For higher-dimensional predictors, standard nonparametric methods face the 'curse of dimensionality,' making estimation less practical due to the sparsity of data in high-dimensional spaces.", "folder": "", "qid": "statistic-mcqm-3382", "question": "In the nonparametric regression model $Y = g(t) + \\varepsilon$, where $\\varepsilon$ has mean 0 and variance $\\sigma^2 > 0$, what are the challenges associated with estimating the regression function $g$ for higher-dimensional predictors?", "options": ["A) The curse of dimensionality makes standard nonparametric methods less attractive.", "B) The variance of the error term $\\varepsilon$ increases with the dimension of the predictor.", "C) The design points must satisfy a Sacks–Ylvisaker condition for consistent estimation.", "D) The regression function $g$ becomes linear in higher dimensions."], "category": "PARTIALLY_CORRECT", "question_context_used": "The nonparametric regression model involves estimating a function $g$ without assuming a parametric form. For higher-dimensional predictors, standard nonparametric methods face the 'curse of dimensionality,' making estimation less practical due to the sparsity of data in high-dimensional spaces.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A**: Correct. The third moment matrix $\\mathbf{M}_{3,\\mathbf{x}}$ is indeed a $d^{2}\\times d$ matrix as defined.\n- **B**: Correct. The formula for the third moment of a linear transformation is accurately given by $(\\mathbf{A}\\otimes\\mathbf{A})M_{3,\\mathbf{x}}\\mathbf{A}^{T}$.\n- **C**: Incorrect. The third central moment is the third moment of $\\mathbf{x}-\\pmb{\\mu}}$, not the same as the third moment of $\\mathbf{x}}$.\n- **D**: Correct. The third standardized cumulant is defined as the third cumulant of the standardized random vector $\\mathbf{z}}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Skewness-based-projection-pursuit--A-compu_2018_Co.md"], "background_text": "The third moment of a $d$-dimensional random vector $\\mathbf{x}$ is a $d^{2}\\times d$ matrix $\\mathbf{M}_{3,\\mathbf{x}}=E\\left(\\mathbf{x}\\otimes\\mathbf{x}^{T}\\otimes\\mathbf{x}\\right)$. The third moment of a linear transformation $\\mathbf{A}\\mathbf{x}$ is $(\\mathbf{A}\\otimes\\mathbf{A})M_{3,\\mathbf{x}}\\mathbf{A}^{T}$. The third central moment of $\\mathbf{x}$ is $\\pmb{\\mathrm{K}}_{3,\\mathbf{x}}=\\pmb{\\mathrm{M}}_{3,\\mathbf{x}-\\pmb{\\mu}}$. The third standardized cumulant of $\\mathbf{x}$ is the third cumulant of $\\mathbf{z}=\\Sigma^{-1/2}\\left(\\mathbf{x}-\\pmb{\\mu}\\right)$.", "folder": "", "qid": "statistic-mcqm-5145", "question": "Given a $d$-dimensional random vector $\\mathbf{x}=(X_{1},\\ldots,X_{d})^{T}$ with $E\\left(\\left|X_{i}^{3}\\right|\\right)<+\\infty$ for $i=1,\\ldots,d$, which of the following statements about the third moment matrix $\\mathbf{M}_{3,\\mathbf{x}}$ are correct?", "options": ["A) $\\mathbf{M}_{3,\\mathbf{x}}$ is a $d^{2}\\times d$ matrix.", "B) The third moment of a linear transformation $\\mathbf{A}\\mathbf{x}$ is given by $(\\mathbf{A}\\otimes\\mathbf{A})M_{3,\\mathbf{x}}\\mathbf{A}^{T}$.", "C) The third central moment of $\\mathbf{x}$ is the same as its third moment.", "D) The third standardized cumulant of $\\mathbf{x}$ is the third cumulant of $\\mathbf{z}=\\Sigma^{-1/2}\\left(\\mathbf{x}-\\pmb{\\mu}\\right)$."], "category": "CORRECT", "question_context_used": "The third moment of a $d$-dimensional random vector $\\mathbf{x}$ is a $d^{2}\\times d$ matrix $\\mathbf{M}_{3,\\mathbf{x}}=E\\left(\\mathbf{x}\\otimes\\mathbf{x}^{T}\\otimes\\mathbf{x}\\right)$. The third moment of a linear transformation $\\mathbf{A}\\mathbf{x}$ is $(\\mathbf{A}\\otimes\\mathbf{A})M_{3,\\mathbf{x}}\\mathbf{A}^{T}$. The third central moment of $\\mathbf{x}$ is $\\pmb{\\mathrm{K}}_{3,\\mathbf{x}}=\\pmb{\\mathrm{M}}_{3,\\mathbf{x}-\\pmb{\\mu}}$. The third standardized cumulant of $\\mathbf{x}$ is the third cumulant of $\\mathbf{z}=\\Sigma^{-1/2}\\left(\\mathbf{x}-\\pmb{\\mu}\\right)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct as per Theorem A.1, which states that the given family is a tight frame of $L^{2}(\\mathbb{R}^{2})$.\n- **B)** Correct because the frame bounds are explicitly mentioned to be equal to 1 in the context.\n- **D)** Correct as the context states that this family is a directional wavelet tight frame of $L^{2}(\\mathbb{R}^{2})$.\n\n- **C)** Incorrect because the tight frame is constructed using curvelets, not an orthonormal base of $L^{2}(\\mathbb{R})^{2}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["An-algorithm-for-automatic-curve-det_2011_Computat.md"], "background_text": "The context discusses the construction of tight frames for $L^{2}(\\mathbb{R}^{2})$ using curvelets, with specific families of functions forming tight frames and the frame bounds being equal to 1.", "folder": "", "qid": "statistic-mcqm-2300", "question": "Which of the following statements about the tight frame of $L^{2}(\\mathbb{R}^{2})$ constructed from curvelets are correct?", "options": ["A) The family $\\left\\{\\phi_{j_{0},n}(t),\\rho_{j,k,n}^{\\binom{l_{j}}{j}}(t)\\right\\}_{j\\leq j_{0},0\\leq k\\leq2^{l_{j}}-1,n\\in\\mathbb{Z}^{2}}$ is a tight frame of $L^{2}(\\mathbb{R}^{2})$.", "B) The frame bounds for the tight frame are equal to 1.", "C) The tight frame is constructed using an orthonormal base of $L^{2}(\\mathbb{R})^{2}$.", "D) The family $\\left\\{\\rho_{j,k,n}^{\\left(l_{j}\\right)}(t)\\right\\}_{j\\in\\mathbb{Z},0\\le k\\le2^{l_{j}}-1,n\\in\\mathbb{Z}^{2}}$ is a directional wavelet tight frame of $L^{2}(\\mathbb{R}^{2})$."], "category": "CORRECT", "question_context_used": "The context discusses the construction of tight frames for $L^{2}(\\mathbb{R}^{2})$ using curvelets, with specific families of functions forming tight frames and the frame bounds being equal to 1.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A\n\n**Explanation**:\n- **A)** Correct. The text explicitly states that $0 < p, \\gamma < 1$ are the conditions under which $F(x)$ is completely monotone.\n- **B)** Incorrect. The text does not support these conditions; in fact, they would likely not satisfy the derivative sign requirements.\n- **C)** Incorrect. When $p = 1$ and $\\gamma = 1$, $F(x) = 1$ for all $x$, which trivially is completely monotone, but this case is not covered in the provided text.\n- **D)** Incorrect. Negative values for $p$ and $\\gamma$ are not considered in the text and would not make sense in the context of $F(x) = p^{x^{\\gamma}}$ being real-valued for $x > 0$.\n\n**Final Answer**: The final answer is $\\boxed{{A}}$", "source": ["A Litter-Based Approach to Risk Assessment in Deve.md"], "background_text": "The completely monotone property of $F(x) = p^{x^{\\gamma}}$ is established under the conditions $0 < p, \\gamma < 1$. The derivatives of $F(x)$ and $G(x) = \\log{F(x)}$ are analyzed to show that $(-1)^{k}F^{(k)}(x) \\geqslant 0$ for all $k$ and $x > 0$.", "folder": "", "qid": "statistic-mcqm-5443", "question": "Which of the following conditions ensure that $F(x) = p^{x^{\\gamma}}$ is completely monotone?", "options": ["A) $0 < p < 1$ and $0 < \\gamma < 1$", "B) $p > 1$ and $\\gamma > 1$", "C) $p = 1$ and $\\gamma = 1$", "D) $p < 0$ and $\\gamma < 0$"], "category": "CORRECT", "question_context_used": "The completely monotone property of $F(x) = p^{x^{\\gamma}}$ is established under the conditions $0 < p, \\gamma < 1$. The derivatives of $F(x)$ and $G(x) = \\log{F(x)}$ are analyzed to show that $(-1)^{k}F^{(k)}(x) \\geqslant 0$ for all $k$ and $x > 0$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n- **A)** Correct. The text states that the stability of variance property assures that the information and thereby the asymptotic variance is known.\n- **B)** Incorrect. The text mentions that Wald's test can be misleading in other parametrizations, not all.\n- **C)** Incorrect. The text advises against transforming data for analysis of variance techniques, calling it unnecessary and potentially misleading.\n- **D)** Correct. The text recommends fitting multiplicative models directly rather than using transformation techniques.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Parametrizations of Non-Linear Models.md"], "background_text": "The text discusses the stability of variance property, which assures that the information and asymptotic variance are known. It mentions that Wald's test is appropriate in this context but can be misleading in other parametrizations. It also warns against transforming data to use analysis of variance techniques, recommending instead to fit multiplicative models directly.", "folder": "", "qid": "statistic-mcqm-5239", "question": "Which of the following are true about the stability of variance and related statistical techniques as described in the text?", "options": ["A) The stability of variance property assures that the information and thereby the asymptotic variance is known.", "B) Wald's test is recommended in all parametrizations according to the text.", "C) Transforming data to use analysis of variance techniques, like fitting Latin squares on square roots of Poisson data, is recommended.", "D) Multiplicative models should be fitted directly rather than using transformation techniques."], "category": "CORRECT", "question_context_used": "The text discusses the stability of variance property, which assures that the information and asymptotic variance are known. It mentions that Wald's test is appropriate in this context but can be misleading in other parametrizations. It also warns against transforming data to use analysis of variance techniques, recommending instead to fit multiplicative models directly.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n- **A) Correct**: The text states that $f^{(i)}$ is a bounded increasing function.\n- **B) Incorrect**: The limit exists because $f$ is bounded and supermodular.\n- **C) Correct**: $f^{(i)}(t)$ is used to define the lattice $S_m$ as $S_m(f^{(1)}) \\times \\cdots \\times S_m(f^{(d)})$.\n- **D) Incorrect**: $f^{(i)}(t)$ is increasing, not decreasing.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Some-Remarks-on-the-Supermodular-Order_2000_Journa.md"], "background_text": "The text defines functions $f^{(i)}(t)$ for a bounded supermodular function $f \\in \\mathbf{ISM}$, stating that these limits exist and that $f^{(i)}$ are bounded increasing functions. These functions are used to define a lattice $S_m$ for approximating $f$.", "folder": "", "qid": "statistic-mcqm-4996", "question": "For a bounded supermodular function $f \\in \\mathbf{ISM}$, consider the functions $f^{(i)}(t) := \\lim_{x_j \\to \\infty; j \\neq i} f(x_1, ..., x_{i-1}, t, x_{i+1}, ..., x_d)$ for $i = 1, ..., d$. Which of the following are true?", "options": ["A) $f^{(i)}$ is a bounded increasing function.", "B) The limit defining $f^{(i)}(t)$ does not exist for supermodular functions.", "C) $f^{(i)}(t)$ is used to define the lattice $S_m$ in the approximation of $f$.", "D) $f^{(i)}(t)$ is decreasing for all $i = 1, ..., d$."], "category": "CORRECT", "question_context_used": "The text defines functions $f^{(i)}(t)$ for a bounded supermodular function $f \\in \\mathbf{ISM}$, stating that these limits exist and that $f^{(i)}$ are bounded increasing functions. These functions are used to define a lattice $S_m$ for approximating $f$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. This is the definition of the fractional Sobolev space $W^{\\alpha,2}$.\n- **B)** Correct. $W^{\\alpha,2}$ indeed interpolates between $W^{1,2}$ and $L^{2}$.\n- **C)** Correct. This is a known embedding result for fractional Sobolev spaces.\n- **D)** Correct. This is the statement of the Cramér–Rao inequality in $W^{\\alpha,2}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Functional-Cram-r-Rao-bounds-and-Stein-estimators-.md"], "background_text": "The fractional Sobolev space $W^{\\alpha,2}$ for $\\alpha\\in(0,1)$ is defined as the space of functions $u\\in L^{2}(0,T)$ such that $\\left\\|u\\right\\|_{W_{0}^{\\alpha,2}}^{2}=\\int_{0}^{T}\\int_{0}^{T}\\frac{|u_{t}-u_{s}|^{2}}{|t-s|^{2\\alpha+1}}d t d s<\\infty$. It interpolates between the Sobolev space $W^{1,2}$ and $L^{2}$. For $\\alpha p>1$, $W^{\\alpha,p}(0,T)$ is continuously embedded in $\\mathcal{C}^{\\beta}(0,T)$ with $\\beta=\\alpha-1/p$. The Cramér–Rao inequality in $W^{\\alpha,2}$ states that $\\mathbb{E}^{u}\\left(\\|\\xi-u\\|_{W_{0}^{\\alpha,2}}^{2}\\right)\\geq\\int_{0}^{T}\\int_{0}^{T}\\frac{1}{|t-s|^{2\\alpha}}d t d s$ for every $u\\in\\Theta$.", "folder": "", "qid": "statistic-mcqm-6166", "question": "Which of the following statements about the fractional Sobolev space $W^{\\alpha,2}$ are correct?", "options": ["A) For $\\alpha\\in(0,1)$, the space $W^{\\alpha,2}$ is defined as the space of functions $u\\in L^{2}(0,T)$ such that $\\left\\|u\\right\\|_{W_{0}^{\\alpha,2}}^{2}=\\int_{0}^{T}\\int_{0}^{T}\\frac{|u_{t}-u_{s}|^{2}}{|t-s|^{2\\alpha+1}}d t d s<\\infty$.", "B) $W^{\\alpha,2}$ interpolates between the Sobolev space $W^{1,2}$ and $L^{2}$.", "C) For $\\alpha p>1$, $W^{\\alpha,p}(0,T)$ is continuously embedded in $\\mathcal{C}^{\\beta}(0,T)$ with $\\beta=\\alpha-1/p$.", "D) The Cramér–Rao inequality in $W^{\\alpha,2}$ states that $\\mathbb{E}^{u}\\left(\\|\\xi-u\\|_{W_{0}^{\\alpha,2}}^{2}\\right)\\geq\\int_{0}^{T}\\int_{0}^{T}\\frac{1}{|t-s|^{2\\alpha}}d t d s$ for every $u\\in\\Theta$."], "category": "CORRECT", "question_context_used": "The fractional Sobolev space $W^{\\alpha,2}$ for $\\alpha\\in(0,1)$ is defined as the space of functions $u\\in L^{2}(0,T)$ such that $\\left\\|u\\right\\|_{W_{0}^{\\alpha,2}}^{2}=\\int_{0}^{T}\\int_{0}^{T}\\frac{|u_{t}-u_{s}|^{2}}{|t-s|^{2\\alpha+1}}d t d s<\\infty$. It interpolates between the Sobolev space $W^{1,2}$ and $L^{2}$. For $\\alpha p>1$, $W^{\\alpha,p}(0,T)$ is continuously embedded in $\\mathcal{C}^{\\beta}(0,T)$ with $\\beta=\\alpha-1/p$. The Cramér–Rao inequality in $W^{\\alpha,2}$ states that $\\mathbb{E}^{u}\\left(\\|\\xi-u\\|_{W_{0}^{\\alpha,2}}^{2}\\right)\\geq\\int_{0}^{T}\\int_{0}^{T}\\frac{1}{|t-s|^{2\\alpha}}d t d s$ for every $u\\in\\Theta$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The response data was log-transformed to remove skewness.\n- **B)** Correct. Temperature profiles were smoothed using functional principal component analysis.\n- **C)** Correct. The data was split into training and test sets of size 89 and 16.\n- **D)** Incorrect. The number of eigenfunctions $K$ was selected to explain 95%, not 100%, of the variance.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Additive_Function-on-Function_Regression__Additive_Function-on-Function_Regression.md"], "background_text": "The response data was log-transformed to remove skewness. Temperature profiles were smoothed using functional principal component analysis due to missingness. The data was split into training and test sets of size 89 and 16. The number of eigenfunctions $K$ was selected to explain 95% of the variance.", "folder": "", "qid": "statistic-mcqm-5112", "question": "In the context of the bike rental analysis, which of the following preprocessing steps were applied to the data?", "options": ["A) The response data was log-transformed to remove skewness.", "B) Temperature profiles were smoothed using functional principal component analysis.", "C) The data was split into training and test sets of size 89 and 16, respectively.", "D) The number of eigenfunctions $K$ was selected to explain 100% of the variance."], "category": "CORRECT", "question_context_used": "The response data was log-transformed to remove skewness. Temperature profiles were smoothed using functional principal component analysis due to missingness. The data was split into training and test sets of size 89 and 16. The number of eigenfunctions $K$ was selected to explain 95% of the variance.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A\n\n- **A)** Correct. The text explicitly states that under $H_{0}$, $S_{0}^{*}$ asymptotically follows the $F(k-1,1)$ distribution.\n- **B)** Incorrect. The text does not mention a $\\chi^2$ distribution for $S_{0}^{*}$ under $H_{0}$.\n- **C)** Incorrect. There is no reference to a normal distribution for $S_{0}^{*}$ in the given context.\n- **D)** Incorrect. The $t$-distribution is not mentioned in relation to $S_{0}^{*}$.\n\n**Final Answer**: The final answer is $\boxed{{A}}$", "source": ["A-new-class-of-generalized-log-rank-tests-for-i_20.md"], "background_text": "In the $k$-sample comparison problem, the test statistic $S_{0}^{*}$ is defined and shown to asymptotically follow the $F(k-1,1)$ distribution under the null hypothesis $H_{0}:F_{1}(t)=\\cdots=F_{k}(t)$. This allows for the construction of a test procedure based on this statistic.", "folder": "", "qid": "statistic-mcqm-6140", "question": "For the $k$-sample comparison problem, what distribution does $S_{0}^{*}$ asymptotically follow under the null hypothesis $H_{0}:F_{1}(t)=\\cdots=F_{k}(t)$?", "options": ["A) $F(k-1,1)$ distribution.", "B) $\\chi^2$ distribution with $k-1$ degrees of freedom.", "C) Normal distribution with mean 0 and variance 1.", "D) $t$-distribution with $n-k$ degrees of freedom."], "category": "CORRECT", "question_context_used": "In the $k$-sample comparison problem, the test statistic $S_{0}^{*}$ is defined and shown to asymptotically follow the $F(k-1,1)$ distribution under the null hypothesis $H_{0}:F_{1}(t)=\\cdots=F_{k}(t)$. This allows for the construction of a test procedure based on this statistic.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: C, D\n\n- **Option A**: Incorrect. The location parameter $\\xi$ is not necessarily equal to the expectation $\\mu$; they are related but not identical.\n- **Option B**: Incorrect. The scale parameter $\\psi$ is not equal to the standard deviation $\\sigma$; $\\sigma$ is a function of $\\psi$ and $\\lambda$.\n- **Option C**: Correct. The shape parameter $\\lambda$ directly influences the skewness $\\gamma_1$ of the distribution.\n- **Option D**: Correct. While $\\xi$, $\\psi$, and $\\lambda$ influence the kurtosis $\\beta_2$, they are not equal to it, and their relationship is more complex.\n\n**Final Answer**: The final answer is $\\boxed{{C, D}}$", "source": ["Skewness-based-projection-pursuit--A-compu_2018_Co.md"], "background_text": "The skew-normal distribution is defined with location parameter $\\xi$, scale parameter $\\psi$, and shape parameter $\\lambda$. The expectation $\\mu$, standard deviation $\\sigma$, skewness $\\gamma_1$, and kurtosis $\\beta_2$ are functions of $\\xi$, $\\psi$, and $\\lambda$, but these parameters do not generally equal $\\mu$, $\\sigma$, $\\gamma_1$, or $\\beta_2$.", "folder": "", "qid": "statistic-mcqm-5211", "question": "Given a univariate skew-normal distribution $X \\sim SN(\\xi, \\psi, \\lambda)$, which of the following statements are true about its parameters?", "options": ["A) The location parameter $\\xi$ is equal to the expectation $\\mu$ of the distribution.", "B) The scale parameter $\\psi$ is equal to the standard deviation $\\sigma$ of the distribution.", "C) The shape parameter $\\lambda$ influences the skewness $\\gamma_1$ of the distribution.", "D) The parameters $\\xi$, $\\psi$, and $\\lambda$ are independent of the kurtosis $\\beta_2$ of the distribution."], "category": "PARTIALLY_CORRECT", "question_context_used": "The skew-normal distribution is defined with location parameter $\\xi$, scale parameter $\\psi$, and shape parameter $\\lambda$. The expectation $\\mu$, standard deviation $\\sigma$, skewness $\\gamma_1$, and kurtosis $\\beta_2$ are functions of $\\xi$, $\\psi$, and $\\lambda$, but these parameters do not generally equal $\\mu$, $\\sigma$, $\\gamma_1$, or $\\beta_2$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"C\"]\n}", "model_answer": ["C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n**Explanation**:\n- **Option A** is correct as it directly states the necessary condition from Theorem 4.\n- **Option B** is incorrect because it refers to conditions from Theorem 6, not Theorem 4.\n- **Option C** is incorrect as it describes a property of the distribution of $Y$ under a different transformation, not a condition of Theorem 4.\n- **Option D** is correct because it describes the decomposition of $X$ used in the proof of Theorem 4, where $Z$ is normally distributed.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["A-note-about-measures-and-Jacobians-of-singular_20.md"], "background_text": "Theorem 4 states that for $Y = A X$ with $X \\in \\mathcal{L}_{m,N}^{+}(q)$, $A \\in \\mathcal{L}_{N,p}^{+}(r)$ constant, and $Y \\in \\mathcal{L}_{m,p}^{+}(q)$, the condition $\\operatorname{min}(p, N) \\geqslant r \\geqslant q$ must hold. Additionally, the Jacobian of the transform is given by a specific formula involving the eigenvalues of $A C C^{\\prime} A^{\\prime}$ and $C C^{\\prime}$.", "folder": "", "qid": "statistic-mcqm-5952", "question": "In the context of Theorem 4, where $Y = A X$ with $X \\in \\mathcal{L}_{m,N}^{+}(q)$, $A \\in \\mathcal{L}_{N,p}^{+}(r)$ constant, and $Y \\in \\mathcal{L}_{m,p}^{+}(q)$, which of the following conditions must hold?", "options": ["A) $\\operatorname{min}(p, N) \\geqslant r \\geqslant q$", "B) $r_A \\geqslant q$ and $r_B \\geqslant q$", "C) $Y \\sim \\mathcal{N}_{p \\times s}(A \\mu B, B^{\\prime} \\Sigma B, A \\Theta A^{\\prime})$", "D) $X = C Z E$ where $Z \\sim \\mathcal{N}_{k \\times r}(0, I_r, I_k)$"], "category": "PARTIALLY_CORRECT", "question_context_used": "Theorem 4 states that for $Y = A X$ with $X \\in \\mathcal{L}_{m,N}^{+}(q)$, $A \\in \\mathcal{L}_{N,p}^{+}(r)$ constant, and $Y \\in \\mathcal{L}_{m,p}^{+}(q)$, the condition $\\operatorname{min}(p, N) \\geqslant r \\geqslant q$ must hold. Additionally, the Jacobian of the transform is given by a specific formula involving the eigenvalues of $A C C^{\\prime} A^{\\prime}$ and $C C^{\\prime}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n- **A)** Correct. The design data set $D$ indeed consists of design points and corresponding model outputs.\n- **B)** Incorrect. While latin hypercube sampling is common, it's not the only method; adaptive sampling is also possible.\n- **C)** Incorrect. Design points are for training the emulator, not the points where emulation is needed.\n- **D)** Correct. Adaptive sampling can target regions of large curvature for more efficient emulator training.\n\n**Final Answer**: The final answer is $\boxed{{A, D}}$", "source": ["Mechanism-based-emulation-of-dynamic-simulation-m_.md"], "background_text": "The design data set $D$ for the emulator includes design points in parameter space and their corresponding model outputs. Design points can be generated by methods like latin hypercube sampling or adaptive sampling, the latter aiming to increase density in regions of large curvature. The design points are not the same as the parameter values where output emulation is needed; they are used to train the emulator.", "folder": "", "qid": "statistic-mcqm-5655", "question": "Regarding the design data set $D$ for the emulator, which statements are correct?", "options": ["A) It consists of design points in parameter space and corresponding model outputs.", "B) Design points are always generated by latin hypercube sampling.", "C) The design points correspond to the parameter values at which the model output needs to be emulated.", "D) Adaptive sampling can be used to increase sampling density in regions of large curvature of the response surface."], "category": "CORRECT", "question_context_used": "The design data set $D$ for the emulator includes design points in parameter space and their corresponding model outputs. Design points can be generated by methods like latin hypercube sampling or adaptive sampling, the latter aiming to increase density in regions of large curvature. The design points are not the same as the parameter values where output emulation is needed; they are used to train the emulator.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, D\n\n- **A)** Incorrect. The model variance $\\psi$ is unknown, not known and fixed, as stated in the context.\n- **B)** Correct. The sampling variances $D_i$ are treated as known in the model, often after smoothing estimates.\n- **C)** Incorrect. While $\\psi$ is estimated, the $D_i$ are treated as known, and the distributions of $v_i$ and $e_i$ are assumed, not estimated from the data in the context provided.\n- **D)** Correct. The variance-covariance matrix of $Y$ is correctly given by $\\Sigma(\\psi) = \\mathrm{diag}(D_1, \\ldots, D_m) + \\psi I_m$, combining the known sampling variances and the model variance.\n\n**Final Answer**: The final answer is $\\boxed{{B, D}}$", "source": ["On_measuring_the_variability_of_small_area_estimators_under_a_basic_area_level_model.md"], "background_text": "In the Fay-Herriot model, the model variance $\\psi$ is unknown, while the sampling variances $D_i$ are treated as known. The random effects $v_i$ and sampling errors $e_i$ are assumed to be mutually independent, with $v_i \\sim N(0,\\psi)$ and $e_i \\sim N(0,D_i)$. The variance-covariance matrix of $Y$ is given by $\\Sigma(\\psi) = \\mathrm{diag}(D_1, \\ldots, D_m) + \\psi I_m$.", "folder": "", "qid": "statistic-mcqm-4974", "question": "Which assumptions are made about the variance components in the Fay-Herriot model?", "options": ["A) The model variance $\\psi$ is known and fixed.", "B) The sampling variances $D_i$ are known and treated as fixed.", "C) The random effects $v_i$ and sampling errors $e_i$ have variances that are estimated from the data.", "D) The variance-covariance matrix of $Y$ is $\\Sigma(\\psi) = \\mathrm{diag}(D_1, \\ldots, D_m) + \\psi I_m$."], "category": "CORRECT", "question_context_used": "In the Fay-Herriot model, the model variance $\\psi$ is unknown, while the sampling variances $D_i$ are treated as known. The random effects $v_i$ and sampling errors $e_i$ are assumed to be mutually independent, with $v_i \\sim N(0,\\psi)$ and $e_i \\sim N(0,D_i)$. The variance-covariance matrix of $Y$ is given by $\\Sigma(\\psi) = \\mathrm{diag}(D_1, \\ldots, D_m) + \\psi I_m$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}", "model_answer": ["B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct, as stated in Lemma A1, both statistics converge to zero in probability under strict concavity.\n- **B)** Correct, the lemma states the sup-norm difference converges to zero in probability.\n- **C)** Incorrect, this describes behavior under $H_{1}^{n s}$, not $H_{1}^{s}$.\n- **D)** Correct, $H_{1}^{s}$ is defined as the condition under which the true ordinal dominance curve is strictly concave, and the lemma applies here.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Nonparametric_tests_for_and_against_likelihood_ratio_ordering_in_the_two-sample_problem.md"], "background_text": "Lemma A1 discusses the behavior of test statistics when the true ordinal dominance curve is strictly concave, indicating convergence in probability to zero for certain statistics under specific conditions.", "folder": "", "qid": "statistic-mcqm-5741", "question": "According to Lemma A1, under the condition that the true ordinal dominance curve is strictly concave ($F<_{\\mathrm{LR}}G$), which of the following statements are correct?", "options": ["A) Both $D_{m,n}^{*(12)}$ and $M_{m,n}^{*(12)}$ converge to zero in probability.", "B) The sup-norm difference between the restricted and unrestricted empirical ordinal dominance curves converges in probability to zero.", "C) The test statistics $M_{m,n}^{*(12)}$ and $D_{m,n}^{*(12)}$ converge to distributions that place no probability on zero.", "D) Lemma A1 implies that under $H_{1}^{s}$, both statistics converge in probability to zero."], "category": "CORRECT", "question_context_used": "Lemma A1 discusses the behavior of test statistics when the true ordinal dominance curve is strictly concave, indicating convergence in probability to zero for certain statistics under specific conditions.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **Option A** is correct because the indicator variables $z_{j}$ assign each observation $\\pmb{x_{i}}$ to a particular component.\n- **Option B** is correct because the $z_{j}$ are introduced as hyper-parameters in the hierarchical model.\n- **Option C** is incorrect because the weights $w_{k}$ are separate parameters, not represented by $z_{j}$.\n- **Option D** is incorrect because the parameters $\\pmb{\\theta}_{k}$ are distinct from the indicator variables $z_{j}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Markov_Chain_Monte_Carlo_Convergence_Assessment_via_Two-Way_Analysis_of_Variance__Markov_Chain_Monte_Carlo_Convergence_Assessment_via_Two-Way_Analysis_of_Variance.md"], "background_text": "In finite mixture models, the indicator variables $z_{j}$ assign each observation $\\pmb{x_{i}}$ to a particular component, and they are part of the hierarchical model with conditional distributions.", "folder": "", "qid": "statistic-mcqm-3510", "question": "In the context of finite mixture models, what do the indicator variables $z_{j}$ represent?", "options": ["A) The component assignment of each observation $\\pmb{x_{i}}$", "B) The hyper-parameters for the densities $\\pi_{k}$", "C) The weights $w_{k}$ of the mixture components", "D) The parameters $\\pmb{\\theta}_{k}$ of the densities $\\pi_{k}$"], "category": "PARTIALLY_CORRECT", "question_context_used": "In finite mixture models, the indicator variables $z_{j}$ assign each observation $\\pmb{x_{i}}$ to a particular component, and they are part of the hierarchical model with conditional distributions.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **Option A** is correct as it accurately describes the initialization and steps involved in the EM algorithm.\n- **Option B** is correct because the M-step indeed involves the maximization of the conditional log-likelihood function.\n- **Option C** is correct as the E-step involves computing the conditional expectation of the full data log-likelihood function given the observed data.\n- **Option D** is incorrect because the EM algorithm requires initial parameter values to start the estimation process.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Scandinavian J Statistics - 2015 - Hatefi - Mixtur.md"], "background_text": "The EM algorithm starts with an initial value of the population parameter $\\pmb{\\Omega}^{(0)}$ and involves two steps: the expectation (E-step) and maximization (M-step). The E-step computes the conditional expectation of the full data log-likelihood function given the observed data, and the M-step involves the maximization of the conditional log-likelihood function $\\mathcal{Q}(\\pmb{\\Omega},\\pmb{\\Omega}^{(p)})$ over the parameter space $\\pmb{\\Omega}$ to obtain updated estimates.", "folder": "", "qid": "statistic-mcqm-5463", "question": "Regarding the EM algorithm for parameter estimation in PROS sampling from FMMs, which statements are accurate?", "options": ["A) The EM algorithm starts with an initial value of the population parameter $\\pmb{\\Omega}^{(0)}$ and involves expectation and maximization steps.", "B) The M-step of the EM algorithm involves the maximization of the conditional log-likelihood function $\\mathcal{Q}(\\pmb{\\Omega},\\pmb{\\Omega}^{(p)})$ over the parameter space $\\pmb{\\Omega}$.", "C) The E-step computes the conditional expectation of the full data log-likelihood function given the observed data.", "D) The EM algorithm does not require any initial parameter values to start the estimation process."], "category": "CORRECT", "question_context_used": "The EM algorithm starts with an initial value of the population parameter $\\pmb{\\Omega}^{(0)}$ and involves two steps: the expectation (E-step) and maximization (M-step). The E-step computes the conditional expectation of the full data log-likelihood function given the observed data, and the M-step involves the maximization of the conditional log-likelihood function $\\mathcal{Q}(\\pmb{\\Omega},\\pmb{\\Omega}^{(p)})$ over the parameter space $\\pmb{\\Omega}$ to obtain updated estimates.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The standard deviation is explicitly stated as $2.93 \\pm 0.20$.\n- **B)** Correct. The text mentions that the value is lower than that of the Whitechapel series.\n- **C)** Correct. The text states that the other indices show variabilities consistent with a homogeneous population.\n- **D)** Incorrect. The standard deviation is not significantly greater than that of the Whitechapel English skulls.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["A_FIRST_STUDY_OF_THE_CRANIOLOGY_OF_ENGLAND_AND_SCOTLAND_FROM_NEOLITHIC_TO_EARLY_HISTORIC_TIMES,_WITH_SPECIAL_REFERENCE_TO_THE_ANGLO-SAXON_SKULLS_IN_LONDON_MUSEUMS.md"], "background_text": "The standard deviation of the cephalic index of all the adult skulls excluding the distorted specimens is $2.93 \\pm 0.20$, a value lower than that of the Whitechapel series. The other indices show variabilities that are not at all greater than those of a homogeneous population.", "folder": "", "qid": "statistic-mcqm-3210", "question": "Regarding the standard deviation of the cephalic index of the adult skulls from the Brandon cemetery, which of the following statements are true?", "options": ["A) The standard deviation is $2.93 \\pm 0.20$.", "B) The variability of the cephalic index is lower than that of the Whitechapel series.", "C) The standard deviation suggests a homogeneous population.", "D) The standard deviation is significantly greater than that of the Whitechapel English skulls."], "category": "CORRECT", "question_context_used": "The standard deviation of the cephalic index of all the adult skulls excluding the distorted specimens is $2.93 \\pm 0.20$, a value lower than that of the Whitechapel series. The other indices show variabilities that are not at all greater than those of a homogeneous population.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A)** Correct. The text mentions that synthetic responses are obtained through a transformation that depends on the data.\n- **B)** Incorrect. The text explicitly states that synthetic observations are not i.i.d. even if the original data were.\n- **C)** Correct. The text describes the use of synthetic observations to derive new representations of empirical means involving Kaplan-Meier integrals.\n- **D)** Correct. In the case of artificial stratification, synthetic observations are computed using a Kaplan-Meier estimator specific to the stratum, as mentioned in the text.\n\n**Final Answer**: The final answer is $\boxed{{A, C, D}}$", "source": ["Scandinavian J Statistics - 2008 - DELECROIX - Non.md"], "background_text": "Synthetic responses are obtained from original observations through a data-dependent transformation, making them not i.i.d. even if the original data were. They are used to derive new representations of empirical means involving Kaplan-Meier integrals. In cases of artificial stratification, synthetic observations are computed using a Kaplan-Meier estimator specific to the stratum of the observation.", "folder": "", "qid": "statistic-mcqm-5209", "question": "What are the key features of the synthetic responses used in the proposed estimation method?", "options": ["A) They are obtained through a transformation that depends on the data.", "B) They are always i.i.d. regardless of the original data's properties.", "C) They are used to derive new representations of empirical means involving Kaplan-Meier integrals.", "D) They are obtained by replacing the Kaplan-Meier estimator with a stratified version in the case of artificial stratification."], "category": "CORRECT", "question_context_used": "Synthetic responses are obtained from original observations through a data-dependent transformation, making them not i.i.d. even if the original data were. They are used to derive new representations of empirical means involving Kaplan-Meier integrals. In cases of artificial stratification, synthetic observations are computed using a Kaplan-Meier estimator specific to the stratum of the observation.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**:\n- **A) Correct**: The operator $\\mathcal{G}$ is shown to be unitarily similar to a multiplication operator via the Fourier transform $\\mathcal{F}_{\\times}$ in the context.\n- **B) Correct**: The mapping $\\mathcal{M}$ is indeed defined as $(\\mathcal{M}u)(x)=|x|^{(c+1)/2}u(x)$ and is used in the similarity transform.\n- **C) Correct**: The operator $\\tilde{\\varsigma}$ is correctly defined as $\\tilde{\\varsigma}:=\\mathcal{F}_{\\times}\\mathcal{M G M}^{-1}\\mathcal{F}_{\\times}^{-1}$ and acts on the specified space.\n- **D) Correct**: The similarity between $\\mathcal{G}$ and $\\tilde{\\boldsymbol{\\mathcal{G}}}$ is explicitly stated, including the injectivity condition.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Scandinavian J Statistics - 2021 - Glück - A solut.md"], "background_text": "The operator $\\mathcal{G}$ is a bounded linear operator on $L^{2}(\\mathbb{R},|x|^{c}\\mathrm{~d}x)=L^{2}(\\mathbb{R}^{\\times},|x|^{c}\\mathrm{~d}x)$. The Fourier transform $\\mathcal{F}_{\\times}$ is an operator on $L^{2}(\\mathbb{R}^{\\times},\\mathrm{d}x/|x|)$. The mapping $\\mathcal{M}:L^{2}(\\mathbb{R}^{\\times},|x|^{c}\\mathrm{d}x)\\to L^{2}\\left(\\mathbb{R}^{\\times},\\frac{\\mathrm{d}x}{|x|}\\right)$ defined by $(\\mathcal{M}u)(x)=|x|^{(c+1)/2}u(x)$ is a unitary linear operator. The operator $\\tilde{\\varsigma}$ is defined as $\\tilde{\\varsigma}:=\\mathcal{F}_{\\times}\\mathcal{M G M}^{-1}\\mathcal{F}_{\\times}^{-1}$ and acts on the space $L^{2}(\\mathbb{R}^{\\times},\\mathrm{d}x/|x|)$. The operators $\\mathcal{G}$ and $\\tilde{\\boldsymbol{\\mathcal{G}}}$ are similar in the sense that they are intertwined by a Hilbert space isomorphism, and $\\mathcal{G}$ is injective if and only if $\\tilde{\\boldsymbol{\\mathcal{G}}}$ is injective.", "folder": "", "qid": "statistic-mcqm-5312", "question": "Which of the following statements about the operator $\\mathcal{G}$ and its similarity to a multiplication operator are correct?", "options": ["A) The operator $\\mathcal{G}$ is unitarily similar to a multiplication operator via the Fourier transform $\\mathcal{F}_{\\times}$.", "B) The similarity transform between $\\mathcal{G}$ and the multiplication operator involves the mapping $\\mathcal{M}$ defined by $(\\mathcal{M}u)(x)=|x|^{(c+1)/2}u(x)$.", "C) The operator $\\tilde{\\varsigma}$ is defined as $\\tilde{\\varsigma}:=\\mathcal{F}_{\\times}\\mathcal{M G M}^{-1}\\mathcal{F}_{\\times}^{-1}$ and acts on the space $L^{2}(\\mathbb{R}^{\\times},\\mathrm{d}x/|x|)$.", "D) The operators $\\mathcal{G}$ and $\\tilde{\\boldsymbol{\\mathcal{G}}}$ are similar in the sense that they are intertwined by a Hilbert space isomorphism, and $\\mathcal{G}$ is injective if and only if $\\tilde{\\boldsymbol{\\mathcal{G}}}$ is injective."], "category": "PARTIALLY_CORRECT", "question_context_used": "The operator $\\mathcal{G}$ is a bounded linear operator on $L^{2}(\\mathbb{R},|x|^{c}\\mathrm{~d}x)=L^{2}(\\mathbb{R}^{\\times},|x|^{c}\\mathrm{~d}x)$. The Fourier transform $\\mathcal{F}_{\\times}$ is an operator on $L^{2}(\\mathbb{R}^{\\times},\\mathrm{d}x/|x|)$. The mapping $\\mathcal{M}:L^{2}(\\mathbb{R}^{\\times},|x|^{c}\\mathrm{d}x)\\to L^{2}\\left(\\mathbb{R}^{\\times},\\frac{\\mathrm{d}x}{|x|}\\right)$ defined by $(\\mathcal{M}u)(x)=|x|^{(c+1)/2}u(x)$ is a unitary linear operator. The operator $\\tilde{\\varsigma}$ is defined as $\\tilde{\\varsigma}:=\\mathcal{F}_{\\times}\\mathcal{M G M}^{-1}\\mathcal{F}_{\\times}^{-1}$ and acts on the space $L^{2}(\\mathbb{R}^{\\times},\\mathrm{d}x/|x|)$. The operators $\\mathcal{G}$ and $\\tilde{\\boldsymbol{\\mathcal{G}}}$ are similar in the sense that they are intertwined by a Hilbert space isomorphism, and $\\mathcal{G}$ is injective if and only if $\\tilde{\\boldsymbol{\\mathcal{G}}}$ is injective.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A)** Correct. The value $\\sigma = 0.37 \\mathrm{year}^{-1/2}$ is used as a parameter for the volatility process.\n- **B)** Correct. The value $\\overline{{\\sigma}} = 0.15 \\mathrm{year}^{-1/2}$ is used as a parameter for the volatility process.\n- **C)** Incorrect. The value $\\sigma = 0.15 \\mathrm{year}^{-1/2}$ is not mentioned in the context.\n- **D)** Incorrect. The value $\\overline{{\\sigma}} = 0.37 \\mathrm{year}^{-1/2}$ is not mentioned in the context.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Scandinavian J Statistics - 2019 - Féron - Efficie.md"], "background_text": "In the simulated dataset, the log-prices of the forward contracts are simulated according to a two-factor model. The parameter values are chosen to be close to values estimated on real data, where the volatility processes are constant, and the estimated values are $\\sigma= 0.37 \\mathrm{year}^{-1/2}$ and $\\overline{{\\sigma}}=0.15\\mathrm{year}^{-1/2}$.", "folder": "", "qid": "statistic-mcqm-5814", "question": "Which of the following values are used as parameters for the volatility processes in the simulated data?", "options": ["A) $\\sigma = 0.37 \\mathrm{year}^{-1/2}$", "B) $\\overline{{\\sigma}} = 0.15 \\mathrm{year}^{-1/2}$", "C) $\\sigma = 0.15 \\mathrm{year}^{-1/2}$", "D) $\\overline{{\\sigma}} = 0.37 \\mathrm{year}^{-1/2}$"], "category": "CORRECT", "question_context_used": "In the simulated dataset, the log-prices of the forward contracts are simulated according to a two-factor model. The parameter values are chosen to be close to values estimated on real data, where the volatility processes are constant, and the estimated values are $\\sigma= 0.37 \\mathrm{year}^{-1/2}$ and $\\overline{{\\sigma}}=0.15\\mathrm{year}^{-1/2}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n- **Option A** is correct because it matches the given constraints $\\alpha \\geqslant 0$ and $\\beta > 0$.\n- **Option D** is also correct because it is a stricter version of the constraints where both $\\alpha$ and $\\beta$ must be greater than 0, which is a subset of the given constraints.\n- **Option B** is incorrect because it allows $\\beta$ to be 0, which is not permitted.\n- **Option C** is incorrect because it allows both $\\alpha$ and $\\beta$ to be 0, which is not permitted.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Estimating_the_tumour_onset_distribution_in_animal_carcinogenesis_experiments.md"], "background_text": "The two-parameter Weibull model is given by $F(t;\\theta)=1-\\exp\\left(-\\alpha t^{\\beta}\\right)$ with constraints $\\alpha \\geqslant 0$ and $\\beta > 0$.", "folder": "", "qid": "statistic-mcqm-5076", "question": "In the two-parameter Weibull model $F(t;\\theta)=1-\\exp\\left(-\\alpha t^{\\beta}\\right)$, what are the constraints on the parameters $\\alpha$ and $\\beta$?", "options": ["A) $\\alpha \\geqslant 0, \\beta > 0$", "B) $\\alpha > 0, \\beta \\geqslant 0$", "C) $\\alpha \\geqslant 0, \\beta \\geqslant 0$", "D) $\\alpha > 0, \\beta > 0$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The two-parameter Weibull model is given by $F(t;\\theta)=1-\\exp\\left(-\\alpha t^{\\beta}\\right)$ with constraints $\\alpha \\geqslant 0$ and $\\beta > 0$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**: \n- **Option A** is correct because the penalized version reduces the error for small samples, as mentioned in the context.\n- **Option B** is correct because the penalized version reduces the error for clusters with similar distributions, as mentioned in the context.\n- **Option C** is incorrect because the penalized version does not always reduce the error; for large samples, both procedures lead to similar results.\n- **Option D** is correct because the penalized version reduces the error more significantly for small samples than for large samples, as indicated by the example provided.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Estimating Finite Mixtures of Semi-Markov Chains A.md"], "background_text": "The introduction of the penalty improves the accuracy of the estimates, especially for small samples, few transitions, or with clusters with a similar distribution of the semi-Markov processes. For example, when $n=60$ with only four observed transitions and clusters that are not well separated, $\\mathrm{Err}(\\lambda)=0.70$ without penalty whereas this error is reduced to $\\mathrm{Err}(\\lambda)=0.22$ with the penalty. When the sample size becomes larger ($n=200$ or $n=600$) and the number of transitions is large, both estimation procedures lead to similar results.", "folder": "", "qid": "statistic-mcqm-5775", "question": "Which of the following statements are true regarding the error reduction when using the penalized version of the EM algorithm compared to the unpenalized version?", "options": ["A) The penalized version reduces the error for small samples.", "B) The penalized version reduces the error for clusters with similar distributions.", "C) The penalized version always reduces the error regardless of the sample size.", "D) The penalized version reduces the error more significantly for small samples than for large samples."], "category": "CORRECT", "question_context_used": "The introduction of the penalty improves the accuracy of the estimates, especially for small samples, few transitions, or with clusters with a similar distribution of the semi-Markov processes. For example, when $n=60$ with only four observed transitions and clusters that are not well separated, $\\mathrm{Err}(\\lambda)=0.70$ without penalty whereas this error is reduced to $\\mathrm{Err}(\\lambda)=0.22$ with the penalty. When the sample size becomes larger ($n=200$ or $n=600$) and the number of transitions is large, both estimation procedures lead to similar results.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: The text states that the posterior predictive distribution is a Bernoulli distribution with parameter $\\operatorname{pr}(y_{\\mathrm{new}}=1\\mid y,X,x_{\\mathrm{new}})$.\n- **B) Correct**: Corollary 2 provides an explicit formula for this probability, eliminating the need for sampling or Monte Carlo integration.\n- **C) Incorrect**: The text highlights that the computational burden does not depend on $p$, making this statement false.\n- **D) Incorrect**: The explicit formula in Corollary 2 means that Monte Carlo integration is not required, contrary to this option.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Conjugate_Bayes_for_probit_regression_via_unified_skew-normal_distributions.md"], "background_text": "The posterior predictive distribution for a new response $y_{\\mathrm{new}}$ in the probit model is a Bernoulli distribution. The probability $\\operatorname{pr}(y_{\\mathrm{new}}=1\\mid y,X,x_{\\mathrm{new}})$ is given explicitly in Corollary 2, avoiding the need for Monte Carlo integration. This method is particularly advantageous in large-$p$ settings.", "folder": "", "qid": "statistic-mcqm-5633", "question": "Regarding the prediction of a new response $y_{\\mathrm{new}}$ in the probit model, which of the following statements are correct?", "options": ["A) The posterior predictive distribution is a Bernoulli distribution.", "B) The probability $\\operatorname{pr}(y_{\\mathrm{new}}=1\\mid y,X,x_{\\mathrm{new}})$ can be computed without sampling from the posterior.", "C) The computational burden of prediction increases with the number of covariates $p$.", "D) The prediction requires Monte Carlo integration for $\\int\\Phi(x_{\\mathrm{new}}^{\\mathrm{T}}\\beta)\\pi(\\beta\\mid y,X)\\mathrm{d}\\beta$."], "category": "CORRECT", "question_context_used": "The posterior predictive distribution for a new response $y_{\\mathrm{new}}$ in the probit model is a Bernoulli distribution. The probability $\\operatorname{pr}(y_{\\mathrm{new}}=1\\mid y,X,x_{\\mathrm{new}})$ is given explicitly in Corollary 2, avoiding the need for Monte Carlo integration. This method is particularly advantageous in large-$p$ settings.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. $R_{\\mathcal{P}_{j}^{c}\\mathcal{P}_{j}^{0}$ indeed measures the accuracy of the partition created by $k$-means applied to the sample before censoring, as stated in the context.\n- **B)** Correct. $R_{\\mathcal{P}_{j}\\mathcal{P}_{j}^{0}$ measures the accuracy of the partition created by the algorithm applied to the censored data, which is also confirmed by the context.\n- **C)** Correct. The context explicitly provides these formulas for calculating the mean values of corrected Rand's statistics over the simulated data sets.\n- **D)** Incorrect. The context clearly states that the agreement between partitions decreases as the level of censoring increases, indicating that Rand's statistics are affected by censoring.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Vector-quantization-and-clustering-in-the-prese_20.md"], "background_text": "Rand's statistics are used to measure the accuracy of partitions created by clustering algorithms. $R_{\\mathcal{P}_{j}^{c}\\mathcal{P}_{j}^{0}$ refers to the accuracy of the partition created by $k$-means applied to the sample before censoring, while $R_{\\mathcal{P}_{j}\\mathcal{P}_{j}^{0}$ refers to the accuracy of the partition created by the algorithm applied to the censored data. The mean values of these statistics are calculated over simulated data sets. The agreement between partitions decreases as the proportion of censored observations increases.", "folder": "", "qid": "statistic-mcqm-6032", "question": "Given the context of simulating data samples and applying clustering algorithms, which of the following statements are true about Rand's statistics?", "options": ["A) $R_{\\mathcal{P}_{j}^{c}\\mathcal{P}_{j}^{0}$ measures the accuracy of the partition created by $k$-means applied to the sample before censoring.", "B) $R_{\\mathcal{P}_{j}\\mathcal{P}_{j}^{0}$ measures the accuracy of the partition created by the algorithm described in Section 5 applied to the censored data.", "C) The mean values of corrected Rand's statistics over the simulated data sets are calculated as $R_{\\mathcal{P P}^{0}}^{N}=\\frac{1}{N}\\sum_{j=1}^{n}R_{\\mathcal{P}_{j}\\mathcal{P}_{j}^{0}$ and $R_{\\mathcal{P}^{c}\\mathcal{P}^{0}}^{N}=\\frac{1}{n}\\sum_{1}^{n}R_{\\mathcal{P}_{j}^{c}\\mathcal{P}_{j}^{0}$.", "D) Rand's statistics are not affected by the level of censoring in the data."], "category": "PARTIALLY_CORRECT", "question_context_used": "Rand's statistics are used to measure the accuracy of partitions created by clustering algorithms. $R_{\\mathcal{P}_{j}^{c}\\mathcal{P}_{j}^{0}$ refers to the accuracy of the partition created by $k$-means applied to the sample before censoring, while $R_{\\mathcal{P}_{j}\\mathcal{P}_{j}^{0}$ refers to the accuracy of the partition created by the algorithm applied to the censored data. The mean values of these statistics are calculated over simulated data sets. The agreement between partitions decreases as the proportion of censored observations increases.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C\n\n**Explanation**:\n- **A) Incorrect**: The text states that the covariance term is not needed for deriving means and variances in the sub-spaces.\n- **B) Correct**: The text mentions that the covariance term is not needed for deriving means and variances but is required in the filtering step and will be used in the smoothing step.\n- **C) Correct**: The covariance term is derived from the covariance between the \"design\" and \"emulation\" sub-spaces, as indicated by the notation.\n- **D) Incorrect**: The covariance term does not increase the dimensionality of the state space; it relates existing sub-spaces.\n\n**Final Answer**: The final answer is $\\boxed{{B, C}}$", "source": ["Mechanism-based-emulation-of-dynamic-simulation-m_.md"], "background_text": "The covariance term (58c) between the \"design\" and \"emulation\" sub-spaces is not needed for deriving means and variances in these sub-spaces but is required in the filtering step below and will be used in the smoothing step.", "folder": "", "qid": "statistic-mcqm-5651", "question": "Which of the following statements are true about the covariance term (58c) in the context of the Kalman filtering steps?", "options": ["A) It is necessary for deriving means and variances in the \"design\" and \"emulation\" sub-spaces", "B) It is not needed for the filtering step but is required for the smoothing step", "C) It is derived from the covariance between the \"design\" and \"emulation\" sub-spaces", "D) It is used to increase the dimensionality of the state space"], "category": "PARTIALLY_CORRECT", "question_context_used": "The covariance term (58c) between the \"design\" and \"emulation\" sub-spaces is not needed for deriving means and variances in these sub-spaces but is required in the filtering step below and will be used in the smoothing step.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The Markov model's assumption is explicitly stated in the context.\n- **B)** Correct. The semi-Markov model's dependence on the age of the tumour is clearly mentioned.\n- **C)** Correct. The context specifies that the semi-Markov model's interpretability is contingent on constant differences between death times.\n- **D)** Correct. The context mentions that closed form solutions for maximum likelihood estimates do not exist, necessitating numerical methods.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["Estimating_the_tumour_onset_distribution_in_animal_carcinogenesis_experiments.md"], "background_text": "The Markov model assumes that the probability of dying at time $t_{i}$ given the tumour has occurred is independent of when the tumour occurred, while the semi-Markov model assumes it depends only on the age of the tumour. The semi-Markov model is interpretable only under constant time differences between death times. Both models require numerical methods for maximum likelihood estimation as closed form solutions do not exist.", "folder": "", "qid": "statistic-mcqm-5065", "question": "Which of the following statements are true about the Markov and semi-Markov models described in the context?", "options": ["A) The Markov model assumes the probability of dying at time $t_{i}$ is independent of when the tumour occurred.", "B) The semi-Markov model assumes the probability of dying at time $t_{i}$ depends only on the age of the tumour.", "C) The semi-Markov model is interpretable only when the difference between death times is constant.", "D) The Markov model requires numerical methods for maximum likelihood estimation because closed form solutions do not exist."], "category": "CORRECT", "question_context_used": "The Markov model assumes that the probability of dying at time $t_{i}$ given the tumour has occurred is independent of when the tumour occurred, while the semi-Markov model assumes it depends only on the age of the tumour. The semi-Markov model is interpretable only under constant time differences between death times. Both models require numerical methods for maximum likelihood estimation as closed form solutions do not exist.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The model is indeed specified separately for each stratum, as mentioned in the context.\n- **B)** Correct. The odds ratio in each stratum is correctly given by the specified form.\n- **C)** Incorrect. The context explicitly states that stratum sizes must be allowed to become large as the overall sample size becomes large, contradicting this option.\n- **D)** Correct. The context discusses the equivalence of applying the model directly to case-control data and the induced retrospective logistic model.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Logistic_disease_incidence_models_and_case-control_studies.md"], "background_text": "A prospective logistic disease incidence model is specified in each stratum, with the odds ratio in each stratum being of the form $\\exp(\\{z-z_{0}\\}\\beta_{i}\\}(\\mathfrak{i}=1,...,k)$. Stratum sizes must be allowed to become large as the overall sample size becomes large, with a nonzero limiting ratio, for the asymptotic likelihood theory to apply.", "folder": "", "qid": "statistic-mcqm-2251", "question": "In the context of case-control studies with stratified sampling, which of the following statements are true about the logistic disease incidence model?", "options": ["A) The model is specified separately for each stratum.", "B) The odds ratio in each stratum is of the form $\\exp(\\{z-z_{0}\\}\\beta_{i}\\}(\\mathfrak{i}=1,...,k)$.", "C) The model assumes that stratum sizes remain fixed as the overall sample size increases.", "D) The model can be applied directly to case-control data without any modifications."], "category": "PARTIALLY_CORRECT", "question_context_used": "A prospective logistic disease incidence model is specified in each stratum, with the odds ratio in each stratum being of the form $\\exp(\\{z-z_{0}\\}\\beta_{i}\\}(\\mathfrak{i}=1,...,k)$. Stratum sizes must be allowed to become large as the overall sample size becomes large, with a nonzero limiting ratio, for the asymptotic likelihood theory to apply.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The sum of squares for assigned weights is explicitly given as 31.7844.\n- **B)** Correct. The mean square for departures from assigned weights is given as 0.3092 (n).\n- **C)** Correct. The sum of squares for interactions is explicitly given as 1.7851.\n- **D)** Correct. The total sum of squares for A+AB is explicitly given as 34.1878.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["THE_INTERPRETATION_OF_INTERACTIONS_IN_FACTORIAL_EXPERIMENTS.md"], "background_text": "The table shows degrees of freedom, sum of squares, and mean squares for assigned weights, departures from assigned weights, and interactions. The total sum of squares for A+AB is 34.1878.", "folder": "", "qid": "statistic-mcqm-2017", "question": "In the weighted analysis of variance table provided, which of the following statements are correct regarding the sum of squares and mean squares?", "options": ["A) The sum of squares for assigned weights is 31.7844.", "B) The mean square for departures from assigned weights is 0.3092.", "C) The sum of squares for interactions is 1.7851.", "D) The total sum of squares (A+AB) is 34.1878."], "category": "PARTIALLY_CORRECT", "question_context_used": "The table shows degrees of freedom, sum of squares, and mean squares for assigned weights, departures from assigned weights, and interactions. The total sum of squares for A+AB is 34.1878.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A) Correct**: Simes' test controls the Familywise Error Rate strongly at $\\alpha$ when the test statistics are independent.\n- **B) Correct**: Simes' test also controls the Familywise Error Rate strongly at $\\alpha$ when the test statistics have a distribution with the multivariate total positivity of order two property or a scale mixture thereof.\n- **C) Incorrect**: Normality of the test statistics is not mentioned as a condition for Simes' test to control the Familywise Error Rate.\n- **D) Incorrect**: Identical distribution of the test statistics is not mentioned as a condition for Simes' test to control the Familywise Error Rate.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Hochberg's_Step-Up_Method_Cutting_Corners_Off_Holm's_Step-Down_Method.md"], "background_text": "Simes' test is a level-$\\alpha$ test for $H_{0I}$ and controls the Familywise Error Rate strongly at $\\alpha$ when the test statistics are independent or have a distribution with the multivariate total positivity of order two property or a scale mixture thereof.", "folder": "", "qid": "statistic-mcqm-5965", "question": "Under what conditions does Simes' test control the Familywise Error Rate strongly at $\\alpha$?", "options": ["A) When the test statistics are independent.", "B) When the test statistics have a distribution with the multivariate total positivity of order two property.", "C) When the test statistics are normally distributed.", "D) When the test statistics are identically distributed."], "category": "CORRECT", "question_context_used": "Simes' test is a level-$\\alpha$ test for $H_{0I}$ and controls the Familywise Error Rate strongly at $\\alpha$ when the test statistics are independent or have a distribution with the multivariate total positivity of order two property or a scale mixture thereof.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The category 'always wrong' (category 1) has the highest observed count (140) in the table.\n- **B) Correct**: The category 'not wrong' (category 4) has adjusted residuals exceeding 2.0, as indicated by the asterisks in the table.\n- **C) Incorrect**: The category 'wrong only sometimes' (category 3) does have observed counts in the table (e.g., 0, 1, 2, etc.).\n- **D) Correct**: For some responses, the fitted values for 'almost always wrong' (category 2) match the observed counts exactly (e.g., 30 vs. 30.4, 3 vs. 3.1).\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["A_proportional_odds_model_with_subject-specific_effects_for_repeated_ordered_categorical_responses.md"], "background_text": "The data is from the 1989 General Social Survey, with categories for opinions on extramarital sex defined as $1=$ always wrong, $2=$ almost always wrong, $3=$ wrong only sometimes, $4=$ not wrong. Adjusted residuals exceeding 2.0 are noted with an asterisk in the table.", "folder": "", "qid": "statistic-mcqm-5608", "question": "Given the proportional odds Rasch model and the data from the 1989 General Social Survey, which of the following statements about the categories for opinions on extramarital sex are correct? (Categories: $1=$ always wrong, $2=$ almost always wrong, $3=$ wrong only sometimes, $4=$ not wrong)", "options": ["A) The category 'always wrong' has the highest observed count in the table.", "B) The category 'not wrong' has an adjusted residual exceeding 2.0 for some responses.", "C) The category 'wrong only sometimes' has no observed counts in the table.", "D) The category 'almost always wrong' has fitted values that match the observed counts exactly."], "category": "INVALID_FORMAT", "question_context_used": "The data is from the 1989 General Social Survey, with categories for opinions on extramarital sex defined as $1=$ always wrong, $2=$ almost always wrong, $3=$ wrong only sometimes, $4=$ not wrong. Adjusted residuals exceeding 2.0 are noted with an asterisk in the table.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**: \n- **Option A** is correct because the theorem explicitly requires the positivity condition to hold.\n- **Option B** is correct as the positivity condition, apart from trivial cases, results in the support $S$ being a Cartesian product of $n$ sets.\n- **Option D** is correct because the theorem's application depends on the existence of such a point and permutation where the conditional densities are positive.\n- **Option C** is incorrect because the theorem does not require the joint density $\\pi$ to be continuous; it can apply to discrete or mixed cases as well.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["An-extension-of-the-factorization-theorem-to-the_2.md"], "background_text": "The Hammersley-Clifford Theorem applies under the positivity condition, which implies certain properties about the support $S$ and requires the existence of a point and permutation where specific conditional densities are positive.", "folder": "", "qid": "statistic-mcqm-2775", "question": "In the context of the Hammersley-Clifford Theorem, which of the following are necessary for the theorem's application?", "options": ["A) The positivity condition must be satisfied.", "B) The support $S$ of the density $\\pi$ must be a Cartesian product of $n$ sets.", "C) The joint density $\\pi$ must be continuous.", "D) There must exist at least one point $x^{\\prime}\\in S$ and a permutation $(r_{1},\\ldots,r_{n})$ such that certain conditional densities are positive."], "category": "PARTIALLY_CORRECT", "question_context_used": "The Hammersley-Clifford Theorem applies under the positivity condition, which implies certain properties about the support $S$ and requires the existence of a point and permutation where specific conditional densities are positive.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A) Birnbaum–Saunders**: Correct, as it is explicitly mentioned in the context.\n- **B) Gamma**: Correct, as it is explicitly mentioned in the context.\n- **C) Poisson**: Incorrect, the Poisson distribution is not mentioned in the context.\n- **D) Weibull**: Correct, as it is explicitly mentioned in the context.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["On-the-Fern-ndez-Steel-distribution--Infere_2011_C.md"], "background_text": "Distributions commonly employed for modeling data from earth and environmental sciences are usually positively skewed and unimodal, and have often two parameters. These distributions include the Birnbaum–Saunders, gamma, inverse Gaussian, inverted beta, lognormal and Weibull models.", "folder": "", "qid": "statistic-mcqm-5910", "question": "Which of the following distributions are mentioned as commonly used for modeling positively skewed and unimodal data in earth and environmental sciences?", "options": ["A) Birnbaum–Saunders", "B) Gamma", "C) Poisson", "D) Weibull"], "category": "CORRECT", "question_context_used": "Distributions commonly employed for modeling data from earth and environmental sciences are usually positively skewed and unimodal, and have often two parameters. These distributions include the Birnbaum–Saunders, gamma, inverse Gaussian, inverted beta, lognormal and Weibull models.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **A) Correct**: The text explicitly states that the methodology provides optimal spatial predictions at multiple resolutions.\n- **B) Correct**: The mass balance guarantees consistent predictors and prediction variances as resolution requirements change.\n- **C) Correct**: The methodology allows for the incorporation of data at different levels of resolution.\n- **D) Incorrect**: While the text mentions that the methodology can handle massive amounts of possibly irregularly sampled data, it does not claim it is the only methodology capable of doing so.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Fast,_Resolution-Consistent_Spatial_Prediction_of_Global_Processes_From_Satellite_Data__Fast__Resolution-Consistent_Spatial_Prediction_of_Global_Processes_From_Satellite_Data.md"], "background_text": "The advantages of this methodology are first that it provides optimal spatial predictions at multiple resolutions, and associated prediction standard errors. Second, the mass balance guarantees consistent predictors and prediction variances as resolution requirements change, according to whether predictions are to be used in local, regional, or global calculations. This property also allows us to incorporate data at different levels of resolution. Third, and by no means least, our spatial prediction algorithms are extremely fast, allowing us to handle massive amounts of possibly irregularly sampled data.", "folder": "", "qid": "statistic-mcqm-5435", "question": "What are the advantages of the methodology based on mass-balanced, tree-structured models as mentioned in the text?", "options": ["A) It provides optimal spatial predictions at multiple resolutions.", "B) It guarantees consistent predictors and prediction variances as resolution requirements change.", "C) It allows for the incorporation of data at different levels of resolution.", "D) It is the only methodology that can handle massive amounts of irregularly sampled data."], "category": "CORRECT", "question_context_used": "The advantages of this methodology are first that it provides optimal spatial predictions at multiple resolutions, and associated prediction standard errors. Second, the mass balance guarantees consistent predictors and prediction variances as resolution requirements change, according to whether predictions are to be used in local, regional, or global calculations. This property also allows us to incorporate data at different levels of resolution. Third, and by no means least, our spatial prediction algorithms are extremely fast, allowing us to handle massive amounts of possibly irregularly sampled data.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Solution**:\n\n1. **Option A**: Correct. $D$ is defined as $0.01 \\times I$, which means it is a diagonal matrix with all diagonal elements equal to 0.01.\n\n2. **Option B**: Correct. $T$ is defined as $- (\\phi_{i j})$ with $\\phi_{i i} = 1$, so the diagonal elements of $T$ are $-1$. The matrix is lower triangular because $\\phi_{i j} = 0$ for $i < j$.\n\n3. **Option C**: Correct. The non-zero elements of $T$ are only on the diagonal ($\\phi_{i i} = 1$) and the subdiagonal ($\\phi_{i+1, i} = 0.8$).\n\n4. **Option D**: Correct. $C$ is defined as $T^{\\prime} D^{-1} T$, and since $D$ is diagonal and $T$ is lower triangular, $C$ is symmetric.\n\n**Correct Choices**: A, B, C, D\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Large_Gaussian_Covariance_Matrix_Estimation_With_Markov_Structures__Large_Gaussian_Covariance_Matrix_Estimation_With_Markov_Structures.md"], "background_text": "Model 5 is defined by $C=T^{\\prime}D^{-1}T$, where $D=0.01\\times I$ (a diagonal matrix with 0.01 on the diagonal), and $T=-(\\phi_{i j})$ with specific conditions on $\\phi_{i j}$: $\\phi_{i i}=1$, $\\phi_{i+1,i}=0.8$, and $\\phi_{i j}=0$ otherwise.", "folder": "", "qid": "statistic-mcqm-5353", "question": "Given Model 5: $C=T^{\\prime}D^{-1}T$, where $D=0.01\\times I$, and $T=-(\\phi_{i j})$, with $\\phi_{i i}=1$, $\\phi_{i+1,i}=0.8$, and $\\phi_{i j}=0$ otherwise, which of the following statements are correct?", "options": ["A) The matrix $D$ is a diagonal matrix with all diagonal elements equal to 0.01.", "B) The matrix $T$ is a lower triangular matrix with diagonal elements equal to -1.", "C) The matrix $T$ has non-zero elements only on the diagonal and the subdiagonal.", "D) The matrix $C$ is symmetric."], "category": "CORRECT", "question_context_used": "Model 5 is defined by $C=T^{\\prime}D^{-1}T$, where $D=0.01\\times I$ (a diagonal matrix with 0.01 on the diagonal), and $T=-(\\phi_{i j})$ with specific conditions on $\\phi_{i j}$: $\\phi_{i i}=1$, $\\phi_{i+1,i}=0.8$, and $\\phi_{i j}=0$ otherwise.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "To determine the correct options, let's analyze each statement based on the provided context:\n\n- **Option A**: This is correct. The text explicitly states that the approximate Bahadur efficiency of $W_{N}$ relative to $V_{N}$ is greater than or equal to one at every alternative.\n\n- **Option B**: This is correct. The text mentions that when the set of distribution functions contains exactly two distinct distributions, the approximate Bahadur efficiency is exactly one.\n\n- **Option C**: This is incorrect. The text shows that the efficiency is greater than or equal to one, not less than one.\n\n- **Option D**: This is incorrect. The text does not state that the efficiency can be greater than one only if the distribution functions are bimodal. In fact, it provides examples where the efficiency can be greater than one under other conditions.\n\n**Correct Choices**: A, B\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["Pairwise_versus_joint_ranking_Another_look_at_the_Kruskal-Wallis_statistic.md"], "background_text": "The approximate Bahadur efficiency of the pairwise rank analogue $W_{N}$ relative to the Kruskal-Wallis statistic $V_{N}$ is shown to be greater than or equal to one at every alternative. It is exactly one when the set of distribution functions contains exactly two distinct distributions. The efficiency results are explained by the algebraic relationship between $V_{N}$ and $W_{N}$, and the contribution of the random variable $Z_{N}$ under fixed alternatives.", "folder": "", "qid": "statistic-mcqm-5490", "question": "Given the Kruskal-Wallis statistic $V_{N}$ and its pairwise rank analogue $W_{N}$, which of the following statements are true regarding their approximate Bahadur efficiency?", "options": ["A) The approximate Bahadur efficiency of $W_{N}$ relative to $V_{N}$ is greater than or equal to one at every alternative.", "B) The approximate Bahadur efficiency of $W_{N}$ relative to $V_{N}$ is exactly one when the set of distribution functions contains exactly two distinct distributions.", "C) The approximate Bahadur efficiency of $W_{N}$ relative to $V_{N}$ is less than one under all alternatives.", "D) The approximate Bahadur efficiency of $W_{N}$ relative to $V_{N}$ can be greater than one only if the distribution functions are bimodal."], "category": "CORRECT", "question_context_used": "The approximate Bahadur efficiency of the pairwise rank analogue $W_{N}$ relative to the Kruskal-Wallis statistic $V_{N}$ is shown to be greater than or equal to one at every alternative. It is exactly one when the set of distribution functions contains exactly two distinct distributions. The efficiency results are explained by the algebraic relationship between $V_{N}$ and $W_{N}$, and the contribution of the random variable $Z_{N}$ under fixed alternatives.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**: \n- **A)** Correct. The text states that if $Z(\\boldsymbol{\\mathbf{\\uptheta}})$ is stochastically increasing in $\\boldsymbol{\\mathbf{\\uptheta}}$, it is also Schur-convex.\n- **B)** Incorrect. The text does not explicitly state that stochastic decreasing implies Schur-concavity; it only discusses the increasing case.\n- **C)** Correct. The notation $\\mathbf{SI}.\\mathbf{SchurC}\\mathbf{X}(\\mathrm{st})$ is defined in the text as indicating stochastic increasing and Schur-convexity.\n- **D)** Correct. The text explains that stochastic Schur-convexity implies $Z(\\bullet)\\geqslant_{\\mathrm{st}}Z(\\bullet^{\\prime})$ when $\\boldsymbol{\\mathbf{\\uptheta}}\\geqslant_{\\mathrm{wm}}\\boldsymbol{\\mathbf{\\uptheta}}^{\\prime}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Parametric-Schur-Convexity-and-Arrangement-Monoton.md"], "background_text": "The text discusses the conditions under which $Z(\\boldsymbol{\\mathbf{\\uptheta}})$ is stochastically increasing (decreasing) and Schur-convex (concave). It introduces notations like $\\mathbf{SI}.\\mathbf{SchurC}\\mathbf{X}(\\mathrm{st})$ for stochastic increasing and Schur-convexity, and mentions that stochastic Schur-convexity implies certain stochastic order relations.", "folder": "", "qid": "statistic-mcqm-3119", "question": "Which of the following statements are true regarding the stochastic order and Schur-convexity of $Z(\\boldsymbol{\\mathbf{\\uptheta}})$?", "options": ["A) If $Z(\\boldsymbol{\\mathbf{\\uptheta}})$ is stochastically increasing in $\\boldsymbol{\\mathbf{\\uptheta}}$, then it is Schur-convex.", "B) If $Z(\\boldsymbol{\\mathbf{\\uptheta}})$ is stochastically decreasing in $\\boldsymbol{\\mathbf{\\uptheta}}$, then it is Schur-concave.", "C) The notation $\\{Z(\\boldsymbol{\\mathbf{\\theta}}),\\boldsymbol{\\mathbf{\\theta}}\\in\\mathcal{X}\\}\\in\\mathbf{SI}.\\mathbf{SchurC}\\mathbf{X}(\\mathrm{st})$ indicates stochastic increasing and Schur-convexity.", "D) Stochastic Schur-convexity implies that for any $\\boldsymbol{\\mathbf{\\uptheta}}\\geqslant_{\\mathrm{wm}}\\boldsymbol{\\mathbf{\\uptheta}}^{\\prime}$, $Z(\\bullet)\\geqslant_{\\mathrm{st}}Z(\\bullet^{\\prime})$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The text discusses the conditions under which $Z(\\boldsymbol{\\mathbf{\\uptheta}})$ is stochastically increasing (decreasing) and Schur-convex (concave). It introduces notations like $\\mathbf{SI}.\\mathbf{SchurC}\\mathbf{X}(\\mathrm{st})$ for stochastic increasing and Schur-convexity, and mentions that stochastic Schur-convexity implies certain stochastic order relations.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. This equation is given in the context as part of the foundational relationships in resolvable designs.\n- **B)** Correct. This inequality is presented, with equality if and only if the design is affine-resolvable.\n- **C)** Correct. This equality holds specifically for affine-resolvable designs, as derived in the proof.\n- **D)** Correct. This is stated for any treatment $t$ in any binary incomplete-block design.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Construction_and_optimality_of_affine-resolvable_designs.md"], "background_text": "In resolvable designs, several equations relate the concurrences $\\lambda_{tu}$ between treatments. These include sums over all pairs of treatments and specific conditions under which equality holds, such as in affine-resolvable designs.", "folder": "", "qid": "statistic-mcqm-1331", "question": "In the context of resolvable designs, which of the following equations are correct?", "options": ["A) $\\sum_{t,u\\atop t\\not=u}\\lambda_{t u}=v r(k-1)$", "B) $\\sum_{t,u\\atop t\\not=u}\\lambda_{t u}^{2}\\geqslant r\\{k^{2}(r-1)+v(k-r)\\}$", "C) $\\sum_{t,u\\atop t\\not=u}\\lambda_{t u}^{2}=r(r-1)k^{2}+v r(k-r)$ for affine-resolvable designs", "D) $\\sum_{t,u\\atop t\\not=u}\\lambda_{t u}=r(k-1)$ for any treatment $t$ in any binary incomplete-block design"], "category": "PARTIALLY_CORRECT", "question_context_used": "In resolvable designs, several equations relate the concurrences $\\lambda_{tu}$ between treatments. These include sums over all pairs of treatments and specific conditions under which equality holds, such as in affine-resolvable designs.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The estimate of $\\beta_{1}$ based on $L_{T}$ is negative ($-0.3298$), implying that as the mileage accumulation rate increases, the hazard function decreases.\n- **B)** Correct. The estimate of $\\beta_{1}$ based on $PS$ is positive ($0.0947$), implying that as the mileage accumulation rate increases, the hazard function increases.\n- **C)** Incorrect. The estimate of $\\delta$ based on $L_{T}$ ($1.2674$) is actually greater than the estimate based on $PS$ ($1.1859$), but the question asks for the opposite, making this statement incorrect.\n- **D)** Correct. The estimate of $\\beta_{0}$ based on $L_{T}$ ($1.8186$) is greater than the estimate based on $PS$ ($-3.3014$).\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Estimation_from_truncated_lifetime_data_with_supplementary_information_on_covariates_and_censoring_times.md"], "background_text": "The Weibull proportional hazard regression model is used to study the effect of mileage accumulation rate on failure time. Parameter estimates and standard errors are provided for two methods: $L_{T}$ (truncated data likelihood) and $PS$ (supplementary information method). The estimate of $\\beta_{1}$ based on $L_{T}$ is negative, while the estimate based on $PS$ is positive. The estimates of $\\delta$ and $\\beta_{0}$ also differ between the two methods.", "folder": "", "qid": "statistic-mcqm-1583", "question": "Given the Weibull proportional hazard regression model $f(t\\mid u_{i};\\beta_{0},\\beta_{1},\\delta)=\\delta t^{\\delta-1}e^{\\beta_{0}+\\beta_{1}u_{i}}\\exp(-t^{\\delta}e^{\\beta_{0}+\\beta_{1}u_{i}})$, which of the following statements are correct based on the parameter estimates from Table 4?", "options": ["A) The estimate of $\\beta_{1}$ based on $L_{T}$ is negative, implying that as the mileage accumulation rate increases, the hazard function decreases.", "B) The estimate of $\\beta_{1}$ based on $PS$ is positive, implying that as the mileage accumulation rate increases, the hazard function increases.", "C) The estimate of $\\delta$ based on $L_{T}$ is greater than the estimate based on $PS$.", "D) The estimate of $\\beta_{0}$ based on $L_{T}$ is greater than the estimate based on $PS$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The Weibull proportional hazard regression model is used to study the effect of mileage accumulation rate on failure time. Parameter estimates and standard errors are provided for two methods: $L_{T}$ (truncated data likelihood) and $PS$ (supplementary information method). The estimate of $\\beta_{1}$ based on $L_{T}$ is negative, while the estimate based on $PS$ is positive. The estimates of $\\delta$ and $\\beta_{0}$ also differ between the two methods.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The comparison of conditional means is described as a method to test whether partitions induced by missingness indicators have the same conditional distributions.\n- **B)** Correct. Directly testing a postulated missingness mechanism, such as through logistic regression, is outlined as a method to detect violations.\n- **C)** Correct. The Gaussian copula approach is presented as a way to model the joint distribution and test conditional independence.\n- **D)** Incorrect. A t-test on complete-case data is not mentioned as a diagnostic test for missing always at random violations.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["Diagnosing_missing_always_at_random_in_multivariate_data.md"], "background_text": "The text describes three diagnostic tests for violations of the missing always at random assumption: comparison of conditional means, directly testing a postulated missingness mechanism, and using a Gaussian copula model.", "folder": "", "qid": "statistic-mcqm-5308", "question": "Which of the following are valid diagnostic tests for detecting violations of the missing always at random assumption?", "options": ["A) Comparison of conditional means of observed and missing data partitions.", "B) Directly testing a postulated missingness mechanism using logistic regression.", "C) Using a Gaussian copula to model the joint distribution of $(Y, R)$.", "D) Performing a t-test on the complete-case data."], "category": "CORRECT", "question_context_used": "The text describes three diagnostic tests for violations of the missing always at random assumption: comparison of conditional means, directly testing a postulated missingness mechanism, and using a Gaussian copula model.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A) Correct**: $\\mathrm{NO}$ is listed as a significant covariate in the rate parameter.\n- **B) Correct**: Temperature is listed as a significant covariate in the rate parameter.\n- **C) Correct**: $\\mathrm{Sun}$ is listed as a significant covariate in the rate parameter.\n- **D) Incorrect**: The level of $\\mathrm{NO}_{2}$ is not significant in the models fitted by the standard approach.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Modelling Non-Stationary Extremes with Application.md"], "background_text": "For the standard method under Model 1, significant covariates in the rate parameter include $\\mathrm{NO}$, Temperature, $\\mathrm{Sun}$, and an indicator for spring. The level of $\\mathrm{NO}_{2}$ is not significant in either of the models fitted by using the standard approach.", "folder": "", "qid": "statistic-mcqm-5281", "question": "For the standard method under Model 1, which covariates are significant in the rate parameter?", "options": ["A) $\\mathrm{NO}$", "B) Temperature", "C) $\\mathrm{Sun}$", "D) $\\mathrm{NO}_{2}$"], "category": "CORRECT", "question_context_used": "For the standard method under Model 1, significant covariates in the rate parameter include $\\mathrm{NO}$, Temperature, $\\mathrm{Sun}$, and an indicator for spring. The level of $\\mathrm{NO}_{2}$ is not significant in either of the models fitted by using the standard approach.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**:\n- **A)** Correct. The 0-factor interaction is defined as the mean of the log probabilities.\n- **B)** Correct. The 1-factor interaction is defined with averages taken over the third factor.\n- **C)** Correct. The 2-factor interaction is obtained by averaging over the third factor.\n- **D)** Correct. The context provides examples (e.g., Table 1) where the 3-factor interaction is non-zero but the table is collapsible over one factor with respect to the remaining two.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Collapsibility of Multidimensional Contingency Tab.md"], "background_text": "For 3-dimensional tables, the 0-factor interaction $u_{\\phi}^{(3)}$ is the mean of the log probabilities. The 1-factor interactions are defined with averages taken over the unspecified factors. The 2-factor interactions are obtained from the 2-dimensional interaction formula by averaging over the unspecified factor. The 3-factor interaction involves a more complex combination of log probabilities and their averages. The context provides examples where the 3-factor interaction is non-zero but the table is still collapsible over one factor with respect to the remaining two.", "folder": "", "qid": "statistic-mcqm-4899", "question": "In the context of 3-dimensional tables, which of the following statements about interactions are true?", "options": ["A) The 0-factor interaction $u_{\\phi}^{(3)}$ is the mean of the log probabilities.", "B) The 1-factor interaction $u_{1}^{(3)}(i_{1})$ is given by $\\hat{l}_{1}(i_{1})-\\hat{l}_{\\phi}$, with all averages taken over the third factor.", "C) The 2-factor interaction $u_{12}^{(3)}$ is obtained from the 2-dimensional interaction formula by averaging over the third factor.", "D) The 3-factor interaction $u_{123}^{(3)}(i_{1},i_{2},i_{3})$ can be non-zero even if the table is collapsible over one factor with respect to the remaining two."], "category": "CORRECT", "question_context_used": "For 3-dimensional tables, the 0-factor interaction $u_{\\phi}^{(3)}$ is the mean of the log probabilities. The 1-factor interactions are defined with averages taken over the unspecified factors. The 2-factor interactions are obtained from the 2-dimensional interaction formula by averaging over the unspecified factor. The 3-factor interaction involves a more complex combination of log probabilities and their averages. The context provides examples where the 3-factor interaction is non-zero but the table is still collapsible over one factor with respect to the remaining two.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A) Correct**: Simes' test controls the Familywise Error Rate strongly at $\\alpha$ when the test statistics are independent or have certain distribution properties.\n- **B) Correct**: The Simes–Hochberg test is more conservative than Simes' test because it uses a stricter criterion for rejection.\n- **C) Incorrect**: Simes' test does not allow a step-up short-cut; it is the Simes–Hochberg test that allows this.\n- **D) Correct**: The Simes–Hochberg test is in the form of Conditions $1^{\\prime}-3^{\\prime}$ and thus allows a step-up short-cut, which is Hochberg’s step-up method.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Hochberg's_Step-Up_Method_Cutting_Corners_Off_Holm's_Step-Down_Method.md"], "background_text": "Simes' test is a level-$\\alpha$ test for $H_{0I}$ and controls the Familywise Error Rate strongly at $\\alpha$ under certain conditions. The Simes–Hochberg test is more conservative than Simes' test and is in the form of Conditions $1^{\\prime}-3^{\\prime}$, allowing a step-up short-cut.", "folder": "", "qid": "statistic-mcqm-5946", "question": "Which of the following statements are true about Simes' test and the Simes–Hochberg test?", "options": ["A) Simes' test controls the Familywise Error Rate strongly at $\\alpha$ when the test statistics are independent.", "B) The Simes–Hochberg test is more conservative than Simes' test.", "C) Simes' test allows a step-up short-cut.", "D) The Simes–Hochberg test is in the form of Conditions $1^{\\prime}-3^{\\prime}$ and allows a step-up short-cut."], "category": "CORRECT", "question_context_used": "Simes' test is a level-$\\alpha$ test for $H_{0I}$ and controls the Familywise Error Rate strongly at $\\alpha$ under certain conditions. The Simes–Hochberg test is more conservative than Simes' test and is in the form of Conditions $1^{\\prime}-3^{\\prime}$, allowing a step-up short-cut.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}", "model_answer": ["B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A) $S L_{n}$**: The conclusion explicitly states that the $S L_{n}$ test is very unreliable.\n- **B) Wald**: The conclusion mentions that the Wald test can better be avoided as it is not really reliable.\n- **C) Scores**: The Scores test is not recommended to be avoided; it showed excellent behavior in terms of the level of the test.\n- **D) LRT**: The LRT test is not mentioned as unreliable; in fact, the $L_{n}$ test is compared favorably to it.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["Robust-tests-for-linear-regression-models-b_2016_C.md"], "background_text": "The conclusion states that the $S L_{n}$ test is very unreliable and the Wald test can better be avoided as it is not really reliable. The Scores test showed excellent behavior in terms of the level of the test, but may come at a high price in terms of power. The $L_{n}$ test shows a power behavior similar to the LRT test while being more stable in terms of level.", "folder": "", "qid": "statistic-mcqm-5714", "question": "Which tests are recommended to be avoided based on their reliability as mentioned in the conclusion?", "options": ["A) $S L_{n}$", "B) Wald", "C) Scores", "D) LRT"], "category": "CORRECT", "question_context_used": "The conclusion states that the $S L_{n}$ test is very unreliable and the Wald test can better be avoided as it is not really reliable. The Scores test showed excellent behavior in terms of the level of the test, but may come at a high price in terms of power. The $L_{n}$ test shows a power behavior similar to the LRT test while being more stable in terms of level.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **Option A**: Correct. The text provides the inverse of $\\mathbf{G}_{n}$, implying it is invertible.\n- **Option B**: Correct. The inverse of $\\mathbf{G}_{n}$ is explicitly given in the text.\n- **Option C**: Correct. Multiplying $\\mathbf{G}_{n}$ by $\\mathbf{1}_{n}$ results in $\\mathbf{1}_{n}$ because the term $\\frac{1}{(n-1)^{2}}\\mathbf{1}_{n}\\mathbf{1}_{n}^{\\prime}\\mathbf{1}_{n} = \\frac{n}{(n-1)^{2}}\\mathbf{1}_{n}$ combines with the other term to give $\\mathbf{1}_{n}$.\n- **Option D**: Incorrect. $\\mathbf{G}_{n}$ is not a diagonal matrix due to the presence of the $\\mathbf{1}_{n}\\mathbf{1}_{n}^{\\prime}$ term.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Fast,_Resolution-Consistent_Spatial_Prediction_of_Global_Processes_From_Satellite_Data__Fast__Resolution-Consistent_Spatial_Prediction_of_Global_Processes_From_Satellite_Data.md"], "background_text": "The matrix $\\mathbf{G}_{n}$ is defined as $\\left(1-\\frac{1}{(n-1)^{2}}\\right)\\mathbf{I}_{n} + \\frac{1}{(n-1)^{2}}\\mathbf{1}_{n}\\mathbf{1}_{n}^{\\prime}$. Its inverse is given by $\\mathbf{G}_{n}^{-1} = \\frac{n-1}{n^{2}(n-2)}\\left\\{n(n-1)\\mathbf{I}_{n}-\\mathbf{1}_{n}\\mathbf{1}_{n}^{\\prime}\\right\\}$.", "folder": "", "qid": "statistic-mcqm-5465", "question": "For the matrix $\\mathbf{G}_{n} \\equiv \\left(1-\\frac{1}{(n-1)^{2}}\\right)\\mathbf{I}_{n} + \\frac{1}{(n-1)^{2}}\\mathbf{1}_{n}\\mathbf{1}_{n}^{\\prime}$, which of the following statements are correct?", "options": ["A) $\\mathbf{G}_{n}$ is invertible", "B) $\\mathbf{G}_{n}^{-1} = \\frac{n-1}{n^{2}(n-2)}\\left\\{n(n-1)\\mathbf{I}_{n}-\\mathbf{1}_{n}\\mathbf{1}_{n}^{\\prime}\\right\\}$", "C) $\\mathbf{G}_{n}\\mathbf{1}_{n} = \\mathbf{1}_{n}$", "D) $\\mathbf{G}_{n}$ is a diagonal matrix"], "category": "CORRECT", "question_context_used": "The matrix $\\mathbf{G}_{n}$ is defined as $\\left(1-\\frac{1}{(n-1)^{2}}\\right)\\mathbf{I}_{n} + \\frac{1}{(n-1)^{2}}\\mathbf{1}_{n}\\mathbf{1}_{n}^{\\prime}$. Its inverse is given by $\\mathbf{G}_{n}^{-1} = \\frac{n-1}{n^{2}(n-2)}\\left\\{n(n-1)\\mathbf{I}_{n}-\\mathbf{1}_{n}\\mathbf{1}_{n}^{\\prime}\\right\\}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The text states that minimization of $F_{5}$ is desirable under the specified normal distribution approximation.\n- **B)** Correct. The text mentions that optimization of $F_{5}$ produces tests highly efficient over a broad range of $\\mu$ values.\n- **C)** Correct. The text recommends $F_{5}$ as a general criterion due to its robustness of efficiency.\n- **D)** Incorrect. The text does not mention that minimization of $F_{5}$ requires a fixed sample size without any interim analyses; in fact, it discusses group sequential tests.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["An_improved_method_for_deriving_optimal_one-sided_group_sequential_tests.md"], "background_text": "Minimization of $F_{5}$ is desirable if prior expectations about the true value of $\\mu$ are well approximated by a normal distribution with mean 0 and variance $\\delta^{2}$. Optimization of $F_{5}$ produces tests that are highly efficient over a broad range of $\\mu$ values. $F_{5}$ is recommended as a general criterion due to its robustness of efficiency.", "folder": "", "qid": "statistic-mcqm-5581", "question": "Which of the following are true about the objective function $F_{5}$ and its minimization?", "options": ["A) Minimization of $F_{5}$ is desirable if prior expectations about the true value of $\\mu$ are well approximated by a normal distribution with mean 0 and variance $\\delta^{2}$.", "B) Optimization of $F_{5}$ produces tests that are highly efficient over a broad range of $\\mu$ values.", "C) $F_{5}$ is recommended as a general criterion due to its robustness of efficiency.", "D) Minimization of $F_{5}$ requires a fixed sample size without any interim analyses."], "category": "CORRECT", "question_context_used": "Minimization of $F_{5}$ is desirable if prior expectations about the true value of $\\mu$ are well approximated by a normal distribution with mean 0 and variance $\\delta^{2}$. Optimization of $F_{5}$ produces tests that are highly efficient over a broad range of $\\mu$ values. $F_{5}$ is recommended as a general criterion due to its robustness of efficiency.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **A)** Correct. This condition is explicitly stated to ensure consistency for normally distributed errors.\n- **B)** Correct. The errors being i.i.d. according to a symmetric distribution $F$ with center zero and scale $\\sigma_0$ is a general condition for the model.\n- **C)** Correct. The distribution $F$ is often assumed to be Gaussian, which is relevant for the consistency conditions.\n- **D)** Incorrect. The functions $\\rho_j$ are not linear; they are defined piecewise and include quadratic and higher-order terms.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Robust-tests-for-linear-regression-models-b_2016_C.md"], "background_text": "The parameters $b_1$ and $b_2$ are chosen such that $E_{\\Phi}(\\rho_j(u)) = b_j$, $j=1,2$, where $\\Phi$ denotes the standard normal distribution. These conditions ensure consistency of the scale estimators $\\hat{\\tau}_{n} = \\tau_{n}(\\hat{\\pmb{\\theta}}_{n})$ and $\\hat{\\sigma}_{n} = s_{n}(\\hat{\\pmb{\\theta}}_{n})$ for normally distributed errors.", "folder": "", "qid": "statistic-mcqm-5711", "question": "What are the conditions for the consistency of the scale estimators $\\hat{\\tau}_{n}$ and $\\hat{\\sigma}_{n}$ for normally distributed errors?", "options": ["A) The parameters $b_1$ and $b_2$ are chosen such that $E_{\\Phi}(\\rho_j(u)) = b_j$, $j=1,2$.", "B) The errors $\\epsilon_i$ are i.i.d. according to a symmetric distribution $F$ with center zero and scale $\\sigma_0$.", "C) The distribution $F$ is Gaussian.", "D) The functions $\\rho_j$ are linear."], "category": "PARTIALLY_CORRECT", "question_context_used": "The parameters $b_1$ and $b_2$ are chosen such that $E_{\\Phi}(\\rho_j(u)) = b_j$, $j=1,2$, where $\\Phi$ denotes the standard normal distribution. These conditions ensure consistency of the scale estimators $\\hat{\\tau}_{n} = \\tau_{n}(\\hat{\\pmb{\\theta}}_{n})$ and $\\hat{\\sigma}_{n} = s_{n}(\\hat{\\pmb{\\theta}}_{n})$ for normally distributed errors.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: C\n\n- **A)** Incorrect. The context explicitly states that kernel methods are not used in finding centers of clusters in Step 1.\n- **B)** Incorrect. Similarly, kernel methods are not used in assigning non-censored observations to their clusters in Step 1.\n- **C)** Correct. Step 2 requires smoothing techniques to assign labels to censored observations by estimating the conditional distribution function.\n- **D)** Incorrect. The calculation of the weighted sum of squares for non-censored observations does not involve kernel methods or smoothing techniques.\n\n**Final Answer**: The final answer is $\\boxed{{C}}$", "source": ["Vector-quantization-and-clustering-in-the-prese_20.md"], "background_text": "The algorithm does not use kernel methods or smoothing techniques in Step 1 for finding centers of clusters or assigning non-censored observations to their clusters. However, Step 2, which involves assigning labels to censored observations, requires some smoothing techniques to estimate the conditional distribution function.", "folder": "", "qid": "statistic-mcqm-6031", "question": "Which of the following steps in the algorithm described in the context require the use of kernel methods or smoothing techniques?", "options": ["A) Finding centers of clusters in Step 1.", "B) Assigning non-censored observations to their clusters in Step 1.", "C) Assigning labels to censored observations in Step 2.", "D) Calculating the weighted sum of squares for non-censored observations."], "category": "CORRECT", "question_context_used": "The algorithm does not use kernel methods or smoothing techniques in Step 1 for finding centers of clusters or assigning non-censored observations to their clusters. However, Step 2, which involves assigning labels to censored observations, requires some smoothing techniques to estimate the conditional distribution function.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"C\"]\n}", "model_answer": ["C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- Option A is correct as it represents the first-order bias approximation for the AR(1) model without a constant.\n- Option B is correct as it represents the first-order bias approximation for the AR(1) model with a constant included.\n- Option C is incorrect as it does not match the given bias approximations.\n- Option D is incorrect as it suggests there is no bias, which contradicts the provided context.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["The-ability-to-correct-the-bias-in-the-stable-AD_2.md"], "background_text": "In the stable AR(1) model without a constant, the bias of the AR(1)-coefficient estimator to the order $O(T^{-1})$ is given by $\\mathbb{E}[\\hat{\\lambda}-\\lambda]=-2\\lambda/T$, where $\\lambda$ denotes the AR(1)-coefficient. When a constant is included, the first-order bias approximation equals $-(1+3\\lambda)/T$.", "folder": "", "qid": "statistic-mcqm-5755", "question": "In the AR(1) model without a constant, the first-order bias approximation of the AR(1)-coefficient estimator is given by:", "options": ["A) $\\mathbb{E}[\\hat{\\lambda}-\\lambda]=-2\\lambda/T$", "B) $\\mathbb{E}[\\hat{\\lambda}-\\lambda]=-(1+3\\lambda)/T$", "C) $\\mathbb{E}[\\hat{\\lambda}-\\lambda]=\\lambda/T$", "D) $\\mathbb{E}[\\hat{\\lambda}-\\lambda]=0$"], "category": "PARTIALLY_CORRECT", "question_context_used": "In the stable AR(1) model without a constant, the bias of the AR(1)-coefficient estimator to the order $O(T^{-1})$ is given by $\\mathbb{E}[\\hat{\\lambda}-\\lambda]=-2\\lambda/T$, where $\\lambda$ denotes the AR(1)-coefficient. When a constant is included, the first-order bias approximation equals $-(1+3\\lambda)/T$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The test statistic $t_s$ is indeed based on the sum of the smallest eigenvalues of $\\hat{P}$, specifically those from $\\hat{\\rho}_{s+1}$ to $\\hat{\\rho}_{m}$.\n- **B)** Correct. The asymptotic null distribution of $t_s$ is a linear combination of independent $\\chi^2_1$ random variables, as stated in Theorem 1.\n- **C)** Incorrect. The matrix $C$ is not known and requires estimation from sample data, as mentioned in the text.\n- **D)** Correct. Under $H_{0s}$, the rank of $P$ is indeed $s$, as explicitly stated in the context.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Weighted_chi‐squared_tests_for_partial_common_principal_component_subspaces.md"], "background_text": "The test statistic $t_s$ is defined as $n\\sum_{i=s+1}^{m}\\hat{\\rho}_{i}$, where $\\hat{\\rho}_{1}\\geqslant...\\geqslant\\hat{\\rho}_{m}$ are the eigenvalues of $\\hat{P}$. Under $H_{0s}$, the asymptotic distribution of $t_s$ is the same as the distribution of $T=\\sum_{i=1}^{L}\\omega_{i}v_{i}$, where $v_{1},\\dots,v_{L}$ are independent $\\chi_{1}^{2}$ random variables and $\\omega_{1}\\geqslant...\\geqslant\\omega_{g k(m-s)}$ are the eigenvalues of $C$. The rank of $P$ under $H_{0s}$ is $s$.", "folder": "", "qid": "statistic-mcqm-5691", "question": "In the context of testing hypotheses about the rank of a matrix $P$ in multivariate normal populations, which of the following statements are correct?", "options": ["A) The test statistic $t_s$ is based on the sum of the smallest eigenvalues of the estimator $\\hat{P}$.", "B) The asymptotic null distribution of $t_s$ is a linear combination of independent $\\chi^2_1$ random variables.", "C) The matrix $C$ in Theorem 1 is known and does not require estimation from sample data.", "D) The rank of $P$ under $H_{0s}$ is equal to $s$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The test statistic $t_s$ is defined as $n\\sum_{i=s+1}^{m}\\hat{\\rho}_{i}$, where $\\hat{\\rho}_{1}\\geqslant...\\geqslant\\hat{\\rho}_{m}$ are the eigenvalues of $\\hat{P}$. Under $H_{0s}$, the asymptotic distribution of $t_s$ is the same as the distribution of $T=\\sum_{i=1}^{L}\\omega_{i}v_{i}$, where $v_{1},\\dots,v_{L}$ are independent $\\chi_{1}^{2}$ random variables and $\\omega_{1}\\geqslant...\\geqslant\\omega_{g k(m-s)}$ are the eigenvalues of $C$. The rank of $P$ under $H_{0s}$ is $s$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "To determine the correct statements about MSE:\n\n- **A)** MSE decreases as the sample size $n$ increases: This is generally true because the denominator $n$ in the MSE formula means that as $n$ increases, the MSE tends to decrease, assuming the sum of squared errors does not increase proportionally with $n$.\n- **B)** MSE is a measure of the average squared difference between the estimated values and the actual values: This is the definition of MSE, making this statement true.\n- **C)** MSE is unaffected by the signal-to-noise ratio (snr): This is false because the snr affects the variance of the noise, which in turn affects the MSE.\n- **D)** MSE can be used to compare the performance of different estimation methods: This is true as MSE provides a quantitative measure to compare the accuracy of different methods.\n\n**Correct Choices**: A, B, D\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Hybrid-local-polynomial-wavelet-shrinkage--wavelet.md"], "background_text": "The mean-squared error (MSE) is defined as $\\operatorname{MSE}(\\hat{f})=n^{-1}\\sum_{i=1}^{n}\\{f(x_{i})-\\hat{f}(x_{i})\\}^{2}$. It is used to assess the quality of an estimate $\\hat{f}$ by measuring the average squared difference between the estimated values and the actual values. The context mentions experimental settings with $\\mathrm{snr}=8$ and $n=256$, and discusses the performance of methods near the boundaries using a modified MSE formula.", "folder": "", "qid": "statistic-mcqm-2464", "question": "Given the formula for mean-squared error (MSE) as $\\operatorname{MSE}(\\hat{f})=n^{-1}\\sum_{i=1}^{n}\\{f(x_{i})-\\hat{f}(x_{i})\\}^{2}$, which of the following statements are true when assessing the quality of an estimate $\\hat{f}$?", "options": ["A) MSE decreases as the sample size $n$ increases.", "B) MSE is a measure of the average squared difference between the estimated values and the actual values.", "C) MSE is unaffected by the signal-to-noise ratio (snr).", "D) MSE can be used to compare the performance of different estimation methods."], "category": "PARTIALLY_CORRECT", "question_context_used": "The mean-squared error (MSE) is defined as $\\operatorname{MSE}(\\hat{f})=n^{-1}\\sum_{i=1}^{n}\\{f(x_{i})-\\hat{f}(x_{i})\\}^{2}$. It is used to assess the quality of an estimate $\\hat{f}$ by measuring the average squared difference between the estimated values and the actual values. The context mentions experimental settings with $\\mathrm{snr}=8$ and $n=256$, and discusses the performance of methods near the boundaries using a modified MSE formula.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"B\", \"D\"]\n}", "model_answer": ["B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. $\\alpha(t)$ is indeed the marginal mean of the response on the log-scale.\n- **B)** Correct. $F(\\cdot, \\cdot, \\cdot)$ is designed to capture the effect of daily temperature on bike rentals.\n- **C)** Incorrect. $\\gamma(t)$ quantifies the time-varying, not constant, effect of average humidity.\n- **D)** Correct. The model explicitly includes $\\gamma(t)$ to account for the time-varying effect of average humidity.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Additive_Function-on-Function_Regression__Additive_Function-on-Function_Regression.md"], "background_text": "The model includes $\\alpha(t)$ as the marginal mean of the response, $F(\\cdot, \\cdot, \\cdot)$ as an unknown trivariate function capturing the effect of daily temperature, and $\\gamma(t)$ as a smooth univariate function quantifying the time-varying effect of average humidity.", "folder": "", "qid": "statistic-mcqm-5111", "question": "Given the function-on-function regression model for bike rentals, which of the following statements are correct about the model components?", "options": ["A) The term $\\alpha(t)$ represents the marginal mean of the response on the log-scale.", "B) The function $F(\\cdot, \\cdot, \\cdot)$ captures the effect of daily temperature on bike rentals.", "C) The term $\\gamma(t)$ quantifies the constant effect of average humidity throughout the day.", "D) The model accounts for the time-varying effect of average humidity through $\\gamma(t)$."], "category": "CORRECT", "question_context_used": "The model includes $\\alpha(t)$ as the marginal mean of the response, $F(\\cdot, \\cdot, \\cdot)$ as an unknown trivariate function capturing the effect of daily temperature, and $\\gamma(t)$ as a smooth univariate function quantifying the time-varying effect of average humidity.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **A) Correct**: The context states that $\\frac{1}{2}\\pi$ times the mean of the measured $L(\\theta)$ values is an estimate of $L$.\n- **B) Incorrect**: The context mentions that if $n$ is odd, it is equivalent to using $2n$ directions, but this is not a correct statement as per the given context.\n- **C) Correct**: For $n$ even ($n = 2m$), the estimate $\\hat{L}$ satisfies the given inequalities.\n- **D) Incorrect**: The method is applicable to any rectifiable curve, not just straight lines.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Measuring_the_length_of_a_curve.md"], "background_text": "Steinhaus suggested measuring $L(\\theta)$ for a finite number $n$ of equally spaced values of $\\theta$ in $(0, 2\\pi)$ and calculating the mean. The estimate $\\hat{L}$ is $\\frac{1}{2}\\pi$ times this mean. For $n$ even ($n = 2m$), $\\hat{L}$ satisfies the inequalities $\\frac{\\pi}{2m}\\cos\\left(\\frac{\\pi}{2m}\\right)\\left\\{\\sin\\left(\\frac{\\pi}{2m}\\right)\\right\\}^{-1} \\leqslant \\hat{L}L^{-1} \\leqslant \\frac{\\pi}{2m}\\left\\{\\sin\\left(\\frac{\\pi}{2m}\\right)\\right\\}^{-1}$.", "folder": "", "qid": "statistic-mcqm-5406", "question": "Steinhaus suggested a practical method for measuring the length $L$ of a curve by measuring $L(\\theta)$ for a finite number of $n$ equally spaced values of $\\theta$ in the range $(0, 2\\pi)$. Which of the following statements about this method are correct?", "options": ["A) The estimate $\\hat{L}$ is given by $\\frac{1}{2}\\pi$ times the mean of the measured $L(\\theta)$ values.", "B) If $n$ is odd, it is equivalent to using $2n$ directions.", "C) For $n$ even, the estimate $\\hat{L}$ satisfies certain inequalities involving $\\frac{\\pi}{2m}$ where $n = 2m$.", "D) The method is only applicable when the curve is a straight line."], "category": "CORRECT", "question_context_used": "Steinhaus suggested measuring $L(\\theta)$ for a finite number $n$ of equally spaced values of $\\theta$ in $(0, 2\\pi)$ and calculating the mean. The estimate $\\hat{L}$ is $\\frac{1}{2}\\pi$ times this mean. For $n$ even ($n = 2m$), $\\hat{L}$ satisfies the inequalities $\\frac{\\pi}{2m}\\cos\\left(\\frac{\\pi}{2m}\\right)\\left\\{\\sin\\left(\\frac{\\pi}{2m}\\right)\\right\\}^{-1} \\leqslant \\hat{L}L^{-1} \\leqslant \\frac{\\pi}{2m}\\left\\{\\sin\\left(\\frac{\\pi}{2m}\\right)\\right\\}^{-1}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The MLE of $\\varDelta$ is indeed the difference between the ratios $\\frac{x_{1}}{x_{1}+Y_{1}}$ and $\\frac{x_{0}}{x_{0}+Y_{0}}$, as given by the formula.\n- **B) Correct**: The MLE of $\\lambda$ is the sum of the same ratios, as explicitly stated in the formula.\n- **C) Incorrect**: The MLE of $\\varDelta$ depends on $Y_{1}$ and $Y_{0}$ through the denominators $x_{1}+Y_{1}$ and $x_{0}+Y_{0}$.\n- **D) Correct**: The MLE of $\\lambda$ could be negative if the term $\\frac{x_{0}}{x_{0}+Y_{0}}$ is sufficiently larger than $\\frac{x_{1}}{x_{1}+Y_{1}}$, making the sum negative.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Asymptotic-confidence-interval-construction-for-_2.md"], "background_text": "The MLEs of $\\varDelta$ and $\\lambda$ are derived from the roots of the equations obtained by setting the partial derivatives of the log-likelihood function with respect to $\\varDelta$ and $\\lambda$ to zero. The expressions for $\\widehat{\\varDelta}$ and $\\widehat{\\lambda}$ are given as $\\widehat{\\varDelta}=\\frac{x_{1}}{x_{1}+Y_{1}}-\\frac{x_{0}}{x_{0}+Y_{0}}$ and $\\widehat{\\lambda}=\\frac{x_{1}}{x_{1}+Y_{1}}+\\frac{x_{0}}{x_{0}+Y_{0}}$.", "folder": "", "qid": "statistic-mcqm-5681", "question": "Given the maximum likelihood estimators (MLEs) for $\\varDelta$ and $\\lambda$ as $\\widehat{\\varDelta}=\\frac{x_{1}}{x_{1}+Y_{1}}-\\frac{x_{0}}{x_{0}+Y_{0}}$ and $\\widehat{\\lambda}=\\frac{x_{1}}{x_{1}+Y_{1}}+\\frac{x_{0}}{x_{0}+Y_{0}}$, which of the following statements are correct?", "options": ["A) The MLE of $\\varDelta$ is the difference between the ratios of $x_{1}$ to $x_{1}+Y_{1}$ and $x_{0}$ to $x_{0}+Y_{0}$.", "B) The MLE of $\\lambda$ is the sum of the ratios of $x_{1}$ to $x_{1}+Y_{1}$ and $x_{0}$ to $x_{0}+Y_{0}$.", "C) The MLE of $\\varDelta$ is independent of $Y_{1}$ and $Y_{0}$.", "D) The MLE of $\\lambda$ can be negative if $x_{0}$ is sufficiently larger than $x_{1}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The MLEs of $\\varDelta$ and $\\lambda$ are derived from the roots of the equations obtained by setting the partial derivatives of the log-likelihood function with respect to $\\varDelta$ and $\\lambda$ to zero. The expressions for $\\widehat{\\varDelta}$ and $\\widehat{\\lambda}$ are given as $\\widehat{\\varDelta}=\\frac{x_{1}}{x_{1}+Y_{1}}-\\frac{x_{0}}{x_{0}+Y_{0}}$ and $\\widehat{\\lambda}=\\frac{x_{1}}{x_{1}+Y_{1}}+\\frac{x_{0}}{x_{0}+Y_{0}}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A)** Correct. The inequality $B_{2}(\\theta,\\sigma^{2})\\geqslant B_{2}(\\theta_{0},\\sigma^{2})=b_{2}^{(l)}$ is explicitly stated in the proof for any $\\theta\\in M$ and $\\theta_{0}\\in H$.\n- **B)** Correct. The proof states that $B_{2}(\\theta,\\sigma^{2})<b_{2}^{(u)}$ with a positive probability of strict inequality.\n- **C)** Incorrect. $B_{2}(\\theta,\\sigma^{2})$ is not independent of the sample size $N$; the approximation for $b_{2}^{(l)}$ involves terms of order $N^{-1}$.\n- **D)** Correct. The approximation for $b_{2}^{(l)}$ up to the order of $N^{-1}$ is given as $1+\\sum_{i=1}^{k}P(i,k,w)i+O(N^{-1})$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["An_information_criterion_for_parameters_under_a_simple_order_restriction.md"], "background_text": "In the proof of Theorem 2, $B_{2}(\\theta,\\sigma^{2})$ is discussed in relation to inequalities (A1) and (A2), showing that $B_{2}(\\theta,\\sigma^{2})\\geqslant B_{2}(\\theta_{0},\\sigma^{2})=b_{2}^{(l)}$ for any $\\theta\\in M$ and $\\theta_{0}\\in H$, and $B_{2}(\\theta,\\sigma^{2})<b_{2}^{(u)}$ with a positive probability of strict inequality. An approximation for $b_{2}^{(l)}$ up to the order of $N^{-1}$ is also provided.", "folder": "", "qid": "statistic-mcqm-3728", "question": "In the context of the proof of Theorem 2, which of the following statements about $B_{2}(\\theta,\\sigma^{2})$ are correct?", "options": ["A) $B_{2}(\\theta,\\sigma^{2})\\geqslant B_{2}(\\theta_{0},\\sigma^{2})=b_{2}^{(l)}$ for any $\\theta\\in M$ and $\\theta_{0}\\in H$.", "B) $B_{2}(\\theta,\\sigma^{2})<b_{2}^{(u)}$ with a positive probability of strict inequality.", "C) $B_{2}(\\theta,\\sigma^{2})$ is independent of the sample size $N$.", "D) The approximation for $b_{2}^{(l)}$ up to the order of $N^{-1}$ is $1+\\sum_{i=1}^{k}P(i,k,w)i+O(N^{-1})$."], "category": "PARTIALLY_CORRECT", "question_context_used": "In the proof of Theorem 2, $B_{2}(\\theta,\\sigma^{2})$ is discussed in relation to inequalities (A1) and (A2), showing that $B_{2}(\\theta,\\sigma^{2})\\geqslant B_{2}(\\theta_{0},\\sigma^{2})=b_{2}^{(l)}$ for any $\\theta\\in M$ and $\\theta_{0}\\in H$, and $B_{2}(\\theta,\\sigma^{2})<b_{2}^{(u)}$ with a positive probability of strict inequality. An approximation for $b_{2}^{(l)}$ up to the order of $N^{-1}$ is also provided.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A)** Correct. The bootstrap confidence interval is consistent when $\\theta$ is not in $\\Theta_{0}$.\n- **B)** Correct. The bootstrap confidence interval is inconsistent when $\\theta$ is in $\\Theta_{0}$.\n- **C)** Incorrect. The asymptotic coverage probability does not equal $\\alpha$ for $\\theta \\in \\Theta_{0}$.\n- **D)** Incorrect. The consistency depends on whether $\\theta$ is in $\\Theta_{0}$ or not, not directly on the smoothness of $f$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Bootstrapping-in-non-regular-smooth-function_2012_.md"], "background_text": "An $\\alpha$ level confidence interval is consistent if its asymptotic coverage probability equals the nominal level $\\alpha$. The bootstrap confidence interval $\\hat{\\mathcal{A}}_{n}(\\alpha)$ is consistent for $\\theta \\in \\Theta \\setminus \\Theta_{0}$ and inconsistent for $\\theta \\in \\Theta_{0}$. The asymptotic coverage probability for $\\theta \\in \\Theta_{0}$ does not equal $\\alpha$.", "folder": "", "qid": "statistic-mcqm-5389", "question": "Which of the following statements about the consistency of bootstrap confidence intervals in the smooth function model are true?", "options": ["A) The bootstrap confidence interval $\\hat{\\mathcal{A}}_{n}(\\alpha)$ is consistent for $\\theta \\in \\Theta \\setminus \\Theta_{0}$.", "B) The bootstrap confidence interval $\\hat{\\mathcal{A}}_{n}(\\alpha)$ is inconsistent for $\\theta \\in \\Theta_{0}$.", "C) The asymptotic coverage probability of $\\hat{\\mathcal{I}}_{n}(\\alpha)$ for $\\theta \\in \\Theta_{0}$ equals the nominal level $\\alpha$.", "D) The consistency of bootstrap confidence intervals depends on the smoothness of the function $f$."], "category": "CORRECT", "question_context_used": "An $\\alpha$ level confidence interval is consistent if its asymptotic coverage probability equals the nominal level $\\alpha$. The bootstrap confidence interval $\\hat{\\mathcal{A}}_{n}(\\alpha)$ is consistent for $\\theta \\in \\Theta \\setminus \\Theta_{0}$ and inconsistent for $\\theta \\in \\Theta_{0}$. The asymptotic coverage probability for $\\theta \\in \\Theta_{0}$ does not equal $\\alpha$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**:\n- **A** is correct as the text provides this exact formula for the skewness of the linear combination.\n- **B** is correct because the text states that $\\mathbf{v}$ is proportional to $(1,0)^{T}$ if and only if $\\alpha_{21}$ equals zero.\n- **C** is correct as the text describes reducing the skewness maximization problem to solving a cubic equation in $t$.\n- **D** is correct because the text mentions that the absolute skewness of a linear combination is never greater than the first singular value of $\\mathbf{K}_{3,\\mathbf{z}}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Skewness-based-projection-pursuit--A-compu_2018_Co.md"], "background_text": "The text outlines the method for skewness maximization in bivariate random vectors, detailing the mathematical formulation and the solution approach involving a cubic equation.", "folder": "", "qid": "statistic-mcqm-5220", "question": "Regarding the skewness maximization for bivariate random vectors, which of the following statements are true?", "options": ["A) The skewness of a linear combination $v_{1}Z_{1}+v_{2}Z_{2}$ is given by $\\gamma_{1}\\left(v_{1}Z_{1}+v_{2}Z_{2}\\right)=\\left(\\mathbf{v}^{T}\\otimes\\mathbf{v}^{T}\\right)\\mathbf{K}_{3,\\mathbf{z}}\\mathbf{v}$.", "B) The vector $\\mathbf{v}$ is proportional to $(1,0)^{T}$ if and only if $\\alpha_{21}$ equals zero.", "C) The skewness maximization problem can be reduced to solving a cubic equation in $t=v_{1}/v_{2}$.", "D) The absolute skewness of a linear combination is never greater than the first singular value of $\\mathbf{K}_{3,\\mathbf{z}}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The text outlines the method for skewness maximization in bivariate random vectors, detailing the mathematical formulation and the solution approach involving a cubic equation.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A) Correct**: The posterior distribution of $\\beta$ is indeed a unified skew-normal distribution, as stated in Corollary 4.\n- **B) Correct**: The parameters $\\xi_{\\mathrm{post}}$ and $\\Omega_{\\mathrm{post}}$ are explicitly stated to remain unchanged from the prior parameters.\n- **C) Incorrect**: The posterior distribution is not always a multivariate Gaussian; it is a unified skew-normal distribution, which includes but is not limited to Gaussian distributions.\n- **D) Correct**: The unified skew-normal prior allows for increased flexibility in prior specification, including departures from normality, as mentioned in the text.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Conjugate_Bayes_for_probit_regression_via_unified_skew-normal_distributions.md"], "background_text": "The derivations suggest that under a $\\operatorname{SUN}_{p,m}(\\xi,\\Omega,\\Delta,\\gamma,\\Gamma)$ prior for $\\beta$ in Bayesian probit regression, the posterior distribution of $\\beta$ is also a unified skew-normal distribution with updated parameters. This allows for tractable inference under more flexible priors, including departures from normality.", "folder": "", "qid": "statistic-mcqm-5634", "question": "In the context of Bayesian probit regression with a conjugate unified skew-normal prior, which of the following statements about the posterior distribution of $\\beta$ are correct?", "options": ["A) The posterior distribution of $\\beta$ is a unified skew-normal distribution.", "B) The posterior parameters $\\xi_{\\mathrm{post}}$ and $\\Omega_{\\mathrm{post}}$ remain unchanged from the prior parameters.", "C) The posterior distribution of $\\beta$ is always a multivariate Gaussian distribution.", "D) The posterior distribution allows for increased flexibility in prior specification, including departures from normality."], "category": "PARTIALLY_CORRECT", "question_context_used": "The derivations suggest that under a $\\operatorname{SUN}_{p,m}(\\xi,\\Omega,\\Delta,\\gamma,\\Gamma)$ prior for $\\beta$ in Bayesian probit regression, the posterior distribution of $\\beta$ is also a unified skew-normal distribution with updated parameters. This allows for tractable inference under more flexible priors, including departures from normality.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n- **Option A** is correct as it represents the first term of $\\mathrm{UCV}_{a b}$, which is the integral of the squared density estimate over $[a, b]$.\n- **Option B** is incorrect because the function does not involve data points outside the interval $[a, b]$.\n- **Option C** is correct as it describes the second term of $\\mathrm{UCV}_{a b}$, which involves the leave-one-out density estimates for points within $[a, b]$.\n- **Option D** is incorrect because the function does not involve a product of density estimates and the indicator function in the manner described.\n\n**Final Answer**: The final answer is $\boxed{{A, C}}$", "source": ["Scandinavian J Statistics - 2002 - SAIN - Zero‐Bia.md"], "background_text": "The modified cross-validation function $\\mathrm{UCV}_{a b}$ includes the integral of the squared density estimate over the interval $[a, b]$ and the sum of the leave-one-out density estimates for data points within the interval $[a, b]$, weighted by an indicator function.", "folder": "", "qid": "statistic-mcqm-3244", "question": "What are the components of the modified cross-validation function $\\mathrm{UCV}_{a b}$?", "options": ["A) The integral of the squared density estimate over the interval $[a, b]$.", "B) The sum of the density estimates at each data point outside the interval $[a, b]$.", "C) The average of the leave-one-out density estimates for data points within the interval $[a, b]$.", "D) The product of the density estimates and the indicator function for the interval $[a, b]$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The modified cross-validation function $\\mathrm{UCV}_{a b}$ includes the integral of the squared density estimate over the interval $[a, b]$ and the sum of the leave-one-out density estimates for data points within the interval $[a, b]$, weighted by an indicator function.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. LVP was included in the final equation.\n- **B)** Correct. HR was included in the final equation.\n- **C)** Correct. SV was included in the final equation.\n- **D)** Incorrect. TTI was omitted from the final equation to avoid over-complicating the model.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["A Bayesian Analysis of Some Pharmacological Data U.md"], "background_text": "The final equation from the stepwise regression procedure included four haemodynamic variables: LVP, HR, SV, and AP. Extremes of TTI were omitted from further consideration to avoid over-complicating the model.", "folder": "", "qid": "statistic-mcqm-6016", "question": "Which haemodynamic variables were included in the final equation from the stepwise regression procedure?", "options": ["A) LVP", "B) HR", "C) SV", "D) TTI"], "category": "CORRECT", "question_context_used": "The final equation from the stepwise regression procedure included four haemodynamic variables: LVP, HR, SV, and AP. Extremes of TTI were omitted from further consideration to avoid over-complicating the model.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A)** Correct. The table shows that $R_{\\mathcal{P}^{c}\\mathcal{P}^{0}}^{N}$ decreases from 0.905 at 15% censoring to 0.851 at 45% censoring.\n- **B)** Incorrect. $R_{\\mathcal{P P}^{0}}^{N}$ varies slightly (0.931, 0.929, 0.930) across different levels of censoring, not remaining constant.\n- **C)** Correct. The difference between $R_{\\mathcal{P P}^{0}}^{N}$ and $R_{\\mathcal{P}^{c}\\mathcal{P}^{0}}^{N}$ increases from 0.026 at 15% censoring to 0.079 at 45% censoring.\n- **D)** Correct. The table indicates that $R_{\\mathcal{P P}^{0}}^{N}$ is 0.930 at 45% censoring.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Vector-quantization-and-clustering-in-the-prese_20.md"], "background_text": "Table 1 presents corrected Rand's statistics for simulated data with close groups at different levels of censoring (15%, 30%, 45%). The values of $R_{\\mathcal{P P}^{0}}^{N}$ and $R_{\\mathcal{P}^{c}\\mathcal{P}^{0}}^{N}$ are shown, along with the difference between them. The context notes that the agreement between partitions decreases as censoring increases.", "folder": "", "qid": "statistic-mcqm-6033", "question": "Based on the results presented in Table 1 for close groups, which of the following observations are correct?", "options": ["A) The value of $R_{\\mathcal{P}^{c}\\mathcal{P}^{0}}^{N}$ decreases as the level of censoring increases.", "B) The value of $R_{\\mathcal{P P}^{0}}^{N}$ remains constant across different levels of censoring.", "C) The difference between $R_{\\mathcal{P P}^{0}}^{N}$ and $R_{\\mathcal{P}^{c}\\mathcal{P}^{0}}^{N}$ increases with the level of censoring.", "D) At 45% censoring, the value of $R_{\\mathcal{P P}^{0}}^{N}$ is approximately 0.930."], "category": "PARTIALLY_CORRECT", "question_context_used": "Table 1 presents corrected Rand's statistics for simulated data with close groups at different levels of censoring (15%, 30%, 45%). The values of $R_{\\mathcal{P P}^{0}}^{N}$ and $R_{\\mathcal{P}^{c}\\mathcal{P}^{0}}^{N}$ are shown, along with the difference between them. The context notes that the agreement between partitions decreases as censoring increases.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A)** Correct. The text states that the procedure assumes no exact observations on the survival time, which is a limitation.\n- **B)** Correct. The procedure assumes identical censoring interval distributions across treatment groups, which may not always hold.\n- **C)** Incorrect. The text mentions that the procedure is valid for distributions with finite support points, not just continuous ones.\n- **D)** Incorrect. The text highlights that the proposed method does not involve high-dimensional matrix calculations, contrasting it with some existing methods.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["A-new-class-of-generalized-log-rank-tests-for-i_20.md"], "background_text": "The proposed nonparametric test procedure assumes no exact observations on the survival time and identical distributions generating censoring intervals across treatment groups. While initially presented for continuous distributions, it is also valid for distributions with finite support points. The procedure avoids the instability of high-dimensional matrix operations for variance estimation.", "folder": "", "qid": "statistic-mcqm-6139", "question": "What are the limitations of the proposed nonparametric test procedure as mentioned in the text?", "options": ["A) It cannot handle exact observations on the survival time.", "B) It assumes identical censoring interval distributions across all treatment groups.", "C) It is only applicable to continuous distributions.", "D) It requires the calculation of high dimension matrices for variance estimation."], "category": "CORRECT", "question_context_used": "The proposed nonparametric test procedure assumes no exact observations on the survival time and identical distributions generating censoring intervals across treatment groups. While initially presented for continuous distributions, it is also valid for distributions with finite support points. The procedure avoids the instability of high-dimensional matrix operations for variance estimation.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The constraint $\\sum_{i=1}^{n}w_{i}=1$ ensures the weights sum to 1.\n- **B)** Correct. Each weight must be non-negative, as stated by the constraint $w_{i}\\geq0$.\n- **C)** Correct. The weighted sum of the sample must equal a given vector $\\nu$, as per the constraint $\\sum_{i=1}^{n}w_{i}X_{i}=\\nu$.\n- **D)** Incorrect. The weights are not required to be uniformly distributed; they are chosen to minimize divergence from uniform weights.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["Bootstrapping-in-non-regular-smooth-function_2012_.md"], "background_text": "In the oracle bootstrap, the weights $(w_{1},\\ldots,w_{n})$ are chosen to minimize the Kullback–Leibler divergence of $(w_{1},\\ldots,w_{n})$ to the vector of uniform weights $(n^{-1},\\ldots,n^{-1})$, subject to the constraints $\\sum_{i=1}^{n}w_{i}X_{i}=\\nu$, $\\sum_{i=1}^{n}w_{i}=1$, and $w_{i}\\geq0$ for all $i=1,\\ldots,n$.", "folder": "", "qid": "statistic-mcqm-5394", "question": "Which of the following are true about the constraints in the oracle bootstrap weight optimization problem?", "options": ["A) The sum of the weights must equal 1.", "B) Each weight must be non-negative.", "C) The weighted sum of the sample must equal a given vector $\\nu$.", "D) The weights must be uniformly distributed."], "category": "CORRECT", "question_context_used": "In the oracle bootstrap, the weights $(w_{1},\\ldots,w_{n})$ are chosen to minimize the Kullback–Leibler divergence of $(w_{1},\\ldots,w_{n})$ to the vector of uniform weights $(n^{-1},\\ldots,n^{-1})$, subject to the constraints $\\sum_{i=1}^{n}w_{i}X_{i}=\\nu$, $\\sum_{i=1}^{n}w_{i}=1$, and $w_{i}\\geq0$ for all $i=1,\\ldots,n$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct: The text discusses how $\\varDelta_{m,k}$ simplifies expressions for moment matrices of the normal distribution.\n- **B)** Correct: Expected values of quadratic forms are computed using $\\varDelta_{m,k}$ as shown in the applications section.\n- **C)** Correct: Generalized Wald statistics are constructed using $\\varDelta_{m,k}$ for hypothesis testing.\n- **D)** Incorrect: There is no mention of $\\varDelta_{m,k}$ being used for direct computation of matrix inverses.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Kronecker-product-permutation-matrices-and-their-a.md"], "background_text": "The Kronecker product permutation matrix $\\varDelta_{m,k}$ is used to simplify the $2k$th moment matrix of the normal distribution, calculate expected values of quadratic forms, and construct generalized Wald statistics. It is not used for directly computing the inverse of any matrix.", "folder": "", "qid": "statistic-mcqm-5809", "question": "Which of the following are applications or properties of the Kronecker product permutation matrix $\\varDelta_{m,k}$ as discussed in the context?", "options": ["A) Simplification of the $2k$th moment matrix of the normal distribution", "B) Calculation of expected values of quadratic forms", "C) Construction of generalized Wald statistics", "D) Direct computation of the inverse of any matrix"], "category": "CORRECT", "question_context_used": "The Kronecker product permutation matrix $\\varDelta_{m,k}$ is used to simplify the $2k$th moment matrix of the normal distribution, calculate expected values of quadratic forms, and construct generalized Wald statistics. It is not used for directly computing the inverse of any matrix.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: B, D\n\n**Explanation**: \n- **Option A** is incorrect because $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ can be negative if $T_k > T_{\\ell}$ and $\\vartheta$ is sufficiently large.\n- **Option B** is correct because as $\\vartheta$ increases, the exponential terms decrease, making $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ smaller.\n- **Option C** is incorrect because $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ is not symmetric; swapping $\\ell$ and $k$ changes the sign of the function.\n- **Option D** is correct because both exponential terms approach 0 as $\\vartheta$ approaches infinity, so their difference also approaches 0.\n\n**Final Answer**: The final answer is $\\boxed{{B, D}}$.", "source": ["Scandinavian J Statistics - 2019 - Féron - Efficie.md"], "background_text": "The function $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ is defined as $e^{-\\vartheta T_{k}} - e^{-\\vartheta T_{\\ell}}$, where $T_k$ and $T_{\\ell}$ are time points, and $\\vartheta$ is a parameter.", "folder": "", "qid": "statistic-mcqm-5828", "question": "Given the definition of $\\mathfrak{e}_{\\ell,k}(\\vartheta) = e^{-\\vartheta T_{k}} - e^{-\\vartheta T_{\\ell}}$, which of the following statements are true?", "options": ["A) $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ is always positive for $T_k > T_{\\ell}$.", "B) $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ decreases as $\\vartheta$ increases.", "C) $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ is symmetric in $\\ell$ and $k$.", "D) $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ approaches 0 as $\\vartheta$ approaches infinity."], "category": "CORRECT", "question_context_used": "The function $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ is defined as $e^{-\\vartheta T_{k}} - e^{-\\vartheta T_{\\ell}}$, where $T_k$ and $T_{\\ell}$ are time points, and $\\vartheta$ is a parameter.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"B\", \"D\"]\n}", "model_answer": ["B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A)** Correct: One consequence of using D.L.S. is that the variances of the estimators may not approach zero as $t \\rightarrow \\infty$, which is discussed in the context.\n- **B)** Incorrect: While the D.L.S. estimators are unbiased under the given conditions, this property depends on the correct specification of the model and weights.\n- **C)** Correct: The D.L.S. estimator is indeed given by this formula, as derived in the context.\n- **D)** Correct: The expectation of the D.L.S. estimator $\\hat{\\Theta}$ is equal to the true parameter $\\Theta$, ensuring it is unbiased.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Methods of Estimation Involving Discounting.md"], "background_text": "The D.L.S. estimator $\\hat{\\Theta}$ minimizes a weighted sum of squared errors and is given by $(\\mathbf{A}^{\\prime}\\mathbf{W}^{*}\\mathbf{A})^{-1}\\mathbf{A}^{\\prime}\\mathbf{W}^{*}\\mathbf{X}$. It is unbiased, with $E(\\hat{\\Theta}) = \\Theta$, but its variances may not approach zero as $t \\rightarrow \\infty$, unlike ordinary least squares estimators.", "folder": "", "qid": "statistic-mcqm-1103", "question": "Which of the following statements are true regarding the properties of D.L.S. estimators?", "options": ["A) The variances of the D.L.S. estimators may not approach zero as $t \\rightarrow \\infty$.", "B) D.L.S. estimators are always unbiased regardless of the choice of weights.", "C) The D.L.S. estimator $\\hat{\\Theta}$ is given by $(\\mathbf{A}^{\\prime}\\mathbf{W}^{*}\\mathbf{A})^{-1}\\mathbf{A}^{\\prime}\\mathbf{W}^{*}\\mathbf{X}$.", "D) The expectation of the D.L.S. estimator $\\hat{\\Theta}$ is equal to the true parameter $\\Theta$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The D.L.S. estimator $\\hat{\\Theta}$ minimizes a weighted sum of squared errors and is given by $(\\mathbf{A}^{\\prime}\\mathbf{W}^{*}\\mathbf{A})^{-1}\\mathbf{A}^{\\prime}\\mathbf{W}^{*}\\mathbf{X}$. It is unbiased, with $E(\\hat{\\Theta}) = \\Theta$, but its variances may not approach zero as $t \\rightarrow \\infty$, unlike ordinary least squares estimators.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **Option A** is correct as the text states that the variance depends on $\\rho_{11}$ and the third-order moments.\n- **Option B** is correct because the text provides this exact formula for the variance.\n- **Option C** is incorrect as the variance does depend on the third-order moments.\n- **Option D** is correct because the text mentions that the variance can be estimated by substituting sample values.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["The Distribution of Higher-Order Interactions in C.md"], "background_text": "The asymptotic variance of $\\sqrt{(n)r_{11}^{\\prime}}$ depends on the theoretical correlation $\\rho_{11}$ and on the third-order moments of the orthonormal functions, and is given by $\\sigma^{2}=1-\\rho_{11}^{2}+(\\rho_{11}+\\frac12\\rho_{11}^{3})\\mu_{30}\\mu_{03}-\\frac34\\rho_{11}^{2}(\\mu_{30}^{2}+\\mu_{03}^{2})$. In practice, the variance is estimated by the substitution of sample values.", "folder": "", "qid": "statistic-mcqm-6148", "question": "Regarding the asymptotic variance of $\\sqrt{(n)r_{11}^{\\prime}}$ in $2\\times2$ tables, which of the following statements are correct?", "options": ["A) The variance depends on the theoretical correlation $\\rho_{11}$ and on the third-order moments of the orthonormal functions.", "B) The variance is given by $\\sigma^{2}=1-\\rho_{11}^{2}+(\\rho_{11}+\\frac12\\rho_{11}^{3})\\mu_{30}\\mu_{03}-\\frac34\\rho_{11}^{2}(\\mu_{30}^{2}+\\mu_{03}^{2})$.", "C) The variance is independent of the third-order moments of the orthonormal functions.", "D) The variance can be estimated by substituting sample values."], "category": "CORRECT", "question_context_used": "The asymptotic variance of $\\sqrt{(n)r_{11}^{\\prime}}$ depends on the theoretical correlation $\\rho_{11}$ and on the third-order moments of the orthonormal functions, and is given by $\\sigma^{2}=1-\\rho_{11}^{2}+(\\rho_{11}+\\frac12\\rho_{11}^{3})\\mu_{30}\\mu_{03}-\\frac34\\rho_{11}^{2}(\\mu_{30}^{2}+\\mu_{03}^{2})$. In practice, the variance is estimated by the substitution of sample values.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The text states that the asymptotically optimal first-stage sample sizes are consistently good estimates of the exact optimal first-stage sample sizes over different prior distributions and moderate to large horizon size $N$.\n- **B)** Correct. The text mentions that $W_{2}=W_{2}(N,n_{1_{a}},n_{2_{a}},\\pi)$ is a good approximation for $V_{2}=V_{2}(N,\\pi)$.\n- **C)** Incorrect. The table shows that the exact optimal sample sizes are not always larger than the asymptotically optimal sample sizes; for example, in the row with $(a2,b2)=(0·5,0·5)$ and $N=102$, $n_{1_{o}}=3$ and $n_{1_{a}}=5.0$.\n- **D)** Correct. The table shows that as $N$ increases, the difference between $n_{1_{o}}$ and $n_{1_{a}}$ tends to decrease, indicating better approximation.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Choosing_sample_size_for_a_clinical_trial_using_decision_analysis.md"], "background_text": "The context discusses a two-stage trial where both arms are unknown. It highlights that the asymptotically optimal first-stage sample sizes $(n_{1_{a}}, n_{2_{a}})$ are good estimates of the exact optimal first-stage sample sizes $(n_{1_{o}}, n_{2_{o}})$ for moderate to large $N$. Additionally, $W_{2}$ is a good approximation for $V_{2}$.", "folder": "", "qid": "statistic-mcqm-5563", "question": "In a two-stage trial with both arms unknown, the asymptotically optimal first-stage sample sizes $(n_{1_{a}}, n_{2_{a}})$ are compared to the exact optimal first-stage sample sizes $(n_{1_{o}}, n_{2_{o}})$. Based on Table 2, which of the following statements are correct?", "options": ["A) The asymptotically optimal sample sizes are consistently good estimates of the exact optimal sample sizes for moderate to large horizon size $N$.", "B) The expected utility $W_{2}$ is a good approximation for $V_{2}$.", "C) The exact optimal sample sizes are always larger than the asymptotically optimal sample sizes.", "D) The difference between $n_{1_{o}}$ and $n_{1_{a}}$ decreases as $N$ increases."], "category": "CORRECT", "question_context_used": "The context discusses a two-stage trial where both arms are unknown. It highlights that the asymptotically optimal first-stage sample sizes $(n_{1_{a}}, n_{2_{a}})$ are good estimates of the exact optimal first-stage sample sizes $(n_{1_{o}}, n_{2_{o}})$ for moderate to large $N$. Additionally, $W_{2}$ is a good approximation for $V_{2}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: This is a direct description of Holm's procedure as stated in the text.\n- **B) Correct**: This accurately describes the new procedure's approach to rejecting hypotheses.\n- **C) Incorrect**: The text states that both procedures contrast the ordered $p$-values with the same set of critical values, not different sets.\n- **D) Incorrect**: The text explicitly states that the new procedure is sharper and more powerful than Holm's procedure.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["A_sharper_Bonferroni_procedure_for_multiple_tests_of_significance.md"], "background_text": "Holm's procedure and the new procedure both contrast the ordered $p$-values with the same set of critical values. Holm's procedure rejects an hypothesis only if its $p$-value and each of the smaller $p$-values are less than their corresponding critical values. The new procedure rejects all hypotheses with smaller or equal $p$-values to that of any one found less than its critical value. The new procedure is described as sharper and more powerful than Holm's procedure.", "folder": "", "qid": "statistic-mcqm-5529", "question": "Which of the following statements are true about Holm's procedure and the new procedure described in the text?", "options": ["A) Holm's procedure rejects an hypothesis only if its $p$-value and each of the smaller $p$-values are less than their corresponding critical values.", "B) The new procedure rejects all hypotheses with smaller or equal $p$-values to that of any one found less than its critical value.", "C) Both procedures use different sets of critical values for comparison.", "D) The new procedure is less powerful than Holm's procedure."], "category": "CORRECT", "question_context_used": "Holm's procedure and the new procedure both contrast the ordered $p$-values with the same set of critical values. Holm's procedure rejects an hypothesis only if its $p$-value and each of the smaller $p$-values are less than their corresponding critical values. The new procedure rejects all hypotheses with smaller or equal $p$-values to that of any one found less than its critical value. The new procedure is described as sharper and more powerful than Holm's procedure.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The text states that the empirical conditions for sufficient cause interactions with and without monotonicity are the same in this case.\n- **B) Correct**: The marginal tests are mathematically equivalent because the underlying models impose the same restrictions on the observed data law.\n- **C) Incorrect**: The conditional tests for sufficient cause interaction with and without monotonicity are different, not the same.\n- **D) Correct**: The text mentions that the test for conditional sufficient cause interaction without monotonicity is slightly less powerful than the one with monotonicity.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Semiparametric Tests for Sufficient Cause Interact.md"], "background_text": "When $P(Y_{00}=1|X)=0$, the empirical conditions for sufficient cause interactions with and without monotonicity coincide. This makes the marginal tests for sufficient cause interaction mathematically equivalent. However, the conditional tests differ, with the non-monotonic test being slightly less powerful.", "folder": "", "qid": "statistic-mcqm-6108", "question": "Under the simulation models where $P(Y_{00}=1|X)=0$, which of the following statements are correct about the tests for sufficient cause interaction?", "options": ["A) The empirical conditions for sufficient cause interactions with and without monotonicity are the same.", "B) The marginal tests for sufficient cause interaction with and without monotonicity are mathematically equivalent.", "C) The conditional tests for sufficient cause interaction with and without monotonicity are the same.", "D) The test for conditional sufficient cause interaction without monotonicity is slightly less powerful than the one with monotonicity."], "category": "CORRECT", "question_context_used": "When $P(Y_{00}=1|X)=0$, the empirical conditions for sufficient cause interactions with and without monotonicity coincide. This makes the marginal tests for sufficient cause interaction mathematically equivalent. However, the conditional tests differ, with the non-monotonic test being slightly less powerful.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **Option A** is correct because hierarchical Bayes estimation can leverage population-level data to inform individual-level estimates, making it robust even with sparse individual data.\n- **Option C** is correct as the method incorporates average choice behavior from the population through the random-effects distribution, enhancing estimation precision.\n- **Option B** is incorrect because hierarchical Bayes estimation specifically borrows information from the population, not relying solely on individual data.\n- **Option D** is incorrect because hierarchical Bayes estimation is known for being computationally intensive, not less so, compared to other techniques.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Improving-the-efficiency-of-individualized-designs.md"], "background_text": "Hierarchical Bayes estimation is advantageous for estimating random-effects models like the mixed logit choice model because it can borrow data from the entire sample to obtain estimates at the individual level, even when individual choice data is sparse. It uses both individual choice data and information from the population level distribution.", "folder": "", "qid": "statistic-mcqm-4923", "question": "What are the advantages of hierarchical Bayes estimation in the context of the mixed logit choice model?", "options": ["A) It can produce good estimates for respondent-specific partworths even with sparse individual choice data.", "B) It relies solely on individual data without borrowing information from the population.", "C) It uses information from the average choice behavior in the population through the random-effects distribution.", "D) It is less computationally intensive than other estimation techniques."], "category": "CORRECT", "question_context_used": "Hierarchical Bayes estimation is advantageous for estimating random-effects models like the mixed logit choice model because it can borrow data from the entire sample to obtain estimates at the individual level, even when individual choice data is sparse. It uses both individual choice data and information from the population level distribution.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The text states that $B$ will contain the whole sequence except possibly the initial $f^{0}$, and Lemma A1 confirms that $B$ is convex.\n- **B)** Correct. Lemma A1 explicitly states that $\\ell(f,\\lambda)$ is strictly convex on $B$ for fixed $\\lambda$.\n- **C)** Incorrect. The text mentions that under the additional assumption of Lipschitz continuity (Lemma A2), the sequence has a uniformly convergent subsequence, but it does not guarantee uniform convergence of the sequence itself without this assumption.\n- **D)** Correct. Lemma A3 states that the functional $f\\mapsto\\ell(f,\\lambda)$ is lower semicontinuous on $B$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Maximum_smoothed_likelihood_for_multivariate_mixtures.md"], "background_text": "The set $B$ is defined as $B=\\left\\{S\\phi:0\\leqslant\\phi\\in\\mathcal{F},\\int_{\\Omega}\\phi_{j}(x)d x=1,j=1,\\ldots,m\\right\\}$ and is convex. The functional $\\ell(f,\\lambda)$ is strictly convex on $B$ for fixed $\\lambda$. Under the assumption of Lipschitz continuity of the kernel $K_{h}$, every functional sequence defined by (A1) has a uniformly convergent subsequence. The functional $f\\mapsto\\ell(f,\\lambda)$ is lower semicontinuous on $B$.", "folder": "", "qid": "statistic-mcqm-4416", "question": "Given the sequence $(f^{0},\\lambda^{0}),(f^{1},\\lambda^{0}),(f^{2},\\lambda^{0}),\\ldots$ generated by iteratively applying the function defined by (10), which of the following statements are true regarding the set $B$ and the sequence?", "options": ["A) The set $B$ is convex and contains all elements of the sequence except possibly the initial $f^{0}$.", "B) The functional $\\ell(f,\\lambda)$ is strictly convex on $B$ for fixed $\\lambda$.", "C) The sequence $f^{0},f^{1},f^{2},\\dots$ is guaranteed to converge uniformly under the assumption that the kernel $K(\\cdot)$ is strictly positive on the whole real line.", "D) The functional $f\\mapsto\\ell(f,\\lambda)$ is lower semicontinuous on $B$."], "category": "CORRECT", "question_context_used": "The set $B$ is defined as $B=\\left\\{S\\phi:0\\leqslant\\phi\\in\\mathcal{F},\\int_{\\Omega}\\phi_{j}(x)d x=1,j=1,\\ldots,m\\right\\}$ and is convex. The functional $\\ell(f,\\lambda)$ is strictly convex on $B$ for fixed $\\lambda$. Under the assumption of Lipschitz continuity of the kernel $K_{h}$, every functional sequence defined by (A1) has a uniformly convergent subsequence. The functional $f\\mapsto\\ell(f,\\lambda)$ is lower semicontinuous on $B$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "Let's evaluate each option based on the background text:\n- **A)** Correct. The text states that the empirical results in Table 2 are better than those in Colubi et al. (2006).\n- **B)** Incorrect. The text indicates that the results are better than those in Wu et al. (2001), not worse.\n- **C)** Correct. The improvement is noted to remain valid for the case of dependence.\n- **D)** Incorrect. While $n=5$ and $r=5$ are mentioned for fixed design, this is not related to the empirical results in Table 2 but rather to Table 4.\n\n**Correct Choices**: A, C\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Testing-constancy-in-monotone-response_2014_Comput.md"], "background_text": "The empirical results for the case of independence in Table 2 are better than those in Colubi et al. (2006), and hence they are also better than those in Wu et al. (2001). The improvement also remains valid for the case of dependence. The fixed design parameters are $n=5$ and $r=5$.", "folder": "", "qid": "statistic-mcqm-1073", "question": "Which of the following statements are true regarding the comparison of empirical results in the case of independence?", "options": ["A) The empirical results in Table 2 are better than those in Colubi et al. (2006).", "B) The empirical results in Table 2 are worse than those in Wu et al. (2001).", "C) The improvement remains valid for the case of dependence.", "D) The empirical results are based on $n=5$ and $r=5$ for fixed design."], "category": "PARTIALLY_CORRECT", "question_context_used": "The empirical results for the case of independence in Table 2 are better than those in Colubi et al. (2006), and hence they are also better than those in Wu et al. (2001). The improvement also remains valid for the case of dependence. The fixed design parameters are $n=5$ and $r=5$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The sum $X$ of $n$ random values from the exponential distribution is indeed distributed as $\\chi^{2}_{2n}/2\\theta$.\n- **B)** Correct. The probability statement is described as a fiducial probability statement in the context.\n- **C)** Correct. The context states that the probability statement is also true when interpreted as a long-run relative frequency.\n- **D)** Incorrect. The context does not mention anything about the sum $X$ approaching a normal distribution as $n$ increases.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["National Income Visualized.md"], "background_text": "The exponential distribution is given by $\\mathrm{d}f=\\theta\\mathrm{e}^{-\\theta x}\\mathrm{d}x$ ($\\theta>0,0\\leqslant x<\\infty$). The sum $X$ of $n$ random values from this distribution is distributed as $\\chi^{2}_{2n}/2\\theta$. The probability statement $\\operatorname{prob}(\\theta\\leqslant k/2X)=\\operatorname{prob}({\\chi^{2}}_{2n}\\leqslant k)$ is both a fiducial probability statement and true when interpreted as a long-run relative frequency.", "folder": "", "qid": "statistic-mcqm-898", "question": "Given the exponential distribution with probability density function $\\mathrm{d}f=\\theta\\mathrm{e}^{-\\theta x}\\mathrm{d}x$ ($\\theta>0,0\\leqslant x<\\infty$), which of the following statements are true about the sum $X$ of $n$ random values from this distribution?", "options": ["A) The sum $X$ is distributed as $\\chi^{2}_{2n}/2\\theta$.", "B) The probability statement $\\operatorname{prob}(\\theta\\leqslant k/2X)=\\operatorname{prob}({\\chi^{2}}_{2n}\\leqslant k)$ is a fiducial probability statement.", "C) The probability statement $\\operatorname{prob}(\\theta\\leqslant k/2X)=\\operatorname{prob}({\\chi^{2}}_{2n}\\leqslant k)$ is only true for long-run relative frequencies.", "D) The sum $X$ follows a normal distribution as $n$ increases."], "category": "PARTIALLY_CORRECT", "question_context_used": "The exponential distribution is given by $\\mathrm{d}f=\\theta\\mathrm{e}^{-\\theta x}\\mathrm{d}x$ ($\\theta>0,0\\leqslant x<\\infty$). The sum $X$ of $n$ random values from this distribution is distributed as $\\chi^{2}_{2n}/2\\theta$. The probability statement $\\operatorname{prob}(\\theta\\leqslant k/2X)=\\operatorname{prob}({\\chi^{2}}_{2n}\\leqslant k)$ is both a fiducial probability statement and true when interpreted as a long-run relative frequency.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The symbol $\\mathcal{D}$ over an arrow $\\longleftrightarrow$ is explicitly stated to denote convergence in distribution.\n- **B)** Correct. Convergence in distribution is also referred to as convergence in law in the context.\n- **C)** Incorrect. The bandwidth parameter $b=b(n)$ is a function of the number of observations, except in specific cases mentioned in Theorem 2 and its proof.\n- **D)** Correct. The shorthand $[d]:=\\{1,\\dots,d\\}$ is defined and used in the context.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["A-symmetric-matrix-variate-normal-local-approximat.md"], "background_text": "The symbol $\\mathcal{D}$ over an arrow $\\longleftrightarrow$ denotes the convergence in distribution (or law). The bandwidth parameter $b=b(n)$ is implicitly a function of the number of observations, with exceptions in Theorem 2 and its proof. The shorthand $[d]:=\\{1,\\dots,d\\}$ is used in several places.", "folder": "", "qid": "statistic-mcqm-5543", "question": "Which of the following statements about the convergence in distribution are correct based on the provided context?", "options": ["A) The symbol $\\mathcal{D}$ over an arrow $\\longleftrightarrow$ denotes convergence in distribution.", "B) Convergence in distribution is also known as convergence in law.", "C) The bandwidth parameter $b=b(n)$ is always a constant and does not depend on the number of observations.", "D) The shorthand $[d]:=\\{1,\\dots,d\\}$ is used to denote a set of integers from 1 to $d$."], "category": "CORRECT", "question_context_used": "The symbol $\\mathcal{D}$ over an arrow $\\longleftrightarrow$ denotes the convergence in distribution (or law). The bandwidth parameter $b=b(n)$ is implicitly a function of the number of observations, with exceptions in Theorem 2 and its proof. The shorthand $[d]:=\\{1,\\dots,d\\}$ is used in several places.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A)** Correct as per Theorem 1, which states that the Context algorithm produces context trees whose truncated versions consistently estimate the underlying truncated context tree.\n- **B)** Correct as per Theorem 2, which asserts that the transition probabilities, given finite contexts, are consistently estimated.\n- **C)** Incorrect because Theorem 1 explicitly states that the Context algorithm's asymptotic selection of correct finite dimensional model components cannot be achieved by traditional criteria like AIC or BIC.\n- **D)** Incorrect as the convergence in Kullback–Leibler distance is mentioned in the context of other algorithms (Learn-PSA and Context Tree Weighting algorithm), not the Context algorithm under Theorems 1 and 2.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Scandinavian J Statistics - 2003 - Ferrari - Estim.md"], "background_text": "Under Assumptions 1 and 2, Theorems 1 and 2 guarantee the consistency of the Context algorithm in producing context trees and estimating transition probabilities. The algorithm's power lies in its ability to select the correct finite dimensional model components asymptotically, a feat not achievable by traditional criteria like AIC or BIC due to the large number of possible sub-models.", "folder": "", "qid": "statistic-mcqm-3047", "question": "Under Assumptions 1 and 2, which of the following outcomes are guaranteed by Theorems 1 and 2?", "options": ["A) The Context algorithm produces context trees whose truncated versions consistently estimate the underlying truncated context tree.", "B) The transition probabilities, given finite contexts, are consistently estimated.", "C) The Context algorithm selects asymptotically the correct finite dimensional model components, which can also be achieved by AIC or BIC.", "D) The estimated probability distribution converges to the true underlying probability distribution in Kullback–Leibler distance for fixed finite-order VLMCs."], "category": "PARTIALLY_CORRECT", "question_context_used": "Under Assumptions 1 and 2, Theorems 1 and 2 guarantee the consistency of the Context algorithm in producing context trees and estimating transition probabilities. The algorithm's power lies in its ability to select the correct finite dimensional model components asymptotically, a feat not achievable by traditional criteria like AIC or BIC due to the large number of possible sub-models.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: The definition of $L^{2}(S,\\mu)$ explicitly includes all $\\mu$-measurable functions with finite $\\int_{S}|f(x)|^{2}\\mu(d x)$.\n- **B) Correct**: When $S\\subseteq\\mathbb{R}$ is Borel measurable, $S=B(S)$ is indeed the Borel $\\sigma$-field on $S$.\n- **C) Correct**: The notation $L^{2}(S)$ is shorthand for $L^{2}(S,g(x)d x)$ when $g\\equiv1$.\n- **D) Incorrect**: The space $L^{2}(S,\\mu)$ is defined without requiring $\\mu$ to have a density $g$ with respect to the Lebesgue measure.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Scandinavian J Statistics - 2021 - Glück - A solut.md"], "background_text": "The space $L^{2}(S,\\mu)$ is defined for all $\\mu$-measurable functions $f$ such that $\\int_{S}|f(x)|^{2}\\mu(d x)$ is finite. If $S\\subseteq\\mathbb{R}$ is Borel measurable, $S=B(S)$ is the Borel $\\sigma$-field on $S$. The notation $L^{2}(S)$ is used when $g\\equiv1$.", "folder": "", "qid": "statistic-mcqm-5332", "question": "Which of the following statements about the space $L^{2}(S,\\mu)$ are correct?", "options": ["A) It consists of all $\\mu$-measurable functions $f$ such that $\\int_{S}|f(x)|^{2}\\mu(d x)$ is finite.", "B) If $S\\subseteq\\mathbb{R}$ is Borel measurable, then $S=B(S)$ is the Borel $\\sigma$-field on $S$.", "C) For $g\\equiv1$, the notation $L^{2}(S)$ is used.", "D) The space $L^{2}(S,\\mu)$ is only defined when $\\mu$ has a density $g$ with respect to the Lebesgue measure."], "category": "CORRECT", "question_context_used": "The space $L^{2}(S,\\mu)$ is defined for all $\\mu$-measurable functions $f$ such that $\\int_{S}|f(x)|^{2}\\mu(d x)$ is finite. If $S\\subseteq\\mathbb{R}$ is Borel measurable, $S=B(S)$ is the Borel $\\sigma$-field on $S$. The notation $L^{2}(S)$ is used when $g\\equiv1$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The GPd is defined for non-negative observations, as mentioned in the context.\n- **B)** Correct. The context states that these estimates do not exist for the whole parameter space.\n- **C)** Correct. The context shows a logarithmic transformation in the definition of GPd.\n- **D)** Correct. The context mentions that these methods are used for estimation, despite their limitations.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["A-bootstrap-goodness-of-fit-test-for-the-gener_200.md"], "background_text": "The GPd is defined for non-negative observations (excesses over a threshold). Its parameter estimates (maximum likelihood or moment) do not exist for the entire parameter space. The definition involves a logarithmic transformation, and the shape parameter $\\gamma$ can be estimated using the mentioned methods, albeit with limitations.", "folder": "", "qid": "statistic-mcqm-5192", "question": "Which of the following statements are true about the Generalized Pareto Distribution (GPd) based on the given context?", "options": ["A) The GPd is defined for non-negative observations only.", "B) The GPd's parameter estimates do not exist for the entire parameter space.", "C) The GPd's definition involves a logarithmic transformation.", "D) The GPd's shape parameter $\\gamma$ can be estimated using maximum likelihood or moment methods."], "category": "CORRECT", "question_context_used": "The GPd is defined for non-negative observations (excesses over a threshold). Its parameter estimates (maximum likelihood or moment) do not exist for the entire parameter space. The definition involves a logarithmic transformation, and the shape parameter $\\gamma$ can be estimated using the mentioned methods, albeit with limitations.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "To determine the correct statements, let's analyze each option step-by-step:\n\n- **Option A**: $\\lambda_{0}=p_{1}(1-p)+(1-p_{0})p$. As $p_{1}$ increases, the term $p_{1}(1-p)$ increases, leading to an increase in $\\lambda_{0}$. **This is correct.**\n- **Option B**: As $p_{0}$ increases, the term $(1-p_{0})p$ decreases, leading to a decrease in $\\lambda_{0}$. **This is correct.**\n- **Option C**: $\\lambda_{0}$ depends on $p$ through both terms $p_{1}(1-p)$ and $(1-p_{0})p$. **This is incorrect.**\n- **Option D**: If $p_{1} = 1$ and $p_{0} = 0$, then $\\lambda_{0}=(1)(1-p)+(1-0)p = 1-p + p = 1$, which is the maximum possible value. **This is correct.**\n\n**Correct Choices**: A, B, D\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Maximum_likelihood_estimation_of_multiple_change_points.md"], "background_text": "The theoretical value of $\\pmb{\\lambda}$ is given by $\\lambda_{0}=p_{1}(1-p)+(1-p_{0})p$, where $p$ is the proportion of observations in unchanged segments, and $p_{0}$ and $p_{1}$ are probabilities associated with the segments.", "folder": "", "qid": "statistic-mcqm-5170", "question": "Given the formula for the theoretical value of $\\pmb{\\lambda}$ when there is no minimum length for either kind of segment, $\\lambda_{0}=p_{1}(1-p)+(1-p_{0})p$, which of the following statements are correct?", "options": ["A) $\\lambda_{0}$ increases as $p_{1}$ increases.", "B) $\\lambda_{0}$ decreases as $p_{0}$ increases.", "C) $\\lambda_{0}$ is independent of $p$.", "D) $\\lambda_{0}$ reaches its maximum when $p_{1} = 1$ and $p_{0} = 0$."], "category": "CORRECT", "question_context_used": "The theoretical value of $\\pmb{\\lambda}$ is given by $\\lambda_{0}=p_{1}(1-p)+(1-p_{0})p$, where $p$ is the proportion of observations in unchanged segments, and $p_{0}$ and $p_{1}$ are probabilities associated with the segments.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **Option A** is correct because the context suggests replacing the parameter $_{c v}$ with ICV.\n- **Option B** is correct because the context suggests using CV as a real local variable.\n- **Option C** is correct because the context suggests inserting the statement $I C V=C V+S I G N(0\\cdot5,C V)$ before the return from the algorithm.\n- **Option D** is incorrect because the context specifically mentions avoiding the use of the SIGN function with a zero argument in the special case.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["Critical Values of the Sign Test.md"], "background_text": "To overcome the rounding behavior difference, it is suggested that the parameter $_{c v}$ should become ICV, CV should be a real local variable, and the statement $I C V=C V+S I G N(0\\cdot5,C V)$ should be inserted immediately before the return from the algorithm. The special case (when $s l=5$ and $n=8$) should avoid using the SIGN function with a zero argument.", "folder": "", "qid": "statistic-mcqm-5360", "question": "In the context of translating from Algol to FORTRAN, which of the following modifications are suggested to overcome the rounding behavior difference?", "options": ["A) Replace the parameter $_{c v}$ with ICV.", "B) Use CV as a real local variable.", "C) Insert the statement $I C V=C V+S I G N(0\\cdot5,C V)$ before the return from the algorithm.", "D) Always use the SIGN function with a zero argument."], "category": "CORRECT", "question_context_used": "To overcome the rounding behavior difference, it is suggested that the parameter $_{c v}$ should become ICV, CV should be a real local variable, and the statement $I C V=C V+S I G N(0\\cdot5,C V)$ should be inserted immediately before the return from the algorithm. The special case (when $s l=5$ and $n=8$) should avoid using the SIGN function with a zero argument.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The text states that the MC test has more accurate empirical size, very close to the nominal size.\n- **B) Correct**: The gamma test (both exact and approximate) is described as slightly too conservative.\n- **C) Incorrect**: The exact method without spatial correlation has inflated size, not deflated.\n- **D) Correct**: The exact method for the simple model shows inflated size compared to the spatial model, as indicated by the higher percentages in the 'Results(%) for simple model' columns.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Testing the Equality of Mean Functions of Ionosphe.md"], "background_text": "The empirical size of the tests is evaluated with $m_{1}(t)=m_{2}(t)=0$. The MC test shows accurate empirical size, close to nominal. The gamma test is slightly conservative, and the exact method without spatial correlation has inflated size.", "folder": "", "qid": "statistic-mcqm-4571", "question": "Based on Table 4, which of the following statements are correct about the empirical size of the tests?", "options": ["A) The MC test has empirical size very close to the nominal size.", "B) The gamma test (exact and approximate) is slightly too conservative.", "C) The exact method without spatial correlation has deflated size.", "D) The simple model's exact method has inflated size compared to the spatial model."], "category": "PARTIALLY_CORRECT", "question_context_used": "The empirical size of the tests is evaluated with $m_{1}(t)=m_{2}(t)=0$. The MC test shows accurate empirical size, close to nominal. The gamma test is slightly conservative, and the exact method without spatial correlation has inflated size.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The estimate for $\\hat{\\eta}_{0}$ is $-2.572$ with a standard error of 0.13.\n- **B)** Correct. The estimate for $\\hat{\\beta}_{0}$ is $-3.407$ with a standard error of 0.182.\n- **C)** Correct. The estimate for $\\hat{\\beta}_{1}$ is $0.0499$ with a standard error of 0.00312.\n- **D)** Correct. The estimate for $\\hat{\\alpha}_{0}$ is $0.000437$ with a standard error of 0.0377.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["A Litter-Based Approach to Risk Assessment in Deve.md"], "background_text": "The parameter estimates for the reduced model are $\\hat{\\eta}_{0}=-2.572$ (0.13), $\\hat{\\beta}_{0}=-3.407$ (0.182), $\\hat{\\beta}_{1}=0.0499$ (0.00312), $\\hat{\\alpha}_{0}=0.000437$ (0.0377), and $\\hat{\\alpha}_{1}=0.0352$ (0.00254).", "folder": "", "qid": "statistic-mcqm-5412", "question": "What are the correct parameter estimates (with standard errors) for the reduced model fitted to the 2,4,5-T data?", "options": ["A) $\\hat{\\eta}_{0}=-2.572$ (0.13)", "B) $\\hat{\\beta}_{0}=-3.407$ (0.182)", "C) $\\hat{\\beta}_{1}=0.0499$ (0.00312)", "D) $\\hat{\\alpha}_{0}=0.000437$ (0.0377)"], "category": "CORRECT", "question_context_used": "The parameter estimates for the reduced model are $\\hat{\\eta}_{0}=-2.572$ (0.13), $\\hat{\\beta}_{0}=-3.407$ (0.182), $\\hat{\\beta}_{1}=0.0499$ (0.00312), $\\hat{\\alpha}_{0}=0.000437$ (0.0377), and $\\hat{\\alpha}_{1}=0.0352$ (0.00254).", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **Option A**: Correct. The error terms $e_j$ are assumed to be independent and identically distributed according to a Gaussian distribution $G$ for the maximum likelihood estimators to be asymptotically unbiased.\n- **Option B**: Correct. The joint distribution of $(X,Y)$ must be Gaussian for the maximum likelihood estimators to be asymptotically unbiased, as mentioned in the context.\n- **Option C**: Correct. The stratified sample must be contained in a simple random sample from the marginal distribution of $Y$, with all $y_{1},...,y_{N}$ known, as discussed by DeMets & Halperin.\n- **Option D**: Incorrect. While knowing the sampling fractions can be important for some estimators, it is not a general requirement for the maximum likelihood estimators to be asymptotically unbiased under the Gaussian assumption.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Least_squares_regression_with_data_arising_from_stratified_samples_of_the_dependent_variable.md"], "background_text": "For maximum likelihood estimators of $\\beta$ to be asymptotically unbiased in stratified sampling based on $Y$, additional conditions such as the joint distribution of $(X,Y)$ being Gaussian and knowledge of the marginal distribution of $Y$ are required. The error terms are assumed to be independent and identically distributed, and in some cases, the sampling fractions for each stratum must be known.", "folder": "", "qid": "statistic-mcqm-1365", "question": "Which of the following conditions are required for the maximum likelihood estimators of $\\beta$ to be asymptotically unbiased in the context of stratified sampling based on $Y$?", "options": ["A) The error terms $e_j$ are independent and identically distributed according to a Gaussian distribution $G$.", "B) The joint distribution of $(X,Y)$ is Gaussian.", "C) The stratified sample is contained in a simple random sample from the marginal distribution of $Y$.", "D) The sampling fractions for each stratum are known."], "category": "PARTIALLY_CORRECT", "question_context_used": "For maximum likelihood estimators of $\\beta$ to be asymptotically unbiased in stratified sampling based on $Y$, additional conditions such as the joint distribution of $(X,Y)$ being Gaussian and knowledge of the marginal distribution of $Y$ are required. The error terms are assumed to be independent and identically distributed, and in some cases, the sampling fractions for each stratum must be known.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}", "model_answer": ["B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**: \n- **A) Correct**: $V(12, 13)$ corresponds to $i=1, j=2, k=1, l=3$. Here, $i=k$ and $j\\neq l$, so $V(12, 13) = 1$.\n- **B) Incorrect**: $V(13, 23)$ corresponds to $i=1, j=3, k=2, l=3$. Here, $j=l$, so $V(13, 23) = -1$.\n- **C) Correct**: $V(12, 34)$ corresponds to $i=1, j=2, k=3, l=4$. None of the conditions for $1$ or $-1$ are met, so $V(12, 34) = 0$.\n- **D) Incorrect**: $V(23, 12)$ corresponds to $i=2, j=3, k=1, l=2$. Here, $j=k$ is not true, and $i=l$ is not true, but $j=k$ is not a condition for $-1$. The correct condition is $i=l$ or $j=k$, but neither is met here, so $V(23, 12) = 0$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Design_of_order-of-addition_experiments.md"], "background_text": "The matrix $V$ is defined for $m=4$ with $S=\\{12,13,14,23,24,34\\}$. The elements of $V$ are given by $V(ij, kl) = 1$ if $i=k, j\\neq l$ or $i\\neq k, j=l$, $-1$ if $i=l$ or $j=k$, and $0$ otherwise.", "folder": "", "qid": "statistic-mcqm-6090", "question": "Given the matrix $V$ defined for $m=4$ with elements $V(ij, kl)$ as per the context, which of the following statements about $V$ are correct?", "options": ["A) $V(12, 13) = 1$", "B) $V(13, 23) = -1$", "C) $V(12, 34) = 0$", "D) $V(23, 12) = 1$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The matrix $V$ is defined for $m=4$ with $S=\\{12,13,14,23,24,34\\}$. The elements of $V$ are given by $V(ij, kl) = 1$ if $i=k, j\\neq l$ or $i\\neq k, j=l$, $-1$ if $i=l$ or $j=k$, and $0$ otherwise.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A) Correct**: The text states that 'the latter needs a slightly higher number of explorations (except for $n=15$).'\n- **B) Correct**: It is mentioned that 'The three variants present a very similar evolution of the number of explorations.'\n- **C) Incorrect**: There is no evidence in the text that variant 1 always needs fewer explorations than variant 0.\n- **D) Incorrect**: The text discusses the computation time in relation to the number of explorations, indicating that it does affect the computation time.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["Exact-methods-for-variable-selection-in-principal-.md"], "background_text": "The three variants of the BnB method present a very similar evolution of the number of explorations. Comparing variants 0 and 2, the latter needs a slightly higher number of explorations (except for $n=15$). The explorations of variant 0 appear to be more effective, but considering the computation time, the explorations of variant 0 are more efficient.", "folder": "", "qid": "statistic-mcqm-4887", "question": "What are the correct observations about the number of explored nodes in the BnB method variants?", "options": ["A) Variant 2 needs a slightly higher number of explorations than variant 0.", "B) Variant 0 and variant 1 have a very similar evolution of the number of explorations.", "C) Variant 1 always needs fewer explorations than variant 0.", "D) The number of explorations does not affect the computation time."], "category": "CORRECT", "question_context_used": "The three variants of the BnB method present a very similar evolution of the number of explorations. Comparing variants 0 and 2, the latter needs a slightly higher number of explorations (except for $n=15$). The explorations of variant 0 appear to be more effective, but considering the computation time, the explorations of variant 0 are more efficient.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**:\n- **Option A** is correct because $T r(P^{\\prime}P)$ is one of the traces mentioned that can be calculated from the basic traces.\n- **Option B** is incorrect as $T r(C^{\\prime}C)$ is not directly mentioned among the traces that can be derived from the basic traces provided.\n- **Option C** is correct as $T r(P^{\\prime}V)$ is explicitly mentioned as one of the traces that can be calculated from the basic traces.\n- **Option D** is correct because $T r(V^{\\prime}W)$ is also mentioned as one of the traces that can be derived from the basic traces.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["The-ability-to-correct-the-bias-in-the-stable-AD_2.md"], "background_text": "The context introduces several trace results that can be derived from three basic traces involving matrices $U$, $V$, and $W$. These results are essential for understanding the computation of traces in the given statistical framework.", "folder": "", "qid": "statistic-mcqm-5793", "question": "Considering the trace results involving two matrices, which of the following traces can be calculated from the basic traces $T r(U^{\\prime}U)=\\omega\\{-1+\\psi\\}T+O(1)$, $T r(U^{\\prime}V)=-\\omega\\phi T+O(1)$, and $T r(U^{\\prime}W)=\\psi T r(U^{\\prime}U)+\\phi T r(U^{\\prime}V)$?", "options": ["A) $T r(P^{\\prime}P)$", "B) $T r(C^{\\prime}C)$", "C) $T r(P^{\\prime}V)$", "D) $T r(V^{\\prime}W)$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The context introduces several trace results that can be derived from three basic traces involving matrices $U$, $V$, and $W$. These results are essential for understanding the computation of traces in the given statistical framework.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"D\"]\n}", "model_answer": ["D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The text states that $\\theta$ is the reciprocal of the entry in column (2) of Table I.\n- **B)** Correct. The sampling variance of $\\mathscr{s}$ is indeed $\\sigma_{0}^{2}/w$ as mentioned in the text.\n- **C)** Correct. The text explicitly mentions that $\\theta$ and $w$ are strictly appropriate only when $x$ is normally distributed about the regression on $t$.\n- **D)** Correct. The factor $\\theta$ is used to correct the bias in $\\mathscr{s}_{0}$, making $\\mathscr{s}$ an unbiased estimate of $\\sigma_{0}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Studies in the Variability of Two Blood-Clotting T.md"], "background_text": "An unbiased estimate of the residual standard deviation is provided by $\\mathscr{s}=\\theta\\mathscr{s}_{0}$, where $\\theta$ is the factor given in Table I. The sampling variance of $\\mathscr{s}$ is $\\sigma_{0}^{2}/w$ where $w$ is given in Table I. These factors may be obtained from Table 35 of Pearson and Hartley. The factors $\\theta$ and $w$ are strictly appropriate only when $x$ is normally distributed about the regression on $t$.", "folder": "", "qid": "statistic-mcqm-3759", "question": "Regarding the unbiased estimate of the residual standard deviation, $\\mathscr{s}=\\theta\\mathscr{s}_{0}$, which of the following statements are true?", "options": ["A) $\\theta$ is obtained from Table I and is the reciprocal of the entry in column (2).", "B) The sampling variance of $\\mathscr{s}$ is $\\sigma_{0}^{2}/w$ where $w$ is given in Table I.", "C) $\\theta$ and $w$ are strictly appropriate only when $x$ is normally distributed about the regression on $t$.", "D) The factor $\\theta$ corrects the bias in $\\mathscr{s}_{0}$ to make $\\mathscr{s}$ an unbiased estimate of $\\sigma_{0}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "An unbiased estimate of the residual standard deviation is provided by $\\mathscr{s}=\\theta\\mathscr{s}_{0}$, where $\\theta$ is the factor given in Table I. The sampling variance of $\\mathscr{s}$ is $\\sigma_{0}^{2}/w$ where $w$ is given in Table I. These factors may be obtained from Table 35 of Pearson and Hartley. The factors $\\theta$ and $w$ are strictly appropriate only when $x$ is normally distributed about the regression on $t$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct because $h_{\\gamma}(u)$ is indeed flat in $(-\\infty, s]$ as stated in the background text.\n- **B)** Correct because the maximum of $h_{\\gamma}(u)$ is $\\ell_{1}(s)$.\n- **C)** Incorrect because $h_{\\gamma}(u)$ is not strictly increasing for all $u$; it is flat in $(-\\infty, s]$.\n- **D)** Correct because $\\langle\\pmb{f}^{*},\\pmb{W}_{j}\\rangle\\le s\\le0$ for all $j\\in\\{1,\\ldots,k\\}$ as per the background text.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Adaptively-weighted-large-margin-angle-based-_2018.md"], "background_text": "For $\\gamma=1$, $h_{\\gamma}(u)=T_{s}(u)=\\operatorname*{min}\\{\\ell_{1}(s),\\ell_{1}(u)\\}$. It is flat in $(-\\infty, s]$, and its maximum is $\\ell_{1}(s)$. Also, $\\langle\\pmb{f}^{*},\\pmb{W}_{j}\\rangle\\le s\\le0$ for all $j\\in\\{1,\\ldots,k\\}$.", "folder": "", "qid": "statistic-mcqm-6118", "question": "Given the function $h_{\\gamma}(u)$ for $\\gamma=1$, which of the following statements are true?", "options": ["A) $h_{\\gamma}(u)$ is flat in $(-\\infty, s]$", "B) The maximum of $h_{\\gamma}(u)$ is $\\ell_{1}(s)$", "C) $h_{\\gamma}(u)$ is strictly increasing for all $u$", "D) $\\langle\\pmb{f}^{*},\\pmb{W}_{j}\\rangle\\le s\\le0$ for all $j\\in\\{1,\\ldots,k\\}$"], "category": "CORRECT", "question_context_used": "For $\\gamma=1$, $h_{\\gamma}(u)=T_{s}(u)=\\operatorname*{min}\\{\\ell_{1}(s),\\ell_{1}(u)\\}$. It is flat in $(-\\infty, s]$, and its maximum is $\\ell_{1}(s)$. Also, $\\langle\\pmb{f}^{*},\\pmb{W}_{j}\\rangle\\le s\\le0$ for all $j\\in\\{1,\\ldots,k\\}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A) FRUIT 1 has more distinct genes than FRUIT 2.** This is correct because FRUIT 1 has 806 distinct genes, while FRUIT 2 has 687.\n- **B) FRUIT 2 has more distinct genes than FRUIT 1.** This is incorrect as FRUIT 2 has fewer distinct genes than FRUIT 1.\n- **C) The two libraries share 183 distinct genes.** This is correct as stated in the background text.\n- **D) The total number of distinct genes in both libraries combined is less than the sum of distinct genes in each library separately.** This is correct because the sum of distinct genes in each library is 806 + 687 = 1493, but since they share 183 genes, the combined total is less.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Bayesian-prediction-with-multiple-samples-in_2017_.md"], "background_text": "The EST sample corresponding to FRUIT 1, X1(n1), contains $n_{1}=1593$ ESTs with $K_{n_{1}}=806$ distinct genes, whereas the sample corresponding to FRUIT 2, ${\\pmb X}_{2}^{(n_{2})}$, is made of $n_{2}=900$ ESTs with $K_{n_{2}}=687$ unique genes. Moreover, the two libraries share 183 distinct genes.", "folder": "", "qid": "statistic-mcqm-3943", "question": "Given the EST data from FRUIT 1 and FRUIT 2, which of the following statements are true regarding the number of distinct genes?", "options": ["A) FRUIT 1 has more distinct genes than FRUIT 2.", "B) FRUIT 2 has more distinct genes than FRUIT 1.", "C) The two libraries share 183 distinct genes.", "D) The total number of distinct genes in both libraries combined is less than the sum of distinct genes in each library separately."], "category": "CORRECT", "question_context_used": "The EST sample corresponding to FRUIT 1, X1(n1), contains $n_{1}=1593$ ESTs with $K_{n_{1}}=806$ distinct genes, whereas the sample corresponding to FRUIT 2, ${\\pmb X}_{2}^{(n_{2})}$, is made of $n_{2}=900$ ESTs with $K_{n_{2}}=687$ unique genes. Moreover, the two libraries share 183 distinct genes.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}", "model_answer": ["B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The analysis found that any direct drug effect appeared to be proportional to $\\mathbf{MVO}_{i}$.\n- **B)** Correct. The residuals $\\left\\{\\varepsilon_{i}\\right\\vert$ were roughly proportional to $\\mathbf{MVO}_{i}$.\n- **C)** Correct. These observations suggested the use of a multiplicative model.\n- **D)** Incorrect. The functional form of the relationship should be the same for all dogs, as stated in the context.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["A Bayesian Analysis of Some Pharmacological Data U.md"], "background_text": "Preliminary multiple regression analysis revealed that any direct drug effect appeared to be proportional to $\\mathbf{MVO}_{i}$ and that the residuals $\\left\\{\\varepsilon_{i}\\right\\vert$ were also roughly proportional to $\\mathbf{MVO}_{i}$. These observations suggested the use of a multiplicative model.", "folder": "", "qid": "statistic-mcqm-5992", "question": "What were the findings from the preliminary multiple regression analysis of $\\mathbf{MVO}_{i}$ on seven haemodynamic variables?", "options": ["A) Any direct drug effect appeared to be proportional to $\\mathbf{MVO}_{i}$.", "B) The residuals $\\left\\{\\varepsilon_{i}\\right\\vert$ were roughly proportional to $\\mathbf{MVO}_{i}$.", "C) A multiplicative model was suggested by the observations.", "D) The functional form of the relationship varied from dog to dog."], "category": "CORRECT", "question_context_used": "Preliminary multiple regression analysis revealed that any direct drug effect appeared to be proportional to $\\mathbf{MVO}_{i}$ and that the residuals $\\left\\{\\varepsilon_{i}\\right\\vert$ were also roughly proportional to $\\mathbf{MVO}_{i}$. These observations suggested the use of a multiplicative model.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n**Explanation**:\n- **A) Correct**: The function $g(t,\\gamma)$ is indeed an exponential function of $t$ as it includes $\\mathrm{exp}(\\gamma(t-1.5))$.\n- **B) Incorrect**: The function's behavior with respect to $\\gamma$ depends on the value of $t$; it does not universally decrease as $\\gamma$ increases.\n- **C) Incorrect**: The function is not linear in $t$ due to the exponential term.\n- **D) Correct**: The function is part of the Level 1 model, which includes modeling the baseline effect among other parameters.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Warp_Bridge_Sampling__Warp_Bridge_Sampling.md"], "background_text": "The Level 1 model includes the function $g(t,\\gamma)=\\gamma^{-1}\\mathrm{exp}(\\gamma(t-1.5))$, where $\\gamma$ is a parameter vector describing person-level baseline, relapse effect, treatment effect, and declining rate. The function is part of a reparameterization to address near nonidentifiability and to harmonize the Level 1 and Level 2 models.", "folder": "", "qid": "statistic-mcqm-4938", "question": "In the Level 1 model, the function $g(t,\\gamma)$ is defined as $\\gamma^{-1}\\mathrm{exp}(\\gamma(t-1.5))$. Which of the following statements are true about $g(t,\\gamma)$?", "options": ["A) $g(t,\\gamma)$ is an exponential function of $t$.", "B) $g(t,\\gamma)$ decreases as $\\gamma$ increases.", "C) $g(t,\\gamma)$ is linear in $t$.", "D) $g(t,\\gamma)$ is used to model the baseline effect in the Level 1 model."], "category": "CORRECT", "question_context_used": "The Level 1 model includes the function $g(t,\\gamma)=\\gamma^{-1}\\mathrm{exp}(\\gamma(t-1.5))$, where $\\gamma$ is a parameter vector describing person-level baseline, relapse effect, treatment effect, and declining rate. The function is part of a reparameterization to address near nonidentifiability and to harmonize the Level 1 and Level 2 models.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The adapted criterion indeed replaces $S_k$ with a weighted sum of squares involving only non-censored observations, as stated in the context.\n- **B)** Correct. The optimal number of clusters is chosen by maximizing the absolute value of the ratio involving $\\mathcal{D}_{k-1}$, $\\mathcal{D}_{k}$, and $\\mathcal{D}_{k+1}$, which is a direct adaptation of the Krzanowski and Lai criterion.\n- **C)** Incorrect. The context does not state that the original criterion cannot be used without modification; it simply describes an adaptation for censored data.\n- **D)** Correct. The context mentions that smoothing techniques are required for assigning labels to censored observations in Step 2, but the calculation of the adapted criterion does not involve smoothing techniques.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Vector-quantization-and-clustering-in-the-prese_20.md"], "background_text": "In the presence of censoring, the Krzanowski and Lai criterion is adapted by replacing $S_k$ with a weighted sum of squares involving only non-censored observations. The optimal number of clusters is chosen by maximizing the absolute value of a ratio involving $\\mathcal{D}_{k-1}$, $\\mathcal{D}_{k}$, and $\\mathcal{D}_{k+1}$. Smoothing techniques are required for assigning labels to censored observations but not for the calculation of the adapted criterion itself.", "folder": "", "qid": "statistic-mcqm-6030", "question": "In the context of choosing the number of clusters $k$ in the presence of censoring, which of the following statements are correct regarding the adaptation of the Krzanowski and Lai criterion?", "options": ["A) The adapted criterion replaces $S_k$ with a weighted sum of squares involving only non-censored observations.", "B) The optimal number of clusters is chosen by maximizing the absolute value of a ratio involving $\\mathcal{D}_{k-1}$, $\\mathcal{D}_{k}$, and $\\mathcal{D}_{k+1}$.", "C) The original Krzanowski and Lai criterion cannot be used in the presence of censoring without modification.", "D) The adapted criterion does not require any smoothing techniques for its calculation."], "category": "PARTIALLY_CORRECT", "question_context_used": "In the presence of censoring, the Krzanowski and Lai criterion is adapted by replacing $S_k$ with a weighted sum of squares involving only non-censored observations. The optimal number of clusters is chosen by maximizing the absolute value of a ratio involving $\\mathcal{D}_{k-1}$, $\\mathcal{D}_{k}$, and $\\mathcal{D}_{k+1}$. Smoothing techniques are required for assigning labels to censored observations but not for the calculation of the adapted criterion itself.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "To answer this question, let's evaluate each option based on the background text:\n\n- **A) Correct**: The proof explicitly states that $f_n$ is increasing and right-continuous.\n- **B) Correct**: The lemma's conclusion is that the sequence $f_n$ converges monotonically to $f$.\n- **C) Incorrect**: $S_n$ is defined as a discrete set, not a continuous one.\n- **D) Correct**: The proof shows that $f_n \\leqslant f_{n+1} \\leqslant f$ for all $n \\in \\mathbb{N}$.\n\n**Correct Choices**: A, B, D\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Some-Remarks-on-the-Supermodular-Order_2000_Journa.md"], "background_text": "Lemma 3.2 states that for an increasing function $f\\colon\\mathbb{R}\\to\\mathbb{R}}$, there exists a sequence of right-continuous increasing functions $f_n$ that converges monotonically to $f$. The proof defines $S_n$ as a discrete set and $f_n(x) := \\max \\{f(t)\\colon t \\in S_n, t \\leqslant x\\}$, showing that $f_n$ is increasing, right-continuous, and satisfies $f_n \\leqslant f_{n+1} \\leqslant f$.", "folder": "", "qid": "statistic-mcqm-4967", "question": "Which of the following statements are true about the sequence $f_n$ defined in Lemma 3.2?", "options": ["A) Each $f_n$ is right-continuous and increasing.", "B) The sequence $f_n$ converges monotonically to $f$.", "C) The set $S_n$ is continuous for each $n$.", "D) $f_n \\leqslant f_{n+1} \\leqslant f$ for all $n \\in \\mathbb{N}$."], "category": "CORRECT", "question_context_used": "Lemma 3.2 states that for an increasing function $f\\colon\\mathbb{R}\\to\\mathbb{R}}$, there exists a sequence of right-continuous increasing functions $f_n$ that converges monotonically to $f$. The proof defines $S_n$ as a discrete set and $f_n(x) := \\max \\{f(t)\\colon t \\in S_n, t \\leqslant x\\}$, showing that $f_n$ is increasing, right-continuous, and satisfies $f_n \\leqslant f_{n+1} \\leqslant f$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A)** Correct. This is the definition of collapsibility over a set of factors $B$ with respect to a set of factors $A$.\n- **B)** Correct. Strict collapsibility requires that the 3-factor interaction $u_{123}^{(3)}$ must be zero, as stated in the context.\n- **C)** Incorrect. The context explicitly states that vanishing of the 3-factor interaction is neither necessary nor sufficient for collapsibility over a factor with respect to the remaining two factors.\n- **D)** Correct. This is the definition of strict collapsibility for a 3-dimensional table over a factor $k$ with respect to the set $\\{i,j\\}$ of remaining factors.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Collapsibility of Multidimensional Contingency Tab.md"], "background_text": "A marginal table or collapsed table is obtained by summing over all levels of the factors in a set $B$. A table $p$ is collapsible over $B$ with respect to certain functions of $p$ if these functions can be obtained from the collapsed table. The functions considered are the log-linear parameters or $r$-factor interactions $u_{\\mathcal{A}}^{(n)}$ associated with a set $A$ of $r$ factors, with $A$ disjoint from $B$. For a 3-dimensional table, vanishing of the 3-factor interaction $u_{123}^{(3)}$ does not imply collapsibility over a factor with respect to the remaining two factors, nor is it implied by it. Strict collapsibility requires both $u_{ij}^{(3)}=u_{ij,k}^{(2)}$ and $u_{123}^{(3)}=0$.", "folder": "", "qid": "statistic-mcqm-4898", "question": "Which of the following statements about collapsibility in multidimensional tables are correct?", "options": ["A) A table is collapsible over a set of factors $B$ with respect to a set of factors $A$ if the $r$-factor interaction $u_{\\mathcal{A}}^{(n)}$ computed from the original table equals the corresponding $r$-factor interaction $u_{A,B}^{(s)}$ computed from the collapsed table.", "B) Collapsibility over a factor $k$ with respect to the set $\\{i,j\\}$ of remaining factors requires that the 3-factor interaction $u_{123}^{(3)}$ must be zero.", "C) Vanishing of the 3-factor interaction $u_{123}^{(3)}$ is both necessary and sufficient for collapsibility over a factor with respect to the remaining two factors.", "D) A table can be strictly collapsible over a factor $k$ with respect to the set $\\{i,j\\}$ of remaining factors if $u_{ij}^{(3)}=u_{ij,k}^{(2)}$ and $u_{123}^{(3)}=0$."], "category": "PARTIALLY_CORRECT", "question_context_used": "A marginal table or collapsed table is obtained by summing over all levels of the factors in a set $B$. A table $p$ is collapsible over $B$ with respect to certain functions of $p$ if these functions can be obtained from the collapsed table. The functions considered are the log-linear parameters or $r$-factor interactions $u_{\\mathcal{A}}^{(n)}$ associated with a set $A$ of $r$ factors, with $A$ disjoint from $B$. For a 3-dimensional table, vanishing of the 3-factor interaction $u_{123}^{(3)}$ does not imply collapsibility over a factor with respect to the remaining two factors, nor is it implied by it. Strict collapsibility requires both $u_{ij}^{(3)}=u_{ij,k}^{(2)}$ and $u_{123}^{(3)}=0$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The text highlights the need to capture spatial dependencies in level-2 data.\n- **B)** Correct. Computational feasibility is a significant challenge when dealing with massive amounts of data.\n- **C)** Incorrect. The text stresses the importance of considering spatial dependencies for optimal predictions.\n- **D)** Correct. Maintaining aggregation consistency is mentioned as a necessary requirement for level-3 data.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Fast,_Resolution-Consistent_Spatial_Prediction_of_Global_Processes_From_Satellite_Data__Fast__Resolution-Consistent_Spatial_Prediction_of_Global_Processes_From_Satellite_Data.md"], "background_text": "Producing level-3 data from level-2 satellite data involves several challenges: capturing spatial dependencies, ensuring computational feasibility with massive data, and maintaining aggregation consistency for predictions and prediction variances. The text emphasizes the importance of incorporating spatial dependencies and the computational challenges posed by massive datasets.", "folder": "", "qid": "statistic-mcqm-5468", "question": "Which of the following are challenges in producing level-3 data from level-2 satellite data as described in the text?", "options": ["A) Capturing spatial dependencies inherent in the level-2 data.", "B) Ensuring the computational feasibility of processing massive amounts of data.", "C) Producing predictions that are statistically optimal without considering spatial dependencies.", "D) Maintaining aggregation consistency for predictions and prediction variances."], "category": "CORRECT", "question_context_used": "Producing level-3 data from level-2 satellite data involves several challenges: capturing spatial dependencies, ensuring computational feasibility with massive data, and maintaining aggregation consistency for predictions and prediction variances. The text emphasizes the importance of incorporating spatial dependencies and the computational challenges posed by massive datasets.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A)** Correct. The event with id 3 has $t_i = 3$, $Z_j = 1$, and $m_j = 8$.\n- **B)** Incorrect. The event with id 7 has Team_id 1 and mark $m_j = 19$, but the event with id 3 has Team_id 2 and mark $m_j = 8$.\n- **C)** Correct. The event with id 8 is the only one with Team_id 2 in the provided data.\n- **D)** Correct. The event with id 4 has $t_i = 6$, $Z_j = 3$, and $m_j = 16$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Seconder of the vote of thanks to Narayanan, Kosmi.md"], "background_text": "Table 4 provides a snapshot of the final data prepared for modelling, where each event consists of the time of occurrence $t_i$, the zone $Z_j$, and the mark $m_j$. The data includes events with different Team_id, zones, and marks.", "folder": "", "qid": "statistic-mcqm-2478", "question": "Based on the data in Table 4, which of the following statements are true?", "options": ["A) The event with id 3 occurred at time $t_i = 3$ in zone $Z_j = 1$ with mark $m_j = 8$.", "B) All events with Team_id 1 have marks greater than 15.", "C) The event with id 8 is the only one with Team_id 2.", "D) The event with id 4 occurred at time $t_i = 6$ in zone $Z_j = 3$ with mark $m_j = 16$."], "category": "PARTIALLY_CORRECT", "question_context_used": "Table 4 provides a snapshot of the final data prepared for modelling, where each event consists of the time of occurrence $t_i$, the zone $Z_j$, and the mark $m_j$. The data includes events with different Team_id, zones, and marks.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": []\n}", "model_answer": [], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A) Correct**: The FSSN model's higher log-likelihood (1115.406 vs. not provided for normal but implied poorer fit) indicates a better fit.\n- **B) Correct**: The LR test's significant result (p < 0.001) strongly supports the FSSN model over the normal model.\n- **C) Correct**: Fig. 2 visually confirms the FSSN model's better fit compared to the normal model.\n- **D) Incorrect**: The text states that ML estimators have smaller SE's than moment estimators, making this statement false.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["On-the-Fern-ndez-Steel-distribution--Infere_2011_C.md"], "background_text": "The comparison between the FSSN and normal models is based on log-likelihood values, LR test results, and visual fits shown in Fig. 2. The LR test statistic is 37.27 with a p-value < 0.001, favoring the FSSN model. Visual comparisons in Fig. 2 also support the FSSN model's better fit. The text mentions that moment estimates' SE's are greater than those of ML estimators, contrary to option D.", "folder": "", "qid": "statistic-mcqm-5909", "question": "Which of the following conclusions can be drawn from the comparison of the FSSN and normal models based on the volcano data?", "options": ["A) The FSSN model provides a better fit to the volcano data than the normal model, as evidenced by the higher log-likelihood.", "B) The LR test statistic value of 37.27 with a p-value < 0.001 indicates that the FSSN model is significantly more appropriate than the normal model.", "C) The visual comparison in Fig. 2 shows a good agreement between the histogram of the volcano data and the estimated FSSN p.d.f., but a poor fit for the estimated normal p.d.f.", "D) The moment estimates for the FSSN model are more precise than the ML estimates, as their SE's are smaller."], "category": "CORRECT", "question_context_used": "The comparison between the FSSN and normal models is based on log-likelihood values, LR test results, and visual fits shown in Fig. 2. The LR test statistic is 37.27 with a p-value < 0.001, favoring the FSSN model. Visual comparisons in Fig. 2 also support the FSSN model's better fit. The text mentions that moment estimates' SE's are greater than those of ML estimators, contrary to option D.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A)** Correct. Partition testing indeed divides the parameter space into $2^k$ disjoint subspaces for $k$ hypotheses.\n- **B)** Correct. The inference that $\\theta_i > 0$ is made if and only if all relevant $H_{0I}^*$ hypotheses are rejected.\n- **C)** Incorrect. Because the $H_{0I}^*$’s are disjoint, no multiplicity adjustment is needed among them.\n- **D)** Correct. The disjoint nature of $H_{0I}^*$ means at most one can be true.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Hochberg's_Step-Up_Method_Cutting_Corners_Off_Holm's_Step-Down_Method.md"], "background_text": "Partition testing is a general principle for multiple testing that divides the parameter space into $2^k$ disjoint subspaces for $k$ hypotheses. It infers that $\\theta_i > 0$ if and only if all $H_{0I}^*$ with $i \\in I$ are rejected. The null hypotheses $H_{0I}^*$ are disjoint, so at most one $H_{0I}^*$ can be true, eliminating the need for multiplicity adjustment among the $H_{0I}^*$’s.", "folder": "", "qid": "statistic-mcqm-5901", "question": "In the context of partition testing for multiple hypotheses, which of the following statements are correct?", "options": ["A) Partition testing divides the parameter space into $2^k$ disjoint subspaces for $k$ hypotheses.", "B) For each $i$, infer that $\\theta_i > 0$ if and only if all $H_{0I}^*$ with $i \\in I$ are rejected.", "C) Partition testing requires multiplicity adjustment among the $H_{0I}^*$’s to control the Familywise Error Rate.", "D) The null hypotheses $H_{0I}^*$ are disjoint, meaning at most one $H_{0I}^*$ can be true."], "category": "CORRECT", "question_context_used": "Partition testing is a general principle for multiple testing that divides the parameter space into $2^k$ disjoint subspaces for $k$ hypotheses. It infers that $\\theta_i > 0$ if and only if all $H_{0I}^*$ with $i \\in I$ are rejected. The null hypotheses $H_{0I}^*$ are disjoint, so at most one $H_{0I}^*$ can be true, eliminating the need for multiplicity adjustment among the $H_{0I}^*$’s.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A)** Correct. Entropy $H[T]$ indeed measures the uncertainty about the outcome of $T$ in bits, as it quantifies the average number of yes/no questions needed to ascertain the outcome.\n- **B)** Correct. The entropy $H[T]$ reaches its maximum value when the probability distribution is uniform, i.e., $p_{T}(t)=1/|\\mathcal{T}|$ for all $t\\in\\mathcal T$.\n- **C)** Incorrect. Entropy cannot be negative because probabilities $p_{T}(t)$ are between 0 and 1, making $\\log_{2}p_{T}(t)$ non-positive, and thus $-p_{T}(t)\\log_{2}p_{T}(t)$ is non-negative.\n- **D)** Incorrect. $H[T]$ provides bounds (between $H[T]$ and $H[T]+1$) for the average number of yes/no questions needed, not an exact count.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Scandinavian J Statistics - 2019 - Lagerås - How t.md"], "background_text": "Entropy $H[T]$ is defined as $-\\sum_{t\\in T}p_{T}(t)\\log_{2}p_{T}(t)$. It measures uncertainty in bits, with its maximum value occurring when all outcomes are equally likely. It bounds the average number of yes/no questions needed to ascertain the outcome but does not provide an exact count.", "folder": "", "qid": "statistic-mcqm-5810", "question": "Which of the following statements about entropy $H[T]$ are correct?", "options": ["A) $H[T]$ measures the uncertainty about the outcome of $T$ in bits.", "B) $H[T]$ is maximized when $p_{T}(t)=1/|\\mathcal{T}|$ for all $t\\in\\mathcal T$.", "C) $H[T]$ can be negative if the outcomes of $T$ are highly predictable.", "D) $H[T]$ provides an exact count of the number of yes/no questions needed to ascertain the outcome of $T$."], "category": "CORRECT", "question_context_used": "Entropy $H[T]$ is defined as $-\\sum_{t\\in T}p_{T}(t)\\log_{2}p_{T}(t)$. It measures uncertainty in bits, with its maximum value occurring when all outcomes are equally likely. It bounds the average number of yes/no questions needed to ascertain the outcome but does not provide an exact count.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The coefficient for $y$ is indeed 0.5432 with the given confidence interval.\n- **B)** Correct. The coefficient for $y^2$ is -0.7330 with the specified confidence interval.\n- **C)** Correct. The coefficient for $y^3$ is 0.4440 with the provided confidence interval.\n- **D)** Correct. The constant term is -0.0972 with the given confidence interval.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["Semi-parametric-nonlinear-regression-and-transf_20.md"], "background_text": "The CLS criterion model is given by $0.5440~y-0.7343~y^{2}+0.4447~y^{3}+0.0971=x_{1}+0.0889~x_{2}$. Table 7 provides point and confidence interval estimates for the coefficients under the CLS criterion.", "folder": "", "qid": "statistic-mcqm-5147", "question": "Based on the CLS criterion model, which of the following statements are correct about the coefficients?", "options": ["A) The coefficient for $y$ is 0.5432 with a 95% confidence interval of (0.5086, 0.5752).", "B) The coefficient for $y^2$ is -0.7330 with a 95% confidence interval of (-0.8319, -0.6420).", "C) The coefficient for $y^3$ is 0.4440 with a 95% confidence interval of (0.3739, 0.5186).", "D) The constant term is -0.0972 with a 95% confidence interval of (-0.1073, -0.0882)."], "category": "PARTIALLY_CORRECT", "question_context_used": "The CLS criterion model is given by $0.5440~y-0.7343~y^{2}+0.4447~y^{3}+0.0971=x_{1}+0.0889~x_{2}$. Table 7 provides point and confidence interval estimates for the coefficients under the CLS criterion.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **A)** Correct. The text states that $\\pmb{M}$ is calculated from $d_{\\bullet}$ and $\\pmb{\\nu}$ as in equation (6).\n- **B)** Correct. Similarly, $\\pmb{V}$ is also calculated from $d_{\\bullet}$ and $\\pmb{\\nu}$ as in equation (6).\n- **C)** Incorrect. While both $\\pmb{M}$ and $\\pmb{V}$ are derived from the same components, they represent different quantities and are not derived from the same equation.\n- **D)** Incorrect. The text does not specify that $\\pmb{M}$ represents the mean and $\\pmb{V}$ represents the variance of the sample ranges.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["THE_TWO-SAMPLE_t-TEST_BASED_ON_RANGE.md"], "background_text": "For each of the 190 pairs of sample sizes, where $\\pmb{n}_{1}$ and $\\pmb{n_{1}}$ go from 2 to 20, the quantities $\\pmb{M}$ and $\\pmb{V}$ were calculated from $d_{\\bullet}$ and $\\pmb{\\nu}$ as in (6).", "folder": "", "qid": "statistic-mcqm-3724", "question": "Given the quantities calculated for each pair of sample sizes, which of the following statements are correct about the values of $\\pmb{M}$ and $\\pmb{V}$?", "options": ["A) $\\pmb{M}$ is calculated from $d_{\\bullet}$ and $\\pmb{\\nu}$ as in equation (6).", "B) $\\pmb{V}$ is calculated from $d_{\\bullet}$ and $\\pmb{\\nu}$ as in equation (6).", "C) $\\pmb{M}$ and $\\pmb{V}$ are both derived from the same equation.", "D) $\\pmb{M}$ represents the mean and $\\pmb{V}$ represents the variance of the sample ranges."], "category": "PARTIALLY_CORRECT", "question_context_used": "For each of the 190 pairs of sample sizes, where $\\pmb{n}_{1}$ and $\\pmb{n_{1}}$ go from 2 to 20, the quantities $\\pmb{M}$ and $\\pmb{V}$ were calculated from $d_{\\bullet}$ and $\\pmb{\\nu}$ as in (6).", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**: \n- **A) Correct**: The curse of dimensionality refers to the sparsity of data in high-dimensional spaces, making nonparametric estimation challenging.\n- **B) Incorrect**: The variance of the error term $\\varepsilon$ is constant and does not depend on the dimension of the predictor.\n- **C) Correct**: The Sacks–Ylvisaker condition is a requirement on the design points for consistent estimation in nonparametric regression.\n- **D) Incorrect**: The regression function $g$ does not necessarily become linear in higher dimensions; the challenge is due to the curse of dimensionality.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Analysis-of-Variance-in-Nonparametric-Regress_2001.md"], "background_text": "The nonparametric regression model involves estimating a function $g$ without assuming a parametric form. For higher-dimensional predictors, standard nonparametric methods face the 'curse of dimensionality,' making estimation less practical due to the sparsity of data in high-dimensional spaces.", "folder": "", "qid": "statistic-mcqm-3347", "question": "In the nonparametric regression model $Y = g(t) + \\varepsilon$, where $\\varepsilon$ has mean 0 and variance $\\sigma^2 > 0$, what are the challenges associated with estimating the regression function $g$ for higher-dimensional predictors?", "options": ["A) The curse of dimensionality makes standard nonparametric methods less attractive.", "B) The variance of the error term $\\varepsilon$ increases with the dimension of the predictor.", "C) The design points must satisfy a Sacks–Ylvisaker condition for consistent estimation.", "D) The regression function $g$ becomes linear in higher dimensions."], "category": "PARTIALLY_CORRECT", "question_context_used": "The nonparametric regression model involves estimating a function $g$ without assuming a parametric form. For higher-dimensional predictors, standard nonparametric methods face the 'curse of dimensionality,' making estimation less practical due to the sparsity of data in high-dimensional spaces.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**: \n- **Option A** is correct because it directly states one of the definitions of a supermodular function.\n- **Option B** is correct because it provides an alternative definition of supermodularity using lattice operators.\n- **Option C** is correct because it gives a necessary and sufficient condition for supermodularity of twice differentiable functions.\n- **Option D** is correct because it states a property of supermodular functions when composed with increasing functions.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Some-Remarks-on-the-Supermodular-Order_2000_Journa.md"], "background_text": "Supermodularity of a function $f\\colon\\mathbb{R}^{d}\\to\\mathbb{R}$ can be defined by $\\varDelta_{i}^{\\varepsilon}\\varDelta_{j}^{\\delta}f(\\mathbf{x})\\geqslant0$ for all $\\mathbf{x}\\in\\mathbb{R}^{d},1\\leqslant i<j\\leqslant d$ and all $\\varepsilon,\\delta>0$. Alternatively, it can be defined by $f(\\mathbf{x}\\wedge\\mathbf{y})+f(\\mathbf{x}\\vee\\mathbf{y})\\geqslant f(\\mathbf{x})+f(\\mathbf{y})$ for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$. For a twice differentiable function, supermodularity is equivalent to $\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}f(\\mathbf{x})\\geqslant0$ for all $\\mathbf{x}\\in\\mathbb{R}^{d},1\\leqslant i<j\\leqslant d$. Additionally, if $g_{1},...,g_{d}\\colon\\mathbb{R}\\to\\mathbb{R}$ are increasing functions and $f$ is supermodular, then $f(g_{1}(\\cdot),...,g_{d}(\\cdot))$ is also supermodular.", "folder": "", "qid": "statistic-mcqm-4951", "question": "Which of the following statements are true regarding supermodular functions?", "options": ["A) A function $f$ is supermodular if $\\varDelta_{i}^{\\varepsilon}\\varDelta_{j}^{\\delta}f(\\mathbf{x})\\geqslant0$ for all $\\mathbf{x}\\in\\mathbb{R}^{d},1\\leqslant i<j\\leqslant d$ and all $\\varepsilon,\\delta>0$.", "B) A function $f$ is supermodular if $f(\\mathbf{x}\\wedge\\mathbf{y})+f(\\mathbf{x}\\vee\\mathbf{y})\\geqslant f(\\mathbf{x})+f(\\mathbf{y})$ for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$.", "C) If $f$ is twice differentiable, then $f$ is supermodular if and only if $\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}f(\\mathbf{x})\\geqslant0$ for all $\\mathbf{x}\\in\\mathbb{R}^{d},1\\leqslant i<j\\leqslant d$.", "D) If $g_{1},...,g_{d}\\colon\\mathbb{R}\\to\\mathbb{R}$ are increasing functions and $f$ is supermodular, then $f(g_{1}(\\cdot),...,g_{d}(\\cdot))$ is also supermodular."], "category": "CORRECT", "question_context_used": "Supermodularity of a function $f\\colon\\mathbb{R}^{d}\\to\\mathbb{R}$ can be defined by $\\varDelta_{i}^{\\varepsilon}\\varDelta_{j}^{\\delta}f(\\mathbf{x})\\geqslant0$ for all $\\mathbf{x}\\in\\mathbb{R}^{d},1\\leqslant i<j\\leqslant d$ and all $\\varepsilon,\\delta>0$. Alternatively, it can be defined by $f(\\mathbf{x}\\wedge\\mathbf{y})+f(\\mathbf{x}\\vee\\mathbf{y})\\geqslant f(\\mathbf{x})+f(\\mathbf{y})$ for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$. For a twice differentiable function, supermodularity is equivalent to $\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}f(\\mathbf{x})\\geqslant0$ for all $\\mathbf{x}\\in\\mathbb{R}^{d},1\\leqslant i<j\\leqslant d$. Additionally, if $g_{1},...,g_{d}\\colon\\mathbb{R}\\to\\mathbb{R}$ are increasing functions and $f$ is supermodular, then $f(g_{1}(\\cdot),...,g_{d}(\\cdot))$ is also supermodular.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. LVP was included in the final equation.\n- **B)** Correct. HR was included in the final equation.\n- **C)** Correct. SV was included in the final equation.\n- **D)** Incorrect. TTI was omitted from the final equation to avoid over-complicating the model.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["A Bayesian Analysis of Some Pharmacological Data U.md"], "background_text": "The final equation from the stepwise regression procedure included four haemodynamic variables: LVP, HR, SV, and AP. Extremes of TTI were omitted from further consideration to avoid over-complicating the model.", "folder": "", "qid": "statistic-mcqm-5993", "question": "Which haemodynamic variables were included in the final equation from the stepwise regression procedure?", "options": ["A) LVP", "B) HR", "C) SV", "D) TTI"], "category": "CORRECT", "question_context_used": "The final equation from the stepwise regression procedure included four haemodynamic variables: LVP, HR, SV, and AP. Extremes of TTI were omitted from further consideration to avoid over-complicating the model.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The first value of $\\vartheta$ is indeed 1.4 year$^{-1}$, as mentioned in the context.\n- **B)** Correct. The values of $\\vartheta$ are chosen to cover a wide range to observe different behaviors of the estimators.\n- **C)** Correct. The initial value of each simulated log-price series is the logarithm of a random variable taken uniformly over the interval [20, 40].\n- **D)** Incorrect. The parameter $\\vartheta$ does affect the behavior of the estimators, as different values are chosen to observe different behaviors.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Scandinavian J Statistics - 2019 - Féron - Efficie.md"], "background_text": "Various values of $\\vartheta$ (values in year $^{-1}$) are adopted: 1.4, 10, 40. The first value is the estimated parameter shown in Kiesel et al. (2009) and the others are chosen to cover a wide range of possible values to observe different behaviors of the estimators. The initial value of each simulated log-price series is the logarithm of a random variable taken uniformly over the interval [20, 40], which is a usual range for prices in the market of forward contracts on electricity.", "folder": "", "qid": "statistic-mcqm-5815", "question": "Which of the following statements about the parameter $\\vartheta$ in the simulated data are correct?", "options": ["A) The first value of $\\vartheta$ is 1.4 year$^{-1}$, which is the estimated parameter shown in Kiesel et al. (2009).", "B) The values of $\\vartheta$ are chosen to cover a wide range of possible values to observe different behaviors of the estimators.", "C) The initial value of each simulated log-price series is the logarithm of a random variable taken uniformly over the interval [20, 40].", "D) The parameter $\\vartheta$ does not affect the behavior of the estimators."], "category": "PARTIALLY_CORRECT", "question_context_used": "Various values of $\\vartheta$ (values in year $^{-1}$) are adopted: 1.4, 10, 40. The first value is the estimated parameter shown in Kiesel et al. (2009) and the others are chosen to cover a wide range of possible values to observe different behaviors of the estimators. The initial value of each simulated log-price series is the logarithm of a random variable taken uniformly over the interval [20, 40], which is a usual range for prices in the market of forward contracts on electricity.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: $\\xi_{\\mathrm{post}}$ is indeed set to 0 as per the derivation.\n- **B) Correct**: $\\Omega_{\\mathrm{post}}$ is correctly set to 1.\n- **C) Correct**: $\\Delta_{\\mathrm{post}}$ is accurately defined as $(2y-1)x(x^{2}+1)^{-1/2}$.\n- **D) Incorrect**: $\\Gamma_{\\mathrm{post}}$ is set to 1, not 2, in the derivation.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Conjugate_Bayes_for_probit_regression_via_unified_skew-normal_distributions.md"], "background_text": "The kernel of the unified skew-normal distribution in Lemma 1 is derived by setting specific parameters: $\\xi_{\\mathrm{post}}=0$, $\\Omega_{\\mathrm{post}}=1$, $\\Delta_{\\mathrm{post}}=(2y-1)x(x^{2}+1)^{-1/2}$, $\\gamma_{\\mathrm{post}}=0$, and $\\Gamma_{\\mathrm{post}}=(x^{2}+1)^{-1}+\\Delta_{\\mathrm{post}}^{\\mathrm{T}}\\Delta_{\\mathrm{post}}=1$.", "folder": "", "qid": "statistic-mcqm-5636", "question": "In the context of the unified skew-normal distribution, which of the following parameters are correctly set to derive the kernel as per Lemma 1?", "options": ["A) $\\xi_{\\mathrm{post}}=0$", "B) $\\Omega_{\\mathrm{post}}=1$", "C) $\\Delta_{\\mathrm{post}}=(2y-1)x(x^{2}+1)^{-1/2}$", "D) $\\Gamma_{\\mathrm{post}}=(x^{2}+1)^{-1}+\\Delta_{\\mathrm{post}}^{\\mathrm{T}}\\Delta_{\\mathrm{post}}=2$"], "category": "CORRECT", "question_context_used": "The kernel of the unified skew-normal distribution in Lemma 1 is derived by setting specific parameters: $\\xi_{\\mathrm{post}}=0$, $\\Omega_{\\mathrm{post}}=1$, $\\Delta_{\\mathrm{post}}=(2y-1)x(x^{2}+1)^{-1/2}$, $\\gamma_{\\mathrm{post}}=0$, and $\\Gamma_{\\mathrm{post}}=(x^{2}+1)^{-1}+\\Delta_{\\mathrm{post}}^{\\mathrm{T}}\\Delta_{\\mathrm{post}}=1$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A) Correct**: The parameter $k$ must be set to $p_{n}$ for the joint density function to match that of generalized order statistics.\n- **B) Correct**: The parameters $m_{j}$ must be defined as $(n-j+1)p_{j} - (n-j)p_{j+1} - 1$ for all $j$ to align with the generalized order statistics model.\n- **C) Correct**: The given joint density function must hold for $0 \\leq t_{1} \\leq \\cdots \\leq t_{n}$ to be a particular case of generalized order statistics.\n- **D) Incorrect**: The condition $X_{(1:n)} \\leq_{i c x} Y_{(1:n)}$ is related to the ordering of random vectors, not the parameterization of the joint density function.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Increasing-directionally-convex-orderings-of-rando.md"], "background_text": "The joint density function of $(X_{(1:n)},\\dots,X_{(n:n)})$ under a minimal repair policy is given by $f(t_{1},\\ldots,t_{n})=n!\\prod_{j=1}^{n}p_{j}f(t_{j})(\\overline{F}(t_{j}))^{(n-j+1)p_{j}-(n-j)p_{j+1}-1}$ for $0 \\leq t_{1} \\leq \\cdots \\leq t_{n}$. This is a particular case of the joint density function of generalized order statistics based on $F$ for the choice of parameters $k=p_{n}$ and $m_{j}=(n-j+1)p_{j}-(n-j)p_{j+1}-1$.", "folder": "", "qid": "statistic-mcqm-2118", "question": "Which of the following conditions must be satisfied for the joint density function of $(X_{(1:n)},\\dots,X_{(n:n)})$ under a minimal repair policy to be a particular case of the joint density function of generalized order statistics based on $F$?", "options": ["A) $k = p_{n}$", "B) $m_{j} = (n-j+1)p_{j} - (n-j)p_{j+1} - 1$ for all $j$", "C) $n!\\prod_{j=1}^{n}p_{j}f(t_{j})(\\overline{F}(t_{j}))^{(n-j+1)p_{j}-(n-j)p_{j+1}-1}$ for $0 \\leq t_{1} \\leq \\cdots \\leq t_{n}$", "D) $X_{(1:n)} \\leq_{i c x} Y_{(1:n)}$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The joint density function of $(X_{(1:n)},\\dots,X_{(n:n)})$ under a minimal repair policy is given by $f(t_{1},\\ldots,t_{n})=n!\\prod_{j=1}^{n}p_{j}f(t_{j})(\\overline{F}(t_{j}))^{(n-j+1)p_{j}-(n-j)p_{j+1}-1}$ for $0 \\leq t_{1} \\leq \\cdots \\leq t_{n}$. This is a particular case of the joint density function of generalized order statistics based on $F$ for the choice of parameters $k=p_{n}$ and $m_{j}=(n-j+1)p_{j}-(n-j)p_{j+1}-1$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. Warp-II transformations involve scaling and rotation to match the center and spread of two distributions.\n- **B)** Correct. The Warp-II transformations are defined using $T_{i}(w_{i})=S_{i}^{-1}(w_{i}-\\mu_{i}), i=1,2$, where $\\mu_{i}$ and $S_{i}$ are measures of the center and spread of $p_{i}$.\n- **C)** Incorrect. Warp-II transformations do not require new draws from the transformed distributions; they use the available draws from $p_{1}$ and $p_{2}$.\n- **D)** Correct. Warp-II transformations can lead to dramatic gains in efficiency over Warp-I transformations without unduly increasing the computational load.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Warp_Bridge_Sampling__Warp_Bridge_Sampling.md"], "background_text": "Warp-II transformations involve second-order warping, such as scaling and rotation, to match the center and spread of two distributions. Specifically, a Warp-II estimator is obtained by using $T_{i}(w_{i})=S_{i}^{-1}(w_{i}-\\mu_{i}), i=1,2$, where $\\mu_{i}$ and $S_{i}$ are measures of the center and spread of $p_{i}$. These transformations can lead to dramatic gains in efficiency over Warp-I transformations without unduly increasing the computational load and do not require new draws from the transformed distributions.", "folder": "", "qid": "statistic-mcqm-4893", "question": "Which of the following are correct about the Warp-II transformations?", "options": ["A) Warp-II transformations involve scaling and rotation to match the center and spread of two distributions.", "B) Warp-II transformations are obtained by using $T_{i}(w_{i})=S_{i}^{-1}(w_{i}-\\mu_{i}), i=1,2$, where $\\mu_{i}$ and $S_{i}$ are measures of the center and spread of $p_{i}$.", "C) Warp-II transformations require new draws from the transformed distributions.", "D) Warp-II transformations can lead to dramatic gains in efficiency over Warp-I transformations without unduly increasing the computational load."], "category": "CORRECT", "question_context_used": "Warp-II transformations involve second-order warping, such as scaling and rotation, to match the center and spread of two distributions. Specifically, a Warp-II estimator is obtained by using $T_{i}(w_{i})=S_{i}^{-1}(w_{i}-\\mu_{i}), i=1,2$, where $\\mu_{i}$ and $S_{i}$ are measures of the center and spread of $p_{i}$. These transformations can lead to dramatic gains in efficiency over Warp-I transformations without unduly increasing the computational load and do not require new draws from the transformed distributions.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**:\n- **A) Correct**: The BLUE formula explicitly includes $T_{d}$ and $F_{d}$, indicating dependence on the design $d$.\n- **B) Correct**: The text states that for $d\\in\\Lambda_{t,n,p}^{*}$, $\\mathrm{var}_{1}({\\hat{\\psi}})=\\sigma^{2}/c_{d}$.\n- **C) Correct**: The text mentions that $\\hat{\\psi}$ remains unbiased under correlation structure (2), implying it is unbiased under any correlation structure.\n- **D) Correct**: The text states that $\\hat{\\sigma}^{2}$ is an unbiased estimate of $\\sigma^{2}$ under model (1).\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Estimation of Variance in Crossover Designs.md"], "background_text": "The best linear unbiased estimate for $\\psi$ in model (1) equals $\\hat{\\psi}(Y_{d})=l^{\\prime}{\\cal C}_{d}^{-}T_{d}^{\\prime}\\mathrm{pr}^{\\perp}\\left\\{(P,U,F_{d})\\right\\}Y_{d}$. The variance of $\\hat{\\psi}$ under model (1) is $\\mathrm{var}_{1}({\\hat{\\psi}})=\\sigma^{2}/c_{d}$. The estimate $\\hat{\\sigma}^{2}$ is unbiased for $\\sigma^{2}$ under model (1) with $f$ degrees of freedom.", "folder": "", "qid": "statistic-mcqm-4778", "question": "For the best linear unbiased estimate (BLUE) $\\hat{\\psi}(Y_{d})$ of a standardized contrast $\\psi$ in model (1), which of the following are true?", "options": ["A) The BLUE depends on the design $d$ through $T_{d}$ and $F_{d}$.", "B) The variance of $\\hat{\\psi}$ under model (1) is $\\sigma^{2}/c_{d}$.", "C) $\\hat{\\psi}$ remains unbiased under any correlation structure.", "D) The estimate $\\hat{\\sigma}^{2}$ is unbiased for $\\sigma^{2}$ under model (1)."], "category": "PARTIALLY_CORRECT", "question_context_used": "The best linear unbiased estimate for $\\psi$ in model (1) equals $\\hat{\\psi}(Y_{d})=l^{\\prime}{\\cal C}_{d}^{-}T_{d}^{\\prime}\\mathrm{pr}^{\\perp}\\left\\{(P,U,F_{d})\\right\\}Y_{d}$. The variance of $\\hat{\\psi}$ under model (1) is $\\mathrm{var}_{1}({\\hat{\\psi}})=\\sigma^{2}/c_{d}$. The estimate $\\hat{\\sigma}^{2}$ is unbiased for $\\sigma^{2}$ under model (1) with $f$ degrees of freedom.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: This is the definition of identifiability provided in the context.\n- **B) Correct**: The context provides this exact formula for $p(u,\\theta)$.\n- **C) Correct**: The set of covariates $\\mathcal{U}$ is indeed a subset of $\\mathbb{R}^{d}$.\n- **D) Incorrect**: The parameters $a$ and $b$ are part of the model and are related to the covariates $u$ through the function $p(u,\\theta)$.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["Identifiability-of-cure-models-revisite_2014_Journ.md"], "background_text": "In the logistic regression setting, the function $p(u,\\theta)$ is given by $\\frac{\\exp(a+b\\cdot u)}{1+\\exp(a+b\\cdot u)}$, where $a\\in\\mathbb R$ and $b\\in\\mathbb{R}^{d}$. The set of covariates $\\mathcal{U}$ is a subset of $\\mathbb{R}^{d}$. The model is identifiable if $p(u,\\theta_{1})=p(u,\\theta_{2})$ for all $u$ implies $\\theta_{1}=\\theta_{2}$.", "folder": "", "qid": "statistic-mcqm-1948", "question": "Regarding the logistic regression setting for the function $p(u,\\theta)$, which of the following statements are correct?", "options": ["A) The model is identifiable if $p(u,\\theta_{1})=p(u,\\theta_{2})$ for all $u$ implies $\\theta_{1}=\\theta_{2}$.", "B) The function $p(u,\\theta)$ is given by $\\frac{\\exp(a+b\\cdot u)}{1+\\exp(a+b\\cdot u)}$.", "C) The set of covariates $\\mathcal{U}$ for function $p$ is a subset of $\\mathbb{R}^{d}$.", "D) The parameters $a$ and $b$ are independent of the covariates $u$."], "category": "INCORRECT", "question_context_used": "In the logistic regression setting, the function $p(u,\\theta)$ is given by $\\frac{\\exp(a+b\\cdot u)}{1+\\exp(a+b\\cdot u)}$, where $a\\in\\mathbb R$ and $b\\in\\mathbb{R}^{d}$. The set of covariates $\\mathcal{U}$ is a subset of $\\mathbb{R}^{d}$. The model is identifiable if $p(u,\\theta_{1})=p(u,\\theta_{2})$ for all $u$ implies $\\theta_{1}=\\theta_{2}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "To determine the correct options, we evaluate each statement based on the theorems provided in the context.\n\n- **Option A**: This is correct as per Theorem 1, which states that if $d_{i} + u_{i} < 1$, then $p_{11}^{*} - p_{10}^{*} - p_{01}^{*} + p_{00}^{*} > 0$ implies $p_{11} - p_{10} - p_{01} + p_{00} > 0$.\n\n- **Option B**: This is correct as per Theorem 2, which states that if $d_{i} < 1/2$ and $u_{i} < 1/4$, then $p_{11}^{*} - p_{10}^{*} - p_{01}^{*} > 0$ implies $p_{11} - p_{10} - p_{01} > 0$.\n\n- **Option C**: This is correct as per Theorem 3, which states that if $d_{i} \\leqslant 1/3$ and $u_{i} \\leqslant 1/4$, then $p_{11}^{*} - p_{10}^{*} - p_{01}^{*} - p_{00}^{*} > 0$ implies $p_{11} - p_{10} - p_{01} - p_{00} > 0$.\n\n- **Option D**: This is correct as per Theorem 4, which states that if $d_{i} < 1/4$ and $u_{i} < 1/4$, then $p_{111}^{*} - p_{110}^{*} - p_{101}^{*} - p_{011}^{*} > 0$ implies $p_{111} - p_{110} - p_{101} - p_{011} > 0$.\n\n**Correct Choices**: A, B, C, D\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["Inference_for_additive_interaction_under_exposure_misclassification.md"], "background_text": "Under assumptions of nondifferential misclassification and independent predicted probabilities, certain inequalities for the observed exposures imply the corresponding inequalities for the true exposures under specific conditions on the misclassification probabilities. These conditions include $d_{i} + u_{i} < 1$ for additive interaction, $d_{i} < 1/2$ and $u_{i} < 1/4$ for sufficient cause interaction, $d_{i} \\leqslant 1/3$ and $u_{i} \\leqslant 1/4$ for epistatic interaction, and $d_{i} < 1/4$ and $u_{i} < 1/4$ for three-way sufficient cause interaction.", "folder": "", "qid": "statistic-mcqm-6157", "question": "Under the assumptions of nondifferential misclassification and independent predicted probabilities, which of the following inequalities for the observed exposures imply the corresponding inequalities for the true exposures?", "options": ["A) $p_{11}^{*} - p_{10}^{*} - p_{01}^{*} + p_{00}^{*} > 0$ implies $p_{11} - p_{10} - p_{01} + p_{00} > 0$ if $d_{i} + u_{i} < 1$", "B) $p_{11}^{*} - p_{10}^{*} - p_{01}^{*} > 0$ implies $p_{11} - p_{10} - p_{01} > 0$ if $d_{i} < 1/2$ and $u_{i} < 1/4$", "C) $p_{11}^{*} - p_{10}^{*} - p_{01}^{*} - p_{00}^{*} > 0$ implies $p_{11} - p_{10} - p_{01} - p_{00} > 0$ if $d_{i} \\leqslant 1/3$ and $u_{i} \\leqslant 1/4$", "D) $p_{111}^{*} - p_{110}^{*} - p_{101}^{*} - p_{011}^{*} > 0$ implies $p_{111} - p_{110} - p_{101} - p_{011} > 0$ if $d_{i} < 1/4$ and $u_{i} < 1/4$"], "category": "CORRECT", "question_context_used": "Under assumptions of nondifferential misclassification and independent predicted probabilities, certain inequalities for the observed exposures imply the corresponding inequalities for the true exposures under specific conditions on the misclassification probabilities. These conditions include $d_{i} + u_{i} < 1$ for additive interaction, $d_{i} < 1/2$ and $u_{i} < 1/4$ for sufficient cause interaction, $d_{i} \\leqslant 1/3$ and $u_{i} \\leqslant 1/4$ for epistatic interaction, and $d_{i} < 1/4$ and $u_{i} < 1/4$ for three-way sufficient cause interaction.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A)** Correct. AMSE is discussed as a criterion for selecting instruments by minimizing an approximate mean-square error.\n- **B)** Correct. MSC is mentioned as a criterion based on the overidentifying restrictions J statistic.\n- **C)** Correct. RMSC is proposed based on the entropy of the limiting distribution of the GMM estimator.\n- **D)** Incorrect. OLS is an estimation method, not an instrument selection criterion.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Linear-instrumental-variables-model-avera_2014_Com.md"], "background_text": "The context discusses several instrument selection criteria including AMSE, MSC, and RMSC. OLS is not an instrument selection criterion but a method of estimation.", "folder": "", "qid": "statistic-mcqm-5721", "question": "Which of the following are valid instrument selection criteria discussed in the context?", "options": ["A) AMSE (Approximate Mean-Square Error)", "B) MSC (Model Selection Criterion)", "C) RMSC (Relevant Moment Selection Criterion)", "D) OLS (Ordinary Least Squares)"], "category": "CORRECT", "question_context_used": "The context discusses several instrument selection criteria including AMSE, MSC, and RMSC. OLS is not an instrument selection criterion but a method of estimation.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**: \n- **Option A** is correct because the general model includes unknown parameters $c_{r_{1}...r_{k}}$.\n- **Option C** is correct because the model can include a transformation of the response variable $y$ as shown in the generalization.\n- **Option D** is correct because the absence of converging arrows means the model cannot be simplified.\n- **Option B** is incorrect because the sets of functions $\\phi_{s}$ are linearly independent, not dependent.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Semi-parametric-nonlinear-regression-and-transf_20.md"], "background_text": "The general model is given by $y=\\sum_{r_{1}=1}^{q_{1}}\\cdot\\cdot\\cdot\\sum_{r_{k}=1}^{q_{k}}c_{r_{1}\\ldots r_{k}}\\phi_{r_{1}}(x_{1})\\ldots\\phi_{r_{k}}(x_{k})$, where $c_{r_{1}...r_{k}}$ are unknown parameters and the sets of functions $\\phi_{s}$ are linearly independent. A generalization includes a transformation of $y$ as $f(y)$, and simplification is not possible due to no converging arrows.", "folder": "", "qid": "statistic-mcqm-5074", "question": "Which of the following are characteristics of the general model in functional networks?", "options": ["A) It includes unknown parameters $c_{r_{1}...r_{k}}$.", "B) The sets of functions $\\phi_{s}$ are linearly dependent.", "C) It can include a transformation of the response variable $y$.", "D) The model cannot be simplified due to the absence of converging arrows."], "category": "CORRECT", "question_context_used": "The general model is given by $y=\\sum_{r_{1}=1}^{q_{1}}\\cdot\\cdot\\cdot\\sum_{r_{k}=1}^{q_{k}}c_{r_{1}\\ldots r_{k}}\\phi_{r_{1}}(x_{1})\\ldots\\phi_{r_{k}}(x_{k})$, where $c_{r_{1}...r_{k}}$ are unknown parameters and the sets of functions $\\phi_{s}$ are linearly independent. A generalization includes a transformation of $y$ as $f(y)$, and simplification is not possible due to no converging arrows.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A)** Correct. The random coefficient regression model was used to address the imprecision of estimates from standard regression, as mentioned in the context.\n- **B)** Correct. The model allows generalization to the population of all dogs, which is a limitation of the standard regression approach.\n- **C)** Incorrect. The model was fitted using Bayesian techniques, not frequentist techniques.\n- **D)** Incorrect. The model specifically accounts for variability in coefficients from dog to dog.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["A Bayesian Analysis of Some Pharmacological Data U.md"], "background_text": "The study employed a random coefficient regression model to address the limitations of standard regression, such as imprecision in estimates and inability to generalize to the population of all dogs. The model was fitted using Bayesian techniques and accounts for variability in coefficients from dog to dog.", "folder": "", "qid": "statistic-mcqm-6014", "question": "Which of the following statements are true about the random coefficient regression model used in the study?", "options": ["A) It addresses the imprecision of estimates from standard regression.", "B) It allows generalization to the population of all dogs.", "C) It was fitted using frequentist techniques.", "D) It does not account for variability in coefficients from dog to dog."], "category": "CORRECT", "question_context_used": "The study employed a random coefficient regression model to address the limitations of standard regression, such as imprecision in estimates and inability to generalize to the population of all dogs. The model was fitted using Bayesian techniques and accounts for variability in coefficients from dog to dog.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **Option A** is correct as it directly states the asymptotic distribution of $\\hat{\\theta}_{n,\\varepsilon}$ under the given conditions.\n- **Option B** is correct because it reflects the consistency of the estimator $\\hat{\\theta}_{n,\\varepsilon}$ converging to the true parameter $\\theta_{0}$.\n- **Option C** is incorrect because the limiting distribution is not necessarily normal; it depends on the Lévy process $L$.\n- **Option D** is incorrect as it misrepresents the convergence; it's $\\varepsilon^{-1}(\\hat{\\theta}_{n,\\varepsilon}-\\theta_{0})$ that converges to $I^{-1}(\\theta_{0})S(\\theta_{0})$, not $\\hat{\\theta}_{n,\\varepsilon}$ itself.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Least-squares-estimators-for-discretely-observed-s.md"], "background_text": "The asymptotic distribution of $\\hat{\\theta}_{n,\\varepsilon}$ is given by $\\varepsilon^{-1}(\\hat{\\theta}_{n,\\varepsilon}-\\theta_{0})\\xrightarrow{P_{\\theta_{0}}}I^{-1}(\\theta_{0})S(\\theta_{0})$ as $\\varepsilon\\to0$, $n\\to\\infty$, and $n\\varepsilon\\to\\infty$, where $S(\\theta_{0})$ is a random vector defined by integrals involving the partial derivatives of the drift function $b$ and the Lévy process $L$.", "folder": "", "qid": "statistic-mcqm-847", "question": "Under the conditions (A1)–(A4), what is the asymptotic distribution of $\\hat{\\theta}_{n,\\varepsilon}$ as $\\varepsilon\\to0$, $n\\to\\infty$, and $n\\varepsilon\\to\\infty$?", "options": ["A) $\\varepsilon^{-1}(\\hat{\\theta}_{n,\\varepsilon}-\\theta_{0})\\xrightarrow{P_{\\theta_{0}}}I^{-1}(\\theta_{0})S(\\theta_{0})$", "B) $\\hat{\\theta}_{n,\\varepsilon}\\xrightarrow{P_{\\theta_{0}}}\\theta_{0}$", "C) $\\varepsilon^{-1}(\\hat{\\theta}_{n,\\varepsilon}-\\theta_{0})\\xrightarrow{P_{\\theta_{0}}}N(0,I^{-1}(\\theta_{0}))$", "D) $\\hat{\\theta}_{n,\\varepsilon}\\xrightarrow{P_{\\theta_{0}}}I^{-1}(\\theta_{0})S(\\theta_{0})$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The asymptotic distribution of $\\hat{\\theta}_{n,\\varepsilon}$ is given by $\\varepsilon^{-1}(\\hat{\\theta}_{n,\\varepsilon}-\\theta_{0})\\xrightarrow{P_{\\theta_{0}}}I^{-1}(\\theta_{0})S(\\theta_{0})$ as $\\varepsilon\\to0$, $n\\to\\infty$, and $n\\varepsilon\\to\\infty$, where $S(\\theta_{0})$ is a random vector defined by integrals involving the partial derivatives of the drift function $b$ and the Lévy process $L$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **Option A** is correct because it accurately represents the linear-Weibull model given in the text.\n- **Option B** is correct because the text states that $z$ is the dose of Red 40 expressed in terms of percentage of diet.\n- **Option D** is correct because the text specifies the constraints $\\alpha \\geqslant 0, \\rho \\geqslant 0, \\beta > 0$.\n- **Option C** is incorrect because the time unit is 104 weeks, not 42 weeks.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Estimating_the_tumour_onset_distribution_in_animal_carcinogenesis_experiments.md"], "background_text": "The linear-Weibull model for the Red 40 data is $F(t|z;\\theta)=1-\\exp\\left\\{-(\\alpha+\\rho z)t^{\\beta}\\right\\}$ where $z$ is the dose of Red 40 expressed in terms of percentage of diet. The time unit is taken as 104 weeks. The constraints on the parameters are $\\alpha \\geqslant 0, \\rho \\geqslant 0, \\beta > 0$.", "folder": "", "qid": "statistic-mcqm-5103", "question": "Which statements are true regarding the linear-Weibull model used for the Red 40 data?", "options": ["A) The model is $F(t|z;\\theta)=1-\\exp\\left\\{-(\\alpha+\\rho z)t^{\\beta}\\right\\}$.", "B) The parameter $z$ represents the dose of Red 40 expressed in terms of percentage of diet.", "C) The time unit is taken as 42 weeks.", "D) The constraints on the parameters are $\\alpha \\geqslant 0, \\rho \\geqslant 0, \\beta > 0$."], "category": "CORRECT", "question_context_used": "The linear-Weibull model for the Red 40 data is $F(t|z;\\theta)=1-\\exp\\left\\{-(\\alpha+\\rho z)t^{\\beta}\\right\\}$ where $z$ is the dose of Red 40 expressed in terms of percentage of diet. The time unit is taken as 104 weeks. The constraints on the parameters are $\\alpha \\geqslant 0, \\rho \\geqslant 0, \\beta > 0$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct because when $P\\{0\\}=0$, $\\epsilon(P) = \\beta(P)$ as stated in the context.\n- **B)** Correct because if $P$ is smooth, $\\epsilon(P) = 1/q$ as mentioned.\n- **D)** Correct because in general, the breakdown point is given by $\\epsilon(P) = \\frac{(1-P\\{0\\})\\beta(P_{o})}{1-P\\{0\\}\\beta(P_{o})}$.\n- **C)** Incorrect because $\\epsilon(P) = r/q$ only applies to the restricted breakdown point $\\epsilon(P|\\mathcal{H}_{r})$, not the general $\\epsilon(P)$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Scandinavian J Statistics - 2005 - DÜMBGEN - On th.md"], "background_text": "The breakdown point $\\epsilon(P)$ is defined under condition (4) and depends on the smoothness of $P$. For $P$ with $P\\{0\\}=0$, $\\epsilon(P) = \\beta(P)$. If $P$ is smooth, $\\epsilon(P) = 1/q$. In general, $\\epsilon(P) = \\frac{(1-P\\{0\\})\\beta(P_{o})}{1-P\\{0\\}\\beta(P_{o})}$.", "folder": "", "qid": "statistic-mcqm-4042", "question": "Given a probability distribution $P$ with $P\\{0\\}=0$ and condition (4) holds, which of the following statements are true about the breakdown point $\\epsilon(P)$?", "options": ["A) $\\epsilon(P) = \\beta(P)$", "B) $\\epsilon(P) = 1/q$ if $P$ is smooth", "C) $\\epsilon(P) = r/q$ for any $r$", "D) $\\epsilon(P) = \\frac{(1-P\\{0\\})\\beta(P_{o})}{1-P\\{0\\}\\beta(P_{o})}$ in general"], "category": "CORRECT", "question_context_used": "The breakdown point $\\epsilon(P)$ is defined under condition (4) and depends on the smoothness of $P$. For $P$ with $P\\{0\\}=0$, $\\epsilon(P) = \\beta(P)$. If $P$ is smooth, $\\epsilon(P) = 1/q$. In general, $\\epsilon(P) = \\frac{(1-P\\{0\\})\\beta(P_{o})}{1-P\\{0\\}\\beta(P_{o})}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct because it directly follows from the condition for singular information matrices when $k$ is even.\n- **B)** Correct as it is a simplified form of option A.\n- **C)** Correct because it is another simplification leading to the conclusion that $n=2$.\n- **D)** Correct because the relation holds only when $n=2$.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["A Unified Approach for Constructing a Useful Class.md"], "background_text": "When $k$ is even, singular information matrices occur if and only if the selected pairs lead to two $(n-1)$-dimensional lattices of the $n$-dimensional lattice. This implies the relation $2(k n)/2=2k^{n-1}$ or $k n=2k^{n-1}$ or $n=2k^{n-2}$, which holds only when $n=2$.", "folder": "", "qid": "statistic-mcqm-5540", "question": "In the context of singular information matrices, which of the following conditions must hold when $k$ is even?", "options": ["A) $2(k n)/2=2k^{n-1}$", "B) $k n=2k^{n-1}$", "C) $n=2k^{n-2}$", "D) $n=2$"], "category": "CORRECT", "question_context_used": "When $k$ is even, singular information matrices occur if and only if the selected pairs lead to two $(n-1)$-dimensional lattices of the $n$-dimensional lattice. This implies the relation $2(k n)/2=2k^{n-1}$ or $k n=2k^{n-1}$ or $n=2k^{n-2}$, which holds only when $n=2$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**:\n- **A) Correct**: The text states that sample sizes of $m=n=2000$ are needed for the area-based statistic $M_{m,n}^{*(12)}$.\n- **B) Correct**: The text states that sample sizes of $m=n=280$ are needed for the sup-norm statistic $D_{m,n}^{*(12)}$.\n- **C) Correct**: Conditioning on the correct interval reduces the required sample sizes for $M_{m,n}^{*(12)}$ to $m=n=95$.\n- **D) Correct**: The text mentions that conditioning on an incorrect interval does not significantly increase the required sample sizes for $D_{m,n}^{*(12)}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Nonparametric_tests_for_and_against_likelihood_ratio_ordering_in_the_two-sample_problem.md"], "background_text": "The text discusses the sample sizes required to reject $H_{1}$ using the area-based statistic $M_{m,n}^{*(12)}$ and the sup-norm statistic $D_{m,n}^{*(12)}$. Unconditionally, $m=n=2000$ and $m=n=280$ are needed for $M_{m,n}^{*(12)}$ and $D_{m,n}^{*(12)}$, respectively. Conditioning on the correct interval reduces these to $m=n=95$ and $m=n=210$, respectively. Conditioning on an incorrect interval does not significantly increase these sample sizes.", "folder": "", "qid": "statistic-mcqm-5732", "question": "Which of the following statements about the sample sizes required to reject $H_{1}$ are correct?", "options": ["A) For the area-based statistic $M_{m,n}^{*(12)}$, sample sizes of $m=n=2000$ are needed.", "B) For the sup-norm statistic $D_{m,n}^{*(12)}$, sample sizes of $m=n=280$ are needed.", "C) Conditioning on the correct interval reduces the required sample sizes for $M_{m,n}^{*(12)}$ to $m=n=95$.", "D) Conditioning on an incorrect interval does not significantly increase the required sample sizes for $D_{m,n}^{*(12)}$."], "category": "CORRECT", "question_context_used": "The text discusses the sample sizes required to reject $H_{1}$ using the area-based statistic $M_{m,n}^{*(12)}$ and the sup-norm statistic $D_{m,n}^{*(12)}$. Unconditionally, $m=n=2000$ and $m=n=280$ are needed for $M_{m,n}^{*(12)}$ and $D_{m,n}^{*(12)}$, respectively. Conditioning on the correct interval reduces these to $m=n=95$ and $m=n=210$, respectively. Conditioning on an incorrect interval does not significantly increase these sample sizes.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A)** Correct. Non-stationarity is accounted for by allowing GPD parameters to vary with covariates.\n- **B)** Correct. The specified functional form for $\\psi_v(\\mathbf{x})$ ensures threshold stability is retained.\n- **C)** Incorrect. Including different covariates in $\\psi_u(\\mathbf{x})$ and $\\xi(\\mathbf{x})$ leads to inconsistency for higher thresholds.\n- **D)** Incorrect. The log-link formulation may conflict with threshold stability unless $\\xi(\\mathbf{x})$ is constant or $\\psi_u(\\mathbf{x})$ is linear.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Modelling Non-Stationary Extremes with Application.md"], "background_text": "For non-stationary processes, GPD parameters ($\\psi_u(\\mathbf{x})$, $\\xi(\\mathbf{x})$, $\\phi_u(\\mathbf{x})$) can be functions of covariates. Threshold stability requires $\\psi_v(\\mathbf{x}) = \\psi_u(\\mathbf{x}) + (v-u)\\xi(\\mathbf{x})$ for any $v>u$, implying the same covariates must be included in $\\psi_u(\\mathbf{x})$ and $\\xi(\\mathbf{x})$. The log-link formulation may not always satisfy this, especially if $\\xi(\\mathbf{x})$ is not constant.", "folder": "", "qid": "statistic-mcqm-5277", "question": "In the context of non-stationary processes, which of the following are true about modeling exceedances with the GPD?", "options": ["A) The GPD parameters can be modeled as functions of covariates to account for non-stationarity.", "B) The threshold stability property is retained in non-stationary models by ensuring the scale parameter satisfies a specific functional form.", "C) Different covariates can be included in the scale and shape parameters without affecting the model's consistency.", "D) The log-link formulation for the scale parameter is always consistent with the threshold stability property."], "category": "CORRECT", "question_context_used": "For non-stationary processes, GPD parameters ($\\psi_u(\\mathbf{x})$, $\\xi(\\mathbf{x})$, $\\phi_u(\\mathbf{x})$) can be functions of covariates. Threshold stability requires $\\psi_v(\\mathbf{x}) = \\psi_u(\\mathbf{x}) + (v-u)\\xi(\\mathbf{x})$ for any $v>u$, implying the same covariates must be included in $\\psi_u(\\mathbf{x})$ and $\\xi(\\mathbf{x})$. The log-link formulation may not always satisfy this, especially if $\\xi(\\mathbf{x})$ is not constant.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "To evaluate the statements:\n\n- **Option A** is correct because the context states that $T_{t_{1},t}^{(2)}\\leq_{ST}T_{t}^{*}$ for all $0<t_{1}<t$ and all $F_{1},F_{2}$.\n- **Option B** is correct as it is mentioned that when $X_{1}\\leq_{HR}X_{2}$, then $T_{t_{1},t}^{(2)}\\leq_{HR}T_{t}$.\n- **Option C** is incorrect because the context explicitly states that $T_{t}$ and $T_{t}^{*}$ are not HR-ordered, even if $X_{1}\\leq_{HR}X_{2}$ or $X_{1}\\geq_{HR}X_{2}$ hold.\n- **Option D** is correct as the context indicates that $T_{t_{1},t}^{(2)}=_{ST}T_{t}^{\\{2\\}}$ for all $0<t_{1}<t$ and all $F_{1},F_{2}$.\n\n**Correct Choices**: A, B, D\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Copula-based-representations-for-the-reliability-o.md"], "background_text": "The context discusses a parallel system with two independent exponential components and compares different residual lifetimes of the system under various conditions. It highlights the stochastic and hazard rate orderings between these residual lifetimes, especially when $X_{1}\\leq_{HR}X_{2}$.", "folder": "", "qid": "statistic-mcqm-2325", "question": "Consider a parallel system with two independent components whose lifetimes $X_{1}$ and $X_{2}$ have exponential distributions with means $\\mu_{1}=1$ and $\\mu_{2}=1/2$, respectively. Which of the following statements about the system's residual lifetimes are true?", "options": ["A) $T_{t_{1},t}^{(2)}\\leq_{ST}T_{t}^{*}$ for all $0<t_{1}<t$.", "B) $T_{t_{1},t}^{(2)}\\leq_{HR}T_{t}$ when $X_{1}\\leq_{HR}X_{2}$.", "C) $T_{t}$ and $T_{t}^{*}$ are always HR-ordered.", "D) $T_{t_{1},t}^{(2)}=_{ST}T_{t}^{\\{2\\}}$ for all $0<t_{1}<t$."], "category": "CORRECT", "question_context_used": "The context discusses a parallel system with two independent exponential components and compares different residual lifetimes of the system under various conditions. It highlights the stochastic and hazard rate orderings between these residual lifetimes, especially when $X_{1}\\leq_{HR}X_{2}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A**: Correct. This is the condition for the CLT to hold when the proposal distribution is $t(\\eta)$, $0<\\eta<2$.\n- **B**: Correct. This is the condition for the CLT to hold when the proposal distribution has finite variance.\n- **C**: Incorrect. The condition is more restrictive than $s<r$.\n- **D**: Incorrect. The CLT conditions do not require $\\eta=1$ specifically.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Scandinavian J Statistics - 2007 - JARNER - Conver.md"], "background_text": "For a symmetric random-walk Metropolis algorithm with target distribution $t(r)$ and a proposal distribution with finite variance, the CLT holds for $g(x)=|x|^{s}$ for all $s<r/2-1$. If the proposal distribution is $t(\\eta)$, $0<\\eta<2$, the CLT holds for all $s<(r-\\eta)/2$.", "folder": "", "qid": "statistic-mcqm-5508", "question": "For a symmetric random-walk Metropolis algorithm with target distribution $t(r)$ and proposal distribution $t(\\eta)$, $0<\\eta<2$, the CLT will hold for $g(x)=|x|^{s}$ under which conditions?", "options": ["A) For all $s<(r-\\eta)/2$.", "B) For all $s<r/2-1$ if the proposal distribution has finite variance.", "C) For all $s<r$.", "D) Only if $\\eta=1$."], "category": "CORRECT", "question_context_used": "For a symmetric random-walk Metropolis algorithm with target distribution $t(r)$ and a proposal distribution with finite variance, the CLT holds for $g(x)=|x|^{s}$ for all $s<r/2-1$. If the proposal distribution is $t(\\eta)$, $0<\\eta<2$, the CLT holds for all $s<(r-\\eta)/2$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C, D\n\n- **A)** Incorrect. The denominator is incorrectly formatted; it should be $(\\beta-t)^{\\alpha} - e^{s}\\beta^{\\alpha}(1-p)$.\n- **B)** Correct. The validity condition for the mgf is correctly stated.\n- **C)** Correct. The marginal mgf of $X$ is correctly given.\n- **D)** Correct. The mgf is used to derive the product moments of $(X, N)$.\n\n**Final Answer**: The final answer is $\\boxed{{B, C, D}}$", "source": ["Bivariate-gamma-geometric-law-and-its-induced-_201.md"], "background_text": "The mgf of the BGG law is given by $\\varphi(t,s) = \\frac{p e^{s}\\beta^{\\alpha}}{(\\beta-t)^{\\alpha} - e^{s}\\beta^{\\alpha}(1-p)}$ and is valid for $t < \\beta\\{1 - [(1-p)e^{s}]^{1/\\alpha}\\}$. The marginal mgf of $X$ is $\\varphi(t) = \\frac{p\\beta^{\\alpha}}{(\\beta-t)^{\\alpha} - \\beta^{\\alpha}(1-p)}$. The mgf can be used to derive the product moments of $(X, N)$.", "folder": "", "qid": "statistic-mcqm-3287", "question": "Which of the following are properties of the BGG law's moment generation function (mgf)?", "options": ["A) The mgf is given by $\\varphi(t,s) = \\frac{p e^{s}\\beta^{\\alpha}}{(\\beta-t)^{\\alpha} - e^{s}\\beta^{\\alpha}(1-p)}$.", "B) The mgf is valid for $t < \\beta\\{1 - [(1-p)e^{s}]^{1/\\alpha}\\}$.", "C) The marginal mgf of $X$ is $\\varphi(t) = \\frac{p\\beta^{\\alpha}}{(\\beta-t)^{\\alpha} - \\beta^{\\alpha}(1-p)}$.", "D) The mgf can be used to derive the product moments of $(X, N)$."], "category": "INCORRECT", "question_context_used": "The mgf of the BGG law is given by $\\varphi(t,s) = \\frac{p e^{s}\\beta^{\\alpha}}{(\\beta-t)^{\\alpha} - e^{s}\\beta^{\\alpha}(1-p)}$ and is valid for $t < \\beta\\{1 - [(1-p)e^{s}]^{1/\\alpha}\\}$. The marginal mgf of $X$ is $\\varphi(t) = \\frac{p\\beta^{\\alpha}}{(\\beta-t)^{\\alpha} - \\beta^{\\alpha}(1-p)}$. The mgf can be used to derive the product moments of $(X, N)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **A) Correct**: The central DRS is indeed spanned by the e.d.r. directions $\\beta_i$.\n- **B) Correct**: The central DRS is unique, despite the non-uniqueness of the set of $\\beta_i$.\n- **C) Correct**: The CANCOR method estimates the central DRS by performing sequential chi-square tests to determine the number of nonzero canonical correlations.\n- **D) Incorrect**: The central DRS does not require the specification of the function $f$ in the model, as $f$ is unknown and not specified.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["Robust-dimension-reduction-based-on-canonical_2009.md"], "background_text": "The central dimension reduction subspace (DRS) is $S_{y|x}=\\mathsf{span}\\{\\beta_{1},\\beta_{2},\\ldots,\\beta_{k}\\}$, where $\\beta_i$ are the e.d.r. directions. It is unique, and its estimation involves determining the dimensionality $k$ and the e.d.r. directions, without specifying the function $f$.", "folder": "", "qid": "statistic-mcqm-5009", "question": "In the context of the CANCOR method, which of the following are correct about the central dimension reduction subspace (DRS)?", "options": ["A) It is spanned by the effective dimension reduction (e.d.r.) directions.", "B) It is unique even though the set of $\\beta_i$ is not unique.", "C) It is estimated by performing a set of sequential chi-square tests.", "D) It requires the specification of the function $f$ in the model."], "category": "CORRECT", "question_context_used": "The central dimension reduction subspace (DRS) is $S_{y|x}=\\mathsf{span}\\{\\beta_{1},\\beta_{2},\\ldots,\\beta_{k}\\}$, where $\\beta_i$ are the e.d.r. directions. It is unique, and its estimation involves determining the dimensionality $k$ and the e.d.r. directions, without specifying the function $f$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The text states that under the null hypothesis, $\\hat{B}$ and $S$ are sufficient for the nuisance parameters $(B,\\Sigma)$.\n- **B)** Correct. The text mentions that these statistics are (boundedly) complete by Theorem 1, p. 144 of [7].\n- **C)** Incorrect. The text does not mention that $\\hat{B}$ and $S$ are invariant under the group of transformations $G$.\n- **D)** Correct. The text explicitly states that under the null hypothesis, the matrix $V$ (which is derived from $W=CY$ and related to $S$) is uniformly distributed over the Stiefel manifold $V^{\\prime}V=I_{m}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Similar-tests-for-covariance-structures-in-multi_2.md"], "background_text": "Under the null hypothesis, the statistics $\\hat{B}=(X^{\\prime}X)^{-1}X^{\\prime}Y$ and $S=Y^{\\prime}M_{X}Y$ are sufficient for the nuisance parameters $(B,\\Sigma)$ and are (boundedly) complete. The matrix $V$ is uniformly distributed over the Stiefel manifold $V^{\\prime}V=I_{m}$ under the null hypothesis.", "folder": "", "qid": "statistic-mcqm-5573", "question": "Under the null hypothesis, which of the following statements are true about the statistics $\\hat{B}=(X^{\\prime}X)^{-1}X^{\\prime}Y$ and $S=Y^{\\prime}M_{X}Y$?", "options": ["A) They are sufficient for the nuisance parameters $(B,\\Sigma)$.", "B) They are (boundedly) complete.", "C) They are invariant under the group of transformations $G$.", "D) They are uniformly distributed over the Stiefel manifold $V^{\\prime}V=I_{m}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "Under the null hypothesis, the statistics $\\hat{B}=(X^{\\prime}X)^{-1}X^{\\prime}Y$ and $S=Y^{\\prime}M_{X}Y$ are sufficient for the nuisance parameters $(B,\\Sigma)$ and are (boundedly) complete. The matrix $V$ is uniformly distributed over the Stiefel manifold $V^{\\prime}V=I_{m}$ under the null hypothesis.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The $q$-power distribution fit has a log-likelihood of $-700.89$ as stated in the text.\n- **B)** Correct. The beta-binomial fit has a log-likelihood of $-736.22$ as mentioned.\n- **C)** Correct. The folded logistic fit has a log-likelihood of $-785.76$ as provided.\n- **D)** Correct. The reduced model with seven parameters fewer has a log-likelihood of $-702.75$ as indicated.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["A Litter-Based Approach to Risk Assessment in Deve.md"], "background_text": "The log-likelihood values for the $q$-power distribution fit, beta-binomial fit, and folded logistic fit are $-700.89$, $-736.22$, and $-785.76$ respectively. A reduced model with seven parameters fewer than the separate fit model has a log-likelihood of $-702.75$.", "folder": "", "qid": "statistic-mcqm-5450", "question": "Based on the analysis of the 2,4,5-T data, which of the following statements are correct regarding the log-likelihood values of the fitted models?", "options": ["A) The $q$-power distribution fit has a log-likelihood of $-700.89$.", "B) The beta-binomial fit has a log-likelihood of $-736.22$.", "C) The folded logistic fit has a log-likelihood of $-785.76$.", "D) The reduced model with seven parameters fewer has a log-likelihood of $-702.75$."], "category": "CORRECT", "question_context_used": "The log-likelihood values for the $q$-power distribution fit, beta-binomial fit, and folded logistic fit are $-700.89$, $-736.22$, and $-785.76$ respectively. A reduced model with seven parameters fewer than the separate fit model has a log-likelihood of $-702.75$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The estimator $\\hat{\\boldsymbol{p}}_i$ is unbiased for $p_i$, as stated in Theorem 1.\n- **B)** Correct. The variance of $\\hat{\\boldsymbol{p}}_i$ is inversely proportional to the sample size $n$, as shown in the variance formula $\\mathrm{Var}[\\hat{p}_i] = \\frac{1}{n}\\left(\\frac{1+(N-3)p_i}{(N-2)} - p_i^2\\right)$.\n- **C)** Incorrect. The covariance between $\\hat{\\boldsymbol{p}}_i$ and $\\hat{\\boldsymbol{p}}_j$ is negative for $i \\neq j$, as indicated by the formula $\\mathrm{Cov}[\\hat{p}_i, \\hat{p}_j] = -\\frac{1}{n}\\left(\\frac{1-p_i-p_j}{(N-2)^2} + p_i p_j\\right)$.\n- **D)** Correct. The matrix $A$ is defined as $\\frac{1}{N-1}B^{\\prime}$ and is used in the derivation of the estimator $\\hat{\\boldsymbol{p}}_i$, as mentioned in the context.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$.", "source": ["Scandinavian J Statistics - 2019 - Lagerås - How t.md"], "background_text": "The estimator $\\hat{\\boldsymbol{p}}_i$ is given by $\\frac{N-1}{N-2}\\boldsymbol{b}_i^{\\prime}\\hat{\\boldsymbol{u}} - \\frac{1}{N-2}$, where $\\hat{\\boldsymbol{u}}$ is the ML estimator for $\\boldsymbol{u}$. The variance and covariance of $\\hat{\\boldsymbol{p}}_i$ are provided, showing that the estimator is unbiased and its variance decreases with increasing sample size. The matrix $A$ is used in the derivation of the estimator.", "folder": "", "qid": "statistic-mcqm-5875", "question": "Given a voting scenario with $N$ parties and a single sensitive party, the probability of receiving the answer $\\{i,j\\}$ is $u_{ij} = \\frac{1}{N-1}(p_i + p_j)$. Which of the following statements are correct about the estimator $\\hat{\\boldsymbol{p}}_i$?", "options": ["A) The estimator $\\hat{\\boldsymbol{p}}_i$ is unbiased for $p_i$.", "B) The variance of $\\hat{\\boldsymbol{p}}_i$ decreases as the sample size $n$ increases.", "C) The covariance between $\\hat{\\boldsymbol{p}}_i$ and $\\hat{\\boldsymbol{p}}_j$ is positive for $i \\neq j$.", "D) The estimator $\\hat{\\boldsymbol{p}}_i$ can be derived using the matrix $A$ defined as $\\frac{1}{N-1}B^{\\prime}$."], "category": "CORRECT", "question_context_used": "The estimator $\\hat{\\boldsymbol{p}}_i$ is given by $\\frac{N-1}{N-2}\\boldsymbol{b}_i^{\\prime}\\hat{\\boldsymbol{u}} - \\frac{1}{N-2}$, where $\\hat{\\boldsymbol{u}}$ is the ML estimator for $\\boldsymbol{u}$. The variance and covariance of $\\hat{\\boldsymbol{p}}_i$ are provided, showing that the estimator is unbiased and its variance decreases with increasing sample size. The matrix $A$ is used in the derivation of the estimator.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **Option A** is correct because Assumption 3 directly implies $E[Y_{\\alpha}^{*2}]<\\infty$ for any $\\alpha\\in\\mathbb{R}$.\n- **Option B** is correct because Assumption 3 is explicitly stated to be equivalent to $E[Y_{\\mathrm{KSV}}^{*2}]<\\infty$.\n- **Option C** is correct because Assumption 3 suffices to prove consistency for $\\hat{\\theta}_{\\alpha}$ even with the replacement mentioned.\n- **Option D** is incorrect because the identification of $\\theta_{0}$ is guaranteed by Assumption 1(i), not Assumption 3.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Scandinavian J Statistics - 2008 - DELECROIX - Non.md"], "background_text": "Assumption 3 states that $\\int_{0}^{\\tau_{H}}y^{2}\\left\\{1-G(y)\\right\\}^{-1}\\mathrm{d}F(y)<\\infty$. This assumption implies $E[Y_{\\alpha}^{*2}]<\\infty$ for any $\\alpha\\in\\mathbb{R}$ and is equivalent to $E[Y_{\\mathrm{KSV}}^{*2}]<\\infty$. It suffices to prove consistency for $\\hat{\\theta}_{\\alpha}$ even if $\\boldsymbol{Y}_{\\boldsymbol{\\alpha}}^{*}$ is replaced by $\\hat{Y}_{\\alpha}^{*}$ in the definition of the nonlinear least squares estimator.", "folder": "", "qid": "statistic-mcqm-5204", "question": "Which of the following statements are true regarding Assumption 3?", "options": ["A) It implies $E[Y_{\\alpha}^{*2}]<\\infty$ for any $\\alpha\\in\\mathbb{R}$.", "B) It is equivalent to $E[Y_{\\mathrm{KSV}}^{*2}]<\\infty$.", "C) It ensures the consistency of $\\hat{\\theta}_{\\alpha}$ even if $\\boldsymbol{Y}_{\\boldsymbol{\\alpha}}^{*}$ is replaced by $\\hat{Y}_{\\alpha}^{*}$.", "D) It guarantees the identification of $\\theta_{0}$."], "category": "CORRECT", "question_context_used": "Assumption 3 states that $\\int_{0}^{\\tau_{H}}y^{2}\\left\\{1-G(y)\\right\\}^{-1}\\mathrm{d}F(y)<\\infty$. This assumption implies $E[Y_{\\alpha}^{*2}]<\\infty$ for any $\\alpha\\in\\mathbb{R}$ and is equivalent to $E[Y_{\\mathrm{KSV}}^{*2}]<\\infty$. It suffices to prove consistency for $\\hat{\\theta}_{\\alpha}$ even if $\\boldsymbol{Y}_{\\boldsymbol{\\alpha}}^{*}$ is replaced by $\\hat{Y}_{\\alpha}^{*}$ in the definition of the nonlinear least squares estimator.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: C, D\n\n- **Option A**: Incorrect. The location parameter $\\xi$ is not necessarily equal to the expectation $\\mu$; they are related but not identical.\n- **Option B**: Incorrect. The scale parameter $\\psi$ is not equal to the standard deviation $\\sigma$; $\\sigma$ is a function of $\\psi$ and $\\lambda$.\n- **Option C**: Correct. The shape parameter $\\lambda$ directly influences the skewness $\\gamma_1$ of the distribution.\n- **Option D**: Correct. While $\\xi$, $\\psi$, and $\\lambda$ influence the kurtosis $\\beta_2$, they are not equal to it, and their relationship is more complex.\n\n**Final Answer**: The final answer is $\\boxed{{C, D}}$", "source": ["Skewness-based-projection-pursuit--A-compu_2018_Co.md"], "background_text": "The skew-normal distribution is defined with location parameter $\\xi$, scale parameter $\\psi$, and shape parameter $\\lambda$. The expectation $\\mu$, standard deviation $\\sigma$, skewness $\\gamma_1$, and kurtosis $\\beta_2$ are functions of $\\xi$, $\\psi$, and $\\lambda$, but these parameters do not generally equal $\\mu$, $\\sigma$, $\\gamma_1$, or $\\beta_2$.", "folder": "", "qid": "statistic-mcqm-5221", "question": "Given a univariate skew-normal distribution $X \\sim SN(\\xi, \\psi, \\lambda)$, which of the following statements are true about its parameters?", "options": ["A) The location parameter $\\xi$ is equal to the expectation $\\mu$ of the distribution.", "B) The scale parameter $\\psi$ is equal to the standard deviation $\\sigma$ of the distribution.", "C) The shape parameter $\\lambda$ influences the skewness $\\gamma_1$ of the distribution.", "D) The parameters $\\xi$, $\\psi$, and $\\lambda$ are independent of the kurtosis $\\beta_2$ of the distribution."], "category": "PARTIALLY_CORRECT", "question_context_used": "The skew-normal distribution is defined with location parameter $\\xi$, scale parameter $\\psi$, and shape parameter $\\lambda$. The expectation $\\mu$, standard deviation $\\sigma$, skewness $\\gamma_1$, and kurtosis $\\beta_2$ are functions of $\\xi$, $\\psi$, and $\\lambda$, but these parameters do not generally equal $\\mu$, $\\sigma$, $\\gamma_1$, or $\\beta_2$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"C\"]\n}", "model_answer": ["C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. $\\tau$-estimators are known for their high-breakdown point of 50%.\n- **B)** Correct. They can be tuned for high efficiency under normal error conditions.\n- **C)** Incorrect. The text does not suggest that $\\tau$-estimators become less robust than M-estimators as the number of variables increases; in fact, M-estimators have a breakdown point less than $1/p$.\n- **D)** Correct. $\\tau$-estimators come with a robust and efficient scale estimator.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Robust-tests-for-linear-regression-models-b_2016_C.md"], "background_text": "$\\tau$-estimators are a class of high-breakdown and highly efficient regression estimators that are naturally accompanied by an associated estimator of the error scale which is also highly robust and highly efficient. These estimators can be tuned to simultaneously have a high-breakdown point (50%) and achieve high-efficiency (e.g. 85% or 95%) at the central model with normal errors.", "folder": "", "qid": "statistic-mcqm-5639", "question": "Which of the following are properties of $\\tau$-estimators?", "options": ["A) They have a high-breakdown point of 50%.", "B) They can achieve high efficiency (e.g., 85% or 95%) at the central model with normal errors.", "C) They are less robust than M-estimators when the number of explanatory variables increases.", "D) They are naturally accompanied by an associated estimator of the error scale which is highly robust and efficient."], "category": "CORRECT", "question_context_used": "$\\tau$-estimators are a class of high-breakdown and highly efficient regression estimators that are naturally accompanied by an associated estimator of the error scale which is also highly robust and highly efficient. These estimators can be tuned to simultaneously have a high-breakdown point (50%) and achieve high-efficiency (e.g. 85% or 95%) at the central model with normal errors.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct, the text describes $\\hat{\\lambda} = U/V$ with $U$ and $V$ as defined.\n- **B)** Correct, the variance and expectation of $\\mathbf{N}(B)$ in a Poisson process are given as $\\lambda \\mathbf{vol}(B)$.\n- **C)** Correct, the ratio estimator is unbiased and consistent under the assumption of spatial independence, as implied by the context.\n- **D)** Incorrect, the text states that the effective range of spatial dependence must be small, not large, compared to the sampling region for central limit theorems to apply.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Analysis of a Three-Dimensional Point Pattern with.md"], "background_text": "The estimator $\\hat{\\lambda} = U/V$ is discussed where $U = \\mathbf{N}(B)$ and $V = \\mathbf{vol}(B)$. For a Poisson point process, the variance and expectation of $\\mathbf{N}(B)$ are equal to $\\lambda \\mathbf{vol}(B)$. The text mentions that for more general processes, if the effective range of spatial dependence is small compared to the sampling region, central limit theorems can be appealed to.", "folder": "", "qid": "statistic-mcqm-5619", "question": "In the context of estimating $\\lambda$ in a Poisson point process, which of the following statements are correct?", "options": ["A) The estimator $\\hat{\\lambda} = U/V$ where $U = \\mathbf{N}(B)$ and $V = \\mathbf{vol}(B)$ is considered.", "B) For a Poisson point process, $\\text{var}\\{\\mathbf{N}(B)\\} = \\mathbf{E}\\mathbf{N}(B) = \\lambda \\mathbf{vol}(B)$.", "C) The ratio estimator $\\hat{\\lambda}$ is unbiased and consistent under the assumption of spatial independence.", "D) The effective range of spatial dependence must be large compared to the size of the sampling region for central limit theorems to apply."], "category": "CORRECT", "question_context_used": "The estimator $\\hat{\\lambda} = U/V$ is discussed where $U = \\mathbf{N}(B)$ and $V = \\mathbf{vol}(B)$. For a Poisson point process, the variance and expectation of $\\mathbf{N}(B)$ are equal to $\\lambda \\mathbf{vol}(B)$. The text mentions that for more general processes, if the effective range of spatial dependence is small compared to the sampling region, central limit theorems can be appealed to.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A)** Correct. From the text, $F^{(1)}(x) = F(x)G^{(1)}(x)$ is negative for $x > 0$ because $F(x) > 0$ and $G^{(1)}(x) = \\gamma x^{\\gamma-1}\\log(p) < 0$ (since $0 < p, \\gamma < 1$ and $\\log(p) < 0$).\n- **B)** Correct. The text provides the expression for $F^{(k)}(x)$ as a sum involving $F^{(r)}(x)$ and $G^{(k-r)}(x)$ for $r = 0, \\ldots, k-1$ with positive constants $a_{k r}$.\n- **C)** Correct. By mathematical induction, it is shown that $(-1)^{k}F^{(k)}(x) \\geqslant 0$ for all $k$ and $x > 0$, proving $F(x)$ is completely monotone.\n- **D)** Incorrect. $G^{(k)}(x)$ is not always positive; its sign depends on $k$ and the term $(-1)^{k}$ as shown in the text.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["A Litter-Based Approach to Risk Assessment in Deve.md"], "background_text": "The function $F(x) = p^{x^{\\gamma}}$ is analyzed for its completely monotone property. It is given that $0 < p, \\gamma < 1$. The derivatives of $F(x)$ and $G(x) = \\log{F(x)} = x^{\\gamma}\\log(p)$ are discussed, with $F^{(k)}(x)$ expressed in terms of $F^{(r)}(x)$ and $G^{(k-r)}(x)$. The sign of the derivatives is determined by induction, showing $(-1)^{k}F^{(k)}(x) \\geqslant 0$.", "folder": "", "qid": "statistic-mcqm-5456", "question": "Given the function $F(x) = p^{x^{\\gamma}}$ where $0 < p, \\gamma < 1$, which of the following statements are true about its derivatives?", "options": ["A) $F^{(1)}(x) = F(x)G^{(1)}(x)$ is negative for $x > 0$", "B) $F^{(k)}(x)$ can be expressed as a sum involving $F^{(r)}(x)$ and $G^{(k-r)}(x)$ for $r = 0, \\ldots, k-1$", "C) $(-1)^{k}F^{(k)}(x) \\geqslant 0$ for all $k$ and $x > 0$", "D) $G^{(k)}(x) = \\gamma(\\gamma-1)...(\\gamma-k+1)x^{\\gamma-k}\\log(p)$ is always positive for $x > 0$"], "category": "CORRECT", "question_context_used": "The function $F(x) = p^{x^{\\gamma}}$ is analyzed for its completely monotone property. It is given that $0 < p, \\gamma < 1$. The derivatives of $F(x)$ and $G(x) = \\log{F(x)} = x^{\\gamma}\\log(p)$ are discussed, with $F^{(k)}(x)$ expressed in terms of $F^{(r)}(x)$ and $G^{(k-r)}(x)$. The sign of the derivatives is determined by induction, showing $(-1)^{k}F^{(k)}(x) \\geqslant 0$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**: \n- **A** is correct as per LEMMA 3.1, which states the boundedness condition for $m_{F}(z)$.\n- **B** is incorrect because the text provides conditions under which $m_{F}(z)$ is bounded.\n- **C** is correct because the text explicitly states that $m_{F}$ is analytic on $\\mathbb{C}-S_{F}$.\n- **D** is correct as the text discusses the convergence of $\\mathrm{Im} m_{F}(z)$ to 0 under certain conditions.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Analysis-of-the-Limiting-Spectral-Distribution-of-.md"], "background_text": "LEMMA 3.1 states that $m_{2}(x+i y)$ is bounded for $x+i y$ lying in any bounded region of $D$ away from the imaginary axis. The text also mentions that $m_{F}$ is analytic on $\\mathbb{C}-S_{F}$ and discusses the behavior of $\\mathrm{Im} m_{F}(z)$ as $z$ approaches real points.", "folder": "", "qid": "statistic-mcqm-2987", "question": "Which of the following statements about the function $m_{F}(z)$ are supported by the text?", "options": ["A) $m_{F}(z)$ is bounded for $z$ in any bounded region of $D$ away from the imaginary axis.", "B) $m_{F}(z)$ is unbounded for all $z \\in D$.", "C) $m_{F}(z)$ is analytic on $\\mathbb{C}-S_{F}$.", "D) The imaginary part of $m_{F}(z)$ converges to 0 as $z$ approaches any real $x_{0} \\neq 0$."], "category": "PARTIALLY_CORRECT", "question_context_used": "LEMMA 3.1 states that $m_{2}(x+i y)$ is bounded for $x+i y$ lying in any bounded region of $D$ away from the imaginary axis. The text also mentions that $m_{F}$ is analytic on $\\mathbb{C}-S_{F}$ and discusses the behavior of $\\mathrm{Im} m_{F}(z)$ as $z$ approaches real points.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**: \n- **Option A** is correct as the correlation structure is designed to represent decreasing correlations with increasing distance between parameter sets.\n- **Option B** is correct because the diagonal elements are explicitly given by $\\sigma_{1}^{2}, \\ldots, \\sigma_{m}^{2}$.\n- **Option C** is correct as the structure assumes no correlation between different output components at the same or different parameter values.\n- **Option D** is correct because the smoothness parameters $\\beta$ are the same for all output components.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Mechanism-based-emulation-of-dynamic-simulation-m_.md"], "background_text": "The correlation structure $\\mathbf{K}_{(m)}(\\theta_{M}^{i},\\theta_{M}^{j})$ is defined with diagonal elements $\\sigma_{1}^{2}, \\ldots, \\sigma_{m}^{2}$ and assumes decreasing correlations with increasing distance between parameter sets. It also assumes the same smoothness parameters $\\beta$ for all output components and no correlation between different output components.", "folder": "", "qid": "statistic-mcqm-5657", "question": "Which of the following are correct about the correlation structure $\\mathbf{K}_{(m)}(\\theta_{M}^{i},\\theta_{M}^{j})$ in Gaussian process emulation?", "options": ["A) The correlation decreases as the distance between parameter sets $\\theta_{M}^{i}$ and $\\theta_{M}^{j}$ increases.", "B) The diagonal elements of $\\mathbf{K}_{(m)}(\\theta_{M}^{i},\\theta_{M}^{j})$ are given by $\\sigma_{1}^{2}, \\ldots, \\sigma_{m}^{2}$.", "C) The correlation structure assumes no correlation between different output components at the same parameter values.", "D) The smoothness parameters $\\beta$ are the same for all output components."], "category": "CORRECT", "question_context_used": "The correlation structure $\\mathbf{K}_{(m)}(\\theta_{M}^{i},\\theta_{M}^{j})$ is defined with diagonal elements $\\sigma_{1}^{2}, \\ldots, \\sigma_{m}^{2}$ and assumes decreasing correlations with increasing distance between parameter sets. It also assumes the same smoothness parameters $\\beta$ for all output components and no correlation between different output components.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **Option A** is correct because the text explicitly states that it is impossible to identify $\\vartheta$ asymptotically under the given conditions.\n- **Option B** is correct as it directly quotes the text regarding the law of $X$ being the same under the specified choices of parameters.\n- **Option C** is incorrect because the text clearly states the impossibility of estimating $\\vartheta$ without knowledge of $\\sigma_{t}$ and $\\overline{{\\sigma}}_{t}$.\n- **Option D** is incorrect because while the convergence results do depend on the regularity of the processes, the impossibility of identifying $\\vartheta$ is the main point, not the convergence results.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Scandinavian J Statistics - 2019 - Féron - Efficie.md"], "background_text": "In the setting described, it is impossible to identify $\\vartheta$ from data $X_{i\\Delta_{n}},i=1,\\dots,n$ asymptotically when $t\\sim\\sigma_{t}$ and $t\\{\\sim\\overline{{\\sigma}}_{t}$ are unknown. Indeed, $X$ has the same law under the choice of $(\\vartheta,\\sigma,\\overline{{\\sigma}})$ and $(\\vartheta+1,e^{T_{1}-\\cdot}\\sigma,\\overline{{\\sigma}})$.", "folder": "", "qid": "statistic-mcqm-5801", "question": "In the context of estimating $\\vartheta$ from data $X_{i\\Delta_{n}},i=1,\\dots,n$ asymptotically, which of the following statements are correct when $t\\sim\\sigma_{t}$ and $t\\{\\sim\\overline{{\\sigma}}_{t}$ are unknown?", "options": ["A) It is impossible to identify $\\vartheta$ asymptotically under the given conditions.", "B) The law of $X$ is the same under the choice of $(\\vartheta,\\sigma,\\overline{{\\sigma}})$ and $(\\vartheta+1,e^{T_{1}-\\cdot}\\sigma,\\overline{{\\sigma}})$.", "C) The estimation of $\\vartheta$ is always possible regardless of the knowledge of $\\sigma_{t}$ and $\\overline{{\\sigma}}_{t}$.", "D) The convergence results depend solely on the regularity of the processes $b,\\sigma$ and $\\overline{{\\sigma}}$."], "category": "CORRECT", "question_context_used": "In the setting described, it is impossible to identify $\\vartheta$ from data $X_{i\\Delta_{n}},i=1,\\dots,n$ asymptotically when $t\\sim\\sigma_{t}$ and $t\\{\\sim\\overline{{\\sigma}}_{t}$ are unknown. Indeed, $X$ has the same law under the choice of $(\\vartheta,\\sigma,\\overline{{\\sigma}})$ and $(\\vartheta+1,e^{T_{1}-\\cdot}\\sigma,\\overline{{\\sigma}})$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "To determine the correct options, we analyze each condition in the context of sufficient cause interaction under unconfounded effects and positive monotonicity.\n\n- **Option A**: The condition $p_{11} - p_{10} - p_{01} + p_{00} > 0$ is the standard condition for positive interaction on the linear additive scale. Under positive monotonicity, this condition suffices to imply a sufficient cause interaction. **This is correct.**\n\n- **Option B**: The condition $p_{11} - p_{10} - p_{01} > 0$ is a stronger condition that implies a sufficient cause interaction even without monotonicity. **This is correct.**\n\n- **Option C**: The condition $p_{11} - p_{10} - p_{01} - p_{00} > 0$ implies compositional epistasis, which is a stronger notion than sufficient cause interaction. **This is incorrect** for the question as it asks about sufficient cause interaction specifically.\n\n- **Option D**: The condition $p_{111} - p_{110} - p_{101} - p_{011} > 0$ pertains to three-way interactions, not two-way. **This is incorrect** for the question.\n\n**Correct Choices**: A, B\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["Inference_for_additive_interaction_under_exposure_misclassification.md"], "background_text": "The standard condition for positive interaction on the linear additive scale is $p_{11} - p_{10} - p_{01} + p_{00} > 0$. Under unconfounded effects and positive monotonicity, this condition suffices to draw the conclusion of a sufficient cause interaction. A stronger notion of interaction is given by $p_{11} - p_{10} - p_{01} > 0$, which implies the presence of a sufficient cause interaction without monotonicity. An even stronger notion is $p_{11} - p_{10} - p_{01} - p_{00} > 0$, which implies compositional epistasis. The condition $p_{111} - p_{110} - p_{101} - p_{011} > 0$ is relevant for three-way sufficient cause interactions.", "folder": "", "qid": "statistic-mcqm-6156", "question": "Which of the following conditions imply the presence of a sufficient cause interaction between two binary exposures $X_{1}$ and $X_{2}$ under the assumption of unconfounded effects and positive monotonicity?", "options": ["A) $p_{11} - p_{10} - p_{01} + p_{00} > 0$", "B) $p_{11} - p_{10} - p_{01} > 0$", "C) $p_{11} - p_{10} - p_{01} - p_{00} > 0$", "D) $p_{111} - p_{110} - p_{101} - p_{011} > 0$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The standard condition for positive interaction on the linear additive scale is $p_{11} - p_{10} - p_{01} + p_{00} > 0$. Under unconfounded effects and positive monotonicity, this condition suffices to draw the conclusion of a sufficient cause interaction. A stronger notion of interaction is given by $p_{11} - p_{10} - p_{01} > 0$, which implies the presence of a sufficient cause interaction without monotonicity. An even stronger notion is $p_{11} - p_{10} - p_{01} - p_{00} > 0$, which implies compositional epistasis. The condition $p_{111} - p_{110} - p_{101} - p_{011} > 0$ is relevant for three-way sufficient cause interactions.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The text states that incorrectly assuming monotonic effects can yield a sizable inflation of the type I error.\n- **B) Correct**: The OLS analysis can be severely biased under model misspecification, as mentioned in the text.\n- **C) Incorrect**: The conditional estimator has the smallest standard errors, not the largest.\n- **D) Correct**: Both tests for sufficient cause interaction allowing for non-monotonic effects perform well, according to the text.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Semiparametric Tests for Sufficient Cause Interact.md"], "background_text": "In simulations with non-monotonic effects, incorrectly assuming monotonic effects can inflate the type I error. The OLS analysis can be biased under misspecification. The conditional estimator for non-monotonic effects has the smallest standard errors, and both proposed tests perform well.", "folder": "", "qid": "statistic-mcqm-6109", "question": "In the final series of simulation experiments with non-monotonic effects, which of the following statements are true about the tests for sufficient cause interaction?", "options": ["A) Incorrectly assuming monotonic effects can lead to a sizable inflation of the type I error.", "B) The ordinary least squares analysis can be severely biased under model misspecification.", "C) The conditional estimator for non-monotonic effects has the largest standard errors.", "D) Both proposed tests for sufficient cause interaction which allow for non-monotonic effects perform well."], "category": "CORRECT", "question_context_used": "In simulations with non-monotonic effects, incorrectly assuming monotonic effects can inflate the type I error. The OLS analysis can be biased under misspecification. The conditional estimator for non-monotonic effects has the smallest standard errors, and both proposed tests perform well.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The distortion $D(P,q)$ is indeed defined as the expected squared Euclidean distance between $(T,X)$ and its quantized version $q(T,X)$.\n- **B)** Correct. $D_{k}^{*}(P)$ represents the minimal distortion achievable by any $k$-point quantizer.\n- **C)** Incorrect. An optimal quantizer $q^{*}$ must satisfy $D(P,q^{*})=D_{k}^{*}(P)$, not $D(P,q^{*})>D_{k}^{*}(P)$.\n- **D)** Correct. It has been shown that an optimal quantizer always exists and is a nearest neighbor quantizer.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Vector-quantization-and-clustering-in-the-prese_20.md"], "background_text": "The distortion (error) of a $k$-point quantizer $q$ representing $(T,X)$ by $q(T,X)$ is defined as $D(P,q)=\\mathbb{E}_{P}\\|(T,X)-q(T,X)\\|^{2}$. The optimal performance over the class of $k$-point quantizers is $D_{k}^{*}(P)=\\operatorname*{inf}_{q\\in Q_{k}}D(P,q)$. A quantizer $q^{*}$ is optimal if $D(P,q^{*})=D_{k}^{*}(P)$, and such a quantizer always exists and is a nearest neighbor quantizer.", "folder": "", "qid": "statistic-mcqm-5953", "question": "Which of the following statements about the distortion $D(P,q)$ of a $k$-point quantizer $q$ are correct?", "options": ["A) $D(P,q)$ is defined as $\\mathbb{E}_{P}\\|(T,X)-q(T,X)\\|^{2}$.", "B) The optimal performance over the class of $k$-point quantizers is given by $D_{k}^{*}(P)=\\operatorname*{inf}_{q\\in Q_{k}}D(P,q)$.", "C) A quantizer $q^{*}$ is called optimal if $D(P,q^{*})>D_{k}^{*}(P)$.", "D) The optimal quantizer $q^{*}$ always exists and belongs to the class of nearest neighbor quantizers."], "category": "CORRECT", "question_context_used": "The distortion (error) of a $k$-point quantizer $q$ representing $(T,X)$ by $q(T,X)$ is defined as $D(P,q)=\\mathbb{E}_{P}\\|(T,X)-q(T,X)\\|^{2}$. The optimal performance over the class of $k$-point quantizers is $D_{k}^{*}(P)=\\operatorname*{inf}_{q\\in Q_{k}}D(P,q)$. A quantizer $q^{*}$ is optimal if $D(P,q^{*})=D_{k}^{*}(P)$, and such a quantizer always exists and is a nearest neighbor quantizer.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. $L_{n}$ is initially defined based on the original sample $(y_{i}, \\mathbf{x}_{i})$ for testing $H_{0}$.\n- **B)** Correct. Due to the affine equivariance of $\\tau$-estimates, $L_{n}$ is equivalent when computed on the transformed sample $(y_{i}, \\mathbf{z}_{i})$ for $H_{0}^{*}$.\n- **C)** Incorrect. The equivalence allows us to compute $L_{n}$ using the transformed model only, not requiring separate computations for both models.\n- **D)** Correct. The affine equivariance property ensures that $L_{n}$ is invariant under affine transformations.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Robust-tests-for-linear-regression-models-b_2016_C.md"], "background_text": "The test statistic $L_{n}$ based on the sample $(y_{i}, \\mathbf{x}_{i})$ for the hypothesis $H_{0}$ coincides with the statistic $L_{n}$ based on the transformed sample $(y_{i}, \\mathbf{z}_{i})$ for the hypothesis $H_{0}^{*}$. This equivalence is due to the affine equivariance of the $\\tau$-estimates, allowing us to work with the transformed model without loss of generality.", "folder": "", "qid": "statistic-mcqm-5716", "question": "Consider the null hypothesis $H_{0}^{*}: \\gamma_{0}^{(2)} = \\mathbf{0}$ in the transformed model $y_{i} = \\gamma_{0}^{\\prime} \\mathbf{z}_{i} + \\epsilon_{i}$. Which of the following statements are true about the test statistic $L_{n}$?", "options": ["A) $L_{n}$ is based on the sample $(y_{i}, \\mathbf{x}_{i})$ for the hypothesis $H_{0}$.", "B) $L_{n}$ coincides with the statistic based on the transformed sample $(y_{i}, \\mathbf{z}_{i})$ for the hypothesis $H_{0}^{*}$.", "C) $L_{n}$ requires the computation of $\\tau$-estimates for both the original and transformed models.", "D) $L_{n}$ is invariant under affine transformations of the data."], "category": "CORRECT", "question_context_used": "The test statistic $L_{n}$ based on the sample $(y_{i}, \\mathbf{x}_{i})$ for the hypothesis $H_{0}$ coincides with the statistic $L_{n}$ based on the transformed sample $(y_{i}, \\mathbf{z}_{i})$ for the hypothesis $H_{0}^{*}$. This equivalence is due to the affine equivariance of the $\\tau$-estimates, allowing us to work with the transformed model without loss of generality.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}", "model_answer": ["B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The theorem specifies that $S^{\\prime}(k)$ and $S^{\\prime}(k+s)$ must be linked groups.\n- **B)** Correct. The number of segments in $S^{\\prime}(k+s)$ must be larger than in $S^{\\prime}(k)$ for the theorem to apply.\n- **C)** Correct. The proof discusses the scenario where a segment in $S^{\\prime}(k+s)$ overlaps with more than two segments in $S^{\\prime}(k)$, which is part of the conditions leading to the conclusion.\n- **D)** Incorrect. The inequality $f(\\bigcup B) \\leqslant f(\\bigcup A)$ is considered in the proof but is not a condition for the theorem's conclusion about the absence of a double $\\pmb{B}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Maximum_likelihood_estimation_of_multiple_change_points.md"], "background_text": "Theorem 5 states that if $S^{\\prime}(k)$ and $S^{\\prime}(k+s)$ are linked groups and the number of segments in $S^{\\prime}(k+s)$ is larger than that in $S^{\\prime}(k)$, then the $\\pmb{A}$ and $\\pmb{B}$ sequence will not have a double $\\pmb{B}$. The proof involves the condition where a segment in $S^{\\prime}(k+s)$ overlaps with more than two segments in $S^{\\prime}(k)$ and the inequality $f(\\bigcup B) \\leqslant f(\\bigcup A)$.", "folder": "", "qid": "statistic-mcqm-5142", "question": "According to Theorem 5, which of the following conditions prevent the $\\pmb{A}$ and $\\pmb{B}$ sequence from having a double $\\pmb{B}$?", "options": ["A) $S^{\\prime}(k)$ and $S^{\\prime}(k+s)$ are linked groups.", "B) The number of segments in $S^{\\prime}(k+s)$ is larger than that in $S^{\\prime}(k)$.", "C) There is one segment in $S^{\\prime}(k+s)$ that overlaps with more than two segments in $S^{\\prime}(k)$.", "D) The inequality $f(\\bigcup B) \\leqslant f(\\bigcup A)$ holds in the block."], "category": "PARTIALLY_CORRECT", "question_context_used": "Theorem 5 states that if $S^{\\prime}(k)$ and $S^{\\prime}(k+s)$ are linked groups and the number of segments in $S^{\\prime}(k+s)$ is larger than that in $S^{\\prime}(k)$, then the $\\pmb{A}$ and $\\pmb{B}$ sequence will not have a double $\\pmb{B}$. The proof involves the condition where a segment in $S^{\\prime}(k+s)$ overlaps with more than two segments in $S^{\\prime}(k)$ and the inequality $f(\\bigcup B) \\leqslant f(\\bigcup A)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: The log linear model (6·1) is a generalized quasi-symmetry model.\n- **B) Correct**: The final term in the log linear model (6·1) is permutation invariant for like permutations of $(h_{1},\\dots,h_{k})$ and $(c_{1},\\ldots,c_{k})$.\n- **C) Incorrect**: The log linear model (6·1) does account for subject parameters through the term $\\delta(c_{1},\\ldots,c_{k};h_{1},\\ldots,h_{k})$.\n- **D) Incorrect**: The log linear model (6·1) is not limited to $k=2$.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["A_proportional_odds_model_with_subject-specific_effects_for_repeated_ordered_categorical_responses.md"], "background_text": "The log linear model (6·1) is a generalized quasi-symmetry model resulting from using the cumulative Rasch model (1·2). The final term in the model is permutation invariant for like permutations of $(h_{1},\\dots,h_{k})$ and $(c_{1},\\ldots,c_{k})$. The model is not limited to $k=2$ and accounts for subject parameters through the term $\\delta(c_{1},\\ldots,c_{k};h_{1},\\ldots,h_{k})$.", "folder": "", "qid": "statistic-mcqm-5610", "question": "In the context of the cumulative Rasch model, which of the following statements about the log linear model (6·1) are correct?", "options": ["A) The log linear model (6·1) is a generalized quasi-symmetry model.", "B) The final term in the log linear model (6·1) is permutation invariant for like permutations of $(h_{1},\\dots,h_{k})$ and $(c_{1},\\ldots,c_{k})$.", "C) The log linear model (6·1) does not account for the subject parameters.", "D) The log linear model (6·1) is only applicable when $k=2$."], "category": "CORRECT", "question_context_used": "The log linear model (6·1) is a generalized quasi-symmetry model resulting from using the cumulative Rasch model (1·2). The final term in the model is permutation invariant for like permutations of $(h_{1},\\dots,h_{k})$ and $(c_{1},\\ldots,c_{k})$. The model is not limited to $k=2$ and accounts for subject parameters through the term $\\delta(c_{1},\\ldots,c_{k};h_{1},\\ldots,h_{k})$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: The text states this exact condition for determining $k_{1}$ and $k_{2}$.\n- **B) Correct**: The text reduces the condition to this probability statement involving $Y = \\widehat{\\theta}/\\theta$.\n- **C) Correct**: The text specifies that $k_{1} > 0$ is a requirement for the condition to hold.\n- **D) Incorrect**: While minimizing the width is mentioned as a desirable trait, it is not a condition that must hold for determining $k_{1}$ and $k_{2}$ in general.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Two-sided-tolerance-intervals-in-the-exponential_2.md"], "background_text": "In the exponential case, the tolerance limits $U_{1} = k_{1}\\widehat{\\theta}$ and $U_{2} = k_{2}\\widehat{\\theta}$ are determined such that $\\operatorname{Pr}\\left\\{1 - \\exp(-k_{1}\\widehat{\\theta}/\\theta) \\leq (1-\\beta)/2 \\quad \\text{and} \\quad \\exp(-k_{2}\\widehat{\\theta}/\\theta) \\leq (1-\\beta)/2\\right\\} = \\gamma$. This reduces to $\\operatorname{Pr}\\left\\{-\\ln(p_{2})/k_{2} \\leq Y \\leq -\\ln(p_{1})/k_{1}\\right\\} = \\gamma$, where $Y = \\widehat{\\theta}/\\theta$ and $k_{1} > 0$. The width of the tolerance interval can be minimized by choosing $k_{1}$ and $k_{2}$ appropriately.", "folder": "", "qid": "statistic-mcqm-4982", "question": "In the exponential case, the tolerance limits $U_{1} = k_{1}\\widehat{\\theta}$ and $U_{2} = k_{2}\\widehat{\\theta}$ are determined such that which of the following conditions hold?", "options": ["A) $\\operatorname{Pr}\\left\\{1 - \\exp(-k_{1}\\widehat{\\theta}/\\theta) \\leq (1-\\beta)/2 \\quad \\text{and} \\quad \\exp(-k_{2}\\widehat{\\theta}/\\theta) \\leq (1-\\beta)/2\\right\\} = \\gamma$.", "B) $\\operatorname{Pr}\\left\\{-\\ln(p_{2})/k_{2} \\leq Y \\leq -\\ln(p_{1})/k_{1}\\right\\} = \\gamma$, where $Y = \\widehat{\\theta}/\\theta$.", "C) $k_{1}$ and $k_{2}$ must be positive.", "D) The width of the tolerance interval $U_{2} - U_{1}$ is minimized."], "category": "PARTIALLY_CORRECT", "question_context_used": "In the exponential case, the tolerance limits $U_{1} = k_{1}\\widehat{\\theta}$ and $U_{2} = k_{2}\\widehat{\\theta}$ are determined such that $\\operatorname{Pr}\\left\\{1 - \\exp(-k_{1}\\widehat{\\theta}/\\theta) \\leq (1-\\beta)/2 \\quad \\text{and} \\quad \\exp(-k_{2}\\widehat{\\theta}/\\theta) \\leq (1-\\beta)/2\\right\\} = \\gamma$. This reduces to $\\operatorname{Pr}\\left\\{-\\ln(p_{2})/k_{2} \\leq Y \\leq -\\ln(p_{1})/k_{1}\\right\\} = \\gamma$, where $Y = \\widehat{\\theta}/\\theta$ and $k_{1} > 0$. The width of the tolerance interval can be minimized by choosing $k_{1}$ and $k_{2}$ appropriately.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n**Explanation**: \n- **Option A** is correct as it directly matches the given formulae for mean and variance under independence.\n- **Option D** is also correct because it presents an alternative correct expression for the variance, considering $s = N - r$, which is equivalent to the given formula when expanded.\n- **Option B** is incorrect because the denominator for the variance is $N^3$ instead of $N^2(N-1)$.\n- **Option C** is incorrect because the mean is incorrectly stated as $m n / N$ instead of $m r / N$.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Mean_and_variance_of_an_entry_in_a_contingency_table.md"], "background_text": "Under independence, the mean and variance of a tabular entry $a$ in a $2\\times2$ contingency table are given by $\\mu = m r / N$ and $\\sigma^2 = m n r s / \\{N^2(N-1)\\}$.", "folder": "", "qid": "statistic-mcqm-3417", "question": "In a $2\\times2$ contingency table under the assumption of independence, the mean ($\\mu$) and variance ($\\sigma^2$) of a tabular entry $a$ are given by:", "options": ["A) $\\mu = \\frac{m r}{N}$, $\\sigma^2 = \\frac{m n r s}{N^2(N-1)}$", "B) $\\mu = \\frac{m r}{N}$, $\\sigma^2 = \\frac{m n r s}{N^3}$", "C) $\\mu = \\frac{m n}{N}$, $\\sigma^2 = \\frac{m n r s}{N^2}$", "D) $\\mu = \\frac{r s}{N}$, $\\sigma^2 = \\frac{m n r s}{N^2(N-1)}$"], "category": "PARTIALLY_CORRECT", "question_context_used": "Under independence, the mean and variance of a tabular entry $a$ in a $2\\times2$ contingency table are given by $\\mu = m r / N$ and $\\sigma^2 = m n r s / \\{N^2(N-1)\\}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A) Polynomial family**: Correct, as mentioned in the text.\n- **B) Exponential family**: Correct, as mentioned in the text.\n- **C) Logarithmic family**: Incorrect, not mentioned in the text.\n- **D) Fourier series family**: Correct, as mentioned in the text.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Semi-parametric-nonlinear-regression-and-transf_20.md"], "background_text": "The text discusses the use of linearly independent functions in approximate learning for regression modeling, specifically mentioning polynomial, exponential, and Fourier series families as examples.", "folder": "", "qid": "statistic-mcqm-5125", "question": "Which of the following are valid families of linearly independent functions used in approximate learning for regression modeling?", "options": ["A) Polynomial family: $\\{1, x, x^{2}, \\ldots, x^{q}\\}$", "B) Exponential family: $\\{1, e^{x}, e^{-x}, e^{2x}, e^{-2x}, \\ldots, e^{q x}, e^{-q x}\\}$", "C) Logarithmic family: $\\{1, \\log x, \\log x^{2}, \\ldots, \\log x^{q}\\}$", "D) Fourier series family: $\\{1, \\sin x, \\cos x, \\sin(2x), \\cos(2x), \\ldots, \\sin(q x), \\cos(q x)\\}$"], "category": "CORRECT", "question_context_used": "The text discusses the use of linearly independent functions in approximate learning for regression modeling, specifically mentioning polynomial, exponential, and Fourier series families as examples.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C, D\n\n- **Option A**: Incorrect. The text states that $a_{0} \\in \\mathbb{R}$, meaning it can be any real number, not necessarily non-negative.\n- **Option B**: Correct. The text explicitly states that $b_{0} \\geq 0$.\n- **Option C**: Correct. The text identifies $\\nu_{0}$ as a Lévy density.\n- **Option D**: Correct. As mentioned, $a_{0}$ can be any real number.\n\n**Final Answer**: The final answer is $\\boxed{{B, C, D}}$", "source": ["Scandinavian J Statistics - 2021 - Glück - A solut.md"], "background_text": "The Lévy characteristics of the random measure $\\Lambda$ are given by $(a_{0}, b_{0}, \\nu_{0})$, where $a_{0} \\in \\mathbb{R}$, $b_{0} \\geq 0$, and $\\nu_{0}$ is a Lévy density.", "folder": "", "qid": "statistic-mcqm-5227", "question": "Which of the following are true about the Lévy characteristics $(a_{0}, b_{0}, \\nu_{0})$ of the stationary infinitely divisible independently scattered random measure $\\Lambda$?", "options": ["A) $a_{0}$ must be non-negative.", "B) $b_{0}$ is a non-negative real number.", "C) $\\nu_{0}$ is a Lévy density.", "D) $a_{0}$ can be any real number."], "category": "CORRECT", "question_context_used": "The Lévy characteristics of the random measure $\\Lambda$ are given by $(a_{0}, b_{0}, \\nu_{0})$, where $a_{0} \\in \\mathbb{R}$, $b_{0} \\geq 0$, and $\\nu_{0}$ is a Lévy density.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"B\", \"C\", \"D\"]\n}", "model_answer": ["B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct, as the text states both statistics exceed the 50th percentiles.\n- **B)** Correct, as neither statistic exceeds the 95th percentiles.\n- **C)** Incorrect, the text suggests likelihood ratio ordering is doubtful, not confirmed.\n- **D)** Correct, the sample sizes are explicitly given as $m=90$ and $n=51$ after conditioning.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Nonparametric_tests_for_and_against_likelihood_ratio_ordering_in_the_two-sample_problem.md"], "background_text": "The context discusses conditioning on intervals where concavity is violated in the ordinal dominance curve, with specific sample sizes and test statistics provided. The test statistics are compared to percentiles of certain distributions to draw conclusions about likelihood ratio ordering.", "folder": "", "qid": "statistic-mcqm-5739", "question": "Given the conditional ordinal dominance curve analysis with $b=700$, the sample sizes are $m=90$ and $n=51$, and the observed test statistics are $M_{m,n}^{*(12)}=0.583$ and $D_{m,n}^{*(12)}=1.205$. Which of the following statements are correct based on the provided context?", "options": ["A) Both $M_{m,n}^{*(12)}$ and $D_{m,n}^{*(12)}$ exceed the 50th percentiles of their respective distributions.", "B) Neither $M_{m,n}^{*(12)}$ nor $D_{m,n}^{*(12)}$ exceeds the 95th percentiles of their respective distributions.", "C) The likelihood ratio ordering is confirmed based on the observed test statistics.", "D) The sample sizes after conditioning are $m=90$ and $n=51$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The context discusses conditioning on intervals where concavity is violated in the ordinal dominance curve, with specific sample sizes and test statistics provided. The test statistics are compared to percentiles of certain distributions to draw conclusions about likelihood ratio ordering.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"D\"]\n}", "model_answer": ["D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**: \n- **Option A** is correct because it directly states one of the definitions of a supermodular function.\n- **Option B** is correct because it provides an alternative definition of supermodularity using lattice operators.\n- **Option C** is correct because it gives a necessary and sufficient condition for supermodularity of twice differentiable functions.\n- **Option D** is correct because it states a property of supermodular functions when composed with increasing functions.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Some-Remarks-on-the-Supermodular-Order_2000_Journa.md"], "background_text": "Supermodularity of a function $f\\colon\\mathbb{R}^{d}\\to\\mathbb{R}$ can be defined by $\\varDelta_{i}^{\\varepsilon}\\varDelta_{j}^{\\delta}f(\\mathbf{x})\\geqslant0$ for all $\\mathbf{x}\\in\\mathbb{R}^{d},1\\leqslant i<j\\leqslant d$ and all $\\varepsilon,\\delta>0$. Alternatively, it can be defined by $f(\\mathbf{x}\\wedge\\mathbf{y})+f(\\mathbf{x}\\vee\\mathbf{y})\\geqslant f(\\mathbf{x})+f(\\mathbf{y})$ for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$. For a twice differentiable function, supermodularity is equivalent to $\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}f(\\mathbf{x})\\geqslant0$ for all $\\mathbf{x}\\in\\mathbb{R}^{d},1\\leqslant i<j\\leqslant d$. Additionally, if $g_{1},...,g_{d}\\colon\\mathbb{R}\\to\\mathbb{R}$ are increasing functions and $f$ is supermodular, then $f(g_{1}(\\cdot),...,g_{d}(\\cdot))$ is also supermodular.", "folder": "", "qid": "statistic-mcqm-5035", "question": "Which of the following statements are true regarding supermodular functions?", "options": ["A) A function $f$ is supermodular if $\\varDelta_{i}^{\\varepsilon}\\varDelta_{j}^{\\delta}f(\\mathbf{x})\\geqslant0$ for all $\\mathbf{x}\\in\\mathbb{R}^{d},1\\leqslant i<j\\leqslant d$ and all $\\varepsilon,\\delta>0$.", "B) A function $f$ is supermodular if $f(\\mathbf{x}\\wedge\\mathbf{y})+f(\\mathbf{x}\\vee\\mathbf{y})\\geqslant f(\\mathbf{x})+f(\\mathbf{y})$ for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$.", "C) If $f$ is twice differentiable, then $f$ is supermodular if and only if $\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}f(\\mathbf{x})\\geqslant0$ for all $\\mathbf{x}\\in\\mathbb{R}^{d},1\\leqslant i<j\\leqslant d$.", "D) If $g_{1},...,g_{d}\\colon\\mathbb{R}\\to\\mathbb{R}$ are increasing functions and $f$ is supermodular, then $f(g_{1}(\\cdot),...,g_{d}(\\cdot))$ is also supermodular."], "category": "CORRECT", "question_context_used": "Supermodularity of a function $f\\colon\\mathbb{R}^{d}\\to\\mathbb{R}$ can be defined by $\\varDelta_{i}^{\\varepsilon}\\varDelta_{j}^{\\delta}f(\\mathbf{x})\\geqslant0$ for all $\\mathbf{x}\\in\\mathbb{R}^{d},1\\leqslant i<j\\leqslant d$ and all $\\varepsilon,\\delta>0$. Alternatively, it can be defined by $f(\\mathbf{x}\\wedge\\mathbf{y})+f(\\mathbf{x}\\vee\\mathbf{y})\\geqslant f(\\mathbf{x})+f(\\mathbf{y})$ for all $\\mathbf{x},\\mathbf{y}\\in\\mathbb{R}^{d}$. For a twice differentiable function, supermodularity is equivalent to $\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}f(\\mathbf{x})\\geqslant0$ for all $\\mathbf{x}\\in\\mathbb{R}^{d},1\\leqslant i<j\\leqslant d$. Additionally, if $g_{1},...,g_{d}\\colon\\mathbb{R}\\to\\mathbb{R}$ are increasing functions and $f$ is supermodular, then $f(g_{1}(\\cdot),...,g_{d}(\\cdot))$ is also supermodular.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A)** Correct. The dual outer weight vectors are indeed constructed from the respective blocks with the ith row removed.\n- **B)** Correct. The outer components are constructed using the given formula for each block.\n- **C)** Incorrect. The Bayesian Discriminant Analysis is performed using both $\\mathbf{y}_{1}^{-i}$ and $\\mathbf{y}_{2}^{-i}$, not just $\\mathbf{y}_{1}^{-i}$.\n- **D)** Incorrect. The construction of $\\mathbf{y}_{3}^{-i}$ is not prioritized to explain its own block well but to correlate with its neighboring components, $\\mathbf{y}_{1}$ and $\\mathbf{y}_{2}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Kernel-Generalized-Canonical-Correlatio_2015_Compu.md"], "background_text": "The dual outer weight vectors $\\pmb{\\alpha}_{1}^{-i}, \\pmb{\\alpha}_{2}^{-i}$, and $\\pmb{\\alpha}_{3}^{-i}$ are constructed from $\\mathbf{X}_{1}^{-i}, \\mathbf{X}_{2}^{-i}$, and $\\mathbf{X}_{3}^{-i}$ respectively using Algorithm 3. The outer components $\\mathbf{y}_{1}^{-i}, \\mathbf{y}_{2}^{-i}$, and $\\mathbf{y}_{3}^{-i}$ are then constructed using these vectors. A Bayesian Discriminant Analysis is performed on the qualitative variable $\\mathbf{Loc}^{-i}$ using $\\mathbf{y}_{1}^{-i}$ and $\\mathbf{y}_{2}^{-i}$. The construction of $\\mathbf{y}_{3}^{-i}$ is not prioritized to explain its own block well but to correlate with $\\mathbf{y}_{1}$ and $\\mathbf{y}_{2}$.", "folder": "", "qid": "statistic-mcqm-2586", "question": "Given the context of constructing dual outer weight vectors and components, which of the following statements are correct?", "options": ["A) The dual outer weight vectors $\\pmb{\\alpha}_{1}^{-i}, \\pmb{\\alpha}_{2}^{-i}$, and $\\pmb{\\alpha}_{3}^{-i}$ are constructed from $\\mathbf{X}_{1}^{-i}, \\mathbf{X}_{2}^{-i}$, and $\\mathbf{X}_{3}^{-i}$ respectively.", "B) The outer components $\\mathbf{y}_{1}^{-i}, \\mathbf{y}_{2}^{-i}$, and $\\mathbf{y}_{3}^{-i}$ are constructed using the formula $\\mathbf{y}_{k}^{-i} = \\mathbf{X}_{k}^{-i} \\mathbf{X}_{k}^{-i^{\\top}} \\pmb{\\alpha}_{k}^{-i}$ for $k=1,2,3$.", "C) A Bayesian Discriminant Analysis is performed on $\\mathbf{Loc}^{-i}$ using only $\\mathbf{y}_{1}^{-i}$.", "D) The construction of $\\mathbf{y}_{3}^{-i}$ is prioritized to explain its own block well."], "category": "PARTIALLY_CORRECT", "question_context_used": "The dual outer weight vectors $\\pmb{\\alpha}_{1}^{-i}, \\pmb{\\alpha}_{2}^{-i}$, and $\\pmb{\\alpha}_{3}^{-i}$ are constructed from $\\mathbf{X}_{1}^{-i}, \\mathbf{X}_{2}^{-i}$, and $\\mathbf{X}_{3}^{-i}$ respectively using Algorithm 3. The outer components $\\mathbf{y}_{1}^{-i}, \\mathbf{y}_{2}^{-i}$, and $\\mathbf{y}_{3}^{-i}$ are then constructed using these vectors. A Bayesian Discriminant Analysis is performed on the qualitative variable $\\mathbf{Loc}^{-i}$ using $\\mathbf{y}_{1}^{-i}$ and $\\mathbf{y}_{2}^{-i}$. The construction of $\\mathbf{y}_{3}^{-i}$ is not prioritized to explain its own block well but to correlate with $\\mathbf{y}_{1}$ and $\\mathbf{y}_{2}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. Robust estimators are designed to provide reliable estimates even when the usual assumptions (like normality) are violated.\n- **B)** Correct. The influence function indeed measures the sensitivity of the estimator to small amounts of contamination.\n- **C)** Incorrect. The breakdown point is the largest (not smallest) fraction of contamination the estimator can handle before the bias becomes unbounded.\n- **D)** Correct. The maximum asymptotic bias is a criterion that evaluates the impact of a non-infinitesimal fraction of contamination.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Robust-tests-for-linear-regression-models-b_2016_C.md"], "background_text": "Robust estimators yield reliable point estimates for the model parameters even when the ideal distributional assumptions are not satisfied. Robustness properties of such estimators have been investigated via their influence function and breakdown point. The influence function provides information on the effect of a small amount of contamination on the estimator while the breakdown point is the largest fraction of arbitrary contamination that can be present in the data without driving the bias of the estimator to infinity. Another robustness criterion is the maximum asymptotic bias which measures the effect of a positive fraction of contamination.", "folder": "", "qid": "statistic-mcqm-5707", "question": "Which of the following statements about robust estimators are true?", "options": ["A) Robust estimators provide reliable point estimates even when ideal distributional assumptions are not satisfied.", "B) The influence function measures the effect of a small amount of contamination on the estimator.", "C) The breakdown point is the smallest fraction of arbitrary contamination that can be present in the data without driving the bias of the estimator to infinity.", "D) The maximum asymptotic bias measures the effect of a positive fraction of contamination."], "category": "CORRECT", "question_context_used": "Robust estimators yield reliable point estimates for the model parameters even when the ideal distributional assumptions are not satisfied. Robustness properties of such estimators have been investigated via their influence function and breakdown point. The influence function provides information on the effect of a small amount of contamination on the estimator while the breakdown point is the largest fraction of arbitrary contamination that can be present in the data without driving the bias of the estimator to infinity. Another robustness criterion is the maximum asymptotic bias which measures the effect of a positive fraction of contamination.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: Options A and B are correct as they are part of the expression for the natural logarithm of the likelihood function. Option C is correct because the likelihood function for the complete set of comparisons is the product of the likelihood functions for each judge. Option D is incorrect because it represents a sum of ranks, not a term in the likelihood function.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["AN_ANALYSIS_OF_PAIRED_COMPARISON_DESIGNS_WITH_INCOMPLETE_REPETITIONS.md"], "background_text": "The likelihood function for the complete set of comparisons made by $v$ judges includes terms like $\\sum_{i} a_{i}^{*} \\ln \\pi_{i u}$ and $-\\sum_{i < j} \\sum_{u} n_{i j u} \\ln (\\pi_{i u} + \\pi_{j u})$, where $a_{i}^{*} = 2 \\alpha v - \\sum_{j} \\sum_{u} n_{i j u} r_{i j u}$ and $L_{u}$ is the likelihood function for the set of comparisons made by judge $u$.", "folder": "", "qid": "statistic-mcqm-1859", "question": "In the likelihood function for the complete set of comparisons made by $v$ judges, which of the following terms are included?", "options": ["A) $\\sum_{i} a_{i}^{*} \\ln \\pi_{i u}$", "B) $-\\sum_{i < j} \\sum_{u} n_{i j u} \\ln (\\pi_{i u} + \\pi_{j u})$", "C) $\\prod_{u} L_{u}$", "D) $\\sum_{i} \\sum_{j} \\sum_{u} n_{i j u} r_{i j u}$"], "category": "CORRECT", "question_context_used": "The likelihood function for the complete set of comparisons made by $v$ judges includes terms like $\\sum_{i} a_{i}^{*} \\ln \\pi_{i u}$ and $-\\sum_{i < j} \\sum_{u} n_{i j u} \\ln (\\pi_{i u} + \\pi_{j u})$, where $a_{i}^{*} = 2 \\alpha v - \\sum_{j} \\sum_{u} n_{i j u} r_{i j u}$ and $L_{u}$ is the likelihood function for the set of comparisons made by judge $u$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A)** Correct. Linear independence of the functions $\\phi_{s}$ ensures the uniqueness of the model representation.\n- **B)** Correct. Equality of the parameters $c_{r_{1}r_{2}\\ldots r_{k}}$ and $c_{r_{1}r_{2}\\ldots r_{k}}^{\\star}$ for all indices confirms uniqueness.\n- **C)** Incorrect. Scaling the model by a constant does not ensure uniqueness; it introduces non-uniqueness.\n- **D)** Incorrect. Linear dependence of the functions would prevent uniqueness.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Semi-parametric-nonlinear-regression-and-transf_20.md"], "background_text": "The general model representation in (14) is unique if the sets of functions $\\phi_{s}$ are linearly independent, ensuring that the parameters $c_{r_{1}r_{2}\\ldots r_{k}}$ are uniquely determined.", "folder": "", "qid": "statistic-mcqm-5092", "question": "Which conditions ensure the uniqueness of the general model representation in (14)?", "options": ["A) The sets of functions $\\phi_{s}$ are linearly independent.", "B) The parameters $c_{r_{1}r_{2}\\ldots r_{k}}$ and $c_{r_{1}r_{2}\\ldots r_{k}}^{\\star}$ are equal for all $r_{1}, \\ldots, r_{k}$.", "C) The model is scaled by an arbitrary nonzero constant.", "D) The functions $\\phi_{r_{1}}(x_{1})\\phi_{r_{2}}(x_{2})\\ldots\\phi_{r_{k}}(x_{k})$ are linearly dependent."], "category": "PARTIALLY_CORRECT", "question_context_used": "The general model representation in (14) is unique if the sets of functions $\\phi_{s}$ are linearly independent, ensuring that the parameters $c_{r_{1}r_{2}\\ldots r_{k}}$ are uniquely determined.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The context specifies that $\\beta \\sim \\mathrm{N}(\\mu, \\sigma^{2}V)$.\n- **B)** Correct. It is stated that $\\frac{\\nu\\lambda}{\\sigma^{2}} \\sim \\chi_{\\nu}^{2}$.\n- **C)** Correct. The prior centers $\\beta$ on zero for all coefficients except $\\beta_{0}$, which is set to $\\hat{\\beta}_{0}$.\n- **D)** Correct. The hyperparameters were indeed chosen to maximize $\\Pr(\\sigma^{2} \\leq 1)$ while maintaining reasonable flatness for other probabilities.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Bayesian_Variable_and_Transformation_Selection_in_Linear_Regression__Bayesian_Variable_and_Transformation_Selection_in_Linear_Regression.md"], "background_text": "The prior for $\\beta$ is a normal distribution $\\mathrm{N}(\\mu, \\sigma^{2}V)$, and $\\frac{\\\nu\\lambda}{\\sigma^{2}}$ follows a $\\chi_{\\nu}^{2}$ distribution. The distribution of $\\beta$ is centered on zero except for $\\beta_{0}$. Hyperparameters $\\nu=2.58$, $\\lambda=0.28$, and $\\phi=2.85$ were chosen to maximize $\\Pr(\\sigma^{2} \\leq 1)$ while keeping other probabilities reasonably flat.", "folder": "", "qid": "statistic-mcqm-4124", "question": "In the Bayesian framework described, which of the following are correct about the prior distributions for $\\beta$ and $\\sigma^{2}$?", "options": ["A) $\\beta$ is assumed to follow a normal distribution with mean $\\mu$ and variance $\\sigma^{2}V$.", "B) $\\frac{\\nu\\lambda}{\\sigma^{2}}$ follows a chi-squared distribution with $\\nu$ degrees of freedom.", "C) The prior for $\\beta$ is centered on zero for all coefficients except $\\beta_{0}$.", "D) The hyperparameters $\\nu$, $\\lambda$, and $\\phi$ are chosen to maximize $\\Pr(\\sigma^{2} \\leq 1)$ while keeping other probabilities reasonably flat."], "category": "PARTIALLY_CORRECT", "question_context_used": "The prior for $\\beta$ is a normal distribution $\\mathrm{N}(\\mu, \\sigma^{2}V)$, and $\\frac{\\\nu\\lambda}{\\sigma^{2}}$ follows a $\\chi_{\\nu}^{2}$ distribution. The distribution of $\\beta$ is centered on zero except for $\\beta_{0}$. Hyperparameters $\\nu=2.58$, $\\lambda=0.28$, and $\\phi=2.85$ were chosen to maximize $\\Pr(\\sigma^{2} \\leq 1)$ while keeping other probabilities reasonably flat.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A) Correct**: The text states that if $p_{\\mathrm{a}} = 2.042$, the curve can be treated as the difference of two curves with modal ordinates at 2.2 and 0.9.\n- **B) Correct**: The text mentions that if $p_{\\mathrm{s}} = -0.178$, the compound curve is the sum of two curves with modes that do not differ much.\n- **C) Correct**: The text notes that if $p_{\\mathrm{s}} = -0.389$, the curve is the sum of two curves with extremely different modes.\n- **D) Incorrect**: The text explicitly states that the number of cases of the first component for $p_{\\mathrm{s}} = -0.389$ is very low, not high.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["BIOMETRICAL_STUDIES_ON_THE_ANTHROPOLOGY_OF_RUSSIA.md"], "background_text": "The text provides three scenarios for the compound curve solutions for East Russians, detailing the nature of the curves based on different values of $p_{\\mathrm{s}}$.", "folder": "", "qid": "statistic-mcqm-1290", "question": "Regarding the compound curve solutions for East Russians, which of the following are correct?", "options": ["A) If $p_{\\mathrm{a}} = 2.042$, the curve is the difference of two curves with modal ordinates at 2.2 and 0.9.", "B) If $p_{\\mathrm{s}} = -0.178$, the curve is the sum of two curves with similar modes.", "C) If $p_{\\mathrm{s}} = -0.389$, the curve is the sum of two curves with extremely different modes.", "D) The first component of the curve for $p_{\\mathrm{s}} = -0.389$ has a very high number of cases."], "category": "INCORRECT", "question_context_used": "The text provides three scenarios for the compound curve solutions for East Russians, detailing the nature of the curves based on different values of $p_{\\mathrm{s}}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **Option A**: Correct. The text explicitly states that $\\mathbf{a}^{\\prime}\\mathbf{U}\\mathbf{a} = 0$.\n- **Option B**: Correct. The text states that $u_{i,i} = \\sigma_{i}^{2}$ for $i=1,\\ldots,n$.\n- **Option C**: Incorrect. $\\mathbf{U}$ is non-negative definite only under the condition given in the text, not always.\n- **Option D**: Incorrect. $\\mathbf{U}$ is not necessarily a diagonal matrix as it involves $\\mathbf{F}_{n}$ and $\\mathrm{diag}(\\mathbf{c})$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Fast,_Resolution-Consistent_Spatial_Prediction_of_Global_Processes_From_Satellite_Data__Fast__Resolution-Consistent_Spatial_Prediction_of_Global_Processes_From_Satellite_Data.md"], "background_text": "The matrix $\\mathbf{U}$ is defined as $\\left(\\mathrm{diag}(\\mathbf{a})\\right)^{-1}\\mathbf{F}_{n}\\mathrm{diag}(\\mathbf{c})\\mathbf{F}_{n}\\left(\\mathrm{diag}(\\mathbf{a})\\right)^{-1}$, where $\\mathbf{c} \\equiv \\mathbf{G}_{n}^{-1}(a_{1}^{2}\\sigma_{1}^{2},\\ldots,a_{n}^{2}\\sigma_{n}^{2})^{\\prime}$. The text states that $\\mathbf{a}^{\\prime}\\mathbf{U}\\mathbf{a} = 0$, $u_{i,i} = \\sigma_{i}^{2}$, and provides a condition for $\\mathbf{U}$ to be non-negative definite.", "folder": "", "qid": "statistic-mcqm-5480", "question": "Given the matrix $\\mathbf{U} \\equiv \\left(\\mathrm{diag}(\\mathbf{a})\\right)^{-1}\\mathbf{F}_{n}\\mathrm{diag}(\\mathbf{c})\\mathbf{F}_{n}\\left(\\mathrm{diag}(\\mathbf{a})\\right)^{-1}$, where $\\mathbf{c} \\equiv \\mathbf{G}_{n}^{-1}(a_{1}^{2}\\sigma_{1}^{2},\\ldots,a_{n}^{2}\\sigma_{n}^{2})^{\\prime}$, which of the following properties hold true?", "options": ["A) $\\mathbf{a}^{\\prime}\\mathbf{U}\\mathbf{a} = 0$", "B) $u_{i,i} = \\sigma_{i}^{2}$ for $i=1,\\ldots,n$", "C) $\\mathbf{U}$ is always non-negative definite", "D) $\\mathbf{U}$ is a diagonal matrix"], "category": "CORRECT", "question_context_used": "The matrix $\\mathbf{U}$ is defined as $\\left(\\mathrm{diag}(\\mathbf{a})\\right)^{-1}\\mathbf{F}_{n}\\mathrm{diag}(\\mathbf{c})\\mathbf{F}_{n}\\left(\\mathrm{diag}(\\mathbf{a})\\right)^{-1}$, where $\\mathbf{c} \\equiv \\mathbf{G}_{n}^{-1}(a_{1}^{2}\\sigma_{1}^{2},\\ldots,a_{n}^{2}\\sigma_{n}^{2})^{\\prime}$. The text states that $\\mathbf{a}^{\\prime}\\mathbf{U}\\mathbf{a} = 0$, $u_{i,i} = \\sigma_{i}^{2}$, and provides a condition for $\\mathbf{U}$ to be non-negative definite.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A)** Correct. Under normality, the null distribution of $T/h$ is indeed $F_{h,n-k}$.\n- **B)** Correct. Under nonnormality, $T$ converges to $\\chi_{h}^{2}$ as $n \\to \\infty$ under certain conditions.\n- **C)** Correct. The test statistic $T$ is invariant under the transformation from $\\boldsymbol{y}$ to $y/\\sigma$.\n- **D)** Incorrect. The nominal test size based on the limiting distribution under nonnormality is often far from the actual test size, as mentioned in the context.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$.", "source": ["Asymptotic-expansion-of-the-null-distribution-of-t.md"], "background_text": "The test statistic $T$ for testing the null hypothesis $\\mathrm{H}_{0}: H{\\pmb\\beta}={\\bf0}$ in a linear model is given by $T=\\frac{\\hat{\\beta}^{\\prime}H^{\\prime}\\{H(X^{\\prime}X)^{-1}H^{\\prime}\\}^{-1}H\\hat{\\beta}}{\\hat{\\sigma}^{2}}$. Under normality, $T/h$ follows an $F_{h,n-k}$ distribution. Under nonnormality, $T$ converges to $\\chi_{h}^{2}$ as $n \\to \\infty$, but the nominal test size based on this limiting distribution may not match the actual test size.", "folder": "", "qid": "statistic-mcqm-999", "question": "In the context of testing the null hypothesis $\\mathrm{H}_{0}: H{\\pmb\\beta}={\\bf0}$ in a linear model, which of the following statements about the test statistic $T$ are correct?", "options": ["A) Under normality, the null distribution of $T/h$ is distributed as $F_{h,n-k}$.", "B) Under nonnormality, the null distribution of $T$ converges to $\\chi_{h}^{2}$ as the sample size $n$ tends to infinity.", "C) The test statistic $T$ is invariant under the transformation from $\\boldsymbol{y}$ to $y/\\sigma$.", "D) The nominal test size based on the limiting distribution under nonnormality is always close to the actual test size."], "category": "PARTIALLY_CORRECT", "question_context_used": "The test statistic $T$ for testing the null hypothesis $\\mathrm{H}_{0}: H{\\pmb\\beta}={\\bf0}$ in a linear model is given by $T=\\frac{\\hat{\\beta}^{\\prime}H^{\\prime}\\{H(X^{\\prime}X)^{-1}H^{\\prime}\\}^{-1}H\\hat{\\beta}}{\\hat{\\sigma}^{2}}$. Under normality, $T/h$ follows an $F_{h,n-k}$ distribution. Under nonnormality, $T$ converges to $\\chi_{h}^{2}$ as $n \\to \\infty$, but the nominal test size based on this limiting distribution may not match the actual test size.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **A) Correct**: The nugget effect is included to account for instrument measurement error and data aggregation errors.\n- **B) Correct**: The Matern spatial correlation function $C_{S}$ is used.\n- **C) Correct**: The temporal correlation function $C_{T}$ is used, which is specified as the product of the exponential and the spherical correlation functions.\n- **D) Incorrect**: The covariance function does not assume independence between spatial and temporal components; it models their interaction through $C_{S}$ and $C_{T}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Efficient_Interpolation_of_Computationally_Expensive_Posterior_Densities_With_Variable_Parameter_Costs__Efficient_Interpolation_of_Computationally_Expensive_Posterior_Densities_With_Variable_Parameter_Costs.md"], "background_text": "The covariance function for $\\epsilon$ has the form $\\mathrm{cov}\\{\\epsilon(s,t),\\epsilon(s^{\\prime},t^{\\prime})\\}=\\sigma_{1}^{2}\\cdot\\mathbb{I}\\{s=s^{\\prime},t=t^{\\prime}\\}+\\sigma_{2}^{2}\\cdot C_{S}(s,s^{\\prime}|\\theta_{S})\\cdot C_{T}(t,t^{\\prime}|\\theta_{T})$, where $C_{S}$ is the Matern spatial correlation function, $C_{T}$ is the temporal correlation function, and $\\mathbb{I}$ is the indicator function. A nugget effect is included to account for instrument measurement error and data aggregation errors.", "folder": "", "qid": "statistic-mcqm-3537", "question": "Which of the following are true about the covariance function for $\\epsilon$?", "options": ["A) It includes a nugget effect to account for instrument measurement error and data aggregation errors.", "B) It uses the Matern spatial correlation function $C_{S}$.", "C) It uses the exponential temporal correlation function $C_{T}$.", "D) It assumes independence between spatial and temporal components."], "category": "PARTIALLY_CORRECT", "question_context_used": "The covariance function for $\\epsilon$ has the form $\\mathrm{cov}\\{\\epsilon(s,t),\\epsilon(s^{\\prime},t^{\\prime})\\}=\\sigma_{1}^{2}\\cdot\\mathbb{I}\\{s=s^{\\prime},t=t^{\\prime}\\}+\\sigma_{2}^{2}\\cdot C_{S}(s,s^{\\prime}|\\theta_{S})\\cdot C_{T}(t,t^{\\prime}|\\theta_{T})$, where $C_{S}$ is the Matern spatial correlation function, $C_{T}$ is the temporal correlation function, and $\\mathbb{I}$ is the indicator function. A nugget effect is included to account for instrument measurement error and data aggregation errors.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A) Correct**: Simes' test controls the Familywise Error Rate strongly at $\\alpha$ when the test statistics are independent.\n- **B) Correct**: Simes' test also controls the Familywise Error Rate strongly at $\\alpha$ when the test statistics have a distribution with the multivariate total positivity of order two property or a scale mixture thereof.\n- **C) Incorrect**: Normality of the test statistics is not mentioned as a condition for Simes' test to control the Familywise Error Rate.\n- **D) Incorrect**: Identical distribution of the test statistics is not mentioned as a condition for Simes' test to control the Familywise Error Rate.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Hochberg's_Step-Up_Method_Cutting_Corners_Off_Holm's_Step-Down_Method.md"], "background_text": "Simes' test is a level-$\\alpha$ test for $H_{0I}$ and controls the Familywise Error Rate strongly at $\\alpha$ when the test statistics are independent or have a distribution with the multivariate total positivity of order two property or a scale mixture thereof.", "folder": "", "qid": "statistic-mcqm-5947", "question": "Under what conditions does Simes' test control the Familywise Error Rate strongly at $\\alpha$?", "options": ["A) When the test statistics are independent.", "B) When the test statistics have a distribution with the multivariate total positivity of order two property.", "C) When the test statistics are normally distributed.", "D) When the test statistics are identically distributed."], "category": "CORRECT", "question_context_used": "Simes' test is a level-$\\alpha$ test for $H_{0I}$ and controls the Familywise Error Rate strongly at $\\alpha$ when the test statistics are independent or have a distribution with the multivariate total positivity of order two property or a scale mixture thereof.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C\n\n**Explanation**:\n- **Option A** is incorrect because $\\xi_{k}^{(i)}$ is not defined for all $k$; it is zero for $k > i + 1$.\n- **Option B** is correct as the text explicitly states that $\\xi_{k}^{(i)}$ is zero for $k > i + 1$.\n- **Option C** is correct because the definition of $\\xi_{k}^{(i)}$ involves $P_{i}=\\Sigma_{k=1}^{i}p_{k}$.\n- **Option D** is incorrect as $\\xi_{k}^{(i)}$ depends on $p_{i}$ through $P_{i}$.\n\n**Final Answer**: The final answer is $\\boxed{{B, C}}$", "source": ["The Distribution of Higher-Order Interactions in C.md"], "background_text": "The Helmert-based functions orthonormal over the distribution $(p_{i},i=1,...,r)$ are defined by $\\xi_{k}^{(i)}$ which is zero for $k > i + 1$ and depends on $P_{i}=\\Sigma_{k=1}^{i}p_{k}$.", "folder": "", "qid": "statistic-mcqm-6147", "question": "In the context of Helmert-based orthonormal functions, which of the following statements about the function $\\xi_{k}^{(i)}$ are true?", "options": ["A) $\\xi_{k}^{(i)}$ is defined for all $k$.", "B) $\\xi_{k}^{(i)}$ is zero for $k > i + 1$.", "C) $\\xi_{k}^{(i)}$ depends on $P_{i}=\\Sigma_{k=1}^{i}p_{k}$.", "D) $\\xi_{k}^{(i)}$ is independent of $p_{i}$."], "category": "CORRECT", "question_context_used": "The Helmert-based functions orthonormal over the distribution $(p_{i},i=1,...,r)$ are defined by $\\xi_{k}^{(i)}$ which is zero for $k > i + 1$ and depends on $P_{i}=\\Sigma_{k=1}^{i}p_{k}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **A) Correct**: The context states that $\\frac{1}{2}\\pi$ times the mean of the measured $L(\\theta)$ values is an estimate of $L$.\n- **B) Incorrect**: The context mentions that if $n$ is odd, it is equivalent to using $2n$ directions, but this is not a correct statement as per the given context.\n- **C) Correct**: For $n$ even ($n = 2m$), the estimate $\\hat{L}$ satisfies the given inequalities.\n- **D) Incorrect**: The method is applicable to any rectifiable curve, not just straight lines.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Measuring_the_length_of_a_curve.md"], "background_text": "Steinhaus suggested measuring $L(\\theta)$ for a finite number $n$ of equally spaced values of $\\theta$ in $(0, 2\\pi)$ and calculating the mean. The estimate $\\hat{L}$ is $\\frac{1}{2}\\pi$ times this mean. For $n$ even ($n = 2m$), $\\hat{L}$ satisfies the inequalities $\\frac{\\pi}{2m}\\cos\\left(\\frac{\\pi}{2m}\\right)\\left\\{\\sin\\left(\\frac{\\pi}{2m}\\right)\\right\\}^{-1} \\leqslant \\hat{L}L^{-1} \\leqslant \\frac{\\pi}{2m}\\left\\{\\sin\\left(\\frac{\\pi}{2m}\\right)\\right\\}^{-1}$.", "folder": "", "qid": "statistic-mcqm-5425", "question": "Steinhaus suggested a practical method for measuring the length $L$ of a curve by measuring $L(\\theta)$ for a finite number of $n$ equally spaced values of $\\theta$ in the range $(0, 2\\pi)$. Which of the following statements about this method are correct?", "options": ["A) The estimate $\\hat{L}$ is given by $\\frac{1}{2}\\pi$ times the mean of the measured $L(\\theta)$ values.", "B) If $n$ is odd, it is equivalent to using $2n$ directions.", "C) For $n$ even, the estimate $\\hat{L}$ satisfies certain inequalities involving $\\frac{\\pi}{2m}$ where $n = 2m$.", "D) The method is only applicable when the curve is a straight line."], "category": "CORRECT", "question_context_used": "Steinhaus suggested measuring $L(\\theta)$ for a finite number $n$ of equally spaced values of $\\theta$ in $(0, 2\\pi)$ and calculating the mean. The estimate $\\hat{L}$ is $\\frac{1}{2}\\pi$ times this mean. For $n$ even ($n = 2m$), $\\hat{L}$ satisfies the inequalities $\\frac{\\pi}{2m}\\cos\\left(\\frac{\\pi}{2m}\\right)\\left\\{\\sin\\left(\\frac{\\pi}{2m}\\right)\\right\\}^{-1} \\leqslant \\hat{L}L^{-1} \\leqslant \\frac{\\pi}{2m}\\left\\{\\sin\\left(\\frac{\\pi}{2m}\\right)\\right\\}^{-1}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n**Explanation**:\n- **A) Correct**: The text states that at $90\\%$ power, the difference in detectable bias in the support for SD is approximately one percentage point.\n- **B) Incorrect**: The detectable bias is not independent of the total survey size $n$; larger $n$ generally allows for detection of smaller biases.\n- **C) Incorrect**: The detectable bias typically increases (not decreases) as the power of the test increases, because higher power allows detection of smaller effects.\n- **D) Correct**: The detectable bias is influenced by the allocation between the list and pair methods, as the optimized allocation leads to a substantial gain in power.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Scandinavian J Statistics - 2019 - Lagerås - How t.md"], "background_text": "The power calculation for bias detection is performed with a total survey size of $n=15000$, $p=10\\%$, $N=10$, and a confidence level (type I error) of $\\gamma=5\\%$. The allocation between the list and pair methods is optimized to maximize power, and at $90\\%$ power, the difference in detectable bias in the support for SD is approximately one percentage point.", "folder": "", "qid": "statistic-mcqm-5888", "question": "Given the power calculation for bias detection with $n=15000$, $p=10\\%$, $N=10$, and a confidence level (type I error) of $\\gamma=5\\%$, which of the following statements about the detectable bias are correct?", "options": ["A) At $90\\%$ power, the difference in detectable bias in the support for SD is approximately one percentage point.", "B) The detectable bias is independent of the total survey size $n$.", "C) The detectable bias decreases as the power of the test increases.", "D) The detectable bias is influenced by the allocation between the list and pair methods."], "category": "CORRECT", "question_context_used": "The power calculation for bias detection is performed with a total survey size of $n=15000$, $p=10\\%$, $N=10$, and a confidence level (type I error) of $\\gamma=5\\%$. The allocation between the list and pair methods is optimized to maximize power, and at $90\\%$ power, the difference in detectable bias in the support for SD is approximately one percentage point.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: This is explicitly mentioned as the first step of the C-VEM algorithm.\n- **B) Correct**: This is explicitly mentioned as the second step of the C-VEM algorithm.\n- **C) Incorrect**: The algorithm stops when the distance is smaller than ε, not necessarily zero.\n- **D) Incorrect**: The text states that Q, L, and K are assumed to be known and not part of the estimation.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["The-latent-topic-block-model-for-the-co-cluster_20.md"], "background_text": "The C-VEM algorithm involves two main steps: applying a VEM algorithm to maximize the lower bound with respect to q(·) and the model parameters, and using a greedy search strategy to maximize the lower bound with respect to Y and X. Convergence is assumed when the Euclidean distance between two consecutive values of the lower bound is smaller than a positive value ε. The values of Q, L, and K are assumed to be known and not estimated during the procedure.", "folder": "", "qid": "statistic-mcqm-5928", "question": "Regarding the C-VEM algorithm, which of the following steps are part of its iterative procedure?", "options": ["A) A VEM algorithm is applied to maximize the lower bound with respect to q(·) and the model parameters.", "B) A greedy search strategy is used to maximize the lower bound with respect to Y and X.", "C) The algorithm stops when the Euclidean distance between two consecutive values of the lower bound is zero.", "D) The algorithm involves estimating the values of Q, L, and K during each iteration."], "category": "CORRECT", "question_context_used": "The C-VEM algorithm involves two main steps: applying a VEM algorithm to maximize the lower bound with respect to q(·) and the model parameters, and using a greedy search strategy to maximize the lower bound with respect to Y and X. Convergence is assumed when the Euclidean distance between two consecutive values of the lower bound is smaller than a positive value ε. The values of Q, L, and K are assumed to be known and not estimated during the procedure.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **A) Correct**: Nonparametric methods are valued for their flexibility and minimal assumptions about the functional form of the relationship between variables.\n- **B) Incorrect**: While nonparametric methods can be more flexible, they do not always provide more accurate predictions than parametric methods; the accuracy depends on the data and context.\n- **C) Correct**: One of the advantages of nonparametric additive models is that they allow for the separate examination of each explanatory variable's role in modeling the response.\n- **D) Incorrect**: Nonparametric methods, especially those involving smooth functions, often require iterative procedures for model fitting, as mentioned in the context.\n\n**Final Answer**: The final answer is $\boxed{{A, C}}$", "source": ["Semi-parametric-nonlinear-regression-and-transf_20.md"], "background_text": "Nonparametric methods in regression modeling are flexible as they make minimal assumptions about the form of the relationship between variables. They retain important features like additivity in explanatory variables effects and allow for separate examination of each variable's role. However, they may require iterative procedures for fitting.", "folder": "", "qid": "statistic-mcqm-5064", "question": "What are the advantages of using nonparametric methods in regression modeling?", "options": ["A) They make minimal assumptions about the form of the relationship between response and predictor variables.", "B) They always provide more accurate predictions than parametric methods.", "C) They allow for the separate examination of the roles of explanatory variables in modeling the response.", "D) They require no iterative procedures for model fitting."], "category": "CORRECT", "question_context_used": "Nonparametric methods in regression modeling are flexible as they make minimal assumptions about the form of the relationship between variables. They retain important features like additivity in explanatory variables effects and allow for separate examination of each variable's role. However, they may require iterative procedures for fitting.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The theorem specifies that $S^{\\prime}(k)$ and $S^{\\prime}(k+s)$ must be linked groups.\n- **B)** Correct. The number of segments in $S^{\\prime}(k+s)$ must be larger than in $S^{\\prime}(k)$ for the theorem to apply.\n- **C)** Correct. The proof discusses the scenario where a segment in $S^{\\prime}(k+s)$ overlaps with more than two segments in $S^{\\prime}(k)$, which is part of the conditions leading to the conclusion.\n- **D)** Incorrect. The inequality $f(\\bigcup B) \\leqslant f(\\bigcup A)$ is considered in the proof but is not a condition for the theorem's conclusion about the absence of a double $\\pmb{B}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Maximum_likelihood_estimation_of_multiple_change_points.md"], "background_text": "Theorem 5 states that if $S^{\\prime}(k)$ and $S^{\\prime}(k+s)$ are linked groups and the number of segments in $S^{\\prime}(k+s)$ is larger than that in $S^{\\prime}(k)$, then the $\\pmb{A}$ and $\\pmb{B}$ sequence will not have a double $\\pmb{B}$. The proof involves the condition where a segment in $S^{\\prime}(k+s)$ overlaps with more than two segments in $S^{\\prime}(k)$ and the inequality $f(\\bigcup B) \\leqslant f(\\bigcup A)$.", "folder": "", "qid": "statistic-mcqm-5179", "question": "According to Theorem 5, which of the following conditions prevent the $\\pmb{A}$ and $\\pmb{B}$ sequence from having a double $\\pmb{B}$?", "options": ["A) $S^{\\prime}(k)$ and $S^{\\prime}(k+s)$ are linked groups.", "B) The number of segments in $S^{\\prime}(k+s)$ is larger than that in $S^{\\prime}(k)$.", "C) There is one segment in $S^{\\prime}(k+s)$ that overlaps with more than two segments in $S^{\\prime}(k)$.", "D) The inequality $f(\\bigcup B) \\leqslant f(\\bigcup A)$ holds in the block."], "category": "PARTIALLY_CORRECT", "question_context_used": "Theorem 5 states that if $S^{\\prime}(k)$ and $S^{\\prime}(k+s)$ are linked groups and the number of segments in $S^{\\prime}(k+s)$ is larger than that in $S^{\\prime}(k)$, then the $\\pmb{A}$ and $\\pmb{B}$ sequence will not have a double $\\pmb{B}$. The proof involves the condition where a segment in $S^{\\prime}(k+s)$ overlaps with more than two segments in $S^{\\prime}(k)$ and the inequality $f(\\bigcup B) \\leqslant f(\\bigcup A)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A)** Correct. $P$-splines indeed use a generous set of equidistant knots combined with a smoothness penalty.\n- **B)** Correct. The penalty matrix is constructed to enforce smoothness through squared differences of adjacent coefficients.\n- **C)** Correct. The approximation's form depends on the knots' number and location, though $P$-splines mitigate this by using many knots.\n- **D)** Incorrect. $P$-splines can be extended to bivariate smoothing, as mentioned in the context.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Geoadditive-expectile-regression_2012_Computationa.md"], "background_text": "$P$-splines employ a generous set of $K$ equidistant knots (usually 20–40) in combination with a smoothness penalty augmented to the fitting criterion. The penalty matrix ${\\pmb K}^{(d)}={\\pmb D}^{(d)^{\\prime}}{\\pmb D}^{(d)}$ yields a penalty composed of squared $d$th-order differences in the sequence of basis coefficients. The idea of $P$-splines can be extended to smoothing bivariate surfaces.", "folder": "", "qid": "statistic-mcqm-4015", "question": "Which of the following statements about $P$-splines are correct?", "options": ["A) $P$-splines employ a generous set of $K$ equidistant knots (usually 20–40) in combination with a smoothness penalty.", "B) The penalty matrix ${\\pmb K}^{(d)}={\\pmb D}^{(d)^{\\prime}}{\\pmb D}^{(d)}$ yields a penalty composed of squared $d$th-order differences in the sequence of basis coefficients.", "C) The form of the polynomial spline approximation crucially depends on the number and location of the knots.", "D) $P$-splines can only be applied to univariate smoothing."], "category": "PARTIALLY_CORRECT", "question_context_used": "$P$-splines employ a generous set of $K$ equidistant knots (usually 20–40) in combination with a smoothness penalty augmented to the fitting criterion. The penalty matrix ${\\pmb K}^{(d)}={\\pmb D}^{(d)^{\\prime}}{\\pmb D}^{(d)}$ yields a penalty composed of squared $d$th-order differences in the sequence of basis coefficients. The idea of $P$-splines can be extended to smoothing bivariate surfaces.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A) Correct**: $E_{N} = N/2$ correctly represents the mean of the statistic.\n- **B) Correct**: $D_{N} = \\sqrt{(N/4)}$ correctly represents the standard deviation of the statistic.\n- **C) Correct**: The coefficients $A_{0}, A_{1}, A_{2}$ are indeed computed to minimize the maximum absolute difference.\n- **D) Incorrect**: $C_{S}$ are the critical values for a given probability, but they are not directly used in the approximation formula; instead, the formula aims to approximate these values.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Critical Values of the Sign Test.md"], "background_text": "The approximation for critical values of the sign test is given by $A_{0}+A_{1}E_{N}+A_{2}D_{N}$, where $E_{N}=N/2$ are the means of the statistic, $D_{N}=\\sqrt{(N/4)}$ the standard deviations, and $A_{0}, A_{1}, A_{2}$ are coefficients computed to minimize the maximum absolute difference between the approximation and a critical value.", "folder": "", "qid": "statistic-mcqm-5322", "question": "In the approximation of critical values for the sign test, which of the following components are used in the formula $A_{0}+A_{1}E_{N}+A_{2}D_{N}$?", "options": ["A) $E_{N} = N/2$, representing the mean of the statistic.", "B) $D_{N} = \\sqrt{(N/4)}$, representing the standard deviation of the statistic.", "C) $A_{0}, A_{1}, A_{2}$ are coefficients computed to minimize the maximum absolute difference.", "D) $C_{S}$, the critical values for a given probability, are directly used in the formula."], "category": "CORRECT", "question_context_used": "The approximation for critical values of the sign test is given by $A_{0}+A_{1}E_{N}+A_{2}D_{N}$, where $E_{N}=N/2$ are the means of the statistic, $D_{N}=\\sqrt{(N/4)}$ the standard deviations, and $A_{0}, A_{1}, A_{2}$ are coefficients computed to minimize the maximum absolute difference between the approximation and a critical value.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **A) Correct**: As per the context, when $c_h = 1$ for all $h$, $b_0 = 1$.\n- **B) Correct**: Similarly, $b_1 = 1/3$ under the same conditions.\n- **C) Incorrect**: $b_0 = 1$ and $b_1 = 1/3$ are not equal.\n- **D) Incorrect**: $b_0 = 1$ and $b_1 = 1/3$ do not satisfy $b_0 = 2b_1$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Design_of_order-of-addition_experiments.md"], "background_text": "Under the usual pairwise order model with $c_h = 1$ for all $h$, the expressions for $b_0$ and $b_1$ simplify to $b_0 = 1$ and $b_1 = 1/3$.", "folder": "", "qid": "statistic-mcqm-6092", "question": "Under the usual pairwise order model with $c_h = 1$ for all $h$, which of the following statements about $b_0$ and $b_1$ are correct?", "options": ["A) $b_0 = 1$", "B) $b_1 = 1/3$", "C) $b_0 = b_1$", "D) $b_0 = 2b_1$"], "category": "PARTIALLY_CORRECT", "question_context_used": "Under the usual pairwise order model with $c_h = 1$ for all $h$, the expressions for $b_0$ and $b_1$ simplify to $b_0 = 1$ and $b_1 = 1/3$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A)** Correct. Identifiability requires that the cumulative distribution functions of the elements are linearly independent.\n- **B)** Incorrect. The condition $D\\geqslant2G$ is mentioned for mixtures of Markov chains, not necessarily for Markov renewal processes.\n- **C)** Correct. All initialization probabilities $\\alpha_{l}^{g}$ and transition probabilities $P_{l j}^{g}$ must be strictly positive for identifiability.\n- **D)** Correct. For identifiability, it is necessary that for any two subpopulations, there exists at least one pair $(l,j)$ with different sojourn time distribution parameters.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Estimating Finite Mixtures of Semi-Markov Chains A.md"], "background_text": "A finite mixture of Markov renewal processes is identifiable if the cumulative distribution functions of the elements are linearly independent. Additional conditions include all initialization and transition probabilities being strictly positive, and for any two subpopulations, there exists at least one pair of states with different sojourn time distribution parameters.", "folder": "", "qid": "statistic-mcqm-5734", "question": "Under what conditions is a finite mixture of Markov renewal processes identifiable?", "options": ["A) The cumulative distribution functions of the elements are linearly independent.", "B) The number of mixture components $G$ is less than or equal to half the number of states $D$.", "C) All initialization probabilities $\\alpha_{l}^{g}$ and transition probabilities $P_{l j}^{g}$ are strictly positive.", "D) For any two subpopulations $g$ and $g^{\\prime}$, there exists at least one pair $(l,j)$ such that $\\mathbf{T}_{l j}^{g}\\neq\\mathbf{T}_{l j}^{g^{\\prime}}$."], "category": "CORRECT", "question_context_used": "A finite mixture of Markov renewal processes is identifiable if the cumulative distribution functions of the elements are linearly independent. Additional conditions include all initialization and transition probabilities being strictly positive, and for any two subpopulations, there exists at least one pair of states with different sojourn time distribution parameters.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B\n\n- **Option A**: Incorrect. The text does not mention any continuity requirement for $f$.\n- **Option B**: Correct. The text specifies that $f$ must be $\\Lambda$-integrable.\n- **Option C**: Incorrect. While the support of $f$ is mentioned in the context of the relation between $\\nu_{0}$ and $\\nu_{1}$, there is no requirement for $f$ to have compact support for $X(t)$ to be defined.\n- **Option D**: Incorrect. Differentiability of $f$ is not mentioned as a requirement.\n\n**Final Answer**: The final answer is $\\boxed{{B}}$", "source": ["Scandinavian J Statistics - 2021 - Glück - A solut.md"], "background_text": "For the infinitely divisible moving average random field $X(t) = \\int_{\\mathbb{R}^{d}} f(t - x) \\Lambda(dx)$ to be defined, the function $f$ must be $\\Lambda$-integrable.", "folder": "", "qid": "statistic-mcqm-5328", "question": "What conditions must the function $f: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}$ satisfy for $X(t) = \\int_{\\mathbb{R}^{d}} f(t - x) \\Lambda(dx)$ to be defined?", "options": ["A) $f$ must be continuous.", "B) $f$ must be $\\Lambda$-integrable.", "C) $f$ must have compact support.", "D) $f$ must be differentiable."], "category": "CORRECT", "question_context_used": "For the infinitely divisible moving average random field $X(t) = \\int_{\\mathbb{R}^{d}} f(t - x) \\Lambda(dx)$ to be defined, the function $f$ must be $\\Lambda$-integrable.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\"]\n}", "model_answer": ["B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: The text explicitly mentions that the squared $L^{2}$ distance is chosen for its computational advantages and analytical tractability.\n- **B) Correct**: The text states that the distance should capture the particular discrepancies between observations that are relevant for clustering purposes.\n- **C) Incorrect**: The text mentions that in practice, density forecasts are unknown, so the distances cannot be directly computed, making this option incorrect.\n- **D) Incorrect**: The text mentions that other measures between densities can be used, such as $L^{1}$ distance or Kullback–Leibler discrepancy, making this option incorrect.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["Time-series-clustering-based-on-forecas_2006_Compu.md"], "background_text": "The squared $L^{2}$ distance is chosen for clustering time series based on their full forecast densities due to its computational advantages and analytical tractability. It captures the particular discrepancies between observations that are relevant for clustering purposes. However, in practice, density forecasts are unknown, so the distances cannot be directly computed and must be approximated from the data.", "folder": "", "qid": "statistic-mcqm-3794", "question": "Which of the following are correct about the squared $L^{2}$ distance used in the clustering of time series based on their full forecast densities?", "options": ["A) It is chosen for its computational advantages and analytical tractability.", "B) It captures the discrepancies between observations relevant for clustering purposes.", "C) It requires the exact knowledge of the density forecasts to be computed directly.", "D) It is the only possible measure between densities that can be used for clustering."], "category": "PARTIALLY_CORRECT", "question_context_used": "The squared $L^{2}$ distance is chosen for clustering time series based on their full forecast densities due to its computational advantages and analytical tractability. It captures the particular discrepancies between observations that are relevant for clustering purposes. However, in practice, density forecasts are unknown, so the distances cannot be directly computed and must be approximated from the data.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: The text states that $\\kappa(t,\\omega)$ is obtained by integrating $m(t+\\omega)$ with respect to $t$, resulting in $M(t+\\omega)+c$.\n- **B) Correct**: The constant $c$ is determined by setting $\\kappa=0$ in the equation, which gives $c=-M(\\omega)$.\n- **C) Correct**: The text explicitly mentions that the distribution with the given cumulant generating function is of exponential-type.\n- **D) Incorrect**: The function $M(t+\\omega)$ is the antiderivative of $m(t+\\omega)$ with respect to $t$, not $\\omega$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["On_the_amount_of_information_supplied_by_truncated_samples_of_grouped_observations_in_the_estimation_of_the_parameters_of_normal_populations.md"], "background_text": "The cumulant generating function $\\kappa(t,\\omega)$ is given by $\\kappa(t,\\omega)=M(t+\\omega)-M(\\omega)$, where $M$ is the antiderivative of $m$. The constant $c$ is found by setting $\\kappa=0$, leading to $c=-M(\\omega)$. The distribution with this cumulant generating function is of exponential-type.", "folder": "", "qid": "statistic-mcqm-2899", "question": "Given the cumulant generating function $\\kappa(t,\\omega)=M(t+\\omega)-M(\\omega)$, which of the following statements are correct?", "options": ["A) The function $\\kappa(t,\\omega)$ is derived by integrating $m(t+\\omega)$ with respect to $t$.", "B) The constant $c$ in the expression $\\kappa(t,\\omega)=\\int_{\\mathbf{\\Omega}}m(t+\\omega)d t+c$ is determined by setting $\\kappa=0$ and solving for $c$.", "C) The distribution with cumulant generating function $\\kappa(t,\\omega)$ is of exponential-type.", "D) The function $M(t+\\omega)$ is the antiderivative of $m(t+\\omega)$ with respect to $\\omega$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The cumulant generating function $\\kappa(t,\\omega)$ is given by $\\kappa(t,\\omega)=M(t+\\omega)-M(\\omega)$, where $M$ is the antiderivative of $m$. The constant $c$ is found by setting $\\kappa=0$, leading to $c=-M(\\omega)$. The distribution with this cumulant generating function is of exponential-type.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A)** Correct. The measurability of $\\pmb{u}$ is derived from the measurability of each $\\pmb{u}_{\\alpha}$ under the given conditions.\n- **B)** Correct. This is a direct consequence of the proof provided in the context, showing that $\\pmb{u}^{-1}\\langle Y \\rangle$ is in the specified subset of $\\mathfrak{M}$.\n- **C)** Correct. The context explicitly states that the restriction of $\\pmb{u}$ to $f_{\\alpha}\\langle E_{\\alpha}\\rangle$ has the specified measurability for each $\\alpha \\in I$.\n- **D)** Incorrect. The context does not state that the measurability of $\\pmb{u}$ is conditional upon $\\mathfrak{M}$ and $\\mathfrak{N}$ being direct limits in the traditional sense; it discusses pseudo $\\sigma$-algebras.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Some-remarks-on-the-direct-limits-of-measure_1974_.md"], "background_text": "The direct limit mapping $\\pmb{u} = \\underline{\\operatorname*{lim}} \\pmb{u}_{\\alpha}$ is considered under the conditions of pseudo $\\sigma$-algebras $\\mathfrak{M} = \\underline{\\mathrm{lim}} \\mathfrak{M}_{\\alpha}$ and $\\mathfrak{N} = \\underline{\\mathrm{lim}} \\mathfrak{N}_{\\alpha}$. The measurability of $\\pmb{u}$ is discussed in terms of the measurability of its components $\\pmb{u}_{\\alpha}$ and the structure of the pseudo $\\sigma$-algebras.", "folder": "", "qid": "statistic-mcqm-4368", "question": "Given a direct limit mapping $\\pmb{u} = \\underline{\\operatorname*{lim}} \\pmb{u}_{\\alpha}$ and a pseudo $\\sigma$-algebra $\\mathfrak{N} = \\bigcup_{\\alpha \\in I} \\hat{\\theta}_{\\mathfrak{N}_{\\alpha}}\\langle\\mathfrak{N}_{\\alpha}\\rangle$, which of the following statements are true regarding the measurability of $\\pmb{u}$?", "options": ["A) $\\pmb{u}$ is $(\\mathfrak{M}, \\mathfrak{N})$ measurable if $\\pmb{u}_{\\alpha}$ is $(\\mathfrak{M}_{\\alpha}, \\mathfrak{N}_{\\alpha})$ measurable for each $\\alpha \\in I$.", "B) $\\pmb{u}^{-1}\\langle Y \\rangle \\in \\hat{\\Psi}_{\\mathfrak{M}_{\\alpha}}\\langle\\mathfrak{M}_{\\alpha}\\rangle \\subset \\mathfrak{M}$ for each $Y \\in \\mathfrak{N}$.", "C) The restriction of $\\pmb{u}$ to $f_{\\alpha}\\langle E_{\\alpha}\\rangle$ is $(\\hat{\\Psi}_{\\mathfrak{M}_{\\alpha}}\\langle\\mathfrak{M}_{\\alpha}\\rangle, \\hat{\\theta}_{\\mathfrak{M}_{\\alpha}}\\langle\\mathfrak{M}_{\\alpha}\\rangle)$ measurable for each $\\alpha \\in I$.", "D) $\\pmb{u}$ is $(\\mathfrak{M}, \\mathfrak{N})$ measurable only if $\\mathfrak{M} = \\varinjlim \\mathfrak{M}_{\\alpha}$ and $\\mathfrak{N} = \\varinjlim \\mathfrak{N}_{\\alpha}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The direct limit mapping $\\pmb{u} = \\underline{\\operatorname*{lim}} \\pmb{u}_{\\alpha}$ is considered under the conditions of pseudo $\\sigma$-algebras $\\mathfrak{M} = \\underline{\\mathrm{lim}} \\mathfrak{M}_{\\alpha}$ and $\\mathfrak{N} = \\underline{\\mathrm{lim}} \\mathfrak{N}_{\\alpha}$. The measurability of $\\pmb{u}$ is discussed in terms of the measurability of its components $\\pmb{u}_{\\alpha}$ and the structure of the pseudo $\\sigma$-algebras.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**:\n- **A) Correct**: The text states that a perfect fit would be achieved if the error $e_k$ had the same variance as the variance within the group, $\\sigma_k^2$.\n- **B) Correct**: The probability of a well-fitted set of observations is given as proportional to $\\exp(-\\frac{1}{2}\\chi_1^2)$ in the context.\n- **C) Correct**: Pearson noted that the distribution of $X$ is markedly leptokurtic for small $w_k$ in the outlying groups.\n- **D) Correct**: Pearson criticized Slutsky's derivation from the viewpoint of $p$-variate sampling, as mentioned in the text.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Studies_in_the_History_of_Probability_and_Statistics._XV_The_historical_development_of_the_Gauss_linear_model.md"], "background_text": "Slutsky's (1914) attempt to test the goodness of fit of a regression equation involved comparing the error $e_k$ to the variance within the group, $\\sigma_k^2$. The probability of a well-fitted set of observations was considered proportional to $\\exp(-\\frac{1}{2}\\chi_1^2)$. Pearson criticized this approach, noting that the distribution of $X$ is markedly leptokurtic for small $w_k$ in outlying groups.", "folder": "", "qid": "statistic-mcqm-2591", "question": "Regarding Slutsky's attempt to test the goodness of fit of a regression equation, which of the following are correct?", "options": ["A) A perfect fit is achieved if the error $e_k$ has the same variance as the variance within the group, $\\sigma_k^2$.", "B) The probability of a well-fitted set of observations is proportional to $\\exp(-\\frac{1}{2}\\chi_1^2)$.", "C) The distribution of $X$ is markedly leptokurtic for small $w_k$ in the outlying groups.", "D) Pearson criticized Slutsky's derivation from the viewpoint of $p$-variate sampling."], "category": "PARTIALLY_CORRECT", "question_context_used": "Slutsky's (1914) attempt to test the goodness of fit of a regression equation involved comparing the error $e_k$ to the variance within the group, $\\sigma_k^2$. The probability of a well-fitted set of observations was considered proportional to $\\exp(-\\frac{1}{2}\\chi_1^2)$. Pearson criticized this approach, noting that the distribution of $X$ is markedly leptokurtic for small $w_k$ in outlying groups.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}", "model_answer": ["B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **A) Correct**: The 'curse of dimensionality' indeed refers to the challenges in analyzing relationships when the number of explanatory variables is large.\n- **B) Correct**: Dimension reduction techniques aim to mitigate this issue by reducing the number of variables while preserving as much predictive information as possible.\n- **C) Incorrect**: The curse of dimensionality typically implies that as the number of variables increases, the required sample size increases, not decreases.\n- **D) Incorrect**: The curse of dimensionality directly impacts the performance and feasibility of regression and classification analyses by complicating the relationship study.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["Robust-dimension-reduction-based-on-canonical_2009.md"], "background_text": "The 'curse of dimensionality' makes studying the relationship between a response variable and a large number of explanatory variables challenging. Dimension reduction aims to simplify this by reducing the number of variables while capturing predictive information, thus alleviating the curse and enabling parsimonious statistical analyses.", "folder": "", "qid": "statistic-mcqm-5008", "question": "Which of the following statements are true about the 'curse of dimensionality' in the context of statistical analysis?", "options": ["A) It refers to the difficulty in studying the relationship between a response variable and a large number of explanatory variables.", "B) It can be alleviated by reducing the number of explanatory variables while retaining predictive information.", "C) It implies that as the number of explanatory variables increases, the required sample size decreases.", "D) It is unrelated to the performance of regression and classification analyses."], "category": "CORRECT", "question_context_used": "The 'curse of dimensionality' makes studying the relationship between a response variable and a large number of explanatory variables challenging. Dimension reduction aims to simplify this by reducing the number of variables while capturing predictive information, thus alleviating the curse and enabling parsimonious statistical analyses.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **A)** Correct. The EM algorithm can converge to local optima or minima, which may not be the global optimum.\n- **B)** Incorrect. The true model $f(\\mathbf{x})$ is usually unknown, and the EM algorithm is used to estimate the parameters of an assumed model.\n- **C)** Correct. The choice of initialization (e.g., random, $k$-means) can greatly influence the final estimates due to the algorithm's sensitivity to starting points.\n- **D)** Incorrect. The EM algorithm does not guarantee convergence to the global maximum likelihood estimates; it may converge to local maxima.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Addressing-overfitting-and-underfitting-in-Gau_201.md"], "background_text": "The EM algorithm is commonly used for fitting finite mixture models but suffers from drawbacks such as converging to local optima and sensitivity to initialization. The true model $f(\\mathbf{x})$ is generally unknown in practice.", "folder": "", "qid": "statistic-mcqm-5083", "question": "Which of the following are challenges associated with the Expectation-Maximization (EM) algorithm when fitting finite mixture models?", "options": ["A) The EM algorithm can converge to local optima or local minima.", "B) The EM algorithm requires the true model $f(\\mathbf{x})$ to be known in advance.", "C) Initialization of the EM algorithm can significantly affect the final parameter estimates.", "D) The EM algorithm always guarantees convergence to the global maximum likelihood estimates."], "category": "CORRECT", "question_context_used": "The EM algorithm is commonly used for fitting finite mixture models but suffers from drawbacks such as converging to local optima and sensitivity to initialization. The true model $f(\\mathbf{x})$ is generally unknown in practice.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "Substituting the given values into the formula: $A = 4(100) - 3(20) + 2 - 10 = 400 - 60 + 2 - 10 = 332$. However, none of the options match this calculation, indicating a possible error in the options provided.\n\n**Correct Choices**: None of the options are correct based on the given formula and values.\n\n**Final Answer**: The final answer is $\\boxed{{}}$", "source": ["SPREAD_OF_DISEASES_IN_A_RECTANGULAR_PLANTATION_WITH_VACANCIES.md"], "background_text": "In the queen case, the total number of joins of existent points on the lattice is calculated using the formula $A = 4b - 3a + 2 - V$, where $b$ is the total number of existent points, $a$ is the sum of the lattice dimensions, and $V$ is a correction for vacancies.", "folder": "", "qid": "statistic-mcqm-3501", "question": "In the queen case, the total number of joins of existent points on the lattice is given by $A = 4b - 3a + 2 - V$. If $b = 100$, $a = 20$, and $V = 10$, what is the value of $A$?", "options": ["A) $A = 342$", "B) $A = 352$", "C) $A = 362$", "D) $A = 372$"], "category": "INCORRECT", "question_context_used": "In the queen case, the total number of joins of existent points on the lattice is calculated using the formula $A = 4b - 3a + 2 - V$, where $b$ is the total number of existent points, $a$ is the sum of the lattice dimensions, and $V$ is a correction for vacancies.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\"]\n}", "model_answer": ["B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The text explicitly states that the estimation approach for $\\nu_{0}$ is plug-in and does not differ between low and high frequency data.\n- **B)** Correct. The text mentions that high-frequency estimators of $\\nu_{1}$ can be used to slightly improve their rates of convergence to $\\nu_{1}$.\n- **C)** Correct. The text indicates that the difference in estimation between low and high frequency data first appears at the level of estimation of the Lévy density $\\nu_{1}$.\n- **D)** Correct. The text describes the estimation approach as involving solving an integral equation for $w$ where $w = u\\nu_{0}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Scandinavian J Statistics - 2021 - Glück - A solut.md"], "background_text": "The estimation approach for $\\nu_{0}$ is plug-in and does not differ between low and high frequency data. The difference appears first on the level of estimation of the Lévy density $\\nu_{1}$. High-frequency estimators of $\\nu_{1}$ can be used to improve their rates of convergence to $\\nu_{1}$ and, hence, the final performance of the estimate of $\\nu_{0}$. The estimation approach involves solving an integral equation for $w$ where $w = u\\nu_{0}$.", "folder": "", "qid": "statistic-mcqm-5236", "question": "Which of the following statements are true regarding the estimation approach for $\\nu_{0}$ as described in the context?", "options": ["A) The estimation approach for $\\nu_{0}$ is plug-in and does not differ between low and high frequency data.", "B) High-frequency estimators of $\\nu_{1}$ can be used to improve the rates of convergence to $\\nu_{1}$.", "C) The difference in estimation between low and high frequency data first appears at the level of estimation of the Lévy density $\\nu_{1}$.", "D) The estimation approach for $\\nu_{0}$ requires solving an integral equation for $w$ where $w = u\\nu_{0}$."], "category": "CORRECT", "question_context_used": "The estimation approach for $\\nu_{0}$ is plug-in and does not differ between low and high frequency data. The difference appears first on the level of estimation of the Lévy density $\\nu_{1}$. High-frequency estimators of $\\nu_{1}$ can be used to improve their rates of convergence to $\\nu_{1}$ and, hence, the final performance of the estimate of $\\nu_{0}$. The estimation approach involves solving an integral equation for $w$ where $w = u\\nu_{0}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The table shows that for $n=10$ and $p=0.1$, the observed mean variance is 0.0729.\n- **B) Correct**: For $n=30$, the theoretical mean variance is 0.0170.\n- **C) Incorrect**: For $n=50$, the observed mean variance is 0.00956, which is slightly higher than the theoretical value of 0.00949.\n- **D) Correct**: The mean variance decreases as $n$ increases, as seen from the table.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Tests_for_rank_correlation_coefficients__III._Distribution_of_the_transformed_Kendall_coefficient_.md"], "background_text": "The table provides the mean variance of $z_K$ for different values of $n$ (10, 30, 50) and $p$ (0.1 to 0.8). The observed and theoretical values are given, showing that the mean variance decreases as $n$ increases.", "folder": "", "qid": "statistic-mcqm-4419", "question": "Based on the table showing the mean variance of $z_K$ for different values of $n$ and $p$, which of the following statements are correct?", "options": ["A) For $n=10$ and $p=0.1$, the observed mean variance is 0.0729.", "B) For $n=30$, the theoretical mean variance is 0.0170.", "C) For $n=50$, the observed mean variance is less than the theoretical mean variance.", "D) The mean variance decreases as $n$ increases."], "category": "PARTIALLY_CORRECT", "question_context_used": "The table provides the mean variance of $z_K$ for different values of $n$ (10, 30, 50) and $p$ (0.1 to 0.8). The observed and theoretical values are given, showing that the mean variance decreases as $n$ increases.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The text explicitly states that Wald's sequential probability ratio test minimizes $F_{2}$ under the specified conditions.\n- **B)** Correct. The text mentions that for $\\alpha=0.05$, the minimum value of $F_{2}$ is $49.0$, which represents a saving of more than $50\\%$ on the fixed sample test.\n- **C)** Incorrect. The text indicates that a large part of the savings can be achieved with as few as 5 or 10 interim analyses, not at least 20.\n- **D)** Correct. The text specifies that the last few percent of these savings depend critically on a high maximum sample size.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["An_improved_method_for_deriving_optimal_one-sided_group_sequential_tests.md"], "background_text": "The minimization of $F_{2}$ is achieved by Wald's sequential probability ratio test with fully sequential monitoring and an unbounded maximum sample size. For $\\alpha=0.05$, the minimum is $49.0$, representing a saving of more than $50\\%$ on the fixed sample test. Achieving a large part of these savings is possible with as few as 5 or 10 interim analyses, but the last few percent depend critically on a high maximum sample size.", "folder": "", "qid": "statistic-mcqm-5555", "question": "Which of the following statements are true regarding the minimization of $F_{2}$ as described in the context?", "options": ["A) Wald's sequential probability ratio test minimizes $F_{2}$ with fully sequential monitoring and an unbounded maximum sample size.", "B) For $\\alpha=0.05$, the minimum value of $F_{2}$ is $49.0$, representing a saving of more than $50\\%$ on the fixed sample test.", "C) Achieving a large part of these savings requires at least 20 interim analyses.", "D) The last few percent of these savings depend critically on a high maximum sample size."], "category": "CORRECT", "question_context_used": "The minimization of $F_{2}$ is achieved by Wald's sequential probability ratio test with fully sequential monitoring and an unbounded maximum sample size. For $\\alpha=0.05$, the minimum is $49.0$, representing a saving of more than $50\\%$ on the fixed sample test. Achieving a large part of these savings is possible with as few as 5 or 10 interim analyses, but the last few percent depend critically on a high maximum sample size.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "To determine the correct options, let's analyze each one:\n\n- **A)** This describes a drift condition for polynomial ergodicity, not geometric ergodicity. For geometric ergodicity, the condition should involve $\\alpha=1$.\n- **B)** $\\psi$-irreducibility and aperiodicity are fundamental properties required for any form of ergodicity, including geometric ergodicity.\n- **C)** The existence of an invariant distribution $\\pi$ is necessary for the chain to converge to $\\pi$.\n- **D)** This is the definition of geometric ergodicity, requiring the convergence to be at a geometric rate.\n\n**Correct Choices**: B, C, D\n\n**Final Answer**: The final answer is $\\boxed{{B, C, D}}$", "source": ["Scandinavian J Statistics - 2007 - JARNER - Conver.md"], "background_text": "A Markov chain is geometrically ergodic if it converges to its invariant distribution at a geometric rate. This requires the chain to be $\\psi$-irreducible, aperiodic, and to satisfy a drift condition with a geometric rate of convergence.", "folder": "", "qid": "statistic-mcqm-5419", "question": "Which of the following conditions are necessary for a Markov chain to be geometrically ergodic?", "options": ["A) There exists a function $V\\geq1$ and constants $0<c, b<\\infty$ such that $P V\\leq V-c V^{\\alpha}+b1_{C}$ for some $0\\leq\\alpha<1$ and a small set $C$.", "B) The Markov chain is $\\psi$-irreducible and aperiodic.", "C) The Markov chain admits an invariant distribution $\\pi$.", "D) There exists $\\rho>1$ such that $\\rho^{n}\\|P^{n}(x,\\cdot)-\\pi\\|\\to0$ as $n\\to\\infty$ for all $x$."], "category": "PARTIALLY_CORRECT", "question_context_used": "A Markov chain is geometrically ergodic if it converges to its invariant distribution at a geometric rate. This requires the chain to be $\\psi$-irreducible, aperiodic, and to satisfy a drift condition with a geometric rate of convergence.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\"answer\": [\"A\", \"B\"]}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The formula for $\\delta_{2}^{2}$ is accurately presented as $\\frac{1}{n-2}\\sum_{i=1}^{n-2}(x_{i}-2x_{i+1}+x_{i+2})^{2}$.\n- **B)** Correct. The formula for $d_{2}$ is accurately presented as $\\frac{1}{n-2}\\sum_{i=1}^{n-2}\\left|x_{i}-2x_{i+1}+x_{i+2}\\right|$.\n- **C)** Correct. The asymptotic efficiencies of $\\delta_{2}^{2}$ and $d_{2}$ are correctly stated as 51.4% and 47.1%, respectively.\n- **D)** Incorrect. While these estimators are valuable in the presence of a trend that can be locally represented by a straight line or a parabola with moderate curvature, they are not more efficient than the usual estimator $\\vartheta^{2}$ under normal and independent distribution conditions.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["DISTRIBUTION_THEORY_OF_TWO_ESTIMATES_FOR_STANDARD_DEVIATION_BASED_ON_SECOND_VARIATE_DIFFERENCES.md"], "background_text": "The mean square successive second difference ($\\delta_{2}^{2}$) and the mean successive second difference ($d_{2}$) are estimators based on the second variate difference. $\\delta_{2}^{2}$ is calculated as $\\frac{1}{n-2}\\sum_{i=1}^{n-2}(x_{i}-2x_{i+1}+x_{i+2})^{2}$, and $d_{2}$ is calculated as $\\frac{1}{n-2}\\sum_{i=1}^{n-2}\\left|x_{i}-2x_{i+1}+x_{i+2}\\right|$. Their asymptotic efficiencies are 51.4% and 47.1%, respectively. These estimators are valuable when the trend function may be locally represented by a straight line or a parabola with moderate curvature, as they are free of the heavy bias that affects the usual estimator $\\vartheta^{2}$.", "folder": "", "qid": "statistic-mcqm-2940", "question": "Which of the following are correct about the mean square successive second difference ($\\delta_{2}^{2}$) and the mean successive second difference ($d_{2}$)?", "options": ["A) $\\delta_{2}^{2}$ is calculated as $\\frac{1}{n-2}\\sum_{i=1}^{n-2}(x_{i}-2x_{i+1}+x_{i+2})^{2}$.", "B) $d_{2}$ is calculated as $\\frac{1}{n-2}\\sum_{i=1}^{n-2}\\left|x_{i}-2x_{i+1}+x_{i+2}\\right|$.", "C) The asymptotic efficiency of $\\delta_{2}^{2}$ is 51.4% and that of $d_{2}$ is 47.1%.", "D) These estimators are more efficient than the usual estimator $\\vartheta^{2}$ when the trend function may be locally represented by a straight line or a parabola with moderate curvature."], "category": "INCORRECT", "question_context_used": "The mean square successive second difference ($\\delta_{2}^{2}$) and the mean successive second difference ($d_{2}$) are estimators based on the second variate difference. $\\delta_{2}^{2}$ is calculated as $\\frac{1}{n-2}\\sum_{i=1}^{n-2}(x_{i}-2x_{i+1}+x_{i+2})^{2}$, and $d_{2}$ is calculated as $\\frac{1}{n-2}\\sum_{i=1}^{n-2}\\left|x_{i}-2x_{i+1}+x_{i+2}\\right|$. Their asymptotic efficiencies are 51.4% and 47.1%, respectively. These estimators are valuable when the trend function may be locally represented by a straight line or a parabola with moderate curvature, as they are free of the heavy bias that affects the usual estimator $\\vartheta^{2}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The function $\\alpha(w)$ must satisfy the given integrability condition for the bridge sampling identity to hold.\n- **B)** Correct. The optimal $\\alpha(w)$ is indeed given by $\\alpha_{O}(w)\\propto\\frac{1}{s_{1}q_{1}+s_{2}r q_{2}}$ for independent draws.\n- **C)** Incorrect. $\\alpha(w)$ cannot be any integrable function; it must satisfy the specific condition mentioned in option A.\n- **D)** Correct. For independent draws, $\\alpha_{O}(w)$ is the optimal choice that minimizes the asymptotic variance of $\\hat{\\lambda}_{\\alpha}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Warp_Bridge_Sampling__Warp_Bridge_Sampling.md"], "background_text": "The function $\\alpha(w)$ in bridge sampling must satisfy a specific integrability condition. The optimal choice of $\\alpha(w)$, denoted $\\alpha_{O}(w)$, is proportional to $\\frac{1}{s_{1}q_{1}+s_{2}r q_{2}}$ and minimizes the asymptotic variance of $\\hat{\\lambda}_{\\alpha}$ for independent draws.", "folder": "", "qid": "statistic-mcqm-4933", "question": "In the context of bridge sampling, which of the following are true about the function $\\alpha(w)$?", "options": ["A) $\\alpha(w)$ must satisfy $0<\\left|\\int_{\\Omega_{1}\\cap\\Omega_{2}}\\alpha(w)p_{1}(w)p_{2}(w)\\mu(d w)\\right|<\\infty$.", "B) The optimal $\\alpha(w)$ is given by $\\alpha_{O}(w)\\propto\\frac{1}{s_{1}q_{1}+s_{2}r q_{2}}$.", "C) $\\alpha(w)$ can be any function as long as it is integrable.", "D) For independent draws, $\\alpha_{O}(w)$ minimizes the asymptotic variance of $\\hat{\\lambda}_{\\alpha}$."], "category": "CORRECT", "question_context_used": "The function $\\alpha(w)$ in bridge sampling must satisfy a specific integrability condition. The optimal choice of $\\alpha(w)$, denoted $\\alpha_{O}(w)$, is proportional to $\\frac{1}{s_{1}q_{1}+s_{2}r q_{2}}$ and minimizes the asymptotic variance of $\\hat{\\lambda}_{\\alpha}$ for independent draws.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**:\n- **A)** Correct. The background text states that the reduction in the MSE for both BC estimators is approximately 9%.\n- **B)** Correct. The BC estimator $\\hat{\\lambda}_{strong}^{bc}$ is more biased in absolute value than $\\hat{\\lambda}_{weak}^{bc}$.\n- **C)** Correct. The BC estimator $\\hat{\\beta}_{0, strong}^{bc}$ has a higher MSE than $\\hat{\\beta}_{0}$.\n- **D)** Correct. There are 345 feasible $(\\pi, \\gamma)$ combinations.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["The-ability-to-correct-the-bias-in-the-stable-AD_2.md"], "background_text": "For $R^{2} = 0.8$, the reduction in the MSE for both BC estimators is approximately 9%. The BC estimator $\\hat{\\lambda}_{strong}^{bc}$ is more biased in absolute value than $\\hat{\\lambda}_{weak}^{bc}$. The BC estimator $\\hat{\\beta}_{0, strong}^{bc}$ has a higher MSE than $\\hat{\\beta}_{0}$. There are 345 feasible $(\\pi, \\gamma)$ combinations for $\\pi = -35:0.25:5$ and $\\gamma = -3:0.1:15$.", "folder": "", "qid": "statistic-mcqm-5791", "question": "In the simulation results for the AD(1, 1) model with $R^{2} = 0.8$, which of the following statements are correct?", "options": ["A) The reduction in the MSE for both BC estimators is approximately 9%.", "B) The BC estimator $\\hat{\\lambda}_{strong}^{bc}$ is more biased in absolute value than $\\hat{\\lambda}_{weak}^{bc}$.", "C) The BC estimator $\\hat{\\beta}_{0, strong}^{bc}$ has a higher MSE than $\\hat{\\beta}_{0}$.", "D) The number of feasible $(\\pi, \\gamma)$ combinations is 345."], "category": "CORRECT", "question_context_used": "For $R^{2} = 0.8$, the reduction in the MSE for both BC estimators is approximately 9%. The BC estimator $\\hat{\\lambda}_{strong}^{bc}$ is more biased in absolute value than $\\hat{\\lambda}_{weak}^{bc}$. The BC estimator $\\hat{\\beta}_{0, strong}^{bc}$ has a higher MSE than $\\hat{\\beta}_{0}$. There are 345 feasible $(\\pi, \\gamma)$ combinations for $\\pi = -35:0.25:5$ and $\\gamma = -3:0.1:15$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "To determine the correct options, let's analyze each statement based on the provided context:\n\n- **Option A**: This is correct. The text states that $e(W^{\\prime}, V^{\\prime}) = 1$ when the set of points $\\{\\theta_{1},...,\\theta_{k}\\}$ contains exactly two distinct values.\n\n- **Option B**: This is correct. The text specifies that under the location model with $F$ unimodal, $e(W^{\\prime}, V^{\\prime}) = 1$ if and only if the set of points $\\{\\theta_{1},...,\\theta_{k}\\}$ contains exactly two distinct values.\n\n- **Option C**: This is incorrect. While the text mentions that examples can be constructed with $f$ bimodal where $e(W^{\\prime}, V^{\\prime}) = 1$, it does not state that this is a general condition.\n\n- **Option D**: This is incorrect. The text mentions that examples can be constructed where the set of distribution functions contains more than two distinct unimodal distributions but $e(W^{\\prime}, V^{\\prime}) = 1$, but this is not a general condition.\n\n**Correct Choices**: A, B\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["Pairwise_versus_joint_ranking_Another_look_at_the_Kruskal-Wallis_statistic.md"], "background_text": "The approximate Bahadur efficiency $e(W^{\\prime}, V^{\\prime})$ equals one when the set of points $\\{\\theta_{1},...,\\theta_{k}\\}$ contains exactly two distinct values, assuming the location model $F_{i}(x) = F(x - \\theta_{i})$ for some absolutely continuous distribution function $F$ and that $f(x) = F^{\\prime}(x)$ is unimodal. If $f$ is allowed to be bimodal, examples can be constructed where $e(W^{\\prime}, V^{\\prime}) = 1$ even with more than two distinct values.", "folder": "", "qid": "statistic-mcqm-5491", "question": "Under what conditions does the approximate Bahadur efficiency $e(W^{\\prime}, V^{\\prime})$ equal one?", "options": ["A) When the set of points $\\{\\theta_{1},...,\\theta_{k}\\}$ contains exactly two distinct values.", "B) When the distribution function $F$ is unimodal and the set of points $\\{\\theta_{1},...,\\theta_{k}\\}$ contains exactly two distinct values.", "C) When the distribution function $F$ is bimodal.", "D) When the set of distribution functions $\\{F_{1},...,F_{k}\\}$ contains more than two distinct unimodal distributions."], "category": "PARTIALLY_CORRECT", "question_context_used": "The approximate Bahadur efficiency $e(W^{\\prime}, V^{\\prime})$ equals one when the set of points $\\{\\theta_{1},...,\\theta_{k}\\}$ contains exactly two distinct values, assuming the location model $F_{i}(x) = F(x - \\theta_{i})$ for some absolutely continuous distribution function $F$ and that $f(x) = F^{\\prime}(x)$ is unimodal. If $f$ is allowed to be bimodal, examples can be constructed where $e(W^{\\prime}, V^{\\prime}) = 1$ even with more than two distinct values.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\"]\n}", "model_answer": ["B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n- **A)** Correct because the existence of $P^{-1}$ and the finiteness of $J(h)$ rely on $P$ being strictly increasing.\n- **B)** Incorrect because the symmetry condition $P(s) = P(-s)$ is not directly related to the finiteness of $J(h)$.\n- **C)** Correct because this condition is explicitly mentioned as ensuring the finiteness of $J(h)$.\n- **D)** Incorrect because while this inequality is part of the assumptions, it is not directly tied to the finiteness of $J(h)$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$.", "source": ["Consistency-and-asymptotic-normality-for-a-nonpara.md"], "background_text": "The integral $J(h) \\equiv \\int_{P(c_{1})}^{\\infty} \\exp[-P(h P^{-1}(t))] dt < \\infty$ for all $0 < h < 1$. The inverse of $P$ exists because $P$ is strictly increasing on $[c_{1}, \\infty)$. The condition $\\int_{c_{1}}^{\\infty} \\exp(-P(s))(1 + |P^{(1)}(s)|^{2}) ds < \\infty$ is also relevant.", "folder": "", "qid": "statistic-mcqm-4896", "question": "Which of the following conditions ensure that $J(h) < \\infty$ for all $0 < h < 1$?", "options": ["A) $P$ is strictly increasing on $[c_{1}, \\infty)$.", "B) $P(s) = P(-s)$ for all $s$.", "C) $\\int_{c_{1}}^{\\infty} \\exp(-P(s))(1 + |P^{(1)}(s)|^{2}) ds < \\infty$.", "D) $P(s) \\leq c_{3}P^{\\prime}(s)$ for all $s \\geq c_{1}$."], "category": "CORRECT", "question_context_used": "The integral $J(h) \\equiv \\int_{P(c_{1})}^{\\infty} \\exp[-P(h P^{-1}(t))] dt < \\infty$ for all $0 < h < 1$. The inverse of $P$ exists because $P$ is strictly increasing on $[c_{1}, \\infty)$. The condition $\\int_{c_{1}}^{\\infty} \\exp(-P(s))(1 + |P^{(1)}(s)|^{2}) ds < \\infty$ is also relevant.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **Option A** is correct because the text explicitly states that the double shrinkage estimator $\\hat{\\beta}$ is better than single shrinkage estimators in terms of matrix risk.\n- **Option B** is correct because the text mentions that single shrinkage estimators are inadmissible due to the superiority of the double shrinkage estimator.\n- **Option C** is incorrect as it contradicts the text which states the double shrinkage estimator is better.\n- **Option D** is incorrect because the text states single shrinkage estimators are inadmissible.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Double-Shrinkage-Estimators-in-the-GMANOVA_1996_Jo.md"], "background_text": "Corollary 3.1(1) gives a solution to the shrinkage problem (2.15), shrinking $X_{1}$ by $G_{1}=G_{1}(j)$ which improves on $r_{b}=p_{1}I_{m}$ in (3.15a). Corollary 3.1(2) gives a solution to the problem (2.16) shrinking $C_{0}$ by $G_{2}=G_{2}(l)$ , which improves on $p_{1}I_{p_{2}}$ in (3.15b). However, the double shrinkage estimator $\\hat{\\beta}$ in (3.23) is better than these single shrinkage estimators in terms of matrix risk and hence these are inadmissible.", "folder": "", "qid": "statistic-mcqm-5294", "question": "Based on Corollary 3.1, which of the following statements are true regarding the shrinkage estimators?", "options": ["A) The double shrinkage estimator $\\hat{\\beta}$ is better than single shrinkage estimators in terms of matrix risk.", "B) Single shrinkage estimators are inadmissible.", "C) The double shrinkage estimator $\\hat{\\beta}$ is worse than single shrinkage estimators in terms of matrix risk.", "D) Single shrinkage estimators are admissible."], "category": "CORRECT", "question_context_used": "Corollary 3.1(1) gives a solution to the shrinkage problem (2.15), shrinking $X_{1}$ by $G_{1}=G_{1}(j)$ which improves on $r_{b}=p_{1}I_{m}$ in (3.15a). Corollary 3.1(2) gives a solution to the problem (2.16) shrinking $C_{0}$ by $G_{2}=G_{2}(l)$ , which improves on $p_{1}I_{p_{2}}$ in (3.15b). However, the double shrinkage estimator $\\hat{\\beta}$ in (3.23) is better than these single shrinkage estimators in terms of matrix risk and hence these are inadmissible.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **Option A** is correct as the text states that exact identification of $\\vartheta$ is possible from a single increment of $X$ when $b^{1}=b^{2}=b^{3}$.\n- **Option B** is correct because the text explicitly describes the injective mapping of the function $\\vartheta\\sim\\psi_{T_{1},T_{2},T_{3}}(\\vartheta)$.\n- **Option C** is correct as the estimator $\\hat{\\vartheta}_{3,n}$ is defined only within the specified interval.\n- **Option D** is incorrect because the text mentions that under certain conditions, the convergence rate can be faster than $\\Delta_{n}^{1/2}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Scandinavian J Statistics - 2019 - Féron - Efficie.md"], "background_text": "For $d=3$, when $b^{1}=b^{2}=b^{3}$, it is possible to identify $\\vartheta$ exactly from the observation of a single increment of $X$ itself. The function $\\vartheta\\sim\\psi_{T_{1},T_{2},T_{3}}(\\vartheta)$ maps $(0,\\infty)$ injectively onto $\\left(0,\\left(\\frac{T_{3}-T_{2}}{T_{2}-T_{1}}\\right)^{2}\\right)$. The estimator $\\hat{\\vartheta}_{3,n}$ is defined as $\\psi_{T_{1},T_{2},T_{3}}^{-1}(\\Psi_{T_{1},T_{2},T_{3}}^{n})$, whenever $\\Psi_{T_{1},T_{2},T_{3}}^{n}\\in\\left(0,\\left(\\frac{T_{3}-T_{2}}{T_{2}-T_{1}}\\right)^{2}\\right)$ and 0 otherwise. The convergence rate of estimators of $\\vartheta$ can be faster than $\\Delta_{n}^{1/2}$ under certain conditions.", "folder": "", "qid": "statistic-mcqm-5803", "question": "In the case $d=3$, which of the following statements are true regarding the estimation of $\\vartheta$?", "options": ["A) It is possible to identify $\\vartheta$ exactly from the observation of a single increment of $X$ itself when $b^{1}=b^{2}=b^{3}$.", "B) The function $\\vartheta\\sim\\psi_{T_{1},T_{2},T_{3}}(\\vartheta)$ maps $(0,\\infty)$ injectively onto $\\left(0,\\left(\\frac{T_{3}-T_{2}}{T_{2}-T_{1}}\\right)^{2}\\right)$.", "C) The estimator $\\hat{\\vartheta}_{3,n}$ is defined only when $\\Psi_{T_{1},T_{2},T_{3}}^{n}\\in\\left(0,\\left(\\frac{T_{3}-T_{2}}{T_{2}-T_{1}}\\right)^{2}\\right)$.", "D) The convergence rate of estimators of $\\vartheta$ is always slower than $\\Delta_{n}^{1/2}$."], "category": "CORRECT", "question_context_used": "For $d=3$, when $b^{1}=b^{2}=b^{3}$, it is possible to identify $\\vartheta$ exactly from the observation of a single increment of $X$ itself. The function $\\vartheta\\sim\\psi_{T_{1},T_{2},T_{3}}(\\vartheta)$ maps $(0,\\infty)$ injectively onto $\\left(0,\\left(\\frac{T_{3}-T_{2}}{T_{2}-T_{1}}\\right)^{2}\\right)$. The estimator $\\hat{\\vartheta}_{3,n}$ is defined as $\\psi_{T_{1},T_{2},T_{3}}^{-1}(\\Psi_{T_{1},T_{2},T_{3}}^{n})$, whenever $\\Psi_{T_{1},T_{2},T_{3}}^{n}\\in\\left(0,\\left(\\frac{T_{3}-T_{2}}{T_{2}-T_{1}}\\right)^{2}\\right)$ and 0 otherwise. The convergence rate of estimators of $\\vartheta$ can be faster than $\\Delta_{n}^{1/2}$ under certain conditions.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **A** is correct because the $\\lambda$-function includes a sum of marginal likelihoods over observed margins.\n- **B** is correct because the observed likelihood is part of the $\\lambda$-function and has a closed form expression.\n- **C** is correct because the $\\lambda$-function is defined as the expected value of the log likelihood function with respect to the conditional distribution of missing data.\n- **D** is incorrect because the Fisher Information matrix is not a component of the $\\lambda$-function; it is used for standard error calculation.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["EM-algorithm-in-Gaussian-copula-with-mi_2016_Compu.md"], "background_text": "The $\\lambda$-function in the E-step consists of three terms: a sum of marginal likelihoods over observed margins, the observed likelihood, and an integral involving the log copula density.", "folder": "", "qid": "statistic-mcqm-3860", "question": "In the context of the EM algorithm described, which of the following are components of the $\\lambda$-function?", "options": ["A) A sum of marginal likelihoods over observed margins.", "B) The observed likelihood, which has a closed form expression.", "C) The expected value of the log likelihood function with respect to the conditional distribution of $y_{\\mathrm{mis}}$.", "D) The Fisher Information matrix calculated using Louis’ formula."], "category": "CORRECT", "question_context_used": "The $\\lambda$-function in the E-step consists of three terms: a sum of marginal likelihoods over observed margins, the observed likelihood, and an integral involving the log copula density.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **A) MAR**: The CP for $X1$ under MAR is 0.706, which is less than 0.8, so this option is correct.\n- **B) CC**: The CP for $X1$ under CC is 0.490, which is less than 0.8, so this option is correct.\n- **C) F**: The CP for $X1$ under F is 0.932, which is greater than 0.8, so this option is incorrect.\n- **D) None of the above**: Since options A and B are correct, this option is incorrect.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Model-free-feature-screening-for-ultrahigh-dimens_.md"], "background_text": "The model includes $\\mathbf{y}=\\mathbf{x}\\beta+\\varepsilon$ with outliers added by replacing some $X_{k}$ with samples from a $t$-distribution. The simulation evaluates coverage probabilities (CP) for different methods (MAR, CC, F) under various conditions, including $p=0.5$ and $MR=0.4$.", "folder": "", "qid": "statistic-mcqm-4744", "question": "In the simulation with outliers, which methods have a coverage probability (CP) for $\\beta_{0}$ less than 0.8 for $X1$ when $p=0.5$, $MR=0.4$, and $n=200, p=5000$?", "options": ["A) MAR", "B) CC", "C) F", "D) None of the above"], "category": "PARTIALLY_CORRECT", "question_context_used": "The model includes $\\mathbf{y}=\\mathbf{x}\\beta+\\varepsilon$ with outliers added by replacing some $X_{k}$ with samples from a $t$-distribution. The simulation evaluates coverage probabilities (CP) for different methods (MAR, CC, F) under various conditions, including $p=0.5$ and $MR=0.4$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct, as the Monte-Carlo approximation involves generating $B$ oracle bootstrap resamples.\n- **B)** Correct, as the approximation uses the $(1-\\alpha)B$th order statistic of the bootstrap resamples.\n- **C)** Correct, as the bootstrap version of $\\hat{\\theta}_{n}$ on $X_{1:n}^{\\dagger b}$ is denoted by $\\hat{\\theta}_{n}^{\\dagger b}$.\n- **D)** Incorrect, as the Monte-Carlo approximation requires resampling.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Bootstrapping-in-non-regular-smooth-function_2012_.md"], "background_text": "The Monte-Carlo approximation of bootstrap confidence bounds involves generating $B$ oracle bootstrap resamples, $X_{1:n}^{\\dagger1},\\ldots,X_{1:n}^{\\dagger B}$, and using the $(1-\\alpha)B$th order statistic of these resamples. The bootstrap version of $\\hat{\\theta}_{n}$ on $X_{1:n}^{\\dagger b}$ is denoted by $\\hat{\\theta}_{n}^{\\dagger b}$.", "folder": "", "qid": "statistic-mcqm-5397", "question": "In the context of Monte-Carlo approximation of bootstrap confidence bounds, which of the following statements are true?", "options": ["A) The Monte-Carlo approximation involves generating $B$ oracle bootstrap resamples.", "B) The approximation uses the $(1-\\alpha)B$th order statistic of the bootstrap resamples.", "C) The bootstrap version of $\\hat{\\theta}_{n}$ on $X_{1:n}^{\\dagger b}$ is denoted by $\\hat{\\theta}_{n}^{\\dagger b}$.", "D) The Monte-Carlo approximation does not require any resampling."], "category": "CORRECT", "question_context_used": "The Monte-Carlo approximation of bootstrap confidence bounds involves generating $B$ oracle bootstrap resamples, $X_{1:n}^{\\dagger1},\\ldots,X_{1:n}^{\\dagger B}$, and using the $(1-\\alpha)B$th order statistic of these resamples. The bootstrap version of $\\hat{\\theta}_{n}$ on $X_{1:n}^{\\dagger b}$ is denoted by $\\hat{\\theta}_{n}^{\\dagger b}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A)** Correctly represents the surface area using the Hausdorff measure.\n- **B)** Correctly describes the analogous decomposition for surface area.\n- **C)** Incorrect because $A(\\mathbf{x}, r) = 4\\pi r^2$ only if the sphere is entirely within the box and not intersecting any faces, which is a specific case not generally true.\n- **D)** Correct because if the sphere does not intersect the box, the surface area of the intersection is indeed zero.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Analysis of a Three-Dimensional Point Pattern with.md"], "background_text": "The surface area $A(\\mathbf{x}, r)$ of the intersection between a rectangular box $B$ and a sphere $S(\\mathbf{x}, r)$ is given by $\\mathcal{M}^2\\{B \\cap \\partial S(\\mathbf{x}, r)\\}$, where $\\mathcal{M}^2$ is the two-dimensional Hausdorff measure. The decomposition of $A(\\mathbf{x}, r)$ is analogous to that of $V(\\mathbf{x}, r)$, with $A_1, A_2, A_3$ replacing $V_1, V_2, V_3$.", "folder": "", "qid": "statistic-mcqm-5622", "question": "Which of the following statements are true regarding the surface area $A(\\mathbf{x}, r)$ of the intersection between a rectangular box $B$ and a sphere $S(\\mathbf{x}, r)$?", "options": ["A) $A(\\mathbf{x}, r) = \\mathcal{M}^2\\{B \\cap \\partial S(\\mathbf{x}, r)\\}$", "B) $A(\\mathbf{x}, r)$ can be decomposed analogously to the volume $V(\\mathbf{x}, r)$ with $A_1, A_2, A_3$ replacing $V_1, V_2, V_3$.", "C) $A(\\mathbf{x}, r) = 4\\pi r^2$ when the sphere is entirely within the box.", "D) $A(\\mathbf{x}, r) = 0$ when the sphere does not intersect the box."], "category": "CORRECT", "question_context_used": "The surface area $A(\\mathbf{x}, r)$ of the intersection between a rectangular box $B$ and a sphere $S(\\mathbf{x}, r)$ is given by $\\mathcal{M}^2\\{B \\cap \\partial S(\\mathbf{x}, r)\\}$, where $\\mathcal{M}^2$ is the two-dimensional Hausdorff measure. The decomposition of $A(\\mathbf{x}, r)$ is analogous to that of $V(\\mathbf{x}, r)$, with $A_1, A_2, A_3$ replacing $V_1, V_2, V_3$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: The proof of Theorem 5.2 explicitly states that the first nonvanishing derivative at a local extreme must be of even order, which is a general property of functions at local extrema.\n- **B) Correct**: The proof shows that assuming $x_{F}^{\\prime\\prime}(m_{a})=0$ leads to $x_{F}^{\\prime\\prime\\prime}(m_{a}) < 0$, which contradicts the requirement that the first nonvanishing derivative at a local extreme must be of even order.\n- **C) Correct**: The proof starts by assuming $x_{F}^{\\prime\\prime}(m_{a})=0$ and derives that $\\int\\lambda^{2}/(1+\\lambda m_{a})^{3} d H(\\lambda) = 0$ as a consequence.\n- **D) Incorrect**: The text does not provide any information about the impossibility of $x_{F}(m_{a})$ having a local maximum at $m_{a}$. The theorem and proof focus on the properties of the derivatives and the contradiction arising from assuming $x_{F}^{\\prime\\prime}(m_{a})=0$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Analysis-of-the-Limiting-Spectral-Distribution-of-.md"], "background_text": "Theorem 5.2 states that $x_{F}^{\\prime\\prime}(m_{a})<0$. The proof involves showing that if $x_{F}^{\\prime\\prime}(m_{a})=0$, then certain integrals involving $\\lambda$ and $m_{a}$ would also be zero, leading to a contradiction because the first nonvanishing derivative at a local extreme must be of even order.", "folder": "", "qid": "statistic-mcqm-2980", "question": "Given the function $x_{F}(m_{a})$ with $x_{F}^{\\prime\\prime}(m_{a})<0$, which of the following statements are correct based on Theorem 5.2?", "options": ["A) The first nonvanishing derivative of $x_{F}(m_{a})$ at a local extreme must be of even order.", "B) $x_{F}^{\\prime\\prime\\prime}(m_{a}) < 0$ implies a contradiction in the context of Theorem 5.2.", "C) $x_{F}^{\\prime\\prime}(m_{a}) = 0$ leads to $\\int\\lambda^{2}/(1+\\lambda m_{a})^{3} d H(\\lambda) = 0$.", "D) The function $x_{F}(m_{a})$ cannot have a local maximum at $m_{a}$."], "category": "CORRECT", "question_context_used": "Theorem 5.2 states that $x_{F}^{\\prime\\prime}(m_{a})<0$. The proof involves showing that if $x_{F}^{\\prime\\prime}(m_{a})=0$, then certain integrals involving $\\lambda$ and $m_{a}$ would also be zero, leading to a contradiction because the first nonvanishing derivative at a local extreme must be of even order.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **A)** Correct. For $i<K-1$, $\\beta^{(i)}(S_{n_{i}})$ includes the cost of sampling for the next stage, as indicated by the term $(n_{i+1}-n_{i})\\sum_{\\mu\\in{\\cal M}}c(\\mu)p^{(i)}(\\mu|S_{n_{i}})$.\n- **B)** Incorrect. For $i=K-1$, $\\beta^{(i)}(S_{n_{i}})$ still includes the cost of sampling for the next stage, as shown by the term $(n_{K}-n_{K-1})\\sum_{\\mu\\in{\\cal M}}c(\\mu)p^{(K-1)}(\\mu|S_{n_{K-1}})$.\n- **C)** Correct. The integral in the expression for $\\beta^{(i)}(S_{n_{i}})$ represents the expected minimum loss from continuing to sample or stopping at the next stage, as it involves $\\min\\{\\beta^{(i+1)}(S_{n_{i+1}}),\\gamma^{(i+1)}(S_{n_{i+1}})\\}$.\n- **D)** Incorrect. The functions $\\beta^{(i)}(S_{n_{i}})$ are calculated recursively starting at $i=K-1$, not from $i=1$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["An_improved_method_for_deriving_optimal_one-sided_group_sequential_tests.md"], "background_text": "The expected additional loss from continuing to sample at stage $i$ and then proceeding optimally is denoted by $\\beta^{(i)}(S_{n_{i}})$. For $i<K-1$, it includes the cost of sampling for the next stage and the expected minimum loss from continuing to sample or stopping at the next stage. For $i=K-1$, it includes the cost of sampling for the next stage and the integral of the minimum loss from stopping at the next stage. The functions $\\beta^{(i)}(S_{n_{i}})$ are calculated recursively starting at $i=K-1$.", "folder": "", "qid": "statistic-mcqm-5579", "question": "Regarding the expected additional loss from continuing to sample at stage $i$, $\\beta^{(i)}(S_{n_{i}})$, which of the following statements are true?", "options": ["A) For $i<K-1$, $\\beta^{(i)}(S_{n_{i}})$ includes the cost of sampling for the next stage.", "B) For $i=K-1$, $\\beta^{(i)}(S_{n_{i}})$ does not include the cost of sampling for the next stage.", "C) The integral in the expression for $\\beta^{(i)}(S_{n_{i}})$ represents the expected minimum loss from continuing to sample or stopping at the next stage.", "D) The functions $\\beta^{(i)}(S_{n_{i}})$ are calculated starting from $i=1$ to $i=K-1$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The expected additional loss from continuing to sample at stage $i$ and then proceeding optimally is denoted by $\\beta^{(i)}(S_{n_{i}})$. For $i<K-1$, it includes the cost of sampling for the next stage and the expected minimum loss from continuing to sample or stopping at the next stage. For $i=K-1$, it includes the cost of sampling for the next stage and the integral of the minimum loss from stopping at the next stage. The functions $\\beta^{(i)}(S_{n_{i}})$ are calculated recursively starting at $i=K-1$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**: \n- **A)** Correct. The continuous variables are modeled as conditionally normal given the corresponding binary variables are positive.\n- **B)** Correct. The model assumes separate trivariate normal distributions for inpatient ($k=2,3,4$) and outpatient ($k=6,7,8$) measures.\n- **C)** Incorrect. The covariance matrix $\\Sigma_{t}^{(U)}$ is specific to each time point $t$, not necessarily the same across all time points.\n- **D)** Correct. The mean $\\xi_{i t j k}$ includes the network random effect $\\eta_{i t k}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Longitudinal profiling of health care units based.md"], "background_text": "The within-network model for continuous outcomes assumes $Y_{i t j}^{(U)} | x_{i t j}^{(U)} = 1 \\sim N_{3}(\\xi_{i t j}^{(U)}, \\Sigma_{t}^{(U)})$. The mean includes covariates and network random effects $\\eta_{i t k}$, and the covariance matrix $\\Sigma_{t}^{(U)}$ is specific to time $t$.", "folder": "", "qid": "statistic-mcqm-2672", "question": "Which of the following are correct about the within-network model for continuous outcomes?", "options": ["A) The continuous variables are conditionally normal given the corresponding binary variables are positive.", "B) The model assumes trivariate normal distributions for inpatient and outpatient measures separately.", "C) The covariance matrix $\\Sigma_{t}^{(U)}$ is the same for all time points $t$.", "D) The mean $\\xi_{i t j k}$ includes a network random effect $\\eta_{i t k}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The within-network model for continuous outcomes assumes $Y_{i t j}^{(U)} | x_{i t j}^{(U)} = 1 \\sim N_{3}(\\xi_{i t j}^{(U)}, \\Sigma_{t}^{(U)})$. The mean includes covariates and network random effects $\\eta_{i t k}$, and the covariance matrix $\\Sigma_{t}^{(U)}$ is specific to time $t$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\"answer\": [\"A\", \"D\"]}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, D\n\n- **A)** Incorrect. While the Buckley & James (1979) estimator requires an iterative algorithm, the synthetic data estimators discussed in the context do not necessarily require one.\n- **B)** Correct. The class of synthetic data estimators extends the estimators of Koul, Susarla & Van Ryzin (1981) and Leurgans (1987) to nonlinear regression.\n- **C)** Incorrect. The asymptotic results for synthetic data estimators in nonlinear regression are derived without using martingale techniques, unlike in the linear case.\n- **D)** Correct. Synthetic data estimators are connected to Kaplan–Meier integrals, as studied by Stute (1993, 1996).\n\n**Final Answer**: The final answer is $\\boxed{{B, D}}$", "source": ["Scandinavian J Statistics - 2008 - DELECROIX - Non.md"], "background_text": "Synthetic data estimators in nonlinear regression extend the estimators of Koul, Susarla & Van Ryzin (1981) and Leurgans (1987). They are connected to Kaplan–Meier integrals, as studied by Stute (1993, 1996), and do not necessarily require martingale techniques or iterative algorithms.", "folder": "", "qid": "statistic-mcqm-5199", "question": "Which of the following are characteristics of synthetic data estimators in nonlinear regression with right censoring?", "options": ["A) They require an iterative algorithm for implementation.", "B) They extend the estimators of Koul, Susarla & Van Ryzin (1981) and Leurgans (1987) to nonlinear regression.", "C) They are based on martingale techniques.", "D) They are connected to Kaplan–Meier integrals."], "category": "CORRECT", "question_context_used": "Synthetic data estimators in nonlinear regression extend the estimators of Koul, Susarla & Van Ryzin (1981) and Leurgans (1987). They are connected to Kaplan–Meier integrals, as studied by Stute (1993, 1996), and do not necessarily require martingale techniques or iterative algorithms.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}", "model_answer": ["B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: The text states that SIR is highly sensitive to outliers.\n- **B) Correct**: The text also states that CANCOR is highly sensitive to outliers.\n- **C) Incorrect**: WCANCOR performs well in the presence of outliers, as the outlier is downweighted.\n- **D) Incorrect**: The weighted permutation test is not mentioned in the context of being sensitive to outliers in this study.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Robust-dimension-reduction-based-on-canonical_2009.md"], "background_text": "Table 1 shows that the dimension and direction estimates by SIR and CANCOR are highly sensitive to outliers. This is because they both depend on nonrobust estimates of the moments of $x$. On the other hand, WCANCOR performs well in this study because the outlier is downweighted when estimating the moments.", "folder": "", "qid": "statistic-mcqm-5018", "question": "Which of the following methods are shown to be highly sensitive to outliers in the simulation study?", "options": ["A) SIR", "B) CANCOR", "C) WCANCOR", "D) Weighted permutation test"], "category": "CORRECT", "question_context_used": "Table 1 shows that the dimension and direction estimates by SIR and CANCOR are highly sensitive to outliers. This is because they both depend on nonrobust estimates of the moments of $x$. On the other hand, WCANCOR performs well in this study because the outlier is downweighted when estimating the moments.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n**Explanation**: \n- **Option A** is correct as it directly matches the given formulae for mean and variance under independence.\n- **Option D** is also correct because it presents an alternative correct expression for the variance, considering $s = N - r$, which is equivalent to the given formula when expanded.\n- **Option B** is incorrect because the denominator for the variance is $N^3$ instead of $N^2(N-1)$.\n- **Option C** is incorrect because the mean is incorrectly stated as $m n / N$ instead of $m r / N$.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Mean_and_variance_of_an_entry_in_a_contingency_table.md"], "background_text": "Under independence, the mean and variance of a tabular entry $a$ in a $2\\times2$ contingency table are given by $\\mu = m r / N$ and $\\sigma^2 = m n r s / \\{N^2(N-1)\\}$.", "folder": "", "qid": "statistic-mcqm-3420", "question": "In a $2\\times2$ contingency table under the assumption of independence, the mean ($\\mu$) and variance ($\\sigma^2$) of a tabular entry $a$ are given by:", "options": ["A) $\\mu = \\frac{m r}{N}$, $\\sigma^2 = \\frac{m n r s}{N^2(N-1)}$", "B) $\\mu = \\frac{m r}{N}$, $\\sigma^2 = \\frac{m n r s}{N^3}$", "C) $\\mu = \\frac{m n}{N}$, $\\sigma^2 = \\frac{m n r s}{N^2}$", "D) $\\mu = \\frac{r s}{N}$, $\\sigma^2 = \\frac{m n r s}{N^2(N-1)}$"], "category": "PARTIALLY_CORRECT", "question_context_used": "Under independence, the mean and variance of a tabular entry $a$ in a $2\\times2$ contingency table are given by $\\mu = m r / N$ and $\\sigma^2 = m n r s / \\{N^2(N-1)\\}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. $\\mathbf{W}^{[k]}$ is updated iteratively based on the current model estimates.\n- **B)** Correct. $\\lambda_j$ are crucial for controlling the smoothness of the fitted functions.\n- **C)** Incorrect. While the response vector $\\mathbf{y}$ is fixed, this is not a unique component of P-IRLS but a general feature of regression.\n- **D)** Correct. $\\mathbf{X}$ combines parametric and spline basis components for model fitting.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Scandinavian J Statistics - 2012 - MARRA - Coverag.md"], "background_text": "The P-IRLS algorithm for GAM estimation involves updating the weight matrix $\\mathbf{W}^{[k]}$ at each iteration, using smoothing parameters $\\lambda_j$ to balance fit and smoothness, and a design matrix $\\mathbf{X}$ that includes both parametric and spline basis components. The response vector $\\mathbf{y}$ is fixed.", "folder": "", "qid": "statistic-mcqm-2106", "question": "In the estimation of GAMs, which of the following are components of the penalized iteratively reweighted least squares (P-IRLS) algorithm?", "options": ["A) The weight matrix $\\mathbf{W}^{[k]}$ which is updated at each iteration.", "B) The smoothing parameters $\\lambda_j$ which control the trade-off between fit and smoothness.", "C) The response vector $\\mathbf{y}$ which remains unchanged throughout the iterations.", "D) The design matrix $\\mathbf{X}$ which includes columns for parametric components and spline bases."], "category": "PARTIALLY_CORRECT", "question_context_used": "The P-IRLS algorithm for GAM estimation involves updating the weight matrix $\\mathbf{W}^{[k]}$ at each iteration, using smoothing parameters $\\lambda_j$ to balance fit and smoothness, and a design matrix $\\mathbf{X}$ that includes both parametric and spline basis components. The response vector $\\mathbf{y}$ is fixed.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n**Explanation**:\n- **A) Correct**: Transformations are indeed applied to both response and predictor variables to meet certain conditions such as linearity, normality, and homoscedasticity, as mentioned in the context.\n- **B) Incorrect**: The classical multiple linear regression model does not inherently assume no transformations are needed; transformations can be applied if necessary to meet model assumptions.\n- **C) Incorrect**: Nonparametric methods, while making minimal assumptions, can still involve transformations, especially when dealing with the response variable, as indicated by the use of techniques like ACE and AVAS.\n- **D) Correct**: The ACE (alternating conditional expectation) and AVAS (additivity and variance stability) algorithms are specifically mentioned as techniques for fitting models when transformation of the response variable is necessary.\n\n**Final Answer**: The final answer is $\boxed{{A, D}}$", "source": ["Semi-parametric-nonlinear-regression-and-transf_20.md"], "background_text": "Transformations in regression modeling can involve both response and predictor variables to achieve desirable conditions like linearity, normality, and homoscedasticity. Nonparametric methods make minimal assumptions about the form of the function relating response and predictors. Techniques like ACE and AVAS are used for fitting models when response variable transformation is necessary.", "folder": "", "qid": "statistic-mcqm-5150", "question": "In the context of regression modeling, which of the following statements are true regarding transformations of variables?", "options": ["A) Transformations can be applied to both response and predictor variables to achieve linearity, normality, and homoscedasticity.", "B) The classical multiple linear regression model assumes no transformations are needed for the variables.", "C) Nonparametric methods do not require any transformations of variables.", "D) The ACE and AVAS algorithms are techniques used when transformation of the response variable is needed."], "category": "CORRECT", "question_context_used": "Transformations in regression modeling can involve both response and predictor variables to achieve desirable conditions like linearity, normality, and homoscedasticity. Nonparametric methods make minimal assumptions about the form of the function relating response and predictors. Techniques like ACE and AVAS are used for fitting models when response variable transformation is necessary.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. This is the definition of the oracle bootstrap percentile upper confidence interval.\n- **B)** Correct. This is the definition of the oracle bootstrap percentile lower confidence interval.\n- **C)** Incorrect. The equal-tailed confidence interval is not necessarily symmetric around $\\hat{\\theta}_{n}$; it is defined by the $(1+\\alpha)/2$ and $(1-\\alpha)/2$ quantiles.\n- **D)** Correct. This is the definition of the oracle bootstrap symmetric confidence interval.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Bootstrapping-in-non-regular-smooth-function_2012_.md"], "background_text": "The oracle bootstrap percentile upper confidence interval for $\\theta$ is $\\tilde{\\mathcal{L}}_{n}(\\alpha)=\\left(-\\infty,\\hat{\\theta}_{n}-\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}}^{-1}(1-\\alpha)\\right)$. The oracle bootstrap percentile lower, equal-tailed, and symmetric confidence intervals for $\\theta$ are defined as $\\tilde{\\mathcal{I}}_{n}^{L}(\\alpha)=\\bigl(\\hat{\\theta}_{n}-\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}}^{-1}(\\alpha),\\infty\\bigr)$, $\\tilde{\\mathcal{I}}_{n}^{E T}(\\alpha)=\\bigl(\\hat{\\theta}_{n}-\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}}^{-1}((1+\\alpha)/2),\\hat{\\theta}_{n}-\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}}^{-1}((1-\\alpha)/2)\\bigr)$, and $\\tilde{\\mathcal{L}}_{n}^{S}(\\alpha)=\\bigl(\\hat{\\theta}_{n}-\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}}^{'-1}(\\alpha),\\hat{\\theta}_{n}+\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}}^{'-1}(\\alpha)\\bigr)$, respectively.", "folder": "", "qid": "statistic-mcqm-5346", "question": "Which of the following statements about the oracle bootstrap confidence intervals are correct?", "options": ["A) The oracle bootstrap percentile upper confidence interval is defined as $\\left(-\\infty,\\hat{\\theta}_{n}-\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}}^{-1}(1-\\alpha)\\right)$.", "B) The oracle bootstrap percentile lower confidence interval is defined as $\\bigl(\\hat{\\theta}_{n}-\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}}^{-1}(\\alpha),\\infty\\bigr)$.", "C) The oracle bootstrap equal-tailed confidence interval is symmetric around $\\hat{\\theta}_{n}$.", "D) The oracle bootstrap symmetric confidence interval is defined as $\\bigl(\\hat{\\theta}_{n}-\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}^{'-1}(\\alpha),\\hat{\\theta}_{n}+\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}^{'-1}(\\alpha)\\bigr)$."], "category": "CORRECT", "question_context_used": "The oracle bootstrap percentile upper confidence interval for $\\theta$ is $\\tilde{\\mathcal{L}}_{n}(\\alpha)=\\left(-\\infty,\\hat{\\theta}_{n}-\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}}^{-1}(1-\\alpha)\\right)$. The oracle bootstrap percentile lower, equal-tailed, and symmetric confidence intervals for $\\theta$ are defined as $\\tilde{\\mathcal{I}}_{n}^{L}(\\alpha)=\\bigl(\\hat{\\theta}_{n}-\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}}^{-1}(\\alpha),\\infty\\bigr)$, $\\tilde{\\mathcal{I}}_{n}^{E T}(\\alpha)=\\bigl(\\hat{\\theta}_{n}-\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}}^{-1}((1+\\alpha)/2),\\hat{\\theta}_{n}-\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}}^{-1}((1-\\alpha)/2)\\bigr)$, and $\\tilde{\\mathcal{L}}_{n}^{S}(\\alpha)=\\bigl(\\hat{\\theta}_{n}-\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}}^{'-1}(\\alpha),\\hat{\\theta}_{n}+\\tilde{L}_{\\hat{\\theta}_{n}^{\\dagger}}^{'-1}(\\alpha)\\bigr)$, respectively.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**: \n- **A** is correct because the M-step involves updating marginal parameters by maximizing the observed likelihood as described.\n- **C** is correct because the M-step updates quantiles by the posterior mean for each margin.\n- **D** is correct because the M-step updates outcome values based on updated parameters and quantiles.\n- **B** is incorrect because the M-step updates correlation parameters regardless of whether $\\varGamma$ is structured or unstructured, though the method differs.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["EM-algorithm-in-Gaussian-copula-with-mi_2016_Compu.md"], "background_text": "The M-step involves updating parameters by maximizing the observed likelihood. It includes updating marginal parameters, correlation parameters, quantiles, and outcome values through a series of steps.", "folder": "", "qid": "statistic-mcqm-3859", "question": "Which of the following statements are true about the M-step in the EM algorithm described?", "options": ["A) The M-step updates marginal parameters $\\theta_{j}$ by sequentially maximizing the observed likelihood.", "B) The M-step updates correlation parameters $\\gamma_{j_{1}j_{2}}$ only if $\\varGamma$ is an unstructured correlation matrix.", "C) The M-step updates quantiles for each subject by the posterior mean for each margin.", "D) The M-step updates outcome values based on the updated parameter $\\theta^{(t+1)}$ and quantiles $q_{i j}^{(t+1)}$."], "category": "CORRECT", "question_context_used": "The M-step involves updating parameters by maximizing the observed likelihood. It includes updating marginal parameters, correlation parameters, quantiles, and outcome values through a series of steps.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The relationship holds for any subsequence starting and ending with an $\\pmb{A}$ segment, as per the theorem.\n- **B)** Correct. The relationship also holds if the subsequence starts and ends with a $\\pmb{B}$ segment, as the theorem specifies conditions for the first and last elements of the block.\n- **C)** Incorrect. The theorem does not exclude subsequences that start with an $\\pmb{A}$ and end with a $\\pmb{B}$; it generalizes the condition based on the block's start and end.\n- **D)** Correct. The theorem states the equivalence between the sum inequality and the function inequality over unions of segments.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Maximum_likelihood_estimation_of_multiple_change_points.md"], "background_text": "Within each block of a sequence, for any subsequence that starts with an $\\pmb{A}$ segment, or $\\pmb{B}$ if it is the first element of the block, and ends with an $\\pmb{A}$ segment, or $\\pmb{B}$ if it is the last element of the block, the relationship $\\sum f(B) \\geqslant \\sum f(A)$ holds. This is equivalent to $f\\{\\bigcup S(k+s)_{i}\\} \\geqslant f\\{\\bigcup S(k)_{j}\\}$.", "folder": "", "qid": "statistic-mcqm-5141", "question": "Given the relationship $\\sum f(B) \\geqslant \\sum f(A)$ within a block, which of the following statements are true?", "options": ["A) For any subsequence starting with an $\\pmb{A}$ segment and ending with an $\\pmb{A}$ segment, the sum of $f(B)$ is greater than or equal to the sum of $f(A)$.", "B) For any subsequence starting with a $\\pmb{B}$ segment and ending with a $\\pmb{B}$ segment, the sum of $f(B)$ is greater than or equal to the sum of $f(A)$.", "C) The relationship $\\sum f(B) \\geqslant \\sum f(A)$ does not hold if the subsequence starts with an $\\pmb{A}$ segment and ends with a $\\pmb{B}$ segment.", "D) The relationship $\\sum f(B) \\geqslant \\sum f(A)$ is equivalent to $f\\{\\bigcup S(k+s)_{i}\\} \\geqslant f\\{\\bigcup S(k)_{j}\\}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "Within each block of a sequence, for any subsequence that starts with an $\\pmb{A}$ segment, or $\\pmb{B}$ if it is the first element of the block, and ends with an $\\pmb{A}$ segment, or $\\pmb{B}$ if it is the last element of the block, the relationship $\\sum f(B) \\geqslant \\sum f(A)$ holds. This is equivalent to $f\\{\\bigcup S(k+s)_{i}\\} \\geqslant f\\{\\bigcup S(k)_{j}\\}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The spatial covariance function is indeed defined as the covariance between $\\eta_{i}(\\pmb{\\mathscr{s}})$ and $\\eta_{i}(\\pmb{\\mathscr{s}}^{*})$.\n- **B) Correct**: The expression provided is the correct expansion of the spatial covariance function in terms of the basis functions and the entries of $\\pmb{\\Lambda}$.\n- **C) Incorrect**: The matrix $\\mathbf{A}$ is not required to be diagonal; it can be any symmetric nonnegative-definite matrix.\n- **D) Correct**: The Karhunen-Loéve expansion is used to represent $\\eta_{i}(\\cdot)$ in terms of the eigenfunctions $\\varphi_{k}^{*}(\\cdot)$, which are related to the estimation of $C_{\\eta}(\\cdot,\\cdot)$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Regularized_Principal_Component_Analysis_for_Spatial_Data__Regularized_Principal_Component_Analysis_for_Spatial_Data.md"], "background_text": "The spatial covariance function $C_{\\eta}(\\pmb{\\mathscr{s}},\\pmb{\\mathscr{s}}^{*})=\\mathrm{cov}(\\eta_{i}(\\pmb{\\mathscr{s}}),\\eta_{i}(\\pmb{\\mathscr{s}}^{*}))$ is derived from the model $\\eta_{i}(\\pmb{\\mathscr{s}})=\\sum_{k=1}^{K}\\xi_{i k}\\varphi_{k}(\\pmb{\\mathscr{s}})$, where $\\{\\varphi_{k}(.)\\}$ are orthonormal basis functions and $\\pmb{\\xi}_{i}\\sim(0,\\Lambda)$. The covariance function is expressed as $C_{\\eta}(\\pmb{\\mathscr{s}},\\pmb{\\mathscr{s}}^{*})=\\sum_{k=1}^{K}\\sum_{k^{\\prime}=1}^{K}\\lambda_{k k^{\\prime}}\\varphi_{k}(\\pmb{\\mathscr{s}})\\varphi_{k^{\\prime}}(\\pmb{\\mathscr{s}}^{*})$, with $\\mathbf{A}$ not restricted to be diagonal.", "folder": "", "qid": "statistic-mcqm-4559", "question": "In the SpatPCA method, which of the following are true about the spatial covariance function $C_{\\eta}(\\pmb{\\mathscr{s}},\\pmb{\\mathscr{s}}^{*})$?", "options": ["A) It is defined as $\\mathrm{cov}(\\eta_{i}(\\pmb{\\mathscr{s}}),\\eta_{i}(\\pmb{\\mathscr{s}}^{*}))$.", "B) It can be expressed as $\\sum_{k=1}^{K}\\sum_{k^{\\prime}=1}^{K}\\lambda_{k k^{\\prime}}\\varphi_{k}(\\pmb{\\mathscr{s}})\\varphi_{k^{\\prime}}(\\pmb{\\mathscr{s}}^{*})$.", "C) It requires $\\mathbf{A}$ to be a diagonal matrix.", "D) It is estimated using the Karhunen-Loéve expansion."], "category": "PARTIALLY_CORRECT", "question_context_used": "The spatial covariance function $C_{\\eta}(\\pmb{\\mathscr{s}},\\pmb{\\mathscr{s}}^{*})=\\mathrm{cov}(\\eta_{i}(\\pmb{\\mathscr{s}}),\\eta_{i}(\\pmb{\\mathscr{s}}^{*}))$ is derived from the model $\\eta_{i}(\\pmb{\\mathscr{s}})=\\sum_{k=1}^{K}\\xi_{i k}\\varphi_{k}(\\pmb{\\mathscr{s}})$, where $\\{\\varphi_{k}(.)\\}$ are orthonormal basis functions and $\\pmb{\\xi}_{i}\\sim(0,\\Lambda)$. The covariance function is expressed as $C_{\\eta}(\\pmb{\\mathscr{s}},\\pmb{\\mathscr{s}}^{*})=\\sum_{k=1}^{K}\\sum_{k^{\\prime}=1}^{K}\\lambda_{k k^{\\prime}}\\varphi_{k}(\\pmb{\\mathscr{s}})\\varphi_{k^{\\prime}}(\\pmb{\\mathscr{s}}^{*})$, with $\\mathbf{A}$ not restricted to be diagonal.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C, D\n\n- **Option A** is incorrect because the proportion of pollution does not affect the rate of convergence of $\\hat{\\theta}_{n}$, which remains $n^{-1/2}$.\n- **Option B** is correct as the consistency rate of $V_{n}$ is indeed $n^{-1/2}h^{-1/4}$, slower than $n^{-1/2}$.\n- **Option C** is correct because the proportion of pollution only enlarges the asymptotic variance and does not affect the asymptotic behavior of $V_{n}$.\n- **Option D** is correct as the rate of convergence of $\\hat{\\theta}_{n}$ is $n^{-1/2}$.\n\n**Final Answer**: The final answer is $\\boxed{{B, C, D}}$", "source": ["A-goodness-of-fit-test-for-variable-adju_2019_Comp.md"], "background_text": "The proportion of pollution enlarges the asymptotic variance of $\\hat{\\theta}_{n}$ but does not affect its rate $n^{-1/2}$ of convergence. The consistency rate of $V_{n}$ is $n^{-1/2}h^{-1/4}$, which is slower than $n^{-1/2}$. The proportion of pollution does not affect the asymptotic behavior of $V_{n}$.", "folder": "", "qid": "statistic-mcqm-5905", "question": "Which of the following statements about the asymptotic behavior of $\\hat{\\theta}_{n}$ and $V_{n}$ are correct?", "options": ["A) The proportion of pollution affects the rate of convergence of $\\hat{\\theta}_{n}$.", "B) The consistency rate of $V_{n}$ is slower than $n^{-1/2}$.", "C) The proportion of pollution does not affect the asymptotic behavior of $V_{n}$.", "D) The rate of convergence of $\\hat{\\theta}_{n}$ is $n^{-1/2}$."], "category": "CORRECT", "question_context_used": "The proportion of pollution enlarges the asymptotic variance of $\\hat{\\theta}_{n}$ but does not affect its rate $n^{-1/2}$ of convergence. The consistency rate of $V_{n}$ is $n^{-1/2}h^{-1/4}$, which is slower than $n^{-1/2}$. The proportion of pollution does not affect the asymptotic behavior of $V_{n}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"C\", \"D\"]\n}", "model_answer": ["C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. Including all original model parameters in $M'$ is ideal for a mechanism-based approach.\n- **B)** Correct. $M'$ can have additional parameters optimized during emulation, though this is less critical in mechanism-based emulation.\n- **C)** Correct. The likelihood function of $M'$ indeed describes the probability density of outputs given inputs and parameters.\n- **D)** Incorrect. While possible, not depending on some simulation model parameters is not in strict accordance with a mechanism-based approach.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["Mechanism-based-emulation-of-dynamic-simulation-m_.md"], "background_text": "The simplified probabilistic model ($M'$) approximates the deterministic simulation model ($M$) and should ideally include all parameters of the original model to adhere to a mechanism-based approach. It may contain additional parameters for optimization. The likelihood function of $M'$ describes the probability density of model outputs given inputs and parameters. While it's possible for the simplified model not to depend on some parameters, this is not in strict accordance with a mechanism-based approach.", "folder": "", "qid": "statistic-mcqm-5654", "question": "In the context of the emulator, which of the following are true about the simplified probabilistic model ($M'$)?", "options": ["A) It must include all parameters present in the original model to adhere to a mechanism-based approach.", "B) It can contain additional parameters that are optimized during the emulation process.", "C) Its likelihood function describes the probability density of model outputs given inputs and parameters.", "D) It is not required to depend on the parameters of the simulation model if the probabilistic terms can model their effects."], "category": "CORRECT", "question_context_used": "The simplified probabilistic model ($M'$) approximates the deterministic simulation model ($M$) and should ideally include all parameters of the original model to adhere to a mechanism-based approach. It may contain additional parameters for optimization. The likelihood function of $M'$ describes the probability density of model outputs given inputs and parameters. While it's possible for the simplified model not to depend on some parameters, this is not in strict accordance with a mechanism-based approach.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **Option A** is correct as the point estimate $\\widehat{{\\bf B}{\\bf M}{\\bf D}_{\\alpha}}$ is indeed found by solving $\\hat{r}^{*}(d) = \\alpha$.\n- **Option B** is correct because the conventional 95% lower confidence limit is calculated as $\\widehat{\\mathbf{BMD}}_{\\alpha} - 1.645\\sqrt{\\widehat{\\mathbf{var}}(\\widehat{\\mathbf{BMD}})_{\\alpha}}$.\n- **Option C** is correct as Kimmel and Gaylor's method involves inverting the hypothesis test $H: r^{*}(d) = \\alpha$ versus $H_{\\mathrm{a}}: r^{*}(d) < \\alpha$.\n- **Option D** is incorrect because $\\hat{r}_{\\mathrm{U}}(d)$ is the 95% upper confidence limit for $r^{*}(d)$, not the lower limit.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["A Litter-Based Approach to Risk Assessment in Deve.md"], "background_text": "The bench-mark dose $\\mathbf{BMD}_{\\alpha}$ is the dose level that produces an excess risk of $\\alpha$. A point estimate is obtained by solving $\\hat{r}^{*}(d) = \\alpha$. The conventional 95% lower confidence limit is calculated using the delta method. Kimmel and Gaylor proposed an alternative method based on test inversion, and the lower effective dose $\\mathrm{LED}_{\\alpha}$ is derived from the upper confidence limit for the excess risk.", "folder": "", "qid": "statistic-mcqm-5455", "question": "Regarding the determination of the bench-mark dose $\\mathbf{BMD}_{\\alpha}$ and its confidence intervals, which of the following statements are true?", "options": ["A) A point estimate $\\widehat{{\\bf B}{\\bf M}{\\bf D}_{\\alpha}}$ is obtained by solving $\\hat{r}^{*}(d) = \\alpha$.", "B) The conventional 95% lower confidence limit for $\\mathbf{BMD}_{\\alpha}$ is given by $\\widehat{\\mathbf{BMD}}_{\\alpha} - 1.645\\sqrt{\\widehat{\\mathbf{var}}(\\widehat{\\mathbf{BMD}})_{\\alpha}}$.", "C) Kimmel and Gaylor's method for constructing a 95% lower confidence interval for $\\mathbf{BMD}_{\\alpha}$ involves inverting a hypothesis test.", "D) The lower effective dose $\\mathrm{LED}_{\\alpha}$ is obtained by setting $\\hat{r}_{\\mathrm{U}}(d) = \\alpha$, where $\\hat{r}_{\\mathrm{U}}(d)$ is the 95% lower confidence limit for $r^{*}(d)$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The bench-mark dose $\\mathbf{BMD}_{\\alpha}$ is the dose level that produces an excess risk of $\\alpha$. A point estimate is obtained by solving $\\hat{r}^{*}(d) = \\alpha$. The conventional 95% lower confidence limit is calculated using the delta method. Kimmel and Gaylor proposed an alternative method based on test inversion, and the lower effective dose $\\mathrm{LED}_{\\alpha}$ is derived from the upper confidence limit for the excess risk.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "To answer this question, let's evaluate each option based on the background text:\n\n- **A) Incorrect**: Left continuity ensures $\\tilde{f}(x) \\geqslant f(x)$, not necessarily equality.\n- **B) Incorrect**: Right continuity is not discussed in the context of ensuring $\\tilde{f}(x) = f(x)$.\n- **C) Correct**: This condition leads to $x \\in S_n$ and thus $\\tilde{f}(x) = f(x)$ for all $n \\geqslant n_0$.\n- **D) Correct**: If $x \\in S_n$ for all $n \\geqslant n_0$, then $\\tilde{f}(x) = f(x)$.\n\n**Correct Choices**: C, D\n\n**Final Answer**: The final answer is $\\boxed{{C, D}}$", "source": ["Some-Remarks-on-the-Supermodular-Order_2000_Journa.md"], "background_text": "Lemma 3.2's proof distinguishes two cases for the convergence of $f_n$ to $f$: (i) if $f$ is left continuous at $x$, then $\\tilde{f}(x) \\geqslant f(x)$; (ii) if $f$ is not left continuous at $x$, then there exists $n_0$ such that $x \\in S_n$ and $\\tilde{f}(x) = f_n(x) = f(x)$ for all $n \\geqslant n_0$.", "folder": "", "qid": "statistic-mcqm-4968", "question": "In the context of Lemma 3.2, which of the following conditions ensure that $\\tilde{f}(x) = f(x)$?", "options": ["A) $f$ is left continuous at $x$.", "B) $f$ is right continuous at $x$.", "C) There exists some $n_0$ such that $\\lim_{t\\uparrow x}f(t) < f(x) - \\frac{1}{2^{n_0}}$.", "D) $x \\in S_n$ for all $n \\geqslant n_0$."], "category": "CORRECT", "question_context_used": "Lemma 3.2's proof distinguishes two cases for the convergence of $f_n$ to $f$: (i) if $f$ is left continuous at $x$, then $\\tilde{f}(x) \\geqslant f(x)$; (ii) if $f$ is not left continuous at $x$, then there exists $n_0$ such that $x \\in S_n$ and $\\tilde{f}(x) = f_n(x) = f(x)$ for all $n \\geqslant n_0$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "Let's evaluate each option based on the provided context:\n\n- **A)** Correct. The text states that $|E(\\hat{T}_{M})-T| \\leq \\frac{T}{(M+2)\\pi_{0}}$, which bounds the bias.\n- **B)** Incorrect. The variance calculation involves approximations and bounds, as seen in the inequalities provided in the context.\n- **C)** Correct. The text compares $|\\mathrm{MSE}(\\hat{T}_{M})-\\mathrm{var}(\\hat{T})|$ and provides a bound for it.\n- **D)** Incorrect. The expectation of $\\hat{T}_{M}$ is not exactly $T$ but is close, with a bounded difference as mentioned.\n\n**Correct Choices**: A, C\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Applying_the_Horvitz-Thompson_criterion_in_complex_designs_A_computer-intensive_perspective_for_estimating_inclusion_probabilities.md"], "background_text": "The estimator $\\hat{T}_{M}$ for the total amount of carbon involves complex calculations including expectation and variance, with bounds provided for bias and comparisons made for MSE. The context discusses the theoretical underpinnings and bounds related to $\\hat{T}_{M}$.", "folder": "", "qid": "statistic-mcqm-5051", "question": "Regarding the estimation of the total amount of carbon in Trentino's forests, which of the following statements are true about the estimator $\\hat{T}_{M}$?", "options": ["A) The bias of $\\hat{T}_{M}$ is bounded by $\\frac{T}{(M+2)\\pi_{0}}$.", "B) The variance of $\\hat{T}_{M}$ is calculated without any approximation.", "C) The mean squared error (MSE) of $\\hat{T}_{M}$ is compared to the variance of $\\hat{T}$.", "D) The expectation of $\\hat{T}_{M}$ is exactly equal to the true total $T$."], "category": "CORRECT", "question_context_used": "The estimator $\\hat{T}_{M}$ for the total amount of carbon involves complex calculations including expectation and variance, with bounds provided for bias and comparisons made for MSE. The context discusses the theoretical underpinnings and bounds related to $\\hat{T}_{M}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A)** Correct. The text defines $\\lambda(\\omega)$ as the largest eigenvalue of the given determinantal equation.\n- **B)** Incorrect. The text states that $\\lambda(\\omega)$ does not depend on the choice of $A$.\n- **C)** Correct. The text mentions that the eigenvector $b(\\omega)$ depends on the particular choice of $A$.\n- **D)** Correct. The text specifies that the equivalence class of scalings associated with $\\beta(\\omega) = Ab(\\omega)$ does not depend on $A$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Spectral_analysis_for_categorical_time_series_Scaling_and_the_spectral_envelope.md"], "background_text": "The eigenvalue $\\lambda(\\omega)$ is defined as the largest eigenvalue of the determinantal equation $|A^{\\prime}f^{\\text{re}}(\\omega)A - \\lambda A^{\\prime}VA| = 0$, and $b(\\omega)$ is any corresponding eigenvector. $\\lambda(\\omega)$ does not depend on the choice of $A$, but $b(\\omega)$ does. However, the equivalence class of scalings associated with $\\beta(\\omega) = Ab(\\omega)$ does not depend on $A$.", "folder": "", "qid": "statistic-mcqm-1040", "question": "Regarding the eigenvalue $\\lambda(\\omega)$ and eigenvector $b(\\omega)$, which of the following are true?", "options": ["A) $\\lambda(\\omega)$ is the largest eigenvalue of $A^{\\prime}f^{\\text{re}}(\\omega)A - \\lambda A^{\\prime}VA = 0$.", "B) $\\lambda(\\omega)$ depends on the choice of matrix $A$.", "C) The eigenvector $b(\\omega)$ depends on the choice of matrix $A$.", "D) The equivalence class of scalings associated with $\\beta(\\omega) = Ab(\\omega)$ does not depend on $A$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The eigenvalue $\\lambda(\\omega)$ is defined as the largest eigenvalue of the determinantal equation $|A^{\\prime}f^{\\text{re}}(\\omega)A - \\lambda A^{\\prime}VA| = 0$, and $b(\\omega)$ is any corresponding eigenvector. $\\lambda(\\omega)$ does not depend on the choice of $A$, but $b(\\omega)$ does. However, the equivalence class of scalings associated with $\\beta(\\omega) = Ab(\\omega)$ does not depend on $A$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}", "model_answer": ["B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The series $\\sum_{1}^{\\infty}a_{n}\\left|\\lambda_{i}-\\left\\langle X_{n}^{i},B X_{n}^{i}\\right\\rangle\\right|$ is guaranteed to converge almost surely under the assumptions of Corollary 2.\n- **B)** Correct. The series $\\sum_{1}^{\\infty}a_{n}(\\lambda_{i}-\\left\\langle X_{n}^{i},B_{n}X_{n}^{i}\\right\\rangle)$ is also guaranteed to converge almost surely under the same assumptions.\n- **C)** Correct. The series $\\sum_{1}^{\\infty}a_{n}\\left|\\lambda_{i}-\\left\\langle X_{n}^{i},B_{n}X_{n}^{i}\\right\\rangle\\right|$ is mentioned to converge almost surely under different sets of assumptions in Corollary 2.\n- **D)** Incorrect. There is no mention of the series $\\sum_{1}^{\\infty}a_{n}\\left\\langle X_{n}^{i},B X_{n}^{i}\\right\\rangle$ converging almost surely in the provided context.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Widening-the-scope-of-an-eigenvector-stochastic-ap.md"], "background_text": "Corollary 2 guarantees the almost sure convergence of the series $\\sum_{1}^{\\infty}a_{n}\\left|\\lambda_{i}-\\left\\langle X_{n}^{i},B X_{n}^{i}\\right\\rangle\\right|$ and $\\sum_{1}^{\\infty}a_{n}(\\lambda_{i}-\\left\\langle X_{n}^{i},B_{n}X_{n}^{i}\\right\\rangle)$ under its assumptions. Additionally, it mentions the convergence of $\\sum_{1}^{\\infty}a_{n}\\left|\\lambda_{i}-\\left\\langle X_{n}^{i},B_{n}X_{n}^{i}\\right\\rangle\\right|$ under different sets of assumptions.", "folder": "", "qid": "statistic-mcqm-3368", "question": "Which of the following series are guaranteed to converge almost surely under the assumptions of Corollary 2?", "options": ["A) $\\sum_{1}^{\\infty}a_{n}\\left|\\lambda_{i}-\\left\\langle X_{n}^{i},B X_{n}^{i}\\right\\rangle\\right|$", "B) $\\sum_{1}^{\\infty}a_{n}(\\lambda_{i}-\\left\\langle X_{n}^{i},B_{n}X_{n}^{i}\\right\\rangle)$", "C) $\\sum_{1}^{\\infty}a_{n}\\left|\\lambda_{i}-\\left\\langle X_{n}^{i},B_{n}X_{n}^{i}\\right\\rangle\\right|$", "D) $\\sum_{1}^{\\infty}a_{n}\\left\\langle X_{n}^{i},B X_{n}^{i}\\right\\rangle$"], "category": "PARTIALLY_CORRECT", "question_context_used": "Corollary 2 guarantees the almost sure convergence of the series $\\sum_{1}^{\\infty}a_{n}\\left|\\lambda_{i}-\\left\\langle X_{n}^{i},B X_{n}^{i}\\right\\rangle\\right|$ and $\\sum_{1}^{\\infty}a_{n}(\\lambda_{i}-\\left\\langle X_{n}^{i},B_{n}X_{n}^{i}\\right\\rangle)$ under its assumptions. Additionally, it mentions the convergence of $\\sum_{1}^{\\infty}a_{n}\\left|\\lambda_{i}-\\left\\langle X_{n}^{i},B_{n}X_{n}^{i}\\right\\rangle\\right|$ under different sets of assumptions.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The sum of squares for assigned weights is explicitly given as 31.7844.\n- **B)** Correct. The mean square for departures from assigned weights is given as 0.3092 (n).\n- **C)** Correct. The sum of squares for interactions is explicitly given as 1.7851.\n- **D)** Correct. The total sum of squares for A+AB is explicitly given as 34.1878.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["THE_INTERPRETATION_OF_INTERACTIONS_IN_FACTORIAL_EXPERIMENTS.md"], "background_text": "The table shows degrees of freedom, sum of squares, and mean squares for assigned weights, departures from assigned weights, and interactions. The total sum of squares for A+AB is 34.1878.", "folder": "", "qid": "statistic-mcqm-2028", "question": "In the weighted analysis of variance table provided, which of the following statements are correct regarding the sum of squares and mean squares?", "options": ["A) The sum of squares for assigned weights is 31.7844.", "B) The mean square for departures from assigned weights is 0.3092.", "C) The sum of squares for interactions is 1.7851.", "D) The total sum of squares (A+AB) is 34.1878."], "category": "PARTIALLY_CORRECT", "question_context_used": "The table shows degrees of freedom, sum of squares, and mean squares for assigned weights, departures from assigned weights, and interactions. The total sum of squares for A+AB is 34.1878.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n- **A)** Correct. The text explicitly states that these estimators force the first term of expression (24) to be 0.\n- **B)** Incorrect. The text does not state that these estimators require both models to be correctly specified; their advantage is in robustness to misspecification.\n- **C)** Correct. The text mentions that these estimators prevent bias inflation in regions with small exposure probabilities.\n- **D)** Incorrect. The text does not compare the efficiency of these estimators to others; it focuses on their bias prevention properties.\n\n**Final Answer**: The final answer is $\boxed{{A, C}}$", "source": ["Semiparametric Tests for Sufficient Cause Interact.md"], "background_text": "The text discusses the use of regression doubly robust estimators to prevent bias inflation, particularly in regions with small exposure probabilities, by forcing a specific term to be zero.", "folder": "", "qid": "statistic-mcqm-6103", "question": "According to the text, what is a key feature of the regression doubly robust estimators recommended to prevent bias inflation?", "options": ["A) They force the first term of expression (24) to be 0.", "B) They are only applicable when both exposure and outcome models are correctly specified.", "C) They prevent bias inflation in regions where the exposure probabilities are small.", "D) They are less efficient than other doubly robust estimators."], "category": "CORRECT", "question_context_used": "The text discusses the use of regression doubly robust estimators to prevent bias inflation, particularly in regions with small exposure probabilities, by forcing a specific term to be zero.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A** is correct as it directly defines $\\psi_{U}(\\underline{\\mu}, \\delta)$.\n- **B** is correct as it directly defines $\\psi_{L}(\\underline{\\mu}, \\delta)$.\n- **C** is incorrect because if both sets are empty, $b(\\underline{\\mu}, \\delta) = 1$, not 0.\n- **D** is correct because the cardinality of $\\psi_{L}(\\underline{\\mu}, \\delta)$ influences the minimum value of $b(\\delta)$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["A-new-two-stage-multiple-comparison-procedure-for-.md"], "background_text": "For given $\\delta > 0$ and $\\underline{\\mu} \\in \\mathcal{R}^{k+1}$, we define the sets $\\psi_{U}(\\underline{\\mu}, \\delta) = \\{i: \\mu_{i} \\geq \\mu_{0} + \\delta\\}$, $\\psi_{L}(\\underline{\\mu}, \\delta) = \\{i: \\mu_{i} \\leq \\mu_{0} - \\delta\\}$. If both the sets $\\psi_{U}(\\underline{\\mu}, \\delta)$ and $\\psi_{L}(\\underline{\\mu}, \\delta)$ are empty, then $b(\\underline{\\mu}, \\delta) = 1$.", "folder": "", "qid": "statistic-mcqm-1308", "question": "Regarding the sets $\\psi_{U}(\\underline{\\mu}, \\delta)$ and $\\psi_{L}(\\underline{\\mu}, \\delta)$, which of the following are correct?", "options": ["A) $\\psi_{U}(\\underline{\\mu}, \\delta) = \\{i: \\mu_{i} \\geq \\mu_{0} + \\delta\\}$.", "B) $\\psi_{L}(\\underline{\\mu}, \\delta) = \\{i: \\mu_{i} \\leq \\mu_{0} - \\delta\\}$.", "C) If both $\\psi_{U}(\\underline{\\mu}, \\delta)$ and $\\psi_{L}(\\underline{\\mu}, \\delta)$ are empty, then $b(\\underline{\\mu}, \\delta) = 0$.", "D) The cardinality of $\\psi_{L}(\\underline{\\mu}, \\delta)$ affects the value of $b(\\delta)$."], "category": "PARTIALLY_CORRECT", "question_context_used": "For given $\\delta > 0$ and $\\underline{\\mu} \\in \\mathcal{R}^{k+1}$, we define the sets $\\psi_{U}(\\underline{\\mu}, \\delta) = \\{i: \\mu_{i} \\geq \\mu_{0} + \\delta\\}$, $\\psi_{L}(\\underline{\\mu}, \\delta) = \\{i: \\mu_{i} \\leq \\mu_{0} - \\delta\\}$. If both the sets $\\psi_{U}(\\underline{\\mu}, \\delta)$ and $\\psi_{L}(\\underline{\\mu}, \\delta)$ are empty, then $b(\\underline{\\mu}, \\delta) = 1$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A\n\n**Explanation**: According to Theorem 2, under the assumptions $\\mathsf{A}_{1}$ to $\\mathsf{A}_{4}$, both $\\hat{\\eta}_{l}$ and $\\hat{\\lambda}_{l}$ converge in probability to their true values $\\eta_{l}$ and $\\lambda_{l}$, respectively. Therefore, option A is correct. Options B, C, and D are incorrect because they suggest partial or no convergence, which contradicts the theorem.\n\n**Final Answer**: The final answer is $\\boxed{{A}}$", "source": ["Robust-dimension-reduction-based-on-canonical_2009.md"], "background_text": "Given the conditions $\\mathsf{A}_{1}$ to $\\mathsf{A}_{4}$, Theorem 2 states that $\\hat{\\eta}_{l}\\overset{P}{\\rightarrow}\\eta_{l}$ and $\\hat{\\lambda}_{l}\\xrightarrow{P}\\lambda_{l}$.", "folder": "", "qid": "statistic-mcqm-4964", "question": "Under the assumptions $\\mathsf{A}_{1}$ to $\\mathsf{A}_{4}$, which of the following statements are true regarding the convergence of $\\hat{\\eta}_{l}$ and $\\hat{\\lambda}_{l}$?", "options": ["A) $\\hat{\\eta}_{l}\\overset{P}{\\rightarrow}\\eta_{l}$ and $\\hat{\\lambda}_{l}\\xrightarrow{P}\\lambda_{l}$.", "B) $\\hat{\\eta}_{l}\\overset{P}{\\rightarrow}\\eta_{l}$ but $\\hat{\\lambda}_{l}$ does not necessarily converge to $\\lambda_{l}$.", "C) $\\hat{\\lambda}_{l}\\xrightarrow{P}\\lambda_{l}$ but $\\hat{\\eta}_{l}$ does not necessarily converge to $\\eta_{l}$.", "D) Neither $\\hat{\\eta}_{l}$ nor $\\hat{\\lambda}_{l}$ converge to their true values."], "category": "CORRECT", "question_context_used": "Given the conditions $\\mathsf{A}_{1}$ to $\\mathsf{A}_{4}$, Theorem 2 states that $\\hat{\\eta}_{l}\\overset{P}{\\rightarrow}\\eta_{l}$ and $\\hat{\\lambda}_{l}\\xrightarrow{P}\\lambda_{l}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The text states that under $H_{0}$, the scaled ODC converges to a standard Brownian bridge process.\n- **B)** Correct. The text mentions that the scaled Mann–Whitney statistic converges to the integral of a Brownian bridge.\n- **C)** Correct. The text states that the scaled one-sided Kolmogorov–Smirnov distance converges to a Weibull random variable.\n- **D)** Incorrect. The ODC converges to a standard Brownian bridge process, not a standard normal distribution.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Nonparametric_tests_for_and_against_likelihood_ratio_ordering_in_the_two-sample_problem.md"], "background_text": "Under the null hypothesis $H_{0}\\colon F=G$, the text provides several convergence results, including the convergence of the ODC to a standard Brownian bridge process, the Mann–Whitney statistic to the integral of a Brownian bridge, and the one-sided Kolmogorov–Smirnov distance to a Weibull random variable.", "folder": "", "qid": "statistic-mcqm-5744", "question": "Under the null hypothesis $H_{0}\\colon F=G$, which of the following convergence results are true?", "options": ["A) $c_{m,n}\\big[\\big\\{\\mathrm{ODC}_{m,n}(t)-t\\big\\}:0\\leqslant t\\leqslant1\\big]\\to\\{W_{0}(t):0\\leqslant t\\leqslant1\\}$ in distribution.", "B) $c_{m,n}\\left(\\int F_{m}d G_{n}-\\frac{1}{2}\\right)\\to\\int_{0}^{1}W_{0}(t)d t$ in distribution.", "C) $c_{m,n}\\Bigg[\\operatorname*{sup}_{0\\leqslant t\\leqslant1}\\left\\{\\mathrm{ODC}_{m,n}(t)-t\\right\\}\\Bigg]\\to\\|W_{0}^{+}(t)\\|$ in distribution.", "D) The ODC converges to a standard normal distribution."], "category": "CORRECT", "question_context_used": "Under the null hypothesis $H_{0}\\colon F=G$, the text provides several convergence results, including the convergence of the ODC to a standard Brownian bridge process, the Mann–Whitney statistic to the integral of a Brownian bridge, and the one-sided Kolmogorov–Smirnov distance to a Weibull random variable.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A)** Correct. The estimator is indeed obtained by fitting an $\\mathrm{FAR}(p,d)$ model using a likelihood procedure.\n- **B)** Incorrect. The order $p$ is not fixed; it varies with the sample size $n$.\n- **C)** Correct. The paper's focus is on developing theoretical foundations for this approach.\n- **D)** Correct. The dimension of the matrices is dependent on the sample size, as explicitly mentioned.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Estimation-of-the-memory-parameter-by-fitting-frac.md"], "background_text": "The paper develops theoretical foundations for estimating the memory parameter $d$ using a fractional autoregressive model fitting approach. The estimator of $d$ is obtained by fitting an $\\mathrm{FAR}(p,d)$ model using a likelihood procedure, with $p$ treated as a function of the sample size $n$ such that $p\\rightarrow\\infty$ as $n\\rightarrow\\infty$. The dimension of the matrices and vectors related to the information matrix depends on the sample size.", "folder": "", "qid": "statistic-mcqm-6003", "question": "What are the correct statements about the estimation of the memory parameter $d$ in the context of the paper?", "options": ["A) The estimator of $d$ is obtained by fitting an $\\mathrm{FAR}(p,d)$ model using a likelihood procedure.", "B) The order $p$ of the $\\mathrm{FAR}(p,d)$ model is treated as a fixed value regardless of the sample size.", "C) The paper develops theoretical foundations for a fractional autoregressive model fitting approach.", "D) The dimension of the matrices related to the information matrix depends on the sample size."], "category": "CORRECT", "question_context_used": "The paper develops theoretical foundations for estimating the memory parameter $d$ using a fractional autoregressive model fitting approach. The estimator of $d$ is obtained by fitting an $\\mathrm{FAR}(p,d)$ model using a likelihood procedure, with $p$ treated as a function of the sample size $n$ such that $p\\rightarrow\\infty$ as $n\\rightarrow\\infty$. The dimension of the matrices and vectors related to the information matrix depends on the sample size.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: C\n\n- **A)** Incorrect. The context explicitly states that kernel methods are not used in finding centers of clusters in Step 1.\n- **B)** Incorrect. Similarly, kernel methods are not used in assigning non-censored observations to their clusters in Step 1.\n- **C)** Correct. Step 2 requires smoothing techniques to assign labels to censored observations by estimating the conditional distribution function.\n- **D)** Incorrect. The calculation of the weighted sum of squares for non-censored observations does not involve kernel methods or smoothing techniques.\n\n**Final Answer**: The final answer is $\\boxed{{C}}$", "source": ["Vector-quantization-and-clustering-in-the-prese_20.md"], "background_text": "The algorithm does not use kernel methods or smoothing techniques in Step 1 for finding centers of clusters or assigning non-censored observations to their clusters. However, Step 2, which involves assigning labels to censored observations, requires some smoothing techniques to estimate the conditional distribution function.", "folder": "", "qid": "statistic-mcqm-5995", "question": "Which of the following steps in the algorithm described in the context require the use of kernel methods or smoothing techniques?", "options": ["A) Finding centers of clusters in Step 1.", "B) Assigning non-censored observations to their clusters in Step 1.", "C) Assigning labels to censored observations in Step 2.", "D) Calculating the weighted sum of squares for non-censored observations."], "category": "CORRECT", "question_context_used": "The algorithm does not use kernel methods or smoothing techniques in Step 1 for finding centers of clusters or assigning non-censored observations to their clusters. However, Step 2, which involves assigning labels to censored observations, requires some smoothing techniques to estimate the conditional distribution function.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"C\"]\n}", "model_answer": ["C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A) Correct**: The ICL criterion for LTBM is indeed inspired by the Integrate Classification Likelihood (ICL) introduced by Biernacki et al.\n- **B) Correct**: The derivation of the ICL criterion relies on Laplace approximations and the Stirling formula.\n- **C) Incorrect**: The ICL criterion does not directly compute the optimal number of topics; it uses approximations and estimations.\n- **D) Correct**: The first term in the ICL criterion involves a variational estimation of the likelihood.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["The-latent-topic-block-model-for-the-co-cluster_20.md"], "background_text": "The ICL criterion for LTBM is used to estimate the number of row clusters Q, column clusters L, and topics K. It is inspired by the ICL criterion for mixture models and involves Laplace approximations, the Stirling formula, and a variational estimation of the likelihood.", "folder": "", "qid": "statistic-mcqm-5932", "question": "Which of the following statements about the ICL criterion for LTBM are correct?", "options": ["A) It is inspired by the Integrate Classification Likelihood (ICL)", "B) It uses Laplace approximations and the Stirling formula", "C) It directly computes the optimal number of topics without approximations", "D) It includes a term for the variational estimation of the likelihood"], "category": "CORRECT", "question_context_used": "The ICL criterion for LTBM is used to estimate the number of row clusters Q, column clusters L, and topics K. It is inspired by the ICL criterion for mixture models and involves Laplace approximations, the Stirling formula, and a variational estimation of the likelihood.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The text defines $V$ as an $n\\times m$ matrix such that $V^{\\prime}V=I_{m}$.\n- **B)** Correct. The text states that $V$ is a maximal invariant under the action of the group $G$.\n- **C)** Incorrect. The text specifies that $V$ is uniformly distributed over the Stiefel manifold under the null hypothesis, not the alternative.\n- **D)** Correct. The text describes $V$ as being written in the form $V=W T^{-1}$, where $T$ is an upper triangular matrix with positive diagonal elements.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Similar-tests-for-covariance-structures-in-multi_2.md"], "background_text": "The matrix $V$ is defined as $V=W T^{-1}$, where $W=CY$ and $T$ is an $m\\times m$ upper triangular matrix with positive diagonal elements such that $T^{\\prime}T=S$. $V$ is an $n\\times m$ matrix such that $V^{\\prime}V=I_{m}$ and is a maximal invariant under the action of the group $G$. Under the null hypothesis, $V$ is uniformly distributed over the Stiefel manifold $V^{\\prime}V=I_{m}$.", "folder": "", "qid": "statistic-mcqm-5553", "question": "Which of the following are correct about the matrix $V$ as defined in the context?", "options": ["A) $V$ is an $n\\times m$ matrix such that $V^{\\prime}V=I_{m}$.", "B) $V$ is a maximal invariant under the action of the group $G$.", "C) $V$ is uniformly distributed over the Stiefel manifold under the alternative hypothesis.", "D) $V$ can be written as $V=W T^{-1}$ where $T$ is an upper triangular matrix with positive diagonal elements."], "category": "CORRECT", "question_context_used": "The matrix $V$ is defined as $V=W T^{-1}$, where $W=CY$ and $T$ is an $m\\times m$ upper triangular matrix with positive diagonal elements such that $T^{\\prime}T=S$. $V$ is an $n\\times m$ matrix such that $V^{\\prime}V=I_{m}$ and is a maximal invariant under the action of the group $G$. Under the null hypothesis, $V$ is uniformly distributed over the Stiefel manifold $V^{\\prime}V=I_{m}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: The text states that $\\kappa(t,\\omega)$ is obtained by integrating $m(t+\\omega)$ with respect to $t$, resulting in $M(t+\\omega)+c$.\n- **B) Correct**: The constant $c$ is determined by setting $\\kappa=0$ in the equation, which gives $c=-M(\\omega)$.\n- **C) Correct**: The text explicitly mentions that the distribution with the given cumulant generating function is of exponential-type.\n- **D) Incorrect**: The function $M(t+\\omega)$ is the antiderivative of $m(t+\\omega)$ with respect to $t$, not $\\omega$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["On_the_amount_of_information_supplied_by_truncated_samples_of_grouped_observations_in_the_estimation_of_the_parameters_of_normal_populations.md"], "background_text": "The cumulant generating function $\\kappa(t,\\omega)$ is given by $\\kappa(t,\\omega)=M(t+\\omega)-M(\\omega)$, where $M$ is the antiderivative of $m$. The constant $c$ is found by setting $\\kappa=0$, leading to $c=-M(\\omega)$. The distribution with this cumulant generating function is of exponential-type.", "folder": "", "qid": "statistic-mcqm-2935", "question": "Given the cumulant generating function $\\kappa(t,\\omega)=M(t+\\omega)-M(\\omega)$, which of the following statements are correct?", "options": ["A) The function $\\kappa(t,\\omega)$ is derived by integrating $m(t+\\omega)$ with respect to $t$.", "B) The constant $c$ in the expression $\\kappa(t,\\omega)=\\int_{\\mathbf{\\Omega}}m(t+\\omega)d t+c$ is determined by setting $\\kappa=0$ and solving for $c$.", "C) The distribution with cumulant generating function $\\kappa(t,\\omega)$ is of exponential-type.", "D) The function $M(t+\\omega)$ is the antiderivative of $m(t+\\omega)$ with respect to $\\omega$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The cumulant generating function $\\kappa(t,\\omega)$ is given by $\\kappa(t,\\omega)=M(t+\\omega)-M(\\omega)$, where $M$ is the antiderivative of $m$. The constant $c$ is found by setting $\\kappa=0$, leading to $c=-M(\\omega)$. The distribution with this cumulant generating function is of exponential-type.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "To determine the correct options, let's analyze each choice step-by-step:\n\n- **A) Correct**: The weights are explicitly given as $(j\\pi)^{-2}$ in the context.\n- **B) Correct**: The context states that the test statistic satisfies the conditions of Theorem 3.4, which is necessary for the asymptotic distribution.\n- **C) Incorrect**: While the asymptotic result implies $n$ approaches infinity, the context does not explicitly state this as a condition for the distribution.\n- **D) Incorrect**: The distribution is derived under the null hypothesis, but the context does not state that $H_0$ must be true for the asymptotic distribution to hold.\n\n**Correct Choices**: A, B\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Goodness-of-Fit-Tests-for-a-Multivariate-Distribut.md"], "background_text": "The test statistic in Examples (1) satisfies the conditions of Theorem 3.4 and hence follows asymptotically the distribution of an infinite weighted sum of independent $\\chi_{[1]}^2$ random variables with weights given by $(j\\pi)^{-2}$.", "folder": "", "qid": "statistic-mcqm-4216", "question": "Which of the following conditions must be satisfied for the test statistic $T_n$ to follow asymptotically the distribution of an infinite weighted sum of independent $\\chi_{[1]}^2$ random variables?", "options": ["A) The weights are given by $(j\\pi)^{-2}$.", "B) The test statistic satisfies the conditions of Theorem 3.4.", "C) The sample size $n$ approaches infinity.", "D) The null hypothesis $H_0$ is true."], "category": "PARTIALLY_CORRECT", "question_context_used": "The test statistic in Examples (1) satisfies the conditions of Theorem 3.4 and hence follows asymptotically the distribution of an infinite weighted sum of independent $\\chi_{[1]}^2$ random variables with weights given by $(j\\pi)^{-2}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- Option A is correct as it represents the first-order bias approximation for the AR(1) model without a constant.\n- Option B is correct as it represents the first-order bias approximation for the AR(1) model with a constant included.\n- Option C is incorrect as it does not match the given bias approximations.\n- Option D is incorrect as it suggests there is no bias, which contradicts the provided context.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["The-ability-to-correct-the-bias-in-the-stable-AD_2.md"], "background_text": "In the stable AR(1) model without a constant, the bias of the AR(1)-coefficient estimator to the order $O(T^{-1})$ is given by $\\mathbb{E}[\\hat{\\lambda}-\\lambda]=-2\\lambda/T$, where $\\lambda$ denotes the AR(1)-coefficient. When a constant is included, the first-order bias approximation equals $-(1+3\\lambda)/T$.", "folder": "", "qid": "statistic-mcqm-5786", "question": "In the AR(1) model without a constant, the first-order bias approximation of the AR(1)-coefficient estimator is given by:", "options": ["A) $\\mathbb{E}[\\hat{\\lambda}-\\lambda]=-2\\lambda/T$", "B) $\\mathbb{E}[\\hat{\\lambda}-\\lambda]=-(1+3\\lambda)/T$", "C) $\\mathbb{E}[\\hat{\\lambda}-\\lambda]=\\lambda/T$", "D) $\\mathbb{E}[\\hat{\\lambda}-\\lambda]=0$"], "category": "PARTIALLY_CORRECT", "question_context_used": "In the stable AR(1) model without a constant, the bias of the AR(1)-coefficient estimator to the order $O(T^{-1})$ is given by $\\mathbb{E}[\\hat{\\lambda}-\\lambda]=-2\\lambda/T$, where $\\lambda$ denotes the AR(1)-coefficient. When a constant is included, the first-order bias approximation equals $-(1+3\\lambda)/T$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\"answer\": [\"A\"]}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C, D\n\n- **Option A** is incorrect because $T_{n}^{Z X}$ does not perform better than $T_{n}$ in high-dimensional cases; in fact, $T_{n}$ performs significantly better.\n- **Option B** is correct as $T_{n}$ is less affected by the curse of dimensionality compared to $T_{n}^{Z X}$.\n- **Option C** is correct because in low-dimensional cases, $T_{n}^{Z X}$ has slightly higher empirical power than $T_{n}$.\n- **Option D** is correct as both $T_{n}$ and $T_{n}^{Z X}$ are affected by the dimension $p$.\n\n**Final Answer**: The final answer is $\\boxed{{B, C, D}}$", "source": ["A-goodness-of-fit-test-for-variable-adju_2019_Comp.md"], "background_text": "In low-dimensional cases, $T_{n}^{Z X}$ is comparable to $T_{n}$ with slightly higher empirical power. In high-dimensional cases, $T_{n}$ performs significantly better than $T_{n}^{Z X}$ and is less affected by the curse of dimensionality. Both tests are affected by the dimension $p$.", "folder": "", "qid": "statistic-mcqm-5906", "question": "In the simulation studies, which of the following statements about the tests $T_{n}$ and $T_{n}^{Z X}$ are correct?", "options": ["A) $T_{n}^{Z X}$ performs better than $T_{n}$ in high-dimensional cases.", "B) $T_{n}$ is less affected by the curse of dimensionality compared to $T_{n}^{Z X}$.", "C) In low-dimensional cases, $T_{n}^{Z X}$ has slightly higher empirical power than $T_{n}$.", "D) Both $T_{n}$ and $T_{n}^{Z X}$ are affected by the dimension $p$."], "category": "CORRECT", "question_context_used": "In low-dimensional cases, $T_{n}^{Z X}$ is comparable to $T_{n}$ with slightly higher empirical power. In high-dimensional cases, $T_{n}$ performs significantly better than $T_{n}^{Z X}$ and is less affected by the curse of dimensionality. Both tests are affected by the dimension $p$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**: \n- **Option A** is correct because the general model includes unknown parameters $c_{r_{1}...r_{k}}$.\n- **Option C** is correct because the model can include a transformation of the response variable $y$ as shown in the generalization.\n- **Option D** is correct because the absence of converging arrows means the model cannot be simplified.\n- **Option B** is incorrect because the sets of functions $\\phi_{s}$ are linearly independent, not dependent.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Semi-parametric-nonlinear-regression-and-transf_20.md"], "background_text": "The general model is given by $y=\\sum_{r_{1}=1}^{q_{1}}\\cdot\\cdot\\cdot\\sum_{r_{k}=1}^{q_{k}}c_{r_{1}\\ldots r_{k}}\\phi_{r_{1}}(x_{1})\\ldots\\phi_{r_{k}}(x_{k})$, where $c_{r_{1}...r_{k}}$ are unknown parameters and the sets of functions $\\phi_{s}$ are linearly independent. A generalization includes a transformation of $y$ as $f(y)$, and simplification is not possible due to no converging arrows.", "folder": "", "qid": "statistic-mcqm-5154", "question": "Which of the following are characteristics of the general model in functional networks?", "options": ["A) It includes unknown parameters $c_{r_{1}...r_{k}}$.", "B) The sets of functions $\\phi_{s}$ are linearly dependent.", "C) It can include a transformation of the response variable $y$.", "D) The model cannot be simplified due to the absence of converging arrows."], "category": "CORRECT", "question_context_used": "The general model is given by $y=\\sum_{r_{1}=1}^{q_{1}}\\cdot\\cdot\\cdot\\sum_{r_{k}=1}^{q_{k}}c_{r_{1}\\ldots r_{k}}\\phi_{r_{1}}(x_{1})\\ldots\\phi_{r_{k}}(x_{k})$, where $c_{r_{1}...r_{k}}$ are unknown parameters and the sets of functions $\\phi_{s}$ are linearly independent. A generalization includes a transformation of $y$ as $f(y)$, and simplification is not possible due to no converging arrows.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A) Correct**: The text explicitly states that the 95% confidence interval for $\\beta_{J}$ contains zero.\n- **B) Correct**: The table shows the 95% interval for $\\mu_{k}$ as [-1.0877,1.0743], which includes zero.\n- **C) Incorrect**: The 95% interval for $p$ is [-0.5960,-0.4462], which does not contain zero.\n- **D) Incorrect**: The 95% interval for $K$ is [0.0002,0.0013], which does not contain zero.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Leverage--heavy-tails-and-correlated-jumps-in-_200.md"], "background_text": "The posterior estimates for the SVLCJ model include parameters with 95% confidence intervals. For $\\beta_{J}$, although the posterior mean is negative, its 95% confidence interval contains zero. Similar observations are noted for other parameters.", "folder": "", "qid": "statistic-mcqm-3834", "question": "Based on the posterior estimates of the SVLCJ model for S&P500 return, which parameters have their 95% confidence intervals containing zero?", "options": ["A) $\\beta_{J}$", "B) $\\mu_{k}$", "C) $p$", "D) $K$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The posterior estimates for the SVLCJ model include parameters with 95% confidence intervals. For $\\beta_{J}$, although the posterior mean is negative, its 95% confidence interval contains zero. Similar observations are noted for other parameters.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A)** Correct. The formula for BIC is accurately described in the context.\n- **B)** Incorrect. The context states that neither BIC nor the deviance information criterion is ideally suited for model selection when data are missing.\n- **C)** Correct. The BIC values for models I and II are given as approximately 43975 and 43950, respectively, with model II showing a better fit.\n- **D)** Correct. The context mentions that since the amount of missing data is identical for both models, the BIC values should not be too misleading.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Using Bayesian Inference to Understand the Allocat.md"], "background_text": "The BIC for standard (non-hierarchical) models is calculated as $-2\\log(\\hat{l}) + p\\log(n)$, where $\\hat{l}$ is the maximized likelihood value, $p$ is the number of fitted parameters, and $n=6981$ is the sample size. The BIC is not ideally suited for model selection when data are missing. For models I and II, the BIC values are approximately 43975 and 43950, respectively, indicating that model II provides a better fit. However, these results should be treated with caution due to the presence of missing data.", "folder": "", "qid": "statistic-mcqm-4253", "question": "Given the context of model selection with missing data, which of the following statements are correct regarding the Bayes Information Criterion (BIC)?", "options": ["A) The BIC is calculated as $-2\\log(\\hat{l}) + p\\log(n)$, where $\\hat{l}$ is the maximized likelihood value, $p$ is the number of fitted parameters, and $n$ is the sample size.", "B) The BIC is ideally suited for model selection when data are missing.", "C) The BIC values for models I and II are approximately 43975 and 43950, respectively, indicating that model II provides a better fit.", "D) The presence of missing data does not affect the BIC values because the amount of missing data is identical for both models."], "category": "PARTIALLY_CORRECT", "question_context_used": "The BIC for standard (non-hierarchical) models is calculated as $-2\\log(\\hat{l}) + p\\log(n)$, where $\\hat{l}$ is the maximized likelihood value, $p$ is the number of fitted parameters, and $n=6981$ is the sample size. The BIC is not ideally suited for model selection when data are missing. For models I and II, the BIC values are approximately 43975 and 43950, respectively, indicating that model II provides a better fit. However, these results should be treated with caution due to the presence of missing data.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "Let's evaluate each option based on the drift condition for polynomial ergodicity:\n\n- **A)** The function $V$ is indeed required to satisfy $V\\geq1$ as part of the drift condition.\n- **B)** The constant $\\alpha$ must satisfy $0\\leq\\alpha<1$ for the drift condition to imply polynomial ergodicity.\n- **C)** The set $C$ must be small, meaning there exist $n>0$, $\\delta>0$, and a probability measure $\\nu$ such that $P^{n}(x,\\cdot)\\geq\\delta\\nu(\\cdot)$ for all $x$ in $C$.\n- **D)** The convergence rate in total variation norm is indeed $\\alpha/(1-\\alpha)$, as derived from the drift condition.\n\n**Correct Choices**: A, B, C, D\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Scandinavian J Statistics - 2007 - JARNER - Conver.md"], "background_text": "Polynomial ergodicity is characterized by a drift condition involving a function $V$, constants $c$, $b$, and $\\alpha$, and a small set $C$. The convergence rate in total variation norm is given by $\\alpha/(1-\\alpha)$.", "folder": "", "qid": "statistic-mcqm-5420", "question": "For a Markov chain with polynomial ergodicity, which of the following statements are true regarding the drift condition $P V\\leq V-c V^{\\alpha}+b1_{C}$?", "options": ["A) The function $V$ must satisfy $V\\geq1$.", "B) The constant $\\alpha$ must satisfy $0\\leq\\alpha<1$.", "C) The set $C$ must be small.", "D) The convergence rate in total variation norm is $\\alpha/(1-\\alpha)$."], "category": "CORRECT", "question_context_used": "Polynomial ergodicity is characterized by a drift condition involving a function $V$, constants $c$, $b$, and $\\alpha$, and a small set $C$. The convergence rate in total variation norm is given by $\\alpha/(1-\\alpha)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct: This is the heuristic definition of the intensity process.\n- **B)** Correct: This is the explicit form of the intensity process given in the context.\n- **C)** Correct: The intensity process $\\tilde{\\lambda}(t)$ depends on $\\lambda\\{t,Z^{H}(t)\\}$, which involves the covariate history.\n- **D)** Incorrect: This statement is true for the hypothetical randomised study, not the observational data.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Semiparametric_inference_in_observational_duration-response_studies,_with_duration_possibly_right-censored.md"], "background_text": "The intensity process $\\tilde{\\lambda}(t)$ for the observational data is defined heuristically as $\\tilde{\\lambda}(t)dt = \\mathrm{pr}\\{d A(t)=1|\\mathcal{F}(t^{-})\\}$ and is equal to $\\lambda\\{t,Z^{H}(t)\\}I(U\\geqslant t)$. In the hypothetical randomised study, the intensity process is not affected by the introduction of the covariate history $Z^{H}(t)$ due to randomisation.", "folder": "", "qid": "statistic-mcqm-1826", "question": "Which of the following are correct about the intensity process $\\tilde{\\lambda}(t)$ in the observational data?", "options": ["A) $\\tilde{\\lambda}(t)$ is defined heuristically as $\\tilde{\\lambda}(t)dt = \\mathrm{pr}\\{d A(t)=1|\\mathcal{F}(t^{-})\\}$.", "B) $\\tilde{\\lambda}(t)$ is equal to $\\lambda\\{t,Z^{H}(t)\\}I(U\\geqslant t)$.", "C) The intensity process depends on the covariate history $Z^{H}(t)$.", "D) The intensity process is not affected by the introduction of the covariate history $Z^{H}(t)$ in the filtration for the hypothetical randomised study."], "category": "CORRECT", "question_context_used": "The intensity process $\\tilde{\\lambda}(t)$ for the observational data is defined heuristically as $\\tilde{\\lambda}(t)dt = \\mathrm{pr}\\{d A(t)=1|\\mathcal{F}(t^{-})\\}$ and is equal to $\\lambda\\{t,Z^{H}(t)\\}I(U\\geqslant t)$. In the hypothetical randomised study, the intensity process is not affected by the introduction of the covariate history $Z^{H}(t)$ due to randomisation.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C\n\n**Explanation**: \n- **Option B** is correct because under the null hypothesis, $\\hat{B}$ converges to $\\beta_{0}$ as stated in the background text.\n- **Option C** is correct because the matrix $B$ plays no role in the model under the null hypothesis, implying $\\hat{B}$ does not contribute to the model.\n- **Option A** is incorrect because it describes the behavior of $\\hat{B}$ when $\\beta_{0}$ is not a linear combination of the columns of $B$, not under the null hypothesis.\n- **Option D** is incorrect because orthogonality is mentioned when (A1) is not true, not under the null hypothesis.\n\n**Final Answer**: The final answer is $\\boxed{{B, C}}$", "source": ["A-goodness-of-fit-test-for-variable-adju_2019_Comp.md"], "background_text": "Under the null hypothesis with $C_{n}=0$, the matrix $B$ plays no role in the model and then the estimator $\\hat{B}$ is still convergent to $\\beta_{0}$.", "folder": "", "qid": "statistic-mcqm-5985", "question": "Under the null hypothesis with $C_{n}=0$, what can be said about the estimator $\\hat{B}$?", "options": ["A) $\\hat{B}$ estimates a basis matrix of $span([\\beta_{0},B])$.", "B) $\\hat{B}$ is still convergent to $\\beta_{0}$.", "C) $\\hat{B}$ plays no role in the model.", "D) $\\hat{B}$ becomes orthogonal to each column of $B$."], "category": "CORRECT", "question_context_used": "Under the null hypothesis with $C_{n}=0$, the matrix $B$ plays no role in the model and then the estimator $\\hat{B}$ is still convergent to $\\beta_{0}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A)** Correct. The cumulative noncentral beta distribution function involves an infinite series summation as shown in the equation $I_{x}(a,b;\\lambda)=\\int_{j=0}^{\\infty}q(j;\\lambda/2)\\cdot I_{x}(a+j,b;0).$\n- **B)** Correct. The text explicitly states that the noncentral beta distribution can be used to evaluate the noncentral ${\\pmb F}$ distribution function.\n- **C)** Correct. The parameters and variables are restricted as stated in the text.\n- **D)** Incorrect. The noncentral beta distribution is applicable for $\\lambda \\geqslant 0,$ not just when $\\lambda = 0.$\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Computing Noncentral Beta Probabilities.md"], "background_text": "The cumulative noncentral beta distribution function is given by an integral involving a Poisson distribution. It can also be used to evaluate the noncentral ${\\pmb F}$ distribution function. The parameters and variables are restricted as follows: $0 \\leqslant x \\leqslant 1, a > 0, b > 0,$ and $\\lambda \\geqslant 0.$", "folder": "", "qid": "statistic-mcqm-5686", "question": "Which of the following statements about the noncentral beta distribution are correct?", "options": ["A) The cumulative noncentral beta distribution function involves an infinite series summation.", "B) The noncentral beta distribution can be used to evaluate the noncentral ${\\pmb F}$ distribution function.", "C) The parameters of the noncentral beta distribution must satisfy $0 \\leqslant x \\leqslant 1, a > 0, b > 0,$ and $\\lambda \\geqslant 0.$", "D) The noncentral beta distribution is only applicable when $\\lambda = 0.$"], "category": "CORRECT", "question_context_used": "The cumulative noncentral beta distribution function is given by an integral involving a Poisson distribution. It can also be used to evaluate the noncentral ${\\pmb F}$ distribution function. The parameters and variables are restricted as follows: $0 \\leqslant x \\leqslant 1, a > 0, b > 0,$ and $\\lambda \\geqslant 0.$", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C\n\n- **A** is incorrect because the estimator is doubly robust, not solely dependent on the outcome following a linear mixed-effects model.\n- **B** is correct as it accurately describes the double robustness property of $\\hat{\\theta}_{w}$.\n- **C** is correct because Theorem 1 explicitly states the asymptotic normality of $n^{1/2}(\\hat{\\theta}_{w}-\\theta_{N})$ with variance $V_{1}+V_{2}$.\n- **D** is incorrect because the consistency of $\\hat{\\theta}_{w}$ does depend on the correctness of either the outcome model or the propensity score model.\n\n**Final Answer**: The final answer is $\\boxed{{B, C}}$", "source": ["Soft_calibration_for_selection_bias_problems_under_mixed-effects_models.md"], "background_text": "Theorem 1 states that the soft-calibration estimator $\\hat{\\theta}_{w}$ is doubly robust, meaning it remains consistent under correct specification of either the outcome model or the propensity score model. It also describes the asymptotic normality of the estimator.", "folder": "", "qid": "statistic-mcqm-5559", "question": "What are the implications of Theorem 1 regarding the soft-calibration estimator $\\hat{\\theta}_{w}$?", "options": ["A) The estimator $\\hat{\\theta}_{w}$ is consistent only if the outcome $y_{i}$ follows a linear mixed-effects model.", "B) The estimator $\\hat{\\theta}_{w}$ is doubly robust, requiring either the outcome to follow a linear mixed-effects model or the propensity score model to be correctly specified.", "C) The asymptotic distribution of $n^{1/2}(\\hat{\\theta}_{w}-\\theta_{N})$ is normal with mean 0 and variance $V_{1}+V_{2}$.", "D) The consistency of $\\hat{\\theta}_{w}$ does not depend on the correctness of the propensity score model."], "category": "CORRECT", "question_context_used": "Theorem 1 states that the soft-calibration estimator $\\hat{\\theta}_{w}$ is doubly robust, meaning it remains consistent under correct specification of either the outcome model or the propensity score model. It also describes the asymptotic normality of the estimator.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n**Explanation**:\n- **A** is correct because the text explicitly states that $\\mathbf{K}_{3,\\mathbf{w}}^{T}\\mathbf{K}_{3,\\mathbf{w}}=2\\gamma^{2}\\mathbf{I}_{3}$, which is proportional to a $3\\times3$ identity matrix.\n- **B** is incorrect because the text mentions that right singular vectors of $\\mathbf{K}_{3,\\mathbf{w}}$ do not give any hint of $\\mathbf{1}_{3}^{T}\\mathbf{w}$ being the projection with maximal skewness.\n- **C** is incorrect as the text provides a cautionary note against always using the third standardized moment's right singular vectors as starting projections, citing an example where their performance is poor.\n- **D** is correct because the text recommends a closer examination of the singular value decomposition of $\\mathbf{K}_{3,\\mathbf{w}}$ before proceeding with the skewness-maximizing routine.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Skewness-based-projection-pursuit--A-compu_2018_Co.md"], "background_text": "The text discusses the use of dominant right singular vectors of the third standardized moment as starting projections for skewness maximization, highlighting a case where their performance is poor due to the structure of the third cumulant matrix.", "folder": "", "qid": "statistic-mcqm-5189", "question": "Given the trivariate random vector $\\dot{\\pmb w}=(W_{1},W_{2},W_{3})^{T}$ with density $2\\phi\\left(w_{1}\\right)\\phi\\left(w_{2}\\right)\\phi\\left(w_{3}\\right)\\phi\\left(\\alpha w_{1}w_{2}w_{3}\\right)$ and third cumulant $\\mathtt{K}_{3,\\mathbf{w}}$, which of the following statements are true?", "options": ["A) $\\mathbf{K}_{3,\\mathbf{w}}^{T}\\mathbf{K}_{3,\\mathbf{w}}=2\\gamma^{2}\\mathbf{I}_{3}$ is proportional to a $3\\times3$ identity matrix.", "B) Right singular vectors of $\\mathbf{K}_{3,\\mathbf{w}}$ give a hint of $\\mathbf{1}_{3}^{T}\\mathbf{w}$ being the projection with maximal skewness.", "C) The third standardized moment's right singular vectors are always the best starting projections for skewness maximization.", "D) A closer look at the singular value decomposition of $\\mathbf{K}_{3,\\mathbf{w}}$ is recommended before using its right dominant singular vector for starting the skewness-maximizing routine."], "category": "CORRECT", "question_context_used": "The text discusses the use of dominant right singular vectors of the third standardized moment as starting projections for skewness maximization, highlighting a case where their performance is poor due to the structure of the third cumulant matrix.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}", "model_answer": ["B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The Hellinger distance reaches its maximum value $\\sqrt{2}$ when the overlap between $p_{1}$ and $p_{2}$ is zero, i.e., when $\\mu(\\Omega_{1}\\cap\\Omega_{2})=0$.\n- **B)** Correct. This is the exact definition of the Hellinger distance as provided in the context.\n- **C)** Incorrect. The Hellinger distance does depend on the normalizing constants of $p_{1}$ and $p_{2}$ because it involves the integral of the product of their square roots.\n- **D)** Correct. The Hellinger distance is minimized to 0 when $p_{1}$ and $p_{2}$ are identical, indicating perfect overlap.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Warp_Bridge_Sampling__Warp_Bridge_Sampling.md"], "background_text": "The Hellinger distance between two probability distributions $p_{1}$ and $p_{2}$ is defined as $H(p_{1}, p_{2}) = \\left[\\int(\\sqrt{p_{1}(w)} - \\sqrt{p_{2}(w)})^{2}\\mu(d w)\\right]^{\\frac{1}{2}} = \\left[2(1-\\int\\sqrt{p_{1}(w)p_{2}(w)}\\mu(d w))\\right]^{\\frac{1}{2}}$. It reaches its maximum value $\\sqrt{2}$ when the overlap between $p_{1}$ and $p_{2}$ is zero, and is minimized to 0 when $p_{1}$ and $p_{2}$ are identical.", "folder": "", "qid": "statistic-mcqm-4935", "question": "Which of the following statements about the Hellinger distance $H(p_{1}, p_{2})$ are correct?", "options": ["A) The Hellinger distance reaches its maximum value when the overlap between $p_{1}$ and $p_{2}$ is zero.", "B) The Hellinger distance is defined as $H(p_{1}, p_{2}) = \\left[2(1-\\int\\sqrt{p_{1}(w)p_{2}(w)}\\mu(d w))\\right]^{\\frac{1}{2}}$.", "C) The Hellinger distance does not depend on the normalizing constants of $p_{1}$ and $p_{2}$.", "D) The Hellinger distance is minimized when $p_{1}$ and $p_{2}$ are identical."], "category": "CORRECT", "question_context_used": "The Hellinger distance between two probability distributions $p_{1}$ and $p_{2}$ is defined as $H(p_{1}, p_{2}) = \\left[\\int(\\sqrt{p_{1}(w)} - \\sqrt{p_{2}(w)})^{2}\\mu(d w)\\right]^{\\frac{1}{2}} = \\left[2(1-\\int\\sqrt{p_{1}(w)p_{2}(w)}\\mu(d w))\\right]^{\\frac{1}{2}}$. It reaches its maximum value $\\sqrt{2}$ when the overlap between $p_{1}$ and $p_{2}$ is zero, and is minimized to 0 when $p_{1}$ and $p_{2}$ are identical.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **Option A** is correct because Theorem 1(a) states that the solution is unique if and only if $m_{f,+} \\neq 0$ and $m_{f,-} \\neq 0$ almost everywhere on $\\mathbb{R}$.\n- **Option B** is correct because Theorem 1(b) states that the equation has a solution for all $u\\nu_{1}$ in $L^{2}(\\mathbb{R}^{\\times},|x|^{c}\\mathrm{d}x)$ if and only if $\\inf_{x\\in\\mathbb{R}} |m_{f,+}(x)| > 0$ and $\\inf_{x\\in\\mathbb{R}} |m_{f,-}(x)| > 0$.\n- **Option C** is correct because the integrability condition (7) is necessary for the operator $\\mathcal{G}$ to be bounded and for the integral equation to be well-defined.\n- **Option D** is incorrect because the multiplicativity of $u$ is a property used in the derivation of the equation but is not a condition for the existence or uniqueness of solutions.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$.", "source": ["Scandinavian J Statistics - 2021 - Glück - A solut.md"], "background_text": "The integral equation (6) is given by $(u\\nu_{1})(x) = \\int_{\\mathrm{supp}(f)} \\frac{u(f(s))}{|f(s)|} (u\\nu_{0})\\left(\\frac{x}{f(s)}\\right) \\mathrm{d}s$. The conditions for the existence and uniqueness of solutions are provided in Theorem 1, which states that the solution is unique for all $u\\nu_{1}$ in $L^{2}(\\mathbb{R}^{\\times},|x|^{c}\\mathrm{d}x)$ if and only if $m_{f,+} \\neq 0$ and $m_{f,-} \\neq 0$ almost everywhere on $\\mathbb{R}$. The equation has a solution for all $u\\nu_{1}$ in the same space if and only if $\\inf_{x\\in\\mathbb{R}} |m_{f,+}(x)| > 0$ and $\\inf_{x\\in\\mathbb{R}} |m_{f,-}(x)| > 0$. The integrability condition (7) is $\\int_{\\operatorname{supp}(f)} |f(s)|^{\\beta + \\frac{c-1}{2}} \\mathrm{d}s < \\infty$.", "folder": "", "qid": "statistic-mcqm-5335", "question": "Which of the following conditions must be satisfied for the integral equation (6) to have a unique solution $u\\nu_{0} \\in L^{2}(\\mathbb{R}^{\\times},|x|^{c}\\mathrm{d}x)$ for all $u\\nu_{1} \\in L^{2}(\\mathbb{R}^{\\times},|x|^{c}\\mathrm{d}x)$?", "options": ["A) $m_{f,+} \\neq 0$ and $m_{f,-} \\neq 0$ almost everywhere on $\\mathbb{R}$.", "B) $\\inf_{x\\in\\mathbb{R}} |m_{f,+}(x)| > 0$ and $\\inf_{x\\in\\mathbb{R}} |m_{f,-}(x)| > 0$.", "C) The integrability condition (7) is satisfied.", "D) The function $u$ is multiplicative."], "category": "PARTIALLY_CORRECT", "question_context_used": "The integral equation (6) is given by $(u\\nu_{1})(x) = \\int_{\\mathrm{supp}(f)} \\frac{u(f(s))}{|f(s)|} (u\\nu_{0})\\left(\\frac{x}{f(s)}\\right) \\mathrm{d}s$. The conditions for the existence and uniqueness of solutions are provided in Theorem 1, which states that the solution is unique for all $u\\nu_{1}$ in $L^{2}(\\mathbb{R}^{\\times},|x|^{c}\\mathrm{d}x)$ if and only if $m_{f,+} \\neq 0$ and $m_{f,-} \\neq 0$ almost everywhere on $\\mathbb{R}$. The equation has a solution for all $u\\nu_{1}$ in the same space if and only if $\\inf_{x\\in\\mathbb{R}} |m_{f,+}(x)| > 0$ and $\\inf_{x\\in\\mathbb{R}} |m_{f,-}(x)| > 0$. The integrability condition (7) is $\\int_{\\operatorname{supp}(f)} |f(s)|^{\\beta + \\frac{c-1}{2}} \\mathrm{d}s < \\infty$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A)** Correct. This is the statement of the Cramér–Rao inequality in $L^{2}(\\mu)$.\n- **B)** Correct. The estimator $\\hat{u}=X$ attains equality in the Cramér–Rao inequality.\n- **C)** Incorrect. The Cramér–Rao lower bound does not depend on the parameter $u\\in\\Theta$ in this context.\n- **D)** Incorrect. The inequality applies to any unbiased estimator, not necessarily adapted.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Functional-Cram-r-Rao-bounds-and-Stein-estimators-.md"], "background_text": "The Cramér–Rao inequality in $L^{2}(\\mu)$ for unbiased estimators states that $\\mathsf{E}^{u}(\\|\\xi-u\\|_{L^{2}(\\mu)}^{2})\\geq\\int_{0}^{T}t\\mu(d t)$ for every $u\\in\\Theta$, with equality attained by the estimator $\\hat{u}=X$. The inequality does not depend on the parameter $u\\in\\Theta$ and applies to any unbiased estimator, not necessarily adapted.", "folder": "", "qid": "statistic-mcqm-6165", "question": "Which of the following are true about the Cramér–Rao inequality in $L^{2}(\\mu)$ for unbiased estimators?", "options": ["A) The inequality states that $\\mathsf{E}^{u}(\\|\\xi-u\\|_{L^{2}(\\mu)}^{2})\\geq\\int_{0}^{T}t\\mu(d t)$ for every $u\\in\\Theta$.", "B) Equality in the Cramér–Rao inequality is attained by the estimator $\\hat{u}=X$.", "C) The Cramér–Rao lower bound depends on the parameter $u\\in\\Theta$.", "D) The inequality applies only to adapted estimators."], "category": "CORRECT", "question_context_used": "The Cramér–Rao inequality in $L^{2}(\\mu)$ for unbiased estimators states that $\\mathsf{E}^{u}(\\|\\xi-u\\|_{L^{2}(\\mu)}^{2})\\geq\\int_{0}^{T}t\\mu(d t)$ for every $u\\in\\Theta$, with equality attained by the estimator $\\hat{u}=X$. The inequality does not depend on the parameter $u\\in\\Theta$ and applies to any unbiased estimator, not necessarily adapted.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A) Correct**: The text explicitly states that the procedure does not require explicit evaluation of any Edgeworth polynomials.\n- **B) Incorrect**: The text mentions that the procedure is computationally much less involved than the traditional double bootstrap percentile method, not more expensive.\n- **C) Correct**: The text describes equating the coverage probability to $\\alpha$ to yield the given expression for $\\gamma^{1/2}$.\n- **D) Correct**: The text states that $k=\\gamma m_{0}$, where $m_{0}$ is a fixed proportion of $n$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Scandinavian J Statistics - 2001 - Chung - Optimal.md"], "background_text": "The double bootstrap procedure approximates $\\hat{m}_{1}$ and $\\hat{\\delta}_{1}$ without explicit evaluation of Edgeworth polynomials, at the expense of increased computational cost. The coverage probability is equated to $\\alpha$ to find $\\gamma^{1/2}=\\hat{r}$. The double bootstrap sample size $k$ is set as $k=\\gamma m_{0}$, where $m_{0}$ is a fixed proportion of $n$.", "folder": "", "qid": "statistic-mcqm-1020", "question": "In the double bootstrap procedure described, which of the following statements are correct regarding the approximation of $\\hat{m}_{1}$ and $\\hat{\\delta}_{1}$?", "options": ["A) The procedure does not require explicit evaluation of Edgeworth polynomials.", "B) The procedure is computationally more expensive than the traditional double bootstrap percentile method.", "C) The coverage probability (8) is equated to $\\alpha$ to yield $\\gamma^{1/2}=\\hat{r}=-\\hat{p}_{1}(z_{\\alpha})/\\hat{q}_{1}(z_{\\alpha})+O_{p}(m_{0}^{-1/2})$.", "D) The double bootstrap sample size $k$ is set to be a fixed proportion of $n$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The double bootstrap procedure approximates $\\hat{m}_{1}$ and $\\hat{\\delta}_{1}$ without explicit evaluation of Edgeworth polynomials, at the expense of increased computational cost. The coverage probability is equated to $\\alpha$ to find $\\gamma^{1/2}=\\hat{r}$. The double bootstrap sample size $k$ is set as $k=\\gamma m_{0}$, where $m_{0}$ is a fixed proportion of $n$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **Option A** is correct as it directly follows from the independence assumption and the given characteristic functions.\n- **Option B** is also correct as it is derived from the properties of the characteristic functions and the independence assumption.\n- **Option C** is incorrect because it incorrectly relates the characteristic functions of $W_{j}$ and $T$.\n- **Option D** is incorrect as there is no such summation property given for the characteristic functions $\\phi_{f_{U_{j}}}(t)$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Consistency-and-asymptotic-normality-for-a-nonpara.md"], "background_text": "The characteristic functions of $W_{j}$ and $T$ are given by $\\phi_{f_{W_{j}}}(t)=\\phi_{f_{X}}(t)\\phi_{f_{U_{j}}}(t)$ and $\\phi_{f_{T}}(t)=\\phi_{f_{X}}(t)\\phi_{f_{U^{F}}}(t)$, respectively. From the independence assumption, certain equations involving $\\psi_{j}(t)$ and $\\phi_{f_{U_{j}}}(t)$ can be derived.", "folder": "", "qid": "statistic-mcqm-4906", "question": "Given the characteristic functions of $W_{j}$ and $T$ as $\\phi_{f_{W_{j}}}(t)=\\phi_{f_{X}}(t)\\phi_{f_{U_{j}}}(t)$ and $\\phi_{f_{T}}(t)=\\phi_{f_{X}}(t)\\phi_{f_{U^{F}}}(t)$, which of the following equations are correct based on the independence assumption?", "options": ["A) $\\sum_{j=1}^{n}\\psi_{j}(t)\\phi_{f_{U_{j}}}(t)=1$", "B) $\\sum_{j=1}^{n}\\left|\\psi_{j}(t)\\right|^{2}=1/\\sum_{j=1}^{n}\\left|\\phi_{f_{U_{j}}}(t)\\right|^{2}$", "C) $\\phi_{f_{W_{j}}}(t)=\\phi_{f_{T}}(t)\\phi_{f_{U_{j}}}(t)$", "D) $\\sum_{j=1}^{n}\\phi_{f_{U_{j}}}(t)=1$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The characteristic functions of $W_{j}$ and $T$ are given by $\\phi_{f_{W_{j}}}(t)=\\phi_{f_{X}}(t)\\phi_{f_{U_{j}}}(t)$ and $\\phi_{f_{T}}(t)=\\phi_{f_{X}}(t)\\phi_{f_{U^{F}}}(t)$, respectively. From the independence assumption, certain equations involving $\\psi_{j}(t)$ and $\\phi_{f_{U_{j}}}(t)$ can be derived.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": []\n}", "model_answer": [], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: The text explicitly states that $f(x) = (1/\\pi)m_{2}(x) > 0$ for all $x \\in (a, a + \\delta)$.\n- **B) Correct**: The text mentions that $f(x)$ has more in common with $(1/\\pi)((a - x)/C_{2})^{1/2}$ than what can be inferred from (5.3), indicating a 'square root' behavior near $x = a$.\n- **C) Correct**: The text states that $g(x) = 2m_{2}(x)m_{2}^{\\prime}(x)$ is right continuous at $x = a$, which implies that $f(x)$ is also right continuous at $x = a$.\n- **D) Incorrect**: The text does not provide any information about the differentiability of $f(x)$ at $x = a$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Analysis-of-the-Limiting-Spectral-Distribution-of-.md"], "background_text": "The function $f(x) = (1/\\pi)m_{2}(x)$ is discussed in the context of its behavior near and to the right of $x = a$. It is shown that $f(x) > 0$ for all $x \\in (a, a + \\delta)$ and that it exhibits a 'square root' behavior near $x = a$. The function $g(x) = 2m_{2}(x)m_{2}^{\\prime}(x)$ is right continuous at $x = a$.", "folder": "", "qid": "statistic-mcqm-2981", "question": "Which of the following statements about the function $f(x) = (1/\\pi)m_{2}(x)$ are correct based on the provided context?", "options": ["A) $f(x) > 0$ for all $x \\in (a, a + \\delta)$.", "B) $f(x)$ has a 'square root' behavior near $x = a$.", "C) $f(x)$ is right continuous at $x = a$.", "D) $f(x)$ is differentiable at $x = a$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The function $f(x) = (1/\\pi)m_{2}(x)$ is discussed in the context of its behavior near and to the right of $x = a$. It is shown that $f(x) > 0$ for all $x \\in (a, a + \\delta)$ and that it exhibits a 'square root' behavior near $x = a$. The function $g(x) = 2m_{2}(x)m_{2}^{\\prime}(x)$ is right continuous at $x = a$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A) $Q_{n}$**: This is correct because $Q_{n}$ is mentioned as one of the robust scale estimators.\n- **B) $\\tau$-scale**: This is correct because the $\\tau$-scale is described in the context.\n- **C) $P_{n}$**: This is correct because $P_{n}$, the interquartile range of the pairwise means, is mentioned.\n- **D) Mean absolute deviation (MAD)**: This is correct because the median absolute deviation from the median (MAD) is mentioned.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["Robust-estimation-of-precision-matrices-under_2016.md"], "background_text": "The text mentions several robust scale estimators including $Q_{n}$, the $\\tau$-scale, $P_{n}$, its adaptively trimmed variant $\\widetilde{P}_{n}$, the interquartile range (IQR), and the median absolute deviation from the median (MAD).", "folder": "", "qid": "statistic-mcqm-1106", "question": "Which robust scale estimators are mentioned for replacing the variance in the covariance identity?", "options": ["A) $Q_{n}$", "B) $\\tau$-scale", "C) $P_{n}$", "D) Mean absolute deviation (MAD)"], "category": "PARTIALLY_CORRECT", "question_context_used": "The text mentions several robust scale estimators including $Q_{n}$, the $\\tau$-scale, $P_{n}$, its adaptively trimmed variant $\\widetilde{P}_{n}$, the interquartile range (IQR), and the median absolute deviation from the median (MAD).", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A)** True, the text describes the Horvitz–Thompson estimator as $\\hat{\\hat{N}}=n/\\hat{p}$.\n- **B)** True, the text states that inclusion probabilities are modeled as i.i.d. variates from a common distribution $F$.\n- **C)** False, the text mentions that most methods arrive at the expectation via an estimate of $F$.\n- **D)** False, the text discusses how heterogeneous capturability leads to significant uncertainty in estimates.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Low-risk_population_size_estimates_in_the_presence_of_capture_heterogeneity.md"], "background_text": "The Horvitz–Thompson estimator in the context of heterogeneous capturability is given by $\\hat{\\hat{N}}=n/\\hat{p}$, where $n$ is the sample size and $\\hat{p}$ is an estimate of the observation probability. Inclusion probabilities are modeled as i.i.d. variates from a distribution $F$. The expectation $E_{\\hat{F}}(P)$ typically requires an estimate of $F$. Heterogeneous capturability affects the estimator's performance.", "folder": "", "qid": "statistic-mcqm-6009", "question": "Which of the following are correct about the Horvitz–Thompson estimator in the context of heterogeneous capturability?", "options": ["A) It is given by $\\hat{\\hat{N}}=n/\\hat{p}$ where $n$ is the sample size and $\\hat{p}$ is an estimate of the probability of being observed.", "B) It models inclusion probabilities as independent identically distributed variates from a common distribution $F$.", "C) It does not require an estimate of the distribution $F$ to compute the expectation $E_{\\hat{F}}(P)$.", "D) It is unaffected by the presence of heterogeneous capturability."], "category": "CORRECT", "question_context_used": "The Horvitz–Thompson estimator in the context of heterogeneous capturability is given by $\\hat{\\hat{N}}=n/\\hat{p}$, where $n$ is the sample size and $\\hat{p}$ is an estimate of the observation probability. Inclusion probabilities are modeled as i.i.d. variates from a distribution $F$. The expectation $E_{\\hat{F}}(P)$ typically requires an estimate of $F$. Heterogeneous capturability affects the estimator's performance.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **Option A** is correct because it directly states the definition of the supermodular order.\n- **Option B** is correct because it directly states the definition of the increasing supermodular order.\n- **Option C** is correct because every increasing supermodular function is a supermodular function, so the condition for $\\leqslant_{sm}$ is more restrictive than for $\\leqslant_{ism}$.\n- **Option D** is incorrect because not every supermodular function is increasing, so $\\leqslant_{ism}$ does not imply $\\leqslant_{sm}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Some-Remarks-on-the-Supermodular-Order_2000_Journa.md"], "background_text": "The supermodular order ($\\leqslant_{sm}$) is defined by $\\mathbf{X}\\leqslant_{sm}\\mathbf{Y}$ if $E f(\\mathbf{X})\\leqslant E f(\\mathbf{Y})$ for all supermodular functions $f$ such that the expectations exist. The increasing supermodular order ($\\leqslant_{ism}$) is defined by $\\mathbf{X}\\leqslant_{ism}\\mathbf{Y}$ if $E f(\\mathbf{X})\\leqslant E f(\\mathbf{Y})$ for all increasing supermodular functions $f$ such that the expectations exist. The set of all increasing supermodular functions is a subset of all supermodular functions.", "folder": "", "qid": "statistic-mcqm-4952", "question": "Which of the following are true about the supermodular order ($\\leqslant_{sm}$) and the increasing supermodular order ($\\leqslant_{ism}$)?", "options": ["A) $\\mathbf{X}\\leqslant_{sm}\\mathbf{Y}$ if $E f(\\mathbf{X})\\leqslant E f(\\mathbf{Y})$ for all supermodular functions $f$ such that the expectations exist.", "B) $\\mathbf{X}\\leqslant_{ism}\\mathbf{Y}$ if $E f(\\mathbf{X})\\leqslant E f(\\mathbf{Y})$ for all increasing supermodular functions $f$ such that the expectations exist.", "C) $\\mathbf{X}\\leqslant_{sm}\\mathbf{Y}$ implies $\\mathbf{X}\\leqslant_{ism}\\mathbf{Y}$.", "D) $\\mathbf{X}\\leqslant_{ism}\\mathbf{Y}$ implies $\\mathbf{X}\\leqslant_{sm}\\mathbf{Y}$."], "category": "CORRECT", "question_context_used": "The supermodular order ($\\leqslant_{sm}$) is defined by $\\mathbf{X}\\leqslant_{sm}\\mathbf{Y}$ if $E f(\\mathbf{X})\\leqslant E f(\\mathbf{Y})$ for all supermodular functions $f$ such that the expectations exist. The increasing supermodular order ($\\leqslant_{ism}$) is defined by $\\mathbf{X}\\leqslant_{ism}\\mathbf{Y}$ if $E f(\\mathbf{X})\\leqslant E f(\\mathbf{Y})$ for all increasing supermodular functions $f$ such that the expectations exist. The set of all increasing supermodular functions is a subset of all supermodular functions.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**: \n- **Option A** is correct because Theorem 2 states that if $d=1$ and $\\mathcal{X}$ contains at least three points, model (4) is identifiable for any family $\\mathcal{F}$.\n- **Option C** is correct because the theorem also states that if $d=1$ and $\\mathcal{X}$ consists of one or two points, model (4) is non-identifiable for any weakly scalable family $\\mathcal{F}$.\n- **Option B** is incorrect because the non-identifiability is specifically for weakly scalable families, not any family $\\mathcal{F}$.\n- **Option D** is incorrect because the theorem clearly shows that the identifiability depends on the number of points in $\\mathcal{X}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Identifiability-of-cure-models-revisite_2014_Journ.md"], "background_text": "Theorem 2 provides specific conditions for the identifiability of model (4) when $d=1$. It states that the identifiability depends on the number of points in $\\mathcal{X}$ and the scalability of the family $\\mathcal{F}$.", "folder": "", "qid": "statistic-mcqm-1953", "question": "Which of the following statements about the identifiability of model (4) with $d=1$ are correct?", "options": ["A) If $\\mathcal{X}$ contains at least three points, model (4) is identifiable for any family $\\mathcal{F}$", "B) If $\\mathcal{X}$ consists of one or two points, model (4) is non-identifiable for any family $\\mathcal{F}$", "C) If $\\mathcal{X}$ consists of one or two points, model (4) is non-identifiable for any weakly scalable family $\\mathcal{F}$", "D) The identifiability of model (4) does not depend on the number of points in $\\mathcal{X}$"], "category": "PARTIALLY_CORRECT", "question_context_used": "Theorem 2 provides specific conditions for the identifiability of model (4) when $d=1$. It states that the identifiability depends on the number of points in $\\mathcal{X}$ and the scalability of the family $\\mathcal{F}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A) Resampling method**: The coverage errors of the resampling method are close to those of the exact method, as mentioned in the background text.\n- **C) Bootstrap-t method**: The bootstrap-t method also has coverage errors close to those of the exact method, as indicated in the context.\n- **D) Exact method**: The exact method is the benchmark against which the other methods are compared, and its coverage errors are naturally close to itself.\n\n- **B) Naive normal method**: The coverage errors of the naive normal method are substantially larger than $5\\%$, making it incorrect in this context.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Resampling_methods_for_confidence_intervals_in_group_sequential_trials.md"], "background_text": "The coverage errors and averages of the confidence limits of the resampling method are close to those of the exact method. In contrast, the coverage errors of the naive normal and bootstrap confidence limits are substantially larger than $5\\%$ for the upper limit at $(15)^{\\frac{4}{3}}\\mu=-0{\\cdot}5$ and for the lower limit at $(15)^{\\sharp}\\mu=0{\\cdot}5$.", "folder": "", "qid": "statistic-mcqm-3978", "question": "In the context of constructing confidence intervals for the mean when the variance is unknown, which of the following methods have coverage errors close to those of the exact method?", "options": ["A) Resampling method", "B) Naive normal method", "C) Bootstrap-t method", "D) Exact method"], "category": "PARTIALLY_CORRECT", "question_context_used": "The coverage errors and averages of the confidence limits of the resampling method are close to those of the exact method. In contrast, the coverage errors of the naive normal and bootstrap confidence limits are substantially larger than $5\\%$ for the upper limit at $(15)^{\\frac{4}{3}}\\mu=-0{\\cdot}5$ and for the lower limit at $(15)^{\\sharp}\\mu=0{\\cdot}5$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **Option A** is correct because the text explicitly states that it is impossible to identify $\\vartheta$ asymptotically under the given conditions.\n- **Option B** is correct as it directly quotes the text regarding the law of $X$ being the same under the specified choices of parameters.\n- **Option C** is incorrect because the text clearly states the impossibility of estimating $\\vartheta$ without knowledge of $\\sigma_{t}$ and $\\overline{{\\sigma}}_{t}$.\n- **Option D** is incorrect because while the convergence results do depend on the regularity of the processes, the impossibility of identifying $\\vartheta$ is the main point, not the convergence results.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Scandinavian J Statistics - 2019 - Féron - Efficie.md"], "background_text": "In the setting described, it is impossible to identify $\\vartheta$ from data $X_{i\\Delta_{n}},i=1,\\dots,n$ asymptotically when $t\\sim\\sigma_{t}$ and $t\\{\\sim\\overline{{\\sigma}}_{t}$ are unknown. Indeed, $X$ has the same law under the choice of $(\\vartheta,\\sigma,\\overline{{\\sigma}})$ and $(\\vartheta+1,e^{T_{1}-\\cdot}\\sigma,\\overline{{\\sigma}})$.", "folder": "", "qid": "statistic-mcqm-5833", "question": "In the context of estimating $\\vartheta$ from data $X_{i\\Delta_{n}},i=1,\\dots,n$ asymptotically, which of the following statements are correct when $t\\sim\\sigma_{t}$ and $t\\{\\sim\\overline{{\\sigma}}_{t}$ are unknown?", "options": ["A) It is impossible to identify $\\vartheta$ asymptotically under the given conditions.", "B) The law of $X$ is the same under the choice of $(\\vartheta,\\sigma,\\overline{{\\sigma}})$ and $(\\vartheta+1,e^{T_{1}-\\cdot}\\sigma,\\overline{{\\sigma}})$.", "C) The estimation of $\\vartheta$ is always possible regardless of the knowledge of $\\sigma_{t}$ and $\\overline{{\\sigma}}_{t}$.", "D) The convergence results depend solely on the regularity of the processes $b,\\sigma$ and $\\overline{{\\sigma}}$."], "category": "CORRECT", "question_context_used": "In the setting described, it is impossible to identify $\\vartheta$ from data $X_{i\\Delta_{n}},i=1,\\dots,n$ asymptotically when $t\\sim\\sigma_{t}$ and $t\\{\\sim\\overline{{\\sigma}}_{t}$ are unknown. Indeed, $X$ has the same law under the choice of $(\\vartheta,\\sigma,\\overline{{\\sigma}})$ and $(\\vartheta+1,e^{T_{1}-\\cdot}\\sigma,\\overline{{\\sigma}})$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n**Explanation**: \n- **A)** Correct. The hazard function $\\lambda_{i}(t)$ indeed depends on both the baseline covariates $\\pmb{S}_{i}$ and the random effects $\\pmb{b}_{i k}$ through the association structure.\n- **B)** Incorrect. The association parameter $\\gamma_{k}$ measures the strength of association between the random effects for the $k^{\\mathrm{th}}$ longitudinal response and the hazard, not the fixed effects.\n- **C)** Incorrect. The baseline hazard $\\lambda_{0}(t)$ is unspecified (non-parametric) in this model, not parametric.\n- **D)** Correct. The vector function $\\pmb{W}_{k}(t)$ can indeed include various terms such as intercept and slope or natural cubic splines, as mentioned in the text.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Fast-estimation-for-generalised-multivariate-join_.md"], "background_text": "The time-to-event sub-model is given by $\\lambda_{i}(t) = \\lambda_{0}(t)\\exp\\left\\{\\pmb{S}_{i}^{\\top}\\pmb{\\zeta} + \\sum_{k=1}^{K}\\gamma_{k}\\pmb{W}_{k}(t)^{\\top}\\pmb{b}_{i k}\\right\\}$, where $\\pmb{S}_{i}$ are baseline covariates, $\\gamma_{k}$ are association parameters, $\\lambda_{0}(t)$ is an unspecified baseline hazard, and $\\pmb{W}_{k}(t)$ is a vector function of time.", "folder": "", "qid": "statistic-mcqm-1612", "question": "Regarding the time-to-event sub-model in the joint model framework, which of the following statements are true?", "options": ["A) The hazard function $\\lambda_{i}(t)$ depends on the baseline covariates $\\pmb{S}_{i}$ and the random effects $\\pmb{b}_{i k}$.", "B) The association parameter $\\gamma_{k}$ measures the strength of association between the fixed effects for the $k^{\\mathrm{th}}$ longitudinal response and the hazard.", "C) The baseline hazard $\\lambda_{0}(t)$ is specified as a parametric function in the model.", "D) The vector function $\\pmb{W}_{k}(t)$ can include terms like intercept and slope or natural cubic splines."], "category": "PARTIALLY_CORRECT", "question_context_used": "The time-to-event sub-model is given by $\\lambda_{i}(t) = \\lambda_{0}(t)\\exp\\left\\{\\pmb{S}_{i}^{\\top}\\pmb{\\zeta} + \\sum_{k=1}^{K}\\gamma_{k}\\pmb{W}_{k}(t)^{\\top}\\pmb{b}_{i k}\\right\\}$, where $\\pmb{S}_{i}$ are baseline covariates, $\\gamma_{k}$ are association parameters, $\\lambda_{0}(t)$ is an unspecified baseline hazard, and $\\pmb{W}_{k}(t)$ is a vector function of time.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The text defines $\\tilde{\\mu}(x)$ as $\\frac{\\tilde{d}_{T}(x)}{\\tilde{f}_{T}(x)}$.\n- **B) Correct**: The convergence rate of $\\tilde{\\mu}(x)$ to $\\mu(x)$ is given as $O_{p}\\left(\\left(\\frac{v(h)}{n}\\right)^{1/2}+h^{r}\\right)$.\n- **C) Incorrect**: The bandwidth $h$ depends on $n$ (denoted as $h=h_{n}$) and tends to zero as $n$ increases.\n- **D) Correct**: The estimator $\\tilde{f}_{T}(x)$ is defined using the kernel function $K_{T,j,h}$ and the observations $W_{j}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Consistency-and-asymptotic-normality-for-a-nonpara.md"], "background_text": "The estimator $\\tilde{\\mu}(x)$ is defined as the ratio of $\\tilde{d}_{T}(x)$ to $\\tilde{f}_{T}(x)$. It converges to $\\mu(x)$ at a specified rate under certain conditions. The bandwidth $h$ depends on $n$ and tends to zero as $n$ increases. The estimator $\\tilde{f}_{T}(x)$ is constructed using a kernel function and the contaminated observations $W_{j}$.", "folder": "", "qid": "statistic-mcqm-4910", "question": "Which of the following are correct about the estimator $\\tilde{\\mu}(x)$?", "options": ["A) $\\tilde{\\mu}(x)$ is defined as $\\frac{\\tilde{d}_{T}(x)}{\\tilde{f}_{T}(x)}$.", "B) $\\tilde{\\mu}(x)$ converges to $\\mu(x)$ at a rate of $O_{p}\\left(\\left(\\frac{v(h)}{n}\\right)^{1/2}+h^{r}\\right)$.", "C) The bandwidth $h$ is fixed and does not depend on $n$.", "D) The estimator $\\tilde{f}_{T}(x)$ is defined by $\\frac{1}{h}\\sum_{j=1}^{n}K_{T,j,h}\\left(\\frac{x-W_{j}}{h}\\right)$."], "category": "CORRECT", "question_context_used": "The estimator $\\tilde{\\mu}(x)$ is defined as the ratio of $\\tilde{d}_{T}(x)$ to $\\tilde{f}_{T}(x)$. It converges to $\\mu(x)$ at a specified rate under certain conditions. The bandwidth $h$ depends on $n$ and tends to zero as $n$ increases. The estimator $\\tilde{f}_{T}(x)$ is constructed using a kernel function and the contaminated observations $W_{j}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**: \n- **Option A** is correct as it directly represents the Hammersley-Clifford Theorem's formula for the joint density under the positivity condition.\n- **Option C** is correct because it simplifies the theorem's expression by omitting the denominator, which is a valid representation under certain interpretations of the theorem, focusing only on the product of conditionals.\n- **Option B** is incorrect because it uses a summation instead of a product, which does not align with the theorem's multiplicative form.\n- **Option D** is incorrect for the same reason as B, plus it incorrectly applies a summation to the ratio form, which is not part of the theorem's statement.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["An-extension-of-the-factorization-theorem-to-the_2.md"], "background_text": "The Hammersley-Clifford Theorem states that under the positivity condition, the joint density $\\pi(x_{1},\\ldots,x_{n})$ is proportional to a product of ratios of conditional densities for every permutation on ${1,\\ldots,n}$ and every $x^{\\prime}\\in S$.", "folder": "", "qid": "statistic-mcqm-2778", "question": "Under the positivity condition, the joint density $\\pi(x_{1},\\ldots,x_{n})$ is proportional to which of the following expressions?", "options": ["A) $\\prod_{j=1}^{n}{\\frac{\\pi_{r_{j}}(x_{r_{j}}|x_{r_{1}},\\ldots,x_{r_{j-1}},x_{r_{j+1}}^{\\prime},\\ldots,x_{r_{n}}^{\\prime})}{\\pi_{r_{j}}(x_{r_{j}}^{\\prime}|x_{r_{1}},\\ldots,x_{r_{j-1}},x_{r_{j+1}}^{\\prime},\\ldots,x_{r_{n}}^{\\prime})}}$", "B) $\\sum_{j=1}^{n}{\\pi_{r_{j}}(x_{r_{j}}|x_{r_{1}},\\ldots,x_{r_{j-1}},x_{r_{j+1}}^{\\prime},\\ldots,x_{r_{n}}^{\\prime})}$", "C) $\\prod_{j=1}^{n}{\\pi_{r_{j}}(x_{r_{j}}|x_{r_{1}},\\ldots,x_{r_{j-1}},x_{r_{j+1}}^{\\prime},\\ldots,x_{r_{n}}^{\\prime})$", "D) $\\sum_{j=1}^{n}{\\frac{\\pi_{r_{j}}(x_{r_{j}}|x_{r_{1}},\\ldots,x_{r_{j-1}},x_{r_{j+1}}^{\\prime},\\ldots,x_{r_{n}}^{\\prime})}{\\pi_{r_{j}}(x_{r_{j}}^{\\prime}|x_{r_{1}},\\ldots,x_{r_{j-1}},x_{r_{j+1}}^{\\prime},\\ldots,x_{r_{n}}^{\\prime})}$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The Hammersley-Clifford Theorem states that under the positivity condition, the joint density $\\pi(x_{1},\\ldots,x_{n})$ is proportional to a product of ratios of conditional densities for every permutation on ${1,\\ldots,n}$ and every $x^{\\prime}\\in S$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**:\n- **A) The linear term $\beta_1$ is not significant.**: Correct, as the text states that the linear term is not significant.\n- **B) The linear term $\beta_1$ suggests that longer infusion lengths improve prognosis.**: Incorrect, as the text states that longer infusion lengths do not improve prognosis.\n- **C) The linear term $\beta_1$ has an estimated value of $0.12$ with a standard error of $0.07$ for $\\hat{\\beta}$.**: Correct, as this matches the data in Table 3.\n- **D) The linear term $\beta_1$ has an estimated value of $-0.09$ with a standard error of $0.02$ for $\\tilde{\\beta}$.**: Correct, as this matches the data in Table 3.\n\n**Final Answer**: The final answer is $\boxed{{A, C, D}}$", "source": ["Semiparametric_inference_in_observational_duration-response_studies,_with_duration_possibly_right-censored.md"], "background_text": "As we can see, the linear term is not significant which leads us to believe that patients will have about the same probability of the endpoint regardless of the length of infusion. These results confirm earlier suggestions that longer infusion lengths do not improve prognosis. Table 3 shows estimates for $\\hat{\\beta}$, $\\tilde{\\beta}$, and $\\beta^{*}$ with their standard errors.", "folder": "", "qid": "statistic-mcqm-1801", "question": "Based on the results from Table 3, which of the following statements about the linear term $\beta_1$ are correct?", "options": ["A) The linear term $\beta_1$ is not significant.", "B) The linear term $\beta_1$ suggests that longer infusion lengths improve prognosis.", "C) The linear term $\beta_1$ has an estimated value of $0.12$ with a standard error of $0.07$ for $\\hat{\\beta}$.", "D) The linear term $\beta_1$ has an estimated value of $-0.09$ with a standard error of $0.02$ for $\\tilde{\\beta}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "As we can see, the linear term is not significant which leads us to believe that patients will have about the same probability of the endpoint regardless of the length of infusion. These results confirm earlier suggestions that longer infusion lengths do not improve prognosis. Table 3 shows estimates for $\\hat{\\beta}$, $\\tilde{\\beta}$, and $\\beta^{*}$ with their standard errors.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**: The background text states that only CCT type MSE estimators are available for GWEBLUP and GWSYN. Option A is a CCT type estimator, and option C is also a CCT type estimator as per the context. Options B and D are PR type estimators, which are not available for GWEBLUP and GWSYN, making them incorrect.\n\n**Final Answer**: The final answer is $\boxed{{A, C}}$", "source": ["Small-area-estimation-under-spatial-nons_2012_Comp.md"], "background_text": "Two classes of MSE estimators were evaluated: CCT (conditional MSE based on Chambers et al., 2011) and PR (unconditional MSE based on Prasad and Rao, 1990). Only CCT type MSE estimators are available for GWEBLUP and GWSYN.", "folder": "", "qid": "statistic-mcqm-4371", "question": "Which MSE estimators are available for GWEBLUP and GWSYN?", "options": ["A) CCT: Conditional Chambers et al. (2011) MSE estimator", "B) PR: Unconditional Prasad and Rao (1990) MSE estimator", "C) CCT: Method of Salvati et al. (2010)", "D) PR: Method of Opsomer et al. (2008)"], "category": "PARTIALLY_CORRECT", "question_context_used": "Two classes of MSE estimators were evaluated: CCT (conditional MSE based on Chambers et al., 2011) and PR (unconditional MSE based on Prasad and Rao, 1990). Only CCT type MSE estimators are available for GWEBLUP and GWSYN.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **A) Correct**: The text describes the PIN prior as potentially improper, with a constant noninformative prior on the null space of the precision matrix.\n- **B) Incorrect**: The PIN prior is not always proper; it can be improper when the precision matrix is singular.\n- **C) Correct**: The PIN prior includes a proper degenerate normal distribution on the range of the precision matrix, as mentioned in the description of the prior.\n- **D) Incorrect**: The PIN prior is specifically designed to handle cases where the precision matrix is not full rank, making it applicable in a wider range of scenarios.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Fully_Bayesian_spline_smoothing_and_intrinsic_autoregressive_priors.md"], "background_text": "The text introduces the partially informative normal (PIN) prior, which can be improper and consists of two parts: a constant noninformative prior on the null space of the precision matrix and a proper degenerate normal on the range of the precision matrix. This prior is applicable even when the precision matrix is singular.", "folder": "", "qid": "statistic-mcqm-5624", "question": "Which of the following are properties of the partially informative normal (PIN) prior as described in the text?", "options": ["A) The PIN prior can be improper, with a constant noninformative prior on the null space of the precision matrix.", "B) The PIN prior is always proper and does not allow for any noninformative components.", "C) The PIN prior includes a proper degenerate normal distribution on the range of the precision matrix.", "D) The PIN prior is only applicable when the precision matrix is full rank."], "category": "CORRECT", "question_context_used": "The text introduces the partially informative normal (PIN) prior, which can be improper and consists of two parts: a constant noninformative prior on the null space of the precision matrix and a proper degenerate normal on the range of the precision matrix. This prior is applicable even when the precision matrix is singular.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The text states that Siegel's test is based on all large values of $\\{Y_{k}\\}$.\n- **B) Correct**: The number of contributing frequencies is data-dependent.\n- **C) Incorrect**: The text mentions that Siegel's test is only slightly less powerful, not less powerful, against simple periodicity.\n- **D) Correct**: The text explicitly states that Siegel's test outperforms Fisher's test against compound periodicity.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Asymptotic_percentage_points_for_Siegel's_test_statistic_for_compound_periodicities.md"], "background_text": "Siegel's test statistic $T(m,\\xi g_{F;\\alpha})$ is based on all large values of $\\{Y_{k}\\}$ instead of only their maximum. The number of contributing frequencies depends on the data. It was found that use of $\\xi=0.6$ proved only slightly less powerful than Fisher's test against an alternative of simple periodicity, but that it outperformed Fisher's test against an alternative of compound periodicity.", "folder": "", "qid": "statistic-mcqm-1875", "question": "Regarding Siegel's test statistic $T(m,\\xi g_{F;\\alpha})$, which of the following statements are correct?", "options": ["A) It is based on all large values of $\\{Y_{k}\\}$ instead of only their maximum.", "B) The number of contributing frequencies depends on the data.", "C) It is less powerful than Fisher's test against an alternative of simple periodicity.", "D) It outperforms Fisher's test against an alternative of compound periodicity."], "category": "CORRECT", "question_context_used": "Siegel's test statistic $T(m,\\xi g_{F;\\alpha})$ is based on all large values of $\\{Y_{k}\\}$ instead of only their maximum. The number of contributing frequencies depends on the data. It was found that use of $\\xi=0.6$ proved only slightly less powerful than Fisher's test against an alternative of simple periodicity, but that it outperformed Fisher's test against an alternative of compound periodicity.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A**: Correct. This is the general condition for uniform ergodicity of the independence sampler.\n- **B**: Correct. When $\\pi(x)$ and $q(x)$ are both proportional to $\\frac{1}{x^{1+r}}$ and $\\eta\\le r$, the condition in A is satisfied.\n- **C**: Correct. When $\\pi(x)$ and $q(x)$ are exponential with $b\\leq a$, the condition in A is satisfied.\n- **D**: Incorrect. The independence sampler's ergodicity properties are not directly related to the geometry of $R$ but to the ratio $\\frac{q(x)}{\\pi(x)}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Scandinavian J Statistics - 2007 - JARNER - Conver.md"], "background_text": "The independence sampler is uniformly ergodic if there exists $\\beta>0$ such that $\\frac{q(x)}{\\pi(x)}\\geq\\beta$ for all $x$. Specific cases include when $\\pi(x)$ and $q(x)$ are both proportional to $\\frac{1}{x^{1+r}}$ with $\\eta\\le r$, or when they are exponential with $b\\leq a$.", "folder": "", "qid": "statistic-mcqm-5507", "question": "Under what conditions is the independence sampler uniformly ergodic?", "options": ["A) If there exists $\\beta>0$ such that $\\frac{q(x)}{\\pi(x)}\\geq\\beta$ for all $x$.", "B) If $\\pi(x)$ and $q(x)$ are both proportional to $\\frac{1}{x^{1+r}}$ for some $r>0$ and $\\eta\\le r$.", "C) If $\\pi(x)$ and $q(x)$ are both proportional to $\\exp(-a x)$ and $\\exp(-b x)$ respectively, with $b\\leq a$.", "D) If the independence sampler is applied to a bounded open-connected region $R\\subset\\mathbb{R}^{d}$."], "category": "CORRECT", "question_context_used": "The independence sampler is uniformly ergodic if there exists $\\beta>0$ such that $\\frac{q(x)}{\\pi(x)}\\geq\\beta$ for all $x$. Specific cases include when $\\pi(x)$ and $q(x)$ are both proportional to $\\frac{1}{x^{1+r}}$ with $\\eta\\le r$, or when they are exponential with $b\\leq a$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. BootEM's MSE (1.77) is lower than EM's (1.99), indicating better performance.\n- **B)** Correct. The standard error for BootEM (0.045) is lower than for EM (0.069), suggesting more reliable estimates.\n- **C)** Incorrect. The text does not provide information on the statistical significance of the MSE difference.\n- **D)** Correct. Both algorithms were initialized with cluster memberships from the k-means algorithm.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Addressing-overfitting-and-underfitting-in-Gau_201.md"], "background_text": "The MSE for BootEM is 1.77 with a standard error of 0.045, while for EM it is 1.99 with a standard error of 0.069. Both algorithms are initialized with cluster memberships from the k-means algorithm.", "folder": "", "qid": "statistic-mcqm-5080", "question": "Regarding the MSE values for the BootEM and EM algorithms, which of the following statements are true?", "options": ["A) BootEM has a lower MSE than EM, indicating better performance.", "B) The standard error for BootEM's MSE is lower than that for EM, suggesting more reliable estimates.", "C) The difference in MSE between BootEM and EM is statistically significant.", "D) Both algorithms were initialized with cluster memberships arising from the k-means algorithm."], "category": "CORRECT", "question_context_used": "The MSE for BootEM is 1.77 with a standard error of 0.045, while for EM it is 1.99 with a standard error of 0.069. Both algorithms are initialized with cluster memberships from the k-means algorithm.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The text explicitly states that the log has been applied to the elements of $A$ such that $\\Sigma$ will not have big values.\n- **B)** Correct. The construction $\\Sigma = A^{\\top}A$ ensures that $\\Sigma$ is positive definite, as mentioned in the background text.\n- **C)** Incorrect. The text does not provide information on how the dimensionality $p$ affects the positive definiteness of $\\Sigma$.\n- **D)** Correct. The text mentions that $A$ was chosen for its natural incremental form, which is due to the logarithmic transformation of its elements.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Widening-the-scope-of-an-eigenvector-stochastic-ap.md"], "background_text": "$Z \\in \\mathbb{R}^{p}$ is simulated following a multivariate normal distribution $\\mathcal{N}(\\mu, \\Sigma)$ with $p = 50, 100, 500$. $\\Sigma$ is constructed as $\\Sigma = A^{\\top}A$ where $A$ has elements in logarithmic form to prevent $\\Sigma$ from having big values. $\\Sigma$ is a positive definite matrix.", "folder": "", "qid": "statistic-mcqm-3450", "question": "Given the simulation setup where $Z \\in \\mathbb{R}^{p}$ follows a multivariate normal distribution $\\mathcal{N}(\\mu, \\Sigma)$ with $p = 50, 100, 500$, and $\\Sigma$ is constructed as $\\Sigma = A^{\\top}A$ with $A$ having elements in logarithmic form, which of the following statements are true?", "options": ["A) The use of logarithmic values in $A$ ensures that $\\Sigma$ does not contain excessively large values.", "B) $\\Sigma$ is guaranteed to be a positive definite matrix because it is constructed as $A^{\\top}A$.", "C) The dimensionality $p$ does not affect the positive definiteness of $\\Sigma$.", "D) The matrix $A$ has a natural incremental form due to the logarithmic transformation of its elements."], "category": "PARTIALLY_CORRECT", "question_context_used": "$Z \\in \\mathbb{R}^{p}$ is simulated following a multivariate normal distribution $\\mathcal{N}(\\mu, \\Sigma)$ with $p = 50, 100, 500$. $\\Sigma$ is constructed as $\\Sigma = A^{\\top}A$ where $A$ has elements in logarithmic form to prevent $\\Sigma$ from having big values. $\\Sigma$ is a positive definite matrix.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The trimmed mean excludes a proportion $A$ of the smallest and largest observations, as described in the context.\n- **B)** Correct. The Winsorized mean replaces a proportion $A$ of the smallest and largest observations with the nearest remaining values, which is accurately described in the context.\n- **C)** Incorrect. While trimming and Winsorizing reduce the effect of outliers, they do not completely eliminate their influence, especially if the outliers are not in the trimmed or Winsorized portions.\n- **D)** Correct. The formula for the trimmed mean includes a correction factor $(k - A n)(x_k + x_{n-k+1})$ for the proportion of data trimmed, as shown in the context.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Sequential_trimmed_t_tests_.md"], "background_text": "The $A$ trimmed and Winsorized means are defined for an ordered sample. The trimmed mean excludes a proportion $A$ of the smallest and largest observations and includes a correction factor for the proportion of data trimmed. The Winsorized mean replaces a proportion $A$ of the smallest and largest observations with the nearest remaining values.", "folder": "", "qid": "statistic-mcqm-5701", "question": "Which of the following statements are true about the trimmed and Winsorized means as defined in the context?", "options": ["A) The trimmed mean excludes a proportion $A$ of the smallest and largest observations.", "B) The Winsorized mean replaces a proportion $A$ of the smallest and largest observations with the nearest remaining values.", "C) Both the trimmed and Winsorized means are unaffected by outliers in the data.", "D) The formula for the trimmed mean includes a correction factor for the proportion of data trimmed."], "category": "CORRECT", "question_context_used": "The $A$ trimmed and Winsorized means are defined for an ordered sample. The trimmed mean excludes a proportion $A$ of the smallest and largest observations and includes a correction factor for the proportion of data trimmed. The Winsorized mean replaces a proportion $A$ of the smallest and largest observations with the nearest remaining values.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The expansion explicitly includes terms of order $\\nu^{-1/2}$ and $\\nu^{-1}$.\n- **B)** Correct. The error term is given as $\\mathcal{O}_{d,\\eta}\\left(\\frac{1+\\max_{1\\leq i\\leq d}|\\lambda_{i}(\\mathcal{A}_{\\nu,S})|^{5}}{\\nu^{3/2}}\\right)$.\n- **C)** Correct. The expansion is stated to be valid uniformly for $\\mathbb{X}\\in B_{\\nu,\\mathbb{S}}(\\eta)$ as $\\nu\\rightarrow\\infty$.\n- **D)** Incorrect. The expansion and error term explicitly depend on the dimension $d$ through $\\mathcal{O}_{d,\\eta}$ and terms involving $d$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["A-symmetric-matrix-variate-normal-local-approximat.md"], "background_text": "Theorem 1 provides an asymptotic expansion for the ratio of the Wishart density to the SMN density, including terms of order $\\nu^{-1/2}$ and $\\nu^{-1}$, with a specified error term. The expansion is valid uniformly for $\\mathbb{X}$ in the bulk of the Wishart distribution as $\\nu\\rightarrow\\infty$.", "folder": "", "qid": "statistic-mcqm-5545", "question": "Which of the following are correct about the asymptotic expansions provided in Theorem 1?", "options": ["A) The expansion includes terms of order $\\nu^{-1/2}$ and $\\nu^{-1}$.", "B) The error term is of order $\\mathcal{O}_{d,\\eta}\\left(\\frac{1+\\max_{1\\leq i\\leq d}|\\lambda_{i}(\\mathcal{A}_{\\nu,S})|^{5}}{\\nu^{3/2}}\\right)$.", "C) The expansion is valid uniformly for $\\mathbb{X}\\in B_{\\nu,\\mathbb{S}}(\\eta)$ as $\\nu\\rightarrow\\infty$.", "D) The expansion does not depend on the dimension $d$."], "category": "CORRECT", "question_context_used": "Theorem 1 provides an asymptotic expansion for the ratio of the Wishart density to the SMN density, including terms of order $\\nu^{-1/2}$ and $\\nu^{-1}$, with a specified error term. The expansion is valid uniformly for $\\mathbb{X}$ in the bulk of the Wishart distribution as $\\nu\\rightarrow\\infty$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **A) Correct**: The equation directly represents the expected effect of combining specific levels of A and B.\n- **B) Correct**: The main effects of A are part of the model, represented by $a_{i}$.\n- **C) Correct**: $c_{j}$ are indeed weights applied to the results at different levels of B.\n- **D) Incorrect**: The representation is explicitly noted as not symmetrical.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["THE_INTERPRETATION_OF_INTERACTIONS_IN_FACTORIAL_EXPERIMENTS.md"], "background_text": "In a factorial experiment with factors A and B, where effects are not additive, the expected effect of combining level i of A with level j of B is represented by $a_{i}c_{j}+b_{j}$. The main effects for levels of A are given by constants $\\alpha_{i}$, and $c_{j}$ are weights for B levels. The representation is not symmetrical with respect to A and B.", "folder": "", "qid": "statistic-mcqm-2021", "question": "What does the equation $a_{i}c_{j}+b_{j}$ represent in the context of a factorial experiment with factors A and B?", "options": ["A) The expected effect of the combination of level i of A with level j of B.", "B) The main effects for the different levels of A.", "C) The weights to be applied to the results at different levels of B.", "D) The symmetrical representation of the treatment effects with respect to factors A and B."], "category": "PARTIALLY_CORRECT", "question_context_used": "In a factorial experiment with factors A and B, where effects are not additive, the expected effect of combining level i of A with level j of B is represented by $a_{i}c_{j}+b_{j}$. The main effects for levels of A are given by constants $\\alpha_{i}$, and $c_{j}$ are weights for B levels. The representation is not symmetrical with respect to A and B.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The test uses rejection regions defined by inequalities involving both $F_{+}$ and $F_{-}$.\n- **B)** Correct. The text states that the test's Type I error is certainly less than $\\alpha$.\n- **C)** Incorrect. The degrees of freedom are $2n_{1}$ and $2n_{2}+2$ for $F_{+}$, but $2n_{1}$ and $2n_{2}+2$ for $F_{-}$ as well, making this option partially correct but not fully accurate in context.\n- **D)** Correct. The text mentions that the test's Type I error is generally more than desired.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["The_mathematical_relation_between_Greenberg's_index_of_linguistic_diversity_and_Yule's_characteristic.md"], "background_text": "Przyborowski & Wilenski's test without continuity correction uses rejection regions defined by inequalities involving $F_{+}(\\frac{1}{2}\\alpha;2n_{1},2n_{2}+2)$ and $F_{-}(\\frac{1}{2}\\alpha;2n_{1},2n_{2}+2)$. The test's Type I error is certainly less than $\\alpha$ but generally more than desired.", "folder": "", "qid": "statistic-mcqm-2205", "question": "Regarding Przyborowski & Wilenski's test without continuity correction, which of the following statements are correct?", "options": ["A) It uses rejection regions defined by inequalities involving $F_{+}$ and $F_{-}$.", "B) The test's Type I error is always less than $\\alpha$.", "C) The degrees of freedom in the inequalities are $2n_{1}$ and $2n_{2}+2$.", "D) The test's Type I error is generally more than desired."], "category": "PARTIALLY_CORRECT", "question_context_used": "Przyborowski & Wilenski's test without continuity correction uses rejection regions defined by inequalities involving $F_{+}(\\frac{1}{2}\\alpha;2n_{1},2n_{2}+2)$ and $F_{-}(\\frac{1}{2}\\alpha;2n_{1},2n_{2}+2)$. The test's Type I error is certainly less than $\\alpha$ but generally more than desired.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **A)** Correct. The full column rank condition for $E(\\overline{Z}_i \\overline{X}_i')$ is essential for identification, as stated in the context.\n- **B)** Incorrect. Homoskedasticity is not a requirement for the model to be identified; it relates to the efficiency of the estimator.\n- **C)** Correct. The matrix $\\overline{Z}$ must be full ranked to ensure that the instruments are valid and the model is identified.\n- **D)** Incorrect. The number of instruments $K_2$ must be at least as large as the number of endogenous regressors $n$ ($K_2 \\geq n$), but equality is not required.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Linear-instrumental-variables-model-avera_2014_Com.md"], "background_text": "The linear IV regression model requires that $E(\\overline{Z}_i \\overline{X}_i')$ is of full column rank for identification, ensuring that $\\beta$ is identified. The model also assumes $\\overline{Z} = [X, Z]$ is full ranked and $K_2 \\geq n$, but homoskedasticity is not a necessary condition for identification.", "folder": "", "qid": "statistic-mcqm-5736", "question": "In the linear IV regression model $y = Y\\beta + X\\gamma + u$, which of the following conditions must hold for the model to be identified?", "options": ["A) $E(\\overline{Z}_i \\overline{X}_i')$ is of full column rank for any choice of instruments such that $K_2 \\geq n$.", "B) The error term $u_i$ is homoskedastic.", "C) The matrix $\\overline{Z} = [X, Z]$ is full ranked.", "D) The number of instruments $K_2$ is equal to the number of endogenous regressors $n$."], "category": "CORRECT", "question_context_used": "The linear IV regression model requires that $E(\\overline{Z}_i \\overline{X}_i')$ is of full column rank for identification, ensuring that $\\beta$ is identified. The model also assumes $\\overline{Z} = [X, Z]$ is full ranked and $K_2 \\geq n$, but homoskedasticity is not a necessary condition for identification.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**: \n- **Option A** is correct as it is part of the system of equations for $k=3$.\n- **Option B** is correct as it is another part of the same system.\n- **Option C** is correct as it completes the set of equations for $k=3$.\n- **Option D** is correct as it combines all parts of the system into a single statement.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Semi-parametric-nonlinear-regression-and-transf_20.md"], "background_text": "For $k=3$, the system of equations becomes $H_{12}(G_{12}(x_{1},x_{2}),x_{3})=H_{13}(G_{13}(x_{1},x_{3}),x_{2})=H_{23}(G_{23}(x_{2},x_{3}),x_{1})$, ensuring the final result is independent of the order in which information is received.", "folder": "", "qid": "statistic-mcqm-5155", "question": "For the additive model with $k=3$, which equations must hold true?", "options": ["A) $H_{12}(G_{12}(x_{1},x_{2}),x_{3})=H_{13}(G_{13}(x_{1},x_{3}),x_{2})$", "B) $H_{12}(G_{12}(x_{1},x_{2}),x_{3})=H_{23}(G_{23}(x_{2},x_{3}),x_{1})$", "C) $H_{13}(G_{13}(x_{1},x_{3}),x_{2})=H_{23}(G_{23}(x_{2},x_{3}),x_{1})$", "D) $H_{12}(G_{12}(x_{1},x_{2}),x_{3})=H_{13}(G_{13}(x_{1},x_{3}),x_{2})=H_{23}(G_{23}(x_{2},x_{3}),x_{1})$"], "category": "CORRECT", "question_context_used": "For $k=3$, the system of equations becomes $H_{12}(G_{12}(x_{1},x_{2}),x_{3})=H_{13}(G_{13}(x_{1},x_{3}),x_{2})=H_{23}(G_{23}(x_{2},x_{3}),x_{1})$, ensuring the final result is independent of the order in which information is received.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"D\"]\n}", "model_answer": ["D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct: Equation (34) explicitly states that $P/I$ overestimates $m$ when a non-progressive preclinical state exists.\n- **B)** Correct: This is the exact expression provided for $P/I$.\n- **C)** Incorrect: The ratio depends on $\\Pi$ through the term $\\frac{1-\\Pi}{\\Pi}$.\n- **D)** Correct: This term adjusts the ratio to account for individuals in the non-progressive preclinical state.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["On_the_theory_of_screening_for_chronic_diseases.md"], "background_text": "The ratio $P/I$ represents the probability of being in a preclinical state (progressive or non-progressive) over the marginal incidence $I$. It is given by $P/I = m + \\frac{1-\\Pi}{\\Pi}\\int_{0}^{\\infty}\\frac{W(z)}{w}Q_{n}(z)dz$, where $m$ is the mean sojourn time in the progressive preclinical state, and $\\Pi$ is the probability of entering this state.", "folder": "", "qid": "statistic-mcqm-1554", "question": "Which of the following statements about the ratio $P/I$ are true?", "options": ["A) $P/I$ overestimates the mean sojourn time $m$ when there is a non-progressive preclinical state.", "B) The ratio $P/I$ is given by $m + \\frac{1-\\Pi}{\\Pi}\\int_{0}^{\\infty}\\frac{W(z)}{w}Q_{n}(z)dz$.", "C) $P/I$ is independent of the probability $\\Pi$ of entering the progressive preclinical state.", "D) The term $\\frac{1-\\Pi}{\\Pi}\\int_{0}^{\\infty}\\frac{W(z)}{w}Q_{n}(z)dz$ accounts for the contribution of the non-progressive preclinical state."], "category": "PARTIALLY_CORRECT", "question_context_used": "The ratio $P/I$ represents the probability of being in a preclinical state (progressive or non-progressive) over the marginal incidence $I$. It is given by $P/I = m + \\frac{1-\\Pi}{\\Pi}\\int_{0}^{\\infty}\\frac{W(z)}{w}Q_{n}(z)dz$, where $m$ is the mean sojourn time in the progressive preclinical state, and $\\Pi$ is the probability of entering this state.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**: \n- **A) Correct**: $V(12, 13)$ corresponds to $i=1, j=2, k=1, l=3$. Here, $i=k$ and $j\\neq l$, so $V(12, 13) = 1$.\n- **B) Incorrect**: $V(13, 23)$ corresponds to $i=1, j=3, k=2, l=3$. Here, $j=l$, so $V(13, 23) = -1$.\n- **C) Correct**: $V(12, 34)$ corresponds to $i=1, j=2, k=3, l=4$. None of the conditions for $1$ or $-1$ are met, so $V(12, 34) = 0$.\n- **D) Incorrect**: $V(23, 12)$ corresponds to $i=2, j=3, k=1, l=2$. Here, $j=k$ is not true, and $i=l$ is not true, but $j=k$ is not a condition for $-1$. The correct condition is $i=l$ or $j=k$, but neither is met here, so $V(23, 12) = 0$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Design_of_order-of-addition_experiments.md"], "background_text": "The matrix $V$ is defined for $m=4$ with $S=\\{12,13,14,23,24,34\\}$. The elements of $V$ are given by $V(ij, kl) = 1$ if $i=k, j\\neq l$ or $i\\neq k, j=l$, $-1$ if $i=l$ or $j=k$, and $0$ otherwise.", "folder": "", "qid": "statistic-mcqm-6076", "question": "Given the matrix $V$ defined for $m=4$ with elements $V(ij, kl)$ as per the context, which of the following statements about $V$ are correct?", "options": ["A) $V(12, 13) = 1$", "B) $V(13, 23) = -1$", "C) $V(12, 34) = 0$", "D) $V(23, 12) = 1$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The matrix $V$ is defined for $m=4$ with $S=\\{12,13,14,23,24,34\\}$. The elements of $V$ are given by $V(ij, kl) = 1$ if $i=k, j\\neq l$ or $i\\neq k, j=l$, $-1$ if $i=l$ or $j=k$, and $0$ otherwise.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "To determine the correct statements, we refer to the background text which states that under condition $\\mathsf{C}_{1}$, $z/\\lVert z\\rVert$ is uniformly distributed on the unit sphere surface in $\\mathbf{R}^{p}$ and that $z/\\lVert z\\rVert$ and $\\lVert z\\rVert$ are independent. This directly supports options A and B. Option C is incorrect because the text does not mention any normal distribution properties for $z/\\lVert z\\rVert$. Option D is incorrect because it is $z/\\lVert z\\rVert$, not $\\lVert z\\rVert$, that is uniformly distributed on the unit sphere surface.\n\n**Correct Choices**: A, B\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Robust-dimension-reduction-based-on-canonical_2009.md"], "background_text": "Given the condition $\\mathsf{C}_{1}$, by Theorem 2.3 in [7], we have that $z/\\lVert z\\rVert$ is uniformly distributed on the unit sphere surface in $\\mathbf{R}^{p}$, and $z/\\lVert z\\rVert$ and $\\lVert z\\rVert$ are independent.", "folder": "", "qid": "statistic-mcqm-5020", "question": "Given the condition $\\mathsf{C}_{1}$, which of the following statements are true about $z/\\lVert z\\rVert$ and $\\lVert z\\rVert$?", "options": ["A) $z/\\lVert z\\rVert$ is uniformly distributed on the unit sphere surface in $\\mathbf{R}^{p}$.", "B) $z/\\lVert z\\rVert$ and $\\lVert z\\rVert$ are independent.", "C) $z/\\lVert z\\rVert$ has a normal distribution.", "D) $\\lVert z\\rVert$ is uniformly distributed on the unit sphere surface in $\\mathbf{R}^{p}$."], "category": "CORRECT", "question_context_used": "Given the condition $\\mathsf{C}_{1}$, by Theorem 2.3 in [7], we have that $z/\\lVert z\\rVert$ is uniformly distributed on the unit sphere surface in $\\mathbf{R}^{p}$, and $z/\\lVert z\\rVert$ and $\\lVert z\\rVert$ are independent.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: This is a direct description of Holm's procedure as stated in the text.\n- **B) Correct**: This accurately describes the new procedure's approach to rejecting hypotheses.\n- **C) Incorrect**: The text states that both procedures contrast the ordered $p$-values with the same set of critical values, not different sets.\n- **D) Incorrect**: The text explicitly states that the new procedure is sharper and more powerful than Holm's procedure.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["A_sharper_Bonferroni_procedure_for_multiple_tests_of_significance.md"], "background_text": "Holm's procedure and the new procedure both contrast the ordered $p$-values with the same set of critical values. Holm's procedure rejects an hypothesis only if its $p$-value and each of the smaller $p$-values are less than their corresponding critical values. The new procedure rejects all hypotheses with smaller or equal $p$-values to that of any one found less than its critical value. The new procedure is described as sharper and more powerful than Holm's procedure.", "folder": "", "qid": "statistic-mcqm-5526", "question": "Which of the following statements are true about Holm's procedure and the new procedure described in the text?", "options": ["A) Holm's procedure rejects an hypothesis only if its $p$-value and each of the smaller $p$-values are less than their corresponding critical values.", "B) The new procedure rejects all hypotheses with smaller or equal $p$-values to that of any one found less than its critical value.", "C) Both procedures use different sets of critical values for comparison.", "D) The new procedure is less powerful than Holm's procedure."], "category": "CORRECT", "question_context_used": "Holm's procedure and the new procedure both contrast the ordered $p$-values with the same set of critical values. Holm's procedure rejects an hypothesis only if its $p$-value and each of the smaller $p$-values are less than their corresponding critical values. The new procedure rejects all hypotheses with smaller or equal $p$-values to that of any one found less than its critical value. The new procedure is described as sharper and more powerful than Holm's procedure.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C, D\n\n- **A)** Incorrect. The ESS must be greater than 5 for the slope to be tested for significance.\n- **B)** Correct. The context states that the absolute value of the standardized slope must be greater than $C_{R}$ for the slope to be declared significant.\n- **C)** Correct. If the slope is greater than $C_{R}$, it is declared significant and colored blue.\n- **D)** Correct. If the slope is less than $-C_{R}$, it is declared significant and colored red.\n\n**Final Answer**: The final answer is $\boxed{{B, C, D}}$", "source": ["Robust_SiZer_for_Exploration_of_Regression_Structures_and_Outlier_Detection__Robust_SiZer_for_Exploration_of_Regression_Structures_and_Outlier_Detection.md"], "background_text": "In the proposed robust SiZer, an estimated slope is declared significant if the absolute value of the standardized slope is greater than the critical value $C_{R}$. The ESS of the estimated slope must be large enough (greater than 5) for this test to be performed. The slope is colored blue if it is greater than $C_{R}$, red if it is less than $-C_{R}$, and purple otherwise.", "folder": "", "qid": "statistic-mcqm-865", "question": "In the proposed robust SiZer, which of the following conditions must be met for an estimated slope to be declared significant?", "options": ["A) The ESS of the estimated slope must be less than or equal to 5.", "B) The absolute value of the standardized slope must be greater than the critical value $C_{R}$.", "C) The estimated slope must be greater than the critical value $C_{R}$.", "D) The estimated slope must be less than the negative of the critical value $-C_{R}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "In the proposed robust SiZer, an estimated slope is declared significant if the absolute value of the standardized slope is greater than the critical value $C_{R}$. The ESS of the estimated slope must be large enough (greater than 5) for this test to be performed. The slope is colored blue if it is greater than $C_{R}$, red if it is less than $-C_{R}$, and purple otherwise.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\"]\n}", "model_answer": ["B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The text states that all of $u^{*}, \\gamma^{*}, \\gamma_{M}^{*}$ and $g^{*}$ are numerically the same.\n- **B)** Correct. The text mentions that $\\hat{y}(s)$ does not reflect differences among sampling schemes, thus uses $c$ uniformly.\n- **C)** Incorrect. The text specifies that $\\hat{\\delta}_{P}(s)$ and $\\hat{\\delta}_{M}(s)$ may be different from $\\hat{y}(s)$ when $\\mathcal{A}$ is present, but not always.\n- **D)** Correct. The text highlights that in retrospective sampling, the comparison is made with $c^{*}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Isotonic_logistic_discrimination.md"], "background_text": "The text explains the relationship between isotonic regression and discriminant rules, including the conditions under which certain rules are equivalent or differ, and how sampling schemes affect the comparison.", "folder": "", "qid": "statistic-mcqm-4597", "question": "Regarding the discriminant rules and isotonic regression, which of the following are correct?", "options": ["A) The isotonic regression $g^{*}$ is numerically the same as $u^{*}, \\gamma^{*}, \\gamma_{M}^{*}$.", "B) The discriminant rule $\\hat{y}(s)$ in (12) uses $c$ regardless of the sampling scheme.", "C) The rules $\\hat{\\delta}_{P}(s)$ and $\\hat{\\delta}_{M}(s)$ are always different from $\\hat{y}(s)$ when a data ambiguity set $\\mathcal{A}$ is present.", "D) In retrospective sampling, $\\gamma^{*}(s)$ of $\\hat{\\delta}_{R}(s)$ is compared with $c^{*}$ instead of $c$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The text explains the relationship between isotonic regression and discriminant rules, including the conditions under which certain rules are equivalent or differ, and how sampling schemes affect the comparison.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"B\", \"D\"]\n}", "model_answer": ["B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**: \n- **A) Correct**: The text defines the longitude range for grid cell $(j,k)$ as between $i_{j,k}$ and $i_{j,k}+45(N_{j}/N_{1})^{-1/2}$.\n- **B) Correct**: The text defines the latitude range for grid cell $(j,k)$ as between $l_{j,k}$ and $l_{j,k}+36(N_{j}/N_{1})^{-1/2}$.\n- **C) Correct**: The text states that $5(N_{j}/N_{1})^{1/2}$ is the number of grid cells for a given longitude at scale $j$.\n- **D) Correct**: The text describes the consecutive sequence of grid cells from the South Pole to the North Pole as $(j,0),\\ldots,(j,5(N_{j}/N_{1})^{1/2}-1)$.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["Fast,_Resolution-Consistent_Spatial_Prediction_of_Global_Processes_From_Satellite_Data__Fast__Resolution-Consistent_Spatial_Prediction_of_Global_Processes_From_Satellite_Data.md"], "background_text": "For each resolution $j=1,\\ldots,5$, the grid cells $(j,0),\\dots,(j,N_{j}-1)$ are defined according to the lexicographic order of longitude-latitude pairs. The grid cell $(j,k)$ is defined between specific longitudes and latitudes, and the number of grid cells for a given longitude at scale $j$ is $5(N_{j}/N_{1})^{1/2}$. A consecutive sequence of grid cells from the South Pole to the North Pole is also defined.", "folder": "", "qid": "statistic-mcqm-5474", "question": "Which of the following statements about the grid cells at resolution $j$ are correct?", "options": ["A) Grid cell $(j,k)$ is defined between longitudes $i_{j,k}$ and $i_{j,k}+45(N_{j}/N_{1})^{-1/2}$.", "B) Grid cell $(j,k)$ is defined between latitudes $l_{j,k}$ and $l_{j,k}+36(N_{j}/N_{1})^{-1/2}$.", "C) The number of grid cells for a given longitude at scale $j$ is $5(N_{j}/N_{1})^{1/2}$.", "D) A consecutive sequence of grid cells starting from the South Pole and finishing at the North Pole is given by $(j,0),\\ldots,(j,5(N_{j}/N_{1})^{1/2}-1)$."], "category": "PARTIALLY_CORRECT", "question_context_used": "For each resolution $j=1,\\ldots,5$, the grid cells $(j,0),\\dots,(j,N_{j}-1)$ are defined according to the lexicographic order of longitude-latitude pairs. The grid cell $(j,k)$ is defined between specific longitudes and latitudes, and the number of grid cells for a given longitude at scale $j$ is $5(N_{j}/N_{1})^{1/2}$. A consecutive sequence of grid cells from the South Pole to the North Pole is also defined.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A)** Correct. The formula for $S_{\\mathrm{E}}$ is indeed based on the probability of $T$ given $D$, as shown in the Bayes formula provided.\n- **B)** Correct. Similarly, $S_{\\mathrm{P}}$ is estimated using the probability of $\\overline{T}$ given $\\overline{D}$, which is part of the given Bayes formula.\n- **C)** Incorrect. While Zhou (1993) introduced $k_{1}$ and $k_{2}$ to reduce the number of parameters, the initial Bayes formula for $S_{\\mathrm{E}}$ and $S_{\\mathrm{P}}$ does not require knowledge of these terms.\n- **D)** Correct. The Bayes formula for $S_{\\mathrm{E}}$ explicitly includes the term $P(T)P(D|T V)$ in its numerator.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Estimators-of-sensitivity-and-specificity-in-the-p.md"], "background_text": "Under the assumption of independence of $D$ and $V$, $S_{\\mathrm{E}}$ is given by $P(T|D)=\\frac{P(T)P(D|T V)}{P(T)P(D|T V)+P(\\overline{T})P(D|\\overline{T}V)}$ and $S_{\\mathrm{P}}$ is given by $P(\\overline{T}|\\overline{D})=\\frac{P(\\overline{T})P(\\overline{D}|\\overline{T}V)}{P(\\overline{T})P(\\overline{D}|\\overline{T}V)+P(T)P(\\overline{D}|T V)}$. Zhou (1993) defined $k_{1}=\\lambda_{11}/\\lambda_{01}$ and $k_{2}=\\lambda_{10}/\\lambda_{00}$ to reduce the number of parameters.", "folder": "", "qid": "statistic-mcqm-5587", "question": "Given the Bayes formula for estimating $S_{\\mathrm{E}}$ and $S_{\\mathrm{P}}$ under the assumption of independence of $D$ and $V$, which of the following statements are correct?", "options": ["A) $S_{\\mathrm{E}}$ is estimated using the probability of $T$ given $D$.", "B) $S_{\\mathrm{P}}$ is estimated using the probability of $\\overline{T}$ given $\\overline{D}$.", "C) The estimates for $S_{\\mathrm{E}}$ and $S_{\\mathrm{P}}$ require the knowledge of $k_{1}$ and $k_{2}$.", "D) The Bayes formula for $S_{\\mathrm{E}}$ involves the term $P(T)P(D|T V)$."], "category": "CORRECT", "question_context_used": "Under the assumption of independence of $D$ and $V$, $S_{\\mathrm{E}}$ is given by $P(T|D)=\\frac{P(T)P(D|T V)}{P(T)P(D|T V)+P(\\overline{T})P(D|\\overline{T}V)}$ and $S_{\\mathrm{P}}$ is given by $P(\\overline{T}|\\overline{D})=\\frac{P(\\overline{T})P(\\overline{D}|\\overline{T}V)}{P(\\overline{T})P(\\overline{D}|\\overline{T}V)+P(T)P(\\overline{D}|T V)}$. Zhou (1993) defined $k_{1}=\\lambda_{11}/\\lambda_{01}$ and $k_{2}=\\lambda_{10}/\\lambda_{00}$ to reduce the number of parameters.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**: Option A correctly describes the computation of the observed test statistic. Option C accurately describes how the $p$-value is estimated. Option D is correct because the permutation test is designed to be valid without assuming the asymptotic normality of $\\hat{\\varDelta}_{n}^{\\ast}$. Option B is incorrect because the permutation involves $U_{2}$, not $U_{1}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Robust-dimension-reduction-based-on-canonical_2009.md"], "background_text": "The permutation test for WCANCOR involves computing a test statistic based on eigenvalues, permuting parts of the projected predictor matrix, and estimating a $p$-value based on the permutation distribution. The test is designed to work without relying on the asymptotic normality of $\\hat{\\varDelta}_{n}^{\\ast}$.", "folder": "", "qid": "statistic-mcqm-5016", "question": "In the context of the permutation test for WCANCOR, which of the following steps are correctly described?", "options": ["A) Compute the observed value of the test statistic $\\varOmega_{s,obs}^{\\ast}$ using the formula $-\\{n-(p+m+k_{n}+2)/2\\}\\sum_{j=s+1}^{p}\\log(1-\\hat{\\gamma}_{j}^{2})$.", "B) Randomly permute the rows of $U_{1}$ to get $U_{1}^{\\prime}$ and use $U^{\\prime}=(U_{1}^{\\prime},U_{2})$ for the permutation test.", "C) The $p$-value for testing $H_{0,s}$ is estimated as the fraction of $\\varOmega_{s}^{\\ast\\prime}$ exceeding $\\varOmega_{s,obs}^{\\ast}$.", "D) The permutation test does not depend on the asymptotic normality of $\\hat{\\varDelta}_{n}^{\\ast}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The permutation test for WCANCOR involves computing a test statistic based on eigenvalues, permuting parts of the projected predictor matrix, and estimating a $p$-value based on the permutation distribution. The test is designed to work without relying on the asymptotic normality of $\\hat{\\varDelta}_{n}^{\\ast}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A)** Correct. This is the correct formula for the BIC criterion as given in the context.\n- **B)** Correct. The degrees of freedom $d f$ is indeed calculated as $d f=d(d-1)/2+|\\boldsymbol{\\mathcal{S}}|$, though the constant $d(d-1)/2$ can be ignored.\n- **C)** Correct. The optimal portfolio selected by BIC is correctly given as $\\hat{\\mathcal{S}}_{\\mathrm{BIC}}=\\mathrm{argmin}_{\\mathcal{S}\\subset\\mathcal{S}_{F}}\\mathrm{BIC}(\\mathcal{S})$.\n- **D)** Incorrect. The BIC criterion is a selection consistent criterion, not a loss efficient criterion.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["A-Bayesian-information-criterion-for-port_2012_Com.md"], "background_text": "The BIC criterion for portfolio selection is given by $-2\\left(\\log\\hat{\\ell}(\\boldsymbol{\\Sigma}_{(\\boldsymbol{\\xi})})+\\log\\hat{\\ell}(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\xi}^{c}|\\boldsymbol{\\xi}})\\right)/n+d f\\times\\log n/n$, where $d f$ is the number of unknown parameters. The degrees of freedom $d f$ is simplified to $d f=|\\boldsymbol{\\mathcal{S}}|$. The optimal portfolio selected by BIC is $\\hat{\\mathcal{S}}_{\\mathrm{BIC}}=\\mathrm{argmin}_{\\mathcal{S}\\subset\\mathcal{S}_{F}}\\mathrm{BIC}(\\mathcal{S})$. BIC is a selection consistent criterion, not a loss efficient criterion.", "folder": "", "qid": "statistic-mcqm-4760", "question": "Which of the following statements about the BIC criterion for portfolio selection are true?", "options": ["A) The BIC criterion is given by $-2\\left(\\log\\hat{\\ell}(\\boldsymbol{\\Sigma}_{(\\boldsymbol{\\xi})})+\\log\\hat{\\ell}(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\xi}^{c}|\\boldsymbol{\\xi}})\\right)/n+d f\\times\\log n/n$.", "B) The degrees of freedom $d f$ is calculated as $d f=d(d-1)/2+|\\boldsymbol{\\mathcal{S}}|$.", "C) The optimal portfolio selected by BIC is $\\hat{\\mathcal{S}}_{\\mathrm{BIC}}=\\mathrm{argmin}_{\\mathcal{S}\\subset\\mathcal{S}_{F}}\\mathrm{BIC}(\\mathcal{S})$.", "D) The BIC criterion is a loss efficient criterion."], "category": "PARTIALLY_CORRECT", "question_context_used": "The BIC criterion for portfolio selection is given by $-2\\left(\\log\\hat{\\ell}(\\boldsymbol{\\Sigma}_{(\\boldsymbol{\\xi})})+\\log\\hat{\\ell}(\\boldsymbol{\\Sigma}_{\\boldsymbol{\\xi}^{c}|\\boldsymbol{\\xi}})\\right)/n+d f\\times\\log n/n$, where $d f$ is the number of unknown parameters. The degrees of freedom $d f$ is simplified to $d f=|\\boldsymbol{\\mathcal{S}}|$. The optimal portfolio selected by BIC is $\\hat{\\mathcal{S}}_{\\mathrm{BIC}}=\\mathrm{argmin}_{\\mathcal{S}\\subset\\mathcal{S}_{F}}\\mathrm{BIC}(\\mathcal{S})$. BIC is a selection consistent criterion, not a loss efficient criterion.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: RAMSVM includes a convex combination parameter $\\gamma \\in [0,1]$ in its problem formulation.\n- **B) Correct**: The text states that RAMSVM is Fisher consistent with $\\gamma \\in [0,0.5]$.\n- **C) Correct**: The value $\\gamma = 0.5$ was suggested based on numerical and real data experiments.\n- **D) Incorrect**: Problem (1) with the hinge loss $\\ell(u) = H_{k-1}(u)$ is equivalent to problem (2) with $\\gamma = 1$, not $\\gamma = 0.5$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Adaptively-weighted-large-margin-angle-based-_2018.md"], "background_text": "RAMSVM solves a problem that includes a convex combination parameter $\\gamma \\in [0,1]$. RAMSVM is Fisher consistent with $\\gamma \\in [0,0.5]$. The value $\\gamma = 0.5$ was suggested based on numerical and real data experiments. Problem (1) with the hinge loss $\\ell(u) = H_{k-1}(u)$ is equivalent to problem (2) with $\\gamma = 1$.", "folder": "", "qid": "statistic-mcqm-6051", "question": "Which of the following statements about the reinforced angle-based multicategory SVM (RAMSVM) are correct?", "options": ["A) RAMSVM solves a problem that includes a convex combination parameter $\\gamma \\in [0,1]$.", "B) RAMSVM is Fisher consistent with $\\gamma \\in [0,0.5]$.", "C) The value $\\gamma = 0.5$ was suggested based on numerical and real data experiments.", "D) Problem (1) with the hinge loss $\\ell(u) = H_{k-1}(u)$ is equivalent to problem (2) with $\\gamma = 0.5$."], "category": "CORRECT", "question_context_used": "RAMSVM solves a problem that includes a convex combination parameter $\\gamma \\in [0,1]$. RAMSVM is Fisher consistent with $\\gamma \\in [0,0.5]$. The value $\\gamma = 0.5$ was suggested based on numerical and real data experiments. Problem (1) with the hinge loss $\\ell(u) = H_{k-1}(u)$ is equivalent to problem (2) with $\\gamma = 1$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C\n\n- **A) $b_{1}$ must be negative**: This is incorrect because a negative $b_{1}$ makes the integral infinite.\n- **B) $b_{1}$ must be nonnegative**: This is correct as nonnegative $b_{1}$ ensures the integral is finite.\n- **C) $b_{1}$ must be zero**: This is also correct because zero is a nonnegative value, satisfying the condition.\n- **D) $b_{1}$ can be any real number**: This is incorrect because negative values of $b_{1}$ lead to an infinite integral.\n\n**Final Answer**: The final answer is $\\boxed{{B, C}}$", "source": ["Fully_Bayesian_spline_smoothing_and_intrinsic_autoregressive_priors.md"], "background_text": "When $b_{1}$ is negative, the integral of the right-hand side with respect to $\\delta_{1}$ is not finite. Thus $b_{1}$ must be nonnegative.", "folder": "", "qid": "statistic-mcqm-5648", "question": "What must be true about $b_{1}$ for the integral of the right-hand side with respect to $\\delta_{1}$ to be finite?", "options": ["A) $b_{1}$ must be negative", "B) $b_{1}$ must be nonnegative", "C) $b_{1}$ must be zero", "D) $b_{1}$ can be any real number"], "category": "PARTIALLY_CORRECT", "question_context_used": "When $b_{1}$ is negative, the integral of the right-hand side with respect to $\\delta_{1}$ is not finite. Thus $b_{1}$ must be nonnegative.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\"]\n}", "model_answer": ["B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The area between the true ordinal dominance curve and its least concave majorant is indeed $\\frac{1}{48}$, as stated in the text.\n- **B) Correct**: The sup-norm distance between the true ordinal dominance curve and its least concave majorant is $\\frac{1}{8}$, as mentioned in the text.\n- **C) Incorrect**: The area-based statistic $M_{m,n}^{*(12)}$ converges to $\\frac{1}{48}\\{m n/(m+n)\\}^{\\frac{1}{2}}$ under $H_{0}$, not $\\{m n/(m+n)\\}^{\\frac{1}{2}}$.\n- **D) Correct**: The sup-norm statistic $D_{m,n}^{*(12)}$ converges to $\\frac{1}{8}\\{m n/(m+n)\\}^{\\frac{1}{2}}$ under $H_{0}$, as stated in the text.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Nonparametric_tests_for_and_against_likelihood_ratio_ordering_in_the_two-sample_problem.md"], "background_text": "The true ordinal dominance curve is given by $\\mathrm{oDC}(t) = \\begin{cases} 8t^{2} & (0 \\leqslant t \\leqslant 0.25), \\\\ t^{\\frac{1}{2}} & (0.25 \\leqslant t \\leqslant 1), \\end{cases}$ which is not concave. The area between the true ordinal dominance curve and its least concave majorant is $\\frac{1}{48}$, and the sup-norm distance is $\\frac{1}{8}$. The statistics $M_{m,n}^{*(12)}$ and $D_{m,n}^{*(12)}$ converge to $\\{m n/(m+n)\\}^{\\frac{1}{2}}$ and $\\frac{1}{8}\\{m n/(m+n)\\}^{\\frac{1}{2}}$ under $H_{0}$, respectively.", "folder": "", "qid": "statistic-mcqm-5731", "question": "Given the true ordinal dominance curve $\\mathrm{oDC}(t) = \\begin{cases} 8t^{2} & (0 \\leqslant t \\leqslant 0.25), \\\\ t^{\\frac{1}{2}} & (0.25 \\leqslant t \\leqslant 1), \\end{cases}$ and its least concave majorant, which of the following statements are correct?", "options": ["A) The area between the true ordinal dominance curve and its least concave majorant is $\\frac{1}{48}$.", "B) The sup-norm distance between the true ordinal dominance curve and its least concave majorant is $\\frac{1}{8}$.", "C) The area-based statistic $M_{m,n}^{*(12)}$ converges to $\\{m n/(m+n)\\}^{\\frac{1}{2}}$ under $H_{0}$.", "D) The sup-norm statistic $D_{m,n}^{*(12)}$ converges to $\\frac{1}{8}\\{m n/(m+n)\\}^{\\frac{1}{2}}$ under $H_{0}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The true ordinal dominance curve is given by $\\mathrm{oDC}(t) = \\begin{cases} 8t^{2} & (0 \\leqslant t \\leqslant 0.25), \\\\ t^{\\frac{1}{2}} & (0.25 \\leqslant t \\leqslant 1), \\end{cases}$ which is not concave. The area between the true ordinal dominance curve and its least concave majorant is $\\frac{1}{48}$, and the sup-norm distance is $\\frac{1}{8}$. The statistics $M_{m,n}^{*(12)}$ and $D_{m,n}^{*(12)}$ converge to $\\{m n/(m+n)\\}^{\\frac{1}{2}}$ and $\\frac{1}{8}\\{m n/(m+n)\\}^{\\frac{1}{2}}$ under $H_{0}$, respectively.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "To determine the correct options, let's analyze each statement based on the provided context:\n\n- **Option A**: This is correct. The text states that $W_{N}$ and $V_{N}$ have the same limiting asymptotic normal distribution under $H_{0}$.\n\n- **Option B**: This is correct. The text shows that $W_{N} = V_{N} + Z_{N}$, where $Z_{N}$ is a nonnegative random variable.\n\n- **Option C**: This is correct. The text mentions that $Z_{N}$ converges in probability to zero under $H_{0}$ and under a sequence of contiguous location alternatives.\n\n- **Option D**: This is correct. The text explains that under fixed alternatives, $Z_{N}$ makes a nonnegligible contribution to $W_{N}$.\n\n**Correct Choices**: A, B, C, D\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["Pairwise_versus_joint_ranking_Another_look_at_the_Kruskal-Wallis_statistic.md"], "background_text": "The Kruskal-Wallis statistic $V_{N}$ and its pairwise rank analogue $W_{N}$ have the same limiting asymptotic normal distribution under $H_{0}$. $W_{N}$ can be expressed as $V_{N} + Z_{N}$, where $Z_{N}$ is a nonnegative random variable that converges in probability to zero under $H_{0}$ and under a sequence of contiguous location alternatives. However, under fixed alternatives, $Z_{N}$ makes a nonnegligible contribution to $W_{N}$.", "folder": "", "qid": "statistic-mcqm-5492", "question": "Which of the following statements about the Kruskal-Wallis statistic $V_{N}$ and its pairwise rank analogue $W_{N}$ are correct?", "options": ["A) $W_{N}$ and $V_{N}$ have the same limiting asymptotic normal distribution under $H_{0}$.", "B) $W_{N}$ can be expressed as $V_{N} + Z_{N}$, where $Z_{N}$ is a nonnegative random variable.", "C) $Z_{N}$ converges in probability to zero under $H_{0}$ and under a sequence of contiguous location alternatives.", "D) $Z_{N}$ makes a nonnegligible contribution to $W_{N}$ under fixed alternatives."], "category": "CORRECT", "question_context_used": "The Kruskal-Wallis statistic $V_{N}$ and its pairwise rank analogue $W_{N}$ have the same limiting asymptotic normal distribution under $H_{0}$. $W_{N}$ can be expressed as $V_{N} + Z_{N}$, where $Z_{N}$ is a nonnegative random variable that converges in probability to zero under $H_{0}$ and under a sequence of contiguous location alternatives. However, under fixed alternatives, $Z_{N}$ makes a nonnegligible contribution to $W_{N}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. $f(x)$ is periodic with period $\\pi m^{-1}$, as stated in the context.\n- **B)** Correct. For $m=2k$, $f(x)$ is symmetric about $x=\\pi(4k)^{-1}$.\n- **C)** Correct. For $n=2m=2(2k+1)$, $f(x)$ has a minimum at $x=\\pi n^{-1}$.\n- **D)** Correct. $f(x)$ is indeed used to evaluate the integral in (8) for the estimator $\\hat{L}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Measuring_the_length_of_a_curve.md"], "background_text": "The function $f(x)$ is defined as a sum of absolute cosines and is periodic with period $\\pi m^{-1}$. It has specific symmetry properties depending on whether $m$ is even or odd and is used in the evaluation of the estimator $\\hat{L}$.", "folder": "", "qid": "statistic-mcqm-5414", "question": "Which of the following are true about the function $f(x)$ defined in the context?", "options": ["A) $f(x)$ is a periodic function with period $\\pi m^{-1}$.", "B) For $m=2k$, $f(x)$ is symmetric about $x=\\pi(4k)^{-1}$.", "C) For $n=2m=2(2k+1)$, $f(x)$ has a minimum at $x=\\pi n^{-1}$.", "D) $f(x)$ is used to evaluate the integral in (8) for the estimator $\\hat{L}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The function $f(x)$ is defined as a sum of absolute cosines and is periodic with period $\\pi m^{-1}$. It has specific symmetry properties depending on whether $m$ is even or odd and is used in the evaluation of the estimator $\\hat{L}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The text states that $n_{1_{o}} / N^{\\alpha} = \\beta + o(1)$ for some constants $\\alpha > 0$ and $\\beta$, indicating $\\alpha$ is a constant greater than 0.\n- **B)** Correct. The text explicitly states that $\\lim_{N \\to \\infty} \\log n_{1_{o}} / \\log N = \\alpha$.\n- **C)** Correct. The text mentions that Table 3 suggests the optimal first-stage sample size in a three-stage trial is asymptotically proportional to $N^{1/3}$, implying $\\alpha = \\frac{1}{3}$.\n- **D)** Incorrect. The text does not mention that $\\alpha$ depends on the success rate of the known arm; it is suggested to be $\\frac{1}{3}$ regardless of the success rate.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["Choosing_sample_size_for_a_clinical_trial_using_decision_analysis.md"], "background_text": "The context discusses a three-stage trial with one known arm, where the optimal first-stage sample size $n_{1_{o}}$ is asymptotically proportional to $N^{\\alpha}$. It is mentioned that $\\lim_{N \\to \\infty} \\log n_{1_{o}} / \\log N = \\alpha$ and Table 3 suggests that $\\alpha = \\frac{1}{3}$ for a three-stage trial.", "folder": "", "qid": "statistic-mcqm-5564", "question": "For a three-stage trial with one known arm, the optimal first-stage sample size $n_{1_{o}}$ is asymptotically proportional to $N^{\\alpha}$. Based on the context, which of the following statements about $\\alpha$ are correct?", "options": ["A) $\\alpha$ is a constant greater than 0.", "B) The limit of $\\log n_{1_{o}} / \\log N$ as $N \\to \\infty$ is $\\alpha$.", "C) Table 3 suggests that $\\alpha = \\frac{1}{3}$ for a three-stage trial.", "D) The value of $\\alpha$ depends on the success rate of the known arm."], "category": "CORRECT", "question_context_used": "The context discusses a three-stage trial with one known arm, where the optimal first-stage sample size $n_{1_{o}}$ is asymptotically proportional to $N^{\\alpha}$. It is mentioned that $\\lim_{N \\to \\infty} \\log n_{1_{o}} / \\log N = \\alpha$ and Table 3 suggests that $\\alpha = \\frac{1}{3}$ for a three-stage trial.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: The allocation is optimized to maximize the power of the test, as indicated by the substantial gain in power when using the optimized allocation.\n- **B) Correct**: The allocation is based on the variances of the estimators from the list and pair methods, as shown by the formula involving $\\sqrt{\\mathrm{Var}_{\\hat{p}}[\\hat{p}]}$ and $\\sqrt{\\mathrm{Var}_{\\tilde{p}}[\\tilde{p}]}$.\n- **C) Incorrect**: The allocation does not ensure equal sample sizes for the list and pair methods; $n_{List}=11250$ and $n_{Pair}=9000$ are not equal.\n- **D) Incorrect**: The allocation is not independent of the confidence level used; the calculations use a confidence level (type I error) of $\\gamma=5\\%$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Scandinavian J Statistics - 2019 - Lagerås - How t.md"], "background_text": "The total size of both surveys is $n=15000$, with optimized allocation between the standard binomial survey and the list and the pair method according to $n_{List}=11250$ and $n_{Pair}=9000$. The allocation is based on the formula involving the variances of the estimators from the list and pair methods, and a confidence level (type I error) of $\\gamma=5\\%$ is used.", "folder": "", "qid": "statistic-mcqm-5870", "question": "In the context of power calculation for bias detection with a total survey size of $n=15000$, optimized allocation between the standard binomial survey and the list and the pair method is given by $n_{List}=11250$ and $n_{Pair}=9000$. Which of the following statements are correct regarding the allocation?", "options": ["A) The allocation is optimized to maximize the power of the test.", "B) The allocation is based on the variances of the estimators from the list and pair methods.", "C) The allocation ensures equal sample sizes for the list and pair methods.", "D) The allocation is independent of the confidence level (type I error) used."], "category": "CORRECT", "question_context_used": "The total size of both surveys is $n=15000$, with optimized allocation between the standard binomial survey and the list and the pair method according to $n_{List}=11250$ and $n_{Pair}=9000$. The allocation is based on the formula involving the variances of the estimators from the list and pair methods, and a confidence level (type I error) of $\\gamma=5\\%$ is used.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "The background text specifies that an integer overflow is unlikely except when $N$ is very large, and it provides the condition $|S|\\leqslant N^{2}\\{1-1/\\operatorname*{min}(R,C)\\}/2$. Therefore, options B and D are correct. Option A is incorrect because the size of $N$ being small is not mentioned as a condition for overflow. Option C is incorrect because the size of $R$ and $C$ is not directly related to the overflow condition.\n\n**Correct Choices**: B, D\n\n**Final Answer**: The final answer is $\\boxed{{B, D}}$", "source": ["Computing the Numerator of Ordinal Measures of Ass.md"], "background_text": "An integer overflow condition is unlikely to arise when calculating $S$ except where $N$ is very large, because it can be shown that $|S|\\leqslant N^{2}\\{1-1/\\operatorname*{min}(R,C)\\}/2$.", "folder": "", "qid": "statistic-mcqm-2062", "question": "What is the condition under which an integer overflow is unlikely to occur when calculating $S$?", "options": ["A) When $N$ is very small", "B) When $|S|\\leqslant N^{2}\\{1-1/\\operatorname*{min}(R,C)\\}/2$", "C) When $R$ and $C$ are very small", "D) When $N$ is very large"], "category": "PARTIALLY_CORRECT", "question_context_used": "An integer overflow condition is unlikely to arise when calculating $S$ except where $N$ is very large, because it can be shown that $|S|\\leqslant N^{2}\\{1-1/\\operatorname*{min}(R,C)\\}/2$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **A)** Correct. The text states that $\\pmb{M}$ is calculated from $d_{\\bullet}$ and $\\pmb{\\nu}$ as in equation (6).\n- **B)** Correct. Similarly, $\\pmb{V}$ is also calculated from $d_{\\bullet}$ and $\\pmb{\\nu}$ as in equation (6).\n- **C)** Incorrect. While both $\\pmb{M}$ and $\\pmb{V}$ are derived from the same components, they represent different quantities and are not derived from the same equation.\n- **D)** Incorrect. The text does not specify that $\\pmb{M}$ represents the mean and $\\pmb{V}$ represents the variance of the sample ranges.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["THE_TWO-SAMPLE_t-TEST_BASED_ON_RANGE.md"], "background_text": "For each of the 190 pairs of sample sizes, where $\\pmb{n}_{1}$ and $\\pmb{n_{1}}$ go from 2 to 20, the quantities $\\pmb{M}$ and $\\pmb{V}$ were calculated from $d_{\\bullet}$ and $\\pmb{\\nu}$ as in (6).", "folder": "", "qid": "statistic-mcqm-3715", "question": "Given the quantities calculated for each pair of sample sizes, which of the following statements are correct about the values of $\\pmb{M}$ and $\\pmb{V}$?", "options": ["A) $\\pmb{M}$ is calculated from $d_{\\bullet}$ and $\\pmb{\\nu}$ as in equation (6).", "B) $\\pmb{V}$ is calculated from $d_{\\bullet}$ and $\\pmb{\\nu}$ as in equation (6).", "C) $\\pmb{M}$ and $\\pmb{V}$ are both derived from the same equation.", "D) $\\pmb{M}$ represents the mean and $\\pmb{V}$ represents the variance of the sample ranges."], "category": "PARTIALLY_CORRECT", "question_context_used": "For each of the 190 pairs of sample sizes, where $\\pmb{n}_{1}$ and $\\pmb{n_{1}}$ go from 2 to 20, the quantities $\\pmb{M}$ and $\\pmb{V}$ were calculated from $d_{\\bullet}$ and $\\pmb{\\nu}$ as in (6).", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: Variables 1 and 2 belong to the same group (first block), so their correlation is $wcor=0.7$.\n- **B) Correct**: Variables 1 and 4 belong to different groups (first and second blocks), so their correlation is $lcor=0.2$.\n- **C) Incorrect**: Variables 4 and 5 belong to the same group (second block), so their correlation should be $wcor=0.7$, not $lcor=0.2$.\n- **D) Correct**: Variables 7 and 8 belong to the same group (third block), so their correlation is $wcor=0.7$.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Exact-methods-for-variable-selection-in-principal-.md"], "background_text": "The population correlation matrices $L$ are designed with correlations between variables of the same group (high value, $wcor=0.7$) and different groups (low value, $lcor=0.2$). The matrix is structured such that variables of the same group follow each other, starting with variable 1.", "folder": "", "qid": "statistic-mcqm-4884", "question": "Given a population correlation matrix $L$ with $n=12$, nblocks $=4$, sizeblock $=3$, $wcor=0.7$, and $lcor=0.2$, which of the following statements are true?", "options": ["A) The correlation between variables 1 and 2 is 0.7.", "B) The correlation between variables 1 and 4 is 0.2.", "C) The correlation between variables 4 and 5 is 0.2.", "D) The correlation between variables 7 and 8 is 0.7."], "category": "CORRECT", "question_context_used": "The population correlation matrices $L$ are designed with correlations between variables of the same group (high value, $wcor=0.7$) and different groups (low value, $lcor=0.2$). The matrix is structured such that variables of the same group follow each other, starting with variable 1.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **A** is correct because the function to be minimized includes $-\\ln{|C|}$, which is the log determinant of $C$.\n- **B** is correct because the penalty term $\\lambda\\sum_{i\\ne j}{|c_{i j}|}$ involves the sum of absolute values of the off-diagonal elements of $C$.\n- **C** is correct because the methods described, including graphLasso, are sensitive to the ordering of the variables.\n- **D** is incorrect because the structured graphGarrote is the method designed to take advantage of spatial structure explicitly, not graphLasso.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["Large_Gaussian_Covariance_Matrix_Estimation_With_Markov_Structures__Large_Gaussian_Covariance_Matrix_Estimation_With_Markov_Structures.md"], "background_text": "The graphLasso estimate is given by $\\hat{C}^{\\mathrm{graphLasso}}=\\operatorname{argmin}\\biggl[-\\ln{|C|}+\\operatorname{trace}(C\\bar{A})+\\lambda\\sum_{i\\ne j}{|c_{i j}|}\\biggr]$, where the minimization is over all symmetric and positive definite matrices $C$, and $\\lambda>0$ is a tuning parameter. It does not take advantage of spatial structure explicitly and is sensitive to the ordering of the variables.", "folder": "", "qid": "statistic-mcqm-5349", "question": "Which of the following statements about the graphLasso estimate are correct?", "options": ["A) It minimizes a function involving the log determinant of $C$.", "B) It includes a penalty term proportional to the sum of absolute values of the off-diagonal elements of $C$.", "C) It is sensitive to the ordering of the variables.", "D) It is designed to take advantage of spatial structure explicitly."], "category": "CORRECT", "question_context_used": "The graphLasso estimate is given by $\\hat{C}^{\\mathrm{graphLasso}}=\\operatorname{argmin}\\biggl[-\\ln{|C|}+\\operatorname{trace}(C\\bar{A})+\\lambda\\sum_{i\\ne j}{|c_{i j}|}\\biggr]$, where the minimization is over all symmetric and positive definite matrices $C$, and $\\lambda>0$ is a tuning parameter. It does not take advantage of spatial structure explicitly and is sensitive to the ordering of the variables.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: The context states that $\\theta(p^{3})$ operations are required for each iteration.\n- **B) Correct**: The methods mentioned are based on QR decomposition.\n- **C) Incorrect**: The computation time is likely to increase, not decrease, with larger problem sizes.\n- **D) Incorrect**: The number of iterations required for convergence can vary depending on the matrix and is not fixed.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Exact-methods-for-variable-selection-in-principal-.md"], "background_text": "Computing the largest $k$ eigenvalues of a positive-defined symmetric matrix of order $p$ involves iterative methods based on QR decomposition. Each iteration requires $\\theta(p^{3})$ operations. The computation time can be high for large problems due to the number of solutions to evaluate.", "folder": "", "qid": "statistic-mcqm-4876", "question": "In the context of computing the largest $k$ eigenvalues of a positive-defined symmetric matrix of order $p$, which of the following statements are true?", "options": ["A) The computation requires $\\theta(p^{3})$ operations per iteration.", "B) The methods used are based on QR decomposition.", "C) The computation time decreases as the size of the problem $n$ increases.", "D) The number of iterations required for convergence is fixed and does not depend on the matrix."], "category": "CORRECT", "question_context_used": "Computing the largest $k$ eigenvalues of a positive-defined symmetric matrix of order $p$ involves iterative methods based on QR decomposition. Each iteration requires $\\theta(p^{3})$ operations. The computation time can be high for large problems due to the number of solutions to evaluate.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A) Correct**: The adjacency matrix A is used to determine if nodes i and i' have a common connection j.\n- **B) Correct**: The main topic matrix T is used to check if the main topics associated with the common connection are the same.\n- **C) Incorrect**: The document-term matrix W is not directly involved in the creation of the similarity matrix S.\n- **D) Incorrect**: The label vector Y is estimated based on S, not used in its creation.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["The-latent-topic-block-model-for-the-co-cluster_20.md"], "background_text": "The similarity matrix S is created based on the adjacency matrix A and the main topic matrix T. It increases the similarity between nodes i and i' if they share a common connection j and the same main topic associated with this connection.", "folder": "", "qid": "statistic-mcqm-5904", "question": "What components are involved in the similarity matrix S creation?", "options": ["A) The adjacency matrix A", "B) The main topic matrix T", "C) The document-term matrix W", "D) The label vector Y"], "category": "CORRECT", "question_context_used": "The similarity matrix S is created based on the adjacency matrix A and the main topic matrix T. It increases the similarity between nodes i and i' if they share a common connection j and the same main topic associated with this connection.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct because $\\Sigma_{Y}$ includes the term $\\Gamma_{1}\\Omega_{1}\\Gamma_{1}^{\\top} + \\Gamma_{0}\\Omega_{0}\\Gamma_{0}^{\\top}$.\n- **B)** Correct because $\\Sigma_{Y}$ also includes the term $(\\Gamma_{1}\\eta_{1}^{\\top} + \\Gamma_{0}B\\eta_{2}^{\\top})\\Sigma_{X}(\\Gamma_{1}\\eta_{1}^{\\top} + \\Gamma_{0}B\\eta_{2}^{\\top})^{\\top}$.\n- **C)** Correct because $\\hat{\\Sigma}_{Y}$ is the sum of $\\hat{\\Sigma}_{\\mathrm{fit}}$ and $\\hat{\\Sigma}_{\\mathrm{res}}$.\n- **D)** Correct because $\\hat{\\Sigma}_{\\mathrm{res}}$ is given by $\\Gamma_{1}\\Omega_{1}\\Gamma_{1}^{\\top} + \\Gamma_{0}\\Omega_{0}\\Gamma_{0}^{\\top}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Inner_envelopes_efficient_estimation_in_multivariate_linear_regression.md"], "background_text": "$\\Sigma_{Y} = \\operatorname{var}(Y) = \\operatorname{E}\\{\\operatorname{var}(Y \\mid X)\\} + \\operatorname{var}\\{\\operatorname{E}(Y \\mid X)\\}$. It is expressed as $\\Gamma_{1}\\Omega_{1}\\Gamma_{1}^{\\top} + \\Gamma_{0}\\Omega_{0}\\Gamma_{0}^{\\top} + (\\Gamma_{1}\\eta_{1}^{\\top} + \\Gamma_{0}B\\eta_{2}^{\\top})\\Sigma_{X}(\\Gamma_{1}\\eta_{1}^{\\top} + \\Gamma_{0}B\\eta_{2}^{\\top})^{\\top}$. $\\hat{\\Sigma}_{Y} = \\hat{\\Sigma}_{\\mathrm{fit}} + \\hat{\\Sigma}_{\\mathrm{res}}$, where $\\hat{\\Sigma}_{\\mathrm{res}} = \\Gamma_{1}\\Omega_{1}\\Gamma_{1}^{\\top} + \\Gamma_{0}\\Omega_{0}\\Gamma_{0}^{\\top}$.", "folder": "", "qid": "statistic-mcqm-3327", "question": "In the context of Proposition 3, which of the following statements about $\\Sigma_{Y}$ are correct?", "options": ["A) $\\Sigma_{Y} = \\Gamma_{1}\\Omega_{1}\\Gamma_{1}^{\\top} + \\Gamma_{0}\\Omega_{0}\\Gamma_{0}^{\\top}$", "B) $\\Sigma_{Y}$ includes a term $(\\Gamma_{1}\\eta_{1}^{\\top} + \\Gamma_{0}B\\eta_{2}^{\\top})\\Sigma_{X}(\\Gamma_{1}\\eta_{1}^{\\top} + \\Gamma_{0}B\\eta_{2}^{\\top})^{\\top}$", "C) $\\hat{\\Sigma}_{Y} = \\hat{\\Sigma}_{\\mathrm{fit}} + \\hat{\\Sigma}_{\\mathrm{res}}$", "D) $\\hat{\\Sigma}_{\\mathrm{res}} = \\Gamma_{1}\\Omega_{1}\\Gamma_{1}^{\\top} + \\Gamma_{0}\\Omega_{0}\\Gamma_{0}^{\\top}$"], "category": "PARTIALLY_CORRECT", "question_context_used": "$\\Sigma_{Y} = \\operatorname{var}(Y) = \\operatorname{E}\\{\\operatorname{var}(Y \\mid X)\\} + \\operatorname{var}\\{\\operatorname{E}(Y \\mid X)\\}$. It is expressed as $\\Gamma_{1}\\Omega_{1}\\Gamma_{1}^{\\top} + \\Gamma_{0}\\Omega_{0}\\Gamma_{0}^{\\top} + (\\Gamma_{1}\\eta_{1}^{\\top} + \\Gamma_{0}B\\eta_{2}^{\\top})\\Sigma_{X}(\\Gamma_{1}\\eta_{1}^{\\top} + \\Gamma_{0}B\\eta_{2}^{\\top})^{\\top}$. $\\hat{\\Sigma}_{Y} = \\hat{\\Sigma}_{\\mathrm{fit}} + \\hat{\\Sigma}_{\\mathrm{res}}$, where $\\hat{\\Sigma}_{\\mathrm{res}} = \\Gamma_{1}\\Omega_{1}\\Gamma_{1}^{\\top} + \\Gamma_{0}\\Omega_{0}\\Gamma_{0}^{\\top}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**:\n- **A) Correct**: The exponential covariance function is specified with $\\nu=0.5$, $\\sigma^{2}=0.5$, and $\\phi=1$.\n- **B) Correct**: The coefficients $\\pmb\\beta_{1}$ for the geostatistical response are indeed set to $(1, 5)^{\\top}$.\n- **C) Correct**: The precision $\\tau^{2}$ for the geostatistical response is specified as 1.\n- **D) Correct**: The offset term for the point-pattern response is assigned a value of 0.25.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$.", "source": ["Combining-heterogeneous-spatial-datasets-with-proc.md"], "background_text": "The simulation uses an exponential covariance function with specific parameters ($\\nu=0.5$, $\\sigma^{2}=0.5$, $\\phi=1$). Coefficients and precision for the geostatistical response are specified, and an offset term is used for the point-pattern response.", "folder": "", "qid": "statistic-mcqm-1467", "question": "Which of the following are true about the simulation setup in the first study?", "options": ["A) The covariance function used is exponential with $\\nu=0.5$, $\\sigma^{2}=0.5$, and $\\phi=1$.", "B) The coefficients for the geostatistical response are set to $\\pmb\\beta_{1} = (1, 5)^{\\top}$.", "C) The precision $\\tau^{2}$ for the geostatistical response is set to 1.", "D) The offset term for the point-pattern response is set to 0.25."], "category": "PARTIALLY_CORRECT", "question_context_used": "The simulation uses an exponential covariance function with specific parameters ($\\nu=0.5$, $\\sigma^{2}=0.5$, $\\phi=1$). Coefficients and precision for the geostatistical response are specified, and an offset term is used for the point-pattern response.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **A) Correct**: The algorithm starts by initializing the centers and evaluating weights based on the Kaplan-Meier estimator.\n- **B) Correct**: Voronoï cells are calculated for non-censored observations to update the centers iteratively.\n- **C) Incorrect**: The assignment of censored observations is handled in Step 2, not Step 1.\n- **D) Incorrect**: The truncation bound $\\tau$ is mentioned in a remark but is not a key component of Step 1.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["Vector-quantization-and-clustering-in-the-prese_20.md"], "background_text": "Step 1 of the algorithm involves initializing the centers, evaluating weights using the Kaplan-Meier estimator, and iteratively updating the centers by calculating Voronoï cells for non-censored observations. The assignment of censored observations occurs in Step 2, and the truncation bound $\\tau$ is mentioned in a remark but is not a key component of Step 1.", "folder": "", "qid": "statistic-mcqm-6029", "question": "What are the key components of the proposed algorithm's Step 1 for evaluating $k$ centers?", "options": ["A) Initialization of the centers and evaluation of weights based on the Kaplan-Meier estimator.", "B) Calculation of Voronoï cells for non-censored observations.", "C) Assignment of censored observations to clusters based on estimated distances.", "D) Use of a truncation bound $\\tau$ to approximate the support of the distribution."], "category": "CORRECT", "question_context_used": "Step 1 of the algorithm involves initializing the centers, evaluating weights using the Kaplan-Meier estimator, and iteratively updating the centers by calculating Voronoï cells for non-censored observations. The assignment of censored observations occurs in Step 2, and the truncation bound $\\tau$ is mentioned in a remark but is not a key component of Step 1.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**:\n- **Option A** is correct as it directly states the asymptotic distribution of $n_{i}^{1/2}\\operatorname{vec}(A_{i})$ as given in the context.\n- **Option B** is incorrect because the context does not state that $\\Phi_{i}$ is idempotent; it mentions that $\\Gamma$ is idempotent later in the text.\n- **Option C** is correct as it defines $\\operatorname{vec}(A_{i*})$ using $\\Phi_{i}^{1/2+}$ as given in the context.\n- **Option D** is correct as it states the asymptotic distribution of $n_{i}^{1/2}\\mathrm{vec}(A_{i*})$ as given in the context.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Weighted_chi‐squared_tests_for_partial_common_principal_component_subspaces.md"], "background_text": "The asymptotic distribution of $n_{i}^{1/2}\\operatorname{vec}(A_{i})$ is normal with mean vector 0 and covariance matrix $\\Phi_{i}=(I_{m^{2}}+K_{m m})(\\Omega_{i}\\otimes\\Omega_{i})$. The transformed vector $\\operatorname{vec}(A_{i*})$ is defined using $\\Phi_{i}^{1/2+}$, and its asymptotic distribution is also normal with a specified covariance matrix.", "folder": "", "qid": "statistic-mcqm-5697", "question": "Which of the following statements about the asymptotic distribution of $n_{i}^{1/2}\\operatorname{vec}(A_{i})$ are correct?", "options": ["A) It is normal with mean vector 0 and covariance matrix $\\Phi_{i}=(I_{m^{2}}+K_{m m})(\\Omega_{i}\\otimes\\Omega_{i})$", "B) The covariance matrix $\\Phi_{i}$ is idempotent", "C) $\\operatorname{vec}(A_{i*})=\\Phi_{i}^{1/2+}\\operatorname{vec}(A_{i})$ where $\\Phi_{i}^{1/2+}$ satisfies $\\Phi_{i}^{1/2+\\prime}\\Phi_{i}^{1/2+}=\\Phi_{i}^{+}$", "D) The asymptotic distribution of $n_{i}^{1/2}\\mathrm{vec}(A_{i*})$ is normal with mean vector 0 and covariance matrix $\\frac{1}{2}(I_{m^{2}}+K_{m m})$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The asymptotic distribution of $n_{i}^{1/2}\\operatorname{vec}(A_{i})$ is normal with mean vector 0 and covariance matrix $\\Phi_{i}=(I_{m^{2}}+K_{m m})(\\Omega_{i}\\otimes\\Omega_{i})$. The transformed vector $\\operatorname{vec}(A_{i*})$ is defined using $\\Phi_{i}^{1/2+}$, and its asymptotic distribution is also normal with a specified covariance matrix.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct because it directly follows from the condition for singular information matrices when $k$ is even.\n- **B)** Correct as it is a simplified form of option A.\n- **C)** Correct because it is another simplification leading to the conclusion that $n=2$.\n- **D)** Correct because the relation holds only when $n=2$.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["A Unified Approach for Constructing a Useful Class.md"], "background_text": "When $k$ is even, singular information matrices occur if and only if the selected pairs lead to two $(n-1)$-dimensional lattices of the $n$-dimensional lattice. This implies the relation $2(k n)/2=2k^{n-1}$ or $k n=2k^{n-1}$ or $n=2k^{n-2}$, which holds only when $n=2$.", "folder": "", "qid": "statistic-mcqm-5535", "question": "In the context of singular information matrices, which of the following conditions must hold when $k$ is even?", "options": ["A) $2(k n)/2=2k^{n-1}$", "B) $k n=2k^{n-1}$", "C) $n=2k^{n-2}$", "D) $n=2$"], "category": "CORRECT", "question_context_used": "When $k$ is even, singular information matrices occur if and only if the selected pairs lead to two $(n-1)$-dimensional lattices of the $n$-dimensional lattice. This implies the relation $2(k n)/2=2k^{n-1}$ or $k n=2k^{n-1}$ or $n=2k^{n-2}$, which holds only when $n=2$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The function $f_{q}(\\pmb{x})$ explicitly includes an intercept term $\\theta_{q,0}$.\n- **B)** Correct. The penalty term $J(\\pmb{f})$ involves the Gram matrix $\\pmb{K}$ in the term $\\pmb{\\theta}_{q}^{\\top}\\pmb{K}\\pmb{\\theta}_{q}$.\n- **C)** Correct. The Gram matrix $\\pmb{K}$ is assumed to be invertible for the optimization problem.\n- **D)** Correct. The optimization problem does not require the computation of $K^{-1}$ directly, as it leverages properties of the kernel matrix.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Adaptively-weighted-large-margin-angle-based-_2018.md"], "background_text": "For nonlinear learning with kernel $K(\\cdot,\\cdot)$, the function $f_{q}(\\pmb{x})$ is defined as $\\theta_{q,0}+\\sum_{i=1}^{n}\\theta_{q,i}K(\\pmb{x}_{i},\\pmb{x})$. The penalty term $J(\\pmb{f})$ is $\\sum_{q=1}^{k-1}\\pmb{\\theta}_{q}^{\\top}\\pmb{K}\\pmb{\\theta}_{q}+\\sum_{q=1}^{k-1}\\theta_{q,0}^{2}$. The Gram matrix $\\pmb{K}$ is assumed to be invertible, but the optimization problem does not require computing $K^{-1}$ directly.", "folder": "", "qid": "statistic-mcqm-6100", "question": "In the context of nonlinear learning with kernel $K(\\cdot,\\cdot)$ for WASVM1, which of the following statements are correct?", "options": ["A) The function $f_{q}(\\pmb{x})$ includes an intercept term $\\theta_{q,0}$.", "B) The penalty term $J(\\pmb{f})$ involves the Gram matrix $\\pmb{K}$.", "C) The Gram matrix $\\pmb{K}$ must be invertible for the optimization problem.", "D) The optimization problem does not require the computation of the inverse matrix $K^{-1}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "For nonlinear learning with kernel $K(\\cdot,\\cdot)$, the function $f_{q}(\\pmb{x})$ is defined as $\\theta_{q,0}+\\sum_{i=1}^{n}\\theta_{q,i}K(\\pmb{x}_{i},\\pmb{x})$. The penalty term $J(\\pmb{f})$ is $\\sum_{q=1}^{k-1}\\pmb{\\theta}_{q}^{\\top}\\pmb{K}\\pmb{\\theta}_{q}+\\sum_{q=1}^{k-1}\\theta_{q,0}^{2}$. The Gram matrix $\\pmb{K}$ is assumed to be invertible, but the optimization problem does not require computing $K^{-1}$ directly.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C, D\n\n- **B)** Correct: Re-estimating $\\beta$ on each simulated data set makes the observed test statistic more extreme, decreasing its significance from $1\\cdot4\\%$ to $0\\cdot2\\%$.\n- **C)** Correct: Fixing $\\beta$ throughout simulations is conservative and avoids the computational burden of re-estimation.\n- **D)** Correct: For each simulation, re-estimating $\\beta$ minimizes the test statistic.\n- **A)** Incorrect: Re-estimation decreases, not increases, the significance of the observed test statistic.\n\n**Final Answer**: The final answer is $\\boxed{{B, C, D}}$", "source": ["On_the_use_of_nonparametric_regression_for_model_checking.md"], "background_text": "The test statistic (3) involves a maximum-likelihood fit to the data. Re-estimating $\\beta$ on each simulated data set affects the distribution of the test statistic, making the observed test statistic more extreme compared to simulated values, thus decreasing its significance. Keeping $\\beta$ fixed is conservative and avoids additional maximum likelihood calculations.", "folder": "", "qid": "statistic-mcqm-2005", "question": "In the context of the test statistic (3), which of the following statements are true regarding the re-estimation of $\\beta$ on simulated data sets?", "options": ["A) Re-estimating $\\beta$ on each simulated data set increases the significance of the observed test statistic.", "B) Re-estimating $\\beta$ on each simulated data set decreases the significance of the observed test statistic.", "C) Keeping $\\beta$ fixed throughout simulations is a conservative strategy.", "D) Re-estimating $\\beta$ minimizes the test statistic for each simulation."], "category": "PARTIALLY_CORRECT", "question_context_used": "The test statistic (3) involves a maximum-likelihood fit to the data. Re-estimating $\\beta$ on each simulated data set affects the distribution of the test statistic, making the observed test statistic more extreme compared to simulated values, thus decreasing its significance. Keeping $\\beta$ fixed is conservative and avoids additional maximum likelihood calculations.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A** is correct because the text states that missing rates of 20% and 40% are achieved by setting $(\\theta_1, \\theta_2, \\theta_p)$ accordingly.\n- **B** is correct because the text explicitly mentions using $(n,p)=(200,1000)$ and $(200, 5000)$.\n- **C** is incorrect because the text states that results for other cases are similar, not identical, hence omitted.\n- **D** is correct because the text references a table for the signal-to-noise ratio indexed by $\\mathcal{A}_0$ under those specific conditions.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Model-free-feature-screening-for-ultrahigh-dimens_.md"], "background_text": "The context discusses simulation settings including missing rates, sample sizes, and dimensions, and mentions the omission of other cases due to similarity.", "folder": "", "qid": "statistic-mcqm-4747", "question": "Which of the following statements about the simulation settings are correct?", "options": ["A) The missing rates are set to 20% and 40% by adjusting $(\\theta_1, \\theta_2, \\theta_p)$.", "B) The sample sizes and dimensions used are $(n,p)=(200,1000)$ and $(200, 5000)$.", "C) The results for other cases are omitted because they are identical to the presented cases.", "D) The signal-to-noise ratio is indexed by $\\mathcal{A}_0$ when $n=200, p=5000, MR=0.2$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The context discusses simulation settings including missing rates, sample sizes, and dimensions, and mentions the omission of other cases due to similarity.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C, D\n\n- **Option A** is incorrect because $T_{n}^{Z X}$ does not perform better than $T_{n}$ in high-dimensional cases; in fact, $T_{n}$ performs significantly better.\n- **Option B** is correct as $T_{n}$ is less affected by the curse of dimensionality compared to $T_{n}^{Z X}$.\n- **Option C** is correct because in low-dimensional cases, $T_{n}^{Z X}$ has slightly higher empirical power than $T_{n}$.\n- **Option D** is correct as both $T_{n}$ and $T_{n}^{Z X}$ are affected by the dimension $p$.\n\n**Final Answer**: The final answer is $\\boxed{{B, C, D}}$", "source": ["A-goodness-of-fit-test-for-variable-adju_2019_Comp.md"], "background_text": "In low-dimensional cases, $T_{n}^{Z X}$ is comparable to $T_{n}$ with slightly higher empirical power. In high-dimensional cases, $T_{n}$ performs significantly better than $T_{n}^{Z X}$ and is less affected by the curse of dimensionality. Both tests are affected by the dimension $p$.", "folder": "", "qid": "statistic-mcqm-5978", "question": "In the simulation studies, which of the following statements about the tests $T_{n}$ and $T_{n}^{Z X}$ are correct?", "options": ["A) $T_{n}^{Z X}$ performs better than $T_{n}$ in high-dimensional cases.", "B) $T_{n}$ is less affected by the curse of dimensionality compared to $T_{n}^{Z X}$.", "C) In low-dimensional cases, $T_{n}^{Z X}$ has slightly higher empirical power than $T_{n}$.", "D) Both $T_{n}$ and $T_{n}^{Z X}$ are affected by the dimension $p$."], "category": "CORRECT", "question_context_used": "In low-dimensional cases, $T_{n}^{Z X}$ is comparable to $T_{n}$ with slightly higher empirical power. In high-dimensional cases, $T_{n}$ performs significantly better than $T_{n}^{Z X}$ and is less affected by the curse of dimensionality. Both tests are affected by the dimension $p$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}", "model_answer": ["B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **Option A** is correct because it directly states the reduced power values due to 25% misclassification as given in the context.\n- **Option B** is incorrect because it provides the power values without any misclassification, which are higher than the values under misclassification.\n- **Option C** is correct because it provides the power values for a one-tailed test under 25% misclassification, which are also mentioned in the context.\n- **Option D** is correct because it gives the power values under a 10% misclassification rate, which are different from the 25% scenario but still relevant and mentioned in the context.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["An_investigation_of_the_effect_of_misclassification_on_the_properties_of_x,2-tests_in_the_analysis_of_categorical_data.md"], "background_text": "The context discusses the effect of misclassification errors on the power of a hypothesis test in a binomial population. It provides specific power values under different misclassification rates and scenarios, including one-tailed and two-tailed tests.", "folder": "", "qid": "statistic-mcqm-3624", "question": "In a binomial population with sample size 4, the null hypothesis $p = \\frac{1}{2}$ is rejected if 0 or 4 individuals have the characteristic. Without misclassification, the type I error rate is $\\alpha = 0.125$ and the power for $p = 0.25$ or $0.75$ is 0.320. If 25% of those with the characteristic are misclassified as not having it and 25% of those without it are misclassified as having it, what are the new power values for $p = 0.75$ and $p = 0.90$?", "options": ["A) 0.172 for $p = 0.75$ and 0.248 for $p = 0.90$", "B) 0.320 for $p = 0.75$ and 0.656 for $p = 0.90$", "C) 0.153 for $p = 0.75$ and 0.240 for $p = 0.90$ (one-tailed test)", "D) 0.248 for $p = 0.75$ and 0.453 for $p = 0.90$ (with 10% misclassification)"], "category": "PARTIALLY_CORRECT", "question_context_used": "The context discusses the effect of misclassification errors on the power of a hypothesis test in a binomial population. It provides specific power values under different misclassification rates and scenarios, including one-tailed and two-tailed tests.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": []\n}", "model_answer": [], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: The first expression converges to the second expression plus an error term $o_{p}(1)$, as stated in the context.\n- **B) Correct**: The second expression also converges to a similar form plus an error term $o_{p}(1)$, as per the context.\n- **C) Incorrect**: The expressions are not exactly equal; they converge to each other plus an error term.\n- **D) Incorrect**: The convergence is valid under the given assumptions without needing additional ones.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Efficient-estimation-of-a-partially-linear-panel-d.md"], "background_text": "Under Assumptions 2, 7, and 9–10, as $T\\to\\infty$, the expressions involving $\\hat{X}$ and $\\tilde{X}$ converge to each other plus an error term $o_{p}(1)$. This is part of the asymptotic analysis in the provided context.", "folder": "", "qid": "statistic-mcqm-2350", "question": "Under Assumptions 2, 7, and 9–10, as $T\\to\\infty$, which of the following statements are true regarding the expressions involving $\\hat{X}$ and $\\tilde{X}$?", "options": ["A) $(N T)^{-1}\\hat{X}^{\\top}\\left(I_{N}\\otimes M_{\\hat{X}}\\right)\\varOmega^{-1}(Z)\\left(I_{N}\\otimes M_{\\hat{X}}\\right)\\hat{X}$ converges to $(N T)^{-1}\\tilde{X}^{\\top}\\left(I_{N}\\otimes M_{\\tilde{X}}\\right)\\varOmega^{-1}(Z)\\left(I_{N}\\otimes M_{\\tilde{X}}\\right)\\tilde{X}$ plus $o_{p}(1)$.", "B) $(N T)^{-1}\\hat{X}^{\\top}\\left(I_{N}\\otimes M_{\\hat{X}}\\right)\\varOmega^{-1}(Z)\\left(I_{N}\\otimes M_{\\hat{X}}\\right)$ converges to $(N T)^{-1}\\tilde{X}^{\\top}\\left(I_{N}\\otimes M_{\\tilde{\\Lambda}}\\right)\\varOmega^{-1}(Z)\\left(I_{N}\\otimes M_{\\tilde{\\lambda}}\\right)$ plus $o_{p}(1)$.", "C) The expressions are exactly equal without any error term.", "D) The convergence is only valid under additional assumptions not listed."], "category": "PARTIALLY_CORRECT", "question_context_used": "Under Assumptions 2, 7, and 9–10, as $T\\to\\infty$, the expressions involving $\\hat{X}$ and $\\tilde{X}$ converge to each other plus an error term $o_{p}(1)$. This is part of the asymptotic analysis in the provided context.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A)** Correct. The text mentions that synthetic responses are obtained through a transformation that depends on the data.\n- **B)** Incorrect. The text explicitly states that synthetic observations are not i.i.d. even if the original data were.\n- **C)** Correct. The text describes the use of synthetic observations to derive new representations of empirical means involving Kaplan-Meier integrals.\n- **D)** Correct. In the case of artificial stratification, synthetic observations are computed using a Kaplan-Meier estimator specific to the stratum, as mentioned in the text.\n\n**Final Answer**: The final answer is $\boxed{{A, C, D}}$", "source": ["Scandinavian J Statistics - 2008 - DELECROIX - Non.md"], "background_text": "Synthetic responses are obtained from original observations through a data-dependent transformation, making them not i.i.d. even if the original data were. They are used to derive new representations of empirical means involving Kaplan-Meier integrals. In cases of artificial stratification, synthetic observations are computed using a Kaplan-Meier estimator specific to the stratum of the observation.", "folder": "", "qid": "statistic-mcqm-5196", "question": "What are the key features of the synthetic responses used in the proposed estimation method?", "options": ["A) They are obtained through a transformation that depends on the data.", "B) They are always i.i.d. regardless of the original data's properties.", "C) They are used to derive new representations of empirical means involving Kaplan-Meier integrals.", "D) They are obtained by replacing the Kaplan-Meier estimator with a stratified version in the case of artificial stratification."], "category": "CORRECT", "question_context_used": "Synthetic responses are obtained from original observations through a data-dependent transformation, making them not i.i.d. even if the original data were. They are used to derive new representations of empirical means involving Kaplan-Meier integrals. In cases of artificial stratification, synthetic observations are computed using a Kaplan-Meier estimator specific to the stratum of the observation.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C\n\n**Explanation**: \n- **Option A** is incorrect because the convergence is in probability, not almost surely.\n- **Option B** is correct as stated in the background text.\n- **Option C** is correct because the text states that $\\sum_{i=1}^{n}\\zeta_{i}^{n}$ converges in probability with the same limit as $\\psi_{T_{1},T_{2}}(\\vartheta) \\sum_{i=1}^{n} \\xi_{i}^{n}$.\n- **Option D** is incorrect because the convergence of $\\sum_{i=1}^{n}\\xi_{i}^{n}$ is not directly dependent on the independence of $B$ and $\\overline{B}$.\n\n**Final Answer**: The final answer is $\\boxed{{B, C}}$.", "source": ["Scandinavian J Statistics - 2019 - Féron - Efficie.md"], "background_text": "The term $\\xi_{i}^{n}$ is defined as $(\\Delta_{i}^{n}X^{2} - \\Delta_{i}^{n}X^{1})^{2}$, and its sum converges to $\\mathfrak{e}_{1,2}(\\vartheta)^{2}\\int_{0}^{T}e^{2\\vartheta t}\\sigma_{t}^{2}d t$ in probability. The limit is almost surely positive by Assumption 1.", "folder": "", "qid": "statistic-mcqm-5829", "question": "Which of the following statements about the convergence of $\\sum_{i=1}^{n}\\xi_{i}^{n}$ are correct?", "options": ["A) $\\sum_{i=1}^{n}\\xi_{i}^{n}$ converges to $\\mathfrak{e}_{1,2}(\\vartheta)^{2}\\int_{0}^{T}e^{2\\vartheta t}\\sigma_{t}^{2}d t$ almost surely.", "B) The limit of $\\sum_{i=1}^{n}\\xi_{i}^{n}$ is almost surely positive under Assumption 1.", "C) $\\sum_{i=1}^{n}\\xi_{i}^{n}$ converges in probability to the same limit as $\\psi_{T_{1},T_{2}}(\\vartheta) \\sum_{i=1}^{n} \\xi_{i}^{n}$.", "D) The convergence of $\\sum_{i=1}^{n}\\xi_{i}^{n}$ depends on the independence of $B$ and $\\overline{B}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The term $\\xi_{i}^{n}$ is defined as $(\\Delta_{i}^{n}X^{2} - \\Delta_{i}^{n}X^{1})^{2}$, and its sum converges to $\\mathfrak{e}_{1,2}(\\vartheta)^{2}\\int_{0}^{T}e^{2\\vartheta t}\\sigma_{t}^{2}d t$ in probability. The limit is almost surely positive by Assumption 1.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **Option A** is correct because the text states that angle-based methods perform better than non-angle-based ones.\n- **Option B** is correct because the text states that weighted methods perform better than unweighted classifiers.\n- **Option C** is incorrect because the text states that as the contamination percentage increases, all methods worsen, not improve.\n- **Option D** is incorrect because the text states that the improvements of the OWASVM1,2 and IWASVM1,2 over the SVMs get smaller, not larger, in terms of misclassification errors compared with the linear examples.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Adaptively-weighted-large-margin-angle-based-_2018.md"], "background_text": "The angle-based methods perform better than non-angle-based ones, and the weighted methods perform better than unweighted classifiers. As the contamination percentage increases, all methods worsen to some degree. The improvements of the OWASVM1,2 and IWASVM1,2 over the SVMs get smaller in terms of misclassification errors compared with the linear examples.", "folder": "", "qid": "statistic-mcqm-6086", "question": "Which of the following statements about the performance of the methods in the nonlinear classification example are correct?", "options": ["A) Angle-based methods perform better than non-angle-based ones.", "B) Weighted methods perform better than unweighted classifiers.", "C) As the contamination percentage increases, all methods improve in performance.", "D) The improvements of the OWASVM1,2 and IWASVM1,2 over the SVMs in terms of misclassification errors are larger in nonlinear examples than in linear examples."], "category": "CORRECT", "question_context_used": "The angle-based methods perform better than non-angle-based ones, and the weighted methods perform better than unweighted classifiers. As the contamination percentage increases, all methods worsen to some degree. The improvements of the OWASVM1,2 and IWASVM1,2 over the SVMs get smaller in terms of misclassification errors compared with the linear examples.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**: The context states that high association (T8) leads to worse performance for misspecified variable orders compared to low association (T1), making the indices more sensitive to variable order misspecification (A, C). Conversely, low association makes the indices less sensitive to variable order misspecification (D). Option B is incorrect as it contradicts the provided information.\n\n**Final Answer**: The final answer is $\boxed{{A, C, D}}$", "source": ["Standardized Drought Indices A Novel Univariate an.md"], "background_text": "The results show that for high association (T8) drought indices for both methods perform considerably worse for misspecified variable order compared with low association (T1), according to all measures under consideration. This indicates that high association makes the indices more sensitive to variable order misspecification.", "folder": "", "qid": "statistic-mcqm-5821", "question": "What are the implications of high association (T8) versus low association (T1) on the performance of drought indices for misspecified variable orders?", "options": ["A) High association leads to worse performance for misspecified variable orders compared to low association.", "B) Low association leads to worse performance for misspecified variable orders compared to high association.", "C) High association makes the indices more sensitive to variable order misspecification.", "D) Low association makes the indices less sensitive to variable order misspecification."], "category": "CORRECT", "question_context_used": "The results show that for high association (T8) drought indices for both methods perform considerably worse for misspecified variable order compared with low association (T1), according to all measures under consideration. This indicates that high association makes the indices more sensitive to variable order misspecification.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C, D\n\n- **A)** Incorrect. The text specifies $c_h$ are non-increasing ($1=c_{1}\\geqslant\\dots\\geqslant c_{m-1}\\geqslant0$), not strictly decreasing.\n- **B)** Correct. $c_h=1/h$ is given as an example to model tapering effects.\n- **C)** Correct. The text states that setting all $c_h=1$ reduces the model to the pairwise order model.\n- **D)** Correct. Theorem 1's optimality of $w_0$ holds irrespective of the $c_h$ in (2).\n\n**Final Answer**: The final answer is $\\boxed{{B, C, D}}$", "source": ["Design_of_order-of-addition_experiments.md"], "background_text": "The coefficients $c_h$ are known quantities such that $1=c_{1}\\geqslant\\dots\\geqslant c_{m-1}\\geqslant0$, capturing possible tapering effects. Examples include $c_h=1/h$ or $c_h=c^{h-1}$ with $0<c<1$. Setting all $c_h=1$ reduces the model to the pairwise order model. The optimality of $w_0$ is independent of the $c_h$ choice.", "folder": "", "qid": "statistic-mcqm-6089", "question": "Regarding the coefficients $c_h$ in the model, which statements are correct?", "options": ["A) $c_h$ must be strictly decreasing in $h$.", "B) $c_h$ can be chosen as $c_h = 1/h$ to model tapering effects.", "C) Setting all $c_h = 1$ reduces the model to the pairwise order model of van Nostrand and Voelkel.", "D) The choice of $c_h$ does not affect the optimality of the uniform design measure $w_0$."], "category": "CORRECT", "question_context_used": "The coefficients $c_h$ are known quantities such that $1=c_{1}\\geqslant\\dots\\geqslant c_{m-1}\\geqslant0$, capturing possible tapering effects. Examples include $c_h=1/h$ or $c_h=c^{h-1}$ with $0<c<1$. Setting all $c_h=1$ reduces the model to the pairwise order model. The optimality of $w_0$ is independent of the $c_h$ choice.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"B\", \"C\", \"D\"]\n}", "model_answer": ["B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **Option A** is correct because the central limit theorem requires that the terms $y_{j+}-\\phi_{1}$ and $y_{j-}-\\phi_{2}$ be independent and identically distributed.\n- **Option B** is correct because the context specifies that $s_{n}$ must be replaced by $c_{n}=n\\eta_{\\theta}^{*}(-1)$ for the convergence to occur.\n- **Option C** is incorrect because the context does not mention the necessity of the sample size $n$ approaching infinity for this specific convergence.\n- **Option D** is incorrect because the context does not mention any requirement for the variances of $y_{j+}$ and $y_{j-}$ to be equal.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Asymptotic_inference_from_sequential_design_in_a_nonlinear_situation.md"], "background_text": "The convergence of (A·1) to the right-hand side of (2.8) is based on the central limit theorem, which requires certain conditions such as independence and identical distribution of the terms, and replacement of $s_{n}$ by $c_{n}=n\\eta_{\\theta}^{*}(-1)$.", "folder": "", "qid": "statistic-mcqm-2194", "question": "What conditions are necessary for the convergence of (A·1) to the right-hand side of (2.8) according to the central limit theorem?", "options": ["A) $y_{j+}-\\phi_{1}$ and $y_{j-}-\\phi_{2}$ must be independent and identically distributed.", "B) $s_{n}$ must be replaced by $c_{n}=n\\eta_{\\theta}^{*}(-1)$.", "C) The sample size $n$ must approach infinity.", "D) The variances of $y_{j+}$ and $y_{j-}$ must be equal."], "category": "PARTIALLY_CORRECT", "question_context_used": "The convergence of (A·1) to the right-hand side of (2.8) is based on the central limit theorem, which requires certain conditions such as independence and identical distribution of the terms, and replacement of $s_{n}$ by $c_{n}=n\\eta_{\\theta}^{*}(-1)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **A) Correct**: The theorem shows that the estimation procedure is consistent as $n\\to\\infty$ because $\\eta(\\delta,n)\\to0$.\n- **B) Correct**: Since $\\eta(\\delta,n)\\to0$ as $n\\to\\infty$, it is a decreasing function of $n$.\n- **C) Incorrect**: The probability $P\\left(\\left\\|C-\\widehat{C}_{\\widehat{m}}\\right\\|<\\delta\\right)$ actually increases as $n$ increases because $\\eta(\\delta,n)$ decreases.\n- **D) Incorrect**: The theorem guarantees that the error is less than $\\delta$ with high probability, not exact recovery.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["An-algorithm-for-automatic-curve-det_2011_Computat.md"], "background_text": "Theorem 3.1 states that for each $\\delta$ there exists $\\eta(\\delta,n)$ with $\\eta(\\delta,n)\\to0$ as $n\\to\\infty$ such that $P\\left(\\left\\|C-\\widehat{C}_{\\widehat{m}}\\right\\|<\\delta\\right)>1-\\eta(\\delta,n)$.", "folder": "", "qid": "statistic-mcqm-2282", "question": "According to Theorem 3.1, for each $\\delta$ there exists $\\eta(\\delta,n)$ with $\\eta(\\delta,n)\\to0$ as $n\\to\\infty$ such that $P\\left(\\left\\|C-\\widehat{C}_{\\widehat{m}}\\right\\|<\\delta\\right)>1-\\eta(\\delta,n)$. Which of the following statements are correct?", "options": ["A) The estimation procedure is consistent as $n\\to\\infty$.", "B) $\\eta(\\delta,n)$ is a decreasing function of $n$.", "C) The probability $P\\left(\\left\\|C-\\widehat{C}_{\\widehat{m}}\\right\\|<\\delta\\right)$ decreases as $n$ increases.", "D) The theorem guarantees exact recovery of $C$ as $n\\to\\infty$."], "category": "CORRECT", "question_context_used": "Theorem 3.1 states that for each $\\delta$ there exists $\\eta(\\delta,n)$ with $\\eta(\\delta,n)\\to0$ as $n\\to\\infty$ such that $P\\left(\\left\\|C-\\widehat{C}_{\\widehat{m}}\\right\\|<\\delta\\right)>1-\\eta(\\delta,n)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A) MultiNeSS**: Correct, as it is one of the methods compared in the logistic model results.\n- **B) MultiNeSS+**: Correct, as it is another method compared, denoted by MultiNeSS with the refitting step.\n- **C) Nonconvex**: Correct, as it is explicitly mentioned as one of the methods under comparison.\n- **D) COSIE and M-GRAF**: Correct, as both are mentioned in the context as methods compared, despite COSIE not using the correct model.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["Latent_space_models_for_multiplex_networks_with_shared_structure.md"], "background_text": "The methods under comparison are MultiNeSS, MultiNeSS+, Nonconvex, COSIE, and M-GRAF. COSIE does not use the correct model for these data, since it assumes a random dot product graph model without a logistic link.", "folder": "", "qid": "statistic-mcqm-3212", "question": "In the logistic model results, which methods are compared for evaluating the recovery of the common and individual structures?", "options": ["A) MultiNeSS", "B) MultiNeSS+", "C) Nonconvex", "D) COSIE and M-GRAF"], "category": "PARTIALLY_CORRECT", "question_context_used": "The methods under comparison are MultiNeSS, MultiNeSS+, Nonconvex, COSIE, and M-GRAF. COSIE does not use the correct model for these data, since it assumes a random dot product graph model without a logistic link.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A\n\n**Explanation**:\n- **A) Correct**: This is the expression for $\\tilde{\\gamma}$ as given in the context.\n- **B) Incorrect**: This is the expression derived from the method of moments, not the combined estimator.\n- **C) Incorrect**: This is the expression for $\\widehat{\\gamma}_{k}$ in the case $\\gamma > 0$, not for $\\tilde{\\gamma}$ in the case $\\gamma < 0$.\n- **D) Incorrect**: This is the expression for $\\widehat{\\sigma}_{k}$ in the case $\\gamma > 0$, not for $\\tilde{\\gamma}$ in the case $\\gamma < 0$.\n\n**Final Answer**: The final answer is $\\boxed{{A}}$", "source": ["A-bootstrap-goodness-of-fit-test-for-the-gener_200.md"], "background_text": "For the case $\\gamma < 0$, the combined estimator $\\tilde{\\gamma}$ is given by $\\tilde{\\gamma} = \\frac{\\bar{X}}{\\bar{X} - X_{(n)}}$.", "folder": "", "qid": "statistic-mcqm-5232", "question": "For the case $\\gamma < 0$, the combined estimator $\\tilde{\\gamma}$ is given by which of the following expressions?", "options": ["A) $\\tilde{\\gamma} = \\frac{\\bar{X}}{\bar{X} - X_{(n)}}$", "B) $\\tilde{\\gamma} = 1 - \\frac{\\sigma}{\\bar{X}}$", "C) $\\tilde{\\gamma} = -\\left(W_{n-k+1} - \\frac{1}{k}\\sum_{j=1}^{k} W_{n-j+1}\\right)$", "D) $\\tilde{\\gamma} = \\widehat{\\gamma}_{k} \\exp\\left\\{W_{n-k+1} + \\widehat{\\gamma}_{k} \\log(k/n)\\right\\}$"], "category": "CORRECT", "question_context_used": "For the case $\\gamma < 0$, the combined estimator $\\tilde{\\gamma}$ is given by $\\tilde{\\gamma} = \\frac{\\bar{X}}{\\bar{X} - X_{(n)}}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n- **A** is correct because the context explicitly states that if $n/N$ is negligible, $V_{2}$ can be ignored.\n- **B** is incorrect because the condition to ignore $V_{2}$ is related to the sampling fraction, not the outcome model.\n- **C** is incorrect for the same reason as B; it's not about the propensity score model.\n- **D** is correct as it combines the conditions mentioned in the context regarding the order of $V_{2}/V_{1}$ and the negligibility of $n/N$.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Soft_calibration_for_selection_bias_problems_under_mixed-effects_models.md"], "background_text": "The context mentions that if the sampling fraction $n/N$ is negligible, $V_{2}$ can be ignored in variance estimation, as $V_{2}/V_{1}$ is of order $O(n/N)$.", "folder": "", "qid": "statistic-mcqm-5560", "question": "Under what conditions can we ignore $V_{2}$ in the estimation of the variance of $\\hat{\\theta}_{w}$ according to the context?", "options": ["A) When the sampling fraction $n/N$ is negligible.", "B) When the outcome $y_{i}$ follows a linear mixed-effects model.", "C) When the propensity score model is correctly specified.", "D) When $V_{2}/V_{1}$ is of order $O(n/N)$ and $n/N$ is negligible."], "category": "CORRECT", "question_context_used": "The context mentions that if the sampling fraction $n/N$ is negligible, $V_{2}$ can be ignored in variance estimation, as $V_{2}/V_{1}$ is of order $O(n/N)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "From the background text, we know that the breakdown point of $\\Sigma_{\\nu}^{o}(P)$ is equal to $1/(q+\\nu)$ when $P$ is smooth in the sense of (5) (Option A) and is given by $\\beta(P;\\nu)$ in general (Option C). Option B is incorrect because the breakdown point is not just a lower bound but is exactly $1/(q+\\nu)$ under the specified condition. Option D is incorrect because the breakdown point is not always greater than $1/(q+\\nu)$.\n\n**Correct Choices**: A, C\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Scandinavian J Statistics - 2005 - DÜMBGEN - On th.md"], "background_text": "The breakdown point of $\\Sigma_{\\nu}^{o}(P)$, denoted by $\\epsilon_{\\nu}^{o}(P)$, is equal to $1/(q+\\nu)$ when $P$ is smooth in the sense of (5). In general, it is given by $\\beta(P;\\nu)$. The breakdown point is a lower bound for the breakdown point of $\\Sigma_{\\nu}^{o}(P)$ when $P$ is smooth in the sense of (5), but it is not always greater than $1/(q+\\nu)$.", "folder": "", "qid": "statistic-mcqm-4038", "question": "Which of the following statements are true regarding the breakdown point of $\\Sigma_{\\nu}^{o}(P)$?", "options": ["A) The breakdown point of $\\Sigma_{\\nu}^{o}(P)$ is equal to $1/(q+\\nu)$ when $P$ is smooth in the sense of (5).", "B) The breakdown point of $\\Sigma_{\\nu}^{o}(P)$ is lower bounded by $1/(q+\\nu)$ when $P$ is smooth in the sense of (5).", "C) The breakdown point of $\\Sigma_{\\nu}^{o}(P)$ is given by $\\beta(P;\\nu)$ in general.", "D) The breakdown point of $\\Sigma_{\\nu}^{o}(P)$ is always greater than $1/(q+\\nu)$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The breakdown point of $\\Sigma_{\\nu}^{o}(P)$, denoted by $\\epsilon_{\\nu}^{o}(P)$, is equal to $1/(q+\\nu)$ when $P$ is smooth in the sense of (5). In general, it is given by $\\beta(P;\\nu)$. The breakdown point is a lower bound for the breakdown point of $\\Sigma_{\\nu}^{o}(P)$ when $P$ is smooth in the sense of (5), but it is not always greater than $1/(q+\\nu)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct, as the true error variance for $Y_{U}$ in population A is explicitly stated as 0.823.\n- **B)** Correct, the mean of biased estimates for $Y_{U}$ in population B is given as 0.0557.\n- **C)** Correct, the true error variance for $Y_{B}$ in population C is listed as 0.0674.\n- **D)** Correct, the mean of biased estimates for $Y_{B}$ in population A is provided as 0.352.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Selection without Replacement from within Strata w.md"], "background_text": "The true error variance $V(Y_{U})$ of $Y_{U}$ is given by $\\Sigma\\:_{4}^{1}p^{\\prime}{}_{i j}(r^{\\prime}{}_{i}+r^{\\prime}{}_{j}-2\\bar{r}^{\\prime})^{2}{}_{i}$ and the mean value of the biased estimates $V_{B}(Y_{U})$ by $\\Sigma\\frac{}{^8}p_{~i j}^{\\prime}(r_{~i}^{\\prime}-r_{~j}^{\\prime})^{2}$. The values for populations A, B, and C are provided in the context.", "folder": "", "qid": "statistic-mcqm-3826", "question": "Given the true error variance $V(Y_{U})$ of $Y_{U}$ is calculated by $\\Sigma\\:_{4}^{1}p^{\\prime}{}_{i j}(r^{\\prime}{}_{i}+r^{\\prime}{}_{j}-2\\bar{r}^{\\prime})^{2}{}_{i}$ and the mean value of the biased estimates $V_{B}(Y_{U})$ by $\\Sigma\\frac{}{^8}p_{~i j}^{\\prime}(r_{~i}^{\\prime}-r_{~j}^{\\prime})^{2}$, which of the following statements are correct based on the provided data for populations A, B, and C?", "options": ["A) The true error variance for $Y_{U}$ in population A is 0.823.", "B) The mean of biased estimates for $Y_{U}$ in population B is 0.0557.", "C) The true error variance for $Y_{B}$ in population C is 0.0674.", "D) The mean of biased estimates for $Y_{B}$ in population A is 0.352."], "category": "PARTIALLY_CORRECT", "question_context_used": "The true error variance $V(Y_{U})$ of $Y_{U}$ is given by $\\Sigma\\:_{4}^{1}p^{\\prime}{}_{i j}(r^{\\prime}{}_{i}+r^{\\prime}{}_{j}-2\\bar{r}^{\\prime})^{2}{}_{i}$ and the mean value of the biased estimates $V_{B}(Y_{U})$ by $\\Sigma\\frac{}{^8}p_{~i j}^{\\prime}(r_{~i}^{\\prime}-r_{~j}^{\\prime})^{2}$. The values for populations A, B, and C are provided in the context.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A)** Correct. $N_{1}^{*}$ is adjusted based on the trimming proportion $A$ to maintain the specified risks, as described in the context.\n- **B)** Correct. The context states that the denominator in the formula for $N_{1}^{*}$ is an approximation for the efficiency of the $A$ trimmed $t$.\n- **C)** Incorrect. The context does not state that $N_{1}^{*}$ is always greater than $N_{1}$; it depends on the value of $A$ and the adjustment formula.\n- **D)** Incorrect. The context explicitly mentions that the values of $N_{1}$ given by Schneiderman & Armitage (1962b) are a little underestimated and require adjustment.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["Sequential_trimmed_t_tests_.md"], "background_text": "The terminal sample size $N_{1}^{*}$ depends on the trimming proportion $A$ and is adjusted to maintain specified risks $2\\alpha$ and $\\beta$. The formula for $N_{1}^{*}$ includes a denominator that approximates the efficiency of the $A$ trimmed $t$. The values of $N_{1}$ given by Schneiderman & Armitage (1962b) are a little underestimated and require adjustment.", "folder": "", "qid": "statistic-mcqm-5703", "question": "Which of the following are true about the terminal sample size $N_{1}^{*}$ in the context of the restricted sequential trimmed $t$ test?", "options": ["A) $N_{1}^{*}$ is adjusted based on the trimming proportion $A$ to maintain specified risks $2\\alpha$ and $\\beta$.", "B) The formula for $N_{1}^{*}$ includes a denominator that approximates the efficiency of the $A$ trimmed $t$.", "C) $N_{1}^{*}$ is always greater than $N_{1}$, the terminal sample size with no trimming.", "D) The values of $N_{1}$ given by Schneiderman & Armitage (1962b) are exact and require no adjustment."], "category": "CORRECT", "question_context_used": "The terminal sample size $N_{1}^{*}$ depends on the trimming proportion $A$ and is adjusted to maintain specified risks $2\\alpha$ and $\\beta$. The formula for $N_{1}^{*}$ includes a denominator that approximates the efficiency of the $A$ trimmed $t$. The values of $N_{1}$ given by Schneiderman & Armitage (1962b) are a little underestimated and require adjustment.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The text states that under $H_{0}$, the scaled ODC converges to a standard Brownian bridge process.\n- **B)** Correct. The text mentions that the scaled Mann–Whitney statistic converges to the integral of a Brownian bridge.\n- **C)** Correct. The text states that the scaled one-sided Kolmogorov–Smirnov distance converges to a Weibull random variable.\n- **D)** Incorrect. The ODC converges to a standard Brownian bridge process, not a standard normal distribution.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Nonparametric_tests_for_and_against_likelihood_ratio_ordering_in_the_two-sample_problem.md"], "background_text": "Under the null hypothesis $H_{0}\\colon F=G$, the text provides several convergence results, including the convergence of the ODC to a standard Brownian bridge process, the Mann–Whitney statistic to the integral of a Brownian bridge, and the one-sided Kolmogorov–Smirnov distance to a Weibull random variable.", "folder": "", "qid": "statistic-mcqm-5700", "question": "Under the null hypothesis $H_{0}\\colon F=G$, which of the following convergence results are true?", "options": ["A) $c_{m,n}\\big[\\big\\{\\mathrm{ODC}_{m,n}(t)-t\\big\\}:0\\leqslant t\\leqslant1\\big]\\to\\{W_{0}(t):0\\leqslant t\\leqslant1\\}$ in distribution.", "B) $c_{m,n}\\left(\\int F_{m}d G_{n}-\\frac{1}{2}\\right)\\to\\int_{0}^{1}W_{0}(t)d t$ in distribution.", "C) $c_{m,n}\\Bigg[\\operatorname*{sup}_{0\\leqslant t\\leqslant1}\\left\\{\\mathrm{ODC}_{m,n}(t)-t\\right\\}\\Bigg]\\to\\|W_{0}^{+}(t)\\|$ in distribution.", "D) The ODC converges to a standard normal distribution."], "category": "CORRECT", "question_context_used": "Under the null hypothesis $H_{0}\\colon F=G$, the text provides several convergence results, including the convergence of the ODC to a standard Brownian bridge process, the Mann–Whitney statistic to the integral of a Brownian bridge, and the one-sided Kolmogorov–Smirnov distance to a Weibull random variable.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A) Ln**: The FRB value for Ln is 0.32, which is greater than 0.30.\n- **B) Wald**: The FRB value for Wald is 0.41, which is greater than 0.30.\n- **C) Scores**: The FRB value for Scores is 0.26, which is not greater than 0.30.\n- **D) LRT**: The FRB value for LRT is 0.27, which is not greater than 0.30.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["Robust-tests-for-linear-regression-models-b_2016_C.md"], "background_text": "The table provides test results for various statistical tests, including their Asymptotic and FRB values. The FRB values for Ln, Wald, Scores, and LRT are 0.32, 0.41, 0.26, and 0.27 respectively.", "folder": "", "qid": "statistic-mcqm-5713", "question": "Based on the table of test results, which tests have a FRB value greater than 0.30?", "options": ["A) Ln", "B) Wald", "C) Scores", "D) LRT"], "category": "CORRECT", "question_context_used": "The table provides test results for various statistical tests, including their Asymptotic and FRB values. The FRB values for Ln, Wald, Scores, and LRT are 0.32, 0.41, 0.26, and 0.27 respectively.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **A) Correct**: The product over $i$ indeed represents the multiplication of probabilities for all segments, as the likelihood function is a product over all segments $i$ and within each segment, a product over all positions $j$ from $a_i$ to $b_i$.\n- **B) Incorrect**: $p_{c_{i}}$ is the probability parameter for the $i^{th}$ segment, where $c_i=0$ for unchanged segments and $c_i=1$ for changed segments. It is not exclusively the probability of no change.\n- **C) Correct**: $x_{j}$ represents the observation at position $j$, as it is used in the likelihood function to weigh the probabilities.\n- **D) Incorrect**: The likelihood function considers both changed and unchanged segments, as indicated by $c_i$ being 0 or 1.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Maximum_likelihood_estimation_of_multiple_change_points.md"], "background_text": "The likelihood function under the hypothesis that there are some changed segments is given by $\\prod_{i}\\left\\{\\prod_{j=a_{i}}^{b_{i}}p_{c_{i}}^{1-x_{j}}(1-p_{c_{i}})^{x_{j}}\\right\\}$, where $a_{i},b_{i}$ are the first and the last position of the $i^{th}$ segment respectively, and $c_{i}=0$ for unchanged segments and $c_{i}=1$ for changed segments.", "folder": "", "qid": "statistic-mcqm-5106", "question": "Given the likelihood function for changed segments is $\\prod_{i}\\left\\{\\prod_{j=a_{i}}^{b_{i}}p_{c_{i}}^{1-x_{j}}(1-p_{c_{i}})^{x_{j}}\\right\\}$, which of the following statements are true?", "options": ["A) The product over $i$ represents the multiplication of probabilities for all segments.", "B) $p_{c_{i}}$ is the probability of no change in the $i^{th}$ segment.", "C) $x_{j}$ represents the observation at position $j$.", "D) The likelihood function only considers changed segments."], "category": "CORRECT", "question_context_used": "The likelihood function under the hypothesis that there are some changed segments is given by $\\prod_{i}\\left\\{\\prod_{j=a_{i}}^{b_{i}}p_{c_{i}}^{1-x_{j}}(1-p_{c_{i}})^{x_{j}}\\right\\}$, where $a_{i},b_{i}$ are the first and the last position of the $i^{th}$ segment respectively, and $c_{i}=0$ for unchanged segments and $c_{i}=1$ for changed segments.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **A) Correct**: The theorem explicitly states that the supermodular order is strictly stronger than the concordance order for $d=3$.\n- **B) Correct**: The text mentions that $\\mathbf{X}$ and $\\mathbf{Y}$ have the same marginal distributions.\n- **C) Incorrect**: The example provided shows a supermodular function $f$ where $E f(\\mathbf{X}) > E f(\\mathbf{Y})$, contradicting this statement.\n- **D) Incorrect**: The function $f$ is explicitly described as supermodular in the text.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Some-Remarks-on-the-Supermodular-Order_2000_Journa.md"], "background_text": "The text discusses the comparison between the supermodular order and the concordance order for $d=3$, using specific examples of random vectors $\\mathbf{X}$ and $\\mathbf{Y}$ with uniform distributions on certain points. It shows that while $\\mathbf{X}$ and $\\mathbf{Y}$ have the same marginal distributions and $\\mathbf{X} \\leqslant_{c} \\mathbf{Y}$, there exists a supermodular function $f$ such that $E f(\\mathbf{X}) > E f(\\mathbf{Y})$, demonstrating that the supermodular order is strictly stronger than the concordance order.", "folder": "", "qid": "statistic-mcqm-4956", "question": "In the context of the supermodular order and concordance order for $d=3$, which of the following statements are correct based on the provided example?", "options": ["A) The supermodular order is strictly stronger than the concordance order for $d=3$.", "B) The random vectors $\\mathbf{X}$ and $\\mathbf{Y}$ have the same marginal distributions.", "C) For all supermodular functions $f$, $E f(\\mathbf{X}) \\leq E f(\\mathbf{Y})$.", "D) The function $f(x_{1},x_{2},x_{3})=\\operatorname*{max}\\{x_{1}+x_{2}+x_{3}-4,0\\}$ is not supermodular."], "category": "CORRECT", "question_context_used": "The text discusses the comparison between the supermodular order and the concordance order for $d=3$, using specific examples of random vectors $\\mathbf{X}$ and $\\mathbf{Y}$ with uniform distributions on certain points. It shows that while $\\mathbf{X}$ and $\\mathbf{Y}$ have the same marginal distributions and $\\mathbf{X} \\leqslant_{c} \\mathbf{Y}$, there exists a supermodular function $f$ such that $E f(\\mathbf{X}) > E f(\\mathbf{Y})$, demonstrating that the supermodular order is strictly stronger than the concordance order.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **Option A** is correct as it represents an intermediate step in the derivation of $E\\bar{f}_{T}(x)$.\n- **Option B** is correct because it shows the final form of $E\\bar{f}_{T}(x)$ as a convolution of $K_{h}$ and $f_{T}(x)$.\n- **Option C** is correct as it is another representation of the convolution result.\n- **Option D** is incorrect because it represents an earlier form in the derivation, not the simplified final result.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Consistency-and-asymptotic-normality-for-a-nonpara.md"], "background_text": "The expected value of $\\bar{f}_{T}(x)$ is derived using the characteristic functions and involves convolution with the kernel $K_{h}$. The proof involves several steps of integration and transformation based on the given assumptions and properties of the characteristic functions.", "folder": "", "qid": "statistic-mcqm-4907", "question": "In the context of the proof for Lemma 1, which of the following statements about the expected value of $\\bar{f}_{T}(x)$ are correct?", "options": ["A) $E\\bar{f}_{T}(x) = \\frac{1}{2\\pi}\\int e^{-i x}\\mathcal{F}_{K}(t h)\\phi_{f_{l^{\\prime}}}(t)d t$", "B) $E\\bar{f}_{T}(x) = K_{h}*f_{T}(x)$", "C) $E\\bar{f}_{T}(x) = \\int K(z)f_{T}(x-h z)d z$", "D) $E\\bar{f}_{T}(x) = \\frac{1}{h}\\sum_{j=1}^{n}E\\frac{1}{2\\pi}\\int\\exp\\left(-i t\\frac{x-W_{j}}{h}\\right)\\mathcal{F}_{K}(t)\\phi_{f_{l^{\\prime}}}\\left(\\frac{t}{h}\\right)\\psi_{j}\\left(\\frac{t}{h}\\right)d t$"], "category": "CORRECT", "question_context_used": "The expected value of $\\bar{f}_{T}(x)$ is derived using the characteristic functions and involves convolution with the kernel $K_{h}$. The proof involves several steps of integration and transformation based on the given assumptions and properties of the characteristic functions.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A)** Correct. Under normality, the null distribution of $T/h$ is indeed $F_{h,n-k}$.\n- **B)** Correct. Under nonnormality, $T$ converges to $\\chi_{h}^{2}$ as $n \\to \\infty$ under certain conditions.\n- **C)** Correct. The test statistic $T$ is invariant under the transformation from $\\boldsymbol{y}$ to $y/\\sigma$.\n- **D)** Incorrect. The nominal test size based on the limiting distribution under nonnormality is often far from the actual test size, as mentioned in the context.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$.", "source": ["Asymptotic-expansion-of-the-null-distribution-of-t.md"], "background_text": "The test statistic $T$ for testing the null hypothesis $\\mathrm{H}_{0}: H{\\pmb\\beta}={\\bf0}$ in a linear model is given by $T=\\frac{\\hat{\\beta}^{\\prime}H^{\\prime}\\{H(X^{\\prime}X)^{-1}H^{\\prime}\\}^{-1}H\\hat{\\beta}}{\\hat{\\sigma}^{2}}$. Under normality, $T/h$ follows an $F_{h,n-k}$ distribution. Under nonnormality, $T$ converges to $\\chi_{h}^{2}$ as $n \\to \\infty$, but the nominal test size based on this limiting distribution may not match the actual test size.", "folder": "", "qid": "statistic-mcqm-978", "question": "In the context of testing the null hypothesis $\\mathrm{H}_{0}: H{\\pmb\\beta}={\\bf0}$ in a linear model, which of the following statements about the test statistic $T$ are correct?", "options": ["A) Under normality, the null distribution of $T/h$ is distributed as $F_{h,n-k}$.", "B) Under nonnormality, the null distribution of $T$ converges to $\\chi_{h}^{2}$ as the sample size $n$ tends to infinity.", "C) The test statistic $T$ is invariant under the transformation from $\\boldsymbol{y}$ to $y/\\sigma$.", "D) The nominal test size based on the limiting distribution under nonnormality is always close to the actual test size."], "category": "PARTIALLY_CORRECT", "question_context_used": "The test statistic $T$ for testing the null hypothesis $\\mathrm{H}_{0}: H{\\pmb\\beta}={\\bf0}$ in a linear model is given by $T=\\frac{\\hat{\\beta}^{\\prime}H^{\\prime}\\{H(X^{\\prime}X)^{-1}H^{\\prime}\\}^{-1}H\\hat{\\beta}}{\\hat{\\sigma}^{2}}$. Under normality, $T/h$ follows an $F_{h,n-k}$ distribution. Under nonnormality, $T$ converges to $\\chi_{h}^{2}$ as $n \\to \\infty$, but the nominal test size based on this limiting distribution may not match the actual test size.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**: \n- **A** is correct because the coefficient $c_{i,j}$ is indeed 1 when the distance $d_{i j}=0$.\n- **B** is correct because the coefficient $c_{i,j}$ is 0.4 when the distance $d_{i j}=1$.\n- **C** is correct because the coefficient $c_{i,j}$ is 0.15 when the distance $d_{i j}=2$.\n- **D** is correct because the coefficient $c_{i,j}$ is 0 for any distance $d_{i j}$ not equal to 0, 1, or 2, which includes $d_{i j}=3$.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["Large_Gaussian_Covariance_Matrix_Estimation_With_Markov_Structures__Large_Gaussian_Covariance_Matrix_Estimation_With_Markov_Structures.md"], "background_text": "Model 9 (Markov random field of order 2) defines coefficients $c_{i,j}$ as follows: $c_{i,j}=1$ if $d_{i j}=0$, 0.4 if $d_{i j}=1$, 0.15 if $d_{i j}=2$, and 0 otherwise.", "folder": "", "qid": "statistic-mcqm-5347", "question": "For Model 9 (Markov random field of order 2), which of the following statements about the coefficients $c_{i,j}$ are correct?", "options": ["A) $c_{i,j}=1$ if $d_{i j}=0$", "B) $c_{i,j}=0.4$ if $d_{i j}=1$", "C) $c_{i,j}=0.15$ if $d_{i j}=2$", "D) $c_{i,j}=0$ if $d_{i j}=3$"], "category": "CORRECT", "question_context_used": "Model 9 (Markov random field of order 2) defines coefficients $c_{i,j}$ as follows: $c_{i,j}=1$ if $d_{i j}=0$, 0.4 if $d_{i j}=1$, 0.15 if $d_{i j}=2$, and 0 otherwise.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct, the unbiased estimate $V_{U}(Y_{U})$ for units 1,2 is listed as 0.41 in Table 3.\n- **B)** Correct, the biased estimate $V_{B}(Y_{U})$ for units 1,3 is given as 0.87.\n- **C)** Correct, the Horvitz & Thompson estimate for units 2,4 is shown as +1.20.\n- **D)** Incorrect, the weighted mean of the unbiased estimates is 0.823, not 0.971 (which is the mean of the biased estimates).\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Selection without Replacement from within Strata w.md"], "background_text": "Table 3 shows the values of the unbiased estimate $V_{U}(Y_{U})$, the biased estimate $V_{B}(Y_{U})$, and the Horvitz & Thompson unbiased estimate for individual samples (one degree of freedom each) from population A. The table includes data for different unit pairs and a weighted mean.", "folder": "", "qid": "statistic-mcqm-3789", "question": "Based on Table 3, which of the following statements about the estimates of error (1 d.f.) from population A are correct?", "options": ["A) The unbiased estimate $V_{U}(Y_{U})$ for units 1,2 is 0.41.", "B) The biased estimate $V_{B}(Y_{U})$ for units 1,3 is 0.87.", "C) The Horvitz & Thompson estimate for units 2,4 is +1.20.", "D) The weighted mean of the unbiased estimates is 0.971."], "category": "PARTIALLY_CORRECT", "question_context_used": "Table 3 shows the values of the unbiased estimate $V_{U}(Y_{U})$, the biased estimate $V_{B}(Y_{U})$, and the Horvitz & Thompson unbiased estimate for individual samples (one degree of freedom each) from population A. The table includes data for different unit pairs and a weighted mean.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": []\n}", "model_answer": [], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: Independence for disjoint sets is a defining property.\n- **B) Correct**: Infinite divisibility of $\\Lambda(A)$ is a key characteristic.\n- **C) Incorrect**: The characteristic function does have a Lévy–Khintchin representation.\n- **D) Incorrect**: $\\Lambda$ is stationary with a specific control measure.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Scandinavian J Statistics - 2021 - Glück - A solut.md"], "background_text": "An infinitely divisible random measure $\\Lambda$ satisfies: (a) for disjoint sets $(E_{m})_{m\\in\\mathbb{N}}$ in $\\mathcal{E}_{0}(\\mathbb{R}^{d})$, $(\\Lambda(E_{m}))_{m\\in\\mathbb{N}}$ are independent; if $\\cup_{m=1}^{\\infty}E_{m}\\in\\mathcal{E}_{0}(\\mathbb{R}^{d})$, then $\\Lambda(\\cup_{m=1}^{\\infty}E_{m})=\\sum_{m=1}^{\\infty}\\Lambda(E_{m})$ a.s. (b) $\\Lambda(A)$ is infinitely divisible for any $A\\in\\mathcal{E}_{0}(\\mathbb{R}^{d})$. The characteristic function $\\varphi_{\\Lambda(A)}$ has a Lévy–Khintchin representation, and $\\Lambda$ is stationary with a specific control measure.", "folder": "", "qid": "statistic-mcqm-5334", "question": "Which of the following are properties of an infinitely divisible random measure $\\Lambda$?", "options": ["A) For disjoint sets $(E_{m})_{m\\in\\mathbb{N}}$ in $\\mathcal{E}_{0}(\\mathbb{R}^{d})$, the sequence $(\\Lambda(E_{m}))_{m\\in\\mathbb{N}}$ consists of independent random variables.", "B) $\\Lambda(A)$ has an infinitely divisible distribution for any $A\\in\\mathcal{E}_{0}(\\mathbb{R}^{d})$.", "C) The characteristic function $\\varphi_{\\Lambda(A)}$ does not have a Lévy–Khintchin representation.", "D) The random measure $\\Lambda$ is always non-stationary."], "category": "CORRECT", "question_context_used": "An infinitely divisible random measure $\\Lambda$ satisfies: (a) for disjoint sets $(E_{m})_{m\\in\\mathbb{N}}$ in $\\mathcal{E}_{0}(\\mathbb{R}^{d})$, $(\\Lambda(E_{m}))_{m\\in\\mathbb{N}}$ are independent; if $\\cup_{m=1}^{\\infty}E_{m}\\in\\mathcal{E}_{0}(\\mathbb{R}^{d})$, then $\\Lambda(\\cup_{m=1}^{\\infty}E_{m})=\\sum_{m=1}^{\\infty}\\Lambda(E_{m})$ a.s. (b) $\\Lambda(A)$ is infinitely divisible for any $A\\in\\mathcal{E}_{0}(\\mathbb{R}^{d})$. The characteristic function $\\varphi_{\\Lambda(A)}$ has a Lévy–Khintchin representation, and $\\Lambda$ is stationary with a specific control measure.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A)** Correct. Partition testing indeed divides the parameter space into $2^k$ disjoint subspaces for $k$ hypotheses.\n- **B)** Correct. The inference that $\\theta_i > 0$ is made if and only if all relevant $H_{0I}^*$ hypotheses are rejected.\n- **C)** Incorrect. Because the $H_{0I}^*$’s are disjoint, no multiplicity adjustment is needed among them.\n- **D)** Correct. The disjoint nature of $H_{0I}^*$ means at most one can be true.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Hochberg's_Step-Up_Method_Cutting_Corners_Off_Holm's_Step-Down_Method.md"], "background_text": "Partition testing is a general principle for multiple testing that divides the parameter space into $2^k$ disjoint subspaces for $k$ hypotheses. It infers that $\\theta_i > 0$ if and only if all $H_{0I}^*$ with $i \\in I$ are rejected. The null hypotheses $H_{0I}^*$ are disjoint, so at most one $H_{0I}^*$ can be true, eliminating the need for multiplicity adjustment among the $H_{0I}^*$’s.", "folder": "", "qid": "statistic-mcqm-5959", "question": "In the context of partition testing for multiple hypotheses, which of the following statements are correct?", "options": ["A) Partition testing divides the parameter space into $2^k$ disjoint subspaces for $k$ hypotheses.", "B) For each $i$, infer that $\\theta_i > 0$ if and only if all $H_{0I}^*$ with $i \\in I$ are rejected.", "C) Partition testing requires multiplicity adjustment among the $H_{0I}^*$’s to control the Familywise Error Rate.", "D) The null hypotheses $H_{0I}^*$ are disjoint, meaning at most one $H_{0I}^*$ can be true."], "category": "CORRECT", "question_context_used": "Partition testing is a general principle for multiple testing that divides the parameter space into $2^k$ disjoint subspaces for $k$ hypotheses. It infers that $\\theta_i > 0$ if and only if all $H_{0I}^*$ with $i \\in I$ are rejected. The null hypotheses $H_{0I}^*$ are disjoint, so at most one $H_{0I}^*$ can be true, eliminating the need for multiplicity adjustment among the $H_{0I}^*$’s.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. Under the assumptions of Corollary 2, $X_{n}^{i}$ converges almost surely to one of the eigenvectors of $B$.\n- **B)** Correct. The condition specified ensures that $X_{n}^{i}$ converges almost surely to $V_{i}$ or $-V_{i}$.\n- **C)** Correct. Under all assumptions of Corollary 2, $X_{n}^{i}$ converges almost surely to $V_{i}$ or $-V_{i}$.\n- **D)** Incorrect. The text clearly states that $X_{n}^{i}$ does converge almost surely to an eigenvector of $B$ under the given assumptions.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Widening-the-scope-of-an-eigenvector-stochastic-ap.md"], "background_text": "Corollary 2 states that under certain assumptions, the process $X_{n}^{i}$ converges almost surely to one of the eigenvectors of $B$. Specifically, if on $\\cup_{j=i+1}^{p}\\left\\{X_{n}^{i}\\longrightarrow\\pm V_{j}\\right\\}$, lim inf E $\\left[\\left<B_{n}X_{n}^{i},V_{i}\\right>^{2}|T_{n}\\right]>0$ a.s., then $X_{n}^{i}$ converges a.s. to $V_{i}$ or $-V_{i}$. Additionally, under different sets of assumptions, $X_{n}^{i}$ converges a.s. to $V_{i}$ or $-V_{i}$.", "folder": "", "qid": "statistic-mcqm-3441", "question": "Under the assumptions of Corollary 2, which of the following statements are true regarding the convergence of $X_{n}^{i}$?", "options": ["A) $X_{n}^{i}$ converges almost surely to one of the eigenvectors of $B$.", "B) $X_{n}^{i}$ converges almost surely to $V_{i}$ or $-V_{i}$ if the condition on $\\cup_{j=i+1}^{p}\\left\\{X_{n}^{i}\\longrightarrow\\pm V_{j}\\right\\}$ is met.", "C) $X_{n}^{i}$ converges almost surely to $V_{i}$ or $-V_{i}$ under all assumptions of Corollary 2.", "D) $X_{n}^{i}$ does not converge almost surely to any eigenvector of $B$."], "category": "PARTIALLY_CORRECT", "question_context_used": "Corollary 2 states that under certain assumptions, the process $X_{n}^{i}$ converges almost surely to one of the eigenvectors of $B$. Specifically, if on $\\cup_{j=i+1}^{p}\\left\\{X_{n}^{i}\\longrightarrow\\pm V_{j}\\right\\}$, lim inf E $\\left[\\left<B_{n}X_{n}^{i},V_{i}\\right>^{2}|T_{n}\\right]>0$ a.s., then $X_{n}^{i}$ converges a.s. to $V_{i}$ or $-V_{i}$. Additionally, under different sets of assumptions, $X_{n}^{i}$ converges a.s. to $V_{i}$ or $-V_{i}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **A** is correct because the context states that the unknown parameters $\\Theta_k$ are estimated by penalized least squares.\n- **B** is correct because the penalized criterion uses quadratic penalties for the directions $x$ and $s$.\n- **C** is correct because the roughness in the direction $t$ is controlled by the number of orthogonal basis functions, $K$.\n- **D** is incorrect because the context mentions that the smoothness parameters $\\lambda_x$ and $\\lambda_s$ are chosen based on GCV or REML, not AIC.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Additive_Function-on-Function_Regression__Additive_Function-on-Function_Regression.md"], "background_text": "The unknown parameters $\\Theta_k$ in the additive function-on-function regression model are estimated by penalized least squares. The penalized criterion uses quadratic penalties for the directions $x$ and $s$, and controls the roughness in the direction $t$ by the number of orthogonal basis functions, $K$. The smoothness parameters $\\lambda_x$ and $\\lambda_s$ can be chosen based on criteria such as generalized cross-validation (GCV) or restricted maximum likelihood (REML).", "folder": "", "qid": "statistic-mcqm-5060", "question": "Which of the following are true about the estimation of the unknown parameters $\\Theta_k$ in the additive function-on-function regression model?", "options": ["A) The parameters are estimated by penalized least squares.", "B) The penalized criterion uses quadratic penalties for the directions $x$ and $s$.", "C) The roughness in the direction $t$ is controlled by the number of orthogonal basis functions, $K$.", "D) The smoothness parameters $\\lambda_x$ and $\\lambda_s$ are chosen based on the Akaike Information Criterion (AIC)."], "category": "CORRECT", "question_context_used": "The unknown parameters $\\Theta_k$ in the additive function-on-function regression model are estimated by penalized least squares. The penalized criterion uses quadratic penalties for the directions $x$ and $s$, and controls the roughness in the direction $t$ by the number of orthogonal basis functions, $K$. The smoothness parameters $\\lambda_x$ and $\\lambda_s$ can be chosen based on criteria such as generalized cross-validation (GCV) or restricted maximum likelihood (REML).", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C\n\n- **A) $b_{1}$ must be negative**: This is incorrect because a negative $b_{1}$ makes the integral infinite.\n- **B) $b_{1}$ must be nonnegative**: This is correct as nonnegative $b_{1}$ ensures the integral is finite.\n- **C) $b_{1}$ must be zero**: This is also correct because zero is a nonnegative value, satisfying the condition.\n- **D) $b_{1}$ can be any real number**: This is incorrect because negative values of $b_{1}$ lead to an infinite integral.\n\n**Final Answer**: The final answer is $\\boxed{{B, C}}$", "source": ["Fully_Bayesian_spline_smoothing_and_intrinsic_autoregressive_priors.md"], "background_text": "When $b_{1}$ is negative, the integral of the right-hand side with respect to $\\delta_{1}$ is not finite. Thus $b_{1}$ must be nonnegative.", "folder": "", "qid": "statistic-mcqm-5642", "question": "What must be true about $b_{1}$ for the integral of the right-hand side with respect to $\\delta_{1}$ to be finite?", "options": ["A) $b_{1}$ must be negative", "B) $b_{1}$ must be nonnegative", "C) $b_{1}$ must be zero", "D) $b_{1}$ can be any real number"], "category": "PARTIALLY_CORRECT", "question_context_used": "When $b_{1}$ is negative, the integral of the right-hand side with respect to $\\delta_{1}$ is not finite. Thus $b_{1}$ must be nonnegative.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\"answer\": [\"B\"]}", "model_answer": ["B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The text mentions Wald regions based on Fisher information as one type of confidence region.\n- **B)** Correct. Wald regions based on observed information are also described.\n- **C)** Correct. Deviance regions, based on profile likelihood, are listed among the types of confidence regions.\n- **D)** Correct. Score regions based on expected information are mentioned in the text.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Information-on-parameters-of-interest-decreases-_2.md"], "background_text": "The text describes several types of confidence regions for the parameter $\\psi$, including Wald regions based on Fisher or observed information, deviance regions based on profile likelihood, and score regions based on expected or observed information.", "folder": "", "qid": "statistic-mcqm-6072", "question": "Which of the following are types of confidence regions for the parameter $\\psi$ based on observations on random variables $X_1, \\dots, X_n$?", "options": ["A) Wald regions based on Fisher information.", "B) Wald regions based on observed information.", "C) Deviance regions based on profile likelihood.", "D) Score regions based on expected information."], "category": "CORRECT", "question_context_used": "The text describes several types of confidence regions for the parameter $\\psi$, including Wald regions based on Fisher or observed information, deviance regions based on profile likelihood, and score regions based on expected or observed information.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The procedure flags an observation as an outlier based on the comparison of the absolute value of its standardized residual divided by $\\hat{\\sigma}$ to a critical value from the $t$-distribution.\n- **B) Correct**: The degrees of freedom $\\nu$ are indeed calculated as the nearest integer to $n-\\sum_{i=1}^{n}w_{i}(X_{i})$.\n- **C) Incorrect**: The Bonferroni and Rootzen’s adjustments were found to be almost identical, not significantly different.\n- **D) Correct**: The critical value $t_{q,\\nu}$ is set to the $q$ quantile of the $t$-distribution, where $q = 2n\\alpha$ with $\\alpha = 0.05$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Robust_SiZer_for_Exploration_of_Regression_Structures_and_Outlier_Detection__Robust_SiZer_for_Exploration_of_Regression_Structures_and_Outlier_Detection.md"], "background_text": "For outlier testing, if the ESS of $\\hat{m}_{h,c}(X_{i})$ is greater than 5, an observation is flagged as an outlier if $|\\frac{\\hat{\\epsilon}_{i}}{\\hat{\\sigma}}| > t_{q,\\nu}$, where $t_{q,\\nu}$ is the $q$ quantile of the $t$-distribution with $\\nu$ degrees of freedom. The degrees of freedom $\\nu$ are defined as the nearest integer to $n-\\sum_{i=1}^{n}w_{i}(X_{i})$. The Bonferroni and Rootzen’s adjustments for multiple testing were found to be almost identical, as illustrated in Figure 7.", "folder": "", "qid": "statistic-mcqm-921", "question": "Which of the following are true about the outlier testing procedure described in the context?", "options": ["A) An observation is flagged as an outlier if the absolute value of its standardized residual divided by $\\hat{\\sigma}$ exceeds a critical value from the $t$-distribution.", "B) The degrees of freedom $\\nu$ for the $t$-distribution are defined as the nearest integer to $n-\\sum_{i=1}^{n}w_{i}(X_{i})$.", "C) The Bonferroni adjustment and Rootzen’s adjustment for multiple testing yield significantly different critical values.", "D) The critical value $t_{q,\\nu}$ is set to the $q$ quantile of the $t$-distribution, where $q = 2n\\alpha$ with $\\alpha = 0.05$."], "category": "PARTIALLY_CORRECT", "question_context_used": "For outlier testing, if the ESS of $\\hat{m}_{h,c}(X_{i})$ is greater than 5, an observation is flagged as an outlier if $|\\frac{\\hat{\\epsilon}_{i}}{\\hat{\\sigma}}| > t_{q,\\nu}$, where $t_{q,\\nu}$ is the $q$ quantile of the $t$-distribution with $\\nu$ degrees of freedom. The degrees of freedom $\\nu$ are defined as the nearest integer to $n-\\sum_{i=1}^{n}w_{i}(X_{i})$. The Bonferroni and Rootzen’s adjustments for multiple testing were found to be almost identical, as illustrated in Figure 7.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}", "model_answer": ["B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **A)** Correct. The model assumes the rows of $E$ are independent, though violations are possible.\n- **B)** Correct. The condition $n \\geqslant p + m$ is necessary for the model to be valid.\n- **C)** Correct. $X$ must be of rank $p$ for the model assumptions to hold.\n- **D)** Incorrect. This describes a specific case considered by Nyblom [9], not a general assumption of the model.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Similar-tests-for-covariance-structures-in-multi_2.md"], "background_text": "The model assumes $Y = X B + E$ with $Y$ and $E$ as $n \\times m$ matrices, $X$ as $n \\times p$ fixed and of rank $p$, and $n \\geqslant p + m$. It is often assumed that the rows of $E$ are independent, but violations occur in plausible situations like multivariate time series models.", "folder": "", "qid": "statistic-mcqm-5570", "question": "In the multivariate linear regression model $Y = X B + E$, where $Y$ and $E$ are $n \\times m$ matrices, $X$ is $n \\times p$ and fixed, and $B$ is $p \\times m$, what conditions must hold for the model assumptions to be valid?", "options": ["A) The rows of $E$ are independent.", "B) $n \\geqslant p + m$.", "C) $X$ is of rank $p$.", "D) $E$ has a multivariate normal distribution with mean matrix 0 and covariance matrix $\\Omega(\\theta) \\otimes \\Sigma$."], "category": "CORRECT", "question_context_used": "The model assumes $Y = X B + E$ with $Y$ and $E$ as $n \\times m$ matrices, $X$ as $n \\times p$ fixed and of rank $p$, and $n \\geqslant p + m$. It is often assumed that the rows of $E$ are independent, but violations occur in plausible situations like multivariate time series models.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "To determine the correct options, let's analyze each statement step-by-step:\n\n1. **Option A**: The sum of the roots of the equation $\\lambda^{\\bullet}-19\\lambda^{\\bullet}+70\\lambda-14=0$ is indeed 19, as per Vieta's formulas. **This is correct.**\n2. **Option B**: The product of the roots is 14, again by Vieta's formulas. **This is correct.**\n3. **Option C**: The roots of the characteristic equation are indeed the characteristic roots of $\\pmb{A}$. **This is correct.**\n4. **Option D**: It is stated that each one of them gives one zero root for $\\lambda$. **This is correct.**\n\n**Correct Choices**: A, B, C, D\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["DISTRIBUTION_THEORY_OF_TWO_ESTIMATES_FOR_STANDARD_DEVIATION_BASED_ON_SECOND_VARIATE_DIFFERENCES.md"], "background_text": "The characteristic equation of the matrix $\\pmb{A}$ for $n=7$ is given by $\\lambda^{\\bullet}-19\\lambda^{\\bullet}+70\\lambda-14=0$. The roots of this equation are the characteristic roots of $\\pmb{A}$, and it can be proved that each one of them gives one zero root for $\\lambda$.", "folder": "", "qid": "statistic-mcqm-2976", "question": "For the case when $n=7$, the characteristic equation of the matrix $\\pmb{A}$ is $\\lambda^{\\bullet}-19\\lambda^{\\bullet}+70\\lambda-14=0$. Which of the following statements are correct?", "options": ["A) The sum of the roots is 19.", "B) The product of the roots is 14.", "C) The roots are the characteristic roots of $\\pmb{A}$.", "D) The equation has one zero root."], "category": "PARTIALLY_CORRECT", "question_context_used": "The characteristic equation of the matrix $\\pmb{A}$ for $n=7$ is given by $\\lambda^{\\bullet}-19\\lambda^{\\bullet}+70\\lambda-14=0$. The roots of this equation are the characteristic roots of $\\pmb{A}$, and it can be proved that each one of them gives one zero root for $\\lambda$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A** is correct because the text states that for $\\nu>1$, $U_{n0}$ is just the standard score statistic.\n- **B** is correct because for $0<\\nu<1$, $\\bar{U}_{n0}$ converges in distribution to $S(\\nu)$, a strictly stable distribution.\n- **C** is correct because for $\\nu=2$, $\\bar{U}_{n0}$ converges to a zero-mean normal distribution.\n- **D** is correct because the text describes anomalous behavior of the test based on $U_{n0}$ for $\\nu>1$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["On_some_nonregular_tests_for_a_modified_Weibull_model.md"], "background_text": "The test statistic $U_{n0}$ is defined for testing $H_{0}: \\kappa=0$. For $\\nu>1$, $U_{n0}$ is the standard score statistic. For $0<\\nu<1$, $\\bar{U}_{n0}$ converges to a strictly stable distribution $S(\\nu)$, and for $\\nu=2$, it converges to a zero-mean normal distribution. The test based on $U_{n0}$ shows anomalous behavior for $\\nu>1$.", "folder": "", "qid": "statistic-mcqm-5521", "question": "In the context of testing the null hypothesis $H_{0}: \\kappa=0$, which of the following statements about the test statistic $U_{n0}$ are correct?", "options": ["A) For $\\nu>1$, $U_{n0}$ is the standard score statistic.", "B) For $0<\\nu<1$, $\\bar{U}_{n0}$ converges in distribution to a strictly stable distribution $S(\\nu)$.", "C) For $\\nu=2$, $\\bar{U}_{n0}$ converges in distribution to a zero-mean normal distribution.", "D) The test based on $U_{n0}$ exhibits anomalous behavior for $\\nu>1$."], "category": "CORRECT", "question_context_used": "The test statistic $U_{n0}$ is defined for testing $H_{0}: \\kappa=0$. For $\\nu>1$, $U_{n0}$ is the standard score statistic. For $0<\\nu<1$, $\\bar{U}_{n0}$ converges to a strictly stable distribution $S(\\nu)$, and for $\\nu=2$, it converges to a zero-mean normal distribution. The test based on $U_{n0}$ shows anomalous behavior for $\\nu>1$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. As stated in the background text, the sum of the values of $u_{A}^{(n)}$ over any argument is zero.\n- **B)** Correct. The background text mentions that there are $\\left\\{{n\\atop r}\\right\\}$ $r$-factor interactions in an $n$-dimensional table.\n- **C)** Incorrect. The definition of $u_{A}^{(n)}$ is a generalization for any $n$-dimensional table, not just for $n=2,3$.\n- **D)** Correct. The background text indicates that $u_{A}^{(n)}$ can be used to factorize the function $p$ through the unique representation $l=\\ln p=\\sum_{A\\subseteq\\overline{{{n}}}}u_{A}^{(n)}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Collapsibility of Multidimensional Contingency Tab.md"], "background_text": "An r-factor interaction $u_{A}^{(n)}$ for an $n$-dimensional table corresponding to a subset $A$ of r factors is given by $u_{A}^{(n)}(\\mathbf{i}_{A})=\\sum_{z\\subseteq A}(-1)^{|A-Z|}\\bar{l}_{Z}(\\mathbf{i}_{Z})$, where $l=\\ln p,\\bar{l}_{z}$ is as defined in Section 3 and $|A-Z|$ denotes the number of factors in the set $A-Z$. There are $\\left\\{{n\\atop r}\\right\\}$ $r$-factor interactions in an $n$-dimensional table, corresponding to each subset $A=\\{a_{1},...,a_{r}\\}$ with $r$ elements. It is shown in the Appendix that the sum of the values of $u_{A}^{(n)}$ over any argument is zero: $\\sum_{i_{k}}u_{A}^{(n)}(i_{a_{1}},...,i_{a_{r}})=0,k=a_{1},...,a_{r}$.", "folder": "", "qid": "statistic-mcqm-4904", "question": "In an $n$-dimensional table, which of the following statements are true about $r$-factor interactions $u_{A}^{(n)}$?", "options": ["A) The sum of the values of $u_{A}^{(n)}$ over any argument is zero.", "B) There are $\\left\\{{n\\atop r}\\right\\}$ $r$-factor interactions in an $n$-dimensional table.", "C) $u_{A}^{(n)}$ is defined only for $n=2,3$.", "D) $u_{A}^{(n)}$ can be used to factorize the function $p$."], "category": "PARTIALLY_CORRECT", "question_context_used": "An r-factor interaction $u_{A}^{(n)}$ for an $n$-dimensional table corresponding to a subset $A$ of r factors is given by $u_{A}^{(n)}(\\mathbf{i}_{A})=\\sum_{z\\subseteq A}(-1)^{|A-Z|}\\bar{l}_{Z}(\\mathbf{i}_{Z})$, where $l=\\ln p,\\bar{l}_{z}$ is as defined in Section 3 and $|A-Z|$ denotes the number of factors in the set $A-Z$. There are $\\left\\{{n\\atop r}\\right\\}$ $r$-factor interactions in an $n$-dimensional table, corresponding to each subset $A=\\{a_{1},...,a_{r}\\}$ with $r$ elements. It is shown in the Appendix that the sum of the values of $u_{A}^{(n)}$ over any argument is zero: $\\sum_{i_{k}}u_{A}^{(n)}(i_{a_{1}},...,i_{a_{r}})=0,k=a_{1},...,a_{r}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, D\n\n- **A)** Incorrect. Pidigits (l) shows the same number of accurate digits (15) for all statistics in both Excel versions.\n- **B)** Correct. Mavro (l) shows differences in the number of accurate digits for $s$ and $\\rho$ between Excel97/00/02 and Excel2003.\n- **C)** Incorrect. Numacc1 (l) shows the same number of accurate digits (15) for all statistics in both Excel versions.\n- **D)** Correct. Numacc4 (h) shows differences in the number of accurate digits for $\\bar{x}$, $s$, and $\\rho$ between Excel97/00/02 and Excel2003.\n\n**Final Answer**: The final answer is $\\boxed{{B, D}}$", "source": ["On-the-accuracy-of-statistical-procedures-in_2005_.md"], "background_text": "Table 3 shows the number of accurate digits for $\\bar{x}$, $s$, and $\\rho$ (the mean, standard deviation, and correlation coefficient) for different data sets across Excel97/00/02 and Excel2003 versions. Some data sets show differences in the number of accurate digits between these versions.", "folder": "", "qid": "statistic-mcqm-940", "question": "Which of the following data sets show a difference in the number of accurate digits for $\\bar{x}$, $s$, and $\\rho$ between Excel97/00/02 and Excel2003?", "options": ["A) Pidigits (l)", "B) Mavro (l)", "C) Numacc1 (l)", "D) Numacc4 (h)"], "category": "PARTIALLY_CORRECT", "question_context_used": "Table 3 shows the number of accurate digits for $\\bar{x}$, $s$, and $\\rho$ (the mean, standard deviation, and correlation coefficient) for different data sets across Excel97/00/02 and Excel2003 versions. Some data sets show differences in the number of accurate digits between these versions.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"C\", \"D\"]\n}", "model_answer": ["C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C\n\n- **A)** Incorrect. The rate $e_{n}$ does not involve $s_{c}$ and $\\hat{s}_{c}$; they are replaced by the upper bound $k$.\n- **B)** Correct. Theorem 3 indeed states this convergence rate for $W(\\widehat{\\delta}_{\\mathbf{y}_{c}})-W(\\delta_{\\mathbf{y}_{c}})$.\n- **C)** Correct. Theorem 4 provides a bound on $\\|\\mathbf{y}_{k}^{\\lambda}-\\mathbf{y}^{0}\\|_{2}$ involving $\\lambda$, $k$, and $\\lambda_{\\operatorname*{min}}(\\pmb{\\Sigma}_{k})$.\n- **D)** Incorrect. The subspace $M_{k}$ is defined with the last $p-k$ components being $0$, not the first.\n\n**Final Answer**: The final answer is $\\boxed{{B, C}}$", "source": ["A Road to Classification in High Dimensional Space.md"], "background_text": "Theoretical properties of S-ROAD1 include rates of convergence and bounds on estimation errors. Theorem 3 provides rates involving $e_{n}$, and Theorem 4 gives bounds on the distance between estimated and true parameters. The subspace $M_{k}$ is defined with the last $p-k$ components being $0$.", "folder": "", "qid": "statistic-mcqm-1426", "question": "Which of the following statements are true about the theoretical properties of S-ROAD1 as described in the context?", "options": ["A) The rate $e_{n}$ in Theorem 3 involves $s_{c}$ and $\\hat{s}_{c}$ as upper bounds.", "B) Theorem 3 shows that $W(\\widehat{\\delta}_{\\mathbf{y}_{c}})-W(\\delta_{\\mathbf{y}_{c}})=O_{p}(e_{n})$ where $e_{n}=(c^{2}\\vee k)\\surd\\{\\log(k)/n\\}$.", "C) Theorem 4 provides a bound on $\\|\\mathbf{y}_{k}^{\\lambda}-\\mathbf{y}^{0}\\|_{2}$ in terms of $\\lambda$, $k$, and $\\lambda_{\\operatorname*{min}}(\\pmb{\\Sigma}_{k})$.", "D) The subspace $M_{k}$ in Theorem 4 is defined as the subspace in $R^{p}$ with the first $p-k$ components being $0$."], "category": "PARTIALLY_CORRECT", "question_context_used": "Theoretical properties of S-ROAD1 include rates of convergence and bounds on estimation errors. Theorem 3 provides rates involving $e_{n}$, and Theorem 4 gives bounds on the distance between estimated and true parameters. The subspace $M_{k}$ is defined with the last $p-k$ components being $0$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **A** is correct because the context describes reconstructing the smooth trajectory of the response by smoothing the data for each $i$ and de-meaning it.\n- **B** is correct because functional PCA is used to estimate the eigenbasis of the marginal covariance of the response.\n- **C** is correct because the basis coefficients are estimated by minimizing the penalized criterion with respect to $\\Theta_k$.\n- **D** is incorrect because the algorithm uses a principal component basis for $t$ in $F(x, s, t)$, not a spline basis.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Additive_Function-on-Function_Regression__Additive_Function-on-Function_Regression.md"], "background_text": "The prediction of response curves involves reconstructing the smooth trajectory of the response by smoothing the data for each $i$ and de-meaning it, using functional PCA to estimate the eigenbasis of the marginal covariance of the response, and obtaining estimates of the basis coefficients by minimizing the penalized criterion with respect to $\\Theta_k$. The algorithm uses a principal component basis for $t$ in $F(x, s, t)$, not a spline basis.", "folder": "", "qid": "statistic-mcqm-5116", "question": "In the prediction of response curves in the additive function-on-function regression model, which of the following steps are involved?", "options": ["A) Reconstruct the smooth trajectory of the response by smoothing the data for each $i$ and de-mean it.", "B) Use functional principal components analysis (PCA) to estimate the eigenbasis of the marginal covariance of the response.", "C) Obtain estimates of the basis coefficients by minimizing the penalized criterion with respect to $\\Theta_k$.", "D) Use a spline basis for $t$ in $F(x, s, t)$ to predict the response curves."], "category": "CORRECT", "question_context_used": "The prediction of response curves involves reconstructing the smooth trajectory of the response by smoothing the data for each $i$ and de-meaning it, using functional PCA to estimate the eigenbasis of the marginal covariance of the response, and obtaining estimates of the basis coefficients by minimizing the penalized criterion with respect to $\\Theta_k$. The algorithm uses a principal component basis for $t$ in $F(x, s, t)$, not a spline basis.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n- **A)** Correct because Condition 1 explicitly ensures that $p$-values are not recomputed as $I$ changes.\n- **B)** Incorrect because scaling based on data for treatments with indices in $I$ only would recompute $p$-values, violating Condition 1.\n- **C)** Incorrect because the ordering of critical values does not relate to the recomputation of $p$-values.\n- **D)** Correct if the $p$-values are independent of $I$, they would not need to be recomputed as $I$ changes, thus satisfying Condition 1.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Hochberg's_Step-Up_Method_Cutting_Corners_Off_Holm's_Step-Down_Method.md"], "background_text": "Condition 1 ensures that the $p$-values $p_{i}$, $i=1,\\ldots,k$, are not recomputed as $I$ changes. An example of the $p$-values being recomputed and thus violating Condition 1 is when scaling of the test statistics, by mean squared error for example, is done based on data for treatments with indices in $I$ only.", "folder": "", "qid": "statistic-mcqm-5919", "question": "Which of the following conditions ensure that the $p$-values are not recomputed as the index set $I$ changes?", "options": ["A) Condition 1", "B) The $p$-values are scaled based on data for treatments with indices in $I$ only.", "C) The critical values are ordered as $d_{1} \\geqslant d_{2} \\geqslant \\dotsb \\geqslant d_{k}$.", "D) The $p$-values are independent of the index set $I$."], "category": "CORRECT", "question_context_used": "Condition 1 ensures that the $p$-values $p_{i}$, $i=1,\\ldots,k$, are not recomputed as $I$ changes. An example of the $p$-values being recomputed and thus violating Condition 1 is when scaling of the test statistics, by mean squared error for example, is done based on data for treatments with indices in $I$ only.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **A)** Correct. This condition is explicitly stated to ensure consistency for normally distributed errors.\n- **B)** Correct. The errors being i.i.d. according to a symmetric distribution $F$ with center zero and scale $\\sigma_0$ is a general condition for the model.\n- **C)** Correct. The distribution $F$ is often assumed to be Gaussian, which is relevant for the consistency conditions.\n- **D)** Incorrect. The functions $\\rho_j$ are not linear; they are defined piecewise and include quadratic and higher-order terms.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Robust-tests-for-linear-regression-models-b_2016_C.md"], "background_text": "The parameters $b_1$ and $b_2$ are chosen such that $E_{\\Phi}(\\rho_j(u)) = b_j$, $j=1,2$, where $\\Phi$ denotes the standard normal distribution. These conditions ensure consistency of the scale estimators $\\hat{\\tau}_{n} = \\tau_{n}(\\hat{\\pmb{\\theta}}_{n})$ and $\\hat{\\sigma}_{n} = s_{n}(\\hat{\\pmb{\\theta}}_{n})$ for normally distributed errors.", "folder": "", "qid": "statistic-mcqm-5666", "question": "What are the conditions for the consistency of the scale estimators $\\hat{\\tau}_{n}$ and $\\hat{\\sigma}_{n}$ for normally distributed errors?", "options": ["A) The parameters $b_1$ and $b_2$ are chosen such that $E_{\\Phi}(\\rho_j(u)) = b_j$, $j=1,2$.", "B) The errors $\\epsilon_i$ are i.i.d. according to a symmetric distribution $F$ with center zero and scale $\\sigma_0$.", "C) The distribution $F$ is Gaussian.", "D) The functions $\\rho_j$ are linear."], "category": "PARTIALLY_CORRECT", "question_context_used": "The parameters $b_1$ and $b_2$ are chosen such that $E_{\\Phi}(\\rho_j(u)) = b_j$, $j=1,2$, where $\\Phi$ denotes the standard normal distribution. These conditions ensure consistency of the scale estimators $\\hat{\\tau}_{n} = \\tau_{n}(\\hat{\\pmb{\\theta}}_{n})$ and $\\hat{\\sigma}_{n} = s_{n}(\\hat{\\pmb{\\theta}}_{n})$ for normally distributed errors.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A\n\n**Explanation**:\n- **A)** Correct. The text explicitly states that $0 < p, \\gamma < 1$ are the conditions under which $F(x)$ is completely monotone.\n- **B)** Incorrect. The text does not support these conditions; in fact, they would likely not satisfy the derivative sign requirements.\n- **C)** Incorrect. When $p = 1$ and $\\gamma = 1$, $F(x) = 1$ for all $x$, which trivially is completely monotone, but this case is not covered in the provided text.\n- **D)** Incorrect. Negative values for $p$ and $\\gamma$ are not considered in the text and would not make sense in the context of $F(x) = p^{x^{\\gamma}}$ being real-valued for $x > 0$.\n\n**Final Answer**: The final answer is $\\boxed{{A}}$", "source": ["A Litter-Based Approach to Risk Assessment in Deve.md"], "background_text": "The completely monotone property of $F(x) = p^{x^{\\gamma}}$ is established under the conditions $0 < p, \\gamma < 1$. The derivatives of $F(x)$ and $G(x) = \\log{F(x)}$ are analyzed to show that $(-1)^{k}F^{(k)}(x) \\geqslant 0$ for all $k$ and $x > 0$.", "folder": "", "qid": "statistic-mcqm-5457", "question": "Which of the following conditions ensure that $F(x) = p^{x^{\\gamma}}$ is completely monotone?", "options": ["A) $0 < p < 1$ and $0 < \\gamma < 1$", "B) $p > 1$ and $\\gamma > 1$", "C) $p = 1$ and $\\gamma = 1$", "D) $p < 0$ and $\\gamma < 0$"], "category": "CORRECT", "question_context_used": "The completely monotone property of $F(x) = p^{x^{\\gamma}}$ is established under the conditions $0 < p, \\gamma < 1$. The derivatives of $F(x)$ and $G(x) = \\log{F(x)}$ are analyzed to show that $(-1)^{k}F^{(k)}(x) \\geqslant 0$ for all $k$ and $x > 0$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **A)** Correct. The full column rank condition for $E(\\overline{Z}_i \\overline{X}_i')$ is essential for identification, as stated in the context.\n- **B)** Incorrect. Homoskedasticity is not a requirement for the model to be identified; it relates to the efficiency of the estimator.\n- **C)** Correct. The matrix $\\overline{Z}$ must be full ranked to ensure that the instruments are valid and the model is identified.\n- **D)** Incorrect. The number of instruments $K_2$ must be at least as large as the number of endogenous regressors $n$ ($K_2 \\geq n$), but equality is not required.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Linear-instrumental-variables-model-avera_2014_Com.md"], "background_text": "The linear IV regression model requires that $E(\\overline{Z}_i \\overline{X}_i')$ is of full column rank for identification, ensuring that $\\beta$ is identified. The model also assumes $\\overline{Z} = [X, Z]$ is full ranked and $K_2 \\geq n$, but homoskedasticity is not a necessary condition for identification.", "folder": "", "qid": "statistic-mcqm-5719", "question": "In the linear IV regression model $y = Y\\beta + X\\gamma + u$, which of the following conditions must hold for the model to be identified?", "options": ["A) $E(\\overline{Z}_i \\overline{X}_i')$ is of full column rank for any choice of instruments such that $K_2 \\geq n$.", "B) The error term $u_i$ is homoskedastic.", "C) The matrix $\\overline{Z} = [X, Z]$ is full ranked.", "D) The number of instruments $K_2$ is equal to the number of endogenous regressors $n$."], "category": "CORRECT", "question_context_used": "The linear IV regression model requires that $E(\\overline{Z}_i \\overline{X}_i')$ is of full column rank for identification, ensuring that $\\beta$ is identified. The model also assumes $\\overline{Z} = [X, Z]$ is full ranked and $K_2 \\geq n$, but homoskedasticity is not a necessary condition for identification.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct, as the text explicitly states that the restricted and unrestricted sup-norm statistics are always equal.\n- **B)** Correct, the text mentions that $D_{m,n}^{*(01)}\\to\\lVert W_{0}^{+}(t)\\rVert$ under $H_{0}$.\n- **C)** Incorrect, the least concave majorant interpretation is indeed applicable to $\\|F_{m}^{*}-G_{n}^{*}\\|$ as discussed in the context.\n- **D)** Correct, the text states that under $H_{0}$, $\\|F_{m}^{*}-G_{n}^{*}\\|=\\|(F_{m}-G_{n})^{+}\\|$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Nonparametric_tests_for_and_against_likelihood_ratio_ordering_in_the_two-sample_problem.md"], "background_text": "The context discusses testing $H_{0}$ versus $H_{1}-H_{0}$ using a one-sided Kolmogorov–Smirnov distance, mentioning that the restricted and unrestricted sup-norm statistics are always equal and that $D_{m,n}^{*(01)}$ converges to $\\lVert W_{0}^{+}(t)\\rVert$ under $H_{0}$. It also states $\\|F_{m}^{*}-G_{n}^{*}\\|=\\|(F_{m}-G_{n})^{+}\\|$ under $H_{0}$.", "folder": "", "qid": "statistic-mcqm-5722", "question": "Given the context of testing $H_{0}$ versus $H_{1}-H_{0}$ using a one-sided Kolmogorov–Smirnov distance, which of the following statements are correct?", "options": ["A) The restricted and unrestricted sup-norm statistics are always equal.", "B) $D_{m,n}^{*(01)}$ converges to $\\lVert W_{0}^{+}(t)\\rVert$ under $H_{0}$.", "C) The least concave majorant interpretation is not applicable to $\\|F_{m}^{*}-G_{n}^{*}\\|$.", "D) $\\|F_{m}^{*}-G_{n}^{*}\\|=\\|(F_{m}-G_{n})^{+}\\|$ under $H_{0}$."], "category": "CORRECT", "question_context_used": "The context discusses testing $H_{0}$ versus $H_{1}-H_{0}$ using a one-sided Kolmogorov–Smirnov distance, mentioning that the restricted and unrestricted sup-norm statistics are always equal and that $D_{m,n}^{*(01)}$ converges to $\\lVert W_{0}^{+}(t)\\rVert$ under $H_{0}$. It also states $\\|F_{m}^{*}-G_{n}^{*}\\|=\\|(F_{m}-G_{n})^{+}\\|$ under $H_{0}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**: \n- **A)** Correct. The text states that if $Z(\\boldsymbol{\\mathbf{\\uptheta}})$ is stochastically increasing in $\\boldsymbol{\\mathbf{\\uptheta}}$, it is also Schur-convex.\n- **B)** Incorrect. The text does not explicitly state that stochastic decreasing implies Schur-concavity; it only discusses the increasing case.\n- **C)** Correct. The notation $\\mathbf{SI}.\\mathbf{SchurC}\\mathbf{X}(\\mathrm{st})$ is defined in the text as indicating stochastic increasing and Schur-convexity.\n- **D)** Correct. The text explains that stochastic Schur-convexity implies $Z(\\bullet)\\geqslant_{\\mathrm{st}}Z(\\bullet^{\\prime})$ when $\\boldsymbol{\\mathbf{\\uptheta}}\\geqslant_{\\mathrm{wm}}\\boldsymbol{\\mathbf{\\uptheta}}^{\\prime}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Parametric-Schur-Convexity-and-Arrangement-Monoton.md"], "background_text": "The text discusses the conditions under which $Z(\\boldsymbol{\\mathbf{\\uptheta}})$ is stochastically increasing (decreasing) and Schur-convex (concave). It introduces notations like $\\mathbf{SI}.\\mathbf{SchurC}\\mathbf{X}(\\mathrm{st})$ for stochastic increasing and Schur-convexity, and mentions that stochastic Schur-convexity implies certain stochastic order relations.", "folder": "", "qid": "statistic-mcqm-3163", "question": "Which of the following statements are true regarding the stochastic order and Schur-convexity of $Z(\\boldsymbol{\\mathbf{\\uptheta}})$?", "options": ["A) If $Z(\\boldsymbol{\\mathbf{\\uptheta}})$ is stochastically increasing in $\\boldsymbol{\\mathbf{\\uptheta}}$, then it is Schur-convex.", "B) If $Z(\\boldsymbol{\\mathbf{\\uptheta}})$ is stochastically decreasing in $\\boldsymbol{\\mathbf{\\uptheta}}$, then it is Schur-concave.", "C) The notation $\\{Z(\\boldsymbol{\\mathbf{\\theta}}),\\boldsymbol{\\mathbf{\\theta}}\\in\\mathcal{X}\\}\\in\\mathbf{SI}.\\mathbf{SchurC}\\mathbf{X}(\\mathrm{st})$ indicates stochastic increasing and Schur-convexity.", "D) Stochastic Schur-convexity implies that for any $\\boldsymbol{\\mathbf{\\uptheta}}\\geqslant_{\\mathrm{wm}}\\boldsymbol{\\mathbf{\\uptheta}}^{\\prime}$, $Z(\\bullet)\\geqslant_{\\mathrm{st}}Z(\\bullet^{\\prime})$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The text discusses the conditions under which $Z(\\boldsymbol{\\mathbf{\\uptheta}})$ is stochastically increasing (decreasing) and Schur-convex (concave). It introduces notations like $\\mathbf{SI}.\\mathbf{SchurC}\\mathbf{X}(\\mathrm{st})$ for stochastic increasing and Schur-convexity, and mentions that stochastic Schur-convexity implies certain stochastic order relations.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"C\", \"D\"]\n}", "model_answer": ["C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **A** is correct because the text states that both $J(r)$ and $\\bar{J}$ are greater than or equal to one for both methods.\n- **B** is correct because the text mentions that an answer will increase the odds of the respondent to actually have the sensitive voting preference.\n- **C** is correct because the text specifies that for uniform voting preferences, the pair method reveals more information about the true voting intention than the list method.\n- **D** is incorrect because the text does not support the claim that the list method always performs better than the pair method in all situations; in fact, the pair method reveals more information for uniform voting preferences.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["Scandinavian J Statistics - 2019 - Lagerås - How t.md"], "background_text": "Table 4 shows that both $J(r)$ and $\\bar{J}$ are greater than or equal to one for both methods. An answer increases the odds of the respondent to actually have the sensitive voting preference. For uniform voting preferences, the pair method reveals more information about the true voting intention than the list method.", "folder": "", "qid": "statistic-mcqm-5848", "question": "From Table 4, which of the following statements are correct about $J(r)$ and $\\bar{J}$?", "options": ["A) Both $J(r)$ and $\\bar{J}$ are greater than or equal to one for both methods.", "B) An answer will increase the odds of the respondent to actually have the sensitive voting preference.", "C) The pair method reveals more information about the true voting intention than the list method for uniform voting preferences.", "D) The list method always performs better than the pair method in all situations."], "category": "CORRECT", "question_context_used": "Table 4 shows that both $J(r)$ and $\\bar{J}$ are greater than or equal to one for both methods. An answer increases the odds of the respondent to actually have the sensitive voting preference. For uniform voting preferences, the pair method reveals more information about the true voting intention than the list method.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The equation is indeed solved for $s=u$ to determine the truncation point $x_{u}$.\n- **B)** Correct. The combined results of equations (1) and (3) provide the probability condition $\\mathbf{Pr}(x_{l}<x_{p}<x_{u})=1-\\alpha-\\gamma$.\n- **C)** Correct. The value of $u$ is calculated as $\\mathcal{N}-l$ for given $n$ and $p$.\n- **D)** Correct. $E_{2}$ is calculated under the same assumptions as $E_{1}$ and involves adjusting the selected group by adding individuals from a reserve group.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["An Application of Non-Parametric Statistics to Tru.md"], "background_text": "The alternative procedure involves solving an equation for $s=u$ to find the truncation point $x_{u}$, ensuring $\\mathbf{Pr}(x_{l}<x_{p}<x_{u})=1-\\alpha-\\gamma$. The value of $u$ is found by $u=\\mathcal{N}-l$ for given $n$ and $p$. Efficiency $E_{2}$ is calculated similarly to $E_{1}$ and involves adjusting the selected group by adding individuals from a reserve group.", "folder": "", "qid": "statistic-mcqm-3270", "question": "In the context of the alternative procedure for truncation, which of the following statements are correct?", "options": ["A) The equation $\\frac{1}{\\mathrm{B}(s,\\mathcal{N}-s)}\\int_{0}^{p}w^{s-1}(1-w)^{\\mathcal{N}-s-1}\\mathrm{d}w=1-\\gamma$ is solved for $s=u$ to find the truncation point $x_{u}$.", "B) The combined results of equations (1) and (3) ensure that $\\mathbf{Pr}(x_{l}<x_{p}<x_{u})=1-\\alpha-\\gamma$.", "C) The value of $u$ for a particular $n$ and $p$ is found by $u=\\mathcal{N}-l$ where $l$ is the value calculated for $n, 1-p$.", "D) The efficiency $E_{2}$ is calculated under the same assumptions as $E_{1}$ and involves adding individuals from the group where $x_{\\iota}<x<x_{\\iota}$ to achieve the required proportion $p$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The alternative procedure involves solving an equation for $s=u$ to find the truncation point $x_{u}$, ensuring $\\mathbf{Pr}(x_{l}<x_{p}<x_{u})=1-\\alpha-\\gamma$. The value of $u$ is found by $u=\\mathcal{N}-l$ for given $n$ and $p$. Efficiency $E_{2}$ is calculated similarly to $E_{1}$ and involves adjusting the selected group by adding individuals from a reserve group.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C\n\n**Explanation**: \n- **Option A** is incorrect because the k-means method has a correct classification rate of 0.85, which is not above 0.85.\n- **Option B** is correct because the Mixture model has a correct classification rate of 1, which is above 0.85.\n- **Option C** is correct because the Mixture model's rate is above 0.85, even though the k-means rate is exactly 0.85.\n- **Option D** is incorrect because the Mixture model's rate is above 0.85.\n\n**Final Answer**: The final answer is $\\boxed{{B, C}}$", "source": ["Estimating Finite Mixtures of Semi-Markov Chains A.md"], "background_text": "For not-well-separated components with $n=600$ and 10 transitions, the k-means method has a correct classification rate of 0.85 (0.03) and the Mixture model has a rate of 1 (<0.01).", "folder": "", "qid": "statistic-mcqm-5774", "question": "For not-well-separated components with $n=600$ and 10 transitions, which methods have a correct classification rate above 0.85?", "options": ["A) k-means", "B) Mixture model", "C) Both k-means and Mixture model", "D) Neither k-means nor Mixture model"], "category": "PARTIALLY_CORRECT", "question_context_used": "For not-well-separated components with $n=600$ and 10 transitions, the k-means method has a correct classification rate of 0.85 (0.03) and the Mixture model has a rate of 1 (<0.01).", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A)** Correct. In the linear setting, the least squares estimator $\\hat{\\theta}$ is indeed a linear function of the $\\hat{Y}_{i}^{*}$, as stated in the context.\n- **B)** Incorrect. The context explicitly mentions that in nonlinear regression models, there is generally no explicit solution for the equation defining $\\hat{\\theta}$.\n- **C)** Correct. The i.i.d. representations by Stute (1996) become a valuable tool when no explicit solution for $\\hat{\\theta}$ exists in nonlinear regression models.\n- **D)** Correct. The asymptotic normality of Leurgans’ estimator strongly relies on the particular shape of the regression function, as mentioned in the context.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Scandinavian J Statistics - 2008 - DELECROIX - Non.md"], "background_text": "In linear regression, the least squares estimator $\\hat{\\theta}$ is a linear function of the $\\hat{Y}_{i}^{*}$. In nonlinear regression, there is generally no explicit solution for $\\hat{\\theta}$, making Stute's i.i.d. representations of Kaplan–Meier integrals valuable. The asymptotic normality of Leurgans’ estimator depends on the regression function's shape.", "folder": "", "qid": "statistic-mcqm-5205", "question": "In the context of linear regression and Leurgans’ estimator, which of the following statements are true regarding the asymptotic normality and the role of the regression function?", "options": ["A) The least squares estimator $\\hat{\\theta}$ is a linear function of the $\\hat{Y}_{i}^{*}$ in the linear setting.", "B) In nonlinear regression models, there is always an explicit solution for the equation defining $\\hat{\\theta}$.", "C) The i.i.d. representations of the Kaplan–Meier integrals by Stute (1996) are useful when no explicit solution for $\\hat{\\theta}$ exists.", "D) The asymptotic normality of Leurgans’ estimator relies on the specific shape of the regression function."], "category": "CORRECT", "question_context_used": "In linear regression, the least squares estimator $\\hat{\\theta}$ is a linear function of the $\\hat{Y}_{i}^{*}$. In nonlinear regression, there is generally no explicit solution for $\\hat{\\theta}$, making Stute's i.i.d. representations of Kaplan–Meier integrals valuable. The asymptotic normality of Leurgans’ estimator depends on the regression function's shape.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. $R_{1}$ is indeed the sample correlation coefficient of $X_{i}$ and $Y_{i}$ for $0 \\leq \\hat{\\gamma} < 0.5$.\n- **B)** Correct. $R_{2}$ is the sample correlation coefficient of $Y_{i}^{*}$ and $X_{i}^{*}$ for $\\hat{\\gamma} \\geq 0.5$.\n- **C)** Correct. $R_{1}$ is defined when the second moment of $X$ is finite, which is true for $0 \\leq \\gamma < 0.5$.\n- **D)** Correct. $R_{2}$ is defined when the second moment of $X^{*}$ is finite, which is true for $\\gamma \\geq 0.5$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["A-bootstrap-goodness-of-fit-test-for-the-gener_200.md"], "background_text": "$R_{1}$ is the sample correlation coefficient of $X_{i}$ and $Y_{i}$ for $0 \\leq \\hat{\\gamma} < 0.5$, and $R_{2}$ is the sample correlation coefficient of $Y_{i}^{*}$ and $X_{i}^{*}$ for $\\hat{\\gamma} \\geq 0.5$. $R_{1}$ is defined when the second moment of $X$ is finite, and $R_{2}$ is defined when the second moment of $X^{*}$ is finite.", "folder": "", "qid": "statistic-mcqm-5262", "question": "Which of the following are correct about the sample correlation coefficients $R_{1}$ and $R_{2}$?", "options": ["A) $R_{1}$ is the sample correlation coefficient of $X_{i}$ and $Y_{i}$ for $0 \\leq \\hat{\\gamma} < 0.5$.", "B) $R_{2}$ is the sample correlation coefficient of $Y_{i}^{*}$ and $X_{i}^{*}$ for $\\hat{\\gamma} \\geq 0.5$.", "C) $R_{1}$ is defined when the second moment of $X$ is finite.", "D) $R_{2}$ is defined when the second moment of $X^{*}$ is finite."], "category": "CORRECT", "question_context_used": "$R_{1}$ is the sample correlation coefficient of $X_{i}$ and $Y_{i}$ for $0 \\leq \\hat{\\gamma} < 0.5$, and $R_{2}$ is the sample correlation coefficient of $Y_{i}^{*}$ and $X_{i}^{*}$ for $\\hat{\\gamma} \\geq 0.5$. $R_{1}$ is defined when the second moment of $X$ is finite, and $R_{2}$ is defined when the second moment of $X^{*}$ is finite.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **A) Correct**: The algorithm updates the centers using a weighted sum of uncensored observations, where weights compensate for censoring, as described in the context.\n- **B) Correct**: For censored observations, the algorithm estimates distances to cluster centers and assigns them to the nearest cluster based on these estimates.\n- **C) Incorrect**: The algorithm does require an iterative procedure to approximate the centers of clusters, similar to the standard $k$-means algorithm but modified for censored data.\n- **D) Incorrect**: The algorithm modifies the standard $k$-means approach to handle censored data, particularly in updating centers and assigning censored observations.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["Vector-quantization-and-clustering-in-the-prese_20.md"], "background_text": "The proposed algorithm generalizes the $k$-means algorithm to handle censored data by using weighted sums of uncensored observations to update cluster centers and estimating distances for censored observations to assign them to clusters. It involves an iterative procedure to approximate the centers and addresses the challenge of unobserved distances due to censoring.", "folder": "", "qid": "statistic-mcqm-5988", "question": "In the context of clustering with censored data, which of the following statements are true about the proposed algorithm?", "options": ["A) The algorithm uses a weighted sum of uncensored observations to update the centers of clusters.", "B) The algorithm assigns censored observations to clusters based on estimated distances to the centers.", "C) The algorithm does not require any iterative procedure to find the centers of clusters.", "D) The algorithm uses the standard $k$-means approach without any modifications for censored data."], "category": "CORRECT", "question_context_used": "The proposed algorithm generalizes the $k$-means algorithm to handle censored data by using weighted sums of uncensored observations to update cluster centers and estimating distances for censored observations to assign them to clusters. It involves an iterative procedure to approximate the centers and addresses the challenge of unobserved distances due to censoring.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The MLE $B$ can indeed be decomposed into $B = b - c$ where $b = X_1 - X_2 \\gamma$ and $c = X_2 C - X_2 \\gamma$.\n- **B)** Correct. The components $b$ and $c$ are independent, as stated in the context.\n- **C)** Correct. The conditional distribution of $B$ given $(X_2, S_{22})$ is $N(\\beta, (I + T_2) \\otimes \\Sigma_{11.2})$.\n- **D)** Correct. The risk matrix of the MLE $B$ is the sum of the risk matrices of $b$ and $c$.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$.", "source": ["Double-Shrinkage-Estimators-in-the-GMANOVA_1996_Jo.md"], "background_text": "The MLE $B$ in the GMANOVA model can be decomposed into $B = b - c$ where $b = X_1 - X_2 \\gamma$ and $c = X_2 C - X_2 \\gamma$. The components $b$ and $c$ are independent, and the conditional distribution of $B$ given $(X_2, S_{22})$ is $N(\\beta, (I + T_2) \\otimes \\Sigma_{11.2})$. The risk matrix of the MLE $B$ is the sum of the risk matrices of $b$ and $c$.", "folder": "", "qid": "statistic-mcqm-5299", "question": "In the context of the GMANOVA model, which of the following statements about the decomposition of the MLE $B$ are true?", "options": ["A) The MLE $B$ can be decomposed into $B = b - c$ where $b = X_1 - X_2 \\gamma$ and $c = X_2 C - X_2 \\gamma$.", "B) The components $b$ and $c$ are independent.", "C) The conditional distribution of $B$ given $(X_2, S_{22})$ is $N(\\beta, (I + T_2) \\otimes \\Sigma_{11.2})$.", "D) The risk matrix of the MLE $B$ is the sum of the risk matrices of $b$ and $c$."], "category": "CORRECT", "question_context_used": "The MLE $B$ in the GMANOVA model can be decomposed into $B = b - c$ where $b = X_1 - X_2 \\gamma$ and $c = X_2 C - X_2 \\gamma$. The components $b$ and $c$ are independent, and the conditional distribution of $B$ given $(X_2, S_{22})$ is $N(\\beta, (I + T_2) \\otimes \\Sigma_{11.2})$. The risk matrix of the MLE $B$ is the sum of the risk matrices of $b$ and $c$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, D\n\n- **Option B** is correct because the binary labels are explicitly assigned using the linear SVM decision rule as stated in the context.\n- **Option D** is correct because the number of causal variables $k$ influences the ground truth weight vector $\\pmb{w}_{0}$, which is used in the decision rule to assign labels.\n- **Option A** is incorrect because sampling from a standard multivariate distribution describes how $\\pmb{x}_{i}$ is generated, not how labels are assigned.\n- **Option C** is incorrect because labels are not assigned randomly but are determined by the decision rule involving $\\pmb{w}_{0}$.\n\n**Final Answer**: The final answer is $\\boxed{{B, D}}$", "source": ["Algorithms_for_Sparse_Support_Vector_Machines__Algorithms_for_Sparse_Support_Vector_Machines.md"], "background_text": "Binary labels are assigned via the linear SVM decision rule $y_{i}=\\pmb{x}_{i}^{\\top}\\pmb{w}_{0}$, where $\\pmb{x}_{i}$ is sampled from a standard multivariate distribution and $\\pmb{w}_{0}$ is the ground truth weight vector with $k$ randomized components.", "folder": "", "qid": "statistic-mcqm-3338", "question": "In the simulation study, how are the binary labels $y_i$ assigned?", "options": ["A) By sampling from a standard multivariate distribution", "B) Using the linear SVM decision rule $y_{i}=\\pmb{x}_{i}^{\\top}\\pmb{w}_{0}$", "C) By randomly assigning labels with equal probability", "D) Based on the number of causal variables $k$"], "category": "PARTIALLY_CORRECT", "question_context_used": "Binary labels are assigned via the linear SVM decision rule $y_{i}=\\pmb{x}_{i}^{\\top}\\pmb{w}_{0}$, where $\\pmb{x}_{i}$ is sampled from a standard multivariate distribution and $\\pmb{w}_{0}$ is the ground truth weight vector with $k$ randomized components.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"B\"]\n}", "model_answer": ["B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct, as continuity is an assumed property of $f(u,\\lambda)$.\n- **B)** Correct, under the null hypothesis, $f(u,\\lambda)$ belongs to a specific parametric class.\n- **D)** Correct, the spectral density is symmetric and $2\\pi$-periodic in $\\lambda$.\n- **C)** Incorrect, the spectral density depends on $u$ as it is time varying.", "source": ["Scandinavian J Statistics - 2012 - PREUSS - Testin.md"], "background_text": "The time varying spectral density $f(u,\\lambda)$ is continuous by assumption and belongs to the set $\\mathcal{F}_{L S}$ of all time varying spectral densities which satisfy certain technical conditions. Under the null hypothesis, it possesses a semiparametric structure, meaning it belongs locally to a specific parametric class.", "folder": "", "qid": "statistic-mcqm-3155", "question": "What are the properties of the time varying spectral density $f(u,\\lambda)$ under the null hypothesis?", "options": ["A) It is continuous.", "B) It belongs to a specific parametric class.", "C) It is independent of $u$.", "D) It is symmetric and $2\\pi$-periodic in $\\lambda$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The time varying spectral density $f(u,\\lambda)$ is continuous by assumption and belongs to the set $\\mathcal{F}_{L S}$ of all time varying spectral densities which satisfy certain technical conditions. Under the null hypothesis, it possesses a semiparametric structure, meaning it belongs locally to a specific parametric class.", "gold_answer_parsed": ["A", "B", "D"], "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The estimator $\\hat{\\boldsymbol{p}}_i$ is unbiased for $p_i$, as stated in Theorem 1.\n- **B)** Correct. The variance of $\\hat{\\boldsymbol{p}}_i$ is inversely proportional to the sample size $n$, as shown in the variance formula $\\mathrm{Var}[\\hat{p}_i] = \\frac{1}{n}\\left(\\frac{1+(N-3)p_i}{(N-2)} - p_i^2\\right)$.\n- **C)** Incorrect. The covariance between $\\hat{\\boldsymbol{p}}_i$ and $\\hat{\\boldsymbol{p}}_j$ is negative for $i \\neq j$, as indicated by the formula $\\mathrm{Cov}[\\hat{p}_i, \\hat{p}_j] = -\\frac{1}{n}\\left(\\frac{1-p_i-p_j}{(N-2)^2} + p_i p_j\\right)$.\n- **D)** Correct. The matrix $A$ is defined as $\\frac{1}{N-1}B^{\\prime}$ and is used in the derivation of the estimator $\\hat{\\boldsymbol{p}}_i$, as mentioned in the context.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$.", "source": ["Scandinavian J Statistics - 2019 - Lagerås - How t.md"], "background_text": "The estimator $\\hat{\\boldsymbol{p}}_i$ is given by $\\frac{N-1}{N-2}\\boldsymbol{b}_i^{\\prime}\\hat{\\boldsymbol{u}} - \\frac{1}{N-2}$, where $\\hat{\\boldsymbol{u}}$ is the ML estimator for $\\boldsymbol{u}$. The variance and covariance of $\\hat{\\boldsymbol{p}}_i$ are provided, showing that the estimator is unbiased and its variance decreases with increasing sample size. The matrix $A$ is used in the derivation of the estimator.", "folder": "", "qid": "statistic-mcqm-5781", "question": "Given a voting scenario with $N$ parties and a single sensitive party, the probability of receiving the answer $\\{i,j\\}$ is $u_{ij} = \\frac{1}{N-1}(p_i + p_j)$. Which of the following statements are correct about the estimator $\\hat{\\boldsymbol{p}}_i$?", "options": ["A) The estimator $\\hat{\\boldsymbol{p}}_i$ is unbiased for $p_i$.", "B) The variance of $\\hat{\\boldsymbol{p}}_i$ decreases as the sample size $n$ increases.", "C) The covariance between $\\hat{\\boldsymbol{p}}_i$ and $\\hat{\\boldsymbol{p}}_j$ is positive for $i \\neq j$.", "D) The estimator $\\hat{\\boldsymbol{p}}_i$ can be derived using the matrix $A$ defined as $\\frac{1}{N-1}B^{\\prime}$."], "category": "CORRECT", "question_context_used": "The estimator $\\hat{\\boldsymbol{p}}_i$ is given by $\\frac{N-1}{N-2}\\boldsymbol{b}_i^{\\prime}\\hat{\\boldsymbol{u}} - \\frac{1}{N-2}$, where $\\hat{\\boldsymbol{u}}$ is the ML estimator for $\\boldsymbol{u}$. The variance and covariance of $\\hat{\\boldsymbol{p}}_i$ are provided, showing that the estimator is unbiased and its variance decreases with increasing sample size. The matrix $A$ is used in the derivation of the estimator.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**: \n- **Option A** is correct because the joint likelihood ratio ordering holds for the normalized spacings.\n- **Option B** is correct because likelihood ratio ordering holds between successive normalized spacings.\n- **Option C** is incorrect because hazard rate ordering is conjectured but not proven in the general case.\n- **Option D** is correct because dispersive ordering holds for the normalized spacings.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Stochastic-Orders-for-Spacings-of-Heterogeneous-Ex.md"], "background_text": "For independent random variables with log-convex densities, the normalized spacings satisfy $D_{1:n} \\prec^{\\mathrm{lr}:j} \\cdots \\prec^{\\mathrm{lr}:j} D_{n:n}$ and $D_{i:n} \\prec^{\\mathrm{lr}} D_{i+1:n}$. Also, $D_{i:n}^{*} \\prec^{\\mathrm{disp}} D_{i:n}$ for $i=2,...,n$.", "folder": "", "qid": "statistic-mcqm-5863", "question": "For independent exponential random variables $X_{1}, X_{2}, ..., X_{n}$ with log-convex densities, which of the following statements about normalized spacings $D_{i:n}$ are true?", "options": ["A) $D_{1:n} \\prec^{\\mathrm{lr}:j} D_{2:n} \\prec^{\\mathrm{lr}:j} \\cdots \\prec^{\\mathrm{lr}:j} D_{n:n}$.", "B) $D_{i:n} \\prec^{\\mathrm{lr}} D_{i+1:n}$ for $i=1,...,n-1$.", "C) $D_{i:n} \\prec^{\\mathrm{hr}} D_{i+1:n}$ for $i=1,...,n-1$.", "D) $D_{i:n}^{*} \\prec^{\\mathrm{disp}} D_{i:n}$ for $i=2,...,n$."], "category": "CORRECT", "question_context_used": "For independent random variables with log-convex densities, the normalized spacings satisfy $D_{1:n} \\prec^{\\mathrm{lr}:j} \\cdots \\prec^{\\mathrm{lr}:j} D_{n:n}$ and $D_{i:n} \\prec^{\\mathrm{lr}} D_{i+1:n}$. Also, $D_{i:n}^{*} \\prec^{\\mathrm{disp}} D_{i:n}$ for $i=2,...,n$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. Assumption 4 explicitly states that $f(\\theta, x)$ is twice differentiable on $\\Theta$ for each $x \\in \\mathbb{R}^{d}$.\n- **B)** Correct. The assumption includes the condition on the difference in second-order derivatives being bounded by $r(\\|\\theta_{1}-\\theta_{2}\\|)\\Phi(X)$.\n- **C)** Correct. Assumption 4 (ii) imposes a uniform bound on the norms of the first and second-order derivatives.\n- **D)** Incorrect. The assumption does not require $f(\\theta, x)$ to be linear in $\\theta$; it applies to more general functions.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Scandinavian J Statistics - 2008 - DELECROIX - Non.md"], "background_text": "Assumption 4 states that for each $x \\in \\mathbb{R}^{d}$, $f(\\theta, x)$ is twice differentiable on $\\Theta$. It also requires the existence of a function $r(\\cdot)$ with $r(0)=0$ and a measurable function $\\Phi(X)$ with $E[\\Phi^{2}(X)] < \\infty$ to bound the difference in second-order derivatives. Additionally, it imposes a uniform bound on the norms of the first and second-order derivatives.", "folder": "", "qid": "statistic-mcqm-5183", "question": "Which of the following conditions are implied by Assumption 4 in the context of the regression function $f(\\theta, x)$?", "options": ["A) $f(\\theta, x)$ is twice differentiable on $\\Theta$ for each $x \\in \\mathbb{R}^{d}$.", "B) There exists a continuous real-valued function $r(\\cdot)$ with $r(0)=0$ and a measurable function $\\Phi(X)$ such that $\\forall \\theta_{1}, \\theta_{2} \\in \\Theta$, $\\|\\nabla_{\\theta}^{2}f(\\theta_{1},x)-\\nabla_{\\theta}^{2}f(\\theta_{2},x)\\| \\le r(\\|\\theta_{1}-\\theta_{2}\\|)\\Phi(X)$.", "C) $\\sup_{\\theta \\in \\Theta, x \\in \\mathbb{R}^{d}} \\{ \\|\\nabla_{\\theta}f(\\theta,x)\\| + \\|\\nabla_{\\theta}^{2}f(\\theta,x)\\| \\} \\leq C < \\infty$ for some constant $C$.", "D) The function $f(\\theta, x)$ must be linear in $\\theta$."], "category": "CORRECT", "question_context_used": "Assumption 4 states that for each $x \\in \\mathbb{R}^{d}$, $f(\\theta, x)$ is twice differentiable on $\\Theta$. It also requires the existence of a function $r(\\cdot)$ with $r(0)=0$ and a measurable function $\\Phi(X)$ with $E[\\Phi^{2}(X)] < \\infty$ to bound the difference in second-order derivatives. Additionally, it imposes a uniform bound on the norms of the first and second-order derivatives.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. Exponential distributions are mentioned as identifiable in the context of finite mixtures.\n- **B)** Correct. Gamma distributions are also mentioned as identifiable.\n- **C)** Incorrect. Normal distributions are not mentioned in the text regarding identifiability.\n- **D)** Correct. Negative binomial distributions are mentioned as identifiable.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Estimating Finite Mixtures of Semi-Markov Chains A.md"], "background_text": "The text mentions that finite mixtures of gamma distributions, exponential distributions, and negative binomial distributions are identifiable. These distributions are parametric and have been proven to satisfy the conditions for identifiability in the context of finite mixtures.", "folder": "", "qid": "statistic-mcqm-5769", "question": "Which of the following distributions for sojourn times are mentioned as identifiable in the context of finite mixtures?", "options": ["A) Exponential distributions", "B) Gamma distributions", "C) Normal distributions", "D) Negative binomial distributions"], "category": "CORRECT", "question_context_used": "The text mentions that finite mixtures of gamma distributions, exponential distributions, and negative binomial distributions are identifiable. These distributions are parametric and have been proven to satisfy the conditions for identifiability in the context of finite mixtures.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C\n\n**Explanation**: \n- **Option A** is incorrect because the k-means method has a correct classification rate of 0.85, which is not above 0.85.\n- **Option B** is correct because the Mixture model has a correct classification rate of 1, which is above 0.85.\n- **Option C** is correct because the Mixture model's rate is above 0.85, even though the k-means rate is exactly 0.85.\n- **Option D** is incorrect because the Mixture model's rate is above 0.85.\n\n**Final Answer**: The final answer is $\\boxed{{B, C}}$", "source": ["Estimating Finite Mixtures of Semi-Markov Chains A.md"], "background_text": "For not-well-separated components with $n=600$ and 10 transitions, the k-means method has a correct classification rate of 0.85 (0.03) and the Mixture model has a rate of 1 (<0.01).", "folder": "", "qid": "statistic-mcqm-5765", "question": "For not-well-separated components with $n=600$ and 10 transitions, which methods have a correct classification rate above 0.85?", "options": ["A) k-means", "B) Mixture model", "C) Both k-means and Mixture model", "D) Neither k-means nor Mixture model"], "category": "PARTIALLY_CORRECT", "question_context_used": "For not-well-separated components with $n=600$ and 10 transitions, the k-means method has a correct classification rate of 0.85 (0.03) and the Mixture model has a rate of 1 (<0.01).", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**:\n- **Option A** is correct as it directly states the asymptotic distribution of $n_{i}^{1/2}\\operatorname{vec}(A_{i})$ as given in the context.\n- **Option B** is incorrect because the context does not state that $\\Phi_{i}$ is idempotent; it mentions that $\\Gamma$ is idempotent later in the text.\n- **Option C** is correct as it defines $\\operatorname{vec}(A_{i*})$ using $\\Phi_{i}^{1/2+}$ as given in the context.\n- **Option D** is correct as it states the asymptotic distribution of $n_{i}^{1/2}\\mathrm{vec}(A_{i*})$ as given in the context.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Weighted_chi‐squared_tests_for_partial_common_principal_component_subspaces.md"], "background_text": "The asymptotic distribution of $n_{i}^{1/2}\\operatorname{vec}(A_{i})$ is normal with mean vector 0 and covariance matrix $\\Phi_{i}=(I_{m^{2}}+K_{m m})(\\Omega_{i}\\otimes\\Omega_{i})$. The transformed vector $\\operatorname{vec}(A_{i*})$ is defined using $\\Phi_{i}^{1/2+}$, and its asymptotic distribution is also normal with a specified covariance matrix.", "folder": "", "qid": "statistic-mcqm-5690", "question": "Which of the following statements about the asymptotic distribution of $n_{i}^{1/2}\\operatorname{vec}(A_{i})$ are correct?", "options": ["A) It is normal with mean vector 0 and covariance matrix $\\Phi_{i}=(I_{m^{2}}+K_{m m})(\\Omega_{i}\\otimes\\Omega_{i})$", "B) The covariance matrix $\\Phi_{i}$ is idempotent", "C) $\\operatorname{vec}(A_{i*})=\\Phi_{i}^{1/2+}\\operatorname{vec}(A_{i})$ where $\\Phi_{i}^{1/2+}$ satisfies $\\Phi_{i}^{1/2+\\prime}\\Phi_{i}^{1/2+}=\\Phi_{i}^{+}$", "D) The asymptotic distribution of $n_{i}^{1/2}\\mathrm{vec}(A_{i*})$ is normal with mean vector 0 and covariance matrix $\\frac{1}{2}(I_{m^{2}}+K_{m m})$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The asymptotic distribution of $n_{i}^{1/2}\\operatorname{vec}(A_{i})$ is normal with mean vector 0 and covariance matrix $\\Phi_{i}=(I_{m^{2}}+K_{m m})(\\Omega_{i}\\otimes\\Omega_{i})$. The transformed vector $\\operatorname{vec}(A_{i*})$ is defined using $\\Phi_{i}^{1/2+}$, and its asymptotic distribution is also normal with a specified covariance matrix.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A) The number of distinct design points increases.** This is correct because as $\rho$ decreases, the design shifts from minimizing variance to minimizing bias, leading to an increase in the number of distinct design points to account for the bias.\n- **B) The designs become more clustered around the variance-minimizing design points.** This is correct because the text mentions that designs are clusters of observations about the points that would have been the design points for the variance-minimizing design.\n- **D) The number of support points increases with increasing bias in the model.** This is correct because the text explicitly states that the number of support points increases with increasing bias in the model.\n\n**Incorrect Choices**:\n- **C) The spread of the clusters decreases.** This is incorrect because the text states that the spread of the clusters increases with increasing bias, not decreases.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["New-criteria-for-robust-integer-valued-desi_2006_C.md"], "background_text": "The full second-order model is considered on $\\mathcal{S}=\\left[-\\frac{1}{2},\\frac{1}{2}\\right]\\times\\left[-\\frac{1}{2},\\frac{1}{2}\\right]$ with $\\mathbf{x}=(x_{1},x_{2})^{\\mathrm{T}}$ and $z(\\mathbf{x})=\\left(1,x_{1},x_{2},x_{1}x_{2},x_{1}^{2},x_{2}^{2}\\right)^{\\mathrm{T}}$. The model is exchangeable in $x_{1}$ and $x_{2}$. Designs corresponding to $\\rho=1$ (minimizing variance only), 0.75, 0.5, and 0 (minimizing bias only) were obtained. The number of distinct design points for each value of $\\rho$ is greater than that for the corresponding design for partial second-order model.", "folder": "", "qid": "statistic-mcqm-2242", "question": "For the full second-order model with $q=2$, which of the following statements are true regarding the design points as $\rho$ decreases from 1 to 0?", "options": ["A) The number of distinct design points increases.", "B) The designs become more clustered around the variance-minimizing design points.", "C) The spread of the clusters decreases.", "D) The number of support points increases with increasing bias in the model."], "category": "PARTIALLY_CORRECT", "question_context_used": "The full second-order model is considered on $\\mathcal{S}=\\left[-\\frac{1}{2},\\frac{1}{2}\\right]\\times\\left[-\\frac{1}{2},\\frac{1}{2}\\right]$ with $\\mathbf{x}=(x_{1},x_{2})^{\\mathrm{T}}$ and $z(\\mathbf{x})=\\left(1,x_{1},x_{2},x_{1}x_{2},x_{1}^{2},x_{2}^{2}\\right)^{\\mathrm{T}}$. The model is exchangeable in $x_{1}$ and $x_{2}$. Designs corresponding to $\\rho=1$ (minimizing variance only), 0.75, 0.5, and 0 (minimizing bias only) were obtained. The number of distinct design points for each value of $\\rho$ is greater than that for the corresponding design for partial second-order model.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. This is the definition of the Cameron-Martin space $H_{0}^{1}$.\n- **B)** Correct. The norm on $H_{0}^{1}$ is indeed induced by the $L^{2}$ norm of the derivative.\n- **C)** Correct. $H_{0}^{1}$ is continuously included in the space of $1/2$-Hölder continuous functions.\n- **D)** Incorrect. The paths of the Brownian motion are not $H_{0}^{1}$-valued.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Functional-Cram-r-Rao-bounds-and-Stein-estimators-.md"], "background_text": "The Cameron-Martin space $H_{0}^{1}$ is defined as the space of continuous functions of the form $h(t)=\\int_{0}^{t}\\dot{h}(s)ds$ with $\\dot{h}\\in L^{2}(0,T)$. It is a Hilbert space endowed with the norm $\\|h\\|_{H_{0}^{1}}=\\|\\dot{h}\\|_{L^{2}(0,T)}$. $H_{0}^{1}$ is continuously included in the space of $1/2$-Hölder continuous functions, but the paths of the Brownian motion are not $H_{0}^{1}$-valued.", "folder": "", "qid": "statistic-mcqm-6164", "question": "Which of the following statements about the Cameron-Martin space $H_{0}^{1}$ are correct?", "options": ["A) $H_{0}^{1}$ is defined as the space of continuous functions of the form $h(t)=\\int_{0}^{t}\\dot{h}(s)ds$ with $\\dot{h}\\in L^{2}(0,T)$.", "B) $H_{0}^{1}$ is a Hilbert space endowed with the norm $\\|h\\|_{H_{0}^{1}}=\\|\\dot{h}\\|_{L^{2}(0,T)}$.", "C) $H_{0}^{1}$ is continuously included in the space of $1/2$-Hölder continuous functions.", "D) The paths of the Brownian motion are $H_{0}^{1}$-valued."], "category": "CORRECT", "question_context_used": "The Cameron-Martin space $H_{0}^{1}$ is defined as the space of continuous functions of the form $h(t)=\\int_{0}^{t}\\dot{h}(s)ds$ with $\\dot{h}\\in L^{2}(0,T)$. It is a Hilbert space endowed with the norm $\\|h\\|_{H_{0}^{1}}=\\|\\dot{h}\\|_{L^{2}(0,T)}$. $H_{0}^{1}$ is continuously included in the space of $1/2$-Hölder continuous functions, but the paths of the Brownian motion are not $H_{0}^{1}$-valued.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**: \n- **Option A** is correct as both conditions are necessary for the approximation to hold.\n- **Option C** is correct because even if $N$ is not tending to infinity, the approximation can be valid if $\\psi$ is close to 1.\n- **Option B** is incorrect because $\\psi \\to 1$ is also required.\n- **Option D** is incorrect as the approximation is not valid for arbitrary finite $N$ and $\\psi$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Mean_and_variance_of_an_entry_in_a_contingency_table.md"], "background_text": "Patnaik's derivation for the mean of $a$ in a non-independent $2\\times2$ table is valid under the conditions that $N \\to \\infty$ and $\\psi \\to 1$. This is because the term $\\sigma^2 \\log \\psi$ in the mean expression must remain finite.", "folder": "", "qid": "statistic-mcqm-3422", "question": "Patnaik's approximation for the mean of $a$ in a non-independent $2\\times2$ table is valid under which conditions?", "options": ["A) $N \\to \\infty$ and $\\psi \\to 1$", "B) $N \\to \\infty$ only", "C) $\\psi \\to 1$ only", "D) For any finite $N$ and $\\psi$"], "category": "PARTIALLY_CORRECT", "question_context_used": "Patnaik's derivation for the mean of $a$ in a non-independent $2\\times2$ table is valid under the conditions that $N \\to \\infty$ and $\\psi \\to 1$. This is because the term $\\sigma^2 \\log \\psi$ in the mean expression must remain finite.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A) Correct**: $\\mathrm{NO}$ is listed as a significant covariate in the rate parameter.\n- **B) Correct**: Temperature is listed as a significant covariate in the rate parameter.\n- **C) Correct**: $\\mathrm{Sun}$ is listed as a significant covariate in the rate parameter.\n- **D) Incorrect**: The level of $\\mathrm{NO}_{2}$ is not significant in the models fitted by the standard approach.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Modelling Non-Stationary Extremes with Application.md"], "background_text": "For the standard method under Model 1, significant covariates in the rate parameter include $\\mathrm{NO}$, Temperature, $\\mathrm{Sun}$, and an indicator for spring. The level of $\\mathrm{NO}_{2}$ is not significant in either of the models fitted by using the standard approach.", "folder": "", "qid": "statistic-mcqm-5275", "question": "For the standard method under Model 1, which covariates are significant in the rate parameter?", "options": ["A) $\\mathrm{NO}$", "B) Temperature", "C) $\\mathrm{Sun}$", "D) $\\mathrm{NO}_{2}$"], "category": "CORRECT", "question_context_used": "For the standard method under Model 1, significant covariates in the rate parameter include $\\mathrm{NO}$, Temperature, $\\mathrm{Sun}$, and an indicator for spring. The level of $\\mathrm{NO}_{2}$ is not significant in either of the models fitted by using the standard approach.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n**Explanation**:\n- **A) Correct**: The expected Fisher information matrix is symmetric, as seen from the structure of its inverse.\n- **B) Incorrect**: The diagonal elements are not necessarily equal; they depend on the values of $x_{0}$, $x_{1}$, $\\varDelta$, and $\\lambda$.\n- **C) Incorrect**: The off-diagonal elements are not zero; they are given by $-\\frac{(\\lambda-\\Delta)^{2}}{x_{0}}+\\frac{(\\lambda+\\Delta)^{2}}{x_{1}}$.\n- **D) Correct**: The inverse of the expected Fisher information matrix is explicitly given in the context with the provided formula.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Asymptotic-confidence-interval-construction-for-_2.md"], "background_text": "The expected Fisher information matrix $I(\\varDelta,\\lambda)$ is derived from the second partial derivatives of the log-likelihood function. Its inverse is provided in the context, showing a specific symmetric structure.", "folder": "", "qid": "statistic-mcqm-5682", "question": "Considering the expected Fisher information matrix $I(\\varDelta,\\lambda)$, which of the following properties are true?", "options": ["A) The expected Fisher information matrix is symmetric.", "B) The diagonal elements of the expected Fisher information matrix are equal.", "C) The off-diagonal elements of the expected Fisher information matrix are zero.", "D) The inverse of the expected Fisher information matrix is given by $I(\\varDelta,\\lambda)^{-1}=\\left(\\begin{array}{c c}{\\displaystyle\\frac{(\\lambda-\\Delta)^{2}}{x_{0}}+\\frac{(\\lambda+\\Delta)^{2}}{x_{1}}}&{\\displaystyle-\\frac{(\\lambda-\\Delta)^{2}}{x_{0}}+\\frac{(\\lambda+\\Delta)^{2}}{x_{1}}}\\\\{\\displaystyle-\\frac{(\\lambda-\\Delta)^{2}}{x_{0}}+\\frac{(\\lambda+\\Delta)^{2}}{x_{1}}}&{\\displaystyle\\frac{(\\lambda-\\Delta)^{2}}{x_{0}}+\\frac{(\\lambda+\\Delta)^{2}}{x_{1}}}\\end{array}\\right)\\cdot(2-\\Delta-\\lambda)/8$."], "category": "CORRECT", "question_context_used": "The expected Fisher information matrix $I(\\varDelta,\\lambda)$ is derived from the second partial derivatives of the log-likelihood function. Its inverse is provided in the context, showing a specific symmetric structure.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The equation is indeed solved for $s=u$ to determine the truncation point $x_{u}$.\n- **B)** Correct. The combined results of equations (1) and (3) provide the probability condition $\\mathbf{Pr}(x_{l}<x_{p}<x_{u})=1-\\alpha-\\gamma$.\n- **C)** Correct. The value of $u$ is calculated as $\\mathcal{N}-l$ for given $n$ and $p$.\n- **D)** Correct. $E_{2}$ is calculated under the same assumptions as $E_{1}$ and involves adjusting the selected group by adding individuals from a reserve group.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["An Application of Non-Parametric Statistics to Tru.md"], "background_text": "The alternative procedure involves solving an equation for $s=u$ to find the truncation point $x_{u}$, ensuring $\\mathbf{Pr}(x_{l}<x_{p}<x_{u})=1-\\alpha-\\gamma$. The value of $u$ is found by $u=\\mathcal{N}-l$ for given $n$ and $p$. Efficiency $E_{2}$ is calculated similarly to $E_{1}$ and involves adjusting the selected group by adding individuals from a reserve group.", "folder": "", "qid": "statistic-mcqm-3267", "question": "In the context of the alternative procedure for truncation, which of the following statements are correct?", "options": ["A) The equation $\\frac{1}{\\mathrm{B}(s,\\mathcal{N}-s)}\\int_{0}^{p}w^{s-1}(1-w)^{\\mathcal{N}-s-1}\\mathrm{d}w=1-\\gamma$ is solved for $s=u$ to find the truncation point $x_{u}$.", "B) The combined results of equations (1) and (3) ensure that $\\mathbf{Pr}(x_{l}<x_{p}<x_{u})=1-\\alpha-\\gamma$.", "C) The value of $u$ for a particular $n$ and $p$ is found by $u=\\mathcal{N}-l$ where $l$ is the value calculated for $n, 1-p$.", "D) The efficiency $E_{2}$ is calculated under the same assumptions as $E_{1}$ and involves adding individuals from the group where $x_{\\iota}<x<x_{\\iota}$ to achieve the required proportion $p$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The alternative procedure involves solving an equation for $s=u$ to find the truncation point $x_{u}$, ensuring $\\mathbf{Pr}(x_{l}<x_{p}<x_{u})=1-\\alpha-\\gamma$. The value of $u$ is found by $u=\\mathcal{N}-l$ for given $n$ and $p$. Efficiency $E_{2}$ is calculated similarly to $E_{1}$ and involves adjusting the selected group by adding individuals from a reserve group.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**:\n- **Option A** is correct because the model is designed to account for correlation between an individual’s successive choices, mirroring the structure of a discrete choice experiment.\n- **Option C** is correct as the model assumes a random-effects distribution over the partworths $\\beta_{n}$, which is central to its formulation.\n- **Option D** is correct because it accurately represents the likelihood function for a respondent's series of choices as described in the context.\n- **Option B** is incorrect because the partworths $\\beta_{n}$ are assumed constant over all choice sets for each individual, not varying.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Improving-the-efficiency-of-individualized-designs.md"], "background_text": "The mixed logit choice model accounts for preference heterogeneity and assumes partworths $\\beta_{n}$ are constant over all choice sets for each individual. It accounts for correlation between an individual’s successive choices and assumes a random-effects distribution over the partworths. The likelihood of a respondent's choices is given by a specific product of probabilities.", "folder": "", "qid": "statistic-mcqm-4863", "question": "Which of the following statements are true about the mixed logit choice model?", "options": ["A) The model accounts for correlation between an individual’s successive choices.", "B) The partworths $\\beta_{n}$ are assumed to vary over choice sets for each individual.", "C) The model assumes a random-effects distribution over the partworths $\\beta_{n}$.", "D) The likelihood of respondent n’s series of choices is given by $L(\\pmb{\\beta}_{n}|\\mathbf{y}_{n}^{\\cal S},\\pmb{X}_{n}^{\\cal S})=\\prod_{s=1}^{S}\\prod_{k=1}^{K}(p_{k s n}(\\pmb{\\beta}_{n}))^{y_{k s n}}$."], "category": "CORRECT", "question_context_used": "The mixed logit choice model accounts for preference heterogeneity and assumes partworths $\\beta_{n}$ are constant over all choice sets for each individual. It accounts for correlation between an individual’s successive choices and assumes a random-effects distribution over the partworths. The likelihood of a respondent's choices is given by a specific product of probabilities.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **A)** Correct. According to the theorem, $\\hat{\\theta}_{m}^{*}$ is consistent for $\\theta \\in \\Theta \\setminus \\Theta_{0}$.\n- **B)** Incorrect. The theorem states that $\\hat{\\theta}_{m}^{*}$ is inconsistent for $\\theta \\in \\Theta_{0}$.\n- **C)** Correct. The text discusses Edgeworth expansions for approximating the distribution of $\\hat{\\theta}_{m}^{*}$.\n- **D)** Incorrect. The distribution of $\\hat{\\theta}_{m}^{*}$ depends on $\\theta$'s location and is not always normal.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Bootstrapping-in-non-regular-smooth-function_2012_.md"], "background_text": "The $m$-out-of-$n$ bootstrap resample involves drawing a sample of size $m$ with replacement from $X_{1:n}$, where $m=o(n)$ and $m\\rightarrow\\infty$. The bootstrap version of the estimator is $\\hat{\\theta}_{m}^{*} = f(\\bar{X}_{m}^{*})$. The consistency and distributional properties of $\\hat{\\theta}_{m}^{*}$ depend on whether $\\theta$ is in $\\Theta_{0}$ or $\\Theta \\setminus \\Theta_{0}$.", "folder": "", "qid": "statistic-mcqm-5390", "question": "Given the $m$-out-of-$n$ bootstrap resample $X_{1:m}^{*}$ from $X_{1:n}$, with $m=o(n)$ and $m\\rightarrow\\infty$, which of the following statements about the bootstrap version $\\hat{\\theta}_{m}^{*} = f(\\bar{X}_{m}^{*})$ are correct?", "options": ["A) $\\hat{\\theta}_{m}^{*}$ is a consistent estimator for $\\theta$ when $\\theta \\in \\Theta \\setminus \\Theta_{0}$.", "B) $\\hat{\\theta}_{m}^{*}$ is a consistent estimator for $\\theta$ when $\\theta \\in \\Theta_{0}$.", "C) The distribution of $\\hat{\\theta}_{m}^{*}$ can be approximated using Edgeworth expansions.", "D) $\\hat{\\theta}_{m}^{*}$ is always normally distributed regardless of $\\theta$'s location in $\\Theta$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The $m$-out-of-$n$ bootstrap resample involves drawing a sample of size $m$ with replacement from $X_{1:n}$, where $m=o(n)$ and $m\\rightarrow\\infty$. The bootstrap version of the estimator is $\\hat{\\theta}_{m}^{*} = f(\\bar{X}_{m}^{*})$. The consistency and distributional properties of $\\hat{\\theta}_{m}^{*}$ depend on whether $\\theta$ is in $\\Theta_{0}$ or $\\Theta \\setminus \\Theta_{0}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A)** Correct. The text states that if $c > 0$ almost surely on $(0,1)^{2d}$ and $\\phi_C^2 < \\infty$, then $\\rho_C < 1$.\n- **B)** Correct. The text mentions that if $c(u,v) \\geq \\epsilon q(u)q(v)$ almost surely for some $\\epsilon > 0$, then $\\rho_C < 1$.\n- **C)** Correct. For Gaussian copulas, $\\phi_C^2 < \\infty$ if and only if $A - 4BMB^\\top$ is positive definite, which ensures $\\rho_C < 1$.\n- **D)** Incorrect. The text does not directly state that $\\rho_C < 1$ simply because the Markov chain is stationary and $\\rho_1 < 1$. The conditions are more specific.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Copula-based-semiparametric-models-for-multivar_20.md"], "background_text": "The condition $\\rho_C < 1$ for a Markov chain $U$ with $(U_{n-1}, U_n) \\sim C$ can be ensured under several sufficient conditions. These include the density $c$ of $C$ being positive almost surely on $(0,1)^{2d}$ with $\\phi_C^2 < \\infty$, or $c(u,v) \\geq \\epsilon q(u)q(v)$ almost surely for some $\\epsilon > 0$. For Gaussian copulas, $\\phi_C^2 < \\infty$ if and only if $A - 4BMB^\\top$ is positive definite. Additionally, if the Markov chain is stationary and $\\rho_1 < 1$, then $\\rho_C < 1$.", "folder": "", "qid": "statistic-mcqm-4302", "question": "Under what conditions does $\\rho_C < 1$ hold for a Markov chain $U$ with $(U_{n-1}, U_n) \\sim C$?", "options": ["A) The density $c$ of $C$ satisfies $c > 0$ almost surely on $(0,1)^{2d}$ and $\\phi_C^2 < \\infty$.", "B) The density $c$ of $C$ satisfies $c(u,v) \\geq \\epsilon q(u)q(v)$ almost surely for some $\\epsilon > 0$.", "C) The Markov chain is Gaussian and $A - 4BMB^\\top$ is positive definite.", "D) The Markov chain is stationary and $\\rho_1 < 1$."], "category": "INCORRECT", "question_context_used": "The condition $\\rho_C < 1$ for a Markov chain $U$ with $(U_{n-1}, U_n) \\sim C$ can be ensured under several sufficient conditions. These include the density $c$ of $C$ being positive almost surely on $(0,1)^{2d}$ with $\\phi_C^2 < \\infty$, or $c(u,v) \\geq \\epsilon q(u)q(v)$ almost surely for some $\\epsilon > 0$. For Gaussian copulas, $\\phi_C^2 < \\infty$ if and only if $A - 4BMB^\\top$ is positive definite. Additionally, if the Markov chain is stationary and $\\rho_1 < 1$, then $\\rho_C < 1$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The EM algorithm decomposes the optimization procedure into two simple steps: expectation and maximization, making it easier to handle.\n- **B) Correct**: The complete-data log-likelihood is easier to maximize because it considers the unknown mixture components as missing observations.\n- **C) Incorrect**: The initial value of the parameters $\\pmb{\\theta}^{(0)}$ can greatly affect the convergence of the EM algorithm, making careful selection important.\n- **D) Correct**: The EM algorithm requires the number of mixture components $G$ to be known for the estimation process.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Estimating Finite Mixtures of Semi-Markov Chains A.md"], "background_text": "The EM algorithm is an iterative optimization technique of the likelihood that can be very effective for estimating mixture models by considering the unknown mixture components as missing observations. The complete-data log-likelihood is easier to maximize than the observed-data log-likelihood. The choice of the starting point can be of great importance for the EM algorithm. The number of mixture components $G$ is supposed to be known in this section.", "folder": "", "qid": "statistic-mcqm-5771", "question": "Which of the following statements are true regarding the EM algorithm for estimating the parameters of a mixture of semi-Markov processes?", "options": ["A) The EM algorithm is preferred because it decomposes the optimization procedure into two simple steps: expectation and maximization", "B) The complete-data log-likelihood is easier to maximize than the observed-data log-likelihood", "C) The initial value of the parameters $\\pmb{\\theta}^{(0)}$ does not affect the convergence of the EM algorithm", "D) The EM algorithm requires the number of mixture components $G$ to be known"], "category": "CORRECT", "question_context_used": "The EM algorithm is an iterative optimization technique of the likelihood that can be very effective for estimating mixture models by considering the unknown mixture components as missing observations. The complete-data log-likelihood is easier to maximize than the observed-data log-likelihood. The choice of the starting point can be of great importance for the EM algorithm. The number of mixture components $G$ is supposed to be known in this section.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A)** Correct. The text states that if $c > 0$ almost surely on $(0,1)^{2d}$ and $\\phi_C^2 < \\infty$, then $\\rho_C < 1$.\n- **B)** Correct. The text mentions that if $c(u,v) \\geq \\epsilon q(u)q(v)$ almost surely for some $\\epsilon > 0$, then $\\rho_C < 1$.\n- **C)** Correct. For Gaussian copulas, $\\phi_C^2 < \\infty$ if and only if $A - 4BMB^\\top$ is positive definite, which ensures $\\rho_C < 1$.\n- **D)** Incorrect. The text does not directly state that $\\rho_C < 1$ simply because the Markov chain is stationary and $\\rho_1 < 1$. The conditions are more specific.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Copula-based-semiparametric-models-for-multivar_20.md"], "background_text": "The condition $\\rho_C < 1$ for a Markov chain $U$ with $(U_{n-1}, U_n) \\sim C$ can be ensured under several sufficient conditions. These include the density $c$ of $C$ being positive almost surely on $(0,1)^{2d}$ with $\\phi_C^2 < \\infty$, or $c(u,v) \\geq \\epsilon q(u)q(v)$ almost surely for some $\\epsilon > 0$. For Gaussian copulas, $\\phi_C^2 < \\infty$ if and only if $A - 4BMB^\\top$ is positive definite. Additionally, if the Markov chain is stationary and $\\rho_1 < 1$, then $\\rho_C < 1$.", "folder": "", "qid": "statistic-mcqm-4307", "question": "Under what conditions does $\\rho_C < 1$ hold for a Markov chain $U$ with $(U_{n-1}, U_n) \\sim C$?", "options": ["A) The density $c$ of $C$ satisfies $c > 0$ almost surely on $(0,1)^{2d}$ and $\\phi_C^2 < \\infty$.", "B) The density $c$ of $C$ satisfies $c(u,v) \\geq \\epsilon q(u)q(v)$ almost surely for some $\\epsilon > 0$.", "C) The Markov chain is Gaussian and $A - 4BMB^\\top$ is positive definite.", "D) The Markov chain is stationary and $\\rho_1 < 1$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The condition $\\rho_C < 1$ for a Markov chain $U$ with $(U_{n-1}, U_n) \\sim C$ can be ensured under several sufficient conditions. These include the density $c$ of $C$ being positive almost surely on $(0,1)^{2d}$ with $\\phi_C^2 < \\infty$, or $c(u,v) \\geq \\epsilon q(u)q(v)$ almost surely for some $\\epsilon > 0$. For Gaussian copulas, $\\phi_C^2 < \\infty$ if and only if $A - 4BMB^\\top$ is positive definite. Additionally, if the Markov chain is stationary and $\\rho_1 < 1$, then $\\rho_C < 1$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: The text states that the distribution of $\\Omega(Y)$ is $\\operatorname{Exp}(\\theta)$, which is an exponential distribution with rate parameter $\\theta$.\n- **B) Correct**: The text mentions that equal-tailed tolerance intervals for $Y$ can be easily derived because $\\Omega(Y)$ follows an exponential distribution.\n- **C) Incorrect**: The text does not specify that the hazard rate must be constant for $\\Omega(Y)$ to follow an exponential distribution. The hazard rate can be any function $\\omega(y)$.\n- **D) Incorrect**: The text does not mention deriving the cumulative distribution function of $Y$ directly from $\\Omega(Y)$. It focuses on the distribution of $\\Omega(Y)$ and tolerance intervals.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Two-sided-tolerance-intervals-in-the-exponential_2.md"], "background_text": "For a continuous random variable $Y$ with hazard rate $\\omega(y)/\\theta, y > 0$, the integrated hazard rate $\\Omega(Y) = \\int_{0}^{Y} \\omega(y) dy$ follows an $\\operatorname{Exp}(\\theta)$ distribution. This property allows for the derivation of equal-tailed tolerance intervals for $Y$.", "folder": "", "qid": "statistic-mcqm-4980", "question": "Given a continuous random variable $Y$ with hazard rate $\\omega(y)/\\theta, y > 0$, and the integrated hazard rate $\\Omega(Y) = \\int_{0}^{Y} \\omega(y) dy$ following an $\\operatorname{Exp}(\\theta)$ distribution, which of the following statements are correct?", "options": ["A) The distribution of $\\Omega(Y)$ is exponential with rate parameter $\\theta$.", "B) Equal-tailed tolerance intervals for $Y$ can be derived using the exponential distribution of $\\Omega(Y)$.", "C) The hazard rate $\\omega(y)/\\theta$ must be constant for $\\Omega(Y)$ to follow an exponential distribution.", "D) The cumulative distribution function of $Y$ can be directly derived from $\\Omega(Y)$."], "category": "PARTIALLY_CORRECT", "question_context_used": "For a continuous random variable $Y$ with hazard rate $\\omega(y)/\\theta, y > 0$, the integrated hazard rate $\\Omega(Y) = \\int_{0}^{Y} \\omega(y) dy$ follows an $\\operatorname{Exp}(\\theta)$ distribution. This property allows for the derivation of equal-tailed tolerance intervals for $Y$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**: \n- **A)** Correct. Assumption (A1) states that $F$ is twice differentiable on its support $(a,b)$, which is necessary for the convergence.\n- **B)** Incorrect. While (A1)' mentions that $F$ admits a density function $f=F^{\\prime}$ which is positive on the open support of $F$, it is not listed as a necessary condition for the convergence in the context.\n- **C)** Correct. Assumption (A2) provides a bound involving $\\gamma$, which is necessary for the convergence.\n- **D)** Correct. Assumption (A3) involves an integral condition with $\\lambda$, which is also necessary for the convergence.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Strassen-s-Law-of-the-Iterated-Logarithm-for-th_19.md"], "background_text": "Under some mild conditions, Goldie (1977) proved that the empirical Lorenz curves converge, with probability one, uniformly to the population ones. The assumptions (A1)-(A4) are necessary for this convergence, including twice differentiability of $F$, positivity of $f$, and certain bounds on $f^{\\prime}$ and integrals involving $F$.", "folder": "", "qid": "statistic-mcqm-4066", "question": "Which assumptions are necessary for the empirical Lorenz curves to converge uniformly to the population ones?", "options": ["A) $F$ is twice differentiable on its support $(a,b)$.", "B) $F$ admits a density function $f=F^{\\prime}$ which is positive on the open support of $F$.", "C) For some $\\gamma>0$, $\\operatorname*{sup}_{a<x<b}F(x)(1-F(x))|f^{\\prime}(x)|/f^{2}(x)\\leqslant\\gamma$.", "D) For some $\\lambda\\in(0,\\frac{1}{2})$, $\\int_{0}^{\\infty}(1-F(x))^{1/2-\\lambda}d x<\\infty$."], "category": "PARTIALLY_CORRECT", "question_context_used": "Under some mild conditions, Goldie (1977) proved that the empirical Lorenz curves converge, with probability one, uniformly to the population ones. The assumptions (A1)-(A4) are necessary for this convergence, including twice differentiability of $F$, positivity of $f$, and certain bounds on $f^{\\prime}$ and integrals involving $F$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A** is correct because the constraints for the parameters are explicitly stated as $\\xi > 0$, $\\kappa \\geqslant 0$, and $\\nu > 0$.\n- **B** is correct because setting $\\kappa=0$ indeed yields a Weibull distribution as mentioned in the text.\n- **C** is correct because setting $\\nu=1$ results in an exponential distribution.\n- **D** is correct because the text describes the model interpretation where $Y$ is the scaled excess in a Weibull random variable $T$ over a threshold value $\\kappa$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["On_some_nonregular_tests_for_a_modified_Weibull_model.md"], "background_text": "The survivor function is defined as $\\bar{F}(y)=\\exp\\{\\kappa^{\\nu}-(\\kappa+\\xi y)^{\\nu}\\}$ for $y\\geqslant0$ with constraints $\\xi > 0$, $\\kappa \\geqslant 0$, and $\\nu > 0$. Setting $\\kappa=0$ yields a Weibull distribution, and setting $\\nu=1$ yields an exponential distribution. The model interprets $Y$ as the scaled excess in a Weibull random variable $T$ over a threshold value $\\kappa$.", "folder": "", "qid": "statistic-mcqm-5520", "question": "Given the survivor function $\\bar{F}(y)=\\exp\\{\\kappa^{\\nu}-(\\kappa+\\xi y)^{\\nu}\\}$ for $y\\geqslant0$, which of the following statements are true about the parameters?", "options": ["A) $\\xi > 0$, $\\kappa \\geqslant 0$, and $\\nu > 0$ are the constraints for the parameters.", "B) Setting $\\kappa=0$ yields a Weibull distribution.", "C) Setting $\\nu=1$ yields an exponential distribution.", "D) The model can be interpreted as $Y$ being the scaled excess in a Weibull random variable $T$ over a threshold value $\\kappa$."], "category": "CORRECT", "question_context_used": "The survivor function is defined as $\\bar{F}(y)=\\exp\\{\\kappa^{\\nu}-(\\kappa+\\xi y)^{\\nu}\\}$ for $y\\geqslant0$ with constraints $\\xi > 0$, $\\kappa \\geqslant 0$, and $\\nu > 0$. Setting $\\kappa=0$ yields a Weibull distribution, and setting $\\nu=1$ yields an exponential distribution. The model interprets $Y$ as the scaled excess in a Weibull random variable $T$ over a threshold value $\\kappa$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The trial is stopped for equivalence if $\\text{PP} < \\theta_{\\mathrm{L}}$.\n- **B)** Correct. The trial is stopped for superiority if $\\text{PP} > \\theta_{\\mathrm{U}}$.\n- **C)** Correct. The threshold $\\theta_{\\mathrm{T}}$ is used to assess non-equivalence based on the posterior probability.\n- **D)** Incorrect. The PP explicitly averages over the future data $Y_{1}$ and $Y_{2}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Phase II Trial Design with Bayesian Adaptive Rando.md"], "background_text": "The predictive probability (PP) is used to make adaptive decisions in the trial. If $\\text{PP} < \\theta_{\\mathrm{L}}$, the trial is stopped for equivalence. If $\\text{PP} > \\theta_{\\mathrm{U}}$, the trial is stopped for superiority. The PP involves averaging over the future data $Y_{1}$ and $Y_{2}$.", "folder": "", "qid": "statistic-mcqm-1750", "question": "In the context of predictive probability (PP) for a Bayesian trial design comparing two treatments, which of the following statements are true regarding the decision rules based on PP?", "options": ["A) If $\\text{PP} < \\theta_{\\mathrm{L}}$, the trial is stopped for equivalence.", "B) If $\\text{PP} > \\theta_{\\mathrm{U}}$, the trial is stopped for superiority.", "C) The threshold $\\theta_{\\mathrm{T}}$ is used to determine non-equivalence of treatments.", "D) The PP is computed without considering the future data $Y_{1}$ and $Y_{2}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The predictive probability (PP) is used to make adaptive decisions in the trial. If $\\text{PP} < \\theta_{\\mathrm{L}}$, the trial is stopped for equivalence. If $\\text{PP} > \\theta_{\\mathrm{U}}$, the trial is stopped for superiority. The PP involves averaging over the future data $Y_{1}$ and $Y_{2}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: The text states that the DSMCEM algorithm is expected to be computationally more efficient.\n- **B) Correct**: The text mentions that convergence diagnosis required by MCMC algorithms can be avoided with the DSMCEM algorithm.\n- **C) Incorrect**: The text indicates that the DSMCEM algorithm involves more Monte Carlo Error (MCE) as compared with the MCMCEM algorithm.\n- **D) Incorrect**: The text clarifies that the DSMCEM algorithm uses samples from the prior distribution, not the posterior distributions, in the $E$ step.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["Efficient-direct-sampling-MCEM-algorithm-for-late_.md"], "background_text": "Since DSMCEM uses direct samples in the E step, convergence diagnosis required by MCMC algorithms can be avoided, and it is expected to be computationally more efficient. On the other hand, because the DSMCEM algorithm uses samples from the prior distribution whereas the MCMCEM algorithm uses samples from the posterior distributions in the $E$ step, it can be expected that the DSMCEM algorithm will involve more Monte Carlo Error (MCE) as compared with the MCMCEM algorithm.", "folder": "", "qid": "statistic-mcqm-5381", "question": "Which of the following are advantages of the DSMCEM algorithm over the MCMCEM algorithm as mentioned in the text?", "options": ["A) The DSMCEM algorithm is computationally more efficient.", "B) The DSMCEM algorithm avoids convergence diagnosis required by MCMC algorithms.", "C) The DSMCEM algorithm involves less Monte Carlo Error (MCE) as compared with the MCMCEM algorithm.", "D) The DSMCEM algorithm uses samples from the posterior distributions in the $E$ step."], "category": "CORRECT", "question_context_used": "Since DSMCEM uses direct samples in the E step, convergence diagnosis required by MCMC algorithms can be avoided, and it is expected to be computationally more efficient. On the other hand, because the DSMCEM algorithm uses samples from the prior distribution whereas the MCMCEM algorithm uses samples from the posterior distributions in the $E$ step, it can be expected that the DSMCEM algorithm will involve more Monte Carlo Error (MCE) as compared with the MCMCEM algorithm.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The text states that the asymptotically optimal first-stage sample sizes are consistently good estimates of the exact optimal first-stage sample sizes over different prior distributions and moderate to large horizon size $N$.\n- **B)** Correct. The text mentions that $W_{2}=W_{2}(N,n_{1_{a}},n_{2_{a}},\\pi)$ is a good approximation for $V_{2}=V_{2}(N,\\pi)$.\n- **C)** Incorrect. The table shows that the exact optimal sample sizes are not always larger than the asymptotically optimal sample sizes; for example, in the row with $(a2,b2)=(0·5,0·5)$ and $N=102$, $n_{1_{o}}=3$ and $n_{1_{a}}=5.0$.\n- **D)** Correct. The table shows that as $N$ increases, the difference between $n_{1_{o}}$ and $n_{1_{a}}$ tends to decrease, indicating better approximation.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Choosing_sample_size_for_a_clinical_trial_using_decision_analysis.md"], "background_text": "The context discusses a two-stage trial where both arms are unknown. It highlights that the asymptotically optimal first-stage sample sizes $(n_{1_{a}}, n_{2_{a}})$ are good estimates of the exact optimal first-stage sample sizes $(n_{1_{o}}, n_{2_{o}})$ for moderate to large $N$. Additionally, $W_{2}$ is a good approximation for $V_{2}$.", "folder": "", "qid": "statistic-mcqm-5561", "question": "In a two-stage trial with both arms unknown, the asymptotically optimal first-stage sample sizes $(n_{1_{a}}, n_{2_{a}})$ are compared to the exact optimal first-stage sample sizes $(n_{1_{o}}, n_{2_{o}})$. Based on Table 2, which of the following statements are correct?", "options": ["A) The asymptotically optimal sample sizes are consistently good estimates of the exact optimal sample sizes for moderate to large horizon size $N$.", "B) The expected utility $W_{2}$ is a good approximation for $V_{2}$.", "C) The exact optimal sample sizes are always larger than the asymptotically optimal sample sizes.", "D) The difference between $n_{1_{o}}$ and $n_{1_{a}}$ decreases as $N$ increases."], "category": "CORRECT", "question_context_used": "The context discusses a two-stage trial where both arms are unknown. It highlights that the asymptotically optimal first-stage sample sizes $(n_{1_{a}}, n_{2_{a}})$ are good estimates of the exact optimal first-stage sample sizes $(n_{1_{o}}, n_{2_{o}})$ for moderate to large $N$. Additionally, $W_{2}$ is a good approximation for $V_{2}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The text explicitly states that the folded logistic model has no additional parameters to model the correlation structure.\n- **B)** Correct. The text mentions that the intralitter correlation is fixed automatically once the response probability is given, which is a limitation of the folded logistic model.\n- **C)** Incorrect. The text indicates that the folded logistic model fits the data poorly compared to the beta-binomial fit.\n- **D)** Correct. The marginal response probability is indeed given by $p = \\frac{2}{1 + 2^{\\beta}}$ in the folded logistic model.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["A Litter-Based Approach to Risk Assessment in Deve.md"], "background_text": "The folded logistic model lacks parameters to model the correlation structure, fixing the intralitter correlation once the response probability is determined. It fits Skellam’s Brassica data poorly compared to the beta-binomial model. The marginal response probability is defined as $p = \\frac{2}{1 + 2^{\\beta}}$.", "folder": "", "qid": "statistic-mcqm-5444", "question": "Which of the following statements are true about the folded logistic model?", "options": ["A) The folded logistic model has no additional parameters to model the correlation structure.", "B) The intralitter correlation is fixed once the response probability is given in the folded logistic model.", "C) The folded logistic model fits Skellam’s Brassica data better than the beta-binomial model.", "D) The marginal response probability in the folded logistic model is given by $p = \\frac{2}{1 + 2^{\\beta}}$."], "category": "CORRECT", "question_context_used": "The folded logistic model lacks parameters to model the correlation structure, fixing the intralitter correlation once the response probability is determined. It fits Skellam’s Brassica data poorly compared to the beta-binomial model. The marginal response probability is defined as $p = \\frac{2}{1 + 2^{\\beta}}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C, D\n\n- **A) Incorrect**: The expected value of $x_{i j}$ is $a_{i}c_{j}\\theta + b_{j}$, not necessarily zero.\n- **B) Correct**: The constraints $\\Sigma a_{i}^{2} = \\Sigma c_{j}^{2} = 1$ imply orthogonality conditions.\n- **C) Correct**: The method of least squares is explicitly mentioned for estimating the constants.\n- **D) Correct**: The constant $\\theta$ is introduced to satisfy the scaling conditions for $a_{i}$ and $c_{j}$.\n\n**Final Answer**: The final answer is $\\boxed{{B, C, D}}$", "source": ["THE_INTERPRETATION_OF_INTERACTIONS_IN_FACTORIAL_EXPERIMENTS.md"], "background_text": "The model assumes the mean of all results is zero and uses the method of least squares to estimate constants under given constraints. The constant $\\theta$ is introduced for scaling purposes, and the constraints $\\Sigma a_{i} = \\Sigma b_{j} = 0$ and $\\Sigma a_{i}^{2} = \\Sigma c_{j}^{2} = 1$ are applied.", "folder": "", "qid": "statistic-mcqm-2023", "question": "Given the model $x_{i j} = a_{i}c_{j}\\theta + b_{j}$ with the constraints $\\Sigma a_{i} = \\Sigma b_{j} = 0$ and $\\Sigma a_{i}^{2} = \\Sigma c_{j}^{2} = 1$, which of the following statements are correct?", "options": ["A) The expected value of $x_{i j}$ is zero.", "B) The constants $a_{i}$ and $c_{j}$ are chosen to satisfy orthogonality conditions.", "C) The method of least squares is used to estimate the constants $a_{i}$, $b_{j}$, and $c_{j}$.", "D) The constant $\\theta$ is introduced to ensure the sum of squares of $a_{i}$ and $c_{j}$ equals one."], "category": "PARTIALLY_CORRECT", "question_context_used": "The model assumes the mean of all results is zero and uses the method of least squares to estimate constants under given constraints. The constant $\\theta$ is introduced for scaling purposes, and the constraints $\\Sigma a_{i} = \\Sigma b_{j} = 0$ and $\\Sigma a_{i}^{2} = \\Sigma c_{j}^{2} = 1$ are applied.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. $T_{i}(b_{i})$ is indeed used for testing the specified hypothesis.\n- **B)** Correct. The asymptotic power of the test is correctly given by the provided formula.\n- **C)** Correct. The context states that $T_{i}(b_{i})$ is only truly computable in practice given that $b_{i}=0$.\n- **D)** Incorrect. The statistic is derived from a Gaussian approximation, not a Poisson distribution.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Scandinavian J Statistics - 2019 - Lagerås - How t.md"], "background_text": "The statistic $T_{i}(b_{i})$ is defined as $\\frac{\\hat{p}_{i}-\\tilde{p}_{i}-b_{i}}{\\sqrt{\\operatorname{Var}_{\\hat{p}_{i}}[\\hat{p}_{i}]+\\operatorname{Var}_{\\tilde{p}_{i}}[\\tilde{p}_{i}]}$ and is used to test the hypothesis $H_{0}:b_{i}=0$ against $H_{1}:b_{i}>0$. The asymptotic power of this test is given by $1-\\Phi\\left(z_{1-\\gamma}-\\frac{b_{i}}{\\sqrt{\\mathrm{Var}_{\\hat{p}_{i}}[\\hat{p}_{i}]+\\mathrm{Var}_{\\hat{p}_{i}}[\\hat{p}_{i}]}}\\right)$. $T_{i}(b_{i})$ is only truly computable in practice given that $b_{i}=0$.", "folder": "", "qid": "statistic-mcqm-5882", "question": "Which of the following are correct interpretations or applications of the statistic $T_{i}(b_{i})$?", "options": ["A) $T_{i}(b_{i})$ is used to test the hypothesis $H_{0}:b_{i}=0$ against $H_{1}:b_{i}>0$.", "B) The asymptotic power of the test based on $T_{i}(b_{i})$ is given by $1-\\Phi\\left(z_{1-\\gamma}-\\frac{b_{i}}{\\sqrt{\\mathrm{Var}_{\\hat{p}_{i}}[\\hat{p}_{i}]+\\mathrm{Var}_{\\hat{p}_{i}}[\\hat{p}_{i}]}\\right)$.", "C) $T_{i}(b_{i})$ is only truly computable in practice given that $b_{i}=0$.", "D) The statistic $T_{i}(b_{i})$ is derived from a Poisson distribution."], "category": "CORRECT", "question_context_used": "The statistic $T_{i}(b_{i})$ is defined as $\\frac{\\hat{p}_{i}-\\tilde{p}_{i}-b_{i}}{\\sqrt{\\operatorname{Var}_{\\hat{p}_{i}}[\\hat{p}_{i}]+\\operatorname{Var}_{\\tilde{p}_{i}}[\\tilde{p}_{i}]}$ and is used to test the hypothesis $H_{0}:b_{i}=0$ against $H_{1}:b_{i}>0$. The asymptotic power of this test is given by $1-\\Phi\\left(z_{1-\\gamma}-\\frac{b_{i}}{\\sqrt{\\mathrm{Var}_{\\hat{p}_{i}}[\\hat{p}_{i}]+\\mathrm{Var}_{\\hat{p}_{i}}[\\hat{p}_{i}]}}\\right)$. $T_{i}(b_{i})$ is only truly computable in practice given that $b_{i}=0$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **Option A** is correct as it directly represents the Jacobian using the singular values of $A$.\n- **Option B** is correct because it equivalently represents the Jacobian using the eigenvalues of $A A^{\\prime}$.\n- **Option C** is correct as it represents the Jacobian when using the QR decomposition of $A$, where $t_{i i}$ are the diagonal elements of the upper triangular matrix $T$.\n- **Option D** is incorrect because the exponent should be $m$, not $N$, matching the dimension of $X$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["A-note-about-measures-and-Jacobians-of-singular_20.md"], "background_text": "The Jacobian of the transform $Y = A X$ is given by $(d Y) = \\prod_{i=1}^{N} \\sigma_{i}(A)^{m}(d X) = \\prod_{i=1}^{N} c h_{i}(A A^{\\prime})^{m/2}(d X)$, where $\\sigma_{i}(A)$ are the singular values of $A$ and $c h_{i}(A A^{\\prime})$ are the nonnull eigenvalues of $A A^{\\prime}$. Alternatively, using QR decomposition, it can also be expressed as $(d Y) = \\prod_{i=1}^{N} t_{i i}^{m}(d X)$ where $t_{i i}$ are the diagonal elements of the upper triangular matrix $T$ in the QR decomposition of $A$.", "folder": "", "qid": "statistic-mcqm-5949", "question": "Given the transform $Y = A X$ where $X \\in \\mathcal{L}_{m,N}^{+}(N)$, $A \\in \\mathcal{L}_{N,p}^{+}(N)$ is constant, and $Y \\in \\mathcal{L}_{m,p}^{+}(N)$, which of the following expressions correctly represent the Jacobian of the transform $(d Y)$?", "options": ["A) $(d Y) = \\prod_{i=1}^{N} \\sigma_{i}(A)^{m}(d X)$", "B) $(d Y) = \\prod_{i=1}^{N} c h_{i}(A A^{\\prime})^{m/2}(d X)$", "C) $(d Y) = \\prod_{i=1}^{N} t_{i i}^{m}(d X)$", "D) $(d Y) = \\prod_{i=1}^{N} \\sigma_{i}(A)^{N}(d X)$"], "category": "CORRECT", "question_context_used": "The Jacobian of the transform $Y = A X$ is given by $(d Y) = \\prod_{i=1}^{N} \\sigma_{i}(A)^{m}(d X) = \\prod_{i=1}^{N} c h_{i}(A A^{\\prime})^{m/2}(d X)$, where $\\sigma_{i}(A)$ are the singular values of $A$ and $c h_{i}(A A^{\\prime})$ are the nonnull eigenvalues of $A A^{\\prime}$. Alternatively, using QR decomposition, it can also be expressed as $(d Y) = \\prod_{i=1}^{N} t_{i i}^{m}(d X)$ where $t_{i i}$ are the diagonal elements of the upper triangular matrix $T$ in the QR decomposition of $A$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The unified skew-normal distribution indeed generalizes the multivariate skew-normal distribution by introducing additional parameters such as $\\gamma$ and $\\Delta$.\n- **B)** Correct. When the skewness parameter $\\alpha$ is zero, the distribution reduces to a multivariate normal distribution, as stated in the text.\n- **C)** Incorrect. The density of the unified skew-normal distribution is expressed in terms of the cumulative distribution function of a standard normal, as seen in the density formula provided.\n- **D)** Correct. This is the exact mechanism described in the text for obtaining the unified skew-normal distribution from a multivariate Gaussian.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Conjugate_Bayes_for_probit_regression_via_unified_skew-normal_distributions.md"], "background_text": "The unified skew-normal distribution unifies different generalizations of the multivariate skew-normal distribution. Its density is obtained by modifying that of a $p$-variate Gaussian $N_{p}(\\xi,\\Omega)$ with the cumulative distribution function of a standard normal evaluated at $\\alpha^{\\mathrm{T}}\\omega^{-1}(z-\\xi)$. When $\\alpha=0_{p}$, the multivariate skew-normal distribution coincides with $N_{p}(\\xi,\\Omega)$.", "folder": "", "qid": "statistic-mcqm-5594", "question": "Which of the following statements are true about the unified skew-normal distribution?", "options": ["A) The unified skew-normal distribution generalizes the multivariate skew-normal distribution by introducing additional parameters.", "B) When the skewness parameter $\\alpha$ is zero, the unified skew-normal distribution reduces to a multivariate normal distribution.", "C) The density of the unified skew-normal distribution cannot be expressed in terms of the cumulative distribution function of a standard normal.", "D) The unified skew-normal distribution is obtained by modifying the density of a $p$-variate Gaussian with the cumulative distribution function of a standard normal evaluated at $\\alpha^{\\mathrm{T}}\\omega^{-1}(z-\\xi)$."], "category": "CORRECT", "question_context_used": "The unified skew-normal distribution unifies different generalizations of the multivariate skew-normal distribution. Its density is obtained by modifying that of a $p$-variate Gaussian $N_{p}(\\xi,\\Omega)$ with the cumulative distribution function of a standard normal evaluated at $\\alpha^{\\mathrm{T}}\\omega^{-1}(z-\\xi)$. When $\\alpha=0_{p}$, the multivariate skew-normal distribution coincides with $N_{p}(\\xi,\\Omega)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "The background text confirms that the linearity condition can be expressed as $\\sum_{i=1}^{k}d_{i}\\tilde{z}^{\\mathrm{T}}\\eta_{i}$ where $d_{i}$ are constants associated with $b$, supporting option A. It also states that $\\|z^{*}\\|$ is independent of both $\\tilde{z}^{\\mathrm{T}}b$ and $\\tilde{z}^{\\mathrm{T}}\\eta_{i}$, supporting option B. The text further shows that $E[\\tilde{z}^{\\mathrm{T}}b|(\\tilde{z}^{\\mathrm{T}}\\eta_{1},\\dots,\\tilde{z}^{\\mathrm{T}}\\eta_{k},\\Vert z^{*}\\Vert)]=E[\\tilde{z}^{\\mathrm{T}}b|(\\tilde{z}^{\\mathrm{T}}\\eta_{1},\\dots,\\tilde{z}^{\\mathrm{T}}\\eta_{k})]$, supporting option D. Option C is not supported by the text as it does not discuss the condition under which the linearity condition does not hold.\n\n**Correct Choices**: A, B, D\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Robust-dimension-reduction-based-on-canonical_2009.md"], "background_text": "Since $\\Tilde{z}=z^{*}/\\|z^{*}\\|$ is uniformly distributed on the unit sphere surface in $\\mathbf{R}^{p}$, it has a spherical distribution and satisfies the linearity condition $E[\\tilde{z}^{\\mathrm{T}}b|(\\tilde{z}^{\\mathrm{T}}\\eta_{1},\\dots,\\tilde{z}^{\\mathrm{T}}\\eta_{k})]=\\sum_{i=1}^{k}d_{i}\\tilde{z}^{\\mathrm{T}}\\eta_{i}$, where $b$ is any vector in $\\mathbf{R}^{p}$ and $d_{i}$ are constants associated with $b$. Since $\\|z^{*}\\|$ is independent of both $\\tilde{z}^{\\mathrm{T}}b$ and $\\tilde{z}^{\\mathrm{T}}\\eta_{i}$, we have $E[\\tilde{z}^{\\mathrm{T}}b|(\\tilde{z}^{\\mathrm{T}}\\eta_{1},\\dots,\\tilde{z}^{\\mathrm{T}}\\eta_{k},\\Vert z^{*}\\Vert)]=E[\\tilde{z}^{\\mathrm{T}}b|(\\tilde{z}^{\\mathrm{T}}\\eta_{1},\\dots,\\tilde{z}^{\\mathrm{T}}\\eta_{k})]$.", "folder": "", "qid": "statistic-mcqm-5022", "question": "Which of the following statements are true about the linearity condition involving $E[\\tilde{z}^{\\mathrm{T}}b|(\\tilde{z}^{\\mathrm{T}}\\eta_{1},\\dots,\\tilde{z}^{\\mathrm{T}}\\eta_{k})]$?", "options": ["A) It can be expressed as $\\sum_{i=1}^{k}d_{i}\\tilde{z}^{\\mathrm{T}}\\eta_{i}$ where $d_{i}$ are constants associated with $b$.", "B) $\\|z^{*}\\|$ is independent of both $\\tilde{z}^{\\mathrm{T}}b$ and $\\tilde{z}^{\\mathrm{T}}\\eta_{i}$.", "C) The linearity condition does not hold if $\\tilde{z}$ is not uniformly distributed.", "D) The condition implies that $E[\\tilde{z}^{\\mathrm{T}}b|(\\tilde{z}^{\\mathrm{T}}\\eta_{1},\\dots,\\tilde{z}^{\\mathrm{T}}\\eta_{k},\\Vert z^{*}\\Vert)]=E[\\tilde{z}^{\\mathrm{T}}b|(\\tilde{z}^{\\mathrm{T}}\\eta_{1},\\dots,\\tilde{z}^{\\mathrm{T}}\\eta_{k})]$."], "category": "CORRECT", "question_context_used": "Since $\\Tilde{z}=z^{*}/\\|z^{*}\\|$ is uniformly distributed on the unit sphere surface in $\\mathbf{R}^{p}$, it has a spherical distribution and satisfies the linearity condition $E[\\tilde{z}^{\\mathrm{T}}b|(\\tilde{z}^{\\mathrm{T}}\\eta_{1},\\dots,\\tilde{z}^{\\mathrm{T}}\\eta_{k})]=\\sum_{i=1}^{k}d_{i}\\tilde{z}^{\\mathrm{T}}\\eta_{i}$, where $b$ is any vector in $\\mathbf{R}^{p}$ and $d_{i}$ are constants associated with $b$. Since $\\|z^{*}\\|$ is independent of both $\\tilde{z}^{\\mathrm{T}}b$ and $\\tilde{z}^{\\mathrm{T}}\\eta_{i}$, we have $E[\\tilde{z}^{\\mathrm{T}}b|(\\tilde{z}^{\\mathrm{T}}\\eta_{1},\\dots,\\tilde{z}^{\\mathrm{T}}\\eta_{k},\\Vert z^{*}\\Vert)]=E[\\tilde{z}^{\\mathrm{T}}b|(\\tilde{z}^{\\mathrm{T}}\\eta_{1},\\dots,\\tilde{z}^{\\mathrm{T}}\\eta_{k})]$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "To determine the correct options, let's analyze each one based on the context provided:\n\n- **Option A**: Correct. The text explicitly states that when $\\pmb{\\beta}_{k} \\neq \\mathbf{0}_{p_{k}}$, the first KKT condition is satisfied.\n- **Option B**: Correct. The text states that when $\\pmb{\\\beta}_{k} = \\mathbf{0}_{p_{k}}$, the second KKT condition is satisfied if $\\|\\mathbf{g}_{k}\\| \\leq \\alpha\\lambda\\omega_{k}$.\n- **Option C**: Correct. This is the rearranged form of the update when $\\beta_{k} \\neq \\mathbf{0}_{p_{k}}$, as shown in the text.\n- **Option D**: Correct. This is the form of the update when $\\pmb{\\beta}_{k} = \\mathbf{0}_{p_{k}}$, as derived in the text.\n\n**Correct Choices**: A, B, C, D\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Versatile_Descent_Algorithms_for_Group_Regularization_and_Variable_Selection_in_Generalized_Linear_Models__Versatile_Descent_Algorithms_for_Group_Regularization_and_Variable_Selection_in_Generalized_Linear_Models.md"], "background_text": "Theorem 1 states that for the update to be optimal when $\\mathbf{H}_{k} = \\phi\\mathbf{I}_{p_{k}}$, two KKT conditions must be satisfied at convergence. If $\\beta_{k} \\neq \\mathbf{0}_{p_{k}}$, the first KKT condition is satisfied through a specific rearrangement of the update. If $\\beta_{k} = \\mathbf{0}_{p_{k}}$, the second KKT condition is satisfied if $\\|\\mathbf{g}_{k}\\| \\leq \\alpha\\lambda\\omega_{k}$.", "folder": "", "qid": "statistic-mcqm-2003", "question": "In the context of Theorem 1, which conditions must be satisfied at the convergence of the algorithm for the update to be optimal when $\\mathbf{H}_{k} = \\phi\\mathbf{I}_{p_{k}}$?", "options": ["A) The first KKT condition is satisfied when $\\pmb{\\beta}_{k} \\neq \\mathbf{0}_{p_{k}}$.", "B) The second KKT condition is satisfied when $\\pmb{\\beta}_{k} = \\mathbf{0}_{p_{k}}$.", "C) The update can be rearranged to show $\\mathbf{0}_{P_{k}} = -\\mathbf{g}_{k} + \\frac{\\alpha\\lambda\\omega_{k}\\beta_{k}}{\\|\\beta_{k}\\|} + (1-\\alpha)\\lambda\\omega_{k}\\beta_{k}$ when $\\beta_{k} \\neq \\mathbf{0}_{p_{k}}$.", "D) The update can be rearranged to show $\\mathbf{0}_{P k} = \\frac{1}{\\phi + (1-\\alpha)\\lambda\\omega_{k}}\\left(1 - \\frac{\\alpha\\lambda\\omega_{k}}{\\|\\mathbf{g}_{k}\\|}\\right)_{+}\\mathbf{g}_{k}$ when $\\pmb{\\beta}_{k} = \\mathbf{0}_{p_{k}}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "Theorem 1 states that for the update to be optimal when $\\mathbf{H}_{k} = \\phi\\mathbf{I}_{p_{k}}$, two KKT conditions must be satisfied at convergence. If $\\beta_{k} \\neq \\mathbf{0}_{p_{k}}$, the first KKT condition is satisfied through a specific rearrangement of the update. If $\\beta_{k} = \\mathbf{0}_{p_{k}}$, the second KKT condition is satisfied if $\\|\\mathbf{g}_{k}\\| \\leq \\alpha\\lambda\\omega_{k}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **Option A** is correct because the background text explicitly states that if (A1) is not true, $B_{0}$ is orthogonal to each column of $B$.\n- **Option B** is correct as the text mentions that both $E[\\varDelta(B^{\\top}X)|B_{0}^{\\top}X]$ and $H(\\theta_{0})$ may be zero for some $\\varDelta(B^{\\intercal}X)$.\n- **Option C** is correct because the text concludes that the proposed test is an asymptotic directional test for some sequence of local alternative models under this condition.\n- **Option D** is incorrect because the convergence of $\\hat{B}$ is not discussed in the context of (A1) being untrue.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["A-goodness-of-fit-test-for-variable-adju_2019_Comp.md"], "background_text": "If (A1) is not true, the vector $B_{0}$ is orthogonal to each column of B. Then both $E[\\varDelta(B^{\\top}X)|B_{0}^{\\top}X]$ and $H(\\theta_{0})$ may be zero for some $\\varDelta(B^{\\intercal}X)$. This implies that the proposed test is an asymptotic directional test for some sequence of local alternative models.", "folder": "", "qid": "statistic-mcqm-5986", "question": "What are the implications if Assumption (A1) is not true?", "options": ["A) The vector $B_{0}$ is orthogonal to each column of $B$.", "B) Both $E[\\varDelta(B^{\\top}X)|B_{0}^{\\top}X]$ and $H(\\theta_{0})$ may be zero for some $\\varDelta(B^{\\intercal}X)$.", "C) The proposed test becomes an asymptotic directional test for some sequence of local alternative models.", "D) The estimator $\\hat{B}$ fails to converge."], "category": "CORRECT", "question_context_used": "If (A1) is not true, the vector $B_{0}$ is orthogonal to each column of B. Then both $E[\\varDelta(B^{\\top}X)|B_{0}^{\\top}X]$ and $H(\\theta_{0})$ may be zero for some $\\varDelta(B^{\\intercal}X)$. This implies that the proposed test is an asymptotic directional test for some sequence of local alternative models.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **A) Correct**: The SMC model reduces the parameter space dimension by grouping states with identical transition probability vectors, as specified in the partition.\n- **B) Incorrect**: The partition must satisfy $k<|\\mathcal{X}^{m}|$ to reduce the parameter space, not $k=|\\mathcal{X}^{m}|$.\n- **C) Correct**: The defining feature of an SMC is that states within the same partition subset share identical transition probability vectors.\n- **D) Incorrect**: While the initial state probability is often ignored in likelihood inference for MCs, it is not a defining property of SMC models, and the statement is too absolute.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Scandinavian J Statistics - 2013 - Jääskinen - Spa.md"], "background_text": "An SMC model is defined by a partition $\\boldsymbol{S}=(s_{1},\\ldots,s_{k})$ of $\\mathcal{X}^{m}$ where transition probability vectors are identical for all pairs of states within the same subset $s_{c}$. This reduces the parameter space dimension if $k<|\\mathcal{X}^{m}|$. The likelihood computation typically ignores the initial state probability for inference purposes.", "folder": "", "qid": "statistic-mcqm-5001", "question": "In the context of Sparse Markov Chains (SMC), which of the following statements accurately describe the properties of an SMC model?", "options": ["A) An SMC model reduces the effective dimension of the parameter space by having identical transition probability vectors for certain states.", "B) The partition $\\boldsymbol{S}=(s_{1},\\ldots,s_{k})$ of $\\mathcal{X}^{m}$ must satisfy $k=|\\mathcal{X}^{m}|$ to be considered an SMC.", "C) An SMC model is a special case of a Markov chain where the transition probability vectors are identical for all states in the same partition subset $s_{c}$.", "D) The likelihood of a data sequence under an SMC model can be computed without considering the initial state probability."], "category": "PARTIALLY_CORRECT", "question_context_used": "An SMC model is defined by a partition $\\boldsymbol{S}=(s_{1},\\ldots,s_{k})$ of $\\mathcal{X}^{m}$ where transition probability vectors are identical for all pairs of states within the same subset $s_{c}$. This reduces the parameter space dimension if $k<|\\mathcal{X}^{m}|$. The likelihood computation typically ignores the initial state probability for inference purposes.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\nAll the options A, B, C, and D are correct because they list the necessary conditions for the Laplace asymptotic approximation to be valid. The conditions ensure that the function $m(\\pmb{\\alpha})$ is well-behaved, the integral converges, and the Hessian matrix at the point of approximation is positive definite, which is crucial for the approximation's accuracy.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["A-Bayesian-propensity-score-adjustment-for-laten_2.md"], "background_text": "The Laplace asymptotic approximation to the integral $R(m)$ is valid under the following conditions: (1) $m(\\pmb{\\alpha})$ is continuous, (2) $h(\\pmb{\\alpha})$ has continuous second-order partial derivatives in the neighborhood of $\\hat{\\pmb{\\alpha}}$, (3) $R(\\pmb{\\alpha})$ converges absolutely for all $\\tau\\geqslant\\tau_{0}$ and (4) $H(\\hat{\\pmb{\\alpha}})=\\hat{\\partial}^{2}h(\\pmb{\\alpha})/\\hat{\\partial}\\pmb{\\alpha}\\hat{\\partial}\\pmb{\\alpha}^{t}|_{\\hat{\\pmb{\\alpha}}}$ is positive definite.", "folder": "", "qid": "statistic-mcqm-6113", "question": "Under what conditions can the Laplace asymptotic approximation to the integral $R(m)$ be applied?", "options": ["A) $m(\\pmb{\\alpha})$ is continuous", "B) $h(\\pmb{\\alpha})$ has continuous second-order partial derivatives in the neighborhood of $\\hat{\\pmb{\\alpha}}$", "C) $R(\\pmb{\\alpha})$ converges absolutely for all $\\tau\\geqslant\\tau_{0}$", "D) $H(\\hat{\\pmb{\\alpha}})=\\hat{\\partial}^{2}h(\\pmb{\\alpha})/\\hat{\\partial}\\pmb{\\alpha}\\hat{\\partial}\\pmb{\\alpha}^{t}|_{\\hat{\\pmb{\\alpha}}}$ is positive definite"], "category": "CORRECT", "question_context_used": "The Laplace asymptotic approximation to the integral $R(m)$ is valid under the following conditions: (1) $m(\\pmb{\\alpha})$ is continuous, (2) $h(\\pmb{\\alpha})$ has continuous second-order partial derivatives in the neighborhood of $\\hat{\\pmb{\\alpha}}$, (3) $R(\\pmb{\\alpha})$ converges absolutely for all $\\tau\\geqslant\\tau_{0}$ and (4) $H(\\hat{\\pmb{\\alpha}})=\\hat{\\partial}^{2}h(\\pmb{\\alpha})/\\hat{\\partial}\\pmb{\\alpha}\\hat{\\partial}\\pmb{\\alpha}^{t}|_{\\hat{\\pmb{\\alpha}}}$ is positive definite.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n- **A)** Correct. Maximum likelihood estimation is mentioned in the context.\n- **B)** Correct. Moment estimation is also mentioned.\n- **C)** Incorrect. Least squares estimation is not mentioned.\n- **D)** Incorrect. Bayesian estimation is not mentioned.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["A-bootstrap-goodness-of-fit-test-for-the-gener_200.md"], "background_text": "The context mentions maximum likelihood and moment estimation methods for estimating the shape parameter $\\gamma$ of the GPd, but notes that these estimates do not exist for the whole parameter space. Least squares and Bayesian estimation are not mentioned.", "folder": "", "qid": "statistic-mcqm-5194", "question": "Which of the following methods are mentioned in the context for estimating the shape parameter $\\gamma$ of the GPd?", "options": ["A) Maximum likelihood estimation.", "B) Moment estimation.", "C) Least squares estimation.", "D) Bayesian estimation."], "category": "CORRECT", "question_context_used": "The context mentions maximum likelihood and moment estimation methods for estimating the shape parameter $\\gamma$ of the GPd, but notes that these estimates do not exist for the whole parameter space. Least squares and Bayesian estimation are not mentioned.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The model is indeed specified separately for each stratum, as mentioned in the context.\n- **B)** Correct. The odds ratio in each stratum is correctly given by the specified form.\n- **C)** Incorrect. The context explicitly states that stratum sizes must be allowed to become large as the overall sample size becomes large, contradicting this option.\n- **D)** Correct. The context discusses the equivalence of applying the model directly to case-control data and the induced retrospective logistic model.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Logistic_disease_incidence_models_and_case-control_studies.md"], "background_text": "A prospective logistic disease incidence model is specified in each stratum, with the odds ratio in each stratum being of the form $\\exp(\\{z-z_{0}\\}\\beta_{i}\\}(\\mathfrak{i}=1,...,k)$. Stratum sizes must be allowed to become large as the overall sample size becomes large, with a nonzero limiting ratio, for the asymptotic likelihood theory to apply.", "folder": "", "qid": "statistic-mcqm-2256", "question": "In the context of case-control studies with stratified sampling, which of the following statements are true about the logistic disease incidence model?", "options": ["A) The model is specified separately for each stratum.", "B) The odds ratio in each stratum is of the form $\\exp(\\{z-z_{0}\\}\\beta_{i}\\}(\\mathfrak{i}=1,...,k)$.", "C) The model assumes that stratum sizes remain fixed as the overall sample size increases.", "D) The model can be applied directly to case-control data without any modifications."], "category": "PARTIALLY_CORRECT", "question_context_used": "A prospective logistic disease incidence model is specified in each stratum, with the odds ratio in each stratum being of the form $\\exp(\\{z-z_{0}\\}\\beta_{i}\\}(\\mathfrak{i}=1,...,k)$. Stratum sizes must be allowed to become large as the overall sample size becomes large, with a nonzero limiting ratio, for the asymptotic likelihood theory to apply.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\"answer\": [\"A\", \"B\"]}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. RMSPE is explicitly mentioned as a metric for predictive accuracy.\n- **B)** Correct. Average computation time is listed as one of the performance metrics.\n- **C)** Correct. Coverage probabilities of prediction intervals are also mentioned as part of the assessment.\n\n- **D)** Incorrect. Mean absolute error (MAE) is not mentioned in the context.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["Additive_Function-on-Function_Regression__Additive_Function-on-Function_Regression.md"], "background_text": "The performance of AFF-PC was evaluated based on in-sample and out-of-sample predictive accuracy, measured by RMSPE, average computation time, and coverage probabilities of prediction intervals.", "folder": "", "qid": "statistic-mcqm-5120", "question": "Which of the following metrics were used to assess the performance of AFF-PC?", "options": ["A) Root mean squared prediction error (RMSPE)", "B) Average computation time", "C) Coverage probabilities of prediction intervals", "D) Mean absolute error (MAE)"], "category": "CORRECT", "question_context_used": "The performance of AFF-PC was evaluated based on in-sample and out-of-sample predictive accuracy, measured by RMSPE, average computation time, and coverage probabilities of prediction intervals.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. This is the system equation as stated in the context.\n- **B)** Correct. This equation defines the system error $W_{n+1}$.\n- **C)** Correct. This equation describes how the mean is updated in the dynamic calculation.\n- **D)** Correct. This is the definition of $S_{n}^{-1}$ as given in the context.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["Discussion of Dr Silverman's Paper.md"], "background_text": "The context includes several equations related to the dynamic calculation of the spline smoother, such as the system equation, the system error, and the update of the mean. It also mentions the inverse of the matrix $S_{n}$.", "folder": "", "qid": "statistic-mcqm-3665", "question": "In the dynamic calculation of the spline smoother discussed by Dr A. Robinson, which of the following equations are correctly stated?", "options": ["A) $\\gamma_{n+1}=G_{n+1}\\gamma_{n}+w_{n+1}$ where $G_{n+1}=B_{n+1}^{-1}~B_{n}$.", "B) $W_{n+1}=(S_{n+1}-\\sigma^{-2}b_{\\nu}^{\\mathrm{T}}b_{\\nu})^{-1}-G_{n+1}S_{n}^{-1}G_{n+1}^{\\mathrm{T}}$.", "C) $\\gamma_{n+1}^{\\mathrm{post}}=\\gamma_{n+1}^{\\mathrm{prior}}+c_{\\nu}(\\nu_{\\nu}-y_{\\nu}^{\\mathrm{prior}})=G_{n+1}\\gamma_{n}^{\\mathrm{post}}+c_{\\nu}(\\nu_{\\nu}-y_{\\nu}^{\\mathrm{prior}})$.", "D) $S_{n}^{-1}=(\\alpha\\Omega_{n}+\\sigma^{2}B_{n}^{\\mathrm{T}}B_{n})^{-1}$."], "category": "CORRECT", "question_context_used": "The context includes several equations related to the dynamic calculation of the spline smoother, such as the system equation, the system error, and the update of the mean. It also mentions the inverse of the matrix $S_{n}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The inequality $P \\geq \\mathrm{Pr}(\\mathrm{PU}|\\mathrm{SU})$ is explicitly stated in the context.\n- **B) Correct**: The context mentions that $\\mathrm{Pr}(\\mathrm{PU}|\\mathrm{SU})$ fails to reflect the risk arising from values of $X$ which are twins, triples, and so forth, implying the same for $P$.\n- **C) Incorrect**: There is no statement in the context suggesting that $P$ is always equal to $\\theta$.\n- **D) Correct**: The context suggests that whether $\\theta$ or $P$ is more appropriate depends on the method of attack, implying that $P$ is more appropriate under attack method 2.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$.", "source": ["A Measure of Disclosure Risk for Microdata.md"], "background_text": "Under attack method 2, the intruder draws one microdata record at random from the sample unique records and searches through the population at random until a unit is located which matches the selected record. The probability $P$ that the selected record belongs to the unit located is at least $\\mathrm{Pr}(\\mathrm{PU}|\\mathrm{SU})$, and it fails to reflect the risk arising from values of $X$ which are twins, triples, and so forth. Whether $\\theta$ or $P$ is a more appropriate measure of risk depends on the method of attack.", "folder": "", "qid": "statistic-mcqm-4028", "question": "Under attack method 2, which of the following statements are true regarding the probability $P$ that the selected record belongs to the unit located?", "options": ["A) $P$ is at least $\\mathrm{Pr}(\\mathrm{PU}|\\mathrm{SU})$.", "B) $P$ fails to reflect the risk arising from values of $X$ which are twins, triples, and so forth.", "C) $P$ is always equal to $\\theta$.", "D) $P$ is a more appropriate measure of risk than $\\theta$ under attack method 2."], "category": "PARTIALLY_CORRECT", "question_context_used": "Under attack method 2, the intruder draws one microdata record at random from the sample unique records and searches through the population at random until a unit is located which matches the selected record. The probability $P$ that the selected record belongs to the unit located is at least $\\mathrm{Pr}(\\mathrm{PU}|\\mathrm{SU})$, and it fails to reflect the risk arising from values of $X$ which are twins, triples, and so forth. Whether $\\theta$ or $P$ is a more appropriate measure of risk depends on the method of attack.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The text mentions that the extensions confer closure properties for marginals, conditionals, and joint distributions.\n- **B)** Correct. The text describes the incorporation of a skewness matrix $\\Delta$ and a scale matrix $\\Gamma$.\n- **C)** Correct. Setting $p=1$ yields a univariate skew-normal distribution, as stated in the text.\n- **D)** Correct. The text mentions the requirement that the matrix $\\Omega^{*}$ must be a full-rank correlation matrix.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Conjugate_Bayes_for_probit_regression_via_unified_skew-normal_distributions.md"], "background_text": "The unified skew-normal distribution incorporates a skewness matrix $\\Delta\\in\\mathbb{R}^{p\\times n}$ and an $n\\times n$ full-rank scale $\\Gamma$. It also confers closure properties for marginals, conditionals, and joint distributions. When $p=1$, it yields a univariate skew-normal distribution. Arellano-Valle & Azzalini (2006) added a condition that constrains the matrix $\\Omega^{*}$ to be a full-rank correlation matrix.", "folder": "", "qid": "statistic-mcqm-5630", "question": "Which of the following are properties or extensions of the unified skew-normal distribution?", "options": ["A) Closure properties for marginals, conditionals, and joint distributions.", "B) The introduction of a skewness matrix $\\Delta$ and a scale matrix $\\Gamma$.", "C) The reduction to a univariate skew-normal distribution when $p=1$.", "D) The requirement that the matrix $\\Omega^{*}$ must be a full-rank correlation matrix."], "category": "CORRECT", "question_context_used": "The unified skew-normal distribution incorporates a skewness matrix $\\Delta\\in\\mathbb{R}^{p\\times n}$ and an $n\\times n$ full-rank scale $\\Gamma$. It also confers closure properties for marginals, conditionals, and joint distributions. When $p=1$, it yields a univariate skew-normal distribution. Arellano-Valle & Azzalini (2006) added a condition that constrains the matrix $\\Omega^{*}$ to be a full-rank correlation matrix.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**:\n- **A)** Correct. The first term in the grouped formula for $M_{(k-1)}^{2}$ is indeed a grouping correction.\n- **B)** Correct. The second term in the grouped formula for $M_{(k-1)}^{2}$ is also a grouping correction.\n- **C)** Correct. The third term and beyond in the grouped formula for $M_{(k-1)}^{2}$ form a grouped analogue of equation (9).\n- **D)** Correct. The large sample limit distribution for $M_{(k-1)}^{2}$ is correctly described as the convolution of $k-1$ copies of the usual Cramér-von Mises limit.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Grouping Corrections for Circular Goodness-Of-Fit.md"], "background_text": "The grouped formula for $M_{(k-1)}^{2}$ includes the first and second terms as grouping corrections, whereas the other terms form a grouped analogue of equation (9). The large sample limit distribution for $M_{(k-1)}^{2}$ is the convolution of $k-1$ copies of the usual Cramér-von Mises limit.", "folder": "", "qid": "statistic-mcqm-2175", "question": "Regarding the grouped formula for $M_{(k-1)}^{2}$, which of the following statements are true?", "options": ["A) The first term is a grouping correction.", "B) The second term is a grouping correction.", "C) The third term is a direct grouped analogue of equation (9).", "D) The large sample limit distribution for $M_{(k-1)}^{2}$ is the convolution of $k-1$ copies of the usual Cramér-von Mises limit."], "category": "PARTIALLY_CORRECT", "question_context_used": "The grouped formula for $M_{(k-1)}^{2}$ includes the first and second terms as grouping corrections, whereas the other terms form a grouped analogue of equation (9). The large sample limit distribution for $M_{(k-1)}^{2}$ is the convolution of $k-1$ copies of the usual Cramér-von Mises limit.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **A) Correct**: The table shows that for any given alternative $p$, the rejection proportion generally increases as the sample size $n$ increases. For example, for $p \\equiv 0.05$ and $c_h=1$, the rejection proportion increases from 40% at $n=50$ to 73% at $n=100$ and 88% at $n=200$.\n- **B) Incorrect**: The table does not consistently show a decrease in rejection proportion as $c_h$ increases. For example, for $p \\equiv 0.05$ and $n=50$, the rejection proportion increases from 40% at $c_h=1$ to 52% at $c_h=2$.\n- **C) Correct**: For $p \\equiv 0.25$, the rejection proportion is 100% across all sample sizes and values of $c_h$ shown in the table.\n- **D) Incorrect**: The table shows that for $n=50$ and $c_h=1$, the rejection proportion is 40% for $p \\equiv 0.05$ and 94% for $p \\equiv 0.25$, which means the rejection proportion is higher for $p \\equiv 0.25$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Goodness-of-fit_tests_for_the_cure_rate_in_a_mixture_cure_model.md"], "background_text": "The table shows the proportion of censoring $p_{cens}$ and rejection proportion (%) of $\\tilde{\\mathcal{T}}_{n}$ for several values of $n$, $c_{h}$, and $c_{T}$. The null hypothesis is $p \\equiv 0$ with alternatives $p \\equiv 0.05, 0.15, 0.25$ at a significance level $\\alpha = 0.05$.", "folder": "", "qid": "statistic-mcqm-4167", "question": "Given the context of testing for a cure rate with a null hypothesis $p \\equiv 0$ and alternatives $p \\equiv 0.05, 0.15, 0.25$, at a significance level $\\alpha = 0.05$, which of the following statements are correct based on the rejection proportions provided in the table?", "options": ["A) The rejection proportion increases as the sample size $n$ increases for a given alternative $p$.", "B) The rejection proportion decreases as the value of $c_h$ increases for a given alternative $p$ and sample size $n$.", "C) For $p \\equiv 0.25$, the rejection proportion reaches 100% for all sample sizes $n$ and values of $c_h$.", "D) The rejection proportion is higher for $p \\equiv 0.05$ than for $p \\equiv 0.25$ at $n=50$ and $c_h=1$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The table shows the proportion of censoring $p_{cens}$ and rejection proportion (%) of $\\tilde{\\mathcal{T}}_{n}$ for several values of $n$, $c_{h}$, and $c_{T}$. The null hypothesis is $p \\equiv 0$ with alternatives $p \\equiv 0.05, 0.15, 0.25$ at a significance level $\\alpha = 0.05$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "Let's evaluate each option step-by-step:\n\n- **Option A**: This is correct. The text states that the algorithm uses a re-parameterization from $\\mu$ to $\\xi=\\log\\mu$ to avoid interim out-of-range values during iterations.\n\n- **Option B**: This is correct. The algorithm involves solving the likelihood equations using a modified Newton-Raphson iterative scheme, as described in the text.\n\n- **Option C**: This is incorrect. The text mentions that the advantage of using $G(\\theta)$ is the simplicity of its inverse, not its complexity.\n\n- **Option D**: This is correct. The initial values for the iterative scheme are indeed set as ${\\xi}^{(0)}=\\log\\left(n+\\varepsilon\\right)$ for some small $\\varepsilon>0$ and $\\boldsymbol{\\lambda}^{(0)}=\\mathbf{0}$.\n\n**Correct Choices**: A, B, D\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["A_proportional_odds_model_with_subject-specific_effects_for_repeated_ordered_categorical_responses.md"], "background_text": "The algorithm for fitting generalized log linear models involves a re-parameterization to $\\xi=\\log\\mu$, a modified Newton-Raphson iterative scheme, and the use of matrix $G(\\theta)$ for its simple inverse. Initial values are set as ${\\xi}^{(0)}=\\log\\left(n+\\varepsilon\\right)$ and $\\boldsymbol{\\lambda}^{(0)}=\\mathbf{0}$.", "folder": "", "qid": "statistic-mcqm-5607", "question": "Which of the following statements are true regarding the algorithm for fitting generalized log linear models as described in the text?", "options": ["A) The algorithm uses a re-parameterization from $\\mu$ to $\\xi=\\log\\mu$ to avoid interim out-of-range values during the iterative process.", "B) The algorithm involves solving for $\\hat{\\pmb{\\theta}}$ in the likelihood equations using a modified Newton-Raphson iterative scheme.", "C) The matrix $G(\\theta)$ is used in the algorithm because its inverse is complex and difficult to compute.", "D) The initial values for the iterative scheme are set as ${\\xi}^{(0)}=\\log\\left(n+\\varepsilon\\right)$ for some small $\\pmb{\\varepsilon}>0$ and $\\boldsymbol{\\lambda}^{(0)}=\\mathbf{0}$."], "category": "CORRECT", "question_context_used": "The algorithm for fitting generalized log linear models involves a re-parameterization to $\\xi=\\log\\mu$, a modified Newton-Raphson iterative scheme, and the use of matrix $G(\\theta)$ for its simple inverse. Initial values are set as ${\\xi}^{(0)}=\\log\\left(n+\\varepsilon\\right)$ and $\\boldsymbol{\\lambda}^{(0)}=\\mathbf{0}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A)** Correct. $\\delta=0$ is associated with the canonical parameter.\n- **B)** Incorrect. $\\delta=\\frac{1}{3}$ is associated with normal likelihood, but this option incorrectly states it as a property of $\\delta=\\frac{1}{3}$.\n- **C)** Correct. $\\delta=\\frac{1}{2}$ corresponds to stability of variance.\n- **D)** Correct. $\\delta=1$ corresponds to the mean value parameter $\\tau(\\theta)=E_{\\theta}t(X_{1})$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Parametrizations of Non-Linear Models.md"], "background_text": "Wedderburn examined parametrizations in the one-dimensional exponential family, showing that different values of $\\delta$ in the formula $\\psi(\\theta_{1})=\\int_{\\theta_{0}}^{\\theta_{1}}\\left\\{{\\frac{d^{2}}{d\\theta^{2}}}\\ln{\\phi(\\theta)}\\right\\}^{\\delta}d\\theta$ correspond to different properties: $\\delta=0$ to the canonical parameter, $\\delta=\\frac{1}{3}$ to normal likelihood, $\\delta=\\frac{1}{2}$ to stability of variance, $\\delta=\\frac{2}{3}$ to zero asymptotic skewness, and $\\delta=1$ to the mean value parameter $\\tau(\\theta)=E_{\\theta}t(X_{1})$.", "folder": "", "qid": "statistic-mcqm-5188", "question": "What are the properties associated with different values of $\\delta$ in the parametrization formula $\\psi(\\theta_{1})=\\int_{\\theta_{0}}^{\\theta_{1}}\\left\\{{\\frac{d^{2}}{d\\theta^{2}}}\\ln{\\phi(\\theta)}\\right\\}^{\\delta}d\\theta$ according to Wedderburn?", "options": ["A) $\\delta=0$ corresponds to the canonical parameter.", "B) $\\delta=\\frac{1}{3}$ corresponds to normal likelihood.", "C) $\\delta=\\frac{1}{2}$ corresponds to stability of variance.", "D) $\\delta=1$ corresponds to the mean value parameter $\\tau(\\theta)=E_{\\theta}t(X_{1})$."], "category": "PARTIALLY_CORRECT", "question_context_used": "Wedderburn examined parametrizations in the one-dimensional exponential family, showing that different values of $\\delta$ in the formula $\\psi(\\theta_{1})=\\int_{\\theta_{0}}^{\\theta_{1}}\\left\\{{\\frac{d^{2}}{d\\theta^{2}}}\\ln{\\phi(\\theta)}\\right\\}^{\\delta}d\\theta$ correspond to different properties: $\\delta=0$ to the canonical parameter, $\\delta=\\frac{1}{3}$ to normal likelihood, $\\delta=\\frac{1}{2}$ to stability of variance, $\\delta=\\frac{2}{3}$ to zero asymptotic skewness, and $\\delta=1$ to the mean value parameter $\\tau(\\theta)=E_{\\theta}t(X_{1})$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **NAIVE**: The TR is consistently 100% for all sample sizes (n=120, n=240, n=600).\n- **MV**: The TR is consistently 100% for all sample sizes.\n- **MU**: The TR is consistently 100% for all sample sizes.\n- **LASSO**: The TR varies and is not 100% for any sample size.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["A-Bayesian-information-criterion-for-port_2012_Com.md"], "background_text": "The table shows simulation results for various portfolio construction methods under different sample sizes. The transaction cost (TR) is one of the performance measures reported.", "folder": "", "qid": "statistic-mcqm-4761", "question": "Based on the simulation results, which portfolio construction methods have a transaction cost (TR) of 100% across all sample sizes (n=120, n=240, n=600)?", "options": ["A) NAIVE", "B) MV", "C) MU", "D) LASSO"], "category": "PARTIALLY_CORRECT", "question_context_used": "The table shows simulation results for various portfolio construction methods under different sample sizes. The transaction cost (TR) is one of the performance measures reported.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": []\n}", "model_answer": [], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**: \n- **Option A** is correct because the doubly robust property ensures that the estimator is CAN if either of the two models (exposure or outcome) is correctly specified.\n- **Option B** is incorrect because the estimator does not require the joint exposure distribution to be known a priori; it estimates it from the data.\n- **Option C** is correct because the estimator is obtained by solving an estimating equation that imposes a condition on the weighted score equations, enhancing its robustness.\n- **Option D** is incorrect because the estimator can be adapted for use when some confounders are intermediate variables, as discussed in the supplementary material.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Semiparametric Tests for Sufficient Cause Interact.md"], "background_text": "The doubly robust estimator $\\hat{\\beta}_{\\mathrm{mar}}$ for marginal sufficient cause interaction is designed to be consistent and asymptotically normal if either the model for the exposure distribution or the model for the conditional outcome mean is correctly specified. It is obtained through a specific estimating equation approach that enhances its robustness against model misspecification.", "folder": "", "qid": "statistic-mcqm-6081", "question": "Which of the following statements are true about the doubly robust estimator $\\hat{\\beta}_{\\mathrm{mar}}$ for marginal sufficient cause interaction?", "options": ["A) It is consistent and asymptotically normal (CAN) if either the model for the exposure distribution or the model for the conditional outcome mean is correctly specified.", "B) It requires the joint exposure distribution to be known a priori.", "C) It is obtained by solving an estimating equation that imposes a specific condition on the weighted score equations.", "D) It cannot be used when some confounders are intermediate variables."], "category": "CORRECT", "question_context_used": "The doubly robust estimator $\\hat{\\beta}_{\\mathrm{mar}}$ for marginal sufficient cause interaction is designed to be consistent and asymptotically normal if either the model for the exposure distribution or the model for the conditional outcome mean is correctly specified. It is obtained through a specific estimating equation approach that enhances its robustness against model misspecification.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: B, D\n\n**Explanation**: \n- **Option A** is incorrect because a smaller $\\nu$ indicates a heavier tail, and the S&P500 returns have a smaller $\\nu$ (around 10) compared to the TOPIX returns (around 20).\n- **Option B** is correct because the TOPIX returns have a larger $\\nu$ (around 20), indicating a lighter tail compared to the S&P500 returns.\n- **Option C** is incorrect as both returns have heavy-tailed distributions, as indicated by the values of $\\nu$ and the context.\n- **Option D** is correct as the context explicitly states that the measurement errors of stock returns have heavy-tailed distributions.\n\n**Final Answer**: The final answer is $\\boxed{{B, D}}$", "source": ["Leverage--heavy-tails-and-correlated-jumps-in-_200.md"], "background_text": "The magnitude of tail-fatness is measured by the parameter $\\nu$ in the Models SVLt and SVLJt. The posterior means of $\\nu$’s are around 10 for the S&P500 returns and 20 for the TOPIX returns. This indicates that measurement errors of stock returns have heavy-tailed distributions as pointed out in the past literature.", "folder": "", "qid": "statistic-mcqm-3804", "question": "Based on the posterior means of $\\nu$ for the S&P500 and TOPIX returns, which of the following statements are correct?", "options": ["A) The S&P500 returns have a heavier tail than the TOPIX returns.", "B) The TOPIX returns have a heavier tail than the S&P500 returns.", "C) Both S&P500 and TOPIX returns have light-tailed distributions.", "D) The measurement errors of stock returns are heavy-tailed."], "category": "PARTIALLY_CORRECT", "question_context_used": "The magnitude of tail-fatness is measured by the parameter $\\nu$ in the Models SVLt and SVLJt. The posterior means of $\\nu$’s are around 10 for the S&P500 returns and 20 for the TOPIX returns. This indicates that measurement errors of stock returns have heavy-tailed distributions as pointed out in the past literature.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}", "model_answer": ["B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: The text explicitly states that 'the linearity condition holds for this model.'\n- **B) Correct**: The text mentions 'we have $k=1$,' indicating the true dimension is 1.\n- **C) Correct**: Since $\\beta_{1}=(1,0,0,0,0)^{\\mathrm{T}}$ and the model is correctly specified, the first estimated direction should be close to this vector.\n- **D) Incorrect**: The single outlier is generated by replacing the first predictor value by 1000,000, not 1000.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Robust-dimension-reduction-based-on-canonical_2009.md"], "background_text": "Observations of $x=(x_{1},\\ldots,x_{5})$ are generated from the normal distribution $N(\\underline{0},I_{5})$. We let $y=x_{1}$ without a random error. Thus, we have $k=1$, $\\beta_{1}=(1,0,0,0,0)^{\\mathrm{T}}$, and the linearity condition holds for this model. The single outlier in each sample is generated by replacing the first predictor value in the first observation by 1000,000.", "folder": "", "qid": "statistic-mcqm-4992", "question": "In the simulation study where observations of $x=(x_{1},\\ldots,x_{5})$ are generated from the normal distribution $N(\\underline{0},I_{5})$ and $y=x_{1}$ without a random error, which of the following statements are true?", "options": ["A) The linearity condition holds for this model.", "B) The true dimension $k$ is 1.", "C) The first estimated direction $\\hat{\\boldsymbol{\\beta}}_{1}$ should be close to $(1,0,0,0,0)^{\\mathrm{T}}$.", "D) The single outlier is generated by replacing the first predictor value in the first observation by 1000."], "category": "INCORRECT", "question_context_used": "Observations of $x=(x_{1},\\ldots,x_{5})$ are generated from the normal distribution $N(\\underline{0},I_{5})$. We let $y=x_{1}$ without a random error. Thus, we have $k=1$, $\\beta_{1}=(1,0,0,0,0)^{\\mathrm{T}}$, and the linearity condition holds for this model. The single outlier in each sample is generated by replacing the first predictor value in the first observation by 1000,000.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **Option A** is correct because the semi-Markov model assumes constant time intervals between adjacent observations.\n- **Option B** is correct because $\\gamma(t_{i}-\\omega)$ is assumed to be constant over each interval of observation in the semi-Markov model.\n- **Option C** is incorrect because the probability of observing an animal dying naturally and being tumour free has not changed in the semi-Markov model.\n- **Option D** is incorrect because the probability of observing an animal without the tumour at sacrifice has not changed in the semi-Markov model.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Estimating_the_tumour_onset_distribution_in_animal_carcinogenesis_experiments.md"], "background_text": "For the semi-Markov model, it is assumed that the time intervals between adjacent observations are constant and that $\\gamma(t_{i}-\\omega)$ is constant over each interval of observation. The probability of observing an animal dying naturally and being tumour free has not changed, and similarly, the probability of observing an animal without the tumour at sacrifice has not changed.", "folder": "", "qid": "statistic-mcqm-5105", "question": "Under the semi-Markov model, which of the following assumptions are made?", "options": ["A) The time intervals between adjacent observations are constant.", "B) $\\gamma(t_{i}-\\omega)$ is constant over each interval of observation.", "C) The probability of observing an animal dying naturally and being tumour free has changed.", "D) The probability of observing an animal without the tumour at sacrifice has changed."], "category": "CORRECT", "question_context_used": "For the semi-Markov model, it is assumed that the time intervals between adjacent observations are constant and that $\\gamma(t_{i}-\\omega)$ is constant over each interval of observation. The probability of observing an animal dying naturally and being tumour free has not changed, and similarly, the probability of observing an animal without the tumour at sacrifice has not changed.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**:\n- **A) Correct**: The exponential covariance function is specified with $\\nu=0.5$, $\\sigma^{2}=0.5$, and $\\phi=1$.\n- **B) Correct**: The coefficients $\\pmb\\beta_{1}$ for the geostatistical response are indeed set to $(1, 5)^{\\top}$.\n- **C) Correct**: The precision $\\tau^{2}$ for the geostatistical response is specified as 1.\n- **D) Correct**: The offset term for the point-pattern response is assigned a value of 0.25.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$.", "source": ["Combining-heterogeneous-spatial-datasets-with-proc.md"], "background_text": "The simulation uses an exponential covariance function with specific parameters ($\\nu=0.5$, $\\sigma^{2}=0.5$, $\\phi=1$). Coefficients and precision for the geostatistical response are specified, and an offset term is used for the point-pattern response.", "folder": "", "qid": "statistic-mcqm-1448", "question": "Which of the following are true about the simulation setup in the first study?", "options": ["A) The covariance function used is exponential with $\\nu=0.5$, $\\sigma^{2}=0.5$, and $\\phi=1$.", "B) The coefficients for the geostatistical response are set to $\\pmb\\beta_{1} = (1, 5)^{\\top}$.", "C) The precision $\\tau^{2}$ for the geostatistical response is set to 1.", "D) The offset term for the point-pattern response is set to 0.25."], "category": "PARTIALLY_CORRECT", "question_context_used": "The simulation uses an exponential covariance function with specific parameters ($\\nu=0.5$, $\\sigma^{2}=0.5$, $\\phi=1$). Coefficients and precision for the geostatistical response are specified, and an offset term is used for the point-pattern response.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The equation provided accurately represents the measurement model as described in the context.\n- **B)** Correct. $f_{j}$ is indeed the vector of latent variables for the jth group.\n- **C)** Correct. The context specifies that $\\pmb{e}_{j}\\sim\\mathbf{N}(0,\\Psi_{j})$.\n- **D)** Incorrect. The measurement model is used when the independent variable is not manipulated by researchers, contrary to this option's statement.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["A-Bayesian-propensity-score-adjustment-for-laten_2.md"], "background_text": "The measurement model in structural equation modeling is expressed by the equation ${\\bf{y}}_{j}={\\pmb{\\mu}}_{j}+{\\pmb{\\Lambda}}f_{j}+{\\pmb{e}}_{j}$, where ${\\bf y}_{j}$ is the vector of observed variables, $f_{j}$ is the vector of latent variables, and $\\pmb{e}_{j}\\sim\\mathbf{N}(0,\\Psi_{j})$. This model is used in studies where the independent variable is not manipulated by researchers.", "folder": "", "qid": "statistic-mcqm-6158", "question": "In the context of structural equation modeling, which of the following statements are true about the measurement model?", "options": ["A) The measurement model is expressed as ${\\bf{y}}_{j}={\\pmb{\\mu}}_{j}+{\\pmb{\\Lambda}}f_{j}+{\\pmb{e}}_{j}$ where ${\\bf y}_{j}$ is the vector of observed variables.", "B) $f_{j}$ represents the vector of latent variables for the jth group.", "C) The measurement model assumes that $\\pmb{e}_{j}\\sim\\mathbf{N}(0,\\Psi_{j})$.", "D) The measurement model is only applicable when the independent variable is manipulated by researchers."], "category": "CORRECT", "question_context_used": "The measurement model in structural equation modeling is expressed by the equation ${\\bf{y}}_{j}={\\pmb{\\mu}}_{j}+{\\pmb{\\Lambda}}f_{j}+{\\pmb{e}}_{j}$, where ${\\bf y}_{j}$ is the vector of observed variables, $f_{j}$ is the vector of latent variables, and $\\pmb{e}_{j}\\sim\\mathbf{N}(0,\\Psi_{j})$. This model is used in studies where the independent variable is not manipulated by researchers.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct, as the text states: 'This is an extension of Adichie's (1967) result.'\n- **B)** Correct, according to the text: 'Hence, $\\pmb{n}^{\\dagger}(\\hat{b}^{(1)} - \\hat{b}) \\dot{-} 0$ in probability.'\n- **C)** Correct, as the text shows the asymptotic distribution: '$n^{\\mathbf{i}}\\tau^{-1}(\\bar{d}, \\Sigma b) \\to \\mathbf{M}\\mathbf{V}\\mathbf{\\vec{x}}\\left(0,\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & \\Sigma\\end{array}\\right]\\right)$ in distribution.'\n- **D)** Incorrect, as the text provides sufficient information to determine the joint asymptotic distribution.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["A_robust_analysis_of_the_general_linear_model_based_on_one_step_R-estimates.md"], "background_text": "The text mentions the joint asymptotic distribution of $(\\hat{b}_{0}^{(1)}, \\hat{b}^{(1)})'$ as an extension of Adichie's (1967) result and discusses the convergence in probability of $n^{\\dagger}(\\hat{b}^{(1)} - \\hat{b})$ to $0$. It also presents the asymptotic distribution of $n^{\\mathbf{i}}\\tau^{-1}(\\bar{d}, \\Sigma b)$ as multivariate normal with a specified covariance matrix.", "folder": "", "qid": "statistic-mcqm-1818", "question": "Which of the following are correct regarding the joint asymptotic distribution of $(\\hat{b}_{0}^{(1)}, \\hat{b}^{(1)})'$?", "options": ["A) It is an extension of Adichie's (1967) result.", "B) $n^{\\dagger}(\\hat{b}^{(1)} - \\hat{b}) \\dot{-} 0$ in probability.", "C) The joint asymptotic distribution is multivariate normal with mean $0$ and covariance matrix $\\left[\\begin{array}{cc}1 & 0 \\\\ 0 & \\Sigma\\end{array}\\right]$.", "D) The joint asymptotic distribution cannot be determined from the given context."], "category": "PARTIALLY_CORRECT", "question_context_used": "The text mentions the joint asymptotic distribution of $(\\hat{b}_{0}^{(1)}, \\hat{b}^{(1)})'$ as an extension of Adichie's (1967) result and discusses the convergence in probability of $n^{\\dagger}(\\hat{b}^{(1)} - \\hat{b})$ to $0$. It also presents the asymptotic distribution of $n^{\\mathbf{i}}\\tau^{-1}(\\bar{d}, \\Sigma b)$ as multivariate normal with a specified covariance matrix.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**: \n- **A) Correct**: For $C_{0}$, the model $H$ was selected correctly by the AIC on more than 50% of the runs, indicating correct identification.\n- **B) Incorrect**: The text does not provide specific information about the identification of $C_{1}$ by the order-restricted criteria.\n- **C) Correct**: For $C_{2}$, $M_{2}$ was selected correctly in general, indicating correct identification by the order-restricted criteria.\n- **D) Incorrect**: For $C_{5}$, the AIC surpassed the order-restricted information criterion, indicating it was not correctly identified by the latter.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["An_information_criterion_for_parameters_under_a_simple_order_restriction.md"], "background_text": "The Monte Carlo study examined the performance of the order-restricted information criterion for $k=4$ with normal variates having means $\\theta_{1},\\theta_{2},\\theta_{3}, \\theta_{4}$ and variance 1. The configurations $C_{0}$ to $C_{5}$ were tested. The order-restricted criteria correctly identified the corresponding configurations in general, except for $C_{0}$, where the model $H$ was selected correctly by the AIC on more than 50% of the runs. For $C_{5}$, the AIC surpassed the order-restricted information criterion.", "folder": "", "qid": "statistic-mcqm-3707", "question": "Given the Monte Carlo study with $k=4$ and variance 1, which of the following parameter configurations are correctly identified by the order-restricted information criteria?", "options": ["A) $C_{0}\\colon(0,0,0,0)$", "B) $C_{1}\\colon2^{-1}(0,0,0,1)$", "C) $C_{2}\\colon2^{-1}(0,0,1,1)$", "D) $C_{5}\\colon2^{-1}(0,1,0,0)$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The Monte Carlo study examined the performance of the order-restricted information criterion for $k=4$ with normal variates having means $\\theta_{1},\\theta_{2},\\theta_{3}, \\theta_{4}$ and variance 1. The configurations $C_{0}$ to $C_{5}$ were tested. The order-restricted criteria correctly identified the corresponding configurations in general, except for $C_{0}$, where the model $H$ was selected correctly by the AIC on more than 50% of the runs. For $C_{5}$, the AIC surpassed the order-restricted information criterion.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**: \n- **Option A** is correct because the text states that $\\Sigma y_{i}^{-\\tau}$ has finite variance for $\\tau < \\nu/2$.\n- **Option B** is correct because the text mentions the convergence in distribution to a normal distribution.\n- **Option C** is correct because $b$ is defined as $\\Gamma(1 - 2/\\nu)$ in the text.\n- **Option D** is correct because $\\upsilon$ is defined as $\\Gamma(1 - 2/\\nu) - b^{2}$ in the text.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["On_some_nonregular_tests_for_a_modified_Weibull_model.md"], "background_text": "For $\\tau < \\nu/2$, $\\Sigma y_{i}^{-\\tau}$ has finite variance under $H_{0}$ $\\pmb{\\kappa}=\\mathbf{0}$ and $n^{-\\frac{1}{2}}(\\Sigma y_{i}^{-\\tau} - n b) \\Rightarrow N(0, \\upsilon)$ with $b = \\Gamma(1 - 2/\\nu)$ and $\\pmb{v} = \\Gamma(1 - 2/\\nu) - \\pmb{b}^{2}$.", "folder": "", "qid": "statistic-mcqm-5518", "question": "For $\\tau < \\nu/2$, which of the following statements are true about $\\Sigma y_{i}^{-\\tau}$ under $H_{0}$ $\\pmb{\\kappa}=\\mathbf{0}$?", "options": ["A) $\\Sigma y_{i}^{-\\tau}$ has finite variance", "B) $n^{-\\frac{1}{2}}(\\Sigma y_{i}^{-\\tau} - n b) \\Rightarrow N(0, \\upsilon)$", "C) $b = \\Gamma(1 - 2/\\nu)$", "D) $\\upsilon = \\Gamma(1 - 2/\\nu) - b^{2}$"], "category": "CORRECT", "question_context_used": "For $\\tau < \\nu/2$, $\\Sigma y_{i}^{-\\tau}$ has finite variance under $H_{0}$ $\\pmb{\\kappa}=\\mathbf{0}$ and $n^{-\\frac{1}{2}}(\\Sigma y_{i}^{-\\tau} - n b) \\Rightarrow N(0, \\upsilon)$ with $b = \\Gamma(1 - 2/\\nu)$ and $\\pmb{v} = \\Gamma(1 - 2/\\nu) - \\pmb{b}^{2}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A)** Correct. The test statistic sums over $h$ from $-H$ to $H$, as explicitly shown in its definition.\n- **B)** Correct. The definition includes integrals over $\\mathbb{R}$ for both $u$ and $v$.\n- **C)** Incorrect. The weight functions $w(u)$ and $w(v)$ are integral to the definition of $\\widehat{T}_{n,H}^{(w)}$.\n- **D)** Incorrect. The test statistic is defined for both $h > 0$ and $h \\leq 0$, as indicated by the summation from $-H$ to $H$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Fourier-type-tests-of-mutual-independence-between_.md"], "background_text": "The test statistic $\\widehat{T}_{n,H}^{(w)}$ is defined as a sum over $h$ from $-H$ to $H$, involving integrals over $\\mathbb{R}$ for both $u$ and $v$, and includes weight functions $w(u)$ and $w(v)$. It is defined for both positive and negative values of $h$.", "folder": "", "qid": "statistic-mcqm-5800", "question": "Which of the following statements about the test statistic $\\widehat{T}_{n,H}^{(w)}$ are true?", "options": ["A) It involves summing over $h$ from $-H$ to $H$", "B) It includes an integral over $\\mathbb{R}$ for both $u$ and $v$", "C) The weight functions $w(u)$ and $w(v)$ are not necessary for its definition", "D) It is defined only for $h > 0$"], "category": "CORRECT", "question_context_used": "The test statistic $\\widehat{T}_{n,H}^{(w)}$ is defined as a sum over $h$ from $-H$ to $H$, involving integrals over $\\mathbb{R}$ for both $u$ and $v$, and includes weight functions $w(u)$ and $w(v)$. It is defined for both positive and negative values of $h$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The formula for $\\beta^{*}$ is given and it's noted that $\\lambda^{*} \\leqslant \\lambda$.\n- **B)** Correct. The text mentions that misclassification reduces the limiting power.\n- **C)** Correct. $\\lambda^{*}$ involves terms $\\nu_{j}^{\\prime}$ and $p_{o j}^{\\prime}$.\n- **D)** Correct. The limiting power without misclassification is indeed $1-F(C_{1},r-1,\\lambda)$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["An_investigation_of_the_effect_of_misclassification_on_the_properties_of_x,2-tests_in_the_analysis_of_categorical_data.md"], "background_text": "The limiting power $\\beta^{*}$ under misclassification is defined in terms of a non-centrality parameter $\\lambda^{*}$ that is less than or equal to $\\lambda$, the non-centrality parameter without misclassification. This results in a reduction of asymptotic power due to misclassification.", "folder": "", "qid": "statistic-mcqm-3696", "question": "Regarding the limiting power $\\beta^{*}$ in the presence of misclassification, which statements are true?", "options": ["A) $\\beta^{*}=1-F(C_{2},r-2,\\lambda^{*})$, where $\\lambda^{*} \\leqslant \\lambda$.", "B) Misclassification generally reduces the asymptotic power.", "C) $\\lambda^{*}$ is calculated using $\\nu_{j}^{\\prime}$ and $p_{o j}^{\\prime}$.", "D) The limiting power without misclassification is $1-F(C_{1},r-1,\\lambda)$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The limiting power $\\beta^{*}$ under misclassification is defined in terms of a non-centrality parameter $\\lambda^{*}$ that is less than or equal to $\\lambda$, the non-centrality parameter without misclassification. This results in a reduction of asymptotic power due to misclassification.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**:\n- **A) Correct**: This is the correct expression for the $r$-th moment of $Y$ as given in Proposition 5.\n- **B) Correct**: The mean of $Y$ is indeed $\\frac{\\sqrt{2}[\\delta^2 - 1]}{\\sqrt{\\pi}\\delta}$ as derived from the moments.\n- **C) Correct**: The variance of $Y$ is a complex expression involving $\\delta$, as shown in the context.\n- **D) Correct**: The skewness and kurtosis of $Y$ are bounded within specific ranges, as mentioned in Remark 1.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$.", "source": ["On-the-Fern-ndez-Steel-distribution--Infere_2011_C.md"], "background_text": "The moments of the standard FSSN distribution $Y \\sim \\operatorname{FSSN}(\\delta)$ are given by $\\mathbb{E}[Y^r] = \\sqrt{\\frac{2^r}{\\pi}} \\Gamma\\left(\\frac{1 + r}{2}\\right) \\frac{[(-1)^r + \\delta^{2(r+1)}]}{\\delta^r [1 + \\delta^2]}$. The mean of $Y$ is $\\frac{\\sqrt{2}[\\delta^2 - 1]}{\\sqrt{\\pi}\\delta}$, and the variance is a complex expression involving $\\delta$. The skewness and kurtosis of $Y$ are bounded within the ranges [-0.9953, 0.9953] and [3.0000, 3.8692], respectively, similar to the SN distribution.", "folder": "", "qid": "statistic-mcqm-5915", "question": "Which of the following are correct about the moments of the standard FSSN distribution $Y \\sim \\operatorname{FSSN}(\\delta)$?", "options": ["A) The $r$-th moment of $Y$ is given by $\\mathbb{E}[Y^r] = \\sqrt{\\frac{2^r}{\\pi}} \\Gamma\\left(\\frac{1 + r}{2}\\right) \\frac{[(-1)^r + \\delta^{2(r+1)}]}{\\delta^r [1 + \\delta^2]}$.", "B) The mean of $Y$ is $\\frac{\\sqrt{2}[\\delta^2 - 1]}{\\sqrt{\\pi}\\delta}$.", "C) The variance of $Y$ depends on $\\delta$ and is given by a complex expression involving $\\delta$.", "D) The skewness and kurtosis of $Y$ are bounded within specific ranges, similar to the SN distribution."], "category": "CORRECT", "question_context_used": "The moments of the standard FSSN distribution $Y \\sim \\operatorname{FSSN}(\\delta)$ are given by $\\mathbb{E}[Y^r] = \\sqrt{\\frac{2^r}{\\pi}} \\Gamma\\left(\\frac{1 + r}{2}\\right) \\frac{[(-1)^r + \\delta^{2(r+1)}]}{\\delta^r [1 + \\delta^2]}$. The mean of $Y$ is $\\frac{\\sqrt{2}[\\delta^2 - 1]}{\\sqrt{\\pi}\\delta}$, and the variance is a complex expression involving $\\delta$. The skewness and kurtosis of $Y$ are bounded within the ranges [-0.9953, 0.9953] and [3.0000, 3.8692], respectively, similar to the SN distribution.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Step-by-Step Solution**:\n1. **Option A**: Incorrect. The text explicitly mentions that assuming a parametric form for the link function is restrictive and can lead to bias, hence it is desirable to have the link function unspecified.\n2. **Option B**: Incorrect. The varying coefficient functions $\\beta_{k}(\\cdot)$ are not assumed to be known; they are to be estimated.\n3. **Option C**: Correct. The model assumes $E(\\epsilon_{i}\\mid U_{i},X_{i})=0$.\n4. **Option D**: Correct. The text states that $X_{i}$ is $p$-dimensional and $U_{i}$ is a univariate random variable.\n\n**Correct Choices**: C, D\n**Final Answer**: The final answer is $\\boxed{{C, D}}$", "source": ["Generalized_varying_coefficient_models_with_unknown_link_function.md"], "background_text": "The generalized varying coefficient model is given by $Y_{i}=g\\left\\{\\sum_{k=1}^{p}\\beta_{k}(U_{i})X_{i k}\\right\\}+\\epsilon_{i}$ with $E(\\epsilon_{i}\\mid U_{i},X_{i})=0$. Here, $g(\\cdot)$ is the link function, $\\beta_{k}(\\cdot)$ are the varying coefficient functions, $X_{i}$ is a $p$-dimensional covariate, and $U_{i}$ is a univariate random variable called the effect modifier or index variable.", "folder": "", "qid": "statistic-mcqm-1475", "question": "In the generalized varying coefficient model $Y_{i}=g\\left\\{\\sum_{k=1}^{p}\\beta_{k}(U_{i})X_{i k}\\right\\}+\\epsilon_{i}$, which of the following statements are correct?", "options": ["A) The link function $g(\\cdot)$ is assumed to be known.", "B) The varying coefficient functions $\\beta_{k}(\\cdot)$ are assumed to be known.", "C) The error term $\\epsilon_{i}$ has a conditional mean of zero given $U_{i}$ and $X_{i}$.", "D) The covariate $X_{i}$ is $p$-dimensional and $U_{i}$ is a univariate random variable."], "category": "PARTIALLY_CORRECT", "question_context_used": "The generalized varying coefficient model is given by $Y_{i}=g\\left\\{\\sum_{k=1}^{p}\\beta_{k}(U_{i})X_{i k}\\right\\}+\\epsilon_{i}$ with $E(\\epsilon_{i}\\mid U_{i},X_{i})=0$. Here, $g(\\cdot)$ is the link function, $\\beta_{k}(\\cdot)$ are the varying coefficient functions, $X_{i}$ is a $p$-dimensional covariate, and $U_{i}$ is a univariate random variable called the effect modifier or index variable.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A) Conditions A and B hold**: This is partially correct because the posterior is proper only if all three conditions (A, B, and C) hold. However, since the question allows for multiple correct answers, this option is included as part of the correct set.\n- **B) Condition C holds**: This is partially correct for the same reason as option A. Condition C is necessary but not sufficient alone.\n- **C) Conditions A, B, and C hold**: This is fully correct as the posterior is proper if and only if all three conditions are satisfied.\n- **D) Only Condition C holds**: This is incorrect because Conditions A and B are also required for the posterior to be proper.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Fully_Bayesian_spline_smoothing_and_intrinsic_autoregressive_priors.md"], "background_text": "The posterior of $(z,\\delta_{0},\\delta_{1})$ is proper if and only if Conditions A, B and C hold. Condition C requires that $N-p+2a_{0}+2a_{1}>0$.", "folder": "", "qid": "statistic-mcqm-5641", "question": "Under what conditions is the posterior of $(z,\\delta_{0},\\delta_{1})$ proper in model (14) with prior given by (9) and (20)?", "options": ["A) Conditions A and B hold", "B) Condition C holds", "C) Conditions A, B, and C hold", "D) Only Condition C holds"], "category": "PARTIALLY_CORRECT", "question_context_used": "The posterior of $(z,\\delta_{0},\\delta_{1})$ is proper if and only if Conditions A, B and C hold. Condition C requires that $N-p+2a_{0}+2a_{1}>0$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"C\"]\n}", "model_answer": ["C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**: \n- **Option A** is correct because Theorem 2 states that if $d=1$ and $\\mathcal{X}$ contains at least three points, model (4) is identifiable for any family $\\mathcal{F}$.\n- **Option C** is correct because the theorem also states that if $d=1$ and $\\mathcal{X}$ consists of one or two points, model (4) is non-identifiable for any weakly scalable family $\\mathcal{F}$.\n- **Option B** is incorrect because the non-identifiability is specifically for weakly scalable families, not any family $\\mathcal{F}$.\n- **Option D** is incorrect because the theorem clearly shows that the identifiability depends on the number of points in $\\mathcal{X}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Identifiability-of-cure-models-revisite_2014_Journ.md"], "background_text": "Theorem 2 provides specific conditions for the identifiability of model (4) when $d=1$. It states that the identifiability depends on the number of points in $\\mathcal{X}$ and the scalability of the family $\\mathcal{F}$.", "folder": "", "qid": "statistic-mcqm-1945", "question": "Which of the following statements about the identifiability of model (4) with $d=1$ are correct?", "options": ["A) If $\\mathcal{X}$ contains at least three points, model (4) is identifiable for any family $\\mathcal{F}$", "B) If $\\mathcal{X}$ consists of one or two points, model (4) is non-identifiable for any family $\\mathcal{F}$", "C) If $\\mathcal{X}$ consists of one or two points, model (4) is non-identifiable for any weakly scalable family $\\mathcal{F}$", "D) The identifiability of model (4) does not depend on the number of points in $\\mathcal{X}$"], "category": "PARTIALLY_CORRECT", "question_context_used": "Theorem 2 provides specific conditions for the identifiability of model (4) when $d=1$. It states that the identifiability depends on the number of points in $\\mathcal{X}$ and the scalability of the family $\\mathcal{F}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A) Correct**: The text states that $\\mathbf{X}$ is a $k^{n} \\times k^{n}$ orthogonal matrix, as $\\mathbf{X}^{\\prime}\\mathbf{X}=\\mathbf{D}$.\n- **B) Correct**: The design matrix for the full replicate is explicitly given as $[\\mathbf{X}_{k} \\otimes \\mathbf{X}_{k} \\otimes \\ldots \\otimes \\mathbf{X}_{k}]/k$.\n- **C) Incorrect**: $\\mathbf{c}_{1}$ is a vector of plusses, not minuses.\n- **D) Correct**: The Kronecker product is used to generalize the design matrix from $2^{n}$ to $k^{n}$ factorials.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["A Unified Approach for Constructing a Useful Class.md"], "background_text": "The design matrix $\\mathbf{X}$ for a full replicate of a $k^{n}$ factorial is constructed using the Kronecker product of $\\mathbf{X}_{k}$, which is a matrix with $\\mathbf{c}_{1}$ as a $k$ column vector of plusses and other orthogonal $k$-column vectors. This matrix is orthogonal in the sense that $\\mathbf{X}^{\\prime}\\mathbf{X}=\\mathbf{D}$, where $\\mathbf{D}$ is a diagonal matrix.", "folder": "", "qid": "statistic-mcqm-5515", "question": "In the context of constructing non-orthogonal unsaturated main effect plans for a $k^{n}$ factorial, which of the following statements are true about the design matrix $\\mathbf{X}$?", "options": ["A) $\\mathbf{X}$ is a $k^{n} \\times k^{n}$ orthogonal matrix.", "B) The design matrix for a full replicate of the $k^{n}$ factorial is given by $[\\mathbf{X}_{k} \\otimes \\mathbf{X}_{k} \\otimes \\ldots \\otimes \\mathbf{X}_{k}]/k$.", "C) $\\mathbf{X}_{k}$ is a matrix where $\\mathbf{c}_{1}$ is a $k$ column vector of minuses and the other $\\mathbf{c}$'s are orthogonal $k$-column vectors.", "D) The Kronecker product is used to extend the design matrix from a $2^{n}$ factorial to a general $k^{n}$ factorial."], "category": "PARTIALLY_CORRECT", "question_context_used": "The design matrix $\\mathbf{X}$ for a full replicate of a $k^{n}$ factorial is constructed using the Kronecker product of $\\mathbf{X}_{k}$, which is a matrix with $\\mathbf{c}_{1}$ as a $k$ column vector of plusses and other orthogonal $k$-column vectors. This matrix is orthogonal in the sense that $\\mathbf{X}^{\\prime}\\mathbf{X}=\\mathbf{D}$, where $\\mathbf{D}$ is a diagonal matrix.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}", "model_answer": ["B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: This is the definition of identifiability provided in the context.\n- **B) Correct**: The context provides this exact formula for $p(u,\\theta)$.\n- **C) Correct**: The set of covariates $\\mathcal{U}$ is indeed a subset of $\\mathbb{R}^{d}$.\n- **D) Incorrect**: The parameters $a$ and $b$ are part of the model and are related to the covariates $u$ through the function $p(u,\\theta)$.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["Identifiability-of-cure-models-revisite_2014_Journ.md"], "background_text": "In the logistic regression setting, the function $p(u,\\theta)$ is given by $\\frac{\\exp(a+b\\cdot u)}{1+\\exp(a+b\\cdot u)}$, where $a\\in\\mathbb R$ and $b\\in\\mathbb{R}^{d}$. The set of covariates $\\mathcal{U}$ is a subset of $\\mathbb{R}^{d}$. The model is identifiable if $p(u,\\theta_{1})=p(u,\\theta_{2})$ for all $u$ implies $\\theta_{1}=\\theta_{2}$.", "folder": "", "qid": "statistic-mcqm-1910", "question": "Regarding the logistic regression setting for the function $p(u,\\theta)$, which of the following statements are correct?", "options": ["A) The model is identifiable if $p(u,\\theta_{1})=p(u,\\theta_{2})$ for all $u$ implies $\\theta_{1}=\\theta_{2}$.", "B) The function $p(u,\\theta)$ is given by $\\frac{\\exp(a+b\\cdot u)}{1+\\exp(a+b\\cdot u)}$.", "C) The set of covariates $\\mathcal{U}$ for function $p$ is a subset of $\\mathbb{R}^{d}$.", "D) The parameters $a$ and $b$ are independent of the covariates $u$."], "category": "INCORRECT", "question_context_used": "In the logistic regression setting, the function $p(u,\\theta)$ is given by $\\frac{\\exp(a+b\\cdot u)}{1+\\exp(a+b\\cdot u)}$, where $a\\in\\mathbb R$ and $b\\in\\mathbb{R}^{d}$. The set of covariates $\\mathcal{U}$ is a subset of $\\mathbb{R}^{d}$. The model is identifiable if $p(u,\\theta_{1})=p(u,\\theta_{2})$ for all $u$ implies $\\theta_{1}=\\theta_{2}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The text explicitly states the form of the weights $w_j$.\n- **B)** Correct. The text mentions that mean square consistency results under the given conditions on $\\lambda$.\n- **C)** Correct. The text states that $w_j \\to 1$ for each fixed $j$ as $n \\to \\infty$.\n- **D)** Incorrect. The weights $w_j$ are directly dependent on the smoothing parameter $\\lambda$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Common Structure of Techniques for Choosing Smooth.md"], "background_text": "In the context of periodic smoothing splines, Wahba's weights are given by $w_j = (1 + \\lambda j^p)^{-1}$ for fixed $p > 1$ and $\\lambda > 0$. Mean square consistency results if $\\lambda = \\lambda(n) \\to 0$ sufficiently slowly for $\\lambda n^p \\to \\infty$. In this situation, $w_j \\to 1$ for each fixed $j$, and $w_j \\simeq 0$ for large $j$.", "folder": "", "qid": "statistic-mcqm-5272", "question": "In the context of periodic smoothing splines, which of the following statements are true about the weights $w_j$?", "options": ["A) The weights $w_j$ are given by $(1 + \\lambda j^p)^{-1}$ for fixed $p > 1$ and $\\lambda > 0$.", "B) Mean square consistency results if $\\lambda = \\lambda(n) \\to 0$ sufficiently slowly for $\\lambda n^p \\to \\infty$.", "C) The weights $w_j$ approach 1 for each fixed $j$ as $n \\to \\infty$.", "D) The weights $w_j$ are independent of the smoothing parameter $\\lambda$."], "category": "CORRECT", "question_context_used": "In the context of periodic smoothing splines, Wahba's weights are given by $w_j = (1 + \\lambda j^p)^{-1}$ for fixed $p > 1$ and $\\lambda > 0$. Mean square consistency results if $\\lambda = \\lambda(n) \\to 0$ sufficiently slowly for $\\lambda n^p \\to \\infty$. In this situation, $w_j \\to 1$ for each fixed $j$, and $w_j \\simeq 0$ for large $j$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n- **A)** Correct. The text states that equality holds if $A_{22}^{-1}A_{21} = B_{22}^{-1}B_{21}$.\n- **B)** Incorrect. While invertibility of $A_{22} + B_{22}$ is a condition for the inequality, it is not sufficient for equality.\n- **C)** Incorrect. The condition $A_{22} > 0$ and $B_{22} > 0$ ensures the inequality holds but does not guarantee equality.\n- **D)** Correct. The text states that equality holds if $(A_{22} + B_{22})^{-1}(A_{21} + B_{21}) = A_{22}^{-1}A_{21}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Information-on-parameters-of-interest-decreases-_2.md"], "background_text": "For symmetric matrices $A$ and $B$ partitioned conformably, the inequality $(A + B)_{11\\cdot2} \\geq A_{11\\cdot2} + B_{11\\cdot2}$ holds under certain conditions, with equality if and only if specific conditions involving the submatrices are met.", "folder": "", "qid": "statistic-mcqm-6068", "question": "Under what conditions does equality hold in the inequality $(A + B)_{11\\cdot2} \\geq A_{11\\cdot2} + B_{11\\cdot2}$ for symmetric matrices $A$ and $B$?", "options": ["A) When $A_{22}^{-1}A_{21} = B_{22}^{-1}B_{21}$.", "B) When $A_{22} + B_{22}$ is invertible.", "C) When $A_{22} > 0$ and $B_{22} > 0$.", "D) When $(A_{22} + B_{22})^{-1}(A_{21} + B_{21}) = A_{22}^{-1}A_{21}$."], "category": "CORRECT", "question_context_used": "For symmetric matrices $A$ and $B$ partitioned conformably, the inequality $(A + B)_{11\\cdot2} \\geq A_{11\\cdot2} + B_{11\\cdot2}$ holds under certain conditions, with equality if and only if specific conditions involving the submatrices are met.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\"answer\": [\"A\"]}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **Option A** is correct as it directly quotes the definition of a Schur convex function from the context.\n- **Option B** is correct because Theorem 3.2 states that the survival function of $D_{2:n}$ is Schur convex in $(\\lambda_{1},...,\\lambda_{n})$ for any $n$.\n- **Option C** is incorrect because the context provides an example showing that the survival function of $D_{3:3}$ is not Schur convex.\n- **Option D** is incorrect because Example 3.2 shows that the hazard rate of $D_{2:n}$ is not Schur concave for $n=3$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Stochastic-Orders-for-Spacings-of-Heterogeneous-Ex.md"], "background_text": "A real-valued function $\\phi$ defined on a set $\\mathcal{A}\\subset\\mathcal{R}^{n}$ is said to be Schur convex (Schur concave) on $\\mathcal{A}$ if $\\mathbf{x}\\{\\mathbf{\\sigma}^{\\infty}\\mathbf{y}\\Rightarrow\\phi(\\mathbf{x})\\leqslant\\mathbf{\\sigma}(\\geqslant)\\phi(\\mathbf{y})$. The survival function of $D_{2:n}$ is Schur convex in $(\\lambda_{1},...,\\lambda_{n})$ for any $n$, but the survival function of $D_{3:3}$ is not Schur convex. The hazard rate of $D_{2:n}$ is not Schur concave for $n=3$.", "folder": "", "qid": "statistic-mcqm-5859", "question": "Which of the following statements are true regarding Schur convex functions based on the provided context?", "options": ["A) A real-valued function $\\phi$ defined on a set $\\mathcal{A}\\subset\\mathcal{R}^{n}$ is Schur convex on $\\mathcal{A}$ if $\\mathbf{x}\\{\\mathbf{\\sigma}^{\\infty}\\mathbf{y}\\Rightarrow\\phi(\\mathbf{x})\\leqslant\\mathbf{\\sigma}(\\geqslant)\\phi(\\mathbf{y})$.", "B) The survival function of $D_{2:n}$ is Schur convex in $(\\lambda_{1},...,\\lambda_{n})$ for any $n$.", "C) The survival function of $D_{3:3}$ is Schur convex in $(\\lambda_{1},...,\\lambda_{n})$ for any $n$.", "D) The hazard rate of $D_{2:n}$ is Schur concave for $n=3$."], "category": "CORRECT", "question_context_used": "A real-valued function $\\phi$ defined on a set $\\mathcal{A}\\subset\\mathcal{R}^{n}$ is said to be Schur convex (Schur concave) on $\\mathcal{A}$ if $\\mathbf{x}\\{\\mathbf{\\sigma}^{\\infty}\\mathbf{y}\\Rightarrow\\phi(\\mathbf{x})\\leqslant\\mathbf{\\sigma}(\\geqslant)\\phi(\\mathbf{y})$. The survival function of $D_{2:n}$ is Schur convex in $(\\lambda_{1},...,\\lambda_{n})$ for any $n$, but the survival function of $D_{3:3}$ is not Schur convex. The hazard rate of $D_{2:n}$ is not Schur concave for $n=3$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\"]\n}", "model_answer": ["B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**: \n- **Option A** is correct because Theorem 1 explicitly states that $\\widehat{\\beta}$ is a root-$NT$ consistent estimator of $\\beta$.\n- **Option B** is correct because the text mentions that the asymptotic variance of $\\widehat{\\beta}$ depends on the particular specification of $\\varOmega(Z)$.\n- **Option C** is correct because Theorem 2 shows that $\\widehat{m}_{i}(\\cdot;H_{1})$ has a rate of convergence of order $\\sqrt{T|H_{1}|}$ regardless of the rank condition.\n- **Option D** is correct because Theorem 3 indicates that the rate of convergence of $\\widehat{\\overline{m}}(\\cdot;H_{1})$ depends on the rate of increase of $\\nu_{N}(z)$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Efficient-estimation-of-a-partially-linear-panel-d.md"], "background_text": "Theorem 1 shows that $\\widehat{\\beta}$ is a root-$NT$ consistent estimator of $\\beta$. The asymptotic variance depends on $\\varOmega(Z)$. Theorem 2 shows that $\\widehat{m}_{i}(\\cdot;H_{1})$ has a rate of convergence of order $\\sqrt{T|H_{1}|}$. Theorem 3 shows that the rate of convergence of $\\widehat{\\overline{m}}(\\cdot;H_{1})$ depends on $\\nu_{N}(z)$.", "folder": "", "qid": "statistic-mcqm-2304", "question": "Which of the following are correct about the asymptotic properties of the estimators discussed in the text?", "options": ["A) $\\widehat{\\beta}$ is a root-$NT$ consistent estimator of $\\beta$ in the presence of unobserved common factors.", "B) The asymptotic variance of $\\widehat{\\beta}$ depends on the particular specification of $\\varOmega(Z)$.", "C) $\\widehat{m}_{i}(\\cdot;H_{1})$ exhibits a rate of convergence of order $\\sqrt{T|H_{1}|}$ regardless of the rank condition.", "D) The rate of convergence of $\\widehat{\\overline{m}}(\\cdot;H_{1})$ depends on the rate of increase of $\\nu_{N}(z)$."], "category": "PARTIALLY_CORRECT", "question_context_used": "Theorem 1 shows that $\\widehat{\\beta}$ is a root-$NT$ consistent estimator of $\\beta$. The asymptotic variance depends on $\\varOmega(Z)$. Theorem 2 shows that $\\widehat{m}_{i}(\\cdot;H_{1})$ has a rate of convergence of order $\\sqrt{T|H_{1}|}$. Theorem 3 shows that the rate of convergence of $\\widehat{\\overline{m}}(\\cdot;H_{1})$ depends on $\\nu_{N}(z)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **A** is correct because $w(x_{2i})$ is indeed defined to be 1 when $x_{2i}$ is within the risk window $(c_{1}, c_{2})$.\n- **B** is correct because $w(x_{2i})$ is part of the interaction term $\\beta_{3}x_{1i}w(x_{2i})$ in the logistic regression model, which models the interaction between treatment ($x_{1i}$) and the risk window.\n- **C** is incorrect because $w(x_{2i})$ is a binary indicator variable, not a continuous variable.\n- **D** is incorrect because $w(x_{2i})$ is explicitly included in the logistic regression model.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["A-Bayesian-method-for-risk-window-estimation-wit_2.md"], "background_text": "The indicator variable $w(x_{2i})$ is defined as 1 if the marker factor $x_{2i}$ (e.g., time from vaccination to conception) is within the risk window $(c_{1}, c_{2})$, and 0 otherwise. It is used in the logistic regression model to account for the effect of being within the risk window.", "folder": "", "qid": "statistic-mcqm-835", "question": "In the Bayesian logistic model described, which of the following statements are true about the indicator variable $w(x_{2i})$?", "options": ["A) $w(x_{2i})$ equals 1 if the marker factor $x_{2i}$ is within the risk window $(c_{1}, c_{2})$.", "B) $w(x_{2i})$ is used to model the interaction between treatment and the risk window.", "C) $w(x_{2i})$ is a continuous variable representing the time from vaccination to conception.", "D) $w(x_{2i})$ is not involved in the logistic regression model."], "category": "PARTIALLY_CORRECT", "question_context_used": "The indicator variable $w(x_{2i})$ is defined as 1 if the marker factor $x_{2i}$ (e.g., time from vaccination to conception) is within the risk window $(c_{1}, c_{2})$, and 0 otherwise. It is used in the logistic regression model to account for the effect of being within the risk window.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The rate is $r/2$ for a proposal distribution with bounded support.\n- **B) Correct**: The rate is $r/\\eta$ for a heavy-tailed proposal distribution with $0<\\eta<2$ degrees of freedom.\n- **C) Incorrect**: The drift function $V(x)=(\\pi(x))^{-\\nu}$ leads to a suboptimal result.\n- **D) Correct**: The level sets are ellipsoids of the same shape.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Scandinavian J Statistics - 2007 - JARNER - Conver.md"], "background_text": "For the multivariate $t$-distribution as a target distribution, the polynomial rate of convergence in total variation is $r/2$ for a proposal distribution with bounded support and $r/\\eta$ for a heavy-tailed proposal distribution with $0<\\eta<2$ degrees of freedom. The level sets are ellipsoids of the same shape. The drift function $V(x)=(\\pi(x))^{-\\nu}$ leads to a suboptimal result.", "folder": "", "qid": "statistic-mcqm-5505", "question": "Which of the following are true about the multivariate $t$-distribution as a target distribution for the random-walk Metropolis algorithm?", "options": ["A) The polynomial rate of convergence in total variation is $r/2$ for a proposal distribution with bounded support.", "B) The polynomial rate of convergence in total variation is $r/\\eta$ for a heavy-tailed proposal distribution with $0<\\eta<2$ degrees of freedom.", "C) The drift function $V(x)=(\\pi(x))^{-\\nu}$ leads to an optimal result.", "D) The level sets of the multivariate $t$-distribution are ellipsoids of the same shape."], "category": "CORRECT", "question_context_used": "For the multivariate $t$-distribution as a target distribution, the polynomial rate of convergence in total variation is $r/2$ for a proposal distribution with bounded support and $r/\\eta$ for a heavy-tailed proposal distribution with $0<\\eta<2$ degrees of freedom. The level sets are ellipsoids of the same shape. The drift function $V(x)=(\\pi(x))^{-\\nu}$ leads to a suboptimal result.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A** is correct because Theorem 2.1 explicitly states that $D_{1}$ has an exponential distribution with scale parameter $\\sum_{i=1}^{n}\\lambda_{i}/n$.\n- **B** is correct because Theorem 2.1 also states that $D_{1}$ is independent of $(D_{2},...,D_{n})$.\n- **C** is correct because Theorem 2.2 states that $D_{i}$ has a log-convex density for $i=1,...,n$, which includes $D_{1}$.\n- **D** is incorrect because the distribution of $D_{1}$ is not described as a mixture of independent exponential random variables in the provided context.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Stochastic-Orders-for-Spacings-of-Heterogeneous-Ex.md"], "background_text": "Theorem 2.1 states that $D_{1}$ has an exponential distribution with scale parameter $\\sum_{i=1}^{n}\\lambda_{i}/n$ and is independent of $(D_{2},...,D_{n})$. Theorem 2.2 mentions that $D_{i}$ has a log-convex density for $i=1,...,n$.", "folder": "", "qid": "statistic-mcqm-5822", "question": "Let $X_{1},...,X_{n}$ be independent random variables with $X_{i}$ having the exponential distribution with survival function $\\overline{{F}}_{i}(t)=\\exp(-\\lambda_{i}t)$, $t\\geqslant0$, for $i=1,...,n$. Which of the following statements about $D_{1}$ are correct?", "options": ["A) $D_{1}$ has an exponential distribution with scale parameter $\\sum_{i=1}^{n}\\lambda_{i}/n$.", "B) $D_{1}$ is independent of $(D_{2},...,D_{n})$.", "C) $D_{1}$ has a log-convex density.", "D) The distribution of $D_{1}$ is a mixture of independent exponential random variables."], "category": "CORRECT", "question_context_used": "Theorem 2.1 states that $D_{1}$ has an exponential distribution with scale parameter $\\sum_{i=1}^{n}\\lambda_{i}/n$ and is independent of $(D_{2},...,D_{n})$. Theorem 2.2 mentions that $D_{i}$ has a log-convex density for $i=1,...,n$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **A) Correct**: The E-step involves calculating the expected value of the latent outcomes given the observed data, as described in the text.\n- **B) Correct**: The M-step involves maximizing the log likelihood over the parameter space $\\Theta(\\mathbf{r})$.\n- **C) Correct**: The EM algorithm requires cycling through all possible choices of reference vectors $\\mathbf{r}$ to maximize the log likelihood over $\\Theta$.\n- **D) Incorrect**: While the EM algorithm does not always guarantee convergence to the global maximum, the text describes a method to ensure the global maximum is found by cycling through all possible reference vectors.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$.", "source": ["Relative risk regression reliable and flexible met.md"], "background_text": "The EM algorithm involves an E-step where the expected value of the latent outcomes given the observed data is calculated, and an M-step where the log likelihood is maximized over the parameter space $\\Theta(\\mathbf{r})$. To maximize the log likelihood over the full parameter space $\\Theta$, the EM algorithm must be run for each possible choice of reference vector $\\mathbf{r}$, and the limit points inspected to determine the maximum.", "folder": "", "qid": "statistic-mcqm-4623", "question": "In the EM algorithm for the log-binomial model, which of the following statements are true?", "options": ["A) The E-step involves calculating the expected value of the latent outcomes given the observed data.", "B) The M-step involves maximizing the log likelihood over the parameter space $\\Theta(\\mathbf{r})$.", "C) The EM algorithm requires cycling through all possible choices of reference vectors $\\mathbf{r}$ to maximize the log likelihood over $\\Theta$.", "D) The EM algorithm does not guarantee convergence to the global maximum of the log likelihood."], "category": "PARTIALLY_CORRECT", "question_context_used": "The EM algorithm involves an E-step where the expected value of the latent outcomes given the observed data is calculated, and an M-step where the log likelihood is maximized over the parameter space $\\Theta(\\mathbf{r})$. To maximize the log likelihood over the full parameter space $\\Theta$, the EM algorithm must be run for each possible choice of reference vector $\\mathbf{r}$, and the limit points inspected to determine the maximum.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C, D\n\n**Explanation**:\n- **A)** Incorrect. The matrix is not strictly diagonal because of the submatrix for $\\varphi_{1},\\ldots,\\varphi_{p}$.\n- **B)** Correct. This is the correct entry for the mean $\\mu$.\n- **C)** Correct. This is the correct entry for the variance $\\sigma^{2}$.\n- **D)** Correct. The submatrix for the autoregressive coefficients is indeed $\\frac{1}{\\sigma^{2}}\\Gamma$.\n\n**Final Answer**: The final answer is $\\boxed{{B, C, D}}$", "source": ["Change-detection-in-autoregressive-time-se_2008_Jo.md"], "background_text": "The information matrix $I(\\xi)$ for an autoregressive model under the Gaussian assumption is a $(p+2) \\times (p+2)$ matrix with specific entries for $\\mu$, $\\sigma^{2}$, and the autoregressive coefficients $\\varphi_{1},\\ldots,\\varphi_{p}$. It is not strictly diagonal due to the structure of the submatrix for $\\varphi_{1},\\ldots,\\varphi_{p}$.", "folder": "", "qid": "statistic-mcqm-3064", "question": "What are the properties of the information matrix $I(\\xi)$ for an autoregressive model under the Gaussian assumption?", "options": ["A) It is a $(p+2) \\times (p+2)$ diagonal matrix.", "B) The entry corresponding to $\\mu$ is $\\left(1-\\sum_{j=1}^{p}\\varphi_{j}\\right)^{2}$.", "C) The entry corresponding to $\\sigma^{2}$ is $\\frac{1}{2\\sigma^{4}}$.", "D) The submatrix corresponding to $\\varphi_{1},\\ldots,\\varphi_{p}$ is $\\frac{1}{\\sigma^{2}}\\Gamma$, where $\\Gamma$ is the covariance matrix of $(Y_{1},\\ldots,Y_{p})$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The information matrix $I(\\xi)$ for an autoregressive model under the Gaussian assumption is a $(p+2) \\times (p+2)$ matrix with specific entries for $\\mu$, $\\sigma^{2}$, and the autoregressive coefficients $\\varphi_{1},\\ldots,\\varphi_{p}$. It is not strictly diagonal due to the structure of the submatrix for $\\varphi_{1},\\ldots,\\varphi_{p}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}", "model_answer": ["B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: Both equations include terms for $x_2$, $x_2^2$, and $x_2^3$, indicating a cubic polynomial relationship between temperature and longitude.\n- **B) Correct**: The term for $x_1$ is linear in both equations, confirming a linear relationship between temperature and latitude.\n- **C) Correct**: The coefficients for $x_2^3$ are 12.4537 (CLS) and 12.9559 (MCC), both positive.\n- **D) Incorrect**: The coefficients for $x_1$ are -2.5613 (CLS) and -2.3577 (MCC), which are not the same.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["Semi-parametric-nonlinear-regression-and-transf_20.md"], "background_text": "The study assumes a linear relationship between temperature and latitude, and a cubic polynomial in longitude for predicting temperature. Two models are provided: one using the CLS criterion and another using the MCC criterion, both showing similar scatter plots and significant coefficients.", "folder": "", "qid": "statistic-mcqm-5165", "question": "Based on the provided equations for the CLS and MCC criteria, which of the following statements are true about the relationship between temperature ($y$), latitude ($x_1$), and longitude ($x_2$)?", "options": ["A) The relationship between temperature and longitude is modeled as a cubic polynomial.", "B) The relationship between temperature and latitude is linear.", "C) The coefficient for $x_2^3$ is positive in both models.", "D) The coefficient for $x_1$ is the same in both models."], "category": "PARTIALLY_CORRECT", "question_context_used": "The study assumes a linear relationship between temperature and latitude, and a cubic polynomial in longitude for predicting temperature. Two models are provided: one using the CLS criterion and another using the MCC criterion, both showing similar scatter plots and significant coefficients.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **Option A** is correct because Theorem 2 states that the convergence of $a_{n}$ to 0 must be sufficiently slow for the estimator $\\widehat{u\\nu_{0}}$ to converge to $u\\nu_{0}$.\n- **Option B** is correct because the consistency of $\\widehat{u\\nu_{1}}$ in quadratic mean is a necessary condition for the convergence of $\\widehat{u\\nu_{0}}$.\n- **Option C** is incorrect because the boundedness of $\\mu_{f}$ is not required; instead, the function $1/\\mu_{f,n}$ is bounded by construction.\n- **Option D** is incorrect because the integrability condition (7) is necessary for the operator $\\mathcal{G}$ to be bounded but is not directly related to the convergence of the estimator.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$.", "source": ["Scandinavian J Statistics - 2021 - Glück - A solut.md"], "background_text": "The estimator $\\widehat{u\\nu_{0}}$ is defined by $\\widehat{u\\nu_{0}} := \\mathcal{M}^{-1}\\mathcal{F}_{\\times}^{-1}\\left(\\frac{1}{\\mu_{f,n}}\\mathcal{F}_{\\times}\\mathcal{M}\\widehat{u\\nu_{1}}\\right)$ for all $n \\in \\mathbb{N}$, where $\\frac{1}{\\mu_{f,n}} := \\frac{1}{\\mu_{f}}\\mathbb{1}_{\\{|\\mu_{f}| > a_{n}\\}}$. Theorem 2 states that the estimator $\\widehat{u\\nu_{0}}$ converges to $u\\nu_{0}$ as $n \\to \\infty$ if the estimator $\\widehat{u\\nu_{1}}$ for $u\\nu_{1}$ is consistent in quadratic mean and the convergence of $a_{n}$ to 0 is sufficiently slow.", "folder": "", "qid": "statistic-mcqm-5289", "question": "Under which conditions does the estimator $\\widehat{u\\nu_{0}}$ for $u\\nu_{0}$ converge to $u\\nu_{0}$ as $n \\to \\infty$?", "options": ["A) The sequence $(a_{n})_{n\\in\\mathbb{N}}$ converges to 0 sufficiently slowly.", "B) The estimator $\\widehat{u\\nu_{1}}$ for $u\\nu_{1}$ is consistent in quadratic mean.", "C) The function $\\mu_{f}$ is bounded.", "D) The integrability condition (7) is satisfied."], "category": "CORRECT", "question_context_used": "The estimator $\\widehat{u\\nu_{0}}$ is defined by $\\widehat{u\\nu_{0}} := \\mathcal{M}^{-1}\\mathcal{F}_{\\times}^{-1}\\left(\\frac{1}{\\mu_{f,n}}\\mathcal{F}_{\\times}\\mathcal{M}\\widehat{u\\nu_{1}}\\right)$ for all $n \\in \\mathbb{N}$, where $\\frac{1}{\\mu_{f,n}} := \\frac{1}{\\mu_{f}}\\mathbb{1}_{\\{|\\mu_{f}| > a_{n}\\}}$. Theorem 2 states that the estimator $\\widehat{u\\nu_{0}}$ converges to $u\\nu_{0}$ as $n \\to \\infty$ if the estimator $\\widehat{u\\nu_{1}}$ for $u\\nu_{1}$ is consistent in quadratic mean and the convergence of $a_{n}$ to 0 is sufficiently slow.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **A)** Correct. For $i<K-1$, $\\beta^{(i)}(S_{n_{i}})$ includes the cost of sampling for the next stage, as indicated by the term $(n_{i+1}-n_{i})\\sum_{\\mu\\in{\\cal M}}c(\\mu)p^{(i)}(\\mu|S_{n_{i}})$.\n- **B)** Incorrect. For $i=K-1$, $\\beta^{(i)}(S_{n_{i}})$ still includes the cost of sampling for the next stage, as shown by the term $(n_{K}-n_{K-1})\\sum_{\\mu\\in{\\cal M}}c(\\mu)p^{(K-1)}(\\mu|S_{n_{K-1}})$.\n- **C)** Correct. The integral in the expression for $\\beta^{(i)}(S_{n_{i}})$ represents the expected minimum loss from continuing to sample or stopping at the next stage, as it involves $\\min\\{\\beta^{(i+1)}(S_{n_{i+1}}),\\gamma^{(i+1)}(S_{n_{i+1}})\\}$.\n- **D)** Incorrect. The functions $\\beta^{(i)}(S_{n_{i}})$ are calculated recursively starting at $i=K-1$, not from $i=1$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["An_improved_method_for_deriving_optimal_one-sided_group_sequential_tests.md"], "background_text": "The expected additional loss from continuing to sample at stage $i$ and then proceeding optimally is denoted by $\\beta^{(i)}(S_{n_{i}})$. For $i<K-1$, it includes the cost of sampling for the next stage and the expected minimum loss from continuing to sample or stopping at the next stage. For $i=K-1$, it includes the cost of sampling for the next stage and the integral of the minimum loss from stopping at the next stage. The functions $\\beta^{(i)}(S_{n_{i}})$ are calculated recursively starting at $i=K-1$.", "folder": "", "qid": "statistic-mcqm-5584", "question": "Regarding the expected additional loss from continuing to sample at stage $i$, $\\beta^{(i)}(S_{n_{i}})$, which of the following statements are true?", "options": ["A) For $i<K-1$, $\\beta^{(i)}(S_{n_{i}})$ includes the cost of sampling for the next stage.", "B) For $i=K-1$, $\\beta^{(i)}(S_{n_{i}})$ does not include the cost of sampling for the next stage.", "C) The integral in the expression for $\\beta^{(i)}(S_{n_{i}})$ represents the expected minimum loss from continuing to sample or stopping at the next stage.", "D) The functions $\\beta^{(i)}(S_{n_{i}})$ are calculated starting from $i=1$ to $i=K-1$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The expected additional loss from continuing to sample at stage $i$ and then proceeding optimally is denoted by $\\beta^{(i)}(S_{n_{i}})$. For $i<K-1$, it includes the cost of sampling for the next stage and the expected minimum loss from continuing to sample or stopping at the next stage. For $i=K-1$, it includes the cost of sampling for the next stage and the integral of the minimum loss from stopping at the next stage. The functions $\\beta^{(i)}(S_{n_{i}})$ are calculated recursively starting at $i=K-1$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A)** Correct. The definition of RERI* is indeed $\\mathrm{RR}_{11}^{*}-\\mathrm{RR}_{10}^{*}-\\mathrm{RR}_{01}^{*}+1$.\n- **B)** Correct. When the outcome is rare, risk ratios can be approximated by odds ratios, as mentioned in the text.\n- **C)** Correct. The text explicitly states that conclusions about statistical and causal interaction can be drawn from misclassified data using RERI*.\n- **D)** Incorrect. The text mentions that it is straightforward to obtain confidence intervals for RERI*, contradicting this option.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Inference_for_additive_interaction_under_exposure_misclassification.md"], "background_text": "The relative excess risk due to interaction (RERI*) is defined as $\\mathrm{RR}_{11}^{*}-\\mathrm{RR}_{10}^{*}-\\mathrm{RR}_{01}^{*}+1$ where $\\mathrm{RR}_{x_{1}x_{2}}^{*}=p_{x_{1}x_{2}}^{*}/p_{00}^{*}$. For rare outcomes, these risk ratios can be approximated by odds ratios. Confidence intervals for RERI* can be obtained, and it can be used to draw conclusions about various forms of statistical and causal interaction from misclassified data.", "folder": "", "qid": "statistic-mcqm-6175", "question": "In the context of assessing additive interaction with case-control data, which of the following statements about the relative excess risk due to interaction (RERI*) are correct?", "options": ["A) RERI* is defined as $\\mathrm{RR}_{11}^{*}-\\mathrm{RR}_{10}^{*}-\\mathrm{RR}_{01}^{*}+1$.", "B) When the outcome is rare, risk ratios can be approximated by odds ratios.", "C) RERI* can be used to draw conclusions about statistical and causal interaction from misclassified data.", "D) Confidence intervals for RERI* cannot be obtained."], "category": "CORRECT", "question_context_used": "The relative excess risk due to interaction (RERI*) is defined as $\\mathrm{RR}_{11}^{*}-\\mathrm{RR}_{10}^{*}-\\mathrm{RR}_{01}^{*}+1$ where $\\mathrm{RR}_{x_{1}x_{2}}^{*}=p_{x_{1}x_{2}}^{*}/p_{00}^{*}$. For rare outcomes, these risk ratios can be approximated by odds ratios. Confidence intervals for RERI* can be obtained, and it can be used to draw conclusions about various forms of statistical and causal interaction from misclassified data.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **Option A** is correct because it directly represents the probability of observing an animal that dies naturally and does not have the tumour, as derived in the context.\n- **Option B** is correct because it represents the probability for animals that are sacrificed and do not have the tumour, also derived in the context.\n- **Option C** is incorrect because it represents the probability for animals that die naturally and have the tumour, not those without the tumour.\n- **Option D** is incorrect because it represents the probability for animals that are sacrificed and have the tumour, not those without the tumour.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Estimating_the_tumour_onset_distribution_in_animal_carcinogenesis_experiments.md"], "background_text": "The probability of observing an animal which dies naturally and does not have the tumour is given by $\\phi_{i}^{-}P_{i-1}\\Phi_{i-1}^{-}\\{1-F(t_{i};\\theta)\\}$. For animals which are sacrificed and do not have the tumour, the probability is $\\pi_{i}P_{i-1}\\Phi_{i}^{-}\\{1-F(t_{i};\\theta)\\}$. For animals which die naturally and have the tumour, the probability is $P_{i-1}\\bigg(\\sum_{j=1}^{i}\\Delta_{j}(\\theta)\\phi_{i}^{+}\\Phi_{j-1}^{-}\\Phi_{j,i-1}^{+}\\bigg)$. For animals sacrificed and have the tumour, the probability is $\\pi_{i}P_{i-1}\\bigg(\\sum_{j=1}^{i}\\Delta_{j}(\\theta)\\Phi_{j-1}^{-}\\Phi_{j,i}^{+}\\bigg)$.", "folder": "", "qid": "statistic-mcqm-5094", "question": "In the Markov model, the probability of observing an animal which dies naturally and does not have the tumour at time $t_i$ is given by which of the following expressions?", "options": ["A) $\\phi_{i}^{-}P_{i-1}\\Phi_{i-1}^{-}\\{1-F(t_{i};\\theta)\\}$", "B) $\\pi_{i}P_{i-1}\\Phi_{i}^{-}\\{1-F(t_{i};\\theta)\\}$", "C) $P_{i-1}\\bigg(\\sum_{j=1}^{i}\\Delta_{j}(\\theta)\\phi_{i}^{+}\\Phi_{j-1}^{-}\\Phi_{j,i-1}^{+}\\bigg)$", "D) $\\pi_{i}P_{i-1}\\bigg(\\sum_{j=1}^{i}\\Delta_{j}(\\theta)\\Phi_{j-1}^{-}\\Phi_{j,i}^{+}\\bigg)$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The probability of observing an animal which dies naturally and does not have the tumour is given by $\\phi_{i}^{-}P_{i-1}\\Phi_{i-1}^{-}\\{1-F(t_{i};\\theta)\\}$. For animals which are sacrificed and do not have the tumour, the probability is $\\pi_{i}P_{i-1}\\Phi_{i}^{-}\\{1-F(t_{i};\\theta)\\}$. For animals which die naturally and have the tumour, the probability is $P_{i-1}\\bigg(\\sum_{j=1}^{i}\\Delta_{j}(\\theta)\\phi_{i}^{+}\\Phi_{j-1}^{-}\\Phi_{j,i-1}^{+}\\bigg)$. For animals sacrificed and have the tumour, the probability is $\\pi_{i}P_{i-1}\\bigg(\\sum_{j=1}^{i}\\Delta_{j}(\\theta)\\Phi_{j-1}^{-}\\Phi_{j,i}^{+}\\bigg)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: $\\mathcal{F}_{\\times}$ is indeed a unitary operator on the specified space.\n- **B) Correct**: The transform is defined up to a multiplicative constant.\n- **C) Correct**: It operates on the Haar measure $\\frac{\\mathrm{d}x}{|x|}$.\n- **D) Incorrect**: $\\mathcal{F}_{\\times}$ is distinct from $\\mathcal{F}_{+}$, which operates on the additive group $\\mathbb{R}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Scandinavian J Statistics - 2021 - Glück - A solut.md"], "background_text": "The Fourier transform $\\mathcal{F}_{\\times}$ on the multiplicative group $\\mathbb{R}^{\\times}$ is, up to a multiplicative constant, a unitary operator on $L^{2}\\left(\\mathbb{R}^{\\times},\\frac{\\mathrm{d}x}{|x|}\\right)$, where $\\frac{\\mathrm{d}x}{|x|}$ is the Haar measure on $\\mathbb{R}^{\\times}$.", "folder": "", "qid": "statistic-mcqm-5268", "question": "Which properties are true for the Fourier transform $\\mathcal{F}_{\\times}$ on the multiplicative group $\\mathbb{R}^{\\times}$?", "options": ["A) It is a unitary operator on $L^{2}\\left(\\mathbb{R}^{\\times},\\frac{\\mathrm{d}x}{|x|}\\right)$.", "B) It is defined up to a multiplicative constant.", "C) It operates on the Haar measure $\\frac{\\mathrm{d}x}{|x|}$.", "D) It is identical to the Fourier transform $\\mathcal{F}_{+}$ on the additive group $\\mathbb{R}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The Fourier transform $\\mathcal{F}_{\\times}$ on the multiplicative group $\\mathbb{R}^{\\times}$ is, up to a multiplicative constant, a unitary operator on $L^{2}\\left(\\mathbb{R}^{\\times},\\frac{\\mathrm{d}x}{|x|}\\right)$, where $\\frac{\\mathrm{d}x}{|x|}$ is the Haar measure on $\\mathbb{R}^{\\times}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. This equation is directly provided in the context for the variance of the prediction error.\n- **B)** Correct. This represents part of the conditional variance of $\\widehat{Y}_{0}(t)$ given $\\widetilde{\\eta}$, as detailed in the context.\n- **C)** Incorrect. The correct formula adds the variances, not subtracts them.\n- **D)** Correct. This is the expanded form of the conditional variance given $\\widetilde{\\eta}$, including cross terms for $k \\neq k^{\\prime}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Additive_Function-on-Function_Regression__Additive_Function-on-Function_Regression.md"], "background_text": "The variance of the prediction error is given by $\\mathrm{var}\\{\\widehat{Y}_{0}(t)-Y_{0}(t)\\}=\\mathrm{var}\\{\\widehat{Y}_{0}(t)\\}+\\mathrm{var}\\{\\epsilon_{0}(t)\\}$. The conditional variance of $\\widehat{Y}_{0}(t)$ given $\\widetilde{\\eta}$ is represented by $\\mathrm{var}\\{\\widehat{Y}_{0}(t)|\\widetilde{\\eta}\\}=\\sum_{k=1}^{K}\\nu_{k k}\\widetilde{\\phi}_{k}(t)\\Omega_{0}\\widetilde{\\phi}_{k}(t)+\\sum_{k\\neq k^{\\prime}}\\nu_{k k^{\\prime}}\\widetilde{\\phi}_{k}(t)\\Omega_{0}\\widetilde{\\phi}_{k^{\\prime}}(t)$.", "folder": "", "qid": "statistic-mcqm-5072", "question": "Which of the following equations correctly represent the variance of the prediction error $\\widehat{Y}_{0}(t) - Y_{0}(t)$?", "options": ["A) $\\mathrm{var}\\{\\widehat{Y}_{0}(t)-Y_{0}(t)\\}=\\mathrm{var}\\{\\widehat{Y}_{0}(t)\\}+\\mathrm{var}\\{\\epsilon_{0}(t)\\}$", "B) $\\mathrm{var}\\{\\widehat{Y}_{0}(t)|\\widetilde{\\eta}\\}=\\sum_{k=1}^{K}\\widetilde{\\phi}_{k}(t)\\mathbb{Z}_{0}^{T}\\mathrm{var}(\\widehat{\\Theta}_{k})\\mathbb{Z}_{0}\\widetilde{\\phi}_{k}(t)$", "C) $\\mathrm{var}\\{\\widehat{Y}_{0}(t)-Y_{0}(t)\\}=\\mathrm{var}\\{\\widehat{Y}_{0}(t)\\}-\\mathrm{var}\\{\\epsilon_{0}(t)\\}$", "D) $\\mathrm{var}\\{\\widehat{Y}_{0}(t)|\\widetilde{\\eta}\\}=\\sum_{k=1}^{K}\\nu_{k k}\\widetilde{\\phi}_{k}(t)\\Omega_{0}\\widetilde{\\phi}_{k}(t)+\\sum_{k\\neq k^{\\prime}}\\nu_{k k^{\\prime}}\\widetilde{\\phi}_{k}(t)\\Omega_{0}\\widetilde{\\phi}_{k^{\\prime}}(t)$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The variance of the prediction error is given by $\\mathrm{var}\\{\\widehat{Y}_{0}(t)-Y_{0}(t)\\}=\\mathrm{var}\\{\\widehat{Y}_{0}(t)\\}+\\mathrm{var}\\{\\epsilon_{0}(t)\\}$. The conditional variance of $\\widehat{Y}_{0}(t)$ given $\\widetilde{\\eta}$ is represented by $\\mathrm{var}\\{\\widehat{Y}_{0}(t)|\\widetilde{\\eta}\\}=\\sum_{k=1}^{K}\\nu_{k k}\\widetilde{\\phi}_{k}(t)\\Omega_{0}\\widetilde{\\phi}_{k}(t)+\\sum_{k\\neq k^{\\prime}}\\nu_{k k^{\\prime}}\\widetilde{\\phi}_{k}(t)\\Omega_{0}\\widetilde{\\phi}_{k^{\\prime}}(t)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. $I[T;R]$ indeed measures how much the uncertainty about $T$ decreases after knowing $R$.\n- **B)** Correct. $I[T;R]$ can be expressed as $H[T] - H[T|R]$, which is a direct consequence of its definition.\n- **C)** Correct. $I[T;R]$ is always non-negative because it is a Kullback-Leibler divergence, which is always non-negative.\n- **D)** Correct. $I[T;R]$ equals zero only when $T$ and $R$ are statistically independent, meaning knowing $R$ provides no information about $T$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Scandinavian J Statistics - 2019 - Lagerås - How t.md"], "background_text": "Mutual information $I[T;R]$ is defined as $H[T] + H[R] - H[T,R] = H[T] - H[T|R]$. It measures the amount of information shared between $T$ and $R$, is always non-negative, and equals zero only when $T$ and $R$ are independent.", "folder": "", "qid": "statistic-mcqm-5811", "question": "Which of the following are true about mutual information $I[T;R]$?", "options": ["A) $I[T;R]$ measures the decrease in uncertainty about $T$ after knowing $R$.", "B) $I[T;R]$ can be expressed as $H[T] - H[T|R]$.", "C) $I[T;R]$ is always non-negative.", "D) $I[T;R]$ equals zero only when $T$ and $R$ are statistically independent."], "category": "CORRECT", "question_context_used": "Mutual information $I[T;R]$ is defined as $H[T] + H[R] - H[T,R] = H[T] - H[T|R]$. It measures the amount of information shared between $T$ and $R$, is always non-negative, and equals zero only when $T$ and $R$ are independent.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct, as the net reproductive index $R=1.145>1$ implies the chance of extinction is less than one.\n- **B)** Correct, as Lotka found the exact chance of extinction to be 0.880.\n- **C)** Correct, the approximate chance of extinction using the given formula is 0.892.\n- **D)** Incorrect, the chance of extinction decreases with the increase in population size when $R>1$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Some Evolutionary Stochastic Processes.md"], "background_text": "From Lotka's distribution, the net reproductive index for males is $R=1.145$. The exact chance of extinction of a male line is 0.880, and an approximate value obtained by fitting a geometric series to the distribution is 0.872. Using an approximate formula with the variance of the distribution (2.544), the chance of extinction is calculated as $\\exp(-0.29 / 2.544)=0.892$.", "folder": "", "qid": "statistic-mcqm-2398", "question": "Given the net reproductive index $R=1.145$ for males in the United States (1920), which of the following statements about the chance of extinction of a male line are correct?", "options": ["A) The chance of extinction is less than one.", "B) The exact chance of extinction found by Lotka is 0.880.", "C) The approximate chance of extinction using a geometric series is 0.892.", "D) The chance of extinction increases with the size of the population."], "category": "PARTIALLY_CORRECT", "question_context_used": "From Lotka's distribution, the net reproductive index for males is $R=1.145$. The exact chance of extinction of a male line is 0.880, and an approximate value obtained by fitting a geometric series to the distribution is 0.872. Using an approximate formula with the variance of the distribution (2.544), the chance of extinction is calculated as $\\exp(-0.29 / 2.544)=0.892$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n**Explanation**: \n- **Option A** is correct as it matches the given definition of $\\psi$.\n- **Option D** is correct because it is the reciprocal of $\\psi$, which is another valid representation depending on the context.\n- **Option B** and **Option C** are incorrect as they do not match the defined form of $\\psi$.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Mean_and_variance_of_an_entry_in_a_contingency_table.md"], "background_text": "For a non-independent $2\\times2$ table, the parameter $\\psi$ is defined as $\\psi = p_1 q_2 / (p_2 q_1)$, where $p_1$ and $p_2$ are the probabilities of an event falling in the first column for the first and second rows, respectively, and $q_1 = 1 - p_1$, $q_2 = 1 - p_2$.", "folder": "", "qid": "statistic-mcqm-3418", "question": "When the $2\\times2$ table is not independent, the distribution of $a$ is influenced by the parameter $\\psi$, defined as:", "options": ["A) $\\psi = \\frac{p_1 q_2}{p_2 q_1}$", "B) $\\psi = \\frac{p_1 p_2}{q_1 q_2}$", "C) $\\psi = \\frac{q_1 q_2}{p_1 p_2}$", "D) $\\psi = \\frac{p_2 q_1}{p_1 q_2}$"], "category": "PARTIALLY_CORRECT", "question_context_used": "For a non-independent $2\\times2$ table, the parameter $\\psi$ is defined as $\\psi = p_1 q_2 / (p_2 q_1)$, where $p_1$ and $p_2$ are the probabilities of an event falling in the first column for the first and second rows, respectively, and $q_1 = 1 - p_1$, $q_2 = 1 - p_2$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A)** Correct. The numerator is indeed decomposed into three terms as stated.\n- **B)** Correct. The text explicitly shows this result for $I_{1}$.\n- **C)** Correct. $I_{3}$ is decomposed into $I_{31}$ and $I_{32}$ with the given properties.\n- **D)** Incorrect. The text further decomposes $I_{4}$ and $I_{5}$ into more terms, so this statement is not accurate.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Stein-s-method-in-high-dimensional-classific_2015_.md"], "background_text": "The text discusses the decomposition of $\\hat{V}_{S}-V_{S}$ into numerator and denominator parts, with the numerator further decomposed into $I_{1}, I_{2}, I_{3}$ and the denominator into $I_{4}, I_{5}$. It provides detailed expressions and convergence properties for these components.", "folder": "", "qid": "statistic-mcqm-1398", "question": "Given $Z_{j}=\\bar{X}_{1j}-\\bar{X}_{2j}=\\varDelta_{j}+\\epsilon_{j}$ where $\\epsilon_{i j}\\sim N(0,\\sigma_{n}^{2})$ for $\\sigma_{n}^{2}=n/(n_{1}n_{2})$, which of the following statements are correct about the decomposition of $\\hat{V}_{S}-V_{S}$?", "options": ["A) The numerator can be decomposed into three terms: $I_{1}, I_{2},$ and $I_{3}$.", "B) $I_{1}$ is shown to be $\\sqrt{\\frac{\\Delta_{\\mathcal{A}}}{n}}O_{p}(1)+o_{p}(1)$.", "C) $I_{3}$ is decomposed into $I_{31}$ and $I_{32}$, where $I_{31}=\\frac{\\sqrt{m_{n}}n}{n_{1}n_{2}}O_{p}(1)$ and $I_{32}=o_{p}(1)$.", "D) The denominator in $\\hat{V}_{S}-V_{S}$ is $\\sum_{j=1}^{p}\\delta^{2}(Z_{j})=I_{4}+I_{5}$, where $I_{4}$ and $I_{5}$ are not further decomposed."], "category": "INCORRECT", "question_context_used": "The text discusses the decomposition of $\\hat{V}_{S}-V_{S}$ into numerator and denominator parts, with the numerator further decomposed into $I_{1}, I_{2}, I_{3}$ and the denominator into $I_{4}, I_{5}$. It provides detailed expressions and convergence properties for these components.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The error terms are designed to have a mean of 0 and variance of 1.\n- **B)** Correct. The factor loadings are uniformly distributed within the specified ranges.\n- **C)** Correct. The mean vector is generated from normal, double exponential, and uniform distributions.\n- **D)** Correct. The results are based on averaging 100 replications for each configuration.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Stein-s-method-in-high-dimensional-classific_2015_.md"], "background_text": "In the correlated case, the error $\\varepsilon_{1i j}$ has $E(\\varepsilon_{1i j})=0$ and $Var(\\varepsilon_{1i j})=1$. The factor loading coefficients $\\{a_{j}\\}$ and $\\{b_{j}\\}$ are generated from Unif $(0,0.4)$ and Unif $(0,0.2)$. The mean vector $\\mu_{1}$ is generated from various distributions, and the misclassification rates are averaged over 100 replications.", "folder": "", "qid": "statistic-mcqm-1385", "question": "Which of the following statements about the correlated case simulation are correct?", "options": ["A) The error $\\varepsilon_{1i j}$ has a mean of 0 and a variance of 1.", "B) The factor loading coefficients $\\{a_{j}\\}$ and $\\{b_{j}\\}$ are generated from Unif $(0,0.4)$ and Unif $(0,0.2)$, respectively.", "C) The mean vector $\\mu_{1}$ is generated from normal, double exponential, and uniform distributions.", "D) The misclassification rates are averaged over 100 replications for each configuration."], "category": "PARTIALLY_CORRECT", "question_context_used": "In the correlated case, the error $\\varepsilon_{1i j}$ has $E(\\varepsilon_{1i j})=0$ and $Var(\\varepsilon_{1i j})=1$. The factor loading coefficients $\\{a_{j}\\}$ and $\\{b_{j}\\}$ are generated from Unif $(0,0.4)$ and Unif $(0,0.2)$. The mean vector $\\mu_{1}$ is generated from various distributions, and the misclassification rates are averaged over 100 replications.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, D\n\n- **A) The expected Fisher information matrix is non-singular when the skewness parameter equals zero.**: Incorrect, the context states it is non-singular when the skewness parameter equals one, not zero.\n- **B) The p.d.f. of the FSSN distribution is defined piecewise based on the sign of $y$.**: Correct, as the p.d.f. is given piecewise for $y<0$ and $y\\ge0$.\n- **C) The FSSN distribution cannot model symmetric data.**: Incorrect, the context implies it can model symmetric data when $\\delta=1$.\n- **D) The notation $Y\\sim\\operatorname{FSSN}\\left(\\delta\\right)$ is used for a standard FSSN distribution with skewness parameter $\\delta>0$.**: Correct, as explicitly stated in the context.\n\n**Final Answer**: The final answer is $\boxed{{B, D}}$", "source": ["On-the-Fern-ndez-Steel-distribution--Infere_2011_C.md"], "background_text": "The expected Fisher information matrix associated with the FSSN model is non-singular, but it is still non-singular when its skewness parameter equals one (normal case). If an r.v. Y follows a standard FSSN distribution with skewness parameter $\\delta>0$, the notation $Y\\sim\\operatorname{FSSN}\\left(\\delta\\right)$ is used and its p.d.f. is given by $h\\left(y;\\delta\\right)=\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{2\\delta}{[1+\\delta^{2}]}\\phi\\left(\\delta y\\right),}&{\\mathrm{~if}y<0,}\\\\{\\displaystyle\\frac{2\\delta}{[1+\\delta^{2}]}\\phi\\left(\\frac{y}{\\delta}\\right),}&{\\mathrm{~if}y\\ge0.}\\end{array}\\right.$", "folder": "", "qid": "statistic-mcqm-5912", "question": "Which of the following statements about the Fernández–Steel skew-normal (FSSN) distribution are correct?", "options": ["A) The expected Fisher information matrix is non-singular when the skewness parameter equals zero.", "B) The p.d.f. of the FSSN distribution is defined piecewise based on the sign of $y$.", "C) The FSSN distribution cannot model symmetric data.", "D) The notation $Y\\sim\\operatorname{FSSN}\\left(\\delta\\right)$ is used for a standard FSSN distribution with skewness parameter $\\delta>0$."], "category": "CORRECT", "question_context_used": "The expected Fisher information matrix associated with the FSSN model is non-singular, but it is still non-singular when its skewness parameter equals one (normal case). If an r.v. Y follows a standard FSSN distribution with skewness parameter $\\delta>0$, the notation $Y\\sim\\operatorname{FSSN}\\left(\\delta\\right)$ is used and its p.d.f. is given by $h\\left(y;\\delta\\right)=\\left\\{\\begin{array}{l l}{\\displaystyle\\frac{2\\delta}{[1+\\delta^{2}]}\\phi\\left(\\delta y\\right),}&{\\mathrm{~if}y<0,}\\\\{\\displaystyle\\frac{2\\delta}{[1+\\delta^{2}]}\\phi\\left(\\frac{y}{\\delta}\\right),}&{\\mathrm{~if}y\\ge0.}\\end{array}\\right.$", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct, the text specifies that for any $2 < s < 2 + r$, the drift equation is satisfied.\n- **B)** Correct, $\\alpha$ is defined as $(s - 2)/s$ in the text.\n- **C)** Correct, $V(x)$ is defined piecewise for $x \\ge 0$ and $x < 0$ in the text.\n- **D)** Incorrect, $l^{+}$ and $l^{-}$ are normalized slowly varying functions, not necessarily constant.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Scandinavian J Statistics - 2007 - JARNER - Conver.md"], "background_text": "The context specifies conditions under which the drift equation (4) is satisfied for a density $\\pi$ of a given form and a proposal distribution $Q$ with finite variance. It mentions the range for $s$, the form of $\\alpha$, and the definition of $V(x)$ for positive and negative $x$. It also discusses the properties of $l^{+}$ and $l^{-}$ as normalized slowly varying functions.", "folder": "", "qid": "statistic-mcqm-5433", "question": "Which of the following conditions must be satisfied for the drift equation (4) to hold, given the form of $\\pi$ and the proposal distribution $Q$ with finite variance?", "options": ["A) $2 < s < 2 + r$", "B) $\\alpha = (s - 2)/s$", "C) $V(x)$ must be defined as specified for $x \\ge 0$ and $x < 0$.", "D) $l^{+}$ and $l^{-}$ must be constant functions."], "category": "CORRECT", "question_context_used": "The context specifies conditions under which the drift equation (4) is satisfied for a density $\\pi$ of a given form and a proposal distribution $Q$ with finite variance. It mentions the range for $s$, the form of $\\alpha$, and the definition of $V(x)$ for positive and negative $x$. It also discusses the properties of $l^{+}$ and $l^{-}$ as normalized slowly varying functions.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: The text explicitly mentions that the squared $L^{2}$ distance is chosen for its computational advantages and analytical tractability.\n- **B) Correct**: The text states that the distance should capture the particular discrepancies between observations that are relevant for clustering purposes.\n- **C) Incorrect**: The text mentions that in practice, density forecasts are unknown, so the distances cannot be directly computed, making this option incorrect.\n- **D) Incorrect**: The text mentions that other measures between densities can be used, such as $L^{1}$ distance or Kullback–Leibler discrepancy, making this option incorrect.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["Time-series-clustering-based-on-forecas_2006_Compu.md"], "background_text": "The squared $L^{2}$ distance is chosen for clustering time series based on their full forecast densities due to its computational advantages and analytical tractability. It captures the particular discrepancies between observations that are relevant for clustering purposes. However, in practice, density forecasts are unknown, so the distances cannot be directly computed and must be approximated from the data.", "folder": "", "qid": "statistic-mcqm-3838", "question": "Which of the following are correct about the squared $L^{2}$ distance used in the clustering of time series based on their full forecast densities?", "options": ["A) It is chosen for its computational advantages and analytical tractability.", "B) It captures the discrepancies between observations relevant for clustering purposes.", "C) It requires the exact knowledge of the density forecasts to be computed directly.", "D) It is the only possible measure between densities that can be used for clustering."], "category": "PARTIALLY_CORRECT", "question_context_used": "The squared $L^{2}$ distance is chosen for clustering time series based on their full forecast densities due to its computational advantages and analytical tractability. It captures the particular discrepancies between observations that are relevant for clustering purposes. However, in practice, density forecasts are unknown, so the distances cannot be directly computed and must be approximated from the data.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. The posterior distribution is indeed proportional to the given expression.\n- **B)** Correct. The condition $m-p-2>0$ ensures the posterior is proper for any bounded prior $\\pi(\\psi)$.\n- **C)** Correct. $h(\\psi)$ is minimized at $\\hat{\\psi}_{\\scriptscriptstyle\\mathrm{RE}}$.\n- **D)** Incorrect. The posterior distribution is dependent on the data $Y$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["On_measuring_the_variability_of_small_area_estimators_under_a_basic_area_level_model.md"], "background_text": "The posterior distribution $\\pi(\\psi|Y)$ is given by a specific proportionality involving $\\pi(\\psi)$, $\\Sigma(\\psi)$, $X$, and $Q(\\psi)$. It is proper for any bounded prior $\\pi(\\psi)$ if $m-p-2>0$. The function $h(\\psi)$ is minimized at $\\hat{\\psi}_{\\scriptscriptstyle\\mathrm{RE}}$.", "folder": "", "qid": "statistic-mcqm-4977", "question": "Which of the following statements about the posterior distribution $\\pi(\\psi|Y)$ are correct?", "options": ["A) It is proportional to $\\pi(\\psi)|\\Sigma(\\psi)|^{-\\frac{1}{2}}|X^{\\prime}\\Sigma^{-1}(\\psi)X|^{-\\frac{1}{2}}\\exp\\left\\{-\\frac{1}{2}Y^{\\prime}Q(\\psi)Y\\right\\}$.", "B) It is proper for any bounded prior $\\pi(\\psi)$ if $m-p-2>0$.", "C) It minimizes $h(\\psi)$ at $\\hat{\\psi}_{\\scriptscriptstyle\\mathrm{RE}}$.", "D) It is independent of the data $Y$."], "category": "CORRECT", "question_context_used": "The posterior distribution $\\pi(\\psi|Y)$ is given by a specific proportionality involving $\\pi(\\psi)$, $\\Sigma(\\psi)$, $X$, and $Q(\\psi)$. It is proper for any bounded prior $\\pi(\\psi)$ if $m-p-2>0$. The function $h(\\psi)$ is minimized at $\\hat{\\psi}_{\\scriptscriptstyle\\mathrm{RE}}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct: Lemma 1 states that $\\theta_{0}^{\\prime} \\in [\\tilde{\\theta}, \\theta_{0}]$.\n- **B)** Correct: Lemma 1 also states that $\\theta_{0}^{\\prime} - \\theta_{0} = O_{\\mathrm{p}}(n^{-3/2})$.\n- **C)** Incorrect: $\\theta_{0}^{\\prime}$ is not always equal to $\\theta_{0}$; it is only asymptotically close to $\\theta_{0}$.\n- **D)** Correct: The definition of $h_{n}^{-\\mathrm{C}}(\\theta)$ implies that $\\theta_{0}^{\\prime}$ minimizes $\\|\\theta^{\\prime} - \\theta\\|$ over $\\theta^{\\prime} \\in s(\\theta)$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Extended_empirical_likelihood_for_estimating_equations.md"], "background_text": "Under Conditions 1 and 2, the point $\\theta_{0}^{\\prime}$ defined by (15) satisfies: (i) $\\theta_{0}^{\\prime} \\in [\\tilde{\\theta}, \\theta_{0}]$; (ii) $\\theta_{0}^{\\prime} - \\theta_{0} = O_{\\mathrm{p}}(n^{-3/2})$. The generalized inverse $h_{n}^{-\\mathrm{C}}(\\theta)$ is defined as $\\operatorname{argmin}_{\\theta^{\\prime} \\in s(\\theta)}\\{\\|\\theta^{\\prime}-\\theta\\|\\}$.", "folder": "", "qid": "statistic-mcqm-958", "question": "Under Conditions 1 and 2, which of the following statements about the point $\\theta_{0}^{\\prime}$ defined by (15) are correct?", "options": ["A) $\\theta_{0}^{\\prime} \\in [\\tilde{\\theta}, \\theta_{0}]$", "B) $\\theta_{0}^{\\prime} - \\theta_{0} = O_{\\mathrm{p}}(n^{-3/2})$", "C) $\\theta_{0}^{\\prime}$ is always equal to $\\theta_{0}$", "D) $\\theta_{0}^{\\prime}$ minimizes $\\|\\theta^{\\prime} - \\theta\\|$ over $\\theta^{\\prime} \\in s(\\theta)$"], "category": "PARTIALLY_CORRECT", "question_context_used": "Under Conditions 1 and 2, the point $\\theta_{0}^{\\prime}$ defined by (15) satisfies: (i) $\\theta_{0}^{\\prime} \\in [\\tilde{\\theta}, \\theta_{0}]$; (ii) $\\theta_{0}^{\\prime} - \\theta_{0} = O_{\\mathrm{p}}(n^{-3/2})$. The generalized inverse $h_{n}^{-\\mathrm{C}}(\\theta)$ is defined as $\\operatorname{argmin}_{\\theta^{\\prime} \\in s(\\theta)}\\{\\|\\theta^{\\prime}-\\theta\\|\\}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The estimators are indeed obtained by least squares based on synthetic responses, as mentioned in the text.\n- **B)** Correct. The text explicitly states that the proposed estimators outperform Stute's estimator because they use all the information from censored observations, which Stute's estimator does not.\n- **C)** Incorrect. The text clarifies that synthetic observations are not i.i.d. even if the original data were.\n- **D)** Correct. The method allows for heteroscedastic errors with unknown conditional variance and discrete covariates, as stated in the text.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Scandinavian J Statistics - 2008 - DELECROIX - Non.md"], "background_text": "The proposed estimators for nonlinear regression in the presence of censoring are obtained by least squares based on synthetic responses. They outperform Stute's (1999) estimator in finite samples by utilizing all information from censored observations. The synthetic responses are not i.i.d. even if the original data were. The method allows for heteroscedastic errors with unknown conditional variance and discrete covariates.", "folder": "", "qid": "statistic-mcqm-5195", "question": "Which of the following statements are true about the proposed estimators in the context of nonlinear regression with censoring?", "options": ["A) The estimators are obtained by least squares based on synthetic responses.", "B) The estimators outperform Stute's (1999) estimator in finite samples because they use all the information carried by censored observations.", "C) The synthetic responses are i.i.d. if the original data were i.i.d.", "D) The estimators allow for heteroscedastic errors with unknown conditional variance and discrete covariates."], "category": "CORRECT", "question_context_used": "The proposed estimators for nonlinear regression in the presence of censoring are obtained by least squares based on synthetic responses. They outperform Stute's (1999) estimator in finite samples by utilizing all information from censored observations. The synthetic responses are not i.i.d. even if the original data were. The method allows for heteroscedastic errors with unknown conditional variance and discrete covariates.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**: \n- **Option A** is correct because $\\chi_{i}^{n}$ is a martingale difference sequence due to the properties of stochastic integrals.\n- **Option B** is correct because the expectation of the product of independent stochastic integrals with respect to $B$ and $\\overline{B}$ is zero.\n- **Option C** is incorrect because the variance of $\\chi_{i}^{n}$ is of order $\\Delta_{n}$, not $\\Delta_{n}^{2}$.\n- **Option D** is correct as the text mentions that $\\chi_{n}(t)$ converges stably in law to a continuous process $\\chi(t)$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$.", "source": ["Scandinavian J Statistics - 2019 - Féron - Efficie.md"], "background_text": "The term $\\chi_{i}^{n}$ involves stochastic integrals with respect to Brownian motions $B$ and $\\overline{B}$, which are independent. The expectation of $\\chi_{i}^{n}$ is zero due to the independence and the properties of stochastic integrals.", "folder": "", "qid": "statistic-mcqm-5830", "question": "Regarding the term $\\chi_{i}^{n} = 2\\mathfrak{e}_{1,2}(\\vartheta)\\int_{(i-1)\\Delta_{n}}^{i\\Delta_{n}}e^{\\vartheta t}\\sigma_{t}d B_{t}\\int_{(i-1)\\Delta_{n}}^{i\\Delta_{n}}\\overline{\\sigma}_{t}d\\overline{B}_{t}$, which of the following are true?", "options": ["A) $\\chi_{i}^{n}$ is a martingale difference sequence.", "B) The expectation of $\\chi_{i}^{n}$ is zero.", "C) The variance of $\\chi_{i}^{n}$ is of order $\\Delta_{n}^{2}$.", "D) $\\sum_{i=1}^{n}\\chi_{i}^{n}$ converges to a Gaussian process."], "category": "PARTIALLY_CORRECT", "question_context_used": "The term $\\chi_{i}^{n}$ involves stochastic integrals with respect to Brownian motions $B$ and $\\overline{B}$, which are independent. The expectation of $\\chi_{i}^{n}$ is zero due to the independence and the properties of stochastic integrals.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The inequality $P \\geq \\mathrm{Pr}(\\mathrm{PU}|\\mathrm{SU})$ is explicitly stated in the context.\n- **B) Correct**: The context mentions that $\\mathrm{Pr}(\\mathrm{PU}|\\mathrm{SU})$ fails to reflect the risk arising from values of $X$ which are twins, triples, and so forth, implying the same for $P$.\n- **C) Incorrect**: There is no statement in the context suggesting that $P$ is always equal to $\\theta$.\n- **D) Correct**: The context suggests that whether $\\theta$ or $P$ is more appropriate depends on the method of attack, implying that $P$ is more appropriate under attack method 2.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$.", "source": ["A Measure of Disclosure Risk for Microdata.md"], "background_text": "Under attack method 2, the intruder draws one microdata record at random from the sample unique records and searches through the population at random until a unit is located which matches the selected record. The probability $P$ that the selected record belongs to the unit located is at least $\\mathrm{Pr}(\\mathrm{PU}|\\mathrm{SU})$, and it fails to reflect the risk arising from values of $X$ which are twins, triples, and so forth. Whether $\\theta$ or $P$ is a more appropriate measure of risk depends on the method of attack.", "folder": "", "qid": "statistic-mcqm-3997", "question": "Under attack method 2, which of the following statements are true regarding the probability $P$ that the selected record belongs to the unit located?", "options": ["A) $P$ is at least $\\mathrm{Pr}(\\mathrm{PU}|\\mathrm{SU})$.", "B) $P$ fails to reflect the risk arising from values of $X$ which are twins, triples, and so forth.", "C) $P$ is always equal to $\\theta$.", "D) $P$ is a more appropriate measure of risk than $\\theta$ under attack method 2."], "category": "PARTIALLY_CORRECT", "question_context_used": "Under attack method 2, the intruder draws one microdata record at random from the sample unique records and searches through the population at random until a unit is located which matches the selected record. The probability $P$ that the selected record belongs to the unit located is at least $\\mathrm{Pr}(\\mathrm{PU}|\\mathrm{SU})$, and it fails to reflect the risk arising from values of $X$ which are twins, triples, and so forth. Whether $\\theta$ or $P$ is a more appropriate measure of risk depends on the method of attack.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A)** Correct. The test statistic sums over $h$ from $-H$ to $H$, as explicitly shown in its definition.\n- **B)** Correct. The definition includes integrals over $\\mathbb{R}$ for both $u$ and $v$.\n- **C)** Incorrect. The weight functions $w(u)$ and $w(v)$ are integral to the definition of $\\widehat{T}_{n,H}^{(w)}$.\n- **D)** Incorrect. The test statistic is defined for both $h > 0$ and $h \\leq 0$, as indicated by the summation from $-H$ to $H$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Fourier-type-tests-of-mutual-independence-between_.md"], "background_text": "The test statistic $\\widehat{T}_{n,H}^{(w)}$ is defined as a sum over $h$ from $-H$ to $H$, involving integrals over $\\mathbb{R}$ for both $u$ and $v$, and includes weight functions $w(u)$ and $w(v)$. It is defined for both positive and negative values of $h$.", "folder": "", "qid": "statistic-mcqm-5795", "question": "Which of the following statements about the test statistic $\\widehat{T}_{n,H}^{(w)}$ are true?", "options": ["A) It involves summing over $h$ from $-H$ to $H$", "B) It includes an integral over $\\mathbb{R}$ for both $u$ and $v$", "C) The weight functions $w(u)$ and $w(v)$ are not necessary for its definition", "D) It is defined only for $h > 0$"], "category": "CORRECT", "question_context_used": "The test statistic $\\widehat{T}_{n,H}^{(w)}$ is defined as a sum over $h$ from $-H$ to $H$, involving integrals over $\\mathbb{R}$ for both $u$ and $v$, and includes weight functions $w(u)$ and $w(v)$. It is defined for both positive and negative values of $h$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct: The text states that the empirical levels are reasonably close to the nominal value $5\\%$ for all test statistics.\n- **B)** Correct: The HR test is mentioned to perform best for IID Wiener processes.\n- **C)** Incorrect: The text specifies that the DCov-test performs better under certain alternatives, not all.\n- **D)** Correct: The power of the CF-S test is always significantly lower than some other tests.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Fourier-type-tests-of-mutual-independence-between_.md"], "background_text": "The empirical levels in Table 2 are close to the nominal value $5\\%$ for all test statistics. The HR test performs best for IID Wiener processes but loses power for heavy-tailed sequences. The CF-test performs better under certain alternatives, while the DCov-test performs better under others. The power of the simplified CF-based test CF-S is always significantly lower than some of the other tests.", "folder": "", "qid": "statistic-mcqm-5798", "question": "Regarding the empirical level and power investigations in Table 2, which of the following statements are correct?", "options": ["A) The empirical levels are close to the nominal value of $5\\%$ for all test statistics.", "B) The HR test performs best for IID Wiener processes.", "C) The DCov-test performs better than the CF-test under all alternatives.", "D) The power of the simplified CF-based test CF-S is always significantly lower than some of the other tests."], "category": "CORRECT", "question_context_used": "The empirical levels in Table 2 are close to the nominal value $5\\%$ for all test statistics. The HR test performs best for IID Wiener processes but loses power for heavy-tailed sequences. The CF-test performs better under certain alternatives, while the DCov-test performs better under others. The power of the simplified CF-based test CF-S is always significantly lower than some of the other tests.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**: \n- **Option A** is correct because the context suggests replacing the parameter $_{c v}$ with ICV.\n- **Option B** is correct because the context suggests using CV as a real local variable.\n- **Option C** is correct because the context suggests inserting the statement $I C V=C V+S I G N(0\\cdot5,C V)$ before the return from the algorithm.\n- **Option D** is incorrect because the context specifically mentions avoiding the use of the SIGN function with a zero argument in the special case.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C}}$", "source": ["Critical Values of the Sign Test.md"], "background_text": "To overcome the rounding behavior difference, it is suggested that the parameter $_{c v}$ should become ICV, CV should be a real local variable, and the statement $I C V=C V+S I G N(0\\cdot5,C V)$ should be inserted immediately before the return from the algorithm. The special case (when $s l=5$ and $n=8$) should avoid using the SIGN function with a zero argument.", "folder": "", "qid": "statistic-mcqm-5366", "question": "In the context of translating from Algol to FORTRAN, which of the following modifications are suggested to overcome the rounding behavior difference?", "options": ["A) Replace the parameter $_{c v}$ with ICV.", "B) Use CV as a real local variable.", "C) Insert the statement $I C V=C V+S I G N(0\\cdot5,C V)$ before the return from the algorithm.", "D) Always use the SIGN function with a zero argument."], "category": "CORRECT", "question_context_used": "To overcome the rounding behavior difference, it is suggested that the parameter $_{c v}$ should become ICV, CV should be a real local variable, and the statement $I C V=C V+S I G N(0\\cdot5,C V)$ should be inserted immediately before the return from the algorithm. The special case (when $s l=5$ and $n=8$) should avoid using the SIGN function with a zero argument.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "To answer this question, let's evaluate each option based on the background text:\n\n- **A) Correct**: This is the first equivalent statement in Theorem 3.3.\n- **B) Correct**: This is the second equivalent statement in Theorem 3.3.\n- **C) Incorrect**: The theorem specifies $E f(\\mathbf{X}) \\leqslant E f(\\mathbf{Y})$, not $\\geqslant$.\n- **D) Incorrect**: The theorem discusses $\\leqslant_{ism}$, not $\\geqslant_{ism}$.\n\n**Correct Choices**: A, B\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Some-Remarks-on-the-Supermodular-Order_2000_Journa.md"], "background_text": "Theorem 3.3 states that the following statements are equivalent: (i) $\\mathbf{X} \\leqslant_{ism} \\mathbf{Y}$; (ii) $E f(\\mathbf{X}) \\leqslant E f(\\mathbf{Y})$ for all bounded continuous increasing supermodular functions $f$.", "folder": "", "qid": "statistic-mcqm-5041", "question": "Which of the following are equivalent statements according to Theorem 3.3?", "options": ["A) $\\mathbf{X} \\leqslant_{ism} \\mathbf{Y}$.", "B) $E f(\\mathbf{X}) \\leqslant E f(\\mathbf{Y})$ for all bounded continuous increasing supermodular functions $f$.", "C) $E f(\\mathbf{X}) \\geqslant E f(\\mathbf{Y})$ for all bounded continuous increasing supermodular functions $f$.", "D) $\\mathbf{X} \\geqslant_{ism} \\mathbf{Y}$."], "category": "CORRECT", "question_context_used": "Theorem 3.3 states that the following statements are equivalent: (i) $\\mathbf{X} \\leqslant_{ism} \\mathbf{Y}$; (ii) $E f(\\mathbf{X}) \\leqslant E f(\\mathbf{Y})$ for all bounded continuous increasing supermodular functions $f$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The estimate for $\\hat{\\eta}_{0}$ is $-2.572$ with a standard error of 0.13.\n- **B)** Correct. The estimate for $\\hat{\\beta}_{0}$ is $-3.407$ with a standard error of 0.182.\n- **C)** Correct. The estimate for $\\hat{\\beta}_{1}$ is $0.0499$ with a standard error of 0.00312.\n- **D)** Correct. The estimate for $\\hat{\\alpha}_{0}$ is $0.000437$ with a standard error of 0.0377.\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["A Litter-Based Approach to Risk Assessment in Deve.md"], "background_text": "The parameter estimates for the reduced model are $\\hat{\\eta}_{0}=-2.572$ (0.13), $\\hat{\\beta}_{0}=-3.407$ (0.182), $\\hat{\\beta}_{1}=0.0499$ (0.00312), $\\hat{\\alpha}_{0}=0.000437$ (0.0377), and $\\hat{\\alpha}_{1}=0.0352$ (0.00254).", "folder": "", "qid": "statistic-mcqm-5452", "question": "What are the correct parameter estimates (with standard errors) for the reduced model fitted to the 2,4,5-T data?", "options": ["A) $\\hat{\\eta}_{0}=-2.572$ (0.13)", "B) $\\hat{\\beta}_{0}=-3.407$ (0.182)", "C) $\\hat{\\beta}_{1}=0.0499$ (0.00312)", "D) $\\hat{\\alpha}_{0}=0.000437$ (0.0377)"], "category": "CORRECT", "question_context_used": "The parameter estimates for the reduced model are $\\hat{\\eta}_{0}=-2.572$ (0.13), $\\hat{\\beta}_{0}=-3.407$ (0.182), $\\hat{\\beta}_{1}=0.0499$ (0.00312), $\\hat{\\alpha}_{0}=0.000437$ (0.0377), and $\\hat{\\alpha}_{1}=0.0352$ (0.00254).", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n**Explanation**: \n- **Option A** is correct as it provides an upper bound for the variance.\n- **Option B** is correct because it breaks down the total variance into the sum of variances of individual components.\n- **Option C** is correct as it gives an integral expression for the variance.\n- **Option D** is correct because it simplifies the expression for the variance in terms of $v(h)$ and $n$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Consistency-and-asymptotic-normality-for-a-nonpara.md"], "background_text": "The variance of $\\tilde{f}_{T}(x)$ is analyzed in Lemma 2, showing that it can be bounded and expressed in terms of the variance of individual components $f_{n,j}(x)$ and the function $v(h)$.", "folder": "", "qid": "statistic-mcqm-4908", "question": "Which of the following statements about the variance of $\\tilde{f}_{T}(x)$ are correct based on Lemma 2?", "options": ["A) $V(\\tilde{f}_{T}(x)) \\leq c_{2}\\frac{v(h)}{n}$", "B) $V(\\tilde{f}_{T}(x)) = \\sum_{j=1}^{n}V(f_{n,j}(x))$", "C) $V(\\tilde{f}_{T}(x)) = c_{2}\\int\\left|\\mathcal{F}_{K}(h t)\\phi_{f_{U^{F}}}(t)\\right|^{2}/\\sum_{j=1}^{n}\\left|\\phi_{f_{U_{j}}}(t)\\right|^{2}d t$", "D) $V(\\tilde{f}_{T}(x)) = \\frac{v(h)}{n}c_{2}$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The variance of $\\tilde{f}_{T}(x)$ is analyzed in Lemma 2, showing that it can be bounded and expressed in terms of the variance of individual components $f_{n,j}(x)$ and the function $v(h)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n- **A)** Correct. This is the exact expression provided in the context for $E(Y^{r})$ when $\\nu_{0}=\\nu=2m$.\n- **B)** Incorrect. This expression is for the special case when $p=1$, i.e., when $Y$ has an $F$ distribution, not the general case.\n- **C)** Incorrect. This expression is for $\\mu_{r}^{\\prime}(1,p,2m)$, not $E(Y^{r})$.\n- **D)** Correct. This is another form of the expression for $E(Y^{r})$ given in the context, where $\\mu_{r}^{\\prime}(1,p,\\nu)$ is the $r$th moment of the first order statistic.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["On_the_smallest_of_several_correlated_F_statistics_.md"], "background_text": "For the case when $\\nu_{0}=\\nu=2m$, the expected value of $Y^{r}$, where $Y=\\chi_{\\mathrm{min}}^{2}/\\chi_{0}^{2}$, is given by $E(Y^{r})=\\frac{p\\Gamma(m-r)}{\\Gamma^{2}(m)}\\sum_{t=0}^{(p-1)(m-1)}a_{t}(m,p-1)\\frac{\\Gamma(r+m+t)}{p^{r+m+t}}$. This is a special case of the more general expression involving the moments of order statistics from the gamma distribution.", "folder": "", "qid": "statistic-mcqm-1310", "question": "For the case when $\\nu_{0}=\\nu=2m$, which of the following expressions correctly represent $E(Y^{r})$ where $Y=\\chi_{\\mathrm{min}}^{2}/\\chi_{0}^{2}$?", "options": ["A) $E(Y^{r})=\\frac{p\\Gamma(m-r)}{\\Gamma^{2}(m)}\\sum_{t=0}^{(p-1)(m-1)}a_{t}(m,p-1)\\frac{\\Gamma(r+m+t)}{p^{r+m+t}}$", "B) $E(Y^{r})=\\frac{\\Gamma(\\frac{1}{2}\\nu_{0}-r)\\Gamma(\\frac{1}{2}\\nu+r)}{\\Gamma(\\frac{1}{2}\\nu_{0})\\Gamma(\\frac{1}{2}\\nu)}$", "C) $E(Y^{r})=\\frac{2^{r}}{\\Gamma(m)}\\sum_{t=0}^{(p-1)(m-1)}a_{t}(m,p-1)\\frac{\\Gamma(r+m+t)}{p^{r+m+t}}$", "D) $E(Y^{r})=\\frac{p\\Gamma(\\frac{1}{2}\\nu_{0}-r)}{2^{r}\\Gamma(\\frac{1}{2}\\nu_{0})}\\mu_{r}^{\\prime}(1,p,\\nu)$"], "category": "PARTIALLY_CORRECT", "question_context_used": "For the case when $\\nu_{0}=\\nu=2m$, the expected value of $Y^{r}$, where $Y=\\chi_{\\mathrm{min}}^{2}/\\chi_{0}^{2}$, is given by $E(Y^{r})=\\frac{p\\Gamma(m-r)}{\\Gamma^{2}(m)}\\sum_{t=0}^{(p-1)(m-1)}a_{t}(m,p-1)\\frac{\\Gamma(r+m+t)}{p^{r+m+t}}$. This is a special case of the more general expression involving the moments of order statistics from the gamma distribution.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct because Lemma 1 states that under $H_0$, $\\mathrm{pr}(\\tilde{X}_i < X_j, X_k)$ is invariant under any permutation of the subscripts i, j, and k.\n- **B)** Correct because the text explicitly states this reduction when $F_i^0 = F_j^0 = F_k^0 = F^0$.\n- **D)** Correct because Lemma 2 provides this equality under the given conditions.\n- **C)** Incorrect because the text does not state that the probability is zero under these conditions; it only discusses invariance and reduction under $H_0$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["A_generalized_Kruskal-Wallis_test_for_comparing_K_samples_subject_to_unequal_patterns_of_censorship.md"], "background_text": "Under $H_0$, $\\mathrm{pr}(\\tilde{X}_i < X_j, X_k)$ is invariant under any permutation of the subscripts i, j, and k. When $F_i^0 = F_j^0 = F_k^0 = F^0$, the probability reduces to $\\int (1 - I_i)(1 - I_j)(1 - I_k)(1 - F^0)^3 dF^0$. Additionally, $\\mathrm{pr}(\\tilde{X}_i, \\tilde{X}_j < X_k) = \\mathrm{pr}(\\tilde{X}_i < \\tilde{X}_j < X_k) + \\mathrm{pr}(\\tilde{X}_j < \\tilde{X}_i < X_k)$.", "folder": "", "qid": "statistic-mcqm-3736", "question": "Under Model I and $H_0$, which of the following statements about the probability $\\mathrm{pr}(\\tilde{X}_i < X_j, X_k)$ are correct?", "options": ["A) It is invariant under any permutation of the subscripts i, j, and k.", "B) It reduces to $\\int (1 - I_i)(1 - I_j)(1 - I_k)(1 - F^0)^3 dF^0$ when $F_i^0 = F_j^0 = F_k^0 = F^0$.", "C) It is always zero if $F_i^0$, $F_j^0$, and $F_k^0$ are not all equal.", "D) It can be expressed as $\\mathrm{pr}(\\tilde{X}_i < \\tilde{X}_j < X_k) + \\mathrm{pr}(\\tilde{X}_j < \\tilde{X}_i < X_k)$."], "category": "PARTIALLY_CORRECT", "question_context_used": "Under $H_0$, $\\mathrm{pr}(\\tilde{X}_i < X_j, X_k)$ is invariant under any permutation of the subscripts i, j, and k. When $F_i^0 = F_j^0 = F_k^0 = F^0$, the probability reduces to $\\int (1 - I_i)(1 - I_j)(1 - I_k)(1 - F^0)^3 dF^0$. Additionally, $\\mathrm{pr}(\\tilde{X}_i, \\tilde{X}_j < X_k) = \\mathrm{pr}(\\tilde{X}_i < \\tilde{X}_j < X_k) + \\mathrm{pr}(\\tilde{X}_j < \\tilde{X}_i < X_k)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. As stated in the background text, the sum of the values of $u_{A}^{(n)}$ over any argument is zero.\n- **B)** Correct. The background text mentions that there are $\\left\\{{n\\atop r}\\right\\}$ $r$-factor interactions in an $n$-dimensional table.\n- **C)** Incorrect. The definition of $u_{A}^{(n)}$ is a generalization for any $n$-dimensional table, not just for $n=2,3$.\n- **D)** Correct. The background text indicates that $u_{A}^{(n)}$ can be used to factorize the function $p$ through the unique representation $l=\\ln p=\\sum_{A\\subseteq\\overline{{{n}}}}u_{A}^{(n)}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Collapsibility of Multidimensional Contingency Tab.md"], "background_text": "An r-factor interaction $u_{A}^{(n)}$ for an $n$-dimensional table corresponding to a subset $A$ of r factors is given by $u_{A}^{(n)}(\\mathbf{i}_{A})=\\sum_{z\\subseteq A}(-1)^{|A-Z|}\\bar{l}_{Z}(\\mathbf{i}_{Z})$, where $l=\\ln p,\\bar{l}_{z}$ is as defined in Section 3 and $|A-Z|$ denotes the number of factors in the set $A-Z$. There are $\\left\\{{n\\atop r}\\right\\}$ $r$-factor interactions in an $n$-dimensional table, corresponding to each subset $A=\\{a_{1},...,a_{r}\\}$ with $r$ elements. It is shown in the Appendix that the sum of the values of $u_{A}^{(n)}$ over any argument is zero: $\\sum_{i_{k}}u_{A}^{(n)}(i_{a_{1}},...,i_{a_{r}})=0,k=a_{1},...,a_{r}$.", "folder": "", "qid": "statistic-mcqm-4900", "question": "In an $n$-dimensional table, which of the following statements are true about $r$-factor interactions $u_{A}^{(n)}$?", "options": ["A) The sum of the values of $u_{A}^{(n)}$ over any argument is zero.", "B) There are $\\left\\{{n\\atop r}\\right\\}$ $r$-factor interactions in an $n$-dimensional table.", "C) $u_{A}^{(n)}$ is defined only for $n=2,3$.", "D) $u_{A}^{(n)}$ can be used to factorize the function $p$."], "category": "PARTIALLY_CORRECT", "question_context_used": "An r-factor interaction $u_{A}^{(n)}$ for an $n$-dimensional table corresponding to a subset $A$ of r factors is given by $u_{A}^{(n)}(\\mathbf{i}_{A})=\\sum_{z\\subseteq A}(-1)^{|A-Z|}\\bar{l}_{Z}(\\mathbf{i}_{Z})$, where $l=\\ln p,\\bar{l}_{z}$ is as defined in Section 3 and $|A-Z|$ denotes the number of factors in the set $A-Z$. There are $\\left\\{{n\\atop r}\\right\\}$ $r$-factor interactions in an $n$-dimensional table, corresponding to each subset $A=\\{a_{1},...,a_{r}\\}$ with $r$ elements. It is shown in the Appendix that the sum of the values of $u_{A}^{(n)}$ over any argument is zero: $\\sum_{i_{k}}u_{A}^{(n)}(i_{a_{1}},...,i_{a_{r}})=0,k=a_{1},...,a_{r}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n- **A)** Correct. The text states that equality holds if $A_{22}^{-1}A_{21} = B_{22}^{-1}B_{21}$.\n- **B)** Incorrect. While invertibility of $A_{22} + B_{22}$ is a condition for the inequality, it is not sufficient for equality.\n- **C)** Incorrect. The condition $A_{22} > 0$ and $B_{22} > 0$ ensures the inequality holds but does not guarantee equality.\n- **D)** Correct. The text states that equality holds if $(A_{22} + B_{22})^{-1}(A_{21} + B_{21}) = A_{22}^{-1}A_{21}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Information-on-parameters-of-interest-decreases-_2.md"], "background_text": "For symmetric matrices $A$ and $B$ partitioned conformably, the inequality $(A + B)_{11\\cdot2} \\geq A_{11\\cdot2} + B_{11\\cdot2}$ holds under certain conditions, with equality if and only if specific conditions involving the submatrices are met.", "folder": "", "qid": "statistic-mcqm-6071", "question": "Under what conditions does equality hold in the inequality $(A + B)_{11\\cdot2} \\geq A_{11\\cdot2} + B_{11\\cdot2}$ for symmetric matrices $A$ and $B$?", "options": ["A) When $A_{22}^{-1}A_{21} = B_{22}^{-1}B_{21}$.", "B) When $A_{22} + B_{22}$ is invertible.", "C) When $A_{22} > 0$ and $B_{22} > 0$.", "D) When $(A_{22} + B_{22})^{-1}(A_{21} + B_{21}) = A_{22}^{-1}A_{21}$."], "category": "CORRECT", "question_context_used": "For symmetric matrices $A$ and $B$ partitioned conformably, the inequality $(A + B)_{11\\cdot2} \\geq A_{11\\cdot2} + B_{11\\cdot2}$ holds under certain conditions, with equality if and only if specific conditions involving the submatrices are met.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"D\"]\n}", "model_answer": ["D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n**Explanation**: Option A correctly describes the computation of the observed test statistic. Option C accurately describes how the $p$-value is estimated. Option D is correct because the permutation test is designed to be valid without assuming the asymptotic normality of $\\hat{\\varDelta}_{n}^{\\ast}$. Option B is incorrect because the permutation involves $U_{2}$, not $U_{1}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["Robust-dimension-reduction-based-on-canonical_2009.md"], "background_text": "The permutation test for WCANCOR involves computing a test statistic based on eigenvalues, permuting parts of the projected predictor matrix, and estimating a $p$-value based on the permutation distribution. The test is designed to work without relying on the asymptotic normality of $\\hat{\\varDelta}_{n}^{\\ast}$.", "folder": "", "qid": "statistic-mcqm-4966", "question": "In the context of the permutation test for WCANCOR, which of the following steps are correctly described?", "options": ["A) Compute the observed value of the test statistic $\\varOmega_{s,obs}^{\\ast}$ using the formula $-\\{n-(p+m+k_{n}+2)/2\\}\\sum_{j=s+1}^{p}\\log(1-\\hat{\\gamma}_{j}^{2})$.", "B) Randomly permute the rows of $U_{1}$ to get $U_{1}^{\\prime}$ and use $U^{\\prime}=(U_{1}^{\\prime},U_{2})$ for the permutation test.", "C) The $p$-value for testing $H_{0,s}$ is estimated as the fraction of $\\varOmega_{s}^{\\ast\\prime}$ exceeding $\\varOmega_{s,obs}^{\\ast}$.", "D) The permutation test does not depend on the asymptotic normality of $\\hat{\\varDelta}_{n}^{\\ast}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The permutation test for WCANCOR involves computing a test statistic based on eigenvalues, permuting parts of the projected predictor matrix, and estimating a $p$-value based on the permutation distribution. The test is designed to work without relying on the asymptotic normality of $\\hat{\\varDelta}_{n}^{\\ast}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C\n\n**Explanation**: \n- **A) Incorrect**: The text states that $\\sigma$ and $\\overline{\\sigma}$ are random themselves, making their estimation non-classical due to this randomness.\n- **B) Correct**: The text mentions that estimators $\\hat{\\sigma}_{n}^{2}$ and $\\hat{\\overline{\\sigma}}_{n}^{2}$ are derived to separate the parts of the random increments linked to each Brownian integral.\n- **C) Correct**: The text explicitly states that the trajectories $t\\sim(\\sigma^{2}(\\omega)_{t},\\overline{\\sigma}^{2}(\\omega)_{t})$ are estimated pointwise as realizations of the volatility process.\n- **D) Incorrect**: The text indicates that the model cannot be identified for $d=1$, but this is not a requirement for estimating $\\sigma$ and $\\overline{\\sigma}$; estimation is possible for $d \\geq 2$.\n\n**Final Answer**: The final answer is $\\boxed{{B, C}}$", "source": ["Scandinavian J Statistics - 2019 - Féron - Efficie.md"], "background_text": "The text discusses the nonparametric estimation of the volatility components $\\sigma$ and $\\overline{\\sigma}$. It highlights that these components are random, and their estimation involves pointwise estimation of their trajectories. The estimators are designed to separate the contributions from each Brownian motion. The model's identification requires $d \\geq 2$.", "folder": "", "qid": "statistic-mcqm-5832", "question": "Which of the following statements about the estimation of nonparametric components $\\sigma$ and $\\overline{\\sigma}$ in the multidimensional diffusion process are true?", "options": ["A) The nonparametric estimation of $\\sigma$ and $\\overline{\\sigma}$ is classical and does not involve any randomness.", "B) The estimators $\\hat{\\sigma}_{n}^{2}$ and $\\hat{\\overline{\\sigma}}_{n}^{2}$ are derived to separate the parts of the random increments linked to each Brownian integral.", "C) The trajectories $t\\sim(\\sigma^{2}(\\omega)_{t},\\overline{\\sigma}^{2}(\\omega)_{t})$ are estimated pointwise as realizations of the volatility process.", "D) The estimation of $\\sigma$ and $\\overline{\\sigma}$ requires the model to be identified for $d=1$."], "category": "CORRECT", "question_context_used": "The text discusses the nonparametric estimation of the volatility components $\\sigma$ and $\\overline{\\sigma}$. It highlights that these components are random, and their estimation involves pointwise estimation of their trajectories. The estimators are designed to separate the contributions from each Brownian motion. The model's identification requires $d \\geq 2$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The matrix $A$ is of size $M \\times N$ where $M = \\binom{N}{2}$, as it represents all unordered pairs of parties.\n- **B)** Correct. The matrix $A$ is defined as $\\frac{1}{N-1}B^{\\prime}$ where $B_{ik} = b_{ik}$, as stated in the context.\n- **C)** Incorrect. The matrix $A$ is used to derive the estimator $\\hat{\\boldsymbol{p}}_i$, not to directly compute the voting intentions $\\boldsymbol{p}$.\n- **D)** Correct. The matrix $A$ ensures that the probability $u_{ij}$ is correctly modeled as $\\frac{1}{N-1}(p_i + p_j)$, as it is part of the equation $\\boldsymbol{u} = A\\boldsymbol{p}$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$.", "source": ["Scandinavian J Statistics - 2019 - Lagerås - How t.md"], "background_text": "The matrix $A$ is defined as $\\frac{1}{N-1}B^{\\prime}$, where $B_{ik} = b_{ik}$ and $b_{ik} = 1$ if $i \\in P_k$ and $0$ otherwise. The matrix $A$ is used in the derivation of the unbiased estimator $\\hat{\\boldsymbol{p}}_i$ and helps in modeling the probability $u_{ij}$.", "folder": "", "qid": "statistic-mcqm-5782", "question": "In the context of the pair method for voting intentions, which of the following statements about the matrix $A$ are true?", "options": ["A) The matrix $A$ is of size $M \\times N$ where $M = \\binom{N}{2}$.", "B) The matrix $A$ is defined as $\\frac{1}{N-1}B^{\\prime}$ where $B_{ik} = b_{ik}$.", "C) The matrix $A$ is used to directly compute the voting intentions $\\boldsymbol{p}$ without any estimator.", "D) The matrix $A$ ensures that the probability $u_{ij}$ is correctly modeled as $\\frac{1}{N-1}(p_i + p_j)$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The matrix $A$ is defined as $\\frac{1}{N-1}B^{\\prime}$, where $B_{ik} = b_{ik}$ and $b_{ik} = 1$ if $i \\in P_k$ and $0$ otherwise. The matrix $A$ is used in the derivation of the unbiased estimator $\\hat{\\boldsymbol{p}}_i$ and helps in modeling the probability $u_{ij}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n**Explanation**:\n- **A) Correct**: The text states that when $d=0$, $T_{n}$ degenerates to the test of Guo et al. (2016).\n- **B) Incorrect**: The text mentions that the proportion of distortion errors slightly, not significantly, affects the proposed test.\n- **C) Incorrect**: The text explicitly states that the limiting null distribution of $T_{n}$ does not change with increasing $d$.\n- **D) Correct**: The text confirms that the estimation accuracy of $\\hat{\\theta}_{n}$ and $\\hat{B}$ deteriorates significantly as $d$ increases.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["A-goodness-of-fit-test-for-variable-adju_2019_Comp.md"], "background_text": "The case with $d=0$ implies that the predictor vector $X$ is well measured without distortion errors. The proposed test $T_{n}$ degenerates to the test of Guo et al. (2016), which is designed for classical regression models. The proportion of distortion errors slightly affects the proposed test. The estimation accuracy of $\\hat{\\theta}_{n}$ and $\\hat{B}$ deteriorates significantly when $d$ increases. The distortion errors enlarge the asymptotic variance of $\\hat{\\theta}_{n}$ and $\\hat{B}$, but do not change the limiting null distribution of $T_{n}$.", "folder": "", "qid": "statistic-mcqm-5940", "question": "Regarding the test $T_{n}$ in the context of distortion errors, which of the following statements are correct?", "options": ["A) $T_{n}$ degenerates to the test of Guo et al. (2016) when $d=0$.", "B) The proportion of distortion errors significantly affects the proposed test $T_{n}$.", "C) The limiting null distribution of $T_{n}$ changes with increasing $d$.", "D) The estimation accuracy of $\\hat{\\theta}_{n}$ and $\\hat{B}$ deteriorates significantly when $d$ increases."], "category": "CORRECT", "question_context_used": "The case with $d=0$ implies that the predictor vector $X$ is well measured without distortion errors. The proposed test $T_{n}$ degenerates to the test of Guo et al. (2016), which is designed for classical regression models. The proportion of distortion errors slightly affects the proposed test. The estimation accuracy of $\\hat{\\theta}_{n}$ and $\\hat{B}$ deteriorates significantly when $d$ increases. The distortion errors enlarge the asymptotic variance of $\\hat{\\theta}_{n}$ and $\\hat{B}$, but do not change the limiting null distribution of $T_{n}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct, as the text explicitly states that $\\hat{\\sigma}_{n}^{2}$ is a consistent estimator of $\\sigma^{2}$.\n- **B)** Correct, because $\\hat{\\sigma}_{n}^{2}$ is used in the computation of $\\widehat{h}(1)$, which is part of the statistic $\\mathcal{T}$ used to test $H_{0}$.\n- **C)** Correct, since the distribution of the residuals under $H_{0}$ is independent of the constant value of the regression function, and $\\hat{\\sigma}_{n}^{2}$ is related to the variance of these residuals.\n- **D)** Incorrect, because the text mentions the use of $\\hat{\\sigma}_{n}^{2}$ in both homoscedastic and heteroscedastic cases, with adjustments like $\\sigma_{Rice}^{2\\star}$ in the latter.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Testing-constancy-in-monotone-response_2014_Comput.md"], "background_text": "The text discusses a consistent estimator $\\hat{\\sigma}_{n}^{2}$ of $\\sigma^{2}$ used in a bootstrap method for testing a null hypothesis $H_{0}$. The estimator is part of a process that involves computing a statistic $\\mathcal{T}$ and generating bootstrap samples to estimate the distribution function $Q$ under $H_{0}$.", "folder": "", "qid": "statistic-mcqm-1048", "question": "Given the context, which of the following statements are true about the estimator $\\hat{\\sigma}_{n}^{2}$?", "options": ["A) $\\hat{\\sigma}_{n}^{2}$ is a consistent estimator of $\\sigma^{2}$.", "B) $\\hat{\\sigma}_{n}^{2}$ is used to estimate the variance under the null hypothesis.", "C) $\\hat{\\sigma}_{n}^{2}$ is independent of the regression function under $H_{0}$.", "D) $\\hat{\\sigma}_{n}^{2}$ is only applicable in the homoscedastic case."], "category": "PARTIALLY_CORRECT", "question_context_used": "The text discusses a consistent estimator $\\hat{\\sigma}_{n}^{2}$ of $\\sigma^{2}$ used in a bootstrap method for testing a null hypothesis $H_{0}$. The estimator is part of a process that involves computing a statistic $\\mathcal{T}$ and generating bootstrap samples to estimate the distribution function $Q$ under $H_{0}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A) Correct**: $V_{\\psi}$ is indeed close to a normal distribution with variance 1.\n- **B) Correct**: The expression for $V_{\\psi}$ is correctly given as $3\\sqrt{n\\{\\bar{X}^{\\frac{1}{2}}-\\psi\\bar{X}^{1/6}\\}$.\n- **C) Correct**: $\\bar{X}^{2/3}$ has zero asymptotic skewness, making it close to a normal distribution.\n- **D) Incorrect**: $V_{\\psi}$ is related to the normal distribution as it is close to it with variance 1.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Parametrizations of Non-Linear Models.md"], "background_text": "For normal likelihood with $\\psi=(E X)^{1/3}$, $V_{\\psi}$ is defined as $3\\sqrt{n\\{\\bar{X}^{\\frac{1}{2}}-\\psi\\bar{X}^{1/6}\\}$ and is close to a normal distribution with variance 1. Additionally, $\\bar{X}^{2/3}$ has zero asymptotic skewness.", "folder": "", "qid": "statistic-mcqm-5246", "question": "In the context of the normal likelihood where $\\psi=(E X)^{1/3}$, which statements about $V_{\\psi}$ are correct?", "options": ["A) $V_{\\psi}$ is close to a normal distribution with variance 1", "B) $V_{\\psi} = 3\\sqrt{n\\{\\bar{X}^{\\frac{1}{2}}-\\psi\\bar{X}^{1/6}\\}$", "C) $\\bar{X}^{2/3}$ has zero asymptotic skewness", "D) $V_{\\psi}$ is not related to the normal distribution"], "category": "CORRECT", "question_context_used": "For normal likelihood with $\\psi=(E X)^{1/3}$, $V_{\\psi}$ is defined as $3\\sqrt{n\\{\\bar{X}^{\\frac{1}{2}}-\\psi\\bar{X}^{1/6}\\}$ and is close to a normal distribution with variance 1. Additionally, $\\bar{X}^{2/3}$ has zero asymptotic skewness.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}", "model_answer": ["A", "B", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The first regularized discriminant co-ordinate is indeed obtained by solving a constrained optimization problem, as described in the context.\n- **B) Correct**: The regularization process involves minimizing $\\mathbf{w}^{\\mathrm{T}}\\pmb{\\Sigma}\\mathbf{w}$ under the constraints $\\|\\mathbf{w}\\|_{1}\\leqslant c$ and $\\mathbf{w}^{\\mathrm{T}}\\mathbf{B}\\mathbf{w}=1$.\n- **C) Incorrect**: The second regularized discriminant co-ordinate is found with an additional constraint $\\bar{\\bf w}_{1}^{*\\mathrm{T}}{\\bf\\Sigma w}=0$, not without any additional constraints.\n- **D) Correct**: Regularization is used to address issues like diverging spectrum and noise accumulation in the sample.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["A Road to Classification in High Dimensional Space.md"], "background_text": "In the large $p$ – small $n$ scenario, regularization is necessary for selecting discriminant co-ordinates. The first regularized discriminant co-ordinate is obtained by solving a constrained optimization problem. The regularization involves minimizing $\\mathbf{w}^{\\mathrm{T}}\\pmb{\\Sigma}\\mathbf{w}$ under specific constraints. Additional constraints are applied for subsequent co-ordinates. Regularization helps address issues like diverging spectrum and noise accumulation.", "folder": "", "qid": "statistic-mcqm-1503", "question": "Which of the following are correct about the regularization in selecting discriminant co-ordinates in the large $p$ – small $n$ scenario?", "options": ["A) The first regularized discriminant co-ordinate $\\bar{\\bf w}_{1}^{*}$ is obtained by solving a constrained optimization problem.", "B) The regularization involves minimizing $\\mathbf{w}^{\\mathrm{T}}\\pmb{\\Sigma}\\mathbf{w}$ under the constraint $\\|\\mathbf{w}\\|_{1}\\leqslant c$ and $\\mathbf{w}^{\\mathrm{T}}\\mathbf{B}\\mathbf{w}=1$.", "C) The second regularized discriminant co-ordinate is found without any additional constraints compared to the first.", "D) Regularization addresses issues like diverging spectrum and noise accumulation in the sample."], "category": "PARTIALLY_CORRECT", "question_context_used": "In the large $p$ – small $n$ scenario, regularization is necessary for selecting discriminant co-ordinates. The first regularized discriminant co-ordinate is obtained by solving a constrained optimization problem. The regularization involves minimizing $\\mathbf{w}^{\\mathrm{T}}\\pmb{\\Sigma}\\mathbf{w}$ under specific constraints. Additional constraints are applied for subsequent co-ordinates. Regularization helps address issues like diverging spectrum and noise accumulation.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C, D\n\n- **A)** Correct. The function $f_{q}(\\pmb{x})$ explicitly includes an intercept term $\\theta_{q,0}$.\n- **B)** Correct. The penalty term $J(\\pmb{f})$ involves the Gram matrix $\\pmb{K}$ in the term $\\pmb{\\theta}_{q}^{\\top}\\pmb{K}\\pmb{\\theta}_{q}$.\n- **C)** Correct. The Gram matrix $\\pmb{K}$ is assumed to be invertible for the optimization problem.\n- **D)** Correct. The optimization problem does not require the computation of $K^{-1}$ directly, as it leverages properties of the kernel matrix.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C, D}}$", "source": ["Adaptively-weighted-large-margin-angle-based-_2018.md"], "background_text": "For nonlinear learning with kernel $K(\\cdot,\\cdot)$, the function $f_{q}(\\pmb{x})$ is defined as $\\theta_{q,0}+\\sum_{i=1}^{n}\\theta_{q,i}K(\\pmb{x}_{i},\\pmb{x})$. The penalty term $J(\\pmb{f})$ is $\\sum_{q=1}^{k-1}\\pmb{\\theta}_{q}^{\\top}\\pmb{K}\\pmb{\\theta}_{q}+\\sum_{q=1}^{k-1}\\theta_{q,0}^{2}$. The Gram matrix $\\pmb{K}$ is assumed to be invertible, but the optimization problem does not require computing $K^{-1}$ directly.", "folder": "", "qid": "statistic-mcqm-6131", "question": "In the context of nonlinear learning with kernel $K(\\cdot,\\cdot)$ for WASVM1, which of the following statements are correct?", "options": ["A) The function $f_{q}(\\pmb{x})$ includes an intercept term $\\theta_{q,0}$.", "B) The penalty term $J(\\pmb{f})$ involves the Gram matrix $\\pmb{K}$.", "C) The Gram matrix $\\pmb{K}$ must be invertible for the optimization problem.", "D) The optimization problem does not require the computation of the inverse matrix $K^{-1}$."], "category": "PARTIALLY_CORRECT", "question_context_used": "For nonlinear learning with kernel $K(\\cdot,\\cdot)$, the function $f_{q}(\\pmb{x})$ is defined as $\\theta_{q,0}+\\sum_{i=1}^{n}\\theta_{q,i}K(\\pmb{x}_{i},\\pmb{x})$. The penalty term $J(\\pmb{f})$ is $\\sum_{q=1}^{k-1}\\pmb{\\theta}_{q}^{\\top}\\pmb{K}\\pmb{\\theta}_{q}+\\sum_{q=1}^{k-1}\\theta_{q,0}^{2}$. The Gram matrix $\\pmb{K}$ is assumed to be invertible, but the optimization problem does not require computing $K^{-1}$ directly.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A) Correct**: The text explicitly states that controlling FDR is inappropriate when efficacy in all endpoints is required.\n- **B) Correct**: The text provides a scenario where most endpoints are highly efficacious, allowing the Type I error rate for the remaining endpoint to exceed $\\alpha$ while FDR is controlled at $\\alpha$.\n- **C) Incorrect**: The text does not support this statement; instead, it shows that FDR control can allow the Type I error rate for individual tests to exceed $\\alpha$ under certain conditions.\n- **D) Correct**: The text clearly states that in the given parameter configuration, $H_{0m+1}$ is tested at $(m+1)\\alpha$.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["Hochberg's_Step-Up_Method_Cutting_Corners_Off_Holm's_Step-Down_Method.md"], "background_text": "The text argues that controlling the False Discovery Rate (FDR) is inappropriate in clinical trials where efficacy in all endpoints is required. It explains that if most endpoints are highly efficacious, the Type I error rate for the remaining endpoint can exceed $\\alpha$ while FDR is still controlled at $\\alpha$. Specifically, $H_{0m+1}$ is tested at $(m+1)\\alpha$ in the given configuration.", "folder": "", "qid": "statistic-mcqm-5968", "question": "Regarding the control of the False Discovery Rate (FDR) in clinical trials, which of the following statements are true?", "options": ["A) Controlling FDR is inappropriate when efficacy in all endpoints is required.", "B) If most endpoints are highly efficacious, the Type I error rate for the remaining endpoint can exceed $\\alpha$ while FDR is still controlled at $\\alpha$.", "C) FDR control ensures that the Type I error rate for each individual test does not exceed $\\alpha$.", "D) In the given parameter configuration, $H_{0m+1}$ is tested at $(m+1)\\alpha$."], "category": "CORRECT", "question_context_used": "The text argues that controlling the False Discovery Rate (FDR) is inappropriate in clinical trials where efficacy in all endpoints is required. It explains that if most endpoints are highly efficacious, the Type I error rate for the remaining endpoint can exceed $\\alpha$ while FDR is still controlled at $\\alpha$. Specifically, $H_{0m+1}$ is tested at $(m+1)\\alpha$ in the given configuration.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A) If $\\lambda = 0$, $X$ follows a standard normal distribution.**: Correct, as the context states that when $\\lambda=0$, the standard normal distribution is recuperated.\n- **B) The probability density function of $X$ is given by $2\\phi(x)\\phi(\\lambda x)$.**: Correct, as this is the given p.d.f. in the context.\n- **C) The skew-normal distribution is always symmetric.**: Incorrect, the skew-normal distribution is asymmetric unless $\\lambda=0$.\n- **D) The skew-normal distribution can model both left and right skewness.**: Correct, as the skewness parameter $\\lambda$ can be positive or negative, allowing for both left and right skewness.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["On-the-Fern-ndez-Steel-distribution--Infere_2011_C.md"], "background_text": "If a random variable (r.v.) $X$ follows a standard SN distribution with skewness (asymmetry) parameter $\\lambda$, the notation $X\\sim\\operatorname{SN}\\left(\\lambda\\right)$ is used and its probability density function (p.d.f.) is given by $f\\left(\\boldsymbol{x};\\lambda\\right)=2\\phi\\left(\\boldsymbol{x}\\right)\\phi\\left(\\lambda\\boldsymbol{x}\\right),\\quad\\boldsymbol{x}\\in\\mathbb{R},\\lambda\\in\\mathbb{R}$, where $\\phi$ and $\\phi$ denote the standard normal p.d.f. and cumulative distribution function (c.d.f.), respectively. Note that if $X\\sim\\operatorname{SN}\\left(\\lambda\\right)$ and $\\lambda=0$, then the standard normal distribution is recuperated.", "folder": "", "qid": "statistic-mcqm-5911", "question": "For a random variable $X$ following a standard skew-normal distribution with skewness parameter $\\lambda$, which of the following statements are true?", "options": ["A) If $\\lambda = 0$, $X$ follows a standard normal distribution.", "B) The probability density function of $X$ is given by $2\\phi(x)\\phi(\\lambda x)$.", "C) The skew-normal distribution is always symmetric.", "D) The skew-normal distribution can model both left and right skewness."], "category": "CORRECT", "question_context_used": "If a random variable (r.v.) $X$ follows a standard SN distribution with skewness (asymmetry) parameter $\\lambda$, the notation $X\\sim\\operatorname{SN}\\left(\\lambda\\right)$ is used and its probability density function (p.d.f.) is given by $f\\left(\\boldsymbol{x};\\lambda\\right)=2\\phi\\left(\\boldsymbol{x}\\right)\\phi\\left(\\lambda\\boldsymbol{x}\\right),\\quad\\boldsymbol{x}\\in\\mathbb{R},\\lambda\\in\\mathbb{R}$, where $\\phi$ and $\\phi$ denote the standard normal p.d.f. and cumulative distribution function (c.d.f.), respectively. Note that if $X\\sim\\operatorname{SN}\\left(\\lambda\\right)$ and $\\lambda=0$, then the standard normal distribution is recuperated.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A) Correct**: Observing at least one transition and the first sojourn times and the first state is necessary to identify the parameters of the mixture.\n- **B) Correct**: The condition $\\alpha_{l}^{g}>0$ ensures that all possible transitions can be observed during the first transition, which is necessary for parameter identification.\n- **C) Correct**: The transition probabilities $P_{l j}^{g}$ must be strictly positive to avoid unobserved transitions, which would prevent associating a duration time distribution with those transitions.\n- **D) Incorrect**: While knowing the number of mixture components $G$ is necessary for estimation, it is not a condition for identifying the parameters of the mixture.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Estimating Finite Mixtures of Semi-Markov Chains A.md"], "background_text": "The parameters of a finite mixture can be identified from $\\mathcal{F}(\\boldsymbol{\\Theta})$ provided that we can observe at least one transition and the first sojourn times and the first state. The condition $\\alpha_{l}^{g}>0$ ensures that all possible transitions can be observed during the first transition. The condition on the transition probabilities that must be strictly positive is essentially of a technical nature and enables us to simplify the demonstration.", "folder": "", "qid": "statistic-mcqm-5752", "question": "In the context of finite mixtures of semi-Markov processes, which of the following conditions are necessary to identify the parameters of the mixture?", "options": ["A) Observing at least one transition and the first sojourn times and the first state", "B) The condition $\\alpha_{l}^{g}>0$ for all possible transitions", "C) The transition probabilities $P_{l j}^{g}$ must be strictly positive", "D) The number of mixture components $G$ must be known"], "category": "CORRECT", "question_context_used": "The parameters of a finite mixture can be identified from $\\mathcal{F}(\\boldsymbol{\\Theta})$ provided that we can observe at least one transition and the first sojourn times and the first state. The condition $\\alpha_{l}^{g}>0$ ensures that all possible transitions can be observed during the first transition. The condition on the transition probabilities that must be strictly positive is essentially of a technical nature and enables us to simplify the demonstration.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**: \n- **A** is correct because it directly quotes the definition of $D$ from the context.\n- **B** is incorrect because the context specifies that $D$ is open, not closed.\n- **C** is correct because the injectivity condition $\\theta_{1}\\neq\\theta_{2} \\Rightarrow P_{\\theta_{1}}\\neq P_{\\theta_{2}}$ is part of the regularity condition that implies $D$ is open.\n- **D** is incorrect because $D$ is the parameter space for $\\theta$, not for the sufficient statistics $t(x)$.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Parametrizations of Non-Linear Models.md"], "background_text": "The parameter set $D$ for an $\\pmb{n}$-dimensional exponential family is defined as $D=\\left\\{\\theta|\\int\\exp\\left\\{\\theta^{\\prime}t(x)\\right\\}\\mu(d x)<\\infty\\right\\}$. It is assumed to be open, and the family is full and regular, meaning $\\theta_{1}\\neq\\theta_{2} \\Rightarrow P_{\\theta_{1}}\\neq P_{\\theta_{2}}$.", "folder": "", "qid": "statistic-mcqm-5224", "question": "In the context of an $\\pmb{n}$-dimensional exponential family with canonical parameter $\\pmb{\\theta}$, which of the following statements are true about the parameter set $D$?", "options": ["A) $D$ is defined as the set of $\\theta$ for which the integral $\\int\\exp\\left\\{\\theta^{\\prime}t(x)\\right\\}\\mu(d x)$ is finite.", "B) $D$ is always a closed set.", "C) The condition $\\theta_{1}\\neq\\theta_{2} \\Rightarrow P_{\\theta_{1}}\\neq P_{\\theta_{2}}$ implies that $D$ is open.", "D) $D$ is the parameter space for the sufficient statistics $t(x)$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The parameter set $D$ for an $\\pmb{n}$-dimensional exponential family is defined as $D=\\left\\{\\theta|\\int\\exp\\left\\{\\theta^{\\prime}t(x)\\right\\}\\mu(d x)<\\infty\\right\\}$. It is assumed to be open, and the family is full and regular, meaning $\\theta_{1}\\neq\\theta_{2} \\Rightarrow P_{\\theta_{1}}\\neq P_{\\theta_{2}}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n**Explanation**:\n- **A)** Correct. According to the theorem, $\\hat{\\theta}_{m}^{*}$ is consistent for $\\theta \\in \\Theta \\setminus \\Theta_{0}$.\n- **B)** Incorrect. The theorem states that $\\hat{\\theta}_{m}^{*}$ is inconsistent for $\\theta \\in \\Theta_{0}$.\n- **C)** Correct. The text discusses Edgeworth expansions for approximating the distribution of $\\hat{\\theta}_{m}^{*}$.\n- **D)** Incorrect. The distribution of $\\hat{\\theta}_{m}^{*}$ depends on $\\theta$'s location and is not always normal.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Bootstrapping-in-non-regular-smooth-function_2012_.md"], "background_text": "The $m$-out-of-$n$ bootstrap resample involves drawing a sample of size $m$ with replacement from $X_{1:n}$, where $m=o(n)$ and $m\\rightarrow\\infty$. The bootstrap version of the estimator is $\\hat{\\theta}_{m}^{*} = f(\\bar{X}_{m}^{*})$. The consistency and distributional properties of $\\hat{\\theta}_{m}^{*}$ depend on whether $\\theta$ is in $\\Theta_{0}$ or $\\Theta \\setminus \\Theta_{0}$.", "folder": "", "qid": "statistic-mcqm-5315", "question": "Given the $m$-out-of-$n$ bootstrap resample $X_{1:m}^{*}$ from $X_{1:n}$, with $m=o(n)$ and $m\\rightarrow\\infty$, which of the following statements about the bootstrap version $\\hat{\\theta}_{m}^{*} = f(\\bar{X}_{m}^{*})$ are correct?", "options": ["A) $\\hat{\\theta}_{m}^{*}$ is a consistent estimator for $\\theta$ when $\\theta \\in \\Theta \\setminus \\Theta_{0}$.", "B) $\\hat{\\theta}_{m}^{*}$ is a consistent estimator for $\\theta$ when $\\theta \\in \\Theta_{0}$.", "C) The distribution of $\\hat{\\theta}_{m}^{*}$ can be approximated using Edgeworth expansions.", "D) $\\hat{\\theta}_{m}^{*}$ is always normally distributed regardless of $\\theta$'s location in $\\Theta$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The $m$-out-of-$n$ bootstrap resample involves drawing a sample of size $m$ with replacement from $X_{1:n}$, where $m=o(n)$ and $m\\rightarrow\\infty$. The bootstrap version of the estimator is $\\hat{\\theta}_{m}^{*} = f(\\bar{X}_{m}^{*})$. The consistency and distributional properties of $\\hat{\\theta}_{m}^{*}$ depend on whether $\\theta$ is in $\\Theta_{0}$ or $\\Theta \\setminus \\Theta_{0}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A) Correct**: The text mentions that ignoring the within-subject correlation structure can be a strategy for estimation in some cases.\n- **C) Correct**: Incorporating information of the within-subject correlations is mentioned as a method to improve estimation efficiency.\n- **D) Correct**: The text proposes using spline approximation with weights obtained from empirical likelihood to improve estimation efficiency.\n\n- **B) Incorrect**: While kernel smoothing methods are mentioned, the text does not highlight them as a method for improving estimation efficiency in the context of the VCQR model.\n\n**Final Answer**: The final answer is $\boxed{{A, C, D}}$", "source": ["Weighted-quantile-regression-in-varying-coeffic_20.md"], "background_text": "The text discusses various methods for improving estimation efficiency in longitudinal data analysis, including ignoring the within-subject correlation structure, using kernel smoothing methods, incorporating within-subject correlations, and using spline approximation with weights from empirical likelihood.", "folder": "", "qid": "statistic-mcqm-3015", "question": "What are the methods mentioned in the text for improving estimation efficiency in longitudinal data analysis?", "options": ["A) Ignoring the within-subject correlation structure.", "B) Using kernel smoothing methods.", "C) Incorporating information of the within-subject correlations.", "D) Using spline approximation with weights obtained from empirical likelihood."], "category": "PARTIALLY_CORRECT", "question_context_used": "The text discusses various methods for improving estimation efficiency in longitudinal data analysis, including ignoring the within-subject correlation structure, using kernel smoothing methods, incorporating within-subject correlations, and using spline approximation with weights from empirical likelihood.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A\n\n**Explanation**:\n- **A) Correct**: This is the expression for $\\tilde{\\gamma}$ as given in the context.\n- **B) Incorrect**: This is the expression derived from the method of moments, not the combined estimator.\n- **C) Incorrect**: This is the expression for $\\widehat{\\gamma}_{k}$ in the case $\\gamma > 0$, not for $\\tilde{\\gamma}$ in the case $\\gamma < 0$.\n- **D) Incorrect**: This is the expression for $\\widehat{\\sigma}_{k}$ in the case $\\gamma > 0$, not for $\\tilde{\\gamma}$ in the case $\\gamma < 0$.\n\n**Final Answer**: The final answer is $\\boxed{{A}}$", "source": ["A-bootstrap-goodness-of-fit-test-for-the-gener_200.md"], "background_text": "For the case $\\gamma < 0$, the combined estimator $\\tilde{\\gamma}$ is given by $\\tilde{\\gamma} = \\frac{\\bar{X}}{\\bar{X} - X_{(n)}}$.", "folder": "", "qid": "statistic-mcqm-5260", "question": "For the case $\\gamma < 0$, the combined estimator $\\tilde{\\gamma}$ is given by which of the following expressions?", "options": ["A) $\\tilde{\\gamma} = \\frac{\\bar{X}}{\bar{X} - X_{(n)}}$", "B) $\\tilde{\\gamma} = 1 - \\frac{\\sigma}{\\bar{X}}$", "C) $\\tilde{\\gamma} = -\\left(W_{n-k+1} - \\frac{1}{k}\\sum_{j=1}^{k} W_{n-j+1}\\right)$", "D) $\\tilde{\\gamma} = \\widehat{\\gamma}_{k} \\exp\\left\\{W_{n-k+1} + \\widehat{\\gamma}_{k} \\log(k/n)\\right\\}$"], "category": "CORRECT", "question_context_used": "For the case $\\gamma < 0$, the combined estimator $\\tilde{\\gamma}$ is given by $\\tilde{\\gamma} = \\frac{\\bar{X}}{\\bar{X} - X_{(n)}}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n**Explanation**:\n- **A) Correct**: The text states that at $90\\%$ power, the difference in detectable bias in the support for SD is approximately one percentage point.\n- **B) Incorrect**: The detectable bias is not independent of the total survey size $n$; larger $n$ generally allows for detection of smaller biases.\n- **C) Incorrect**: The detectable bias typically increases (not decreases) as the power of the test increases, because higher power allows detection of smaller effects.\n- **D) Correct**: The detectable bias is influenced by the allocation between the list and pair methods, as the optimized allocation leads to a substantial gain in power.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Scandinavian J Statistics - 2019 - Lagerås - How t.md"], "background_text": "The power calculation for bias detection is performed with a total survey size of $n=15000$, $p=10\\%$, $N=10$, and a confidence level (type I error) of $\\gamma=5\\%$. The allocation between the list and pair methods is optimized to maximize power, and at $90\\%$ power, the difference in detectable bias in the support for SD is approximately one percentage point.", "folder": "", "qid": "statistic-mcqm-5871", "question": "Given the power calculation for bias detection with $n=15000$, $p=10\\%$, $N=10$, and a confidence level (type I error) of $\\gamma=5\\%$, which of the following statements about the detectable bias are correct?", "options": ["A) At $90\\%$ power, the difference in detectable bias in the support for SD is approximately one percentage point.", "B) The detectable bias is independent of the total survey size $n$.", "C) The detectable bias decreases as the power of the test increases.", "D) The detectable bias is influenced by the allocation between the list and pair methods."], "category": "CORRECT", "question_context_used": "The power calculation for bias detection is performed with a total survey size of $n=15000$, $p=10\\%$, $N=10$, and a confidence level (type I error) of $\\gamma=5\\%$. The allocation between the list and pair methods is optimized to maximize power, and at $90\\%$ power, the difference in detectable bias in the support for SD is approximately one percentage point.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, D\n\n- **A)** Incorrect. Pidigits (l) shows the same number of accurate digits (15) for all statistics in both Excel versions.\n- **B)** Correct. Mavro (l) shows differences in the number of accurate digits for $s$ and $\\rho$ between Excel97/00/02 and Excel2003.\n- **C)** Incorrect. Numacc1 (l) shows the same number of accurate digits (15) for all statistics in both Excel versions.\n- **D)** Correct. Numacc4 (h) shows differences in the number of accurate digits for $\\bar{x}$, $s$, and $\\rho$ between Excel97/00/02 and Excel2003.\n\n**Final Answer**: The final answer is $\\boxed{{B, D}}$", "source": ["On-the-accuracy-of-statistical-procedures-in_2005_.md"], "background_text": "Table 3 shows the number of accurate digits for $\\bar{x}$, $s$, and $\\rho$ (the mean, standard deviation, and correlation coefficient) for different data sets across Excel97/00/02 and Excel2003 versions. Some data sets show differences in the number of accurate digits between these versions.", "folder": "", "qid": "statistic-mcqm-937", "question": "Which of the following data sets show a difference in the number of accurate digits for $\\bar{x}$, $s$, and $\\rho$ between Excel97/00/02 and Excel2003?", "options": ["A) Pidigits (l)", "B) Mavro (l)", "C) Numacc1 (l)", "D) Numacc4 (h)"], "category": "PARTIALLY_CORRECT", "question_context_used": "Table 3 shows the number of accurate digits for $\\bar{x}$, $s$, and $\\rho$ (the mean, standard deviation, and correlation coefficient) for different data sets across Excel97/00/02 and Excel2003 versions. Some data sets show differences in the number of accurate digits between these versions.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"C\", \"D\"]\n}", "model_answer": ["C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n- **A)** Correct. The empirical distortion is defined with respect to the empirical measure $P_{n}^{\\tau}$, which is adapted for censored data.\n- **B)** Incorrect. The empirical distortion under censoring does not involve unobserved quantities directly; it uses the available censored data.\n- **C)** Correct. The empirically optimal quantizer $q_{n}^{*}$ is defined as the minimizer of the empirical distortion $\\mathcal{D}(P_{n}^{\\tau},q)$.\n- **D)** Incorrect. While standard clustering algorithms fail due to unobserved distances, the empirical distortion $\\mathcal{D}(P_{n}^{\\tau},q)$ is specifically designed for censored data.\n\n**Final Answer**: The final answer is $\\boxed{{A, C}}$", "source": ["Vector-quantization-and-clustering-in-the-prese_20.md"], "background_text": "In the presence of censoring, the empirical distortion $\\mathcal{D}(P_{n}^{\\tau},q)$ is defined using the empirical measure $P_{n}^{\\tau}$, which is induced by the available sample and accounts for censoring. The empirically optimal quantizer $q_{n}^{*}$ minimizes this distortion. The classical definition of empirical distortion involves unobserved quantities and is not suitable for censored data.", "folder": "", "qid": "statistic-mcqm-6025", "question": "In the context of censored data, which of the following statements are true regarding the empirical distortion $\\mathcal{D}(P_{n}^{\\tau},q)$?", "options": ["A) It is defined using the empirical measure $P_{n}^{\\tau}$ induced by the sample.", "B) It involves unobserved quantities $(T_{i})_{1\\leq i\\leq n}$ directly.", "C) It is minimized by the empirically optimal quantizer $q_{n}^{*}$.", "D) It cannot be used in the presence of censoring because the distances between censored observations and other points are not observed."], "category": "CORRECT", "question_context_used": "In the presence of censoring, the empirical distortion $\\mathcal{D}(P_{n}^{\\tau},q)$ is defined using the empirical measure $P_{n}^{\\tau}$, which is induced by the available sample and accounts for censoring. The empirically optimal quantizer $q_{n}^{*}$ minimizes this distortion. The classical definition of empirical distortion involves unobserved quantities and is not suitable for censored data.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**: \n- **Option A** is correct because Assumption 2 explicitly states that when $\\alpha \\neq -1$, the censoring $C$ must satisfy condition $I2^{\\prime}$.\n- **Option B** is correct because when $\\alpha = -1$, the censoring $C$ must satisfy conditions $II$ and $\\boldsymbol{{I2}}$. However, the question specifies the scenario when $\\alpha \\neq -1$, making Option B also correct as it is part of the broader condition.\n- **Option C** is incorrect because condition $\\boldsymbol{{I2}}$ is only required when $\\alpha = -1$, not when $\\alpha \\neq -1$.\n- **Option D** is incorrect because condition $I$ is not mentioned in Assumption 2 for any value of $\\alpha$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Scandinavian J Statistics - 2008 - DELECROIX - Non.md"], "background_text": "Assumption 2 states that the censoring $C$ satisfies condition $I2^{\\prime}$ if $\\alpha\\neq-1$, and $II$ and $\\boldsymbol{{I2}}$ if $\\alpha=-1$. Moreover, $0<\\tau_{H}=\\tau_{F}\\leq\\infty$, where $\\tau_{L}=i n f\\{t|L(t)=1\\}$ for any distribution function $L$.", "folder": "", "qid": "statistic-mcqm-5143", "question": "Under Assumption 2, which of the following conditions must be satisfied by the censoring $C$ when $\\alpha \\neq -1$?", "options": ["A) Condition $I2^{\\prime}$", "B) Condition $II$", "C) Condition $\\boldsymbol{{I2}}$", "D) Condition $I$"], "category": "PARTIALLY_CORRECT", "question_context_used": "Assumption 2 states that the censoring $C$ satisfies condition $I2^{\\prime}$ if $\\alpha\\neq-1$, and $II$ and $\\boldsymbol{{I2}}$ if $\\alpha=-1$. Moreover, $0<\\tau_{H}=\\tau_{F}\\leq\\infty$, where $\\tau_{L}=i n f\\{t|L(t)=1\\}$ for any distribution function $L$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: B, C\n\n**Explanation**:\n- **A) Incorrect**: The text states that the covariance term is not needed for deriving means and variances in the sub-spaces.\n- **B) Correct**: The text mentions that the covariance term is not needed for deriving means and variances but is required in the filtering step and will be used in the smoothing step.\n- **C) Correct**: The covariance term is derived from the covariance between the \"design\" and \"emulation\" sub-spaces, as indicated by the notation.\n- **D) Incorrect**: The covariance term does not increase the dimensionality of the state space; it relates existing sub-spaces.\n\n**Final Answer**: The final answer is $\\boxed{{B, C}}$", "source": ["Mechanism-based-emulation-of-dynamic-simulation-m_.md"], "background_text": "The covariance term (58c) between the \"design\" and \"emulation\" sub-spaces is not needed for deriving means and variances in these sub-spaces but is required in the filtering step below and will be used in the smoothing step.", "folder": "", "qid": "statistic-mcqm-5660", "question": "Which of the following statements are true about the covariance term (58c) in the context of the Kalman filtering steps?", "options": ["A) It is necessary for deriving means and variances in the \"design\" and \"emulation\" sub-spaces", "B) It is not needed for the filtering step but is required for the smoothing step", "C) It is derived from the covariance between the \"design\" and \"emulation\" sub-spaces", "D) It is used to increase the dimensionality of the state space"], "category": "PARTIALLY_CORRECT", "question_context_used": "The covariance term (58c) between the \"design\" and \"emulation\" sub-spaces is not needed for deriving means and variances in these sub-spaces but is required in the filtering step below and will be used in the smoothing step.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}", "model_answer": ["B", "C"], "error": null}}
{"gold_answer": "To determine which conditions must be satisfied for points $m_1, ..., m_p$ to be considered similar points of the measure $\\mu$, we refer to the definitions provided. The conditions are: $\\frac{d\\mu_{m_{j,\\epsilon}}}{d\\nu_{\\epsilon}} \\geqslant K > 0$ for some $K$ and for all $\\epsilon$ positive and small enough (Option A), and $\\nu_{\\epsilon}(A_{\\epsilon}) > 0$ for all $\\epsilon$ positive and small enough (Option B). Option C describes the definition of $\\mu_{m,\\epsilon}(B)$ but is not a condition for similarity. Option D is not mentioned as a requirement for similarity in the provided context.\n\n**Correct Choices**: A, B\n\n**Final Answer**: The final answer is $\\boxed{{A, B}}$", "source": ["Sur-un-probl-me-de-R--Shimizu_1971_Journal-of-Mult.md"], "background_text": "Points $m_1, ..., m_p$ are similar points of the measure $\\mu$ if, for every sufficiently small positive $\\epsilon$, the following conditions are met: $\\frac{d\\mu_{m_{j,\\epsilon}}}{d\\nu_{\\epsilon}} \\geqslant K > 0$ (a.e. with respect to $\\nu_{\\epsilon}$) and $\\nu_{\\epsilon}(A_{\\epsilon}) > 0$, where $\\nu_{\\epsilon} = \\sum_{j=1}^{p}\\mu_{m_{j,\\epsilon}}$.", "folder": "", "qid": "statistic-mcqm-6045", "question": "Given the definitions and notations, which of the following conditions must be satisfied for points $m_1, ..., m_p$ to be considered similar points of the measure $\\mu$?", "options": ["A) $\\frac{d\\mu_{m_{j,\\epsilon}}}{d\\nu_{\\epsilon}} \\geqslant K > 0$ for some $K$ and for all $\\epsilon$ positive and small enough", "B) $\\nu_{\\epsilon}(A_{\\epsilon}) > 0$ for all $\\epsilon$ positive and small enough", "C) $\\mu_{m,\\epsilon}(B) = \\mu((B - m) \\cap A_{\\epsilon})$ for any Borel set $B$", "D) The measure $\\mu$ must be absolutely continuous with respect to the Lebesgue measure"], "category": "CORRECT", "question_context_used": "Points $m_1, ..., m_p$ are similar points of the measure $\\mu$ if, for every sufficiently small positive $\\epsilon$, the following conditions are met: $\\frac{d\\mu_{m_{j,\\epsilon}}}{d\\nu_{\\epsilon}} \\geqslant K > 0$ (a.e. with respect to $\\nu_{\\epsilon}$) and $\\nu_{\\epsilon}(A_{\\epsilon}) > 0$, where $\\nu_{\\epsilon} = \\sum_{j=1}^{p}\\mu_{m_{j,\\epsilon}}$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "To determine the correct options, we evaluate each statement based on the theorems provided in the context.\n\n- **Option A**: This is correct as per Theorem 1, which states that if $d_{i} + u_{i} < 1$, then $p_{11}^{*} - p_{10}^{*} - p_{01}^{*} + p_{00}^{*} > 0$ implies $p_{11} - p_{10} - p_{01} + p_{00} > 0$.\n\n- **Option B**: This is correct as per Theorem 2, which states that if $d_{i} < 1/2$ and $u_{i} < 1/4$, then $p_{11}^{*} - p_{10}^{*} - p_{01}^{*} > 0$ implies $p_{11} - p_{10} - p_{01} > 0$.\n\n- **Option C**: This is correct as per Theorem 3, which states that if $d_{i} \\leqslant 1/3$ and $u_{i} \\leqslant 1/4$, then $p_{11}^{*} - p_{10}^{*} - p_{01}^{*} - p_{00}^{*} > 0$ implies $p_{11} - p_{10} - p_{01} - p_{00} > 0$.\n\n- **Option D**: This is correct as per Theorem 4, which states that if $d_{i} < 1/4$ and $u_{i} < 1/4$, then $p_{111}^{*} - p_{110}^{*} - p_{101}^{*} - p_{011}^{*} > 0$ implies $p_{111} - p_{110} - p_{101} - p_{011} > 0$.\n\n**Correct Choices**: A, B, C, D\n\n**Final Answer**: The final answer is $\boxed{{A, B, C, D}}$", "source": ["Inference_for_additive_interaction_under_exposure_misclassification.md"], "background_text": "Under assumptions of nondifferential misclassification and independent predicted probabilities, certain inequalities for the observed exposures imply the corresponding inequalities for the true exposures under specific conditions on the misclassification probabilities. These conditions include $d_{i} + u_{i} < 1$ for additive interaction, $d_{i} < 1/2$ and $u_{i} < 1/4$ for sufficient cause interaction, $d_{i} \\leqslant 1/3$ and $u_{i} \\leqslant 1/4$ for epistatic interaction, and $d_{i} < 1/4$ and $u_{i} < 1/4$ for three-way sufficient cause interaction.", "folder": "", "qid": "statistic-mcqm-6174", "question": "Under the assumptions of nondifferential misclassification and independent predicted probabilities, which of the following inequalities for the observed exposures imply the corresponding inequalities for the true exposures?", "options": ["A) $p_{11}^{*} - p_{10}^{*} - p_{01}^{*} + p_{00}^{*} > 0$ implies $p_{11} - p_{10} - p_{01} + p_{00} > 0$ if $d_{i} + u_{i} < 1$", "B) $p_{11}^{*} - p_{10}^{*} - p_{01}^{*} > 0$ implies $p_{11} - p_{10} - p_{01} > 0$ if $d_{i} < 1/2$ and $u_{i} < 1/4$", "C) $p_{11}^{*} - p_{10}^{*} - p_{01}^{*} - p_{00}^{*} > 0$ implies $p_{11} - p_{10} - p_{01} - p_{00} > 0$ if $d_{i} \\leqslant 1/3$ and $u_{i} \\leqslant 1/4$", "D) $p_{111}^{*} - p_{110}^{*} - p_{101}^{*} - p_{011}^{*} > 0$ implies $p_{111} - p_{110} - p_{101} - p_{011} > 0$ if $d_{i} < 1/4$ and $u_{i} < 1/4$"], "category": "CORRECT", "question_context_used": "Under assumptions of nondifferential misclassification and independent predicted probabilities, certain inequalities for the observed exposures imply the corresponding inequalities for the true exposures under specific conditions on the misclassification probabilities. These conditions include $d_{i} + u_{i} < 1$ for additive interaction, $d_{i} < 1/2$ and $u_{i} < 1/4$ for sufficient cause interaction, $d_{i} \\leqslant 1/3$ and $u_{i} \\leqslant 1/4$ for epistatic interaction, and $d_{i} < 1/4$ and $u_{i} < 1/4$ for three-way sufficient cause interaction.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n**Explanation**: \n- **Portfolio size ($|\\hat{\\mathcal{S}}|$)**: Remains constant at 30.00 for all sample sizes.\n- **Transaction cost (TR)**: Remains constant at 100% for all sample sizes.\n- The text does not provide explicit data for the sample mean and sample standard deviation, but it states that NAIVE's performance measures are identical across sample sizes, implying they remain constant.\n\n**Final Answer**: The final answer is $\boxed{{A, D}}$", "source": ["A-Bayesian-information-criterion-for-port_2012_Com.md"], "background_text": "The text mentions that NAIVE's diversification is independent of the sample size, and its associated performance measures are identical across different sample sizes.", "folder": "", "qid": "statistic-mcqm-4728", "question": "For the NAIVE portfolio construction method, which performance measures remain constant across all sample sizes (n=120, n=240, n=600)?", "options": ["A) Portfolio size ($|\\hat{\\mathcal{S}}|$)", "B) Sample mean ($\\hat{\\mu}^{*}$)", "C) Sample standard deviation ($\\hat{\\sigma}^{*}$)", "D) Transaction cost (TR)"], "category": "INCORRECT", "question_context_used": "The text mentions that NAIVE's diversification is independent of the sample size, and its associated performance measures are identical across different sample sizes.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n- **A)** Correct. As shown in the context, $m_{0}$ goes to zero when $\\theta_{1}-\\theta_{0}$ goes to infinity.\n- **B)** Correct. The inequality is derived under the assumption that $\\theta_{1}>\\theta_{0}$.\n- **C)** Correct. The term $\\int_{0}^{\\infty}\\frac{1}{k(y)}\\mathrm{d}F(y)$ approaches $\\frac{1}{2}$ as $\\theta_{1}-\\theta_{0}$ approaches infinity.\n- **D)** Incorrect. The inequality explicitly involves the cumulative distribution function $F(y)$ through the integral term.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Improved-estimation-of-accuracy-in-simple-hypothes.md"], "background_text": "The inequality provides an upper bound for $m_{0}$ based on the difference between $\\theta_{1}$ and $\\theta_{0}$ and an integral involving the cumulative distribution function $F(y)$ and the function $k(y)$. The behavior of $m_{0}$ as $\\theta_{1}-\\theta_{0}$ approaches infinity is also discussed.", "folder": "", "qid": "statistic-mcqm-2823", "question": "Considering the inequality $m_{0}\\leqslant e^{-\\frac{\\left(\\theta_{0}-\\theta_{1}\\right)^{2}}{2}}\\left/\\left(\\frac{1}{2\\left(\\frac{2}{3}-\\int_{0}^{\\infty}\\frac{1}{k(y)}\\mathrm{d}F(y)\\right)}-1\\right)\\right.$, which of the following statements are true?", "options": ["A) As $\\theta_{1}-\\theta_{0}$ approaches infinity, $m_{0}$ approaches zero.", "B) The inequality is derived under the assumption that $\\theta_{1}>\\theta_{0}$.", "C) The term $\\int_{0}^{\\infty}\\frac{1}{k(y)}\\mathrm{d}F(y)$ approaches $\\frac{1}{2}$ as $\\theta_{1}-\\theta_{0}$ approaches infinity.", "D) The inequality does not depend on the cumulative distribution function $F(y)$."], "category": "PARTIALLY_CORRECT", "question_context_used": "The inequality provides an upper bound for $m_{0}$ based on the difference between $\\theta_{1}$ and $\\theta_{0}$ and an integral involving the cumulative distribution function $F(y)$ and the function $k(y)$. The behavior of $m_{0}$ as $\\theta_{1}-\\theta_{0}$ approaches infinity is also discussed.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\"]\n}", "model_answer": ["A"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n- **A** is correct because the context explicitly states that if $n/N$ is negligible, $V_{2}$ can be ignored.\n- **B** is incorrect because the condition to ignore $V_{2}$ is related to the sampling fraction, not the outcome model.\n- **C** is incorrect for the same reason as B; it's not about the propensity score model.\n- **D** is correct as it combines the conditions mentioned in the context regarding the order of $V_{2}/V_{1}$ and the negligibility of $n/N$.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Soft_calibration_for_selection_bias_problems_under_mixed-effects_models.md"], "background_text": "The context mentions that if the sampling fraction $n/N$ is negligible, $V_{2}$ can be ignored in variance estimation, as $V_{2}/V_{1}$ is of order $O(n/N)$.", "folder": "", "qid": "statistic-mcqm-5567", "question": "Under what conditions can we ignore $V_{2}$ in the estimation of the variance of $\\hat{\\theta}_{w}$ according to the context?", "options": ["A) When the sampling fraction $n/N$ is negligible.", "B) When the outcome $y_{i}$ follows a linear mixed-effects model.", "C) When the propensity score model is correctly specified.", "D) When $V_{2}/V_{1}$ is of order $O(n/N)$ and $n/N$ is negligible."], "category": "CORRECT", "question_context_used": "The context mentions that if the sampling fraction $n/N$ is negligible, $V_{2}$ can be ignored in variance estimation, as $V_{2}/V_{1}$ is of order $O(n/N)$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. $Y(-1)$ corresponds to $\\rho=-1$.\n- **B)** Correct. $Y(0)$ corresponds to $\\rho=0$.\n- **C)** Incorrect. $Y(1)$ is not mentioned in the context.\n- **D)** Correct. $Y(1/2)$ corresponds to $\\rho=0.5$.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Bayesian_Variable_and_Transformation_Selection_in_Linear_Regression__Bayesian_Variable_and_Transformation_Selection_in_Linear_Regression.md"], "background_text": "The model space includes four transformations of the response corresponding to $\\rho=(-1,0,.5,1)$ in (2.1). The neighborhood of the model with predictors 1 and 2 and with response equal to $\\gamma^{(1/2)}$ is given.", "folder": "", "qid": "statistic-mcqm-4126", "question": "Which of the following are valid response transformations mentioned in the context?", "options": ["A) $Y(-1)$", "B) $Y(0)$", "C) $Y(1)$", "D) $Y(1/2)$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The model space includes four transformations of the response corresponding to $\\rho=(-1,0,.5,1)$ in (2.1). The neighborhood of the model with predictors 1 and 2 and with response equal to $\\gamma^{(1/2)}$ is given.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}", "model_answer": ["A", "B", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A) Correct**: The context suggests that the equation $y = y_{0}(1 - x^{2})^{\\frac{n-1}{2}}$ likely represents the theoretical distribution.\n- **B) Correct**: For $n=2$, the context states that all frequency is equally divided between $x=1$ and $x=-1$.\n- **C) Incorrect**: The context mentions that the standard deviation is larger than $\\frac{1 - r^{2}}{\\sqrt{n - 1}}$ for small samples.\n- **D) Correct**: The context confirms that the distribution is normal when $n$ is large.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["PROBABLE_ERROR_OF_A_CORRELATION_COEFFICIENT.md"], "background_text": "The context discusses the theoretical distribution of $r$ for samples of size $n$ from a normally distributed population with no correlation. It mentions the equation $y = y_{0}(1 - x^{2})^{\\frac{n-1}{2}}$ and specific cases for $n=2$ and large $n$. It also touches on the standard deviation and the normality of the distribution for large $n$.", "folder": "", "qid": "statistic-mcqm-5894", "question": "Which of the following statements are true about the distribution of $r$ when samples of size $n$ are drawn from a normally distributed population with no correlation?", "options": ["A) The equation $y = y_{0}(1 - x^{2})^{\\frac{n-1}{2}}$ represents the theoretical distribution.", "B) For $n=2$, all the frequency is equally divided between $x=1$ and $x=-1$.", "C) The standard deviation is given by $\\frac{1 - r^{2}}{\\sqrt{n - 1}}$.", "D) The distribution is normal when $n$ is large."], "category": "CORRECT", "question_context_used": "The context discusses the theoretical distribution of $r$ for samples of size $n$ from a normally distributed population with no correlation. It mentions the equation $y = y_{0}(1 - x^{2})^{\\frac{n-1}{2}}$ and specific cases for $n=2$ and large $n$. It also touches on the standard deviation and the normality of the distribution for large $n$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**:\n- **A) Correct**: The text states that Simes's procedure provides a test only for $H_{0}$.\n- **B) Correct**: This is the criterion for rejecting $H_{0}$ according to Simes's procedure.\n- **C) Incorrect**: The original Simes's procedure does not control the familywise error rate in the strong sense; this is a property of the extended Simes procedure.\n- **D) Correct**: The text mentions that Simes proved the procedure has level $\\alpha$ under $H_{0}$ when the $p$-values are independent.\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["A_sharper_Bonferroni_procedure_for_multiple_tests_of_significance.md"], "background_text": "Simes's procedure provides a test only for the global null hypothesis $H_{0}$ and rejects $H_{0}$ when, for any $j=1,\\ldots,m$, $P_{(j)}\\leqslant j\\alpha/m$. Simes proved that this procedure has level $\\alpha$ under $H_{0}$ when the $p$-values are independent. The extended Simes procedure, which controls the familywise error rate in the strong sense, is derived from the original Simes procedure but is not the same as the original procedure described here.", "folder": "", "qid": "statistic-mcqm-5528", "question": "Which of the following are properties of Simes's procedure as described in the text?", "options": ["A) It provides a test only for the global null hypothesis $H_{0}$.", "B) It rejects $H_{0}$ when, for any $j=1,\\ldots,m$, $P_{(j)}\\leqslant j\\alpha/m$.", "C) It controls the familywise error rate in the strong sense under any $H_{0}^{\\prime}$.", "D) It requires the $p$-values to be independent for its level to be $\\alpha$ under $H_{0}$."], "category": "CORRECT", "question_context_used": "Simes's procedure provides a test only for the global null hypothesis $H_{0}$ and rejects $H_{0}$ when, for any $j=1,\\ldots,m$, $P_{(j)}\\leqslant j\\alpha/m$. Simes proved that this procedure has level $\\alpha$ under $H_{0}$ when the $p$-values are independent. The extended Simes procedure, which controls the familywise error rate in the strong sense, is derived from the original Simes procedure but is not the same as the original procedure described here.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, C\n\n**Explanation**:\n- **A)** Correctly represents the volume of the intersection using the Lebesgue volume measure.\n- **B)** Correctly represents the volume by subtracting the volumes delimited by the half-spaces from the total volume of the sphere.\n- **C)** Correctly uses the scaled version $U(a, b, c)$ to represent the volume.\n- **D)** Incorrect because it uses the Hausdorff measure $\\mathcal{H}^2$, which is for surface area, not volume.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, C}}$", "source": ["Analysis of a Three-Dimensional Point Pattern with.md"], "background_text": "The volume $V(\\mathbf{x}, r)$ of the intersection between a rectangular box $B$ and a sphere $S(\\mathbf{x}, r)$ is given by $\\mathcal{L}^3\\{B \\cap S(\\mathbf{x}, r)\\}$, where $\\mathcal{L}^3$ is the Lebesgue volume measure. The expression involves terms like $V_1, V_2, V_3$ representing volumes delimited by 1, 2, or 3 half-spaces, and $U(a, b, c)$ is a scaled version of $V_3$.", "folder": "", "qid": "statistic-mcqm-5616", "question": "Given a sphere $S(\\mathbf{x}, r)$ with center $\\mathbf{x} \\in \\mathbb{R}^3$ and radius $r > 0$, and a rectangular box $B$ defined by $B = \\{\\mathbf{x} \\in \\mathbb{R}^3; 0 \\leqslant x_i < b_i, i = 1, 2, 3\\}$ with sides $b_i > 0$, which of the following expressions correctly represent the volume $V(\\mathbf{x}, r)$ of the intersection $B \\cap S(\\mathbf{x}, r)$?", "options": ["A) $V(\\mathbf{x}, r) = \\mathcal{L}^3\\{B \\cap S(\\mathbf{x}, r)\\}$", "B) $V(\\mathbf{x}, r) = \\frac{4}{3}\\pi r^3 - \\sum_{i=1}^{3}\\{V_1(x_i, r) + V_1(b_i - x_i, r)\\}$", "C) $V(\\mathbf{x}, r) = r^3 U\\left(\\frac{x_1}{r}, \\frac{x_2}{r}, \\frac{x_3}{r}\\right)$", "D) $V(\\mathbf{x}, r) = \\mathcal{H}^2\\{B \\cap \\partial S(\\mathbf{x}, r)\\}$"], "category": "PARTIALLY_CORRECT", "question_context_used": "The volume $V(\\mathbf{x}, r)$ of the intersection between a rectangular box $B$ and a sphere $S(\\mathbf{x}, r)$ is given by $\\mathcal{L}^3\\{B \\cap S(\\mathbf{x}, r)\\}$, where $\\mathcal{L}^3$ is the Lebesgue volume measure. The expression involves terms like $V_1, V_2, V_3$ representing volumes delimited by 1, 2, or 3 half-spaces, and $U(a, b, c)$ is a scaled version of $V_3$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C\n\n- **A)** Correct, $\\mathcal{M}$ is defined as the maximum of the $k$ area-based statistics.\n- **B)** Incorrect, the sum is mentioned but not recommended for adjusting contributions.\n- **C)** Correct, the critical value is obtained by simulating the $1-\\alpha$ quantile of the maximum integral involving independent Brownian bridge processes.\n- **D)** Incorrect, partitioning is suggested as a method to adjust contributions.\n\n**Final Answer**: The final answer is $\boxed{{A, C}}$", "source": ["Nonparametric_tests_for_and_against_likelihood_ratio_ordering_in_the_two-sample_problem.md"], "background_text": "The context describes a method for adjusting the relative contributions from concave and non-concave regions by partitioning the unit interval into subintervals and computing a maximum statistic, with critical values obtained through simulation.", "folder": "", "qid": "statistic-mcqm-5750", "question": "In the context of partitioning the unit interval into $k>1$ subintervals $I_{1},I_{2},\\ldots,I_{k}$ and computing the area-based statistic $M_{m_{i},n_{i};I_{i}}^{*(12)}$ for each, which of the following statements are true regarding the recommended statistic $\\mathcal{M}$?", "options": ["A) $\\mathcal{M}$ is defined as the maximum of the $k$ area-based statistics computed on each subinterval.", "B) The sum of the $k$ area-based statistics is recommended for adjusting contributions from concave and non-concave regions.", "C) The critical value for $\\mathcal{M}$ can be obtained by simulating the $1-\\alpha$ quantile of a specific maximum integral involving Brownian bridge processes.", "D) Partitioning the interval does not help in adjusting the relative contributions from concave and non-concave regions."], "category": "CORRECT", "question_context_used": "The context describes a method for adjusting the relative contributions from concave and non-concave regions by partitioning the unit interval into subintervals and computing a maximum statistic, with critical values obtained through simulation.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"C\"]\n}", "model_answer": ["A", "C"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The text states that Fisher information does not increase under transformation of random variables, i.e., $i_{t(X)} \\leq i_X$.\n- **B)** Correct. Fisher information is defined as the expectation of the squared derivative of the log-likelihood function, as given in the definition.\n- **C)** Incorrect. Fisher information is a matrix when the parameter $\\theta$ is a vector, not necessarily a scalar.\n- **D)** Correct. The text describes partitioning Fisher information into components for parameters of interest ($\\psi$) and nuisance parameters ($\\nu$).\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Information-on-parameters-of-interest-decreases-_2.md"], "background_text": "Fisher information for a random variable $X$ with density $f(x;\\theta)$ is defined as $i_X(\\theta) = E_\\theta\\left[\\left(\\frac{\\partial\\log f(X;\\theta)}{\\partial\\theta}\\right)^\\mathrm{T}\\frac{\\partial\\log f(X;\\theta)}{\\partial\\theta}\\right]$. It does not increase under transformation of random variables and can be partitioned for parameters of interest and nuisance parameters.", "folder": "", "qid": "statistic-mcqm-6067", "question": "Which of the following statements about Fisher information $i_X(\\theta)$ are correct?", "options": ["A) Fisher information does not increase under transformation of random variables.", "B) Fisher information is defined as the expectation of the squared derivative of the log-likelihood function with respect to the parameter.", "C) Fisher information is always a scalar quantity.", "D) Fisher information can be partitioned into components corresponding to parameters of interest and nuisance parameters."], "category": "CORRECT", "question_context_used": "Fisher information for a random variable $X$ with density $f(x;\\theta)$ is defined as $i_X(\\theta) = E_\\theta\\left[\\left(\\frac{\\partial\\log f(X;\\theta)}{\\partial\\theta}\\right)^\\mathrm{T}\\frac{\\partial\\log f(X;\\theta)}{\\partial\\theta}\\right]$. It does not increase under transformation of random variables and can be partitioned for parameters of interest and nuisance parameters.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B\n\n**Explanation**:\n- **A) Correct**: The residual mean square (5·83) is lower than its expected value (8·37), indicating it is not significantly high.\n- **B) Correct**: The text explicitly states there is no evidence of heterogeneity in $z$ within plasma samples.\n- **C) Incorrect**: The residual mean square is lower, not higher, than its expected value.\n- **D) Incorrect**: The degrees of freedom (10) are sufficient to draw conclusions, as evidenced by the analysis.\n\n**Final Answer**: The final answer is $\boxed{{A, B}}$", "source": ["Studies in the Variability of Two Blood-Clotting T.md"], "background_text": "The within-samples residual mean square is 5·83 with 10 degrees of freedom, and its expected value is 8·37. The residual mean square is not significantly high; it is, in fact, rather lower than its expected value. There is, therefore, no evidence of heterogeneity in $z$ within plasma samples.", "folder": "", "qid": "statistic-mcqm-3712", "question": "Given the within-samples residual mean square is 5·83 with 10 degrees of freedom, and its expected value is 8·37, which of the following statements are correct?", "options": ["A) The residual mean square is significantly lower than its expected value.", "B) There is no evidence of heterogeneity in $z$ within plasma samples.", "C) The residual mean square is significantly higher than its expected value.", "D) The degrees of freedom for the residual mean square are insufficient to draw any conclusions."], "category": "PARTIALLY_CORRECT", "question_context_used": "The within-samples residual mean square is 5·83 with 10 degrees of freedom, and its expected value is 8·37. The residual mean square is not significantly high; it is, in fact, rather lower than its expected value. There is, therefore, no evidence of heterogeneity in $z$ within plasma samples.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}", "model_answer": ["A", "B"], "error": null}}
{"gold_answer": "**Step-by-Step Solution**:\n1. **Option A**: Correct. The text states that if $x$ grows at a rate between $n^{1/6}$ and $n^{1/3}$, the bootstrap approximation is asymptotically much better than the $r$-term Edgeworth approximation.\n2. **Option B**: Correct. Singh showed that the bootstrap automatically corrects for the first term in an Edgeworth expansion, improving on the normal approximation.\n3. **Option C**: Incorrect. The text states that the bootstrap and $r$-term Edgeworth approximations are asymptotically equivalent if $x$ grows more slowly than $n^{(1/6)-(1/6r)}$, not that the bootstrap performs worse.\n4. **Option D**: Correct. The text mentions that simulation studies show the bootstrap provides intervals with greater coverage accuracy than Hall's approach to Edgeworth correction.\n\n**Correct Choices**: A, B, D\n\n**Final Answer**: The final answer is $\boxed{{A, B, D}}$", "source": ["On-the-relative-performance-of-bootstrap-and-Edgew.md"], "background_text": "The bootstrap method, introduced by Efron, is a powerful tool for estimating statistical features, including tail probabilities. It corrects for the first term in an Edgeworth expansion, improving on the normal approximation. Simulation studies show that the bootstrap performs better than Edgeworth approximation for estimating tail probabilities, especially when $x$ grows at certain rates. The bootstrap provides intervals with greater coverage accuracy than Hall's approach to Edgeworth correction.", "folder": "", "qid": "statistic-mcqm-1525", "question": "Which of the following statements about the bootstrap method are correct based on the provided context?", "options": ["A) The bootstrap method is superior to the Edgeworth approximation for estimating tail probabilities when $x$ grows at a rate between $n^{1/6}$ and $n^{1/3}$.", "B) The bootstrap method corrects for the first term in an Edgeworth expansion, improving on the normal approximation.", "C) The bootstrap method performs worse than the Edgeworth approximation when $x$ grows more slowly than $n^{(1/6)-(1/6r)}$.", "D) The bootstrap method provides intervals with greater coverage accuracy than Hall's approach to Edgeworth correction."], "category": "PARTIALLY_CORRECT", "question_context_used": "The bootstrap method, introduced by Efron, is a powerful tool for estimating statistical features, including tail probabilities. It corrects for the first term in an Edgeworth expansion, improving on the normal approximation. Simulation studies show that the bootstrap performs better than Edgeworth approximation for estimating tail probabilities, especially when $x$ grows at certain rates. The bootstrap provides intervals with greater coverage accuracy than Hall's approach to Edgeworth correction.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n**Explanation**: \n- **Option A** is correct because the context states that $\\|v_1\\| = \\|v_2\\| = 1$.\n- **Option B** is correct because the context states that $v_1' \\cdot v_2 = 0$.\n- **Option C** is incorrect because orthonormal vectors are linearly independent.\n- **Option D** is correct because vectors with a dot product of zero are orthogonal.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Exact-methods-for-variable-selection-in-principal-.md"], "background_text": "The vectors $v_1$ and $v_2$ are defined such that $\\|v_1\\| = \\|v_2\\| = 1$ and $v_1' \\cdot v_2 = 0$, indicating they are orthonormal vectors.", "folder": "", "qid": "statistic-mcqm-4872", "question": "Considering the vectors $v_1$ and $v_2$ defined in the context, which of the following properties hold true?", "options": ["A) $\\|v_1\\| = \\|v_2\\| = 1$", "B) $v_1' \\cdot v_2 = 0$", "C) $v_1$ and $v_2$ are linearly dependent", "D) $v_1$ and $v_2$ are orthogonal"], "category": "CORRECT", "question_context_used": "The vectors $v_1$ and $v_2$ are defined such that $\\|v_1\\| = \\|v_2\\| = 1$ and $v_1' \\cdot v_2 = 0$, indicating they are orthonormal vectors.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, C, D\n\n- **A)** Correct. Mother's age at childbirth is listed as a confounding variable.\n- **B)** Incorrect. Child's gender is not mentioned as a confounding variable in the text.\n- **C)** Correct. Language spoken at home is listed as a confounding variable.\n- **D)** Correct. Mother's alcohol consumption is listed as a confounding variable.\n\n**Final Answer**: The final answer is $\\boxed{{A, C, D}}$", "source": ["A-Bayesian-propensity-score-adjustment-for-laten_2.md"], "background_text": "The study accounts for several confounding variables that affect both the assignment to smoking conditions and cognitive functioning. These include the mother's age at childbirth, race, job status, education status, language spoken at home, and mother's alcohol consumption, among others.", "folder": "", "qid": "statistic-mcqm-6162", "question": "Which of the following variables were considered as confounding variables in the study comparing cognitive functioning between children of smoking and non-smoking mothers?", "options": ["A) Mother's age at childbirth", "B) Child's gender", "C) Language spoken at home", "D) Mother's alcohol consumption"], "category": "CORRECT", "question_context_used": "The study accounts for several confounding variables that affect both the assignment to smoking conditions and cognitive functioning. These include the mother's age at childbirth, race, job status, education status, language spoken at home, and mother's alcohol consumption, among others.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}", "model_answer": ["A", "C", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, D\n\n**Explanation**: \n- **Option A** is correct because it directly states the formula for the number of nodes at the $j$th scale as given in the background text: $N_{j}=N_{1}n_{1}\\dots n_{j-1}$.\n- **Option D** is also correct as it represents the same formula in a different notation, using the multiplication symbol ($\\times$) instead of the dots ($\\dots$).\n- **Option B** is incorrect because it includes $n_{j}$ in the product, which is not part of the formula for the $j$th scale.\n- **Option C** is incorrect because it suggests addition of terms instead of multiplication, which does not match the given formula.\n\n**Final Answer**: The final answer is $\\boxed{{A, D}}$", "source": ["Fast,_Resolution-Consistent_Spatial_Prediction_of_Global_Processes_From_Satellite_Data__Fast__Resolution-Consistent_Spatial_Prediction_of_Global_Processes_From_Satellite_Data.md"], "background_text": "Consider a forest (i.e., a set) of $N_{1}$ trees with $J$ scales. Assume that there are $N_{1}$ nodes at the first scale (the coarsest scale) that are the roots of the $N_{1}$ trees, and each node at the $j$ th scale has $n_{j}$ children; $j=1,\\dots,J-1.5\\mathrm{o}$ , there are $N_{j}=N_{1}n_{1}\\dots n_{j-1}$ nodes at the $j$ th scale; $j=2,\\ldots,J$.", "folder": "", "qid": "statistic-mcqm-5470", "question": "In a forest of $N_{1}$ trees with $J$ scales, where each node at the $j$th scale has $n_{j}$ children, how many nodes are there at the $j$th scale for $j=2,\\ldots,J$?", "options": ["A) $N_{j}=N_{1}n_{1}\\dots n_{j-1}$", "B) $N_{j}=N_{1}n_{1}\\dots n_{j}$", "C) $N_{j}=N_{1}+n_{1}+\\dots+n_{j-1}$", "D) $N_{j}=N_{1}\\times n_{1}\\times \\dots \\times n_{j-1}$"], "category": "CORRECT", "question_context_used": "Consider a forest (i.e., a set) of $N_{1}$ trees with $J$ scales. Assume that there are $N_{1}$ nodes at the first scale (the coarsest scale) that are the roots of the $N_{1}$ trees, and each node at the $j$ th scale has $n_{j}$ children; $j=1,\\dots,J-1.5\\mathrm{o}$ , there are $N_{j}=N_{1}n_{1}\\dots n_{j-1}$ nodes at the $j$ th scale; $j=2,\\ldots,J$.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}", "model_answer": ["A", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: B, D\n\n**Explanation**: \n- **Option A** is incorrect because $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ can be negative if $T_k > T_{\\ell}$ and $\\vartheta$ is sufficiently large.\n- **Option B** is correct because as $\\vartheta$ increases, the exponential terms decrease, making $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ smaller.\n- **Option C** is incorrect because $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ is not symmetric; swapping $\\ell$ and $k$ changes the sign of the function.\n- **Option D** is correct because both exponential terms approach 0 as $\\vartheta$ approaches infinity, so their difference also approaches 0.\n\n**Final Answer**: The final answer is $\\boxed{{B, D}}$.", "source": ["Scandinavian J Statistics - 2019 - Féron - Efficie.md"], "background_text": "The function $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ is defined as $e^{-\\vartheta T_{k}} - e^{-\\vartheta T_{\\ell}}$, where $T_k$ and $T_{\\ell}$ are time points, and $\\vartheta$ is a parameter.", "folder": "", "qid": "statistic-mcqm-5839", "question": "Given the definition of $\\mathfrak{e}_{\\ell,k}(\\vartheta) = e^{-\\vartheta T_{k}} - e^{-\\vartheta T_{\\ell}}$, which of the following statements are true?", "options": ["A) $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ is always positive for $T_k > T_{\\ell}$.", "B) $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ decreases as $\\vartheta$ increases.", "C) $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ is symmetric in $\\ell$ and $k$.", "D) $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ approaches 0 as $\\vartheta$ approaches infinity."], "category": "CORRECT", "question_context_used": "The function $\\mathfrak{e}_{\\ell,k}(\\vartheta)$ is defined as $e^{-\\vartheta T_{k}} - e^{-\\vartheta T_{\\ell}}$, where $T_k$ and $T_{\\ell}$ are time points, and $\\vartheta$ is a parameter.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}", "model_answer": ["B", "D"], "error": null}}
{"gold_answer": "**Correct Choices**: A, B, D\n\n- **A)** Correct. The forward contract $F(t,T_{\\mathrm{s}},T_{\\mathrm{e}})$ indeed delivers 1 MWh of electricity continuously between dates $T_{\\mathrm{s}}$ and $T_{\\mathrm{e}}$.\n- **B)** Correct. The forward contract can be bought during any time before $T_{\\mathrm{s}}$, specifically during the quotation period $[t_{0},T]$ with $T<T_{\\mathrm{s}}$.\n- **C)** Incorrect. The forward contract is no longer available for purchase once $t>T$.\n- **D)** Correct. Typical observed contracts have delivery periods ranging from 1 week to 1 year, as mentioned in the context.\n\n**Final Answer**: The final answer is $\\boxed{{A, B, D}}$", "source": ["Scandinavian J Statistics - 2019 - Féron - Efficie.md"], "background_text": "The prices of existing forward contracts in the electricity markets are characterized by three time components: the quotation date $t$ and the dates $T_{\\mathrm{s}}$ and $T_{\\mathrm{e}}$ of starting and ending power delivery, respectively. A forward contract $F(t,T_{\\mathrm{s}},T_{\\mathrm{e}})$ delivers 1 MWh of electricity continuously between dates $T_{\\mathrm{s}}$ and $T_{\\mathrm{e}}$. It may be bought during a quotation period $[t_{0},T]$ with $T<T_{\\mathrm{s}}$ and is no longer available once $t>T$. Typical observed contracts are of various delivery periods: 1 week, 1 month, 1 quarter (3 months), one season (6 months), or 1 year.", "folder": "", "qid": "statistic-mcqm-5813", "question": "Which of the following statements are true about the forward contracts $F(t,T_{\\mathrm{s}},T_{\\mathrm{e}})$ in electricity markets?", "options": ["A) The forward contract delivers 1 MWh of electricity continuously between dates $T_{\\mathrm{s}}$ and $T_{\\mathrm{e}}$.", "B) The forward contract can be bought during any time period before $T_{\\mathrm{s}}$.", "C) The forward contract is available for purchase even after the quotation period $[t_{0},T]$.", "D) Typical observed contracts have delivery periods ranging from 1 week to 1 year."], "category": "PARTIALLY_CORRECT", "question_context_used": "The prices of existing forward contracts in the electricity markets are characterized by three time components: the quotation date $t$ and the dates $T_{\\mathrm{s}}$ and $T_{\\mathrm{e}}$ of starting and ending power delivery, respectively. A forward contract $F(t,T_{\\mathrm{s}},T_{\\mathrm{e}})$ delivers 1 MWh of electricity continuously between dates $T_{\\mathrm{s}}$ and $T_{\\mathrm{e}}$. It may be bought during a quotation period $[t_{0},T]$ with $T<T_{\\mathrm{s}}$ and is no longer available once $t>T$. Typical observed contracts are of various delivery periods: 1 week, 1 month, 1 quarter (3 months), one season (6 months), or 1 year.", "gold_answer_parsed": null, "model_evaluation_result": {"model_raw_response": "{\n  \"answer\": [\"A\", \"B\", \"D\"]\n}", "model_answer": ["A", "B", "D"], "error": null}}
