[
  {
    "qid": "economic-multiple-choice-2755",
    "question": "Under what conditions does the marketing board's profit-maximizing distortion between p* and MRT exceed the socially optimal distortion?",
    "options": [
      "A) When domestic demand elasticity is less than foreign excess demand elasticity",
      "B) When domestic demand elasticity is greater than foreign excess demand elasticity",
      "C) When the domestic market is large relative to the world market",
      "D) When foreign excess demand elasticity embodies a supply response that complements the demand response"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "The-welfare-and-allocative-effects-of-export-taxe_1984_Journal-of-Developmen.md"
    ],
    "question_context": "Given the assumption that $\\pmb{\\eta}_{e}^{*}$ is a constant, it then follows from (17) that the marketing board's profit-maximizing distortion between p* and MRT is greater than the socially optimal distortion if and only if $\\eta_{d}<\\eta_{e}^{*}$ (domestic demand is less elastic than foreign excess demand). While the relation between $\\pmb{\\eta}_{d}$ and $\\pmb{\\eta}_{e}^{*}$ is an empirical matter, the case presented in fig. 2 $(\\eta_{d}\\leq\\eta_{e}^{*})$ may be the most relevant for two reasons. First, the domestic market may be small relative to the world market. Second, $\\eta_{e}^{*}$ is an excess demand elasticity while $\\eta_{d}$ is a simple demand elasticity. $\\pmb{\\eta}_{e}^{*}$ thus embodies a supply response which complements the demand response (a price increase reduces foreign demand and increases foreign supply).",
    "category": "INCORRECT",
    "question_context_used": "Given the assumption that $\\pmb{\\eta}_{e}^{*}$ is a constant, it then follows from (17) that the marketing board's profit-maximizing distortion between p* and MRT is greater than the socially optimal distortion if and only if $\\eta_{d}<\\eta_{e}^{*}$ (domestic demand is less elastic than foreign excess demand). While the relation between $\\pmb{\\eta}_{d}$ and $\\pmb{\\eta}_{e}^{*}$ is an empirical matter, the case presented in fig. 2 $(\\eta_{d}\\leq\\eta_{e}^{*})$ may be the most relevant for two reasons. First, the domestic market may be small relative to the world market. Second, $\\eta_{e}^{*}$ is an excess demand elasticity while $\\eta_{d}$ is a simple demand elasticity. $\\pmb{\\eta}_{e}^{*}$ thus embodies a supply response which complements the demand response (a price increase reduces foreign demand and increases foreign supply).",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-336",
    "question": "Based on Chang and Winters (1999), which of the following statements are empirically supported regarding the welfare implications of MERCOSUR's tariff preferences?",
    "options": [
      "A) Nonmember countries experienced significant terms-of-trade losses due to MERCOSUR's preferential tariff reductions.",
      "B) The welfare losses for nonmembers were partially offset by m.f.n. tariff reductions by MERCOSUR members.",
      "C) The United States and Germany were the only nonmember countries to experience welfare losses.",
      "D) The welfare estimates incorporate all possible efficiency gains and quantity changes induced by MERCOSUR."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Chang-RegionalBlocsAffect-2002.md"
    ],
    "question_context": "The first-order estimate of the welfare effect of a price change is $q^{*}\\Delta p$. Assuming that all variables except tariff rates were unaffected by MERCOSUR and taking unweighted averages of the latter, we can use the coefficients of Table 2 to make such estimates (see Table 3, column A). The United States exported $\\$5.4$ billion worth of goods to Brazil in 1991. With partner tariffs falling by an average of 26 percentage points by 1996 and a coefficient of 0.445, this implies a loss of $\\$624$ million in that year. Losses similarly occurred for the other countries that reported export data——Japan (with losses of $\\$59$ million), Germany ( $\\$236$ million), Korea (\\$14 million), and Chile ( $\\$17$ million). If we incorporate the terms-of-trade gains that nonmembers may have earned as Brazil reduced its m.f.n. tariffs, U.S. welfare losses fall to $\\$545$ million, and Germany's to $\\$169$ million. Korea now gains and Chile and Japan would lose more because of their coefficients exceeding unity in Table 2. The preference effect dominates the welfare calculation because, first, the “pass-through\" of m.f.n. tariff changes to consumers is quite large (close to one in the aggregate, except for Korea) and the changes in m.f.n. tariffs are smaller than those for MERCOSUR members.",
    "category": "CORRECT",
    "question_context_used": "The first-order estimate of the welfare effect of a price change is $q^{*}\\Delta p$. Assuming that all variables except tariff rates were unaffected by MERCOSUR and taking unweighted averages of the latter, we can use the coefficients of Table 2 to make such estimates (see Table 3, column A). The United States exported $\\$5.4$ billion worth of goods to Brazil in 1991. With partner tariffs falling by an average of 26 percentage points by 1996 and a coefficient of 0.445, this implies a loss of $\\$624$ million in that year. Losses similarly occurred for the other countries that reported export data——Japan (with losses of $\\$59$ million), Germany ( $\\$236$ million), Korea (\\$14 million), and Chile ( $\\$17$ million). If we incorporate the terms-of-trade gains that nonmembers may have earned as Brazil reduced its m.f.n. tariffs, U.S. welfare losses fall to $\\$545$ million, and Germany's to $\\$169$ million. Korea now gains and Chile and Japan would lose more because of their coefficients exceeding unity in Table 2. The preference effect dominates the welfare calculation because, first, the “pass-through\" of m.f.n. tariff changes to consumers is quite large (close to one in the aggregate, except for Korea) and the changes in m.f.n. tariffs are smaller than those for MERCOSUR members.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-953",
    "question": "Which of the following statements are necessary and sufficient for a pair of choice correspondences $(C_{1}, C_{2})$ to be a CMI mechanism?",
    "options": [
      "A) Expansion (EXP), Nullipotency (NULL), and Consistency of Influence (CoI)",
      "B) Expansion (EXP) and Nullipotency (NULL) only",
      "C) Consistency of Influence (CoI) and Nullipotency (NULL) only",
      "D) Expansion (EXP) and Consistency of Influence (CoI) only"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Choosing on influence.md"
    ],
    "question_context": "The text discusses the characterization of Choice on Mutual Influence (CMI) through three key properties: Expansion (EXP), Nullipotency (NULL), and Consistency of Influence (CoI). EXP is an individual rationality property stating that if an alternative is chosen from two different sets, it is also chosen from their union. NULL links $C_{1}$ and $C_{2}$, ensuring that the decision outcomes of one cannot refine the other's choices. CoI is the key property, ensuring that any choice inconsistency in the form of $\\langle x,y\\rangle$ reversals is due to mutual influence. The text proves that these properties are necessary and sufficient for CMI, with the formula: \n\n$$x y\\in\\succ_{i}\\quad{\\mathrm{iff}}\\quad C_{i}(S)=C_{i}(S\\setminus y)\\quad{\\mathrm{for~all~}}S{\\mathrm{~with~}}x,y\\in S.$$\n\nAdditionally, when choice is restricted to a unique alternative, NULL becomes redundant, and only EXP and CoI are needed for characterization.",
    "category": "INCORRECT",
    "question_context_used": "The text discusses the characterization of Choice on Mutual Influence (CMI) through three key properties: Expansion (EXP), Nullipotency (NULL), and Consistency of Influence (CoI). EXP is an individual rationality property stating that if an alternative is chosen from two different sets, it is also chosen from their union. NULL links $C_{1}$ and $C_{2}$, ensuring that the decision outcomes of one cannot refine the other's choices. CoI is the key property, ensuring that any choice inconsistency in the form of $\\langle x,y\\rangle$ reversals is due to mutual influence. The text proves that these properties are necessary and sufficient for CMI, with the formula: \n\n$$x y\\in\\succ_{i}\\quad{\\mathrm{iff}}\\quad C_{i}(S)=C_{i}(S\\setminus y)\\quad{\\mathrm{for~all~}}S{\\mathrm{~with~}}x,y\\in S.$$\n\nAdditionally, when choice is restricted to a unique alternative, NULL becomes redundant, and only EXP and CoI are needed for characterization.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-210",
    "question": "In the context of defense economics, which of the following are identified as key areas where game theory with incomplete information could provide significant insights?",
    "options": [
      "A) Analysis of alliances and demand for military expenditure",
      "B) Credibility, commitment, and asymmetric information in interactions between governments, suppliers, and the military",
      "C) The impact of military expenditure on endogenous growth through spillover effects",
      "D) The design of procurement contracts under perfect information"
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Economics_of_Defense..md"
    ],
    "question_context": "The book 'The Economics of Defense' highlights two areas ripe for research in defense economics. First, questions of credibility, commitment, cooperation, and asymmetric information are prominent, with recent advances in game theory offering new tools. The extreme departure from complete and perfect information in interactions between governments, suppliers, and the military is noted as a distinctive feature. Second, the relationship between growth and the defense sector is explored, with endogenous growth theory providing insights into spillover effects from defense to the civilian economy, balanced against the opportunity costs of government spending. Key formula: The interplay between knowledge spillovers and crowding out effects is crucial for understanding growth impacts.",
    "category": "INCORRECT",
    "question_context_used": "The book 'The Economics of Defense' highlights two areas ripe for research in defense economics. First, questions of credibility, commitment, cooperation, and asymmetric information are prominent, with recent advances in game theory offering new tools. The extreme departure from complete and perfect information in interactions between governments, suppliers, and the military is noted as a distinctive feature. Second, the relationship between growth and the defense sector is explored, with endogenous growth theory providing insights into spillover effects from defense to the civilian economy, balanced against the opportunity costs of government spending. Key formula: The interplay between knowledge spillovers and crowding out effects is crucial for understanding growth impacts.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4351",
    "question": "In the context of the trivariate censored data model estimation, which of the following statements are supported by the analysis of the log-likelihood function and parameter estimates?",
    "options": [
      "A) The correlation between unobservables affecting worker and employer preferences (p12 and p23) is estimated precisely, indicating strong data information on these dimensions.",
      "B) The likelihood function is very flat in the dimensions of p12 and p23, suggesting little information in the data about these correlations.",
      "C) The correlation p13 between unobservables affecting worker preferences at different times is significantly greater than zero, indicating persistent unmeasured attributes.",
      "D) Imposing constraints p12 = p23 = 0 leads to a substantial change in the estimates of other parameters, rejecting the unconstrained model."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Farber-DeterminationUnionStatus-1983.md"
    ],
    "question_context": "The log-likelihood function for the trivariate censored data model is defined using equations (9) through (1l) as:\n\n$$\nL=\\sum_{i=1}^{n}\\left\\{U_{i}\\mathrm{ln}\\operatorname*{Pr}(u_{1}>-X_{0i}G_{1},u_{2}<-X_{0i}G_{2})\\right.\n$$\n\n$$\n+(1-U_{i})V F U_{i}\\mathrm{ln}\\big[\\mathrm{Pr}(u_{1}>-X_{0i}G_{1},u_{2}>-X_{0i}G_{2},\n$$\n\n$$\nu_{3}>-X_{c i}G_{1})\n$$\n\nThe maximum likelihood estimates reveal that two of the three estimated correlations (p12 and p23) are estimated very imprecisely, suggesting the likelihood function is flat in these dimensions. This implies little information in the data regarding whether workers who are more likely to desire union representation are more or less likely to be hired by union employers. In contrast, the remaining correlation (p13) between unobservables affecting worker preferences at different times is asymptotically significantly greater than zero, consistent with persistent unmeasured attributes affecting preferences for union representation.",
    "category": "CORRECT",
    "question_context_used": "The log-likelihood function for the trivariate censored data model is defined using equations (9) through (1l) as:\n\n$$\nL=\\sum_{i=1}^{n}\\left\\{U_{i}\\mathrm{ln}\\operatorname*{Pr}(u_{1}>-X_{0i}G_{1},u_{2}<-X_{0i}G_{2})\\right.\n$$\n\n$$\n+(1-U_{i})V F U_{i}\\mathrm{ln}\\big[\\mathrm{Pr}(u_{1}>-X_{0i}G_{1},u_{2}>-X_{0i}G_{2},\n$$\n\n$$\nu_{3}>-X_{c i}G_{1})\n$$\n\nThe maximum likelihood estimates reveal that two of the three estimated correlations (p12 and p23) are estimated very imprecisely, suggesting the likelihood function is flat in these dimensions. This implies little information in the data regarding whether workers who are more likely to desire union representation are more or less likely to be hired by union employers. In contrast, the remaining correlation (p13) between unobservables affecting worker preferences at different times is asymptotically significantly greater than zero, consistent with persistent unmeasured attributes affecting preferences for union representation.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2318",
    "question": "In the context of Bayesian inference for partially identified parameters in the Rust (1987) model, which of the following statements are correct according to Moon and Schorfheide (2012)?",
    "options": [
      "A) Bayesian credible sets for partially identified parameters are asymptotically equivalent to classical confidence sets.",
      "B) Bayesian credible sets for partially identified parameters exclude about 5% of the volume of the identified set asymptotically.",
      "C) Classical confidence sets for partially identified parameters typically include the entire identified set.",
      "D) The Bernstein-von Mises theorem holds for both point-identified and partially identified parameters."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Semiparametric Inference in Dynamic Binary Choice Models.md"
    ],
    "question_context": "Moon and Schorfheide (2012) show that credible sets for partially identified parameters are strictly smaller than the corresponding confidence sets asymptotically. Their results apply to our settings. To understand these results suppose that the prior of $\\tilde{p}$ conditional on $(p,G)$ is a uniform distribution on ${\\mathcal{P}}(p,G)$. As the sample size increases, the posterior for $(p,G)$ concentrates around the DGP values and the posterior for $\\tilde{p}$ converges to the uniform distribution on the identified set for $\\tilde{p}$. Thus, a Bayesian credible set for $\\tilde{p}$ excludes about $5\\%$ of the volume of the identified set. In contrast, a $95\\%$ classical confidence set for $\\tilde{p}$ typically includes the identified set.\n\nThe conceptual differences between classical and Bayesian inference for partially identified parameters can also be described as follows. Bayesian inference does not distinguish between the uncertainty from the lack of point identification and that from the sampling variability. In contrast, a standard classical $95\\%$ confidence set allows for errors in $5\\%$ of hypothetical repeated samples; however, the lower bound on the coverage rate is imposed at all parameter values in the parameter space.",
    "category": "CORRECT",
    "question_context_used": "Moon and Schorfheide (2012) show that credible sets for partially identified parameters are strictly smaller than the corresponding confidence sets asymptotically. Their results apply to our settings. To understand these results suppose that the prior of $\\tilde{p}$ conditional on $(p,G)$ is a uniform distribution on ${\\mathcal{P}}(p,G)$. As the sample size increases, the posterior for $(p,G)$ concentrates around the DGP values and the posterior for $\\tilde{p}$ converges to the uniform distribution on the identified set for $\\tilde{p}$. Thus, a Bayesian credible set for $\\tilde{p}$ excludes about $5\\%$ of the volume of the identified set. In contrast, a $95\\%$ classical confidence set for $\\tilde{p}$ typically includes the identified set.\n\nThe conceptual differences between classical and Bayesian inference for partially identified parameters can also be described as follows. Bayesian inference does not distinguish between the uncertainty from the lack of point identification and that from the sampling variability. In contrast, a standard classical $95\\%$ confidence set allows for errors in $5\\%$ of hypothetical repeated samples; however, the lower bound on the coverage rate is imposed at all parameter values in the parameter space.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2022",
    "question": "In the context of the Baron-Ferejohn model of legislative bargaining under weighted voting, which of the following statements accurately describe the equilibrium outcomes?",
    "options": [
      "A) In interior equilibria, players' expected payoffs are proportional to their voting weights.",
      "B) Power indices typically assign disproportionate power to players with higher voting weights, unlike the noncooperative model.",
      "C) Corner equilibria occur when the weakest players receive expected payoffs less than their shares of the total voting weight.",
      "D) The formateur effect implies that the party recognized to form the government receives a share of posts much larger than its voting weight."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "James M. Snyder-LegislativeBargainingWeighted-2005.md"
    ],
    "question_context": "The Baron-Ferejohn model extends the Ariel Rubinstein (1982) bargaining model to the case of three or more players operating under simple majority rule. In the first case, where legislators’ proposal probabilities are proportional to their voting weights, each player's expected payoff (continuation value) is equal to his share of the total voting weight. The logic in the noncooperative model differs sharply from that of the power indices. Power indices are based on the idea that all orderings (or winning coalitions, or minimal-winning coalitions) are equally likely to form, regardless of how expensive or cheap they are. Under competitive bargaining, expensive coalitions will form rarely or not at all, and cheap coalitions will form quite often. The results also suggest that the linear payoffs of the interior equilibrium are closely related to low proposal power. Supermajoritarian requirements reduce proposal power and help to produce the interior equilibrium. Setting recognition probabilities to be proportional to voting weights also restores linearity in the payoffs. Consistent with the Baron-Ferejohn framework, we find that the party recognized to form the government receives a share of posts much larger than its voting weight.",
    "category": "INCORRECT",
    "question_context_used": "The Baron-Ferejohn model extends the Ariel Rubinstein (1982) bargaining model to the case of three or more players operating under simple majority rule. In the first case, where legislators’ proposal probabilities are proportional to their voting weights, each player's expected payoff (continuation value) is equal to his share of the total voting weight. The logic in the noncooperative model differs sharply from that of the power indices. Power indices are based on the idea that all orderings (or winning coalitions, or minimal-winning coalitions) are equally likely to form, regardless of how expensive or cheap they are. Under competitive bargaining, expensive coalitions will form rarely or not at all, and cheap coalitions will form quite often. The results also suggest that the linear payoffs of the interior equilibrium are closely related to low proposal power. Supermajoritarian requirements reduce proposal power and help to produce the interior equilibrium. Setting recognition probabilities to be proportional to voting weights also restores linearity in the payoffs. Consistent with the Baron-Ferejohn framework, we find that the party recognized to form the government receives a share of posts much larger than its voting weight.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3755",
    "question": "Based on the robustness checks in Tables B1-B6, which of the following parameters exhibit a non-monotonic relationship with government debt as a fraction of GDP ($B/Y$) over time?",
    "options": [
      "A) Intertemporal Elasticity of Substitution (IES)",
      "B) Subjective Discount Factor ($\\beta$)",
      "C) Frisch Elasticity",
      "D) Depreciation Rate"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Dynamic Capital Tax Competition under the Source Principle.md"
    ],
    "question_context": "The tables present robustness checks for various economic parameters, including the Intertemporal Elasticity of Substitution (IES), Subjective Discount Factor ($\\beta$), Frisch Elasticity, and Depreciation Rate. Key formula: $B/Y$ is government debt as a fraction of GDP. For instance, in Table B2, when $\\beta=0.95$, $B/Y$ decreases from 0.58 to 0.36 over 100 periods, whereas when $\\beta=0.97$, $B/Y$ decreases from 0.59 to 0.38, showing a non-monotonic trend. Similarly, in Table B5, the Depreciation Rate shows varying impacts on $B/Y$ across different periods, indicating complex dynamics.",
    "category": "CORRECT",
    "question_context_used": "The tables present robustness checks for various economic parameters, including the Intertemporal Elasticity of Substitution (IES), Subjective Discount Factor ($\\beta$), Frisch Elasticity, and Depreciation Rate. Key formula: $B/Y$ is government debt as a fraction of GDP. For instance, in Table B2, when $\\beta=0.95$, $B/Y$ decreases from 0.58 to 0.36 over 100 periods, whereas when $\\beta=0.97$, $B/Y$ decreases from 0.59 to 0.38, showing a non-monotonic trend. Similarly, in Table B5, the Depreciation Rate shows varying impacts on $B/Y$ across different periods, indicating complex dynamics.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2137",
    "question": "According to the economic analysis of divergent liability rules and trade distortions, which of the following statements are correct regarding the impacts of liability rule unification?",
    "options": [
      "A) The direct impact of rule unification depends on asymmetry among liability rules and existing conflict-of-law conventions.",
      "B) The indirect impact stems from the relationship between domestic firms and foreign buyers.",
      "C) Both direct and indirect impacts may influence resource allocation and factor rents in the long term.",
      "D) The welfare effects of liability rule unification are straightforward and easily quantifiable."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Divergent_Liability_Rules_and_Economic_Integration.md"
    ],
    "question_context": "Union-wide unification of liability rules implies the adoption of new rules by some member states. This implies in turn that unless liability rules are irrelevant, the relationship binding buyer and seller will be altered, and the change will affect the behaviour of both producers (sellers) and consumers (buyers). Since rule divergence creates a problem in the choice of laws, the conventions governing the resolution of confict-of-law also assume importance, as the existence of such conventions alters the impact of the unification of substantive law. With proper allowance for these circumstances, the effects of the proposed rule unification may be characterised as follows: Trade fows may be affected both directly and indirectly. The direct impact, which hinges on the relationship between domestic firm and foreign buyer, will depend on the asymmetry among liability rules. Its incidence will be determined by existing confict-oflaw conventions. The indirect impact, which stems from the relationship between domestic firm and domestic buyer, will depend entirely on asymmetry among liability rules. In addition, both types of impact may have lasting infuence on resource allocation and factor rents.",
    "category": "INCORRECT",
    "question_context_used": "Union-wide unification of liability rules implies the adoption of new rules by some member states. This implies in turn that unless liability rules are irrelevant, the relationship binding buyer and seller will be altered, and the change will affect the behaviour of both producers (sellers) and consumers (buyers). Since rule divergence creates a problem in the choice of laws, the conventions governing the resolution of confict-of-law also assume importance, as the existence of such conventions alters the impact of the unification of substantive law. With proper allowance for these circumstances, the effects of the proposed rule unification may be characterised as follows: Trade fows may be affected both directly and indirectly. The direct impact, which hinges on the relationship between domestic firm and foreign buyer, will depend on the asymmetry among liability rules. Its incidence will be determined by existing confict-oflaw conventions. The indirect impact, which stems from the relationship between domestic firm and domestic buyer, will depend entirely on asymmetry among liability rules. In addition, both types of impact may have lasting infuence on resource allocation and factor rents.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4744",
    "question": "Under the conditions specified in Lemma B.1 and Lemma B.2, which of the following statements are necessary for the convergence of the spectral radius of the covariance matrix estimator?",
    "options": [
      "A) The maximum absolute difference between estimated and true covariance elements must converge to zero in probability.",
      "B) The thresholding parameter ω must be chosen such that m_N ω_NT = o(1).",
      "C) The Frobenius norm of the difference between estimated and true covariance matrices must be O_P(1).",
      "D) The rank of the estimated covariance matrix must be bounded almost surely."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Bootstrapping-factor-models-with-cross-sectional-de_2020_Journal-of-Economet.md"
    ],
    "question_context": "Lemma B.1 states that if (a) max_{i,j≤N}|σ̂_{ij}−σ_{ij}|=O_P(b_{NT}) for some sequence b_{NT}→0, and (b) ω≡ω_{NT}=C b_{NT}→0 for some sufficiently large constant C>0, then it follows that if ω_{NT} is such that m_N ω_{NT}=o(1). Lemma B.2 further establishes that under Assumptions 1–5 strengthened by Assumptions CS and TS, max_{i,j≤N}|σ̂_{ij}−σ_{ij}|=O_P(1/√N + √(log N/T)). These conditions are crucial for ensuring the spectral radius convergence ρ(Σ̃−Σ)→P 0, which is central to the proofs of Theorems 3.1 and 3.2.",
    "category": "CORRECT",
    "question_context_used": "Lemma B.1 states that if (a) max_{i,j≤N}|σ̂_{ij}−σ_{ij}|=O_P(b_{NT}) for some sequence b_{NT}→0, and (b) ω≡ω_{NT}=C b_{NT}→0 for some sufficiently large constant C>0, then it follows that if ω_{NT} is such that m_N ω_{NT}=o(1). Lemma B.2 further establishes that under Assumptions 1–5 strengthened by Assumptions CS and TS, max_{i,j≤N}|σ̂_{ij}−σ_{ij}|=O_P(1/√N + √(log N/T)). These conditions are crucial for ensuring the spectral radius convergence ρ(Σ̃−Σ)→P 0, which is central to the proofs of Theorems 3.1 and 3.2.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1008",
    "question": "In the context of the grouped fixed-effects (GFE) estimator, which of the following statements are correct regarding the asymptotic properties and inference?",
    "options": [
      "A) For fixed T as N tends to infinity, the GFE estimator is root-N consistent and asymptotically normal for a pseudo-true value.",
      "B) The difference between the pseudo-true value and the true parameter value vanishes as T increases.",
      "C) In the presence of additive fixed-effects, consistent estimation of group membership requires that group-specific profiles are parallel.",
      "D) A fixed-T asymptotic analysis is directly informative for performing valid inference for the population parameters."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Bonhomme-GROUPEDPATTERNSHETEROGENEITY-2015.md"
    ],
    "question_context": "The large-N, T asymptotic analysis provides conditions under which group membership estimation does not affect inference. When T is kept fixed as N tends to infinity, estimation of group membership matters for inference. In this alternative asymptotic framework, the variance reflects the additional contribution of observations that are at the margin between two groups. A fixed-T asymptotic analysis is not directly informative to perform valid inference for the population parameters since, for fixed T, the GFE estimator is root-N consistent and asymptotically normal for a pseudo-true value. This pseudo-true value, which minimizes an expected within-group sum of squared residuals, does not coincide with the true parameter value in general, but the difference between the two vanishes as T increases.\n\nIn the presence of additive fixed-effects, consistent estimation of group membership is only possible if the group-specific profiles are not parallel. The group separation condition requires $\\operatorname{plim}_{T\\rightarrow\\infty}\\frac{1}{T}\\sum_{t=1}^{T}(\\alpha_{g t}^{0}-\\overline{{\\alpha}}_{g}^{0}-\\alpha_{\\widetilde{g}t}^{0}+\\overline{{\\alpha}}_{\\widetilde{g}}^{0})^{2}>0$ where $\\overline{{\\alpha}}_{g}^{0}=\\frac{1}{T}\\sum_{t=1}^{T}\\alpha_{g t}^{0}$.",
    "category": "CORRECT",
    "question_context_used": "The large-N, T asymptotic analysis provides conditions under which group membership estimation does not affect inference. When T is kept fixed as N tends to infinity, estimation of group membership matters for inference. In this alternative asymptotic framework, the variance reflects the additional contribution of observations that are at the margin between two groups. A fixed-T asymptotic analysis is not directly informative to perform valid inference for the population parameters since, for fixed T, the GFE estimator is root-N consistent and asymptotically normal for a pseudo-true value. This pseudo-true value, which minimizes an expected within-group sum of squared residuals, does not coincide with the true parameter value in general, but the difference between the two vanishes as T increases.\n\nIn the presence of additive fixed-effects, consistent estimation of group membership is only possible if the group-specific profiles are not parallel. The group separation condition requires $\\operatorname{plim}_{T\\rightarrow\\infty}\\frac{1}{T}\\sum_{t=1}^{T}(\\alpha_{g t}^{0}-\\overline{{\\alpha}}_{g}^{0}-\\alpha_{\\widetilde{g}t}^{0}+\\overline{{\\alpha}}_{\\widetilde{g}}^{0})^{2}>0$ where $\\overline{{\\alpha}}_{g}^{0}=\\frac{1}{T}\\sum_{t=1}^{T}\\alpha_{g t}^{0}$.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1933",
    "question": "Under Assumptions 1 and 2, which of the following statements are true regarding the equivalence of Local Projection (LP) and Vector Autoregression (VAR) impulse response functions?",
    "options": [
      "A) The LP and VAR impulse response functions are equal up to a constant of proportionality that depends on the horizon h.",
      "B) The equivalence result holds nonparametrically, without relying on functional form assumptions on the true data generating process.",
      "C) In the case of finite lag length p, the LP and VAR estimands agree exactly at all horizons h ≤ p.",
      "D) For nonrecursively identified VARs, the equivalence can be established by considering a local projection on a particular linear combination of the variables in the VAR."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "ecta17813.md"
    ],
    "question_context": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. The vector time series $r_{t}$ and $q_{t}$ (which may each be empty) will serve as control variables. Readers who wish to have a structural interpretation in mind may think of $x_{t}$ as predetermined with respect to $y_{t}$ and $r_{t}$ as predetermined with respect to $\\{x_{t},y_{t}\\}$. However, our reduced-form equivalence result below does not require any such predeterminedness assumptions. The precise roles of the controls $r_{t}$ and $q_{t}$ will become clear in equations (1) and (3) below.\n\nFor now, we only make the following standard nonparametric regularity assumption.7\n\nASSUMPTION 1: The data $\\{w_{t}\\}$ are covariance stationary and purely nondeterministic, with an everywhere nonsingular spectral density matrix and absolutely summable Wold decomposition coefficients.\n\nIn particular, we assume nothing about the underlying causal structure of the economy, as this section is concerned solely with properties of linear projections.8\n\nAs an expositional device, we impose an additional assumption of joint Gaussianity.\n\nASSUMPTION 2: $\\{w_{t}\\}$ is a jointly Gaussian vector time series.\n\nThe Gaussianity assumption is made purely for notational simplicity, as this allows us to write conditional expectations instead of linear projections. If we drop the Gaussianity assumption, all calculations below hold with projections in place of conditional expectations.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. The vector time series $r_{t}$ and $q_{t}$ (which may each be empty) will serve as control variables. Readers who wish to have a structural interpretation in mind may think of $x_{t}$ as predetermined with respect to $y_{t}$ and $r_{t}$ as predetermined with respect to $\\{x_{t},y_{t}\\}$. However, our reduced-form equivalence result below does not require any such predeterminedness assumptions. The precise roles of the controls $r_{t}$ and $q_{t}$ will become clear in equations (1) and (3) below.\n\nFor now, we only make the following standard nonparametric regularity assumption.7\n\nASSUMPTION 1: The data $\\{w_{t}\\}$ are covariance stationary and purely nondeterministic, with an everywhere nonsingular spectral density matrix and absolutely summable Wold decomposition coefficients.\n\nIn particular, we assume nothing about the underlying causal structure of the economy, as this section is concerned solely with properties of linear projections.8\n\nAs an expositional device, we impose an additional assumption of joint Gaussianity.\n\nASSUMPTION 2: $\\{w_{t}\\}$ is a jointly Gaussian vector time series.\n\nThe Gaussianity assumption is made purely for notational simplicity, as this allows us to write conditional expectations instead of linear projections. If we drop the Gaussianity assumption, all calculations below hold with projections in place of conditional expectations.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-240",
    "question": "In the Monte Carlo simulation study described, which of the following parameters are varied to examine the finite sample properties of the methods?",
    "options": [
      "A) The number of individuals (N)",
      "B) The number of time observations (T)",
      "C) The number of replications in each experiment",
      "D) The underlying data generating process (DGP)"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "13-Testing_for_Slope_Heterogeneity_Bias_in_Panel_Data_Models.md"
    ],
    "question_context": "We examine the finite sample properties of the methods considering different DGP. We use $N=500$ for the number of individuals and set the number of time observations, $T$ , alternatively, to 5, 10, 20, and 30. The number of replications in each experiment is 5000.6 We first report the results for evaluation of the estimators. Then, we describe the results for the tests. The study focuses on varying key parameters to assess robustness and performance under different conditions.",
    "category": "INCORRECT",
    "question_context_used": "We examine the finite sample properties of the methods considering different DGP. We use $N=500$ for the number of individuals and set the number of time observations, $T$ , alternatively, to 5, 10, 20, and 30. The number of replications in each experiment is 5000.6 We first report the results for evaluation of the estimators. Then, we describe the results for the tests. The study focuses on varying key parameters to assess robustness and performance under different conditions.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4104",
    "question": "In the context of sparse and dense modelling approaches for high-dimensional econometric models, which of the following statements are correct regarding the assumptions and methodologies of Belloni et al. (2014) and Bai and Ng (2006)?",
    "options": [
      "A) Belloni et al. (2014) assume independence of observations and impose a sparsity condition on the model, while Bai and Ng (2006) use a dense modelling approach with principal components.",
      "B) Both approaches require the identification of a specific subset of variables Z_{k(n)} for consistent estimation of the coefficients.",
      "C) The dense modelling approach of Bai and Ng (2006) allows for heteroskedasticity and autocorrelation in the errors, whereas Belloni et al. (2014) do not.",
      "D) Belloni et al. (2014) use a double-selection LASSO procedure to identify the sparse subset Z_{k(n)}, while Bai and Ng (2006) estimate factors via principal components without requiring sparsity."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Simple-and-reliable-estimators-of-coefficients-of-interest_2020_Journal-of-E.md"
    ],
    "question_context": "The sparse modelling approach (e.g. Belloni et al., 2014) assumes that a relatively small dimension $k\\left(n\\right)$ (possibly growing with $n$ ) subset, $Z_{k(n)}$ , of all the observables, W , importantly enters the conditional mean functions for $y$ and $X$ and provides the information for estimation of $\\beta$. The existence of such a set is assumed; usually it will not be known a priori which variables (or transformations) enter into the subset. Belloni et al. (2014) use the LASSO (Tibshirani, 1996) in a double selection procedure that is shown to be consistent. Once the subset $Z_{k(n)}$ is found by the double-selection LASSO, the coefficient $\\beta$ is estimated in the post-double-selection procedure as in (15).\n\nThe factor-augmented model, with $y$ representing a predictor as in Bai and $\\mathrm{Ng}$ (2006), is $y=X\\beta+\\tilde{W}\\gamma+\\varepsilon$, where $X$ is a small set of observed variables, the columns of $\\Tilde{W}$ represent the unobserved factors, and $g=\\tilde{W}\\gamma$. A large number $(d)$ of observables, columns of the $n\\times d$ matrix $W$ , provide information about $\\Tilde{W}$: $W=\\tilde{W}\\varLambda+e$; with $\\varLambda$ the matrix of loadings and $e$ representing the idiosyncratic error.",
    "category": "CORRECT",
    "question_context_used": "The sparse modelling approach (e.g. Belloni et al., 2014) assumes that a relatively small dimension $k\\left(n\\right)$ (possibly growing with $n$ ) subset, $Z_{k(n)}$ , of all the observables, W , importantly enters the conditional mean functions for $y$ and $X$ and provides the information for estimation of $\\beta$. The existence of such a set is assumed; usually it will not be known a priori which variables (or transformations) enter into the subset. Belloni et al. (2014) use the LASSO (Tibshirani, 1996) in a double selection procedure that is shown to be consistent. Once the subset $Z_{k(n)}$ is found by the double-selection LASSO, the coefficient $\\beta$ is estimated in the post-double-selection procedure as in (15).\n\nThe factor-augmented model, with $y$ representing a predictor as in Bai and $\\mathrm{Ng}$ (2006), is $y=X\\beta+\\tilde{W}\\gamma+\\varepsilon$, where $X$ is a small set of observed variables, the columns of $\\Tilde{W}$ represent the unobserved factors, and $g=\\tilde{W}\\gamma$. A large number $(d)$ of observables, columns of the $n\\times d$ matrix $W$ , provide information about $\\Tilde{W}$: $W=\\tilde{W}\\varLambda+e$; with $\\varLambda$ the matrix of loadings and $e$ representing the idiosyncratic error.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2986",
    "question": "In the hierarchical model (i) with a perfectly informed supervisor, which of the following conditions are necessary for a distribution $F$ to belong to $\\Phi^{P S A(i)}(w)$?",
    "options": [
      "A) There exists a technology $\\mathcal{A} \\supseteq \\mathcal{A}^{P}$ and a contract $w_{A} \\in S$ such that $(F, c)$ maximizes the agent's payoff over $\\mathcal{A}$ given $w_{A}$.",
      "B) The supervisor offers a contract $w_{A} = w_{0}$ (zero contract) and the action $(F, 0)$ is included in the technology $\\mathcal{A}$.",
      "C) The distribution $F$ maximizes the principal's payoff over all distributions that are optimal for the agent and supervisor, given the contracts $w$ and $w_{A}$.",
      "D) The supervisor's choice of $w_{A}$ must be a linear contract with slope in the interval [0, 1]."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "ecta17386.md"
    ],
    "question_context": "In hierarchical model (i), the principal contracts with a supervisor who is perfectly informed of the technology $\\mathcal{A} \\supseteq \\mathcal{A}^{P}$. The supervisor then contracts with the agent, who chooses an action $(F, c)$ to maximize their payoff. The supervisor aims to maximize the difference between payments from the principal and payments to the agent, subject to the agent's optimal behavior and tie-breaking conditions favoring the principal. A distribution $F$ is in $\\Phi^{P S A(i)}(w)$ if there exists a $P S A(i)$-certificate $(\\mathcal{A}, c, w_{A})$ satisfying: (a) supervisor maximization, (b) agent maximization, (c) supervisor-preferred tie-breaking, and (d) principal-preferred tie-breaking. Lemma 7 shows that we can restrict attention to certificates where $c = 0$ and $w_{A} = w_{0}$, simplifying the analysis. The model ensures that $\\Phi^{P S A(i)}(w)$ is nonempty and allows for the use of linear contracts to maximize the principal's objective $V_{P}^{P S A(i)}(w)$.",
    "category": "INCORRECT",
    "question_context_used": "In hierarchical model (i), the principal contracts with a supervisor who is perfectly informed of the technology $\\mathcal{A} \\supseteq \\mathcal{A}^{P}$. The supervisor then contracts with the agent, who chooses an action $(F, c)$ to maximize their payoff. The supervisor aims to maximize the difference between payments from the principal and payments to the agent, subject to the agent's optimal behavior and tie-breaking conditions favoring the principal. A distribution $F$ is in $\\Phi^{P S A(i)}(w)$ if there exists a $P S A(i)$-certificate $(\\mathcal{A}, c, w_{A})$ satisfying: (a) supervisor maximization, (b) agent maximization, (c) supervisor-preferred tie-breaking, and (d) principal-preferred tie-breaking. Lemma 7 shows that we can restrict attention to certificates where $c = 0$ and $w_{A} = w_{0}$, simplifying the analysis. The model ensures that $\\Phi^{P S A(i)}(w)$ is nonempty and allows for the use of linear contracts to maximize the principal's objective $V_{P}^{P S A(i)}(w)$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1318",
    "question": "In the context of the Calvo (1978) model and Ramsey planning, which of the following statements accurately describe the properties or implications of the Ramsey plan?",
    "options": [
      "A) The Ramsey plan exhibits time inconsistency due to the government's incentive to deviate from promised inflation rates.",
      "B) The Ramsey planner's optimal decision rule for money growth rates is linear in the state variable πt.",
      "C) The Ramsey plan ensures that promised inflation rates always equal actual inflation rates.",
      "D) The 'timeless perspective' eliminates the root cause of time inconsistency by fixing π0 to a constant value."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Critique-and-consequence_2024_Journal-of-Monetary-Economics.md"
    ],
    "question_context": "The Calvo (1978) model endogenizes the money growth rate μ by constructing a theory of optimal public policy. The Ramsey planner maximizes a social welfare function subject to monetarist constraints, leading to a recursive representation of the plan. Key equations include the Bellman equation for the value function J(x) = -x'Px and the optimal decision rule μ = -Fx. The Ramsey plan is characterized by time inconsistency, as continuation planners are incentivized to deviate from the initial plan. The model also introduces the concept of a 'timeless perspective,' which attempts to address time inconsistency by fixing the initial inflation rate but does not eliminate the underlying incentive to deviate. The Ramsey plan's promised inflation rates πt are synthetic targets that, when substituted into the model's constraints, yield actual inflation rates equal to the promised ones.",
    "category": "INCORRECT",
    "question_context_used": "The Calvo (1978) model endogenizes the money growth rate μ by constructing a theory of optimal public policy. The Ramsey planner maximizes a social welfare function subject to monetarist constraints, leading to a recursive representation of the plan. Key equations include the Bellman equation for the value function J(x) = -x'Px and the optimal decision rule μ = -Fx. The Ramsey plan is characterized by time inconsistency, as continuation planners are incentivized to deviate from the initial plan. The model also introduces the concept of a 'timeless perspective,' which attempts to address time inconsistency by fixing the initial inflation rate but does not eliminate the underlying incentive to deviate. The Ramsey plan's promised inflation rates πt are synthetic targets that, when substituted into the model's constraints, yield actual inflation rates equal to the promised ones.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4805",
    "question": "In the context of the logit model of demand for mobile operators, which of the following statements are true regarding the waterbed effect?",
    "options": [
      "A) The waterbed effect is strongest in saturated and competitive markets.",
      "B) The waterbed effect implies that lower termination rates lead to higher customer bills.",
      "C) In a perfectly competitive industry, changes in termination rates have no impact on profits due to a complete waterbed effect.",
      "D) The waterbed effect is independent of the number of competing firms and the degree of product homogeneity."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Genakos-TESTINGWATERBEDEFFECT-2011.md"
    ],
    "question_context": "The logit demand functions are obtained by assuming that all the $\\mu_{i}$ are i.i.d. and follow the double exponential distribution with zero mean. The market share of firm $i$ is given by $\\alpha_{i}=\\frac{\\exp[(U-P_{i})/\\sigma]}{\\displaystyle\\sum_{j=1}^{n}\\exp[(U-P_{j})/\\sigma]+\\exp[V_{0}/\\sigma]}$, where $\\sigma$ is a constant related to the degree of product differentiation. The profit of operator $i$ is $\\Pi_{i}=(P_{i}-c+\\tau)\\alpha_{i}N$, where $\\tau=T N_{F}q_{I}$ is the termination rent per mobile customer. The equilibrium price is defined by $P^{*}=c-\\tau+\\frac{\\sigma}{1-\\alpha^{*}}$, where $\\alpha^{*}=\\frac{1}{n+\\exp[(V_{0}-U+P^{*})/\\sigma]}$. Proposition 1 states that lower termination rates are associated with higher bills to mobile customers, i.e., the waterbed effect. Proposition 2 indicates that the waterbed effect is stronger in more competitive and saturated markets. Proposition 3 shows that in a perfectly competitive industry, changes in termination rates have no impact on profits due to a complete waterbed effect.",
    "category": "CORRECT",
    "question_context_used": "The logit demand functions are obtained by assuming that all the $\\mu_{i}$ are i.i.d. and follow the double exponential distribution with zero mean. The market share of firm $i$ is given by $\\alpha_{i}=\\frac{\\exp[(U-P_{i})/\\sigma]}{\\displaystyle\\sum_{j=1}^{n}\\exp[(U-P_{j})/\\sigma]+\\exp[V_{0}/\\sigma]}$, where $\\sigma$ is a constant related to the degree of product differentiation. The profit of operator $i$ is $\\Pi_{i}=(P_{i}-c+\\tau)\\alpha_{i}N$, where $\\tau=T N_{F}q_{I}$ is the termination rent per mobile customer. The equilibrium price is defined by $P^{*}=c-\\tau+\\frac{\\sigma}{1-\\alpha^{*}}$, where $\\alpha^{*}=\\frac{1}{n+\\exp[(V_{0}-U+P^{*})/\\sigma]}$. Proposition 1 states that lower termination rates are associated with higher bills to mobile customers, i.e., the waterbed effect. Proposition 2 indicates that the waterbed effect is stronger in more competitive and saturated markets. Proposition 3 shows that in a perfectly competitive industry, changes in termination rates have no impact on profits due to a complete waterbed effect.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3973",
    "question": "In the context of the New Keynesian Phillips Curve (NKPC) analysis by Kleibergen & Mavroeidis, which of the following statements are correct regarding weak identification and robust inference methods?",
    "options": [
      "A) The NKPC is weakly identified when λ=0, regardless of the value of γf.",
      "B) Many instrument robust procedures require strong identification and are not suitable when the NKPC is weakly identified.",
      "C) The subset Kleibergen Lagrange multiplier (KLM) and MQLR statistics are robust to both weak identification and many instruments.",
      "D) The concentration parameter in the NKPC is independent of the structural parameters α and ω."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "08-Rejoinder.md"
    ],
    "question_context": "The New Keynesian Phillips Curve (NKPC) is a fundamental model in macroeconomic theory, and its identification has been a subject of extensive research. Kleibergen & Mavroeidis (2009) analyze the conditions under which the NKPC is weakly identified, emphasizing the role of the parameter λ and the forward-looking coefficient γf. Their analysis shows that the NKPC is weakly identified when λ=0, and this condition holds regardless of whether the model is purely forward-looking or hybrid (γb≠0). The strength of identification in the NKPC is measured by the concentration parameter, which depends intricately on the structural parameters α and ω, as shown by the equations γf=α/(α+ω) and λ=(1−ω)(1−α)²/(α+ω). This complexity makes it difficult to apply conventional instrumental variable methods directly. The authors advocate for the use of identification-robust methods like the subset KLM and MQLR statistics, which perform well even in the presence of weak identification and many instruments, unlike many instrument robust procedures that assume strong identification.",
    "category": "INCORRECT",
    "question_context_used": "The New Keynesian Phillips Curve (NKPC) is a fundamental model in macroeconomic theory, and its identification has been a subject of extensive research. Kleibergen & Mavroeidis (2009) analyze the conditions under which the NKPC is weakly identified, emphasizing the role of the parameter λ and the forward-looking coefficient γf. Their analysis shows that the NKPC is weakly identified when λ=0, and this condition holds regardless of whether the model is purely forward-looking or hybrid (γb≠0). The strength of identification in the NKPC is measured by the concentration parameter, which depends intricately on the structural parameters α and ω, as shown by the equations γf=α/(α+ω) and λ=(1−ω)(1−α)²/(α+ω). This complexity makes it difficult to apply conventional instrumental variable methods directly. The authors advocate for the use of identification-robust methods like the subset KLM and MQLR statistics, which perform well even in the presence of weak identification and many instruments, unlike many instrument robust procedures that assume strong identification.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4725",
    "question": "In the empirical model analyzing the effect of instruction time on achievement, which of the following factors are explicitly addressed by the school-by-grade fixed effects?",
    "options": [
      "A) Subject-specific differences in student ability",
      "B) National minimum school starting and leaving ages",
      "C) Family background characteristics",
      "D) Subject-specific instructional quality"
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Instruction_time,_Classroom_Quality,_and_Academic_Achievement.md"
    ],
    "question_context": "The school-by-grade fixed effect $(\\omega)$ accounts for differences in average ability, level of disruption and school quality that are common across subjects for students in a particular cohort, grade, school and country. Therefore all subject invariant differences in academic skills and school quality at each grade level are removed. Only within school and grade instruction time differences among subjects remain for identification. Note, importantly, that the school-by-grade fixed effects fully account for a range of subject invariant influences, including national minimum school starting and leaving ages, school funding and governance structures and family background.\n\nThe school-by-grade fixed effect does not account for differences among subjects in either instruction time or various other factors that could influence achievement. These include, but are not limited to, national curricula, the quality of instruction in one subject in relation to the other and subject-specific student skills. The country-bygrade-by-subject fixed effect (h) captures some such influences, including national curricula but it does not capture subject-specific abilities or instructional quality specific to schools that are related to instruction time.",
    "category": "CORRECT",
    "question_context_used": "The school-by-grade fixed effect $(\\omega)$ accounts for differences in average ability, level of disruption and school quality that are common across subjects for students in a particular cohort, grade, school and country. Therefore all subject invariant differences in academic skills and school quality at each grade level are removed. Only within school and grade instruction time differences among subjects remain for identification. Note, importantly, that the school-by-grade fixed effects fully account for a range of subject invariant influences, including national minimum school starting and leaving ages, school funding and governance structures and family background.\n\nThe school-by-grade fixed effect does not account for differences among subjects in either instruction time or various other factors that could influence achievement. These include, but are not limited to, national curricula, the quality of instruction in one subject in relation to the other and subject-specific student skills. The country-bygrade-by-subject fixed effect (h) captures some such influences, including national curricula but it does not capture subject-specific abilities or instructional quality specific to schools that are related to instruction time.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1791",
    "question": "Which of the following factors are explicitly mentioned as influencing commodity price volatility in the historical data analysis?",
    "options": [
      "A) Periods of war and autarky",
      "B) Implementation of tariffs and embargoes",
      "C) Changes in the base year for price indices",
      "D) Long-run trends in the price series"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Jacks-COMMODITYPRICEVOLATILITY-2011.md"
    ],
    "question_context": "The text analyzes commodity price volatility from 1700 to 2008 using multiple datasets. Key formula: The price series are transformed into index form using $\\ln(P_{t}/P_{t-I})$ for volatility measurement. The analysis explicitly notes that 'periods of war and autarky seem broadly correlated with greater commodity price volatility'. Additionally, the use of local market price quotes is mentioned to reflect 'the impact of tariffs and embargoes'. However, the text emphasizes there is 'very little long-run trend in the series' and that the 1950s-1960s were a period of relatively low volatility compared to other periods. The base year (2000=100) is mentioned only as a methodological detail without connection to volatility patterns.",
    "category": "CORRECT",
    "question_context_used": "The text analyzes commodity price volatility from 1700 to 2008 using multiple datasets. Key formula: The price series are transformed into index form using $\\ln(P_{t}/P_{t-I})$ for volatility measurement. The analysis explicitly notes that 'periods of war and autarky seem broadly correlated with greater commodity price volatility'. Additionally, the use of local market price quotes is mentioned to reflect 'the impact of tariffs and embargoes'. However, the text emphasizes there is 'very little long-run trend in the series' and that the 1950s-1960s were a period of relatively low volatility compared to other periods. The base year (2000=100) is mentioned only as a methodological detail without connection to volatility patterns.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1854",
    "question": "Based on the experimental data on profit maximization vs. worker layoffs, which of the following statements are statistically supported?",
    "options": [
      "A) Economics students exhibited the highest proportion of profit-maximizing choices compared to other disciplines.",
      "B) Philosophy students had the highest median response time, indicating deeper cognitive reasoning.",
      "C) The 'no layoffs' response was most prevalent among Philosophy students, diverging sharply from other groups.",
      "D) The distribution of response times suggests profit maximization is an instinctive outcome due to shorter median times."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Sceptic's_Comment_on_the_Study_of_Economics.md"
    ],
    "question_context": "The study reveals sharp disciplinary differences in responses to the profit maximization vs. worker layoffs dilemma. Among Econ students, 45%–49% chose the profit-maximizing alternative (laying off 96 workers), compared to only 13%–16% of Phil and Math students. The 'no layoffs' response was given by 36% of Philosophy students, far exceeding other groups (6%–13%). Response time analysis showed minimal differences, with the 'no layoffs' answer having the lowest median response time (104s), contradicting the hypothesis that profit maximization is instinctive. Key formula for profit maximization: $\\pi^* = \\argmax_{\\pi} \\sum (R(w) - C(w))$, where $w$ is the number of workers retained.",
    "category": "CORRECT",
    "question_context_used": "The study reveals sharp disciplinary differences in responses to the profit maximization vs. worker layoffs dilemma. Among Econ students, 45%–49% chose the profit-maximizing alternative (laying off 96 workers), compared to only 13%–16% of Phil and Math students. The 'no layoffs' response was given by 36% of Philosophy students, far exceeding other groups (6%–13%). Response time analysis showed minimal differences, with the 'no layoffs' answer having the lowest median response time (104s), contradicting the hypothesis that profit maximization is instinctive. Key formula for profit maximization: $\\pi^* = \\argmax_{\\pi} \\sum (R(w) - C(w))$, where $w$ is the number of workers retained.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-919",
    "question": "In the context of modeling earnings dynamics using copula functions, which of the following statements are correct regarding the t-copula and its estimation?",
    "options": [
      "A) The t-copula's parameters, ρ and ν, describe the overall level of immobility and excess immobility in the tails of the distribution, respectively.",
      "B) The two-step pseudo maximum likelihood procedure is favored over the one-step estimator due to its computational efficiency and robustness to marginal distribution misspecification.",
      "C) The t-copula is the dependence structure implicit in a bivariate normal distribution.",
      "D) The degrees of freedom parameter ν is constrained by the function gν(z) = e^z to ensure it remains within its valid range."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Higher_Education_Funding_Reforms_in_England_The_Distributional_Effects_and_the_Shifting_Balance_of_Costs.md"
    ],
    "question_context": "The approach that uses copula functions is relatively new to the literature on modelling earnings dynamics. The t-copula is the dependence structure implicit in a bivariate t distribution. It has two parameters – the correlation parameter, ρ, and the degrees of freedom parameter, ν. These can be broadly interpreted as describing the overall level of immobility in the distribution and the excess immobility in the tails of the distribution. The function θ(a_i,X_i) describes the way in which we restrict the copula parameters to depend on experience and the observable characteristics. We assume the following functional forms: \n\n$$\n\\begin{array}{l}{{\\rho(a,X)=g_{\\rho}(\\beta_{0}^{X}+\\beta_{1}^{X}a+\\beta_{2}^{X}a^{2}+\\beta_{3}^{X}a^{3});}}\\\\ {{\\nu(a,X)=g_{\\nu}(\\beta_{0}^{X}+\\beta_{1}^{X}a+\\beta_{2}^{X}a^{2}),}}\\end{array}\n$$\n\nwhere g_ρ and g_ν are monotonic functions designed to keep ρ and ν inside their respective ranges:\n\n$$\n\\begin{array}{l}{{\\displaystyle g_{\\rho}(z)=\\frac{1-{\\bf e}^{-z}}{1+{\\bf e}^{-z}}}}\\\\ {{\\displaystyle g_{\\nu}(z)={\\bf e}^{z}.}}\\end{array}\n$$\n\nTo estimate the model, we use the two-step pseudo maximum likelihood procedure of Genest et al. (1995). This two-step semi-parametric approach has the advantage that inference about the copula parameters and copula model selection is robust to misspecification of the marginal distributions. However, it is not efficient; a more efficient one-step estimator is provided in Chen et al. (2006). However, we favour the two-step approach mainly for reasons of computational efficiency.",
    "category": "INCORRECT",
    "question_context_used": "The approach that uses copula functions is relatively new to the literature on modelling earnings dynamics. The t-copula is the dependence structure implicit in a bivariate t distribution. It has two parameters – the correlation parameter, ρ, and the degrees of freedom parameter, ν. These can be broadly interpreted as describing the overall level of immobility in the distribution and the excess immobility in the tails of the distribution. The function θ(a_i,X_i) describes the way in which we restrict the copula parameters to depend on experience and the observable characteristics. We assume the following functional forms: \n\n$$\n\\begin{array}{l}{{\\rho(a,X)=g_{\\rho}(\\beta_{0}^{X}+\\beta_{1}^{X}a+\\beta_{2}^{X}a^{2}+\\beta_{3}^{X}a^{3});}}\\\\ {{\\nu(a,X)=g_{\\nu}(\\beta_{0}^{X}+\\beta_{1}^{X}a+\\beta_{2}^{X}a^{2}),}}\\end{array}\n$$\n\nwhere g_ρ and g_ν are monotonic functions designed to keep ρ and ν inside their respective ranges:\n\n$$\n\\begin{array}{l}{{\\displaystyle g_{\\rho}(z)=\\frac{1-{\\bf e}^{-z}}{1+{\\bf e}^{-z}}}}\\\\ {{\\displaystyle g_{\\nu}(z)={\\bf e}^{z}.}}\\end{array}\n$$\n\nTo estimate the model, we use the two-step pseudo maximum likelihood procedure of Genest et al. (1995). This two-step semi-parametric approach has the advantage that inference about the copula parameters and copula model selection is robust to misspecification of the marginal distributions. However, it is not efficient; a more efficient one-step estimator is provided in Chen et al. (2006). However, we favour the two-step approach mainly for reasons of computational efficiency.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4269",
    "question": "In the context of unemployment insurance (UI) and consumption smoothing, which of the following statements are supported by the text?",
    "options": [
      "A) Private unemployment insurance markets are inefficient due to adverse selection and moral hazard.",
      "B) Government provision of UI exacerbates search behavior distortions without providing any consumption smoothing benefits.",
      "C) The primary benefit of UI is the ability to smooth consumption during unemployment spells, which private markets fail to provide efficiently.",
      "D) Empirical evidence on the benefits of UI is abundant and well-documented, outweighing the evidence on its distorting effects."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Gruber-ConsumptionSmoothingBenefits-1997.md"
    ],
    "question_context": "The presence of these programs often is justified by the failure of private insurance markets along some dimension. This justification implies that increasing program generosity may lead not only to increased distortions to behavior, but to increased consumption smoothing across states of the world as well. These consumption smoothing gains must be weighed against the carefully documented set of behavioral distortions in assessing optimal program size. To date, however, there has been little empirical work on the benefits from increased social insurance generosity. The primary benefit of UI is the ability of the government to smooth consumption during unemployment spells. Private unemployment insurance could provide the same function, but, due to problems such as adverse selection, private consumption insurance markets for spells of unemployment may not exist. Individuals can save for unemployment, but this is less efficient than pooling unemployment risk through insurance, since those who do not end up losing their jobs are inefficiently reducing today's consumption.",
    "category": "CORRECT",
    "question_context_used": "The presence of these programs often is justified by the failure of private insurance markets along some dimension. This justification implies that increasing program generosity may lead not only to increased distortions to behavior, but to increased consumption smoothing across states of the world as well. These consumption smoothing gains must be weighed against the carefully documented set of behavioral distortions in assessing optimal program size. To date, however, there has been little empirical work on the benefits from increased social insurance generosity. The primary benefit of UI is the ability of the government to smooth consumption during unemployment spells. Private unemployment insurance could provide the same function, but, due to problems such as adverse selection, private consumption insurance markets for spells of unemployment may not exist. Individuals can save for unemployment, but this is less efficient than pooling unemployment risk through insurance, since those who do not end up losing their jobs are inefficiently reducing today's consumption.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3261",
    "question": "Which of the following statements are true regarding Nash stable partitions in the Fixed Policy and Voting conditions?",
    "options": [
      "A) The Segregated partition is the only Nash stable partition in both conditions.",
      "B) The Pareto Dominant partition maximizes aggregate payoffs and is strong Nash stable.",
      "C) The Voting condition leads to larger group sizes compared to the Fixed Policy condition.",
      "D) Subjects in the Fixed Policy condition experience their ideal policy more frequently than those in the Voting condition."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Robbett - 2015 - Voting with hands and feet The requirements for optimal group formation.md"
    ],
    "question_context": "A partition of subjects is Nash stable if no subject can receive higher payoffs by unilaterally relocating to a different group. A partition is strong Nash stable if there is no set of agents who can each receive higher payoffs by collectively relocating. The Pareto Dominant partition, which maximizes aggregate payoffs, is also the unique strong Nash stable partition under both Fixed Policy and Voting conditions. However, the Voting condition tends to result in larger group sizes and policies further from ideal points, whereas the Fixed Policy condition often leads to smaller groups and policies closer to ideal points. Key formula: The range of policies for Nash stability is given by the condition $\\text{min distance} > \\gamma(\\alpha_i - \\alpha_j)$, where $\\gamma$ is a parameter and $\\alpha_i$ are the ideal points.",
    "category": "INCORRECT",
    "question_context_used": "A partition of subjects is Nash stable if no subject can receive higher payoffs by unilaterally relocating to a different group. A partition is strong Nash stable if there is no set of agents who can each receive higher payoffs by collectively relocating. The Pareto Dominant partition, which maximizes aggregate payoffs, is also the unique strong Nash stable partition under both Fixed Policy and Voting conditions. However, the Voting condition tends to result in larger group sizes and policies further from ideal points, whereas the Fixed Policy condition often leads to smaller groups and policies closer to ideal points. Key formula: The range of policies for Nash stability is given by the condition $\\text{min distance} > \\gamma(\\alpha_i - \\alpha_j)$, where $\\gamma$ is a parameter and $\\alpha_i$ are the ideal points.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2635",
    "question": "In the field experiment described by Neumark, Burn, and Button (2015), which of the following methodological choices were explicitly designed to address the Heckman critique in correspondence studies?",
    "options": [
      "A) Random assignment of high-skill or low-skill resumes within a triplet to avoid dominance of age effects",
      "B) Inclusion of post-secondary degrees and occupation-specific skills in high-skill resumes",
      "C) Generation of applicants of different skill levels for each job applied to",
      "D) Randomization of resume characteristics unrelated to hiring across all resumes"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Neumark-ExperimentalAgeDiscrimination-2016.md"
    ],
    "question_context": "The standard procedures for correspondence studies include: creation of data on artificial job applicants; applying for jobs; collection of data on hiring-related outcomes; and statistical analysis. The statistical analysis without quality variation in resumes is straightforward, and the extension to consider the Heckman critique follows the previous section. To explore the implications for the Heckman critique, we generated applicants of different skill levels for each job for which we apply. We assign all applicants to the same employer as either high skilled or low skilled, with 50 percent probability for each, so that random assignment of high-skill or low-skill resumes within a triplet does not dominate the effect of age. Other resume characteristics that are not supposed to affect hiring are randomized across resumes, as in other audit and correspondence studies.",
    "category": "INCORRECT",
    "question_context_used": "The standard procedures for correspondence studies include: creation of data on artificial job applicants; applying for jobs; collection of data on hiring-related outcomes; and statistical analysis. The statistical analysis without quality variation in resumes is straightforward, and the extension to consider the Heckman critique follows the previous section. To explore the implications for the Heckman critique, we generated applicants of different skill levels for each job for which we apply. We assign all applicants to the same employer as either high skilled or low skilled, with 50 percent probability for each, so that random assignment of high-skill or low-skill resumes within a triplet does not dominate the effect of age. Other resume characteristics that are not supposed to affect hiring are randomized across resumes, as in other audit and correspondence studies.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3444",
    "question": "In the context of the EGARCH model's asymptotic properties, which of the following statements are correct regarding the long run variance matrix Ξ and its components?",
    "options": [
      "A) Ξ(f,g) is the long run variance of χt and can be decomposed into submatrices ΞLL and ΞNN.",
      "B) The submatrix ΞLL corresponds to the vector [a⊤t,b⊤t]⊤ and is independent of the error density f.",
      "C) The limit in the definition of Ξ(f,g) exists due to the moment and mixing properties of the process χt.",
      "D) For consistent estimation of parameters ωo, θo, αo in the EGARCH model, knowledge of the error density f is not required."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Hafner和Linton - 2017 - AN ALMOST CLOSED FORM ESTIMATOR FOR THE EGARCH MODEL.md"
    ],
    "question_context": "The long run variance matrix Ξ is defined as the limit of the variance of the normalized sum of χt, where χt is a vector composed of components at, bt, and ct. The matrix Ξ(f,g) is structured into submatrices ΞLL and ΞNN, with ΞLL specifically corresponding to the vector [a⊤t,b⊤t]⊤. The existence of this limit is guaranteed by the moment and mixing properties of χt. However, unlike in some other models, consistent estimation of certain parameters in the EGARCH framework, such as ωo, θo, and αo, necessitates knowledge of the error density f. This is a critical distinction from models like the variance targeting GARCH, where such knowledge might not be strictly necessary for consistent estimation.",
    "category": "CORRECT",
    "question_context_used": "The long run variance matrix Ξ is defined as the limit of the variance of the normalized sum of χt, where χt is a vector composed of components at, bt, and ct. The matrix Ξ(f,g) is structured into submatrices ΞLL and ΞNN, with ΞLL specifically corresponding to the vector [a⊤t,b⊤t]⊤. The existence of this limit is guaranteed by the moment and mixing properties of χt. However, unlike in some other models, consistent estimation of certain parameters in the EGARCH framework, such as ωo, θo, and αo, necessitates knowledge of the error density f. This is a critical distinction from models like the variance targeting GARCH, where such knowledge might not be strictly necessary for consistent estimation.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2058",
    "question": "In the context of Bayesian bootstrap analysis of choice-based sampling designs with a Probit model, which of the following statements are supported by the experimental results?",
    "options": [
      "A) Precise knowledge of the marginal choice probability significantly improves the estimation precision of the Probit intercept but introduces non-Normality in its posterior distribution.",
      "B) Balanced choice-based sampling provides a reduction in posterior standard deviation of approximately 30% compared to random sampling when the marginal choice probability is small.",
      "C) Knowledge of the marginal choice probability has negligible impact on the estimation precision of covariate effects in the Probit model.",
      "D) The intercept in a Logit model would be identifiable from the same choice-based sample design as used for the Probit model."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Bayes-WESML-Posterior-inference-from-choice-based-_1997_Journal-of-Econometr.md"
    ],
    "question_context": "The study employs a Probit model $P_{1x}(\\theta)=\\varPhi(\\theta_{0}+\\theta_{1}x)$ with $\\theta_{0}={}-2,\\theta_{1}=1$ and a symmetric, unimodal distribution of $\\pmb{X}$ over 9 points from -2 to 2. The marginal probability of choice 1 is $\\pmb{Q}_{1}=\\mathbf{0.08}$, making the event rare. Experiment 1 compares scenarios with precise knowledge ($N_{q}=1000$) versus no knowledge ($\\pmb{N_{q}}=\\pmb{0}$) of $\\pmb{Q}_{1}$, showing that precise knowledge skews the posterior distribution of the intercept but has little effect on the slope. Experiment 2 demonstrates that balanced choice-based sampling (500 observations per choice) yields a 25% reduction in inter-quartile range for the intercept and similar gains for the slope compared to 'random' sampling (80 vs. 920 observations). The results highlight that knowledge of $\\pmb{Q}_{1}$ primarily affects intercept precision and induces non-Normality, while balanced sampling enhances estimation efficiency for rare events.",
    "category": "INCORRECT",
    "question_context_used": "The study employs a Probit model $P_{1x}(\\theta)=\\varPhi(\\theta_{0}+\\theta_{1}x)$ with $\\theta_{0}={}-2,\\theta_{1}=1$ and a symmetric, unimodal distribution of $\\pmb{X}$ over 9 points from -2 to 2. The marginal probability of choice 1 is $\\pmb{Q}_{1}=\\mathbf{0.08}$, making the event rare. Experiment 1 compares scenarios with precise knowledge ($N_{q}=1000$) versus no knowledge ($\\pmb{N_{q}}=\\pmb{0}$) of $\\pmb{Q}_{1}$, showing that precise knowledge skews the posterior distribution of the intercept but has little effect on the slope. Experiment 2 demonstrates that balanced choice-based sampling (500 observations per choice) yields a 25% reduction in inter-quartile range for the intercept and similar gains for the slope compared to 'random' sampling (80 vs. 920 observations). The results highlight that knowledge of $\\pmb{Q}_{1}$ primarily affects intercept precision and induces non-Normality, while balanced sampling enhances estimation efficiency for rare events.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4498",
    "question": "Based on Russia's collaboration with the IMF, which of the following strategies are essential for successful financial stabilization?",
    "options": [
      "A) Maintaining strict monetary program criteria despite political pressures",
      "B) Assigning political conflict resolution tasks to the IMF",
      "C) Coordinated gradual work between government and central bank with IMF",
      "D) Joint creative analysis of economic and political problems during stabilization"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Gaidar-IMFRussia-1997.md"
    ],
    "question_context": "Key lessons from Russia's collaboration with the IMF highlight that (i) the IMF is not suited for political tasks, (ii) strict monetary program criteria must be maintained to prevent capital outflow, (iii) coordinated work between government, central bank, and IMF can sharply reduce inflation via orthodox methods, and (iv) joint creative analysis of economic and political problems is essential for stabilization success. The formula for success involves: \\[ \text{Stabilization} = \text{Strict Criteria} + \text{Coordination} + \text{Adaptive Analysis} \\].",
    "category": "CORRECT",
    "question_context_used": "Key lessons from Russia's collaboration with the IMF highlight that (i) the IMF is not suited for political tasks, (ii) strict monetary program criteria must be maintained to prevent capital outflow, (iii) coordinated work between government, central bank, and IMF can sharply reduce inflation via orthodox methods, and (iv) joint creative analysis of economic and political problems is essential for stabilization success. The formula for success involves: \\[ \text{Stabilization} = \text{Strict Criteria} + \text{Coordination} + \text{Adaptive Analysis} \\].",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1853",
    "question": "In the context of the ILJK company's profit optimization problem, which of the following statements are mathematically valid based on the provided profit function $2\\sqrt{x} - 0.1x - 8$?",
    "options": [
      "A) The profit-maximizing number of workers to retain is 100",
      "B) The derivative of the profit function with respect to $x$ is $\\frac{1}{\\sqrt{x}} - 0.1$",
      "C) The company would incur losses if it retains all 196 workers",
      "D) The profit function exhibits diminishing marginal returns to labor"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Sceptic's_Comment_on_the_Study_of_Economics.md"
    ],
    "question_context": "The Finance Department's profit forecast is given by the function $2\\sqrt{x} - 0.1x - 8$ where $x$ is the number of workers retained. From the table version, we see that retaining 100 workers yields maximum profit of NIS 2 million, while retaining all 196 workers yields only NIS 0.4 million. The mathematical version allows us to analyze the problem through calculus: the first derivative $\\frac{1}{\\sqrt{x}} - 0.1$ shows the rate of change of profit with respect to labor, while the second derivative $-\\frac{1}{2}x^{-3/2}$ confirms the concave nature of the profit function, indicating diminishing marginal returns.",
    "category": "INCORRECT",
    "question_context_used": "The Finance Department's profit forecast is given by the function $2\\sqrt{x} - 0.1x - 8$ where $x$ is the number of workers retained. From the table version, we see that retaining 100 workers yields maximum profit of NIS 2 million, while retaining all 196 workers yields only NIS 0.4 million. The mathematical version allows us to analyze the problem through calculus: the first derivative $\\frac{1}{\\sqrt{x}} - 0.1$ shows the rate of change of profit with respect to labor, while the second derivative $-\\frac{1}{2}x^{-3/2}$ confirms the concave nature of the profit function, indicating diminishing marginal returns.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-285",
    "question": "In the context of measurement errors in calorie availability and total expenditure models, which of the following statements are correct regarding the direction of bias in OLS and IV estimators?",
    "options": [
      "A) The OLS estimator for the CA,TE pair exhibits upward bias when [k(1-PG-PW)-b] > 0.",
      "B) The IV estimator may be more biased than OLS if instruments are correlated with e_PG and e_PW.",
      "C) The within estimator is always less biased than OLS when cov(X, e_x) ≠ 0.",
      "D) The Hausman-Taylor estimator is consistent only if the assumptions on endogenous/exogenous variables are correct."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Are-estimates-of-calorie-income-fxelasticities-too-hi_1992_Journal-of-Develo.md"
    ],
    "question_context": "The measurement error component of the bias due to OLS estimation of b in (6) will be upwards provided [k(1-PG-PW)-b] > 0. The multiplicative factor k represents the calories made available on average for each peso spent on food, whereas b represents the marginal change in family calorie availability for every extra peso of total expenditure. Because households purchase more expensive sources of calories at the margin, we are confident that [k(1-PG-PW)-b] > 0 for most households. The same exercise for the CI,TE pair drops the first term in the brackets of the numerator of (6), so that the direction of the bias is negative as in (4).\n\nBecause it is so difficult to find instruments correlated with X but uncorrelated with e_PG and e_PW, the IV estimator may be more biased than the OLS estimator. The Hausman-Taylor (HT) estimator uses additional information concerning a priori assumptions on which included right-hand-side variables will be correlated (endogenous) and uncorrelated (exogenous) with the unobserved effects. If these assumptions are correct, the HT estimator is more efficient than the within estimator. However, if the restrictions are incorrect, the HT estimator is inconsistent.",
    "category": "CORRECT",
    "question_context_used": "The measurement error component of the bias due to OLS estimation of b in (6) will be upwards provided [k(1-PG-PW)-b] > 0. The multiplicative factor k represents the calories made available on average for each peso spent on food, whereas b represents the marginal change in family calorie availability for every extra peso of total expenditure. Because households purchase more expensive sources of calories at the margin, we are confident that [k(1-PG-PW)-b] > 0 for most households. The same exercise for the CI,TE pair drops the first term in the brackets of the numerator of (6), so that the direction of the bias is negative as in (4).\n\nBecause it is so difficult to find instruments correlated with X but uncorrelated with e_PG and e_PW, the IV estimator may be more biased than the OLS estimator. The Hausman-Taylor (HT) estimator uses additional information concerning a priori assumptions on which included right-hand-side variables will be correlated (endogenous) and uncorrelated (exogenous) with the unobserved effects. If these assumptions are correct, the HT estimator is more efficient than the within estimator. However, if the restrictions are incorrect, the HT estimator is inconsistent.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4103",
    "question": "Under the conditions specified in Assumptions 1-3, which of the following statements are true regarding the feasible estimator $\\hat{\\beta}_{Z_{k(n)}}$?",
    "options": [
      "A) The feasible estimator is consistent and asymptotically normal if $r_{X,g} = o(n^{-1/2})$.",
      "B) The feasible estimator always has the same asymptotic distribution as the infeasible estimator, regardless of the rate of $r_{X,g}$.",
      "C) The feasible estimator's asymptotic mean squared error is given by $S^2(\\hat{\\beta}_{Z_{k(n)}})$ when $r_{X,g} = o(n^{-1/2})$.",
      "D) The feasible estimator's bias term $b(\\hat{\\beta}_{Z_{k(n)}})$ is $O_p(r_{X,g})$ under Assumption 3(a)."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Simple-and-reliable-estimators-of-coefficients-of-interest_2020_Journal-of-E.md"
    ],
    "question_context": "The model under consideration is $y_n = X_n \\beta + g_n(W, \\tilde{W}) + \\varepsilon_n$, where $\\beta$ is the parameter of interest. The feasible estimator $\\hat{\\beta}_{Z_{k(n)}}$ projects orthogonally to an auxiliary space spanned by $Z_{k(n)}$, a subset of observables or their transformations. Key conditions include Assumption 3(a), which ensures that $\\left\\|\\frac{1}{n}X' M_{Z_{k(n)}} g_n\\right\\| = O_p(r_{X,g})$, and Theorem 2, which states that $\\sqrt{n} \\left(S^2(\\hat{\\beta}_{Z_{k(n)}})\\right)^{-1/2} e(\\hat{\\beta}_{Z_{k(n)}}) \\rightarrow_d N(0, I_m)$. If $r_{X,g} = o(n^{-1/2})$, then $\\sqrt{n} \\left(AMSE(\\hat{\\beta}_{Z_{k(n)}})\\right)^{-1/2} (\\hat{\\beta}_{Z_{k(n)}} - \\beta) \\rightarrow_d N(0, I_m)$; otherwise, $\\hat{\\beta}_{Z_{k(n)}} - \\beta = O_p(r_{X,g})$.",
    "category": "INCORRECT",
    "question_context_used": "The model under consideration is $y_n = X_n \\beta + g_n(W, \\tilde{W}) + \\varepsilon_n$, where $\\beta$ is the parameter of interest. The feasible estimator $\\hat{\\beta}_{Z_{k(n)}}$ projects orthogonally to an auxiliary space spanned by $Z_{k(n)}$, a subset of observables or their transformations. Key conditions include Assumption 3(a), which ensures that $\\left\\|\\frac{1}{n}X' M_{Z_{k(n)}} g_n\\right\\| = O_p(r_{X,g})$, and Theorem 2, which states that $\\sqrt{n} \\left(S^2(\\hat{\\beta}_{Z_{k(n)}})\\right)^{-1/2} e(\\hat{\\beta}_{Z_{k(n)}}) \\rightarrow_d N(0, I_m)$. If $r_{X,g} = o(n^{-1/2})$, then $\\sqrt{n} \\left(AMSE(\\hat{\\beta}_{Z_{k(n)}})\\right)^{-1/2} (\\hat{\\beta}_{Z_{k(n)}} - \\beta) \\rightarrow_d N(0, I_m)$; otherwise, $\\hat{\\beta}_{Z_{k(n)}} - \\beta = O_p(r_{X,g})$.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3925",
    "question": "According to the revealed preference analysis of stable aggregate matchings, which of the following conditions are necessary and sufficient for NTU rationalizability?",
    "options": [
      "A) The graph 𝔾 contains no regular vertex-minimal cycles",
      "B) The graph 𝔾 contains no two distinct, regular, vertex-minimal cycles that are connected",
      "C) The graph 𝔾 is a forest with no cycles",
      "D) The graph 𝔾 has a connected component that is a pseudotree"
    ],
    "gold_answer": "B",
    "source_files": [
      "On the revealed preference analysis of stable aggregate matchings.md"
    ],
    "question_context": "The paper revisits the results of Echenique et al. (2013) by investigating the testable revealed preference implications of stable aggregate matchings in transferable utility (TU) and nontransferable utility (NTU) settings. For NTU rationalizability, the key condition is that the graph 𝔾 constructed from the matching must not contain two distinct, regular, vertex-minimal cycles that are connected. The graph 𝔾 is defined with vertices representing matched type pairs (m,w) where X(m,w) > 0, and edges connecting pairs sharing the same man or woman type. A cycle is regular if it involves at least two types of men and two types of women. The formal condition is given by Theorem 1(i): A matching ℳ is NTU rationalizable if and only if the graph 𝔾 does not contain two distinct, regular, vertex-minimal cycles that are connected.",
    "category": "CORRECT",
    "question_context_used": "The paper revisits the results of Echenique et al. (2013) by investigating the testable revealed preference implications of stable aggregate matchings in transferable utility (TU) and nontransferable utility (NTU) settings. For NTU rationalizability, the key condition is that the graph 𝔾 constructed from the matching must not contain two distinct, regular, vertex-minimal cycles that are connected. The graph 𝔾 is defined with vertices representing matched type pairs (m,w) where X(m,w) > 0, and edges connecting pairs sharing the same man or woman type. A cycle is regular if it involves at least two types of men and two types of women. The formal condition is given by Theorem 1(i): A matching ℳ is NTU rationalizable if and only if the graph 𝔾 does not contain two distinct, regular, vertex-minimal cycles that are connected.",
    "gold_answer_parsed": [
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1451",
    "question": "In the context of Markovian mechanisms and continuation mechanisms, which of the following statements are true regarding the efficiency and feasibility of continuation mechanisms?",
    "options": [
      "A) The efficiency of a continuation mechanism at state $V(l,d)$ is independent of the number of agents already asked.",
      "B) A continuation mechanism $G$ is feasible at $V(l,d)$ with respect to a mechanism $G^{\\prime}$ if $V(l,d)$ is reached with positive probability according to $G^{\\prime}$ and $G$ does not ask more agents than available after $V(l,d)$.",
      "C) In an optimal ex-post efficient mechanism, if a feasible continuation mechanism $G(l,d)$ has higher efficiency than another continuation mechanism $G^{\\prime}(l^{\\prime},d^{\\prime})$ used in $G^{*}$, the mechanism $G^{*}$ must always continue at $V(l,d)$.",
      "D) The efficiency of a continuation mechanism $G(l,d)$ is strictly greater than that of $G(l^{\\prime},d)$ if $l > l^{\\prime}$ and $|e(G(l^{\\prime},d))| \\neq \\infty$."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Optimal-voting-schemes-with-costly-information-ac_2009_Journal-of-Economic-T.md"
    ],
    "question_context": "Key formula: The efficiency of a continuation mechanism $G$ at state $V(l,d)$ is defined as $$e\\big(G(l,d)\\big)=\\left\\{\\begin{array}{l l}{\\frac{\\Delta W(G(l,d))}{|\\Delta I C(G(l,d))|}}&{\\mathrm{if}\\Delta W(G(l,d))\\Delta I C(G(l,d))<0,}\\ {\\infty}&{\\mathrm{if}\\Delta W(G(l,d)),\\Delta I C(G(l,d))\\geqslant0,}\\ {-\\infty}&{\\mathrm{if}\\Delta W(G(l,d)),\\Delta I C(G(l,d))\\leqslant0.}\\end{array}\\right.$$ A continuation mechanism $G$ is feasible at $V(l,d)$ with respect to a mechanism $G^{\\prime}$ if (i) $p(V(l,d):G^{\\prime})>0$, and (ii) if $p(V^{\\prime}:G)>0$ then $l(V^{\\prime})\\leqslant K-l$. Lemma 5 states that for a mechanism $G$ ex-post efficient at $V$ if $d(V)=d$, and feasible at $V(l,d)$ and $V(l^{\\prime},d)$, if $l>l^{\\prime}$, then $e\\big(G(l,d)\\big)\\leqslant e\\big(G(l^{\\prime},d)\\big)$, with strict inequality if $|e(G(l^{\\prime},d))|\\neq\\infty$.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The efficiency of a continuation mechanism $G$ at state $V(l,d)$ is defined as $$e\\big(G(l,d)\\big)=\\left\\{\\begin{array}{l l}{\\frac{\\Delta W(G(l,d))}{|\\Delta I C(G(l,d))|}}&{\\mathrm{if}\\Delta W(G(l,d))\\Delta I C(G(l,d))<0,}\\ {\\infty}&{\\mathrm{if}\\Delta W(G(l,d)),\\Delta I C(G(l,d))\\geqslant0,}\\ {-\\infty}&{\\mathrm{if}\\Delta W(G(l,d)),\\Delta I C(G(l,d))\\leqslant0.}\\end{array}\\right.$$ A continuation mechanism $G$ is feasible at $V(l,d)$ with respect to a mechanism $G^{\\prime}$ if (i) $p(V(l,d):G^{\\prime})>0$, and (ii) if $p(V^{\\prime}:G)>0$ then $l(V^{\\prime})\\leqslant K-l$. Lemma 5 states that for a mechanism $G$ ex-post efficient at $V$ if $d(V)=d$, and feasible at $V(l,d)$ and $V(l^{\\prime},d)$, if $l>l^{\\prime}$, then $e\\big(G(l,d)\\big)\\leqslant e\\big(G(l^{\\prime},d)\\big)$, with strict inequality if $|e(G(l^{\\prime},d))|\\neq\\infty$.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2452",
    "question": "In the proof of Theorem 2, which of the following conditions are essential for ensuring that the cutoff times in $R P^{q}$ converge to the expiration dates in $P S^{*}$ as $q \\to \\infty$?",
    "options": [
      "A) The events $E_{i}^{q}(\\pi)$ hold for all $\\pi$ with $m_{\\pi}^{\\infty} > 0$ and $i \\in \\{1, \\ldots, 6\\}$",
      "B) The weak law of large numbers guarantees $\\Pr[\\overline{E_{i}^{q}(\\pi)}] < \\varepsilon$ for all $i$ and $\\pi$",
      "C) The average rates of consumption in $R P^{q}$ are close to those in $P S^{*}$ during relevant time intervals",
      "D) The expiration dates $t^{*}(v)$ are uniformly bounded away from zero"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Che-ASYMPTOTICEQUIVALENCEPROBABILISTIC-2010.md"
    ],
    "question_context": "The proof of Theorem 2 relies on Lemma 1, which shows that the cutoff times of $R P^{q}$ become arbitrarily close to the expiration dates of $P S^{*}$ as $q \\to \\infty$ when the events $E_{i}^{q}(\\pi)$ hold for every $\\pi$ and $i \\in \\{1, \\ldots, 6\\}$. The proof utilizes the fact that the average rates of consumption of each object type in $R P^{q}$ are close to those under $P S^{*}$ during relevant time intervals. Key formula: $2L^{4\\bar{v}^{*}}\\varepsilon < \\min\\biggr\\{\\min_{v \\in \\{1, \\ldots, \\bar{v}^{*}\\}}\\{t^{*}(v) - t^{*}(v-1)\\}, K\\biggr\\}$. The weak law of large numbers is used to show that the probability of the events $E_{i}^{q}(\\pi)$ approaches 1 as $q \\to \\infty$, ensuring the convergence of the cutoff times.",
    "category": "INCORRECT",
    "question_context_used": "The proof of Theorem 2 relies on Lemma 1, which shows that the cutoff times of $R P^{q}$ become arbitrarily close to the expiration dates of $P S^{*}$ as $q \\to \\infty$ when the events $E_{i}^{q}(\\pi)$ hold for every $\\pi$ and $i \\in \\{1, \\ldots, 6\\}$. The proof utilizes the fact that the average rates of consumption of each object type in $R P^{q}$ are close to those under $P S^{*}$ during relevant time intervals. Key formula: $2L^{4\\bar{v}^{*}}\\varepsilon < \\min\\biggr\\{\\min_{v \\in \\{1, \\ldots, \\bar{v}^{*}\\}}\\{t^{*}(v) - t^{*}(v-1)\\}, K\\biggr\\}$. The weak law of large numbers is used to show that the probability of the events $E_{i}^{q}(\\pi)$ approaches 1 as $q \\to \\infty$, ensuring the convergence of the cutoff times.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4055",
    "question": "In the instrumental variable estimation of the relationship between Scots-Irish settlers and contemporary homicide rates, which of the following statements are correct regarding the identification strategy?",
    "options": [
      "A) The distance to Shallow Ford is a valid instrument because it is uncorrelated with contemporary crime except through historical migration patterns.",
      "B) The total stock of Scots-Irish migrants in 1790 is a valid instrument because it directly affects contemporary crime rates.",
      "C) State fixed effects are included to control for unobservable characteristics that vary at the state level.",
      "D) The F-statistics of the excluded instruments indicate weak instrument problems in the first stage."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Grosjean-HISTORYVIOLENCECULTURE-2014.md"
    ],
    "question_context": "The study investigates the legacy of Scots-Irish settlement on contemporary crime using the following equations:\n\n$$\nm_{c}=\\beta_{0}+\\beta_{1}S I_{c}+\\beta_{2}R_{c}+\\beta_{3}X_{c}+\\beta_{4}S_{c}+u_{c}\n$$\n\n$$\nm_{c}=\\gamma_{0}+\\gamma_{1}S I_{c}+\\gamma_{2}R_{c}+\\gamma_{3}S I_{c}^{*}R_{c}+\\gamma_{4}X_{c}+\\gamma_{5}S_{c}+v_{c}\n$$\n\nwhere $m_{c}$ is the average annual homicide rate, $S I_{c}$ is the proportion of Scots-Irish settlers, and $R_{c}$ is a vector of regional dummies. Identification requires 'as good as random' assignment of the Scots-Irish in 1790. The instrumental variables used are the distance to Shallow Ford and the total stock of Scots-Irish migrants in 1790. The F-statistics for the excluded instruments are well above conventional levels, indicating strong instruments. State fixed effects are included to control for unobservable characteristics at the state level.",
    "category": "CORRECT",
    "question_context_used": "The study investigates the legacy of Scots-Irish settlement on contemporary crime using the following equations:\n\n$$\nm_{c}=\\beta_{0}+\\beta_{1}S I_{c}+\\beta_{2}R_{c}+\\beta_{3}X_{c}+\\beta_{4}S_{c}+u_{c}\n$$\n\n$$\nm_{c}=\\gamma_{0}+\\gamma_{1}S I_{c}+\\gamma_{2}R_{c}+\\gamma_{3}S I_{c}^{*}R_{c}+\\gamma_{4}X_{c}+\\gamma_{5}S_{c}+v_{c}\n$$\n\nwhere $m_{c}$ is the average annual homicide rate, $S I_{c}$ is the proportion of Scots-Irish settlers, and $R_{c}$ is a vector of regional dummies. Identification requires 'as good as random' assignment of the Scots-Irish in 1790. The instrumental variables used are the distance to Shallow Ford and the total stock of Scots-Irish migrants in 1790. The F-statistics for the excluded instruments are well above conventional levels, indicating strong instruments. State fixed effects are included to control for unobservable characteristics at the state level.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3781",
    "question": "In the context of nonparametric testing of Slutsky symmetry in consumer demand systems, which of the following statements are correct?",
    "options": [
      "A) The Slutsky symmetry restriction is a set of nonlinear cross-equation restrictions on the levels and derivatives of consumer demand equations.",
      "B) Nonparametric methods avoid bias due to parametric misspecification and confounding of the test of symmetry with a test of parametric specification.",
      "C) The test statistic for Slutsky symmetry is based on the L2 distance between the Slutsky matrix and its transpose.",
      "D) The imposition of Slutsky symmetry reduces the dimensionality of the problem and increases the speed of convergence of the estimators."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Testing-and-imposing-Slutsky-symmetry-in-nonparametri_2009_Journal-of-Econom.md"
    ],
    "question_context": "The Slutsky symmetry restriction is comprised of a set of nonlinear cross-equation restrictions on the levels and derivatives of expenditure–share equations. In parametric demand system estimation, the complexity of these restrictions and the complexity of share equations seem to go together — in complex demand systems, Slutsky symmetry is similarly complex. Surprisingly, we find that in the nonparametric case, where share equations can be arbitrarily complex, Slutsky symmetry is relatively easy to test and impose. Because the estimated derivatives converge more slowly than estimated levels, and because the symmetry restrictions are linear in the derivatives, Slutsky symmetry behaves asymptotically like a linear restriction on the derivatives. Based on this insight, we provide a new test for symmetry and its asymptotic distribution, show the validity of the bootstrap, and develop an approach to restricted estimation under symmetry.\n\nWe use nonparametric methods in order to avoid bias due to parametric misspecification, and to avoid confounding the test of symmetry with a test of parametric specification. This point can hardly be overemphasized, as we find evidence of both parametric misspecification and confounding of the test in our empirical work. The test statistic is based on unrestricted estimators. It is the sum of L2 distances between all the Slutsky terms which are equal under symmetry. The null hypothesis is equivalent to the condition that the L2-distance of these functions is zero. Using a nonnegative and bounded weighting function a(z) this can be written as $$\\varGamma_{S}=\\mathbb{E}\\Bigl(\\sum_{j<k}(s^{j k}(Z)-s^{k j}(Z))^{2}a(Z)\\Bigr)=0.$$",
    "category": "CORRECT",
    "question_context_used": "The Slutsky symmetry restriction is comprised of a set of nonlinear cross-equation restrictions on the levels and derivatives of expenditure–share equations. In parametric demand system estimation, the complexity of these restrictions and the complexity of share equations seem to go together — in complex demand systems, Slutsky symmetry is similarly complex. Surprisingly, we find that in the nonparametric case, where share equations can be arbitrarily complex, Slutsky symmetry is relatively easy to test and impose. Because the estimated derivatives converge more slowly than estimated levels, and because the symmetry restrictions are linear in the derivatives, Slutsky symmetry behaves asymptotically like a linear restriction on the derivatives. Based on this insight, we provide a new test for symmetry and its asymptotic distribution, show the validity of the bootstrap, and develop an approach to restricted estimation under symmetry.\n\nWe use nonparametric methods in order to avoid bias due to parametric misspecification, and to avoid confounding the test of symmetry with a test of parametric specification. This point can hardly be overemphasized, as we find evidence of both parametric misspecification and confounding of the test in our empirical work. The test statistic is based on unrestricted estimators. It is the sum of L2 distances between all the Slutsky terms which are equal under symmetry. The null hypothesis is equivalent to the condition that the L2-distance of these functions is zero. Using a nonnegative and bounded weighting function a(z) this can be written as $$\\varGamma_{S}=\\mathbb{E}\\Bigl(\\sum_{j<k}(s^{j k}(Z)-s^{k j}(Z))^{2}a(Z)\\Bigr)=0.$$",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2203",
    "question": "Based on the study by Russett and Slemrod (1993), which of the following factors are empirically linked to changes in personal savings behavior?",
    "options": [
      "A) Diminished expectations of nuclear war",
      "B) Increased government defense spending",
      "C) Fluctuations in stock market indices",
      "D) Changes in individual risk perceptions"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Russett-DiminishedExpectationsNuclear-1993.md"
    ],
    "question_context": "The study by Russett and Slemrod (1993) investigates the relationship between geopolitical perceptions and economic behavior at the individual level. Using survey data, the authors demonstrate that diminished expectations of nuclear war lead to significant changes in personal savings rates. The mechanism hinges on shifts in individual risk perceptions, as households adjust their intertemporal consumption choices in response to altered assessments of future security. The empirical framework controls for a variety of confounding factors, isolating the specific effect of nuclear war expectations on savings behavior. Key finding: \\[ \\Delta S = \\beta_0 + \\beta_1 \\cdot \\Delta P_{nuke} + \\epsilon \\], where \\$\\Delta S\\$ is the change in savings rate and \\$\\Delta P_{nuke}\\$ is the change in perceived nuclear war probability.",
    "category": "CORRECT",
    "question_context_used": "The study by Russett and Slemrod (1993) investigates the relationship between geopolitical perceptions and economic behavior at the individual level. Using survey data, the authors demonstrate that diminished expectations of nuclear war lead to significant changes in personal savings rates. The mechanism hinges on shifts in individual risk perceptions, as households adjust their intertemporal consumption choices in response to altered assessments of future security. The empirical framework controls for a variety of confounding factors, isolating the specific effect of nuclear war expectations on savings behavior. Key finding: \\[ \\Delta S = \\beta_0 + \\beta_1 \\cdot \\Delta P_{nuke} + \\epsilon \\], where \\$\\Delta S\\$ is the change in savings rate and \\$\\Delta P_{nuke}\\$ is the change in perceived nuclear war probability.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4522",
    "question": "In the context of multilateral bargaining games with extreme optimism ($\\delta y > 1$), which of the following statements are true regarding agreement-absorption sets and immediate agreement?",
    "options": [
      "A) The set $[1, \\frac{1}{\\delta}]$ remains an agreement-absorption set for all values of $\\delta y$.",
      "B) When $\\delta y > 1$, the unique agreement-absorption set is $\\{S^{*}\\}$.",
      "C) Immediate agreement in all sufficiently long games is guaranteed if $\\delta y \\leq 1$.",
      "D) For almost every $(\\delta, p)$ with $\\delta y > 1$, there exist countably infinite games with immediate agreement and countably infinite games with delay."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Waiting-to-settle--Multilateral-bargaining-with-su_2006_Journal-of-Economic-.md"
    ],
    "question_context": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism...\n\nIn the context of multilateral bargaining games, the agreement-absorption property is defined for a set $X$ such that $X\\subseteq[1,{\\frac{1}{\\delta}}]$, $X\\neq\\emptyset$, and $x\\in X\\Rightarrow f(x)\\in X$. Lemma 2 establishes that for every $(\\delta,p)$ such that $\\delta y\\leqslant1$, $[1,\\frac{1}{\\delta}]$ is an agreement-absorption set. However, when $\\delta y > 1$, Lemma 3 shows that $\\{S^{*}\\}$ is the unique agreement-absorption set. Theorem 2 (Multilateral Immediate Agreement Theorem) states that if $\\delta y \\leq 1$, any sufficiently long bargaining game must end in immediate agreement. Conversely, Theorem 3 demonstrates that for almost every $(\\delta,p)$ with $\\delta y > 1$, there exist countably infinite sets of games with immediate agreement and delay.",
    "category": "CORRECT",
    "question_context_used": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism...\n\nIn the context of multilateral bargaining games, the agreement-absorption property is defined for a set $X$ such that $X\\subseteq[1,{\\frac{1}{\\delta}}]$, $X\\neq\\emptyset$, and $x\\in X\\Rightarrow f(x)\\in X$. Lemma 2 establishes that for every $(\\delta,p)$ such that $\\delta y\\leqslant1$, $[1,\\frac{1}{\\delta}]$ is an agreement-absorption set. However, when $\\delta y > 1$, Lemma 3 shows that $\\{S^{*}\\}$ is the unique agreement-absorption set. Theorem 2 (Multilateral Immediate Agreement Theorem) states that if $\\delta y \\leq 1$, any sufficiently long bargaining game must end in immediate agreement. Conversely, Theorem 3 demonstrates that for almost every $(\\delta,p)$ with $\\delta y > 1$, there exist countably infinite sets of games with immediate agreement and delay.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3136",
    "question": "In the context of the Pacific Salmon Treaty (PST) and its optimal design under two-sided moral hazard, which of the following statements are supported by the analysis?",
    "options": [
      "A) The optimal contract should allocate a larger salmon share to the USA due to its more efficient production function.",
      "B) The existing PST contract is inefficient, and its inefficiency stems from political frictions not modeled in the analysis.",
      "C) Extending the model to include the salmon population as a state variable would invalidate the main finding regarding resource sharing.",
      "D) The dynamic adjustment of sharing rules in the PST is based on historic catch and ecological data."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "On_the_Pacific_Salmon_Treaty.md"
    ],
    "question_context": "The article studies the optimal design of the Pacific Salmon Treaty (PST) under two-sided moral hazard, extending the theory of continuous-time repeated games to include side payments. It identifies two solutions to the optimality equation that characterizes the boundary of the set of equilibrium payoffs, simplifying the calculation of the equilibrium set. The analysis reveals that the existing contract is inefficient and recommends policy changes, such as allocating a larger salmon share to the USA due to its more efficient production function, which could improve welfare by $1.55\\%. The PST's sharing rules are dynamically adjusted based on historic catch and ecological data, and the treaty allows for side-payments between countries to compensate for losses. The model could be extended to include the salmon population as a state variable or to involve more than two countries, though the latter presents theoretical challenges.",
    "category": "INCORRECT",
    "question_context_used": "The article studies the optimal design of the Pacific Salmon Treaty (PST) under two-sided moral hazard, extending the theory of continuous-time repeated games to include side payments. It identifies two solutions to the optimality equation that characterizes the boundary of the set of equilibrium payoffs, simplifying the calculation of the equilibrium set. The analysis reveals that the existing contract is inefficient and recommends policy changes, such as allocating a larger salmon share to the USA due to its more efficient production function, which could improve welfare by $1.55\\%. The PST's sharing rules are dynamically adjusted based on historic catch and ecological data, and the treaty allows for side-payments between countries to compensate for losses. The model could be extended to include the salmon population as a state variable or to involve more than two countries, though the latter presents theoretical challenges.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-429",
    "question": "In the context of the QRLRT and JM tests for H0: β = 0, which of the following statements are supported by the simulation results in Table 2 and Table 3?",
    "options": [
      "A) The QRLRT test maintains size closer to the nominal level compared to the JM test across various values of c.",
      "B) The power of the QRLRT test is uniformly higher than that of the JM test for all configurations of b and ρ.",
      "C) The JM test exhibits superior power when ρ = 0.5 and b = 15, as shown in Table 3.",
      "D) The QRLRT test's conservative size for large c does not significantly impair its power relative to the JM test."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "10-Uniform_Inference_in_Predictive_Regression_Models.md"
    ],
    "question_context": "The simulation study compares the size and power of the sup-bound-based QRLRT test with the JM test under the null hypothesis H0: β = 0. Key configurations include ρ = -0.98, α = 1 - c/n, and β = b/√(n - ρ²). The nominal size is set at 5% with 5000 replications. Table 2 shows that the QRLRT test maintains size closer to the nominal level and achieves uniformly higher power than the JM test, even when conservative for large c. Table 3 further confirms these findings for ρ = -0.5 and ρ = 0.5, with n = 1000 and 500 replications. The QRLRT's power gains are substantial, particularly for larger values of b, without significant size distortion.",
    "category": "INCORRECT",
    "question_context_used": "The simulation study compares the size and power of the sup-bound-based QRLRT test with the JM test under the null hypothesis H0: β = 0. Key configurations include ρ = -0.98, α = 1 - c/n, and β = b/√(n - ρ²). The nominal size is set at 5% with 5000 replications. Table 2 shows that the QRLRT test maintains size closer to the nominal level and achieves uniformly higher power than the JM test, even when conservative for large c. Table 3 further confirms these findings for ρ = -0.5 and ρ = 0.5, with n = 1000 and 500 replications. The QRLRT's power gains are substantial, particularly for larger values of b, without significant size distortion.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4323",
    "question": "In the context of evolutionary game theory with uniform and $2k$ neighbor matching, which of the following statements accurately describe the asymptotic behavior of the steady-state distribution $\\mu(\\varepsilon)$ as $\\varepsilon \\to 0$?",
    "options": [
      "A) The steady-state distribution $\\mu^{u}(\\varepsilon)$ under uniform matching converges to a state where the risk-dominant equilibrium $(A,A)$ is played with probability 1.",
      "B) The steady-state distribution $\\mu^{2k}(\\varepsilon)$ under $2k$ neighbor matching converges to a state where the non-risk-dominant equilibrium $(B,B)$ is played with probability 1.",
      "C) For sufficiently small $\\varepsilon$, the probability of the equilibrium $(B,B)$ under uniform matching is of order $O(\\varepsilon^{N-2[q^{*}(N-1)]+1})$.",
      "D) For sufficiently small $\\varepsilon$, the probability of the equilibrium $(B,B)$ under $2k$ neighbor matching is of order $O(\\varepsilon^{N-2})$ for even $N$ and $O(\\varepsilon^{N-1})$ for odd $N$."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Ellison-LearningLocalInteraction-1993.md"
    ],
    "question_context": "The steady-state distribution reflects the long-run behavior of the process. The evolution of the process is governed by $v_{t+1}=v_{t}P(\\varepsilon)$, where $P(\\varepsilon)$ is the transition matrix. For $\\varepsilon > 0$, each transition matrix $P(\\varepsilon)$ is strictly positive, ensuring a unique steady-state distribution $\\mu(\\varepsilon)$ such that $\\mu(\\varepsilon)=\\mu(\\varepsilon)P(\\varepsilon)$. The steady-state distribution under uniform matching is denoted $\\mu^{u}(\\varepsilon)$, and under $2k$ neighbor matching, $\\mu^{2k}(\\varepsilon)$. The asymptotic behavior of $\\mu(\\varepsilon)$ as $\\varepsilon \\to 0$ is characterized by the following results: $\\lim_{\\varepsilon \\to 0} \\mu_{\\bar{A}}^{u}(\\varepsilon) = 1$, $\\lim_{\\varepsilon \\to 0} \\mu_{\\bar{A}}^{2k}(\\varepsilon) = 1$, $\\mu_{B}^{u}(\\varepsilon) = O(\\varepsilon^{N-2[q^{*}(N-1)]+1})$, and $\\mu_{B}^{2}(\\varepsilon) = O(\\varepsilon^{N-2})$ for $N$ even and $O(\\varepsilon^{N-1})$ for $N$ odd.",
    "category": "INCORRECT",
    "question_context_used": "The steady-state distribution reflects the long-run behavior of the process. The evolution of the process is governed by $v_{t+1}=v_{t}P(\\varepsilon)$, where $P(\\varepsilon)$ is the transition matrix. For $\\varepsilon > 0$, each transition matrix $P(\\varepsilon)$ is strictly positive, ensuring a unique steady-state distribution $\\mu(\\varepsilon)$ such that $\\mu(\\varepsilon)=\\mu(\\varepsilon)P(\\varepsilon)$. The steady-state distribution under uniform matching is denoted $\\mu^{u}(\\varepsilon)$, and under $2k$ neighbor matching, $\\mu^{2k}(\\varepsilon)$. The asymptotic behavior of $\\mu(\\varepsilon)$ as $\\varepsilon \\to 0$ is characterized by the following results: $\\lim_{\\varepsilon \\to 0} \\mu_{\\bar{A}}^{u}(\\varepsilon) = 1$, $\\lim_{\\varepsilon \\to 0} \\mu_{\\bar{A}}^{2k}(\\varepsilon) = 1$, $\\mu_{B}^{u}(\\varepsilon) = O(\\varepsilon^{N-2[q^{*}(N-1)]+1})$, and $\\mu_{B}^{2}(\\varepsilon) = O(\\varepsilon^{N-2})$ for $N$ even and $O(\\varepsilon^{N-1})$ for $N$ odd.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2186",
    "question": "In the context of the model with no annuity markets, which of the following statements are correct regarding its implications for schooling dispersion and credit constraints?",
    "options": [
      "A) The absence of annuity markets increases the incentives to save and accumulate human capital due to higher risk aversion.",
      "B) The model predicts a larger consumption jump at age s compared to the benchmark model with perfect annuity markets.",
      "C) Credit constraints are relatively tighter in countries with lower survival probabilities, typically poorer countries.",
      "D) The model explains a smaller percent of schooling dispersion than the benchmark model due to reduced heterogeneity."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "What-explains-schooling-differences-across-cou_2013_Journal-of-Monetary-Econ.md"
    ],
    "question_context": "The benchmark model assumes perfect annuity markets. This assumption is convenient because it simplifies the solution of the model and avoids heterogeneity. In the absence of annuity markets, unintended bequests are possible and heterogeneity across individuals arises. A way to keep the model tractable is to assume that there are no unintended bequests either because upon death net assets are ‘‘thrown to the sea’’ or retained by international financial institutions. We assume this is the case.16  \n\nWe find that a model with no annuity markets would actually explain an even larger percent of the schooling dispersion across countries. In particular, the risk of dying reduces the incentives to save and to accumulate human capital. Annuity markets serves to offset this effect by lowering the cost of future consumption and increasing the benefits of saving. The lack of annuity markets worsens the credit frictions and therefore enhances the role of family resources in determining schooling outcomes. In terms of the model’s equations, if annuity markets are missing then $q(a)=e^{-r a}$ and the optimal saving/consumption decisions implies the following jump of marginal utility at age s:  \n\n$$  \n\\frac{u^{\\prime}(c^{S}(s))}{u^{\\prime}(c^{W}(s))}=G=\\frac{f}{\\phi(f)}\\frac{1}{\\pi(F)}e^{-(r-\\rho)F}>1.  \n$$  \n\nThe presence of $\\pi(F)<1$ in this equation means that the model with no annuities predicts a larger consumption jump relative to the model with perfect annuity markets. Moreover, the model predicts that credit constraints are relatively tighter in countries with lower $\\pi(F)$ , typically poorer countries, and as a result the model could explain a greater percent of schooling dispersion than the benchmark. The reason is that individuals discount the future more heavily when annuity markets are missing, making investments in education less valuable. A calibration exercise using the same parameters as in Table 1 reveals that the model with no annuities explains $97\\%$ of the schooling and $81\\%$ of the returns to schooling dispersion. Since for richer countries $G$ is very similar in the model with or without annuities, the calibrated parameters in Table 1 continue to fit well the targets.",
    "category": "CORRECT",
    "question_context_used": "The benchmark model assumes perfect annuity markets. This assumption is convenient because it simplifies the solution of the model and avoids heterogeneity. In the absence of annuity markets, unintended bequests are possible and heterogeneity across individuals arises. A way to keep the model tractable is to assume that there are no unintended bequests either because upon death net assets are ‘‘thrown to the sea’’ or retained by international financial institutions. We assume this is the case.16  \n\nWe find that a model with no annuity markets would actually explain an even larger percent of the schooling dispersion across countries. In particular, the risk of dying reduces the incentives to save and to accumulate human capital. Annuity markets serves to offset this effect by lowering the cost of future consumption and increasing the benefits of saving. The lack of annuity markets worsens the credit frictions and therefore enhances the role of family resources in determining schooling outcomes. In terms of the model’s equations, if annuity markets are missing then $q(a)=e^{-r a}$ and the optimal saving/consumption decisions implies the following jump of marginal utility at age s:  \n\n$$  \n\\frac{u^{\\prime}(c^{S}(s))}{u^{\\prime}(c^{W}(s))}=G=\\frac{f}{\\phi(f)}\\frac{1}{\\pi(F)}e^{-(r-\\rho)F}>1.  \n$$  \n\nThe presence of $\\pi(F)<1$ in this equation means that the model with no annuities predicts a larger consumption jump relative to the model with perfect annuity markets. Moreover, the model predicts that credit constraints are relatively tighter in countries with lower $\\pi(F)$ , typically poorer countries, and as a result the model could explain a greater percent of schooling dispersion than the benchmark. The reason is that individuals discount the future more heavily when annuity markets are missing, making investments in education less valuable. A calibration exercise using the same parameters as in Table 1 reveals that the model with no annuities explains $97\\%$ of the schooling and $81\\%$ of the returns to schooling dispersion. Since for richer countries $G$ is very similar in the model with or without annuities, the calibrated parameters in Table 1 continue to fit well the targets.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-57",
    "question": "In the context of time-varying parameter structural vector autoregressive models (TVP-SVARs), which of the following strategies are employed to address computational challenges in large-dimensional systems?",
    "options": [
      "A) Imposing a reduced rank structure on the state equation covariance matrix",
      "B) Using the Kalman filter and smoother for all posterior computations",
      "C) Employing parameter expansions and judicious priors for nonstandard supports",
      "D) Utilizing the precision sampler of Chan and Jeliazkov (2009) for state estimation"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Reducing-the-state-space-dimension-in-a-large-TVP_2020_Journal-of-Econometri.md"
    ],
    "question_context": "The paper proposes a new approach to estimating high dimensional time varying parameter structural vector autoregressive models (TVP-SVARs) by exploiting the near-singularity of the state equation covariance matrix. Key formula: $d\\left(Q_{r_{\\alpha}},Q_{r_{\\alpha}^{*}}\\right)=1-\\frac{t r Q_{r_{\\alpha}}Q_{r_{\\alpha}^{*}}}{\\left\\|Q_{r_{\\alpha}}\\right\\|_{F}\\left\\|Q_{r_{\\alpha}^{*}}\\right\\|_{F}}$ measures the distance between covariance matrices of different ranks. The authors employ multiple strategies to mitigate computational issues: (1) a reduced rank restriction on the state covariance matrix, (2) direct estimation of the structural form to remove sampling steps, (3) generalization of the non-centered specification to matrix form, and (4) use of the precision sampler instead of Kalman filter/smoother. These approaches collectively enable estimation of larger TVP-SVAR systems while maintaining formal inferential capabilities for impulse responses and variance decompositions.",
    "category": "CORRECT",
    "question_context_used": "The paper proposes a new approach to estimating high dimensional time varying parameter structural vector autoregressive models (TVP-SVARs) by exploiting the near-singularity of the state equation covariance matrix. Key formula: $d\\left(Q_{r_{\\alpha}},Q_{r_{\\alpha}^{*}}\\right)=1-\\frac{t r Q_{r_{\\alpha}}Q_{r_{\\alpha}^{*}}}{\\left\\|Q_{r_{\\alpha}}\\right\\|_{F}\\left\\|Q_{r_{\\alpha}^{*}}\\right\\|_{F}}$ measures the distance between covariance matrices of different ranks. The authors employ multiple strategies to mitigate computational issues: (1) a reduced rank restriction on the state covariance matrix, (2) direct estimation of the structural form to remove sampling steps, (3) generalization of the non-centered specification to matrix form, and (4) use of the precision sampler instead of Kalman filter/smoother. These approaches collectively enable estimation of larger TVP-SVAR systems while maintaining formal inferential capabilities for impulse responses and variance decompositions.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4694",
    "question": "In the context of estimating the demand model for journal portfolios, which of the following statements are correct regarding the computational challenges and solutions?",
    "options": [
      "A) The simulation approach becomes cumbersome when library portfolios contain a large number of journals.",
      "B) The marginal probability of choosing a given portfolio is computed by summing over all possible rankings of the journals in the portfolio.",
      "C) The estimator performs well in Monte Carlo experiments even with a single draw (L=1) from the set of possible rankings.",
      "D) The log likelihood function is maximized without any computational approximations."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Nevo-AcademicJournalPricing-2005.md"
    ],
    "question_context": "The estimation involves maximizing the probability of a library choosing its observed portfolio, with the log likelihood given by complex formulas involving parameters $\beta,\\alpha$, matrix A, and distributions $F({\\bf{v}},{\\bf{\\pmb{\\varepsilon}}})$. Computation of this likelihood is difficult, leading to exploration of simulation approaches. For large portfolios, a more efficient alternative is used, where the marginal probability of choosing a given portfolio is the sum of the probabilities of choosing the portfolio given a particular ranking, summing over all possible rankings. The probabilities $\\Pi_{\\mathcal{P}}(k)$ are given by standard discrete choice formulas. The summation is over all the $(\\#{\\mathcal{P}})!$ permutations of the elements in the observed portfolio, and for large portfolios, computing this probability might not be computationally feasible. The estimator $\\hat{\\Pi}({\\bf X},{\\bf A}_{i},B_{i},{\\bf\\uptheta})$ is computed by simulation, drawing $L$ rankings from the set of possible rankings.",
    "category": "INCORRECT",
    "question_context_used": "The estimation involves maximizing the probability of a library choosing its observed portfolio, with the log likelihood given by complex formulas involving parameters $\beta,\\alpha$, matrix A, and distributions $F({\\bf{v}},{\\bf{\\pmb{\\varepsilon}}})$. Computation of this likelihood is difficult, leading to exploration of simulation approaches. For large portfolios, a more efficient alternative is used, where the marginal probability of choosing a given portfolio is the sum of the probabilities of choosing the portfolio given a particular ranking, summing over all possible rankings. The probabilities $\\Pi_{\\mathcal{P}}(k)$ are given by standard discrete choice formulas. The summation is over all the $(\\#{\\mathcal{P}})!$ permutations of the elements in the observed portfolio, and for large portfolios, computing this probability might not be computationally feasible. The estimator $\\hat{\\Pi}({\\bf X},{\\bf A}_{i},B_{i},{\\bf\\uptheta})$ is computed by simulation, drawing $L$ rankings from the set of possible rankings.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-511",
    "question": "In the context of a monopolistic exporting firm facing exchange rate uncertainty, which of the following statements are true regarding the firm's optimal pricing and production decisions under different invoicing strategies?",
    "options": [
      "A) When invoicing in the importer's currency, the firm's total output decision is independent of the distribution of the random exchange rate component θ.",
      "B) Invoicing in the seller's currency allows the firm to precommit to both domestic and foreign prices at time t₁ without facing residual demand uncertainty.",
      "C) The No-Regret Condition ensures that breaching the contract with foreign buyers is not profitable when invoicing in the seller's currency.",
      "D) Under invoicing in the importer's currency, the firm equates marginal revenue from domestic sales with expected marginal revenue from exports for each realization of α."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Pricing of Exports and Exchange Rate Uncertainty.md"
    ],
    "question_context": "The firm's total revenue in home currency when invoicing in the importer's currency is given by: \n\n$$(α+\\widetilde{θ})R^{*}(h^{*}(p^{*}(α)))+R(h(p(α))).$$\n\nThe firm's optimization problem under this strategy is to maximize expected profit subject to the constraint that total output equals the sum of domestic and foreign sales. The necessary conditions for optimality involve equating marginal cost with expected marginal revenue from domestic sales and ensuring that marginal revenue from domestic sales equals expected marginal revenue from exports for each realization of α. \n\nWhen invoicing in the seller's currency, the firm faces uncertainty in the quantity demanded by importers, as it depends on the realization of θ. The No-Regret Condition, \n\n$$R^{\\prime}(\\hat{Q}-\\hat{q}^{*}(α+θ))≤\\hat{p}^{*}(α+θ)\\qquad∀θ,$$\n\nguarantees that breaching the contract with foreign buyers is not profitable. The firm's optimization problem in this case accounts for the impact of price on the quantity of exports demanded after θ is realized, leading to different optimal output and pricing rules compared to invoicing in the importer's currency.",
    "category": "INCORRECT",
    "question_context_used": "The firm's total revenue in home currency when invoicing in the importer's currency is given by: \n\n$$(α+\\widetilde{θ})R^{*}(h^{*}(p^{*}(α)))+R(h(p(α))).$$\n\nThe firm's optimization problem under this strategy is to maximize expected profit subject to the constraint that total output equals the sum of domestic and foreign sales. The necessary conditions for optimality involve equating marginal cost with expected marginal revenue from domestic sales and ensuring that marginal revenue from domestic sales equals expected marginal revenue from exports for each realization of α. \n\nWhen invoicing in the seller's currency, the firm faces uncertainty in the quantity demanded by importers, as it depends on the realization of θ. The No-Regret Condition, \n\n$$R^{\\prime}(\\hat{Q}-\\hat{q}^{*}(α+θ))≤\\hat{p}^{*}(α+θ)\\qquad∀θ,$$\n\nguarantees that breaching the contract with foreign buyers is not profitable. The firm's optimization problem in this case accounts for the impact of price on the quantity of exports demanded after θ is realized, leading to different optimal output and pricing rules compared to invoicing in the importer's currency.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-272",
    "question": "Under the conditions of Proposition B.2, which of the following statements are equivalent to the absolute continuity of measure Q with respect to measure ̃Q?",
    "options": [
      "A) Q approaches ̃Q, Q-a.s.",
      "B) lim_{t→∞} (d̃Q_t/dQ_t) > 0, Q-a.s.",
      "C) Q is locally absolutely continuous with respect to ̃Q.",
      "D) lim_{t→∞} (d̃Q_t/dQ_t) < ∞, Q-a.s."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Sandroni-MarketsFavorAgents-2000.md"
    ],
    "question_context": "Proposition B.2 states that under the assumption Q loc ≪ ̃Q, the following conditions are equivalent: 1. Q is absolutely continuous with respect to ̃Q, 2. Q approaches ̃Q, Q-a.s., and 3. ∞ > lim_{t→∞} (d̃Q_t/dQ_t) > 0, Q-a.s. The measure Q approaches ̃Q on a path s ∈ T^∞ if ||Q_{s_t} - ̃Q_{s_t}|| → 0 as t → ∞. Absolute continuity (Q ≪ ̃Q) requires that every event A ∈ 𝔉 with ̃Q(A) = 0 also satisfies Q(A) = 0, which is a stronger condition than local absolute continuity.",
    "category": "INCORRECT",
    "question_context_used": "Proposition B.2 states that under the assumption Q loc ≪ ̃Q, the following conditions are equivalent: 1. Q is absolutely continuous with respect to ̃Q, 2. Q approaches ̃Q, Q-a.s., and 3. ∞ > lim_{t→∞} (d̃Q_t/dQ_t) > 0, Q-a.s. The measure Q approaches ̃Q on a path s ∈ T^∞ if ||Q_{s_t} - ̃Q_{s_t}|| → 0 as t → ∞. Absolute continuity (Q ≪ ̃Q) requires that every event A ∈ 𝔉 with ̃Q(A) = 0 also satisfies Q(A) = 0, which is a stronger condition than local absolute continuity.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-144",
    "question": "In the context of small-area estimation for farmland values, which of the following statements are true regarding the model proposed by Pfeffermann and Barnard?",
    "options": [
      "A) The model accounts for both state effects and nested county effects, which are represented as random components.",
      "B) The optimal predictors coincide with the conditional expectations under normality assumptions but do not require normality for BLUP properties.",
      "C) The variance components are estimated using a design-based approach to ensure consistency under traditional probability sampling theory.",
      "D) The model assumes that the farmers' reports are free of response bias, distributing randomly around the true county mean."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "PFEFFERMANN和BARNARD - 1991 - Some new estimators for small-area means with application to the assessment of farmland values.md"
    ],
    "question_context": "The model proposed by Pfeffermann and Barnard for small-area estimation of farmland values is defined as: \n\n$$ Y_{s c i}=\\theta_{s c}+\\varepsilon_{s c i};\\theta_{s c}={\\bf x}_{s c}^{\\prime}{\\bf\\bf\\vec{\\theta}}+\\alpha_{s}+\\gamma_{s c}, $$\n\nwhere $\\{\\varepsilon_{s c i}\\}$ are independent errors with zero mean and variance $\\sigma_{e}^{2}$, $\\{\\alpha_{s}\\}$ represent random state effects with zero mean and variance $\\sigma_{b}^{2}$, and $\\{\\gamma_{s c}\\}$ are random county effects nested within the state effects with zero mean and variance $\\sigma_{w}^{2}$. The model assumes that the three random components are mutually independent. The BLUP property does not require that the distribution of the random effects and the residual error terms be normal, although the predictors coincide with the optimal conditional expectations of the county and state means (given the sample data) obtained under normality assumptions. The model also assumes that the land values reported by farmers residing in the same county distribute randomly around the true county mean, which is a reasonable assumption provided that the farmers’ reports are free of response bias.",
    "category": "CORRECT",
    "question_context_used": "The model proposed by Pfeffermann and Barnard for small-area estimation of farmland values is defined as: \n\n$$ Y_{s c i}=\\theta_{s c}+\\varepsilon_{s c i};\\theta_{s c}={\\bf x}_{s c}^{\\prime}{\\bf\\bf\\vec{\\theta}}+\\alpha_{s}+\\gamma_{s c}, $$\n\nwhere $\\{\\varepsilon_{s c i}\\}$ are independent errors with zero mean and variance $\\sigma_{e}^{2}$, $\\{\\alpha_{s}\\}$ represent random state effects with zero mean and variance $\\sigma_{b}^{2}$, and $\\{\\gamma_{s c}\\}$ are random county effects nested within the state effects with zero mean and variance $\\sigma_{w}^{2}$. The model assumes that the three random components are mutually independent. The BLUP property does not require that the distribution of the random effects and the residual error terms be normal, although the predictors coincide with the optimal conditional expectations of the county and state means (given the sample data) obtained under normality assumptions. The model also assumes that the land values reported by farmers residing in the same county distribute randomly around the true county mean, which is a reasonable assumption provided that the farmers’ reports are free of response bias.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-208",
    "question": "In the two-country model with shopping frictions, which of the following statements are true regarding the impact of demand shocks on the real exchange rate (RER) and relative consumption?",
    "options": [
      "A) A positive demand shock in the home country leads to an appreciation of the RER.",
      "B) Demand shocks in either country increase measured TFP in both countries due to increased search effort.",
      "C) The real exchange rate and the ratio of home to foreign consumption are perfectly negatively correlated under demand shocks alone.",
      "D) With productivity shocks alone, the real exchange rate and relative consumption are perfectly negatively correlated."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Bai-DemandShocksOpen-2015.md"
    ],
    "question_context": "The model extends the shopping framework to a two-country, two-good world with search frictions. Households exert effort $d$ to overcome search frictions, and the aggregate amount of fruits sold is given by a Cobb-Douglas matching function $\\breve{Y^{i}}={A\\breve{(}D^{i}\\rbrack}^{\\alpha}(T^{i})^{1-\\alpha}z^{i}\\breve{f}(n^{i})$. The real exchange rate (RER) is defined as ${\\bar{R E R}}=P^{*}/P$, and its deviation is given by $\\widehat{R E R}=-(2\\mu-1)^{2}\\check{(}1-\\alpha)\\big(\\hat{\\boldsymbol{\\theta}}-\\hat{\\boldsymbol{\\theta}}^{*}\\big)+ (2\\mu-1)(\\hat{z}-\\hat{z}^{*})$. Key findings include: (1) With productivity shocks alone, RER and relative consumption are perfectly positively correlated; (2) With demand shocks alone, RER and relative consumption are perfectly negatively correlated, and relative TFP and relative consumption are positively correlated iff $\\alpha>0$.",
    "category": "INCORRECT",
    "question_context_used": "The model extends the shopping framework to a two-country, two-good world with search frictions. Households exert effort $d$ to overcome search frictions, and the aggregate amount of fruits sold is given by a Cobb-Douglas matching function $\\breve{Y^{i}}={A\\breve{(}D^{i}\\rbrack}^{\\alpha}(T^{i})^{1-\\alpha}z^{i}\\breve{f}(n^{i})$. The real exchange rate (RER) is defined as ${\\bar{R E R}}=P^{*}/P$, and its deviation is given by $\\widehat{R E R}=-(2\\mu-1)^{2}\\check{(}1-\\alpha)\\big(\\hat{\\boldsymbol{\\theta}}-\\hat{\\boldsymbol{\\theta}}^{*}\\big)+ (2\\mu-1)(\\hat{z}-\\hat{z}^{*})$. Key findings include: (1) With productivity shocks alone, RER and relative consumption are perfectly positively correlated; (2) With demand shocks alone, RER and relative consumption are perfectly negatively correlated, and relative TFP and relative consumption are positively correlated iff $\\alpha>0$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1641",
    "question": "In the context of sampling designs for short panel data, which of the following methodological considerations are crucial for ensuring unbiased parameter estimates?",
    "options": [
      "A) Stratification based on time-invariant individual characteristics",
      "B) Inclusion of all possible lagged dependent variables",
      "C) Proper handling of initial conditions in dynamic models",
      "D) Weighting schemes to account for attrition bias"
    ],
    "gold_answer": "C, D",
    "source_files": [
      "Stavola-SamplingDesignsShort-1986.md"
    ],
    "question_context": "The paper addresses the challenges of sampling designs in short panel data, where the limited time dimension complicates inference. A key issue is the treatment of initial conditions in dynamic models, as their misspecification can lead to substantial biases. The author emphasizes that the joint distribution of initial observations and individual effects must be modeled explicitly. Another critical aspect is attrition bias, which requires weighting adjustments or imputation methods. The generalized method of moments (GMM) framework is often employed to handle these issues, with moment conditions derived from the model's structural assumptions.",
    "category": "CORRECT",
    "question_context_used": "The paper addresses the challenges of sampling designs in short panel data, where the limited time dimension complicates inference. A key issue is the treatment of initial conditions in dynamic models, as their misspecification can lead to substantial biases. The author emphasizes that the joint distribution of initial observations and individual effects must be modeled explicitly. Another critical aspect is attrition bias, which requires weighting adjustments or imputation methods. The generalized method of moments (GMM) framework is often employed to handle these issues, with moment conditions derived from the model's structural assumptions.",
    "gold_answer_parsed": [
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"C\", \"D\"]\n}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-716",
    "question": "According to Shapiro and Slemrod's study on consumer response to tax rebates, which of the following factors are identified as potential explanations for why only 21.8% of recipients reported increasing spending?",
    "options": [
      "A) Ricardian equivalence leading households to view rebates as non-incremental to their well-being",
      "B) Liquidity constraints preventing immediate expenditure of the rebate",
      "C) The rebate being perceived as a signal of a large increase in permanent after-tax income",
      "D) High-frequency reoptimization of spending versus saving decisions by all households"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Shapiro-ConsumerResponseTax-2003.md"
    ],
    "question_context": "The study by Shapiro and Slemrod (2003) investigates household responses to tax rebates, finding that only 21.8% of recipients planned to mostly increase spending. Key formula: The rebate amount was calculated as 5% of taxable income up to $12,000 for married couples, yielding a maximum rebate of $600. The authors explore several hypotheses for the low spending rate, including Ricardian equivalence (where households anticipate future tax increases to offset the rebate), liquidity constraints (where households lack immediate funds to spend), and perceptions of permanent income changes. The survey methodology allowed direct testing of these hypotheses by asking respondents about their spending plans and expectations regarding future tax cuts and government spending.",
    "category": "INCORRECT",
    "question_context_used": "The study by Shapiro and Slemrod (2003) investigates household responses to tax rebates, finding that only 21.8% of recipients planned to mostly increase spending. Key formula: The rebate amount was calculated as 5% of taxable income up to $12,000 for married couples, yielding a maximum rebate of $600. The authors explore several hypotheses for the low spending rate, including Ricardian equivalence (where households anticipate future tax increases to offset the rebate), liquidity constraints (where households lack immediate funds to spend), and perceptions of permanent income changes. The survey methodology allowed direct testing of these hypotheses by asking respondents about their spending plans and expectations regarding future tax cuts and government spending.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3937",
    "question": "Which of the following statements are correct regarding the decomposition of inequality indexes as discussed in the text?",
    "options": [
      "A) The coefficient of variation squared for family income includes cross-component correlation terms multiplied by their respective shares and coefficients of variation.",
      "B) The Gini coefficient decomposition term for a specific income source directly measures the marginal effect of removing that source on overall inequality.",
      "C) The Gini correlation for an income source depends on the covariance between that source and the rank of total income.",
      "D) If wives' earnings are equal across all families, their Gini contribution to family income inequality is zero, despite potentially reducing inequality."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Cancian-AssessingEffectsWives-1998.md"
    ],
    "question_context": "The coefficient of variation is defined as the standard deviation of income divided by the arithmetic mean. For three income sources, the square of the coefficient of variation $C V$ can be expressed as:\n\n$$\n\\begin{array}{r l}&{C V_{f}^{2}=S_{h}^{2}C V_{h}^{2}+S_{w}^{2}C V_{w}^{2}+S_{r}^{2}C V_{r}^{2}}\\ &{\\qquad+2\\uprho_{h w}S_{h}S_{w}C V_{h}C V_{w}+2\\uprho_{h r}S_{h}S_{r}C V_{h}C V_{r}}\\ &{\\qquad+2\\uprho_{w r}S_{w}S_{r}C V_{w}C V_{r}}\\end{array}\n$$\n\nThe Gini coefficient of family income $G_{f}$ for three income sources can be expressed as:\n\n$$\nG_{f}=S_{h}R_{h}G_{h}+S_{w}R_{w}G_{w}+S_{r}R_{r}G_{r}\n$$\n\nwhere $G_{k}$ is the Gini coefficient of the kth income source, $S_{k}$ is the share of total income from the kth source, and $R_{k}$ is the Gini correlation between income source $k$ and total income. The Gini correlation for source $k$ is defined as the covariance between income from source $k$ and the rank of total income, divided by the covariance between income from source $k$ and the rank of source $k$.",
    "category": "CORRECT",
    "question_context_used": "The coefficient of variation is defined as the standard deviation of income divided by the arithmetic mean. For three income sources, the square of the coefficient of variation $C V$ can be expressed as:\n\n$$\n\\begin{array}{r l}&{C V_{f}^{2}=S_{h}^{2}C V_{h}^{2}+S_{w}^{2}C V_{w}^{2}+S_{r}^{2}C V_{r}^{2}}\\ &{\\qquad+2\\uprho_{h w}S_{h}S_{w}C V_{h}C V_{w}+2\\uprho_{h r}S_{h}S_{r}C V_{h}C V_{r}}\\ &{\\qquad+2\\uprho_{w r}S_{w}S_{r}C V_{w}C V_{r}}\\end{array}\n$$\n\nThe Gini coefficient of family income $G_{f}$ for three income sources can be expressed as:\n\n$$\nG_{f}=S_{h}R_{h}G_{h}+S_{w}R_{w}G_{w}+S_{r}R_{r}G_{r}\n$$\n\nwhere $G_{k}$ is the Gini coefficient of the kth income source, $S_{k}$ is the share of total income from the kth source, and $R_{k}$ is the Gini correlation between income source $k$ and total income. The Gini correlation for source $k$ is defined as the covariance between income from source $k$ and the rank of total income, divided by the covariance between income from source $k$ and the rank of source $k$.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2364",
    "question": "In the context of household labor supply in urban Bolivia, which of the following statements are supported by the empirical findings of Pradhan and van Soest (1997)?",
    "options": [
      "A) A 10% fall in male wages leads to a significant increase in female labor supply hours, with a cross-labor supply elasticity of -0.25.",
      "B) Nonmonetary returns in the formal sector are found to be insignificant for both males and females, aligning with Magnac's (1991) findings in Columbia.",
      "C) The model underpredicts male nonparticipation rates, suggesting potential omitted factors like fixed costs of working or part-time job constraints.",
      "D) A 10% drop in formal sector wages causes a 5.5% increase in informal sector participation, indicating sectoral wage sensitivity."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Pradhan-HouseholdLaborSupply-1997.md"
    ],
    "question_context": "The study analyzes labor supply behavior in Bolivian urban households using a neoclassical model combining features from Ransom (1987) and Magnac (1991). Key findings include substantial cross-wage elasticities, with female labor supply increasing by 2.5% (elasticity: -0.25) when male wages fall by 10%. The model underpredicts male nonparticipation, suggesting unaccounted fixed costs or job constraints. Nonmonetary returns in the formal sector are positive on average ($\\text{Nonmonetary Returns}_{formal} > 0$), contradicting Magnac's null results. A 10% formal wage drop increases informal sector participation by 5.5%, highlighting sectoral wage sensitivity. The quadratic utility function and smooth simulated maximum likelihood estimation are noted as areas for potential extension.",
    "category": "CORRECT",
    "question_context_used": "The study analyzes labor supply behavior in Bolivian urban households using a neoclassical model combining features from Ransom (1987) and Magnac (1991). Key findings include substantial cross-wage elasticities, with female labor supply increasing by 2.5% (elasticity: -0.25) when male wages fall by 10%. The model underpredicts male nonparticipation, suggesting unaccounted fixed costs or job constraints. Nonmonetary returns in the formal sector are positive on average ($\\text{Nonmonetary Returns}_{formal} > 0$), contradicting Magnac's null results. A 10% formal wage drop increases informal sector participation by 5.5%, highlighting sectoral wage sensitivity. The quadratic utility function and smooth simulated maximum likelihood estimation are noted as areas for potential extension.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1663",
    "question": "In the context of international trade between asymmetric countries with vertical product differentiation, which of the following conditions are necessary for a 'leapfrogging' equilibrium to occur?",
    "options": [
      "A) The laggard country must have access to the same technology as the leading country.",
      "B) The initial quality gap between the countries must not be too large.",
      "C) The laggard country must have a larger domestic market than the leading country.",
      "D) The risk dominance criterion must favor the leapfrogging equilibrium."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "On the Persistence of Leadership or Leapfrogging in International Trade.md"
    ],
    "question_context": "The paper investigates whether a country supplying low-quality products can catch up with a rival country supplying high-quality products when both have access to the same technology. It turns out there exist multiple strict Nash equilibria. In the first Nash equilibrium, the leader of the industry in the trade situation is the firm that provided the higher quality at autarky. In the other Nash equilibrium (we call it 'change of leadership' equilibrium or 'leapfrogging' equilibrium), the new leader comes from the smaller (or poorer) country. The existence of multiple equilibria is subject to two main qualifications, however. Firstly, when the countries are very different the only possible equilibrium arising is the one where there is persistence of leadership. Secondly, if the risk dominance criterion of equilibrium selection (Harsanyi and Selten 1988) is used, the only remaining Nash equilibrium will be the persistence of dominance equilibrium.",
    "category": "INCORRECT",
    "question_context_used": "The paper investigates whether a country supplying low-quality products can catch up with a rival country supplying high-quality products when both have access to the same technology. It turns out there exist multiple strict Nash equilibria. In the first Nash equilibrium, the leader of the industry in the trade situation is the firm that provided the higher quality at autarky. In the other Nash equilibrium (we call it 'change of leadership' equilibrium or 'leapfrogging' equilibrium), the new leader comes from the smaller (or poorer) country. The existence of multiple equilibria is subject to two main qualifications, however. Firstly, when the countries are very different the only possible equilibrium arising is the one where there is persistence of leadership. Secondly, if the risk dominance criterion of equilibrium selection (Harsanyi and Selten 1988) is used, the only remaining Nash equilibrium will be the persistence of dominance equilibrium.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-717",
    "question": "Based on the analysis of tax rebate spending behavior, which of the following statements are supported by the empirical findings?",
    "options": [
      "A) Households with higher income levels exhibit a uniformly higher propensity to spend the rebate compared to lower-income groups.",
      "B) The spending fraction of the rebate shows a nonmonotonic relationship with stock ownership, being lowest for those with small stock holdings.",
      "C) Liquidity-constrained households, as indicated by financial condition, demonstrate significantly higher spending rates from the rebate.",
      "D) Receipt of the rebate increases the probability of reporting spending by approximately 7.9 percentage points, as per the linear probability model."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Shapiro-ConsumerResponseTax-2003.md"
    ],
    "question_context": "Key empirical findings from the tax rebate analysis include: The relationship between consumer response and stock ownership is nonmonotonic. Among nonstockholders, 19.5% spent the rebate; this percentage was lower for small stockholders but higher for those with >$50,000 in stocks. The linear probability model estimates that receipt of the rebate increases spending probability by 7.9 percentage points (p < 0.05). Formula for the linear probability model: Spending = 0.166** + 0.079** × ReceivedRebate + ε, where ** indicates significance at 1% level. Contrary to liquidity constraint hypotheses, households reporting worse financial condition had lower spending rates (16.6%) compared to those reporting better conditions (23.0%).",
    "category": "CORRECT",
    "question_context_used": "Key empirical findings from the tax rebate analysis include: The relationship between consumer response and stock ownership is nonmonotonic. Among nonstockholders, 19.5% spent the rebate; this percentage was lower for small stockholders but higher for those with >$50,000 in stocks. The linear probability model estimates that receipt of the rebate increases spending probability by 7.9 percentage points (p < 0.05). Formula for the linear probability model: Spending = 0.166** + 0.079** × ReceivedRebate + ε, where ** indicates significance at 1% level. Contrary to liquidity constraint hypotheses, households reporting worse financial condition had lower spending rates (16.6%) compared to those reporting better conditions (23.0%).",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-105",
    "question": "In the two-sector model described, which of the following assumptions are critical for ensuring full-employment equilibrium and intersectoral labor migration dynamics?",
    "options": [
      "A) Labor is represented in terms of man-hours rather than labor force, with price flexibility assumed",
      "B) Capital is perfectly mobile internationally, facilitating cross-border investment flows",
      "C) Agricultural labor supply depends on both manufacturing and agricultural wage rates, with migration occurring from rural to urban sectors",
      "D) Manufacturing labor supply responds to current manufacturing wages and lagged agricultural wages, reflecting migration time lags"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Income-tax-incidence-in-a-developing-country--T_1980_Journal-of-Development-.md"
    ],
    "question_context": "The model assumes labor is represented in terms of man-hours rather than labor force, with price flexibility ensuring full-employment equilibrium. Agricultural labor supply is given by $L^{A}=L^{4}(w^{M},w^{A}),\\qquad L_{M}^{A}\\leq0,\\quad L_{A}^{A}\\geq0$, where migration occurs from rural to urban sectors based on wage differentials. Manufacturing labor supply follows $L^{M}=L^{M}(w^{M},w_{-1}^{A}),\\qquad L_{M}^{M}\\ge0,\\quad L_{A}^{M}\\le0$, incorporating lagged agricultural wages to reflect migration time lags. Capital is assumed sector-specific with no intersectoral mobility, as shown by $K^{A}=K^{A}\\{r^{A}\\},\\qquad X_{A}^{A}\\geq0$ and $K^{M}=K^{M}(r^{M}),\\qquad K_{M}^{M}\\geq0$.",
    "category": "CORRECT",
    "question_context_used": "The model assumes labor is represented in terms of man-hours rather than labor force, with price flexibility ensuring full-employment equilibrium. Agricultural labor supply is given by $L^{A}=L^{4}(w^{M},w^{A}),\\qquad L_{M}^{A}\\leq0,\\quad L_{A}^{A}\\geq0$, where migration occurs from rural to urban sectors based on wage differentials. Manufacturing labor supply follows $L^{M}=L^{M}(w^{M},w_{-1}^{A}),\\qquad L_{M}^{M}\\ge0,\\quad L_{A}^{M}\\le0$, incorporating lagged agricultural wages to reflect migration time lags. Capital is assumed sector-specific with no intersectoral mobility, as shown by $K^{A}=K^{A}\\{r^{A}\\},\\qquad X_{A}^{A}\\geq0$ and $K^{M}=K^{M}(r^{M}),\\qquad K_{M}^{M}\\geq0$.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-758",
    "question": "In the context of learning with tolerance for inaccuracy, which of the following conditions ensure that the agent's payoff converges to $1-\\varepsilon$?",
    "options": [
      "A) Setting $\\gamma < 1-\\varepsilon$ and using a subjective order to select among $\\gamma$-best fits",
      "B) Setting $\\gamma > 1-\\varepsilon$ and relying solely on likelihood maximization",
      "C) Setting $\\gamma$ close to $1-\\varepsilon$ and $k$ sufficiently large to ensure stability",
      "D) Adjusting $\\gamma$ dynamically based on the agent's own learning process"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Subjectivity in inductive inference.md"
    ],
    "question_context": "The agent's learning can be more effective if it incorporates some tolerance for inaccuracy. For any $\\gamma\\in[0,1]$, a theory $t$ is a $\\gamma$-best fit to the data after history $h$ if $l(t,h)\\geq\\theta(\\gamma)$. The likelihood relation then groups theories that achieve a likelihood of at least $\\theta(\\gamma)$ into a single equivalence class. Setting $\\gamma<1-\\varepsilon$ ensures that the data-generating process is among the $\\gamma$-best fits, while setting $\\gamma$ close to $1-\\varepsilon$ and $k$ large ensures stability and effective learning. Proposition 6 states that under Assumption 4, for every discriminating subjective order and for every $d\\in D$, $\\Pi(d,\\succsim^{LS,\\gamma k})\\to (1-\\varepsilon)$ as $\\gamma\\to 1-\\varepsilon$ and $k\\to\\infty$. Proposition 7 further shows that the agent can adjust $\\gamma$ dynamically to approach the optimal tolerance level.",
    "category": "INCORRECT",
    "question_context_used": "The agent's learning can be more effective if it incorporates some tolerance for inaccuracy. For any $\\gamma\\in[0,1]$, a theory $t$ is a $\\gamma$-best fit to the data after history $h$ if $l(t,h)\\geq\\theta(\\gamma)$. The likelihood relation then groups theories that achieve a likelihood of at least $\\theta(\\gamma)$ into a single equivalence class. Setting $\\gamma<1-\\varepsilon$ ensures that the data-generating process is among the $\\gamma$-best fits, while setting $\\gamma$ close to $1-\\varepsilon$ and $k$ large ensures stability and effective learning. Proposition 6 states that under Assumption 4, for every discriminating subjective order and for every $d\\in D$, $\\Pi(d,\\succsim^{LS,\\gamma k})\\to (1-\\varepsilon)$ as $\\gamma\\to 1-\\varepsilon$ and $k\\to\\infty$. Proposition 7 further shows that the agent can adjust $\\gamma$ dynamically to approach the optimal tolerance level.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2413",
    "question": "Which of the following statements accurately describe the historical and conceptual foundations of the National Income and Product Accounts (NIPA)?",
    "options": [
      "A) The NIPA were initially developed by Simon Kuznets and colleagues in the 1930s to measure national income, investment, and saving.",
      "B) The NIPA were integrated with Wassily Leontief's input-output accounts to provide benchmark values for GDP.",
      "C) The NIPA were primarily designed to measure the impact of information-technology goods and services on the economy.",
      "D) The NIPA were developed during World War II to help stabilize the economy and promote prosperity."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Moulton-Getting21stCenturyGDP-2000.md"
    ],
    "question_context": "During the 1930s, the pioneering contributions of Simon Kuznets and his colleagues at the National Bureau of Economic Research and the U.S. Department of Commerce laid the conceptual and statistical foundations for measuring national income, investment, and saving. Later, the NIPA were integrated with the input-output accounts developed by Wassily Leontief, thereby providing the benchmark values for gross domestic product (GDP). On 7 December 1999, the Department of Commerce designated the development of the National Income and Product Accounts (NIPA) as its 'achievement of the century,' noting the role of these accounts in helping the country to win World War II, to stabilize the economy, and to promote prosperity.",
    "category": "INCORRECT",
    "question_context_used": "During the 1930s, the pioneering contributions of Simon Kuznets and his colleagues at the National Bureau of Economic Research and the U.S. Department of Commerce laid the conceptual and statistical foundations for measuring national income, investment, and saving. Later, the NIPA were integrated with the input-output accounts developed by Wassily Leontief, thereby providing the benchmark values for gross domestic product (GDP). On 7 December 1999, the Department of Commerce designated the development of the National Income and Product Accounts (NIPA) as its 'achievement of the century,' noting the role of these accounts in helping the country to win World War II, to stabilize the economy, and to promote prosperity.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3037",
    "question": "In the context of the disaster risk model with constant probability and conditional intensity, which of the following statements are quantitatively supported by the given calibration?",
    "options": [
      "A) The normal-times equity premium is 5.3 percent, while the unconditional equity premium is 4.5 percent.",
      "B) The volatility of equity returns is 14.3 percent, which accounts for 'excess' volatility of stocks.",
      "C) The predictive regression slope coefficient at a one-year horizon is 0.20, aligning with return predictability literature.",
      "D) The central price/dividend ratio is 18, derived when stock market resilience is at its maximum."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Gabaix-VariableRareDisasters-2008.md"
    ],
    "question_context": "The model uses yearly units with parameters γ=4, δ=0.045, and gc=0.025. The disaster probability p=0.017 is constant, with E[B^(-γ)]=10, yielding a certain-equivalent recovery rate of consumption ≈0.56. Key formula: The normal-times equity premium is R^e - r_f = p(E[B^(-γ)](1-F_*)) ≈ 5.3 percent. The unconditional equity premium in long samples including disasters is 4.5 percent. The central P/D ratio is 18 when Ĥ_t=0. Volatile resilience H_it leads to a log P/D ratio volatility of 9.2 percent and equity return volatility of 14.3 percent. Predictive regression slope for small T is β̄_T ≈ (r_i + ϕ_H)T, yielding β_1=0.20 at one-year horizon with r_i=5 percent and ϕ_H=15 percent.",
    "category": "INCORRECT",
    "question_context_used": "The model uses yearly units with parameters γ=4, δ=0.045, and gc=0.025. The disaster probability p=0.017 is constant, with E[B^(-γ)]=10, yielding a certain-equivalent recovery rate of consumption ≈0.56. Key formula: The normal-times equity premium is R^e - r_f = p(E[B^(-γ)](1-F_*)) ≈ 5.3 percent. The unconditional equity premium in long samples including disasters is 4.5 percent. The central P/D ratio is 18 when Ĥ_t=0. Volatile resilience H_it leads to a log P/D ratio volatility of 9.2 percent and equity return volatility of 14.3 percent. Predictive regression slope for small T is β̄_T ≈ (r_i + ϕ_H)T, yielding β_1=0.20 at one-year horizon with r_i=5 percent and ϕ_H=15 percent.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4635",
    "question": "Based on the analysis of heterogeneous belief formation processes and rational expectations tests, which of the following statements are supported by the empirical findings?",
    "options": [
      "A) Sophisticates exhibit the highest stockholding percentage and trading probability among all groups.",
      "B) Forecast errors are unpredictable with any information at time t under full information rational expectations.",
      "C) All groups show evidence of overreaction in their forecast revisions, with slope coefficients close to -0.5.",
      "D) Controlling for observed characteristics induces vertical shifts in trading behavior but does not reverse patterns."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Heterogeneity-in-households--stock-market-beliefs--Levels_2022_Journal-of-Ec.md"
    ],
    "question_context": "The differences in beliefs and their trajectories translate into very different behaviour when it comes to portfolio choice. Panel I of Fig. 6 shows that stockholding is lowest for ignorants $(15\\%)$ and highest for sophisticates $(44\\%)$ , with the other groups below $20\\%$ , too. Trading behaviour follows a similar pattern (Panel J). In order to further investigate this, we run a Probit regression of buying stocks in the subsequent period on a set of group fixed effects and the return expectation at the time of the survey. Because of the low baseline probabilities, it is important to use a nonlinear model as opposed to a linear probability model. This means that controlling for fixed effects is infeasible due to the incidental parameters problem. The average partial effects of increasing expectations by one group-level standard deviation are $0.14\\%$ for sophisticates and $0.03\\%$ for ignorants, respectively. For extrapolators, due to the comparatively greater volatility of their expectations (see Table 4) the effects are $0.16\\%$ . The other groups are somewhere in between. Forecast errors of full information rational expectations should be unpredictable with any information $I_{t}$ at time $t$ because they equal the true expected value of the variable to be forecasted given the information: $E\\biggl[R_{t\\rightarrow t+12}-\\stackrel{\\leftarrow}{E}[R_{t\\rightarrow t+12}\\mid I_{t}]\\mid I_{t}\\biggr]=0.$ . Non-full information rational expectation forecast errors should be unpredictable with any information in a forecaster’s information set, though they might be with information the forecaster is not aware of or does not use. As can be seen from the table, all groups overreact with a slope coefficient close to $-0.5$ . This is exactly what we would find if time variation in expectations is uncorrelated with future returns.",
    "category": "INCORRECT",
    "question_context_used": "The differences in beliefs and their trajectories translate into very different behaviour when it comes to portfolio choice. Panel I of Fig. 6 shows that stockholding is lowest for ignorants $(15\\%)$ and highest for sophisticates $(44\\%)$ , with the other groups below $20\\%$ , too. Trading behaviour follows a similar pattern (Panel J). In order to further investigate this, we run a Probit regression of buying stocks in the subsequent period on a set of group fixed effects and the return expectation at the time of the survey. Because of the low baseline probabilities, it is important to use a nonlinear model as opposed to a linear probability model. This means that controlling for fixed effects is infeasible due to the incidental parameters problem. The average partial effects of increasing expectations by one group-level standard deviation are $0.14\\%$ for sophisticates and $0.03\\%$ for ignorants, respectively. For extrapolators, due to the comparatively greater volatility of their expectations (see Table 4) the effects are $0.16\\%$ . The other groups are somewhere in between. Forecast errors of full information rational expectations should be unpredictable with any information $I_{t}$ at time $t$ because they equal the true expected value of the variable to be forecasted given the information: $E\\biggl[R_{t\\rightarrow t+12}-\\stackrel{\\leftarrow}{E}[R_{t\\rightarrow t+12}\\mid I_{t}]\\mid I_{t}\\biggr]=0.$ . Non-full information rational expectation forecast errors should be unpredictable with any information in a forecaster’s information set, though they might be with information the forecaster is not aware of or does not use. As can be seen from the table, all groups overreact with a slope coefficient close to $-0.5$ . This is exactly what we would find if time variation in expectations is uncorrelated with future returns.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2431",
    "question": "Which of the following are key findings or methodological contributions of Bloom et al. (2013) in their analysis of R&D spillovers?",
    "options": [
      "A) Social returns to R&D are two to three times larger than private returns after addressing endogeneity issues",
      "B) Business-stealing effects dominate knowledge spillover effects in most industries",
      "C) Instrumental variables based on R&D tax credit idiosyncrasies are used to identify causal effects",
      "D) Positive knowledge spillover effects are quantitatively larger than product-market rivalry effects"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Knowledge_Spillovers,_Innovation_and_Growth.md"
    ],
    "question_context": "Bloom et al. (2013) make several fundamental contributions to the existing literature on R&D spillovers. First, from a substantive, policy perspective they conclude that even after addressing all three of the above problems that have plagued the literature, the social returns to R&D are two to three times as large as the private returns. Second, BsVR identify the causal effects of R&D on firm performance using instrumental variables. They use idiosyncrasies of the state and federal US R&D tax credit system to develop an IV strategy. For example, they exploit the fact that firms will be differentially affected by the unexpected introduction of an R&D tax credit in (say) California, if a firm already has some R&D labs located in California prior to the introduction of the tax credit. More specifically, BsVR set up a general model of oligopolistic competition between firms. They characterise the two offsetting effects of neighbours’ R&D on a firm's value: first, a positive effect from knowledge spillovers (i.e. your research helps me through improving my ideas); second, a negative effect through business-stealing/ product-market rivalry (i.e. your ideas leapfrog my ideas). BsVR demonstrate how the two offsetting sources of spillovers can be identified in the data by considering the distance between firms in different spaces. The technological spillover can be identified through patenting in similar fields, while the business stealing effect through overlaps in the product-market space. Using this methodology, BSVR can successfully identify both product market rivalry and technology spillover effects. And, despite the importance of product market rivalry effects of R&D (the focus of many IO models of R&D), they find that the positive knowledge spillover effects dominates quantitatively.",
    "category": "CORRECT",
    "question_context_used": "Bloom et al. (2013) make several fundamental contributions to the existing literature on R&D spillovers. First, from a substantive, policy perspective they conclude that even after addressing all three of the above problems that have plagued the literature, the social returns to R&D are two to three times as large as the private returns. Second, BsVR identify the causal effects of R&D on firm performance using instrumental variables. They use idiosyncrasies of the state and federal US R&D tax credit system to develop an IV strategy. For example, they exploit the fact that firms will be differentially affected by the unexpected introduction of an R&D tax credit in (say) California, if a firm already has some R&D labs located in California prior to the introduction of the tax credit. More specifically, BsVR set up a general model of oligopolistic competition between firms. They characterise the two offsetting effects of neighbours’ R&D on a firm's value: first, a positive effect from knowledge spillovers (i.e. your research helps me through improving my ideas); second, a negative effect through business-stealing/ product-market rivalry (i.e. your ideas leapfrog my ideas). BsVR demonstrate how the two offsetting sources of spillovers can be identified in the data by considering the distance between firms in different spaces. The technological spillover can be identified through patenting in similar fields, while the business stealing effect through overlaps in the product-market space. Using this methodology, BSVR can successfully identify both product market rivalry and technology spillover effects. And, despite the importance of product market rivalry effects of R&D (the focus of many IO models of R&D), they find that the positive knowledge spillover effects dominates quantitatively.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4099",
    "question": "Based on the aggregate evidence on work and welfare outcomes, which of the following factors are significantly correlated with lower recidivism rates among women leaving welfare?",
    "options": [
      "A) Employment in professional services or government sectors",
      "B) Higher average payroll per worker in the firm",
      "C) Higher net job creation rates in the sector",
      "D) Employment in retail trade or manufacturing sectors"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Lane-FamilyWorkWelfare-1995.md"
    ],
    "question_context": "The recidivism rates of women who work is similar to the 37 percent rate displayed by the group as a whole. However, these rates vary markedly depending on where they were employed. Women who found jobs in professional services and with the government, in particular, show much lower recidivism rates than those in retail trade and manufacturing. These differences are correlated with the difference in earnings between recidivists and nonrecidivists. The mean quarterly earnings of the former group was $1,145; mean earnings of the latter were $1,487. Table 1 reveals a positive relationship between recidivism rates and the average earnings of other workers.",
    "category": "INCORRECT",
    "question_context_used": "The recidivism rates of women who work is similar to the 37 percent rate displayed by the group as a whole. However, these rates vary markedly depending on where they were employed. Women who found jobs in professional services and with the government, in particular, show much lower recidivism rates than those in retail trade and manufacturing. These differences are correlated with the difference in earnings between recidivists and nonrecidivists. The mean quarterly earnings of the former group was $1,145; mean earnings of the latter were $1,487. Table 1 reveals a positive relationship between recidivism rates and the average earnings of other workers.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2253",
    "question": "In the context of the robust planner's Markovian decision problem, which of the following statements are true regarding the structured and unstructured models?",
    "options": [
      "A) Structured models are constructed by adding drift distortion processes that satisfy constraint (9) to the baseline model.",
      "B) Unstructured models are statistically close to structured models as measured by discrepancy measure (8) but violate constraint (9).",
      "C) The relative entropy of an unstructured model relative to a structured model is given by $\\mathsf{q}_{u,s} = \\sqrt{2\\Lambda(M^{U},M^{S})}$.",
      "D) The worst-case unstructured model's drift distortion $U_t$ is independent of the state variable $Z_t$."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Macroeconomic-uncertainty-prices-when-beliefs-are-_2021_Journal-of-Econometr.md"
    ],
    "question_context": "The decision maker evaluates utility consequences of a set of models that includes unstructured models which are difficult to distinguish from structured models $\\mathcal{M}^{o}$. Structured models are created by naming a sequence of closed convex sets $\\{\\Xi_{t}\\}$ that satisfy (14) and associated drift distortion processes $\\left\\{{{S}_{t}}\right\\}$ that satisfy structured model constraint (9). Unstructured models violate (9) but are statistically close to models that do satisfy it, as measured by discrepancy measure (8).\n\nThe relative entropy $\\mathsf{q}^{2}/2$ is computed by solving differential equation (12) under parametric alternatives (19), where $\\rho$ is quadratic in $z-\\bar{z}$. The worst-case unstructured model's drift distortion $U_t = \\eta_u(Z_t)$ depends on the state variable $Z_t$, and the relative entropy of an unstructured model relative to a structured model is given by $\\mathsf{q}_{u,s} = \\sqrt{2\\Lambda(M^{U},M^{S})}$, where $\\Lambda(M^{U},M^{S})$ is the expected log likelihood ratio between the two models.",
    "category": "CORRECT",
    "question_context_used": "The decision maker evaluates utility consequences of a set of models that includes unstructured models which are difficult to distinguish from structured models $\\mathcal{M}^{o}$. Structured models are created by naming a sequence of closed convex sets $\\{\\Xi_{t}\\}$ that satisfy (14) and associated drift distortion processes $\\left\\{{{S}_{t}}\right\\}$ that satisfy structured model constraint (9). Unstructured models violate (9) but are statistically close to models that do satisfy it, as measured by discrepancy measure (8).\n\nThe relative entropy $\\mathsf{q}^{2}/2$ is computed by solving differential equation (12) under parametric alternatives (19), where $\\rho$ is quadratic in $z-\\bar{z}$. The worst-case unstructured model's drift distortion $U_t = \\eta_u(Z_t)$ depends on the state variable $Z_t$, and the relative entropy of an unstructured model relative to a structured model is given by $\\mathsf{q}_{u,s} = \\sqrt{2\\Lambda(M^{U},M^{S})}$, where $\\Lambda(M^{U},M^{S})$ is the expected log likelihood ratio between the two models.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4550",
    "question": "In the context of the truncated Taylor rule and the zero lower bound (ZLB) constraint, which of the following statements are empirically supported by the model's findings?",
    "options": [
      "A) Uncertainty has negligible effects on allocations and prices when the nominal interest rate is above the ZLB.",
      "B) The presence of uncertainty leads to larger declines in consumption, output, and inflation when the ZLB is binding compared to when it is not.",
      "C) The semi-loglinear model accurately quantifies the effects of uncertainty at the ZLB without significant approximation errors.",
      "D) The nonlinearities in the utility function and price adjustment costs are the primary drivers of the differential effects of uncertainty at the ZLB."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Nakata-UncertaintyZeroLower-2017.md"
    ],
    "question_context": "The nominal interest rate is determined by a truncated Taylor rule: \n\n$$ R_{t}=\\operatorname*{max}\\biggl[1,\\frac{1}{\\beta}\\Pi_{t}^{\\phi_{\\pi}}\\biggr], $$ \n\nwhere $\\Pi_{t}\\equiv P_{t}/P_{t-1}$. The model shows that uncertainty alters allocations and prices to a much greater extent when the ZLB is binding than when it is not. For instance, consumption initially declines by 4 percent in the stochastic economy while it declines by 2.4 percent in the deterministic economy when the ZLB is binding. The semi-loglinear model, while qualitatively similar, exhibits large approximation errors, indicating that partial log-linearization is inadequate for quantitative analyses at the ZLB. The primary nonlinearity driving these results is the ZLB constraint itself, not the curvature in the utility function or the quadratic nature of price adjustment costs.",
    "category": "INCORRECT",
    "question_context_used": "The nominal interest rate is determined by a truncated Taylor rule: \n\n$$ R_{t}=\\operatorname*{max}\\biggl[1,\\frac{1}{\\beta}\\Pi_{t}^{\\phi_{\\pi}}\\biggr], $$ \n\nwhere $\\Pi_{t}\\equiv P_{t}/P_{t-1}$. The model shows that uncertainty alters allocations and prices to a much greater extent when the ZLB is binding than when it is not. For instance, consumption initially declines by 4 percent in the stochastic economy while it declines by 2.4 percent in the deterministic economy when the ZLB is binding. The semi-loglinear model, while qualitatively similar, exhibits large approximation errors, indicating that partial log-linearization is inadequate for quantitative analyses at the ZLB. The primary nonlinearity driving these results is the ZLB constraint itself, not the curvature in the utility function or the quadratic nature of price adjustment costs.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4576",
    "question": "In the modified game framework with i.i.d. type realizations, which conditions ensure that the strategy profile μ∗ is an equilibrium in both the original and modified games?",
    "options": [
      "A) μ∗ must maximize the expected utility for all voters in the original game only",
      "B) μ∗ must satisfy μi∗(tj) ∈ arg max ∑ P(τn)EUi(μi, μ−i∗|τn) for each j ∈ {k+1,...,k+s} in both games",
      "C) μ∗ must ensure that the probability of any realization τn is equal to P(τn) = P(t1) × ... × P(tn)",
      "D) μ∗ must be such that the expected utility in the modified game converges to 1 as n → ∞"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Information-aggregation-with-runoff-voting_2021_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The proof establishes that for a strategy profile μ∗ to be an equilibrium in both the original and modified games, it must satisfy specific maximization conditions. Key formula: μi∗(tj) ∈ arg max ∑ P(τn)EUi(μi, μ−i∗|τn) for each j ∈ {k+1,...,k+s}. Additionally, the expected utility in the modified game must converge to 1 as the population grows (EUi(μ∗) → 1 as n → ∞). The modified game introduces nature's intervention, altering strategies with certain probabilities, while maintaining the i.i.d. property of type realizations. The equilibrium conditions ensure that no voter has a profitable deviation in either game when others adhere to μ∗.",
    "category": "CORRECT",
    "question_context_used": "The proof establishes that for a strategy profile μ∗ to be an equilibrium in both the original and modified games, it must satisfy specific maximization conditions. Key formula: μi∗(tj) ∈ arg max ∑ P(τn)EUi(μi, μ−i∗|τn) for each j ∈ {k+1,...,k+s}. Additionally, the expected utility in the modified game must converge to 1 as the population grows (EUi(μ∗) → 1 as n → ∞). The modified game introduces nature's intervention, altering strategies with certain probabilities, while maintaining the i.i.d. property of type realizations. The equilibrium conditions ensure that no voter has a profitable deviation in either game when others adhere to μ∗.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1763",
    "question": "In the context of the Massachusetts health reform, which of the following statements are supported by the empirical evidence regarding changes in hospital costs and access to care?",
    "options": [
      "A) The reform led to a significant increase in total hospital cost growth relative to other states.",
      "B) The average wait time for internal medicine appointments showed no sharp change between pre- and post-reform periods.",
      "C) The reform resulted in a reduction in the average cost of the insured by 8.7 percent.",
      "D) Capacity constraints on physician supply led to a substantial decrease in costs among the newly insured."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Hackmann-AdverseSelectionIndividual-2015.md"
    ],
    "question_context": "The Massachusetts reform, while primarily focused on insurance, may have impacted care delivery through general equilibrium effects on prices or access to care. Kolstad and Kowalski (2012a) discuss the impact of the Massachusetts reform on delivery in detail. While they find an impact on access, notably use of the emergency room as a point of entry into the health care system, they do not find a change in total hospital cost growth in Massachusetts relative to other states before versus after the reform. These findings, both with and without controls for patient illness severity, are not consistent with substantial increases in prices or quantity, holding illness severity fixed. Since hospital cost is the primary driver of total claims, this evidence suggests our findings do not result from changes in prices for care.\n\nFurthermore, if expanding insurance to near-universal levels results in increases in prices charged by health care providers in Massachusetts (relative to other states) our findings would be biased down. That is, claims cost would be rising due to price increases. Therefore, our finding that the average cost of the insured declined is a lowerbound.\n\nOne other potentially important issue is access to physician services. If the Massachusetts reform did increase demand, but capacity constraints on supply meant that the newly insured could not access a provider, then we would see lower cost among the newly insured as a result. While the potential for such constraints has been much discussed in the popular press, the empirical evidence does not suggest major impacts of reform. In 2005, the average wait time for an internal medicine appointment in Massachusetts was 47 days. In 2008, 2009, and 2010 the average wait was 50, 44, and 53 days respectively.49 while these are average measures, we do not see a sharp change between pre- and post-reform in Massachusetts. Similar trends hold for specialties as well. Taken together, the evidence suggests that general equilibrium shifts in health care delivery are unlikely to explain our results. Nevertheless, given the aggregate data on wait times, we are unable to rule out potential impacts of access on cost.",
    "category": "INCORRECT",
    "question_context_used": "The Massachusetts reform, while primarily focused on insurance, may have impacted care delivery through general equilibrium effects on prices or access to care. Kolstad and Kowalski (2012a) discuss the impact of the Massachusetts reform on delivery in detail. While they find an impact on access, notably use of the emergency room as a point of entry into the health care system, they do not find a change in total hospital cost growth in Massachusetts relative to other states before versus after the reform. These findings, both with and without controls for patient illness severity, are not consistent with substantial increases in prices or quantity, holding illness severity fixed. Since hospital cost is the primary driver of total claims, this evidence suggests our findings do not result from changes in prices for care.\n\nFurthermore, if expanding insurance to near-universal levels results in increases in prices charged by health care providers in Massachusetts (relative to other states) our findings would be biased down. That is, claims cost would be rising due to price increases. Therefore, our finding that the average cost of the insured declined is a lowerbound.\n\nOne other potentially important issue is access to physician services. If the Massachusetts reform did increase demand, but capacity constraints on supply meant that the newly insured could not access a provider, then we would see lower cost among the newly insured as a result. While the potential for such constraints has been much discussed in the popular press, the empirical evidence does not suggest major impacts of reform. In 2005, the average wait time for an internal medicine appointment in Massachusetts was 47 days. In 2008, 2009, and 2010 the average wait was 50, 44, and 53 days respectively.49 while these are average measures, we do not see a sharp change between pre- and post-reform in Massachusetts. Similar trends hold for specialties as well. Taken together, the evidence suggests that general equilibrium shifts in health care delivery are unlikely to explain our results. Nevertheless, given the aggregate data on wait times, we are unable to rule out potential impacts of access on cost.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1367",
    "question": "In the context of $\\ell_{1}$ trend filtering and sparse Hodrick-Prescott (HP) trend filtering, which of the following statements are correct regarding their optimization problems and computational implementations?",
    "options": [
      "A) The $\\ell_{1}$ trend filtering objective function includes an $\\ell_{1}$ penalty on the second differences of the trend estimates, promoting sparsity in kinks.",
      "B) The sparse HP trend filtering problem can be reformulated as a mixed integer quadratic programming (MIQP) problem, which is computationally tractable for typical sample sizes in COVID-19 data analysis.",
      "C) The square-root $\\ell_{1}$ trend filtering variant is derived by taking the square root of the entire objective function, including both the fidelity term and the penalty term.",
      "D) The $\\ell_{0}$-constrained trend filtering formulation directly controls the number of kinks via the parameter $\\kappa$, but it may perform poorly under low signal-to-noise ratio (SNR) conditions."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Sparse-HP-filter--Finding-kinks-in-the-COVID-19-con_2021_Journal-of-Economet.md"
    ],
    "question_context": "The $\\ell_{1}$ trend filtering objective function is given by:\n\n$$\\sum_{t=1}^{T}(y_{t}-f_{t})^{2}+\\lambda\\sum_{t=2}^{T-1}|f_{t-1}-2f_{t}+f_{t+1}|,$$\n\nwhich promotes sparsity in the second differences of the trend estimates, thereby identifying kinks. The sparse HP trend filtering problem combines an $\\ell_{2}$ penalty with an $\\ell_{0}$ constraint:\n\n$$\\sum_{t=1}^{T}(y_{t}-f_{t})^{2}+\\lambda\\sum_{t=2}^{T-1}(f_{t-1}-2f_{t}+f_{t+1})^{2}$$\n\nsubject to\n\n$$\\sum_{t=2}^{T-1}1\\{f_{t}-f_{t-1}\\neq f_{t+1}-f_{t}\\}\\leq\\kappa.$$\n\nThis formulation is solved via MIQP, leveraging modern optimization software like Gurobi. The $\\ell_{0}$-constrained approach directly controls the number of kinks but is sensitive to low SNR, necessitating additional regularization.",
    "category": "INCORRECT",
    "question_context_used": "The $\\ell_{1}$ trend filtering objective function is given by:\n\n$$\\sum_{t=1}^{T}(y_{t}-f_{t})^{2}+\\lambda\\sum_{t=2}^{T-1}|f_{t-1}-2f_{t}+f_{t+1}|,$$\n\nwhich promotes sparsity in the second differences of the trend estimates, thereby identifying kinks. The sparse HP trend filtering problem combines an $\\ell_{2}$ penalty with an $\\ell_{0}$ constraint:\n\n$$\\sum_{t=1}^{T}(y_{t}-f_{t})^{2}+\\lambda\\sum_{t=2}^{T-1}(f_{t-1}-2f_{t}+f_{t+1})^{2}$$\n\nsubject to\n\n$$\\sum_{t=2}^{T-1}1\\{f_{t}-f_{t-1}\\neq f_{t+1}-f_{t}\\}\\leq\\kappa.$$\n\nThis formulation is solved via MIQP, leveraging modern optimization software like Gurobi. The $\\ell_{0}$-constrained approach directly controls the number of kinks but is sensitive to low SNR, necessitating additional regularization.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2959",
    "question": "In the context of the plug-in averaging estimator for optimal portfolio selection, which of the following statements are correct regarding the properties and computational aspects of the estimator?",
    "options": [
      "A) The optimal weight vector $w^0$ has a closed-form solution regardless of the number of submodels.",
      "B) The plug-in estimator minimizes the sample analog of SAMSE, $w'\\hat{\\psi}w$, where $\\hat{\\psi}$ is the sample analog of $\\psi$.",
      "C) The estimator $\\bar{\\mu}(w^0)$ weakly dominates individual submodel estimators with respect to SAMSE.",
      "D) The computational burden of solving the optimization problem for submodel weights in FIC model averaging is negligible even for large portfolios."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "10-Multiple_Regression_Model_Averaging_and_the_Focused_Information_Criterion_With_an_Application_.md"
    ],
    "question_context": "The model averaging estimator of $\\mu$ is defined as $\\bar{\\mu}(w)=\\sum_{m=1}^{\\bar{M}}w_{m}\\hat{\\mu}_{m}$, where the weights $w$ are chosen to minimize SAMSE. The optimal weight vector $w^0$ is found by solving the quadratic programming problem $w^0 = \\arg\\min w'\\psi w$ subject to $\\sum_{m=1}^{\\bar{M}}w_{m}=1$ and $w_{m}\\geq0$. The estimator $\\bar{\\mu}(w^0)$ weakly dominates individual submodel estimators with respect to SAMSE. There is no closed-form solution to the optimization problem when more than two submodels are present, and the optimal weights must be found numerically. The plug-in estimator uses sample counterparts to estimate the unknown parameters, minimizing the sample analog of SAMSE, $w'\\hat{\\psi}w$. The computational cost of solving the optimization problem for submodel weights in FIC model averaging is significant, especially for larger portfolios, as evidenced by simulations taking approximately a week on a 60-core computing cluster for a moderately sized case with 2048 submodels.",
    "category": "CORRECT",
    "question_context_used": "The model averaging estimator of $\\mu$ is defined as $\\bar{\\mu}(w)=\\sum_{m=1}^{\\bar{M}}w_{m}\\hat{\\mu}_{m}$, where the weights $w$ are chosen to minimize SAMSE. The optimal weight vector $w^0$ is found by solving the quadratic programming problem $w^0 = \\arg\\min w'\\psi w$ subject to $\\sum_{m=1}^{\\bar{M}}w_{m}=1$ and $w_{m}\\geq0$. The estimator $\\bar{\\mu}(w^0)$ weakly dominates individual submodel estimators with respect to SAMSE. There is no closed-form solution to the optimization problem when more than two submodels are present, and the optimal weights must be found numerically. The plug-in estimator uses sample counterparts to estimate the unknown parameters, minimizing the sample analog of SAMSE, $w'\\hat{\\psi}w$. The computational cost of solving the optimization problem for submodel weights in FIC model averaging is significant, especially for larger portfolios, as evidenced by simulations taking approximately a week on a 60-core computing cluster for a moderately sized case with 2048 submodels.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4153",
    "question": "In the context of the minimum wage's impact on labor supply and fertility, which of the following statements are supported by the quantitative analysis?",
    "options": [
      "A) A higher minimum wage increases fertility rates among high-income households.",
      "B) The elasticity of high-income female labor supply with respect to the minimum wage is approximately -0.1 in the model.",
      "C) The empirical elasticity of labor supply for high-income women is larger in magnitude than the model's prediction.",
      "D) A higher minimum wage leads to a decrease in mother's time at home for high-income households."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Bar-richfamiliesincrease-2018.md"
    ],
    "question_context": "The study estimates the passthrough rate of the minimum wage to HPS sector wages using cross-state variation and conducts a policy experiment to assess the impact of a federal minimum wage increase to $15/h. The regression model used is:\n\n$$w_{i s t}^{\\mathrm{HPS}}=\\alpha+\\beta w_{s t}^{\\mathrm{min}}+\\gamma\\bar{w}_{s t}+\\delta_{b e l o w}+\\delta_{t}+\\delta_{s}+\\delta_{a g e}+\\delta_{e d u c}+\\delta_{H i s p a n}+\\delta_{r a c e}+\\delta_{o c c}+\\epsilon_{i s t},$$\n\nwhere $\\beta$ represents the dollar change in HPS sector wages per dollar increase in the minimum wage. The results show that a $1 increase in the minimum wage yields approximately a 55–75 cent increase in HPS wages. The model predicts that a higher minimum wage decreases fertility and increases mother's time at home, with larger effects for higher-income households. The empirical elasticity of labor supply for high-income women is found to be larger than the model's prediction, ranging from -0.66 to -0.41.",
    "category": "INCORRECT",
    "question_context_used": "The study estimates the passthrough rate of the minimum wage to HPS sector wages using cross-state variation and conducts a policy experiment to assess the impact of a federal minimum wage increase to $15/h. The regression model used is:\n\n$$w_{i s t}^{\\mathrm{HPS}}=\\alpha+\\beta w_{s t}^{\\mathrm{min}}+\\gamma\\bar{w}_{s t}+\\delta_{b e l o w}+\\delta_{t}+\\delta_{s}+\\delta_{a g e}+\\delta_{e d u c}+\\delta_{H i s p a n}+\\delta_{r a c e}+\\delta_{o c c}+\\epsilon_{i s t},$$\n\nwhere $\\beta$ represents the dollar change in HPS sector wages per dollar increase in the minimum wage. The results show that a $1 increase in the minimum wage yields approximately a 55–75 cent increase in HPS wages. The model predicts that a higher minimum wage decreases fertility and increases mother's time at home, with larger effects for higher-income households. The empirical elasticity of labor supply for high-income women is found to be larger than the model's prediction, ranging from -0.66 to -0.41.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\"]}",
      "model_answer": [
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4703",
    "question": "In the context of the wage residual model presented, which of the following assumptions are critical for the variance-covariance matrix specification?",
    "options": [
      "A) The transitory shocks follow a stationary first-order autoregressive process",
      "B) The permanent ability component is uncorrelated with the transitory shocks",
      "C) The pre-conscription wage residual includes an additive component uncorrelated with permanent ability",
      "D) The parameter ψ_t remains constant throughout the entire post-conscription period"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Can Compulsory Military Service Raise Civilian Wages_ Evidence from the Peacetime Draft in Portugal.md"
    ],
    "question_context": "The reduced-form wage residuals for individual $i$ in period $t$ are assumed to be generated by the model: \n\n$$\n\\nu_{i t}=\\psi_{t}\\eta_{i}+\\epsilon_{i t}\\quad t=0,1,\\ldots,T\n$$\n\nWith $\\psi_{0}=1$. Under this model the variance-covariance matrix of the residuals has elements \n\n$$\n\\begin{array}{r}{\\begin{array}{r c l}{\\operatorname{var}[\\nu_{i t}]=(\\psi_{t})^{2}\\sigma_{\\eta}^{2}+\\operatorname{var}[\\epsilon_{i t}]}\\ {\\operatorname{cov}[\\nu_{i t},\\nu_{i s}]=\\psi_{t}\\psi_{s}\\sigma_{\\eta}^{2}+\\operatorname{cov}[\\epsilon_{i t},\\epsilon_{i s}],}\\end{array}}\\end{array}\n$$\n\nwhere $\\sigma_{\\eta}^{2}=\\mathrm{var}[\\eta_{i}]$ is the variance of the permanent ability component. We assume that the transitory shocks $\\epsilon_{t}$ are generated by a stationary first-order autoregressive process: \n\n$$\n\\epsilon_{i t}=\\rho\\epsilon_{i t-1}+\\zeta_{i t}\n$$\n\nwith \n\n$$\n\\operatorname{var}[\\zeta_{i t}]=\\sigma_{\\zeta}^{2}\n$$\n\nand an arbitrary value for the variance of $\\epsilon_{i0}$ \n\n$$\n\\begin{array}{r}{\\mathbf{var}[\\epsilon_{i0}]=\\sigma_{0}^{2}.}\\end{array}\n$$\n\nTo allow for additional potential non-stationarity at the start of the work-life, we also assume that the pre-conscription wage residual contains an additive component: \n\n$$\n\\nu_{i0}=\\eta_{i}+\\epsilon_{i0}+\\kappa_{i0},\n$$\n\nwhere $\\mathrm{var}[\\kappa_{i0}]=\\sigma_{\\kappa}^{2}$, and $\\kappa_{i0}$ is uncorrelated with $\\eta_{i}$ or the sequence of transitory shocks.",
    "category": "INCORRECT",
    "question_context_used": "The reduced-form wage residuals for individual $i$ in period $t$ are assumed to be generated by the model: \n\n$$\n\\nu_{i t}=\\psi_{t}\\eta_{i}+\\epsilon_{i t}\\quad t=0,1,\\ldots,T\n$$\n\nWith $\\psi_{0}=1$. Under this model the variance-covariance matrix of the residuals has elements \n\n$$\n\\begin{array}{r}{\\begin{array}{r c l}{\\operatorname{var}[\\nu_{i t}]=(\\psi_{t})^{2}\\sigma_{\\eta}^{2}+\\operatorname{var}[\\epsilon_{i t}]}\\ {\\operatorname{cov}[\\nu_{i t},\\nu_{i s}]=\\psi_{t}\\psi_{s}\\sigma_{\\eta}^{2}+\\operatorname{cov}[\\epsilon_{i t},\\epsilon_{i s}],}\\end{array}}\\end{array}\n$$\n\nwhere $\\sigma_{\\eta}^{2}=\\mathrm{var}[\\eta_{i}]$ is the variance of the permanent ability component. We assume that the transitory shocks $\\epsilon_{t}$ are generated by a stationary first-order autoregressive process: \n\n$$\n\\epsilon_{i t}=\\rho\\epsilon_{i t-1}+\\zeta_{i t}\n$$\n\nwith \n\n$$\n\\operatorname{var}[\\zeta_{i t}]=\\sigma_{\\zeta}^{2}\n$$\n\nand an arbitrary value for the variance of $\\epsilon_{i0}$ \n\n$$\n\\begin{array}{r}{\\mathbf{var}[\\epsilon_{i0}]=\\sigma_{0}^{2}.}\\end{array}\n$$\n\nTo allow for additional potential non-stationarity at the start of the work-life, we also assume that the pre-conscription wage residual contains an additive component: \n\n$$\n\\nu_{i0}=\\eta_{i}+\\epsilon_{i0}+\\kappa_{i0},\n$$\n\nwhere $\\mathrm{var}[\\kappa_{i0}]=\\sigma_{\\kappa}^{2}$, and $\\kappa_{i0}$ is uncorrelated with $\\eta_{i}$ or the sequence of transitory shocks.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4824",
    "question": "Under Assumption A5 and the given equilibrium conditions, which of the following statements are true regarding the allocation of control rights between agents A and B?",
    "options": [
      "A) Agent A, being impatient, always retains full control to expedite investment decisions.",
      "B) Agent A may relinquish control to Agent B to accelerate project implementation, despite A's impatience.",
      "C) Agent B, being more patient, prefers to retain control to defer decisions until more information is available.",
      "D) The equilibrium contract always specifies complete action plans to avoid any future conflicts."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Satisficing Contracts.md"
    ],
    "question_context": "Under Assumption A5, Agent A is impatient and prefers to invest immediately, while Agent B is more cautious and prefers to deliberate before acting. The equilibrium outcome may involve Agent A relinquishing control to Agent B to accelerate project implementation, as control allows B to block hasty decisions. The key condition for this outcome is given by:\n\n$$\n-I+\\frac{\\delta\\pi}{2}+\\frac{\\delta\\rho_{B}}{2}<\\widehat{\\Lambda}\\left(-I+\\frac{\\delta\\pi}{2}+\\frac{\\delta\\rho_{B}^{*}}{2}\\right)\n$$\n\nWhen this condition holds, Agent A offers contract $C_B$ with probability $y^*$ and $C_R$ with probability $(1-y^*)$, where $y^*$ is determined by the equilibrium condition. This paradoxical result shows that control rights can be used strategically to align timing preferences between impatient and patient agents.",
    "category": "INCORRECT",
    "question_context_used": "Under Assumption A5, Agent A is impatient and prefers to invest immediately, while Agent B is more cautious and prefers to deliberate before acting. The equilibrium outcome may involve Agent A relinquishing control to Agent B to accelerate project implementation, as control allows B to block hasty decisions. The key condition for this outcome is given by:\n\n$$\n-I+\\frac{\\delta\\pi}{2}+\\frac{\\delta\\rho_{B}}{2}<\\widehat{\\Lambda}\\left(-I+\\frac{\\delta\\pi}{2}+\\frac{\\delta\\rho_{B}^{*}}{2}\\right)\n$$\n\nWhen this condition holds, Agent A offers contract $C_B$ with probability $y^*$ and $C_R$ with probability $(1-y^*)$, where $y^*$ is determined by the equilibrium condition. This paradoxical result shows that control rights can be used strategically to align timing preferences between impatient and patient agents.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1152",
    "question": "In the context of the proof of Result 1 and Proposition 2, which of the following conditions are necessary for the solution to (9), (10'), (11), (12) to satisfy (10)?",
    "options": [
      "A) (10')(a) is binding for all i",
      "B) The marginal rate of substitution between income in states s_i and s_{i-1} for the firm is less than that for workers",
      "C) l(s_{i-1}) = l^*(s_{i-1}), the first-best employment level",
      "D) The firm's net profit is higher in state i than in state (i-1)"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Optimal Labour Contracts under Asymmetric Information An Introduction.md"
    ],
    "question_context": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. In the proof of Result 1 and Proposition 2, the following inequality is derived:\n\n$$\n\\begin{array}{r}{f(\\boldsymbol{s}_{i},\\sum_{k}\\ l_{k}(\\boldsymbol{s}_{i}))-f(\\boldsymbol{s}_{i},\\sum_{k}l_{k}(\\boldsymbol{s}_{i-1}))\\ge f(\\boldsymbol{s}_{i-1},\\sum_{k}l_{k}(\\boldsymbol{s}_{i}))-f(\\boldsymbol{s}_{i-1},\\sum_{k}l_{k}(\\boldsymbol{s}_{i-1}))~}\\end{array}\n$$\n\nThis implies, by Assumption 5, (10')(b). Thus (10) implies (10'). Hence, if we can show that the solution to (9), (10'), (11), (12) satisfies (10), we will have established that $(10){\\Rightarrow}(10^{\\prime}){\\Rightarrow}(10)$ , i.e. we will have proved Result 1. Lemma 1 states that if $f(s_{i},m l(s_{i}))-m w(s_{i})>f(s_{i},m l(s_{i-1}))-m w(s_{i-1})$ for some i at a solution to (9), (10), (11), (12), then $l(s_{i-1})=l^{*}(s_{i-1})$ , the first-best employment level, and $w\\left({{s}_{i}}\\right)-R l\\left({{s}_{i}}\\right)>w\\left({{s}_{i-1}}\\right)-R l\\left({{s}_{i-1}}\\right)$.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. In the proof of Result 1 and Proposition 2, the following inequality is derived:\n\n$$\n\\begin{array}{r}{f(\\boldsymbol{s}_{i},\\sum_{k}\\ l_{k}(\\boldsymbol{s}_{i}))-f(\\boldsymbol{s}_{i},\\sum_{k}l_{k}(\\boldsymbol{s}_{i-1}))\\ge f(\\boldsymbol{s}_{i-1},\\sum_{k}l_{k}(\\boldsymbol{s}_{i}))-f(\\boldsymbol{s}_{i-1},\\sum_{k}l_{k}(\\boldsymbol{s}_{i-1}))~}\\end{array}\n$$\n\nThis implies, by Assumption 5, (10')(b). Thus (10) implies (10'). Hence, if we can show that the solution to (9), (10'), (11), (12) satisfies (10), we will have established that $(10){\\Rightarrow}(10^{\\prime}){\\Rightarrow}(10)$ , i.e. we will have proved Result 1. Lemma 1 states that if $f(s_{i},m l(s_{i}))-m w(s_{i})>f(s_{i},m l(s_{i-1}))-m w(s_{i-1})$ for some i at a solution to (9), (10), (11), (12), then $l(s_{i-1})=l^{*}(s_{i-1})$ , the first-best employment level, and $w\\left({{s}_{i}}\\right)-R l\\left({{s}_{i}}\\right)>w\\left({{s}_{i-1}}\\right)-R l\\left({{s}_{i-1}}\\right)$.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1419",
    "question": "Based on the empirical analysis of human capital returns, which of the following statements are supported by the findings?",
    "options": [
      "A) Human capital investments at college levels consistently show superior risk-adjusted returns compared to liquid financial assets",
      "B) The stochastic properties of human capital returns show minimal variation across different demographic groups",
      "C) Marriage and family structures may serve as mechanisms for hedging risks in human capital investments",
      "D) The empirical results are irrelevant for determining optimal social discount rates in public education investments"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "PalaciosHuerta-EmpiricalAnalysisRisk-2003.md"
    ],
    "question_context": "The empirical analysis has yielded a number of results. In particular, the variation in the stochastic properties of different human capital returns is substantial, especially when comparing human capital and liquid assets. These differences indicate how much is gained or lost by individuals and society from investments in human capital assets relative to financial investments. The analysis cannot identify the sources of these risk-adjusted differences, but it does suggest that some 'difficulties,' 'frictions,' or costs of trading claims on the present discounted value of future wages may be present for some individuals, especially at low education levels and for some minorities. The empirical analysis can also explain the superiority of human capital, at least at college levels, over liquid investments. While the level of private returns to schooling in the literature is not sufficient to justify the claims of importance made for education in many areas of economics, the analysis shows how it is possible to justify the superiority of investments in schooling by simply accounting for the riskiness of the investments. The analysis can also be useful in other areas. For instance, to the extent that marriage and families serve to hedge some risks and to diversify human capital investments through altruism or implicit contracts, the analysis may be useful to the understanding of marriage and divorce markets, the structure of the family, and investments in the human capital of children.",
    "category": "CORRECT",
    "question_context_used": "The empirical analysis has yielded a number of results. In particular, the variation in the stochastic properties of different human capital returns is substantial, especially when comparing human capital and liquid assets. These differences indicate how much is gained or lost by individuals and society from investments in human capital assets relative to financial investments. The analysis cannot identify the sources of these risk-adjusted differences, but it does suggest that some 'difficulties,' 'frictions,' or costs of trading claims on the present discounted value of future wages may be present for some individuals, especially at low education levels and for some minorities. The empirical analysis can also explain the superiority of human capital, at least at college levels, over liquid investments. While the level of private returns to schooling in the literature is not sufficient to justify the claims of importance made for education in many areas of economics, the analysis shows how it is possible to justify the superiority of investments in schooling by simply accounting for the riskiness of the investments. The analysis can also be useful in other areas. For instance, to the extent that marriage and families serve to hedge some risks and to diversify human capital investments through altruism or implicit contracts, the analysis may be useful to the understanding of marriage and divorce markets, the structure of the family, and investments in the human capital of children.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-737",
    "question": "Under the maximin social welfare function with quasilinear preferences and an isoelastic disutility of labor function, which of the following skill distributions ensure a decreasing marginal tax profile in skills and a single-peaked average tax rate?",
    "options": [
      "A) Truncated Pareto distribution",
      "B) Weibull distribution",
      "C) Lognormal distribution satisfying condition (26)",
      "D) Untruncated Pareto distribution beyond the mode"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Optimal-marginal-and-average-income-taxation-unde_2008_Journal-of-Economic-T.md"
    ],
    "question_context": "The analysis of the optimal income tax under a maximin objective function reveals that with quasilinear preferences and an isoelastic disutility of labor function, the tax pattern depends only on the distribution of skills through $C(w)$. The optimal marginal tax profile is decreasing in skill (hence the SOIC conditions satisfied) and the asymptotic tax rate is zero with the following distributions: (a) truncated Pareto, (b) Weibull, (c) lognormal if condition (26) applies. The tax profile is increasing and strictly concave in income and the average tax rate is single-peaked in income (if $T(\\underline{y})<0$) with all these distributions. Over the skills where an untruncated Pareto distribution applies, the marginal tax rate is constant. When the skill distribution is untruncated Pareto beyond the mode, the average tax rate is either single-peaked or non-decreasing. The results summarized in Proposition 2 assumed that in the case of quasilinear-in-consumption preferences, the elasticity of labor supply, and thus $A(w)$, is constant. In this case, the marginal tax rate will be decreasing in skills over the entire skill distribution if $\\dot{C}(w)<0$, and this will be satisfied for reasonable skill distributions.",
    "category": "CORRECT",
    "question_context_used": "The analysis of the optimal income tax under a maximin objective function reveals that with quasilinear preferences and an isoelastic disutility of labor function, the tax pattern depends only on the distribution of skills through $C(w)$. The optimal marginal tax profile is decreasing in skill (hence the SOIC conditions satisfied) and the asymptotic tax rate is zero with the following distributions: (a) truncated Pareto, (b) Weibull, (c) lognormal if condition (26) applies. The tax profile is increasing and strictly concave in income and the average tax rate is single-peaked in income (if $T(\\underline{y})<0$) with all these distributions. Over the skills where an untruncated Pareto distribution applies, the marginal tax rate is constant. When the skill distribution is untruncated Pareto beyond the mode, the average tax rate is either single-peaked or non-decreasing. The results summarized in Proposition 2 assumed that in the case of quasilinear-in-consumption preferences, the elasticity of labor supply, and thus $A(w)$, is constant. In this case, the marginal tax rate will be decreasing in skills over the entire skill distribution if $\\dot{C}(w)<0$, and this will be satisfied for reasonable skill distributions.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-363",
    "question": "Under what conditions can an outsider successfully manipulate an agent's preferences over lotteries, given the agent's utility representation $V(\\cdot)$?",
    "options": [
      "A) When $V(\\cdot)$ is linear and the manipulator can observe the realization of the agent's initial wealth ${\\pmb x}$",
      "B) When $V(\\cdot)$ is nonlinear and the manipulator constructs acceptable compound manipulations $c$ of the lottery $L$",
      "C) When the certainty equivalent $c(F)$ of the initial lottery $F$ is equal to the expected value of $F$",
      "D) When the manipulator can condition offers on the realization of ${\\pmb x}$ and $V(\\cdot)$ exhibits stochastic dominance reversals"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "“Making Book Against Oneself,” the Independence Axiom, and Nonlinear Utility Theory.md"
    ],
    "question_context": "The text discusses the manipulation of an agent's preferences over lotteries by an outsider who knows the agent's utility representation $V(\\cdot)$. Key conditions include the nonlinearity of $V(\\cdot)$ and the ability to construct acceptable compound manipulations. The certainty equivalent $c(F)$ is defined such that $V(G_{c(F)})=V(F)$. The formula for the distribution induced by a simple manipulation is given by:\n\n$$F^{\\prime}=\\sum_{S_{L}}p(n_{0},n)F_{L_{n}}.$$\n\nWhen $V(\\cdot)$ is nonlinear, it is possible that $V(F^{\\prime})<V(F_{L})$ even if $V(F_{L_{n}})\\ge V(F_{L(n)})$ for all $n\\in S_{L}$. This reversal of preference is central to the manipulation framework. The outsider's ability to observe the realization of ${\\pmb x}$ further enables conditioning offers on this information, expanding the scope of possible manipulations.",
    "category": "CORRECT",
    "question_context_used": "The text discusses the manipulation of an agent's preferences over lotteries by an outsider who knows the agent's utility representation $V(\\cdot)$. Key conditions include the nonlinearity of $V(\\cdot)$ and the ability to construct acceptable compound manipulations. The certainty equivalent $c(F)$ is defined such that $V(G_{c(F)})=V(F)$. The formula for the distribution induced by a simple manipulation is given by:\n\n$$F^{\\prime}=\\sum_{S_{L}}p(n_{0},n)F_{L_{n}}.$$\n\nWhen $V(\\cdot)$ is nonlinear, it is possible that $V(F^{\\prime})<V(F_{L})$ even if $V(F_{L_{n}})\\ge V(F_{L(n)})$ for all $n\\in S_{L}$. This reversal of preference is central to the manipulation framework. The outsider's ability to observe the realization of ${\\pmb x}$ further enables conditioning offers on this information, expanding the scope of possible manipulations.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-268",
    "question": "In the context of dynamically complete markets with agents having the same intertemporal discount factor, which of the following statements are true regarding wealth accumulation and prediction accuracy?",
    "options": [
      "A) Agents who maximize the expected logarithm of next period's outcomes will eventually hold all wealth.",
      "B) Agents who do not eventually make accurate predictions are driven out of the market by those who do.",
      "C) Agents with incorrect beliefs but higher risk aversion may accumulate more wealth than those with correct beliefs.",
      "D) Convergence to rational expectations occurs only if all agents have identical utility functions."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Sandroni-MarketsFavorAgents-2000.md"
    ],
    "question_context": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism... Blume and Easley (1992) show that if agents have the same savings rule, those who maximize the expected logarithm of next period's outcomes will eventually hold all wealth (i.e., are 'most prosperous'). However, if no agent adopts this rule, then the most prosperous are not necessarily those who make the most accurate predictions. In this paper, it is shown that, among agents who have the same intertemporal discount factor (and who choose savings endogenously), the most prosperous are those who make accurate predictions. Hence, convergence to rational expectations obtains because agents who make inaccurate predictions are driven out of the market. The intuition behind these results is that if agents' choices are restricted to investment decisions, then agents may optimally choose to allocate small amounts of wealth to events they believe likely to occur. However, if agents can maximize over both savings and investment decisions, then they will, to some extent, allocate relatively more wealth to future events they believe more likely to occur than to future events they believe less likely to occur.",
    "category": "CORRECT",
    "question_context_used": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism... Blume and Easley (1992) show that if agents have the same savings rule, those who maximize the expected logarithm of next period's outcomes will eventually hold all wealth (i.e., are 'most prosperous'). However, if no agent adopts this rule, then the most prosperous are not necessarily those who make the most accurate predictions. In this paper, it is shown that, among agents who have the same intertemporal discount factor (and who choose savings endogenously), the most prosperous are those who make accurate predictions. Hence, convergence to rational expectations obtains because agents who make inaccurate predictions are driven out of the market. The intuition behind these results is that if agents' choices are restricted to investment decisions, then agents may optimally choose to allocate small amounts of wealth to events they believe likely to occur. However, if agents can maximize over both savings and investment decisions, then they will, to some extent, allocate relatively more wealth to future events they believe more likely to occur than to future events they believe less likely to occur.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2200",
    "question": "In the context of Pigouvian taxation on short-term liabilities, which of the following statements accurately describe the policy maker's objectives and outcomes?",
    "options": [
      "A) The Pigouvian tax is designed to make intermediaries pay for bailouts in full.",
      "B) The fee η* is set to equate the marginal value of private consumption before the state is revealed to the expected future marginal utility of consumption.",
      "C) The Pigouvian tax corrects the distortion caused by bailouts by increasing the cost of resources paid out before the state is revealed.",
      "D) The policy maker chooses τ according to a standard Samuelson condition, equating the marginal values of public and private consumption in the state where no bailouts occur."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Bailouts and Financial Fragility.md"
    ],
    "question_context": "The Pigouvian tax on intermediaries’ short-term liabilities is introduced to correct the distortion caused by the bailout policy. The fee η increases an intermediary’s cost of using resources paid out before the state is revealed, as shown in equation (24). The policy maker sets η* to make the marginal value an intermediary assigns to these resources equal to the expected future marginal utility of consumption, as shown in equation (25): \nu^{\\prime}(\tau)=\\mu_{\\alpha}. The revenue raised by η* is not intended to cover the bailout package but to impose a marginal cost on each intermediary equal to the external effect its actions have in state β. The allocation vector c* under the Pigouvian regime shows that ρ* is less than under the bailouts regime but more than under a no-bailouts policy, indicating partial correction of the distortion.",
    "category": "INCORRECT",
    "question_context_used": "The Pigouvian tax on intermediaries’ short-term liabilities is introduced to correct the distortion caused by the bailout policy. The fee η increases an intermediary’s cost of using resources paid out before the state is revealed, as shown in equation (24). The policy maker sets η* to make the marginal value an intermediary assigns to these resources equal to the expected future marginal utility of consumption, as shown in equation (25): \nu^{\\prime}(\tau)=\\mu_{\\alpha}. The revenue raised by η* is not intended to cover the bailout package but to impose a marginal cost on each intermediary equal to the external effect its actions have in state β. The allocation vector c* under the Pigouvian regime shows that ρ* is less than under the bailouts regime but more than under a no-bailouts policy, indicating partial correction of the distortion.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1692",
    "question": "Which of the following factors are most likely to influence the observed pollution levels at upstream and downstream international river stations, according to the GEMS data analysis?",
    "options": [
      "A) Strategic reporting incentives by participating countries",
      "B) The deoxygenation rate (k) of the river",
      "C) The GDP per capita of the upstream country",
      "D) The flow rate of the river"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Sigman-InternationalSpilloversWater-2002.md"
    ],
    "question_context": "The analysis of GEMS data highlights several factors influencing pollution levels at international river stations. Upstream and downstream stations may exhibit different pollution levels due to strategic reporting by participating countries. Downstream countries might report higher pollution to show victimization, while upstream countries may underreport to hide free-riding. Additionally, the deoxygenation rate (k), which measures the speed of natural attenuation of BOD, and the river flow rate, which affects dilution rates, are critical determinants of observed pollution levels. The formula for the deoxygenation rate is given by $k$, calculated from river flow and temperature data using nonlinear functions from scientific literature. The flow rate is reported in m³/sec, with international rivers showing dramatically greater flow than domestic rivers, influencing pollution concentration through dilution effects.",
    "category": "CORRECT",
    "question_context_used": "The analysis of GEMS data highlights several factors influencing pollution levels at international river stations. Upstream and downstream stations may exhibit different pollution levels due to strategic reporting by participating countries. Downstream countries might report higher pollution to show victimization, while upstream countries may underreport to hide free-riding. Additionally, the deoxygenation rate (k), which measures the speed of natural attenuation of BOD, and the river flow rate, which affects dilution rates, are critical determinants of observed pollution levels. The formula for the deoxygenation rate is given by $k$, calculated from river flow and temperature data using nonlinear functions from scientific literature. The flow rate is reported in m³/sec, with international rivers showing dramatically greater flow than domestic rivers, influencing pollution concentration through dilution effects.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3614",
    "question": "In the stochastic prisoner's dilemma experiment described, which of the following factors are critical for supporting cooperation in period 0 according to the theoretical predictions?",
    "options": [
      "A) The specific prisoner's dilemma (A or B) played in period 0",
      "B) The probability p_A that game A is played in period 1",
      "C) The discount rate δ applied to future payoffs",
      "D) The subjects' prior experience with the game"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Kloosterman - 2020 - Cooperation in stochastic games A prisoner's dilemma experiment.md"
    ],
    "question_context": "The experimental game consists of playing one of two prisoner’s dilemmas, named A and B, each period over an indefinite horizon t=0,1,2,… The first treatment variable is which of the two games is played in period 0. The second treatment variable is the probability that A is played in period 1, and takes the values p_A=.25 and p_A=.75. The game realized in period 1 is played in all future periods. This stochastic process implies equilibria with the following features that provide testable hypotheses. First, cooperation is possible in period 0 when B is more likely in future periods p_A=.25, and the dilemma played in period 0 (the first treatment variable) is irrelevant. Second, cooperation is possible in periods t≥1 when B is realized in period 1, but not when A is realized in period 1. The main result for period 0 behavior confirms the theoretical effect of beliefs about the future on cooperation in period 0, although only after subjects gain experience with the game. That is, there is a similar amount of period 0 cooperation in the two treatments where B is more likely in future periods and a similar amount in the two treatments where A is more likely in future periods and the former is greater than the latter.",
    "category": "INCORRECT",
    "question_context_used": "The experimental game consists of playing one of two prisoner’s dilemmas, named A and B, each period over an indefinite horizon t=0,1,2,… The first treatment variable is which of the two games is played in period 0. The second treatment variable is the probability that A is played in period 1, and takes the values p_A=.25 and p_A=.75. The game realized in period 1 is played in all future periods. This stochastic process implies equilibria with the following features that provide testable hypotheses. First, cooperation is possible in period 0 when B is more likely in future periods p_A=.25, and the dilemma played in period 0 (the first treatment variable) is irrelevant. Second, cooperation is possible in periods t≥1 when B is realized in period 1, but not when A is realized in period 1. The main result for period 0 behavior confirms the theoretical effect of beliefs about the future on cooperation in period 0, although only after subjects gain experience with the game. That is, there is a similar amount of period 0 cooperation in the two treatments where B is more likely in future periods and a similar amount in the two treatments where A is more likely in future periods and the former is greater than the latter.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2785",
    "question": "Under Assumptions M.1-M.6, H.1, H.2, F.1, F.2 and T.1, which of the following conditions are necessary for the existence of a non-trivial MCI equilibrium?",
    "options": [
      "A) The correspondence $S_{i}$ has a non-empty closed graph for all agents",
      "B) The maximizing offer sets $\\gamma_{i}(\\widetilde{Z}_{i},d)$ are singletons for all agents",
      "C) The functions $\\phi_{i}$ are continuous on $K^{\\varepsilon_{1},\\varepsilon_{2}}\\times R_{+}^{r}$",
      "D) The opportunity sets $S_{i}(\\widetilde{Z}_{i},d)$ contain non-empty open sets for all agents"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Unemployment Equilibrium with Myopic Complete Information.md"
    ],
    "question_context": "Key results from the proofs: Lemma 1 establishes that the correspondence $S_{i}$ has a non-empty closed graph under Assumptions M.1, M.2, H.1 and F.2. Lemma 5 shows that the functions $\\phi_{i}$ are continuous on $K\\times R_{+}$, and Lemma 8 extends this continuity to the restricted domain $K^{\\varepsilon_{1},\\varepsilon_{2}}\\times R_{+}^{r}$. \n\nThe existence of a non-trivial MCI equilibrium is proven in Theorem 1, which relies on these lemmata and the conditions that the maximizing offer sets $\\gamma_{i}(\\widetilde{Z}_{i},d)$ are convex (Lemma 6) and that the opportunity sets $S_{i}(\\widetilde{Z}_{i},d)$ have non-empty interiors (shown in Lemma 2). The equilibrium requires both the structural properties of the correspondences (non-empty closed graph) and the continuity of the value functions $\\phi_{i}$ on appropriately restricted domains.",
    "category": "INCORRECT",
    "question_context_used": "Key results from the proofs: Lemma 1 establishes that the correspondence $S_{i}$ has a non-empty closed graph under Assumptions M.1, M.2, H.1 and F.2. Lemma 5 shows that the functions $\\phi_{i}$ are continuous on $K\\times R_{+}$, and Lemma 8 extends this continuity to the restricted domain $K^{\\varepsilon_{1},\\varepsilon_{2}}\\times R_{+}^{r}$. \n\nThe existence of a non-trivial MCI equilibrium is proven in Theorem 1, which relies on these lemmata and the conditions that the maximizing offer sets $\\gamma_{i}(\\widetilde{Z}_{i},d)$ are convex (Lemma 6) and that the opportunity sets $S_{i}(\\widetilde{Z}_{i},d)$ have non-empty interiors (shown in Lemma 2). The equilibrium requires both the structural properties of the correspondences (non-empty closed graph) and the continuity of the value functions $\\phi_{i}$ on appropriately restricted domains.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1812",
    "question": "In the context of establishing β-mixing with exponential decay rates for the process {Zt′=(Yt,ln(Dt))}, which of the following conditions are sufficient to imply ϕ-irreducibility, aperiodicity, and the Lyapunov criteria?",
    "options": [
      "A) AI.1–AI.5",
      "B) AII.1–AII.4",
      "C) Both AI.1–AI.5 and AII.1–AII.4",
      "D) Neither AI.1–AI.5 nor AII.1–AII.4"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "A-new-semiparametric-spatial-model-for-panel-time-_2001_Journal-of-Econometr.md"
    ],
    "question_context": "To establish β-mixing with exponential decay rates for {Zt′=(Yt,ln(Dt))}, we need to show (e.g. Doukhan, 1994): (i) ϕ-irreducible and aperiodic; (ii) Lyapunov criteria or 'drift condition'. Either AI.1–AI.5 or AII.1–AII.4 are sufficient to imply (i) and (ii). The proof involves verifying these conditions to ensure the process meets the necessary criteria for β-mixing with the desired decay rates. The conditions AI.1–AI.5 and AII.1–AII.4 are derived from theoretical frameworks that ensure the process satisfies the required properties for mixing.",
    "category": "INCORRECT",
    "question_context_used": "To establish β-mixing with exponential decay rates for {Zt′=(Yt,ln(Dt))}, we need to show (e.g. Doukhan, 1994): (i) ϕ-irreducible and aperiodic; (ii) Lyapunov criteria or 'drift condition'. Either AI.1–AI.5 or AII.1–AII.4 are sufficient to imply (i) and (ii). The proof involves verifying these conditions to ensure the process meets the necessary criteria for β-mixing with the desired decay rates. The conditions AI.1–AI.5 and AII.1–AII.4 are derived from theoretical frameworks that ensure the process satisfies the required properties for mixing.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2728",
    "question": "In the context of the household Euler equation derived from the consumption model with liquidity constraints, which of the following statements are correct regarding the interpretation and estimation of the parameters?",
    "options": [
      "A) The term $\\ln(1+\\lambda_{h,t-2})$ represents the normalized Lagrange multiplier on the borrowing constraint and must be proxied using external credit access data to avoid bias in volatility estimates.",
      "B) The precautionary savings term $\\sigma_{h,t-2}^{2}$ is directly observable from household consumption data without requiring any approximation.",
      "C) The Euler equation residuals $\\varsigma_{h,t}$ contain both expectational errors and preference shocks, but their mean is zero conditional on year $t-4$ information for consistent estimation.",
      "D) Non-linear GMM estimation of the exact Euler equation is preferred over linear estimation of the second-order approximation, despite measurement error concerns."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Consumption_Volatility,_Liquidity_Constraints_and_Household_Welfare.md"
    ],
    "question_context": "The household Euler equation is derived from a standard unitary, incomplete markets consumption model, incorporating endogenous income and binding liquidity constraints. The key equation is:\n\n$$\n\\mathbb{E}_{t-2}\\bigg[(1+r_{h,t})e^{\\Delta\\theta_{h,t}-2\\delta_{h}}\\bigg(\\frac{C_{h,t}}{C_{h,t-2}}\\bigg)^{-\\gamma}\\bigg](1+\\lambda_{h,t-2})=1,\n$$\n\nwhere $\\lambda_{h,t-2}$ is the normalized Lagrange multiplier on the borrowing constraint. For unconstrained households, $\\lambda_{h,t-2}=0$. The equation is log-linearized to account for expectational errors and higher-order terms, leading to:\n\n$$\n\\begin{array}{l}{\\displaystyle\\Delta\\ln C_{h,t}=\\frac{1}{\\gamma}[\\ln(1+{r_{h,t}})-2\\delta_{h}+\\Delta\\theta_{h,t}]}\\ {\\displaystyle+\\frac{1}{\\gamma}\\left[\\ln(1+{\\lambda_{h,t-2}})+\\frac{1}{2}\\sigma_{h,t-2}^{2}-R_{h}-e_{h,t}^{R}+\\nu_{h,t}-e_{h,t}\\right].}\\end{array}\n$$\n\nThe term $\\ln(1+\\lambda_{h,t-2})$ is proxied using SCF data on credit denials to avoid bias, while $\\sigma_{h,t-2}^{2}$ represents the conditional variance of expectational errors. The residuals $\\varsigma_{h,t}$ combine expectational errors and preference shocks, with zero mean conditional on year $t-4$ information for consistent IV estimation.",
    "category": "CORRECT",
    "question_context_used": "The household Euler equation is derived from a standard unitary, incomplete markets consumption model, incorporating endogenous income and binding liquidity constraints. The key equation is:\n\n$$\n\\mathbb{E}_{t-2}\\bigg[(1+r_{h,t})e^{\\Delta\\theta_{h,t}-2\\delta_{h}}\\bigg(\\frac{C_{h,t}}{C_{h,t-2}}\\bigg)^{-\\gamma}\\bigg](1+\\lambda_{h,t-2})=1,\n$$\n\nwhere $\\lambda_{h,t-2}$ is the normalized Lagrange multiplier on the borrowing constraint. For unconstrained households, $\\lambda_{h,t-2}=0$. The equation is log-linearized to account for expectational errors and higher-order terms, leading to:\n\n$$\n\\begin{array}{l}{\\displaystyle\\Delta\\ln C_{h,t}=\\frac{1}{\\gamma}[\\ln(1+{r_{h,t}})-2\\delta_{h}+\\Delta\\theta_{h,t}]}\\ {\\displaystyle+\\frac{1}{\\gamma}\\left[\\ln(1+{\\lambda_{h,t-2}})+\\frac{1}{2}\\sigma_{h,t-2}^{2}-R_{h}-e_{h,t}^{R}+\\nu_{h,t}-e_{h,t}\\right].}\\end{array}\n$$\n\nThe term $\\ln(1+\\lambda_{h,t-2})$ is proxied using SCF data on credit denials to avoid bias, while $\\sigma_{h,t-2}^{2}$ represents the conditional variance of expectational errors. The residuals $\\varsigma_{h,t}$ combine expectational errors and preference shocks, with zero mean conditional on year $t-4$ information for consistent IV estimation.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2404",
    "question": "Based on the analysis of institutional measures and their impact on exploration drilling, which of the following statements are supported by the empirical results?",
    "options": [
      "A) A one standard deviation increase in the Polity measure leads to a 113%-139% increase in the number of wells.",
      "B) The ANRR measure shows a statistically significant jump in the intensive margin in all four models for 75 km bandwidths.",
      "C) The extensive margin shows a 6 percentage point difference in drilling likelihood between the left and right sides of the border.",
      "D) Investors respond more strongly to institutional quality in developing countries than in high income countries."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Cust和Harding - 2020 - INSTITUTIONS AND THE LOCATION OF OIL EXPLORATION.md"
    ],
    "question_context": "The upper panel of Table 5 presents Poisson estimates using the Polity IV scores in columns (1)–(4) and the ANRR measure in columns (5)–(8). Both measures produce positive and significant coefficients on the institutions border dummy, suggesting an increase in the number of wells moving from the left to the right hand side of the border of between $18\\%$ and $22\\%$ for Polity and $14\\%{-}31\\%$ for ANRR. The lower part of Table 5 presents second stage estimates for the Polity measure (this is not available for the ANRR-measure, as it is a dummy that is collinear with the border dummy). A jump in Polity of one standard deviation (7.782), increases the number of wells by $113\\%{-}139\\%$ . This is somewhat higher than the $77\\%{-94\\%}$ for an analogous one standard deviation increase in the Freedom House measure. We conclude that the result that exploration drilling responds to institutional quality holds up also for these alternative measures of democracy.33 In the Online Appendix, we also present estimates for the Polity subindices for Democracy, Autocracy and Constraints on the executive branch of government. Again the results consistently show that the side of the border with better institutional quality receives more exploration drilling, and that investors prefer to drill in the more democratic, and less autocratic, neighbour. Table 6 presents descriptive statistics for the two margins. The extensive margin shows that $33\\%$ of the distance-bins on the left hand side have at least one well within $75~\\mathrm{km}$ from the border. The corresponding figure for the right hand side is $39\\%$ , a statistically significant difference of 6 percentage points. Also for the intensive margin there is a statistically significant difference, with on average 1.63 wells per distance-bin on the left hand side and 3.15 on the right hand side. We use the log of the number of exploration wells as the intensive margin. Figure 5 reveals clear jumps in the extensive margin (left panel) and the intensive margin (right panel) at the borders, with more drilling on the side of the border with relatively better institutions.",
    "category": "INCORRECT",
    "question_context_used": "The upper panel of Table 5 presents Poisson estimates using the Polity IV scores in columns (1)–(4) and the ANRR measure in columns (5)–(8). Both measures produce positive and significant coefficients on the institutions border dummy, suggesting an increase in the number of wells moving from the left to the right hand side of the border of between $18\\%$ and $22\\%$ for Polity and $14\\%{-}31\\%$ for ANRR. The lower part of Table 5 presents second stage estimates for the Polity measure (this is not available for the ANRR-measure, as it is a dummy that is collinear with the border dummy). A jump in Polity of one standard deviation (7.782), increases the number of wells by $113\\%{-}139\\%$ . This is somewhat higher than the $77\\%{-94\\%}$ for an analogous one standard deviation increase in the Freedom House measure. We conclude that the result that exploration drilling responds to institutional quality holds up also for these alternative measures of democracy.33 In the Online Appendix, we also present estimates for the Polity subindices for Democracy, Autocracy and Constraints on the executive branch of government. Again the results consistently show that the side of the border with better institutional quality receives more exploration drilling, and that investors prefer to drill in the more democratic, and less autocratic, neighbour. Table 6 presents descriptive statistics for the two margins. The extensive margin shows that $33\\%$ of the distance-bins on the left hand side have at least one well within $75~\\mathrm{km}$ from the border. The corresponding figure for the right hand side is $39\\%$ , a statistically significant difference of 6 percentage points. Also for the intensive margin there is a statistically significant difference, with on average 1.63 wells per distance-bin on the left hand side and 3.15 on the right hand side. We use the log of the number of exploration wells as the intensive margin. Figure 5 reveals clear jumps in the extensive margin (left panel) and the intensive margin (right panel) at the borders, with more drilling on the side of the border with relatively better institutions.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4819",
    "question": "In the context of optimal government policy to correct inefficiencies in competitive equilibria, which of the following statements are true regarding the proposed tax instruments?",
    "options": [
      "A) A proportional bank profit tax can eliminate over-investment by increasing the interest rate, thereby encouraging exit of entrepreneurs with socially inefficient projects.",
      "B) Differential linear income taxation can transform any general m,n problem into the tractable m=1(i) case by eliminating income differentials in PE.",
      "C) Non-linear tax rate schedules are always necessary to achieve the first-best allocation in cases with multiple marginal entrepreneurs and sources of inefficiency.",
      "D) Lump sum transfers are sufficient to satisfy the zero net expected government revenue condition without distorting occupational choices."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Asymmetric_Information,_Occupational_Choice_and_Government_Policy.md"
    ],
    "question_context": "The government aims to minimize occupational misallocations (SI) arising from free choice and credit rationing. Key instruments include a proportional tax on gross bank profits (tπ) and linear taxation of occupational incomes (γj, tj). The optimal policy for case m=1(i) involves a positive proportional bank profit tax and lump sum subsidies to restore first-best allocation. For general m,n problems, a differential linear income tax system (tEN*=0, tPE*=1, γEN*>0, γPE*<0) transforms the problem to m=1(i). The expected profits of banks under instrument 1 are given by: \n\n$$(1-t_{\\pi})\\big\\{\\bar{\\hat{p}}(D)D-[1-\\bar{\\hat{p}}(D)]c_{r}^{\\hat{p}}\\big\\}-R,$$\n\nwhere $\\bar{\\phi}(D)=\\int_{\\sigma_{\\mathrm{EN}}}\\rho(x)\\mathrm{d}G(x)/N$. The government's budget must balance, ensuring zero net expected fiscal effect.",
    "category": "CORRECT",
    "question_context_used": "The government aims to minimize occupational misallocations (SI) arising from free choice and credit rationing. Key instruments include a proportional tax on gross bank profits (tπ) and linear taxation of occupational incomes (γj, tj). The optimal policy for case m=1(i) involves a positive proportional bank profit tax and lump sum subsidies to restore first-best allocation. For general m,n problems, a differential linear income tax system (tEN*=0, tPE*=1, γEN*>0, γPE*<0) transforms the problem to m=1(i). The expected profits of banks under instrument 1 are given by: \n\n$$(1-t_{\\pi})\\big\\{\\bar{\\hat{p}}(D)D-[1-\\bar{\\hat{p}}(D)]c_{r}^{\\hat{p}}\\big\\}-R,$$\n\nwhere $\\bar{\\phi}(D)=\\int_{\\sigma_{\\mathrm{EN}}}\\rho(x)\\mathrm{d}G(x)/N$. The government's budget must balance, ensuring zero net expected fiscal effect.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-307",
    "question": "In the context of equilibrium bidding strategies and reserve prices, which of the following statements are correct?",
    "options": [
      "A) A deviation to a bid below the incremental bidding threshold weakly decreases the variable $k^{s}$ and increases the expected number of bidders.",
      "B) The optimal reserve price $r^{*}$ is strictly greater than $p_{1}^{*}$ and maximizes seller revenue without affecting buyer surplus.",
      "C) Social welfare $W_{k}$ decreases strictly in $k$ for $k \\geq 2$ due to the inefficiency introduced by higher reserve prices.",
      "D) Expected buyer surplus $B_{k}$ is strictly positive for $k \\geq 2$ and decreases strictly in $k$ for $k \\geq 2$."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Che-EntryDeterrenceDynamic-2016.md"
    ],
    "question_context": "Key formula: The expected buyer surplus in 1-equilibrium is given by $B_{1} = E[\\nu_{2:2}^{1} - \\nu_{1:2}^{1}] - N_{1}c = 0$. For $k \\geq 2$, the expected buyer surplus is $B_{k} = E[\\nu_{k:k}^{k} - \\nu_{k-1:k}^{k}] - N_{k}c > 0$. The optimal reserve price $r^{*}$ is defined by $\\overline{\\nu} - r^{*} - \\int_{r^{*}}^{\\overline{\\nu}} F(\\nu)d\\nu = c$, ensuring $r^{*} > p_{1}^{*}$. Social welfare $W_{k}$ is given by $W_{k} = E[\\nu_{k:k}^{k}] - N_{k}c$ and decreases strictly in $k$ for $k \\geq 2$.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The expected buyer surplus in 1-equilibrium is given by $B_{1} = E[\\nu_{2:2}^{1} - \\nu_{1:2}^{1}] - N_{1}c = 0$. For $k \\geq 2$, the expected buyer surplus is $B_{k} = E[\\nu_{k:k}^{k} - \\nu_{k-1:k}^{k}] - N_{k}c > 0$. The optimal reserve price $r^{*}$ is defined by $\\overline{\\nu} - r^{*} - \\int_{r^{*}}^{\\overline{\\nu}} F(\\nu)d\\nu = c$, ensuring $r^{*} > p_{1}^{*}$. Social welfare $W_{k}$ is given by $W_{k} = E[\\nu_{k:k}^{k}] - N_{k}c$ and decreases strictly in $k$ for $k \\geq 2$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2506",
    "question": "In Danziger and Gottschalk's analysis of poverty dynamics, which of the following factors are empirically shown to have significant differential impacts on poverty reduction?",
    "options": [
      "A) Cyclical economic expansions with tight labor markets",
      "B) Secular increases in educational attainment",
      "C) Short-term fluctuations in commodity prices",
      "D) Structural changes in welfare policy design"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Danziger-RisingTidesLift-1986.md"
    ],
    "question_context": "The seminal paper examines how both secular trends (like educational expansion) and cyclical fluctuations (like business cycles) interact to shape poverty outcomes. Key finding: While economic growth reduces poverty, its effectiveness depends on whether growth is accompanied by structural changes that improve earnings distribution. The authors demonstrate that poverty responsiveness to growth varies substantially across different policy regimes and labor market conditions. Formula: ΔPoverty = β₁(ΔGDP) + β₂(ΔGini) + ε, where β₁ captures cyclical effects and β₂ reflects secular distributional changes.",
    "category": "CORRECT",
    "question_context_used": "The seminal paper examines how both secular trends (like educational expansion) and cyclical fluctuations (like business cycles) interact to shape poverty outcomes. Key finding: While economic growth reduces poverty, its effectiveness depends on whether growth is accompanied by structural changes that improve earnings distribution. The authors demonstrate that poverty responsiveness to growth varies substantially across different policy regimes and labor market conditions. Formula: ΔPoverty = β₁(ΔGDP) + β₂(ΔGini) + ε, where β₁ captures cyclical effects and β₂ reflects secular distributional changes.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2533",
    "question": "In the context of the Delegation Game (NCD) and EMU with Delegation (EMUD), which of the following statements are correct regarding the degree of conservatism of bankers?",
    "options": [
      "A) The degree of conservatism under NCD decreases as shocks become more correlated.",
      "B) Under EMUD, if shocks are perfectly antisymmetric, the optimal choice of conservative banker ensures zero average inflation.",
      "C) In the unique subgame-perfect equilibrium of NCD, bankers are less conservative than the representative government.",
      "D) Delegation is always counterproductive under NCD when shocks are symmetric."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Choice_of_‘Conservative’_Bankers_in_Open_Economies_Monetary_Regime_Options_for_Europe.md"
    ],
    "question_context": "The delegation game consists of two stages: in stage 1 the two representative governments simultaneously and non-cooperatively choose the degree of conservatism of the two banks, $b$ and $b^{*}$. In stage 2, the independent banks simultaneously choose inflation rates in a non-cooperative second stage game. They cannot precommit in this choice so that the resulting domestic inflation rate is given by $\\left(\\mathrm{~I~}7\\right)$ with $b$ or $b^{*}$ replacing $b_{m}$. \n\nPRoPosrTIoN 1. There exists a unique subgame-perfect equilibrium of the regime NCD. In this unique equilibrium bankers are conservative, i.e. $b=b^{*}<b_{m}$. PROPosITION 2. The degree of conservatism under NCD decreases as shocks become more correlated, i.e. $d b/d\\rho=d b^{*}/d\\rho>0$. Under EMU the ECB can only respond to the aggregate shock and not the difference. If shocks are perfectly antisymmetric there is no role for stabilisation policy: the optimal choice of conservative banker simply ensures zero average inflation.",
    "category": "INCORRECT",
    "question_context_used": "The delegation game consists of two stages: in stage 1 the two representative governments simultaneously and non-cooperatively choose the degree of conservatism of the two banks, $b$ and $b^{*}$. In stage 2, the independent banks simultaneously choose inflation rates in a non-cooperative second stage game. They cannot precommit in this choice so that the resulting domestic inflation rate is given by $\\left(\\mathrm{~I~}7\\right)$ with $b$ or $b^{*}$ replacing $b_{m}$. \n\nPRoPosrTIoN 1. There exists a unique subgame-perfect equilibrium of the regime NCD. In this unique equilibrium bankers are conservative, i.e. $b=b^{*}<b_{m}$. PROPosITION 2. The degree of conservatism under NCD decreases as shocks become more correlated, i.e. $d b/d\\rho=d b^{*}/d\\rho>0$. Under EMU the ECB can only respond to the aggregate shock and not the difference. If shocks are perfectly antisymmetric there is no role for stabilisation policy: the optimal choice of conservative banker simply ensures zero average inflation.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2828",
    "question": "Which of the following statements accurately describe key contributions or challenges in the early Bayesian econometrics literature as discussed in the text?",
    "options": [
      "A) Marschak demonstrated that repeated Bayesian updating with independent observations diminishes the role of prior beliefs.",
      "B) Fisher's Bayesian approach showed that loss functions for prediction and policy simulation yield identical coefficient estimates.",
      "C) Dreze formalized SEM identification by incorporating prior information on structural parameters via Bayesian methods.",
      "D) Tiao and Zellner's sequential Bayesian method used subsample estimates as priors to avoid explicit prior specification."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Qin - 1996 - Bayesian econometrics - The first twenty years.md"
    ],
    "question_context": "Marschak (1950) illustrated how the ratio of two Bayes' formulae could modify a priori degrees of belief, showing that with many independent observations, \"the role of a priori degrees of belief...diminishes, overshadowed by the results of observations\" (1954). Fisher (1962) derived distinct coefficient estimates for prediction vs. policy simulation by imposing prior densities aligned with different loss functions, though cautioned about subjective probability interpretations. \n\nKey formula: Dreze's Bayesian SEM identification framework formalized a priori information via joint prior density $p(\\theta)$ of structural parameters, where identifiability required $\\int p(\\theta|data)d\\theta$ to concentrate around true values. Tiao and Zellner (1964a) proposed splitting data into subsamples $D_1, D_2$ and using $p(\\theta|D_1)$ as prior for $p(\\theta|D_2, D_1) \\propto p(D_2|\\theta)p(\\theta|D_1)$ to circumvent explicit prior specification.",
    "category": "CORRECT",
    "question_context_used": "Marschak (1950) illustrated how the ratio of two Bayes' formulae could modify a priori degrees of belief, showing that with many independent observations, \"the role of a priori degrees of belief...diminishes, overshadowed by the results of observations\" (1954). Fisher (1962) derived distinct coefficient estimates for prediction vs. policy simulation by imposing prior densities aligned with different loss functions, though cautioned about subjective probability interpretations. \n\nKey formula: Dreze's Bayesian SEM identification framework formalized a priori information via joint prior density $p(\\theta)$ of structural parameters, where identifiability required $\\int p(\\theta|data)d\\theta$ to concentrate around true values. Tiao and Zellner (1964a) proposed splitting data into subsamples $D_1, D_2$ and using $p(\\theta|D_1)$ as prior for $p(\\theta|D_2, D_1) \\propto p(D_2|\\theta)p(\\theta|D_1)$ to circumvent explicit prior specification.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1840",
    "question": "In the context of monopolist durability choice under moral hazard and arbitrage constraints, which of the following statements are correct?",
    "options": [
      "A) Moral hazard problems arise because the monopolist cannot monitor usage or maintenance by renters, leading to accelerated deterioration.",
      "B) Nonlinear sales contracts with fixed entry fees and cost-based purchases are always more profitable than ordinary sales contracts.",
      "C) Arbitrage in secondary markets can undermine the profitability of optimal two-part sales contracts.",
      "D) Stochastic deterioration of durables is necessary to model the moral hazard problem accurately."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Rust-OptimalKillMarket-1986.md"
    ],
    "question_context": "The moral hazard problem arises because the monopolist will typically be unable to monitor the level of usage or maintenance performed by renters creating an incentive to abuse rented equipment. The resulting accelerated rate of deterioration of the rental equipment may cause rental contracts to be less profitable than sales contracts. While this moral hazard story may explain why we do not typically observe rental contracts for durable goods, it still doesn't explain why ordinary sales contracts are the predominant means of providing durable services to consumers. It is easy to show that the following nonlinear sales contract dominates an ordinary sales contract: the consumer pays a fixed entry fee $F$ in exchange for the right to purchase new durables at cost of production $C(\\lambda)$. Under an optimal nonlinear contract the monopolist chooses socially optimal durability $\\lambda^{*}$ and obtains the first-best or maximal profit of $F=R-M(\\gamma(C(\\lambda^{*}),\\lambda^{*}))/(1-\\beta)-\\underline{{P}}.$ A standard answer to this question is that with active secondary markets, arbitrage by consumers would make such contracts unprofitable. A single consumer could pay the fixed fee $F_{\\mathrm{{:}}}$ buy new durables at cost, and sell these durables at a mark-up to other consumers in order to avoid paying the fixed fee $F.$ Allowing for stochastic deterioration is necessary in order to capture the moral hazard problem. If deterioration is a deterministic function of usage and maintenance decisions, the moral hazard problem evaporates since the monopolist can exactly infer the level of usage by observing the physical condition of the durable at the beginning and end of the lease.",
    "category": "CORRECT",
    "question_context_used": "The moral hazard problem arises because the monopolist will typically be unable to monitor the level of usage or maintenance performed by renters creating an incentive to abuse rented equipment. The resulting accelerated rate of deterioration of the rental equipment may cause rental contracts to be less profitable than sales contracts. While this moral hazard story may explain why we do not typically observe rental contracts for durable goods, it still doesn't explain why ordinary sales contracts are the predominant means of providing durable services to consumers. It is easy to show that the following nonlinear sales contract dominates an ordinary sales contract: the consumer pays a fixed entry fee $F$ in exchange for the right to purchase new durables at cost of production $C(\\lambda)$. Under an optimal nonlinear contract the monopolist chooses socially optimal durability $\\lambda^{*}$ and obtains the first-best or maximal profit of $F=R-M(\\gamma(C(\\lambda^{*}),\\lambda^{*}))/(1-\\beta)-\\underline{{P}}.$ A standard answer to this question is that with active secondary markets, arbitrage by consumers would make such contracts unprofitable. A single consumer could pay the fixed fee $F_{\\mathrm{{:}}}$ buy new durables at cost, and sell these durables at a mark-up to other consumers in order to avoid paying the fixed fee $F.$ Allowing for stochastic deterioration is necessary in order to capture the moral hazard problem. If deterioration is a deterministic function of usage and maintenance decisions, the moral hazard problem evaporates since the monopolist can exactly infer the level of usage by observing the physical condition of the durable at the beginning and end of the lease.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1590",
    "question": "Based on the experimental programme's impact on student track choices, which of the following statements are empirically supported by the data?",
    "options": [
      "A) The intervention significantly increased the proportion of students applying to two-year vocational programmes as their first choice.",
      "B) The treatment had no significant effect on the proportion of students enrolled in three-year academic programmes one year after the intervention.",
      "C) The programme induced a decrease in grade repetition primarily among students who initially included it in their choice list.",
      "D) The intervention led to a significant reduction in dropout rates by shifting students towards apprenticeship programmes."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Adjusting_Your_Dreams_High_School_Plans_and_Dropout_Behaviour.md"
    ],
    "question_context": "The experimental programme reveals a significant impact on preselected students' track choices, particularly increasing applications to two-year vocational programmes (+4.9 percentage points, a 30% increase) and reducing focus on three-year vocational programmes or grade repetition (−5.5 percentage points). Key formula: The linear probability model controls for school dummies, academic marks, and demographic factors (N=1,130), with standard errors clustered at the school level. One year post-intervention, enrolment in two-year programmes nearly doubled (+3.3 percentage points), while grade repetition decreased by 28% (−3.5 percentage points). The intervention also reduced dropout rates by 43% (−3.7 percentage points) by increasing apprenticeship uptake. The data supports that principals altered students' perceptions of two-year programmes and apprenticeship viability without affecting three-year academic programme enrolment.",
    "category": "INCORRECT",
    "question_context_used": "The experimental programme reveals a significant impact on preselected students' track choices, particularly increasing applications to two-year vocational programmes (+4.9 percentage points, a 30% increase) and reducing focus on three-year vocational programmes or grade repetition (−5.5 percentage points). Key formula: The linear probability model controls for school dummies, academic marks, and demographic factors (N=1,130), with standard errors clustered at the school level. One year post-intervention, enrolment in two-year programmes nearly doubled (+3.3 percentage points), while grade repetition decreased by 28% (−3.5 percentage points). The intervention also reduced dropout rates by 43% (−3.7 percentage points) by increasing apprenticeship uptake. The data supports that principals altered students' perceptions of two-year programmes and apprenticeship viability without affecting three-year academic programme enrolment.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4200",
    "question": "In the computation of posterior odds ratios under the given regression model, which of the following statements are correct regarding the likelihood function and prior distributions?",
    "options": [
      "A) The unconstrained likelihood function for B and Σ is proportional to |Σ|^(-T/2) exp[-1/2 tr(νG + (B - B̂)'X'X(B - B̂))Σ^(-1)].",
      "B) The prior over Σ is assumed to be a diffuse prior proportional to |Σ|^(-2).",
      "C) The prior over β is assumed to be a Minnesota prior, F_N^P(β|β_M, σV_M).",
      "D) The integral of interest in the unrestricted case involves F_N^P(β|β̂, V) and F_IW^3(Σ, V), where V = Σ ⊗ (X'X)^(-1)."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "06-Aggregate Consumption and the Predictability of Asset Returns.md"
    ],
    "question_context": "The regression model is written as Y = XB + e, where an arbitrary set of restrictions can be expressed as Rβ = r, with β = vec(B). Under the assumption that the rows of e are independent and trivariate normal with covariance matrix Σ, the unconstrained likelihood function for B and Σ is given by:\n\n$$\n\\mathcal{L}(B, \\Sigma|Y, X) \\propto |\\Sigma|^{-T/2} \\exp\\left[-\\frac{1}{2} \\mathrm{tr}(\\nu G + (B - \\hat{B})'X'X(B - \\hat{B}))\\Sigma^{-1}\\right],\n$$\n\nwhere \\(\\hat{B} = (X'X)^{-1}X'Y\\), \\(\\nu = T - K\\), K is the number of estimated coefficients, and \\(G = \\nu^{-1}(Y - X\\hat{B})'(Y - X\\hat{B})\\). The prior over Σ is the usual diffuse prior \\(\\pi(\\Sigma) \\propto |\\Sigma|^{-2}\\), and the prior over β is the Minnesota prior \\(\\pi(\\beta) = F_N^P(\\beta|\\beta_M, \\sigma V_M)\\). The integral of interest in the unrestricted case involves \\(F_N^P(\\beta|\\hat{\\beta}, V)\\) and \\(F_{IW}^3(\\Sigma, V)\\), where \\(V = \\Sigma \\otimes (X'X)^{-1}\\).",
    "category": "CORRECT",
    "question_context_used": "The regression model is written as Y = XB + e, where an arbitrary set of restrictions can be expressed as Rβ = r, with β = vec(B). Under the assumption that the rows of e are independent and trivariate normal with covariance matrix Σ, the unconstrained likelihood function for B and Σ is given by:\n\n$$\n\\mathcal{L}(B, \\Sigma|Y, X) \\propto |\\Sigma|^{-T/2} \\exp\\left[-\\frac{1}{2} \\mathrm{tr}(\\nu G + (B - \\hat{B})'X'X(B - \\hat{B}))\\Sigma^{-1}\\right],\n$$\n\nwhere \\(\\hat{B} = (X'X)^{-1}X'Y\\), \\(\\nu = T - K\\), K is the number of estimated coefficients, and \\(G = \\nu^{-1}(Y - X\\hat{B})'(Y - X\\hat{B})\\). The prior over Σ is the usual diffuse prior \\(\\pi(\\Sigma) \\propto |\\Sigma|^{-2}\\), and the prior over β is the Minnesota prior \\(\\pi(\\beta) = F_N^P(\\beta|\\beta_M, \\sigma V_M)\\). The integral of interest in the unrestricted case involves \\(F_N^P(\\beta|\\hat{\\beta}, V)\\) and \\(F_{IW}^3(\\Sigma, V)\\), where \\(V = \\Sigma \\otimes (X'X)^{-1}\\).",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3392",
    "question": "In the context of fixed effects models with maximum likelihood estimation, which of the following statements are true regarding the consistency of estimators?",
    "options": [
      "A) The ML estimator of β in the linear regression model is consistent as N→∞ for fixed T.",
      "B) The ML estimator of σ² in the linear regression model is consistent as N→∞ for fixed T.",
      "C) The ML estimator of β in the logit model with T=2 is consistent as N→∞.",
      "D) The ML estimator of β in the autoregressive model with T=2 converges to (β-1)/2 as N→∞."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Analysis of Covariance with Qualitative Data.md"
    ],
    "question_context": "The linear regression model assumes $y_{i t}=\\beta^{\\prime}x_{i t}+\\alpha_{i}+\\varepsilon_{i t}$, where $\\varepsilon_{i t}$ is i.i.d. $N(0,\\sigma^{2})$. The ML estimator of $\\beta$ can be obtained from a regression of $y_{i t}-{\\bar{y}}_{i}$ on $\\pmb{x}_{i t}-\\bar{\\pmb{x}}_{i}$, and is consistent as $N\\to\\infty$ for fixed $T$. However, the ML estimator of $\\sigma^{2}$ does not adjust for degrees of freedom, leading to $\\operatorname{plim}_{N\\to\\infty}{\\hat{\\sigma}}^{2}={\\frac{T-1}{T}}\\sigma^{2}$, making it inconsistent for fixed $T$. In the autoregressive model $y_{i t}=\\beta y_{i,t-1}+\\alpha_{i}+\\varepsilon_{i t}$ with $T=2$, the ML estimator of $\\beta$ converges to $(\\beta-1)/2$ as $N\\to\\infty$ due to the correlation between $\\varepsilon_{1}$ and $y_{1}$. For the logit model with $T=2$, the ML estimator of $\\beta$ satisfies $\\hat{\\beta}=2\\beta$ almost surely as $N\\rightarrow\\infty$, demonstrating inconsistency.",
    "category": "INCORRECT",
    "question_context_used": "The linear regression model assumes $y_{i t}=\\beta^{\\prime}x_{i t}+\\alpha_{i}+\\varepsilon_{i t}$, where $\\varepsilon_{i t}$ is i.i.d. $N(0,\\sigma^{2})$. The ML estimator of $\\beta$ can be obtained from a regression of $y_{i t}-{\\bar{y}}_{i}$ on $\\pmb{x}_{i t}-\\bar{\\pmb{x}}_{i}$, and is consistent as $N\\to\\infty$ for fixed $T$. However, the ML estimator of $\\sigma^{2}$ does not adjust for degrees of freedom, leading to $\\operatorname{plim}_{N\\to\\infty}{\\hat{\\sigma}}^{2}={\\frac{T-1}{T}}\\sigma^{2}$, making it inconsistent for fixed $T$. In the autoregressive model $y_{i t}=\\beta y_{i,t-1}+\\alpha_{i}+\\varepsilon_{i t}$ with $T=2$, the ML estimator of $\\beta$ converges to $(\\beta-1)/2$ as $N\\to\\infty$ due to the correlation between $\\varepsilon_{1}$ and $y_{1}$. For the logit model with $T=2$, the ML estimator of $\\beta$ satisfies $\\hat{\\beta}=2\\beta$ almost surely as $N\\rightarrow\\infty$, demonstrating inconsistency.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1401",
    "question": "In the context of the Global Profit-Maximisation (GPM) model with indigenisation, which of the following statements are correct regarding the parent company's regulation of subsidiary exports?",
    "options": [
      "A) An increase in the host country's share in the subsidiary (E_h/E_s) leads to a decrease in the subsidiary's exports (X_h) to maintain GPM equilibrium.",
      "B) The parent company's net global profits (π_ng) are maximized when the first-order condition dπ_ng/dX_h = 0 is satisfied and the second-order condition d²π_ng/dX_h² < 0 holds.",
      "C) The subsidiary's exports (X_h) have no effect on the parent company's profits (π_p) due to product differentiation.",
      "D) The optimal level of indigenisation (E_hg) in the GPM model is always higher than in the Alternative Scenario (AS) model, given the same initial profit levels."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Multinational-firms--global-strategies--host-country_1983_Journal-of-Develop.md"
    ],
    "question_context": "The GPM model analyzes how indigenisation affects subsidiary exports and host country benefits when the parent company controls the subsidiary to maximize net global profits. Key equations include the profit functions: \n\n$$\\begin{array}{r l r}{\\pi_{p}=\\theta_{p}(X_{h}),}&{{}}&{\\theta_{p}^{\\prime}<0,}\\ {\\pi_{h}=\\theta_{h}(X_{h}),}&{{}}&{\\theta_{h}>0,}\\end{array}$$\n\nand the net global profits:\n\n$$\\pi_{n g}=\\theta_{p}(X_{h})(1-t_{p})+\\theta_{h}(X_{h})(1-t_{h})(1-E_{h}/E_{s}).$$\n\nThe first-order condition for profit maximization is:\n\n$${\\cal E}_{h}/{\\cal E}_{s}=[\\theta_{p}^{\\prime}(1-t_{p})/\\theta_{h}^{\\prime}(1-t_{h})]+1.$$\n\nThis shows that higher host country ownership share (E_h/E_s) reduces subsidiary exports (X_h) to rebalance the GPM condition. The optimal indigenisation level (E_hg) is derived by equating marginal benefits and costs, and it is generally lower in the GPM model compared to the AS model when initial profits are equal.",
    "category": "CORRECT",
    "question_context_used": "The GPM model analyzes how indigenisation affects subsidiary exports and host country benefits when the parent company controls the subsidiary to maximize net global profits. Key equations include the profit functions: \n\n$$\\begin{array}{r l r}{\\pi_{p}=\\theta_{p}(X_{h}),}&{{}}&{\\theta_{p}^{\\prime}<0,}\\ {\\pi_{h}=\\theta_{h}(X_{h}),}&{{}}&{\\theta_{h}>0,}\\end{array}$$\n\nand the net global profits:\n\n$$\\pi_{n g}=\\theta_{p}(X_{h})(1-t_{p})+\\theta_{h}(X_{h})(1-t_{h})(1-E_{h}/E_{s}).$$\n\nThe first-order condition for profit maximization is:\n\n$${\\cal E}_{h}/{\\cal E}_{s}=[\\theta_{p}^{\\prime}(1-t_{p})/\\theta_{h}^{\\prime}(1-t_{h})]+1.$$\n\nThis shows that higher host country ownership share (E_h/E_s) reduces subsidiary exports (X_h) to rebalance the GPM condition. The optimal indigenisation level (E_hg) is derived by equating marginal benefits and costs, and it is generally lower in the GPM model compared to the AS model when initial profits are equal.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3120",
    "question": "In the context of Proposition 8, which of the following conditions ensure that the value function v and the optimal intensity level n are MR-increasing in beliefs?",
    "options": [
      "A) The static payoff frontier π is MR-increasing",
      "B) The map c(f) is convex",
      "C) The posterior belief vector updates the prior p after a realization of a signal with likelihood λ_i in state i",
      "D) The optimal action a_j is best in state θ_j and the best payoff for each action π_{a_j}^{θ_j} is increasing in the action j"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Moscarini-OptimalLevelExperimentation-2001.md"
    ],
    "question_context": "PROPOsITION 8: Assume that the static payoff frontier π is MR-increasing: π(p) > π(q) if p ≻ q. Then the value function v and the optimal intensity level n (when positive) are also MR-increasing in p: if p ≻ q then v(p) > v(q), and n(p) > n(q) for p, q ∈ D. The proof uses a policy improvement argument, leveraging the fact that MR is preserved under Bayesian updating (Lemma 3). Consider a three-state, three-action example, with payoffs in the three states (3, 4, 5), (2, 5, 8), and (1, 3, 9). Any MR upward shift in beliefs raises the static expected payoff to the corresponding optimal action; hence, the assumptions of Proposition 8 are met, and the value v, and thus experimentation level, are both MR-monotone in beliefs too.",
    "category": "INCORRECT",
    "question_context_used": "PROPOsITION 8: Assume that the static payoff frontier π is MR-increasing: π(p) > π(q) if p ≻ q. Then the value function v and the optimal intensity level n (when positive) are also MR-increasing in p: if p ≻ q then v(p) > v(q), and n(p) > n(q) for p, q ∈ D. The proof uses a policy improvement argument, leveraging the fact that MR is preserved under Bayesian updating (Lemma 3). Consider a three-state, three-action example, with payoffs in the three states (3, 4, 5), (2, 5, 8), and (1, 3, 9). Any MR upward shift in beliefs raises the static expected payoff to the corresponding optimal action; hence, the assumptions of Proposition 8 are met, and the value v, and thus experimentation level, are both MR-monotone in beliefs too.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-675",
    "question": "Based on the data description and statistical analysis from West Point, which of the following statements are correct regarding the relationship between company-level math SAT scores and academic performance?",
    "options": [
      "A) The variance in company-level math SAT scores is significantly greater than the variance in company-level academic GPAs.",
      "B) The 75-25 differential in math SAT scores shows minimal variation across companies, indicating uniform ability distributions.",
      "C) The mean math SAT score across companies is approximately 640 points, with a standard deviation of about 10 points.",
      "D) The company-level academic GPA has a smaller standard deviation than the company-level math grade, suggesting less variability in overall academic performance compared to math-specific performance."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "The Effects of Peer Group Heterogeneity on the Production of Human Capital at West Point.md"
    ],
    "question_context": "Panel A presents individual-level and company-level summary statistics for plebe-year math grade and academic GPA. Grades range from 0 to 4.3 points, with a mean of 2.70 points (C+) and a standard deviation of 0.23 points across companies for math grades. The plebe-year academic GPA has a similar mean of 2.67 points (C+), but a smaller standard deviation of 0.12 points across companies. Panel B contains summary statistics for measures of the company-level math SAT distribution. The average math SAT score is approximately 640 points with a standard deviation of about 10 points. The variance and the 75-25 differential are used as measures of dispersion in the company math SAT distributions. Despite comparatively equal company-level average SAT scores, some companies have a high concentration of cadets in the tails and others in the middle of the math SAT distributions. The sizeable variation in measures of dispersion, relative to the minimal variation in means across companies, is an important feature of the experimental design in this paper.",
    "category": "INCORRECT",
    "question_context_used": "Panel A presents individual-level and company-level summary statistics for plebe-year math grade and academic GPA. Grades range from 0 to 4.3 points, with a mean of 2.70 points (C+) and a standard deviation of 0.23 points across companies for math grades. The plebe-year academic GPA has a similar mean of 2.67 points (C+), but a smaller standard deviation of 0.12 points across companies. Panel B contains summary statistics for measures of the company-level math SAT distribution. The average math SAT score is approximately 640 points with a standard deviation of about 10 points. The variance and the 75-25 differential are used as measures of dispersion in the company math SAT distributions. Despite comparatively equal company-level average SAT scores, some companies have a high concentration of cadets in the tails and others in the middle of the math SAT distributions. The sizeable variation in measures of dispersion, relative to the minimal variation in means across companies, is an important feature of the experimental design in this paper.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"D\"]}",
      "model_answer": [
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4858",
    "question": "Which of the following statements are true regarding the spaces of generalized functions $D^{*}$ and $S^{*}$?",
    "options": [
      "A) $S^{*}$ is a subset of $D^{*}$ because $D$ is a subset of $S$.",
      "B) A regular generalized function in $S^{*}$ must satisfy a growth condition at infinity, unlike in $D^{*}$.",
      "C) The Dirac delta function is a regular generalized function in both $D^{*}$ and $S^{*}$.",
      "D) Any generalized function in $S^{*}$ can be represented as a generalized derivative of a continuous function."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Zinde-Walsh - 2014 - MEASUREMENT ERROR AND DECONVOLUTION IN SPACES OF GENERALIZED FUNCTIONS.md"
    ],
    "question_context": "The space $D$ consists of infinitely differentiable functions with compact support, while $S$ consists of infinitely differentiable functions that decline at infinity faster than any polynomial. Their dual spaces, $D^{*}$ and $S^{*}$, consist of generalized functions (or distributions). A key relationship is $D \\subset S$, which implies $S^{*} \\subset D^{*}$. Regular generalized functions in $D^{*}$ are locally summable functions, whereas in $S^{*}$, they must additionally satisfy growth conditions like $\\int (1 + t^2)^{-m} |b(t)| dt < \\infty$. The Dirac delta function, defined by $(\\delta, \\psi) = \\psi(0)$, is a singular generalized function in both spaces. An important result by Schwartz states that any generalized function in $S^{*}$ can be represented as a generalized derivative of a continuous function, as shown by the representation $b = \\partial^l c$, where $c(x)$ is continuous and bounded.",
    "category": "CORRECT",
    "question_context_used": "The space $D$ consists of infinitely differentiable functions with compact support, while $S$ consists of infinitely differentiable functions that decline at infinity faster than any polynomial. Their dual spaces, $D^{*}$ and $S^{*}$, consist of generalized functions (or distributions). A key relationship is $D \\subset S$, which implies $S^{*} \\subset D^{*}$. Regular generalized functions in $D^{*}$ are locally summable functions, whereas in $S^{*}$, they must additionally satisfy growth conditions like $\\int (1 + t^2)^{-m} |b(t)| dt < \\infty$. The Dirac delta function, defined by $(\\delta, \\psi) = \\psi(0)$, is a singular generalized function in both spaces. An important result by Schwartz states that any generalized function in $S^{*}$ can be represented as a generalized derivative of a continuous function, as shown by the representation $b = \\partial^l c$, where $c(x)$ is continuous and bounded.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4164",
    "question": "In the context of testing for ARCH and GARCH disturbances in linear regression models, which of the following statements are supported by the empirical results and theoretical discussion?",
    "options": [
      "A) The LBS test demonstrates superior small-sample power compared to the LM test.",
      "B) The LM test for ARCH disturbances proposed by Engle (1982) adequately accounts for the one-sided nature of the testing problem.",
      "C) The new test based on the sum of scores typically has better power than standard versions of the LM test.",
      "D) All three tests (LBS, LM, and the new test) are highly sensitive to the magnitude of the β parameters in GARCH disturbances."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "LEE和KING - 1993 - A locally most mean powerful based score test for arch and garch regression disturbances.md"
    ],
    "question_context": "The paper discusses the application of the LBs test to an empirical example involving weekly gold and silver prices, using the model: $$\\begin{array}{r l r}{\\mathtt{R}_{\\mathtt{t}}}&{{}=}&{\\mathtt{b}_{1}+\\mathtt{b}_{2}\\mathtt{r}_{\\mathtt{t}}+\\mathtt{\\varepsilon}_{\\mathtt{t}}}\\end{array}$$ where $R_{t}$ is the one period holding yield on the metal and $r_{t}$ is the risk-free rate of return. The LM test for ARCH(2) disturbances yielded a test statistic value of 3.721, suggesting no rejection of the null hypothesis, while the LBS test statistic was 2.628, suggesting clear rejection. The conflict in results might be explained by the superior small-sample power of the LBS test. The paper also highlights that the new test based on the sum of scores typically has better power than standard versions of the LM test, while all three tests seem insensitive to the magnitude of the $\\pmb{\\beta}$ parameters in GARCH disturbances.",
    "category": "CORRECT",
    "question_context_used": "The paper discusses the application of the LBs test to an empirical example involving weekly gold and silver prices, using the model: $$\\begin{array}{r l r}{\\mathtt{R}_{\\mathtt{t}}}&{{}=}&{\\mathtt{b}_{1}+\\mathtt{b}_{2}\\mathtt{r}_{\\mathtt{t}}+\\mathtt{\\varepsilon}_{\\mathtt{t}}}\\end{array}$$ where $R_{t}$ is the one period holding yield on the metal and $r_{t}$ is the risk-free rate of return. The LM test for ARCH(2) disturbances yielded a test statistic value of 3.721, suggesting no rejection of the null hypothesis, while the LBS test statistic was 2.628, suggesting clear rejection. The conflict in results might be explained by the superior small-sample power of the LBS test. The paper also highlights that the new test based on the sum of scores typically has better power than standard versions of the LM test, while all three tests seem insensitive to the magnitude of the $\\pmb{\\beta}$ parameters in GARCH disturbances.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1138",
    "question": "In the context of information economics and market signaling, which of the following mechanisms are explicitly discussed in the provided research papers?",
    "options": [
      "A) Self-Reported Signaling in asymmetric information environments",
      "B) Information Spillover in multi-good adverse selection models",
      "C) Judicial Mechanism Design for enforcing contracts",
      "D) Equilibrium Screening leading to categorical inequality"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "American Economic Journal Microeconomics 153 front matter.md"
    ],
    "question_context": "The research papers listed cover advanced topics in microeconomic theory, particularly focusing on information asymmetry, signaling, and market design. Key contributions include Thomas Jungbauer and Michael Waldman's work on 'Self-Reported Signaling', which examines how agents disclose private information strategically. Bingchao Huangfu and Heng Liu analyze 'Information Spillover in Multi-Good Adverse Selection', where the presence of multiple goods complicates the adverse selection problem. Mogens Fosgerau, Rajiv Sethi, and Jörgen Weibull investigate 'Equilibrium Screening and Categorical Inequality', demonstrating how screening mechanisms can perpetuate systemic disparities. These papers collectively address high-stakes theoretical challenges in modern microeconomics.",
    "category": "CORRECT",
    "question_context_used": "The research papers listed cover advanced topics in microeconomic theory, particularly focusing on information asymmetry, signaling, and market design. Key contributions include Thomas Jungbauer and Michael Waldman's work on 'Self-Reported Signaling', which examines how agents disclose private information strategically. Bingchao Huangfu and Heng Liu analyze 'Information Spillover in Multi-Good Adverse Selection', where the presence of multiple goods complicates the adverse selection problem. Mogens Fosgerau, Rajiv Sethi, and Jörgen Weibull investigate 'Equilibrium Screening and Categorical Inequality', demonstrating how screening mechanisms can perpetuate systemic disparities. These papers collectively address high-stakes theoretical challenges in modern microeconomics.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4134",
    "question": "In the context of the generalized mixture model, which of the following statements are supported by the empirical results regarding the relationship between trading volume and return volatility?",
    "options": [
      "A) The information arrival process is calibrated strictly on the behavior of volume and has no effect on return volatility.",
      "B) The sensitivity to news interacts with the news arrivals to contribute to movements in return volatility.",
      "C) The smoothed information arrival process accounts for a significant fraction of the explanatory power for return volatility.",
      "D) The inclusion of the second factor in the generalized mixture model completely disconnects the behavior of volume from that of volatility."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "A-generalized-bivariate-mixture-model-for-stock-price-v_2001_Journal-of-Econ.md"
    ],
    "question_context": "The generalized mixture model posits that the conditional heterogeneity in the first and second-order moments of the returns and volume given their past values is solely due to the dynamic behavior of the information arrival process and traders’ sensitivity to new information. The empirical results indicate that the smoothed information arrival process accounts for 17.7 (IBM) and 17 (Kodak) percent of the explanatory power for return volatility, while the sensitivity to news, interacting with the news arrivals, increases the explanatory power to 28.3 (IBM) and 31.1 (Kodak) percent. The results also show that the inclusion of the second factor in the generalized mixture model does not completely disconnect the behavior of volume from that of volatility, as the information arrival process is calibrated strictly on the behavior of volume. Key formula: $|r_{t}|=a_{0}+a_{1}\\sigma_{t}(\\cdot)+u_{t}$.",
    "category": "CORRECT",
    "question_context_used": "The generalized mixture model posits that the conditional heterogeneity in the first and second-order moments of the returns and volume given their past values is solely due to the dynamic behavior of the information arrival process and traders’ sensitivity to new information. The empirical results indicate that the smoothed information arrival process accounts for 17.7 (IBM) and 17 (Kodak) percent of the explanatory power for return volatility, while the sensitivity to news, interacting with the news arrivals, increases the explanatory power to 28.3 (IBM) and 31.1 (Kodak) percent. The results also show that the inclusion of the second factor in the generalized mixture model does not completely disconnect the behavior of volume from that of volatility, as the information arrival process is calibrated strictly on the behavior of volume. Key formula: $|r_{t}|=a_{0}+a_{1}\\sigma_{t}(\\cdot)+u_{t}$.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-92",
    "question": "In the context of PCDID estimation for ITET and ATET, which of the following statements are supported by the simulation results?",
    "options": [
      "A) PCDID consistently outperforms other estimators in terms of bias and standard deviation across all scenarios.",
      "B) DID-2wfe performs well in scenarios with nonstationary factors with drift.",
      "C) MC-NNM is biased heavily upward in scenarios with stationary factors with break and nonstationary factors with drift.",
      "D) The bootstrap-based inference procedures (b-t and b-se) perform similarly to the asymptotic approach (Asym) in ATET inference."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "21-The PCDID Approach_ Difference-in-Differences When Trends Are Potentially Unparallel and Stoch.md"
    ],
    "question_context": "The DGPs follow the general form $y_{i t}~=~\\Delta_{i t}1_{\\{i\\in E\\}}1_{\\{t>T_{0i}\\}}~+$ $\\beta_{i}^{'}x_{i t}+\\beta_{i}+\\mu_{j}^{'}f_{t}+\\tilde{\\epsilon}_{i t}$. Three interactive effects scenarios are considered: stationary factors, stationary factors with break, and nonstationary factors with drift. PCDID yields the best performance in terms of bias and standard deviation. DID-2wfe performs worst, followed by DID-trend. MC-NNM outperforms DID methods in scenario A but is biased heavily upward in scenarios B and C. For inference procedures, Asym’s rejection rate is similar to TrueF’s, especially when the sample is large; however, it tends to overreject when $N\\ll T$. b-t performs similarly to Asym, and has better performance when $N\\ll T$. b-se is more conservative compared to b-t.",
    "category": "INCORRECT",
    "question_context_used": "The DGPs follow the general form $y_{i t}~=~\\Delta_{i t}1_{\\{i\\in E\\}}1_{\\{t>T_{0i}\\}}~+$ $\\beta_{i}^{'}x_{i t}+\\beta_{i}+\\mu_{j}^{'}f_{t}+\\tilde{\\epsilon}_{i t}$. Three interactive effects scenarios are considered: stationary factors, stationary factors with break, and nonstationary factors with drift. PCDID yields the best performance in terms of bias and standard deviation. DID-2wfe performs worst, followed by DID-trend. MC-NNM outperforms DID methods in scenario A but is biased heavily upward in scenarios B and C. For inference procedures, Asym’s rejection rate is similar to TrueF’s, especially when the sample is large; however, it tends to overreject when $N\\ll T$. b-t performs similarly to Asym, and has better performance when $N\\ll T$. b-se is more conservative compared to b-t.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1690",
    "question": "In the context of international spillovers and water quality in rivers, which of the following factors are identified as influencing the extent of free-riding behavior among countries?",
    "options": [
      "A) The presence of international treaties pertaining to water withdrawal",
      "B) The inclusion of country effects in the analysis to address unobserved heterogeneity",
      "C) The geographical location of monitoring stations relative to international borders",
      "D) The enforcement mechanisms of international law doctrines"
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Sigman-InternationalSpilloversWater-2002.md"
    ],
    "question_context": "The paper examines the extent of free-riding by comparing water quality in rivers in the presence and absence of spillovers. Data on water quality at monitoring stations on rivers around the world are from the United Nations’ Global Environmental Monitoring System Water Quality Monitoring Project (GEMS/Water). The pollution measure is biochemical oxygen demand (BOD), a widely measured and commonly generated form of water pollution. The results provide some evidence that international spillovers cause degradation, suggesting that cooperation is incomplete at best. Stations upstream of international borders may have pollution levels elevated by 40 percent or more, especially when one excludes borders within the European Union (which appears to have less free-riding). However, the results are very sensitive to the inclusion of country effects, indicating a need for caution in interpreting cross-country comparisons in pollution levels. The paper uses a monitoring station, rather than a country, as the level of analysis. Not all identification comes from country-level differences because countries may have monitoring stations on both domestic and international rivers. As a result, it is possible to include country effects to address unobserved country-level heterogeneity that may be correlated with environmental spillovers.",
    "category": "INCORRECT",
    "question_context_used": "The paper examines the extent of free-riding by comparing water quality in rivers in the presence and absence of spillovers. Data on water quality at monitoring stations on rivers around the world are from the United Nations’ Global Environmental Monitoring System Water Quality Monitoring Project (GEMS/Water). The pollution measure is biochemical oxygen demand (BOD), a widely measured and commonly generated form of water pollution. The results provide some evidence that international spillovers cause degradation, suggesting that cooperation is incomplete at best. Stations upstream of international borders may have pollution levels elevated by 40 percent or more, especially when one excludes borders within the European Union (which appears to have less free-riding). However, the results are very sensitive to the inclusion of country effects, indicating a need for caution in interpreting cross-country comparisons in pollution levels. The paper uses a monitoring station, rather than a country, as the level of analysis. Not all identification comes from country-level differences because countries may have monitoring stations on both domestic and international rivers. As a result, it is possible to include country effects to address unobserved country-level heterogeneity that may be correlated with environmental spillovers.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2921",
    "question": "In the context of exchange rate policy for developing countries, which of the following strategies are theoretically effective in mitigating the impact of exogenous exchange rate realignments on terms of trade?",
    "options": [
      "A) Adopting a basket peg weighted to eliminate terms of trade effects",
      "B) Promoting geographic diversification of trade",
      "C) Diversifying trade beyond particular commodity groupings",
      "D) Implementing large, discrete devaluations under a fixed exchange rate system"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Major-currency-realignments-and-the-terms-of-trad_1982_Journal-of-Developmen.md"
    ],
    "question_context": "For a small developing country that is strictly a price-taker in world markets, the terms of trade are invariant to the choice of exchange rate regime [Lipschitz (1979)] and are unaffected by exchange rate adjustments. Countries that have the market power to affect world market prices can, however, in theory select a set of weights for a basket peg that will eliminate the effect of exchange rate realignments on the terms of trade. Branson and Katseli-Papaefstratiou (1978) have shown, however, that such a basket peg will be at variance with other possible objectives such as minimizing the impact on the trade balance or real income.\n\nIn light of this conclusion, perhaps the most effective thing that developing countries can do to minimize the impact of exogenous exchange rate changes is to promote geographic diversification of trade. However, for a broad range of goods (e.g, minerals, metals, fuels and agricultural products), the prices of which are set in unified commodity markets and quoted in a given currency, such a strategy would be to no avail. In this case, diversification of trade beyond particular commodity groupings may be the only effective way to mitigate terms of trade instability.",
    "category": "CORRECT",
    "question_context_used": "For a small developing country that is strictly a price-taker in world markets, the terms of trade are invariant to the choice of exchange rate regime [Lipschitz (1979)] and are unaffected by exchange rate adjustments. Countries that have the market power to affect world market prices can, however, in theory select a set of weights for a basket peg that will eliminate the effect of exchange rate realignments on the terms of trade. Branson and Katseli-Papaefstratiou (1978) have shown, however, that such a basket peg will be at variance with other possible objectives such as minimizing the impact on the trade balance or real income.\n\nIn light of this conclusion, perhaps the most effective thing that developing countries can do to minimize the impact of exogenous exchange rate changes is to promote geographic diversification of trade. However, for a broad range of goods (e.g, minerals, metals, fuels and agricultural products), the prices of which are set in unified commodity markets and quoted in a given currency, such a strategy would be to no avail. In this case, diversification of trade beyond particular commodity groupings may be the only effective way to mitigate terms of trade instability.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4355",
    "question": "Under Assumptions A.1 and A.2, which of the following statements are true regarding the representation of the infinite-dimensional process $\\mathbf{x}_{t}$?",
    "options": [
      "A) $\\mathbf{x}_{t}$ can be decomposed into $\\pmb{\\chi}_{t}$ and $\\pmb{\\xi}_{t}$, where $\\pmb{\\chi}_{t}$ is driven by a finite-dimensional orthonormal white noise.",
      "B) The spectral density matrix $\\pmb{\\Sigma}_{n}^{x}(\\theta)$ has eigenvalues that diverge for all frequencies $\\theta$ in $[-\\pi, \\pi]$.",
      "C) The idiosyncratic component $\\pmb{\\xi}_{t}$ has an essentially bounded supremum eigenvalue $\bar{\\lambda}_{1}^{\\xi}(\\theta)$.",
      "D) The common component $\\pmb{\\chi}_{t}$ and the idiosyncratic component $\\pmb{\\xi}_{t}$ are uncorrelated at all leads and lags."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Dynamic-factor-models-with-infinite-dimensional-factor-s_2015_Journal-of-Eco.md"
    ],
    "question_context": "The GDFM (1.1) can be represented as $\\mathbf{x}_{t}=\\underline{{\\mathbf{b}}}(L)\\mathbf{u}_{t}+\\pmb{\\xi}_{t}$, where $\\underline{{\\mathbf{b}}}(L)$ is an $\\infty\\times q$ matrix of square-summable filters, and $\\mathbf{u}_{t}$ is a $q$-dimensional orthonormal white noise. Under Assumptions A.1 and A.2, the following hold: (i) $\\pmb{\\xi}_{t}$ satisfies Assumption A.1, and $\\bar{\\lambda}_{1}^{\\xi}(\\theta)$ is essentially bounded; (ii) $\\pmb{\\chi}_{t}$ satisfies A.1 and $\\bar{\\lambda}_{q}^{\\chi}(\\theta)=\\infty$ almost everywhere in $\\theta$; (iii) $\\pmb{\\xi}_{t}$ and $\\mathbf{u}_{t-k}$ are uncorrelated for all $t\\in\\mathbb{Z}$ and $k\\in\\mathbb{Z}$; (iv) the components $\\chi_{i t}$ and $\\xi_{i t}$ are unique. The spectral density matrix $\\pmb{\\Sigma}_{n}^{x}(\\theta)$ has eigenvalues $\\lambda_{n j}^{x}(\\theta)$, with $\\bar{\\lambda}_{q}^{x}(\\theta)=\\infty$ for almost all $\\theta$ and $\\bar{\\lambda}_{q+1}^{x}(\\theta)$ essentially bounded.",
    "category": "CORRECT",
    "question_context_used": "The GDFM (1.1) can be represented as $\\mathbf{x}_{t}=\\underline{{\\mathbf{b}}}(L)\\mathbf{u}_{t}+\\pmb{\\xi}_{t}$, where $\\underline{{\\mathbf{b}}}(L)$ is an $\\infty\\times q$ matrix of square-summable filters, and $\\mathbf{u}_{t}$ is a $q$-dimensional orthonormal white noise. Under Assumptions A.1 and A.2, the following hold: (i) $\\pmb{\\xi}_{t}$ satisfies Assumption A.1, and $\\bar{\\lambda}_{1}^{\\xi}(\\theta)$ is essentially bounded; (ii) $\\pmb{\\chi}_{t}$ satisfies A.1 and $\\bar{\\lambda}_{q}^{\\chi}(\\theta)=\\infty$ almost everywhere in $\\theta$; (iii) $\\pmb{\\xi}_{t}$ and $\\mathbf{u}_{t-k}$ are uncorrelated for all $t\\in\\mathbb{Z}$ and $k\\in\\mathbb{Z}$; (iv) the components $\\chi_{i t}$ and $\\xi_{i t}$ are unique. The spectral density matrix $\\pmb{\\Sigma}_{n}^{x}(\\theta)$ has eigenvalues $\\lambda_{n j}^{x}(\\theta)$, with $\\bar{\\lambda}_{q}^{x}(\\theta)=\\infty$ for almost all $\\theta$ and $\\bar{\\lambda}_{q+1}^{x}(\\theta)$ essentially bounded.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2539",
    "question": "According to Beaudry and DiNardo's (1991) model, which of the following conditions are consistent with the insurance contracting with costless mobility scenario?",
    "options": [
      "A) Wages depend only on current labor market conditions",
      "B) Wages depend only on labor market conditions at the beginning of the job",
      "C) Wages depend only on the tightest labor market since the start of the job",
      "D) Wages are downward rigid and increase only when outside options are particularly good"
    ],
    "gold_answer": "C, D",
    "source_files": [
      "Does Wage Persistence Matter for Employment Fluctuations_ Evidence from Displaced Workers.md"
    ],
    "question_context": "Beaudry and DiNardo (henceforth BDN) use the unemployment rate as a measure of labor market tightness and estimate the following model:\n\n$$\n\\begin{array}{r c l}{\\ln(w_{i,t})}&{=}&{U_{t}\\beta_{1}+U_{t_{0}}\\beta_{2}+\\operatorname*{min}\\{U_{h}\\}_{h=t_{0}}^{t}\\beta_{3}+X_{i,t}\\Omega+\\varepsilon_{i,t},}\\end{array}\n$$\n\nwhere $w_{i,t}$ is the wage of individual $i$ in year $t$ $U_{t}$ is the current unemployment rate, $U_{t_{0}}$ is unemployment at the start of the current job, $\\mathrm{min}\\{U_{h}\\}_{h=t_{0}}^{t}$ is the lowest unemployment rate since the start of the job, $X_{i,t}$ is a vector of controls and $\\varepsilon_{i,t}$ the error term. BDN describe how this empirical specification can distinguish between three different model of the labor market. In a simple spot market model without task specific human capital, only current labor market conditions should affect wages andtherefore: $\\beta_{1}<0$ $\\beta_{2}=0$ $\\beta_{3}=0$ . In a model with risk averse workers, risk neutral firms and perfectly enforcible contracts (i.e., workers cannot easily get out of a contract; BDN call this insurance contracting with costly mobility), the optimal contract only depends on labor market conditions at the beginning of the job: $\\beta_{1}=0$ ， $\\beta_{2}<0$ $\\beta_{3}=0$ . Finally, if contracts are only partially enforceable, because workers cannot commit themselves not to leave if they get a better outside offer, the optimal contract (BDN call this insurance contracting with costless mobility) is downward rigid and the wage increases only when the outside options are particularly good. The current wage therefore depends only on the tightest labor market since the start of the job and therefore: $\\beta_{1}=0$ ， $\\beta_{2}=0.$ $\\beta_{3}<0$ . BDN estimate specification (1) with Current Population Survey (CPS) and PSID data and find evidence for the last case.",
    "category": "CORRECT",
    "question_context_used": "Beaudry and DiNardo (henceforth BDN) use the unemployment rate as a measure of labor market tightness and estimate the following model:\n\n$$\n\\begin{array}{r c l}{\\ln(w_{i,t})}&{=}&{U_{t}\\beta_{1}+U_{t_{0}}\\beta_{2}+\\operatorname*{min}\\{U_{h}\\}_{h=t_{0}}^{t}\\beta_{3}+X_{i,t}\\Omega+\\varepsilon_{i,t},}\\end{array}\n$$\n\nwhere $w_{i,t}$ is the wage of individual $i$ in year $t$ $U_{t}$ is the current unemployment rate, $U_{t_{0}}$ is unemployment at the start of the current job, $\\mathrm{min}\\{U_{h}\\}_{h=t_{0}}^{t}$ is the lowest unemployment rate since the start of the job, $X_{i,t}$ is a vector of controls and $\\varepsilon_{i,t}$ the error term. BDN describe how this empirical specification can distinguish between three different model of the labor market. In a simple spot market model without task specific human capital, only current labor market conditions should affect wages andtherefore: $\\beta_{1}<0$ $\\beta_{2}=0$ $\\beta_{3}=0$ . In a model with risk averse workers, risk neutral firms and perfectly enforcible contracts (i.e., workers cannot easily get out of a contract; BDN call this insurance contracting with costly mobility), the optimal contract only depends on labor market conditions at the beginning of the job: $\\beta_{1}=0$ ， $\\beta_{2}<0$ $\\beta_{3}=0$ . Finally, if contracts are only partially enforceable, because workers cannot commit themselves not to leave if they get a better outside offer, the optimal contract (BDN call this insurance contracting with costless mobility) is downward rigid and the wage increases only when the outside options are particularly good. The current wage therefore depends only on the tightest labor market since the start of the job and therefore: $\\beta_{1}=0$ ， $\\beta_{2}=0.$ $\\beta_{3}<0$ . BDN estimate specification (1) with Current Population Survey (CPS) and PSID data and find evidence for the last case.",
    "gold_answer_parsed": [
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2219",
    "question": "In the context of selection bias analysis using the AET framework, which of the following statements are correct regarding the parameter λ and its implications?",
    "options": [
      "A) λ = 1 implies that selection on observables and unobservables is equally strong.",
      "B) A higher λ indicates that selection on unobservables is relatively weaker than selection on observables.",
      "C) The bias in the estimate of α can be entirely attributed to selection if λ equals the ratio of the estimated α to the implied bias.",
      "D) When var(ε) is large relative to var(X′γ), even small shifts in (E(ε|C=1) - E(ε|C=0))/var(ε) can lead to substantial bias in α."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Altonji-UsingSelectionObserved-2008.md"
    ],
    "question_context": "The equation provided by AET relates the strength of selection on unobservables (ε) to selection on observables (X′γ) through the parameter λ. Specifically, the equation is:\n\n$$\n\\frac{E(\\varepsilon|C=1)-E(\\varepsilon|C=0)}{var(\\varepsilon)} = \\lambda \\frac{E(X^{\\prime}\\gamma|C=1)-E(X^{\\prime}\\gamma|C=0)}{var(X^{\\prime}\\gamma)}.\n$$\n\nUnder the null hypothesis that α = 0, the bias in the estimate of α can be approximated by the formula:\n\n$$\n\\operatorname{plim}\\hat{\\alpha} \\simeq \\alpha + \\frac{cov(\\tilde{C},\\varepsilon)}{var(\\tilde{C})} = \\alpha + \\frac{var(C)}{var(\\tilde{C})}[E(\\varepsilon|C=1)-E(\\varepsilon|C=0)].\n$$\n\nThe results in Table 3 show that for 90-day mortality, the ratio of the estimated α to the implied bias is 0.801, meaning that if λ = 0.801, the entire positive effect of C can be attributed to selection bias.",
    "category": "CORRECT",
    "question_context_used": "The equation provided by AET relates the strength of selection on unobservables (ε) to selection on observables (X′γ) through the parameter λ. Specifically, the equation is:\n\n$$\n\\frac{E(\\varepsilon|C=1)-E(\\varepsilon|C=0)}{var(\\varepsilon)} = \\lambda \\frac{E(X^{\\prime}\\gamma|C=1)-E(X^{\\prime}\\gamma|C=0)}{var(X^{\\prime}\\gamma)}.\n$$\n\nUnder the null hypothesis that α = 0, the bias in the estimate of α can be approximated by the formula:\n\n$$\n\\operatorname{plim}\\hat{\\alpha} \\simeq \\alpha + \\frac{cov(\\tilde{C},\\varepsilon)}{var(\\tilde{C})} = \\alpha + \\frac{var(C)}{var(\\tilde{C})}[E(\\varepsilon|C=1)-E(\\varepsilon|C=0)].\n$$\n\nThe results in Table 3 show that for 90-day mortality, the ratio of the estimated α to the implied bias is 0.801, meaning that if λ = 0.801, the entire positive effect of C can be attributed to selection bias.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1540",
    "question": "In the context of reduced-form forecasting of integrated variance, which of the following statements are correct regarding the properties and errors of model-based and reduced-form forecasts?",
    "options": [
      "A) The reduced-form forecast error converges at a stochastic order of O(h), while the model-based forecast error converges at O(√h).",
      "B) An increase in volatility-of-volatility λ improves the performance of reduced-form forecasts relative to model-based forecasts.",
      "C) The total Mean Squared Prediction Error (MSPE) of the model-based forecast includes errors from model misspecification and latent state estimation.",
      "D) The feasible reduced-form forecast's MSPE decomposition includes a term from parameter estimation errors, which is orthogonal to the infeasible error."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Integrated-variance-forecasting--Model-based-vs--re_2011_Journal-of-Economet.md"
    ],
    "question_context": "The reduced-form forecast of the integrated variance IVₜᵗ⁺¹ is a linear projection of IVₜᵗ⁺¹ onto the space generated by its past realizations. The feasible version uses realized variance (RV) as a proxy. The ARMA representation for RV follows from the ARMA representation of integrated variance: \n\n$$\\prod_{i=1}^{p}(1-\\mathrm{e}^{-k_{i}}L)(\\mathrm{RV}_{t}^{t+1}-\\theta)=\\eta_{t+1}(h)-\\sum_{i=1}^{p}\\beta_{i}(h)\\eta_{t+1-i}(h),$$\n\nwhere ηₜ(h) is heteroscedastic white noise. The total MSPE of the feasible reduced-form forecast includes a genuine forecast error and a term from parameter estimation errors, which are orthogonal to the infeasible error. For model-based forecasts under correct specification, the total MSPE contains only the genuine forecast error and the error from latent state estimation, with the latter being approximately:\n\n$$E(\\hat{\\sigma}_{t}^{2}-\\sigma_{t}^{2})^{2}\\approx2\\operatorname{Var}\\sigma_{t}^{2}\\sqrt{\\frac{k h}{\\lambda}}.$$",
    "category": "INCORRECT",
    "question_context_used": "The reduced-form forecast of the integrated variance IVₜᵗ⁺¹ is a linear projection of IVₜᵗ⁺¹ onto the space generated by its past realizations. The feasible version uses realized variance (RV) as a proxy. The ARMA representation for RV follows from the ARMA representation of integrated variance: \n\n$$\\prod_{i=1}^{p}(1-\\mathrm{e}^{-k_{i}}L)(\\mathrm{RV}_{t}^{t+1}-\\theta)=\\eta_{t+1}(h)-\\sum_{i=1}^{p}\\beta_{i}(h)\\eta_{t+1-i}(h),$$\n\nwhere ηₜ(h) is heteroscedastic white noise. The total MSPE of the feasible reduced-form forecast includes a genuine forecast error and a term from parameter estimation errors, which are orthogonal to the infeasible error. For model-based forecasts under correct specification, the total MSPE contains only the genuine forecast error and the error from latent state estimation, with the latter being approximately:\n\n$$E(\\hat{\\sigma}_{t}^{2}-\\sigma_{t}^{2})^{2}\\approx2\\operatorname{Var}\\sigma_{t}^{2}\\sqrt{\\frac{k h}{\\lambda}}.$$",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2270",
    "question": "Under the assumptions of the steady-state price adjustment model with hazard function $\\Lambda(x)$, which of the following statements are correct regarding the invariant distribution of price gaps $f(x)$ and the distribution of price changes $q(\\Delta p)$?",
    "options": [
      "A) The density $f(x)$ must be continuous everywhere and symmetric around $x=0$.",
      "B) The distribution of price changes $q(\\Delta p)$ can have mass points at the boundaries $-X$ and $X$ only if $X$ is finite.",
      "C) The variance of price changes $Var(\\Delta p)$ is inversely proportional to the frequency of price changes $N_a$.",
      "D) The kurtosis of price changes $Kurt(\\Delta p)$ is independent of the shape of the hazard function $\\Lambda(x)$."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Macroeconomics of Sticky Prices with Generalized Hazard Functions.md"
    ],
    "question_context": "The invariant distribution of price gaps, with density $f(x)$, must satisfy several conditions: it must be continuous everywhere, continuously differentiable at $|x|\\in(0,X)$, twice continuously differentiable at all $|x|\\in(0,X)\\setminus\\mathbb{J}$, and symmetric around $x=0$. The distribution of price changes $q(\\Delta p)$ is derived from $f(x)$ and the hazard function $\\Lambda(x)$, and it can have mass points at $-X$ and $X$ only if $X$ is finite. The variance and kurtosis of price changes are given by:\n\n$$\nVar(\\Delta p)=\\frac{2\\left[\\int_{0}^{X}x^{2}\\Lambda(x)f(x)dx-X^{2}\\frac{\\sigma^{2}}{2}f^{\\prime}(X)\\right]}{N_{a}}\n$$\n\n$$\nKurt(\\Delta p)=\\frac{2\\left[\\int_{0}^{X}x^{4}\\Lambda(x)f(x)dx-X^{4}\\frac{\\sigma^{2}}{2}f^{\\prime}(X)\\right]}{N_{a}}\\frac{1}{[Var(\\Delta p)]^{2}}.\n$$\n\nThe shape of the hazard function $\\Lambda(x)$ directly influences these moments, as shown in the analysis of the quadratic hazard function case where $\\Lambda(x)=\\kappa x^{2}$.",
    "category": "INCORRECT",
    "question_context_used": "The invariant distribution of price gaps, with density $f(x)$, must satisfy several conditions: it must be continuous everywhere, continuously differentiable at $|x|\\in(0,X)$, twice continuously differentiable at all $|x|\\in(0,X)\\setminus\\mathbb{J}$, and symmetric around $x=0$. The distribution of price changes $q(\\Delta p)$ is derived from $f(x)$ and the hazard function $\\Lambda(x)$, and it can have mass points at $-X$ and $X$ only if $X$ is finite. The variance and kurtosis of price changes are given by:\n\n$$\nVar(\\Delta p)=\\frac{2\\left[\\int_{0}^{X}x^{2}\\Lambda(x)f(x)dx-X^{2}\\frac{\\sigma^{2}}{2}f^{\\prime}(X)\\right]}{N_{a}}\n$$\n\n$$\nKurt(\\Delta p)=\\frac{2\\left[\\int_{0}^{X}x^{4}\\Lambda(x)f(x)dx-X^{4}\\frac{\\sigma^{2}}{2}f^{\\prime}(X)\\right]}{N_{a}}\\frac{1}{[Var(\\Delta p)]^{2}}.\n$$\n\nThe shape of the hazard function $\\Lambda(x)$ directly influences these moments, as shown in the analysis of the quadratic hazard function case where $\\Lambda(x)=\\kappa x^{2}$.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-665",
    "question": "In the context of economic inequality data analysis, which of the following statements are correct regarding the selection and quality of quintile data from the WIID?",
    "options": [
      "A) When both consumption and income-based data of similar quality are available, consumption-based observations are prioritized.",
      "B) The quality indicator for observations ranges from one (low quality) to four (high quality).",
      "C) The dataset includes 262 observations on the five income quintiles from 88 developing countries.",
      "D) Organizing data into five-year periods is chosen to maximize data frequency from poor and middle-income countries."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Do-elites-benefit-from-democracy-and-foreign-aid-_2010_Journal-of-Developmen.md"
    ],
    "question_context": "The data employed in this paper derive from different survey designs and are of differing quality. As dependent variables, I use information on the shares of national populations belonging to the five income quintiles taken from the World Income Inequality Database (WIID; UNU, 2006), which also provides information of whether the data derive from a survey based on consumption/expenditure or income/earnings. Furthermore, the data include a quality indicator for all observations, ranging from one (high quality) to four (dubious quality). All data are organized in five-year periods with the income inequality data observed at or as close as possible to the end of each period. The specific quintile data entering the dataset have been selected from the WIID based on three criteria: 1) I have used all available data; 2) when more data are available for a period, I use the observation with the highest quality ranking; and 3) when both consumption and income-based data of similar quality are available, I opt for the consumption-based observation.\n\nGiven that all control data are available from other sources, the WIID includes 262 observations on the five income quintiles from 88 developing countries, and an additional seven observations covering only the fifth quintile. In a sample including 88 countries observed in five-year periods over 40 years, having only 262 observations implies that more than half of all potential observations from independent countries are missing. However, this is a reflection of the unfortunate infrequency of economic surveys and lack of proper data in most developing countries. Even a middle-income country with a long tradition of economic surveys and a developed statistical agency such as Argentina only has available data every 2 years on average in the period 1960–2000. Organizing the data into five-year periods is therefore a necessary choice as it is close to the likely limit of how high a data frequency one can expect from a sample of poor and middle-income countries.",
    "category": "INCORRECT",
    "question_context_used": "The data employed in this paper derive from different survey designs and are of differing quality. As dependent variables, I use information on the shares of national populations belonging to the five income quintiles taken from the World Income Inequality Database (WIID; UNU, 2006), which also provides information of whether the data derive from a survey based on consumption/expenditure or income/earnings. Furthermore, the data include a quality indicator for all observations, ranging from one (high quality) to four (dubious quality). All data are organized in five-year periods with the income inequality data observed at or as close as possible to the end of each period. The specific quintile data entering the dataset have been selected from the WIID based on three criteria: 1) I have used all available data; 2) when more data are available for a period, I use the observation with the highest quality ranking; and 3) when both consumption and income-based data of similar quality are available, I opt for the consumption-based observation.\n\nGiven that all control data are available from other sources, the WIID includes 262 observations on the five income quintiles from 88 developing countries, and an additional seven observations covering only the fifth quintile. In a sample including 88 countries observed in five-year periods over 40 years, having only 262 observations implies that more than half of all potential observations from independent countries are missing. However, this is a reflection of the unfortunate infrequency of economic surveys and lack of proper data in most developing countries. Even a middle-income country with a long tradition of economic surveys and a developed statistical agency such as Argentina only has available data every 2 years on average in the period 1960–2000. Organizing the data into five-year periods is therefore a necessary choice as it is close to the likely limit of how high a data frequency one can expect from a sample of poor and middle-income countries.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-834",
    "question": "In the context of common predictable components in regional stock markets, which of the following statements are supported by the empirical findings of Cheung, He, and Ng (1997)?",
    "options": [
      "A) North American instrumental variables can predict excess returns in European and Pacific Rim markets.",
      "B) European instrumental variables have significant predictive power for North American stock returns.",
      "C) The maximal $R^{2}$ procedure is used to test for spurious relationships between asset returns and instruments.",
      "D) Dividend yields and interest rate spreads are among the selected instrumental variables for predictability analysis."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "03-Common Predictable Components in Regional Stock Markets.md"
    ],
    "question_context": "The study employs multivariate methods to analyze the predictability of international stock-market returns, finding significant common predictable components within Pacific, European, and North American markets using region-specific instrumental variables. Key formula: The maximal $R^{2}$ procedure developed by Lo and MacKinlay (LM) is used to determine the maximum predictability in stock returns, allowing for gauging the statistical significance of the maximum predictability with respect to the selected instruments. The study also finds that North American instrumental variables can predict excess returns in other regions, but not vice versa. Instrumental variables include dividend yields, interest rate spreads, and interaction variables, selected based on existing theoretical and empirical studies.",
    "category": "INCORRECT",
    "question_context_used": "The study employs multivariate methods to analyze the predictability of international stock-market returns, finding significant common predictable components within Pacific, European, and North American markets using region-specific instrumental variables. Key formula: The maximal $R^{2}$ procedure developed by Lo and MacKinlay (LM) is used to determine the maximum predictability in stock returns, allowing for gauging the statistical significance of the maximum predictability with respect to the selected instruments. The study also finds that North American instrumental variables can predict excess returns in other regions, but not vice versa. Instrumental variables include dividend yields, interest rate spreads, and interaction variables, selected based on existing theoretical and empirical studies.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4847",
    "question": "In the context of microfinance program evaluations in Thailand and India, which of the following statements are supported by both the empirical microevaluations and the calibrated model predictions?",
    "options": [
      "A) Microfinance leads to a statistically significant increase in entrepreneurial entry rates in both countries",
      "B) The Thai intervention showed a larger percentage increase in consumption compared to the Indian intervention",
      "C) Both studies found that microcredit had a positive impact on business investment probabilities",
      "D) The model predictions for consumption changes fell within the 90% confidence intervals of empirical estimates in both countries"
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Macroeconomics of Microfinance.md"
    ],
    "question_context": "The Thai evaluation involved a government transfer of 1 million baht of seed money to selected rural villages, with loans constituting one-third of total credit in the median village. Key results showed: a $56\\%$ increase in business profits, a $47\\%$ increase in probability of making discrete investments, and a $20\\%$ increase in per-capita consumption. The model predicted milder effects: +8\\% consumption, +3 percentage points entrepreneurship, and +41\\% investment probability.\n\nThe Indian study examined a randomized expansion of microfinance with higher interest rates (12\\% spread). Empirical results showed: +1 percentage point entrepreneurship (from 5\\% baseline), +140\\% business investment, and +17\\% durable goods purchases. The model matched the entrepreneurship increase but predicted smaller effects: +1\\% consumption, +1 percentage point entrepreneurship, and +9\\% business investment. Both studies found wide confidence intervals, with model predictions generally falling within these ranges except for some investment measures.",
    "category": "INCORRECT",
    "question_context_used": "The Thai evaluation involved a government transfer of 1 million baht of seed money to selected rural villages, with loans constituting one-third of total credit in the median village. Key results showed: a $56\\%$ increase in business profits, a $47\\%$ increase in probability of making discrete investments, and a $20\\%$ increase in per-capita consumption. The model predicted milder effects: +8\\% consumption, +3 percentage points entrepreneurship, and +41\\% investment probability.\n\nThe Indian study examined a randomized expansion of microfinance with higher interest rates (12\\% spread). Empirical results showed: +1 percentage point entrepreneurship (from 5\\% baseline), +140\\% business investment, and +17\\% durable goods purchases. The model matched the entrepreneurship increase but predicted smaller effects: +1\\% consumption, +1 percentage point entrepreneurship, and +9\\% business investment. Both studies found wide confidence intervals, with model predictions generally falling within these ranges except for some investment measures.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"C\", \"D\"]\n}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4568",
    "question": "Based on the circulation data of 'Econometric Theory', which of the following statements accurately describe the distribution patterns?",
    "options": [
      "A) The total number of copies distributed has decreased from the 12-month average to the single issue nearest to the filing date.",
      "B) The percentage of paid distribution is higher in the single issue nearest to the filing date compared to the 12-month average.",
      "C) The majority of the paid distribution is through mailed outside-county subscriptions.",
      "D) The free or nominal rate distribution is consistently zero across all categories in both the 12-month average and the single issue."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "ECT_volume_36_issue_6_Cover_and_Back_matter.md"
    ],
    "question_context": "The circulation data for 'Econometric Theory' reveals that the average number of copies each issue during the preceding 12 months was 180, with 129 being paid distribution and 2 being free or nominal rate distribution. The single issue published nearest to the filing date had 163 total copies, with 129 being paid distribution and 0 being free or nominal rate distribution. The percentage paid for the 12-month average was 98.47%, while the single issue was 100%. The majority of the paid distribution comes from mailed outside-county subscriptions (45 in the 12-month average, 47 in the single issue) and paid distribution outside the mails (84 in the 12-month average, 82 in the single issue).",
    "category": "INCORRECT",
    "question_context_used": "The circulation data for 'Econometric Theory' reveals that the average number of copies each issue during the preceding 12 months was 180, with 129 being paid distribution and 2 being free or nominal rate distribution. The single issue published nearest to the filing date had 163 total copies, with 129 being paid distribution and 0 being free or nominal rate distribution. The percentage paid for the 12-month average was 98.47%, while the single issue was 100%. The majority of the paid distribution comes from mailed outside-county subscriptions (45 in the 12-month average, 47 in the single issue) and paid distribution outside the mails (84 in the 12-month average, 82 in the single issue).",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1948",
    "question": "In the two-aggregate model of trade union effects, which of the following statements are correct regarding terms of trade and welfare implications?",
    "options": [
      "A) If B ceases to trade with C, A's terms of trade deteriorate by the height of its tariff on imports from B.",
      "B) If A ceases to trade with C, A's terms of trade improve by the height of the union tariff on X.",
      "C) A welfare gain from union is more likely if A's pre-union trade volume is larger than B's.",
      "D) Bilateral balance between A and B is a necessary condition for overall trade balance in a two-good, three-country world."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Welfare_Effects_of_Customs_Union_An_Anatomy.md"
    ],
    "question_context": "In the case in which B ceases to trade with C, relative prices faced by consumers and producers in A will be unchanged as a result of union if the union adopts as its tariff upon Y that rate which was imposed by A prior to union. However, this would represent a deterioration in A's terms of trade. Prior to union, whilst the domestic price in A of a unit of imports is I+tAY the opportunity cost to the economy is unity since the tariff revenue is returned to A's consumers in some form. Post-union the opportunity cost of a unit of Y supplied by B coincides with the domestic price in A since B suppliers pay no duty, whereas the opportunity cost of a unit of Y supplied by C remains at unity as long as the union follows the practice of allotting to each member nation the tariff revenue paid by its consumers. Hence, in the case in which B ceases to trade with C: A experiences a deterioration in its terms of trade equal to the height of its tariff on that proportion of its imports supplied by B. Analogously, if A ceases to trade with C then A experiences an improvement in its terms of trade equal to the height of the union tariff on X. There is a significant constraint involved in two-aggregate models which has not been stressed previously. In a two-good, three country world, if trade is balanced overall then it must as a matter of logic be in bilateral balance. Hence, A and B must be in bilateral balance. Whereas, in a three-good world bilateral balance does not follow from overall balance.",
    "category": "INCORRECT",
    "question_context_used": "In the case in which B ceases to trade with C, relative prices faced by consumers and producers in A will be unchanged as a result of union if the union adopts as its tariff upon Y that rate which was imposed by A prior to union. However, this would represent a deterioration in A's terms of trade. Prior to union, whilst the domestic price in A of a unit of imports is I+tAY the opportunity cost to the economy is unity since the tariff revenue is returned to A's consumers in some form. Post-union the opportunity cost of a unit of Y supplied by B coincides with the domestic price in A since B suppliers pay no duty, whereas the opportunity cost of a unit of Y supplied by C remains at unity as long as the union follows the practice of allotting to each member nation the tariff revenue paid by its consumers. Hence, in the case in which B ceases to trade with C: A experiences a deterioration in its terms of trade equal to the height of its tariff on that proportion of its imports supplied by B. Analogously, if A ceases to trade with C then A experiences an improvement in its terms of trade equal to the height of the union tariff on X. There is a significant constraint involved in two-aggregate models which has not been stressed previously. In a two-good, three country world, if trade is balanced overall then it must as a matter of logic be in bilateral balance. Hence, A and B must be in bilateral balance. Whereas, in a three-good world bilateral balance does not follow from overall balance.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1684",
    "question": "According to Elizabeth Brainerd's analysis of World War II's impact on Soviet marriage markets, which of the following were direct consequences of the severe male scarcity?",
    "options": [
      "A) Increased rates of nonmarital births due to reduced male commitment incentives",
      "B) Higher female bargaining power within marriages due to scarcity-driven competition",
      "C) Decline in female labor force participation as marriage became prioritized",
      "D) Expansion of pronatalist policies that subsidized unmarried motherhood"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Brainerd-LASTINGEFFECTSEX-2017.md"
    ],
    "question_context": "The Soviet Union's sex ratio fell dramatically post-WWII, with the male-to-female ratio for ages 20-29 dropping from 0.91 in 1941 to 0.65 in 1946. Key formula: The probability of male survival (ages 25-34) declined from 0.96 (prewar) to 0.61. This scarcity led to systemic changes: the 1944 Family Code imposed a 'bachelor tax' while providing child benefits for nonmarital births (Heer, 1977), creating an institutional framework where $\\text{Cost of Marriage}_{men} \\gg \\text{Benefits}_{men}$. Brainerd's archival analysis shows these policies amplified demographic effects: urban out-of-wedlock births rose by 12-15% in high-scarcity regions (p<0.01), while registered marriages fell by 8% per 0.1 decrease in sex ratio (SE=0.03).",
    "category": "INCORRECT",
    "question_context_used": "The Soviet Union's sex ratio fell dramatically post-WWII, with the male-to-female ratio for ages 20-29 dropping from 0.91 in 1941 to 0.65 in 1946. Key formula: The probability of male survival (ages 25-34) declined from 0.96 (prewar) to 0.61. This scarcity led to systemic changes: the 1944 Family Code imposed a 'bachelor tax' while providing child benefits for nonmarital births (Heer, 1977), creating an institutional framework where $\\text{Cost of Marriage}_{men} \\gg \\text{Benefits}_{men}$. Brainerd's archival analysis shows these policies amplified demographic effects: urban out-of-wedlock births rose by 12-15% in high-scarcity regions (p<0.01), while registered marriages fell by 8% per 0.1 decrease in sex ratio (SE=0.03).",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2759",
    "question": "In the context of Gibrat's law and the study of firm growth, which of the following statements are supported by the empirical findings?",
    "options": [
      "A) The average proportionate growth rate of very large firms exceeds that of small firms.",
      "B) The variance of growth rates across different size classes is statistically indistinguishable.",
      "C) Births and deaths of firms significantly distort the lognormal distribution of firm sizes.",
      "D) The logarithms of proportionate changes in net assets follow a skewed distribution."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Size and The Growth of Firms.md"
    ],
    "question_context": "The study examines the law of proportionate effect, focusing on firms that survived from 1950/51 to 1959/60. Key findings include: the implied mean ratios of size in 1959/60 to size in 1950/51 were Very Large (2.657), Medium Large (2.335), Smaller Large (1.972), and Small (1.560), indicating higher growth rates for larger firms. Bartlett's test confirmed no significant difference in variances between size classes at the 5% level, permitting the use of Student's t-test for mean comparisons. The distribution of proportionate changes in net assets was skewed, consistent with the lognormal tendency of firm size distributions. Formula: The mean ratio of size is given by \\[ \\text{Mean Ratio} = \\frac{\\text{Size}_{1959/60}}{\\text{Size}_{1950/51}} \\], with values explicitly calculated for each size class.",
    "category": "INCORRECT",
    "question_context_used": "The study examines the law of proportionate effect, focusing on firms that survived from 1950/51 to 1959/60. Key findings include: the implied mean ratios of size in 1959/60 to size in 1950/51 were Very Large (2.657), Medium Large (2.335), Smaller Large (1.972), and Small (1.560), indicating higher growth rates for larger firms. Bartlett's test confirmed no significant difference in variances between size classes at the 5% level, permitting the use of Student's t-test for mean comparisons. The distribution of proportionate changes in net assets was skewed, consistent with the lognormal tendency of firm size distributions. Formula: The mean ratio of size is given by \\[ \\text{Mean Ratio} = \\frac{\\text{Size}_{1959/60}}{\\text{Size}_{1950/51}} \\], with values explicitly calculated for each size class.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2796",
    "question": "In the production economy model with Cobb-Douglas technology and Greenwood-Hercowitz-Huffman utility, which of the following statements are correct regarding the impact of volatility shocks and financial frictions?",
    "options": [
      "A) An increase in volatility reduces the price of debt faster in high volatility scenarios than in low volatility scenarios.",
      "B) The working capital constraint directly causes output loss during default due to financial intermediation.",
      "C) The optimal wage is inversely related to the labor supply elasticity and the working capital constraint.",
      "D) The model assumes that proceeds from the working capital constraint are redistributed lump-sum, mitigating output loss during default."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "TIME-VARYING VOLATILITY, DEFAULT, AND THE SOVEREIGN RISK PREMIUM.md"
    ],
    "question_context": "The production economy is populated by firms that are owned by households and use a Cobb-Douglas technology. Firms maximize profits, pay wages to the workers, and distribute profits to households in a lump sum fashion. Households supply labor, and their utility function is of a Greenwood-Hercowitz-Huffman form. The optimal wage is given by: $$w=\\frac{A(1-\\alpha)h^{-\\alpha}}{1+\\left(\\frac{1}{q(b^{\\prime},y,\\sigma)}-1\\right)\\eta}.$$ The model assumes that the proceeds from the working capital constraint are given back in a lump-sum fashion to the economy, reducing output loss during default. Figure 3 shows that the price of debt decreases faster in high volatility scenarios, suggesting a positive comovement between volatility and spreads during bad times.",
    "category": "INCORRECT",
    "question_context_used": "The production economy is populated by firms that are owned by households and use a Cobb-Douglas technology. Firms maximize profits, pay wages to the workers, and distribute profits to households in a lump sum fashion. Households supply labor, and their utility function is of a Greenwood-Hercowitz-Huffman form. The optimal wage is given by: $$w=\\frac{A(1-\\alpha)h^{-\\alpha}}{1+\\left(\\frac{1}{q(b^{\\prime},y,\\sigma)}-1\\right)\\eta}.$$ The model assumes that the proceeds from the working capital constraint are given back in a lump-sum fashion to the economy, reducing output loss during default. Figure 3 shows that the price of debt decreases faster in high volatility scenarios, suggesting a positive comovement between volatility and spreads during bad times.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-327",
    "question": "In the context of regression discontinuity design (RDD) applied to summer school effectiveness, which of the following conditions are essential for identifying the causal impact of summer school on student achievement?",
    "options": [
      "A) Students just below and just above the pass-fail cutoff must have nearly identical observed characteristics",
      "B) The probability of attending summer school must exhibit a sharp discontinuity at the pass-fail cutoff",
      "C) The treatment effect must be homogeneous across all student subgroups",
      "D) The baseline test scores must be perfectly normally distributed"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Mandatory-summer-school-and-student-achievement_2008_Journal-of-Econometrics.md"
    ],
    "question_context": "The paper employs a regression discontinuity design (RDD) to estimate the causal impact of summer school on student achievement. Key to this identification strategy is the assumption that all student characteristics affecting achievement vary smoothly with baseline test scores, allowing the difference in outcome scores at the pass–fail cutoff to be attributed to summer school attendance. The design relies on two critical features: (1) a sharp discontinuity in the probability of attending summer school as a function of test scores (students barely failing are much more likely to attend than those barely passing), and (2) the observed characteristics of students near the cutoff being nearly identical, supporting the claim that outcome differences are due to summer school. The formula for the RDD estimator is derived from comparing achievement outcomes of students just below and just above the cutoff, under the smoothness assumption: $\\Delta = E[Y|X = c^+] - E[Y|X = c^-]$, where $c$ is the cutoff.",
    "category": "CORRECT",
    "question_context_used": "The paper employs a regression discontinuity design (RDD) to estimate the causal impact of summer school on student achievement. Key to this identification strategy is the assumption that all student characteristics affecting achievement vary smoothly with baseline test scores, allowing the difference in outcome scores at the pass–fail cutoff to be attributed to summer school attendance. The design relies on two critical features: (1) a sharp discontinuity in the probability of attending summer school as a function of test scores (students barely failing are much more likely to attend than those barely passing), and (2) the observed characteristics of students near the cutoff being nearly identical, supporting the claim that outcome differences are due to summer school. The formula for the RDD estimator is derived from comparing achievement outcomes of students just below and just above the cutoff, under the smoothness assumption: $\\Delta = E[Y|X = c^+] - E[Y|X = c^-]$, where $c$ is the cutoff.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3702",
    "question": "In the context of the Hybrid SVAR model identification, which of the following statements are correct regarding the structural parameters and estimation methodology?",
    "options": [
      "A) The matrices Γ and Ψ, representing intermediate input-use and capital-use, are quantified using U.S. BEA estimates of input-use and capital flow tables.",
      "B) The standard SVAR framework can identify all N² potential contemporaneous linkages amongst the N structural shocks without additional information.",
      "C) The Hybrid model estimation involves a two-stage process where θ0 is estimated first, followed by θ1 and θ2 to maximize the joint normal log-likelihood.",
      "D) The dynamic factor specification of the Hybrid model combines annual and quarterly data by assuming the dynamics of annualized quarterly data are captured by a single lag."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Real-sectoral-spillovers--A-dynamic-factor-analysis_2019_Journal-of-Monetary.md"
    ],
    "question_context": "An important feature of the model in (4) is that shocks specific to particular sectors $(\\eta_{i t})$ can be propagated throughout the entire economy in a way that generates cross-sector covariability with the strength of the contemporaneous spillovers captured by the off-diagonal elements of the matrix $\\Pi_{0}$. Unfortunately, within the standard SVAR framework at most $N(N+1)/2$ linkages between the VAR disturbances $\\varepsilon_{t}$ in (4) and the structural shocks $u_{t}$ in (6) can be identified as a result of the symmetry property of the covariance matrix of $\\varepsilon_{t}$. Without further information, it is not possible to identify all $N^{2}$ potential contemporaneous linkages amongst the $N$ structural shocks on the $N$ sectors. The structural parameters of the model (denoted by ϑ), include parameters governing household’s time preference $(\\beta)$, capital’s depreciation rate $(\\delta_{d}),$ the capital share of production in various sectors $\\mathbf{\\eta}^{\\prime}\\varphi=[\\varphi_{i}]_{N\\times1})$, the intermediate input-use matrix $\\mathrm{^{\\prime}}\\Gamma=[\\gamma_{i j}]_{N\\times N})$ and the capital-use matrix $(\\Psi=[\\psi_{i j}]_{N\\times N})$. That is, $\\vartheta=\\{\\beta,\\delta_{d},\\varphi,\\Gamma,\\Psi\\}$. The matrices $\\Gamma$ and $\\Psi$ can be quantified using the U.S. BEA estimates of the input-use table and capital flow table, respectively. Estimation of the hybrid model is performed in two stages. The first stage consists of obtaining consistent estimates of $\\theta_{0}=\\{\\Phi\\}$ by estimating the VAR in (13) subject to the restriction that the VAR parameter matrix is $\\Phi+\\Pi_{1}\\Pi_{0}^{-1}$ with $\\Pi_{1}\\Pi_{0}^{-1}$ known. Defining the VAR disturbance in the first stage as $e_{t}=\\Pi_{0}S_{i}u_{t}$, the second step involves choosing the set of loading parameters in the two regimes, $\\theta_{1}$ and $\\theta_{2}$, to maximize the joint normal log-likelihood corresponding to the pre-GR $(T_{1})$ and GR periods $(T_{2})$.",
    "category": "INCORRECT",
    "question_context_used": "An important feature of the model in (4) is that shocks specific to particular sectors $(\\eta_{i t})$ can be propagated throughout the entire economy in a way that generates cross-sector covariability with the strength of the contemporaneous spillovers captured by the off-diagonal elements of the matrix $\\Pi_{0}$. Unfortunately, within the standard SVAR framework at most $N(N+1)/2$ linkages between the VAR disturbances $\\varepsilon_{t}$ in (4) and the structural shocks $u_{t}$ in (6) can be identified as a result of the symmetry property of the covariance matrix of $\\varepsilon_{t}$. Without further information, it is not possible to identify all $N^{2}$ potential contemporaneous linkages amongst the $N$ structural shocks on the $N$ sectors. The structural parameters of the model (denoted by ϑ), include parameters governing household’s time preference $(\\beta)$, capital’s depreciation rate $(\\delta_{d}),$ the capital share of production in various sectors $\\mathbf{\\eta}^{\\prime}\\varphi=[\\varphi_{i}]_{N\\times1})$, the intermediate input-use matrix $\\mathrm{^{\\prime}}\\Gamma=[\\gamma_{i j}]_{N\\times N})$ and the capital-use matrix $(\\Psi=[\\psi_{i j}]_{N\\times N})$. That is, $\\vartheta=\\{\\beta,\\delta_{d},\\varphi,\\Gamma,\\Psi\\}$. The matrices $\\Gamma$ and $\\Psi$ can be quantified using the U.S. BEA estimates of the input-use table and capital flow table, respectively. Estimation of the hybrid model is performed in two stages. The first stage consists of obtaining consistent estimates of $\\theta_{0}=\\{\\Phi\\}$ by estimating the VAR in (13) subject to the restriction that the VAR parameter matrix is $\\Phi+\\Pi_{1}\\Pi_{0}^{-1}$ with $\\Pi_{1}\\Pi_{0}^{-1}$ known. Defining the VAR disturbance in the first stage as $e_{t}=\\Pi_{0}S_{i}u_{t}$, the second step involves choosing the set of loading parameters in the two regimes, $\\theta_{1}$ and $\\theta_{2}$, to maximize the joint normal log-likelihood corresponding to the pre-GR $(T_{1})$ and GR periods $(T_{2})$.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3823",
    "question": "In the context of continuous time cointegrated systems with mixed stock and flow variables, which of the following statements are correct regarding the estimation and properties of the frequency domain estimator?",
    "options": [
      "A) The estimator of the cointegrating parameters converges at the rate of the sample size to a mixed normal distribution.",
      "B) The estimator of the short run parameters converges at the rate of the square root of the sample size to a limiting normal distribution.",
      "C) The frequency domain estimator falls within the class of optimal estimators defined by Phillips (1991b).",
      "D) The spectral density function of the discrete time process is obtained by folding all frequencies on the real line back into the interval (−π,π]."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "Frequency-domain-estimation-of-temporally-aggregated-Ga_2007_Journal-of-Econ.md"
    ],
    "question_context": "The paper considers joint estimation of long run equilibrium coefficients and parameters governing the short run dynamics of a fully parametric Gaussian cointegrated system formulated in continuous time. The model allows the stationary disturbances to be generated by a stochastic differential equation system and for the variables to be a mixture of stocks and flows. The estimator of the cointegrating parameters is shown to converge at the rate of the sample size to a mixed normal distribution, while that of the short run parameters converges at the rate of the square root of the sample size to a limiting normal distribution. The frequency domain estimator of the cointegrating parameters falls within the class of optimal estimators defined by Phillips (1991b). The spectral density for the discrete time process is obtained by folding all the frequencies on the real line back into the interval (−π,π] using the formula $f(\\lambda,\\theta)=\\sum_{j=-\\infty}^{\\infty}f^{c}(\\lambda+2\\pi j,\\theta)$.",
    "category": "CORRECT",
    "question_context_used": "The paper considers joint estimation of long run equilibrium coefficients and parameters governing the short run dynamics of a fully parametric Gaussian cointegrated system formulated in continuous time. The model allows the stationary disturbances to be generated by a stochastic differential equation system and for the variables to be a mixture of stocks and flows. The estimator of the cointegrating parameters is shown to converge at the rate of the sample size to a mixed normal distribution, while that of the short run parameters converges at the rate of the square root of the sample size to a limiting normal distribution. The frequency domain estimator of the cointegrating parameters falls within the class of optimal estimators defined by Phillips (1991b). The spectral density for the discrete time process is obtained by folding all the frequencies on the real line back into the interval (−π,π] using the formula $f(\\lambda,\\theta)=\\sum_{j=-\\infty}^{\\infty}f^{c}(\\lambda+2\\pi j,\\theta)$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2900",
    "question": "According to the Papers and Proceedings publication guidelines of the American Economic Association, which of the following statements are true regarding the standards for paper acceptance?",
    "options": [
      "A) Papers must undergo a formal refereeing process similar to regular issues of The American Economic Review.",
      "B) Data used in the analysis must be clearly documented and readily available for replication.",
      "C) Papers must be exploratory and discursive, rather than formal presentations of original research.",
      "D) Papers can be rejected if deemed utterly without merit by the editors."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Baldwin-EditorsIntroduction-1998.md"
    ],
    "question_context": "The American Economic Review's policy regarding availability of data also applies to the Paper and Proceedings: papers are published only if the data used in the analysis are clearly and precisely documented and are readily available to any researcher for purposes of replication. Otherwise, the guidelines under which papers are published in the Papers and Proceedings differ considerably from those governing regular issues of the Review. First, the length of papers is strictly controlled. They must be no more than 12 typescript pages in three-paper sessions, and 10 typescript pages in four-paper sessions. Second, papers are not subjected to a formal refereeing process. However, a paper can be rejected if, after reading it, we conclude that it is utterly without merit. Third, the content and range of subject matter reflect the wishes of the President-elect to investigate and expose the current state of economic research and thinking. In most cases, therefore, the papers are exploratory and discursive, rather than formal presentations of original research.",
    "category": "CORRECT",
    "question_context_used": "The American Economic Review's policy regarding availability of data also applies to the Paper and Proceedings: papers are published only if the data used in the analysis are clearly and precisely documented and are readily available to any researcher for purposes of replication. Otherwise, the guidelines under which papers are published in the Papers and Proceedings differ considerably from those governing regular issues of the Review. First, the length of papers is strictly controlled. They must be no more than 12 typescript pages in three-paper sessions, and 10 typescript pages in four-paper sessions. Second, papers are not subjected to a formal refereeing process. However, a paper can be rejected if, after reading it, we conclude that it is utterly without merit. Third, the content and range of subject matter reflect the wishes of the President-elect to investigate and expose the current state of economic research and thinking. In most cases, therefore, the papers are exploratory and discursive, rather than formal presentations of original research.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\", \"C\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4188",
    "question": "In the context of the Alternating Direction Method of Multipliers (ADMM) algorithm for solving the penalized estimation problem in multi-factor pricing models, which of the following statements are correct regarding the convergence and properties of the algorithm?",
    "options": [
      "A) The ADMM algorithm guarantees convergence to a global optimum under the assumption of a folded-concave penalty function.",
      "B) The projection step in the Θ-update ensures that the smallest eigenvalue of Θ is at least δ.",
      "C) The Σ-update involves an entry-wise thresholding rule that applies the penalty function only to the off-diagonal elements.",
      "D) The Lagrange multiplier update ensures that the equality constraint Σ = Θ is satisfied upon convergence."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Power-enhancement-for-testing-multi-factor-asset-pricin_2024_Journal-of-Econ.md"
    ],
    "question_context": "The ADMM algorithm is employed to solve the penalized estimation problem for ensuring positive definiteness of the covariance matrix estimator in multi-factor pricing models. The problem is formulated as:\n\n$$\n\\widehat{\\pmb{\\mathscr{X}}}_{p d}=\\underset{\\pmb{\\mathscr{X}}\\geq\\delta\\mathbf{I}}{\\operatorname{argmin}}~\\frac{1}{2}\\left\\|\\pmb{\\mathscr{X}}-\\widehat{\\pmb{\\mathscr{X}}}\\right\\|_{F}^{2}+\\sum_{i\\neq j}P_{\\lambda}(|\\sigma_{i j}|).\n$$\n\nThe algorithm iteratively updates Θ, Σ, and the Lagrange multiplier Λ. The Θ-update involves projecting onto the set of matrices with eigenvalues ≥ δ, ensuring positive definiteness. The Σ-update applies a penalty function only to off-diagonal elements via an entry-wise thresholding rule. The Lagrange multiplier update enforces the equality constraint Σ = Θ, which is satisfied upon convergence.",
    "category": "CORRECT",
    "question_context_used": "The ADMM algorithm is employed to solve the penalized estimation problem for ensuring positive definiteness of the covariance matrix estimator in multi-factor pricing models. The problem is formulated as:\n\n$$\n\\widehat{\\pmb{\\mathscr{X}}}_{p d}=\\underset{\\pmb{\\mathscr{X}}\\geq\\delta\\mathbf{I}}{\\operatorname{argmin}}~\\frac{1}{2}\\left\\|\\pmb{\\mathscr{X}}-\\widehat{\\pmb{\\mathscr{X}}}\\right\\|_{F}^{2}+\\sum_{i\\neq j}P_{\\lambda}(|\\sigma_{i j}|).\n$$\n\nThe algorithm iteratively updates Θ, Σ, and the Lagrange multiplier Λ. The Θ-update involves projecting onto the set of matrices with eigenvalues ≥ δ, ensuring positive definiteness. The Σ-update applies a penalty function only to off-diagonal elements via an entry-wise thresholding rule. The Lagrange multiplier update enforces the equality constraint Σ = Θ, which is satisfied upon convergence.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2583",
    "question": "According to the analysis of intrafirm trade determinants, which of the following factors are empirically associated with variations in related-party trade intensity?",
    "options": [
      "A) Higher governance quality in the exporting country initially increases the probability of related-party trade",
      "B) Products with low contractibility experience the largest reductions in intrafirm trade when governance improves",
      "C) The presence of wholesalers as intermediaries has no measurable effect on product contractibility",
      "D) Further improvements in governance quality beyond a threshold coincide with lower shares of related-party trade"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Bernard-IntrafirmTradeProduct-2010.md"
    ],
    "question_context": "The study examines product and country determinants of intrafirm trade, introducing a new measure of products' revealed contractibility based on intermediaries like wholesalers. Key findings show that higher quality country governance is associated with a higher probability of related-party trade initially, but further improvements coincide with lower shares due to easier arm's length contracting. The interaction effects reveal that improvements in governance lead to the largest reductions in intrafirm trade for low contractibility products. Formula: ΔIntrafirm Share = f(Governance Quality × Product Contractibility), where the marginal effect of governance is strongest when contractibility is low.",
    "category": "CORRECT",
    "question_context_used": "The study examines product and country determinants of intrafirm trade, introducing a new measure of products' revealed contractibility based on intermediaries like wholesalers. Key findings show that higher quality country governance is associated with a higher probability of related-party trade initially, but further improvements coincide with lower shares due to easier arm's length contracting. The interaction effects reveal that improvements in governance lead to the largest reductions in intrafirm trade for low contractibility products. Formula: ΔIntrafirm Share = f(Governance Quality × Product Contractibility), where the marginal effect of governance is strongest when contractibility is low.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1000",
    "question": "Under which of the following conditions will the firm serve both rich and poor consumers in equilibrium, according to Proposition 1?",
    "options": [
      "A) The average valuation of the product in the neighborhood is sufficiently high and the income gap between rich and poor is sufficiently low.",
      "B) The rich consumers' valuation of the product is significantly higher than that of the poor consumers, regardless of the proportion of poor consumers.",
      "C) The fixed cost F is zero, allowing the firm to serve all consumers without any restrictions.",
      "D) The travel cost t is negligible, making distance irrelevant to consumer utility."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Inequality--neighbourhoods-and-welfare-of-t_2016_Journal-of-Development-Econ.md"
    ],
    "question_context": "The model adapts frameworks from Hotelling (1929) and Salop (1979), considering a neighborhood represented on a line segment with uniformly distributed rich and poor consumers. The firm's profit is given by \\(\\pi(p)=[p-c]D(p)-F\\), where demand \\(D(p)\\) depends on the price and consumer distribution. Key conditions for serving both rich and poor include:\n\n\\[\\left(\\frac{\\theta Y_{R}-c}{\\sqrt{t F}}\\right)\\leq\\left(1+\\frac{1}{\\sqrt{1-f}}\\right)\\left(\\frac{\\theta Y_{P}-c}{\\sqrt{t F}}\\right),\\]\n\nand\n\n\\[f\\cdot\\left(\\frac{\\theta Y_{P}-c}{\\sqrt{t F}}\\right)+(1-f)\\cdot\\left(\\frac{\\theta Y_{R}-c}{\\sqrt{t F}}\\right)\\geq\\sqrt{2}.\\]\n\nThese conditions ensure that the average valuation is high enough and the income gap is low enough for the firm to find it profitable to serve both groups.",
    "category": "INCORRECT",
    "question_context_used": "The model adapts frameworks from Hotelling (1929) and Salop (1979), considering a neighborhood represented on a line segment with uniformly distributed rich and poor consumers. The firm's profit is given by \\(\\pi(p)=[p-c]D(p)-F\\), where demand \\(D(p)\\) depends on the price and consumer distribution. Key conditions for serving both rich and poor include:\n\n\\[\\left(\\frac{\\theta Y_{R}-c}{\\sqrt{t F}}\\right)\\leq\\left(1+\\frac{1}{\\sqrt{1-f}}\\right)\\left(\\frac{\\theta Y_{P}-c}{\\sqrt{t F}}\\right),\\]\n\nand\n\n\\[f\\cdot\\left(\\frac{\\theta Y_{P}-c}{\\sqrt{t F}}\\right)+(1-f)\\cdot\\left(\\frac{\\theta Y_{R}-c}{\\sqrt{t F}}\\right)\\geq\\sqrt{2}.\\]\n\nThese conditions ensure that the average valuation is high enough and the income gap is low enough for the firm to find it profitable to serve both groups.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-54",
    "question": "In the described overlapping generations model with actuarially fair annuity contracts, which of the following statements are correct regarding the government's role and the agents' behavior?",
    "options": [
      "A) The government must impose social security taxes to fully fund expenditures, as actuarially fair annuity contracts are unavailable on the private market.",
      "B) Under Plan V, agents are allowed to annuitize up to a certain percentage of their savings, and the excess return on annuities is determined by the probability of survival.",
      "C) The production function exhibits increasing returns to scale due to the external effects of aggregate capital on productivity.",
      "D) Agents under Plan M are required to annuitize a fixed amount of their savings, and the return on these annuities is actuarially fair."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Effects_of_Annuities,_Bequests,_and_Aging_in_an_Overlapping_Generations_Model_of_Endogenous_Growth.md"
    ],
    "question_context": "The model considers an infinite-horizon economy with identical two-period lived agents, perfectly competitive firms, annuity markets, and a government. Agents face uncertainty about the time of death but not about the maximum possible length of life. The government can impose social security taxes and establish a market in actuarially fair annuity contracts, either voluntarily (Plan V) or mandatorily (Plan M). Key formulas include the utility function: \n\n$$U=\\ln c_{t}(t)+\\phi\\ln c_{t}(t+\\mathrm{\\boldmath~I~}),$$\n\nand the production function in intensive form: \n\n$$y(t)=A(t)k(t)^{\\beta}.$$\n\nUnder Plan V, the excess return on annuities is given by:\n\n$$\\alpha^{v}(t+\\mathfrak{r})=\\frac{\\gamma^{v}(t)\\left(\\mathfrak{r}-p\\right)\\left[\\mathfrak{r}+\\rho(t+\\mathfrak{r})\\right]}{\\displaystyle p}.$$\n\nUnder Plan M, the return on mandated savings is:\n\n$$[\\mathfrak{n}+\\alpha^{M}(t+\\mathfrak{r})]=\\frac{\\left[\\mathfrak{n}+\\rho(t+\\mathfrak{r})\\right]}{\\mathfrak{p}}.$$",
    "category": "INCORRECT",
    "question_context_used": "The model considers an infinite-horizon economy with identical two-period lived agents, perfectly competitive firms, annuity markets, and a government. Agents face uncertainty about the time of death but not about the maximum possible length of life. The government can impose social security taxes and establish a market in actuarially fair annuity contracts, either voluntarily (Plan V) or mandatorily (Plan M). Key formulas include the utility function: \n\n$$U=\\ln c_{t}(t)+\\phi\\ln c_{t}(t+\\mathrm{\\boldmath~I~}),$$\n\nand the production function in intensive form: \n\n$$y(t)=A(t)k(t)^{\\beta}.$$\n\nUnder Plan V, the excess return on annuities is given by:\n\n$$\\alpha^{v}(t+\\mathfrak{r})=\\frac{\\gamma^{v}(t)\\left(\\mathfrak{r}-p\\right)\\left[\\mathfrak{r}+\\rho(t+\\mathfrak{r})\\right]}{\\displaystyle p}.$$\n\nUnder Plan M, the return on mandated savings is:\n\n$$[\\mathfrak{n}+\\alpha^{M}(t+\\mathfrak{r})]=\\frac{\\left[\\mathfrak{n}+\\rho(t+\\mathfrak{r})\\right]}{\\mathfrak{p}}.$$",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1527",
    "question": "In the context of the foreign monopolist's quality discrimination model, which of the following statements are true regarding optimal quality provision and profit margins?",
    "options": [
      "A) The highest type consumer (θ₃) receives the same quality as under perfect discrimination.",
      "B) Quality distortion is most pronounced for the highest type consumer (θ₃).",
      "C) Profit margins increase with consumer type (p₃* - c(q₃*) ≥ p₂* - c(q₂*) ≥ p₁* - c(q₁*) > 0).",
      "D) The monopolist provides undistorted quality to all consumer types to maximize total surplus."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Commercial Policy and Imperfect Discrimination by a Foreign Monopolist.md"
    ],
    "question_context": "The model analyzes a foreign monopolist facing three consumer types (θ₁, θ₂, θ₃) with different willingness to pay for quality. The utility function V(q;θ) satisfies V_q(q;θ) > 0 (increasing in quality) and V_qq(q;θ) < 0 (concave), with higher types having greater total and marginal willingness to pay. The monopolist's profit maximization problem is:\n\n$$\\pi = \\sum_{i=1}^{3}[p_i - c(q_i)]f_i$$\n\nsubject to incentive compatibility and participation constraints. At the optimum:\n\n- Equation (8) shows V_q(q₃*;θ₃) = c'(q₃*), indicating θ₃ receives the same quality as under perfect discrimination.\n- Equations (6)-(7) reveal quality distortion for lower types, most severe for θ₁.\n- Profit margins satisfy p₃* - c(q₃*) ≥ p₂* - c(q₂*) ≥ p₁* - c(q₁*) > 0, increasing with consumer type.",
    "category": "CORRECT",
    "question_context_used": "The model analyzes a foreign monopolist facing three consumer types (θ₁, θ₂, θ₃) with different willingness to pay for quality. The utility function V(q;θ) satisfies V_q(q;θ) > 0 (increasing in quality) and V_qq(q;θ) < 0 (concave), with higher types having greater total and marginal willingness to pay. The monopolist's profit maximization problem is:\n\n$$\\pi = \\sum_{i=1}^{3}[p_i - c(q_i)]f_i$$\n\nsubject to incentive compatibility and participation constraints. At the optimum:\n\n- Equation (8) shows V_q(q₃*;θ₃) = c'(q₃*), indicating θ₃ receives the same quality as under perfect discrimination.\n- Equations (6)-(7) reveal quality distortion for lower types, most severe for θ₁.\n- Profit margins satisfy p₃* - c(q₃*) ≥ p₂* - c(q₂*) ≥ p₁* - c(q₁*) > 0, increasing with consumer type.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4480",
    "question": "In the context of estimating the long-run effect of institutions on income per capita, which of the following statements are supported by the empirical analysis?",
    "options": [
      "A) The persistence coefficient δ is found to be stable over time, with estimates ranging between 0.44 and 0.69.",
      "B) The non-parametric fit of δ significantly deviates from the linear fit across all values of the Constraints on the Executive index.",
      "C) The Conley et al. (2012) method requires priors for the value of β₂γψ to estimate β₁ when indirect channels are present.",
      "D) The bias correction method can be used to extract estimates of ψ, which is crucial for understanding contemporaneous effects."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Historical-instruments-and-contemporary-endoge_2021_Journal-of-Development-E.md"
    ],
    "question_context": "The empirical analysis investigates the stability of the persistence coefficient δ and the linearity of the relationship between current and lagged values of Constraints on the Executive. The coefficient on lagged institutions appears relatively stable, hovering between 0.44 and 0.69, with a mean of 0.56. The standard deviation of the coefficients is 0.08, and the figure does not reveal any obvious time trends in the estimate of δ. This stability suggests that the assumption of a constant δ is reasonable. The non-parametric and linear regression lines are generally very close to one another, deviating notably only in the sparse extremes of the Constraints on the Executive index. The Conley et al. (2012) method is used to bound estimates of β₁, requiring priors for β₂γψ. The total effect of X_H on Y_C is given by η = β₁δ + β₂γ, where β₁δ is the impact mediated by X_C and β₂γ is the impact mediated by A_C. The bias correction method can be used to extract estimates of ψ, which is crucial for understanding contemporaneous effects.",
    "category": "CORRECT",
    "question_context_used": "The empirical analysis investigates the stability of the persistence coefficient δ and the linearity of the relationship between current and lagged values of Constraints on the Executive. The coefficient on lagged institutions appears relatively stable, hovering between 0.44 and 0.69, with a mean of 0.56. The standard deviation of the coefficients is 0.08, and the figure does not reveal any obvious time trends in the estimate of δ. This stability suggests that the assumption of a constant δ is reasonable. The non-parametric and linear regression lines are generally very close to one another, deviating notably only in the sparse extremes of the Constraints on the Executive index. The Conley et al. (2012) method is used to bound estimates of β₁, requiring priors for β₂γψ. The total effect of X_H on Y_C is given by η = β₁δ + β₂γ, where β₁δ is the impact mediated by X_C and β₂γ is the impact mediated by A_C. The bias correction method can be used to extract estimates of ψ, which is crucial for understanding contemporaneous effects.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1683",
    "question": "In the context of impact evaluation studies, which of the following statements are statistically accurate based on the provided analysis?",
    "options": [
      "A) The mean result is contained within the confidence interval of another study approximately 53% of the time.",
      "B) The median absolute difference between a study's result and the mean result is 48% when calculated in-sample.",
      "C) The average absolute difference increases to 311% when excluding a study from the mean calculation (out-of-sample prediction).",
      "D) Rate ratios and percentage points should be interpreted identically when comparing treatment effects."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Vivalt-HeterogeneousTreatmentEffects-2015.md"
    ],
    "question_context": "The analysis discusses how treatment effects vary within intervention-outcome combinations, using original units (e.g., percentage points). Key findings include: the mean result is contained in the confidence interval of another study about 53% of the time, and studies’ confidence intervals overlap approximately 83% of the time. The average absolute difference between a study's result and the mean is 114% (median: 48%) in-sample, but increases to 311% (median: 52%) out-of-sample. Rate ratios, defined as the ratio of treatment and control group rates, must be interpreted differently than percentage points due to scale differences. Vivalt (2015) provides further standardized analysis, and prespecified subgroup analyses are recommended to avoid specification searching.",
    "category": "CORRECT",
    "question_context_used": "The analysis discusses how treatment effects vary within intervention-outcome combinations, using original units (e.g., percentage points). Key findings include: the mean result is contained in the confidence interval of another study about 53% of the time, and studies’ confidence intervals overlap approximately 83% of the time. The average absolute difference between a study's result and the mean is 114% (median: 48%) in-sample, but increases to 311% (median: 52%) out-of-sample. Rate ratios, defined as the ratio of treatment and control group rates, must be interpreted differently than percentage points due to scale differences. Vivalt (2015) provides further standardized analysis, and prespecified subgroup analyses are recommended to avoid specification searching.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1323",
    "question": "In the context of dynamic panel data models with both observed and unobserved heterogeneity, which of the following statements are correct regarding the LSDV (Least Squares Dummy Variable) estimator?",
    "options": [
      "A) The LSDV estimator is consistent for finite T as N approaches infinity.",
      "B) The LSDV estimator can be used to construct exact inference on the coefficient of the lagged dependent variable.",
      "C) The asymptotic bias of the LSDV estimator does not depend on the variance of the error term.",
      "D) The presence of exogenous regressors in the model reduces the asymptotic bias of the LSDV estimator."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "On-bias--inconsistency--and-efficiency-of-various-estima_1995_Journal-of-Eco.md"
    ],
    "question_context": "The model under consideration is a dynamic panel data model with both observed and unobserved heterogeneity, represented by the equation: \n\n$$ y_{i t}=\\gamma y_{i,t-1}+x_{i t}^{\\prime}\\beta+\\eta_{i}+\\varepsilon_{i t}, \\quad i=1,\\ldots,N, \\quad t=1,\\ldots,T, $$\n\nwhere $y_{i t}$ is the dependent variable, $x_{i t}$ is a vector of explanatory variables, $\\eta_{i}$ denotes the individual effect, and $\\varepsilon_{i t}$ is the disturbance term. The LSDV estimator, given by \n\n$$ \\widehat{\\delta}=(W^{\\prime}A W)^{-1}W^{\\prime}A y, $$\n\nis known to be inconsistent for finite $T$ and $N \\rightarrow \\infty$. However, it is still of interest due to its relatively small variance and its utility in constructing exact inference on $\\gamma$. The asymptotic bias of the LSDV estimator, as derived by Nickell (1981), does not depend on $\\sigma_{\\varepsilon}^{2}$ and is aggravated by the presence of exogenous regressors, contrary to some claims in the literature.",
    "category": "INCORRECT",
    "question_context_used": "The model under consideration is a dynamic panel data model with both observed and unobserved heterogeneity, represented by the equation: \n\n$$ y_{i t}=\\gamma y_{i,t-1}+x_{i t}^{\\prime}\\beta+\\eta_{i}+\\varepsilon_{i t}, \\quad i=1,\\ldots,N, \\quad t=1,\\ldots,T, $$\n\nwhere $y_{i t}$ is the dependent variable, $x_{i t}$ is a vector of explanatory variables, $\\eta_{i}$ denotes the individual effect, and $\\varepsilon_{i t}$ is the disturbance term. The LSDV estimator, given by \n\n$$ \\widehat{\\delta}=(W^{\\prime}A W)^{-1}W^{\\prime}A y, $$\n\nis known to be inconsistent for finite $T$ and $N \\rightarrow \\infty$. However, it is still of interest due to its relatively small variance and its utility in constructing exact inference on $\\gamma$. The asymptotic bias of the LSDV estimator, as derived by Nickell (1981), does not depend on $\\sigma_{\\varepsilon}^{2}$ and is aggravated by the presence of exogenous regressors, contrary to some claims in the literature.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3264",
    "question": "In the context of wage arrears in Russia, which of the following factors are hypothesized to attenuate the marginal costs of wage arrears for a firm, according to the model?",
    "options": [
      "A) Higher levels of wage arrears in the firm's local labor market (Ω)",
      "B) Increased worker mobility costs (Z^Q)",
      "C) Stronger legal enforcement mechanisms (Z^L)",
      "D) Greater specificity of worker skills (Z^E)"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Earle-COMPLEMENTARITYCUSTOMWAGE-2009.md"
    ],
    "question_context": "The model posits that the marginal cost of wage arrears (ω) is attenuated by the level of arrears in the local labor market (Ω), due to reduced worker mobility and protest likelihood. Key formula: $C(\\omega,\\Omega,Z) = E(\\omega,\\Omega,Z^{E}) + Q(\\omega,\\Omega,Z^{Q}) + S(\\omega,\\Omega,Z^{S}) + L(\\omega,\\Omega,Z^{L})$, with $C_{\\omega\\Omega} < 0$. This condition implies strategic complementarities, where higher Ω reduces the marginal cost of ω. Worker characteristics like skill specificity (Z^E) also play a role by reducing quit responsiveness.",
    "category": "INCORRECT",
    "question_context_used": "The model posits that the marginal cost of wage arrears (ω) is attenuated by the level of arrears in the local labor market (Ω), due to reduced worker mobility and protest likelihood. Key formula: $C(\\omega,\\Omega,Z) = E(\\omega,\\Omega,Z^{E}) + Q(\\omega,\\Omega,Z^{Q}) + S(\\omega,\\Omega,Z^{S}) + L(\\omega,\\Omega,Z^{L})$, with $C_{\\omega\\Omega} < 0$. This condition implies strategic complementarities, where higher Ω reduces the marginal cost of ω. Worker characteristics like skill specificity (Z^E) also play a role by reducing quit responsiveness.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3780",
    "question": "In the context of econometric models with multiple equilibria, which of the following methodologies are explicitly mentioned in the text for testing or analyzing such models?",
    "options": [
      "A) Quantile Methods",
      "B) Bootstrapping Realized Volatility",
      "C) Unconditional Quantile Regressions",
      "D) Information Independence and Common Knowledge"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "VolumeInformation-2009.md"
    ],
    "question_context": "The text references several advanced econometric methodologies, including testing models with multiple equilibria by quantile methods (Echenique and Komunjer, 1281) and unconditional quantile regressions (Firpo, Fortin, and Lemieux, 953). These approaches are crucial for addressing complex economic models where multiple equilibria may exist. The quantile methods allow for testing models that do not have a unique solution, while unconditional quantile regressions provide a way to analyze the impact of variables across the entire distribution of the outcome.",
    "category": "CORRECT",
    "question_context_used": "The text references several advanced econometric methodologies, including testing models with multiple equilibria by quantile methods (Echenique and Komunjer, 1281) and unconditional quantile regressions (Firpo, Fortin, and Lemieux, 953). These approaches are crucial for addressing complex economic models where multiple equilibria may exist. The quantile methods allow for testing models that do not have a unique solution, while unconditional quantile regressions provide a way to analyze the impact of variables across the entire distribution of the outcome.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2111",
    "question": "In the context of survival analysis with a Weibull hazard function, which of the following statements are correct regarding the asymptotic relative efficiency (ARE) of the partial likelihood estimator (PLE)?",
    "options": [
      "A) The ARE of the PLE increases as the parametric complexity of the baseline hazard function increases.",
      "B) The ARE of the PLE is higher under a Weibull baseline hazard compared to a constant baseline hazard for the same parameter values.",
      "C) The efficiency of the PLE decreases as the proportion of censored observations increases.",
      "D) The factor $(a-1)/a$ captures the change in relative efficiency due to the increased variance of the MLE relative to the PLE."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Asymptotic-efficiency-calculations-of-the-partial-lik_1989_Journal-of-Econom.md"
    ],
    "question_context": "The Weibull hazard function is given by $\\lambda(t)=\\lambda_{1}t^{\\lambda_{2}-1},\\qquad\\lambda_{1}>0,\\quad\\lambda_{2}>1$. Under no censoring, the factor $\\gamma$ is derived as $\\gamma={\\bigl(}a-1{\\bigr)}/a$, where $a=\\pi^{2}/6$. This implies that the log of the ARE for a Weibull baseline hazard is $(a-1)/a$ times the log of the ARE for a constant baseline hazard. For example, with $p=2$, $\\beta=(0.4,0.4)$, $CP=0.3$, and $N^{-1}(x_{1}x_{2})=0$, the ARE under Weibull baseline hazard is approximately $0.990$, showing an increase compared to the constant baseline hazard case. The complexity in the parametric form of the baseline hazard increases the variance of the MLE for $\\beta$, thereby increasing the efficiency of the PLE.",
    "category": "INCORRECT",
    "question_context_used": "The Weibull hazard function is given by $\\lambda(t)=\\lambda_{1}t^{\\lambda_{2}-1},\\qquad\\lambda_{1}>0,\\quad\\lambda_{2}>1$. Under no censoring, the factor $\\gamma$ is derived as $\\gamma={\\bigl(}a-1{\\bigr)}/a$, where $a=\\pi^{2}/6$. This implies that the log of the ARE for a Weibull baseline hazard is $(a-1)/a$ times the log of the ARE for a constant baseline hazard. For example, with $p=2$, $\\beta=(0.4,0.4)$, $CP=0.3$, and $N^{-1}(x_{1}x_{2})=0$, the ARE under Weibull baseline hazard is approximately $0.990$, showing an increase compared to the constant baseline hazard case. The complexity in the parametric form of the baseline hazard increases the variance of the MLE for $\\beta$, thereby increasing the efficiency of the PLE.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3848",
    "question": "In the context of the CES-Pareto/Power circle, which of the following statements are correct regarding the transformation of distributions under the CES model?",
    "options": [
      "A) A positive power transformation of a Pareto distribution results in another Pareto distribution.",
      "B) A negative power transformation of a Power distribution results in a Pareto distribution.",
      "C) The log-normal distribution forms a closed power family (CPF) under the CES transformation.",
      "D) The normal distribution, without extension, constitutes a CPF under any power transformation."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Decoupling_the_CES_Distribution_Circle_with_Quality_and_Beyond_Equilibrium_Distributions_and_the_CES-Logit_Nexus.md"
    ],
    "question_context": "The CES-Pareto/Power circle establishes that all five primary distributions (profit, revenue, output, price reciprocal, and productivity) are Pareto if any one is. The transformation properties are governed by Lemma 1, which states how distributions are modified by multiplicative and power transformations. For example, a positive power transformation of a Pareto distribution yields another Pareto distribution, while a negative power transformation of a Power distribution yields a Pareto distribution. The log-normal distribution is highlighted as a CPF because it retains its form under both positive and negative power transformations, unlike the normal distribution, which requires extension to an A-family to be closed under power transformations. Key formula: For a random variable U with CDF F_U(u), the CDF after a positive power transformation V=kU^θ is F_V(ν)=F_U[(ν/k)^(1/θ)].",
    "category": "CORRECT",
    "question_context_used": "The CES-Pareto/Power circle establishes that all five primary distributions (profit, revenue, output, price reciprocal, and productivity) are Pareto if any one is. The transformation properties are governed by Lemma 1, which states how distributions are modified by multiplicative and power transformations. For example, a positive power transformation of a Pareto distribution yields another Pareto distribution, while a negative power transformation of a Power distribution yields a Pareto distribution. The log-normal distribution is highlighted as a CPF because it retains its form under both positive and negative power transformations, unlike the normal distribution, which requires extension to an A-family to be closed under power transformations. Key formula: For a random variable U with CDF F_U(u), the CDF after a positive power transformation V=kU^θ is F_V(ν)=F_U[(ν/k)^(1/θ)].",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1085",
    "question": "In the context of automatic stabilizers and fiscal policy rules, which of the following statements are supported by the analysis of present-value multipliers?",
    "options": [
      "A) Stronger automatic stabilizers always reduce short-run output fluctuations without long-run costs",
      "B) Higher expected taxes on capital can reverse the initial stimulus to output from government spending",
      "C) Automatic stabilizers operate symmetrically, amplifying fiscal responses during both output increases and decreases",
      "D) The long-run multiplier for government spending is unaffected by the degree of automatic responsiveness of fiscal variables"
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Dynamics-of-fiscal-financing-in-the-United-State_2010_Journal-of-Econometric.md"
    ],
    "question_context": "The final counterfactual exercise examines the consequences of increasing or decreasing the ‘‘automatic stabilizer’’ aspect of the fiscal rules—the $\\varphi$ coefficients on contemporaneous output in policy rules (11)–(13) and (15). Specifically, we ask how the government spending present-value multiplier for output changes when capital taxes alone adjust to stabilize debt, as in the top-left panel of Fig. 10, and the degree of ‘‘automatic’’ responsiveness of fiscal variables to output fluctuations. Fig. 11 reports the present-value multipliers for four different settings of the $\\varphi$ coefficients: no automatic response (dashed line); estimated responses (solid line); twice the estimated responses (dotted line); three times the estimated responses (dotted–dashed line). Stronger ‘‘automatic stabilizers’’, which may help to reduce the short-run fluctuations induced by various shocks, impose a long-run cost. In this example, higher government spending initially raises output, which, under the estimated policies, raises capital and labor taxes and lowers transfers. But in this scenario, higher expected taxes on capital quickly reverse the initial stimulus to output. Naturally, ‘‘automatic stabilizers’’ are symmetric: when output falls, the declines in tax revenues and increases in government expenditures are amplified, reducing future surpluses still more.",
    "category": "CORRECT",
    "question_context_used": "The final counterfactual exercise examines the consequences of increasing or decreasing the ‘‘automatic stabilizer’’ aspect of the fiscal rules—the $\\varphi$ coefficients on contemporaneous output in policy rules (11)–(13) and (15). Specifically, we ask how the government spending present-value multiplier for output changes when capital taxes alone adjust to stabilize debt, as in the top-left panel of Fig. 10, and the degree of ‘‘automatic’’ responsiveness of fiscal variables to output fluctuations. Fig. 11 reports the present-value multipliers for four different settings of the $\\varphi$ coefficients: no automatic response (dashed line); estimated responses (solid line); twice the estimated responses (dotted line); three times the estimated responses (dotted–dashed line). Stronger ‘‘automatic stabilizers’’, which may help to reduce the short-run fluctuations induced by various shocks, impose a long-run cost. In this example, higher government spending initially raises output, which, under the estimated policies, raises capital and labor taxes and lowers transfers. But in this scenario, higher expected taxes on capital quickly reverse the initial stimulus to output. Naturally, ‘‘automatic stabilizers’’ are symmetric: when output falls, the declines in tax revenues and increases in government expenditures are amplified, reducing future surpluses still more.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-983",
    "question": "Which of the following statements are supported by the empirical findings regarding historical convict sex ratios and their contemporary socio-economic impacts?",
    "options": [
      "A) A one standard deviation increase in the convict sex ratio is associated with a 0.7% point shift in the share of men employed in neutral or stereotypically female occupations towards stereotypically male occupations.",
      "B) The historical sex ratio is significantly and positively associated with the share of women employed in female occupations.",
      "C) A one standard deviation increase in the convict sex ratio is associated with a 2.2% point decrease in the vote share in favor of same-sex marriage.",
      "D) The convict sex ratio alone explains less than 5% of the variation in the 'Yes' vote for same-sex marriage, after accounting for other socio-demographic factors."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Baranov 等 - 2023 - Men. Male-biased sex ratios and masculinity norms evidence from Australia's colonial past.md"
    ],
    "question_context": "The results paint a striking picture. Historical sex ratios significantly contribute to occupational gender segregation today. The coefficient associated with the convict sex ratio is significant for males across all categories of employment. The sign of the coefficient is consistent with our interpretation that historical sex ratios forged a culture of masculinity, which still leads men to seek employment in stereotypically male occupations, and to shun employment in stereotypically female, and even neutral, occupations. Overall, a one standard deviation increase in the convict sex ratio is associated with a $0.7\\%$ point shift away in the share of men employed in neutral or stereotypically female occupations towards stereotypically male occupations. The results show that both the share of votes in favor of marriage equality and the participation rate are substantially lower in areas where convict sex ratios were more male-biased in the past. A one standard deviation increase in the convict sex ratio is associated with a $2.2\\%$ point decrease in the vote share in favor of same-sex marriage (column 1). This amounts to $4.4\\%$ of the mean and corresponds to $39\\%$ of the education gap.",
    "category": "INCORRECT",
    "question_context_used": "The results paint a striking picture. Historical sex ratios significantly contribute to occupational gender segregation today. The coefficient associated with the convict sex ratio is significant for males across all categories of employment. The sign of the coefficient is consistent with our interpretation that historical sex ratios forged a culture of masculinity, which still leads men to seek employment in stereotypically male occupations, and to shun employment in stereotypically female, and even neutral, occupations. Overall, a one standard deviation increase in the convict sex ratio is associated with a $0.7\\%$ point shift away in the share of men employed in neutral or stereotypically female occupations towards stereotypically male occupations. The results show that both the share of votes in favor of marriage equality and the participation rate are substantially lower in areas where convict sex ratios were more male-biased in the past. A one standard deviation increase in the convict sex ratio is associated with a $2.2\\%$ point decrease in the vote share in favor of same-sex marriage (column 1). This amounts to $4.4\\%$ of the mean and corresponds to $39\\%$ of the education gap.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2077",
    "question": "In the context of the optimal growth model's equilibrium path computation, which of the following conditions are necessary for the Dunford–Pettis criterion to ensure compactness of the feasible set?",
    "options": [
      "A) The investment profile must be strictly positive and bounded by an exponential function.",
      "B) The utility function must satisfy the Inada conditions to prevent zero consumption on non-zero measure intervals.",
      "C) The variables must belong to balls of $L^{1}(e^{-\\psi t})$ for some $\\psi > 0$.",
      "D) The feasibility set must be closed under the topology $\\sigma(L^{1},L^{\\infty})$."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Vintage-capital-and-the-dynamics-of-the-AK-mod_2005_Journal-of-Economic-Theo.md"
    ],
    "question_context": "The Dunford–Pettis criterion is used to establish compactness in infinite-dimensional functional spaces. Specifically, a bounded subset $G$ of $L^{1}(e^{-\\psi t})$ is relatively compact for the topology $\\sigma(L^{1},L^{\\infty})$ if and only if: $\\forall\\varepsilon>0$, there exists $\\delta>0$ such that $\\int_{I}|f(t)|e^{-\\psi t}d t<\\varepsilon$, $\\forall f\\in G$, and $\\forall I$ verifying $\\int_{I}e^{-\\psi t}d t<\\delta$. The proof leverages the positivity of feasible investment and the boundedness of variables in $L^{1}(e^{-\\psi t})$, where $\\psi = A(1-\\sigma) - \\rho < 0$ under the assumption $A(1-\\sigma) < \\rho$.",
    "category": "INCORRECT",
    "question_context_used": "The Dunford–Pettis criterion is used to establish compactness in infinite-dimensional functional spaces. Specifically, a bounded subset $G$ of $L^{1}(e^{-\\psi t})$ is relatively compact for the topology $\\sigma(L^{1},L^{\\infty})$ if and only if: $\\forall\\varepsilon>0$, there exists $\\delta>0$ such that $\\int_{I}|f(t)|e^{-\\psi t}d t<\\varepsilon$, $\\forall f\\in G$, and $\\forall I$ verifying $\\int_{I}e^{-\\psi t}d t<\\delta$. The proof leverages the positivity of feasible investment and the boundedness of variables in $L^{1}(e^{-\\psi t})$, where $\\psi = A(1-\\sigma) - \\rho < 0$ under the assumption $A(1-\\sigma) < \\rho$.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4478",
    "question": "In the context of instrumental variables (IV) regression with historical instruments and contemporary regressors, which of the following statements are true regarding the estimation of the long-run effect (η) and the persistence parameter (δ)?",
    "options": [
      "A) The IV coefficient overestimates η when δ < 1 and underestimates it when δ > 1.",
      "B) The conventional IV coefficient is consistent for the ratio of the long-run effect and the persistence of the endogenous variable (η/δ).",
      "C) The inconsistency in estimating β₁ depends on the strength and sign of the A_C channel and the degree of persistence (δ).",
      "D) The long-run effect η can be recovered by multiplying the conventional IV coefficient by δ or by using X_H in the regression."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "Historical-instruments-and-contemporary-endoge_2021_Journal-of-Development-E.md"
    ],
    "question_context": "The framework considers an endogenous explanatory variable X with historical (X_H) and contemporary (X_C) periods, where Z_H is a valid instrument for X_H but X_H is unobserved. The long-run effect η is defined as ∂Y_C/∂X_H = δβ₁ + β₂γ, where δ measures the persistence of historical changes in X. The conventional IV regression yields plim b̂₁^IV = η/δ, indicating that the IV coefficient is consistent for the ratio of the long-run effect to the persistence parameter. The IV coefficient overestimates η when δ < 1 and underestimates it when δ > 1. The inconsistency in estimating the contemporaneous effect β₁ is influenced by the A_C channel (β₂γ) and the persistence δ. The long-run effect η can be recovered by combining the IV coefficient with an estimate of δ or by using X_H directly if observed.",
    "category": "CORRECT",
    "question_context_used": "The framework considers an endogenous explanatory variable X with historical (X_H) and contemporary (X_C) periods, where Z_H is a valid instrument for X_H but X_H is unobserved. The long-run effect η is defined as ∂Y_C/∂X_H = δβ₁ + β₂γ, where δ measures the persistence of historical changes in X. The conventional IV regression yields plim b̂₁^IV = η/δ, indicating that the IV coefficient is consistent for the ratio of the long-run effect to the persistence parameter. The IV coefficient overestimates η when δ < 1 and underestimates it when δ > 1. The inconsistency in estimating the contemporaneous effect β₁ is influenced by the A_C channel (β₂γ) and the persistence δ. The long-run effect η can be recovered by combining the IV coefficient with an estimate of δ or by using X_H directly if observed.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4336",
    "question": "In the described model with Bayesian updating of beliefs and discontinuous risk aversion, which of the following statements are true regarding the issuance of debt at $t=1$ after observing $n_{h}^{0}$ bits of good news?",
    "options": [
      "A) The amount of debt issued is $d_{1} = y_{l}$ due to the investor's risk aversion when the default probability exceeds $\\rho$.",
      "B) The price of debt $p(d_{1})$ equals $y_{h}$ because the good news increases the expected probability of the high state.",
      "C) The debt issuance remains conservative because the Bayesian posterior does not sufficiently shift to make default risk tolerable.",
      "D) The debt is information insensitive, meaning its price does not change at $t=2$ regardless of further news."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Gennaioli-NeglectedRisksPsychology-2015.md"
    ],
    "question_context": "The model features an asset with cash flow $\\boldsymbol{Y}\\equiv\\left\\{\\boldsymbol{y}_{h},\\boldsymbol{y}_{l}\\right\\}$, where $y_{h}>y_{l}$. The investor updates beliefs via Bayesian updating: $\\bar{\\pi_{k}^{t}}=(\\pi_{k}^{0}\\dot{+}n_{k})/(1+\\dot{n}_{h}+\\dot{n_{l}})$. At $t=1$, debt is issued against the asset's cash flow. The investor is risk neutral if the expected default probability is below $\\rho$, but infinitely risk averse if it exceeds $\\rho$. Lemma 1 states that after observing $n_{h}^{0}$ bits of good news, the debt issued is $d_{1}=p(d_{1})=y_{l}$, as the posterior does not shift enough to make default risk tolerable. The debt is information insensitive, so its price remains unchanged at $t=2$.",
    "category": "CORRECT",
    "question_context_used": "The model features an asset with cash flow $\\boldsymbol{Y}\\equiv\\left\\{\\boldsymbol{y}_{h},\\boldsymbol{y}_{l}\\right\\}$, where $y_{h}>y_{l}$. The investor updates beliefs via Bayesian updating: $\\bar{\\pi_{k}^{t}}=(\\pi_{k}^{0}\\dot{+}n_{k})/(1+\\dot{n}_{h}+\\dot{n_{l}})$. At $t=1$, debt is issued against the asset's cash flow. The investor is risk neutral if the expected default probability is below $\\rho$, but infinitely risk averse if it exceeds $\\rho$. Lemma 1 states that after observing $n_{h}^{0}$ bits of good news, the debt issued is $d_{1}=p(d_{1})=y_{l}$, as the posterior does not shift enough to make default risk tolerable. The debt is information insensitive, so its price remains unchanged at $t=2$.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1611",
    "question": "In the context of the general sample selection model with non-randomly missing outcome data, which of the following conditions are crucial for the consistency of the corrected maximum likelihood estimate of the slope vector β?",
    "options": [
      "A) The estimation method minimizes a criterion function strictly convex in its first argument",
      "B) The regressor variable x is sampled from a degenerate distribution",
      "C) The conditional expectation E(vx|βx,ξx) is linear in βx and ξx for all linear combinations vx",
      "D) There exists an identifying constraint β₁=0, ζ₁≠0"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Distribution-free-and-link-free-estimation-for-the-sam_1987_Journal-of-Econo.md"
    ],
    "question_context": "The general sample selection model deals with non-randomly missing outcome data by specifying an outcome equation y = g(α + βx, ε) and a selection equation I = ι{h(η + ξx, δ) > 0}, where g, h, and error distributions are unknown. Key conditions for consistent estimation of β include: (A.1) The estimation method minimizes a strictly convex criterion function L(α + βx, y); (A.2) x is sampled randomly from a non-degenerate distribution; (A.3) E(vx|βx, ξx) is linear in βx and ξx; (A.4) An identifying constraint β₁=0, ζ₁≠0 exists. The corrected maximum likelihood estimate is consistent under these conditions, even with misspecified link functions.",
    "category": "CORRECT",
    "question_context_used": "The general sample selection model deals with non-randomly missing outcome data by specifying an outcome equation y = g(α + βx, ε) and a selection equation I = ι{h(η + ξx, δ) > 0}, where g, h, and error distributions are unknown. Key conditions for consistent estimation of β include: (A.1) The estimation method minimizes a strictly convex criterion function L(α + βx, y); (A.2) x is sampled randomly from a non-degenerate distribution; (A.3) E(vx|βx, ξx) is linear in βx and ξx; (A.4) An identifying constraint β₁=0, ζ₁≠0 exists. The corrected maximum likelihood estimate is consistent under these conditions, even with misspecified link functions.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2946",
    "question": "Based on the experimental results regarding COVID-19 awareness and compliance, which of the following statements are statistically supported by the data?",
    "options": [
      "A) Treatment T3 (both text and call) was significantly more effective than T2 (call only) in increasing awareness across both countries",
      "B) The inclusion of covariates in the regression models did not qualitatively change the treatment effects",
      "C) The effects of T2 and T3 on compliance were not statistically different from each other",
      "D) The awareness scale results were robust to alternative estimation methods like ordered probit and probit models"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "rest_a_01182.md"
    ],
    "question_context": "Key formula: The treatment effects were estimated using OLS regression with the equation: $y_{i} = \\alpha + \\beta_{1}T2_{i} + \\beta_{2}T3_{i} + \\gamma X_{i} + \\epsilon_{i}$, where $y_{i}$ represents awareness or compliance outcomes, $T2_{i}$ and $T3_{i}$ are treatment indicators, and $X_{i}$ is a vector of control variables. The results show that both T2 and T3 had positive and statistically significant effects (all $p<0.01$) on raising awareness and compliance, with T3 being significantly more effective than T2 ($F$-test: $p<0.01$). The robustness checks using ordered probit and probit models confirmed these findings, as reported in tables B11 and B15 in the appendix. The inclusion of various control variables, such as demographic and additional controls, did not qualitatively alter the treatment effects, indicating the robustness of the results across different model specifications.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The treatment effects were estimated using OLS regression with the equation: $y_{i} = \\alpha + \\beta_{1}T2_{i} + \\beta_{2}T3_{i} + \\gamma X_{i} + \\epsilon_{i}$, where $y_{i}$ represents awareness or compliance outcomes, $T2_{i}$ and $T3_{i}$ are treatment indicators, and $X_{i}$ is a vector of control variables. The results show that both T2 and T3 had positive and statistically significant effects (all $p<0.01$) on raising awareness and compliance, with T3 being significantly more effective than T2 ($F$-test: $p<0.01$). The robustness checks using ordered probit and probit models confirmed these findings, as reported in tables B11 and B15 in the appendix. The inclusion of various control variables, such as demographic and additional controls, did not qualitatively alter the treatment effects, indicating the robustness of the results across different model specifications.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-89",
    "question": "In the context of Principal Components Difference-in-Differences (PCDID) estimators, which of the following statements are correct regarding their theoretical properties and assumptions?",
    "options": [
      "A) PCDID estimators require the assumption of parallel trends to be valid for consistent estimation.",
      "B) The asymptotic normality of PCDID estimators holds under minimal assumptions on the specification of time trends, including nonstationary trends.",
      "C) PCDID uses factor-augmented regressions where factor proxies are constructed exclusively from control units to correct for endogeneity.",
      "D) The basic PCDID estimator targets the Individual Treatment Effect on the Treated (ITET), while the mean-group estimator targets the Average Treatment Effect on the Treated (ATET)."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "21-The PCDID Approach_ Difference-in-Differences When Trends Are Potentially Unparallel and Stoch.md"
    ],
    "question_context": "The PCDID approach is a class of regression-based estimators for treatment effect estimation, analogous to a control function approach. It uses factor proxies constructed from control units to control for unobserved trends, assuming that the unobservables follow an interactive effects structure. Key formula: $y_{i t}=\\Delta_{i t}1_{\\{i\\in E\\}}1_{\\{t>T_{0i}\\}}+\\varsigma_{i}+\\mu_{i}^{\\prime}f_{t}+\\tilde{\\epsilon}_{i t}$. The PCDID estimators are shown to be consistent and asymptotically normal under minimal assumptions on the specification of time trends, encompassing nonstationary trends. The basic PCDID estimator converges to the Individual Treatment Effect on the Treated (ITET) $\\bar{\\Delta}_{i}:=E(\\Delta_{i t}|t>T_{0i})$, while the mean-group estimator converges to the Average Treatment Effect on the Treated (ATET) $\\bar{\\Delta}:=E(\\bar{\\Delta}_{i}|i\\in E)$.",
    "category": "INCORRECT",
    "question_context_used": "The PCDID approach is a class of regression-based estimators for treatment effect estimation, analogous to a control function approach. It uses factor proxies constructed from control units to control for unobserved trends, assuming that the unobservables follow an interactive effects structure. Key formula: $y_{i t}=\\Delta_{i t}1_{\\{i\\in E\\}}1_{\\{t>T_{0i}\\}}+\\varsigma_{i}+\\mu_{i}^{\\prime}f_{t}+\\tilde{\\epsilon}_{i t}$. The PCDID estimators are shown to be consistent and asymptotically normal under minimal assumptions on the specification of time trends, encompassing nonstationary trends. The basic PCDID estimator converges to the Individual Treatment Effect on the Treated (ITET) $\\bar{\\Delta}_{i}:=E(\\Delta_{i t}|t>T_{0i})$, while the mean-group estimator converges to the Average Treatment Effect on the Treated (ATET) $\\bar{\\Delta}:=E(\\bar{\\Delta}_{i}|i\\in E)$.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3760",
    "question": "In the context of spatial pricing theory as discussed by Greenhut and Ohta, which of the following conditions lead to the adoption of discriminatory pricing by a spatial monopolist?",
    "options": [
      "A) Transport costs are significant and buyers are evenly distributed over space",
      "B) Demand functions of buyers are linear and marginal costs of production are zero",
      "C) Market areas are hexagon-shaped under competitive equilibrium",
      "D) Production and transport costs are small, leading to non-discriminatory pricing"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Theory_of_Spatial_Pricing_and_Market_Areas.md"
    ],
    "question_context": "The authors examine the similarities and dissimilarities between non-discriminatory and discriminatory pricing. Specific functions have been used in these models: (1) negatively sloping demand, (2) constant, or zero marginal costs of production, (3) linear distribution of buyers. The assumptions adopted are: $(a)$ transport costs are significant, $(b)$ buyers are evenly distributed over the space, and (c) the demand functions of buyers are linear. Under the above conditions, the conclusion is reached that a spatial monopolist will adopt a discriminatory price system in which distant buyers are not charged as much freight costs as are actually incurred. Under the discriminatory price system, more output is obtained than under a non-discriminatory (f.o.b. mill price) price system. The former also yields greater profits for the spatial monopolist. Moreover, market sizes under this price system are larger, since the rates of freight absorption are greater under spatial price discrimination owing to the fact that profits are maximised at each point in space, rather than over the entire space.",
    "category": "CORRECT",
    "question_context_used": "The authors examine the similarities and dissimilarities between non-discriminatory and discriminatory pricing. Specific functions have been used in these models: (1) negatively sloping demand, (2) constant, or zero marginal costs of production, (3) linear distribution of buyers. The assumptions adopted are: $(a)$ transport costs are significant, $(b)$ buyers are evenly distributed over the space, and (c) the demand functions of buyers are linear. Under the above conditions, the conclusion is reached that a spatial monopolist will adopt a discriminatory price system in which distant buyers are not charged as much freight costs as are actually incurred. Under the discriminatory price system, more output is obtained than under a non-discriminatory (f.o.b. mill price) price system. The former also yields greater profits for the spatial monopolist. Moreover, market sizes under this price system are larger, since the rates of freight absorption are greater under spatial price discrimination owing to the fact that profits are maximised at each point in space, rather than over the entire space.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-746",
    "question": "In the context of the transfer paradox in stable markets, which of the following conditions are necessary for the paradox to occur according to Chichilnisky's 1980 results?",
    "options": [
      "A) Competitive and stable markets with pure exchange",
      "B) The condition $\\lambda<0$ must hold",
      "C) The economy must have distortions as defined by Bhagwati, Brecher, and Hatta",
      "D) The number of agents or countries must be $n\\geq3$"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "The-transfer-problem-in-stable-markets--A-rejoi_1984_Journal-of-Development-.md"
    ],
    "question_context": "The applicability of the results depends on the empirical validity of the assumptions of my model. These are: competitive and stable markets, pure exchange, no-substitution utilities, three agents, and $\\lambda<0.$ The first two assumptions are standard, and in any case the results have been extended to economies with production. Dixit (1983) extended the results to smooth utilities with substitution. About the fourth and fifth assumptions: it is clear that the results apply to any number n of agents or countries. $n\\geq3$ and that the condition $\\lambda<0,$ i.e., that the bystanding region imports the goods which the giver consumes more intensively, is realistic when this region is the NIC's, and is satisfied in a set of measure 1/2 of the parameter space. The results therefore appear to be realistic and robust. Transfers may lead to gains for the giver, and to losses for the receiver.",
    "category": "CORRECT",
    "question_context_used": "The applicability of the results depends on the empirical validity of the assumptions of my model. These are: competitive and stable markets, pure exchange, no-substitution utilities, three agents, and $\\lambda<0.$ The first two assumptions are standard, and in any case the results have been extended to economies with production. Dixit (1983) extended the results to smooth utilities with substitution. About the fourth and fifth assumptions: it is clear that the results apply to any number n of agents or countries. $n\\geq3$ and that the condition $\\lambda<0,$ i.e., that the bystanding region imports the goods which the giver consumes more intensively, is realistic when this region is the NIC's, and is satisfied in a set of measure 1/2 of the parameter space. The results therefore appear to be realistic and robust. Transfers may lead to gains for the giver, and to losses for the receiver.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-907",
    "question": "Under what conditions can an economy sustain a constant per capita income with an exhaustible resource input to production, according to Stiglitz (1974a)?",
    "options": [
      "A) The elasticity of substitution between natural resources and capital is greater than unity",
      "B) The elasticity of substitution is unity (Cobb-Douglas) and the share of capital is less than that of resources",
      "C) There is resource-augmenting technical change",
      "D) The elasticity of substitution is unity (Cobb-Douglas) and the share of capital is greater than that of resources"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Exploitation_of_Extractive_Resources_A_Survey.md"
    ],
    "question_context": "One question of particular interest here is whether or not it is possible for an economy to sustain a constant per capita income with an exhaustible resource input to production. As shown by Stiglitz (1974a, pp. 125-31), the answer is that it is possible if any of the following three conditions are satisfied: (a) the elasticity of substitution between natural resources and capital is greater than unity, (b) the elasticity of substitution is unity (Cobb-Douglas) and the share of capital is greater than that of resources, or (c) there is resource augmenting technical change. These are of course empirical propositions, so it has not been shown that the consequences of resource depletion are unimportant. But the simple fact that resources are finite, and are necessary for production, does not imply that the resource-using economy must eventually stagnate and decline. Since planners probably cannot implement the time paths that would maximise a full-blown welfare function, the relevant policy question is, do we or do we not trust the market? What good or bad properties can we attribute to the time paths generated by present value maximising producers and consumers?",
    "category": "CORRECT",
    "question_context_used": "One question of particular interest here is whether or not it is possible for an economy to sustain a constant per capita income with an exhaustible resource input to production. As shown by Stiglitz (1974a, pp. 125-31), the answer is that it is possible if any of the following three conditions are satisfied: (a) the elasticity of substitution between natural resources and capital is greater than unity, (b) the elasticity of substitution is unity (Cobb-Douglas) and the share of capital is greater than that of resources, or (c) there is resource augmenting technical change. These are of course empirical propositions, so it has not been shown that the consequences of resource depletion are unimportant. But the simple fact that resources are finite, and are necessary for production, does not imply that the resource-using economy must eventually stagnate and decline. Since planners probably cannot implement the time paths that would maximise a full-blown welfare function, the relevant policy question is, do we or do we not trust the market? What good or bad properties can we attribute to the time paths generated by present value maximising producers and consumers?",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3748",
    "question": "Based on the empirical analysis of returns to college education, which of the following statements are correct regarding the impact of Roy sorting on wage distributions?",
    "options": [
      "A) Roy sorting creates an upward bias in the estimated returns to college education by shifting the conditional wage distributions upward.",
      "B) The correction for Roy sorting is generally larger for high school graduates due to their lower mobility.",
      "C) The unconditional wage distribution lies below the observed distribution for both high school and college graduates.",
      "D) Controlling for Roy sorting reduces the estimated returns to college education at both the median and the 75th percentile."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "00-Nonparametric_Identification_and_Estimation_in_a_Roy_Model_With_Common_Nonpecuniary_Returns.md"
    ],
    "question_context": "The results of the basic Roy model (1951) suggest that sorting shifts the means of the (observed) conditional wage distributions up from their (unobserved) unconditional values. Whether spatial sorting increases or reduces the estimated returns to a college education will depend on whether this shift is proportionally bigger for high school or college-educated individuals. If, for example, college-educated individuals were more mobile and thus more able to migrate in response to favorable idiosyncratic wage draws, we would expect spatial sorting to create an upward bias in the estimated returns to a college education. In every case, the unconditional wage distribution lies below the observed distribution. Importantly, the correction for Roy sorting is generally larger for college graduates, who are more prone to migrate from their birth regions. On average, our measure of returns falls from $42\\%$ to $18\\%$ at the median and from $45\\%$ to $34\\%$ at the 75th percentile.",
    "category": "INCORRECT",
    "question_context_used": "The results of the basic Roy model (1951) suggest that sorting shifts the means of the (observed) conditional wage distributions up from their (unobserved) unconditional values. Whether spatial sorting increases or reduces the estimated returns to a college education will depend on whether this shift is proportionally bigger for high school or college-educated individuals. If, for example, college-educated individuals were more mobile and thus more able to migrate in response to favorable idiosyncratic wage draws, we would expect spatial sorting to create an upward bias in the estimated returns to a college education. In every case, the unconditional wage distribution lies below the observed distribution. Importantly, the correction for Roy sorting is generally larger for college graduates, who are more prone to migrate from their birth regions. On average, our measure of returns falls from $42\\%$ to $18\\%$ at the median and from $45\\%$ to $34\\%$ at the 75th percentile.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-808",
    "question": "In the context of Markov-modulated Lévy processes, which of the following conditions are necessary for the spectral abscissa ζ(A(s)) to have a positive solution s = α in the interior of T?",
    "options": [
      "A) The spectral abscissa ζ(A(0)) must be strictly negative.",
      "B) At least one of the Lévy exponents ψn(s) must diverge to infinity as s approaches the right endpoint of T.",
      "C) The matrix Π must be reducible.",
      "D) The random time T must satisfy ℙ(T < ∞) = 0."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "TAIL_BEHAVIOR_OF_STOPPED_LEVY_PROCESSES_WITH_MARKOV_MODULATION.md"
    ],
    "question_context": "The spectral abscissa ζ(A(s)) is defined as the maximum of the real parts of the complex eigenvalues of A(s), where A(s) = Ψ(s) + Π⊙Υ(s) - Φ. Proposition 3.1 states that ζ(A(s)) is a convex function of s ∈ T and satisfies ζ(A(0)) ≤ 0. If ζ(A(0)) < 0, then the equation ζ(A(s)) = 0 admits at most one positive solution s = α ∈ T and at most one negative solution s = -β ∈ T. Proposition 3.2 further establishes that ℙ(T < ∞) = 1 if and only if ζ(A(0)) < 0. A sufficient condition for the existence of a positive s in the interior of T solving ζ(A(s)) = 0 is that (i) ℙ(T < ∞) = 1, and (ii) at least one of the Lévy exponents ψn(s) diverges to infinity as s approaches the right endpoint of T.",
    "category": "CORRECT",
    "question_context_used": "The spectral abscissa ζ(A(s)) is defined as the maximum of the real parts of the complex eigenvalues of A(s), where A(s) = Ψ(s) + Π⊙Υ(s) - Φ. Proposition 3.1 states that ζ(A(s)) is a convex function of s ∈ T and satisfies ζ(A(0)) ≤ 0. If ζ(A(0)) < 0, then the equation ζ(A(s)) = 0 admits at most one positive solution s = α ∈ T and at most one negative solution s = -β ∈ T. Proposition 3.2 further establishes that ℙ(T < ∞) = 1 if and only if ζ(A(0)) < 0. A sufficient condition for the existence of a positive s in the interior of T solving ζ(A(s)) = 0 is that (i) ℙ(T < ∞) = 1, and (ii) at least one of the Lévy exponents ψn(s) diverges to infinity as s approaches the right endpoint of T.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-740",
    "question": "In the context of optimal income taxation with quasilinear preferences, which of the following statements are true regarding the properties of marginal tax rates?",
    "options": [
      "A) With quasilinear-in-leisure preferences, the marginal tax rate depends solely on the skill distribution term C(w).",
      "B) For quasilinear-in-consumption preferences with isoelastic labor supply, the marginal tax rate is influenced by both the elasticity of labor supply and the skill distribution.",
      "C) Under a Pareto skill distribution, the marginal tax rate is constant if the distribution is untruncated.",
      "D) The Weibull distribution always results in a decreasing marginal tax rate across the entire skill distribution."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "Optimal-marginal-and-average-income-taxation-unde_2008_Journal-of-Economic-T.md"
    ],
    "question_context": "The optimal tax structure with quasilinear preferences can be characterized by the expression for marginal tax rates: \n\n$$\\frac{T^{\\prime}(y(w))}{1-T^{\\prime}(y(w))}=A(w)B(w)C(w)\\quad\\forall w\\in W$$\n\nwhere \n\n$$A(w)=\\bigg[1+\\frac{h^{\\prime\\prime}(\\ell(w))\\ell}{h^{\\prime}(\\ell(w))}\\bigg],\\quad B(w)=\\frac{v^{\\prime}(x(w))}{1-F(w)}\\int_{w}^{\\overline{w}}\\frac{f(t)}{v^{\\prime}(x(t))}dt,\\quad C(w)=\\frac{1-F(w)}{w f(w)}.$$\n\nFor quasilinear-in-leisure preferences, $h(\\ell)=\\ell$ and $v^{\\prime}(x)>0>v^{\\prime\\prime}(x)$. Since $h^{\\prime\\prime}(\\ell)=0$, $A(w)=1$. Moreover, $B(w)=1/(1-T^{\\prime}(\\cdot))$, leading to the simplification $T^{\\prime}(y(w))=C(w)$. For quasilinear-in-consumption preferences, $v(x)=x$ and $h^{\\prime\\prime}(\\ell)>0$. Here, $B(w)=1$, and the optimal tax schedule becomes $\\frac{T^{\\prime}(y(w))}{1-T^{\\prime}(y(w))}=A(w)C(w)$, where $A(w)=1+1/e(w)$. If the labor supply elasticity $e(w)$ is constant, this further simplifies to $\\frac{T^{\\prime}(y(w))}{1-T^{\\prime}(y(w))}=\\overline{A}C(w)$, where $\\overline{A}$ is constant.",
    "category": "INCORRECT",
    "question_context_used": "The optimal tax structure with quasilinear preferences can be characterized by the expression for marginal tax rates: \n\n$$\\frac{T^{\\prime}(y(w))}{1-T^{\\prime}(y(w))}=A(w)B(w)C(w)\\quad\\forall w\\in W$$\n\nwhere \n\n$$A(w)=\\bigg[1+\\frac{h^{\\prime\\prime}(\\ell(w))\\ell}{h^{\\prime}(\\ell(w))}\\bigg],\\quad B(w)=\\frac{v^{\\prime}(x(w))}{1-F(w)}\\int_{w}^{\\overline{w}}\\frac{f(t)}{v^{\\prime}(x(t))}dt,\\quad C(w)=\\frac{1-F(w)}{w f(w)}.$$\n\nFor quasilinear-in-leisure preferences, $h(\\ell)=\\ell$ and $v^{\\prime}(x)>0>v^{\\prime\\prime}(x)$. Since $h^{\\prime\\prime}(\\ell)=0$, $A(w)=1$. Moreover, $B(w)=1/(1-T^{\\prime}(\\cdot))$, leading to the simplification $T^{\\prime}(y(w))=C(w)$. For quasilinear-in-consumption preferences, $v(x)=x$ and $h^{\\prime\\prime}(\\ell)>0$. Here, $B(w)=1$, and the optimal tax schedule becomes $\\frac{T^{\\prime}(y(w))}{1-T^{\\prime}(y(w))}=A(w)C(w)$, where $A(w)=1+1/e(w)$. If the labor supply elasticity $e(w)$ is constant, this further simplifies to $\\frac{T^{\\prime}(y(w))}{1-T^{\\prime}(y(w))}=\\overline{A}C(w)$, where $\\overline{A}$ is constant.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3012",
    "question": "In the context of the main theorem and its proof, which of the following statements are correct regarding the critical value $v_{1}^{c}$ and its implications?",
    "options": [
      "A) If $v_{1}^{c} > y_{1}^{1}$, a BTPIT can be performed to benefit the poorest agent in category 1 while preserving OPG dominance.",
      "B) If $v_{1}^{c} = y_{1}^{1}$, an FIP must be performed to maintain OPG dominance, as no BTPIT is possible.",
      "C) The critical vector $\\mathbf{v}^{c}$ is used to identify the smallest set of poverty lines where the poverty gap is equal in both distributions.",
      "D) In case $(A)$, an FIP is always preferred over a BTPIT if both are possible, to avoid infinite sequences of transfers."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "Int Economic Review - 2020 - Faure - REDUCING INEQUALITIES AMONG UNEQUALS.md"
    ],
    "question_context": "The main theorem establishes the equivalence of four statements regarding distributions $\\mathbf{x}$ and $\\mathbf{y}$ in $\\mathcal{D}(I)$. Key to the proof is the critical value $v_{1}^{c}$, defined as the lowest poverty threshold above the smallest income in the lowest category in $\\mathbf{y}$ that can be part of a collection of ordered poverty thresholds for which the OPG in $\\mathbf{x}$ and $\\mathbf{y}$ is the same. The critical vector $\\mathbf{v}^{c} := (v_{1}^{c}, \\ldots, v_{k}^{c})$ is a diagnostic tool to identify whether a BTPIT or FIP can be performed at each step of the algorithm. Two cases are distinguished: (A) $v_{1}^{c} > y_{1}^{1}$, where a BTPIT or FIP is possible, and (B) $v_{1}^{c} = y_{1}^{1}$, where only an FIP is possible. The algorithm ensures finiteness by preferring FIPs in certain scenarios to avoid infinite sequences of transfers.",
    "category": "INCORRECT",
    "question_context_used": "The main theorem establishes the equivalence of four statements regarding distributions $\\mathbf{x}$ and $\\mathbf{y}$ in $\\mathcal{D}(I)$. Key to the proof is the critical value $v_{1}^{c}$, defined as the lowest poverty threshold above the smallest income in the lowest category in $\\mathbf{y}$ that can be part of a collection of ordered poverty thresholds for which the OPG in $\\mathbf{x}$ and $\\mathbf{y}$ is the same. The critical vector $\\mathbf{v}^{c} := (v_{1}^{c}, \\ldots, v_{k}^{c})$ is a diagnostic tool to identify whether a BTPIT or FIP can be performed at each step of the algorithm. Two cases are distinguished: (A) $v_{1}^{c} > y_{1}^{1}$, where a BTPIT or FIP is possible, and (B) $v_{1}^{c} = y_{1}^{1}$, where only an FIP is possible. The algorithm ensures finiteness by preferring FIPs in certain scenarios to avoid infinite sequences of transfers.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2213",
    "question": "Under the stochastic unit root process defined in Equation (3), which of the following conditions are necessary for the asymptotic distribution of the DF coefficient test statistic \\( Z_{\\theta} \\) to hold?",
    "options": [
      "A) The latent Markov chain \\( S_{T,t} \\) has constant transition probabilities",
      "B) The parameter \\( \\mu_0 \\) is restricted to zero to exclude linear time trends in the UR regime",
      "C) The ergodic probability associated with the stationary regime is nonzero",
      "D) The process \\( R(u) \\) is defined in terms of a standard Brownian motion independent of \\( S \\)"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Cavaliere和Georgiev - 2008 - Regime-switching autoregressive coefficients and the asymptotics for unit root tests.md"
    ],
    "question_context": "The stochastic unit root process is defined by the two-regime AR process: \n\n$$ Y_{t}=\\left\\{\\begin{array}{l l}{\\mu_{0}+Y_{t-1}+\\varepsilon_{t}\\quad\\mathrm{if}S_{T,t}=0} {\\mu_{1}+\\theta Y_{t-1}+\\varepsilon_{t}\\quad\\mathrm{if}S_{T,t}=1}\\end{array}\\right.\\quad\\quad t=1,\\dots,T, $$ \n\nwhere \\( |\\theta|<1 \\), \\( Y_{0}=O_{p}(1) \\), and \\( \\left\\{\\varepsilon_{t}\\right\\} \\) is \\( \\operatorname{IID}(0,\\sigma^{2}) \\) and is independent of \\( \\{S_{T,t}\\} \\). The asymptotic distribution of the DF coefficient test statistic \\( Z_{\\theta} \\) under this data generating process is derived in Theorem 1, which requires \\( \\mu_0 = 0 \\) to rule out linear time trends in the UR regime and involves the process \\( R(u) \\) defined in terms of a standard Brownian motion independent of \\( S \\).",
    "category": "CORRECT",
    "question_context_used": "The stochastic unit root process is defined by the two-regime AR process: \n\n$$ Y_{t}=\\left\\{\\begin{array}{l l}{\\mu_{0}+Y_{t-1}+\\varepsilon_{t}\\quad\\mathrm{if}S_{T,t}=0} {\\mu_{1}+\\theta Y_{t-1}+\\varepsilon_{t}\\quad\\mathrm{if}S_{T,t}=1}\\end{array}\\right.\\quad\\quad t=1,\\dots,T, $$ \n\nwhere \\( |\\theta|<1 \\), \\( Y_{0}=O_{p}(1) \\), and \\( \\left\\{\\varepsilon_{t}\\right\\} \\) is \\( \\operatorname{IID}(0,\\sigma^{2}) \\) and is independent of \\( \\{S_{T,t}\\} \\). The asymptotic distribution of the DF coefficient test statistic \\( Z_{\\theta} \\) under this data generating process is derived in Theorem 1, which requires \\( \\mu_0 = 0 \\) to rule out linear time trends in the UR regime and involves the process \\( R(u) \\) defined in terms of a standard Brownian motion independent of \\( S \\).",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2194",
    "question": "Which of the following statements are supported by the evidence regarding consumer learning and lock-in effects in the insurance market?",
    "options": [
      "A) The impact of successive claims on consumer expectations declines steeply, suggesting rapid learning.",
      "B) Tenure effects and switching costs significantly dampen the impact of learning on consumer departures.",
      "C) Immediate learning about a firm's quality reduces the five-year survival probability by over 40% when switching costs are minimized.",
      "D) The welfare gain from full information about all firms is negligible compared to immediate learning about only the study firm."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Israel-ServicesExperienceGoods-2005.md"
    ],
    "question_context": "The low value of $\\psi_{p}$ suggests the potential for rapid post-entry learning with claims experience. Table 6 illustrates this by considering a 40-year-old consumer's expectation of the firm's quality as a function of total claims experienced, $C_{t},$ and claims that received good service, $G_{t}$. After one claim, the expected probability of receiving good service ranges from 50 percent to 95 percent, depending on the service received. After five claims, the range is 18 percent to 98 percent. The relative impact of successive claims provides evidence that these claim impacts reflect learning. For any given experience, represented by a cell in the table, the next claim can have two effects—-bad service pushes the individual straight down one row, while good service pushes him diagonally down and to the right. So, define the impact of the service received on each claim as the difference in the expected probability of adjacent cells. For the first claim, this impact is roughly 45 percentage points; it declines to roughly 29 points for the second claim and continues to decline to 16 points for the fifth claim. This steeply declining path suggests learning, as it is harder to generate with other explanations for the impact of claims. Also note that the increase in $\\psi_{p}$ with age means that this path flattens for older consumers, suggesting that they know more at entry, providing further evidence for learning. \n\nPanel D of Table 8 yields a nice summary of the results. Combining no tenure effects and minimum switching costs, immediate learning about the firm's quality reduces the five-year survival probability by 42.1 percent versus no information. However, learning from the true, slow arrival of claims reduces this survival probability by only 8.9 percent. And the presence of tenure effects and sample average switching costs means that even immediate learning reduces the five-year survival probability by only 28.1 percent. Finally, combining slow information arrival and lock-in, the five-year survival probability is reduced by only 4.2 percent, versus the full potential reduction of 42.1 percent.",
    "category": "CORRECT",
    "question_context_used": "The low value of $\\psi_{p}$ suggests the potential for rapid post-entry learning with claims experience. Table 6 illustrates this by considering a 40-year-old consumer's expectation of the firm's quality as a function of total claims experienced, $C_{t},$ and claims that received good service, $G_{t}$. After one claim, the expected probability of receiving good service ranges from 50 percent to 95 percent, depending on the service received. After five claims, the range is 18 percent to 98 percent. The relative impact of successive claims provides evidence that these claim impacts reflect learning. For any given experience, represented by a cell in the table, the next claim can have two effects—-bad service pushes the individual straight down one row, while good service pushes him diagonally down and to the right. So, define the impact of the service received on each claim as the difference in the expected probability of adjacent cells. For the first claim, this impact is roughly 45 percentage points; it declines to roughly 29 points for the second claim and continues to decline to 16 points for the fifth claim. This steeply declining path suggests learning, as it is harder to generate with other explanations for the impact of claims. Also note that the increase in $\\psi_{p}$ with age means that this path flattens for older consumers, suggesting that they know more at entry, providing further evidence for learning. \n\nPanel D of Table 8 yields a nice summary of the results. Combining no tenure effects and minimum switching costs, immediate learning about the firm's quality reduces the five-year survival probability by 42.1 percent versus no information. However, learning from the true, slow arrival of claims reduces this survival probability by only 8.9 percent. And the presence of tenure effects and sample average switching costs means that even immediate learning reduces the five-year survival probability by only 28.1 percent. Finally, combining slow information arrival and lock-in, the five-year survival probability is reduced by only 4.2 percent, versus the full potential reduction of 42.1 percent.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4670",
    "question": "Which of the following statements are supported by the empirical evidence presented in the study regarding dental insurance claims and coverage selection?",
    "options": [
      "A) Households switching to Plan H exhibit a significant short-term increase in claims post-switch, followed by a return to baseline levels.",
      "B) The positive correlation between coverage choice and claim realization is entirely driven by across-household differences rather than within-household dynamics.",
      "C) Approximately 65% of the positive correlation between claims and coverage choices is attributable to within-household variation over time.",
      "D) The study finds no statistically significant evidence of strategic claim delay behavior around annual benefit resets."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Claim Timing and Ex Post Adverse Selection.md"
    ],
    "question_context": "The study examines asymmetric information in dental insurance markets, focusing on claim timing and coverage selection incentives. Key findings include: (1) A positive correlation between coverage choice (Plan H) and claim realization, with 65% of this correlation attributable to within-household variation over time (Table 4). (2) Direct evidence of strategic claim delay, where households switching to more generous coverage show a 62% increase in claims immediately after switching (January), which returns to baseline levels by year-end (Figure 3). The empirical model estimates insurer expenditures per household under Plan H are $144.2 higher for households enrolled in Plan H compared to Plan L when controlling for household fixed effects. The timing patterns suggest ex post adverse selection, where individuals delay treatment in anticipation of switching to more generous coverage. Formula: The test for positive correlation uses the specification: $$\\mathrm{Claims}_{h,t}=\\gamma+\\alpha\\mathrm{Choice}_{h,t}+X_{h,t}\\beta+\\epsilon_{h,t}$$ where $\\alpha$ captures the correlation between coverage choice and claims.",
    "category": "INCORRECT",
    "question_context_used": "The study examines asymmetric information in dental insurance markets, focusing on claim timing and coverage selection incentives. Key findings include: (1) A positive correlation between coverage choice (Plan H) and claim realization, with 65% of this correlation attributable to within-household variation over time (Table 4). (2) Direct evidence of strategic claim delay, where households switching to more generous coverage show a 62% increase in claims immediately after switching (January), which returns to baseline levels by year-end (Figure 3). The empirical model estimates insurer expenditures per household under Plan H are $144.2 higher for households enrolled in Plan H compared to Plan L when controlling for household fixed effects. The timing patterns suggest ex post adverse selection, where individuals delay treatment in anticipation of switching to more generous coverage. Formula: The test for positive correlation uses the specification: $$\\mathrm{Claims}_{h,t}=\\gamma+\\alpha\\mathrm{Choice}_{h,t}+X_{h,t}\\beta+\\epsilon_{h,t}$$ where $\\alpha$ captures the correlation between coverage choice and claims.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1648",
    "question": "Which of the following statements accurately describe the scope and purpose of Econometrica as outlined in its front matter?",
    "options": [
      "A) It focuses exclusively on theoretical econometrics without empirical applications",
      "B) It serves as a platform for advancing economic theory in relation to statistics and mathematics",
      "C) It is published by a national society with limited international reach",
      "D) It prioritizes the publication of articles with immediate policy implications over theoretical contributions"
    ],
    "gold_answer": "B",
    "source_files": [
      "FrontMatter-1996.md"
    ],
    "question_context": "Econometrica is described as an international journal published by The Econometric Society, with a focus on advancing economic theory in its relation to statistics and mathematics. The journal's front matter explicitly states its international scope and its dedication to theoretical advancements, positioning it as a key publication for high-level econometric and economic theory research. The stable URL provided links to the journal's long-standing presence in academic discourse, dating back to at least 1996.",
    "category": "CORRECT",
    "question_context_used": "Econometrica is described as an international journal published by The Econometric Society, with a focus on advancing economic theory in its relation to statistics and mathematics. The journal's front matter explicitly states its international scope and its dedication to theoretical advancements, positioning it as a key publication for high-level econometric and economic theory research. The stable URL provided links to the journal's long-standing presence in academic discourse, dating back to at least 1996.",
    "gold_answer_parsed": [
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3161",
    "question": "Which of the following economists listed in the 1986 COUNCIL are known for their foundational contributions to general equilibrium theory and mechanism design?",
    "options": [
      "A) Kenneth Arrow",
      "B) Jacques Dreze",
      "C) Leonid Hurwicz",
      "D) Joseph E. Stiglitz"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "BackMatter-1986.md"
    ],
    "question_context": "The 1986 COUNCIL includes prominent economists such as Kenneth Arrow, known for his work in general equilibrium theory and social choice theory, and Leonid Hurwicz, a pioneer in mechanism design. Arrow's Impossibility Theorem and Hurwicz's mechanism design framework are cornerstones of modern economic theory. The council also features other distinguished economists like Jacques Dreze, who contributed to public economics, and Joseph E. Stiglitz, known for information asymmetry and market failures. The text provides a snapshot of leading economists in 1986, highlighting their institutional affiliations and geographic distribution.",
    "category": "CORRECT",
    "question_context_used": "The 1986 COUNCIL includes prominent economists such as Kenneth Arrow, known for his work in general equilibrium theory and social choice theory, and Leonid Hurwicz, a pioneer in mechanism design. Arrow's Impossibility Theorem and Hurwicz's mechanism design framework are cornerstones of modern economic theory. The council also features other distinguished economists like Jacques Dreze, who contributed to public economics, and Joseph E. Stiglitz, known for information asymmetry and market failures. The text provides a snapshot of leading economists in 1986, highlighting their institutional affiliations and geographic distribution.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2642",
    "question": "In the stochastic log-volatility model, which of the following statements are correct regarding the moments and derivatives of the discrepancy functions?",
    "options": [
      "A) The expectation of the second moment \\( m_{12} = E(y_t^2) \\) is given by \\( e^{\\mu_h + \\frac{1}{2}\\sigma_h^2} \\).",
      "B) The derivative of the discrepancy function \\( g_{12} \\) with respect to \\( \\alpha \\) is \\( -m_{12} \\frac{1}{1-\\delta} \\).",
      "C) The expectation of the fourth moment \\( m_{14} = E(y_t^4) \\) is given by \\( 3e^{2\\mu_h + 2\\sigma_h^2} \\).",
      "D) The derivative of the discrepancy function \\( g_{14} \\) with respect to \\( \\sigma_\\nu \\) is \\( -m_{14} \\frac{4\\sigma_\\nu}{1-\\delta^2} \\)."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "06-Bayesian_Analysis_of_Stochastic_Volatility_Models.md"
    ],
    "question_context": "The stochastic log-volatility model is defined as:\n\n$$\n\\begin{array}{c}{{y_{t}=\\sqrt{h_{t}}e_{t}=\\sigma_{t}e_{t},\\qquade_{t}\\sim N(0,1)}}\\\\ {{\\log h_{t}=\\alpha+\\delta\\log h_{t-1}+\\sigma_{\\nu}\\nu_{t},\\qquad\\nu_{t}\\sim N(0,1).}}\\end{array}\n$$\n\nThe moments of the model are derived as follows:\n\n$$\n\\begin{array}{r l}&{m_{12}=E(y_{t}^{2})=E(h_{t}e_{t}^{2})=E(h_{t})E(e_{t}^{2})=e^{\\mu_{h}+1/2\\sigma_{h}^{2}}}\\\\ &{m_{14}=E(y_{t}^{4})=E(h_{t}^{2}e_{t}^{4})=E(h_{t}^{2})E(e_{t}^{4})=3e^{2\\mu_{h}+2\\sigma_{h}^{2}}}\\\\ &{m_{16}=E(y_{t}^{6})=E(h_{t}^{3}e_{t}^{6})=E(h_{t}^{3})E(e_{t}^{6})=15e^{3\\mu_{h}+9/2\\sigma_{h}^{2}}.}\\end{array}\n$$\n\nThe derivatives of the discrepancy functions are computed as:\n\n$$\n\\begin{array}{l}{{g_{12}=\\frac{1}{T}\\sum_{p_{t}^{2}}^{\\prime}-e^{\\mu_{h}+1/2\\sigma_{h}^{2}}\\equiv\\widehat{m}_{12}-m_{12}}}\\\\ {{\\displaystyle\\frac{\\partial g_{12}}{\\partial\\alpha}=-m_{12}\\frac{1}{1-\\delta}}}\\\\ {{\\displaystyle\\frac{\\partial g_{12}}{\\partial\\delta}=-m_{12}\\bigg(\\frac{\\alpha}{(1-\\delta)^{2}}+\\frac{\\delta\\sigma_{v}^{2}}{(1-\\delta^{2})^{2}}\\bigg)}}\\\\ {{\\displaystyle\\qquad=-m_{12}\\bigg(\\frac{\\mu_{h}}{1-\\delta}+\\frac{\\delta\\sigma_{h}^{2}}{1-\\delta^{2}}\\bigg)}}\\\\ {{\\displaystyle\\frac{\\partial g_{12}}{\\partial\\sigma_{v}}=-m_{12}\\frac{\\sigma_{v}}{1-\\delta^{2}};}}\\end{array}\n$$\n\n$$\ng_{14}=\\frac{1}{T}\\sum y_{t}^{4}-3e^{2\\mu_{h}+2\\sigma_{h}^{2}}\\equiv\\widehat{m}_{14}-m_{14}\n$$\n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial g_{14}}{\\partial\\alpha}=-m_{14}\\frac{2}{1-\\delta}}\\\\ {\\displaystyle\\frac{\\partial g_{14}}{\\partial\\delta}=-m_{14}\\Biggl(\\frac{2\\alpha}{(1-\\delta)^{2}}+\\frac{4\\delta\\sigma_{\\nu}^{2}}{(1-\\delta^{2})^{2}}\\Biggr)}\\\\ {\\displaystyle\\qquad=-m_{14}\\Biggl(2\\mu_{h}1-\\delta+\\frac{4\\delta\\sigma_{h}^{2}}{1-\\delta^{2}}\\Biggr)}\\end{array}\n$$\n\n$$\n\\frac{\\partial g_{14}}{\\partial\\sigma_{\\nu}}=-m_{14}\\frac{4\\sigma_{\\nu}}{1-\\delta^{2}};\n$$",
    "category": "CORRECT",
    "question_context_used": "The stochastic log-volatility model is defined as:\n\n$$\n\\begin{array}{c}{{y_{t}=\\sqrt{h_{t}}e_{t}=\\sigma_{t}e_{t},\\qquade_{t}\\sim N(0,1)}}\\\\ {{\\log h_{t}=\\alpha+\\delta\\log h_{t-1}+\\sigma_{\\nu}\\nu_{t},\\qquad\\nu_{t}\\sim N(0,1).}}\\end{array}\n$$\n\nThe moments of the model are derived as follows:\n\n$$\n\\begin{array}{r l}&{m_{12}=E(y_{t}^{2})=E(h_{t}e_{t}^{2})=E(h_{t})E(e_{t}^{2})=e^{\\mu_{h}+1/2\\sigma_{h}^{2}}}\\\\ &{m_{14}=E(y_{t}^{4})=E(h_{t}^{2}e_{t}^{4})=E(h_{t}^{2})E(e_{t}^{4})=3e^{2\\mu_{h}+2\\sigma_{h}^{2}}}\\\\ &{m_{16}=E(y_{t}^{6})=E(h_{t}^{3}e_{t}^{6})=E(h_{t}^{3})E(e_{t}^{6})=15e^{3\\mu_{h}+9/2\\sigma_{h}^{2}}.}\\end{array}\n$$\n\nThe derivatives of the discrepancy functions are computed as:\n\n$$\n\\begin{array}{l}{{g_{12}=\\frac{1}{T}\\sum_{p_{t}^{2}}^{\\prime}-e^{\\mu_{h}+1/2\\sigma_{h}^{2}}\\equiv\\widehat{m}_{12}-m_{12}}}\\\\ {{\\displaystyle\\frac{\\partial g_{12}}{\\partial\\alpha}=-m_{12}\\frac{1}{1-\\delta}}}\\\\ {{\\displaystyle\\frac{\\partial g_{12}}{\\partial\\delta}=-m_{12}\\bigg(\\frac{\\alpha}{(1-\\delta)^{2}}+\\frac{\\delta\\sigma_{v}^{2}}{(1-\\delta^{2})^{2}}\\bigg)}}\\\\ {{\\displaystyle\\qquad=-m_{12}\\bigg(\\frac{\\mu_{h}}{1-\\delta}+\\frac{\\delta\\sigma_{h}^{2}}{1-\\delta^{2}}\\bigg)}}\\\\ {{\\displaystyle\\frac{\\partial g_{12}}{\\partial\\sigma_{v}}=-m_{12}\\frac{\\sigma_{v}}{1-\\delta^{2}};}}\\end{array}\n$$\n\n$$\ng_{14}=\\frac{1}{T}\\sum y_{t}^{4}-3e^{2\\mu_{h}+2\\sigma_{h}^{2}}\\equiv\\widehat{m}_{14}-m_{14}\n$$\n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{\\partial g_{14}}{\\partial\\alpha}=-m_{14}\\frac{2}{1-\\delta}}\\\\ {\\displaystyle\\frac{\\partial g_{14}}{\\partial\\delta}=-m_{14}\\Biggl(\\frac{2\\alpha}{(1-\\delta)^{2}}+\\frac{4\\delta\\sigma_{\\nu}^{2}}{(1-\\delta^{2})^{2}}\\Biggr)}\\\\ {\\displaystyle\\qquad=-m_{14}\\Biggl(2\\mu_{h}1-\\delta+\\frac{4\\delta\\sigma_{h}^{2}}{1-\\delta^{2}}\\Biggr)}\\end{array}\n$$\n\n$$\n\\frac{\\partial g_{14}}{\\partial\\sigma_{\\nu}}=-m_{14}\\frac{4\\sigma_{\\nu}}{1-\\delta^{2}};\n$$",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3629",
    "question": "In the context of salience and taxation, which of the following statements are supported by the empirical findings of Chetty, Looney, and Kroft (2009)?",
    "options": [
      "A) Consumers react more strongly to taxes that are included in posted prices than to taxes applied at the register.",
      "B) The economic incidence of a tax is independent of its statutory incidence under full optimization assumptions.",
      "C) Posting tax-inclusive prices reduces demand by an amount comparable to a price increase equivalent to the tax rate.",
      "D) The welfare consequences of taxation can be fully characterized by estimating only the tax elasticity of demand."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Chetty-SalienceTaxationTheory-2009.md"
    ],
    "question_context": "The paper investigates how consumers underreact to taxes that are not salient, using two empirical strategies: a field experiment in a grocery store and an observational study of alcohol taxes. Key formula: The degree of underreaction to taxes is measured by the ratio of the tax elasticity of demand to the price elasticity of demand, denoted as θτ. The empirical results show that posting tax-inclusive prices reduces demand by about 8 percent, comparable to the effect of a price increase equal to the tax rate. The welfare analysis reveals that the economic incidence of a tax depends on its statutory incidence, contrary to the classic tax neutrality result under full optimization. The paper also demonstrates that even policies inducing no change in behavior can create efficiency losses, challenging traditional welfare analysis assumptions.",
    "category": "INCORRECT",
    "question_context_used": "The paper investigates how consumers underreact to taxes that are not salient, using two empirical strategies: a field experiment in a grocery store and an observational study of alcohol taxes. Key formula: The degree of underreaction to taxes is measured by the ratio of the tax elasticity of demand to the price elasticity of demand, denoted as θτ. The empirical results show that posting tax-inclusive prices reduces demand by about 8 percent, comparable to the effect of a price increase equal to the tax rate. The welfare analysis reveals that the economic incidence of a tax depends on its statutory incidence, contrary to the classic tax neutrality result under full optimization. The paper also demonstrates that even policies inducing no change in behavior can create efficiency losses, challenging traditional welfare analysis assumptions.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"A\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1173",
    "question": "In the context of endogenous skill-biased technical change, which of the following conditions are necessary to satisfy the strong induced-bias hypothesis ($\\sigma > 2 - \\delta$)?",
    "options": [
      "A) The elasticity of substitution between skilled and unskilled workers must be greater than 1",
      "B) The degree of state dependence in the innovation possibilities frontier must be sufficiently low",
      "C) The market size effect must dominate both the price effect and the substitution effect",
      "D) The relative supply of skills must increase at an accelerating rate"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Directed Technical Change.md"
    ],
    "question_context": "The strong induced-bias hypothesis, i.e. the case with $\\sigma > 2 - \\delta$, also offers an explanation for the behaviour of the college premium during the 1970's and the 1980's shown in Figure 1. Suppose that the economy is hit by a large increase in $H/L$. If this increase is not anticipated sufficiently in advance, technology will not have adjusted by the time the supply of skills increases. The initial response of the skill premium will be given by equation (18) which takes technology as given. Therefore, the skill premium will first decline, but then as technology starts adjusting, it will rise rapidly. Figure 3 draws this case diagrammatically. \n\nWith $\\sigma$ large enough, the market size effect is so powerful that it not only dominates the price effect on the direction of technical change, but also dominates the usual substitution effect between skilled and unskilled workers at a given technology (as captured by equation (18) above). As a result, an increase in the relative supply of skills makes technology sufficiently skill biased to raise the skill premium. In this case, the framework predicts sufficiently biased technical change over the past 60 years to actually increase the skill premium, consistent with the pattern depicted in Figure 1.",
    "category": "INCORRECT",
    "question_context_used": "The strong induced-bias hypothesis, i.e. the case with $\\sigma > 2 - \\delta$, also offers an explanation for the behaviour of the college premium during the 1970's and the 1980's shown in Figure 1. Suppose that the economy is hit by a large increase in $H/L$. If this increase is not anticipated sufficiently in advance, technology will not have adjusted by the time the supply of skills increases. The initial response of the skill premium will be given by equation (18) which takes technology as given. Therefore, the skill premium will first decline, but then as technology starts adjusting, it will rise rapidly. Figure 3 draws this case diagrammatically. \n\nWith $\\sigma$ large enough, the market size effect is so powerful that it not only dominates the price effect on the direction of technical change, but also dominates the usual substitution effect between skilled and unskilled workers at a given technology (as captured by equation (18) above). As a result, an increase in the relative supply of skills makes technology sufficiently skill biased to raise the skill premium. In this case, the framework predicts sufficiently biased technical change over the past 60 years to actually increase the skill premium, consistent with the pattern depicted in Figure 1.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2333",
    "question": "In the context of climate's impact on regional economic variables, which of the following statements are correct based on the given model?",
    "options": [
      "A) The wage rate in the region with better climate (w1) is always lower than in the region with inferior climate (w2).",
      "B) The price of housing in region 1 (ph1) is always greater than in region 2 (ph2) when 1−σ > 1−δ.",
      "C) The net effect on housing prices due to climate depends on the offsetting interactions between production costs, wage rates, and population size.",
      "D) The market allocation of population is independent of whether workers are renters or owners of housing when regional differences are solely climatic."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Regional Distribution of Population, Migration, and Climate.md"
    ],
    "question_context": "The model examines how climate affects regional economic variables through consumption and production. Key equations include:\n\n$$\\ln(L_{1}/L_{2})=(\\theta/K_{2})\\ln(C_{1}/C_{2}),$$\n\nwhich shows population distribution depends on climate's utility parameter θ. Wage differentials arise due to diseconomies of scale:\n\n$$\\ln(w_{1}/w_{2})=-(\\delta\\theta/K_{2})\\ln(C_{1}/C_{2}).$$\n\nHousing prices are determined by:\n\n$$\\ln(p_{h_{1}}/p_{h_{2}})=(\\theta/K_{2})(\\sigma-\\delta)\\ln(C_{1}/C_{2}),$$\n\nwhere ph1≷ph2 as 1−σ≶1−δ. If climate affects housing productivity, population distribution follows:\n\n$$\\ln(L_{1}/L_{2})=\\alpha_{3}\\sigma/K_{2}\\ln(C_{1}/C_{2}),$$\n\nwith wage and housing price effects offsetting production impacts. The aggregate population distribution is:\n\n$$\\ln(L_{1}/L_{2})=((\\theta+K_{2})/K_{2})\\ln(C_{1}/C_{2}).$$",
    "category": "INCORRECT",
    "question_context_used": "The model examines how climate affects regional economic variables through consumption and production. Key equations include:\n\n$$\\ln(L_{1}/L_{2})=(\\theta/K_{2})\\ln(C_{1}/C_{2}),$$\n\nwhich shows population distribution depends on climate's utility parameter θ. Wage differentials arise due to diseconomies of scale:\n\n$$\\ln(w_{1}/w_{2})=-(\\delta\\theta/K_{2})\\ln(C_{1}/C_{2}).$$\n\nHousing prices are determined by:\n\n$$\\ln(p_{h_{1}}/p_{h_{2}})=(\\theta/K_{2})(\\sigma-\\delta)\\ln(C_{1}/C_{2}),$$\n\nwhere ph1≷ph2 as 1−σ≶1−δ. If climate affects housing productivity, population distribution follows:\n\n$$\\ln(L_{1}/L_{2})=\\alpha_{3}\\sigma/K_{2}\\ln(C_{1}/C_{2}),$$\n\nwith wage and housing price effects offsetting production impacts. The aggregate population distribution is:\n\n$$\\ln(L_{1}/L_{2})=((\\theta+K_{2})/K_{2})\\ln(C_{1}/C_{2}).$$",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-949",
    "question": "In the context of the constrained efficiency model with long-term debt, which of the following statements are true regarding the firm's behavior and equilibrium outcomes?",
    "options": [
      "A) The firm's choice of debt is independent of the existing stock of debt when it internalizes the value of existing debt.",
      "B) Leverage and default risk become constant in productivity when the firm internalizes the value of existing debt.",
      "C) The equilibrium with long-term debt leads to slow debt and deep recessions due to the firm's disregard for existing debt.",
      "D) The issuance cost for short-term debt makes it more attractive than long-term debt due to lower rollover frequency."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Slow Debt Deep Recessions.md"
    ],
    "question_context": "The constrained efficient allocation maximizes firm value (i.e., the value of all equity and debt claims) instead of shareholder value. The planner’s objective reads as: \n\n$$ p b-e+\\frac{1}{1+r}\\int_{\\varepsilon}^{\\infty}\\Bigl[k-\\tilde{b}+\\bigl(1-\\tau\\bigr)\\bigl(y+\\bar{\\varepsilon}k-w l-\\delta k-c\\tilde{b}\\bigr)\\Bigr]\\varphi(\\varepsilon)d\\varepsilon. $$\n\nProposition 4 states that if the firm internalizes the value of existing debt, the choice of debt $\\ddot{b}$ is independent of $b$. Leverage, default risk, and the credit spread are constant in productivity $z$ for any value of $b$. The allocation described by Proposition 4 is markedly different from the equilibrium where neither slow debt nor deep recessions arise. The difference is entirely due to the stock of existing debt $b$.",
    "category": "INCORRECT",
    "question_context_used": "The constrained efficient allocation maximizes firm value (i.e., the value of all equity and debt claims) instead of shareholder value. The planner’s objective reads as: \n\n$$ p b-e+\\frac{1}{1+r}\\int_{\\varepsilon}^{\\infty}\\Bigl[k-\\tilde{b}+\\bigl(1-\\tau\\bigr)\\bigl(y+\\bar{\\varepsilon}k-w l-\\delta k-c\\tilde{b}\\bigr)\\Bigr]\\varphi(\\varepsilon)d\\varepsilon. $$\n\nProposition 4 states that if the firm internalizes the value of existing debt, the choice of debt $\\ddot{b}$ is independent of $b$. Leverage, default risk, and the credit spread are constant in productivity $z$ for any value of $b$. The allocation described by Proposition 4 is markedly different from the equilibrium where neither slow debt nor deep recessions arise. The difference is entirely due to the stock of existing debt $b$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1545",
    "question": "In the context of the Malawi Demographic and Health Survey (MDHS) analysis, which of the following statements are correct regarding the empirical strategy to evaluate the impact of the ban on informal birth attendants?",
    "options": [
      "A) The difference-in-difference estimation relies on parallel trends between high- and low-exposure villages prior to the ban.",
      "B) The treatment variable is defined as the interaction between the high-exposure indicator and the post-ban period.",
      "C) The linear probability model is preferred over fixed effects logit due to its ability to handle arbitrary assumptions about fixed effects.",
      "D) The empirical strategy assumes that high- and low-exposure villages have identical characteristics for identification."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Does-a-ban-on-informal-health-providers-save-live_2016_Journal-of-Developmen.md"
    ],
    "question_context": "The empirical approach uses a difference-in-difference estimation strategy that exploits variation across time and space in 'intensity of exposure' to the government ban. The proxy for intensity of exposure is the village-level historical rate of informal birth attendant use. The treatment variable is the interaction between the high-exposure variable and Post, a binary indicator for births that take place after the ban. The basic regression specification is as follows:\n\n$$\n\\begin{array}{r}{Y_{i c d t}=\\alpha_{1}+\\alpha_{2}P o s t_{t}+\\delta H i g h E x p o s u r e_{c}+\\gamma H i g h E x p o s u r e_{c}*P o s t_{t}}\\\\{+X_{i c t}\\beta+\\eta_{d}+\\tau_{t}+\\epsilon_{i c d t}.\\qquad\\quad}\\end{array}\n$$\n\nEven though most outcomes are binary, a linear probability model is preferred primarily because it allows for ease of interpretation of the coefficients on the interaction terms. With fixed effects logit models, one cannot calculate marginal effects of independent variables without making arbitrary assumptions about the value of the fixed effects. The identification assumption does not require that high and low-exposure villages be similar, only that trends be similar. This is tested by examining pre-ban trends in outcomes across high- and low-exposure villages.",
    "category": "CORRECT",
    "question_context_used": "The empirical approach uses a difference-in-difference estimation strategy that exploits variation across time and space in 'intensity of exposure' to the government ban. The proxy for intensity of exposure is the village-level historical rate of informal birth attendant use. The treatment variable is the interaction between the high-exposure variable and Post, a binary indicator for births that take place after the ban. The basic regression specification is as follows:\n\n$$\n\\begin{array}{r}{Y_{i c d t}=\\alpha_{1}+\\alpha_{2}P o s t_{t}+\\delta H i g h E x p o s u r e_{c}+\\gamma H i g h E x p o s u r e_{c}*P o s t_{t}}\\\\{+X_{i c t}\\beta+\\eta_{d}+\\tau_{t}+\\epsilon_{i c d t}.\\qquad\\quad}\\end{array}\n$$\n\nEven though most outcomes are binary, a linear probability model is preferred primarily because it allows for ease of interpretation of the coefficients on the interaction terms. With fixed effects logit models, one cannot calculate marginal effects of independent variables without making arbitrary assumptions about the value of the fixed effects. The identification assumption does not require that high and low-exposure villages be similar, only that trends be similar. This is tested by examining pre-ban trends in outcomes across high- and low-exposure villages.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-977",
    "question": "In the context of treatment characteristics estimation and clustering, which of the following statements are correct regarding the methodology described in the text?",
    "options": [
      "A) The scaled treatment characteristics for clustering are computed as θ̃ₖ⁽ʳ⁾ = θₖ⁽ʳ⁾ / maxₖ∈𝐀ₜ |θₖ⁽ʳ⁾|, ensuring θ̃ ∈ [-1,1] × [-1,1]",
      "B) The clustering algorithm uses a fixed number of J=3 clusters throughout the empirical application",
      "C) The conditional value function vⱼ(z̃ᵢₜ) includes only immediate utility and does not account for future expected utilities",
      "D) The moment conditions for utility parameter estimation rely on differences between log odds ratios and differences in conditional value functions"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Int Economic Review - 2021 - Hamilton - INNOVATION AND DIFFUSION OF MEDICAL TREATMENT.md"
    ],
    "question_context": "The estimation of treatment characteristics uses the larger sample (visits 6–49) with equations derived from (22) and (23). The health transition and ailments process are described by:\n\n$$\nh_{t+1}=\\sum_{s=0}^{5}\\gamma_{s}^{(h e a l t h)}h_{t}^{s}+\\sum_{k\\in\\mathbf{K}_{t}}\\tilde{d}_{k t}\\theta_{k}^{(h e a l t h)}+d_{J+1,t}\\theta_{e t}^{(h e a l t h)}+\\epsilon_{h t},\n$$\n\nFor clustering, the algorithm rescales treatment characteristics by:\n\n$$\n\\widetilde{\\theta}_{k}^{(r)}=\\frac{\\theta_{k}^{(r)}}{\\operatorname*{max}_{k\\in{\\mathbf A}_{t}}\\left|\\theta_{k}^{(r)}\\right|},\\mathrm{for}r=h e a l t h,a i l.\n$$\n\nThe conditional value function includes both immediate utility and discounted future utilities:\n\n$$\n\\begin{array}{r}{v_{j}(\\bar{z}_{i t})=E\\big\\{u_{j}(h_{i t},y_{i t})+\\beta V(\\bar{z}_{i t+1})\\big|\\bar{z}_{i t},d_{j i t}=1\\big\\}.}\\end{array}\n$$\n\nMoment conditions for utility parameter estimation are formed using differences between log odds ratios and conditional value functions.",
    "category": "INCORRECT",
    "question_context_used": "The estimation of treatment characteristics uses the larger sample (visits 6–49) with equations derived from (22) and (23). The health transition and ailments process are described by:\n\n$$\nh_{t+1}=\\sum_{s=0}^{5}\\gamma_{s}^{(h e a l t h)}h_{t}^{s}+\\sum_{k\\in\\mathbf{K}_{t}}\\tilde{d}_{k t}\\theta_{k}^{(h e a l t h)}+d_{J+1,t}\\theta_{e t}^{(h e a l t h)}+\\epsilon_{h t},\n$$\n\nFor clustering, the algorithm rescales treatment characteristics by:\n\n$$\n\\widetilde{\\theta}_{k}^{(r)}=\\frac{\\theta_{k}^{(r)}}{\\operatorname*{max}_{k\\in{\\mathbf A}_{t}}\\left|\\theta_{k}^{(r)}\\right|},\\mathrm{for}r=h e a l t h,a i l.\n$$\n\nThe conditional value function includes both immediate utility and discounted future utilities:\n\n$$\n\\begin{array}{r}{v_{j}(\\bar{z}_{i t})=E\\big\\{u_{j}(h_{i t},y_{i t})+\\beta V(\\bar{z}_{i t+1})\\big|\\bar{z}_{i t},d_{j i t}=1\\big\\}.}\\end{array}\n$$\n\nMoment conditions for utility parameter estimation are formed using differences between log odds ratios and conditional value functions.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2665",
    "question": "In the context of the efficiency-wage model with partial income insurance, which of the following statements are correct regarding the Frisch no-shirking condition (NSC) and its implications?",
    "options": [
      "A) The Frisch NSC is downward sloping when unemployment insurance is imperfect.",
      "B) The average marginal utility of consumption in the household increases with employment when partial income insurance is present.",
      "C) The no-shirking condition implies a direct relationship between the wage rate and employment given common income.",
      "D) Indeterminacy in equilibrium can be caused by a downward sloping Frisch labor supply curve when utility is strictly concave in consumption and leisure."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Unemployment-and-indeterminacy_2006_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The no-shirking condition (NSC) in the efficiency-wage model is derived from the incentive compatibility condition, $C_{t}/C_{t}^{s}=\\chi$, and is given by:\n\n$$\nh W_{t}=\\frac{\\chi-1}{1-s\\chi+\\sigma(\\chi-1)(1-N_{t})}C_{t}^{h},\n$$\n\nwhich implies an upward relationship between the wage rate $W_{t}$ and employment $N_{t}$ given $C_{t}^{h}$. The Frisch NSC, obtained from (7), (17), and (24), is:\n\n$$\nh W_{t}=\\frac{\\chi-1}{\\chi(1-s)}\\big[\\mu-(\\mu-1)N_{t}\\big]\\frac{1}{A_{t}}.\n$$\n\nThis condition is downward sloping, indicating an inverse relationship between $W_{t}$ and $N_{t}$ when marginal utility of wealth, $\\varLambda_{t}$, is kept constant. The model shows that with partial income insurance ($\\sigma<1$), indeterminacy can arise due to this downward sloping Frisch NSC, even when utility is strictly concave in consumption and leisure.",
    "category": "INCORRECT",
    "question_context_used": "The no-shirking condition (NSC) in the efficiency-wage model is derived from the incentive compatibility condition, $C_{t}/C_{t}^{s}=\\chi$, and is given by:\n\n$$\nh W_{t}=\\frac{\\chi-1}{1-s\\chi+\\sigma(\\chi-1)(1-N_{t})}C_{t}^{h},\n$$\n\nwhich implies an upward relationship between the wage rate $W_{t}$ and employment $N_{t}$ given $C_{t}^{h}$. The Frisch NSC, obtained from (7), (17), and (24), is:\n\n$$\nh W_{t}=\\frac{\\chi-1}{\\chi(1-s)}\\big[\\mu-(\\mu-1)N_{t}\\big]\\frac{1}{A_{t}}.\n$$\n\nThis condition is downward sloping, indicating an inverse relationship between $W_{t}$ and $N_{t}$ when marginal utility of wealth, $\\varLambda_{t}$, is kept constant. The model shows that with partial income insurance ($\\sigma<1$), indeterminacy can arise due to this downward sloping Frisch NSC, even when utility is strictly concave in consumption and leisure.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3683",
    "question": "Under the assumptions of mean-variance utility representation and CARA utilities, which of the following conditions are necessary for the optimality of futures innovation in a dynamic economy?",
    "options": [
      "A) The futures positions must span the eigenvectors corresponding to the largest eigenvalues of the risk factor covariance matrix.",
      "B) The endowment of risk factors must be deterministic to maintain the mean-variance utility representation.",
      "C) The transfer policy must involve non-zero credit transfers at each date to ensure Pareto-improvement.",
      "D) The futures innovation must maximize the sum of quasi-linear utilities for H-optimality."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Optimal-Futures-Innovation-in-a-Dynamic-Economy--The_1997_Journal-of-Economi.md"
    ],
    "question_context": "The eigenvector characterizations of the optimal innovation depend strongly on the assumptions that allow the mean-variance utility representation. As Demange and Laroque [3] shows in a static economy, such simple characterizations are obtained only under these strong assumptions. By restricting our attention to such simple cases, this paper investigated the possible difficulty in extending the results obtained in a static economy to a dynamic economy. One immediate case to which we cannot extend the results above is the one where $\\beta_{k s}$ , the endowment of the risk factors, is stochastic. Although this situation is likely to occur in a dynamic economy, we lose the mean-variance representation, which leads to a more complicated characterization of optimal innovation. The utility representation has a quasi-linear form with the same coefficient on $L_{k0}$ for all $k$ , H-optimality of futures $F$ is equivalent to maximizing the sum of the quasi-linear utilities.",
    "category": "INCORRECT",
    "question_context_used": "The eigenvector characterizations of the optimal innovation depend strongly on the assumptions that allow the mean-variance utility representation. As Demange and Laroque [3] shows in a static economy, such simple characterizations are obtained only under these strong assumptions. By restricting our attention to such simple cases, this paper investigated the possible difficulty in extending the results obtained in a static economy to a dynamic economy. One immediate case to which we cannot extend the results above is the one where $\\beta_{k s}$ , the endowment of the risk factors, is stochastic. Although this situation is likely to occur in a dynamic economy, we lose the mean-variance representation, which leads to a more complicated characterization of optimal innovation. The utility representation has a quasi-linear form with the same coefficient on $L_{k0}$ for all $k$ , H-optimality of futures $F$ is equivalent to maximizing the sum of the quasi-linear utilities.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3747",
    "question": "Based on the Monte Carlo results, which of the following statements are true regarding the performance of the minimum order statistic estimator and the unbounded support estimator?",
    "options": [
      "A) The minimum order statistic estimator performs poorly when wage distributions lack a finite lower bound.",
      "B) The unbounded support estimator is robust to arbitrary forms of correlation in wage draws across locations.",
      "C) The minimum order statistic estimator's MSE decreases as the number of individuals (N) increases, even with model misspecification.",
      "D) The unbounded support estimator's performance is not adversely affected by measurement error in wages."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "00-Nonparametric_Identification_and_Estimation_in_a_Roy_Model_With_Common_Nonpecuniary_Returns.md"
    ],
    "question_context": "The Monte Carlo experiments evaluate two estimators: the minimum order statistic estimator and the unbounded support estimator. The minimum order statistic estimator performs well when properly specified but is sensitive to measurement error and distributions without finite lower bounds. Key formula for wage distributions: \n\n$$\\omega_{1}^{j}=\\sqrt{x_{1}^{2}+2.25}, \\quad \\omega_{2}^{j}=\\sqrt{x_{2}^{2}+1.75}, \\quad \\omega_{3}^{j}=\\sqrt{x_{3}^{2}+2.75}.$$\n\nThe unbounded support estimator performs well under proper specification and is robust to measurement error but performs poorly with correlated wage draws. The experiments show that MSE decreases with larger sample sizes (N) for both estimators, but their sensitivity to different model misspecifications varies significantly.",
    "category": "INCORRECT",
    "question_context_used": "The Monte Carlo experiments evaluate two estimators: the minimum order statistic estimator and the unbounded support estimator. The minimum order statistic estimator performs well when properly specified but is sensitive to measurement error and distributions without finite lower bounds. Key formula for wage distributions: \n\n$$\\omega_{1}^{j}=\\sqrt{x_{1}^{2}+2.25}, \\quad \\omega_{2}^{j}=\\sqrt{x_{2}^{2}+1.75}, \\quad \\omega_{3}^{j}=\\sqrt{x_{3}^{2}+2.75}.$$\n\nThe unbounded support estimator performs well under proper specification and is robust to measurement error but performs poorly with correlated wage draws. The experiments show that MSE decreases with larger sample sizes (N) for both estimators, but their sensitivity to different model misspecifications varies significantly.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1366",
    "question": "In the context of estimating time-varying COVID-19 contact rates using the sparse HP filter and ℓ₁ trend filtering, which of the following statements are supported by the theoretical and empirical findings of the paper?",
    "options": [
      "A) The sparse HP filter produces fewer kinks than the ℓ₁ trend filter while maintaining similar data-fitting performance.",
      "B) The ℓ₁ trend filter is preferred over the sparse HP filter due to its ability to capture more kinks, which are essential for policy analysis.",
      "C) Risk consistency is established for both the sparse HP and ℓ₁ trend filters under certain conditions.",
      "D) The contact rate βₜ is assumed to be constant over time in the SIR model, making trend filtering methods unnecessary."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Sparse-HP-filter--Finding-kinks-in-the-COVID-19-con_2021_Journal-of-Economet.md"
    ],
    "question_context": "The paper proposes a new trend filtering method, termed the sparse HP filter, which is a variant of the Hodrick–Prescott (HP) filter constrained by the number of possible kinks. This method is applied to daily COVID-19 data from five countries. The sparse HP filter yields fewer kinks than the ℓ₁ trend filter while both methods fit the data equally well. Theoretically, the paper establishes risk consistency for both the sparse HP and ℓ₁ trend filters. Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. The time-varying parameter logβₜ follows a piecewise trend with at most κ kinks, where the set of kinks is defined by {t=1,…,T:f₀,t−f₀,t−1≠f₀,t+1−f₀,t} and the locations of kinks are unknown.",
    "category": "CORRECT",
    "question_context_used": "The paper proposes a new trend filtering method, termed the sparse HP filter, which is a variant of the Hodrick–Prescott (HP) filter constrained by the number of possible kinks. This method is applied to daily COVID-19 data from five countries. The sparse HP filter yields fewer kinks than the ℓ₁ trend filter while both methods fit the data equally well. Theoretically, the paper establishes risk consistency for both the sparse HP and ℓ₁ trend filters. Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. The time-varying parameter logβₜ follows a piecewise trend with at most κ kinks, where the set of kinks is defined by {t=1,…,T:f₀,t−f₀,t−1≠f₀,t+1−f₀,t} and the locations of kinks are unknown.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3468",
    "question": "In the context of urban economic equilibrium, which of the following conditions are necessary for the existence of a Central Business District (CBD) with surrounding residential ring?",
    "options": [
      "A) The commuting cost parameter κ must be strictly less than [δγ(1−β)]/[1−αβ]",
      "B) The land rent schedule for firms must have a steeper negative slope than that for households",
      "C) The measure of workers P must be sufficiently large to support urban expansion",
      "D) The productivity decay parameter δ must be zero to ensure constant firm productivity"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "A TRACTABLE CITY MODEL FOR AGGREGATIVE ANALYSIS.md"
    ],
    "question_context": "Key formula: The condition for CBD formation is given by κ < [δγ(1−β)]/[1−αβ]. This ensures that the rise in productivity toward the city center is large enough for businesses to compensate for commuting costs. The land rent schedules are defined as:\n\n$$ q_{F}^{*}(s)=q_{F}^{*}(0)\\exp\\left(-\\frac{\\delta\\gamma-\\kappa\\alpha}{1-\\alpha}s\\right) $$\n\n$$ q_{H}^{*}(s)=q_{H}^{*}(0)\\exp\\left(-\\frac{\\kappa}{1-\\beta}s\\right) $$\n\nWhen κ < [γδ(1−β)]/[1−βα], q_F^*(·) has a steeper (negative) slope than q_H^*(·), leading to a unique intersection point at the CBD boundary S_F^*. This structural condition is critical for the monocentric city model where firms concentrate in the CBD and workers reside in the surrounding ring.",
    "category": "CORRECT",
    "question_context_used": "Key formula: The condition for CBD formation is given by κ < [δγ(1−β)]/[1−αβ]. This ensures that the rise in productivity toward the city center is large enough for businesses to compensate for commuting costs. The land rent schedules are defined as:\n\n$$ q_{F}^{*}(s)=q_{F}^{*}(0)\\exp\\left(-\\frac{\\delta\\gamma-\\kappa\\alpha}{1-\\alpha}s\\right) $$\n\n$$ q_{H}^{*}(s)=q_{H}^{*}(0)\\exp\\left(-\\frac{\\kappa}{1-\\beta}s\\right) $$\n\nWhen κ < [γδ(1−β)]/[1−βα], q_F^*(·) has a steeper (negative) slope than q_H^*(·), leading to a unique intersection point at the CBD boundary S_F^*. This structural condition is critical for the monocentric city model where firms concentrate in the CBD and workers reside in the surrounding ring.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2942",
    "question": "Under the conditions of Proposition 1, which of the following statements are necessary and sufficient for distribution $G$ to be portfolio-riskier than distribution $F$?",
    "options": [
      "A) $E(Y|Y \\leq t) \\leq E(X|X \\leq t)$ for all real $t$",
      "B) $\\int_{-\\infty}^{t}(x-\\mu)d G(x) \\leq \\int_{-\\infty}^{t}(x-\\mu)d F(x)$ for all real $t$",
      "C) $\\alpha_G \\leq \\alpha_F$ for all risk-averse utility functions $U$ and all riskless yields $\\rho$",
      "D) The variance of $G$ is greater than the variance of $F$"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Mean-preserving Portfolio Dominance.md"
    ],
    "question_context": "The text establishes conditions under which one distribution $G$ is portfolio-riskier than another distribution $F$. Key results include the following: \n\n1. The optimal expected value of a basic portfolio is given by $E U(\\alpha_{F}X + (1-\\alpha_{F})\\rho)$, where $\\alpha_{F}$ maximizes $E U(\\alpha X + (1-\\alpha)\\rho)$. The Hamada conditions ensure $\\alpha_{F}$ is finite. \n\n2. Proposition 1 states that for distributions $F$ and $G$ with equal means, the following two conditions are equivalent: (i) $\\alpha_{G} \\leq \\alpha_{F}$ for all non-decreasing concave utility functions $U$ and all $\\rho$ where $\\alpha_{F}$ is finite; (ii) $E(Y|Y \\leq t) \\leq E(X|X \\leq t)$ and $\\int_{-\\infty}^{t}(x-\\mu)d G(x) \\leq \\int_{-\\infty}^{t}(x-\\mu)d F(x)$ for all real $t$.",
    "category": "INCORRECT",
    "question_context_used": "The text establishes conditions under which one distribution $G$ is portfolio-riskier than another distribution $F$. Key results include the following: \n\n1. The optimal expected value of a basic portfolio is given by $E U(\\alpha_{F}X + (1-\\alpha_{F})\\rho)$, where $\\alpha_{F}$ maximizes $E U(\\alpha X + (1-\\alpha)\\rho)$. The Hamada conditions ensure $\\alpha_{F}$ is finite. \n\n2. Proposition 1 states that for distributions $F$ and $G$ with equal means, the following two conditions are equivalent: (i) $\\alpha_{G} \\leq \\alpha_{F}$ for all non-decreasing concave utility functions $U$ and all $\\rho$ where $\\alpha_{F}$ is finite; (ii) $E(Y|Y \\leq t) \\leq E(X|X \\leq t)$ and $\\int_{-\\infty}^{t}(x-\\mu)d G(x) \\leq \\int_{-\\infty}^{t}(x-\\mu)d F(x)$ for all real $t$.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-65",
    "question": "In the context of adverse selection in used car markets, which of the following statements are supported by the literature and theoretical models discussed?",
    "options": [
      "A) Leasing contracts can completely eliminate the adverse selection problem in all market configurations.",
      "B) The presence of dealers with repair facilities can mitigate adverse selection by providing credible quality signals.",
      "C) Empirical evidence uniformly supports the presence of adverse selection in used car markets across all age groups.",
      "D) The model predicts that a steeper price decline coupled with lower trade volume indicates adverse selection."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Hendel-AdverseSelectionDurable-1999.md"
    ],
    "question_context": "The empirical evidence on the presence of adverse selection in used cars is inconclusive. Bond (1982) finds no evidence of adverse selection when comparing the frequency of repair of pickup trucks between those that are kept and those that are sold. Genesove and Lacko report some evidence of adverse selection for older cars only. Our results in Section IV provide a way to test for adverse selection that relies only on observable data on price and quantity profiles over time across brands of cars. According to the model we have evidence of adverse selection if the car that has a steeper price decline also has the lower volume of trade. If instead it has a larger volume of trade, then the steeper price decline may be due to faster physical depreciation. \n\nAnother extension is discussed by Rahul Guha and Michael Waldman (1996) and Hendel and Lizzeri (1998). Guha and Waldman (1996) study leasing under adverse selection and argue that leasing solves the lemons problem. In Hendel and Lizzeri (1998) we use the model presented in this paper to also address the role of leasing under adverse selection. In that paper we show that while leasing ameliorates the adverse selection problem, Guha and Waldman's result only holds in their case of two types of consumers. We also show that there exists a mechanism that generates the ex post efficient allocation.",
    "category": "CORRECT",
    "question_context_used": "The empirical evidence on the presence of adverse selection in used cars is inconclusive. Bond (1982) finds no evidence of adverse selection when comparing the frequency of repair of pickup trucks between those that are kept and those that are sold. Genesove and Lacko report some evidence of adverse selection for older cars only. Our results in Section IV provide a way to test for adverse selection that relies only on observable data on price and quantity profiles over time across brands of cars. According to the model we have evidence of adverse selection if the car that has a steeper price decline also has the lower volume of trade. If instead it has a larger volume of trade, then the steeper price decline may be due to faster physical depreciation. \n\nAnother extension is discussed by Rahul Guha and Michael Waldman (1996) and Hendel and Lizzeri (1998). Guha and Waldman (1996) study leasing under adverse selection and argue that leasing solves the lemons problem. In Hendel and Lizzeri (1998) we use the model presented in this paper to also address the role of leasing under adverse selection. In that paper we show that while leasing ameliorates the adverse selection problem, Guha and Waldman's result only holds in their case of two types of consumers. We also show that there exists a mechanism that generates the ex post efficient allocation.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3791",
    "question": "In the HCW method for estimating treatment effects, which of the following conditions are necessary for the consistent estimation of the average treatment effect Δ?",
    "options": [
      "A) The idiosyncratic error term u₁ₜ must be i.i.d. with mean 0 and constant variance σ²ᵤ",
      "B) Both pre-treatment period T₁ and post-treatment period (T - T₁) must tend to infinity",
      "C) The factor loading matrix B must be full rank",
      "D) The treatment effects Δₜ must be stationary over time"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Disentangling-the-effects-of-multiple-treatments-Measuring-t_2015_Journal-of.md"
    ],
    "question_context": "The HCW method assumes the observed outcome yᵢₜ is a function of common factors fₜ, fixed idiosyncratic component αᵢ, and random idiosyncratic component εᵢₜ. The model is specified as:\n\nyᵢₜ = αᵢ + bᵢ'fₜ + εᵢₜ, i=1,...,N\n\nFor consistent estimation of the average treatment effect Δ, Eq. (4.7) requires that T₁ and (T - T₁) both tend to infinity. Furthermore, the method assumes u₁ₜ are independently, identically distributed with mean 0 and variance σ²ᵤ, and that Δₜ is stationary. The covariance matrix of the estimators is given by:\n\n∇(â, b̂') = σ²ᵤ(X'X)⁻¹\n\nwhere X = (1, ỹₜ*') is the design matrix. The prediction error variance is:\n\nσ²_{yₜ⁰} = σ²ᵤ{1 + xₜ'(X'X)⁻¹xₜ}",
    "category": "CORRECT",
    "question_context_used": "The HCW method assumes the observed outcome yᵢₜ is a function of common factors fₜ, fixed idiosyncratic component αᵢ, and random idiosyncratic component εᵢₜ. The model is specified as:\n\nyᵢₜ = αᵢ + bᵢ'fₜ + εᵢₜ, i=1,...,N\n\nFor consistent estimation of the average treatment effect Δ, Eq. (4.7) requires that T₁ and (T - T₁) both tend to infinity. Furthermore, the method assumes u₁ₜ are independently, identically distributed with mean 0 and variance σ²ᵤ, and that Δₜ is stationary. The covariance matrix of the estimators is given by:\n\n∇(â, b̂') = σ²ᵤ(X'X)⁻¹\n\nwhere X = (1, ỹₜ*') is the design matrix. The prediction error variance is:\n\nσ²_{yₜ⁰} = σ²ᵤ{1 + xₜ'(X'X)⁻¹xₜ}",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4452",
    "question": "According to Murnane and Olsen's analysis, which factors are empirically shown to significantly influence the duration of first teaching spells and the probability of second teaching spells?",
    "options": [
      "A) Opportunity costs and teaching salaries",
      "B) Student enrollment rates and retirement trends",
      "C) Nonpecuniary benefits and district salary schedule changes",
      "D) Academic ability and subject specialty"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Murnane-EnoughTeachers-1989.md"
    ],
    "question_context": "The paper examines the factors influencing the length of first teaching spells and the probability of second teaching spells. The results indicate that length of first spells is highly sensitive to opportunity costs and salaries—findings that have considerable relevance to the design of teacher compensation policies. The key assumption is that new information about opportunity costs, salaries in teaching, or the net nonpecuniary benefits of teaching becomes available after individuals begin to teach, and that this new information influences duration decisions. The results of these reassessments may have depended on the teacher's subject specialty and academic ability (measures of opportunity cost), and on the attractiveness of the current salary schedule in the teacher's school district.",
    "category": "CORRECT",
    "question_context_used": "The paper examines the factors influencing the length of first teaching spells and the probability of second teaching spells. The results indicate that length of first spells is highly sensitive to opportunity costs and salaries—findings that have considerable relevance to the design of teacher compensation policies. The key assumption is that new information about opportunity costs, salaries in teaching, or the net nonpecuniary benefits of teaching becomes available after individuals begin to teach, and that this new information influences duration decisions. The results of these reassessments may have depended on the teacher's subject specialty and academic ability (measures of opportunity cost), and on the attractiveness of the current salary schedule in the teacher's school district.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1742",
    "question": "According to Proposition 1 and the analysis of Taiwan mechanisms, which of the following statements are true regarding manipulability and deduction points?",
    "options": [
      "A) Higher deduction points in the Taiwan mechanism strictly increase its manipulability.",
      "B) The serial dictatorship mechanism is strategy-proof and equivalent to the DA when school priorities are identical.",
      "C) The Boston Mechanism (BM) is less manipulable than any Taiwan mechanism with positive deduction points.",
      "D) Relaxing deduction rules in Taiwan mechanisms reduces their vulnerability to manipulation."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Deduction Dilemmas The Taiwan Assignment Mechanism.md"
    ],
    "question_context": "Key formula: The Taiwan mechanism with deduction rule λ is denoted as TM^λ. Proposition 1 states: Under Assumptions 1 and 2, if λ¹ > λ², then TM^λ¹ is more manipulable than TM^λ². This is proven by showing (i) TM^λ¹ is vulnerable to manipulation whenever TM^λ² is, and (ii) there exists (P,π) where TM^λ¹ is manipulable but TM^λ² is not. \n\nWhen deduction points are zero, the Taiwan mechanism reduces to a serial dictatorship (equivalent to DA with identical school priorities), which is strategy-proof. The Boston Mechanism (BM) corresponds to the case of very large deduction points and is highly manipulable. Empirical evidence shows that districts relaxing deduction rules (e.g., Gaoxiong, Jibei, Zhongtou) made their mechanisms less manipulable, except when moving from zero to positive deductions (e.g., Jinmen in 2014→2015).",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The Taiwan mechanism with deduction rule λ is denoted as TM^λ. Proposition 1 states: Under Assumptions 1 and 2, if λ¹ > λ², then TM^λ¹ is more manipulable than TM^λ². This is proven by showing (i) TM^λ¹ is vulnerable to manipulation whenever TM^λ² is, and (ii) there exists (P,π) where TM^λ¹ is manipulable but TM^λ² is not. \n\nWhen deduction points are zero, the Taiwan mechanism reduces to a serial dictatorship (equivalent to DA with identical school priorities), which is strategy-proof. The Boston Mechanism (BM) corresponds to the case of very large deduction points and is highly manipulable. Empirical evidence shows that districts relaxing deduction rules (e.g., Gaoxiong, Jibei, Zhongtou) made their mechanisms less manipulable, except when moving from zero to positive deductions (e.g., Jinmen in 2014→2015).",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3197",
    "question": "In the context of Village Savings and Loan Associations (VSLAs), which of the following statements accurately describe the financial intermediation function and its implications?",
    "options": [
      "A) The loan-to-savings ratio serves as a proxy for leverage and financial intermediation velocity, with an average ratio of 2.54 indicating funds circulate 2.54 times faster than accumulated savings.",
      "B) The average monthly interest rate on borrowed funds is 13.5%, which is considered fair as it aligns closely with members' average monthly discount rates over the longer term.",
      "C) The predicted annual internal rate of return per share could reach 142% if all saved funds and past paid interest were optimally lent and repaid monthly at 10% interest.",
      "D) Negative assortative matching on occupation is beneficial as it allows farmers to lend to businesspersons, leveraging distinct savings and loan usage patterns."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Banker-my-neighbour--Matching-and-financial-interm_2020_Journal-of-Developme.md"
    ],
    "question_context": "The loan-to-savings ratio is a key metric in VSLAs, calculated as total loans disbursed divided by total savings, averaging 2.54. This implies that the total value of loans made during a cycle is 2.54 times the value of all purchased shares at the cycle's end. The actual velocity of funds is approximately twice this number, as share purchases are equally distributed over the cycle. Formula: Velocity ≈ 2 × (Total Loans / Total Savings). The average monthly interest rate on borrowing is 13.5%, close to members' long-term discount rates. A theoretical benchmark shows that with optimal lending (all funds lent at 10% monthly interest and repaid fully each month), the annual internal rate of return per share could reach 142%. Negative assortative matching on occupation (e.g., farmers lending to businesspersons) is supported by distinct usage patterns: 58% of savings are used for agricultural inputs, while 74% of loans are directed toward trading and business purposes.",
    "category": "INCORRECT",
    "question_context_used": "The loan-to-savings ratio is a key metric in VSLAs, calculated as total loans disbursed divided by total savings, averaging 2.54. This implies that the total value of loans made during a cycle is 2.54 times the value of all purchased shares at the cycle's end. The actual velocity of funds is approximately twice this number, as share purchases are equally distributed over the cycle. Formula: Velocity ≈ 2 × (Total Loans / Total Savings). The average monthly interest rate on borrowing is 13.5%, close to members' long-term discount rates. A theoretical benchmark shows that with optimal lending (all funds lent at 10% monthly interest and repaid fully each month), the annual internal rate of return per share could reach 142%. Negative assortative matching on occupation (e.g., farmers lending to businesspersons) is supported by distinct usage patterns: 58% of savings are used for agricultural inputs, while 74% of loans are directed toward trading and business purposes.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4217",
    "question": "According to the delay aversion ordering $\\precsim$ and Theorem 2, which of the following conditions must hold for separable intertemporal utility functions $(u,\\pmb{\\alpha})$ and $(\\nu,\\pmb{\\beta})$ to satisfy $(u,\\pmb{\\alpha}) \\precsim (\\nu,\\pmb{\\beta})$?",
    "options": [
      "A) $\\nu = h \\circ u$ for some map $h:\\mathbb{R}_{+} \\rightarrow \\mathbb{R}_{+}$ and $\\frac{\\pmb{\\alpha}(t)}{\\pmb{\\alpha}(s)} \\geq \\frac{\\pmb{\\beta}(t)}{\\pmb{\\beta}(s)}$ for all $s < t$",
      "B) $\\nu$ is a linear transformation of $u$ and $\\pmb{\\alpha} = \\pmb{\\beta}$",
      "C) $\\nu$ is a convex transformation of $u$ and $\\frac{\\pmb{\\alpha}(t)}{\\pmb{\\alpha}(s)} \\leq \\frac{\\pmb{\\beta}(t)}{\\pmb{\\beta}(s)}$ for all $s < t$",
      "D) $\\nu$ is a concave transformation of $u$ and $\\pmb{\\alpha} \\leq \\pmb{\\beta}$"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Delay aversion.md"
    ],
    "question_context": "The delay aversion ordering $\\precsim$ is defined for intertemporal utility functions where $V$ is more delay averse than $U$ if certain conditions hold for consumption streams and payments. Theorem 2 provides characterizations for separable intertemporal utility functions $(u,\\pmb{\\alpha})$ and $(\\nu,\\pmb{\\beta})$. Specifically, $(\\nu,\\pmb{\\beta})$ is more delay averse than $(u,\\pmb{\\alpha})$ if and only if there exists a map $h:\\mathbb{R}_{+} \\rightarrow \\mathbb{R}_{+}$ such that $\\nu = h \\circ u$ and the inequality $h\\left(x + \\frac{\\alpha(t)}{\\pmb{\\alpha}(s)}y\\right) \\geq h(x) + \\frac{\\pmb{\\beta}(t)}{\\pmb{\\beta}(s)}(h(y+z) - h(z))$ holds for all $s, t \\in \\mathbb{Z}_{+}$ with $s < t$ and $x, y, z \\geq 0$. Corollary 1 further states that $(u,\\pmb{\\alpha}) \\precsim (\\nu,\\pmb{\\beta})$ implies $\\frac{\\pmb{\\alpha}(t)}{\\pmb{\\alpha}(s)} \\geq \\frac{\\pmb{\\beta}(t)}{\\pmb{\\beta}(s)}$ whenever $s < t$.",
    "category": "INCORRECT",
    "question_context_used": "The delay aversion ordering $\\precsim$ is defined for intertemporal utility functions where $V$ is more delay averse than $U$ if certain conditions hold for consumption streams and payments. Theorem 2 provides characterizations for separable intertemporal utility functions $(u,\\pmb{\\alpha})$ and $(\\nu,\\pmb{\\beta})$. Specifically, $(\\nu,\\pmb{\\beta})$ is more delay averse than $(u,\\pmb{\\alpha})$ if and only if there exists a map $h:\\mathbb{R}_{+} \\rightarrow \\mathbb{R}_{+}$ such that $\\nu = h \\circ u$ and the inequality $h\\left(x + \\frac{\\alpha(t)}{\\pmb{\\alpha}(s)}y\\right) \\geq h(x) + \\frac{\\pmb{\\beta}(t)}{\\pmb{\\beta}(s)}(h(y+z) - h(z))$ holds for all $s, t \\in \\mathbb{Z}_{+}$ with $s < t$ and $x, y, z \\geq 0$. Corollary 1 further states that $(u,\\pmb{\\alpha}) \\precsim (\\nu,\\pmb{\\beta})$ implies $\\frac{\\pmb{\\alpha}(t)}{\\pmb{\\alpha}(s)} \\geq \\frac{\\pmb{\\beta}(t)}{\\pmb{\\beta}(s)}$ whenever $s < t$.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3039",
    "question": "Which of the following statements accurately describe distinguishing features of the Monograph Series of the Society as outlined in the text?",
    "options": [
      "A) The series provides professional referee reports with detailed suggestions for manuscript improvement",
      "B) Books in the series are exclusively published in hardcover versions",
      "C) The reviewing process and publication decision rests solely with Editors appointed by the Society",
      "D) The series focuses exclusively on empirical econometric studies without theoretical contributions"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Dixit-EconometricSocietyResearch-1991.md"
    ],
    "question_context": "The arrangement negotiated in 1985 with Cambridge University Press specifies that the reviewing process and the decision to publish a monograph in the series rests solely in the hands of the Editors appointed by the Society, in the same way as for papers submitted to Econometrica. Our experience shows that this procedure generates quite valuable services to the authors. Referee reports are usually very professional, and contain detailed and specific suggestions on how to improve the manuscript. Such services, which are not normally provided by private publishing companies, are among the features that distinguish the Monograph Series of the Society from others.\n\nPublications may range from more or less extensive accounts of the state of the art in a field to which the authors have made significant contributions to shorter monographs representing important advances on more specific issues. The goal of the series is to promote the publication of high-quality research works in the fields of Economic Theory, Econometrics, and Quantitative Economics more generally.",
    "category": "CORRECT",
    "question_context_used": "The arrangement negotiated in 1985 with Cambridge University Press specifies that the reviewing process and the decision to publish a monograph in the series rests solely in the hands of the Editors appointed by the Society, in the same way as for papers submitted to Econometrica. Our experience shows that this procedure generates quite valuable services to the authors. Referee reports are usually very professional, and contain detailed and specific suggestions on how to improve the manuscript. Such services, which are not normally provided by private publishing companies, are among the features that distinguish the Monograph Series of the Society from others.\n\nPublications may range from more or less extensive accounts of the state of the art in a field to which the authors have made significant contributions to shorter monographs representing important advances on more specific issues. The goal of the series is to promote the publication of high-quality research works in the fields of Economic Theory, Econometrics, and Quantitative Economics more generally.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1078",
    "question": "Which of the following properties are established by Lemma 2 regarding the PPE frontier $u(\\pi)$ in the context of optimal relational contracts?",
    "options": [
      "A) The PPE frontier is concave and differentiable with $-1 < u'(\\pi) \\leq -1/(1 + \\alpha\\theta)$",
      "B) Payoffs on the PPE frontier can be supported by pure actions in the stage game",
      "C) The PPE frontier is linear for all $\\pi \\leq \\hat{\\pi}$ when $u'(\\pi)$ reaches its upper bound",
      "D) The PPE frontier is characterized by randomizing among $(\\underline{\\pi}, \\underline{u}), (\\overline{\\pi}, \\underline{u}),$ and $(\\pi, u(\\pi))$"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Li-ManagingConflictsRelational-2013.md"
    ],
    "question_context": "LEMMA 2: The PPE frontier $\\pmb{u}(\\pi)$ has the following properties: for all $\\pi\\in[\\underline{{\\pi}},\\bar{\\pi}]$ (i) payoffs $(\\pi,u(\\pi))$ can be supported by pure actions in the stage game other than taking the outside options, $(i i)$ $\\pmb{u}$ is concave, and (ii) $\\pmb{u}$ is differentiable with $-1<u^{\\prime}(\\pi)\\le-1/(1+\\alpha\\theta)$. The first part of the lemma shows that payoffs on the PPE frontier can be supported by pure actions, that is, they do not require any public randomization. The concavity of the PPE frontier follows immediately from the availability of a public randomization device, and its differentiability follows from the continuity of effort. Finally, the bounds on the derivative imply that the PPE frontier is downward sloping and that joint surplus $\\pi+u(\\pi)$ is increasing in expected profits $\\pi$ and thus maximized at $\\bar{\\pi}$. In panel B, the derivative reaches its upper bound ${\\pmb u}^{\\prime}(\\pi)=-1/(1+\\alpha\\theta)$ at a critical level of expected profits ${\\hat{\\pi}},$ and as a result the PPE frontier is linear for all $\\pi\\leq{\\hat{\\pi}}$.",
    "category": "CORRECT",
    "question_context_used": "LEMMA 2: The PPE frontier $\\pmb{u}(\\pi)$ has the following properties: for all $\\pi\\in[\\underline{{\\pi}},\\bar{\\pi}]$ (i) payoffs $(\\pi,u(\\pi))$ can be supported by pure actions in the stage game other than taking the outside options, $(i i)$ $\\pmb{u}$ is concave, and (ii) $\\pmb{u}$ is differentiable with $-1<u^{\\prime}(\\pi)\\le-1/(1+\\alpha\\theta)$. The first part of the lemma shows that payoffs on the PPE frontier can be supported by pure actions, that is, they do not require any public randomization. The concavity of the PPE frontier follows immediately from the availability of a public randomization device, and its differentiability follows from the continuity of effort. Finally, the bounds on the derivative imply that the PPE frontier is downward sloping and that joint surplus $\\pi+u(\\pi)$ is increasing in expected profits $\\pi$ and thus maximized at $\\bar{\\pi}$. In panel B, the derivative reaches its upper bound ${\\pmb u}^{\\prime}(\\pi)=-1/(1+\\alpha\\theta)$ at a critical level of expected profits ${\\hat{\\pi}},$ and as a result the PPE frontier is linear for all $\\pi\\leq{\\hat{\\pi}}$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2125",
    "question": "In the context of sorting models versus human-capital models, which of the following statements are correct regarding the measured return to education (dq/ds)?",
    "options": [
      "A) The measured return to education includes both the direct effect of schooling on productivity (qs) and the indirect effect through ability (qi di/ds).",
      "B) In human-capital models, the ability bias arises solely because the econometrician does not observe ability, whereas in sorting models, it arises because neither the econometrician nor the employer observes ability.",
      "C) The measured return to education (dq/ds) is always equal to the social return (qs) in both sorting and human-capital models.",
      "D) Sorting models generally result in a smaller di/ds compared to human-capital models, causing the measured return to education to approximate the social return more closely."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Lang-HumanCapitalEducationalSortingDebateMatter-1994.md"
    ],
    "question_context": "The critical assumption underlying the argument in the remainder of this paper is that the distinguishing characteristic of a sorting model is that knowing an individual's education provides employers with information about that individual's productivity which would be unknown otherwise. In human-capital models, education is not informative, because employers observe productivity directly. Thus, in the following argument, human capital and sorting are not distinguished by assumptions about the human-capital production function. Instead the models are distinguished by the role of education in conveying the individual's private information about his productivity. In both models, units of human capital will be generated in the same way from inputs of innate ability and schooling.\n\nAssuming that $i$ is not observed by the econometrician, the measured return to education is given by $$d q/d s=q_{s}+q_{i}d i/d s$$ where subscripts denote partial derivatives. Equation (2) does not depend on whether the relation between $i$ and $s$ is generated by sorting or by human capital or some combination of the two. Therefore in both models, the measured return to education, $d q/d s$, differs from the social return $q_{s}$ by a factor which can be interpreted as ability bias. The fact that in the human-capital model this ability bias arises solely because the econometrician does not observe ability, while in the sorting model it arises because neither the econometrician nor the employer observes ability, is irrelevant. The only question is in which model ability bias is likely to be greater.",
    "category": "INCORRECT",
    "question_context_used": "The critical assumption underlying the argument in the remainder of this paper is that the distinguishing characteristic of a sorting model is that knowing an individual's education provides employers with information about that individual's productivity which would be unknown otherwise. In human-capital models, education is not informative, because employers observe productivity directly. Thus, in the following argument, human capital and sorting are not distinguished by assumptions about the human-capital production function. Instead the models are distinguished by the role of education in conveying the individual's private information about his productivity. In both models, units of human capital will be generated in the same way from inputs of innate ability and schooling.\n\nAssuming that $i$ is not observed by the econometrician, the measured return to education is given by $$d q/d s=q_{s}+q_{i}d i/d s$$ where subscripts denote partial derivatives. Equation (2) does not depend on whether the relation between $i$ and $s$ is generated by sorting or by human capital or some combination of the two. Therefore in both models, the measured return to education, $d q/d s$, differs from the social return $q_{s}$ by a factor which can be interpreted as ability bias. The fact that in the human-capital model this ability bias arises solely because the econometrician does not observe ability, while in the sorting model it arises because neither the econometrician nor the employer observes ability, is irrelevant. The only question is in which model ability bias is likely to be greater.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1233",
    "question": "In the context of impulse response analysis for macroeconomic and financial uncertainty shocks, which of the following statements are supported by the empirical findings?",
    "options": [
      "A) A shock to macroeconomic uncertainty leads to a significant and persistent decline in industrial production and employment.",
      "B) Financial uncertainty shocks have a more pronounced effect on housing starts and permits compared to macroeconomic uncertainty shocks.",
      "C) The federal funds rate exhibits a delayed but statistically significant decline in response to a macroeconomic uncertainty shock.",
      "D) The response of aggregate stock prices to financial uncertainty shocks is imprecisely estimated and generally insignificant."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "rest_a_00693.md"
    ],
    "question_context": "The study computes impulse response functions for retained draws of the VAR’s parameters, reporting posterior medians and 70% credible sets. Key identification relies on a Cholesky scheme where financial uncertainty is ordered last, allowing it to respond contemporaneously to both macroeconomic and financial uncertainty shocks. The impulse responses are scaled back to typical units in the literature for comparability. A shock to macroeconomic uncertainty produces a rise in uncertainty that dies out over about one year, with financial uncertainty responding similarly but less precisely. Industrial production and employment show significantly negative responses, with production more affected than employment. The federal funds rate declines steadily after a few months, reaching about -20 basis points. Financial uncertainty shocks have similar but sometimes smaller macroeconomic effects and larger financial effects, though stock price responses remain imprecisely estimated. The formula for the one-step-ahead forecast errors is given by: \n\n$$y_{t+1}-E_{t}y_{t+1}=\\Sigma_{t+1}^{0.5}\\epsilon_{t+1}+\\Gamma_{1}u_{t+1}.$$",
    "category": "INCORRECT",
    "question_context_used": "The study computes impulse response functions for retained draws of the VAR’s parameters, reporting posterior medians and 70% credible sets. Key identification relies on a Cholesky scheme where financial uncertainty is ordered last, allowing it to respond contemporaneously to both macroeconomic and financial uncertainty shocks. The impulse responses are scaled back to typical units in the literature for comparability. A shock to macroeconomic uncertainty produces a rise in uncertainty that dies out over about one year, with financial uncertainty responding similarly but less precisely. Industrial production and employment show significantly negative responses, with production more affected than employment. The federal funds rate declines steadily after a few months, reaching about -20 basis points. Financial uncertainty shocks have similar but sometimes smaller macroeconomic effects and larger financial effects, though stock price responses remain imprecisely estimated. The formula for the one-step-ahead forecast errors is given by: \n\n$$y_{t+1}-E_{t}y_{t+1}=\\Sigma_{t+1}^{0.5}\\epsilon_{t+1}+\\Gamma_{1}u_{t+1}.$$",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4231",
    "question": "In the pirate game scenario with four infinitely rational pirates (Antonio, Beatriz, Carla, David) dividing 10 gold doblones, which of the following statements about the subgame perfect Nash equilibrium are correct?",
    "options": [
      "A) Antonio must offer at least one coin to Beatriz to secure her vote",
      "B) Carla would receive zero coins if Antonio's proposal is rejected",
      "C) David's threat of taking all coins serves as a credible commitment device",
      "D) The minimal winning coalition requires exactly two votes including the proposer"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Reasoning-about-others--reasoning_2020_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The pirate game involves sequential voting with the following rules: Antonio proposes a distribution needing ≥2 votes (including his own) to pass. If rejected, he is killed and Beatriz proposes, needing ≥2 votes from remaining pirates. This continues until Carla's proposal needing just 1 vote (her own), with David taking everything if Carla is killed. The solution requires backward induction through all subgames.\n\nKey equilibrium condition: For Antonio to survive, his proposal must account for the alternative outcomes pirates would face if he were killed. The minimal winning coalition size is always half the living pirates (rounded down), giving the proposer strategic advantage through sequential rationality.",
    "category": "INCORRECT",
    "question_context_used": "The pirate game involves sequential voting with the following rules: Antonio proposes a distribution needing ≥2 votes (including his own) to pass. If rejected, he is killed and Beatriz proposes, needing ≥2 votes from remaining pirates. This continues until Carla's proposal needing just 1 vote (her own), with David taking everything if Carla is killed. The solution requires backward induction through all subgames.\n\nKey equilibrium condition: For Antonio to survive, his proposal must account for the alternative outcomes pirates would face if he were killed. The minimal winning coalition size is always half the living pirates (rounded down), giving the proposer strategic advantage through sequential rationality.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3452",
    "question": "In the context of economic theory, which of the following research topics are directly addressed in the forthcoming articles of the Journal of Economic Theory?",
    "options": [
      "A) Government guarantees and self-fulfilling speculative attacks",
      "B) Endogenous lifetime and economic growth",
      "C) Star-shapedness of Richter-Aumann integral on a measure space with atoms",
      "D) Contractual distortions in a market with frictions"
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "Papers-to-Appear-in-Forthcoming-Issues_2003_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The forthcoming articles in the Journal of Economic Theory cover a wide range of advanced topics in economic theory. For instance, Craig Burnside, Martin Eichenbaum, and Sergio Rebelo explore the dynamics of government guarantees and self-fulfilling speculative attacks, while Shankha Chakraborty investigates the relationship between endogenous lifetime and economic growth. Antonio D’Agata's work focuses on the star-shapedness of Richter-Aumann integral on a measure space with atoms, providing theoretical insights with economic applications. Roman Inderst examines contractual distortions in markets with frictions, highlighting the complexities of market interactions under imperfect conditions.",
    "category": "CORRECT",
    "question_context_used": "The forthcoming articles in the Journal of Economic Theory cover a wide range of advanced topics in economic theory. For instance, Craig Burnside, Martin Eichenbaum, and Sergio Rebelo explore the dynamics of government guarantees and self-fulfilling speculative attacks, while Shankha Chakraborty investigates the relationship between endogenous lifetime and economic growth. Antonio D’Agata's work focuses on the star-shapedness of Richter-Aumann integral on a measure space with atoms, providing theoretical insights with economic applications. Roman Inderst examines contractual distortions in markets with frictions, highlighting the complexities of market interactions under imperfect conditions.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-655",
    "question": "Based on the Monte Carlo results for generalized Gini indices, which of the following statements are correct regarding the performance of inference procedures?",
    "options": [
      "A) The BOOT-T procedure consistently exhibits superior size and power characteristics across different sample sizes.",
      "B) The Grouped Gini (GG) procedure performs well in terms of power due to its unbiased point estimates of generalized Gini indices.",
      "C) The ASE procedure shows good performance in small samples but is outperformed by BOOT-T in terms of size and power refinement.",
      "D) The Double Bootstrap (DB) procedure generally underperforms compared to other methods, particularly in small samples."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "00-Statistical_Inference_with_Generalized_Gini_Indices_of_Inequality__Poverty__and_Welfare.md"
    ],
    "question_context": "The Monte Carlo experiments evaluated various inference procedures for generalized Gini indices, including ASE, BSE, BOOT-T, DB, and GG methods. Key findings include: \n\n1. The BOOT-T procedure, which applies bootstrap resampling to asymptotically pivotal test statistics, demonstrated the most superior size and power properties, especially in small samples. For example, in case 1 with N=50, BOOT-T had a rejection rate of 0.086 for S-Gini (δ=1.5), compared to 0.162 for ASE and 0.188 for GG(5).\n\n2. The GG procedure suffered from aggregation bias, leading to poor size characteristics. The formula for its inconsistency is implied by the over-rejection rates (e.g., 0.386 for GG(5) vs. 0.086 for BOOT-T in case 1 with N=50).\n\n3. The DB procedure required significantly more computational resources but generally underperformed, with lower power (e.g., 0.232 rejection rate in case 4 with N=500) compared to BOOT-T (0.614 in the same scenario).",
    "category": "CORRECT",
    "question_context_used": "The Monte Carlo experiments evaluated various inference procedures for generalized Gini indices, including ASE, BSE, BOOT-T, DB, and GG methods. Key findings include: \n\n1. The BOOT-T procedure, which applies bootstrap resampling to asymptotically pivotal test statistics, demonstrated the most superior size and power properties, especially in small samples. For example, in case 1 with N=50, BOOT-T had a rejection rate of 0.086 for S-Gini (δ=1.5), compared to 0.162 for ASE and 0.188 for GG(5).\n\n2. The GG procedure suffered from aggregation bias, leading to poor size characteristics. The formula for its inconsistency is implied by the over-rejection rates (e.g., 0.386 for GG(5) vs. 0.086 for BOOT-T in case 1 with N=50).\n\n3. The DB procedure required significantly more computational resources but generally underperformed, with lower power (e.g., 0.232 rejection rate in case 4 with N=500) compared to BOOT-T (0.614 in the same scenario).",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3019",
    "question": "In the context of sunspot equilibria with endogenous versus random matching, which of the following statements are correct based on the model's implications?",
    "options": [
      "A) Sunspot equilibria are more likely to exist in the endogenous-matching model when $H_{2}$ is high and $H_{1}$ is low.",
      "B) The existence of sunspot equilibria is independent of whether matching is endogenous or random.",
      "C) Money is more valuable in the random-matching model, leading to a higher $H_{1}$ requirement for trade in state 1.",
      "D) The incentive conditions for trade in state 2 are satisfied under opposite conditions in endogenous and random matching models."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Corbae-MatchingMoney-2002.md"
    ],
    "question_context": "The analysis of sunspot equilibria reveals that their existence depends on the transition probabilities $H_{1}$ and $H_{2}$ between states. The model shows that money is more valuable under endogenous matching, which affects the conditions under which agents are willing to trade. Specifically, the equilibrium exists in the endogenous-matching model when $H_{2}$ is high and $H_{1}$ is low, and in the random-matching model under the opposite conditions. The key formulas are:\n\n$$\nV_{m}^{1}=\\beta\\{H_{1}V_{m}^{2}+(1-H_{1})V_{m}^{1}\\}\n$$\n\nand\n\n$$\n\\begin{array}{l}{{V_{0}^{2}=\\beta\\{a_{0}[-C+H_{2}V_{1}^{1}+(1-H_{2})V_{1}^{2}]}}\\ {{{}}}\\ {{\\quad+(1-a_{0})[H_{2}V_{0}^{1}+(1-H_{2})V_{0}^{2}]\\}}}\\ {{{}}}\\ {{V_{1}^{2}=\\beta\\{a_{1}[U+H_{2}V_{0}^{1}+(1-H_{2})V_{0}^{2}]}}\\ {{{}}}\\ {{\\quad+(1-a_{1})[H_{2}V_{1}^{1}+(1-H_{2})V_{1}^{2}]\\}.}}\\end{array}\n$$\n\nThese equations highlight the dynamic interplay between state transitions and agent valuations in both matching frameworks.",
    "category": "CORRECT",
    "question_context_used": "The analysis of sunspot equilibria reveals that their existence depends on the transition probabilities $H_{1}$ and $H_{2}$ between states. The model shows that money is more valuable under endogenous matching, which affects the conditions under which agents are willing to trade. Specifically, the equilibrium exists in the endogenous-matching model when $H_{2}$ is high and $H_{1}$ is low, and in the random-matching model under the opposite conditions. The key formulas are:\n\n$$\nV_{m}^{1}=\\beta\\{H_{1}V_{m}^{2}+(1-H_{1})V_{m}^{1}\\}\n$$\n\nand\n\n$$\n\\begin{array}{l}{{V_{0}^{2}=\\beta\\{a_{0}[-C+H_{2}V_{1}^{1}+(1-H_{2})V_{1}^{2}]}}\\ {{{}}}\\ {{\\quad+(1-a_{0})[H_{2}V_{0}^{1}+(1-H_{2})V_{0}^{2}]\\}}}\\ {{{}}}\\ {{V_{1}^{2}=\\beta\\{a_{1}[U+H_{2}V_{0}^{1}+(1-H_{2})V_{0}^{2}]}}\\ {{{}}}\\ {{\\quad+(1-a_{1})[H_{2}V_{1}^{1}+(1-H_{2})V_{1}^{2}]\\}.}}\\end{array}\n$$\n\nThese equations highlight the dynamic interplay between state transitions and agent valuations in both matching frameworks.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1068",
    "question": "According to the theoretical framework of criminal behavior under Daylight Saving Time (DST), which of the following statements are supported by the model?",
    "options": [
      "A) Increased ambient light unambiguously reduces crime by increasing the probability of capture.",
      "B) The net effect of DST on crime is ambiguous due to opposing partial and general equilibrium effects.",
      "C) DST has a deterrent effect on crime only during evening hours that were formerly dark but are now light.",
      "D) The behavioral model predicts that criminals will increase their hours of activity to maintain a set level of income when net wages decrease."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Doleac-COVERDARKNESSAMBIENT-2015.md"
    ],
    "question_context": "The model posits that criminal behavior is a function of expected benefits and costs, where the expected cost of crime depends on the probability of capture $(P)$, which is influenced by ambient light $(L)$, and other factors $(F)$. The probability of capture increases with light $(\\partial P/\\partial L > 0)$, but the general equilibrium effect is ambiguous because increased light may also decrease criminal search costs $(\\partial D/\\partial L < 0)$. The model equation is:\n\n$$\nE[Benefit_{crime}] > E[Cost(T, P(L, F), D(L))_{crime}].\n$$\n\nIncapacitation effects from DST are evident at all hours, but deterrent effects are specific to hours directly affected by light changes. The labor supply analogy suggests criminals may adjust activity based on net wages, with classic and behavioral models offering differing predictions on daily crime volume.",
    "category": "CORRECT",
    "question_context_used": "The model posits that criminal behavior is a function of expected benefits and costs, where the expected cost of crime depends on the probability of capture $(P)$, which is influenced by ambient light $(L)$, and other factors $(F)$. The probability of capture increases with light $(\\partial P/\\partial L > 0)$, but the general equilibrium effect is ambiguous because increased light may also decrease criminal search costs $(\\partial D/\\partial L < 0)$. The model equation is:\n\n$$\nE[Benefit_{crime}] > E[Cost(T, P(L, F), D(L))_{crime}].\n$$\n\nIncapacitation effects from DST are evident at all hours, but deterrent effects are specific to hours directly affected by light changes. The labor supply analogy suggests criminals may adjust activity based on net wages, with classic and behavioral models offering differing predictions on daily crime volume.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4243",
    "question": "Based on the instrumental variables results in Table A.5, which of the following statements are statistically significant at the 5% level regarding the impact of IDP inflows on rental prices by income level?",
    "options": [
      "A) IDP inflows have a positive and significant effect on low-income rental prices",
      "B) IDP inflows have a negative and significant effect on high-income rental prices",
      "C) IDP inflows have no significant effect on middle-income rental prices",
      "D) IDP inflows have a positive and significant effect on average rental prices"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Unexpected-guests--The-impact-of-internal-displacement_2018_Journal-of-Devel.md"
    ],
    "question_context": "Table A.5 presents instrumental variables results for the impact of IDP inflows on rental prices by income level. The coefficients for low-income rental prices (0.0157***) and high-income rental prices (-0.0293**) are statistically significant at the 1% and 5% levels, respectively. The coefficient for middle-income rental prices (-0.00268) is not statistically significant. The table also shows first-stage F-statistics of 61.03, indicating strong instruments. All variables are expressed in natural logarithms, and standard errors are clustered at the city.year level. The regressions include time and city fixed effects, city-specific linear trends, and time dummies interacted with remoteness, weighted by city population.",
    "category": "CORRECT",
    "question_context_used": "Table A.5 presents instrumental variables results for the impact of IDP inflows on rental prices by income level. The coefficients for low-income rental prices (0.0157***) and high-income rental prices (-0.0293**) are statistically significant at the 1% and 5% levels, respectively. The coefficient for middle-income rental prices (-0.00268) is not statistically significant. The table also shows first-stage F-statistics of 61.03, indicating strong instruments. All variables are expressed in natural logarithms, and standard errors are clustered at the city.year level. The regressions include time and city fixed effects, city-specific linear trends, and time dummies interacted with remoteness, weighted by city population.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-217",
    "question": "In the context of cognitive load and self-control costs, which of the following statements are consistent with the nonlinear specification of self-control costs?",
    "options": [
      "A) The marginal cost of self-control increases with cognitive load, leading to changes in the ranking of alternatives.",
      "B) The linear specification of self-control costs can explain the experimental data but fails to account for psychological evidence on limited self-control resources.",
      "C) The independence of irrelevant alternatives is always satisfied under nonlinear self-control costs.",
      "D) The nonlinear specification implies that the effect of cognitive load on self-control is additive and dependent on the convexity of the cost function."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Fudenberg-DualSelfModelImpulse-2006.md"
    ],
    "question_context": "The nonlinear specification of self-control costs is given by: \n\n$$\nC(d,f)=g(d+u^{h}-u^{f})-g(d),\n$$\n\nwhere $g$ is an increasing convex function with $g^{\\prime}>0$ and $g^{\\prime\\prime}>0$. This formulation captures the psychological evidence that self-control is a limited resource, as discussed by Mark Muraven et al. (1998) and Muraven and Baumeister (2000). The linear specification, while consistent with the experimental data, fails to account for the dynamic changes in the marginal cost of self-control and the ranking of alternatives. The nonlinear model also leads to violations of the independence of irrelevant alternatives, as demonstrated in the example with broccoli, frozen yogurt, and ice cream.",
    "category": "CORRECT",
    "question_context_used": "The nonlinear specification of self-control costs is given by: \n\n$$\nC(d,f)=g(d+u^{h}-u^{f})-g(d),\n$$\n\nwhere $g$ is an increasing convex function with $g^{\\prime}>0$ and $g^{\\prime\\prime}>0$. This formulation captures the psychological evidence that self-control is a limited resource, as discussed by Mark Muraven et al. (1998) and Muraven and Baumeister (2000). The linear specification, while consistent with the experimental data, fails to account for the dynamic changes in the marginal cost of self-control and the ranking of alternatives. The nonlinear model also leads to violations of the independence of irrelevant alternatives, as demonstrated in the example with broccoli, frozen yogurt, and ice cream.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-55",
    "question": "In the context of Plan V and Plan M economies, which of the following statements are true regarding the effects of annuitisation and social security taxes on growth?",
    "options": [
      "A) Higher social security tax rates unambiguously reduce the growth rate under both plans.",
      "B) Full annuitisation (γᵛ=γᴹ=1) always leads to higher growth rates regardless of longevity.",
      "C) Under Plan V with τ>0, higher annuitisation rates can lead to either faster or slower growth.",
      "D) Growth is maximized when both social security taxes and annuitisation rates are zero."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Effects_of_Annuities,_Bequests,_and_Aging_in_an_Overlapping_Generations_Model_of_Endogenous_Growth.md"
    ],
    "question_context": "The dynamic paths of Plan V and Plan M economies are described by complex difference equations involving capital accumulation, annuitisation rates (γᵛ, γᴹ), and social security taxes (τ). Key findings include: (1) Higher social security tax rates reduce growth by diminishing savings incentives (Proposition 2). (2) Under Plan V with τ>0, the effect of annuitisation on growth is ambiguous due to opposing income and bequest effects (Proposition 4). (3) Growth is maximized when τ=0 and γᵛ=γᴹ=0 (Proposition 5). The models highlight the divergence between individual and social optima due to capital accumulation externalities. Formula for Plan V steady-state annuitisation: \n\n$$\nγ^{v}=\\frac{\\displaystyle p}{\\displaystyle\\left({\\mathrm{I}-\\beta^{2}}\\right)\\beta}[\\left({\\mathrm{I}-\\beta-\\eta}\\right)-\\beta({\\mathrm{I}+\\beta})],\n$$",
    "category": "CORRECT",
    "question_context_used": "The dynamic paths of Plan V and Plan M economies are described by complex difference equations involving capital accumulation, annuitisation rates (γᵛ, γᴹ), and social security taxes (τ). Key findings include: (1) Higher social security tax rates reduce growth by diminishing savings incentives (Proposition 2). (2) Under Plan V with τ>0, the effect of annuitisation on growth is ambiguous due to opposing income and bequest effects (Proposition 4). (3) Growth is maximized when τ=0 and γᵛ=γᴹ=0 (Proposition 5). The models highlight the divergence between individual and social optima due to capital accumulation externalities. Formula for Plan V steady-state annuitisation: \n\n$$\nγ^{v}=\\frac{\\displaystyle p}{\\displaystyle\\left({\\mathrm{I}-\\beta^{2}}\\right)\\beta}[\\left({\\mathrm{I}-\\beta-\\eta}\\right)-\\beta({\\mathrm{I}+\\beta})],\n$$",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4083",
    "question": "In the context of the standard game on the rectangle as described in Theorem 5.1, which of the following conditions are essential for the existence of an equilibrium?",
    "options": [
      "A) The payoff functions $l_i$ and $m_i$ are continuous and bounded for all players.",
      "B) The functions $\\varphi_1$ and $\\varphi_2$ are upper semi-continuous with only jump discontinuities.",
      "C) The payoff $u_i$ is continuous at the point $o$ for all players.",
      "D) The strategy sets $X_i$ are identical for both players."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Variational-convergence--Approximation-and-existence-of_2010_Journal-of-Econ.md"
    ],
    "question_context": "The standard game on the rectangle involves two players with strategy sets $X_i = [a_i, b_i]$. The payoff functions are defined piecewise, with discontinuities along a curve parameterized by a strictly increasing function $f$. Key conditions for the existence of equilibrium include:\n\n1. Continuity and boundedness of $l_i$ and $m_i$ for all players.\n2. Upper semi-continuity of $\\varphi_1$ and $\\varphi_2$ with only jump discontinuities.\n3. Continuity of $u_i$ at the point $o$.\n\nThese conditions ensure that the modified game satisfies the requirements of Theorem 4.2, leading to the existence of an equilibrium. The strategy sets need not be identical, as the setting is more general than previous theorems.",
    "category": "CORRECT",
    "question_context_used": "The standard game on the rectangle involves two players with strategy sets $X_i = [a_i, b_i]$. The payoff functions are defined piecewise, with discontinuities along a curve parameterized by a strictly increasing function $f$. Key conditions for the existence of equilibrium include:\n\n1. Continuity and boundedness of $l_i$ and $m_i$ for all players.\n2. Upper semi-continuity of $\\varphi_1$ and $\\varphi_2$ with only jump discontinuities.\n3. Continuity of $u_i$ at the point $o$.\n\nThese conditions ensure that the modified game satisfies the requirements of Theorem 4.2, leading to the existence of an equilibrium. The strategy sets need not be identical, as the setting is more general than previous theorems.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2615",
    "question": "In the context of state-dependent utility models with imprecise beliefs and tastes, which of the following axioms are essential for ensuring a coherent notion of preferences conditional on a state?",
    "options": [
      "A) Monotonicity (A4)",
      "B) State Consistency (A6)",
      "C) Strong Uncertainty Aversion with respect to best-and-worst acts (A8)",
      "D) EC-Independence with respect to best-and-worst acts (A9)"
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Hill-NONBAYESIANTHEORYSTATEDEPENDENT-2019.md"
    ],
    "question_context": "The paper proposes a model of uncertainty averse preferences that incorporates imprecision of beliefs, imprecision of tastes, and state dependence of utility. Key formula: $$\\operatorname*{min}_{p\\in{\\mathcal{C}}}\\sum_{s\\in S}p(s)\\operatorname*{min}_{u\\in\\nu(s)}u\\big(f(s)\\big),$$ where $\\mathcal{C}$ is a set of probability measures and $\\nu(s)$ is a set of utility functions for each state $s$. The model requires specific axioms to handle these phenomena. State Consistency (A6) ensures that preferences conditional on a state can be coherently defined, while Strong Uncertainty Aversion (A8) with respect to best-and-worst acts captures hedging behavior in the presence of imprecise beliefs and tastes. These axioms are crucial for maintaining decision-theoretic coherence without relying on state independence or constant acts.",
    "category": "CORRECT",
    "question_context_used": "The paper proposes a model of uncertainty averse preferences that incorporates imprecision of beliefs, imprecision of tastes, and state dependence of utility. Key formula: $$\\operatorname*{min}_{p\\in{\\mathcal{C}}}\\sum_{s\\in S}p(s)\\operatorname*{min}_{u\\in\\nu(s)}u\\big(f(s)\\big),$$ where $\\mathcal{C}$ is a set of probability measures and $\\nu(s)$ is a set of utility functions for each state $s$. The model requires specific axioms to handle these phenomena. State Consistency (A6) ensures that preferences conditional on a state can be coherently defined, while Strong Uncertainty Aversion (A8) with respect to best-and-worst acts captures hedging behavior in the presence of imprecise beliefs and tastes. These axioms are crucial for maintaining decision-theoretic coherence without relying on state independence or constant acts.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2792",
    "question": "According to the exclusion principle in rent-maximizing lobbying games, under which conditions does a politician benefit from excluding lobbyists with the highest valuations?",
    "options": [
      "A) When the highest valuation is equal to the second-highest valuation",
      "B) When the highest valuation is strictly greater than the second-highest, and the second-highest equals the third-highest",
      "C) When the expected rents from excluding the highest-valuation lobbyist exceed those from including them",
      "D) When all lobbyists have identical valuations"
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Baye-RiggingLobbyingProcess-1993.md"
    ],
    "question_context": "Key formula for expected rents when excluding lobbyists: \n\n$$W\\big(\\hat{v}_{1},\\hat{v}_{2}\\big)=\\left(1+\\frac{\\hat{v}_{2}}{\\hat{v}_{1}}\\right)\\frac{\\hat{v}_{2}}{2}.$$\n\nPROPOSITION 2 establishes that the politician maximizes expected rents by excluding lobbyists with valuations strictly greater than $v_{k}$, where $k$ satisfies the inequality:\n\n$$\\left(1+\\frac{v_{k+1}}{v_{k}}\\right)\\frac{v_{k+1}}{2}\\geq\\left(1+\\frac{v_{i+1}}{v_{i}}\\right)\\frac{v_{i+1}}{2}\\quad\\forall\\:i.$$\n\nCOROLLARY 1 shows no benefit from exclusion when $v_{1} = v_{2}$, while COROLLARY 2 demonstrates that exclusion is optimal when $v_{1}>v_{2}=v_{3}$. The numerical example with $v_{1}=50$, $v_{2}=40$, and $v_{3}=38$ yields higher expected rents ($W=37.05$) when excluding the highest-valuation lobbyist compared to inclusion ($W=36$).",
    "category": "CORRECT",
    "question_context_used": "Key formula for expected rents when excluding lobbyists: \n\n$$W\\big(\\hat{v}_{1},\\hat{v}_{2}\\big)=\\left(1+\\frac{\\hat{v}_{2}}{\\hat{v}_{1}}\\right)\\frac{\\hat{v}_{2}}{2}.$$\n\nPROPOSITION 2 establishes that the politician maximizes expected rents by excluding lobbyists with valuations strictly greater than $v_{k}$, where $k$ satisfies the inequality:\n\n$$\\left(1+\\frac{v_{k+1}}{v_{k}}\\right)\\frac{v_{k+1}}{2}\\geq\\left(1+\\frac{v_{i+1}}{v_{i}}\\right)\\frac{v_{i+1}}{2}\\quad\\forall\\:i.$$\n\nCOROLLARY 1 shows no benefit from exclusion when $v_{1} = v_{2}$, while COROLLARY 2 demonstrates that exclusion is optimal when $v_{1}>v_{2}=v_{3}$. The numerical example with $v_{1}=50$, $v_{2}=40$, and $v_{3}=38$ yields higher expected rents ($W=37.05$) when excluding the highest-valuation lobbyist compared to inclusion ($W=36$).",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\"]}",
      "model_answer": [
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1182",
    "question": "In the analytic solutions for optimal cash management thresholds, which of the following conditions lead to a precautionary demand for money?",
    "options": [
      "A) An increase in the volatility of net disbursements ($\\partial u/\\partial\\sigma_x > 0$)",
      "B) A decrease in the fixed adjustment costs ($\\partial u/\\partial K < 0$)",
      "C) A rise in the interest rate predictability when $r > \\gamma$ ($\\partial u/\\partial\\alpha > 0$)",
      "D) A reduction in the covariance between net disbursements and interest rates ($\\partial u/\\partial\\sigma_{xr} = 0$)"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Transactions Demand for Money with a Stochastic, Time-Varying Interest Rate.md"
    ],
    "question_context": "The optimal targets and thresholds are forward-looking, $\\pmb{F}$ -adapted controls which depend on expected, future interest rates. I assume that the expectations of money-holders are rational in that the expected interest rate process coincides with the actual process almost surely; $\\alpha,\\gamma,$ and $\\pmb{r}$ summarize conditional expectations of the interest rate. If $r$ followed a $p$ 'th-order, closed, linear stochastic differential equation then $\\pmb{u}$ would depend on $r$ and its first ${\\pmb p-1}$ derivatives just as with convex costs. The partial derivatives of $\\pmb{u}$ have signs as follows. In the first case $\\partial u/\\partial k^{-}>0$, $\\partial u/\\partial k^{+}>0$; an increase in the cost of adjusting at either threshold leads to a wider continuation region. Also $\\partial u/\\partial|\\mu|<0$. In the second case, $\\partial u/\\partial K>0$. In both cases $\\partial u/\\partial\\sigma_{x}>0$: there is a precautionary demand for money in that the less predictable the net disbursements process the more cash is held. Note also that the controls do not depend on $\\sigma_{x r}$ although the expected minimized costs obviously are increasing in this covariance.",
    "category": "INCORRECT",
    "question_context_used": "The optimal targets and thresholds are forward-looking, $\\pmb{F}$ -adapted controls which depend on expected, future interest rates. I assume that the expectations of money-holders are rational in that the expected interest rate process coincides with the actual process almost surely; $\\alpha,\\gamma,$ and $\\pmb{r}$ summarize conditional expectations of the interest rate. If $r$ followed a $p$ 'th-order, closed, linear stochastic differential equation then $\\pmb{u}$ would depend on $r$ and its first ${\\pmb p-1}$ derivatives just as with convex costs. The partial derivatives of $\\pmb{u}$ have signs as follows. In the first case $\\partial u/\\partial k^{-}>0$, $\\partial u/\\partial k^{+}>0$; an increase in the cost of adjusting at either threshold leads to a wider continuation region. Also $\\partial u/\\partial|\\mu|<0$. In the second case, $\\partial u/\\partial K>0$. In both cases $\\partial u/\\partial\\sigma_{x}>0$: there is a precautionary demand for money in that the less predictable the net disbursements process the more cash is held. Note also that the controls do not depend on $\\sigma_{x r}$ although the expected minimized costs obviously are increasing in this covariance.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-734",
    "question": "In the context of cointegration and dynamic simultaneous equations models, which of the following conditions are essential for valid inference?",
    "options": [
      "A) Stationarity of the error correction term",
      "B) Existence of a long-run equilibrium relationship among variables",
      "C) Independence of the error terms across equations",
      "D) Homoskedasticity of the residuals in all equations"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "CorrectionCointegrationDynamic-2000.md"
    ],
    "question_context": "The paper discusses the conditions under which cointegration and dynamic simultaneous equations models yield valid inference. Key formula: The error correction term must be stationary for the model to capture the long-run equilibrium relationship. Additionally, the existence of a cointegrating relationship is necessary to avoid spurious regression results. The paper emphasizes that these conditions are critical for the robustness of the estimators in the presence of non-stationary time series data.",
    "category": "CORRECT",
    "question_context_used": "The paper discusses the conditions under which cointegration and dynamic simultaneous equations models yield valid inference. Key formula: The error correction term must be stationary for the model to capture the long-run equilibrium relationship. Additionally, the existence of a cointegrating relationship is necessary to avoid spurious regression results. The paper emphasizes that these conditions are critical for the robustness of the estimators in the presence of non-stationary time series data.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1399",
    "question": "In the context of indigenisation policies, which of the following factors contribute to the social costs as defined by the model?",
    "options": [
      "A) The average rate of gross profit in alternative investments",
      "B) The level of indigenisation, measured by the amount of equity sold to host country nationals",
      "C) The repatriation of the parent company's equity holding",
      "D) The increase in consumer surplus due to differentiated products"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Multinational-firms--global-strategies--host-country_1983_Journal-of-Develop.md"
    ],
    "question_context": "The social costs of indigenisation arise because (i) the purchase of the subsidiary's equity will divert resources from other uses in the economy, and (i) the parent's equity holding will be repatriated. The cost of indigenisation may initially be defined as the loss of gross profits in the alternative investments. It consists of the decrease in the host government's profits-tax revenues and the decrease in the private investors' net profits. The social costs of indigenisation denoted by $c_{h}$ are defined as $$C_{h}=A_{h}E_{h}.$$ In other words, the social costs depend on (i) the average rate of gross profit in the alternative investments, and (i) the level of indigenisation. The marginal costs of indigenisation are got by differentiating (1) with respect to $E_{h}$ $$\\mathrm{d}C_{h}/\\mathrm{d}E_{h}=A_{h}+(\\mathrm{d}A_{h}/\\mathrm{d}E_{h})E_{h},$$ and differentiating again we get the second-order effects $$\\mathrm{d}^{2}C_{h}/\\mathrm{d}E_{h}^{2}=2(\\ensuremath{\\mathrm{d}A_{h}}/\\ensuremath{\\mathrm{d}E_{h}})+(\\ensuremath{\\mathrm{d}^{2}A_{h}}/\\ensuremath{\\mathrm{d}E_{h}^{2}})E_{h}.$$",
    "category": "CORRECT",
    "question_context_used": "The social costs of indigenisation arise because (i) the purchase of the subsidiary's equity will divert resources from other uses in the economy, and (i) the parent's equity holding will be repatriated. The cost of indigenisation may initially be defined as the loss of gross profits in the alternative investments. It consists of the decrease in the host government's profits-tax revenues and the decrease in the private investors' net profits. The social costs of indigenisation denoted by $c_{h}$ are defined as $$C_{h}=A_{h}E_{h}.$$ In other words, the social costs depend on (i) the average rate of gross profit in the alternative investments, and (i) the level of indigenisation. The marginal costs of indigenisation are got by differentiating (1) with respect to $E_{h}$ $$\\mathrm{d}C_{h}/\\mathrm{d}E_{h}=A_{h}+(\\mathrm{d}A_{h}/\\mathrm{d}E_{h})E_{h},$$ and differentiating again we get the second-order effects $$\\mathrm{d}^{2}C_{h}/\\mathrm{d}E_{h}^{2}=2(\\ensuremath{\\mathrm{d}A_{h}}/\\ensuremath{\\mathrm{d}E_{h}})+(\\ensuremath{\\mathrm{d}^{2}A_{h}}/\\ensuremath{\\mathrm{d}E_{h}^{2}})E_{h}.$$",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-901",
    "question": "In the context of incentive-compatible strategies and the Contraction Transform Lemma, which of the following conditions ensure that a strategy S is incentive compatible?",
    "options": [
      "A) S₁ is incentive-compatible over Θ₁ and S₂ is incentive-compatible over Θ₂",
      "B) The strategy S contracts over Θ₂ toward the bliss point of Θ₂ while moving away from the bliss point of Θ₁",
      "C) The utility function satisfies 𝒰(θ′;θ) ≤ 𝒰(θ;θ) for all θ,θ′ ∈ 𝒲",
      "D) The intervals Θ₁ and Θ₂ are non-overlapping and partition Θ"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Linear-Riley-equilibria-in-quadratic-signaling-_2023_Journal-of-Economic-The.md"
    ],
    "question_context": "The Contraction and Expansion Lemmas help identify the optimal incentive-compatible strategy in the proof of Theorem 2. A strategy S is incentive compatible over 𝒲 if 𝒰(θ′;θ) ≤ 𝒰(θ;θ) for all θ,θ′ ∈ 𝒲. The Contraction Transform Lemma states that if Θ₁ and Θ₂ are two non-overlapping intervals partitioning Θ, and S₁(θ) is an incentive-compatible separating strategy over Θ₁ while S₂(θ) is incentive compatible over Θ₂, then the combined strategy S(θ) = {S₁(θ) if θ ∈ Θ₁, S₂(θ) if θ ∈ Θ₂} is incentive compatible if it contracts over Θ₂ toward the bliss point of Θ₂ while moving away from the bliss point of Θ₁. The proof involves showing that neither type θ₁ ∈ Θ₁ has an incentive to mimic θ₂ ∈ Θ₂ nor vice versa, using inequalities derived from the utility functions and the properties of S₁ and S₂.",
    "category": "INCORRECT",
    "question_context_used": "The Contraction and Expansion Lemmas help identify the optimal incentive-compatible strategy in the proof of Theorem 2. A strategy S is incentive compatible over 𝒲 if 𝒰(θ′;θ) ≤ 𝒰(θ;θ) for all θ,θ′ ∈ 𝒲. The Contraction Transform Lemma states that if Θ₁ and Θ₂ are two non-overlapping intervals partitioning Θ, and S₁(θ) is an incentive-compatible separating strategy over Θ₁ while S₂(θ) is incentive compatible over Θ₂, then the combined strategy S(θ) = {S₁(θ) if θ ∈ Θ₁, S₂(θ) if θ ∈ Θ₂} is incentive compatible if it contracts over Θ₂ toward the bliss point of Θ₂ while moving away from the bliss point of Θ₁. The proof involves showing that neither type θ₁ ∈ Θ₁ has an incentive to mimic θ₂ ∈ Θ₂ nor vice versa, using inequalities derived from the utility functions and the properties of S₁ and S₂.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4185",
    "question": "In the context of the proposed maximum test and power-enhanced test simulations, which of the following statements are correct regarding the empirical size and power under different covariance structures?",
    "options": [
      "A) The empirical size of the Wald test (S_Wald) and the maximum test (L_max) is close to the nominal level 0.05 after small-sample correction under the null hypothesis.",
      "B) The power-enhanced test (PE) significantly outperforms the maximum test (L_max) against the sparse alternative hypothesis (H_s) in high-dimensional settings where p > T.",
      "C) The Fisher’s combined probability test (F_joint) exhibits slightly inflated empirical size, especially when both p and T are small, due to finite-sample effects.",
      "D) The liberal p-value-based F-tests (F_1^L, F_2^L, F_3^L, F_4^L, F_max^L) show substantial improvements in empirical power compared to the original F-tests while maintaining well-controlled empirical size."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Power-enhancement-for-testing-multi-factor-asset-pricin_2024_Journal-of-Econ.md"
    ],
    "question_context": "The empirical size of S_Wald and L_max is close to the nominal level 0.05 after the small-sample correction under the null hypothesis. The empirical size of PE and F_joint is also close to the nominal level but slightly larger than that of S_Wald and L_max in these finite-sample cases. The size distortion of F_joint is due to the finite-sample effect of the asymptotic independence. The asymptotic independence (Theorem 3) of S_Wald and L_max is established under the asymptotic regime when p, T → ∞. In finite-sample experiments, especially when both p and T are small, the empirical dependence between S_Wald and L_max may not be small enough to be practically negligible. For instance, the sample correlation between S_Wald and L_max is as large as 0.43 when p, T = 100 in our experiments, which leads to an inflated empirical size of 7.5% in Table 4. Such finite-sample biases can be corrected by adopting empirically determined critical values. The liberal p-value-based F-tests (F_1^L, F_2^L, F_3^L, F_4^L, F_max^L) show substantial improvements in empirical power compared to the original F-tests while maintaining well-controlled empirical size. The F_q^L tests no longer have the non-decreasing trend in q as in the F_q tests. Instead, the F_max^L test achieves the highest power against H_s while the F_2^L test performs the best against H_d.",
    "category": "INCORRECT",
    "question_context_used": "The empirical size of S_Wald and L_max is close to the nominal level 0.05 after the small-sample correction under the null hypothesis. The empirical size of PE and F_joint is also close to the nominal level but slightly larger than that of S_Wald and L_max in these finite-sample cases. The size distortion of F_joint is due to the finite-sample effect of the asymptotic independence. The asymptotic independence (Theorem 3) of S_Wald and L_max is established under the asymptotic regime when p, T → ∞. In finite-sample experiments, especially when both p and T are small, the empirical dependence between S_Wald and L_max may not be small enough to be practically negligible. For instance, the sample correlation between S_Wald and L_max is as large as 0.43 when p, T = 100 in our experiments, which leads to an inflated empirical size of 7.5% in Table 4. Such finite-sample biases can be corrected by adopting empirically determined critical values. The liberal p-value-based F-tests (F_1^L, F_2^L, F_3^L, F_4^L, F_max^L) show substantial improvements in empirical power compared to the original F-tests while maintaining well-controlled empirical size. The F_q^L tests no longer have the non-decreasing trend in q as in the F_q tests. Instead, the F_max^L test achieves the highest power against H_s while the F_2^L test performs the best against H_d.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3192",
    "question": "Under Harrod's 'Law of Diminishing Elasticity of Demand', which of the following conditions ensure that the stabilisation effect is guaranteed?",
    "options": [
      "A) Price-elasticity of demand is greater than unity and constant along the demand curve",
      "B) Price-elasticity of demand is less than unity and decreasing along the demand curve",
      "C) Price-elasticity of demand is greater than unity and decreasing along the demand curve",
      "D) Marginal revenue is increasing with output at the old equilibrium price"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Elasticity of Demand and Stabilisation.md"
    ],
    "question_context": "Key formula: Marginal revenue equals $\\scriptstyle\\not{P}(\\mathbf{r}-{\\frac{\\mathbf{r}}{e}})$, where $\\phi=$ price and $\\pmb{e}=$ elasticity of demand. Harrod's argument hinges on the assumption that elasticity of demand is less for boom than for depression demand curves, implying a 'law of diminishing elasticity of demand'. This leads to marginal revenue being less than marginal cost after expansion. The stabilisation effect is guaranteed if the price-elasticity of demand is greater than unity and is either constant or decreasing as we move along any market demand curve. The analysis further shows that if $e>1$ and $e$ decreases as quantity taken increases, marginal revenue decreases with output, prompting producers to contract output to raise marginal revenue, thus operating the stabilisation effect.",
    "category": "CORRECT",
    "question_context_used": "Key formula: Marginal revenue equals $\\scriptstyle\\not{P}(\\mathbf{r}-{\\frac{\\mathbf{r}}{e}})$, where $\\phi=$ price and $\\pmb{e}=$ elasticity of demand. Harrod's argument hinges on the assumption that elasticity of demand is less for boom than for depression demand curves, implying a 'law of diminishing elasticity of demand'. This leads to marginal revenue being less than marginal cost after expansion. The stabilisation effect is guaranteed if the price-elasticity of demand is greater than unity and is either constant or decreasing as we move along any market demand curve. The analysis further shows that if $e>1$ and $e$ decreases as quantity taken increases, marginal revenue decreases with output, prompting producers to contract output to raise marginal revenue, thus operating the stabilisation effect.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\"]}",
      "model_answer": [
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1417",
    "question": "Based on the empirical evidence presented, which of the following statements are true regarding human capital Sharpe ratios and mean-variance spanning tests?",
    "options": [
      "A) Human capital Sharpe ratios are generally higher than those of the U.S. equity index for most demographic groups.",
      "B) The null hypothesis of mean-variance spanning is typically rejected when human capital returns are added to a benchmark of financial returns for individuals with college or higher education.",
      "C) The GMM and likelihood ratio tests for mean-variance spanning yield significantly different results in most cases.",
      "D) The increase in expected return per unit of risk (Δσ(m)) is negatively correlated with the p-values of the spanning tests."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "PalaciosHuerta-EmpiricalAnalysisRisk-2003.md"
    ],
    "question_context": "The average rates of return are broadly consistent with previous estimates in the literature for most groups. It is important to note, however, that when compared to the return on the U.S. equity index, mean human capital returns are relatively high and the standard deviation of returns relatively low for most demographic groups. Thus, human capital Sharpe ratios $\\left|\\bar{E}(\\bar{R^{e}})\\right|/\\sigma(R^{e})$ are often much larger than the Sharpe ratio of the U.S. equity index. This aspect is important since the information on the relative variability of human capital rates of return can help to explain a subtle but long standing problem in the literature. This problem arises from the fact that 'the private returns to schooling are not nearly large enough to justify the claims of importance made for schooling' as Mark Bils (2000, pp. 59-60) indicates. A more natural resolution, at least in part, to this problem can be offered by simply looking at the reward to variability ratio of the investment; that is, not just at the level of returns but at the Sharpe ratios. There are large differences between human capital Sharpe ratios and financial Sharpe ratios in many instances. The former ones clearly dominate the latter ones for many demographic groups at different education and experience levels. Note, however, that although broad differences in Sharpe ratios are clearly informative, the covariance structure of returns also plays a key role in determining differences in risk-adjusted returns.",
    "category": "CORRECT",
    "question_context_used": "The average rates of return are broadly consistent with previous estimates in the literature for most groups. It is important to note, however, that when compared to the return on the U.S. equity index, mean human capital returns are relatively high and the standard deviation of returns relatively low for most demographic groups. Thus, human capital Sharpe ratios $\\left|\\bar{E}(\\bar{R^{e}})\\right|/\\sigma(R^{e})$ are often much larger than the Sharpe ratio of the U.S. equity index. This aspect is important since the information on the relative variability of human capital rates of return can help to explain a subtle but long standing problem in the literature. This problem arises from the fact that 'the private returns to schooling are not nearly large enough to justify the claims of importance made for schooling' as Mark Bils (2000, pp. 59-60) indicates. A more natural resolution, at least in part, to this problem can be offered by simply looking at the reward to variability ratio of the investment; that is, not just at the level of returns but at the Sharpe ratios. There are large differences between human capital Sharpe ratios and financial Sharpe ratios in many instances. The former ones clearly dominate the latter ones for many demographic groups at different education and experience levels. Note, however, that although broad differences in Sharpe ratios are clearly informative, the covariance structure of returns also plays a key role in determining differences in risk-adjusted returns.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1638",
    "question": "In the context of optimal stopping rules and non-additive probabilities, which of the following conditions are essential for behaviorally consistent decision-making?",
    "options": [
      "A) The utility function must exhibit star-shaped properties",
      "B) The subjective expected utility must account for non-additive probabilities",
      "C) The expenditure function must be strictly convex",
      "D) The Frischian demands must satisfy Slutsky symmetry"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Papers-to-appear-in-forthcoming-issues_1990_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The articles listed discuss various advanced topics in economic theory, including behaviorally consistent optimal stopping rules and non-additive probabilities. Key concepts include the necessity of star-shaped utility functions for lotteries and insurance, as well as the role of subjective expected utility in finite state spaces with non-additive probabilities. The condition for behaviorally consistent optimal stopping rules is particularly relevant for understanding decision-making under uncertainty. The formula for subjective expected utility with non-additive probabilities is given by: \\[\\sum_{i=1}^{n} \\phi(p_i)u(x_i)\\] where \\(\\phi\\) is a non-additive probability measure and \\(u\\) is the utility function.",
    "category": "CORRECT",
    "question_context_used": "The articles listed discuss various advanced topics in economic theory, including behaviorally consistent optimal stopping rules and non-additive probabilities. Key concepts include the necessity of star-shaped utility functions for lotteries and insurance, as well as the role of subjective expected utility in finite state spaces with non-additive probabilities. The condition for behaviorally consistent optimal stopping rules is particularly relevant for understanding decision-making under uncertainty. The formula for subjective expected utility with non-additive probabilities is given by: \\[\\sum_{i=1}^{n} \\phi(p_i)u(x_i)\\] where \\(\\phi\\) is a non-additive probability measure and \\(u\\) is the utility function.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4033",
    "question": "Which of the following factors were identified as key contributors to the observed improvement in cognitive functioning among children aged 8-14 in the Matlab MCH-FP program?",
    "options": [
      "A) On-time measles vaccination and reduced sibling competition",
      "B) Provision of modern contraception and maternal health interventions",
      "C) Vitamin A supplementation and nutrition rehabilitation",
      "D) Reduction in maternal fertility and improved nutrient absorption"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Enhancing Cognitive Functioning_ Medium-Term Effects of a Health and Family Planning Program in Matlab.md"
    ],
    "question_context": "The Matlab MCH-FP program introduced child health interventions, including measles vaccination, DPT, polio, and tuberculosis immunizations, as well as vitamin A supplementation. Preventing diseases like measles not only reduces cognitive impairment but also improves children's ability to absorb nutrients and their overall nutritional status, which in turn enhances cognitive development. The program's phased implementation allowed for the estimation of causal effects on cognitive functioning, with significant improvements observed in children aged 8-14, who showed a 0.39 standard deviation increase in cognitive functioning. Key formula: The effect size is calculated as the difference in cognitive functioning between treatment and comparison areas, adjusted for baseline characteristics.",
    "category": "INCORRECT",
    "question_context_used": "The Matlab MCH-FP program introduced child health interventions, including measles vaccination, DPT, polio, and tuberculosis immunizations, as well as vitamin A supplementation. Preventing diseases like measles not only reduces cognitive impairment but also improves children's ability to absorb nutrients and their overall nutritional status, which in turn enhances cognitive development. The program's phased implementation allowed for the estimation of causal effects on cognitive functioning, with significant improvements observed in children aged 8-14, who showed a 0.39 standard deviation increase in cognitive functioning. Key formula: The effect size is calculated as the difference in cognitive functioning between treatment and comparison areas, adjusted for baseline characteristics.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2609",
    "question": "Under the response error mixture model with multiplicative mean independence, which of the following conditions lead to non-monotonic behavior of the outcome distribution function $w(z)$?",
    "options": [
      "A) $(p - \\frac{1}{\\gamma + 1})$ and $(\\gamma - 1)$ have opposite signs",
      "B) $\\gamma = 1$ and $p = 0.5$",
      "C) $\\gamma > 1$ and $p > \\frac{1}{\\gamma + 1}$",
      "D) $\\gamma < 1$ and $p < \\frac{1}{\\gamma + 1}$"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "04-Identification of Expected Outcomes in a Data Error Mixing Model With Multiplicative Mean Inde.md"
    ],
    "question_context": "The outcome distribution function $w(z)$ is characterized by its behavior across different values of $\\gamma$ and $p$. Specifically, $w(z)$ is weakly concave in $(-\\infty,\\frac{\\gamma}{\\gamma+1})$ and convex in $(\\frac{\\gamma}{\\gamma+1},\\infty)$, or vice versa, depending on the values $\\gamma$ and $p$. The local extrema of $w(z)$, which play a role in defining the bounds, are sensitive to these parameters. \n\nLemma 2 reveals that the function $w(z)$ is not monotonic in $z$ when $(p-\\frac{1}{\\gamma+1})$ and $(\\gamma-1)$ have opposite signs, in which case $w(z)$ exhibits interior extrema lying within $[\\operatorname*{min}\\{p,1-p\\}$ , $\\operatorname*{max}\\{p,1-p\\}]$; otherwise, $w(z)$ is monotonic within each of the two regions. For example, when $\\gamma < 1$ and $p > \\frac{1}{\\gamma + 1}$, $w(z)$ is not monotonic in $z$, and the bounds may be sensitive to interior extrema.",
    "category": "CORRECT",
    "question_context_used": "The outcome distribution function $w(z)$ is characterized by its behavior across different values of $\\gamma$ and $p$. Specifically, $w(z)$ is weakly concave in $(-\\infty,\\frac{\\gamma}{\\gamma+1})$ and convex in $(\\frac{\\gamma}{\\gamma+1},\\infty)$, or vice versa, depending on the values $\\gamma$ and $p$. The local extrema of $w(z)$, which play a role in defining the bounds, are sensitive to these parameters. \n\nLemma 2 reveals that the function $w(z)$ is not monotonic in $z$ when $(p-\\frac{1}{\\gamma+1})$ and $(\\gamma-1)$ have opposite signs, in which case $w(z)$ exhibits interior extrema lying within $[\\operatorname*{min}\\{p,1-p\\}$ , $\\operatorname*{max}\\{p,1-p\\}]$; otherwise, $w(z)$ is monotonic within each of the two regions. For example, when $\\gamma < 1$ and $p > \\frac{1}{\\gamma + 1}$, $w(z)$ is not monotonic in $z$, and the bounds may be sensitive to interior extrema.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4249",
    "question": "Based on the empirical findings regarding the Toxics Release Inventory (TRI) and its impact on housing markets, which of the following statements are supported by the evidence?",
    "options": [
      "A) TRI announcements led to significant reductions in house prices due to community mobilization against polluters.",
      "B) Changes in house prices do not forecast changes in toxic emissions, rejecting the anticipation hypothesis.",
      "C) The introduction of TRI reporting had virtually no effect on housing prices, even in high newspaper readership communities.",
      "D) Firms substituted listed TRI substances with equally toxic but unlisted substances, explaining the lack of price effects."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Bui-RegulationCapitalizationEnvironmental-2003.md"
    ],
    "question_context": "The empirical results in Table 8 seem to reject the hypothesis that house prices anticipate announced TRI emissions. Lagged house prices have little predictive power for future TRI announcements, no matter what the time period. The $\\pmb R$-square for all three equations (TRI announcements in 1987, change in TRI emissions between 1987 and 1988, and change in TRI emissions between 1988 and 1989) is exceedingly low, and $F$-tests cannot reject the hypotheses that the coefficients on all of the lagged house price terms are individually and jointly equal to 0. Furthermore, the introduction of the TRI reporting had virtually no effect on housing prices, and subsequent reductions in aggregate reported emissions between 1987 and 1990 have no significant effect on house prices, either in the aggregate or when disaggregated by type of toxic release. These results persist even in communities with high newspaper readership.",
    "category": "CORRECT",
    "question_context_used": "The empirical results in Table 8 seem to reject the hypothesis that house prices anticipate announced TRI emissions. Lagged house prices have little predictive power for future TRI announcements, no matter what the time period. The $\\pmb R$-square for all three equations (TRI announcements in 1987, change in TRI emissions between 1987 and 1988, and change in TRI emissions between 1988 and 1989) is exceedingly low, and $F$-tests cannot reject the hypotheses that the coefficients on all of the lagged house price terms are individually and jointly equal to 0. Furthermore, the introduction of the TRI reporting had virtually no effect on housing prices, and subsequent reductions in aggregate reported emissions between 1987 and 1990 have no significant effect on house prices, either in the aggregate or when disaggregated by type of toxic release. These results persist even in communities with high newspaper readership.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3951",
    "question": "In the model economy described by Green and Zhou (2002), which of the following conditions are necessary for the existence of a single price equilibrium where all trades occur at a specified price p?",
    "options": [
      "A) The presence of a divisible and non-storable fiat money",
      "B) The assumption of time preference in agents' utility functions",
      "C) The enforcement of a double auction mechanism in pairwise trade meetings",
      "D) The initial money holdings distribution being dominated in the tail by a geometric distribution"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Green-DynamicMonetaryEquilibrium-2002.md"
    ],
    "question_context": "The article by Green and Zhou (2002) analyzes an infinite horizon economy with pairwise trade and fiat money overcoming the lack of double coincidence of wants. Goods are divisible and perishable, and money is divisible without inventory constraints. The model features a double auction mechanism governing trade within pairs, ensuring anonymity and absence of market power. Key formula: The evolution of money holdings is given by \\(\\eta_{t+1}^{\\sigma}(\\eta_{0},\\omega) = \\eta_{t}^{\\sigma}(\\eta_{0},\\omega) + \\sigma_{t3}(\\eta_{0},\\omega)\\omega_{t3}\\) if trade occurs as a seller, and similarly adjusted if trade occurs as a buyer. The main results establish the existence of a continuum of single price equilibria, with the price level indeterminate but relative prices constant across trading pairs. The initial money holdings distribution must be dominated in the tail by a geometric distribution for the equilibrium to exist.",
    "category": "INCORRECT",
    "question_context_used": "The article by Green and Zhou (2002) analyzes an infinite horizon economy with pairwise trade and fiat money overcoming the lack of double coincidence of wants. Goods are divisible and perishable, and money is divisible without inventory constraints. The model features a double auction mechanism governing trade within pairs, ensuring anonymity and absence of market power. Key formula: The evolution of money holdings is given by \\(\\eta_{t+1}^{\\sigma}(\\eta_{0},\\omega) = \\eta_{t}^{\\sigma}(\\eta_{0},\\omega) + \\sigma_{t3}(\\eta_{0},\\omega)\\omega_{t3}\\) if trade occurs as a seller, and similarly adjusted if trade occurs as a buyer. The main results establish the existence of a continuum of single price equilibria, with the price level indeterminate but relative prices constant across trading pairs. The initial money holdings distribution must be dominated in the tail by a geometric distribution for the equilibrium to exist.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3139",
    "question": "In the context of the optimality equation and Pareto frontier analysis, which of the following statements are correct regarding the smooth pasting conditions and the solutions to (9)–(11)?",
    "options": [
      "A) The smooth pasting conditions ensure that the Pareto frontier is differentiable at the boundaries of the utility domain.",
      "B) The solution to (9)–(11) under initial condition $(w_{1},w_{2},\theta)=(\\overline{{u}}_{1},\\underline{{u}}_{2},\\underline{{\\theta}})$ is guaranteed to exist and be unique for any $\\overline{{u}}_{1} \\in [\\underline{{u}}_{1}, \\overline{{\\overline{{u}}}}_{1}]$.",
      "C) Theorem 1 implies that $\\Theta(\\overline{{u}}_{1})$ is single-peaked, leading to two distinct solutions for $\\overline{{u}}_{1}$ that satisfy $\\Theta(\\overline{{u}}_{1})=\\overline{{\\theta}}$.",
      "D) The curvature $\\kappa$ of the Pareto frontier increases with $\\overline{{u}}_{1}$, while the length $L$ decreases with $\\overline{{u}}_{1}$."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "On_the_Pacific_Salmon_Treaty.md"
    ],
    "question_context": "The smooth pasting conditions are imposed on the Pareto frontier to ensure differentiability at the boundaries of the utility domain. These conditions are given by:\n\n$$\n\\begin{array}{l}{{f^{\\prime}(w_{1}={\\underline{{u}}}_{1})=-(1+\\tau)^{-1},}}\\ {{\\qquad}}\\ {{f^{\\prime}(w_{1}={\\overline{{u}}}_{1})=-(1+\\tau).}}\\end{array}\n$$\n\nTheorem 1 shows that $\\Theta(\\overline{{u}}_{1})$ is single-peaked in $[\\underline{{u}}_{1},\\overline{{\\overline{{u}}}}_{1}]$, implying that the equation $\\Theta(\\overline{{u}}_{1})=\\overline{{\\theta}}$ admits two solutions for $\\overline{{u}}_{1}$. This result simplifies the search for the Pareto frontier. The curvature $\\kappa$ of the Pareto frontier decreases with $\\overline{{u}}_{1}$, while the length $L$ increases with $\\overline{{u}}_{1}$, leading to the single-peaked property of $\\Theta(\\overline{{u}}_{1})$.",
    "category": "INCORRECT",
    "question_context_used": "The smooth pasting conditions are imposed on the Pareto frontier to ensure differentiability at the boundaries of the utility domain. These conditions are given by:\n\n$$\n\\begin{array}{l}{{f^{\\prime}(w_{1}={\\underline{{u}}}_{1})=-(1+\\tau)^{-1},}}\\ {{\\qquad}}\\ {{f^{\\prime}(w_{1}={\\overline{{u}}}_{1})=-(1+\\tau).}}\\end{array}\n$$\n\nTheorem 1 shows that $\\Theta(\\overline{{u}}_{1})$ is single-peaked in $[\\underline{{u}}_{1},\\overline{{\\overline{{u}}}}_{1}]$, implying that the equation $\\Theta(\\overline{{u}}_{1})=\\overline{{\\theta}}$ admits two solutions for $\\overline{{u}}_{1}$. This result simplifies the search for the Pareto frontier. The curvature $\\kappa$ of the Pareto frontier decreases with $\\overline{{u}}_{1}$, while the length $L$ increases with $\\overline{{u}}_{1}$, leading to the single-peaked property of $\\Theta(\\overline{{u}}_{1})$.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1330",
    "question": "In the context of the four-equation disequilibrium model for centrally planned economies (CPEs), which of the following statements are correct regarding the consumption demand and supply equations?",
    "options": [
      "A) The consumption demand equation is derived from a Houthakker-Taylor savings function with parameters constrained by a priori hypotheses.",
      "B) The supply equation includes a term for deviations of net financial assets (NFA) from a second-order exponential time trend.",
      "C) The min condition in the model ensures that the observed quantity transacted is the average of quantities demanded and supplied.",
      "D) The plan-adjustment equation makes the consumption plan endogenous, influenced by deviations from plans of output, defense, and investment."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Portes-MacroeconomicPlanningDisequilibrium-1987.md"
    ],
    "question_context": "The consumption demand equation is derived directly from the Houthakker-Taylor savings function: \n\n$$ C D_{t}=\\alpha_{1}D N F A_{t-1}+\\alpha_{2}D Y D_{t}+\\alpha_{3}Y D_{t-1}+u_{1t} $$\n\nwhere CD is household desired expenditure on consumption goods and services; DNFA is household saving, measured as the change in net financial assets of households, NFA, during the period. The modified supply equation is:\n\n$$ C S_{t}=\\beta_{1}C_{t\\mid t-1}^{*}+\\beta_{2}C^{*}Z_{t}+\\beta_{4}R N F A_{t-1}+\\beta_{5}C Z X D_{t}+\\beta_{6}C Z X I_{t}+u_{2t} $$\n\nwhere $C S$ is planned supply of consumption goods and services; $C_{t|t-1}^{*}$ is the plan for consumption in the current period announced at the end of the previous period; RNFA is the deviation of current NFA from a second-order exponential time trend fitted to observed values of NFA. The min condition states that the observed quantity transacted is the lesser of the quantities demanded and supplied: \n\n$$ C_{t}=\\operatorname*{min}\\left(C D_{t},C S_{t}\\right) $$",
    "category": "CORRECT",
    "question_context_used": "The consumption demand equation is derived directly from the Houthakker-Taylor savings function: \n\n$$ C D_{t}=\\alpha_{1}D N F A_{t-1}+\\alpha_{2}D Y D_{t}+\\alpha_{3}Y D_{t-1}+u_{1t} $$\n\nwhere CD is household desired expenditure on consumption goods and services; DNFA is household saving, measured as the change in net financial assets of households, NFA, during the period. The modified supply equation is:\n\n$$ C S_{t}=\\beta_{1}C_{t\\mid t-1}^{*}+\\beta_{2}C^{*}Z_{t}+\\beta_{4}R N F A_{t-1}+\\beta_{5}C Z X D_{t}+\\beta_{6}C Z X I_{t}+u_{2t} $$\n\nwhere $C S$ is planned supply of consumption goods and services; $C_{t|t-1}^{*}$ is the plan for consumption in the current period announced at the end of the previous period; RNFA is the deviation of current NFA from a second-order exponential time trend fitted to observed values of NFA. The min condition states that the observed quantity transacted is the lesser of the quantities demanded and supplied: \n\n$$ C_{t}=\\operatorname*{min}\\left(C D_{t},C S_{t}\\right) $$",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2497",
    "question": "Based on the experimental results, which of the following statements accurately describe the impact of matching protocols and market size on pricing behavior?",
    "options": [
      "A) Duopoly prices under fixed matching are consistently higher than under random matching across all phases.",
      "B) Quadropoly prices under fixed matching show no systematic difference from random matching prices in any phase.",
      "C) The counterintuitive number effect predicted by the model is observed in fixed matching quadropolies but not in random matching quadropolies.",
      "D) Average transaction prices are significantly higher in duopolies than in quadropolies under both matching protocols."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Orzen - 2008 - Counterintuitive number effects in experimental oligopolies.md"
    ],
    "question_context": "The experimental results highlight several key findings regarding pricing behavior in duopoly and quadropoly markets under different matching protocols. In duopolies, prices are consistently above the stage-game equilibrium levels under both fixed and random matching, with fixed matching leading to higher prices. Statistical tests confirm significant differences in phases two and three ($p$-values 0.029 and 0.013, respectively). In contrast, quadropoly prices show no systematic differences between matching protocols, with $p$-values of 0.785, 0.997, and 0.941 across the three phases. The counterintuitive number effect, where convenience shoppers are predicted to be hurt by an increase in firms, is only observed under random matching. Under fixed matching, this effect disappears, and in phase three, convenience shoppers pay significantly lower prices in quadropoly markets. Additionally, average transaction prices are significantly higher in duopolies than in quadropolies under both matching protocols, contradicting the equilibrium prediction of invariance.",
    "category": "CORRECT",
    "question_context_used": "The experimental results highlight several key findings regarding pricing behavior in duopoly and quadropoly markets under different matching protocols. In duopolies, prices are consistently above the stage-game equilibrium levels under both fixed and random matching, with fixed matching leading to higher prices. Statistical tests confirm significant differences in phases two and three ($p$-values 0.029 and 0.013, respectively). In contrast, quadropoly prices show no systematic differences between matching protocols, with $p$-values of 0.785, 0.997, and 0.941 across the three phases. The counterintuitive number effect, where convenience shoppers are predicted to be hurt by an increase in firms, is only observed under random matching. Under fixed matching, this effect disappears, and in phase three, convenience shoppers pay significantly lower prices in quadropoly markets. Additionally, average transaction prices are significantly higher in duopolies than in quadropolies under both matching protocols, contradicting the equilibrium prediction of invariance.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3580",
    "question": "Based on Proposition 3 and Lemma 2, which of the following statements are correct regarding the firm's decision to allow or block class actions?",
    "options": [
      "A) When $k_{p}<\\widehat{k}_{p}\\big(\\mu\\big(1-\\gamma\\big)\\big)$, firm profits are increasing in $\\mu$ regardless of class action policy.",
      "B) For $k_{p}\\in\\Big[\\widehat{k}_{p}\\big(\\mu\\big(1-\\gamma\\big)\\big),\\widehat{k}_{p}(\\mu)\\Big]$, social welfare is higher when class actions are allowed, but the firm will block them.",
      "C) When $k_{p}>\\widehat{k}_{p}(\\mu)$, firms and society are indifferent about allowing class actions because litigation does not occur.",
      "D) Social welfare is always increasing in $\\mu$ when $k_{p}/k_{d}<\\theta/(1-\\theta)$, irrespective of class action policy."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Class Actions and Private Antitrust Litigation.md"
    ],
    "question_context": "Lemma 2 establishes that $\\widehat{k}_{p}(\\mu)$ is increasing in $\\mu$. Since $\\mu(1-\\gamma)<\\mu$, we have $\\widehat{k}_{p}\\big(\\mu\\big(1-\\gamma\\big)\\big)<\\widehat{k}_{p}\\big(\\mu\\big)$. For $k_{p}<\\widehat{k}_{p}\\big(\\mu\\big(1-\\gamma\\big)\\big)$, Proposition 1 shows firm profits increase in $\\mu$, and social welfare increases in $\\mu$ if and only if $k_{p}/k_{d}<\\theta/(1-\\theta)$. For $k_{p}\\in\\Big[\\widehat{k}_{p}\\big(\\mu\\big(1-\\gamma\\big)\\big),\\widehat{k}_{p}(\\mu)\\Big]$, social welfare is higher when class actions are allowed ($W(c+(k_{p}/\\theta),c) > W(p^{m}(c),c)$), but the firm blocks them due to profit maximization. When $k_{p}>\\widehat{k}_{p}(\\mu)$, firms and society are indifferent as litigation does not occur.",
    "category": "INCORRECT",
    "question_context_used": "Lemma 2 establishes that $\\widehat{k}_{p}(\\mu)$ is increasing in $\\mu$. Since $\\mu(1-\\gamma)<\\mu$, we have $\\widehat{k}_{p}\\big(\\mu\\big(1-\\gamma\\big)\\big)<\\widehat{k}_{p}\\big(\\mu\\big)$. For $k_{p}<\\widehat{k}_{p}\\big(\\mu\\big(1-\\gamma\\big)\\big)$, Proposition 1 shows firm profits increase in $\\mu$, and social welfare increases in $\\mu$ if and only if $k_{p}/k_{d}<\\theta/(1-\\theta)$. For $k_{p}\\in\\Big[\\widehat{k}_{p}\\big(\\mu\\big(1-\\gamma\\big)\\big),\\widehat{k}_{p}(\\mu)\\Big]$, social welfare is higher when class actions are allowed ($W(c+(k_{p}/\\theta),c) > W(p^{m}(c),c)$), but the firm blocks them due to profit maximization. When $k_{p}>\\widehat{k}_{p}(\\mu)$, firms and society are indifferent as litigation does not occur.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1290",
    "question": "Which of the following statements are true about the 1981 Summer Meeting of the Econometric Society as referenced in Econometrica?",
    "options": [
      "A) The meeting was announced in the September 1980 issue of Econometrica",
      "B) The meeting was held in collaboration with The Econometric Society",
      "C) The meeting details were published in Vol. 48, No. 6 of Econometrica",
      "D) The stable URL provided links directly to the meeting proceedings"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "1981SummerMeeting-1980.md"
    ],
    "question_context": "The announcement for the 1981 Summer Meeting of the Econometric Society was published in Econometrica, Vol. 48, No. 6 (September, 1980), on page 1581. The publication was produced by The Econometric Society, and a stable URL is provided for reference: https://www.jstor.org/stable/1912829. This context establishes the source and timing of the meeting announcement, but does not contain details about the meeting proceedings themselves.",
    "category": "CORRECT",
    "question_context_used": "The announcement for the 1981 Summer Meeting of the Econometric Society was published in Econometrica, Vol. 48, No. 6 (September, 1980), on page 1581. The publication was produced by The Econometric Society, and a stable URL is provided for reference: https://www.jstor.org/stable/1912829. This context establishes the source and timing of the meeting announcement, but does not contain details about the meeting proceedings themselves.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3940",
    "question": "Which of the following statements accurately describe the impact of changes in wives' earnings on family income inequality between 1979 and 1989, based on the counterfactual measures presented?",
    "options": [
      "A) Changes in wives' earnings reduced inequality when evaluated at 1989 levels of other income sources.",
      "B) Changes in wives' earnings increased inequality when correlations with other income sources are attributed to wives' earnings.",
      "C) Changes in wives' earnings had a negligible impact compared to changes in husbands' earnings.",
      "D) The Gini decomposition accurately reflects the impact of wives' earnings on inequality changes."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Cancian-AssessingEffectsWives-1998.md"
    ],
    "question_context": "The text analyzes the impact of changes in wives' earnings on family income inequality between 1979 and 1989 using two counterfactual measures. The first measure compares observed inequality in 1989 to a scenario where the distribution of wives’ earnings had not changed. The second measure compares the 1979 distribution to a scenario where only wives’ earnings changed to 1989 levels. Key findings include: when evaluated at 1989 levels, changes in wives' earnings reduced inequality (CV decreased from 0.636 to 0.619). However, when changes in correlations with other income sources are attributed to wives' earnings, the changes slightly increased inequality (CV increased to 0.566 when 1979 is the base year). The impact of wives' earnings was small relative to husbands' earnings, which accounted for 60%-110% of inequality changes depending on attribution. The Gini decomposition erroneously suggests wives' earnings explain 81% of inequality increase due to confounding factors.",
    "category": "CORRECT",
    "question_context_used": "The text analyzes the impact of changes in wives' earnings on family income inequality between 1979 and 1989 using two counterfactual measures. The first measure compares observed inequality in 1989 to a scenario where the distribution of wives’ earnings had not changed. The second measure compares the 1979 distribution to a scenario where only wives’ earnings changed to 1989 levels. Key findings include: when evaluated at 1989 levels, changes in wives' earnings reduced inequality (CV decreased from 0.636 to 0.619). However, when changes in correlations with other income sources are attributed to wives' earnings, the changes slightly increased inequality (CV increased to 0.566 when 1979 is the base year). The impact of wives' earnings was small relative to husbands' earnings, which accounted for 60%-110% of inequality changes depending on attribution. The Gini decomposition erroneously suggests wives' earnings explain 81% of inequality increase due to confounding factors.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2709",
    "question": "In the Gertler and Kiyotaki (2015) model, which of the following conditions are necessary for a bank run equilibrium to exist?",
    "options": [
      "A) Liquidity mismatch between bank assets and liabilities",
      "B) Endogenous liquidation price of bank assets below promised deposit returns",
      "C) Perfect foresight by households about future productivity shocks",
      "D) Regulatory constraints on bank capital adequacy ratios"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Gertler-BankingLiquidityBank-2015.md"
    ],
    "question_context": "The model emphasizes two key mechanisms: (1) financial accelerator effects where bank capital depletion raises credit costs, and (2) bank runs enabled by liquidity mismatch. The existence of a bank run equilibrium depends on bank balance sheets and an endogenous liquidation price for bank assets. The framework builds on Diamond-Dybvig liquidity mismatch but differs technically by using Cole-Kehoe's approach to self-fulfilling debt crises. The liquidation price satisfies $x_{t+1} < 1$ when runs occur, where $x_{t+1}$ is the total liquidation value of bank assets per unit of promised deposit obligations. The convex household capital management cost $f(K_t^h) = \\frac{\\alpha}{2}(K_t^h)^2$ creates imperfect asset market liquidity, which is central to the run mechanism.",
    "category": "CORRECT",
    "question_context_used": "The model emphasizes two key mechanisms: (1) financial accelerator effects where bank capital depletion raises credit costs, and (2) bank runs enabled by liquidity mismatch. The existence of a bank run equilibrium depends on bank balance sheets and an endogenous liquidation price for bank assets. The framework builds on Diamond-Dybvig liquidity mismatch but differs technically by using Cole-Kehoe's approach to self-fulfilling debt crises. The liquidation price satisfies $x_{t+1} < 1$ when runs occur, where $x_{t+1}$ is the total liquidation value of bank assets per unit of promised deposit obligations. The convex household capital management cost $f(K_t^h) = \\frac{\\alpha}{2}(K_t^h)^2$ creates imperfect asset market liquidity, which is central to the run mechanism.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4018",
    "question": "In the context of dynamic binary data models with fixed effects, which of the following statements are correct regarding maximum likelihood (ML) estimation?",
    "options": [
      "A) ML estimators are generally consistent for the common parameter β as N→∞.",
      "B) ML estimators exhibit significant bias when the distribution function G(.) is normal.",
      "C) A sufficient statistic for α_i exists only when G(.) is logistic.",
      "D) Consistent estimators for β and ρ can be derived without using the likelihood function."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "The-incidental-parameter-problem-since-1948_2000_Journal-of-Econometrics.md"
    ],
    "question_context": "The dynamic binary data model is specified as follows: \n\n$$\n\\begin{array}{r l}&{\\mathcal{E}(y_{i t}|\\boldsymbol{\\alpha}_{i},\\beta,x_{i1},\\ldots,x_{i T},y_{i,t-1},\\ldots,y_{i1},y_{i0}\\bigr\\rangle)=G(\\boldsymbol{\\alpha}_{i}+\\rho y_{i,t-1}+\\beta x_{i t}),}\\ &{\\quad t=1,\\ldots,T,i=1,\\ldots,N}\\end{array}\n$$\n\nHere, the observations for each agent are dependent, and the common parameter is λ=(ρ,β). The distribution function G(.) is known, but ML estimators are generally inconsistent for β as N→∞. For instance, when G(.) is logistic, a sufficient statistic for α_i exists, allowing for consistent estimation of β. However, for other choices of G(.), such as the normal distribution, ML estimators exhibit significant bias. Recent work by Honore and Kyriazidou (1997) proposes consistent estimators for β and ρ without relying solely on the likelihood function.",
    "category": "INCORRECT",
    "question_context_used": "The dynamic binary data model is specified as follows: \n\n$$\n\\begin{array}{r l}&{\\mathcal{E}(y_{i t}|\\boldsymbol{\\alpha}_{i},\\beta,x_{i1},\\ldots,x_{i T},y_{i,t-1},\\ldots,y_{i1},y_{i0}\\bigr\\rangle)=G(\\boldsymbol{\\alpha}_{i}+\\rho y_{i,t-1}+\\beta x_{i t}),}\\ &{\\quad t=1,\\ldots,T,i=1,\\ldots,N}\\end{array}\n$$\n\nHere, the observations for each agent are dependent, and the common parameter is λ=(ρ,β). The distribution function G(.) is known, but ML estimators are generally inconsistent for β as N→∞. For instance, when G(.) is logistic, a sufficient statistic for α_i exists, allowing for consistent estimation of β. However, for other choices of G(.), such as the normal distribution, ML estimators exhibit significant bias. Recent work by Honore and Kyriazidou (1997) proposes consistent estimators for β and ρ without relying solely on the likelihood function.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1015",
    "question": "In the context of symmetric efficiency and reserve price competition, which of the following statements are true regarding buyers' expected payoffs and sellers' profits?",
    "options": [
      "A) Buyers' expected payoffs are identical whether they learn their valuations before or after selecting a seller, in symmetric situations.",
      "B) Sellers' profits are strictly higher with free entry if they can jointly raise reserve prices above zero.",
      "C) Deviations from symmetric outcomes introduce sorting effects when buyers know their valuations beforehand.",
      "D) The optimal reserve price for sellers is always zero to maximize market entry."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Competition-among-Sellers-Who-Offer-Auctions-Inste_1997_Journal-of-Economic-.md"
    ],
    "question_context": "An outcome is symmetrically efficient if it maximizes the expected profit that each seller earns conditional on each buyer getting the same expected payoff that he gets from his best alternative. At the time that buyers are forced to choose between visiting the auction maker and moving to their alternative, they do not yet know their valuations for the commodity. If they expect sellers to offer the reserve price $r$, and expect $k$ buyers to enter for each seller, then their expected payoffs will be (from Theorem 4): \n\n$$\\begin{array}{r l}&{\\underset{j=-\\infty}{\\operatorname*{lim}}\\displaystyle\\int_{r}^{1}v_{j}(x,r,r,k)f(x)d x}\\ &{\\qquad=\\underset{j\\to\\infty}{\\operatorname*{lim}}\\displaystyle\\int_{r}^{1}\\int_{z}^{x}\\mathcal{L}_{J}^{k-1}(s,r,k)d s f(x)d x}\\ &{\\qquad=\\underset{j\\to\\infty}{\\operatorname*{lim}}\\displaystyle\\int_{r}^{1}(1-F(x))z_{j}^{k J-1}(x,r,r,k)d x}\\ &{\\qquad=\\underset{j\\to\\infty}{\\operatorname*{lim}}\\displaystyle\\int_{r}^{1}(1-F(x))\\left[1-\\frac{1-F(x)}{J}\\right]^{k J-1}d x}\\ &{\\qquad=\\displaystyle\\int_{r}^{1}(1-F(x))e^{-k(1-F(x))}d x}\\end{array}$$\n\nAgain referring to Theorem 4, if all sellers set the reserve price $r$ each of them will earn profit:\n\n$$\\begin{array}{c}{{k\\displaystyle\\int_{r}^{1}\\left\\{\\left[x-\\frac{1-F(x)}{f(x)}\\right]e^{-k(1-F(x))}\\right\\}f(x)d x}}\\ {{=k\\displaystyle\\int_{r}^{1}\\left\\{\\nu(x)e^{-k(1-F(x))}\\right\\}f(x)d x}}\\end{array}$$",
    "category": "INCORRECT",
    "question_context_used": "An outcome is symmetrically efficient if it maximizes the expected profit that each seller earns conditional on each buyer getting the same expected payoff that he gets from his best alternative. At the time that buyers are forced to choose between visiting the auction maker and moving to their alternative, they do not yet know their valuations for the commodity. If they expect sellers to offer the reserve price $r$, and expect $k$ buyers to enter for each seller, then their expected payoffs will be (from Theorem 4): \n\n$$\\begin{array}{r l}&{\\underset{j=-\\infty}{\\operatorname*{lim}}\\displaystyle\\int_{r}^{1}v_{j}(x,r,r,k)f(x)d x}\\ &{\\qquad=\\underset{j\\to\\infty}{\\operatorname*{lim}}\\displaystyle\\int_{r}^{1}\\int_{z}^{x}\\mathcal{L}_{J}^{k-1}(s,r,k)d s f(x)d x}\\ &{\\qquad=\\underset{j\\to\\infty}{\\operatorname*{lim}}\\displaystyle\\int_{r}^{1}(1-F(x))z_{j}^{k J-1}(x,r,r,k)d x}\\ &{\\qquad=\\underset{j\\to\\infty}{\\operatorname*{lim}}\\displaystyle\\int_{r}^{1}(1-F(x))\\left[1-\\frac{1-F(x)}{J}\\right]^{k J-1}d x}\\ &{\\qquad=\\displaystyle\\int_{r}^{1}(1-F(x))e^{-k(1-F(x))}d x}\\end{array}$$\n\nAgain referring to Theorem 4, if all sellers set the reserve price $r$ each of them will earn profit:\n\n$$\\begin{array}{c}{{k\\displaystyle\\int_{r}^{1}\\left\\{\\left[x-\\frac{1-F(x)}{f(x)}\\right]e^{-k(1-F(x))}\\right\\}f(x)d x}}\\ {{=k\\displaystyle\\int_{r}^{1}\\left\\{\\nu(x)e^{-k(1-F(x))}\\right\\}f(x)d x}}\\end{array}$$",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1610",
    "question": "In the context of Proposition 7 and 8, which of the following statements are correct regarding the behavior of $X(\\Delta)$ and $\\tilde{W}(\\Delta)$?",
    "options": [
      "A) The optimal $\\Delta^{*}$ that maximizes $\\tilde{W}(\\Delta)$ is strictly decreasing with $n\\alpha$.",
      "B) $\\frac{\\partial^{2}X}{\\partial\\Delta\\partial\\mu}>0$ implies that as $\\mu$ increases, the second term in $X(\\Delta)$ becomes more important than the first term.",
      "C) For $\\sigma_{D}^{2}>0$, $X_{1}(\\Delta)/n$ is of order $n^{-4/3}$ for a fixed $\\Delta>0$ but of order $n^{-2/3}$ when $\\Delta\\rightarrow0$.",
      "D) The term $\\left(1-\\frac{1+d}{n\\alpha-1}\\right)\\frac{\\Delta e^{-r\\Delta}}{1-e^{-r\\Delta}}$ is increasing in $\\Delta$."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "What is the Optimal Trading Frequency in Financial Markets.md"
    ],
    "question_context": "The total cost $X(\\Delta)$ is given by:\n\n$$\nX(\\Delta)=\\frac{\\lambda(1+d)}{2r(n\\alpha-1)}\\sigma_{0}^{2}-\\frac{\\lambda}{2r}\\left(1-\\frac{1+d}{n\\alpha-1}\\right)\\frac{\\Delta e^{-r\\Delta}}{1-e^{-r\\Delta}}\\mu\\sigma_{z}^{2}+\\frac{\\lambda}{2r^{2}}\\mu\\sigma_{z}^{2}.\n$$\n\nMinimizing $X(\\Delta)$ over $\\Delta$ is equivalent to maximizing $\\tilde{W}(\\Delta)$, where:\n\n$$\n\\tilde{W}(\\Delta)\\equiv\\log\\left(1-\\frac{1+d(\\Delta)}{n\\alpha-1}\\right)+\\log\\left(1+\\frac{\\mu\\Delta e^{-r\\Delta}}{1-e^{-r\\Delta}}\\right).\n$$\n\nIt is established that $\\frac{\\partial^{2}X}{\\partial\\Delta\\partial\\mu}>0$ and as $\\mu$ becomes larger, the second term in $X(\\Delta)$ becomes more important than the first term. Additionally, the optimal $\\Delta^{*}$ that maximizes $\\tilde{W}(\\Delta)$ is strictly decreasing with $n\\alpha$. For $\\sigma_{D}^{2}>0$, $X_{1}(\\Delta)/n$ is of order $n^{-4/3}$ for a fixed $\\Delta>0$ but of order $n^{-2/3}$ when $\\Delta\\rightarrow0$.",
    "category": "CORRECT",
    "question_context_used": "The total cost $X(\\Delta)$ is given by:\n\n$$\nX(\\Delta)=\\frac{\\lambda(1+d)}{2r(n\\alpha-1)}\\sigma_{0}^{2}-\\frac{\\lambda}{2r}\\left(1-\\frac{1+d}{n\\alpha-1}\\right)\\frac{\\Delta e^{-r\\Delta}}{1-e^{-r\\Delta}}\\mu\\sigma_{z}^{2}+\\frac{\\lambda}{2r^{2}}\\mu\\sigma_{z}^{2}.\n$$\n\nMinimizing $X(\\Delta)$ over $\\Delta$ is equivalent to maximizing $\\tilde{W}(\\Delta)$, where:\n\n$$\n\\tilde{W}(\\Delta)\\equiv\\log\\left(1-\\frac{1+d(\\Delta)}{n\\alpha-1}\\right)+\\log\\left(1+\\frac{\\mu\\Delta e^{-r\\Delta}}{1-e^{-r\\Delta}}\\right).\n$$\n\nIt is established that $\\frac{\\partial^{2}X}{\\partial\\Delta\\partial\\mu}>0$ and as $\\mu$ becomes larger, the second term in $X(\\Delta)$ becomes more important than the first term. Additionally, the optimal $\\Delta^{*}$ that maximizes $\\tilde{W}(\\Delta)$ is strictly decreasing with $n\\alpha$. For $\\sigma_{D}^{2}>0$, $X_{1}(\\Delta)/n$ is of order $n^{-4/3}$ for a fixed $\\Delta>0$ but of order $n^{-2/3}$ when $\\Delta\\rightarrow0$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1063",
    "question": "In the context of estimating returns to scale and markups using U.S. Census microdata, which of the following statements are correct regarding the biases and assumptions in the model?",
    "options": [
      "A) The standard estimating equation using revenue elasticities tends to underestimate returns to scale due to price markdowns by productive establishments.",
      "B) The assumption that distortions are unpriced in reported labor expenditures is consistent with using labor share data to proxy for the model's labor share inclusive of distortions.",
      "C) The revenue elasticities βK_i and βL_i understate the returns-to-scale parameters because they are quotients of the returns-to-scale parameters and the markup of price over marginal cost.",
      "D) The sum of revenue elasticities βK_i and βL_i is bounded from above by 1, reflecting the model's implication that establishments earn weakly positive profits."
    ],
    "gold_answer": "C, D",
    "source_files": [
      "rest_a_01121.md"
    ],
    "question_context": "The model interprets high revenue productivity in inputs as an indicator of distortions. In a world without distortions, this model suggests that $P_{i e}Y_{i e}/K_{i e}$ and $P_{i e}Y_{i e}/L_{i e}$ would be equalized across firms. If an establishment has a high revenue productivity in a certain input, it would maximize profits by continuing to hire that input until revenue productivity declined and equaled that of the other establishments in the industry. Through the lens of the model, high average revenue productivity in an input suggests that the establishment must have been prevented from hiring more of the input, and hence faces a high distortion.\n\nThe revenue elasticities $\\upbeta_{L_{i}}$ and $\\upbeta_{K_{i}}$ are quotients of the returns-to-scale parameters and the markup of price over marginal cost. Since we expect establishments to price at or above marginal cost, the gross markup exceeds 1. As a result, even when correctly estimated, the revenue elasticities understate the returns-to-scale parameters. In addition, we show in equation (20) that industry profits are the residual share of value added (i.e., the difference between 1 and the sum of the revenue elasticities): $\\frac{\\Pi_{i}}{P_{i}Y_{i}}=1-(\\upbeta_{K_{i}}+\\upbeta_{L_{i}})$. If we expect establishments to earn weakly positive profits, the expression in equation (20) implies that the sum of revenue elasticities is bounded from above by 1.",
    "category": "INCORRECT",
    "question_context_used": "The model interprets high revenue productivity in inputs as an indicator of distortions. In a world without distortions, this model suggests that $P_{i e}Y_{i e}/K_{i e}$ and $P_{i e}Y_{i e}/L_{i e}$ would be equalized across firms. If an establishment has a high revenue productivity in a certain input, it would maximize profits by continuing to hire that input until revenue productivity declined and equaled that of the other establishments in the industry. Through the lens of the model, high average revenue productivity in an input suggests that the establishment must have been prevented from hiring more of the input, and hence faces a high distortion.\n\nThe revenue elasticities $\\upbeta_{L_{i}}$ and $\\upbeta_{K_{i}}$ are quotients of the returns-to-scale parameters and the markup of price over marginal cost. Since we expect establishments to price at or above marginal cost, the gross markup exceeds 1. As a result, even when correctly estimated, the revenue elasticities understate the returns-to-scale parameters. In addition, we show in equation (20) that industry profits are the residual share of value added (i.e., the difference between 1 and the sum of the revenue elasticities): $\\frac{\\Pi_{i}}{P_{i}Y_{i}}=1-(\\upbeta_{K_{i}}+\\upbeta_{L_{i}})$. If we expect establishments to earn weakly positive profits, the expression in equation (20) implies that the sum of revenue elasticities is bounded from above by 1.",
    "gold_answer_parsed": [
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4702",
    "question": "Based on the quasi-differencing models in Table 4, which of the following statements are correct regarding the effect of conscription on post-conscription wages?",
    "options": [
      "A) The estimated wage effects are highly sensitive to the choice of quasi-differencing factor ψₜ.",
      "B) For low-education conscripts, the mid-career wage effect is approximately 4-5% and statistically significant.",
      "C) The quasi-differenced model with ψₜ=2.62 yields the most precise estimates due to orthogonality assumptions.",
      "D) High-education conscripts show no statistically significant wage effects across all specifications."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Can Compulsory Military Service Raise Civilian Wages_ Evidence from the Peacetime Draft in Portugal.md"
    ],
    "question_context": "Key formula: The quasi-differenced model uses wage at indicated year minus ψₜ times wage at age 20/21, where ψₜ ranges from 0 to 2.62. The analysis shows that for low-education conscripts, the mid-career wage effect (2002-2009) is around 4-5% and statistically significant when ψₜ≤1, while high-education conscripts show no significant effects. The stability of estimates across ψₜ values reflects near-zero pre-conscription wage gaps. Under orthogonality between ability and enlistment status, OLS with schooling and initial wage controls provides least-variance estimates (column 3 of Table 4).",
    "category": "CORRECT",
    "question_context_used": "Key formula: The quasi-differenced model uses wage at indicated year minus ψₜ times wage at age 20/21, where ψₜ ranges from 0 to 2.62. The analysis shows that for low-education conscripts, the mid-career wage effect (2002-2009) is around 4-5% and statistically significant when ψₜ≤1, while high-education conscripts show no significant effects. The stability of estimates across ψₜ values reflects near-zero pre-conscription wage gaps. Under orthogonality between ability and enlistment status, OLS with schooling and initial wage controls provides least-variance estimates (column 3 of Table 4).",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-916",
    "question": "In the context of the multi-factor Equilibrium Arbitrage Pricing Theory (EAPT), which of the following conditions are necessary to ensure that the price of idiosyncratic risks is zero in competitive equilibrium?",
    "options": [
      "A) Differentiability of the utility function for at least one consumer (A.9)",
      "B) Existence of a nontrivial linear manifold of portfolios (A.7')",
      "C) Aggregate endowment of idiosyncratic assets is diversifiable (A.10)",
      "D) Consumer preference for diversification with K factors (A.8')"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Milne-ArbitrageDiversificationGeneral-1988.md"
    ],
    "question_context": "Key formula: The price of idiosyncratic risks is zero if the utility function is differentiable (A.9), the aggregate endowment is diversifiable (A.10), and consumers prefer diversification (A.8'). The multi-factor EAPT extends the single-factor model by introducing K factors, with returns structured as $Z_{j}=\\Sigma_{k=1}^{K}\\beta_{j k}\\delta_{k}+\\varepsilon_{j}$. Assumption A.7' ensures the existence of diversified portfolios, while A.8' generalizes the preference for diversification to multiple factors. The equilibrium condition $p_{j}^{e_{*}}=0$ is derived under these assumptions, ensuring that asset prices are weighted sums of factor prices: $p_{j}^{\\ast}=\\Sigma_{k=1}^{K}\\beta_{j k}p_{k}^{\\ast}$.",
    "category": "CORRECT",
    "question_context_used": "Key formula: The price of idiosyncratic risks is zero if the utility function is differentiable (A.9), the aggregate endowment is diversifiable (A.10), and consumers prefer diversification (A.8'). The multi-factor EAPT extends the single-factor model by introducing K factors, with returns structured as $Z_{j}=\\Sigma_{k=1}^{K}\\beta_{j k}\\delta_{k}+\\varepsilon_{j}$. Assumption A.7' ensures the existence of diversified portfolios, while A.8' generalizes the preference for diversification to multiple factors. The equilibrium condition $p_{j}^{e_{*}}=0$ is derived under these assumptions, ensuring that asset prices are weighted sums of factor prices: $p_{j}^{\\ast}=\\Sigma_{k=1}^{K}\\beta_{j k}p_{k}^{\\ast}$.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2914",
    "question": "In the context of the NLARC and FRSDC models, which of the following statements accurately describe their advantages over traditional DCC and RSDC models?",
    "options": [
      "A) They allow correlation dynamics to differ across asset pairs without a significant increase in parameter dimension.",
      "B) They utilize Hadamard exponential transformations to maintain mathematical tractability.",
      "C) They require composite likelihood methods for estimation with large cross-sections of assets.",
      "D) They consistently outperform all other models across every loss function in empirical evaluations."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Nonlinearities-and-regimes-in-conditional-correlations-_2020_Journal-of-Econ.md"
    ],
    "question_context": "The proposed new versions of the DCC and RSDC models enrich the existing versions by allowing the correlation dynamics to be different for different pairs of assets, without requiring a big increase in the parameter dimension. Indeed, in the scalar versions, the NLARC model has one parameter more than cDCC, and the flexible RSDC models have the same number of parameters as the corresponding less flexible versions. This is achieved by parameterizing some matrices as the Hadamard exponential transformations of the lagged correlation matrix. The Hadamard exponential of a matrix has some mathematical properties that might prove useful in other modeling contexts. The empirical results presented in Sections 5 and 6, though contingent on the data sets used, illustrate that the new parameterizations improve their classical versions in terms of forecasting performance. As often the case in such comparisons, no single model is the best across a set of loss functions, although for the data set of Dow Jones stocks, the FRSDC-2λ models often emerge as the best performers.",
    "category": "CORRECT",
    "question_context_used": "The proposed new versions of the DCC and RSDC models enrich the existing versions by allowing the correlation dynamics to be different for different pairs of assets, without requiring a big increase in the parameter dimension. Indeed, in the scalar versions, the NLARC model has one parameter more than cDCC, and the flexible RSDC models have the same number of parameters as the corresponding less flexible versions. This is achieved by parameterizing some matrices as the Hadamard exponential transformations of the lagged correlation matrix. The Hadamard exponential of a matrix has some mathematical properties that might prove useful in other modeling contexts. The empirical results presented in Sections 5 and 6, though contingent on the data sets used, illustrate that the new parameterizations improve their classical versions in terms of forecasting performance. As often the case in such comparisons, no single model is the best across a set of loss functions, although for the data set of Dow Jones stocks, the FRSDC-2λ models often emerge as the best performers.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2562",
    "question": "In the context of intra-household commitment models, which of the following statements are supported by the findings of Mazzocco's research?",
    "options": [
      "A) The full-efficiency household Euler equations are nested within the no-commitment Euler equations.",
      "B) The standard unitary model is a special case of the full-efficiency model but fails under no-commitment.",
      "C) Policy makers can influence household behavior by altering the decision power of individual members only if commitment is present.",
      "D) The rejection of intra-household commitment implies that household decisions must be renegotiated over time."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Household Intertemporal Behaviour A Collective Characterization and a Test of Commitment.md"
    ],
    "question_context": "Key formula: The full-efficiency household Euler equations are nested in the no-commitment Euler equations. This nesting allows for a formal test of intra-household commitment. The paper demonstrates that the standard unitary framework is a special case of the full-efficiency model, but if household members cannot commit, the unitary model fails to characterize intertemporal behavior. The findings strongly reject the hypothesis of intra-household commitment, indicating that policy makers can modify household behavior by changing the decision power of individual members, as decisions must frequently be renegotiated over time. This has significant implications for evaluating social programs like Progresa, which aim to alter household welfare by targeting specific members.",
    "category": "CORRECT",
    "question_context_used": "Key formula: The full-efficiency household Euler equations are nested in the no-commitment Euler equations. This nesting allows for a formal test of intra-household commitment. The paper demonstrates that the standard unitary framework is a special case of the full-efficiency model, but if household members cannot commit, the unitary model fails to characterize intertemporal behavior. The findings strongly reject the hypothesis of intra-household commitment, indicating that policy makers can modify household behavior by changing the decision power of individual members, as decisions must frequently be renegotiated over time. This has significant implications for evaluating social programs like Progresa, which aim to alter household welfare by targeting specific members.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-332",
    "question": "Based on the empirical analysis of regional integration's price effects, which of the following statements are supported by the findings?",
    "options": [
      "A) Regional integration consistently benefits nonmember exporters through increased market access",
      "B) Even without raising external tariffs, regional integration can adversely affect nonmember countries",
      "C) The GATT Article XXIV provisions effectively prevent all negative impacts on nonmembers",
      "D) Price effects of integration are quantitatively significant for nonmember exporters to integrating markets"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Chang-RegionalBlocsAffect-2002.md"
    ],
    "question_context": "We have shown empirically that regional integration does affect the prices of traded goods. This is no surprise from a theoretical point of view, for price effects lie at the very heart of the analysis of international trade policy. It is novel empirically, however, for with the exception of our own earlier work, which used less appropriate data and a much weaker empirical test, there is not a single ex post empirical study of the price effects of integration. We have also shown that the price effects of integration can be quantitatively significant for nonmember exporters supplying an integrating market such as Brazil. An important policy implication is that even if a PTA aims only to 'facilitate trade between the constituent territories and not to raise barriers to the trade of other contracting parties with such territories' (GATT, Article XXIV), the other contracting parties may still be adversely affected.",
    "category": "CORRECT",
    "question_context_used": "We have shown empirically that regional integration does affect the prices of traded goods. This is no surprise from a theoretical point of view, for price effects lie at the very heart of the analysis of international trade policy. It is novel empirically, however, for with the exception of our own earlier work, which used less appropriate data and a much weaker empirical test, there is not a single ex post empirical study of the price effects of integration. We have also shown that the price effects of integration can be quantitatively significant for nonmember exporters supplying an integrating market such as Brazil. An important policy implication is that even if a PTA aims only to 'facilitate trade between the constituent territories and not to raise barriers to the trade of other contracting parties with such territories' (GATT, Article XXIV), the other contracting parties may still be adversely affected.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4672",
    "question": "In the context of the strategic delay model for dental claims, which of the following assumptions are critical for estimating the distribution of delay costs?",
    "options": [
      "A) Dental events arrive independently over time with a rate governed by a Poisson distribution",
      "B) The timing of events is directly observed in the claims data",
      "C) The cost of treating each event is drawn from an empirical distribution that varies with age",
      "D) Individuals can delay any continuous amount of claims without restrictions"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Claim Timing and Ex Post Adverse Selection.md"
    ],
    "question_context": "The model assumes dental events arrive independently over time where the rate of arrival depends on the individual’s 'risk type', λ_i. This conditional independence assumption implies that the number of events received by the individual in year t, n_t, is governed by a Poisson distribution: n_t∼Poiss(λ_i). For each event l, the treatment cost, c_l, is assumed to be an independent draw from the 'cost intensity' distribution, c_l∼G_a_i, representing the empirical distribution of claim costs in the data. In the estimation, this distribution is allowed to vary with age, a_i, in a categorical manner to capture the fact that the types of treatments done by the middle-aged differ from those done by children. The total cost of treating the events that arrive for the individual in year t can be written as: d_t = Σ_{l≤n_t}c_l.",
    "category": "CORRECT",
    "question_context_used": "The model assumes dental events arrive independently over time where the rate of arrival depends on the individual’s 'risk type', λ_i. This conditional independence assumption implies that the number of events received by the individual in year t, n_t, is governed by a Poisson distribution: n_t∼Poiss(λ_i). For each event l, the treatment cost, c_l, is assumed to be an independent draw from the 'cost intensity' distribution, c_l∼G_a_i, representing the empirical distribution of claim costs in the data. In the estimation, this distribution is allowed to vary with age, a_i, in a categorical manner to capture the fact that the types of treatments done by the middle-aged differ from those done by children. The total cost of treating the events that arrive for the individual in year t can be written as: d_t = Σ_{l≤n_t}c_l.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3206",
    "question": "In the context of the two-stage estimation strategy for the human capital labor supply model, which of the following statements are correct regarding the identification and estimation of structural parameters?",
    "options": [
      "A) The identification of all structural parameters in the wage function requires a minimum of three years of longitudinal data.",
      "B) The rental rates on human capital stock, $R_{t}$, are directly observable and enter the model linearly.",
      "C) The translog utility function is selected because it imposes the restrictions of additivity and homotheticity.",
      "D) Standard instrumental variables procedures can be utilized to recover estimates for all structural parameters, $\\alpha$, and for unobserved rental rates, $R_{t}$."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Life-Cycle Labor Supply with Human Capital Accumulation.md"
    ],
    "question_context": "The human capital production function, (5), which is defined above for a general functional form of the production function, $f(\\mathbf{\\xi})$ , is now assumed to be quadratic in its arguments. This specification of the human capital production function is selected to represent the likely concave nature of the life-cycle earnings profile, arising from the combined influence of the depreciation of the stock of human capital and new investment. The specific functional form is \n\n$$\nK_{i,t+1}=\\alpha_{1}K_{i t}+\\alpha_{2}K_{i t}^{2}+\\alpha_{3}K_{i t}h_{i t}+\\alpha_{4}h_{i t}+\\alpha_{5}h_{i t}^{2}+\\zeta_{t}+\\varepsilon_{i t}\n$$\n\nwhere $\\alpha_{1},\\alpha_{2},\\alpha_{3},\\alpha_{4}$ ,and $\\alpha_{5}$ are the structural parameters of the human capital production function, $\\zeta_{t}$ is an exogenous time-specific growth rate of human capital which is common across all agents, and $\\varepsilon_{i t}$ represents the exogenous individual-specific component of human capital growth. The parameters to be estimated in (19) are the $\\alpha_{j}$ , for $j=1,\\ldots,5$ , and the rates of return $R_{t}$ , for $t=1$ $T+1$ . It is also necessary to normalize one of the rental rates to a constant, chosen to be $R_{1}=1.0$ , to fix the units in which human capital (an unobservable) is measured. It is then easy to demonstrate that the identification of all the structural parameters in wage function (19) requires a minimum of three years of longitudinal data. Conditional on the assumption that $\\varepsilon_{i t}$ has an expected value equal to zero and is orthogonal to all observed variables in $f(\\quad),$ standard instrumental variables procedures can be utilized to recover estimates for all structural parameters, $\\alpha=(\\alpha_{1},\\ldots,\\alpha_{s}),$ and for unobserved rental rates, $R_{t},t=1,...,T+1$.",
    "category": "CORRECT",
    "question_context_used": "The human capital production function, (5), which is defined above for a general functional form of the production function, $f(\\mathbf{\\xi})$ , is now assumed to be quadratic in its arguments. This specification of the human capital production function is selected to represent the likely concave nature of the life-cycle earnings profile, arising from the combined influence of the depreciation of the stock of human capital and new investment. The specific functional form is \n\n$$\nK_{i,t+1}=\\alpha_{1}K_{i t}+\\alpha_{2}K_{i t}^{2}+\\alpha_{3}K_{i t}h_{i t}+\\alpha_{4}h_{i t}+\\alpha_{5}h_{i t}^{2}+\\zeta_{t}+\\varepsilon_{i t}\n$$\n\nwhere $\\alpha_{1},\\alpha_{2},\\alpha_{3},\\alpha_{4}$ ,and $\\alpha_{5}$ are the structural parameters of the human capital production function, $\\zeta_{t}$ is an exogenous time-specific growth rate of human capital which is common across all agents, and $\\varepsilon_{i t}$ represents the exogenous individual-specific component of human capital growth. The parameters to be estimated in (19) are the $\\alpha_{j}$ , for $j=1,\\ldots,5$ , and the rates of return $R_{t}$ , for $t=1$ $T+1$ . It is also necessary to normalize one of the rental rates to a constant, chosen to be $R_{1}=1.0$ , to fix the units in which human capital (an unobservable) is measured. It is then easy to demonstrate that the identification of all the structural parameters in wage function (19) requires a minimum of three years of longitudinal data. Conditional on the assumption that $\\varepsilon_{i t}$ has an expected value equal to zero and is orthogonal to all observed variables in $f(\\quad),$ standard instrumental variables procedures can be utilized to recover estimates for all structural parameters, $\\alpha=(\\alpha_{1},\\ldots,\\alpha_{s}),$ and for unobserved rental rates, $R_{t},t=1,...,T+1$.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3278",
    "question": "In A.K. Dixit's 'Theory of Equilibrium Growth', which of the following are explicitly highlighted as foundational elements for understanding neoclassical growth theory?",
    "options": [
      "A) The distinction between identities and equilibrium conditions",
      "B) The role of vintage models in technological progress",
      "C) The connections between asset prices and their rentals",
      "D) The dismissal of Hicks's temporary equilibrium strategy"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Gambling_and_Society._Interdisciplinary_Studies_on_the_Subject_of_Gambling..md"
    ],
    "question_context": "Professor Dixit prefaces his accounts with a careful, clear, opening chapter on method: the distinction between identities and 'equilibrium conditions which give content to...economic theory', and what exactly is meant by steady states and equilibrium growth. There follows an equally careful account of the microeconomic foundations: no substitution, then substitution, the nature of production functions and technological change, of cost and revenue functions, the connections between asset prices and their rentals. The exposition is often terse, the emphasis is on rigour and conciseness. More than elementary mathematics are needed fully to comprehend every proposition. Key formula: The steady-state growth path is characterized by $\\dot{k}/k = s f(k)/k - (n + \\delta)$, where $k$ is capital per worker, $s$ is the savings rate, $f(k)$ is the production function, $n$ is population growth, and $\\delta$ is depreciation.",
    "category": "CORRECT",
    "question_context_used": "Professor Dixit prefaces his accounts with a careful, clear, opening chapter on method: the distinction between identities and 'equilibrium conditions which give content to...economic theory', and what exactly is meant by steady states and equilibrium growth. There follows an equally careful account of the microeconomic foundations: no substitution, then substitution, the nature of production functions and technological change, of cost and revenue functions, the connections between asset prices and their rentals. The exposition is often terse, the emphasis is on rigour and conciseness. More than elementary mathematics are needed fully to comprehend every proposition. Key formula: The steady-state growth path is characterized by $\\dot{k}/k = s f(k)/k - (n + \\delta)$, where $k$ is capital per worker, $s$ is the savings rate, $f(k)$ is the production function, $n$ is population growth, and $\\delta$ is depreciation.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1350",
    "question": "In the context of expectations adjustment to a regime change, which of the following factors are critical for rational agents to correctly anticipate the new equilibrium?",
    "options": [
      "A) Full knowledge of the structural parameters of the new regime",
      "B) Historical data from the previous regime only",
      "C) Bayesian updating of beliefs with new regime observations",
      "D) Instantaneous adjustment without any learning period"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Mankiw-AdjustmentExpectationsChange-1990.md"
    ],
    "question_context": "The paper discusses how economic agents adjust their expectations when there is a change in policy regime. A key challenge is the identification of structural parameters under the new regime, as historical data may not be directly applicable. The authors emphasize that rational agents must employ Bayesian updating mechanisms to incorporate new information, as instantaneous adjustment is unrealistic in most economic contexts. The adjustment process can be modeled as: \\(E_t[x_{t+1}] = \\int f(x_{t+1}|\\theta_{new})p(\\theta_{new}|I_t)d\\theta_{new}\\), where \\(\\theta_{new}\\) represents the structural parameters of the new regime and \\(I_t\\) is the information set at time t.",
    "category": "CORRECT",
    "question_context_used": "The paper discusses how economic agents adjust their expectations when there is a change in policy regime. A key challenge is the identification of structural parameters under the new regime, as historical data may not be directly applicable. The authors emphasize that rational agents must employ Bayesian updating mechanisms to incorporate new information, as instantaneous adjustment is unrealistic in most economic contexts. The adjustment process can be modeled as: \\(E_t[x_{t+1}] = \\int f(x_{t+1}|\\theta_{new})p(\\theta_{new}|I_t)d\\theta_{new}\\), where \\(\\theta_{new}\\) represents the structural parameters of the new regime and \\(I_t\\) is the information set at time t.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2135",
    "question": "According to the Grossman and Hart (1986) model and its extensions discussed in the text, which of the following are key mechanisms through which vertical integration facilitates self-enforcement?",
    "options": [
      "A) Allocation of residual rights to the owner to minimize nonperformance threats",
      "B) Reduction of asset specificity to ease replacement of nonperforming transactors",
      "C) Increased contractual specification to align incentives ex ante",
      "D) Leveraging reputational capital to align expected gains from nonperformance"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Klein-VerticalIntegrationSelfEnforcing-1997.md"
    ],
    "question_context": "Vertical integration can be explained as a particular contractual arrangement that often facilitates self-enforcement (Klein and Murphy, 1996). Grossman and Hart's fundamental insight is that integration minimizes inefficiencies by allocating all contractually unspecified or residual rights to the owner. Employees, even employee managers with authority to make plant-location decisions, do not have the ability to take advantage of such authority because any threat of nonperformance can legally be overridden by the employer. Vertical integration also facilitates self-enforcement by dividing assets among the transactors so that it is easier to replace nonperforming transactors. The key to aligning transactor incentives and controlling inefficiencies is self-enforcement, which is influenced by the reputational capital possessed by transactors. Vertical integration operates by allocating residual rights and thereby shifting transactors’ expected gains from nonperformance so that each transactor's expected gain more closely coincides with its reputational capital.",
    "category": "INCORRECT",
    "question_context_used": "Vertical integration can be explained as a particular contractual arrangement that often facilitates self-enforcement (Klein and Murphy, 1996). Grossman and Hart's fundamental insight is that integration minimizes inefficiencies by allocating all contractually unspecified or residual rights to the owner. Employees, even employee managers with authority to make plant-location decisions, do not have the ability to take advantage of such authority because any threat of nonperformance can legally be overridden by the employer. Vertical integration also facilitates self-enforcement by dividing assets among the transactors so that it is easier to replace nonperforming transactors. The key to aligning transactor incentives and controlling inefficiencies is self-enforcement, which is influenced by the reputational capital possessed by transactors. Vertical integration operates by allocating residual rights and thereby shifting transactors’ expected gains from nonperformance so that each transactor's expected gain more closely coincides with its reputational capital.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-591",
    "question": "In the context of prior modeling for hidden Markov models with switching regression, which of the following statements are correct regarding the choice of prior distributions and their implications?",
    "options": [
      "A) Using an improper prior for the component parameters $\\xi_{k}$ leads to a proper posterior distribution in most cases.",
      "B) The Dirichlet prior on transition probabilities implicitly assumes the number of distinct regimes equals the upper bound $K$.",
      "C) The uniform prior on the number of states $\\kappa$ allows for more flexibility by considering $K$ as an upper bound.",
      "D) The discriminating factor $d(\\xi_{1},...,\\xi_{K})$ is introduced to counter over-fitting by promoting parsimony."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Bayesian-inference-and-state-number-determination-for-hidden-_2004_Journal-o.md"
    ],
    "question_context": "The mixture structure of hidden Markov models usually prevents a fully noninformative prior modeling, since an improper prior (for the $\\xi_{k}$ ’s) yields in most cases to an improper posterior distribution. Various solutions have been proposed to overcome this problem (Diebolt and Robert, 1994; Wasserman, 2000; Chopin, 2001), but for the sake of simplicity, we only consider proper priors in the following. For the switching regression model presented above a most reasonable prior distribution $\\pi(\\xi_{k})$ for $\\xi_{k}=(\\alpha_{k},\\beta_{k},\\sigma_{k}^{2})$ is the natural conjugate prior for regression models \n\n$$ \n(\\alpha_{k},\\beta_{k}|\\sigma_{k}^{2})\\sim\\mathcal{N}_{2}(\\mu_{0},\\sigma_{k}^{2}S_{0}),1/\\sigma_{k}^{2}\\sim\\varGamma(a_{0},b_{0}), \n$$ \n\nwhere $\\mu_{0},S_{0},a_{0}$ , and $b_{0}$ are hyper-parameters. Our state number determination procedure should be strongly aIected by the choice of prior for the transition probabilities. Assigning for instance a Dirichlet prior to the lines of the transition matrix \n\n$$ \n{(p_{k1},...,p_{k K})\\sim\\mathcal{D}(\\alpha_{k1},...,\\alpha_{k K}),} \n$$ \n\nwe implicitly aIect a null prior probability to the event $p_{1{l}}=\\cdot\\cdot\\cdot=p_{K{l}}=0$ , for a given $l$ , and therefore assume that the number of distinct regimes that will 1nally appear (as $t\\to+\\infty$ ) is exactly $K$.",
    "category": "INCORRECT",
    "question_context_used": "The mixture structure of hidden Markov models usually prevents a fully noninformative prior modeling, since an improper prior (for the $\\xi_{k}$ ’s) yields in most cases to an improper posterior distribution. Various solutions have been proposed to overcome this problem (Diebolt and Robert, 1994; Wasserman, 2000; Chopin, 2001), but for the sake of simplicity, we only consider proper priors in the following. For the switching regression model presented above a most reasonable prior distribution $\\pi(\\xi_{k})$ for $\\xi_{k}=(\\alpha_{k},\\beta_{k},\\sigma_{k}^{2})$ is the natural conjugate prior for regression models \n\n$$ \n(\\alpha_{k},\\beta_{k}|\\sigma_{k}^{2})\\sim\\mathcal{N}_{2}(\\mu_{0},\\sigma_{k}^{2}S_{0}),1/\\sigma_{k}^{2}\\sim\\varGamma(a_{0},b_{0}), \n$$ \n\nwhere $\\mu_{0},S_{0},a_{0}$ , and $b_{0}$ are hyper-parameters. Our state number determination procedure should be strongly aIected by the choice of prior for the transition probabilities. Assigning for instance a Dirichlet prior to the lines of the transition matrix \n\n$$ \n{(p_{k1},...,p_{k K})\\sim\\mathcal{D}(\\alpha_{k1},...,\\alpha_{k K}),} \n$$ \n\nwe implicitly aIect a null prior probability to the event $p_{1{l}}=\\cdot\\cdot\\cdot=p_{K{l}}=0$ , for a given $l$ , and therefore assume that the number of distinct regimes that will 1nally appear (as $t\\to+\\infty$ ) is exactly $K$.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1464",
    "question": "Which of the following statements about the editorial structure of Econometrica are correct based on the provided information?",
    "options": [
      "A) The journal has a single editor located at the University of California, San Diego",
      "B) Associate editors are drawn exclusively from U.S. institutions",
      "C) The managing editor's contact information includes both email and physical address",
      "D) Co-editors represent institutions in at least three different countries"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "FrontMatter-2016.md"
    ],
    "question_context": "The Econometrica journal lists Joel Sobel as its editor based at the University of California, San Diego, with contact information including both email (jsobel@econometricsociety.org) and physical address (9500 Gilman Drive, La Jolla). The managing editor Geri Mattson also provides complete contact details including a Baltimore address and email (mattsonpublishingservices@comcast.net). Co-editors are distributed internationally with representatives from Yale University (U.S.), Stanford University (U.S.), HEC Paris (France), Tel Aviv University (Israel), Harvard University (U.S.), New York University (U.S.), and University of Zurich (Switzerland). Associate editors come from various global institutions including University of Queensland (Australia), Tilburg University (Netherlands), and Toulouse School of Economics (France), demonstrating the journal's international scope.",
    "category": "INCORRECT",
    "question_context_used": "The Econometrica journal lists Joel Sobel as its editor based at the University of California, San Diego, with contact information including both email (jsobel@econometricsociety.org) and physical address (9500 Gilman Drive, La Jolla). The managing editor Geri Mattson also provides complete contact details including a Baltimore address and email (mattsonpublishingservices@comcast.net). Co-editors are distributed internationally with representatives from Yale University (U.S.), Stanford University (U.S.), HEC Paris (France), Tel Aviv University (Israel), Harvard University (U.S.), New York University (U.S.), and University of Zurich (Switzerland). Associate editors come from various global institutions including University of Queensland (Australia), Tilburg University (Netherlands), and Toulouse School of Economics (France), demonstrating the journal's international scope.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"C\", \"D\"]\n}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3738",
    "question": "In the context of the nested logit model for automobile demand estimation, which of the following statements are valid regarding the identification assumptions and empirical challenges?",
    "options": [
      "A) The assumption of no serial correlation in error terms is maintained despite potential issues with lagged dependent variables.",
      "B) Prices are assumed exogenous due to expected profit maximization and independence of demand and cost error terms.",
      "C) Model fixed effects can be fully estimated due to the large sample size with multiple observations per model.",
      "D) Unobserved product characteristics are decomposed into segment, origin, and brand-specific components to address data limitations."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Goldberg-ProductDifferentiationOligopoly-1995.md"
    ],
    "question_context": "The main identification assumption is that the variables on the right-hand side of the demand system are uncorrelated with the individual error term in the demand equation. Among the variables included in the demand equation, there are two sets for which the above assumption may seem suspect: those referring to the existing stock of cars, and prices. Dealing with serial correlation in the nested logit model is difficult, but the assumption of no serial correlation is maintained as eliminating stock variables would lead to implausible purchasing patterns. Prices are assumed exogenous based on two considerations: (1) micro data alleviate simultaneity problems by allowing estimation of aggregate components as fixed effects, and (2) expected profit maximization implies prices are uncorrelated with demand errors under independence of cost and demand shocks. The utility function accounts for unobserved product characteristics via:\n\n$${U}_{j}^{h}=\\overline{{V}}_{j}^{h}+\\xi_{j}+\\varepsilon_{j}^{h}.$$\n\nwhere $\\xi_{j}$ is decomposed as:\n\n$$\\xi_{j}=\\xi_{j}^{c}+\\xi_{j}^{o}+\\xi_{j}^{b r}.$$",
    "category": "INCORRECT",
    "question_context_used": "The main identification assumption is that the variables on the right-hand side of the demand system are uncorrelated with the individual error term in the demand equation. Among the variables included in the demand equation, there are two sets for which the above assumption may seem suspect: those referring to the existing stock of cars, and prices. Dealing with serial correlation in the nested logit model is difficult, but the assumption of no serial correlation is maintained as eliminating stock variables would lead to implausible purchasing patterns. Prices are assumed exogenous based on two considerations: (1) micro data alleviate simultaneity problems by allowing estimation of aggregate components as fixed effects, and (2) expected profit maximization implies prices are uncorrelated with demand errors under independence of cost and demand shocks. The utility function accounts for unobserved product characteristics via:\n\n$${U}_{j}^{h}=\\overline{{V}}_{j}^{h}+\\xi_{j}+\\varepsilon_{j}^{h}.$$\n\nwhere $\\xi_{j}$ is decomposed as:\n\n$$\\xi_{j}=\\xi_{j}^{c}+\\xi_{j}^{o}+\\xi_{j}^{b r}.$$",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2094",
    "question": "In the context of forecasting the Niño3.4 time series using the UCDFS method, which of the following statements are supported by the empirical results?",
    "options": [
      "A) The UCDFS method consistently outperforms all univariate models in terms of RMSE for all forecast horizons.",
      "B) The dynamic factor simulation procedure in UCDFS captures additional information from predictor variables that univariate models miss.",
      "C) The linex loss function is used to measure predictive accuracy with a higher penalty for underestimating the Niño3.4 index.",
      "D) The MRPS statistic indicates that UCDFS has the lowest loss for forecasting El Niño and La Niña events across all horizons."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Long-term-forecasting-of-El-Ni-o-events-via-dynamic-f_2020_Journal-of-Econom.md"
    ],
    "question_context": "The UCDFS method employs a three-step forecasting procedure, where ensemble residuals are simulated from the smoothing density $p_{d f m}\\big(\\upsilon_{t}^{u c}|X_{1},\\dots,X_{T}\\big)$. These residuals capture dynamic structures from predictor variables $X_{t}$, as evidenced by their correlations with variables like '100fin2' and '500fin2'. The forecasting performance is evaluated using RMSE and the linex loss function $L(e_{i,h}^{(j)})=\\exp{(\\beta e_{i,h}^{(j)})}-\\beta e_{i,h}^{(j)}-1$, where $\\beta=1$ to penalize underestimation of the Niño3.4 index. The MRPS statistic $R P S_{i,h}^{(j)}=\\frac{1}{2}\\sum_{k=1}^{3}\\left(\\sum_{J=1}^{k}(\\hat{p}_{J,i,h}^{(j)}-e_{J,i,h})\\right)^{2}$ measures the loss from misforecasting El Niño and La Niña events.",
    "category": "INCORRECT",
    "question_context_used": "The UCDFS method employs a three-step forecasting procedure, where ensemble residuals are simulated from the smoothing density $p_{d f m}\\big(\\upsilon_{t}^{u c}|X_{1},\\dots,X_{T}\\big)$. These residuals capture dynamic structures from predictor variables $X_{t}$, as evidenced by their correlations with variables like '100fin2' and '500fin2'. The forecasting performance is evaluated using RMSE and the linex loss function $L(e_{i,h}^{(j)})=\\exp{(\\beta e_{i,h}^{(j)})}-\\beta e_{i,h}^{(j)}-1$, where $\\beta=1$ to penalize underestimation of the Niño3.4 index. The MRPS statistic $R P S_{i,h}^{(j)}=\\frac{1}{2}\\sum_{k=1}^{3}\\left(\\sum_{J=1}^{k}(\\hat{p}_{J,i,h}^{(j)}-e_{J,i,h})\\right)^{2}$ measures the loss from misforecasting El Niño and La Niña events.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4794",
    "question": "In the context of the generalized Leontief (GL) cost function $G(K,\\Delta K,M,Y,p,r)$, which of the following statements are true regarding its properties and implications for the meat packing industry?",
    "options": [
      "A) The GL function allows for cross-effects among all arguments without imposing a priori restrictions on production technology shapes.",
      "B) The GL function nests a constant-returns-to-scale (CRTS) specification by design.",
      "C) The GL function is more likely to satisfy curvature restrictions compared to the translog form, especially when input levels are incorporated.",
      "D) The GL function's flexibility permits nonneutral representation of short- and long-run scale economies through interaction terms involving $K$ and $Y$."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Paul-CostEconomiesMarket-2001.md"
    ],
    "question_context": "The generalized Leontief (GL) cost function $G(K,\\Delta K,M,Y,p,r)$ is chosen for its flexibility in capturing cross-effects among all arguments without imposing a priori restrictions on the shapes of curves representing the production technology. This is crucial for modeling the meat packing industry, where utilization of quasi-fixed capital stocks and adjustment of livestock input levels are sequential decisions. The GL function's form is given by:\n\n$$\n\\begin{array}{r l}&{G(K,\\Delta K,M,Y,p,r)=\\Sigma_{i}\\Sigma_{j}\\upalpha_{i j}p_{i}^{5}p_{j}^{5}+\\Sigma_{i}\\Sigma_{m}\\updelta_{i m}p_{i}s_{m}^{5}}\\ &{\\phantom{\\Sigma_{i}\\Sigma_{j}\\upalpha_{i j}+\\Sigma_{i}\\Sigma_{j}\\upalpha_{i j}p_{i}^{5}+\\Sigma_{i}\\Sigma_{m}\\updelta_{i m}p_{i}s_{m}^{5}}}\\ &{\\phantom{\\Sigma_{i}\\Sigma_{j}\\upalpha_{i j}p_{i}^{5}+\\Sigma_{i}\\Sigma_{i}\\updelta_{i m}p_{i}^{5}}+\\Sigma_{i}p_{i}(\\Sigma_{m}\\upSigma_{n}\\upgamma_{m n}^{5}s_{m}^{5}s_{n}^{5}}\\ &{\\phantom{\\Sigma_{i}\\Sigma_{j}\\upalpha_{i j}p_{i}^{5}+\\Sigma_{i}\\Sigma_{j}\\upalpha_{i k}^{5}}+\\Sigma_{m}\\Sigma_{k}\\upgamma_{m k}s_{m}^{5}x_{k}^{5}}\\ &{\\phantom{\\Sigma_{i}\\Sigma_{j}\\upalpha_{i j}p_{i}^{5}+\\Sigma_{i}\\Sigma_{j}\\upalpha_{i k}^{5}}+\\Sigma_{k}\\Sigma_{l}\\upgamma_{l k}x_{k}^{5}x_{l}^{5}),}\\end{array}\n$$\n\nwhere $s_{m},s_{n}$ denote $\\Delta K,Y_{:}$ and the components of $r$, and $x_{k},x_{l}$ are the $K$ and $M$ levels. The GL function, compared to alternatives like the translog, is more likely to satisfy curvature restrictions, especially when input levels (rather than prices) are incorporated. It also allows for nonneutral representation of short- and long-run scale economies through interaction terms involving $K$ and $Y$. However, it does not nest a CRTS specification by design, though standard errors can test for CRTS implicitly.",
    "category": "CORRECT",
    "question_context_used": "The generalized Leontief (GL) cost function $G(K,\\Delta K,M,Y,p,r)$ is chosen for its flexibility in capturing cross-effects among all arguments without imposing a priori restrictions on the shapes of curves representing the production technology. This is crucial for modeling the meat packing industry, where utilization of quasi-fixed capital stocks and adjustment of livestock input levels are sequential decisions. The GL function's form is given by:\n\n$$\n\\begin{array}{r l}&{G(K,\\Delta K,M,Y,p,r)=\\Sigma_{i}\\Sigma_{j}\\upalpha_{i j}p_{i}^{5}p_{j}^{5}+\\Sigma_{i}\\Sigma_{m}\\updelta_{i m}p_{i}s_{m}^{5}}\\ &{\\phantom{\\Sigma_{i}\\Sigma_{j}\\upalpha_{i j}+\\Sigma_{i}\\Sigma_{j}\\upalpha_{i j}p_{i}^{5}+\\Sigma_{i}\\Sigma_{m}\\updelta_{i m}p_{i}s_{m}^{5}}}\\ &{\\phantom{\\Sigma_{i}\\Sigma_{j}\\upalpha_{i j}p_{i}^{5}+\\Sigma_{i}\\Sigma_{i}\\updelta_{i m}p_{i}^{5}}+\\Sigma_{i}p_{i}(\\Sigma_{m}\\upSigma_{n}\\upgamma_{m n}^{5}s_{m}^{5}s_{n}^{5}}\\ &{\\phantom{\\Sigma_{i}\\Sigma_{j}\\upalpha_{i j}p_{i}^{5}+\\Sigma_{i}\\Sigma_{j}\\upalpha_{i k}^{5}}+\\Sigma_{m}\\Sigma_{k}\\upgamma_{m k}s_{m}^{5}x_{k}^{5}}\\ &{\\phantom{\\Sigma_{i}\\Sigma_{j}\\upalpha_{i j}p_{i}^{5}+\\Sigma_{i}\\Sigma_{j}\\upalpha_{i k}^{5}}+\\Sigma_{k}\\Sigma_{l}\\upgamma_{l k}x_{k}^{5}x_{l}^{5}),}\\end{array}\n$$\n\nwhere $s_{m},s_{n}$ denote $\\Delta K,Y_{:}$ and the components of $r$, and $x_{k},x_{l}$ are the $K$ and $M$ levels. The GL function, compared to alternatives like the translog, is more likely to satisfy curvature restrictions, especially when input levels (rather than prices) are incorporated. It also allows for nonneutral representation of short- and long-run scale economies through interaction terms involving $K$ and $Y$. However, it does not nest a CRTS specification by design, though standard errors can test for CRTS implicitly.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-690",
    "question": "In the context of the UK corporate tax system from 2001 to 2008, which of the following statements accurately describe the tax kinks and their implications?",
    "options": [
      "A) The tax kink at £300,000 was due to a discrete jump in the marginal corporate tax rate from 19% to 32.75% in 2002.",
      "B) The starting rate of 10% on taxable profits between £0 and £10,000 was abolished in 2006, leaving £300,000 as the only tax kink thereafter.",
      "C) The noncorporate distribution rate (NCDR) of 19% in 2004 and 2005 applied as a minimum rate to corporate profits retained within the company.",
      "D) The effective dividend tax rate was 25% for taxpayers with personal income above the basic rate threshold throughout 2001-2008."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Devereux-ElasticityCorporateTaxable-2014.md"
    ],
    "question_context": "Key formula: The effective marginal tax rate on corporate income is given by $\\tilde{t}_{c}=\\theta_{c}+(1-t_{c})\\theta_{d i\\nu}$, where $\\theta_{c}$ is the marginal corporate tax rate and $\\theta_{d i v}$ is the marginal dividend tax rate. The UK corporate tax system from 2001 to 2008 featured two large tax kinks at £10,000 and £300,000 before the abolition of the starting rate in 2006. The tax kink at £300,000 was due to a discrete jump in the marginal corporate tax rate from 19% to 32.75% in 2002. The starting rate of 10% on taxable profits between £0 and £10,000 was abolished in 2006, leaving £300,000 as the only tax kink thereafter. The noncorporate distribution rate (NCDR) of 19% in 2004 and 2005 applied as a minimum rate to corporate profits distributed to noncorporate shareholders. The effective dividend tax rate was 0% for taxpayers with personal income below the basic rate threshold and 25% for those above throughout 2001-2008.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The effective marginal tax rate on corporate income is given by $\\tilde{t}_{c}=\\theta_{c}+(1-t_{c})\\theta_{d i\\nu}$, where $\\theta_{c}$ is the marginal corporate tax rate and $\\theta_{d i v}$ is the marginal dividend tax rate. The UK corporate tax system from 2001 to 2008 featured two large tax kinks at £10,000 and £300,000 before the abolition of the starting rate in 2006. The tax kink at £300,000 was due to a discrete jump in the marginal corporate tax rate from 19% to 32.75% in 2002. The starting rate of 10% on taxable profits between £0 and £10,000 was abolished in 2006, leaving £300,000 as the only tax kink thereafter. The noncorporate distribution rate (NCDR) of 19% in 2004 and 2005 applied as a minimum rate to corporate profits distributed to noncorporate shareholders. The effective dividend tax rate was 0% for taxpayers with personal income below the basic rate threshold and 25% for those above throughout 2001-2008.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2781",
    "question": "In the context of myopic complete information equilibrium (MCI equilibrium), which of the following statements are correct regarding unemployment and market dynamics?",
    "options": [
      "A) Unemployment arises due to agents' pessimistic expectations about trading possibilities.",
      "B) Unemployment equilibrium can occur even at Walrasian equilibrium prices.",
      "C) Agents' offers to the market are always realized, regardless of disequilibrium prices.",
      "D) In strong MCI equilibria, there is no apparent market imbalance to put pressure on prices."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Unemployment Equilibrium with Myopic Complete Information.md"
    ],
    "question_context": "The paper demonstrates that unemployment equilibrium can arise even at Walrasian equilibrium prices when each non-cooperative optimizing agent has full information about the consequences of his actions. Agents may be unable to trade all they wish at going market prices because of insufficient demand for goods they want to sell (or insufficient supply for goods they wish to buy). The authors make the polar assumption that quantities rather than prices adjust when a market does not clear. They argue that for some plausible disequilibrium dynamics, there is no pressure on prices at some equilibria. Such equilibria are called strong MCI equilibria. Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism.",
    "category": "CORRECT",
    "question_context_used": "The paper demonstrates that unemployment equilibrium can arise even at Walrasian equilibrium prices when each non-cooperative optimizing agent has full information about the consequences of his actions. Agents may be unable to trade all they wish at going market prices because of insufficient demand for goods they want to sell (or insufficient supply for goods they wish to buy). The authors make the polar assumption that quantities rather than prices adjust when a market does not clear. They argue that for some plausible disequilibrium dynamics, there is no pressure on prices at some equilibria. Such equilibria are called strong MCI equilibria. Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-986",
    "question": "In the context of volatility forecasting and tail thickness, which of the following statements are supported by the analysis in the text?",
    "options": [
      "A) Incorporating more information into a volatility forecast always produces thinner-tailed normalized errors.",
      "B) The relationship between tail thickness and conditional heteroskedasticity is straightforward and predictable.",
      "C) Under quadratic utility, an investor may prefer a noisier volatility forecast over a less noisy one in certain cases.",
      "D) The kurtosis of normalized errors can decrease when the information in a volatility forecast is reduced."
    ],
    "gold_answer": "C, D",
    "source_files": [
      "Nelson - 1996 - A note on the normalized errors in ARCH and stochastic volatility models.md"
    ],
    "question_context": "The text explores the complex relationship between volatility forecasting, tail thickness, and information quality. Key formula: $\\xi_{t}=\\sigma_{t}z_{t}$, where $\\{\\sigma_{t}\\}_{t=-\\infty,\\infty}$ is a nonnegative random process with $\\pmb{\\sigma}_{t}$ independent of $\\{z_{t+k}\\}_{k=0,\\infty}$ for all t. The analysis reveals that reducing the information in a volatility forecast may actually reduce the kurtosis of the normalized errors, contrary to initial intuition. Additionally, under quadratic utility, an investor might prefer a noisier forecast over a less noisy one in specific scenarios, as illustrated by the expected utility framework: $E\\big[U\\big\\vert\\hat{\\sigma}^{2},\\sigma^{2}\\big]=k_{1}+k_{2}\\cdot\\{(\\theta+\\hat{\\sigma}^{2})^{-1}-\\frac{1}{2}(\\theta+\\hat{\\sigma}^{2})^{-2}(\\theta+\\sigma^{2})\\}$. The text emphasizes the nonlinear and counterintuitive nature of these relationships, highlighting the limitations of global concavity in such problems.",
    "category": "CORRECT",
    "question_context_used": "The text explores the complex relationship between volatility forecasting, tail thickness, and information quality. Key formula: $\\xi_{t}=\\sigma_{t}z_{t}$, where $\\{\\sigma_{t}\\}_{t=-\\infty,\\infty}$ is a nonnegative random process with $\\pmb{\\sigma}_{t}$ independent of $\\{z_{t+k}\\}_{k=0,\\infty}$ for all t. The analysis reveals that reducing the information in a volatility forecast may actually reduce the kurtosis of the normalized errors, contrary to initial intuition. Additionally, under quadratic utility, an investor might prefer a noisier forecast over a less noisy one in specific scenarios, as illustrated by the expected utility framework: $E\\big[U\\big\\vert\\hat{\\sigma}^{2},\\sigma^{2}\\big]=k_{1}+k_{2}\\cdot\\{(\\theta+\\hat{\\sigma}^{2})^{-1}-\\frac{1}{2}(\\theta+\\hat{\\sigma}^{2})^{-2}(\\theta+\\sigma^{2})\\}$. The text emphasizes the nonlinear and counterintuitive nature of these relationships, highlighting the limitations of global concavity in such problems.",
    "gold_answer_parsed": [
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2548",
    "question": "In the computation of ${\\mathcal{H}}_{4}$ for the case where weak instruments $W$ exactly identify the coefficient, which of the following steps are essential for the algorithm?",
    "options": [
      "A) Compute the 2SLS estimator $\\widehat{\\beta}_{z}$ using instruments $Z=[W,S]$",
      "B) Regress the IV residual $\\widehat{\\varepsilon}_{z}$ on $Z=[S,W]$ to obtain $\\widetilde{\\varepsilon}_{z}$",
      "C) Calculate $\\widetilde{\\sigma}_{\\varepsilon,z}^{2}=\\frac{1}{n}\\widetilde{\\varepsilon}_{z}^{\\prime}\\widetilde{\\varepsilon}_{z}$",
      "D) Use the standard variance estimator $\\widehat{V}_{w}$ for $\\widehat{\\beta}_{w}$"
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "The-Hausman-test-and-weak-instruments_2011_Journal-of-Econometrics.md"
    ],
    "question_context": "The algorithm for computing ${\\mathcal{H}}_{4}$ in the special case when weak instruments $W$ exactly identify the coefficient involves several key steps. First, compute the 2SLS estimator $\\widehat{\\beta}_{z}$ using instruments $Z=[W,S]$, with $\\widehat{V}_{z}=\\widehat{\\sigma}_{\\varepsilon,z}^{2}(Y_{2}^{\\prime}P_{Z}Y_{2})^{-1}$ as the standard variance estimator. The IV residual $\\widehat{\\varepsilon}_{z}=y_{1}-Y_{2}\\widehat{\\beta}_{z}$ is then regressed on $Z=[S,W]$ to obtain $\\widetilde{\\varepsilon}_{z}=M_{Z}\\widehat{\\varepsilon}_{z}$. The variance estimator $\\widetilde{\\sigma}_{\\varepsilon,z}^{2}=\\frac{1}{n}\\widetilde{\\varepsilon}_{z}^{\\prime}\\widetilde{\\varepsilon}_{z}$ is calculated. Additionally, the 2SLS estimate $\\widehat{\\beta}_{w}$ is computed using instruments $W$, with $\\widehat{V}_{w}\\doteq\\widehat{\\sigma}_{\\varepsilon,w}^{2}(Y_{2}^{\\prime}P_{W}Y_{2})^{-1}$ as the standard variance estimator. Finally, ${\\mathcal{H}}_{4}$ is calculated using these components.",
    "category": "CORRECT",
    "question_context_used": "The algorithm for computing ${\\mathcal{H}}_{4}$ in the special case when weak instruments $W$ exactly identify the coefficient involves several key steps. First, compute the 2SLS estimator $\\widehat{\\beta}_{z}$ using instruments $Z=[W,S]$, with $\\widehat{V}_{z}=\\widehat{\\sigma}_{\\varepsilon,z}^{2}(Y_{2}^{\\prime}P_{Z}Y_{2})^{-1}$ as the standard variance estimator. The IV residual $\\widehat{\\varepsilon}_{z}=y_{1}-Y_{2}\\widehat{\\beta}_{z}$ is then regressed on $Z=[S,W]$ to obtain $\\widetilde{\\varepsilon}_{z}=M_{Z}\\widehat{\\varepsilon}_{z}$. The variance estimator $\\widetilde{\\sigma}_{\\varepsilon,z}^{2}=\\frac{1}{n}\\widetilde{\\varepsilon}_{z}^{\\prime}\\widetilde{\\varepsilon}_{z}$ is calculated. Additionally, the 2SLS estimate $\\widehat{\\beta}_{w}$ is computed using instruments $W$, with $\\widehat{V}_{w}\\doteq\\widehat{\\sigma}_{\\varepsilon,w}^{2}(Y_{2}^{\\prime}P_{W}Y_{2})^{-1}$ as the standard variance estimator. Finally, ${\\mathcal{H}}_{4}$ is calculated using these components.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1799",
    "question": "Under the margin condition in PRESCIENCE framework, which of the following conditions are necessary for the rate of convergence in (3.21) to double that in (3.8)?",
    "options": [
      "A) $h$ is fixed and $\\vartheta=1$",
      "B) $b^{*} \\in \\mathcal{B}_{q}$ and $h$ is fixed",
      "C) $\\vartheta > 1$ and $h$ approaches zero rapidly",
      "D) $b^{*} \\notin \\mathcal{B}_{q}$ and $h$ is fixed"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Best-subset-binary-prediction_2018_Journal-of-Econometrics.md"
    ],
    "question_context": "The margin condition is given by: \n\n$$\\widetilde{S}\\left(b^{*}\\right)-\\widetilde{S}\\left(b\\right)\\geq h^{\\vartheta}\\left\\Vert b^{*}-b\\right\\Vert_{1}^{\\vartheta}.$$\n\nTheorem 2 establishes that under this condition, the expected difference $E[U_n]$ is bounded by:\n\n$$E\\left[U_{n}\\right]\\leq\\left[\\widetilde{S}\\left(b^{*}\\right)-\\operatorname*{sup}_{b\\in\\mathcal{B}_{q}}\\widetilde{S}\\left(b\\right)\\right]+K^{\\prime}\\left(\\frac{K^{2}\\rho_{n}}{n h}\\right)^{\\vartheta/(2\\vartheta-1)}.$$\n\nThe rate of convergence in (3.21) doubles that in (3.8) when $h$ is fixed and $\\vartheta=1$. This occurs when $b^{*} \\in \\mathcal{B}_{q}$, eliminating the bias term and allowing the margin condition to fully influence the convergence rate.",
    "category": "CORRECT",
    "question_context_used": "The margin condition is given by: \n\n$$\\widetilde{S}\\left(b^{*}\\right)-\\widetilde{S}\\left(b\\right)\\geq h^{\\vartheta}\\left\\Vert b^{*}-b\\right\\Vert_{1}^{\\vartheta}.$$\n\nTheorem 2 establishes that under this condition, the expected difference $E[U_n]$ is bounded by:\n\n$$E\\left[U_{n}\\right]\\leq\\left[\\widetilde{S}\\left(b^{*}\\right)-\\operatorname*{sup}_{b\\in\\mathcal{B}_{q}}\\widetilde{S}\\left(b\\right)\\right]+K^{\\prime}\\left(\\frac{K^{2}\\rho_{n}}{n h}\\right)^{\\vartheta/(2\\vartheta-1)}.$$\n\nThe rate of convergence in (3.21) doubles that in (3.8) when $h$ is fixed and $\\vartheta=1$. This occurs when $b^{*} \\in \\mathcal{B}_{q}$, eliminating the bias term and allowing the margin condition to fully influence the convergence rate.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4116",
    "question": "In the context of the model's distributional assumptions and calibration, which of the following statements are correct regarding the treatment of endowments and ability signals?",
    "options": [
      "A) The marginal distributions of assets, ability signals, and college costs are assumed to follow truncated normal distributions to ensure non-negativity.",
      "B) The ability grid approximates a normal distribution with each grid point having equal probability, and the conditional distribution of abilities given signals is modeled as a discrete approximation of a joint normal distribution.",
      "C) The correlation coefficients between endowments are fixed a priori and not calibrated to match data moments.",
      "D) The HS GPA is modeled as a direct measure of the agent's true ability without any noise or measurement error."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "THE RETURN TO COLLEGE SELECTION AND DROPOUT RISK.md"
    ],
    "question_context": "The model assumes substantial heterogeneity in assets, ability signals, and college costs, modeled parsimoniously with $J=200$ types. The marginal distributions are given by $\\bar{q}_{j}\\sim N(\\mu_{q},\\sigma_{q}^{2}), \\hat{z}_{j}\\sim\\operatorname*{max}\\{0,N(\\mu_{z},\\sigma_{z}^{2})\\}, \\hat{k}_{j}\\sim\\operatorname*{max}\\{0,N(\\mu_{k},\\sigma_{k}^{2})\\}$, and $m\\sim N(0,1)$. Negative draws of $\\hat{z}_{j}$ and $\\hat{k}_{j}$ are truncated to 0 to ensure non-negativity. The ability grid $\\pmb{\\hat{a}}_{i}$ approximates a normal distribution with mean $\\bar{a}$ and variance 1, with each of the $N_{a}=9$ grid points having equal probability. The conditional distribution of abilities given signals is modeled as $\\pmb{a}=\\bar{\\pmb{a}}+\\frac{\\alpha_{a,m}m+\\varepsilon_{a}}{(\\alpha_{a,m}^{2}+1)^{1/2}}$, where $\\varepsilon_{a}\\sim N(0,1)$. The HS GPA is modeled as $GPA=\\frac{\\alpha_{GPA,m}m+\\varepsilon_{GPA}}{(\\alpha_{GPA,m}^{2}+1)^{1/2}}$, with $\\varepsilon_{GPA}\\sim N(0,1)$, reflecting noise in the measurement of ability signals.",
    "category": "CORRECT",
    "question_context_used": "The model assumes substantial heterogeneity in assets, ability signals, and college costs, modeled parsimoniously with $J=200$ types. The marginal distributions are given by $\\bar{q}_{j}\\sim N(\\mu_{q},\\sigma_{q}^{2}), \\hat{z}_{j}\\sim\\operatorname*{max}\\{0,N(\\mu_{z},\\sigma_{z}^{2})\\}, \\hat{k}_{j}\\sim\\operatorname*{max}\\{0,N(\\mu_{k},\\sigma_{k}^{2})\\}$, and $m\\sim N(0,1)$. Negative draws of $\\hat{z}_{j}$ and $\\hat{k}_{j}$ are truncated to 0 to ensure non-negativity. The ability grid $\\pmb{\\hat{a}}_{i}$ approximates a normal distribution with mean $\\bar{a}$ and variance 1, with each of the $N_{a}=9$ grid points having equal probability. The conditional distribution of abilities given signals is modeled as $\\pmb{a}=\\bar{\\pmb{a}}+\\frac{\\alpha_{a,m}m+\\varepsilon_{a}}{(\\alpha_{a,m}^{2}+1)^{1/2}}$, where $\\varepsilon_{a}\\sim N(0,1)$. The HS GPA is modeled as $GPA=\\frac{\\alpha_{GPA,m}m+\\varepsilon_{GPA}}{(\\alpha_{GPA,m}^{2}+1)^{1/2}}$, with $\\varepsilon_{GPA}\\sim N(0,1)$, reflecting noise in the measurement of ability signals.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1121",
    "question": "In the context of the Spatial Theory of Voting, which of the following conditions are necessary for the existence of a dominant point in committee voting?",
    "options": [
      "A) Separable preferences among all committee members",
      "B) Non-separable preferences and sequential voting on issues",
      "C) Symmetric preferences and a single voting dimension",
      "D) Median ideal points on all issues when voting occurs simultaneously"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Spatial_Theory_of_Voting_An_Introduction.md"
    ],
    "question_context": "The spatial theory of voting, as discussed in the book, extends Hotelling's median voter results to committee voting and mass elections. A key concept is the dominant point, which is a proposal that cannot be beaten in a majority contest. For a dominant point to exist in committee voting, the preferences of the committee members must be separable, and the outcome on each issue must be the median ideal point on that issue. However, when voting takes place on multiple issues simultaneously, a dominant point may not exist unless it is a median in all directions. The analysis also highlights that with non-separable preferences, the order of voting on issues affects the outcome. The book suggests that these results could benefit from a more rigorous game-theoretic approach, particularly in analyzing the robustness of symmetric preference assumptions.",
    "category": "INCORRECT",
    "question_context_used": "The spatial theory of voting, as discussed in the book, extends Hotelling's median voter results to committee voting and mass elections. A key concept is the dominant point, which is a proposal that cannot be beaten in a majority contest. For a dominant point to exist in committee voting, the preferences of the committee members must be separable, and the outcome on each issue must be the median ideal point on that issue. However, when voting takes place on multiple issues simultaneously, a dominant point may not exist unless it is a median in all directions. The analysis also highlights that with non-separable preferences, the order of voting on issues affects the outcome. The book suggests that these results could benefit from a more rigorous game-theoretic approach, particularly in analyzing the robustness of symmetric preference assumptions.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2943",
    "question": "Which of the following statements are true regarding the conditions (2.7) and (2.8) in the context of Second-Degree Stochastic Dominance (SSD)?",
    "options": [
      "A) Condition (2.7) implies that the ratio of integrated distribution functions, φG(t)/φF(t), is non-increasing.",
      "B) Condition (2.8) implies that (φG(t) - φF(t))/|t - μ| is non-decreasing on (-∞, μ) and non-increasing on (μ, ∞).",
      "C) Both conditions (2.7) and (2.8) imply SSD, but neither implies the other.",
      "D) Only condition (2.8) implies SSD, while condition (2.7) does not."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Mean-preserving Portfolio Dominance.md"
    ],
    "question_context": "The text discusses the relationship between conditions (2.7) and (2.8) and Second-Degree Stochastic Dominance (SSD). It is shown that condition (2.7) is satisfied if and only if φG(t)/φF(t) is non-increasing. For distributions with equal means, condition (2.8) is satisfied if and only if (φG(t) - φF(t))/|t - μ| is non-decreasing on (-∞, μ) and non-increasing on (μ, ∞). Both conditions imply SSD, but neither implies the other. The text provides examples of distributions that satisfy one condition but not the other, demonstrating their independence. Key formula: φH(t) = ∫-∞t H(x)dx, where H is the integrated distribution function of a random variable.",
    "category": "CORRECT",
    "question_context_used": "The text discusses the relationship between conditions (2.7) and (2.8) and Second-Degree Stochastic Dominance (SSD). It is shown that condition (2.7) is satisfied if and only if φG(t)/φF(t) is non-increasing. For distributions with equal means, condition (2.8) is satisfied if and only if (φG(t) - φF(t))/|t - μ| is non-decreasing on (-∞, μ) and non-increasing on (μ, ∞). Both conditions imply SSD, but neither implies the other. The text provides examples of distributions that satisfy one condition but not the other, demonstrating their independence. Key formula: φH(t) = ∫-∞t H(x)dx, where H is the integrated distribution function of a random variable.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-900",
    "question": "In the context of solving differential equation (2) for the case when $r\\neq0$, which of the following conditions must hold for the existence of linear special solutions?",
    "options": [
      "A) $\\Delta > 0$ and $w^{2} + k_{1}w - r = 0$ has real roots",
      "B) $\\Delta = 0$ and $w^{2} + k_{1}w - r = 0$ has a double root",
      "C) $\\Delta < 0$ and $w^{2} + k_{1}w - r = 0$ has complex roots",
      "D) $\\Delta \\geq 0$ and $w^{2} + k_{1}w - r = 0$ has at least one real root"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Linear-Riley-equilibria-in-quadratic-signaling-_2023_Journal-of-Economic-The.md"
    ],
    "question_context": "Analysis for $r\\neq0$: Define $\\phi\\triangleq\\theta-\\theta_{0}$. DE (2) is \n\n$$\nS(\\phi)\\left[\\frac{d S(\\phi)}{d\\phi} + k_{1}\\right] = r\\phi.\n$$\n\nIt is centrosymmetric around $(\\phi,S)=(0,0)$. Let $w(\\phi)\\triangleq\\frac{S(\\phi)}{\\phi}$. DE (5) reduces to \n\n$$\n\\frac{w(\\phi)}{w^{2}(\\phi) + k_{1}w(\\phi) - r}\\frac{d w(\\phi)}{d\\phi} = -\\frac{1}{\\phi}.\n$$\n\nThe form of general solutions depends on $\\Delta$. A linear solution exists if and only if $\\Delta\\geq0$. If $\\Delta>0$, there exist two linear special solutions, $S=w_{1}\\phi$ and $S=w_{2}\\phi$, where $w_{1}$ and $w_{2}$ denote solutions to the equation $w^{2} + k_{1}w - r = 0$. Without loss of generality, let us assume $|w_{1}|\\leq|w_{2}|$. \n\n$$\nw_{1} + w_{2} = -k_{1}, \\quad w_{1} \\times w_{2} = -r.\n$$\n\nIf $\\Delta=0$, there is a unique linear special solution $S(\\phi)=w_{1}\\phi$, where $w_{1}=w_{2}=-\\frac{k_{1}}{2}$ is the solution for equation $w^{2} + k_{1}w - r = 0$.",
    "category": "INCORRECT",
    "question_context_used": "Analysis for $r\\neq0$: Define $\\phi\\triangleq\\theta-\\theta_{0}$. DE (2) is \n\n$$\nS(\\phi)\\left[\\frac{d S(\\phi)}{d\\phi} + k_{1}\\right] = r\\phi.\n$$\n\nIt is centrosymmetric around $(\\phi,S)=(0,0)$. Let $w(\\phi)\\triangleq\\frac{S(\\phi)}{\\phi}$. DE (5) reduces to \n\n$$\n\\frac{w(\\phi)}{w^{2}(\\phi) + k_{1}w(\\phi) - r}\\frac{d w(\\phi)}{d\\phi} = -\\frac{1}{\\phi}.\n$$\n\nThe form of general solutions depends on $\\Delta$. A linear solution exists if and only if $\\Delta\\geq0$. If $\\Delta>0$, there exist two linear special solutions, $S=w_{1}\\phi$ and $S=w_{2}\\phi$, where $w_{1}$ and $w_{2}$ denote solutions to the equation $w^{2} + k_{1}w - r = 0$. Without loss of generality, let us assume $|w_{1}|\\leq|w_{2}|$. \n\n$$\nw_{1} + w_{2} = -k_{1}, \\quad w_{1} \\times w_{2} = -r.\n$$\n\nIf $\\Delta=0$, there is a unique linear special solution $S(\\phi)=w_{1}\\phi$, where $w_{1}=w_{2}=-\\frac{k_{1}}{2}$ is the solution for equation $w^{2} + k_{1}w - r = 0$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"D\"]}",
      "model_answer": [
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3108",
    "question": "In the context of firm heterogeneity and offshoring equilibrium, which of the following statements are supported by the model's implications?",
    "options": [
      "A) An increase in firm heterogeneity reduces the likelihood of a no-offshoring equilibrium.",
      "B) Homogeneous firms are more likely to exhibit multiple equilibria in offshoring decisions compared to heterogeneous firms.",
      "C) FDI by high productivity firms can facilitate outsourcing by less productive firms through external economies.",
      "D) The net benefit from outsourcing follows a linear trend as the fraction of offshoring firms increases."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Temporary-shocks-and-offshoring--The-role-of-externa_2008_Journal-of-Develop.md"
    ],
    "question_context": "The model assumes a uniform distribution of firm productivities, where the nth firm's benefit from outsourcing is given by the complex formula involving productivity parameters, wages, and integration terms. Numerical simulations using this uniform distribution show an inverted U shape for the net benefit from outsourcing, indicating that an increase in firm heterogeneity makes a no-offshoring equilibrium less likely. This is because high productivity firms may find it profitable to offshore even when others do not, triggering a cascade effect. The model also explores the complementarity between FDI and outsourcing, showing that FDI by some firms can facilitate outsourcing by others through external economies, as seen in real-world examples like the development of India's BPO industry following FDI by multinationals.",
    "category": "INCORRECT",
    "question_context_used": "The model assumes a uniform distribution of firm productivities, where the nth firm's benefit from outsourcing is given by the complex formula involving productivity parameters, wages, and integration terms. Numerical simulations using this uniform distribution show an inverted U shape for the net benefit from outsourcing, indicating that an increase in firm heterogeneity makes a no-offshoring equilibrium less likely. This is because high productivity firms may find it profitable to offshore even when others do not, triggering a cascade effect. The model also explores the complementarity between FDI and outsourcing, showing that FDI by some firms can facilitate outsourcing by others through external economies, as seen in real-world examples like the development of India's BPO industry following FDI by multinationals.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3369",
    "question": "Based on the empirical analysis of famine effects in Finland (1866–1868), which of the following outcomes were significantly influenced by famine exposure?",
    "options": [
      "A) Increase in income Gini coefficient by 0.020 points",
      "B) Reduction in daily wages by 0.398 Finnish Marks",
      "C) Rise in tenant farm shares by 5.4%–6.5%",
      "D) Decrease in land Gini coefficient by 0.021 points"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Meriläinen 等 - 2023 - Famine, Inequality, and Conflict.md"
    ],
    "question_context": "The empirical strategy employs the regression model: $$y_{m}=\\alpha+\\beta Famine~exposure_{m}+X_{m}^{\\prime}\\gamma+\\varepsilon_{m},$$ where $y_{m}$ represents inequality or labor coercion outcomes. Key findings include: (1) A one standard deviation increase in famine severity correlates with a 0.01 rise in income/land inequality (Panel A). (2) Crop failure increased income Gini by 0.020 points and land Gini by 0.021 points (Panel B). (3) Tenant farm shares rose by 5.4%–6.5% in crop-failure municipalities. The daily wage reduction of 0.078 Finnish Marks was statistically insignificant when scaled by land prices ($p=0.065$).",
    "category": "INCORRECT",
    "question_context_used": "The empirical strategy employs the regression model: $$y_{m}=\\alpha+\\beta Famine~exposure_{m}+X_{m}^{\\prime}\\gamma+\\varepsilon_{m},$$ where $y_{m}$ represents inequality or labor coercion outcomes. Key findings include: (1) A one standard deviation increase in famine severity correlates with a 0.01 rise in income/land inequality (Panel A). (2) Crop failure increased income Gini by 0.020 points and land Gini by 0.021 points (Panel B). (3) Tenant farm shares rose by 5.4%–6.5% in crop-failure municipalities. The daily wage reduction of 0.078 Finnish Marks was statistically insignificant when scaled by land prices ($p=0.065$).",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-101",
    "question": "In nonatomic congestion games with convexity conditions, which of the following statements are true regarding the Harsanyi TU value and equilibrium payoffs?",
    "options": [
      "A) The Harsanyi TU value always coincides with the equilibrium payoff distribution for logarithmic cost functions.",
      "B) The convexity condition requires cost functions to be less convex than the cubic function.",
      "C) The core of the coalitional game is nonempty only if the game is additive.",
      "D) The Harsanyi TU value can be computed without considering the worth of coalitions with biased type ratios."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Social-optimality-and-cooperation-in-nonatomic-con_2004_Journal-of-Economic-.md"
    ],
    "question_context": "The convexity condition in nonatomic congestion games is crucial for ensuring that the worth of coalitions is well-defined. This condition requires that (i) marginal social costs of congestion are increasing, and (ii) for each facility, the function $c_{j}(\\sqrt[3]{x})$ is concave. Proposition 6 states that this is equivalent to cost functions being less convex than the cubic function $x^3$. Theorem 2 establishes that under the convexity condition, the coalitional form $\\upsilon$ of the game is well-defined and belongs to the space $pNA$, allowing the computation of the Harsanyi TU value via formula (7).\n\nProposition 7 highlights a special case where the Harsanyi TU value coincides with the equilibrium payoff distribution: when cost functions are logarithmic. This is because, for such functions, the marginal contributions of players to coalition worths along the diagonal (representing unbiased coalitions) equal their equilibrium payoffs. The core, however, is generally empty unless the game is additive, as the coalitional game is fixed-sum and non-additive in most cases.",
    "category": "INCORRECT",
    "question_context_used": "The convexity condition in nonatomic congestion games is crucial for ensuring that the worth of coalitions is well-defined. This condition requires that (i) marginal social costs of congestion are increasing, and (ii) for each facility, the function $c_{j}(\\sqrt[3]{x})$ is concave. Proposition 6 states that this is equivalent to cost functions being less convex than the cubic function $x^3$. Theorem 2 establishes that under the convexity condition, the coalitional form $\\upsilon$ of the game is well-defined and belongs to the space $pNA$, allowing the computation of the Harsanyi TU value via formula (7).\n\nProposition 7 highlights a special case where the Harsanyi TU value coincides with the equilibrium payoff distribution: when cost functions are logarithmic. This is because, for such functions, the marginal contributions of players to coalition worths along the diagonal (representing unbiased coalitions) equal their equilibrium payoffs. The core, however, is generally empty unless the game is additive, as the coalitional game is fixed-sum and non-additive in most cases.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3333",
    "question": "In the context of Markov-switching models, which of the following conditions lead to the rejection frequency of the hypothesis $d=0$ increasing with sample size $T$?",
    "options": [
      "A) Both $p_{00}$ and $p_{11}$ are well away from unity (e.g., $p_{00}=p_{11}=0.95$)",
      "B) Both $p_{00}$ and $p_{11}$ are large (e.g., $p_{00}=p_{11}=0.999$)",
      "C) The Markov-switching model is $I(0)$ for any fixed $p_{00}$ and $p_{11}$",
      "D) The sample size $T$ is less than 5000"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Long-memory-and-regime-switching_2001_Journal-of-Econometrics.md"
    ],
    "question_context": "The Markov-switching model is given by the equation:\n\n$$\ny_{t}=\\mu_{s_{t}}+\\varepsilon_{t},\n$$\n\nwhere $\\varepsilon_{t}\\overset{\\mathrm{iid}}{\\sim}\\ensuremath{\\mathrm{N}}(0,\\sigma^{2})$, and $s_{t}$ and $\\varepsilon_{\\tau}$ are independent for all $t$ and $\\tau$. In the Monte Carlo analysis, we explore $p_{00}\\in\\{0.95,0.99,0.999\\}$, $p_{11}\\in$ $\\{0.95,0.99,0.999\\}$, and $T\\in\\{100,200,300,400,500,1000,1500,\\ldots,5000\\}$. When both $p_{00}$ and $p_{11}$ are large, such as when $p_{00}=p_{11}=0.999$, the rejection frequency is increasing in $T$. This would appear inconsistent with the fact that the Markov-switching model is $I(0)$ for any fixed $p_{00}$ and $p_{11}$, but it is not, as the dependence of rejection frequency on $T$ is not monotonic. If we included $T>5000$ in the design, we would eventually see the rejection frequency decrease.",
    "category": "INCORRECT",
    "question_context_used": "The Markov-switching model is given by the equation:\n\n$$\ny_{t}=\\mu_{s_{t}}+\\varepsilon_{t},\n$$\n\nwhere $\\varepsilon_{t}\\overset{\\mathrm{iid}}{\\sim}\\ensuremath{\\mathrm{N}}(0,\\sigma^{2})$, and $s_{t}$ and $\\varepsilon_{\\tau}$ are independent for all $t$ and $\\tau$. In the Monte Carlo analysis, we explore $p_{00}\\in\\{0.95,0.99,0.999\\}$, $p_{11}\\in$ $\\{0.95,0.99,0.999\\}$, and $T\\in\\{100,200,300,400,500,1000,1500,\\ldots,5000\\}$. When both $p_{00}$ and $p_{11}$ are large, such as when $p_{00}=p_{11}=0.999$, the rejection frequency is increasing in $T$. This would appear inconsistent with the fact that the Markov-switching model is $I(0)$ for any fixed $p_{00}$ and $p_{11}$, but it is not, as the dependence of rejection frequency on $T$ is not monotonic. If we included $T>5000$ in the design, we would eventually see the rejection frequency decrease.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2968",
    "question": "In the context of spatial-correlation robust t-statistics, which of the following statements are true regarding the experimental designs and results discussed?",
    "options": [
      "A) The Kernel method exhibits substantial size distortions despite optimal bandwidth selection, consistent with findings in time series regressions.",
      "B) SCPC provides perfect size control across all experimental designs, including those with step-function regressors.",
      "C) C-SCPC shows improved size control even when the error process does not match the design assumptions of the critical value.",
      "D) The spatial distribution of economic activity in the 48 U.S. states is proxied by light intensity measurements from space."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "06-Spatial_Correlation_Robust_Inference_in_Linear_Regression_and_Panel_Models.md"
    ],
    "question_context": "The experiments involve two sets of spatial distributions: one using data from the 48 continental U.S. states with spatial density proportional to economic activity (proxied by light intensity), and another using global data from the World Bank’s World Development Indicators. Key formula: The regression model is $y_{I}=x_{I}\\beta+z_{I}^{\\prime}\\gamma+e_{I}$ with $e_{I}=a_{I}\\mathrm{or}e_{I}=\\sin(x_{I})\\cdot a_{I},$ where $s_{I}\\sim$ iid from a U.S. state-specific density. The $t$-statistics' distributions depend on the spatial process generating $(y(s),x(s),z(s))$ and the spatial distribution of sampled locations. Results show that the Kernel method suffers from size distortions, SCPC has variable performance, and C-SCPC offers better size control across designs.",
    "category": "CORRECT",
    "question_context_used": "The experiments involve two sets of spatial distributions: one using data from the 48 continental U.S. states with spatial density proportional to economic activity (proxied by light intensity), and another using global data from the World Bank’s World Development Indicators. Key formula: The regression model is $y_{I}=x_{I}\\beta+z_{I}^{\\prime}\\gamma+e_{I}$ with $e_{I}=a_{I}\\mathrm{or}e_{I}=\\sin(x_{I})\\cdot a_{I},$ where $s_{I}\\sim$ iid from a U.S. state-specific density. The $t$-statistics' distributions depend on the spatial process generating $(y(s),x(s),z(s))$ and the spatial distribution of sampled locations. Results show that the Kernel method suffers from size distortions, SCPC has variable performance, and C-SCPC offers better size control across designs.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3218",
    "question": "Which of the following World Bank publications focus on the intersection of economic policy and environmental sustainability?",
    "options": [
      "A) Environmental and Economic Issues in Forestry: Selected Case Studies in Asia",
      "B) Taxing Bads by Taxing Goods: Pollution Control with Presumptive Charges",
      "C) Trade Performance and Policy in the New Independent States",
      "D) Global Economic Prospects and the Developing Countries"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Books_Received.md"
    ],
    "question_context": "The World Bank's 1996 publications include several works addressing environmental economics. Key examples include 'Environmental and Economic Issues in Forestry: Selected Case Studies in Asia' (Contreras-Hermosilla & Shen, 1995), which examines sustainable forestry practices, and 'Taxing Bads by Taxing Goods: Pollution Control with Presumptive Charges' (Eskeland & Devarajan, 1996), presenting innovative fiscal approaches to environmental regulation. These works demonstrate the Bank's focus on integrating environmental concerns with economic development strategies. Formula: The presumptive charge model can be represented as τ = αP + β, where τ is the tax rate, P is pollution level, and α, β are policy parameters.",
    "category": "CORRECT",
    "question_context_used": "The World Bank's 1996 publications include several works addressing environmental economics. Key examples include 'Environmental and Economic Issues in Forestry: Selected Case Studies in Asia' (Contreras-Hermosilla & Shen, 1995), which examines sustainable forestry practices, and 'Taxing Bads by Taxing Goods: Pollution Control with Presumptive Charges' (Eskeland & Devarajan, 1996), presenting innovative fiscal approaches to environmental regulation. These works demonstrate the Bank's focus on integrating environmental concerns with economic development strategies. Formula: The presumptive charge model can be represented as τ = αP + β, where τ is the tax rate, P is pollution level, and α, β are policy parameters.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-622",
    "question": "In the context of the Lagrangian framework for estimating structural errors with linear dependencies, which of the following statements are true regarding the asymptotic properties of the estimator $\\hat{\\phi}$?",
    "options": [
      "A) The Hessian matrix $\\frac{\\partial^{2}L_{T}}{\\partial\\phi\\partial\\phi}$ is block-diagonal with $\\frac{\\partial^{2}L_{T}}{\\partial\\lambda\\partial\\lambda} = 0$.",
      "B) The asymptotic distribution of $\\sqrt{T}(\\hat{\\phi} - \\phi^{0})$ is normal with mean zero and covariance matrix $C = Q^{-1}S^{*\\prime}Q_{11}^{*}S^{*}Q^{-1}$.",
      "C) The estimators of $\\beta$ and $\\gamma$ are asymptotically dependent on the Lagrange multiplier estimators.",
      "D) The matrix $C_{11}$ is singular due to the relation $C_{11}Q_{12} = 0$."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "DHRYMES - 1994 - AUTOREGRESSIVE ERRORS IN SINGULAR SYSTEMS OF EQUATIONS.md"
    ],
    "question_context": "The Lagrangian framework for estimating structural errors with linear dependencies involves the following key components: \n\n$$ \n\\begin{array}{l}{{\\displaystyle{\\cal L}_{T}(\\phi)=\\frac{1}{T}(y-w)^{\\prime}(\\Sigma_{g}\\otimes I_{T})(y-w)+2\\lambda_{1}^{\\prime}(R_{1}\\beta-r_{1})}}\\ {{\\mathrm{}}}\\ {{\\displaystyle~+2\\lambda_{2}^{\\prime}(R_{2}\\gamma-r_{2})}}\\end{array} \n$$ \n\nwhere $\\phi=(\\beta^{\\prime},\\gamma^{\\prime},\\lambda_{1}^{\\prime},\\lambda_{2}^{\\prime})^{\\prime}$. The estimator $\\hat{\\phi}$ satisfies $\\frac{\\partial L_{T}}{\\partial\\phi}\\left(\\hat{\\phi}\\right)=0$. The Hessian matrix is given by: \n\n$$ \n\\frac{\\partial^{2}L_{T}}{\\partial\\phi\\partial\\phi}\\left(\\phi\\right)=\\left[\\begin{array}{c c}{\\frac{\\partial^{2}L_{T}}{\\partial\\theta\\partial\\theta}}&{\\frac{\\partial^{2}L_{T}}{\\partial\\theta\\partial\\lambda}}\\ {\\frac{\\partial^{2}L_{T}}{\\partial\\lambda\\partial\\theta}}&{\\frac{\\partial^{2}L_{T}}{\\partial\\lambda\\partial\\lambda}}\\end{array}\\right], \n$$ \n\nwhere $\\frac{\\partial^{2}L_{T}}{\\partial\\lambda\\partial\\lambda}=0$. The asymptotic distribution of $\\sqrt{T}(\\hat{\\phi} - \\phi^{0})$ is derived as $N(0,C)$, where $C=Q^{-1}S^{*\\prime}Q_{11}^{*}S^{*}Q^{-1}$. The matrix $C_{11}$ is singular due to $C_{11}Q_{12} = 0$.",
    "category": "CORRECT",
    "question_context_used": "The Lagrangian framework for estimating structural errors with linear dependencies involves the following key components: \n\n$$ \n\\begin{array}{l}{{\\displaystyle{\\cal L}_{T}(\\phi)=\\frac{1}{T}(y-w)^{\\prime}(\\Sigma_{g}\\otimes I_{T})(y-w)+2\\lambda_{1}^{\\prime}(R_{1}\\beta-r_{1})}}\\ {{\\mathrm{}}}\\ {{\\displaystyle~+2\\lambda_{2}^{\\prime}(R_{2}\\gamma-r_{2})}}\\end{array} \n$$ \n\nwhere $\\phi=(\\beta^{\\prime},\\gamma^{\\prime},\\lambda_{1}^{\\prime},\\lambda_{2}^{\\prime})^{\\prime}$. The estimator $\\hat{\\phi}$ satisfies $\\frac{\\partial L_{T}}{\\partial\\phi}\\left(\\hat{\\phi}\\right)=0$. The Hessian matrix is given by: \n\n$$ \n\\frac{\\partial^{2}L_{T}}{\\partial\\phi\\partial\\phi}\\left(\\phi\\right)=\\left[\\begin{array}{c c}{\\frac{\\partial^{2}L_{T}}{\\partial\\theta\\partial\\theta}}&{\\frac{\\partial^{2}L_{T}}{\\partial\\theta\\partial\\lambda}}\\ {\\frac{\\partial^{2}L_{T}}{\\partial\\lambda\\partial\\theta}}&{\\frac{\\partial^{2}L_{T}}{\\partial\\lambda\\partial\\lambda}}\\end{array}\\right], \n$$ \n\nwhere $\\frac{\\partial^{2}L_{T}}{\\partial\\lambda\\partial\\lambda}=0$. The asymptotic distribution of $\\sqrt{T}(\\hat{\\phi} - \\phi^{0})$ is derived as $N(0,C)$, where $C=Q^{-1}S^{*\\prime}Q_{11}^{*}S^{*}Q^{-1}$. The matrix $C_{11}$ is singular due to $C_{11}Q_{12} = 0$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3150",
    "question": "In the context of the mechanism $H^{\\sigma}$ and its $\\sigma$-SPE outcomes, which of the following statements are true according to Proposition 2 and Proposition 3?",
    "options": [
      "A) The set of $\\sigma$-SPE outcomes of $H^{\\sigma}$ coincides with the set of maximum competitive equilibria for any $\\sigma$.",
      "B) Any maximum competitive equilibrium is a $\\sigma$-SPE for any permutation $\\sigma$ of the buyers.",
      "C) The mechanism $H^{\\sigma}$ can lead to outcomes that are not maximum competitive equilibria under certain $\\sigma$.",
      "D) Only the minimum equilibrium prices can be part of a $\\sigma$-SPE strategy profile of the mechanism $H^{\\sigma}$."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "A-Simple-Selling-and-Buying-Procedure_2002_Journal-of-Economic-Theory.md"
    ],
    "question_context": "Proposition 2 asserts that the $\\sigma$-SPE outcomes are always the maximum competitive equilibria. That is, aside from the cases in which a seller sets an expensive price on an unsold object, only the maximum equilibrium prices can be part of a $\\sigma$-SPE strategy profile of the mechanism $H^{\\sigma}$. Proposition 3 states that every maximum competitive equilibrium is a $\\sigma$-SPE for any permutation $\\sigma$ of the buyers. The mechanism $H^{\\sigma}$ implements in $\\sigma$-SPE the set of maximum competitive equilibria, as formalized in Theorem 1. Key formula: $H^{\\sigma}(v,\\mu) = (\\bar{v}, \\mu)$ where $(\\bar{v}, \\mu)$ is a seller-optimal stable outcome.",
    "category": "INCORRECT",
    "question_context_used": "Proposition 2 asserts that the $\\sigma$-SPE outcomes are always the maximum competitive equilibria. That is, aside from the cases in which a seller sets an expensive price on an unsold object, only the maximum equilibrium prices can be part of a $\\sigma$-SPE strategy profile of the mechanism $H^{\\sigma}$. Proposition 3 states that every maximum competitive equilibrium is a $\\sigma$-SPE for any permutation $\\sigma$ of the buyers. The mechanism $H^{\\sigma}$ implements in $\\sigma$-SPE the set of maximum competitive equilibria, as formalized in Theorem 1. Key formula: $H^{\\sigma}(v,\\mu) = (\\bar{v}, \\mu)$ where $(\\bar{v}, \\mu)$ is a seller-optimal stable outcome.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4678",
    "question": "Under the conditions of Hopenhayn–Prescott's theorem for uniqueness and convergence of $G^{F I}$, which of the following statements about the Monotone Mixing Conditions (MMC) are correct?",
    "options": [
      "A) MMC requires existence of a point $b$ in the state space where transition probabilities after $n$ good and bad shocks exceed $\\epsilon$.",
      "B) MMC guarantees that the invariant distribution $G^{F I}$ is independent of initial conditions.",
      "C) MMC is satisfied if the transition probability $P$ is monotonic in first-order stochastic dominance.",
      "D) MMC implies that the success probability $p$ must be strictly increasing in inherited wealth."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Endogenous-capital-market-imperfections--human-capit_2009_Journal-of-Develop.md"
    ],
    "question_context": "The proof of the uniqueness and convergence of $G^{F I}$ follows directly from the Hopenhayn–Prescott's theorem 2. If the linear Markov process satisfies the monotonicity property as well as the following 'concavity property' or 'Monotone Mixing Conditions' (MMC), then there is a unique and convergent distribution $G^{F I}$. The MMC tells us that we can find a point $b\\in[\\underline{{b}},\\bar{b}]$ such that there exists $n\\geq1$ and $\\epsilon>0$, such that in $n$ good realizations of the shock $p({\\underline{{b}}}, [b^{*},\\bar{b}])^{n}>\\epsilon$ and in $n$ bad realizations of the shock $p(\\bar{b},[b^{*},\\underline{{b}}])^{n}>\\epsilon$. Key formula: $p\\left(\\underline{{b}},\\left[\\underline{{b}}^{*},\\overline{{b}}\\right]\\right)=\\gamma\\overline{{p}}$ for the high type and $p\\left(\\overline{{b}},\\left[b^{*},\\underline{{b}}\\right]\\right)=\\gamma(1-\\overline{{p}})$ for the high type.",
    "category": "INCORRECT",
    "question_context_used": "The proof of the uniqueness and convergence of $G^{F I}$ follows directly from the Hopenhayn–Prescott's theorem 2. If the linear Markov process satisfies the monotonicity property as well as the following 'concavity property' or 'Monotone Mixing Conditions' (MMC), then there is a unique and convergent distribution $G^{F I}$. The MMC tells us that we can find a point $b\\in[\\underline{{b}},\\bar{b}]$ such that there exists $n\\geq1$ and $\\epsilon>0$, such that in $n$ good realizations of the shock $p({\\underline{{b}}}, [b^{*},\\bar{b}])^{n}>\\epsilon$ and in $n$ bad realizations of the shock $p(\\bar{b},[b^{*},\\underline{{b}}])^{n}>\\epsilon$. Key formula: $p\\left(\\underline{{b}},\\left[\\underline{{b}}^{*},\\overline{{b}}\\right]\\right)=\\gamma\\overline{{p}}$ for the high type and $p\\left(\\overline{{b}},\\left[b^{*},\\underline{{b}}\\right]\\right)=\\gamma(1-\\overline{{p}})$ for the high type.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1145",
    "question": "In the context of the two-class economy model by Rivera-Batiz (1984), which of the following scenarios would unambiguously lead to a decrease in the real income of class 2 (labor-rich individuals)?",
    "options": [
      "A) Emigration of pure labor (class 2) where $L^{*}<0$ and $K^{*}=0$",
      "B) Emigration of capital-rich individuals (class 1) where $L^{*}=0$ and $K^{*}<0$",
      "C) Emigration of skilled workers (human capital) where $L^{*}<0$ and $K^{*}<0$",
      "D) Emigration of both labor and capital in equal proportions where $L^{*}=K^{*}<0$"
    ],
    "gold_answer": "B, C",
    "source_files": [
      "The-impact-of-international-migration-on-real-w_1989_Journal-of-Development-.md"
    ],
    "question_context": "The model distinguishes two classes based on capital-labor ratios: class 1 ($K_{1}/L_{1}>K/L$) and class 2 ($K_{2}/L_{2}<K/L$). The real income of class $m$ is given by:\n\n$$\ny_{m}^{*}=\\frac{\\theta_{k}^{m}\\beta_{k N}+(1-\\theta_{k}^{m})\\beta_{L N}-\\theta_{N}^{m}}{|\\lambda|(\\sigma_{D}+\\sigma_{S}-\\sigma_{k}/|\\theta|)}(L^{*}-K^{*})\n$$\n\nKey assumptions include $|\\theta|<0$, $|\\lambda|<0$, $\\beta_{L N}>0$, and $\\beta_{k N}<0$. Emigration of pure labor ($L^{*}<0, K^{*}=0$) raises real wages for class 2, but emigration of capital-rich individuals ($L^{*}=0, K^{*}<0$) unambiguously reduces class 2's real income ($y_{2}^{*}<0$). The brain drain (emigration of labor and human capital, $L^{*}<0, K^{*}<0$) can also decrease real income depending on parameter values.",
    "category": "INCORRECT",
    "question_context_used": "The model distinguishes two classes based on capital-labor ratios: class 1 ($K_{1}/L_{1}>K/L$) and class 2 ($K_{2}/L_{2}<K/L$). The real income of class $m$ is given by:\n\n$$\ny_{m}^{*}=\\frac{\\theta_{k}^{m}\\beta_{k N}+(1-\\theta_{k}^{m})\\beta_{L N}-\\theta_{N}^{m}}{|\\lambda|(\\sigma_{D}+\\sigma_{S}-\\sigma_{k}/|\\theta|)}(L^{*}-K^{*})\n$$\n\nKey assumptions include $|\\theta|<0$, $|\\lambda|<0$, $\\beta_{L N}>0$, and $\\beta_{k N}<0$. Emigration of pure labor ($L^{*}<0, K^{*}=0$) raises real wages for class 2, but emigration of capital-rich individuals ($L^{*}=0, K^{*}<0$) unambiguously reduces class 2's real income ($y_{2}^{*}<0$). The brain drain (emigration of labor and human capital, $L^{*}<0, K^{*}<0$) can also decrease real income depending on parameter values.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-795",
    "question": "Based on the experimental results regarding wishful thinking, which of the following statements are empirically supported?",
    "options": [
      "A) Wishful thinking is most pronounced when evidence is ambiguous.",
      "B) Higher material costs of wrong beliefs always reduce wishful thinking.",
      "C) Wishful thinking disappears in the domain of monetary gains.",
      "D) Accuracy incentives operate mostly through increased information gathering."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Anticipatory Anxiety and Wishful Thinking.md"
    ],
    "question_context": "Our experiments show the importance of such anticipatory emotions for belief formation. In each of the four experiments, participants are significantly less accurate in identifying patterns that may result in adverse outcomes. Such wishful thinking is most pronounced when evidence is ambiguous, a result that replicates across tasks with distinct sources of ambiguity. Individuals differ in their propensity to engage in wishful thinking, with some showing the opposite tendency that reflects defensive pessimism. We find evidence that a higher material cost of wrong beliefs can reduce wishful thinking, but only when accuracy in the inference task is elastic to effort, so that participants can obtain more precise representations of signals if they choose to. Whether motivated beliefs respond to material incentives more generally is therefore likely to depend on the cognitive task and context in which beliefs are formed. Finally, we find that wishful thinking disappears in the domain of monetary gains, indicating that negative emotions are an important driver of the phenomenon. Our findings on the role of accuracy incentives indicate that they operate mostly through increased information gathering.",
    "category": "INCORRECT",
    "question_context_used": "Our experiments show the importance of such anticipatory emotions for belief formation. In each of the four experiments, participants are significantly less accurate in identifying patterns that may result in adverse outcomes. Such wishful thinking is most pronounced when evidence is ambiguous, a result that replicates across tasks with distinct sources of ambiguity. Individuals differ in their propensity to engage in wishful thinking, with some showing the opposite tendency that reflects defensive pessimism. We find evidence that a higher material cost of wrong beliefs can reduce wishful thinking, but only when accuracy in the inference task is elastic to effort, so that participants can obtain more precise representations of signals if they choose to. Whether motivated beliefs respond to material incentives more generally is therefore likely to depend on the cognitive task and context in which beliefs are formed. Finally, we find that wishful thinking disappears in the domain of monetary gains, indicating that negative emotions are an important driver of the phenomenon. Our findings on the role of accuracy incentives indicate that they operate mostly through increased information gathering.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1054",
    "question": "Based on the analysis of married women's job search strategies, which of the following factors are empirically supported to explain their suboptimal reservation wage choices?",
    "options": [
      "A) Extreme risk aversion leading to lower asking wages",
      "B) Liquidity constraints limiting access to financial capital for extended search",
      "C) Overestimation of nonpecuniary benefits during unemployment spells",
      "D) Expectation of short tenure at new jobs reducing incentive for prolonged search"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Sandell-UnemploymentRateWomen-1980.md"
    ],
    "question_context": "The text analyzes the cost-benefit trade-offs in married women's job search strategies, showing that a 10-cent increase in the reservation wage yields a monetary gain of \\$23.00–\\$65.67 per year, outweighing the additional cost of \\$5.18–\\$10.95. Key formula: The optimal reservation wage satisfies $\\frac{\\partial E[W]}{\\partial w_r} = 0$, where $E[W]$ is expected wage gain. However, empirical results indicate suboptimal search investment due to: (1) extreme risk aversion (proven to reduce search intensity), (2) liquidity constraints (evidenced by wealthier families choosing riskier strategies), and (3) short expected job tenure (critical period of 12 weeks needed to break even). Nonpecuniary factors like psychic costs are acknowledged but not quantified.",
    "category": "CORRECT",
    "question_context_used": "The text analyzes the cost-benefit trade-offs in married women's job search strategies, showing that a 10-cent increase in the reservation wage yields a monetary gain of \\$23.00–\\$65.67 per year, outweighing the additional cost of \\$5.18–\\$10.95. Key formula: The optimal reservation wage satisfies $\\frac{\\partial E[W]}{\\partial w_r} = 0$, where $E[W]$ is expected wage gain. However, empirical results indicate suboptimal search investment due to: (1) extreme risk aversion (proven to reduce search intensity), (2) liquidity constraints (evidenced by wealthier families choosing riskier strategies), and (3) short expected job tenure (critical period of 12 weeks needed to break even). Nonpecuniary factors like psychic costs are acknowledged but not quantified.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3634",
    "question": "Which of the following mechanisms explain how the introduction of hygiene grade cards in Los Angeles County restaurants led to improved hygiene quality?",
    "options": [
      "A) Mitigation of information asymmetry between restaurants and consumers",
      "B) Increased product differentiation leading to softened price competition",
      "C) Reduction in search costs for consumers regarding hygiene quality",
      "D) Mandatory disclosure causing immediate compliance without actual hygiene improvements"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Effect of Information on Product Quality Evidence from Restaurant Hygiene Grade Cards.md"
    ],
    "question_context": "The study examines the effect of hygiene grade cards on restaurant behavior and health outcomes. Key findings include a 5% increase in inspection scores, a 5% revenue boost for A-grade restaurants, and a 20% decrease in foodborne illness hospitalizations. The grade cards mitigate information asymmetry, allowing consumers to identify hygiene quality differences. This creates economic incentives for restaurants to improve hygiene, as demand becomes sensitive to hygiene quality. The formula for the change in revenue (ΔR) can be expressed as ΔR = α + β(Grade) + ε, where Grade is the hygiene score. Additionally, the cards may increase product differentiation by revealing true hygiene differences between restaurants, potentially softening price competition. However, they also reduce search costs, making consumers less captive to particular restaurants and promoting competition.",
    "category": "CORRECT",
    "question_context_used": "The study examines the effect of hygiene grade cards on restaurant behavior and health outcomes. Key findings include a 5% increase in inspection scores, a 5% revenue boost for A-grade restaurants, and a 20% decrease in foodborne illness hospitalizations. The grade cards mitigate information asymmetry, allowing consumers to identify hygiene quality differences. This creates economic incentives for restaurants to improve hygiene, as demand becomes sensitive to hygiene quality. The formula for the change in revenue (ΔR) can be expressed as ΔR = α + β(Grade) + ε, where Grade is the hygiene score. Additionally, the cards may increase product differentiation by revealing true hygiene differences between restaurants, potentially softening price competition. However, they also reduce search costs, making consumers less captive to particular restaurants and promoting competition.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2734",
    "question": "In the context of the Non-Linear Least Squares Iterative (NLLI) method, which of the following statements are correct based on the matrix derivations provided?",
    "options": [
      "A) NLLI requires matrix inversions in every iteration after the first round.",
      "B) The metric $\\pmb{A}$ is initially in error because $X^{t}u$ is not zero.",
      "C) The term $(V^{t}V)^{-1}$ can be computed using (13) without matrix inversion after the first iteration.",
      "D) The asymptotic efficiency of NLLI over MNL2S arises because $X^{t}u$ must be zero as $X$ is predetermined."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "A-note-on-non-linear-limited-information-maximum-li_1978_Journal-of-Economet.md"
    ],
    "question_context": "The text discusses the Non-Linear Least Squares Iterative (NLLI) method and its computational efficiency compared to MNL2S. Key formulas include: \n\n$$(V^{t}V)^{-1}=\\Sigma_{0}^{-1}(V^{0})^{t}[I{-}\\eta^{-1}u u^{t}](V^{0})\\Sigma_{0}^{-1},$$\n\nwhere \n\n$$\\eta=\\frac{\\rho^{2}}{\\mu}{+}u^{t}(V^{0})[(V^{0})^{t}(V^{0})]^{-1}(V^{0})^{t}u.$$\n\nThe simplified process requires only matrix multiplications after the first round, and no matrix inversions are needed. The metric $\\pmb{A}$ is initially in error because $X^{t}u$ is not zero, but asymptotically, $X^{t}u$ must be zero as the $X$'s are predetermined by definition.",
    "category": "CORRECT",
    "question_context_used": "The text discusses the Non-Linear Least Squares Iterative (NLLI) method and its computational efficiency compared to MNL2S. Key formulas include: \n\n$$(V^{t}V)^{-1}=\\Sigma_{0}^{-1}(V^{0})^{t}[I{-}\\eta^{-1}u u^{t}](V^{0})\\Sigma_{0}^{-1},$$\n\nwhere \n\n$$\\eta=\\frac{\\rho^{2}}{\\mu}{+}u^{t}(V^{0})[(V^{0})^{t}(V^{0})]^{-1}(V^{0})^{t}u.$$\n\nThe simplified process requires only matrix multiplications after the first round, and no matrix inversions are needed. The metric $\\pmb{A}$ is initially in error because $X^{t}u$ is not zero, but asymptotically, $X^{t}u$ must be zero as the $X$'s are predetermined by definition.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4202",
    "question": "Which of the following statements are true regarding Bayesian Beliefs Equilibrium and Nash Equilibrium in games with more than two players?",
    "options": [
      "A) In Bayesian Beliefs Equilibrium, players can disagree about what another player will do, whereas in Nash Equilibrium, marginal beliefs must agree.",
      "B) Both Bayesian Beliefs Equilibrium and Nash Equilibrium require that players' beliefs about opponents' strategies are stochastically independent.",
      "C) A Bayesian Beliefs Equilibrium always constitutes a Nash Equilibrium in games with more than two players.",
      "D) In Nash Equilibrium, the marginal beliefs of all players about any given player's strategy must be represented by the same probability measure."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Equilibrium-in-Beliefs-under-Uncertainty_1996_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The text discusses two interpretations of Nash Equilibrium: the traditional view where the mixed strategy is the actual strategy used by the player, and the beliefs interpretation where it represents the marginal beliefs of other players about the player's strategy. The latter interpretation leads to the definition of Bayesian Beliefs Equilibrium, which is more general than Nash Equilibrium in games with more than two players. Specifically, in Bayesian Beliefs Equilibrium, players can disagree about what another player will do (i.e., $\\mathrm{marg}_{S_{j}}b_{i}\\neq\\mathrm{marg}_{S_{j}}b_{k}$), and players' beliefs about opponents' strategies can be stochastically dependent. In contrast, Nash Equilibrium requires that the marginal beliefs of all players about any given player's strategy are represented by the same probability measure ($\\sigma_{i}^{*}$) and that beliefs are stochastically independent.",
    "category": "CORRECT",
    "question_context_used": "The text discusses two interpretations of Nash Equilibrium: the traditional view where the mixed strategy is the actual strategy used by the player, and the beliefs interpretation where it represents the marginal beliefs of other players about the player's strategy. The latter interpretation leads to the definition of Bayesian Beliefs Equilibrium, which is more general than Nash Equilibrium in games with more than two players. Specifically, in Bayesian Beliefs Equilibrium, players can disagree about what another player will do (i.e., $\\mathrm{marg}_{S_{j}}b_{i}\\neq\\mathrm{marg}_{S_{j}}b_{k}$), and players' beliefs about opponents' strategies can be stochastically dependent. In contrast, Nash Equilibrium requires that the marginal beliefs of all players about any given player's strategy are represented by the same probability measure ($\\sigma_{i}^{*}$) and that beliefs are stochastically independent.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4187",
    "question": "Under the null hypothesis $\\mathcal{H}_0: \\pmb{\\alpha} = \\mathbf{0}$, which of the following statements are true regarding the asymptotic independence of $S_{Wald}$ and $L_{max}$?",
    "options": [
      "A) The joint distribution $P(\\{S_{Wald} \\leq z\\} \\cap \\{L_{max} \\geq y\\})$ converges to $\\Phi(z)(1 - G(y))$ uniformly.",
      "B) The asymptotic independence holds due to the decomposition of $S_{Wald}$ into partial sums $A_{j1}$ and $A_{j2}$.",
      "C) The asymptotic independence is a direct consequence of the i.i.d. assumption on $u_1, \\ldots, u_n$.",
      "D) The asymptotic independence requires the covariance estimator $\\widehat{\\pmb{\\Sigma}}_{pd}$ to satisfy $\\|\\widehat{\\pmb{\\Sigma}}_{pd}^{-1} - (\\pmb{\\Sigma}^\\star)^{-1}\\|_{\\max} = O_p(\\sqrt{(\\log p)/T})$."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Power-enhancement-for-testing-multi-factor-asset-pricin_2024_Journal-of-Econ.md"
    ],
    "question_context": "The joint limiting law for $S_{Wald}$ and $L_{max}$ is studied under the null hypothesis $\\mathcal{H}_0: \\pmb{\\alpha} = \\mathbf{0}$. The key result is the asymptotic independence between $S_{Wald}$ and $L_{max}$, which is derived through the decomposition of the joint distribution $P(\\{S_{Wald} \\leq z\\} \\cap \\{L_{max} \\geq y\\})$. The sum statistic $S_{Wald}$ can be decomposed into two partial sums, $A_{j1}$ and $A_{j2}$, where $A_{j1}$ is associated with $B_j$ and $A_{j2}$ is independent of $B_j$. This decomposition, along with Bonferroni inequality, helps bound the difference between the joint distribution and the product of marginal distributions, leading to the asymptotic independence. The covariance estimator $\\widehat{\\pmb{\\Sigma}}_{pd}$ must satisfy the condition $\\|\\widehat{\\pmb{\\Sigma}}_{pd}^{-1} - (\\pmb{\\Sigma}^\\star)^{-1}\\|_{\\max} = O_p(\\sqrt{(\\log p)/T})$ for the asymptotic independence to hold. The joint distribution uniformly converges to $\\Phi(z)(1 - G(y))$, which is the product of the marginal asymptotic distribution functions.",
    "category": "CORRECT",
    "question_context_used": "The joint limiting law for $S_{Wald}$ and $L_{max}$ is studied under the null hypothesis $\\mathcal{H}_0: \\pmb{\\alpha} = \\mathbf{0}$. The key result is the asymptotic independence between $S_{Wald}$ and $L_{max}$, which is derived through the decomposition of the joint distribution $P(\\{S_{Wald} \\leq z\\} \\cap \\{L_{max} \\geq y\\})$. The sum statistic $S_{Wald}$ can be decomposed into two partial sums, $A_{j1}$ and $A_{j2}$, where $A_{j1}$ is associated with $B_j$ and $A_{j2}$ is independent of $B_j$. This decomposition, along with Bonferroni inequality, helps bound the difference between the joint distribution and the product of marginal distributions, leading to the asymptotic independence. The covariance estimator $\\widehat{\\pmb{\\Sigma}}_{pd}$ must satisfy the condition $\\|\\widehat{\\pmb{\\Sigma}}_{pd}^{-1} - (\\pmb{\\Sigma}^\\star)^{-1}\\|_{\\max} = O_p(\\sqrt{(\\log p)/T})$ for the asymptotic independence to hold. The joint distribution uniformly converges to $\\Phi(z)(1 - G(y))$, which is the product of the marginal asymptotic distribution functions.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4331",
    "question": "In the context of nonlinear factor copula models, which of the following statements are true regarding the general one-factor structure and its nested copulas?",
    "options": [
      "A) The skew t copula can be represented with an inverse gamma distribution for the common factor and a normal distribution for the idiosyncratic factor.",
      "B) The Clayton copula's link function involves an exponential distribution for the idiosyncratic factor and a gamma distribution for the common factor.",
      "C) The Gumbel copula's representation requires a stable distribution for the common factor and an exponential distribution for the idiosyncratic factor.",
      "D) The normal copula assumes both the common and idiosyncratic factors follow normal distributions with identical variances."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "10-Modeling Dependence in High Dimensions With Factor Copulas.md"
    ],
    "question_context": "The general one-factor structure for copula models is given by:\n\n$$\n\\begin{array}{r}{X_{i}=h\\left(Z,\\varepsilon_{i}\\right),i=1,2,\\ldots,N}\\ {Z\\sim F_{z},\\varepsilon_{i}\\sim\\mathrm{iid}F_{\\varepsilon},Z\\perp\\varepsilon_{i}\\forall i}\\ {\\left[X_{1},\\ldots,X_{N}\\right]^{\\prime}\\equiv\\mathbf{X}\\sim\\mathbf{F}_{x}{=}\\mathbf{C}\\left(G_{1},\\ldots,G_{N}\\right)}\\end{array}\n$$\n\nwhere $h:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ is a link function. This framework nests several well-known copulas:\n\n- **Normal copula**: $h(Z,\\varepsilon) = Z + \\varepsilon$, with $Z \\sim N(0, \\sigma^2)$ and $\\varepsilon \\sim N(0, \\sigma^2)$.\n- **Student's t copula**: $h(Z,\\varepsilon) = Z^{1/2}\\varepsilon$, with $Z \\sim Ig(\\nu/2, \\nu/2)$ and $\\varepsilon \\sim N(0, \\sigma^2)$.\n- **Skew t copula**: $h(Z,\\varepsilon) = \\lambda Z + Z^{1/2}\\varepsilon$, with $Z \\sim Ig(\\nu/2, \\nu/2)$ and $\\varepsilon \\sim N(0, \\sigma^2)$.\n- **Clayton copula**: $h(Z,\\varepsilon) = (1 + \\varepsilon/Z)^{-\\alpha}$, with $Z \\sim \\Gamma(\\alpha, 1)$ and $\\varepsilon \\sim Exp(1)$.\n- **Gumbel copula**: $h(Z,\\varepsilon) = -(\\log Z / \\varepsilon)$, with $Z \\sim Stable(1/\\alpha,1,1,0)$ and $\\varepsilon \\sim Exp(1)$.\n\nThe skew t and Generalized hyperbolic copulas are from McNeil, Frey, and Embrechts (2005), the Clayton copula representation is from Cook and Johnson (1981), and the Gumbel copula representation is from Marshall and Olkin (1988).",
    "category": "INCORRECT",
    "question_context_used": "The general one-factor structure for copula models is given by:\n\n$$\n\\begin{array}{r}{X_{i}=h\\left(Z,\\varepsilon_{i}\\right),i=1,2,\\ldots,N}\\ {Z\\sim F_{z},\\varepsilon_{i}\\sim\\mathrm{iid}F_{\\varepsilon},Z\\perp\\varepsilon_{i}\\forall i}\\ {\\left[X_{1},\\ldots,X_{N}\\right]^{\\prime}\\equiv\\mathbf{X}\\sim\\mathbf{F}_{x}{=}\\mathbf{C}\\left(G_{1},\\ldots,G_{N}\\right)}\\end{array}\n$$\n\nwhere $h:\\mathbb{R}^{2}\\rightarrow\\mathbb{R}$ is a link function. This framework nests several well-known copulas:\n\n- **Normal copula**: $h(Z,\\varepsilon) = Z + \\varepsilon$, with $Z \\sim N(0, \\sigma^2)$ and $\\varepsilon \\sim N(0, \\sigma^2)$.\n- **Student's t copula**: $h(Z,\\varepsilon) = Z^{1/2}\\varepsilon$, with $Z \\sim Ig(\\nu/2, \\nu/2)$ and $\\varepsilon \\sim N(0, \\sigma^2)$.\n- **Skew t copula**: $h(Z,\\varepsilon) = \\lambda Z + Z^{1/2}\\varepsilon$, with $Z \\sim Ig(\\nu/2, \\nu/2)$ and $\\varepsilon \\sim N(0, \\sigma^2)$.\n- **Clayton copula**: $h(Z,\\varepsilon) = (1 + \\varepsilon/Z)^{-\\alpha}$, with $Z \\sim \\Gamma(\\alpha, 1)$ and $\\varepsilon \\sim Exp(1)$.\n- **Gumbel copula**: $h(Z,\\varepsilon) = -(\\log Z / \\varepsilon)$, with $Z \\sim Stable(1/\\alpha,1,1,0)$ and $\\varepsilon \\sim Exp(1)$.\n\nThe skew t and Generalized hyperbolic copulas are from McNeil, Frey, and Embrechts (2005), the Clayton copula representation is from Cook and Johnson (1981), and the Gumbel copula representation is from Marshall and Olkin (1988).",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4676",
    "question": "In the context of first-best investment levels under exogenous constraints on borrowing, which of the following statements are correct?",
    "options": [
      "A) The first-best investment level $I_{\\theta}^{*}$ is derived from the non-arbitrage condition between human and physical capital.",
      "B) Talented borrowers invest less in education than less talented borrowers due to higher opportunity costs.",
      "C) The first-best investment level depends negatively on the return gap $(h^{e}G_{\\bar{\\theta}}-h^{u})$ and positively on the riskless interest rate $R$.",
      "D) Overinvestment occurs when the expected marginal profits on the investment are below the riskless interest rate $R$."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Endogenous-capital-market-imperfections--human-capit_2009_Journal-of-Develop.md"
    ],
    "question_context": "The first-best level of investment $I^{*}$ maximizes expected returns net of the opportunity cost of investment, given by the equation:\n\n$$\nI^{*}{=}\\arg\\operatorname*{max}_{I>0}p(\\theta,I)h^{e}G_{\\theta}+(1{-}p(\\theta,I))h^{u}{-}R I.\n$$\n\nThe first-order condition (FOC) is:\n\n$$\n\\frac{\\mathrm{d}p(\\theta,I)}{\\mathrm{d}I}(h^{e}G_{\\theta}-h^{u})=R.\n$$\n\nThis condition represents the non-arbitrage equilibrium between human and physical capital. The first-best investment level is explicitly derived as:\n\n$$\nI_{\\theta}^{*}=\\ln\\bigg({\\frac{B(\\theta)(h^{e}G_{\\theta}{-}h^{u})}{R}}\\bigg),\n$$\n\nwhere $\\theta=\\{\\underline{{\\theta}},\\bar{{\\theta}}\\}$. The first-best level depends positively on the return gap $(h^{e}G_{\\bar{\\theta}}-h^{u})$ and negatively on the riskless interest rate $R$. Talented borrowers invest more in education ($I_{\\bar{\\theta}}^{*} > I_{\\underline{{\\theta}}}^{*}$) due to higher marginal returns. Overinvestment occurs when expected marginal profits fall below $R$, making it optimal to reduce human capital investments and allocate excess resources to the capital market.",
    "category": "CORRECT",
    "question_context_used": "The first-best level of investment $I^{*}$ maximizes expected returns net of the opportunity cost of investment, given by the equation:\n\n$$\nI^{*}{=}\\arg\\operatorname*{max}_{I>0}p(\\theta,I)h^{e}G_{\\theta}+(1{-}p(\\theta,I))h^{u}{-}R I.\n$$\n\nThe first-order condition (FOC) is:\n\n$$\n\\frac{\\mathrm{d}p(\\theta,I)}{\\mathrm{d}I}(h^{e}G_{\\theta}-h^{u})=R.\n$$\n\nThis condition represents the non-arbitrage equilibrium between human and physical capital. The first-best investment level is explicitly derived as:\n\n$$\nI_{\\theta}^{*}=\\ln\\bigg({\\frac{B(\\theta)(h^{e}G_{\\theta}{-}h^{u})}{R}}\\bigg),\n$$\n\nwhere $\\theta=\\{\\underline{{\\theta}},\\bar{{\\theta}}\\}$. The first-best level depends positively on the return gap $(h^{e}G_{\\bar{\\theta}}-h^{u})$ and negatively on the riskless interest rate $R$. Talented borrowers invest more in education ($I_{\\bar{\\theta}}^{*} > I_{\\underline{{\\theta}}}^{*}$) due to higher marginal returns. Overinvestment occurs when expected marginal profits fall below $R$, making it optimal to reduce human capital investments and allocate excess resources to the capital market.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4415",
    "question": "In the Conditional Auction (CA) mechanism, which of the following statements are true regarding the subspace $U_{0}$ and agent utility functions?",
    "options": [
      "A) $U_{0}$ consists of utility functions with zero expectation over all allocations",
      "B) Two utility functions differing by a constant are indistinguishable by s.a.f.s",
      "C) The subspace $U_{0}$ is orthogonal to the space of constant utility functions",
      "D) Agent $i$'s relevant information is fully captured by $\\bar{u}_{i} = u_{i} - E u_{i}$"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Conditional Auction Mechanism for Sharing a Surplus.md"
    ],
    "question_context": "The Conditional Auction (CA) mechanism defines $U_{0}$ as the subspace of $\\boldsymbol{U}$ consisting of utility functions $x$ where $E x = \\sum_{\\alpha\\in A}x(\\alpha) = 0$. This subspace plays a crucial role in information revelation, as two utility functions $u_{i}, v_{i}$ are not distinguished by s.a.f.s if ${\\pmb u}_{i}({\\pmb a})-{\\pmb v}_{i}({\\pmb a})$ is constant across allocations. The relevant information about an agent's utility $u_{i}$ is fully captured by its projection $\\bar{u}_{i} = u_{i} - E u_{i}$ onto $U_{0}$.",
    "category": "INCORRECT",
    "question_context_used": "The Conditional Auction (CA) mechanism defines $U_{0}$ as the subspace of $\\boldsymbol{U}$ consisting of utility functions $x$ where $E x = \\sum_{\\alpha\\in A}x(\\alpha) = 0$. This subspace plays a crucial role in information revelation, as two utility functions $u_{i}, v_{i}$ are not distinguished by s.a.f.s if ${\\pmb u}_{i}({\\pmb a})-{\\pmb v}_{i}({\\pmb a})$ is constant across allocations. The relevant information about an agent's utility $u_{i}$ is fully captured by its projection $\\bar{u}_{i} = u_{i} - E u_{i}$ onto $U_{0}$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4044",
    "question": "In the context of migration equilibrium and broad vulnerability, which of the following statements are correct?",
    "options": [
      "A) A migration equilibrium exists if no individual can improve their allocation by moving to another island, given the current distribution of individuals.",
      "B) The egalitarian rule is broadly vulnerable because it always leads to a migration equilibrium regardless of the endowment list.",
      "C) The uniform rule can result in no migration equilibrium existing for certain endowment lists and preference profiles.",
      "D) A division rule is broadly vulnerable if, for any set of individuals and number of islands, there exists a configuration where no migration equilibrium exists."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Division-Rules-and-Migration-Equilibria_1996_Journal-of-Economic-Theory.md"
    ],
    "question_context": "Definition II.2 states that a global environment is a migration equilibrium if there does not exist an individual $i\\in M(u)$ and two islands $j, k\\in N,$ where $(i,\\succcurlyeq)\\in u_{j}$ and $F_{k}^{i}(u_{k}\\cup\\{(i,\\succcurlyeq)\\}$ , $L_{k})\\succ_{i}F_{j}^{i}(u_{j},L_{j})$. This implies no individual has an incentive to move. Definition III.1 introduces broad vulnerability: a division rule $F$ is broadly vulnerable if for each $M\\subseteq\\mathbb{N}$ and $n$, there exist configurations where no migration equilibrium exists. Example II.5 shows that with $\\mathcal{L}=(1,1)$ and $\\mathcal F=$ $(F_{u n},F_{u n})$, no migration equilibrium exists for certain preference profiles. The uniform rule's allocation is given by $$F^{j}(u,L)=\\left\\{\\begin{array}{l l}{\\operatorname*{min}[p_{L}(\\succcurlyeq_{i}),\\lambda]\\quad}&{\\mathrm{if}\\quad\\sum_{M(u)}p_{L}(\\succcurlyeq_{j})\\gtrsim L}\\\\ {\\operatorname*{max}[p_{L}(\\succcurlyeq_{i}),\\mu]\\quad}&{\\mathrm{if}\\quad\\sum_{M(u)}p_{L}(\\succcurlyeq_{j})<L,}\\end{array}\\right.$$ where $\\lambda$ and $\\mu$ solve the respective equations.",
    "category": "CORRECT",
    "question_context_used": "Definition II.2 states that a global environment is a migration equilibrium if there does not exist an individual $i\\in M(u)$ and two islands $j, k\\in N,$ where $(i,\\succcurlyeq)\\in u_{j}$ and $F_{k}^{i}(u_{k}\\cup\\{(i,\\succcurlyeq)\\}$ , $L_{k})\\succ_{i}F_{j}^{i}(u_{j},L_{j})$. This implies no individual has an incentive to move. Definition III.1 introduces broad vulnerability: a division rule $F$ is broadly vulnerable if for each $M\\subseteq\\mathbb{N}$ and $n$, there exist configurations where no migration equilibrium exists. Example II.5 shows that with $\\mathcal{L}=(1,1)$ and $\\mathcal F=$ $(F_{u n},F_{u n})$, no migration equilibrium exists for certain preference profiles. The uniform rule's allocation is given by $$F^{j}(u,L)=\\left\\{\\begin{array}{l l}{\\operatorname*{min}[p_{L}(\\succcurlyeq_{i}),\\lambda]\\quad}&{\\mathrm{if}\\quad\\sum_{M(u)}p_{L}(\\succcurlyeq_{j})\\gtrsim L}\\\\ {\\operatorname*{max}[p_{L}(\\succcurlyeq_{i}),\\mu]\\quad}&{\\mathrm{if}\\quad\\sum_{M(u)}p_{L}(\\succcurlyeq_{j})<L,}\\end{array}\\right.$$ where $\\lambda$ and $\\mu$ solve the respective equations.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4406",
    "question": "Which of the following are key features of Zero Tolerance (ZT) laws that make them particularly suitable for estimating the causal effect of heavy episodic alcohol use on nuisance crimes?",
    "options": [
      "A) They reduce heavy episodic drinking among young men aged 18-20 but not older males aged 22-24.",
      "B) They are uncorrelated with omitted determinants of criminal behavior.",
      "C) They exhibit substantial variation in the timing of policy adoption across states.",
      "D) They create sharp predictions about which age groups should have been affected."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Carpenter-HeavyAlcoholUse-2005.md"
    ],
    "question_context": "A key difficulty in identifying a compelling research design that would credibly estimate the causal effect of alcohol use on crime is finding a mechanism that generates arguably exogenous variation in alcohol consumption. In this paper, I use adoption of tougher drunk driving laws aimed at youth under age 21 to estimate the effect of heavy episodic alcohol use on the commission of crime. These “Zero Tolerance\" (ZT) laws were enacted by every state over the past two decades and substantially lowered the legal blood alcohol content (BAC) limit for underage drivers. Previous research has shown that these tougher laws reduced heavy episodic drinking by young men ages 18-20 but had no effect on slightly older males ages 22-24 (Carpenter, 2004a). In the context of identifying effects of alcohol use on crime, ZT laws are particularly attractive because (i) adoption of tougher drunk driving policies is arguably uncorrelated with omitted determinants of criminal behavior; (ii) there is substantial variation across states in the timing of policy adoption; and (iii) the laws create sharp predictions about which age groups should have been affected.",
    "category": "CORRECT",
    "question_context_used": "A key difficulty in identifying a compelling research design that would credibly estimate the causal effect of alcohol use on crime is finding a mechanism that generates arguably exogenous variation in alcohol consumption. In this paper, I use adoption of tougher drunk driving laws aimed at youth under age 21 to estimate the effect of heavy episodic alcohol use on the commission of crime. These “Zero Tolerance\" (ZT) laws were enacted by every state over the past two decades and substantially lowered the legal blood alcohol content (BAC) limit for underage drivers. Previous research has shown that these tougher laws reduced heavy episodic drinking by young men ages 18-20 but had no effect on slightly older males ages 22-24 (Carpenter, 2004a). In the context of identifying effects of alcohol use on crime, ZT laws are particularly attractive because (i) adoption of tougher drunk driving policies is arguably uncorrelated with omitted determinants of criminal behavior; (ii) there is substantial variation across states in the timing of policy adoption; and (iii) the laws create sharp predictions about which age groups should have been affected.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-414",
    "question": "Which of the following factors are cited in the text as plausible explanations for the high level of Victorian overseas investment?",
    "options": [
      "A) Irrational bias against domestic securities due to ignorance or prejudice",
      "B) Higher expected returns on foreign assets compared to domestic ones",
      "C) Diversification benefits from low covariance between domestic and foreign assets",
      "D) Government policies mandating a certain percentage of investments abroad"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "That’s_Where_The_Money_Was_Foreign_Bias_and_English_Investment_Abroad,_1866–1907.md"
    ],
    "question_context": "The text discusses the debate surrounding Victorian Britain's significant capital exports, framing it in terms of market efficiency versus irrational bias. Proponents of market failure argued that British investors were biased against domestic industry due to ignorance or prejudice, leading to capital outflows. On the other hand, rational market proponents suggested that diversification benefits—stemming from low covariance between domestic and foreign assets—could explain the high level of overseas investment. The text highlights that foreign asset returns had a low correlation with domestic returns, and Victorian investors were sophisticated enough to recognize these diversification benefits. Key formula: The optimal portfolio choice under uncertainty depends on both return and risk, captured by the mean-variance frontier, where foreign assets expanded the efficient frontier for British investors.",
    "category": "INCORRECT",
    "question_context_used": "The text discusses the debate surrounding Victorian Britain's significant capital exports, framing it in terms of market efficiency versus irrational bias. Proponents of market failure argued that British investors were biased against domestic industry due to ignorance or prejudice, leading to capital outflows. On the other hand, rational market proponents suggested that diversification benefits—stemming from low covariance between domestic and foreign assets—could explain the high level of overseas investment. The text highlights that foreign asset returns had a low correlation with domestic returns, and Victorian investors were sophisticated enough to recognize these diversification benefits. Key formula: The optimal portfolio choice under uncertainty depends on both return and risk, captured by the mean-variance frontier, where foreign assets expanded the efficient frontier for British investors.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1745",
    "question": "In the context of hospital cost function estimation, which of the following factors contribute to specification errors when physician inputs are omitted?",
    "options": [
      "A) The neoclassical theory of the firm does not account for the institutional relationship between hospitals and physicians",
      "B) Hospital data sources (e.g., AHA surveys) do not collect physician labor metrics",
      "C) Physicians' salaries are not directly paid by hospitals, leading to their exclusion",
      "D) Returns to scale are biased due to the omitted variable problem"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Bays-SpecificationErrorEstimation-1980.md"
    ],
    "question_context": "Estimates of cost functions for hospitals have centered largely on investigating the existence and nature of scale economies. Most studies have concluded that average cost declines slightly with increases in hospital size up to approximately 200 beds with either constant or increasing costs in higher size ranges. However, study of the scale efficiency of hospitals has been hampered by the special relationship between hospitals and their admitting physicians. Hospitals typically grant admitting privileges to physicians who then have authority to admit patients and direct the usage of hospitals’ physical capital and employees. Because cost function estimates of American hospitals have ignored the physician input, the results may be biased by the exclusion of an important variable.\n\nOmitting a significant variable from the production function will cause returns to scale to be over- or under-estimated depending upon whether the excluded variable varies greater or less than proportionately to changes in the included variables (see Griliches (1957)). Under cost minimizing behavior these biases will also be reflected in the estimated cost function.",
    "category": "INCORRECT",
    "question_context_used": "Estimates of cost functions for hospitals have centered largely on investigating the existence and nature of scale economies. Most studies have concluded that average cost declines slightly with increases in hospital size up to approximately 200 beds with either constant or increasing costs in higher size ranges. However, study of the scale efficiency of hospitals has been hampered by the special relationship between hospitals and their admitting physicians. Hospitals typically grant admitting privileges to physicians who then have authority to admit patients and direct the usage of hospitals’ physical capital and employees. Because cost function estimates of American hospitals have ignored the physician input, the results may be biased by the exclusion of an important variable.\n\nOmitting a significant variable from the production function will cause returns to scale to be over- or under-estimated depending upon whether the excluded variable varies greater or less than proportionately to changes in the included variables (see Griliches (1957)). Under cost minimizing behavior these biases will also be reflected in the estimated cost function.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4321",
    "question": "In the context of coordination games with local matching rules, which of the following factors are critical for rapid convergence to the risk-dominant equilibrium?",
    "options": [
      "A) High degree of overlap in players' neighbor groups",
      "B) Heterogeneity in players' payoff distributions",
      "C) Uniform matching across the entire population",
      "D) Large number of neighbors per player (e.g., 12 neighbors)"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Ellison-LearningLocalInteraction-1993.md"
    ],
    "question_context": "The paper examines coordination games where players interact with a subset of neighbors, contrasting uniform and local matching rules. Key formula: The payoff to player $i$ is given by $u_{i}(a_{i t},a_{-i t})=\\sum_{j\\neq i}\\pi_{i j}g\\big(a_{i t},a_{j t}\\big)$, where $(A,A)$ is the risk-dominant equilibrium when $(a-d)>(b-c)$. Rapid convergence to this equilibrium depends on the structure of local interactions. For example, with two-neighbor matching, evolutionary forces lead to quick coordination, whereas uniform matching results in slow convergence dominated by historical factors. Heterogeneity in payoffs also accelerates convergence by enabling smaller stable clusters of strategy $A$ adopters.",
    "category": "CORRECT",
    "question_context_used": "The paper examines coordination games where players interact with a subset of neighbors, contrasting uniform and local matching rules. Key formula: The payoff to player $i$ is given by $u_{i}(a_{i t},a_{-i t})=\\sum_{j\\neq i}\\pi_{i j}g\\big(a_{i t},a_{j t}\\big)$, where $(A,A)$ is the risk-dominant equilibrium when $(a-d)>(b-c)$. Rapid convergence to this equilibrium depends on the structure of local interactions. For example, with two-neighbor matching, evolutionary forces lead to quick coordination, whereas uniform matching results in slow convergence dominated by historical factors. Heterogeneity in payoffs also accelerates convergence by enabling smaller stable clusters of strategy $A$ adopters.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-280",
    "question": "In the context of the recursive model of educational advancement, which of the following statements are correct regarding the parameters and assumptions?",
    "options": [
      "A) The parameter γ_k represents the effect of an individual's own date of birth on their school performance.",
      "B) The parameter β_k captures exogenous effects related to the average maturity of children in the neighborhood.",
      "C) The assumption that β_15 and γ_15 are negligible is necessary to separate endogenous from exogenous effects.",
      "D) The fixed effect ω_n is uncorrelated with both υ and E(υ|n) under the model's assumptions."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Close_Neighbours_Matter_Neighbourhood_Effects_on_Early_Performance_at_School.md"
    ],
    "question_context": "The recursive model of educational advancement is defined by the equation: \n\n$$ y_{k}=\\mathsf{\\alpha}_{\\mathsf{A}-1}\\mathrm{E}\\bigl(y_{k-1}|n\\bigr)+\\beta_{k-1}\\mathrm{E}\\bigl(v|n\\bigr)+\\gamma_{k-1}v+\\theta_{k-1}y_{k-1}+\\epsilon_{n}+u, $$\n\nwhere υ represents date of birth within the year, n the neighborhood, ε_n a neighborhood fixed effect, and u omitted individual characteristics. The γ_k parameters represent the effect of own date of birth on school performance, while β_k captures exogenous effects related to neighborhood maturity. The model assumes that υ and E(υ|n) are uncorrelated with ε_n and u, enabling identification of social effects. A key assumption for separating endogenous from exogenous effects is that β_15 and γ_15 are negligible, implying date of birth within the year has no direct effect on late school transitions.",
    "category": "INCORRECT",
    "question_context_used": "The recursive model of educational advancement is defined by the equation: \n\n$$ y_{k}=\\mathsf{\\alpha}_{\\mathsf{A}-1}\\mathrm{E}\\bigl(y_{k-1}|n\\bigr)+\\beta_{k-1}\\mathrm{E}\\bigl(v|n\\bigr)+\\gamma_{k-1}v+\\theta_{k-1}y_{k-1}+\\epsilon_{n}+u, $$\n\nwhere υ represents date of birth within the year, n the neighborhood, ε_n a neighborhood fixed effect, and u omitted individual characteristics. The γ_k parameters represent the effect of own date of birth on school performance, while β_k captures exogenous effects related to neighborhood maturity. The model assumes that υ and E(υ|n) are uncorrelated with ε_n and u, enabling identification of social effects. A key assumption for separating endogenous from exogenous effects is that β_15 and γ_15 are negligible, implying date of birth within the year has no direct effect on late school transitions.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2941",
    "question": "In the context of portfolio-riskier distributions and risk-averse agents, which of the following statements are correct regarding the conditions necessary for a distribution G to be portfolio-riskier than a distribution F?",
    "options": [
      "A) The optimal fraction of wealth invested in the risky asset under G must be less than or equal to that under F for all concave utilities.",
      "B) Second-Order Stochastic Dominance is sufficient to ensure that G is portfolio-riskier than F.",
      "C) The relation between F and G must hold for all concave utilities, not just a subset.",
      "D) Normal, Uniform, and shifted Exponential distributions can be portfolio ordered by their variances."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Mean-preserving Portfolio Dominance.md"
    ],
    "question_context": "The analysis of attitudes towards risk is concerned with relations between utility functions. The notion of more risk-averse, suggested independently by Arrow and Pratt, is a technical relation between utilities which characterizes the behaviour of risk-averse agents under carefully chosen canonical conditions. A utility function $\\pmb{V}$ is (Pratt) more risk-averse than a utility function $U^{1}$ if every risky asset has a lower certainty equivalent under $\\boldsymbol{v}$ than under U. V is (Arrow) more risk-averse than $U$ if for every two assets, as in the Arrow setting, the optimal portfolio has a larger fraction of wealth invested in the risky asset when the utility is $U.$ As is well known, both notions yield the same concept of more risk-averse, which amounts to $V$ being a concave non-decreasing function of $U.$\n\nA natural extension of this analysis from attitudes towards risk to risk itself, from the point of view which motivated Pratt, is to characterize the relation between distributions $F$ and $_G$ under which the certainty equivalent of a $F\\cdot$ -distributed risky asset exceeds that of a $_G$ -distributed asset for all utilities which are non-decreasing and concave. SecondOrder Stochastic Dominance does it. In pursuing this analysis for the conditions set by Arrow, we wish to have a relation between distributions $F$ and $_{G}$ which is necessary and sufficient to induce all risk-averse agents to invest less in the risky asset when the latter is distributed G. Second-Order Stochastic Dominance is too weak to achieve this goal, which shows that a full duality is impossible and calls for the derivation of a new relation.",
    "category": "INCORRECT",
    "question_context_used": "The analysis of attitudes towards risk is concerned with relations between utility functions. The notion of more risk-averse, suggested independently by Arrow and Pratt, is a technical relation between utilities which characterizes the behaviour of risk-averse agents under carefully chosen canonical conditions. A utility function $\\pmb{V}$ is (Pratt) more risk-averse than a utility function $U^{1}$ if every risky asset has a lower certainty equivalent under $\\boldsymbol{v}$ than under U. V is (Arrow) more risk-averse than $U$ if for every two assets, as in the Arrow setting, the optimal portfolio has a larger fraction of wealth invested in the risky asset when the utility is $U.$ As is well known, both notions yield the same concept of more risk-averse, which amounts to $V$ being a concave non-decreasing function of $U.$\n\nA natural extension of this analysis from attitudes towards risk to risk itself, from the point of view which motivated Pratt, is to characterize the relation between distributions $F$ and $_G$ under which the certainty equivalent of a $F\\cdot$ -distributed risky asset exceeds that of a $_G$ -distributed asset for all utilities which are non-decreasing and concave. SecondOrder Stochastic Dominance does it. In pursuing this analysis for the conditions set by Arrow, we wish to have a relation between distributions $F$ and $_{G}$ which is necessary and sufficient to induce all risk-averse agents to invest less in the risky asset when the latter is distributed G. Second-Order Stochastic Dominance is too weak to achieve this goal, which shows that a full duality is impossible and calls for the derivation of a new relation.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1103",
    "question": "In the context of matching procedures parameterized by $\\phi_{M}\\in[0,1]$, which of the following statements are true regarding the efficiency and stability of the matching?",
    "options": [
      "A) A matching procedure with $\\phi_{M}=1$ guarantees perfect efficiency in the post-graduation market.",
      "B) The expected utility of a high productivity worker is asymptotically $\\phi_{M}\\cdot\\bar{\\upsilon}$ in the post-graduation market.",
      "C) Weak stability is a necessary condition for a matching procedure to be parameterized by $\\phi_{M}$.",
      "D) Centralized mechanisms often result in matchings where $\\phi_{M}=0$, indicating low efficiency."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Social-networks-and-unraveling-in-labor-market_2013_Journal-of-Economic-Theo.md"
    ],
    "question_context": "Key formula: The expected utility of a high productivity worker in the post-graduation market is asymptotically $\\phi_{M}\\cdot\\bar{\\upsilon}$, given by $\\begin{array}{r}{\\operatorname*{lim}_{n_{w}\\to\\infty}\\operatorname*{sup}_{w}|E_{G,M,\\alpha}[u_{w}|H,W_{H}^{0},W_{L}^{0},F^{0}]-\\phi_{M}\\cdot\\overline{{\\nu}}|=0.}\\end{array}$\n\nWe interpret $\\phi_{M}$ as a measure of the efficiency of the matching procedure. In Section 8.1 we discuss further our notion of weak stability and the interpretation of $\\phi_{M}$. We also show that many of the centralized mechanisms studied in the literature generate matchings that are characterized by $\\phi_{M}=1$.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The expected utility of a high productivity worker in the post-graduation market is asymptotically $\\phi_{M}\\cdot\\bar{\\upsilon}$, given by $\\begin{array}{r}{\\operatorname*{lim}_{n_{w}\\to\\infty}\\operatorname*{sup}_{w}|E_{G,M,\\alpha}[u_{w}|H,W_{H}^{0},W_{L}^{0},F^{0}]-\\phi_{M}\\cdot\\overline{{\\nu}}|=0.}\\end{array}$\n\nWe interpret $\\phi_{M}$ as a measure of the efficiency of the matching procedure. In Section 8.1 we discuss further our notion of weak stability and the interpretation of $\\phi_{M}$. We also show that many of the centralized mechanisms studied in the literature generate matchings that are characterized by $\\phi_{M}=1$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-856",
    "question": "In the context of the synthetic cohort methodology used to study income shocks and health outcomes, which of the following statements are supported by the authors' findings?",
    "options": [
      "A) Permanent income shocks have a significant negative effect on all measured health outcomes.",
      "B) Permanent income shocks lead to increased mortality but do not affect other health measures.",
      "C) The health-income gradient observed in cross-sectional studies is driven by permanent income shocks.",
      "D) Permanent income shocks influence health behavior but not direct health outcomes."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Adda-IMPACTINCOMESHOCKS-2009.md"
    ],
    "question_context": "The authors employ a synthetic cohort methodology to analyze the effects of permanent income shocks on health outcomes. Their model decomposes health and income shocks into transitory and permanent components, with the income process specified as: $$Y_{i t}=Y_{i,t-1}+v_{i t}^{Y}+(1-L)u_{i t}^{Y}.$$ The health process is similarly modeled as: $$H_{i t}=H_{i,t-1}+v_{i t}^{H}+(1-L)u_{i t}^{H}.$$ The study finds that permanent income shocks lead to increases in mortality and risky health behavior but have little effect on other health measures. This challenges the conventional wisdom that the health-income gradient is primarily driven by income shocks, suggesting instead that other factors may be at play.",
    "category": "INCORRECT",
    "question_context_used": "The authors employ a synthetic cohort methodology to analyze the effects of permanent income shocks on health outcomes. Their model decomposes health and income shocks into transitory and permanent components, with the income process specified as: $$Y_{i t}=Y_{i,t-1}+v_{i t}^{Y}+(1-L)u_{i t}^{Y}.$$ The health process is similarly modeled as: $$H_{i t}=H_{i,t-1}+v_{i t}^{H}+(1-L)u_{i t}^{H}.$$ The study finds that permanent income shocks lead to increases in mortality and risky health behavior but have little effect on other health measures. This challenges the conventional wisdom that the health-income gradient is primarily driven by income shocks, suggesting instead that other factors may be at play.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\"]\n}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3615",
    "question": "In the context of the stochastic prisoner's dilemma described, which of the following conditions are necessary for cooperation to be supported in period 0?",
    "options": [
      "A) The probability of game A being realized in period 1, p_A, must be less than or equal to 14/19.",
      "B) The discount factor δ must be exactly 2/3.",
      "C) The threatened punishment must be the same in both games A and B.",
      "D) The gain from unilateral defection must be 20 in both games A and B."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Kloosterman - 2020 - Cooperation in stochastic games A prisoner's dilemma experiment.md"
    ],
    "question_context": "The proposition states that cooperation in period 0 is possible in an equilibrium if and only if $p_{A}\\le\\frac{14}{19}\\approx.737$. The result is a generalization of the standard analysis of the infinitely repeated prisoner’s dilemma. For the discount factor $\\delta=2/3$, Grim Trigger is an equilibrium when $B$ is infinitely repeated but not when $A$ is infinitely repeated. The game is an infinitely repeated prisoner’s dilemma starting in period 1, meaning cooperation is possible in periods $t\\geq1$ if and only if $B$ is realized. Cooperation in period 0 is similarly supported, with mutual cooperation in future periods only possible when $B$ is repeated. The incentive constraints for cooperation in period 0 are given by:\n\n$$\n\\begin{array}{r l}{\\mathrm{A}\\mathrm{:~}}&{32+2((p_{A})24+(1-p_{A})62)\\geq52+2(24)}\\ {\\mathrm{B}\\mathrm{:~}}&{62+2((p_{A})24+(1-p_{A})62)\\geq82+2(24)}\\end{array}\n$$\n\nThree features of the proposition are worth emphasizing. First, cooperation in period 0 is possible only when $p_{A}$ is sufficiently small. Second, the cutoff is independent of the game played in period 0. This is due to the fact that the threatened punishment is the same (24 instead of 62 with probability $1-p_{A}$) and the gain from a unilateral defection has been chosen to be 20 in both games (52-32=20 in $A$ and 82-62=20 in $B$).",
    "category": "INCORRECT",
    "question_context_used": "The proposition states that cooperation in period 0 is possible in an equilibrium if and only if $p_{A}\\le\\frac{14}{19}\\approx.737$. The result is a generalization of the standard analysis of the infinitely repeated prisoner’s dilemma. For the discount factor $\\delta=2/3$, Grim Trigger is an equilibrium when $B$ is infinitely repeated but not when $A$ is infinitely repeated. The game is an infinitely repeated prisoner’s dilemma starting in period 1, meaning cooperation is possible in periods $t\\geq1$ if and only if $B$ is realized. Cooperation in period 0 is similarly supported, with mutual cooperation in future periods only possible when $B$ is repeated. The incentive constraints for cooperation in period 0 are given by:\n\n$$\n\\begin{array}{r l}{\\mathrm{A}\\mathrm{:~}}&{32+2((p_{A})24+(1-p_{A})62)\\geq52+2(24)}\\ {\\mathrm{B}\\mathrm{:~}}&{62+2((p_{A})24+(1-p_{A})62)\\geq82+2(24)}\\end{array}\n$$\n\nThree features of the proposition are worth emphasizing. First, cooperation in period 0 is possible only when $p_{A}$ is sufficiently small. Second, the cutoff is independent of the game played in period 0. This is due to the fact that the threatened punishment is the same (24 instead of 62 with probability $1-p_{A}$) and the gain from a unilateral defection has been chosen to be 20 in both games (52-32=20 in $A$ and 82-62=20 in $B$).",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2797",
    "question": "In the context of the production economy model with working capital constraints, which of the following statements accurately describe the feedback effects from spreads to output volatility?",
    "options": [
      "A) The model with working capital constraints implies higher average income volatility due to the impact of spreads on equilibrium labor.",
      "B) Volatility shocks account for approximately 90% of the variability of output in the full model.",
      "C) The interaction between TFP shocks and volatility shocks is key to matching the default frequency in the data.",
      "D) The model without working capital constraints shows no feedback effects from spreads to output volatility due to GHH preferences."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "TIME-VARYING VOLATILITY, DEFAULT, AND THE SOVEREIGN RISK PREMIUM.md"
    ],
    "question_context": "Key findings from the text include the role of working capital constraints in amplifying shocks and their impact on output volatility. The model with working capital constraints shows higher average income volatility compared to the model without such constraints, as changes in spreads affect equilibrium labor. This feedback effect is absent in the model without working capital constraints due to GHH preferences. Additionally, the text highlights that TFP shocks generate about 90% of the variability of output in the full model, with the remaining 10% explained by volatility shocks. The interaction between TFP shocks and volatility shocks is crucial for matching the default frequency observed in the data. The variance decomposition analysis further reveals that volatility shocks play a significant role in explaining the variability of spreads and output volatility.",
    "category": "INCORRECT",
    "question_context_used": "Key findings from the text include the role of working capital constraints in amplifying shocks and their impact on output volatility. The model with working capital constraints shows higher average income volatility compared to the model without such constraints, as changes in spreads affect equilibrium labor. This feedback effect is absent in the model without working capital constraints due to GHH preferences. Additionally, the text highlights that TFP shocks generate about 90% of the variability of output in the full model, with the remaining 10% explained by volatility shocks. The interaction between TFP shocks and volatility shocks is crucial for matching the default frequency observed in the data. The variance decomposition analysis further reveals that volatility shocks play a significant role in explaining the variability of spreads and output volatility.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3031",
    "question": "Based on the intergenerational transmission of natural disaster shocks, which of the following statements are empirically supported by the study?",
    "options": [
      "A) The education of the mother has a significantly larger negative impact on the second generation's years of education than the education of the father when exposed to natural disasters.",
      "B) Tropical cyclones have a statistically significant effect on the education of the next generation only when the mother was affected during school age (ages 5–12).",
      "C) The strongest intergenerational effect caused by floods occurs when the mother is affected during the in utero period, reducing the child's education by 0.478 years compared to non-affected mothers.",
      "D) Natural disasters have no statistically significant effect on the probability of being disabled for the second generation of affected parents."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "The-legacy-of-natural-disasters--The-intergenerational_2017_Journal-of-Devel.md"
    ],
    "question_context": "The study analyzes the long-term intergenerational effects of natural disasters on human capital accumulation, disabilities, and child labor. Key findings include: The strongest intergenerational effect caused by floods occurs in the in utero period. The fact of having a mother affected by a flood while in utero reduces the education of that child by 0.478 in comparison with other children of the same age with non-affected mothers. The effect of having a father affected during his in utero period is to reduce the years of education of his child by 0.02. In the case of earthquakes, this study finds statistically and economically relevant results for the education of the next generation when parents are affected before their first three years of life. According to the results, the effect of having an affected mother is ten times larger than the effect of having an affected father. Column 3 of Tables 6 and 7 shows the effects of tropical cyclones on the education of the next generation. The effects are economically and statistically relevant if the mother was affected during school age (ages 5–12). This paper also observes no statistical difference in the probability of being disabled between those with affected parents and those with nonaffected parents.",
    "category": "INCORRECT",
    "question_context_used": "The study analyzes the long-term intergenerational effects of natural disasters on human capital accumulation, disabilities, and child labor. Key findings include: The strongest intergenerational effect caused by floods occurs in the in utero period. The fact of having a mother affected by a flood while in utero reduces the education of that child by 0.478 in comparison with other children of the same age with non-affected mothers. The effect of having a father affected during his in utero period is to reduce the years of education of his child by 0.02. In the case of earthquakes, this study finds statistically and economically relevant results for the education of the next generation when parents are affected before their first three years of life. According to the results, the effect of having an affected mother is ten times larger than the effect of having an affected father. Column 3 of Tables 6 and 7 shows the effects of tropical cyclones on the education of the next generation. The effects are economically and statistically relevant if the mother was affected during school age (ages 5–12). This paper also observes no statistical difference in the probability of being disabled between those with affected parents and those with nonaffected parents.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-257",
    "question": "Based on the empirical analysis of the LTV limit policy shock, which of the following statements are supported by the evidence?",
    "options": [
      "A) Affected households reduce mortgage borrowing by approximately 9.5 percentage points relative to matched controls.",
      "B) The policy leads to a statistically significant increase in other forms of household debt (e.g., personal loans, credit cards).",
      "C) Household liquid assets decrease by approximately 33 percentage points in the year of the home purchase.",
      "D) The average annual mortgage payment falls by 8.8 percentage points for affected households."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "The-real-effects-of-borrower-based-macroprudential-polic_2024_Journal-of-Mon.md"
    ],
    "question_context": "The study employs a matching estimator to analyze the effects of an LTV limit policy shock on household balance sheets. Key findings include a 4.9 percentage point reduction in LTV ratios among affected households, with mortgage borrowing decreasing by 9.5 percentage points (significant at the 1% level). Households also purchase cheaper homes (5.7 percentage points lower in value). The funding gap is approximately €9k, leading to a 33 percentage point reduction in liquid assets. The policy reduces annual mortgage payments by 8.8 percentage points and lowers both mortgage debt-to-income and total debt-to-income ratios by about ten percentage points, with no evidence of substitution toward other forms of credit. Key formula: The difference in outcomes is measured as $\\Delta Y = Y_{\\text{affected}} - Y_{\\text{control}}$, where $Y$ represents various balance sheet components.",
    "category": "CORRECT",
    "question_context_used": "The study employs a matching estimator to analyze the effects of an LTV limit policy shock on household balance sheets. Key findings include a 4.9 percentage point reduction in LTV ratios among affected households, with mortgage borrowing decreasing by 9.5 percentage points (significant at the 1% level). Households also purchase cheaper homes (5.7 percentage points lower in value). The funding gap is approximately €9k, leading to a 33 percentage point reduction in liquid assets. The policy reduces annual mortgage payments by 8.8 percentage points and lowers both mortgage debt-to-income and total debt-to-income ratios by about ten percentage points, with no evidence of substitution toward other forms of credit. Key formula: The difference in outcomes is measured as $\\Delta Y = Y_{\\text{affected}} - Y_{\\text{control}}$, where $Y$ represents various balance sheet components.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1982",
    "question": "Based on the CSWEP Roster data from April 1980, which of the following statements accurately characterize the status and trends of women economists?",
    "options": [
      "A) Women in economic statistics and business/finance fields had significantly higher proportions of only Bachelor's degrees compared to other fields.",
      "B) The proportion of women economists with Ph.D.s was roughly equal to those who had completed all but their dissertations (A.B.D.).",
      "C) Women in manpower, labor, and welfare fields exhibited higher publication rates than those in economic statistics and business/finance.",
      "D) The Chairperson's Group of Universities had a higher percentage of tenured women economists compared to the broader CSWEP roster."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "CommitteeStatusWomen-1981.md"
    ],
    "question_context": "Table 1 displays the distribution of women economists by highest degree. It is seen that nearly 90 percent of the women on the CSWEP roster have advanced degrees. The distribution of advanced degrees across primary field of specialization indicates that the proportion of women with only a Bachelors' degree is significantly higher than average in only two fields, economic statistics and business and finance, where about 20 percent of women economists have only Bachelors’ degrees. Slightly more than a quarter of women economists have a Masters as their highest degree. The economic statistics and business and finance fields are again the two fields in which this proportion is significantly above the average. Thus, women in these two business-related fields tend to stop their education sooner than do women in the more academic fields of specialization in economics. Roughly half the women in the roster have completed their Ph.D.s. This contrasts with about 13 percent who have all but their doctorates. Thus, nearly four times more women economists have continued their education through the Ph.D. level than have quit before completing their dissertations. Table 3 reveals that more than one-third of the roster members have published one or more books and articles in contrast to just under 40 percent who are known to have no publications. While the known publishers thus do not form the majority of our membership, the proportion of publishing women is certainly high enough to dispel the notion that women tend to teach but not to publish. Not surprisingly, the two fields where women had the least proportion of advanced degrees, economic statistics and business and finance, also display the lowest publication rates. The two fields with the highest proportion of publishing women are the fields of manpower, labor, and welfare. Thus, women who are interested in fields where women's issues play a major role tend to write somewhat more than women with other primary fields of specialization.",
    "category": "CORRECT",
    "question_context_used": "Table 1 displays the distribution of women economists by highest degree. It is seen that nearly 90 percent of the women on the CSWEP roster have advanced degrees. The distribution of advanced degrees across primary field of specialization indicates that the proportion of women with only a Bachelors' degree is significantly higher than average in only two fields, economic statistics and business and finance, where about 20 percent of women economists have only Bachelors’ degrees. Slightly more than a quarter of women economists have a Masters as their highest degree. The economic statistics and business and finance fields are again the two fields in which this proportion is significantly above the average. Thus, women in these two business-related fields tend to stop their education sooner than do women in the more academic fields of specialization in economics. Roughly half the women in the roster have completed their Ph.D.s. This contrasts with about 13 percent who have all but their doctorates. Thus, nearly four times more women economists have continued their education through the Ph.D. level than have quit before completing their dissertations. Table 3 reveals that more than one-third of the roster members have published one or more books and articles in contrast to just under 40 percent who are known to have no publications. While the known publishers thus do not form the majority of our membership, the proportion of publishing women is certainly high enough to dispel the notion that women tend to teach but not to publish. Not surprisingly, the two fields where women had the least proportion of advanced degrees, economic statistics and business and finance, also display the lowest publication rates. The two fields with the highest proportion of publishing women are the fields of manpower, labor, and welfare. Thus, women who are interested in fields where women's issues play a major role tend to write somewhat more than women with other primary fields of specialization.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-651",
    "question": "In the state-space estimation algorithm for regime-switching models, which of the following conditions are necessary for the Kalman filter updating equations to hold?",
    "options": [
      "A) The variance-covariance matrix Q* must be known and non-singular",
      "B) The state transition probabilities must be Markovian",
      "C) The conditional expectations Γₜ|ₜ⁽ⁱ,ʲ⁾ must be linear in the observations",
      "D) The initial values (Γᵣ|ᵣ⁽ⁱ⁾, Υᵣ|ᵣ⁽ⁱ⁾) must be set to unconditional mean and variance"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "06-An Unobserved-Component Model With Switching Permanent and Transitory Innovations.md"
    ],
    "question_context": "The Kalman filter for (8) consists of the following updating equations:\n\n$$\n\\begin{array}{r l}&{\\mathbf{P}_{t l-1}^{(i,j)}=\\mathbf{F}\\mathbf{\\Gamma}_{t-1\\mid t-1}^{(i)},}\\ &{\\mathbf{Y}_{t l-1}^{(i,j)}=\\mathbf{F}\\mathbf{\\Upsilon}_{t l-1\\mid t-1}^{(i)}\\mathbf{F}^{\\prime}+\\mathbf{Q}^{*},}\\ &{\\eta_{t l-1}^{(i,j)}=z_{t}-\\mu_{0}-\\mu_{1}\\mathfrak{F}_{t-1,j}^{\\prime}\\mathbf{1}-\\mathbf{H}_{j}\\mathbf{\\Gamma}_{t\\mid t-1}^{(i,j)},}\\ &{f_{l\\mid t-1}^{(i,j)}=\\mathbf{H}_{j}\\mathbf{Y}_{t l-1}^{(i,j)}\\mathbf{H}_{j}^{\\prime},}\\ &{\\mathbf{\\Gamma}_{t l\\mid t}^{(i,j)}=\\mathbf{\\Gamma}_{t l\\mid t-1}^{(i,j)}+\\mathbf{Y}_{t l-1}^{(i,j)}\\mathbf{H}_{j}^{\\prime}[f_{t\\mid t-1}^{(i,j)}]^{-1}\\eta_{t\\mid t-1}^{(i,j)},}\\ &{\\mathbf{Y}_{t l\\mid t}^{(i,j)}=\\big(\\mathbf{I}-\\mathbf{Y}_{t l\\mid t-1}^{(i,j)}\\mathbf{H}_{j}^{\\prime}[f_{t\\mid t-1}^{(i,j)}]^{-1}\\mathbf{H}_{j}\\big)\\mathbf{Y}_{t l\\mid t-1}^{(i,j)},}\\end{array}\n$$\n\nwhere ${\\bf Q}^{*}$ is the variance–covariance matrix of $\\pmb{\\varepsilon}_{t}$ that involves the unknown parameter $\\sigma_{\\varepsilon}^{2}$. Following Kim and Nelson (1999), we set the initial values $(\\mathbf{\\Gamma}_{r|r}^{(i)},\\dot{\\mathbf{Y}}_{r|r}^{(i)})$ to the unconditional mean and variance, $\\Gamma_{r|r}^{(i)}=\\overset{.}{\\underset{.}{\\mathbf{0}}}$ and\n\n$$\n\\begin{array}{r}{\\mathrm{vec}\\big(\\mathbf{Y}_{r|r}^{(i)}\\big)=(\\mathbf{I}-\\mathbf{F}\\otimes\\mathbf{F})^{-1}\\mathrm{vec}(\\mathbf{Q}^{*}),}\\end{array}\n$$",
    "category": "INCORRECT",
    "question_context_used": "The Kalman filter for (8) consists of the following updating equations:\n\n$$\n\\begin{array}{r l}&{\\mathbf{P}_{t l-1}^{(i,j)}=\\mathbf{F}\\mathbf{\\Gamma}_{t-1\\mid t-1}^{(i)},}\\ &{\\mathbf{Y}_{t l-1}^{(i,j)}=\\mathbf{F}\\mathbf{\\Upsilon}_{t l-1\\mid t-1}^{(i)}\\mathbf{F}^{\\prime}+\\mathbf{Q}^{*},}\\ &{\\eta_{t l-1}^{(i,j)}=z_{t}-\\mu_{0}-\\mu_{1}\\mathfrak{F}_{t-1,j}^{\\prime}\\mathbf{1}-\\mathbf{H}_{j}\\mathbf{\\Gamma}_{t\\mid t-1}^{(i,j)},}\\ &{f_{l\\mid t-1}^{(i,j)}=\\mathbf{H}_{j}\\mathbf{Y}_{t l-1}^{(i,j)}\\mathbf{H}_{j}^{\\prime},}\\ &{\\mathbf{\\Gamma}_{t l\\mid t}^{(i,j)}=\\mathbf{\\Gamma}_{t l\\mid t-1}^{(i,j)}+\\mathbf{Y}_{t l-1}^{(i,j)}\\mathbf{H}_{j}^{\\prime}[f_{t\\mid t-1}^{(i,j)}]^{-1}\\eta_{t\\mid t-1}^{(i,j)},}\\ &{\\mathbf{Y}_{t l\\mid t}^{(i,j)}=\\big(\\mathbf{I}-\\mathbf{Y}_{t l\\mid t-1}^{(i,j)}\\mathbf{H}_{j}^{\\prime}[f_{t\\mid t-1}^{(i,j)}]^{-1}\\mathbf{H}_{j}\\big)\\mathbf{Y}_{t l\\mid t-1}^{(i,j)},}\\end{array}\n$$\n\nwhere ${\\bf Q}^{*}$ is the variance–covariance matrix of $\\pmb{\\varepsilon}_{t}$ that involves the unknown parameter $\\sigma_{\\varepsilon}^{2}$. Following Kim and Nelson (1999), we set the initial values $(\\mathbf{\\Gamma}_{r|r}^{(i)},\\dot{\\mathbf{Y}}_{r|r}^{(i)})$ to the unconditional mean and variance, $\\Gamma_{r|r}^{(i)}=\\overset{.}{\\underset{.}{\\mathbf{0}}}$ and\n\n$$\n\\begin{array}{r}{\\mathrm{vec}\\big(\\mathbf{Y}_{r|r}^{(i)}\\big)=(\\mathbf{I}-\\mathbf{F}\\otimes\\mathbf{F})^{-1}\\mathrm{vec}(\\mathbf{Q}^{*}),}\\end{array}\n$$",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4338",
    "question": "Under the representativeness heuristic in financial markets, which of the following statements accurately describe investor behavior during the accumulation of bad news?",
    "options": [
      "A) Investors initially under-react to bad news due to the dominance of good news in their representation.",
      "B) Investors immediately adjust their beliefs proportionally to the ratio of bad to good news.",
      "C) Investors over-react to bad news once it becomes representative, leading to exaggerated pessimism.",
      "D) Investors maintain rational expectations throughout, adjusting beliefs linearly with new information."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Gennaioli-NeglectedRisksPsychology-2015.md"
    ],
    "question_context": "Under representativeness, a few disappointing bits of data intermixed with good news are not enough for neglected risks to become salient. Enough bad news must accumulate for the bad scenario to become representative. As a consequence, investors initially under-react to bad news, but when enough bad news accumulates, investors over-react because their representation changes, causing them to overestimate the probability of the low state. There is an exact parallel here between the psychology of booms and busts.\n\nPROPOSITION 2 (Under- and Over-reaction): Suppose that psychological biases are severe, namely δ→0. Then the market reacts to news (nₗ¹, nₕ¹) as follows: (i) If nₗ¹/nₕ¹ < πₗ⁰/(πₕ⁰ + nₕ⁰), the market neglects the bad news and still believes that debt is perfectly safe, namely πₕʳ² = 1 and pʳ(d₁ʳ) = yₕ. (ii) If nₗ¹/nₕ¹ > πₗ⁰/(πₕ⁰ + nₕ⁰), the market over-reacts to the bad news. It now believes that yₗ occurs with probability 1, namely πₕ² = 0 and pʳ(d₁ʳ) = yₗ.",
    "category": "CORRECT",
    "question_context_used": "Under representativeness, a few disappointing bits of data intermixed with good news are not enough for neglected risks to become salient. Enough bad news must accumulate for the bad scenario to become representative. As a consequence, investors initially under-react to bad news, but when enough bad news accumulates, investors over-react because their representation changes, causing them to overestimate the probability of the low state. There is an exact parallel here between the psychology of booms and busts.\n\nPROPOSITION 2 (Under- and Over-reaction): Suppose that psychological biases are severe, namely δ→0. Then the market reacts to news (nₗ¹, nₕ¹) as follows: (i) If nₗ¹/nₕ¹ < πₗ⁰/(πₕ⁰ + nₕ⁰), the market neglects the bad news and still believes that debt is perfectly safe, namely πₕʳ² = 1 and pʳ(d₁ʳ) = yₕ. (ii) If nₗ¹/nₕ¹ > πₗ⁰/(πₕ⁰ + nₕ⁰), the market over-reacts to the bad news. It now believes that yₗ occurs with probability 1, namely πₕ² = 0 and pʳ(d₁ʳ) = yₗ.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1877",
    "question": "In the described economy, which of the following conditions are necessary for the representative household's optimization problem to yield unbounded growth?",
    "options": [
      "A) The production function must exhibit constant returns to scale in capital",
      "B) The discount factor must satisfy β ∈ (0,1) and A > 1/β",
      "C) The utility function must be strictly concave and twice continuously differentiable",
      "D) The capital stock must not depreciate over time"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Ireland-MoneyGrowthAlternative-1994.md"
    ],
    "question_context": "The production function is given by y_t = A K_t with A > 1/β, where K_t is a composite of physical and human capital. Rebelo (1991) shows that linear production functions capture major features of more complicated convex models of endogenous growth while preserving analytic simplicity. The production function (including the restriction Aβ > 1), along with the assumption that capital does not depreciate, implies that unbounded growth is feasible (Jones and Manuelli, 1990).\n\nHouseholds have preferences represented by the additively time-separable utility function U({c_t}) = ∑_{t=1}^∞ β^t {∫_0^1 u[c_t(i)] di}, where u is strictly increasing, strictly concave, and twice continuously differentiable with lim_{c→0} u'(c) = ∞. The discount factor is β ∈ (0,1).",
    "category": "INCORRECT",
    "question_context_used": "The production function is given by y_t = A K_t with A > 1/β, where K_t is a composite of physical and human capital. Rebelo (1991) shows that linear production functions capture major features of more complicated convex models of endogenous growth while preserving analytic simplicity. The production function (including the restriction Aβ > 1), along with the assumption that capital does not depreciate, implies that unbounded growth is feasible (Jones and Manuelli, 1990).\n\nHouseholds have preferences represented by the additively time-separable utility function U({c_t}) = ∑_{t=1}^∞ β^t {∫_0^1 u[c_t(i)] di}, where u is strictly increasing, strictly concave, and twice continuously differentiable with lim_{c→0} u'(c) = ∞. The discount factor is β ∈ (0,1).",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4851",
    "question": "In the model of market competition with limited comparability, which of the following conditions are necessary for firms to earn max-min profits in symmetric equilibrium?",
    "options": [
      "A) The comparability structure satisfies weighted regularity",
      "B) The comparison probability is always 1 (Bertrand competition)",
      "C) The comparability structure is order-independent",
      "D) Regulatory interventions enforce standardized price formats"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Price Competition Under Limited Comparability.md"
    ],
    "question_context": "Our yardstick for the competitiveness of the market outcome is equilibrium profits. In particular, we ask whether market forces drive profits down to the minimal level that firms can secure given the comparability structure, namely the max-min profit. Our main result is that in any symmetric equilibrium, firms earn max-min profits if and only if the comparability structure is weighted-regular. In the limit case of Bertrand competition, weighted regularity trivially holds (the comparison probability is always 1), max-min profits are equal to 0 and obtained in equilibrium. When the consumers’ ability to make comparisons is limited, firms can typically secure positive profits, so that the market outcome cannot be strictly competitive. However, a market outcome that induces max-min profits is competitive in a 'constrained,' second-best sense: no extra rents are gained in addition to the minimum caused by the consumer’s bounded rationality. Our result establishes a tight link between the emergence of a 'constrained competitive' outcome and the comparability structure’s potential 'format neutrality.'\n\nThis result is also useful for deriving an equilibrium characterization for specific classes of comparability structures. For instance, bisymmetric structures partition the set of formats into two categories, so that comparability depends only on the category to which formats belong. We obtain a closed-form characterization of the (unique) symmetric Nash equilibrium. We use this characterization to convey two important lessons: first, regulatory interventions that enhance comparability may lead to a less competitive market outcome; second, the relationship between comparability and consumer switching is subtle.",
    "category": "CORRECT",
    "question_context_used": "Our yardstick for the competitiveness of the market outcome is equilibrium profits. In particular, we ask whether market forces drive profits down to the minimal level that firms can secure given the comparability structure, namely the max-min profit. Our main result is that in any symmetric equilibrium, firms earn max-min profits if and only if the comparability structure is weighted-regular. In the limit case of Bertrand competition, weighted regularity trivially holds (the comparison probability is always 1), max-min profits are equal to 0 and obtained in equilibrium. When the consumers’ ability to make comparisons is limited, firms can typically secure positive profits, so that the market outcome cannot be strictly competitive. However, a market outcome that induces max-min profits is competitive in a 'constrained,' second-best sense: no extra rents are gained in addition to the minimum caused by the consumer’s bounded rationality. Our result establishes a tight link between the emergence of a 'constrained competitive' outcome and the comparability structure’s potential 'format neutrality.'\n\nThis result is also useful for deriving an equilibrium characterization for specific classes of comparability structures. For instance, bisymmetric structures partition the set of formats into two categories, so that comparability depends only on the category to which formats belong. We obtain a closed-form characterization of the (unique) symmetric Nash equilibrium. We use this characterization to convey two important lessons: first, regulatory interventions that enhance comparability may lead to a less competitive market outcome; second, the relationship between comparability and consumer switching is subtle.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2814",
    "question": "In the context of bias-adjusting methods for index estimation, which of the following statements are supported by the simulation results?",
    "options": [
      "A) The Beta-1 correction performs optimally when the data-generating process (DGP) follows a Beta distribution.",
      "B) The CT-adjusted dissimilarity index is always lower than D(F) except when the true distribution is a discrete distribution with support {0, 0.1, 1}.",
      "C) The ABW method consistently outperforms all other methods for the Theil index across all DGPs.",
      "D) The Beta-mixture method generally provides the closest estimates to I(F) for most DGPs and indices."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "05-Measuring_Segregation_When_Units_are_Small__A_Parametric_Approach.md"
    ],
    "question_context": "The simulations assess the performance of various bias-adjusting methods for index estimation, including direct estimates, Beta-parametric methods (Beta-1, Beta-2), and CT and ABW methods. Seven DGPs with expectation 0.1 are tested, including Beta, mixture Beta, discrete, truncated normal, and Weibull distributions. Key findings include that direct indices suffer from large biases, while adjusting methods generally improve estimates. The Beta-1 correction excels when the DGP is Beta-distributed, and the Beta-mixture method is most frequently the best performer. The CT-adjusted dissimilarity index is typically lower than D(F), except for the discrete distribution D_{0,.1,1}. The ABW method shows upward bias in most cases and is less efficient for the Theil index.",
    "category": "CORRECT",
    "question_context_used": "The simulations assess the performance of various bias-adjusting methods for index estimation, including direct estimates, Beta-parametric methods (Beta-1, Beta-2), and CT and ABW methods. Seven DGPs with expectation 0.1 are tested, including Beta, mixture Beta, discrete, truncated normal, and Weibull distributions. Key findings include that direct indices suffer from large biases, while adjusting methods generally improve estimates. The Beta-1 correction excels when the DGP is Beta-distributed, and the Beta-mixture method is most frequently the best performer. The CT-adjusted dissimilarity index is typically lower than D(F), except for the discrete distribution D_{0,.1,1}. The ABW method shows upward bias in most cases and is less efficient for the Theil index.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1501",
    "question": "In the context of advertising's effect on social welfare, which of the following conditions indicate that advertising is optimally supplied by the free market?",
    "options": [
      "A) The advertising elasticity of the characteristic productivity of X equals the advertising elasticity of the price of X",
      "B) The shadow price of advertising is zero at the profit-maximizing level",
      "C) The market price of X increases proportionately more than the characteristic productivity of X",
      "D) The price of the characteristic Z remains unaffected by advertising"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Nichols-AdvertisingEconomicWelfare-1985.md"
    ],
    "question_context": "The model analyzes advertising's effect on social welfare by differentiating consumer's surplus with respect to advertising at the profit-maximizing level. Key formula: \n\n$$\n(d U/d A)/\\lambda=\\big(P_{x}X^{\\ast}/A\\big)\\big[E_{g,A}-E_{P_{x},A}\\big].\n$$\n\nHere, $E_{g,A}$ is the advertising elasticity of the characteristic productivity of $X$, and $E_{P_{x},A}$ is the advertising elasticity of the price of $X$. Advertising is optimally supplied when $E_{g,A} = E_{P_{x},A}$, meaning the market price and characteristic productivity increase in equal proportions. Alternatively, when advertising does not affect $P_z$ (the price of characteristic $Z$), $E_{P_{z},A}=0$, indicating optimal supply. This condition aligns with Stigler and Becker's definition of a perfectly competitive characteristic market.",
    "category": "CORRECT",
    "question_context_used": "The model analyzes advertising's effect on social welfare by differentiating consumer's surplus with respect to advertising at the profit-maximizing level. Key formula: \n\n$$\n(d U/d A)/\\lambda=\\big(P_{x}X^{\\ast}/A\\big)\\big[E_{g,A}-E_{P_{x},A}\\big].\n$$\n\nHere, $E_{g,A}$ is the advertising elasticity of the characteristic productivity of $X$, and $E_{P_{x},A}$ is the advertising elasticity of the price of $X$. Advertising is optimally supplied when $E_{g,A} = E_{P_{x},A}$, meaning the market price and characteristic productivity increase in equal proportions. Alternatively, when advertising does not affect $P_z$ (the price of characteristic $Z$), $E_{P_{z},A}=0$, indicating optimal supply. This condition aligns with Stigler and Becker's definition of a perfectly competitive characteristic market.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4759",
    "question": "In the context of continuous-time econometric models, which of the following statements are true regarding the estimation of structural parameters and initial state vectors?",
    "options": [
      "A) The exact discrete model provides a basis for Gaussian estimation by maximizing the Gaussian likelihood, regardless of the distribution of innovations.",
      "B) The assumption of stationarity is generally justified for time series used in econometric work.",
      "C) The Kalman filter algorithm is computationally less costly than the algorithm proposed in the paper for all sample sizes.",
      "D) The initial state vector cannot be consistently estimated as the sample size tends to infinity."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Bergstrom-EstimationParametersNonstationary-1985.md"
    ],
    "question_context": "The paper discusses the estimation of structural parameters in continuous-time econometric models, emphasizing the advantages of using exact discrete models derived from the continuous-time formulation. Key formula: The exact Gaussian likelihood function is given by $L(\\theta,\\mu,y)$, where $\\theta$ and $\\mu$ are structural parameters, and $y$ is the initial state vector. The paper highlights that while structural parameter estimates can be consistent under suitable conditions, the initial state vector cannot be consistently estimated as the sample size increases. This is because the influence of the initial state vector on the distribution of observations diminishes over time. The paper also contrasts the proposed algorithm with the Kalman filter, noting computational advantages for sufficiently large samples.",
    "category": "CORRECT",
    "question_context_used": "The paper discusses the estimation of structural parameters in continuous-time econometric models, emphasizing the advantages of using exact discrete models derived from the continuous-time formulation. Key formula: The exact Gaussian likelihood function is given by $L(\\theta,\\mu,y)$, where $\\theta$ and $\\mu$ are structural parameters, and $y$ is the initial state vector. The paper highlights that while structural parameter estimates can be consistent under suitable conditions, the initial state vector cannot be consistently estimated as the sample size increases. This is because the influence of the initial state vector on the distribution of observations diminishes over time. The paper also contrasts the proposed algorithm with the Kalman filter, noting computational advantages for sufficiently large samples.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1041",
    "question": "In the context of the Chilean Social Protection Survey (EPS), which of the following factors are directly associated with the measurement and accuracy of pension wealth?",
    "options": [
      "A) Annual government statements outlining pension system accruals",
      "B) Self-reported market values of housing minus mortgage debt",
      "C) Density of pension contributions from age 18 to survey date",
      "D) Mandatory 10% salary contribution to defined-contribution pension plans"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Behrman-FinancialLiteracyAffects-2012.md"
    ],
    "question_context": "The Chilean Social Protection Survey (EPS) provides detailed data on wealth components, including pension wealth, which averages \\$38,600 or 54% of total net wealth. Pension wealth is reported relatively accurately due to annual government statements outlining pension system accruals. The survey also explores channels through which financial literacy and schooling might affect pension wealth, such as the 'density of pension contributions,' defined as the fraction of months an individual contributed to the pension system from age 18 to the survey date. Another channel is whether the individual had attempted to calculate the money needed for retirement. The mandatory defined-contribution scheme (AFP system) requires formal sector employees to contribute at least 10% of their salaries to licensed pension plans.",
    "category": "INCORRECT",
    "question_context_used": "The Chilean Social Protection Survey (EPS) provides detailed data on wealth components, including pension wealth, which averages \\$38,600 or 54% of total net wealth. Pension wealth is reported relatively accurately due to annual government statements outlining pension system accruals. The survey also explores channels through which financial literacy and schooling might affect pension wealth, such as the 'density of pension contributions,' defined as the fraction of months an individual contributed to the pension system from age 18 to the survey date. Another channel is whether the individual had attempted to calculate the money needed for retirement. The mandatory defined-contribution scheme (AFP system) requires formal sector employees to contribute at least 10% of their salaries to licensed pension plans.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1142",
    "question": "Which of the following empirical findings from the analysis of screening contracts in privatization are consistent with the screening hypothesis?",
    "options": [
      "A) The realized profit share is negatively correlated with the buyout price.",
      "B) The government's paper share is positively correlated with the buyout price.",
      "C) Firm performance increases with the buyout price after controlling for firm quality.",
      "D) The buyout price is higher in townships using screening contracts."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Insider-privatization-with-a-tail--the-screening-contra_2004_Journal-of-Deve.md"
    ],
    "question_context": "The analysis reveals that the realized profit share (the tail) is highly correlated with the buyout price, supporting the screening hypothesis. The regression coefficient of the buyout price is negative and significant at the 5% level, indicating that a 10 percentage point increase in the premium rate reduces the realized profit share by 7 percentage points. Additionally, firm fixed-effect models show that post-privatization performance increases with the buyout price even when controlling for firm quality, with a one standard deviation increase in buyout price (0.21) decreasing the firm arrears rate by 20% and increasing value added per worker by 17%. Key formula: $\\pi=\\beta_{0}+V\\beta_{1}+X\\beta_{2}+Z\\beta_{3}+\\epsilon_{1}$, where $V$ represents the buyout price and $\\pi$ measures firm performance.",
    "category": "INCORRECT",
    "question_context_used": "The analysis reveals that the realized profit share (the tail) is highly correlated with the buyout price, supporting the screening hypothesis. The regression coefficient of the buyout price is negative and significant at the 5% level, indicating that a 10 percentage point increase in the premium rate reduces the realized profit share by 7 percentage points. Additionally, firm fixed-effect models show that post-privatization performance increases with the buyout price even when controlling for firm quality, with a one standard deviation increase in buyout price (0.21) decreasing the firm arrears rate by 20% and increasing value added per worker by 17%. Key formula: $\\pi=\\beta_{0}+V\\beta_{1}+X\\beta_{2}+Z\\beta_{3}+\\epsilon_{1}$, where $V$ represents the buyout price and $\\pi$ measures firm performance.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1091",
    "question": "In the context of putty-putty and putty-clay production functions, which of the following statements are supported by the analysis in Abel's paper?",
    "options": [
      "A) A putty-clay technology implies that the capital stock responds more quickly to an increase in output than to a decrease in the cost of capital services.",
      "B) A putty-putty model with convex adjustment costs can generate the same relative speeds of response to output and interest rate changes as a putty-clay model.",
      "C) The elasticity of substitution of capital for labor is always zero ex post in a putty-putty technology.",
      "D) Investment responds more rapidly to wage rate changes than to interest rate changes in a putty-putty model with convex adjustment costs."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Dynamic Adjustment in a Putty-Putty Model Implications for Testing the Putty-Clay Hypothesis.md"
    ],
    "question_context": "A major criticism of much empirical work on investment is that it assumes a putty-putty technology, that is, capital is assumed to be as malleable ex post as it is ex ante. Stated in terms of the elasticity of substitution of capital for labor, a putty-putty production function is based on the assumption that this elasticity is the same ex post as ex ante. Within a putty-putty framework, it is never optimal for a cost minimizing firm to produce with idle capital because current variable costs could always be reduced by substituting idle capital for labor. An alternative to the putty-putty technology, putty-clay, has been introduced by Johansen [1959]. According to the putty-clay hypothesis, the elasticity of substitution of capital for labor is smaller ex post than it is ex ante. An extreme version of putty-clay technology is one in which the firm can choose capital with any desired capital-labor ratio, ex ante. Once the capital is installed in place, the capital-labor ratio is fixed immutably, i.e., the ex post elasticity of substitution of capital for labor is zero. With an extreme putty-clay technology, there can be situations in which it is optimal to have idle capital. If not all units of capital have the same capital-labor ratio, then a high real wage rate could lead the firm to stop using the labor-intensive units of capital.\n\nBischoff [1971a, b] has popularized and produced empirical support for the putty-clay hypothesis. The putty-clay hypothesis implies that the capital stock responds more quickly to an increase in output than to a decrease in the cost of capital services, and Bischoff has found empirical evidence that these predicted relative speeds of response actually occur. This empirical finding is presented as support of the putty-clay hypothesis. In this paper we demonstrate that the same relative speeds of response can be derived from the behavior of an intertemporally optimizing firm with a putty-putty production function and subject to costs of adjustment. This result weakens the implication of the empirical findings. Although the evidence is consistent with a putty-clay technology, it is also consistent with a putty-putty technology.",
    "category": "INCORRECT",
    "question_context_used": "A major criticism of much empirical work on investment is that it assumes a putty-putty technology, that is, capital is assumed to be as malleable ex post as it is ex ante. Stated in terms of the elasticity of substitution of capital for labor, a putty-putty production function is based on the assumption that this elasticity is the same ex post as ex ante. Within a putty-putty framework, it is never optimal for a cost minimizing firm to produce with idle capital because current variable costs could always be reduced by substituting idle capital for labor. An alternative to the putty-putty technology, putty-clay, has been introduced by Johansen [1959]. According to the putty-clay hypothesis, the elasticity of substitution of capital for labor is smaller ex post than it is ex ante. An extreme version of putty-clay technology is one in which the firm can choose capital with any desired capital-labor ratio, ex ante. Once the capital is installed in place, the capital-labor ratio is fixed immutably, i.e., the ex post elasticity of substitution of capital for labor is zero. With an extreme putty-clay technology, there can be situations in which it is optimal to have idle capital. If not all units of capital have the same capital-labor ratio, then a high real wage rate could lead the firm to stop using the labor-intensive units of capital.\n\nBischoff [1971a, b] has popularized and produced empirical support for the putty-clay hypothesis. The putty-clay hypothesis implies that the capital stock responds more quickly to an increase in output than to a decrease in the cost of capital services, and Bischoff has found empirical evidence that these predicted relative speeds of response actually occur. This empirical finding is presented as support of the putty-clay hypothesis. In this paper we demonstrate that the same relative speeds of response can be derived from the behavior of an intertemporally optimizing firm with a putty-putty production function and subject to costs of adjustment. This result weakens the implication of the empirical findings. Although the evidence is consistent with a putty-clay technology, it is also consistent with a putty-putty technology.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4713",
    "question": "In the context of healthcare congestion during epidemics, which of the following statements are true regarding optimal information disclosure policies?",
    "options": [
      "A) Full disclosure is always optimal as it ensures agents have complete information about disease severity.",
      "B) A censorship policy that pools severity levels around the exhaustion level can reduce congestion and improve welfare.",
      "C) The optimal disclosure policy induces a posterior belief where the expected severity level equals the exhaustion level for true severity levels above the exhaustion level.",
      "D) Congestion externalities are the primary reason why full disclosure is never an optimal policy."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Disclosure-in-epidemics_2022_Journal-of-Economic-Theory.md"
    ],
    "question_context": "In an epidemic, healthcare resources (e.g., ventilators, ICU beds and healthcare workers) are scarce and, what is worse, limited resources may not be used efficiently. The surging demand for care often leads to healthcare congestion, which can have significant welfare consequences. As we have seen in the COVID-19 outbreak, government policies to fight the epidemic are targeted mainly at reducing hospital congestion (i.e., by “flattening the curve”).\n\nCongestion occurs if the total number of hospital visits exceeds the healthcare system’s fixed capacity. At the early stage of an epidemic, due to the lack of knowledge about the novel virus, the healthcare sector does not have an effective means to pre-screen visitors. We assume that, in congestion, a hospital randomly admits the visitors up to its capacity. A critical feature of the congestion game is that one agent’s hospital visit imposes congestion externalities on others, because that visit lowers others’ chances of admission during congestion. The welfare gain from pooling states in $[s_{-},s_{+}]$ stems from avoiding congestion when $s\\in(s_{0},s_{+}]$. On the other hand, for $s\\in[s_{-},s_{0})$, this exaggerates the disease severity and, therefore, distorts the agents’ decision making. However, such distortion never results in congestion. We prove that the welfare losses caused by this distortion are dominated by the welfare gain from avoiding congestion, provided that the interval $[s_{-},s_{+}]$ is properly chosen. As such, full disclosure is never an optimal policy.",
    "category": "INCORRECT",
    "question_context_used": "In an epidemic, healthcare resources (e.g., ventilators, ICU beds and healthcare workers) are scarce and, what is worse, limited resources may not be used efficiently. The surging demand for care often leads to healthcare congestion, which can have significant welfare consequences. As we have seen in the COVID-19 outbreak, government policies to fight the epidemic are targeted mainly at reducing hospital congestion (i.e., by “flattening the curve”).\n\nCongestion occurs if the total number of hospital visits exceeds the healthcare system’s fixed capacity. At the early stage of an epidemic, due to the lack of knowledge about the novel virus, the healthcare sector does not have an effective means to pre-screen visitors. We assume that, in congestion, a hospital randomly admits the visitors up to its capacity. A critical feature of the congestion game is that one agent’s hospital visit imposes congestion externalities on others, because that visit lowers others’ chances of admission during congestion. The welfare gain from pooling states in $[s_{-},s_{+}]$ stems from avoiding congestion when $s\\in(s_{0},s_{+}]$. On the other hand, for $s\\in[s_{-},s_{0})$, this exaggerates the disease severity and, therefore, distorts the agents’ decision making. However, such distortion never results in congestion. We prove that the welfare losses caused by this distortion are dominated by the welfare gain from avoiding congestion, provided that the interval $[s_{-},s_{+}]$ is properly chosen. As such, full disclosure is never an optimal policy.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3533",
    "question": "Under the hybrid indexing proposal for Social Security reform, which of the following statements accurately describe its impact on benefit distribution and solvency?",
    "options": [
      "A) It maintains real benefit levels for the bottom 30% of earners by continuing wage-indexing.",
      "B) It applies a uniform price-indexing adjustment to all earners regardless of income level.",
      "C) It reduces the 75-year deficit estimate by approximately 71% of covered payroll.",
      "D) It increases the income-replacement rate for workers in the top 10% of the AIME distribution."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Pozen-ImprovingSocialSecuritys-2004.md"
    ],
    "question_context": "The hybrid indexing proposal aims to address Social Security's solvency while maintaining progressivity. Key formula: Benefits for the bottom 30% of earners are wage-indexed, while higher earners face a sliding scale adjustment from wage to price indexing. The chief actuary estimates this reduces the 75-year deficit from 1.92% to 0.55% of covered payroll, eliminating 71% of the solvency problem. The proposal also introduces a new bend point in the PIA formula to protect low-income workers, ensuring their replacement rates are maintained. Figure 2 demonstrates that by 2080, the benefit inequality between maximum and 30th-percentile earners is reduced from 168% under current law to just 33% under hybrid indexing.",
    "category": "CORRECT",
    "question_context_used": "The hybrid indexing proposal aims to address Social Security's solvency while maintaining progressivity. Key formula: Benefits for the bottom 30% of earners are wage-indexed, while higher earners face a sliding scale adjustment from wage to price indexing. The chief actuary estimates this reduces the 75-year deficit from 1.92% to 0.55% of covered payroll, eliminating 71% of the solvency problem. The proposal also introduces a new bend point in the PIA formula to protect low-income workers, ensuring their replacement rates are maintained. Figure 2 demonstrates that by 2080, the benefit inequality between maximum and 30th-percentile earners is reduced from 168% under current law to just 33% under hybrid indexing.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1833",
    "question": "Based on the hyperbolic discounting framework and welfare policy simulations, which of the following statements are correct regarding time-consistent individuals versus present-biased individuals?",
    "options": [
      "A) Time-consistent individuals exhibit higher welfare participation rates due to reduced delay in welfare use.",
      "B) Present-biased individuals have higher initial utility when they become time-consistent due to immediate cost avoidance.",
      "C) Time-consistent individuals increase net government expenditure substantially due to higher welfare utilization.",
      "D) Present-biased individuals prefer counterfactual policies that worsen commitment problems if they offer more generous benefits."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Welfare Dependence and Self-Control An Empirical Analysis.md"
    ],
    "question_context": "The results reveal how present bias can affect the pattern and efficiency of intertemporal decisions. Column 1 represents the baseline scenario, in which individuals make decisions under the original discounting estimates. Column 2 assumes individuals are naive $(\\tilde{\\beta}_{i}{=}1)$. Column 3 assumes individuals are time-consistent ($\\tilde{\\beta}_{i}=\\beta_{i}=1$). Simulation is conducted for control group individuals under their policy environment. In all simulations, each decision period represents a quarter (3 months), but all the variables are expressed in monthly units unless otherwise stated. The top panel reports average outcomes over the full horizon.42\n\nIndividuals have no commitment problem when they are time-consistent (column 3). The average employment and welfare participation rates increase to 48.6 and $50.3\\%$, respectively. Because individuals no longer delay employment and welfare use (both incur large initial costs), the proportion of idle individuals reduces from $32.5\\%$ to $21.8\\%; however, many workers become welfare recipients to receive more government benefits. As a whole, earnings increase slightly, and net government expenditure increases substantially to $\\$286.6$. There is a moderate improvement in utility of $\\$23.6$.",
    "category": "INCORRECT",
    "question_context_used": "The results reveal how present bias can affect the pattern and efficiency of intertemporal decisions. Column 1 represents the baseline scenario, in which individuals make decisions under the original discounting estimates. Column 2 assumes individuals are naive $(\\tilde{\\beta}_{i}{=}1)$. Column 3 assumes individuals are time-consistent ($\\tilde{\\beta}_{i}=\\beta_{i}=1$). Simulation is conducted for control group individuals under their policy environment. In all simulations, each decision period represents a quarter (3 months), but all the variables are expressed in monthly units unless otherwise stated. The top panel reports average outcomes over the full horizon.42\n\nIndividuals have no commitment problem when they are time-consistent (column 3). The average employment and welfare participation rates increase to 48.6 and $50.3\\%$, respectively. Because individuals no longer delay employment and welfare use (both incur large initial costs), the proportion of idle individuals reduces from $32.5\\%$ to $21.8\\%; however, many workers become welfare recipients to receive more government benefits. As a whole, earnings increase slightly, and net government expenditure increases substantially to $\\$286.6$. There is a moderate improvement in utility of $\\$23.6$.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1496",
    "question": "Based on the analysis of cattle ownership and transactions during the Rwandan genocide, which of the following statements are empirically supported?",
    "options": [
      "A) Tutsi-headed households sold cattle at higher rates than Hutu-headed households in 1994 as an ex-post consumption smoothing strategy.",
      "B) The nominal price of cattle in 1994 was approximately 11,500 RWF, significantly lower than the average price during 1991–2001.",
      "C) Cattle sales in 1994 were primarily motivated by the need to purchase food or due to insecurity, accounting for over 80% of sales.",
      "D) Ethnicity played no role in cattle transactions during peace years, but was a significant factor in 1994."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Household-coping-in-war--and-peacetime--Cattle-sa_2009_Journal-of-Developmen.md"
    ],
    "question_context": "The data show that in 1994, the number of cattle sold was high compared to other years, with prices dropping to approximately 11,500 RWF per animal, far below the 30,000 RWF average. Over 80% of sales were due to food needs or insecurity, contrasting sharply with peacetime motives like investment or stock adjustment. Formula: The shock index was calculated as the number of shocks that hit household $i$ in year $t$ , divided by 5 (the maximum shocks reported in a single year), yielding values between 0 and 1. Ethnic differences in transactions were negligible in peace years but pronounced in 1994, with only 3.4% of Tutsi-headed households selling cattle versus 23.3% of Hutu-headed households.",
    "category": "CORRECT",
    "question_context_used": "The data show that in 1994, the number of cattle sold was high compared to other years, with prices dropping to approximately 11,500 RWF per animal, far below the 30,000 RWF average. Over 80% of sales were due to food needs or insecurity, contrasting sharply with peacetime motives like investment or stock adjustment. Formula: The shock index was calculated as the number of shocks that hit household $i$ in year $t$ , divided by 5 (the maximum shocks reported in a single year), yielding values between 0 and 1. Ethnic differences in transactions were negligible in peace years but pronounced in 1994, with only 3.4% of Tutsi-headed households selling cattle versus 23.3% of Hutu-headed households.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1470",
    "question": "According to Cutler and Katz's analysis of income and consumption inequality in the 1980s, which of the following factors are identified as potential biases in measuring true changes in economic well-being?",
    "options": [
      "A) Changes in in-kind transfers affecting resource distribution without altering measured money income",
      "B) Systematic misreporting of income in self-reported surveys",
      "C) The use of current money income instead of permanent income or consumption as a measure of resource distribution",
      "D) The exclusion of capital gains from standard income measures"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Cutler-RisingInequalityChanges-1992.md"
    ],
    "question_context": "The 1980's were a period of substantial increase in family income inequality in the United States. Between 1980 and 1990 the share of income accruing to the poorest quintile of the family income distribution fell from 5.2 percent to 4.6 percent; the share to the richest quintile, in contrast, rose from 41.5 percent to 44.3 percent. Although income statistics suggest a deterioration in living standards, some researchers question whether income data accurately capture changes in material well-being. Changes in in-kind transfers can change the distribution of economic resources without commensurate changes in measured money income. Systematic misreporting of income may also make changes in self-reported income unreliable. Economic theory suggests permanent income or consumption is a more accurate measure than current money income.",
    "category": "INCORRECT",
    "question_context_used": "The 1980's were a period of substantial increase in family income inequality in the United States. Between 1980 and 1990 the share of income accruing to the poorest quintile of the family income distribution fell from 5.2 percent to 4.6 percent; the share to the richest quintile, in contrast, rose from 41.5 percent to 44.3 percent. Although income statistics suggest a deterioration in living standards, some researchers question whether income data accurately capture changes in material well-being. Changes in in-kind transfers can change the distribution of economic resources without commensurate changes in measured money income. Systematic misreporting of income may also make changes in self-reported income unreliable. Economic theory suggests permanent income or consumption is a more accurate measure than current money income.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1107",
    "question": "Under the conditions of Lemma 2, which of the following statements are true regarding the asymptotic behavior of the labor market model?",
    "options": [
      "A) The proportion of high productivity workers not hired at S=-1 converges to a positive constant as nw→∞.",
      "B) The number of firms that did not hire at S=-1 grows linearly with nw.",
      "C) The difference between the number of unmatched firms and high productivity workers grows sublinearly with nw.",
      "D) The probability that the normalized difference exceeds a positive ζ converges to 1 as nw→∞."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Social-networks-and-unraveling-in-labor-market_2013_Journal-of-Economic-Theo.md"
    ],
    "question_context": "Lemma 2 states that under scarcity of high productivity workers and given network consistency conditions, there exists ζ>0 such that: \n\n$$\\operatorname*{Pr}\\biggl(\\operatorname*{lim}_{n_{w}\\to\\infty}\\frac{n_{f}^{\\widehat{G}(\\theta_{W},\\theta_{F},n_{w})}(0,\\alpha)-n_{w}^{\\widehat{G}(\\theta_{W},\\theta_{F},n_{w})}(H,0,\\alpha)}{n_{w}}>\\zeta\\biggr)=1.$$\n\nThe proof involves analyzing the proportions of workers receiving signals and being hired, using the strong law of large numbers to establish convergence results. Key variables include pw⁻¹ⁿʷ(s_w) for signal proportions and γ̂ᴳ(·) for early hires. The inequality (19) shows the upper bound on high productivity workers not hired at S=-1, while subsequent steps combine market tightness and firm-worker ratios to derive the asymptotic result.",
    "category": "CORRECT",
    "question_context_used": "Lemma 2 states that under scarcity of high productivity workers and given network consistency conditions, there exists ζ>0 such that: \n\n$$\\operatorname*{Pr}\\biggl(\\operatorname*{lim}_{n_{w}\\to\\infty}\\frac{n_{f}^{\\widehat{G}(\\theta_{W},\\theta_{F},n_{w})}(0,\\alpha)-n_{w}^{\\widehat{G}(\\theta_{W},\\theta_{F},n_{w})}(H,0,\\alpha)}{n_{w}}>\\zeta\\biggr)=1.$$\n\nThe proof involves analyzing the proportions of workers receiving signals and being hired, using the strong law of large numbers to establish convergence results. Key variables include pw⁻¹ⁿʷ(s_w) for signal proportions and γ̂ᴳ(·) for early hires. The inequality (19) shows the upper bound on high productivity workers not hired at S=-1, while subsequent steps combine market tightness and firm-worker ratios to derive the asymptotic result.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1538",
    "question": "In the context of joint Gaussian distributions and Markov properties in economic models, which of the following statements are correct regarding the covariance function σ and its implications?",
    "options": [
      "A) If σ satisfies the triangle property, then the inverse of the covariance matrix is tridiagonal.",
      "B) The Markov property of f implies that σ(a_i, a_j) = σ(a_i, a_m)σ(a_m, a_j)/σ(a_m, a_m) for any i ≤ m ≤ j.",
      "C) The NAP condition implies that the covariance function σ must be non-negative for any a, a' ∈ 𝒜.",
      "D) For any optimal sample a*, the sample weights τ_j^A(a*) and τ_j^P(a*) must have opposite signs."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "ecta18355.md"
    ],
    "question_context": "The joint Gaussian distribution is given by \n$$\n\\binom{f(\\widehat{a})}{f(\\mathbf{a})}\\sim\\mathcal{N}\\left(\\binom{\\mu(\\widehat{a})}{\\mu(\\mathbf{a})},\\binom{\\sigma(\\widehat{a},\\widehat{a})}{\\Sigma(\\widehat{a},\\mathbf{a})^{\\top}}\\quad\\Sigma(\\widehat{a},\\mathbf{a})\\right),\n$$\nwhere $\\Sigma({\\hat{a}},\\mathbf{a})=\\operatorname{cov}(f({\\hat{a}}),f(\\mathbf{a}))=\\left(\\sigma(a_{1},{\\hat{a}})\\quad\\ldots\\quad\\sigma(a_{n},{\\hat{a}})\\right)$ and $\\Sigma(\\mathbf{a})$ is the sample covariance matrix. Hence, $\\mathbb{E}[f(\\hat{a})\\mid\\mathbf{a},f(\\mathbf{a})]=\\mu(\\hat{a})+\\Sigma(\\hat{a},\\mathbf{a})[\\Sigma(\\mathbf{a})]^{-1}(f(\\mathbf{a})-\\mu(\\mathbf{a}))$. Moreover, sample realizations are observed perfectly. So, for any $(\\mathbf{a},f(\\mathbf{a}))$ and any $a_{j}\\in{\\bf a}$, $\\mathbb{E}[f(a_{j})\\mid\\bar{\\mathbf{a}},f(\\mathbf{a})]=f(a_{j})$. Therefore, $\\tau_{j}(a_{j};\\mathbf{a})=1$ and $\\tau_{m}(a_{j};\\mathbf{a})=0$ for $m\\neq j$.\n\nIf σ satisfies NAP, then for any finite selection of realizations $f(a_{1}),\\ldots,f(a_{n})$ for $a_{1}<a_{2}<\\cdots<a_{n}$ and any $i\\leq m\\leq j$,\n$$\n\\sigma(a_{i},a_{j})=\\mathbb{E}_{\\mathcal{F}_{m}}\\big[\\mathbb{E}\\big[f(a_{i})f(a_{j})\\mid\\mathcal{F}_{m}\\big]\\big]=\\mathbb{E}_{\\mathcal{F}_{m}}\\big[f(a_{i})\\mathbb{E}\\big[f(a_{j})\\mid\\mathcal{F}_{m}\\big]\\big]=\\mathbb{E}_{\\mathcal{F}_{m}}\\big[f(a_{i})\\mathbb{E}\\big[f(a_{j})\\mid f(a_{m})\\big]\\big]=\\mathbb{E}_{\\mathcal{F}_{m}}\\bigg[f(a_{i})\\frac{\\sigma(a_{j},a_{m})}{\\sigma(a_{m},a_{m})}f(a_{m})\\bigg]=\\frac{\\sigma(a_{j},a_{m})}{\\sigma(a_{m},a_{m})}\\sigma(a_{i},a_{m}).\n$$\nThe first equality uses the definition of the covariance and the Law of Iterated Expectations, the second equality follows from $i\\leq m\\leq j$, the third equality follows from the Markov property of $f$, the fourth equality follows from the jointly Gaussian distribution of $f(a_{j})$ and $f(a_{m})$, and the last equality uses $\\mathbb{E}[f(a_{i})f(a_{m})]=\\sigma(a_{i},a_{m})$.",
    "category": "INCORRECT",
    "question_context_used": "The joint Gaussian distribution is given by \n$$\n\\binom{f(\\widehat{a})}{f(\\mathbf{a})}\\sim\\mathcal{N}\\left(\\binom{\\mu(\\widehat{a})}{\\mu(\\mathbf{a})},\\binom{\\sigma(\\widehat{a},\\widehat{a})}{\\Sigma(\\widehat{a},\\mathbf{a})^{\\top}}\\quad\\Sigma(\\widehat{a},\\mathbf{a})\\right),\n$$\nwhere $\\Sigma({\\hat{a}},\\mathbf{a})=\\operatorname{cov}(f({\\hat{a}}),f(\\mathbf{a}))=\\left(\\sigma(a_{1},{\\hat{a}})\\quad\\ldots\\quad\\sigma(a_{n},{\\hat{a}})\\right)$ and $\\Sigma(\\mathbf{a})$ is the sample covariance matrix. Hence, $\\mathbb{E}[f(\\hat{a})\\mid\\mathbf{a},f(\\mathbf{a})]=\\mu(\\hat{a})+\\Sigma(\\hat{a},\\mathbf{a})[\\Sigma(\\mathbf{a})]^{-1}(f(\\mathbf{a})-\\mu(\\mathbf{a}))$. Moreover, sample realizations are observed perfectly. So, for any $(\\mathbf{a},f(\\mathbf{a}))$ and any $a_{j}\\in{\\bf a}$, $\\mathbb{E}[f(a_{j})\\mid\\bar{\\mathbf{a}},f(\\mathbf{a})]=f(a_{j})$. Therefore, $\\tau_{j}(a_{j};\\mathbf{a})=1$ and $\\tau_{m}(a_{j};\\mathbf{a})=0$ for $m\\neq j$.\n\nIf σ satisfies NAP, then for any finite selection of realizations $f(a_{1}),\\ldots,f(a_{n})$ for $a_{1}<a_{2}<\\cdots<a_{n}$ and any $i\\leq m\\leq j$,\n$$\n\\sigma(a_{i},a_{j})=\\mathbb{E}_{\\mathcal{F}_{m}}\\big[\\mathbb{E}\\big[f(a_{i})f(a_{j})\\mid\\mathcal{F}_{m}\\big]\\big]=\\mathbb{E}_{\\mathcal{F}_{m}}\\big[f(a_{i})\\mathbb{E}\\big[f(a_{j})\\mid\\mathcal{F}_{m}\\big]\\big]=\\mathbb{E}_{\\mathcal{F}_{m}}\\big[f(a_{i})\\mathbb{E}\\big[f(a_{j})\\mid f(a_{m})\\big]\\big]=\\mathbb{E}_{\\mathcal{F}_{m}}\\bigg[f(a_{i})\\frac{\\sigma(a_{j},a_{m})}{\\sigma(a_{m},a_{m})}f(a_{m})\\bigg]=\\frac{\\sigma(a_{j},a_{m})}{\\sigma(a_{m},a_{m})}\\sigma(a_{i},a_{m}).\n$$\nThe first equality uses the definition of the covariance and the Law of Iterated Expectations, the second equality follows from $i\\leq m\\leq j$, the third equality follows from the Markov property of $f$, the fourth equality follows from the jointly Gaussian distribution of $f(a_{j})$ and $f(a_{m})$, and the last equality uses $\\mathbb{E}[f(a_{i})f(a_{m})]=\\sigma(a_{i},a_{m})$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1294",
    "question": "In the context of dual balance-of-payments and financial crises, which of the following policy measures are traditionally recommended to address each crisis type?",
    "options": [
      "A) Balance-of-payments crisis: Exchange-rate devaluation and tightening of fiscal/monetary policy; Financial crisis: Expansion of money supply and lowering interest rates",
      "B) Balance-of-payments crisis: Expansion of money supply and lowering interest rates; Financial crisis: Exchange-rate devaluation and tightening of fiscal/monetary policy",
      "C) Both crises: Exchange-rate devaluation and tightening of fiscal/monetary policy",
      "D) Both crises: Expansion of money supply and lowering interest rates"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Krueger-ConflictingDemandsInternational-2000.md"
    ],
    "question_context": "In the case of a balance-of-payments crisis, the traditional remedy is for exchange-rate devaluation and a tightening of fiscal and monetary policy to reduce the excess of domestic expenditure over domestic income. In the case of a financial crisis, however, the traditional remedy is to expand the money supply and to lower interest rates. Reduced interest rates ease the burden on debtors and thus enable more of them to continue to meet their (domestic) debt-servicing obligations. Devaluation, by contrast, raises the domestic-currency debt-servicing obligations of some debtors (either the banks or other domestic borrowers), while affecting the asset side of banks' balance sheets by a much smaller amount. Tightening monetary policy and raising interest rates raises the carrying cost of outstanding debt and thus is likely to push some questionable debtors over the line into nonperforming status.",
    "category": "INCORRECT",
    "question_context_used": "In the case of a balance-of-payments crisis, the traditional remedy is for exchange-rate devaluation and a tightening of fiscal and monetary policy to reduce the excess of domestic expenditure over domestic income. In the case of a financial crisis, however, the traditional remedy is to expand the money supply and to lower interest rates. Reduced interest rates ease the burden on debtors and thus enable more of them to continue to meet their (domestic) debt-servicing obligations. Devaluation, by contrast, raises the domestic-currency debt-servicing obligations of some debtors (either the banks or other domestic borrowers), while affecting the asset side of banks' balance sheets by a much smaller amount. Tightening monetary policy and raising interest rates raises the carrying cost of outstanding debt and thus is likely to push some questionable debtors over the line into nonperforming status.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1469",
    "question": "Based on the analysis of income and consumption poverty rates in 1988, which of the following statements are empirically supported?",
    "options": [
      "A) Consumption poverty rates are uniformly lower than income poverty rates across all demographic groups",
      "B) The elderly exhibit the largest disparity between income and consumption poverty rates",
      "C) Nonwhite-headed families show higher consumption poverty rates than income poverty rates",
      "D) Children have higher poverty rates than the elderly under both income and consumption measures"
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Cutler-RisingInequalityChanges-1992.md"
    ],
    "question_context": "The first panel of Table 3 shows income and consumption measures of the overall poverty rate and of poverty rates by age. Income poverty rates (13.1 percent) are higher than consumption poverty rates (10.3 percent), as the permanent-income hypothesis suggests. The difference is particularly acute for the elderly. In 1988, income poverty for the elderly was above that for adults, although the gap has narrowed over time. Consumption poverty for the elderly is far below consumption poverty for adults and children, however. The remaining panels of Table 3 show income and consumption poverty rates for various demographic components of the population. With the exception of families headed by nonwhites, consumption poverty rates are lower than income poverty rates for all groups. The largest differences between income and consumption poverty thus appear to be for different age groups. Poverty rates for children, for example, are more than three times those for the elderly.",
    "category": "INCORRECT",
    "question_context_used": "The first panel of Table 3 shows income and consumption measures of the overall poverty rate and of poverty rates by age. Income poverty rates (13.1 percent) are higher than consumption poverty rates (10.3 percent), as the permanent-income hypothesis suggests. The difference is particularly acute for the elderly. In 1988, income poverty for the elderly was above that for adults, although the gap has narrowed over time. Consumption poverty for the elderly is far below consumption poverty for adults and children, however. The remaining panels of Table 3 show income and consumption poverty rates for various demographic components of the population. With the exception of families headed by nonwhites, consumption poverty rates are lower than income poverty rates for all groups. The largest differences between income and consumption poverty thus appear to be for different age groups. Poverty rates for children, for example, are more than three times those for the elderly.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1732",
    "question": "Based on the analysis of Profamilia's family planning programme in Colombia, which of the following statements are empirically supported by the data?",
    "options": [
      "A) Family planning availability at ages 15-19 was associated with a 4% reduction in lifetime births and a 1% increase in years of schooling.",
      "B) Women exposed to family planning at ages 30-34 showed significant reductions in cohabitation rates but no change in formal sector employment.",
      "C) The programme had no statistically significant effect on infant and child mortality rates across all age groups.",
      "D) Fertility reductions were observed up to age 35, but socio-economic improvements were limited to women gaining access in their teens and early twenties."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Contraception_as_Development_New_Evidence_from_Family_Planning_in_Colombia.md"
    ],
    "question_context": "Key findings from Tables 3 and 5: The introduction of Profamilia’s programmes was negatively associated with lifetime fertility for women in their late teens to early thirties (4% fewer births for ages 15-19, decreasing with age). Socio-economic improvements (0.05 additional years of schooling, 2 percentage points higher formal sector employment) were limited to women gaining access in their teens/early twenties. The formula for percentage change in lifetime births is derived from log-linear OLS estimates: $\\Delta \\text{Births} \\approx 100 \\times \\beta$, where $\\beta$ is the coefficient for each age interval. Infant/child mortality showed no clear relationship with family planning availability (last column of Table 5). The divergence between age patterns of fertility reduction (up to 35) and socio-economic gains (teens/early 20s) suggests timing of first births is more critical than completed fertility for economic outcomes.",
    "category": "INCORRECT",
    "question_context_used": "Key findings from Tables 3 and 5: The introduction of Profamilia’s programmes was negatively associated with lifetime fertility for women in their late teens to early thirties (4% fewer births for ages 15-19, decreasing with age). Socio-economic improvements (0.05 additional years of schooling, 2 percentage points higher formal sector employment) were limited to women gaining access in their teens/early twenties. The formula for percentage change in lifetime births is derived from log-linear OLS estimates: $\\Delta \\text{Births} \\approx 100 \\times \\beta$, where $\\beta$ is the coefficient for each age interval. Infant/child mortality showed no clear relationship with family planning availability (last column of Table 5). The divergence between age patterns of fertility reduction (up to 35) and socio-economic gains (teens/early 20s) suggests timing of first births is more critical than completed fertility for economic outcomes.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1841",
    "question": "In Friedman's analysis of turnpike properties in time-dependent supergames, which of the following conditions are essential for the emergence of turnpike behavior?",
    "options": [
      "A) The existence of a unique Nash equilibrium path",
      "B) The presence of a dominant strategy for all players",
      "C) The convergence of strategies to a steady state over time",
      "D) The assumption of infinite time horizon"
    ],
    "gold_answer": "C, D",
    "source_files": [
      "Friedman-NoteTurnpikeProperties-1981.md"
    ],
    "question_context": "The turnpike properties in time-dependent supergames refer to the phenomenon where strategies converge to a steady state over time, resembling the behavior observed in optimal growth models. Friedman's analysis emphasizes the importance of an infinite time horizon for such convergence to occur. Key formula: The steady state is characterized by strategies that satisfy the condition \\[ \\lim_{t \\to \\infty} s_i(t) = s^* \\] for all players i, where s^* represents the steady-state strategy. The analysis builds on the assumption that players' strategies are interdependent and evolve over time, with the turnpike property emerging as a long-run equilibrium feature.",
    "category": "CORRECT",
    "question_context_used": "The turnpike properties in time-dependent supergames refer to the phenomenon where strategies converge to a steady state over time, resembling the behavior observed in optimal growth models. Friedman's analysis emphasizes the importance of an infinite time horizon for such convergence to occur. Key formula: The steady state is characterized by strategies that satisfy the condition \\[ \\lim_{t \\to \\infty} s_i(t) = s^* \\] for all players i, where s^* represents the steady-state strategy. The analysis builds on the assumption that players' strategies are interdependent and evolve over time, with the turnpike property emerging as a long-run equilibrium feature.",
    "gold_answer_parsed": [
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-998",
    "question": "Based on the empirical findings in Table 2, which of the following firm/market characteristics have statistically significant impacts on bank acquisition probabilities?",
    "options": [
      "A) Market share (MKTSHARₜ) for outside-market acquisitions",
      "B) Capital-asset ratio (CAP/Aₜ) for both inside and outside acquisitions",
      "C) Market growth (MKTGROWTH) and bank growth (BKGROWTHₜ)",
      "D) Market concentration (CR3ₜ) for inside-market acquisitions"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Hannan-AcquisitionTargetsMotives-1987.md"
    ],
    "question_context": "The study distinguishes between acquisitions from inside and outside the target firm's market. Key findings include: Market share (MKTSHARₜ) has a positive and highly significant impact on outside-market acquisitions ($0.02^b$ to $0.036^b$ coefficients) but no significant effect on within-market acquisitions. The capital-asset ratio $(CAP/A_{t})$ shows negative and significant coefficients in all estimations (ranging from $-21.19^a$ to $-30.07^b$), with greater absolute values for outside acquisitions. Market concentration $(CR3_{t})$ has negative and significant coefficients for inside acquisitions ($-83.23^b$ to $-88.15^b$) but positive insignificant coefficients for outside acquisitions. Neither market growth (MKTGROWTH) nor bank growth $(BKGROWTH_{t})$ showed statistically significant impacts in any estimation.",
    "category": "CORRECT",
    "question_context_used": "The study distinguishes between acquisitions from inside and outside the target firm's market. Key findings include: Market share (MKTSHARₜ) has a positive and highly significant impact on outside-market acquisitions ($0.02^b$ to $0.036^b$ coefficients) but no significant effect on within-market acquisitions. The capital-asset ratio $(CAP/A_{t})$ shows negative and significant coefficients in all estimations (ranging from $-21.19^a$ to $-30.07^b$), with greater absolute values for outside acquisitions. Market concentration $(CR3_{t})$ has negative and significant coefficients for inside acquisitions ($-83.23^b$ to $-88.15^b$) but positive insignificant coefficients for outside acquisitions. Neither market growth (MKTGROWTH) nor bank growth $(BKGROWTH_{t})$ showed statistically significant impacts in any estimation.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3281",
    "question": "Which of the following conditions are necessary for a data set to be Nash bargaining rationalisable according to Theorem 3?",
    "options": [
      "A) The existence of utility functions and marginal utilities of income that satisfy Afriat inequalities for both bargaining outcomes and threat points.",
      "B) The bargaining outcome must be Pareto efficient, as guaranteed by constraints (NB-i) and (NB-ii).",
      "C) The threat point bundles must be chosen irrationally, violating individual rationality.",
      "D) The ratio of marginal utilities of income must equal the ratio of excess utilities, as specified in constraint (NB-vi)."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Nash‐Bargained_Consumption_Decisions_A_Revealed_Preference_Analysis.md"
    ],
    "question_context": "Theorem 3 provides a revealed preference characterisation of Nash-bargaining rationalisability. The following conditions are equivalent: (i) S is Nash bargaining rationalisable. (ii) For all $t\\in T_{\\cdot}$, there exist numbers $U_{t}^{A},U_{t}^{B},V_{t}^{A},V_{t}^{B}\\in\\mathbb{R}_{+}$ and $\\lambda_{t}^{A},\\lambda_{t}^{B},\\delta_{t}^{A},\\delta_{t}^{B}\\in\\mathbb{R}_{++}$ such that, for all $t$, $\\upsilon\\in{\\mathrm{~}}T$, the Afriat inequalities (NB-i)–(NB-iv) hold, along with constraints (NB-v) and (NB-vi). The constraints (NB-i)–(NB-ii) guarantee Pareto efficiency of the bargaining outcome, while (NB-iii)–(NB-iv) impose individual rationality for the threat point bundles. Constraint (NB-vi) captures the essence of the Nash-bargaining model, requiring that the ratio of marginal utilities of income equals the ratio of excess utilities: $$\\frac{\\lambda_{t}^{A}}{\\lambda_{t}^{B}}=\\frac{U_{t}^{A}-V_{t}^{A}}{U_{t}^{B}-V_{t}^{B}}.$$",
    "category": "CORRECT",
    "question_context_used": "Theorem 3 provides a revealed preference characterisation of Nash-bargaining rationalisability. The following conditions are equivalent: (i) S is Nash bargaining rationalisable. (ii) For all $t\\in T_{\\cdot}$, there exist numbers $U_{t}^{A},U_{t}^{B},V_{t}^{A},V_{t}^{B}\\in\\mathbb{R}_{+}$ and $\\lambda_{t}^{A},\\lambda_{t}^{B},\\delta_{t}^{A},\\delta_{t}^{B}\\in\\mathbb{R}_{++}$ such that, for all $t$, $\\upsilon\\in{\\mathrm{~}}T$, the Afriat inequalities (NB-i)–(NB-iv) hold, along with constraints (NB-v) and (NB-vi). The constraints (NB-i)–(NB-ii) guarantee Pareto efficiency of the bargaining outcome, while (NB-iii)–(NB-iv) impose individual rationality for the threat point bundles. Constraint (NB-vi) captures the essence of the Nash-bargaining model, requiring that the ratio of marginal utilities of income equals the ratio of excess utilities: $$\\frac{\\lambda_{t}^{A}}{\\lambda_{t}^{B}}=\\frac{U_{t}^{A}-V_{t}^{A}}{U_{t}^{B}-V_{t}^{B}}.$$",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2830",
    "question": "According to Edward E. Leamer's Bayesian econometric research, which of the following statements accurately describe his findings on collinearity and model specification?",
    "options": [
      "A) Collinearity arises from interpreting multidimensional evidence with undominated uncertain prior information.",
      "B) Collinearity is primarily a weak evidence problem as described in econometrics textbooks.",
      "C) The degree of collinearity can be measured by the sensitivity of the posterior distribution to changes in the prior distribution.",
      "D) Post-data model construction often involves double-counting the same data evidence for hypothesis construction and testing."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Qin - 1996 - Bayesian econometrics - The first twenty years.md"
    ],
    "question_context": "Leamer began his investigation with the distributed-lag model (1972b). Unlike those who took the model for granted and who concentrated on deriving Bayesian estimators, Leamer delved into the nature of collinearity. He found that the problem had never been 'rigorously defined,' and he set to filling the gap. From a Bayesian viewpoint, Leamer perceived collinearity as a problem arising from the modelers' attempts at 'interpreting multidimensional evidence' in light of some 'undominated uncertain prior information,' rather than a 'weak evidence problem' described in econometrics textbooks. He then narrowed it down from the problem of 'characterizing and interpreting a multidimensional likelihood function into a problem of characterizing and interpreting a multidimensional prior distribution' and ascribed, as its cause, the inadequate specification of prior information to help 'allocate' data information 'to individual coefficients.' Leamer thus proposed to measure the degree of collinearity by 'the sensitivity of the posterior distribution to changes in the prior distribution' (1973). Such sensitivity analysis taught him the lesson that 'collinearity thus creates an incentive to use prior information more carefully' (1978b, p. 174). This amounts to saying that the problem entails more careful parameterization of theoretical relations in model specification.\n\nIn his Bayesian research into the interpretive searches, Leamer noticed that modelers' specification searches often went beyond interpretation to new hypotheses consistent with data evidence. He labeled these efforts as 'postdata model construction.' He observed two major difficulties in undertaking such searches (1974). One was caused by the implicit prior judgments or decisions upon which data evidence was selected and presented as such. This was bound to bias search for new hypotheses. Leamer described this bias as 'discounted' or 'contaminated' (i.e., not absolutely objective) data. The other difficulty was concerned with the risk of 'double-counting the data evidence' (1978b, p. 285)-- that is, the erroneous use of the same data evidence for both constructing new hypotheses and testing existing hypotheses.",
    "category": "CORRECT",
    "question_context_used": "Leamer began his investigation with the distributed-lag model (1972b). Unlike those who took the model for granted and who concentrated on deriving Bayesian estimators, Leamer delved into the nature of collinearity. He found that the problem had never been 'rigorously defined,' and he set to filling the gap. From a Bayesian viewpoint, Leamer perceived collinearity as a problem arising from the modelers' attempts at 'interpreting multidimensional evidence' in light of some 'undominated uncertain prior information,' rather than a 'weak evidence problem' described in econometrics textbooks. He then narrowed it down from the problem of 'characterizing and interpreting a multidimensional likelihood function into a problem of characterizing and interpreting a multidimensional prior distribution' and ascribed, as its cause, the inadequate specification of prior information to help 'allocate' data information 'to individual coefficients.' Leamer thus proposed to measure the degree of collinearity by 'the sensitivity of the posterior distribution to changes in the prior distribution' (1973). Such sensitivity analysis taught him the lesson that 'collinearity thus creates an incentive to use prior information more carefully' (1978b, p. 174). This amounts to saying that the problem entails more careful parameterization of theoretical relations in model specification.\n\nIn his Bayesian research into the interpretive searches, Leamer noticed that modelers' specification searches often went beyond interpretation to new hypotheses consistent with data evidence. He labeled these efforts as 'postdata model construction.' He observed two major difficulties in undertaking such searches (1974). One was caused by the implicit prior judgments or decisions upon which data evidence was selected and presented as such. This was bound to bias search for new hypotheses. Leamer described this bias as 'discounted' or 'contaminated' (i.e., not absolutely objective) data. The other difficulty was concerned with the risk of 'double-counting the data evidence' (1978b, p. 285)-- that is, the erroneous use of the same data evidence for both constructing new hypotheses and testing existing hypotheses.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1250",
    "question": "In the context of the specification pre-test estimator, which of the following statements are correct regarding its properties when σ² is known?",
    "options": [
      "A) The bias of the specification pre-test estimator is given by E[β̃₁] = β₁ - gλ(2)[(X'X)⁻¹R']₁[R(X'X)⁻¹R']⁻¹Rβ.",
      "B) The risk matrix of the specification pre-test estimator includes a term involving the covariance matrix of β̂₁₀.",
      "C) The equivalence between the specification pre-test estimator and the classical pre-test estimator holds only when K₁ = 1.",
      "D) The non-centrality parameter λ in the χ² distribution is defined as λ = (Rβ)'[R(X'X)⁻¹R']⁻¹Rβ / σ²."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Specification-pre-test-estimator_1984_Journal-of-Econometrics.md"
    ],
    "question_context": "The specification pre-test estimator of β₁ is β̂₁₀ when the test fails to reject b = (X₁'X₁)⁻¹X₁'X₂β₂ = 0 and otherwise it is β̂₁. The classical pre-test estimator of β₁ is β̃₁, the OLS estimator of β₁ constrained by (X₁'X₁)⁻¹X₁X₂β₂ = 0 when the test fails to reject b = 0 and it is β̂₁ otherwise. The equivalence between the specification pre-test estimator and the pre-test estimator associated with the classical test of b = 0 means that all the general results described in Judge and Bock (1978, pp. 83-119) can be applied for the specification pre-test estimator.\n\nKey formula for bias when σ² is known:\nE[β̃₁] = β₁ - gλ(2)[(X'X)⁻¹R']₁[R(X'X)⁻¹R']⁻¹Rβ,\nwhere gλ(l) ≈ P[χ²(rk(R) + l, λ) ≤ α²], and σ²λ = (Rβ)'[R(X'X)⁻¹R']⁻¹Rβ. The non-centrality parameter λ is crucial in determining the distributional properties of the estimator under the alternative hypothesis.",
    "category": "CORRECT",
    "question_context_used": "The specification pre-test estimator of β₁ is β̂₁₀ when the test fails to reject b = (X₁'X₁)⁻¹X₁'X₂β₂ = 0 and otherwise it is β̂₁. The classical pre-test estimator of β₁ is β̃₁, the OLS estimator of β₁ constrained by (X₁'X₁)⁻¹X₁X₂β₂ = 0 when the test fails to reject b = 0 and it is β̂₁ otherwise. The equivalence between the specification pre-test estimator and the pre-test estimator associated with the classical test of b = 0 means that all the general results described in Judge and Bock (1978, pp. 83-119) can be applied for the specification pre-test estimator.\n\nKey formula for bias when σ² is known:\nE[β̃₁] = β₁ - gλ(2)[(X'X)⁻¹R']₁[R(X'X)⁻¹R']⁻¹Rβ,\nwhere gλ(l) ≈ P[χ²(rk(R) + l, λ) ≤ α²], and σ²λ = (Rβ)'[R(X'X)⁻¹R']⁻¹Rβ. The non-centrality parameter λ is crucial in determining the distributional properties of the estimator under the alternative hypothesis.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1006",
    "question": "In the context of the grouped fixed-effects (GFE) estimator proposed by Bonhomme and Manresa (2015), which of the following statements are correct regarding its properties and assumptions?",
    "options": [
      "A) The GFE estimator allows for arbitrary correlation between covariates and group-specific unobservables, provided the number of groups G is known.",
      "B) The GFE estimator is consistent and asymptotically normal as both N and T tend to infinity, even when N grows substantially faster than T.",
      "C) The GFE estimator requires that group membership remains constant over time for consistent estimation.",
      "D) The GFE estimator can be combined with additive time-invariant fixed-effects by transforming the model into deviations from the mean."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Bonhomme-GROUPEDPATTERNSHETEROGENEITY-2015.md"
    ],
    "question_context": "The paper introduces a grouped fixed-effects (GFE) estimator for linear panel data models with time-varying grouped patterns of heterogeneity. A key feature is that group membership is left unrestricted. The estimator minimizes a least squares criterion over all possible groupings of cross-sectional units. The model allows for covariates $x_{it}$ to be contemporaneously uncorrelated with errors $v_{it}$ but arbitrarily correlated with group-specific unobservables $\\alpha_{g_i t}$. The authors show consistency and asymptotic normality of the GFE estimator as both $N$ and $T$ tend to infinity, even when $N$ grows faster than $T$. \n\nKey formula: The baseline model is specified as $$y_{i t}=x_{i t}^{\\prime}\\theta+\\alpha_{g_{i}t}+v_{i t},\\quad i=1,\\ldots,N,t=1,\\ldots,T,$$ where $g_i \\in \\{1,...,G\\}$ denotes group membership. The paper also presents extensions including models with additive time-invariant fixed-effects $\\eta_i$, handled by transforming to deviations from the mean: $$y_{i t}-{\\overline{{y}}}_{i}=(x_{i t}-{\\overline{{x}}}_{i})^{\\prime}\\theta+\\alpha_{g_{i}t}-{\\overline{{\\alpha}}}_{g_{i}}+v_{i t}-{\\overline{{v}}}_{i}.$$",
    "category": "CORRECT",
    "question_context_used": "The paper introduces a grouped fixed-effects (GFE) estimator for linear panel data models with time-varying grouped patterns of heterogeneity. A key feature is that group membership is left unrestricted. The estimator minimizes a least squares criterion over all possible groupings of cross-sectional units. The model allows for covariates $x_{it}$ to be contemporaneously uncorrelated with errors $v_{it}$ but arbitrarily correlated with group-specific unobservables $\\alpha_{g_i t}$. The authors show consistency and asymptotic normality of the GFE estimator as both $N$ and $T$ tend to infinity, even when $N$ grows faster than $T$. \n\nKey formula: The baseline model is specified as $$y_{i t}=x_{i t}^{\\prime}\\theta+\\alpha_{g_{i}t}+v_{i t},\\quad i=1,\\ldots,N,t=1,\\ldots,T,$$ where $g_i \\in \\{1,...,G\\}$ denotes group membership. The paper also presents extensions including models with additive time-invariant fixed-effects $\\eta_i$, handled by transforming to deviations from the mean: $$y_{i t}-{\\overline{{y}}}_{i}=(x_{i t}-{\\overline{{x}}}_{i})^{\\prime}\\theta+\\alpha_{g_{i}t}-{\\overline{{\\alpha}}}_{g_{i}}+v_{i t}-{\\overline{{v}}}_{i}.$$",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4519",
    "question": "Which of the following statements are consistent with the theoretical predictions and simulation results regarding the ACA's impact on single mothers' labor supply and health insurance choices?",
    "options": [
      "A) Health insurance subsidies primarily drive the increase in labor supply by reducing the Medicaid 'notch' effect",
      "B) Medicaid expansions alone lead to significant welfare gains comparable to those from health insurance subsidies",
      "C) Single mothers with medical conditions show stronger labor supply responses to health care reform than those without",
      "D) The employer mandate provisions of the ACA were explicitly modeled in the simulations"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "MEDICAID AND THE LABOR SUPPLY OF SINGLE MOTHERS IMPLICATIONS FOR HEALTH CARE REFORM.md"
    ],
    "question_context": "The theoretical model predicts that Medicaid expansions may change individuals' choice sets by adding alternatives with Medicaid coverage, while health insurance subsidies affect the budget constraint by lowering effective premiums for nongroup insurance. Key formula for welfare analysis: \n\n$$\\operatorname*{max}_{j\\in J_{u}^{p o l,(r)}}U_{i j}^{p o l,(s)}\\left(\\hat{C}_{i j}^{p o l,(r)}-{C V_{i i}^{p o l,(r,s)}},L_{i i j},I_{i i j}\\right)=\\operatorname*{max}_{j\\in J_{u}^{p e,(r)}}U_{i i j}^{p r e,(s)}\\left(\\hat{C}_{i i j}^{p r e,(r)},L_{i i j},I_{i i j}\\right).$$\n\nSimulation results show that combining Medicaid expansion and health insurance subsidies reduces uninsurance rates among children from 7% to 4% and among single mothers from 19% to 13%. The compensating variation analysis reveals average annual welfare gains of about $1,600 under combined policies, with greater benefits for those with medical conditions.",
    "category": "CORRECT",
    "question_context_used": "The theoretical model predicts that Medicaid expansions may change individuals' choice sets by adding alternatives with Medicaid coverage, while health insurance subsidies affect the budget constraint by lowering effective premiums for nongroup insurance. Key formula for welfare analysis: \n\n$$\\operatorname*{max}_{j\\in J_{u}^{p o l,(r)}}U_{i j}^{p o l,(s)}\\left(\\hat{C}_{i j}^{p o l,(r)}-{C V_{i i}^{p o l,(r,s)}},L_{i i j},I_{i i j}\\right)=\\operatorname*{max}_{j\\in J_{u}^{p e,(r)}}U_{i i j}^{p r e,(s)}\\left(\\hat{C}_{i i j}^{p r e,(r)},L_{i i j},I_{i i j}\\right).$$\n\nSimulation results show that combining Medicaid expansion and health insurance subsidies reduces uninsurance rates among children from 7% to 4% and among single mothers from 19% to 13%. The compensating variation analysis reveals average annual welfare gains of about $1,600 under combined policies, with greater benefits for those with medical conditions.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3349",
    "question": "In the context of the provided economic framework, which of the following variables are directly influenced by the accelerator coefficient ($\\alpha_{j}$) in the private investment function?",
    "options": [
      "A) $I_{j}(t)$ - Private investment",
      "B) $K_{j}(t)$ - Stock of capital",
      "C) $\\Omega_{i}(t)$ - Change in private inventories",
      "D) $X_{j}(t)$ - Gross value of production"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "The-transition-to-a-globalized-economy--Poverty--human-_2005_Journal-of-Deve.md"
    ],
    "question_context": "The private investment function includes coefficients such as the accelerator ($\\alpha_{j}$), crowding out ($\\beta_{j}$), crowding in ($\\gamma_{j}$), and inflation ($v_{j}$). The accelerator coefficient specifically links investment to changes in output or demand. Key variables include $I_{j}(t)$ for private investment, $K_{j}(t)$ for capital stock, and $X_{j}(t)$ for gross production value. The relationship is given by: $I_{j}(t) = \\zeta_{i}(0) + \\alpha_{j} \\Delta X_{j}(t) - \\beta_{j} r(t) + \\gamma_{j} G(t) + v_{j} \\pi(t)$, where $\\Delta X_{j}(t)$ represents the change in output.",
    "category": "INCORRECT",
    "question_context_used": "The private investment function includes coefficients such as the accelerator ($\\alpha_{j}$), crowding out ($\\beta_{j}$), crowding in ($\\gamma_{j}$), and inflation ($v_{j}$). The accelerator coefficient specifically links investment to changes in output or demand. Key variables include $I_{j}(t)$ for private investment, $K_{j}(t)$ for capital stock, and $X_{j}(t)$ for gross production value. The relationship is given by: $I_{j}(t) = \\zeta_{i}(0) + \\alpha_{j} \\Delta X_{j}(t) - \\beta_{j} r(t) + \\gamma_{j} G(t) + v_{j} \\pi(t)$, where $\\Delta X_{j}(t)$ represents the change in output.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3607",
    "question": "In the context of optimal editorial policy with network effects, which of the following conditions ensure that babbling is not an optimal strategy for the newspaper?",
    "options": [
      "A) The prior probability q is less than the threshold q̄(γ) and the newspaper sets p0 = 0, p1 = 1",
      "B) The prior probability q is greater than the threshold q̄(γ) and the newspaper sets p0 = p1",
      "C) The prior probability q equals the threshold q̄(γ) and the newspaper sets p0 = p1",
      "D) The prior probability q is less than the threshold q̄(γ) and the newspaper sets p0 = ε, p1 = 1 for some small ε > 0"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Social_Connectivity,_Media_Bias,_and_Correlation_Neglect.md"
    ],
    "question_context": "The proof establishes the existence of a unique threshold q̄(γ) ∈ (1/2,1) such that V(0,r) = 1/2 if and only if q = q̄(γ). If q ≥ q̄(γ), then V(0,r) ≥ 1/2, and since V(1,r) > V(0,r), the newspaper can achieve its maximum payoff by babbling (p0 = p1). However, if q < q̄(γ), then V(0,r) < 1/2. In this case, babbling yields a payoff of at most q, which is identical to the payoff from truthful revelation (p0 = 0, p1 = 1). By setting p1 = 1 and p0 = ε for some sufficiently small ε > 0, the newspaper can achieve a payoff of q + (1-q)ε > q, making babbling suboptimal. The key formula for the threshold is: \n\n$$\n\\bar{q}(\\gamma) = \\frac{1}{1 + e^{-2\\mu^2(N+1)}}\n$$\n\nThus, when q < q̄(γ), the optimal editorial policy must satisfy p0 < p1 to maximize the newspaper's payoff.",
    "category": "CORRECT",
    "question_context_used": "The proof establishes the existence of a unique threshold q̄(γ) ∈ (1/2,1) such that V(0,r) = 1/2 if and only if q = q̄(γ). If q ≥ q̄(γ), then V(0,r) ≥ 1/2, and since V(1,r) > V(0,r), the newspaper can achieve its maximum payoff by babbling (p0 = p1). However, if q < q̄(γ), then V(0,r) < 1/2. In this case, babbling yields a payoff of at most q, which is identical to the payoff from truthful revelation (p0 = 0, p1 = 1). By setting p1 = 1 and p0 = ε for some sufficiently small ε > 0, the newspaper can achieve a payoff of q + (1-q)ε > q, making babbling suboptimal. The key formula for the threshold is: \n\n$$\n\\bar{q}(\\gamma) = \\frac{1}{1 + e^{-2\\mu^2(N+1)}}\n$$\n\nThus, when q < q̄(γ), the optimal editorial policy must satisfy p0 < p1 to maximize the newspaper's payoff.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3577",
    "question": "In the context of class action litigation and surplus extraction by lawyers and third parties, which of the following statements are correct according to Proposition 3?",
    "options": [
      "A) When consumers are completely myopic (μ=0), firm profits and social welfare are unaffected by class actions.",
      "B) If k_p < k̂_p(μ(1-γ)), firms blocking class actions is always socially efficient.",
      "C) For k_p ∈ [k̂_p(μ(1-γ)), k̂_p(μ)], firms blocking class actions is socially inefficient.",
      "D) When γ approaches 1, the effective price perceived by consumers approaches the actual price p."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Class Actions and Private Antitrust Litigation.md"
    ],
    "question_context": "The text discusses the implications of class action lawsuits on firm profits and social welfare, particularly focusing on surplus extraction by lawyers and third parties. Key formulas include the effective price perceived by consumers: \n\n$$ p-\\mu(1-\\gamma)\\mathrm{max}\\big\\{\\theta(p-c)-k_{p},0\\big\\}. $$\n\nProposition 3 outlines conditions under which firms' incentives to block class actions align with social welfare. For instance, when consumers are not completely myopic (μ∈(0,1]), the threshold k̂_p(μ(1-γ)) is lower than k̂_p(μ), affecting firms' decisions. The analysis shows that surplus extraction by lawyers and third parties can act as an ad valorem tax on consumers, influencing both firm profits and social welfare.",
    "category": "INCORRECT",
    "question_context_used": "The text discusses the implications of class action lawsuits on firm profits and social welfare, particularly focusing on surplus extraction by lawyers and third parties. Key formulas include the effective price perceived by consumers: \n\n$$ p-\\mu(1-\\gamma)\\mathrm{max}\\big\\{\\theta(p-c)-k_{p},0\\big\\}. $$\n\nProposition 3 outlines conditions under which firms' incentives to block class actions align with social welfare. For instance, when consumers are not completely myopic (μ∈(0,1]), the threshold k̂_p(μ(1-γ)) is lower than k̂_p(μ), affecting firms' decisions. The analysis shows that surplus extraction by lawyers and third parties can act as an ad valorem tax on consumers, influencing both firm profits and social welfare.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1657",
    "question": "Which of the following research areas were represented by Northwestern University faculty on the 2003 Econometric Society Summer Meeting Program Committee?",
    "options": [
      "A) Political Theory and Decision Theory",
      "B) Behavioral Economics and Theoretical Econometrics",
      "C) Computational Economics and Social Choice",
      "D) Applied Macroeconomics and Financial Economics"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Announcements-2002.md"
    ],
    "question_context": "The 2003 North American Summer Meeting of the Econometric Society was held at the Kellogg School of Management, Northwestern University. The Program Committee included several Northwestern faculty members covering diverse research areas. Key faculty and their specializations included: David Austen-Smith (Political Theory), Kent Daniel (Behavioral Economics), Martin Eichenbaum (Applied Macroeconomics), Joel Horowitz (Theoretical Econometrics), Peter Klibanoff (Decision Theory), and Costis Skiadas (Financial Economics).\n\nThe meeting aimed to cover broad econometric and economic theory topics, with submissions open to all economists. The Program Committee structure reflects the interdisciplinary nature of econometric research, combining theoretical, applied, and computational approaches across microeconomics, macroeconomics, and decision sciences.",
    "category": "CORRECT",
    "question_context_used": "The 2003 North American Summer Meeting of the Econometric Society was held at the Kellogg School of Management, Northwestern University. The Program Committee included several Northwestern faculty members covering diverse research areas. Key faculty and their specializations included: David Austen-Smith (Political Theory), Kent Daniel (Behavioral Economics), Martin Eichenbaum (Applied Macroeconomics), Joel Horowitz (Theoretical Econometrics), Peter Klibanoff (Decision Theory), and Costis Skiadas (Financial Economics).\n\nThe meeting aimed to cover broad econometric and economic theory topics, with submissions open to all economists. The Program Committee structure reflects the interdisciplinary nature of econometric research, combining theoretical, applied, and computational approaches across microeconomics, macroeconomics, and decision sciences.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3393",
    "question": "In the context of fixed effects multinomial logit models with grouped data, which of the following statements are correct regarding the conditional likelihood approach?",
    "options": [
      "A) The conditional likelihood function eliminates the incidental parameters problem by conditioning on sufficient statistics for the fixed effects.",
      "B) The conditional ML estimator for β is consistent only if the number of observations per group (T) approaches infinity.",
      "C) The information matrix for the conditional ML estimator can be consistently estimated without knowledge of the fixed effects.",
      "D) The conditional likelihood approach transforms the problem into a standard conditional logit form with varying alternative sets across observations."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Analysis of Covariance with Qualitative Data.md"
    ],
    "question_context": "The conditional likelihood approach in fixed effects models addresses the incidental parameters problem by conditioning on sufficient statistics for the fixed effects. For the linear regression case, the model is specified as: \n\n$$ y_{i t}=\\mathbf{\\beta}^{\\prime}x_{i t}+\\alpha_{i}+\\varepsilon_{i t}, $$\n\nwith $\\pmb{\\varepsilon}_{i t}$ i.i.d. $N(0,\\sigma^{2})$. The conditional density does not depend on $\\alpha_{i}$, allowing consistent estimation of $\\mathbf{\\beta}$ and $\\sigma$. The conditional log-likelihood function is:\n\n$$ {\\cal L}=-N(T-1)\\ln\\sigma-\\frac{1}{2\\sigma^{2}}\\sum_{i,t}\\lbrack(y_{i t}-\\bar{y}_{i})-\\beta^{\\prime}(x_{i t}-\\bar{x}_{i})\\rbrack^{2}. $$\n\nFor the multinomial logit case, the conditional likelihood function takes the form:\n\n$$ L=\\sum_{i}\\ln\\left[\\exp{(\\mathbf{\\upbeta}^{\\prime}\\sum_{t}x_{i t}y_{i t})}/\\sum_{d\\in B_{i}}\\exp{(\\mathbf{\\upbeta}^{\\prime}\\sum_{t}x_{i t}d_{t})}\\right], $$\n\nwhere $B_{i}$ is the set of all possible sequences of outcomes that match the observed sums. This formulation avoids the incidental parameters problem and allows consistent estimation of $\\mathbf{\\beta}$ using standard conditional logit programs.",
    "category": "CORRECT",
    "question_context_used": "The conditional likelihood approach in fixed effects models addresses the incidental parameters problem by conditioning on sufficient statistics for the fixed effects. For the linear regression case, the model is specified as: \n\n$$ y_{i t}=\\mathbf{\\beta}^{\\prime}x_{i t}+\\alpha_{i}+\\varepsilon_{i t}, $$\n\nwith $\\pmb{\\varepsilon}_{i t}$ i.i.d. $N(0,\\sigma^{2})$. The conditional density does not depend on $\\alpha_{i}$, allowing consistent estimation of $\\mathbf{\\beta}$ and $\\sigma$. The conditional log-likelihood function is:\n\n$$ {\\cal L}=-N(T-1)\\ln\\sigma-\\frac{1}{2\\sigma^{2}}\\sum_{i,t}\\lbrack(y_{i t}-\\bar{y}_{i})-\\beta^{\\prime}(x_{i t}-\\bar{x}_{i})\\rbrack^{2}. $$\n\nFor the multinomial logit case, the conditional likelihood function takes the form:\n\n$$ L=\\sum_{i}\\ln\\left[\\exp{(\\mathbf{\\upbeta}^{\\prime}\\sum_{t}x_{i t}y_{i t})}/\\sum_{d\\in B_{i}}\\exp{(\\mathbf{\\upbeta}^{\\prime}\\sum_{t}x_{i t}d_{t})}\\right], $$\n\nwhere $B_{i}$ is the set of all possible sequences of outcomes that match the observed sums. This formulation avoids the incidental parameters problem and allows consistent estimation of $\\mathbf{\\beta}$ using standard conditional logit programs.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-76",
    "question": "In the context of the gravity model used by Rose (2004) to assess the impact of GATT/WTO on trade, which of the following statements are empirically supported by the study?",
    "options": [
      "A) Membership in GATT/WTO is associated with a statistically significant increase in bilateral trade flows.",
      "B) The Generalized System of Preferences (GSP) has a strong positive effect on trade, approximately doubling it.",
      "C) The gravity model explains most of the variation in international trade and fits the data well.",
      "D) Regional Trade Agreements (RTAs) have a uniformly positive effect on trade across all agreements considered."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Rose-ReallyKnowWTO-2004.md"
    ],
    "question_context": "The paper employs a gravity model to estimate the effect of multilateral trade agreements on international trade. The model includes variables such as distance, GDP, and various cultural, geographical, and historical factors. The key formula used is:\n\n$$\\begin{array}{r l}{\\ln(X_{i j})=\\beta_{0}+\\beta_{1}\\ln D_{i}+\\beta_{2}\\ln(Y_{i})_{j}}&{}\\ {~}&{~+\\beta_{3}\\ln(Y_{i}^{\\prime}Y_{j}^{\\prime}P o r_{j})_{r}+\\beta_{4}L a n g_{i}}\\ {~}&{~+\\beta_{5}C o n q_{i}+\\beta_{6}\\,L a n d_{i}+\\beta_{1}I s i a n d_{i}}\\ {~}&{~}\\ {~}&{~+\\beta_{3}\\ln(A r e a_{i})+\\beta_{5}C o n c O i_{i j}}\\ {~}&{~}\\ {~}&{~+\\beta_{10}C u r C o l_{i j}+\\beta_{1}C o l o n y_{i}}\\ {~}&{~}\\ {~}&{~+\\beta_{11}C o n N a_{i j}+\\beta_{1}S O U_{i j}}\\ {~}&{~}\\ {~}&{~+\\beta_{14}F T A_{i j},~+\\sum_{i,j}F_{i}+\\gamma_{1}B o t h i n_{i i}}\\ {~}&{~}\\ {~}&{~+\\gamma_{2}O n e i n_{i}n_{i}+\\gamma_{3}G P_{B}+\\varepsilon_{i j},}\\end{array}$$\n\nThe study finds little evidence that GATT/WTO membership significantly affects trade patterns, but the GSP has a strong positive impact. The gravity model itself is noted for its reliability and good fit with the data.",
    "category": "INCORRECT",
    "question_context_used": "The paper employs a gravity model to estimate the effect of multilateral trade agreements on international trade. The model includes variables such as distance, GDP, and various cultural, geographical, and historical factors. The key formula used is:\n\n$$\\begin{array}{r l}{\\ln(X_{i j})=\\beta_{0}+\\beta_{1}\\ln D_{i}+\\beta_{2}\\ln(Y_{i})_{j}}&{}\\ {~}&{~+\\beta_{3}\\ln(Y_{i}^{\\prime}Y_{j}^{\\prime}P o r_{j})_{r}+\\beta_{4}L a n g_{i}}\\ {~}&{~+\\beta_{5}C o n q_{i}+\\beta_{6}\\,L a n d_{i}+\\beta_{1}I s i a n d_{i}}\\ {~}&{~}\\ {~}&{~+\\beta_{3}\\ln(A r e a_{i})+\\beta_{5}C o n c O i_{i j}}\\ {~}&{~}\\ {~}&{~+\\beta_{10}C u r C o l_{i j}+\\beta_{1}C o l o n y_{i}}\\ {~}&{~}\\ {~}&{~+\\beta_{11}C o n N a_{i j}+\\beta_{1}S O U_{i j}}\\ {~}&{~}\\ {~}&{~+\\beta_{14}F T A_{i j},~+\\sum_{i,j}F_{i}+\\gamma_{1}B o t h i n_{i i}}\\ {~}&{~}\\ {~}&{~+\\gamma_{2}O n e i n_{i}n_{i}+\\gamma_{3}G P_{B}+\\varepsilon_{i j},}\\end{array}$$\n\nThe study finds little evidence that GATT/WTO membership significantly affects trade patterns, but the GSP has a strong positive impact. The gravity model itself is noted for its reliability and good fit with the data.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-899",
    "question": "In the context of Riley equilibrium in quadratic signaling games, which of the following conditions ensure that the sender's strategy is linear?",
    "options": [
      "A) The discriminant is non-negative and local belief monotonicity holds.",
      "B) The sender's bliss point is always above the receiver's ideal point for all states.",
      "C) The preference parameter λ > 1 and the singularity point θ₀ ∈ Θ.",
      "D) The state space Θ is discrete and the sender's cost of lying is quadratic."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Linear-Riley-equilibria-in-quadratic-signaling-_2023_Journal-of-Economic-The.md"
    ],
    "question_context": "Key formula: The sender’s payoff is given by $V(\\theta,{\\hat{\\theta}},a_{1})=-(a_{1}-k_{1}\\theta-b_{1})^{2}-\\delta(\\mu(a_{1})-k_{2}\\theta-b_{2})^{2}$. Theorem 2 states that Riley equilibrium is linear if and only if λ > 1 and θ₀ ∈ Θ. The discriminant must be non-negative for the existence of a solution. Local belief monotonicity, defined as $V_{2}(\\theta,\\theta,a_{1})\\neq0$ for all $(\\theta,a_{1})$, is a weaker condition than global belief monotonicity. When λ > 1 and θ₀ ∈ Θ, the sender’s strategy in Riley equilibrium becomes linear, coinciding with certain Bayesian Nash equilibria in the literature.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The sender’s payoff is given by $V(\\theta,{\\hat{\\theta}},a_{1})=-(a_{1}-k_{1}\\theta-b_{1})^{2}-\\delta(\\mu(a_{1})-k_{2}\\theta-b_{2})^{2}$. Theorem 2 states that Riley equilibrium is linear if and only if λ > 1 and θ₀ ∈ Θ. The discriminant must be non-negative for the existence of a solution. Local belief monotonicity, defined as $V_{2}(\\theta,\\theta,a_{1})\\neq0$ for all $(\\theta,a_{1})$, is a weaker condition than global belief monotonicity. When λ > 1 and θ₀ ∈ Θ, the sender’s strategy in Riley equilibrium becomes linear, coinciding with certain Bayesian Nash equilibria in the literature.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2923",
    "question": "Which of the following subfields of economics are represented by faculty from Northwestern University on the Program Committee for the 2003 North American Summer Meeting?",
    "options": [
      "A) Political Theory and Behavioral Economics",
      "B) Auction Theory and Growth and Development",
      "C) Decision Theory and Financial Economics",
      "D) Computational Economics and Social Choice"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Announcements-2003.md"
    ],
    "question_context": "The 2003 North American Summer Meeting of the Econometric Society was held at the Kellogg School of Management, Northwestern University, Evanston, Illinois. The Program Committee included faculty from various institutions, with Northwestern University faculty representing multiple subfields. Key Northwestern faculty on the committee included David Austen-Smith (Political Theory), Kent Daniel (Behavioral Economics), Joel Horowitz (Theoretical Econometrics), Peter Klibanoff (Decision Theory), and Costis Skiadas (Financial Economics). The meeting aimed to cover a broad spectrum of economic research, from theoretical to applied domains.",
    "category": "CORRECT",
    "question_context_used": "The 2003 North American Summer Meeting of the Econometric Society was held at the Kellogg School of Management, Northwestern University, Evanston, Illinois. The Program Committee included faculty from various institutions, with Northwestern University faculty representing multiple subfields. Key Northwestern faculty on the committee included David Austen-Smith (Political Theory), Kent Daniel (Behavioral Economics), Joel Horowitz (Theoretical Econometrics), Peter Klibanoff (Decision Theory), and Costis Skiadas (Financial Economics). The meeting aimed to cover a broad spectrum of economic research, from theoretical to applied domains.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3724",
    "question": "Which of the following conditions are necessary and sufficient for the existence of a social planner solution in the given economic framework?",
    "options": [
      "A) The cost of an additional application h must satisfy the inequality involving Φ(z(t*))",
      "B) The condition k ≥ 1 - e^(-z(t*)) - z(t*)e^(-z(t*)) must hold",
      "C) The optimal queue length z(t*) must be low enough to ensure workers send at least one application",
      "D) The expected wage E[w] must equal 1 in all matches formed in Phase 1 and 0 in Phase 2"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Gautier-SimultaneousSearchEfficiency-2017.md"
    ],
    "question_context": "The existence of the social planner solution requires that the marginal return of an additional vacancy equals its cost as given in equation (14), i.e., maximum matching has to satisfy equation (9). According to Theorem 3 in Frieze and Melsted (2012), this requires first that the condition ν/u < θ holds and second that min[u(t*), ν(t*)] = ν(t*). The key formulas are:\n\nΨ(z(t*)) ≡ 1 + (1 - e^(-z(t*))) / (z(t*)e^(-z(t*))) > a*,\n\nΦ(z(t*)) ≡ (z(t*)(1 - e^(-z(t*)))) / (1 - e^(-z(t*)) - z(t*)e^(-z(t*))) > a*.\n\nAdditionally, the cost of an additional application h has to satisfy:\n\n-(1 - e^(-z(t*)))^(Ψ(z(t*))) ln(1 - e^(-z(t*))) < h,\n\n-(1 - e^(-z(t*)))^(Φ(z(t*))) ln(1 - e^(-z(t*))) < h,\n\nto ensure that vacancy creation is optimal. The conditions h(z(t*)) < h ≤ h̅(z(t*)) and k ≥ 1 - e^(-z(t*)) - z(t*)e^(-z(t*)) are necessary and sufficient to ensure the existence of a social planner solution.",
    "category": "INCORRECT",
    "question_context_used": "The existence of the social planner solution requires that the marginal return of an additional vacancy equals its cost as given in equation (14), i.e., maximum matching has to satisfy equation (9). According to Theorem 3 in Frieze and Melsted (2012), this requires first that the condition ν/u < θ holds and second that min[u(t*), ν(t*)] = ν(t*). The key formulas are:\n\nΨ(z(t*)) ≡ 1 + (1 - e^(-z(t*))) / (z(t*)e^(-z(t*))) > a*,\n\nΦ(z(t*)) ≡ (z(t*)(1 - e^(-z(t*)))) / (1 - e^(-z(t*)) - z(t*)e^(-z(t*))) > a*.\n\nAdditionally, the cost of an additional application h has to satisfy:\n\n-(1 - e^(-z(t*)))^(Ψ(z(t*))) ln(1 - e^(-z(t*))) < h,\n\n-(1 - e^(-z(t*)))^(Φ(z(t*))) ln(1 - e^(-z(t*))) < h,\n\nto ensure that vacancy creation is optimal. The conditions h(z(t*)) < h ≤ h̅(z(t*)) and k ≥ 1 - e^(-z(t*)) - z(t*)e^(-z(t*)) are necessary and sufficient to ensure the existence of a social planner solution.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3727",
    "question": "According to the submission guidelines for the 1981 Summer Meeting of the Econometric Society, which of the following conditions must be satisfied for an abstract to be considered?",
    "options": [
      "A) The abstract must not exceed 300 words in length",
      "B) The abstract must include the author's office telephone number",
      "C) The abstract must represent a manuscript previously presented to other professional organizations",
      "D) The abstract must be labeled with the author's complete address"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Winter1981Econometric-1981.md"
    ],
    "question_context": "The 1981 Summer Meeting of the Econometric Society took place in San Diego, California, from June 24-27, 1981. The program primarily consisted of invited papers, but contributed papers were also considered. Abstracts of papers had to be submitted before February 1, 1981, to the program committee chairman, Professor Robert E. Hall. Each submitted abstract had to be labeled at the top with the author's name, affiliation, complete address, and office telephone number (if possible). Abstracts were required not to exceed 300 words in length and had to represent original manuscripts not previously presented to the Econometric Society or other professional organizations.",
    "category": "INCORRECT",
    "question_context_used": "The 1981 Summer Meeting of the Econometric Society took place in San Diego, California, from June 24-27, 1981. The program primarily consisted of invited papers, but contributed papers were also considered. Abstracts of papers had to be submitted before February 1, 1981, to the program committee chairman, Professor Robert E. Hall. Each submitted abstract had to be labeled at the top with the author's name, affiliation, complete address, and office telephone number (if possible). Abstracts were required not to exceed 300 words in length and had to represent original manuscripts not previously presented to the Econometric Society or other professional organizations.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4216",
    "question": "In the context of comparing delay aversion between two agents within the exponential discounting model, which of the following statements are correct?",
    "options": [
      "A) The discount factor alone is sufficient to compare delay aversion if the agents have identical instantaneous utility functions.",
      "B) The instantaneous utility function plays no role in determining an agent's attitude towards time delay.",
      "C) Even with unique discount factors, comparing delay aversion requires considering both the discount factor and the instantaneous utility function.",
      "D) In the exponential discounting model, an agent with a lower discount factor is always more delay averse than one with a higher discount factor."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Delay aversion.md"
    ],
    "question_context": "The text discusses the challenges in comparing delay aversion between agents, particularly within the exponential discounting model. It highlights that the discount factor alone is insufficient for such comparisons unless agents share identical instantaneous utility functions. The text provides the formula for exponential intertemporal utility: $U(\\mathbf{x}) = \\sum_{t=0}^{\\infty} \\delta^{t} u(x_{t})$, where $\\delta$ is the discount factor and $u$ is the instantaneous utility function. It also presents a counterexample where an agent with a higher discount factor may exhibit stronger present bias due to the shape of their instantaneous utility function, undermining the notion that discount factors alone can determine delay aversion.",
    "category": "CORRECT",
    "question_context_used": "The text discusses the challenges in comparing delay aversion between agents, particularly within the exponential discounting model. It highlights that the discount factor alone is insufficient for such comparisons unless agents share identical instantaneous utility functions. The text provides the formula for exponential intertemporal utility: $U(\\mathbf{x}) = \\sum_{t=0}^{\\infty} \\delta^{t} u(x_{t})$, where $\\delta$ is the discount factor and $u$ is the instantaneous utility function. It also presents a counterexample where an agent with a higher discount factor may exhibit stronger present bias due to the shape of their instantaneous utility function, undermining the notion that discount factors alone can determine delay aversion.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2655",
    "question": "According to the analysis of migration restrictions in China, which of the following statements are empirically supported by the inverted U-shape relationship between city size and productivity?",
    "options": [
      "A) The peak productivity point shifts outward as cities transition from manufacturing to service-based economies.",
      "B) Most Chinese cities operate below the optimal size for maximizing output per worker.",
      "C) Rural-urban migration restrictions have no significant impact on city productivity levels.",
      "D) The hukou system uniformly enhances productivity by preventing overcrowding in cities."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "How-migration-restrictions-limit-agglomeration-an_2006_Journal-of-Developmen.md"
    ],
    "question_context": "The paper estimates a city productivity relationship based on city GDP numbers, showing worker productivity as an inverted U-shape function of city employment. The peak point shifts outward as industrial composition moves from manufacturing to services, consistent with urban theory. Empirical evidence indicates that the majority of Chinese cities are undersized—below the lower bound of the 95% confidence interval for optimal productivity. The formula for this relationship is crucial: output per worker = f(city employment), where f(·) is the inverted U-shape function. Migration restrictions, particularly the hukou system, are identified as key factors inhibiting agglomeration economies, leading to significant productivity losses in both urban and rural industrial sectors.",
    "category": "CORRECT",
    "question_context_used": "The paper estimates a city productivity relationship based on city GDP numbers, showing worker productivity as an inverted U-shape function of city employment. The peak point shifts outward as industrial composition moves from manufacturing to services, consistent with urban theory. Empirical evidence indicates that the majority of Chinese cities are undersized—below the lower bound of the 95% confidence interval for optimal productivity. The formula for this relationship is crucial: output per worker = f(city employment), where f(·) is the inverted U-shape function. Migration restrictions, particularly the hukou system, are identified as key factors inhibiting agglomeration economies, leading to significant productivity losses in both urban and rural industrial sectors.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2402",
    "question": "In the regression discontinuity design (RDD) analysis of oil exploration near national borders, which of the following strategies are employed to address potential endogeneity issues?",
    "options": [
      "A) Utilizing predetermined institutional quality measured before drilling took place",
      "B) Including only borders that have not changed during the period of oil exploration",
      "C) Randomizing the assignment of institutional quality across borders",
      "D) Controlling for the bilateral ranking in initial GDP per capita"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Cust和Harding - 2020 - INSTITUTIONS AND THE LOCATION OF OIL EXPLORATION.md"
    ],
    "question_context": "Three sources of endogeneity may induce bias when estimating the effect of institutional quality on oil exploration. The first is that the geology (the actual likelihood of discovering oil) is not observable to us. The second is that institutional quality is itself an outcome that can be affected by the investment activities under study. The third is that the territory of countries may be correlated with both geology and institutional quality. We seek to overcome these three identification challenges by taking advantage of the location of oil exploration relative to national borders and by utilising the particular sequencing of events in our setting. National borders assign institutions to an area. Data on the precise location of oil wells and national borders allow for a regression discontinuity design, as potential geological deposits of oil often stretch across national borders. By studying oil exploration close to borders, we obtain sharp variation in institutions for the same geology. To deal with the potential simultaneity between activities in the oil sector and institutional quality, we use predetermined institutional quality, that is, measured before the drilling took place. Finally, to make sure that the location of the national borders is not affected by the oil exploration we consider, we include only borders that have not changed during our period of oil exploration. Controlling for the bilateral ranking in initial GDP per capita allows us to estimate the effect of institutions for a given ranking of initial economic development. This is intended to capture any variation across borders that is better explained by differences in income rather than institutional quality, for example, human capital, physical capital, or the availability of financial capital.",
    "category": "CORRECT",
    "question_context_used": "Three sources of endogeneity may induce bias when estimating the effect of institutional quality on oil exploration. The first is that the geology (the actual likelihood of discovering oil) is not observable to us. The second is that institutional quality is itself an outcome that can be affected by the investment activities under study. The third is that the territory of countries may be correlated with both geology and institutional quality. We seek to overcome these three identification challenges by taking advantage of the location of oil exploration relative to national borders and by utilising the particular sequencing of events in our setting. National borders assign institutions to an area. Data on the precise location of oil wells and national borders allow for a regression discontinuity design, as potential geological deposits of oil often stretch across national borders. By studying oil exploration close to borders, we obtain sharp variation in institutions for the same geology. To deal with the potential simultaneity between activities in the oil sector and institutional quality, we use predetermined institutional quality, that is, measured before the drilling took place. Finally, to make sure that the location of the national borders is not affected by the oil exploration we consider, we include only borders that have not changed during our period of oil exploration. Controlling for the bilateral ranking in initial GDP per capita allows us to estimate the effect of institutions for a given ranking of initial economic development. This is intended to capture any variation across borders that is better explained by differences in income rather than institutional quality, for example, human capital, physical capital, or the availability of financial capital.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3822",
    "question": "Which of the following factors were central to the Greenspan era's departure from mechanistic monetary policy rules?",
    "options": [
      "A) Increased transparency in Federal Open Market Committee (FOMC) actions",
      "B) The abandonment of M2 and M3 growth targets as policy guides",
      "C) The Federal Reserve's strict adherence to pre-1990 velocity assumptions",
      "D) The prioritization of inflation targeting over employment goals"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Friedman-GreenspanEraDiscretion-2006.md"
    ],
    "question_context": "The Greenspan era (1987-2006) marked a significant shift in U.S. monetary policy, moving away from mechanistic restrictions. Key changes included increased transparency, such as the immediate announcement of the FOMC's interest rate target post-meetings, and the gradual abandonment of monetary aggregate targets (M2, M3). By 1998, these ranges were explicitly stated not to be policy guides but benchmarks assuming pre-1990 velocity behavior. The era was characterized by flexibility in pursuing the dual mandate of price stability and maximum employment, even when it contradicted conventional economic wisdom, as seen when unemployment fell below 5% without triggering inflation. Formula: The noninflationary growth during this period challenged the traditional Phillips curve relationship, encapsulated by the observation that rapid output growth (2.5% in 1995, 3.7% in 1996, 4.5% in 1997) coexisted with declining inflation (2.1% in 1994 to 1.7% in 1997).",
    "category": "CORRECT",
    "question_context_used": "The Greenspan era (1987-2006) marked a significant shift in U.S. monetary policy, moving away from mechanistic restrictions. Key changes included increased transparency, such as the immediate announcement of the FOMC's interest rate target post-meetings, and the gradual abandonment of monetary aggregate targets (M2, M3). By 1998, these ranges were explicitly stated not to be policy guides but benchmarks assuming pre-1990 velocity behavior. The era was characterized by flexibility in pursuing the dual mandate of price stability and maximum employment, even when it contradicted conventional economic wisdom, as seen when unemployment fell below 5% without triggering inflation. Formula: The noninflationary growth during this period challenged the traditional Phillips curve relationship, encapsulated by the observation that rapid output growth (2.5% in 1995, 3.7% in 1996, 4.5% in 1997) coexisted with declining inflation (2.1% in 1994 to 1.7% in 1997).",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-549",
    "question": "Which of the following were key premises of Garcia's heterodox economic program in Peru during the mid-1980s?",
    "options": [
      "A) Fiscal deficits are inherently inflationary and must be minimized to control inflation.",
      "B) Expansion of domestic demand can raise firms' profitability by increasing capacity utilization.",
      "C) High real interest rates are necessary to encourage savings and productive investment.",
      "D) Income redistribution is compatible with sustained growth if it increases consumption demand."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Macroeconomic-populism_1990_Journal-of-Development-Economics.md"
    ],
    "question_context": "The heterodox program emphasized 'Growth with Redistribution', aiming to reconcile economic efficiency with social equity. A central premise was that expansionary fiscal policies could reduce unit costs by increasing demand, thereby utilizing idle capacity. The program also argued that income redistribution could stimulate consumption, justifying the use of idle resources. Key formula: The fiscal deficit is not necessarily inflationary if domestic demand does not exceed potential output. Policymakers believed that transferring resources to the poorest would increase consumption, thereby reducing unit costs and justifying fiscal deficits. This approach diverged sharply from orthodox IMF stabilization programs, which were seen as recessionary and ineffective in Latin America at the time.",
    "category": "CORRECT",
    "question_context_used": "The heterodox program emphasized 'Growth with Redistribution', aiming to reconcile economic efficiency with social equity. A central premise was that expansionary fiscal policies could reduce unit costs by increasing demand, thereby utilizing idle capacity. The program also argued that income redistribution could stimulate consumption, justifying the use of idle resources. Key formula: The fiscal deficit is not necessarily inflationary if domestic demand does not exceed potential output. Policymakers believed that transferring resources to the poorest would increase consumption, thereby reducing unit costs and justifying fiscal deficits. This approach diverged sharply from orthodox IMF stabilization programs, which were seen as recessionary and ineffective in Latin America at the time.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-709",
    "question": "In the context of rural-urban migration and human capital accumulation, which of the following conditions are necessary for the migration option to reduce the incentive to invest in human capital?",
    "options": [
      "A) The return to education is higher in urban areas than in rural areas for migrants",
      "B) The return to education is lower in urban areas than in rural areas for migrants",
      "C) Migration costs are decreasing over time",
      "D) The probability of migration is higher for individuals with lower education levels"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Human-capital-and-migration--A-cautionary-tale_2024_Journal-of-Econometrics.md"
    ],
    "question_context": "The paper analyzes the impact of migration on human capital accumulation, showing that when the return to education for migrants is lower in the destination than in the origin, the overall incentive to accumulate human capital is reduced as migration restrictions are relaxed. The key condition is expressed mathematically as: \n\n$$A\\equiv\\left[\\mu_{h,u}-\\mu_{\\ell,u}\\right]-\\left[\\mu_{h,r}-\\mu_{\\ell,r}\\right]<0,$$\n\nwhere $\\mu_{s,\\tau}$ represents earnings for schooling level $s$ in location $\\tau$. This condition states that the return to human capital in urban areas is lower than in rural areas for migrants. The paper further demonstrates that this pattern leads to reduced educational attainment when migration becomes easier, particularly when the probability of migration is higher for less educated individuals.",
    "category": "CORRECT",
    "question_context_used": "The paper analyzes the impact of migration on human capital accumulation, showing that when the return to education for migrants is lower in the destination than in the origin, the overall incentive to accumulate human capital is reduced as migration restrictions are relaxed. The key condition is expressed mathematically as: \n\n$$A\\equiv\\left[\\mu_{h,u}-\\mu_{\\ell,u}\\right]-\\left[\\mu_{h,r}-\\mu_{\\ell,r}\\right]<0,$$\n\nwhere $\\mu_{s,\\tau}$ represents earnings for schooling level $s$ in location $\\tau$. This condition states that the return to human capital in urban areas is lower than in rural areas for migrants. The paper further demonstrates that this pattern leads to reduced educational attainment when migration becomes easier, particularly when the probability of migration is higher for less educated individuals.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1719",
    "question": "According to the Koszegi-Rabin framework and the discussion on forced trade, which of the following elements are most likely affected by forced trade, leading to the elimination of exchange asymmetry?",
    "options": [
      "A) Consumption utility derived from the goods themselves",
      "B) Gain/loss utility due to changes in loss aversion",
      "C) Expectations about future trading and the reference point",
      "D) Trade uncertainty and perceived risks associated with trading"
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Engelmann-RECONSIDERINGEFFECTMARKET-2010.md"
    ],
    "question_context": "The model of reference-dependent preferences of Koszegi and Rabin (2006) provides a useful framework to address the question of what may have changed. Their model contains three key elements of individual behavior: \\\"consumption\\\" utility, which is derived from consuming goods, \\\"gain/loss\\\" utility, which is derived from the difference between a reference point and actual consumption, and expectations about future consumption that determine this reference point. Each of these elements may be affected by our forced-trade treatment and thus drive our results. We can safely rule out the possibility that forced trade affects consumption utility in a different way from free trade. Hence, to explain our results in the K?szegi-Rabin framework, some change must have occurred in the gain/loss utility function or in expectations and, thus, the reference point. One potential change in the gain/loss utility function would come from lower loss aversion. The third alternative within the Koszegi-Rabin framework is then that forced trade has shifted expectations about future trading and hence the reference point. The explanation we propose is also based on changes in expectations, but does not rely on loss aversion. Assume that subjects form beliefs not only about their future reference points, but also about the possibility that trading will lead to some costly mistakes, which is what we called trade uncertainty.",
    "category": "CORRECT",
    "question_context_used": "The model of reference-dependent preferences of Koszegi and Rabin (2006) provides a useful framework to address the question of what may have changed. Their model contains three key elements of individual behavior: \\\"consumption\\\" utility, which is derived from consuming goods, \\\"gain/loss\\\" utility, which is derived from the difference between a reference point and actual consumption, and expectations about future consumption that determine this reference point. Each of these elements may be affected by our forced-trade treatment and thus drive our results. We can safely rule out the possibility that forced trade affects consumption utility in a different way from free trade. Hence, to explain our results in the K?szegi-Rabin framework, some change must have occurred in the gain/loss utility function or in expectations and, thus, the reference point. One potential change in the gain/loss utility function would come from lower loss aversion. The third alternative within the Koszegi-Rabin framework is then that forced trade has shifted expectations about future trading and hence the reference point. The explanation we propose is also based on changes in expectations, but does not rely on loss aversion. Assume that subjects form beliefs not only about their future reference points, but also about the possibility that trading will lead to some costly mistakes, which is what we called trade uncertainty.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1849",
    "question": "According to the American Economic Association's copyright and permissions policy, which of the following actions require prior specific permission and/or a fee?",
    "options": [
      "A) Making digital copies of AEA publications for personal use",
      "B) Abstracting with credit from AEA publications",
      "C) Using a component of an AEA work in another work by someone other than the author",
      "D) Posting an AEA publication on servers by someone other than the author"
    ],
    "gold_answer": "C, D",
    "source_files": [
      "ads.md"
    ],
    "question_context": "The American Economic Association's copyright policy states: 'Permission to make digital or hard copies of part or all of American Economic Association publications for personal or classroom use is granted without fee provided that copies are not distributed for profit or direct commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation, including the name of the author.' It further specifies: 'The author has the right to republish, post on servers, redistribute to lists and use any component of this work in other works. For others to do so requires prior specific permission and/or a fee.' Abstracting with credit is permitted without additional requirements.",
    "category": "CORRECT",
    "question_context_used": "The American Economic Association's copyright policy states: 'Permission to make digital or hard copies of part or all of American Economic Association publications for personal or classroom use is granted without fee provided that copies are not distributed for profit or direct commercial advantage and that copies show this notice on the first page or initial screen of a display along with the full citation, including the name of the author.' It further specifies: 'The author has the right to republish, post on servers, redistribute to lists and use any component of this work in other works. For others to do so requires prior specific permission and/or a fee.' Abstracting with credit is permitted without additional requirements.",
    "gold_answer_parsed": [
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"C\", \"D\"]\n}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1951",
    "question": "In the context of max-linear regression models, which of the following statements are true regarding their properties and advantages over traditional linear regression models?",
    "options": [
      "A) Max-linear models maintain interpretability similar to linear models while introducing non-linearity through the max operator.",
      "B) The max operator in max-linear models increases robustness by mitigating the impact of outlying values in predictor variables.",
      "C) Max-linear models require all error terms to be independent and identically distributed across all factors for identifiability.",
      "D) Max-linear models can only be applied when the response variable follows a Gaussian distribution."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Max-linear-regression-models-with-regularization_2021_Journal-of-Econometric.md"
    ],
    "question_context": "The max-linear regression model replaces the linear combination in the linear regression model by a max-linear structure. Independent variables are connected with linear relationships within groups forming factors, and the max relationship between groups forms competing factors. This structure allows non-linear relationships between dependent and independent variables while maintaining linear relationships within groups for simplicity and interpretability. Key properties include: 1) The max operator rules out problems caused by outlying values in features, increasing robustness. 2) The model maintains interpretability similar to linear models while introducing non-linearity. 3) Linear regression is a special case when the number of factors reduces to 1. The model is proven more suitable than black-box machine learning models in some econometric examples, yet still maintains interpretability.",
    "category": "CORRECT",
    "question_context_used": "The max-linear regression model replaces the linear combination in the linear regression model by a max-linear structure. Independent variables are connected with linear relationships within groups forming factors, and the max relationship between groups forms competing factors. This structure allows non-linear relationships between dependent and independent variables while maintaining linear relationships within groups for simplicity and interpretability. Key properties include: 1) The max operator rules out problems caused by outlying values in features, increasing robustness. 2) The model maintains interpretability similar to linear models while introducing non-linearity. 3) Linear regression is a special case when the number of factors reduces to 1. The model is proven more suitable than black-box machine learning models in some econometric examples, yet still maintains interpretability.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-896",
    "question": "In the context of dynamic matching models for pharmaceutical demand, which of the following statements are supported by the counterfactual simulations in the text?",
    "options": [
      "A) Complete information leads to a higher Herfindahl Index due to increased market concentration.",
      "B) Prohibiting experimentation results in a 6% decrease in average utility compared to the baseline.",
      "C) Diagnostic matching by doctors increases market concentration for leading drugs.",
      "D) The absence of diagnostic matching leads to a 11.1% decrease in expected utility."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Crawford-UncertaintyLearningPharmaceutical-2005.md"
    ],
    "question_context": "The text analyzes the effects of uncertainty, learning, and diagnostic matching in pharmaceutical markets using dynamic simulations. Key findings include: (1) Prohibiting experimentation reduces average utility by 6% compared to the baseline (from $-28.7$ to $-30.6$), highlighting the value of learning. (2) Diagnostic matching increases utility by 11.1% but also raises market concentration (Herfindahl index increases). (3) Without diagnostic matching, expected utility falls by 11.1% ($-31.9$ vs. $-28.7$ baseline) while treatment costs rise 30-40%. The model uses Bellman's equation: $$W(S)=\\operatorname*{max}_{k}\\{u_{k}(S)+\\beta E_{S^{\\prime}|S,k}W(S^{\\prime})\\}$$ and Keane-Wolpin approximation to handle high-dimensional state spaces.",
    "category": "CORRECT",
    "question_context_used": "The text analyzes the effects of uncertainty, learning, and diagnostic matching in pharmaceutical markets using dynamic simulations. Key findings include: (1) Prohibiting experimentation reduces average utility by 6% compared to the baseline (from $-28.7$ to $-30.6$), highlighting the value of learning. (2) Diagnostic matching increases utility by 11.1% but also raises market concentration (Herfindahl index increases). (3) Without diagnostic matching, expected utility falls by 11.1% ($-31.9$ vs. $-28.7$ baseline) while treatment costs rise 30-40%. The model uses Bellman's equation: $$W(S)=\\operatorname*{max}_{k}\\{u_{k}(S)+\\beta E_{S^{\\prime}|S,k}W(S^{\\prime})\\}$$ and Keane-Wolpin approximation to handle high-dimensional state spaces.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\", \"C\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-652",
    "question": "Which of the following statements are true regarding the generalized Gini indices of inequality and their statistical properties as discussed in Barrett & Donald (2009)?",
    "options": [
      "A) The S-Gini relative indices of inequality are scale-free and S-Concave, with higher values of the inequality aversion parameter δ placing greater weight on transfers at the bottom of the distribution.",
      "B) The E-Gini indices satisfy the principle of diminishing transfers for α > 1, making them sensitive to differences in individuals' income shares.",
      "C) The bootstrap-t method based on asymptotic theory was found to have inferior size and power properties in small samples compared to other bootstrap methods.",
      "D) The generalized Gini poverty indices can be expressed as composite functions of the proportion of the population below the poverty line and the normalized Gini welfare indices defined over the poor."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "00-Statistical_Inference_with_Generalized_Gini_Indices_of_Inequality__Poverty__and_Welfare.md"
    ],
    "question_context": "The S-Gini relative indices of inequality are given by \n\n$$ I_{R}^{\\delta}=1-\\delta(\\delta-1)\\int_{0}^{1}{(1-p)^{\\delta-2}}L(p)d(p), $$ \n\nwhere $1<\\delta<\\infty$. The index is scale free (homogeneous of degree 0 in $y_{.}$) and S-Concave (Blackorby and Donaldson, 1978). The inequality aversion parameter $\\delta$ is set by the researcher. Higher values of $\\delta$ place progressively greater social weight on transfers involving individuals ranked toward the bottom of the distribution. The special case of $\\delta=2$ corresponds to the popular Gini coefficient.\n\nThe key normative distinction between the S-Gini and E-Gini indices is that only the E-Ginis (for $\\alpha>1$) satisfy the principle of diminishing transfers. The S-Ginis are linear in incomes, and the social weight attached to individuals is a function of their rank in the ordered distribution. The effect of a transfer on the level of measured inequality will be a function of the ranks of the individuals involved in the transfer. However, the E-Ginis are a CES function of the difference between the LC and the line of equality, and the effect of a transfer on measured inequality will be sensitive to the differences in individuals' income shares (see Chakravarty, 1988, for a proof and fuller discussion).\n\nIndices of poverty are closely related to measures of inequality and welfare (Sen 1976). Following Blackorby and Donaldson (1980), normative poverty indices can be expressed as a composite function of the proportion of the population below the poverty line (the head-count ratio), the average income of the poor, and the level of income inequality among the poor:\n\n$$ P=F(z)\\bigg(1-\\frac{\\mu(z)(1-I_{R}(y|y<z))}{z}\\bigg), $$\n\nwhere $z$ is the income level representing the poverty line, $I_{R}(y|y<z)$ is the level of relative inequality among the poor, and $\\mu(z)$ is the average income of the poor. Equivalently, the generalized Gini poverty indices can be expressed as a composite function of the proportion of the population below the poverty line $(F(z))$ and the normalized Gini welfare indices defined over the poor $W(y|z)$.",
    "category": "CORRECT",
    "question_context_used": "The S-Gini relative indices of inequality are given by \n\n$$ I_{R}^{\\delta}=1-\\delta(\\delta-1)\\int_{0}^{1}{(1-p)^{\\delta-2}}L(p)d(p), $$ \n\nwhere $1<\\delta<\\infty$. The index is scale free (homogeneous of degree 0 in $y_{.}$) and S-Concave (Blackorby and Donaldson, 1978). The inequality aversion parameter $\\delta$ is set by the researcher. Higher values of $\\delta$ place progressively greater social weight on transfers involving individuals ranked toward the bottom of the distribution. The special case of $\\delta=2$ corresponds to the popular Gini coefficient.\n\nThe key normative distinction between the S-Gini and E-Gini indices is that only the E-Ginis (for $\\alpha>1$) satisfy the principle of diminishing transfers. The S-Ginis are linear in incomes, and the social weight attached to individuals is a function of their rank in the ordered distribution. The effect of a transfer on the level of measured inequality will be a function of the ranks of the individuals involved in the transfer. However, the E-Ginis are a CES function of the difference between the LC and the line of equality, and the effect of a transfer on measured inequality will be sensitive to the differences in individuals' income shares (see Chakravarty, 1988, for a proof and fuller discussion).\n\nIndices of poverty are closely related to measures of inequality and welfare (Sen 1976). Following Blackorby and Donaldson (1980), normative poverty indices can be expressed as a composite function of the proportion of the population below the poverty line (the head-count ratio), the average income of the poor, and the level of income inequality among the poor:\n\n$$ P=F(z)\\bigg(1-\\frac{\\mu(z)(1-I_{R}(y|y<z))}{z}\\bigg), $$\n\nwhere $z$ is the income level representing the poverty line, $I_{R}(y|y<z)$ is the level of relative inequality among the poor, and $\\mu(z)$ is the average income of the poor. Equivalently, the generalized Gini poverty indices can be expressed as a composite function of the proportion of the population below the poverty line $(F(z))$ and the normalized Gini welfare indices defined over the poor $W(y|z)$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3242",
    "question": "In the context of quantifying FOMC text negativity using the Loughran and McDonald (LM) dictionaries, which of the following statements are correct?",
    "options": [
      "A) The LM dictionaries outperform the Harvard Psychosociological Dictionary in financial/economic contexts due to domain-specific word curation.",
      "B) The net negativity score $N_{i t}$ is calculated as the sum of negative and positive word fractions to capture overall emotivity.",
      "C) The Spearman’s rank correlation between human audit scores and LM net negativity is statistically significant at the 0.01% level.",
      "D) The Vader sentiment analysis tool, despite its advanced NLP features, yields a similar rank correlation to LM due to lack of domain specificity in economics/finance."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Taking the Fed at its Word A New Approach to Estimating Central Bank Objectives using Text Analysis.md"
    ],
    "question_context": "The net negativity scores, $N_{i t}$, are calculated as the fraction of negative words by speaker i at a given FOMC meeting in month $t$ minus the fraction of positive words by speaker $i$ at the same meeting. Using the difference between the negative and positive fractions ensures that the measure is independent of overall emotivity. The Spearman’s rank correlation between human scores and LM net negativity is 0.40, which is statistically significant below the $0.01\\%$ level. The LM dictionaries are superior for classifying economic and financial texts compared to the Harvard Psychosociological Dictionary, which mis-categorizes neutral financial terms. The Vader model, while advanced, is not domain-specific to economics/finance and yields a rank correlation of 0.41 with human scores, similar to LM.",
    "category": "CORRECT",
    "question_context_used": "The net negativity scores, $N_{i t}$, are calculated as the fraction of negative words by speaker i at a given FOMC meeting in month $t$ minus the fraction of positive words by speaker $i$ at the same meeting. Using the difference between the negative and positive fractions ensures that the measure is independent of overall emotivity. The Spearman’s rank correlation between human scores and LM net negativity is 0.40, which is statistically significant below the $0.01\\%$ level. The LM dictionaries are superior for classifying economic and financial texts compared to the Harvard Psychosociological Dictionary, which mis-categorizes neutral financial terms. The Vader model, while advanced, is not domain-specific to economics/finance and yields a rank correlation of 0.41 with human scores, similar to LM.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-762",
    "question": "In the context of labor market search theory developed by C. A. Pissarides, which of the following factors are critical for determining the equilibrium unemployment rate?",
    "options": [
      "A) The matching efficiency between job seekers and vacancies",
      "B) The exogenous job destruction rate",
      "C) The average wage level in the economy",
      "D) The bargaining power of workers in wage negotiations"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Erratum_From_School_to_University_The_Demad_for_Post-Compulsory_Education_in_Britain.md"
    ],
    "question_context": "C. A. Pissarides' work on labor market search theory provides a framework for understanding equilibrium unemployment. The key insight is that unemployment arises from frictions in the matching process between job seekers and vacancies. The equilibrium unemployment rate is determined by the balance between job creation and job destruction. The matching function, which depends on the number of unemployed workers and vacancies, plays a central role. Additionally, the exogenous job destruction rate and the bargaining power of workers influence the steady-state unemployment level. The model can be represented by the following equation: \\[ u = \\frac{\\lambda}{\\lambda + \\theta q(\\theta)} \\], where \\u\\ is the unemployment rate, \\lambda\\ is the job destruction rate, \\theta\\ is labor market tightness, and \\q(\\theta)\\ is the matching function.",
    "category": "CORRECT",
    "question_context_used": "C. A. Pissarides' work on labor market search theory provides a framework for understanding equilibrium unemployment. The key insight is that unemployment arises from frictions in the matching process between job seekers and vacancies. The equilibrium unemployment rate is determined by the balance between job creation and job destruction. The matching function, which depends on the number of unemployed workers and vacancies, plays a central role. Additionally, the exogenous job destruction rate and the bargaining power of workers influence the steady-state unemployment level. The model can be represented by the following equation: \\[ u = \\frac{\\lambda}{\\lambda + \\theta q(\\theta)} \\], where \\u\\ is the unemployment rate, \\lambda\\ is the job destruction rate, \\theta\\ is labor market tightness, and \\q(\\theta)\\ is the matching function.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1834",
    "question": "Which of the following statements are true regarding the dynamic type-1 and type-2 sanctions in the context of income support programs for present-biased individuals?",
    "options": [
      "A) The type-1 sanction generates a dynamic work disincentive by making income support unavailable if the individual worked last period.",
      "B) The type-2 sanction generates a dynamic work incentive by making income support unavailable if the individual did not work last period.",
      "C) The type-1 sanction increases average utility by providing a means for the current self to commit future selves to work.",
      "D) The type-2 sanction is preferred by a majority of individuals when policies are made revenue-neutral."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Welfare Dependence and Self-Control An Empirical Analysis.md"
    ],
    "question_context": "A present-biased individual makes inefficient intertemporal choices from a time-consistent perspective. She tends to underestimate the future value of human capital accumulation, leading to low employment. Several types of dynamic interventions are proposed, using income support as an example. The type-1 sanction generates a dynamic work disincentive among all individuals—if an individual does not work now, she can avoid the sanction altogether next period. By contrast, the type-2 sanction generates a dynamic work incentive among all individuals. The type-1 sanction increases average utility by $0.4; this suggests that the policy can alleviate the commitment problem sufficiently to make individuals better off on average. The type-2 sanction increases employment by a larger degree (2.2 percentage points) but reduces average utility by $12.7, and only 3.9% of individuals prefer it to no sanctions.",
    "category": "CORRECT",
    "question_context_used": "A present-biased individual makes inefficient intertemporal choices from a time-consistent perspective. She tends to underestimate the future value of human capital accumulation, leading to low employment. Several types of dynamic interventions are proposed, using income support as an example. The type-1 sanction generates a dynamic work disincentive among all individuals—if an individual does not work now, she can avoid the sanction altogether next period. By contrast, the type-2 sanction generates a dynamic work incentive among all individuals. The type-1 sanction increases average utility by $0.4; this suggests that the policy can alleviate the commitment problem sufficiently to make individuals better off on average. The type-2 sanction increases employment by a larger degree (2.2 percentage points) but reduces average utility by $12.7, and only 3.9% of individuals prefer it to no sanctions.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-226",
    "question": "Based on the analysis of British government consols from 1900 to 1920, which of the following statements are empirically supported by the data?",
    "options": [
      "A) The holding period return formula incorporates both price changes and coupon payments, adjusted for market closures.",
      "B) The implicit yield on consols exhibited a sharp decline during World War I due to reduced government spending.",
      "C) Serial correlation in holding returns was statistically significant, indicating market overreaction to news.",
      "D) The selection of news events excluded those with known advance notice or spanning multiple weeks to avoid ambiguity."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Elmendorf-EffectNewsBond-1996.md"
    ],
    "question_context": "The holding period return on consols is defined as \n\n$$h_{t}=100*\\Bigg[\\frac{c_{t}+(p_{t}-p_{t-1})}{p_{t-1}}\\Bigg],$$\n\nwhere $\\scriptstyle p_{t}$ is the average of closing bid and asked prices for each Friday (or for the previous day if the market was closed on Friday), and $c_{t}$ is the coupon payment (if any) received during that week. The analysis excludes data from August through December 1914 when the market was closed entirely, and from December 1914 through November 1915, when there was a binding floor on the consol price.\n\nWe draw our list of important news events from Steinberg's Historical Tables (1973) and Langer's An Encyclopedia of World History (1972). Importantly, neither source uses financial market activity to identify these events. We use every date listed in these sources with the exception of events that were obviously known in advance (such as coronations) and events that spanned more than one week (such as conferences) because it is unclear when they actually produced news.",
    "category": "INCORRECT",
    "question_context_used": "The holding period return on consols is defined as \n\n$$h_{t}=100*\\Bigg[\\frac{c_{t}+(p_{t}-p_{t-1})}{p_{t-1}}\\Bigg],$$\n\nwhere $\\scriptstyle p_{t}$ is the average of closing bid and asked prices for each Friday (or for the previous day if the market was closed on Friday), and $c_{t}$ is the coupon payment (if any) received during that week. The analysis excludes data from August through December 1914 when the market was closed entirely, and from December 1914 through November 1915, when there was a binding floor on the consol price.\n\nWe draw our list of important news events from Steinberg's Historical Tables (1973) and Langer's An Encyclopedia of World History (1972). Importantly, neither source uses financial market activity to identify these events. We use every date listed in these sources with the exception of events that were obviously known in advance (such as coronations) and events that spanned more than one week (such as conferences) because it is unclear when they actually produced news.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-366",
    "question": "Which of the following statements are supported by the text regarding geopolitical risk (GPR) indices and their implications?",
    "options": [
      "A) GPR indices accurately capture the timing and intensity of adverse geopolitical events across countries and over time.",
      "B) Higher geopolitical risk is associated with increased firm-level investment in exposed industries.",
      "C) Geopolitical risks can influence trade flows and global supply chains through firms' actions and government policies.",
      "D) Newspapers' attention to geopolitical events is a perfect proxy for investor perceptions without requiring additional validation."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Measuring Geopolitical Risk.md"
    ],
    "question_context": "The proposed geopolitical risk (GPR) indices measure the threat, realization, and escalation of adverse geopolitical events, with validation confirming their accuracy in capturing event timing and intensity across countries and over time. Higher geopolitical risk foreshadows lower investment and is associated with higher disaster probability and larger downside risks to GDP growth. The adverse consequences are stronger for firms in more exposed industries, and high firm-level geopolitical risk correlates with lower firm-level investment.\n\nGeopolitical risks can impact the price of risky assets and capital flows across countries, as well as shape trade flows and global supply chains through firms’ actions and government policies. While newspapers’ attention to geopolitical events is hypothesized to reflect investor perceptions, the text suggests the need for future extensions using additional sources like foreign-language publications or parliamentary debate transcripts for more comprehensive measurement.",
    "category": "CORRECT",
    "question_context_used": "The proposed geopolitical risk (GPR) indices measure the threat, realization, and escalation of adverse geopolitical events, with validation confirming their accuracy in capturing event timing and intensity across countries and over time. Higher geopolitical risk foreshadows lower investment and is associated with higher disaster probability and larger downside risks to GDP growth. The adverse consequences are stronger for firms in more exposed industries, and high firm-level geopolitical risk correlates with lower firm-level investment.\n\nGeopolitical risks can impact the price of risky assets and capital flows across countries, as well as shape trade flows and global supply chains through firms’ actions and government policies. While newspapers’ attention to geopolitical events is hypothesized to reflect investor perceptions, the text suggests the need for future extensions using additional sources like foreign-language publications or parliamentary debate transcripts for more comprehensive measurement.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-308",
    "question": "In the context of eye-tracking studies of game-theoretic learning, which of the following statements are supported by the experimental findings?",
    "options": [
      "A) Adaptive learning models predict choices more accurately than sophisticated learning models",
      "B) Sophisticated learning models predict choices more accurately than adaptive learning models",
      "C) Players look at payoff information required by sophisticated models as often as adaptive-model payoffs",
      "D) Eye-tracking data alone can precisely identify learning rules without considering choice data"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Knoepfle-StudyingLearningGames-2009.md"
    ],
    "question_context": "The study uses eye-tracking to observe information acquisition in $4\\times4$ normal-form games, comparing adaptive and sophisticated learning models. Key findings include: (1) Adaptive models (EWA and self-tuning EWA) predict choices more accurately than simple sophisticated models where agents anticipate others' learning. (2) However, lookup data shows players examine payoff information required by sophisticated models about equally as often as adaptive-model payoffs. This discrepancy suggests potential for developing models that better align lookup patterns with choice behavior. The experimental design involved subjects playing four asymmetric non-zero-sum two-player $4\\times4$ games 10 times each, with two subjects per group being eye-tracked.",
    "category": "CORRECT",
    "question_context_used": "The study uses eye-tracking to observe information acquisition in $4\\times4$ normal-form games, comparing adaptive and sophisticated learning models. Key findings include: (1) Adaptive models (EWA and self-tuning EWA) predict choices more accurately than simple sophisticated models where agents anticipate others' learning. (2) However, lookup data shows players examine payoff information required by sophisticated models about equally as often as adaptive-model payoffs. This discrepancy suggests potential for developing models that better align lookup patterns with choice behavior. The experimental design involved subjects playing four asymmetric non-zero-sum two-player $4\\times4$ games 10 times each, with two subjects per group being eye-tracked.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2720",
    "question": "In the context of the social planner's optimization problem with CARA preferences, which of the following conditions are necessary for the optimal benefit level to be characterized by the first-order condition?",
    "options": [
      "A) The implicit functions τ̃(b), s̃(b), and ẽ(b) must satisfy the incentive compatibility constraints ICₑ, ICₛ, and the budget constraint BC.",
      "B) The agent's perceived expected utility must be strictly concave in the benefit level b.",
      "C) The adjusted Baily formula must hold with equality at the optimal benefit level.",
      "D) The term Q(b) must be positive, ensuring ẽ′(b) < 0 for the optimal benefit level."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Spinnewijn-UNEMPLOYEDOPTIMISTICOPTIMAL-2015.md"
    ],
    "question_context": "The social planner solves the optimization problem with the objective function involving utility maximization under constraints. The implicit functions τ̃(b), s̃(b), and ẽ(b) are defined by the incentive compatibility constraints ICₑ, ICₛ, and the budget constraint BC. The first-order condition for the optimal benefit level is derived as:\n\n$$\n\\frac{d U}{d b}=0\\Leftrightarrow\\frac{\\partial U}{\\partial b}+\\frac{\\partial U}{\\partial\\tau}\\tilde{\\tau}^{\\prime}\\left(b\\right)+\\frac{\\partial U}{\\partial e}\\tilde{e}^{\\prime}\\left(b\\right)+\\frac{\\partial U}{\\partial s}\\tilde{s}^{\\prime}\\left(b\\right)=0\n$$\n\nAdditionally, the term Q(b) must be positive to ensure that ẽ′(b) < 0, which is a critical condition for the optimal benefit level. The adjusted Baily formula is derived from this expression, linking the marginal utilities and elasticities in the optimization framework.",
    "category": "INCORRECT",
    "question_context_used": "The social planner solves the optimization problem with the objective function involving utility maximization under constraints. The implicit functions τ̃(b), s̃(b), and ẽ(b) are defined by the incentive compatibility constraints ICₑ, ICₛ, and the budget constraint BC. The first-order condition for the optimal benefit level is derived as:\n\n$$\n\\frac{d U}{d b}=0\\Leftrightarrow\\frac{\\partial U}{\\partial b}+\\frac{\\partial U}{\\partial\\tau}\\tilde{\\tau}^{\\prime}\\left(b\\right)+\\frac{\\partial U}{\\partial e}\\tilde{e}^{\\prime}\\left(b\\right)+\\frac{\\partial U}{\\partial s}\\tilde{s}^{\\prime}\\left(b\\right)=0\n$$\n\nAdditionally, the term Q(b) must be positive to ensure that ẽ′(b) < 0, which is a critical condition for the optimal benefit level. The adjusted Baily formula is derived from this expression, linking the marginal utilities and elasticities in the optimization framework.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1530",
    "question": "In the context of specific and ad-valorem tariffs, which of the following statements are correct based on the propositions and proofs provided?",
    "options": [
      "A) A specific tariff that does not exclude buyers from the market enhances national welfare by shifting profits from the foreign firm to the government.",
      "B) An ad-valorem tariff that does not exclude buyers from the market unambiguously improves national welfare by increasing tariff revenues.",
      "C) A specific tariff that excludes buyers from the market may lead to a decline in national welfare if consumer prices rise by more than the tariff.",
      "D) An ad-valorem tariff that does not exclude buyers from the market leads to a decline in the quality of all imported goods and lowers consumer surplus."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Commercial Policy and Imperfect Discrimination by a Foreign Monopolist.md"
    ],
    "question_context": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism...\n\nSpecific Tariffs. It is well known that in single product industries with variable quality the imposition of a specific tariff leads to higher quality imports, higher consumer prices, and the impact on welfare was found to be ambiguous.14 The question is whether these results extend to the multiproduct case as well.\n\nConsider a unit tariff, $t$ , which is imposed on all imported goods, regardless of quality. After incorporating the tariff in the profit function as defined in (5), we get\n\n$$\n\\pi^{t}=\\pi_{1}^{t}+\\pi_{2}^{t}+\\pi_{3}^{t}\n$$\n\nwhere\n\n$$\n\\begin{array}{r l}&{\\pi_{3}^{t}=f_{3}[V(q_{3};\\theta_{3})-t-c(q_{3})]}\\ &{\\pi_{2}^{t}=(f_{2}+f_{3})V(q_{2};\\theta_{2})-f_{3}V(q_{2};\\theta_{3})-f_{2}[c(q_{2})+t]}\\ &{\\pi_{1}^{t}=F V(q_{1};\\theta_{1})-(f_{2}+f_{3})V(q_{1};\\theta_{2})-f_{1}[c(q_{1})+t]}\\end{array}\n$$\n\nFirst, we consider the case where $\\pi_{1}^{t}(q_{1}^{*})\\geq0$ ; it is still profitable to sell to the marginal buyers, type $\\theta_{1}$ . In this case, the specific tariff does not affect the quality of imports, since the first-order conditions (6)-(8) are not affected by the tariff. In the second case, where $\\pi_{1}^{t}(q_{1}^{*})<0$ (but $\\pi_{2}^{t}(q_{2}^{*})\\geq0)$ , the foreign firm will refrain from selling to type $\\theta_{1}$ buyers. Here, the relevant first-order conditions are (6) and (7). Types $\\theta_{2}$ and $\\theta_{3}$ continue to get the same quality as in the absence of tariffs, whereas $q_{1}=0$ We can state now the following results.\n\nPROPosrTioN 3. A specific tariff that does not entail exclusion of buyers from the market, does not affect consumer prices, and shifts profits from the foreign firm to thetariff-imposing government.Thus,the national welfare is enhanced.\n\nPRooF. First, we show that the tariff does not affect consumer prices. Since $q_{1}^{*}$ is not affected by the tariff, in view of (4), which holds as an equality, $p_{1}^{*}$ remains the same. Since $q_{2}^{*}$ is unaffected, and since the right side of (3) is unaffected, ${p}_{2}^{*}$ stays the same. This also implies that $p_{3}^{*}$ is the same as in the absence of tariffs. Thus, the foreign monopolist absorbs the tariff entirely. National income rises by the amount of tariff revenues collected, which, if redistributed back to consumers, raises national welfare. Q.E.D.\n\nPROPosITioN 4.A specific tariff that entails buyer exclusion leads to higher consumer prices. If the increase in price exceeds the tariff, national welfare declines.\n\nPROOF.Since $\\pi_{1}^{t}(\\cdot)<0$ , the firm does not offer the pair $(p_{1},q_{1})$ .Now the relevant self-selection constraints are (2') and $\\left(3^{\\prime}\\right)$\n\n$$\n\\begin{array}{l}{{V(q_{3}^{*};\\theta_{3})-p_{3}^{t}\\ge V(q_{2}^{*};\\theta_{3})-p_{2}^{t}}}\\\\ {{}}\\\\ {{V(q_{2}^{*};\\theta_{2})-p_{2}^{t}\\ge0.}}\\end{array}\n$$\n\nThe prices with superscript $t$ denote tariff inclusive prices. Now the marginal\n\nbuyers are consumers of type $\\theta_{2}$ . Since $\\left(3^{\\prime}\\right)$ holds as an equality, it follows that ${p}_{2}^{t}>{p}_{2}^{*}$ . This, in turn, implies that $p_{3}^{t}>p_{3}^{*}$ by $\\left(2^{\\prime}\\right)$ . Moreover, the price change will exceed (will be less than) the tariff depending on whether\n\n$$\nV(q_{2}^{*};\\theta_{2})-p_{2}^{*}\\gtrless t.\n$$\n\nIf the consumer prices rise by more than the tariff itself, the loss in consumer surplus exceeds the gains from tariff revenues and thus national welfare declines.\n\nConversely, if prices rise by less than the tariff, and tariff revenues are redistributed back to consumers, the national welfare will improve.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism...\n\nSpecific Tariffs. It is well known that in single product industries with variable quality the imposition of a specific tariff leads to higher quality imports, higher consumer prices, and the impact on welfare was found to be ambiguous.14 The question is whether these results extend to the multiproduct case as well.\n\nConsider a unit tariff, $t$ , which is imposed on all imported goods, regardless of quality. After incorporating the tariff in the profit function as defined in (5), we get\n\n$$\n\\pi^{t}=\\pi_{1}^{t}+\\pi_{2}^{t}+\\pi_{3}^{t}\n$$\n\nwhere\n\n$$\n\\begin{array}{r l}&{\\pi_{3}^{t}=f_{3}[V(q_{3};\\theta_{3})-t-c(q_{3})]}\\ &{\\pi_{2}^{t}=(f_{2}+f_{3})V(q_{2};\\theta_{2})-f_{3}V(q_{2};\\theta_{3})-f_{2}[c(q_{2})+t]}\\ &{\\pi_{1}^{t}=F V(q_{1};\\theta_{1})-(f_{2}+f_{3})V(q_{1};\\theta_{2})-f_{1}[c(q_{1})+t]}\\end{array}\n$$\n\nFirst, we consider the case where $\\pi_{1}^{t}(q_{1}^{*})\\geq0$ ; it is still profitable to sell to the marginal buyers, type $\\theta_{1}$ . In this case, the specific tariff does not affect the quality of imports, since the first-order conditions (6)-(8) are not affected by the tariff. In the second case, where $\\pi_{1}^{t}(q_{1}^{*})<0$ (but $\\pi_{2}^{t}(q_{2}^{*})\\geq0)$ , the foreign firm will refrain from selling to type $\\theta_{1}$ buyers. Here, the relevant first-order conditions are (6) and (7). Types $\\theta_{2}$ and $\\theta_{3}$ continue to get the same quality as in the absence of tariffs, whereas $q_{1}=0$ We can state now the following results.\n\nPROPosrTioN 3. A specific tariff that does not entail exclusion of buyers from the market, does not affect consumer prices, and shifts profits from the foreign firm to thetariff-imposing government.Thus,the national welfare is enhanced.\n\nPRooF. First, we show that the tariff does not affect consumer prices. Since $q_{1}^{*}$ is not affected by the tariff, in view of (4), which holds as an equality, $p_{1}^{*}$ remains the same. Since $q_{2}^{*}$ is unaffected, and since the right side of (3) is unaffected, ${p}_{2}^{*}$ stays the same. This also implies that $p_{3}^{*}$ is the same as in the absence of tariffs. Thus, the foreign monopolist absorbs the tariff entirely. National income rises by the amount of tariff revenues collected, which, if redistributed back to consumers, raises national welfare. Q.E.D.\n\nPROPosITioN 4.A specific tariff that entails buyer exclusion leads to higher consumer prices. If the increase in price exceeds the tariff, national welfare declines.\n\nPROOF.Since $\\pi_{1}^{t}(\\cdot)<0$ , the firm does not offer the pair $(p_{1},q_{1})$ .Now the relevant self-selection constraints are (2') and $\\left(3^{\\prime}\\right)$\n\n$$\n\\begin{array}{l}{{V(q_{3}^{*};\\theta_{3})-p_{3}^{t}\\ge V(q_{2}^{*};\\theta_{3})-p_{2}^{t}}}\\\\ {{}}\\\\ {{V(q_{2}^{*};\\theta_{2})-p_{2}^{t}\\ge0.}}\\end{array}\n$$\n\nThe prices with superscript $t$ denote tariff inclusive prices. Now the marginal\n\nbuyers are consumers of type $\\theta_{2}$ . Since $\\left(3^{\\prime}\\right)$ holds as an equality, it follows that ${p}_{2}^{t}>{p}_{2}^{*}$ . This, in turn, implies that $p_{3}^{t}>p_{3}^{*}$ by $\\left(2^{\\prime}\\right)$ . Moreover, the price change will exceed (will be less than) the tariff depending on whether\n\n$$\nV(q_{2}^{*};\\theta_{2})-p_{2}^{*}\\gtrless t.\n$$\n\nIf the consumer prices rise by more than the tariff itself, the loss in consumer surplus exceeds the gains from tariff revenues and thus national welfare declines.\n\nConversely, if prices rise by less than the tariff, and tariff revenues are redistributed back to consumers, the national welfare will improve.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2982",
    "question": "Under the conditions of Lemma A.3, which of the following statements are true regarding the eigenvalue/eigenvector pairs of $M$ and $Z M Z^{\\prime}$?",
    "options": [
      "A) If $(\\lambda, x)$ is an eigenvalue/eigenvector pair for $M$, then $(\\lambda, Z x)$ must be an eigenvalue/eigenvector pair for $Z M Z^{\\prime}$.",
      "B) The eigenvalues of $M$ and $Z M Z^{\\prime}$ are identical, but their eigenvectors are related by the transformation $Z$.",
      "C) The eigenvectors of $M$ and $Z M Z^{\\prime}$ are identical, but their eigenvalues are related by the transformation $Z$.",
      "D) If $(\\lambda, Z x)$ is an eigenvalue/eigenvector pair for $Z M Z^{\\prime}$, then $(\\lambda, x)$ must be an eigenvalue/eigenvector pair for $M$."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Evolution-in-Bayesian-games-II--Stability-of-purif_2007_Journal-of-Economic-.md"
    ],
    "question_context": "Lemma A.3 states that if $M\\in\\mathbf{R}^{n\\times n}$ maps $\\mathbf{R}_{0}^{n}$ into itself, and $x\\in\\mathbf{R}_{0}^{n}$ (or $\\mathbf{C}_{0}^{n}$), then $(\\lambda,x)$ is an eigenvalue/eigenvector pair for $M$ if and only if $(\\lambda,Z x)$ is an eigenvalue/eigenvector pair for $Z M Z^{\\prime}$. The proof leverages Lemma A.2, which establishes that $Z$ defines a bijective map from $\\mathbf{R}_{0}^{n}$ to $\\mathbf{R}^{n-1}$ and that $Z^{\\prime}Z=\\varPhi$, the orthogonal projection of $\\mathbf{R}^{n}$ onto $\\mathbf{R}_{0}^{n}$. The proof involves showing that $M x=\\lambda x$ implies $Z M Z^{\\prime}Z x=\\lambda Z x$, and conversely, that $Z M Z^{\\prime}Z x=\\lambda Z x$ implies $M x=\\lambda x$ due to the bijective property of $Z$.",
    "category": "INCORRECT",
    "question_context_used": "Lemma A.3 states that if $M\\in\\mathbf{R}^{n\\times n}$ maps $\\mathbf{R}_{0}^{n}$ into itself, and $x\\in\\mathbf{R}_{0}^{n}$ (or $\\mathbf{C}_{0}^{n}$), then $(\\lambda,x)$ is an eigenvalue/eigenvector pair for $M$ if and only if $(\\lambda,Z x)$ is an eigenvalue/eigenvector pair for $Z M Z^{\\prime}$. The proof leverages Lemma A.2, which establishes that $Z$ defines a bijective map from $\\mathbf{R}_{0}^{n}$ to $\\mathbf{R}^{n-1}$ and that $Z^{\\prime}Z=\\varPhi$, the orthogonal projection of $\\mathbf{R}^{n}$ onto $\\mathbf{R}_{0}^{n}$. The proof involves showing that $M x=\\lambda x$ implies $Z M Z^{\\prime}Z x=\\lambda Z x$, and conversely, that $Z M Z^{\\prime}Z x=\\lambda Z x$ implies $M x=\\lambda x$ due to the bijective property of $Z$.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2349",
    "question": "In the context of Spontaneous Order emergence in production environments, which of the following factors are empirically shown to significantly enhance coordination towards specialisation?",
    "options": [
      "A) Messages promoting a team spirit (Coordination)",
      "B) Messages expressing confusion regarding the task",
      "C) Messages proposing strategies or seeking advice (Proposal)",
      "D) References to past successful actions (History)"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Georgalos和Hey - 2020 - Testing for the emergence of spontaneous order.md"
    ],
    "question_context": "The study investigates Spontaneous Order in a production environment where subjects must coordinate to specialize, maximizing social welfare without external intervention. Key findings highlight the role of communication: 'Messages that promote a team spirit (Coordination), messages where subjects seek or provide suggestions on what to do (Proposal) and messages referring to the past actions of the group (History), are always significant and have a positive sign.' The aggregate production function is such that if the aggregate amount provided of any one input was zero, the output would be zero, implying the necessity for co-ordination. Specialization messages, while positive, were not always significant, whereas Proposal and Coordination messages had the strongest effects, especially in complex tasks.",
    "category": "CORRECT",
    "question_context_used": "The study investigates Spontaneous Order in a production environment where subjects must coordinate to specialize, maximizing social welfare without external intervention. Key findings highlight the role of communication: 'Messages that promote a team spirit (Coordination), messages where subjects seek or provide suggestions on what to do (Proposal) and messages referring to the past actions of the group (History), are always significant and have a positive sign.' The aggregate production function is such that if the aggregate amount provided of any one input was zero, the output would be zero, implying the necessity for co-ordination. Specialization messages, while positive, were not always significant, whereas Proposal and Coordination messages had the strongest effects, especially in complex tasks.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-809",
    "question": "In the context of the CARA utility model with idiosyncratic income fluctuations, which of the following conditions are necessary for the stationary distribution of wealth to exhibit exponential tail decay?",
    "options": [
      "A) The income process must have at least two distinct states with differing savings rates",
      "B) The interest rate must adjust to satisfy the general equilibrium condition (4.6)",
      "C) The mortality rate must be strictly positive (φ > 0)",
      "D) The coefficient of absolute risk aversion must be bounded (γ < ∞)"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "TAIL_BEHAVIOR_OF_STOPPED_LEVY_PROCESSES_WITH_MARKOV_MODULATION.md"
    ],
    "question_context": "The model considers agents with CARA utility facing idiosyncratic income fluctuations governed by a finite-state Markov process. The wealth dynamics are given by the ODE: \n\n$$\\frac{\\mathrm{d}W_{t}^{C}}{\\mathrm{d}t}=r W_{t}^{C}+Y_{t}-C_{t},$$\n\nwith optimal consumption rule:\n\n$$C_{t}^{*}=r W_{t}^{*}+b_{n}\\quad while\\ J_{t}=n.$$\n\nProposition 4.2 establishes that exponential tail decay occurs when $b_{n}<y_{n}$ and $b_{n′}>y_{n′}$ for some states $n,n′$. Proposition 4.3 shows this condition is automatically satisfied in general equilibrium when income takes at least two distinct values. The mortality rate φ > 0 is necessary for stationarity but not directly for tail properties. The unboundedness of CARA utility (γ > 0) is precisely what leads to different tail behavior compared to bounded relative risk aversion models.",
    "category": "CORRECT",
    "question_context_used": "The model considers agents with CARA utility facing idiosyncratic income fluctuations governed by a finite-state Markov process. The wealth dynamics are given by the ODE: \n\n$$\\frac{\\mathrm{d}W_{t}^{C}}{\\mathrm{d}t}=r W_{t}^{C}+Y_{t}-C_{t},$$\n\nwith optimal consumption rule:\n\n$$C_{t}^{*}=r W_{t}^{*}+b_{n}\\quad while\\ J_{t}=n.$$\n\nProposition 4.2 establishes that exponential tail decay occurs when $b_{n}<y_{n}$ and $b_{n′}>y_{n′}$ for some states $n,n′$. Proposition 4.3 shows this condition is automatically satisfied in general equilibrium when income takes at least two distinct values. The mortality rate φ > 0 is necessary for stationarity but not directly for tail properties. The unboundedness of CARA utility (γ > 0) is precisely what leads to different tail behavior compared to bounded relative risk aversion models.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4606",
    "question": "In the recursive formulation of the government's problem with state variable $b$, which of the following conditions must hold for the allocation to solve equation (12)?",
    "options": [
      "A) The budget constraint must bind with equality",
      "B) The participation constraints must be satisfied for all $z\\in{\\cal Z}$",
      "C) The promise-keeping constraint (13) must bind",
      "D) The value function $V(b)$ must be strictly increasing"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Investment Cycles and Sovereign Debt Overhang.md"
    ],
    "question_context": "The government’s problem in recursive form with state variable $b$ is given by:\n\n$$\nV(b)=\\operatorname*{max}_{c(z),b(z),k}{\\mathbb{E}}[u(c(z))+\\beta V(b(z))],\n$$\n\nsubject to:\n\n$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}[c(z)+(1+r)b]=\\mathbb{E}[F(z,k)-(r+\\delta)k+b(z)]}\\ &{}&{u(c(z))+\\beta V(b(z))\\ge U(F(z,k))+\\beta V_{\\mathrm{aut}},\\quad\\forall z\\in{\\cal Z}}\\ &{}&{V(b(z))\\ge V_{\\mathrm{aut}},\\quad\\forall z\\in{\\cal Z}.}\\end{array}\n$$\n\nThe first constraint is the expected budget constraint, derived from equation (7), where we have substituted in the first-order condition for foreign direct investment. The second and third constraints ensure participation. Note that optimality ensures that the budget constraint binds with equality. Therefore, $V(b)$ is strictly decreasing and has an inverse. By definition, $B(\\upsilon)/(1+r)$ is this inverse, that is $V(B(\\upsilon)/(1+r))=\\upsilon$. Therefore, an allocation that solves equation (A.7) must also solve equation (12). The converse is true as long as the promise-keeping constraint (13) binds.",
    "category": "CORRECT",
    "question_context_used": "The government’s problem in recursive form with state variable $b$ is given by:\n\n$$\nV(b)=\\operatorname*{max}_{c(z),b(z),k}{\\mathbb{E}}[u(c(z))+\\beta V(b(z))],\n$$\n\nsubject to:\n\n$$\n\\begin{array}{r l r}&{}&{\\mathbb{E}[c(z)+(1+r)b]=\\mathbb{E}[F(z,k)-(r+\\delta)k+b(z)]}\\ &{}&{u(c(z))+\\beta V(b(z))\\ge U(F(z,k))+\\beta V_{\\mathrm{aut}},\\quad\\forall z\\in{\\cal Z}}\\ &{}&{V(b(z))\\ge V_{\\mathrm{aut}},\\quad\\forall z\\in{\\cal Z}.}\\end{array}\n$$\n\nThe first constraint is the expected budget constraint, derived from equation (7), where we have substituted in the first-order condition for foreign direct investment. The second and third constraints ensure participation. Note that optimality ensures that the budget constraint binds with equality. Therefore, $V(b)$ is strictly decreasing and has an inverse. By definition, $B(\\upsilon)/(1+r)$ is this inverse, that is $V(B(\\upsilon)/(1+r))=\\upsilon$. Therefore, an allocation that solves equation (A.7) must also solve equation (12). The converse is true as long as the promise-keeping constraint (13) binds.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4038",
    "question": "In the context of econometric modeling of the UK export sector, which of the following methodological critiques are explicitly raised by the author?",
    "options": [
      "A) The suppression of statistically significant but 'implausible' coefficients introduces subjectivity",
      "B) The reliance on aggregate studies masks important disaggregated patterns in export behavior",
      "C) The use of OLS estimation without rigorous theoretical derivation is methodologically unsound",
      "D) The conversion between SITC and SIC commodity classifications provides a robust data link"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Econometric_Model_of_the_Export_Sector_UK_Visible_Exports_and_Their_Prices_1955-1973.md"
    ],
    "question_context": "The research strategy is clearly spelled out: first a fairly general equation (for which no particular economic theory is offered) is estimated by OLS. Then any 'implausible' coefficients are suppressed and the equations re-estimated. Then any insignificant (at the 5% level) coefficients are suppressed and finally a significant, plausible set of variables is derived. If forecasting really is the name of the game then there is really no case for suppressing implausible coefficients. Furthermore, unless the original equation was rigorously derived from impeccable economic theory then 'plausibility' tends to be rather subjective. One important result of this piece of work is the strong case which emerges for disaggregation by commodity and by destination. Aggregate studies do provide some explanation of exports but mask a great deal more. It is not clear what the optimum level of disaggregation would be but Winters suggests that only by studying individual firms and particular markets can one hope to offer a satisfactory account of export prices and quantities.",
    "category": "CORRECT",
    "question_context_used": "The research strategy is clearly spelled out: first a fairly general equation (for which no particular economic theory is offered) is estimated by OLS. Then any 'implausible' coefficients are suppressed and the equations re-estimated. Then any insignificant (at the 5% level) coefficients are suppressed and finally a significant, plausible set of variables is derived. If forecasting really is the name of the game then there is really no case for suppressing implausible coefficients. Furthermore, unless the original equation was rigorously derived from impeccable economic theory then 'plausibility' tends to be rather subjective. One important result of this piece of work is the strong case which emerges for disaggregation by commodity and by destination. Aggregate studies do provide some explanation of exports but mask a great deal more. It is not clear what the optimum level of disaggregation would be but Winters suggests that only by studying individual firms and particular markets can one hope to offer a satisfactory account of export prices and quantities.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-627",
    "question": "Which of the following empirical findings strongly support the hypothesis that relationship persistence in producer-rig dynamics is driven by relationship-specific learning rather than switching costs or match specificities?",
    "options": [
      "A) The LIFO pattern persists even after controlling for rig fixed effects, with a statistically significant marginal effect of -0.067.",
      "B) Kolmogorov-Smirnov tests fail to reject equality in depth ratings and initial drilling performance between rigs in each pair.",
      "C) Rigs with more producer-specific experience are significantly less likely to exit first, as shown by the conditional logit model with β = -0.061.",
      "D) The hazard rate for relationship termination declines with relationship duration, consistent with learning curve effects."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "Learning by Drilling Interfirm Learning and Relationship Persistence in the Texas Oilpatch .md"
    ],
    "question_context": "The empirical analysis tests whether producer-rig relationship persistence is driven by relationship-specific learning. A key finding is the Last In-First Out (LIFO) pattern where less-experienced rigs are released first, with the probability of exit increasing with experience difference (Pr(ExitFirst) = exp(β·log(Experience_i1)) / [exp(β·log(Experience_i1)) + exp(β·log(Experience_i2))]). The marginal effect of -0.061 implies a 63.7% exit probability for a rig with 1 month vs. 12 months of experience. Rig fixed effects and Kolmogorov-Smirnov tests (p=0.565 for depth ratings, p=0.528 for initial performance) rule out switching cost heterogeneity and match specificities. The hazard rate's negative duration dependence further supports learning over alternative explanations.",
    "category": "CORRECT",
    "question_context_used": "The empirical analysis tests whether producer-rig relationship persistence is driven by relationship-specific learning. A key finding is the Last In-First Out (LIFO) pattern where less-experienced rigs are released first, with the probability of exit increasing with experience difference (Pr(ExitFirst) = exp(β·log(Experience_i1)) / [exp(β·log(Experience_i1)) + exp(β·log(Experience_i2))]). The marginal effect of -0.061 implies a 63.7% exit probability for a rig with 1 month vs. 12 months of experience. Rig fixed effects and Kolmogorov-Smirnov tests (p=0.565 for depth ratings, p=0.528 for initial performance) rule out switching cost heterogeneity and match specificities. The hazard rate's negative duration dependence further supports learning over alternative explanations.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4163",
    "question": "Based on the empirical results for testing against ARCH(2) and GARCH(1,1) disturbances, which of the following statements are correct regarding the LM1, LM2, and LBS tests?",
    "options": [
      "A) The LBS test consistently has estimated sizes closer to the nominal size than the LM tests for normal disturbances.",
      "B) The LM1 test is more powerful than the LM2 test when testing against ARCH(2) disturbances, especially for normal errors.",
      "C) The power advantage of the LBS test over the LM tests is most pronounced when sample sizes are small (n=20).",
      "D) The LBS test demonstrates greater robustness to leptokurtic disturbances in large samples compared to the LM tests."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "LEE和KING - 1993 - A locally most mean powerful based score test for arch and garch regression disturbances.md"
    ],
    "question_context": "Key findings from the empirical analysis include: For normal disturbances, the estimated sizes of the LBS test are almost always closer to 0.05 than those of the LM tests. The LM1 test based on (5) is almost always more powerful than the LM2 test which uses the more approximate $({\\mathtt{n}}{-}{\\mathtt{q}}){\\mathtt{R}}^{2}$ formula. The most powerful test is almost always the LBS test, particularly for larger samples and also for leptokurtic errors. The results clearly demonstrate that a distinct improvement in power results from replacing the popular LM2 test with the LBS test. For leptokurtic disturbances, the typical decline in power going from normal to leptokurtic disturbances is much greater for the LM tests than the LBS test, suggesting that the LBS test is more robust to departures from normality in large samples.",
    "category": "CORRECT",
    "question_context_used": "Key findings from the empirical analysis include: For normal disturbances, the estimated sizes of the LBS test are almost always closer to 0.05 than those of the LM tests. The LM1 test based on (5) is almost always more powerful than the LM2 test which uses the more approximate $({\\mathtt{n}}{-}{\\mathtt{q}}){\\mathtt{R}}^{2}$ formula. The most powerful test is almost always the LBS test, particularly for larger samples and also for leptokurtic errors. The results clearly demonstrate that a distinct improvement in power results from replacing the popular LM2 test with the LBS test. For leptokurtic disturbances, the typical decline in power going from normal to leptokurtic disturbances is much greater for the LM tests than the LBS test, suggesting that the LBS test is more robust to departures from normality in large samples.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-48",
    "question": "In the context of the game defined by Assumptions (A.1)-(A.3), which of the following statements are true regarding the Nash equilibrium when $c_X = c_Y$?",
    "options": [
      "A) Both parties spend the same amount of resources in each district, i.e., $x_i^* = y_i^*$ for all $i$.",
      "B) The level of spending is higher in districts where the race is closer, i.e., $|a_i - \\frac{1}{2}| < |a_k - \\frac{1}{2}|$ implies $x_i^* > x_k^*$.",
      "C) The equilibrium payoffs of both parties are larger if some $a_i$ move closer to 0 or 1, holding $\\sum_{i=1}^n a_i$ fixed.",
      "D) The Nash equilibrium does not exist if $h(x) = x^b$ with $b > 1$."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Snyder-ElectionGoalsAllocation-1989.md"
    ],
    "question_context": "The model assumes that each party aims to maximize the expected number of legislative seats it captures, less the resources it spends. The payoff functions are given by:\n\n$$\n\\begin{array}{r l}&{u_{X}\\big(x,y\\big)=U\\displaystyle\\sum_{i=1}^{n}\\tilde{p}_{i}\\big(x_{i},y_{i}\\big)-c_{X}\\displaystyle\\sum_{i=1}^{n}x_{i},\\qquada n d}\\ &{u_{Y}\\big(x,y\\big)=U\\displaystyle\\sum_{i=1}^{n}\\big[1-\\tilde{p}_{i}\\big(x_{i},y_{i}\\big)\\big]-c_{Y}\\displaystyle\\sum_{i=1}^{n}y_{i},}\\end{array}\n$$\n\nwhere $U\\geqslant c_{X}$ and $U\\geqslant c_{Y}$. The parameters $c_X$ and $c_Y$ denote the marginal cost of raising and spending resources. If $c_X = c_Y$, the Nash equilibrium satisfies $x_i^* = y_i^*$ for all $i$, and spending is higher in districts where the race is closer. Additionally, both parties benefit if some $a_i$ move closer to 0 or 1, as this reduces total campaign spending.",
    "category": "CORRECT",
    "question_context_used": "The model assumes that each party aims to maximize the expected number of legislative seats it captures, less the resources it spends. The payoff functions are given by:\n\n$$\n\\begin{array}{r l}&{u_{X}\\big(x,y\\big)=U\\displaystyle\\sum_{i=1}^{n}\\tilde{p}_{i}\\big(x_{i},y_{i}\\big)-c_{X}\\displaystyle\\sum_{i=1}^{n}x_{i},\\qquada n d}\\ &{u_{Y}\\big(x,y\\big)=U\\displaystyle\\sum_{i=1}^{n}\\big[1-\\tilde{p}_{i}\\big(x_{i},y_{i}\\big)\\big]-c_{Y}\\displaystyle\\sum_{i=1}^{n}y_{i},}\\end{array}\n$$\n\nwhere $U\\geqslant c_{X}$ and $U\\geqslant c_{Y}$. The parameters $c_X$ and $c_Y$ denote the marginal cost of raising and spending resources. If $c_X = c_Y$, the Nash equilibrium satisfies $x_i^* = y_i^*$ for all $i$, and spending is higher in districts where the race is closer. Additionally, both parties benefit if some $a_i$ move closer to 0 or 1, as this reduces total campaign spending.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2470",
    "question": "In the context of bilateral trading with two-type private values, which of the following conditions are crucial for the feasibility of ex post efficient mechanisms with Bayesian incentive compatibility and individual rationality?",
    "options": [
      "A) The specific ordering of the traders' possible valuations",
      "B) The traders' beliefs about each other's valuations",
      "C) The presence of a third-party mediator",
      "D) The traders' risk preferences"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "On-incentive-compatible--individually-rational--and-ex-p_1989_Journal-of-Eco.md"
    ],
    "question_context": "We consider a bargaining problem between one buyer and one seller for a single object with two-type private values for each trader's possible valuation. There exists an ordering of these values such that the feasibility of ex post efficient mechanisms with Bayesian incentive compatibility and individual rationality depends crucially on their specific values and on the traders' beliefs about them. We present a necessary and sufficient condition for ex post efficient mechanisms to be feasible in this specific case. The result implies a possible detrimental effect of information acquisition on ex post efficiency.",
    "category": "CORRECT",
    "question_context_used": "We consider a bargaining problem between one buyer and one seller for a single object with two-type private values for each trader's possible valuation. There exists an ordering of these values such that the feasibility of ex post efficient mechanisms with Bayesian incentive compatibility and individual rationality depends crucially on their specific values and on the traders' beliefs about them. We present a necessary and sufficient condition for ex post efficient mechanisms to be feasible in this specific case. The result implies a possible detrimental effect of information acquisition on ex post efficiency.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2743",
    "question": "In the context of combining seasonal adjustment methods to minimize revision errors, which of the following statements are correct based on the provided analysis?",
    "options": [
      "A) The optimal weight β₀ for combining methods is derived by minimizing the variance of the revision error ωₜᵝ.",
      "B) The daily method of seasonal adjustment produces larger revision errors compared to the monthly method.",
      "C) The linear combination of seasonal factors rₜᵝ = βrₜ⁰ + (1-β)rₜᵃ leads to a decrease in revision errors when β is optimally chosen.",
      "D) The probability of disagreement between monetary authorities using different seasonal adjustment methods is approximately 1 out of 8 under current operating procedures."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Effects-of-alternative-seasonal-adjustment-procedures-_1980_Journal-of-Econo.md"
    ],
    "question_context": "The paper discusses combining two seasonal adjustment methods to minimize revision errors. The optimal weight β₀ for the linear combination rₜᵝ = βrₜ⁰ + (1-β)rₜᵃ is derived by minimizing the variance of ωₜᵝ, given by:\n\n$$\\beta_{0}=\\frac{\\sigma_{\\omega a}^{2}-\\rho\\sigma_{\\omega0}\\sigma_{\\omega a}}{\\sigma_{\\omega a}^{2}+\\sigma_{\\omega0}^{2}-2\\rho\\sigma_{\\omega0}\\sigma_{\\omega a}}.$$\n\nEmpirical results show that the daily method produces smaller revision errors, and combining methods with β ∈ (0, 0.2) yields a lower MSE than ωₜᵃ alone. Additionally, the probability of disagreement between monetary authorities using different methods is estimated at 1 out of 8 under current procedures, highlighting the policy implications of seasonal adjustment choices.",
    "category": "CORRECT",
    "question_context_used": "The paper discusses combining two seasonal adjustment methods to minimize revision errors. The optimal weight β₀ for the linear combination rₜᵝ = βrₜ⁰ + (1-β)rₜᵃ is derived by minimizing the variance of ωₜᵝ, given by:\n\n$$\\beta_{0}=\\frac{\\sigma_{\\omega a}^{2}-\\rho\\sigma_{\\omega0}\\sigma_{\\omega a}}{\\sigma_{\\omega a}^{2}+\\sigma_{\\omega0}^{2}-2\\rho\\sigma_{\\omega0}\\sigma_{\\omega a}}.$$\n\nEmpirical results show that the daily method produces smaller revision errors, and combining methods with β ∈ (0, 0.2) yields a lower MSE than ωₜᵃ alone. Additionally, the probability of disagreement between monetary authorities using different methods is estimated at 1 out of 8 under current procedures, highlighting the policy implications of seasonal adjustment choices.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-324",
    "question": "In the context of migration decision models with imperfect information, which of the following statements are correct regarding the error term q and its impact on migration probabilities?",
    "options": [
      "A) A positive q increases the probability of migration by shifting the distribution of ν to the right",
      "B) A negative q decreases the probability of migration by shifting the distribution of ν to the left",
      "C) When q = 0, all migrants are optimal and all stayers are sub-optimal",
      "D) The presence of q creates sub-optimal migrants when q > 0 and sub-optimal stayers when q < 0"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Media-exposure-and-internal-migration---Evidenc_2013_Journal-of-Development-.md"
    ],
    "question_context": "The migration decision model incorporates an error term q representing individuals' misestimation of net migration gains. The log earnings distributions in source (S) and destination (D) regions are given by: \n\n$$w_{j}={\\mu}_{j}+{\\epsilon}_{j} \\text{ with } j=S,D$$\n\nMigration occurs if:\n\n$$\\mu_{D}+\\epsilon_{D}-\\pi{\\geq}\\mu_{S}+\\epsilon_{S}$$\n\nWith imperfect information, this becomes:\n\n$$(\\mu_{D}+\\epsilon_{D}-\\pi)+q{\\geq}\\mu_{S}+\\epsilon_{S}$$\n\nThe migration probability is:\n\n$$P r(I_{q}=1)=1-\\phi\\bigg(\\frac{\\Delta-q}{\\sigma_{\\nu}}\\bigg)$$\n\nwhere q > 0 (overestimation) increases migration probability by right-shifting ν's distribution, while q < 0 (underestimation) decreases it by left-shifting. This creates sub-optimal migrants when q > 0 and sub-optimal stayers when q < 0.",
    "category": "CORRECT",
    "question_context_used": "The migration decision model incorporates an error term q representing individuals' misestimation of net migration gains. The log earnings distributions in source (S) and destination (D) regions are given by: \n\n$$w_{j}={\\mu}_{j}+{\\epsilon}_{j} \\text{ with } j=S,D$$\n\nMigration occurs if:\n\n$$\\mu_{D}+\\epsilon_{D}-\\pi{\\geq}\\mu_{S}+\\epsilon_{S}$$\n\nWith imperfect information, this becomes:\n\n$$(\\mu_{D}+\\epsilon_{D}-\\pi)+q{\\geq}\\mu_{S}+\\epsilon_{S}$$\n\nThe migration probability is:\n\n$$P r(I_{q}=1)=1-\\phi\\bigg(\\frac{\\Delta-q}{\\sigma_{\\nu}}\\bigg)$$\n\nwhere q > 0 (overestimation) increases migration probability by right-shifting ν's distribution, while q < 0 (underestimation) decreases it by left-shifting. This creates sub-optimal migrants when q > 0 and sub-optimal stayers when q < 0.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4212",
    "question": "Which of the following factors are empirically supported as reasons why players might deviate from backward induction in the centipede game?",
    "options": [
      "A) Aversion to potential surplus loss",
      "B) Social preferences for fairness or cooperation",
      "C) Beliefs about opponents' rationality errors",
      "D) Perfect common knowledge of rationality"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Levitt-CheckmateExploringBackward-2011.md"
    ],
    "question_context": "Backward induction is a cornerstone of game theory, yet empirical evidence shows economic agents engage in it less frequently than theorists might hope. In the centipede game, players may deviate from backward induction due to aversion to potential surplus loss, social preferences for fairness or cooperation, or beliefs that other players may make rationality errors (Rosenthal 1981; Aumann 1992; McKelvey and Palfrey 1992). The formula for expected utility maximization under backward induction fails when initial common beliefs are violated, making passing rational. Palacios-Huerta and Volij (2009) found chess players stop at the first node 70% of the time when matched with other chess players, attributing this to their adherence to common knowledge of rationality without social preference interference.",
    "category": "CORRECT",
    "question_context_used": "Backward induction is a cornerstone of game theory, yet empirical evidence shows economic agents engage in it less frequently than theorists might hope. In the centipede game, players may deviate from backward induction due to aversion to potential surplus loss, social preferences for fairness or cooperation, or beliefs that other players may make rationality errors (Rosenthal 1981; Aumann 1992; McKelvey and Palfrey 1992). The formula for expected utility maximization under backward induction fails when initial common beliefs are violated, making passing rational. Palacios-Huerta and Volij (2009) found chess players stop at the first node 70% of the time when matched with other chess players, attributing this to their adherence to common knowledge of rationality without social preference interference.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-504",
    "question": "In the context of collective consumption equilibrium (CCE) and Pareto optimality, which of the following conditions are necessary for a CCE to be a Pareto optimum in a private ownership economy?",
    "options": [
      "A) Local nonsatiation of consumer preferences for all consumers",
      "B) Convexity and closedness of production sets for all firms",
      "C) Feasibility of the allocation with respect to initial resources",
      "D) Existence of a government budget proposal that is not unanimously rejected"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Externalities-and-fundamental-nonconvexities--A-reconciliatio_2010_Journal-o.md"
    ],
    "question_context": "Theorem 3 states that if $u^{h}$ satisfies local nonsatiation for all $h\\in H$ and the configuration is a CCE of a private ownership economy $E^{P}$, then the CCE is a Pareto optimum of economy $E$. The proof involves showing that if a CCE is not Pareto optimal, there exists a feasible allocation that is preferred by all consumers, leading to a contradiction. Key steps include the use of local nonsatiation to ensure budget constraints hold as equalities and the construction of a government budget proposal that would be unanimously preferred, contradicting the CCE assumption. The formula for the aggregate budget constraint under local nonsatiation is:\n\n$$\n\\begin{array}{c}{{p_{o}\\displaystyle\\sum_{h}x_{o}^{h}-\\sum_{i}\\hat{\\Pi}_{y}^{i}\\big(p_{o},p_{n}^{i},c_{g}\\big)+\\displaystyle\\sum_{h}{\\cal T}^{h}+\\sum_{h}p_{x_{n}}^{h}n}}\\ {{=\\displaystyle\\sum_{j}\\Pi_{z}^{j}(p_{o},p_{z_{n}},p_{z_{g}})+p_{o}\\omega_{o}+\\displaystyle\\sum_{h}p_{x_{n}}^{h}\\omega_{n}.}}\\end{array}\n$$",
    "category": "INCORRECT",
    "question_context_used": "Theorem 3 states that if $u^{h}$ satisfies local nonsatiation for all $h\\in H$ and the configuration is a CCE of a private ownership economy $E^{P}$, then the CCE is a Pareto optimum of economy $E$. The proof involves showing that if a CCE is not Pareto optimal, there exists a feasible allocation that is preferred by all consumers, leading to a contradiction. Key steps include the use of local nonsatiation to ensure budget constraints hold as equalities and the construction of a government budget proposal that would be unanimously preferred, contradicting the CCE assumption. The formula for the aggregate budget constraint under local nonsatiation is:\n\n$$\n\\begin{array}{c}{{p_{o}\\displaystyle\\sum_{h}x_{o}^{h}-\\sum_{i}\\hat{\\Pi}_{y}^{i}\\big(p_{o},p_{n}^{i},c_{g}\\big)+\\displaystyle\\sum_{h}{\\cal T}^{h}+\\sum_{h}p_{x_{n}}^{h}n}}\\ {{=\\displaystyle\\sum_{j}\\Pi_{z}^{j}(p_{o},p_{z_{n}},p_{z_{g}})+p_{o}\\omega_{o}+\\displaystyle\\sum_{h}p_{x_{n}}^{h}\\omega_{n}.}}\\end{array}\n$$",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4578",
    "question": "In the context of identifying MA(q) models with nonnormal innovations, which of the following statements are correct regarding the use of bicovariances and tricovariances?",
    "options": [
      "A) Bicovariances are proportional to individual model coefficients when the third moment of innovations is nonzero.",
      "B) Tricovariances can be used for identification when the fourth-order cumulant is zero.",
      "C) The fourth-order cumulant is identically zero in the Gaussian case, making tricovariances useless for identification.",
      "D) The correlation coefficient ρ_Bi is used to select the model with the best fit to estimated bicovariance magnitudes."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Identification-and-estimation-of-noninvertible-non-Gau_1992_Journal-of-Econo.md"
    ],
    "question_context": "The model is that presented in eq. (2.1), where normality is excluded. We begin by defining the bicovariances for any MA process, where the innovation sequence {ε_t} has zero mean and nonzero third moment μ_3. The bicovariances are proportional to the individual model coefficients, which is the clue to model identification using the bicovariance. When the third moment is zero, we must resort to the next higher-order moment, the tricovariance. The fourth-order cumulant c(t_1,t_2,t_3) is defined as (μ_4 - 3σ_ε^4)∑α_sα_{s+t_1}α_{s+t_2}α_{s+t_3}, which is identically zero in the Gaussian case (μ_4 = 3σ_ε^4). The correlation coefficient ρ_Bi is used to select the model with the highest estimated correlation between the magnitudes of estimated bicovariances and estimated coefficients.",
    "category": "INCORRECT",
    "question_context_used": "The model is that presented in eq. (2.1), where normality is excluded. We begin by defining the bicovariances for any MA process, where the innovation sequence {ε_t} has zero mean and nonzero third moment μ_3. The bicovariances are proportional to the individual model coefficients, which is the clue to model identification using the bicovariance. When the third moment is zero, we must resort to the next higher-order moment, the tricovariance. The fourth-order cumulant c(t_1,t_2,t_3) is defined as (μ_4 - 3σ_ε^4)∑α_sα_{s+t_1}α_{s+t_2}α_{s+t_3}, which is identically zero in the Gaussian case (μ_4 = 3σ_ε^4). The correlation coefficient ρ_Bi is used to select the model with the highest estimated correlation between the magnitudes of estimated bicovariances and estimated coefficients.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2026",
    "question": "In the context of the Limited Shared Weak Preference (LSWP) property and the legislative bargaining model discussed, which of the following conditions must hold for the LSWP property to be satisfied?",
    "options": [
      "A) The closure of the strict upper contour set must equal the weak upper contour set for all coalitions and outcomes.",
      "B) The sum of the legislators' continuation values must be strictly less than one for all types.",
      "C) The voting weight of the coalition must exceed a certain threshold relative to the total weight.",
      "D) The probability of being chosen as a coalition partner must be inversely related to the legislator's continuation value."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "James M. Snyder-LegislativeBargainingWeighted-2005.md"
    ],
    "question_context": "The LSWP property holds if, for all coalitions C and outcomes x, the condition |R_C(x)| > 1 implies R_C(x) is a subset of the closure of P_C(x). Here, R_C(x) and P_C(x) denote the weak and strict upper contour sets of x, respectively. The closure of P_C(x) is given by the set of alternatives weakly preferred to x by all members of C. The model further involves continuation values for legislators, where the probability of being chosen as a coalition partner is influenced by the proposer's strategy and the legislators' types. Key formula: R_C(x) ⊆ closure(P_C(x)).",
    "category": "INCORRECT",
    "question_context_used": "The LSWP property holds if, for all coalitions C and outcomes x, the condition |R_C(x)| > 1 implies R_C(x) is a subset of the closure of P_C(x). Here, R_C(x) and P_C(x) denote the weak and strict upper contour sets of x, respectively. The closure of P_C(x) is given by the set of alternatives weakly preferred to x by all members of C. The model further involves continuation values for legislators, where the probability of being chosen as a coalition partner is influenced by the proposer's strategy and the legislators' types. Key formula: R_C(x) ⊆ closure(P_C(x)).",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1964",
    "question": "Based on the analysis of labor market outcomes for East and West Germans, which of the following statements are supported by the empirical results?",
    "options": [
      "A) For the older cohort (1971-1973), treatment in the East significantly increases wages and professional status for men but decreases employment probability.",
      "B) The younger cohort (1974-1977) shows significant positive effects of treatment on employment and working hours for women.",
      "C) The estimated college wage premium of 20% in Germany implies that treatment effects on wages are entirely explained by increased college attainment.",
      "D) Placebo tests on cohorts born before 1971 show no significant effects, supporting the causal interpretation of socialist education impacts."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "FuchsSchndeln-LONGLASTINGEFFECTSSOCIALIST-2016.md"
    ],
    "question_context": "The study analyzes labor market outcomes for East and West Germans, focusing on cohorts born between 1971-1977. Key findings include: For the older cohort (1971-1973), being treated in the East is associated with a 4.1% higher wage effect and 5.1 percentage point higher professional status for men, but a 2.2 percentage point lower employment probability. The formula for expected wage effect from college alone is given by $0.1 \\times 0.2 = 0.02$ (2%). Placebo tests on pre-1971 cohorts show no significant effects, ruling out alternative explanations. The younger cohort (1974-1977) shows positive employment effects (2.1 percentage points) and working hours (1.5%) for men only. All results are robust to controlling for children and sample restrictions.",
    "category": "CORRECT",
    "question_context_used": "The study analyzes labor market outcomes for East and West Germans, focusing on cohorts born between 1971-1977. Key findings include: For the older cohort (1971-1973), being treated in the East is associated with a 4.1% higher wage effect and 5.1 percentage point higher professional status for men, but a 2.2 percentage point lower employment probability. The formula for expected wage effect from college alone is given by $0.1 \\times 0.2 = 0.02$ (2%). Placebo tests on pre-1971 cohorts show no significant effects, ruling out alternative explanations. The younger cohort (1974-1977) shows positive employment effects (2.1 percentage points) and working hours (1.5%) for men only. All results are robust to controlling for children and sample restrictions.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4854",
    "question": "In the context of bisymmetric comparability structures (CSs), which of the following statements are true regarding the impact of regulatory interventions on market competitiveness and consumer switching?",
    "options": [
      "A) Increasing $q$ or $q_{W}^{*}$ in a non-weighted-regular CS always lowers equilibrium profits.",
      "B) Merging all formats in $Z$ into a single harmonized format in a non-weighted-regular CS raises equilibrium profits without affecting the max-min payoff.",
      "C) Under weighted regularity, any improvement in comparability leads to a higher switching rate and lower equilibrium profits.",
      "D) In cutoff equilibria, the comovement between switching rate and market competitiveness is always positive."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Price Competition Under Limited Comparability.md"
    ],
    "question_context": "An order-independent comparability structure $(X,\\pi)$ is bisymmetric if $X$ can be partitioned into two sets, $W$ and $Z$, such that for every distinct $x,y\\in X$: \n\n$$\\pi(x,y)=\\left\\{\\begin{array}{l l}{q_{W}i f\\quad x,y\\in W}\\ {q z i f\\quad x,y\\in Z}\\ {q i f x\\in W,y\\in Z}\\end{array}\\right.$$ \n\nwhere $\\operatorname*{max}\\{q_{W},q_{Z},q\\}<1$. A bisymmetric CS is weighted-regular if and only if $(q_{W}^{\\ast}-q)(q_{Z}^{\\ast}-q)\\geq0$. When this condition is violated, equilibrium strategies display correlation between price and format decisions, and firms earn 'collusive' profits. Regulatory interventions that increase $q$ or $q_{W}^{*}$ lower equilibrium profits, whereas increasing $q_{Z}^{*}$ (e.g., by merging formats in $Z$) raises equilibrium profits without affecting the max-min payoff. Under weighted regularity, greater comparability leads to higher switching rates and lower equilibrium profits, but this link may break down in cutoff equilibria.",
    "category": "CORRECT",
    "question_context_used": "An order-independent comparability structure $(X,\\pi)$ is bisymmetric if $X$ can be partitioned into two sets, $W$ and $Z$, such that for every distinct $x,y\\in X$: \n\n$$\\pi(x,y)=\\left\\{\\begin{array}{l l}{q_{W}i f\\quad x,y\\in W}\\ {q z i f\\quad x,y\\in Z}\\ {q i f x\\in W,y\\in Z}\\end{array}\\right.$$ \n\nwhere $\\operatorname*{max}\\{q_{W},q_{Z},q\\}<1$. A bisymmetric CS is weighted-regular if and only if $(q_{W}^{\\ast}-q)(q_{Z}^{\\ast}-q)\\geq0$. When this condition is violated, equilibrium strategies display correlation between price and format decisions, and firms earn 'collusive' profits. Regulatory interventions that increase $q$ or $q_{W}^{*}$ lower equilibrium profits, whereas increasing $q_{Z}^{*}$ (e.g., by merging formats in $Z$) raises equilibrium profits without affecting the max-min payoff. Under weighted regularity, greater comparability leads to higher switching rates and lower equilibrium profits, but this link may break down in cutoff equilibria.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4264",
    "question": "In the context of social learning models with network topologies, which of the following conditions are necessary for asymptotic learning to occur in equilibrium?",
    "options": [
      "A) The private beliefs of agents must be unbounded, i.e., agents can receive arbitrarily strong signals favoring either state.",
      "B) The network topology must be deterministic, ensuring that each agent observes a fixed set of previous actions.",
      "C) The signal structure must allow for some signals to be fully revealing about the underlying state.",
      "D) The social beliefs must exhibit monotonicity properties, where observing more agents choosing action x=1 increases the social belief."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Bayesian Learning in Social Networks.md"
    ],
    "question_context": "The model involves a countably infinite number of agents making sequential decisions based on private signals and observations of others' actions. The pay-off of agent $n$ is given by:\n\n$$\nu_{n}(x_{n},\\theta)={\\left\\{\\begin{array}{l l}{1}&{{\\mathrm{if~}}x_{n}=\\theta,}\\ {0}&{{\\mathrm{if~}}x_{n}\\neq\\theta.}\\end{array}\\right.}\n$$\n\nPrivate beliefs are defined as $p_{n}=\\mathbb{P}(\\theta=1\\mid s_{n})$, and social beliefs depend on the observed actions of neighbors. Asymptotic learning occurs if $x_{n}$ converges to $\\theta$ in probability. Key conditions for learning include unbounded private beliefs and certain monotonicity properties in social beliefs, though the latter is not always guaranteed in general networks.",
    "category": "CORRECT",
    "question_context_used": "The model involves a countably infinite number of agents making sequential decisions based on private signals and observations of others' actions. The pay-off of agent $n$ is given by:\n\n$$\nu_{n}(x_{n},\\theta)={\\left\\{\\begin{array}{l l}{1}&{{\\mathrm{if~}}x_{n}=\\theta,}\\ {0}&{{\\mathrm{if~}}x_{n}\\neq\\theta.}\\end{array}\\right.}\n$$\n\nPrivate beliefs are defined as $p_{n}=\\mathbb{P}(\\theta=1\\mid s_{n})$, and social beliefs depend on the observed actions of neighbors. Asymptotic learning occurs if $x_{n}$ converges to $\\theta$ in probability. Key conditions for learning include unbounded private beliefs and certain monotonicity properties in social beliefs, though the latter is not always guaranteed in general networks.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1751",
    "question": "Which of the following statements accurately reflect the methodology used for institutional rankings in the given text?",
    "options": [
      "A) Article pages are allocated based on the current affiliation of authors to predict future research output.",
      "B) Each coauthor in a multi-authored paper receives an equal share of the article's pages, adjusted by the number of affiliations listed.",
      "C) Research output from nonacademic centers like central banks is included to provide a comprehensive comparison.",
      "D) Business school publications are included in the institutional research output to account for a significant portion of current economic research."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Kalaitzidakis-RankingsAcademicJournals-2003.md"
    ],
    "question_context": "The analysis is based on article publications in the top thirty journals according to our pages adjusted rankings (last column of Table 1) for the five-year period 1995 to 1999. The list of articles includes shorter notes, but excludes book reviews and articles in papers and proceedings volumes. The selection of the top 30 journals provides a rich group of research outlets for the core of economic theory and econometrics as well as the most respected field journals. It is an updated “Diamond List’ (see Burton and Phimister 1995) that has been extensively used in the rankings literature as the standard list of quality journals. The last journal that is included in the list, the Journal of International Economics, has an adjusted impact factor of 0.0784 compared with 1.00 for the American Economic Review. The list of journals that are included account for more than $90\\%$ of all citations. For these journals there is a broad agreement among all ranking criteria that they belong to the top group; see Table 1. The impact factors for the journals that are excluded from the list are quite small and even if they were included in the calculations they would not make much difference in the overall construction of rankings especially for the top 200 economics departments that we report.\n\nWe allocate article pages according to the affiliation of the authors at the time of publication. Affiliations taken from the published articles reflect the actual research output produced, in contrast to the current affiliation of the authors that might serve as a proxy for future research output for the institution where the researcher currently resides. In papers with $n$ coauthors, each coauthor is allocated $1/n$ pages of the article. In addition, when $m$ affiliations are listed by some author, then we allocate to each affiliation $1/m$ of the pages that correspond to the specific author. We do not include among the various affiliations those that correspond to certain research centers that act as umbrellas for various researchers but do not offer a permanent home base, such as NBER in the United States of America and CEPR in the United Kingdom. When authors include the above as joint affiliations then all the weight is attached to their primary affiliations. We also excluded from the calculation of rankings the research output that is produced at nonacademic centers such as the various central banks, the World Bank, and the IMF. Since our primary task is to evaluate research carried out at academic institutions, including nonacademic research centers would not constitute a valid comparison, since academics usually have also teaching duties that occupy much of their time. We have included as part of the institutional research output the published research that has been produced by faculty members of business schools that belong to these institutions. That gives an advantage to institutions with large vibrant business schools, such as the top U.S. universities. However, since our task was to record the research output in economics carried out in academic institutions in general, excluding business school output would have left out a significant part of current research. For the same reason we also include as part of a given institution research centers that are located in these institutions and are frequented by researchers. For example, the Institute de Analisis Economico (IAE) has been included as part of Universita Autonoma de Barcelona.",
    "category": "CORRECT",
    "question_context_used": "The analysis is based on article publications in the top thirty journals according to our pages adjusted rankings (last column of Table 1) for the five-year period 1995 to 1999. The list of articles includes shorter notes, but excludes book reviews and articles in papers and proceedings volumes. The selection of the top 30 journals provides a rich group of research outlets for the core of economic theory and econometrics as well as the most respected field journals. It is an updated “Diamond List’ (see Burton and Phimister 1995) that has been extensively used in the rankings literature as the standard list of quality journals. The last journal that is included in the list, the Journal of International Economics, has an adjusted impact factor of 0.0784 compared with 1.00 for the American Economic Review. The list of journals that are included account for more than $90\\%$ of all citations. For these journals there is a broad agreement among all ranking criteria that they belong to the top group; see Table 1. The impact factors for the journals that are excluded from the list are quite small and even if they were included in the calculations they would not make much difference in the overall construction of rankings especially for the top 200 economics departments that we report.\n\nWe allocate article pages according to the affiliation of the authors at the time of publication. Affiliations taken from the published articles reflect the actual research output produced, in contrast to the current affiliation of the authors that might serve as a proxy for future research output for the institution where the researcher currently resides. In papers with $n$ coauthors, each coauthor is allocated $1/n$ pages of the article. In addition, when $m$ affiliations are listed by some author, then we allocate to each affiliation $1/m$ of the pages that correspond to the specific author. We do not include among the various affiliations those that correspond to certain research centers that act as umbrellas for various researchers but do not offer a permanent home base, such as NBER in the United States of America and CEPR in the United Kingdom. When authors include the above as joint affiliations then all the weight is attached to their primary affiliations. We also excluded from the calculation of rankings the research output that is produced at nonacademic centers such as the various central banks, the World Bank, and the IMF. Since our primary task is to evaluate research carried out at academic institutions, including nonacademic research centers would not constitute a valid comparison, since academics usually have also teaching duties that occupy much of their time. We have included as part of the institutional research output the published research that has been produced by faculty members of business schools that belong to these institutions. That gives an advantage to institutions with large vibrant business schools, such as the top U.S. universities. However, since our task was to record the research output in economics carried out in academic institutions in general, excluding business school output would have left out a significant part of current research. For the same reason we also include as part of a given institution research centers that are located in these institutions and are frequented by researchers. For example, the Institute de Analisis Economico (IAE) has been included as part of Universita Autonoma de Barcelona.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2847",
    "question": "In the context of DECO and DCC models for S&P 500 correlations, which of the following statements are supported by the empirical results?",
    "options": [
      "A) DECO outperforms DCC in terms of AIC when no factor structure is used in the first-stage GARCH regressions.",
      "B) The CAPM residual correlations show stronger persistence (higher $\\hat{\\alpha} + \\hat{\\beta}$) than the Fama-French three-factor model residuals.",
      "C) DCC provides a better fit than DECO when pairwise correlations exhibit dissimilar dynamics, as seen in factor model residuals.",
      "D) Block DECO achieves superior AIC compared to both DECO and DCC when applied to CAPM or Fama-French residuals."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "05-Dynamic Equicorrelation.md"
    ],
    "question_context": "Key empirical findings: When no factor is used, DECO achieves a lower AIC than DCC, making it the better model according to this criterion. DECO and DCC use the same number of parameters, so the lower AIC for DECO is due solely to its better likelihood fit. CAPM residual correlations show $\\hat{\\alpha} + \\hat{\\beta} = 0.992$, while Fama-French residuals show much weaker dynamics with $\\hat{\\alpha} + \\hat{\\beta} = 0.814$. DCC finds more persistent dynamics in correlations for Fama-French residuals than DECO, and provides a better fit when pairwise correlations exhibit idiosyncratic dynamics. Block DECO achieves a better AIC than both DCC and DECO when no factors are used, but DCC maintains superior AIC for factor model residuals.",
    "category": "CORRECT",
    "question_context_used": "Key empirical findings: When no factor is used, DECO achieves a lower AIC than DCC, making it the better model according to this criterion. DECO and DCC use the same number of parameters, so the lower AIC for DECO is due solely to its better likelihood fit. CAPM residual correlations show $\\hat{\\alpha} + \\hat{\\beta} = 0.992$, while Fama-French residuals show much weaker dynamics with $\\hat{\\alpha} + \\hat{\\beta} = 0.814$. DCC finds more persistent dynamics in correlations for Fama-French residuals than DECO, and provides a better fit when pairwise correlations exhibit idiosyncratic dynamics. Block DECO achieves a better AIC than both DCC and DECO when no factors are used, but DCC maintains superior AIC for factor model residuals.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1904",
    "question": "In the context of the operator $S:B[0,1]\\longrightarrow B[0,1]$ defined in the proof of Proposition 1, which of the following conditions are necessary to ensure the existence of a unique solution to the recursive equation (3)?",
    "options": [
      "A) $S$ satisfies monotonicity and discounting conditions",
      "B) $S$ preserves continuity and strictly increasing property",
      "C) $S$ is a linear operator with bounded coefficients",
      "D) $S$ maps constant functions to constant functions"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Reputation and Survival Learning in a Dynamic Signalling Model.md"
    ],
    "question_context": "The operator $S:B[0,1]\\longrightarrow B[0,1]$ is defined as follows:\n\n$$\nS(f(\\lambda))=\\lambda g+(1-\\lambda)b-c+\\beta(\\lambda g+(1-\\lambda)b)\\operatorname*{max}\\{0,f(\\lambda^{s})\\}+\\beta(1-\\lambda g-(1-\\lambda)b)\\operatorname*{max}\\{0,f(\\lambda^{f})\\}\n$$\n\nwhere $\\lambda^{s}$ and $\\lambda^{f}$ are as defined in equations (1) and (2). To ensure the existence of a unique solution to the recursive equation (3), we must prove that $S$ satisfies Blackwell's two sufficiency conditions for a contraction: monotonicity and discounting. Monotonicity requires that for any $f,h\\in B[0,1]$ with $f(x)\\geq h(x)$ for all $x\\in[0,1]$, then $(S f)(x)\\geq(S h)(x)$ for all $x\\in[0,1]$. Discounting requires that there exists some constant $\\alpha\\in(0,1)$ such that $(S f)(x)+\\alpha a\\geq(S(f+a))(x)$ for all constants $a\\geq 0$. Additionally, we must show that $S$ preserves continuity and the strictly increasing property of functions.",
    "category": "CORRECT",
    "question_context_used": "The operator $S:B[0,1]\\longrightarrow B[0,1]$ is defined as follows:\n\n$$\nS(f(\\lambda))=\\lambda g+(1-\\lambda)b-c+\\beta(\\lambda g+(1-\\lambda)b)\\operatorname*{max}\\{0,f(\\lambda^{s})\\}+\\beta(1-\\lambda g-(1-\\lambda)b)\\operatorname*{max}\\{0,f(\\lambda^{f})\\}\n$$\n\nwhere $\\lambda^{s}$ and $\\lambda^{f}$ are as defined in equations (1) and (2). To ensure the existence of a unique solution to the recursive equation (3), we must prove that $S$ satisfies Blackwell's two sufficiency conditions for a contraction: monotonicity and discounting. Monotonicity requires that for any $f,h\\in B[0,1]$ with $f(x)\\geq h(x)$ for all $x\\in[0,1]$, then $(S f)(x)\\geq(S h)(x)$ for all $x\\in[0,1]$. Discounting requires that there exists some constant $\\alpha\\in(0,1)$ such that $(S f)(x)+\\alpha a\\geq(S(f+a))(x)$ for all constants $a\\geq 0$. Additionally, we must show that $S$ preserves continuity and the strictly increasing property of functions.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3761",
    "question": "In Gerard Van der Laan's model of equilibrium under rigid prices with consumer compensation, which of the following mechanisms are essential for achieving a stable equilibrium?",
    "options": [
      "A) Price adjustments that respond instantaneously to excess demand",
      "B) A compensation scheme that maintains consumer purchasing power",
      "C) Flexible wage rates that adjust to market conditions",
      "D) Government intervention to enforce price ceilings"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Equilibrium under Rigid Prices with Compensation for the Consumers.md"
    ],
    "question_context": "The paper examines equilibrium conditions in markets where prices are rigid and cannot adjust to clear markets. A key feature is the introduction of a compensation mechanism for consumers to maintain their purchasing power despite price rigidities. The model assumes that prices are fixed in the short run, leading to potential disequilibrium in the absence of compensatory measures. The compensation scheme is designed to offset the welfare losses consumers might face due to the inability of prices to adjust. The formula for the compensation is derived from the difference between the equilibrium price and the rigid price, weighted by the consumer's marginal utility of income.",
    "category": "INCORRECT",
    "question_context_used": "The paper examines equilibrium conditions in markets where prices are rigid and cannot adjust to clear markets. A key feature is the introduction of a compensation mechanism for consumers to maintain their purchasing power despite price rigidities. The model assumes that prices are fixed in the short run, leading to potential disequilibrium in the absence of compensatory measures. The compensation scheme is designed to offset the welfare losses consumers might face due to the inability of prices to adjust. The formula for the compensation is derived from the difference between the equilibrium price and the rigid price, weighted by the consumer's marginal utility of income.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1893",
    "question": "In the context of the California workers' compensation study, which of the following statements accurately describe the methodology or findings of the difference-in-differences (DD) analysis?",
    "options": [
      "A) The DD estimator is calculated as the difference between the pre-post change in average duration for high-wage workers and the pre-post change for low-wage workers.",
      "B) The regression-adjusted DD estimate includes a triple interaction term between Treat, After, and Type variables to control for selection bias.",
      "C) The control group for the 1994 benefit increase experiment consists of workers earning $504 or less per week, who experienced no change in benefits.",
      "D) The covariance between the error terms from the duration and selection equations is expected to be positive if severity and duration are positively related."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Neuhauser-EffectIncreaseWorkers-2004.md"
    ],
    "question_context": "The study utilizes a quasi-experimental design to evaluate behavioral responses to changes in Temporary Disability (TD) benefit rates in California. Key formula for the DD estimator is given by: \n\n$$ D D=D i f f_{\\mathrm{high-wage}}-D i f f_{\\mathrm{low-wage}}. $$ \n\nFor the 1994 experiment, high-wage workers (earning >$504/week) experienced benefit increases, while low-wage workers (earning ≤$504/week) served as the control group with unchanged benefits. The regression-adjusted DD model includes:\n\n$$ t_{i}^{*}=\\upbeta_{0}+\\upbeta_{1}A f t e r_{i}+\\upbeta_{2}T r e a t_{i}+\\upbeta_{3}T r e a t_{i}\\times A f t e r_{i}+\\updelta X_{i}+\\upepsilon_{i}. $$ \n\nSelection bias is addressed via a Heckman correction, where the covariance $\\sigma_{\\epsilon\\eta}$ between duration and selection equation errors is expected to be positive if more severe injuries have longer durations.",
    "category": "CORRECT",
    "question_context_used": "The study utilizes a quasi-experimental design to evaluate behavioral responses to changes in Temporary Disability (TD) benefit rates in California. Key formula for the DD estimator is given by: \n\n$$ D D=D i f f_{\\mathrm{high-wage}}-D i f f_{\\mathrm{low-wage}}. $$ \n\nFor the 1994 experiment, high-wage workers (earning >$504/week) experienced benefit increases, while low-wage workers (earning ≤$504/week) served as the control group with unchanged benefits. The regression-adjusted DD model includes:\n\n$$ t_{i}^{*}=\\upbeta_{0}+\\upbeta_{1}A f t e r_{i}+\\upbeta_{2}T r e a t_{i}+\\upbeta_{3}T r e a t_{i}\\times A f t e r_{i}+\\updelta X_{i}+\\upepsilon_{i}. $$ \n\nSelection bias is addressed via a Heckman correction, where the covariance $\\sigma_{\\epsilon\\eta}$ between duration and selection equation errors is expected to be positive if more severe injuries have longer durations.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2975",
    "question": "Under Assumptions 4.1 and 4.2, which of the following statements are correct regarding the asymptotic behavior of the standardized t-bar statistic in heterogeneous panels with serially correlated errors?",
    "options": [
      "A) The standardized t-bar statistic converges to a standard normal variate sequentially as T→∞ first and then N→∞",
      "B) The individual ADF statistics depend on nuisance parameters when T is fixed, even under βi=0",
      "C) The Lindberg-Levy central limit theorem ensures convergence of the standardized t-bar statistic when N→∞ first and then T→∞",
      "D) The asymptotic distribution remains valid when both N and T tend to infinity simultaneously with N/T→k for finite k"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Reprint-of--Testing-for-unit-roots-in-heterogeneous_2023_Journal-of-Economet.md"
    ],
    "question_context": "The text considers the general case where errors may be serially correlated with different patterns across groups, with T and N sufficiently large. The data generating process is specified as finite-order AR(p_i+1) processes, which can be written equivalently as ADF(p_i) regressions. Key formulas include:\n\n$$\n\\Delta\\mathbf{y}_{i}=\\beta_{i}\\mathbf{y}_{i,-1}+\\mathbf{Q}_{i}\\gamma_{i}+\\varepsilon_{i}\n$$\n\nwhere $\\mathbf{Q}_{i}=(\\tau_{T},\\varDelta\\mathbf{y}_{i,-1},\\varDelta\\mathbf{y}_{i,-2},\\ldots,\\varDelta\\mathbf{y}_{i,-p_{i}})$. The standardized t-bar statistic is defined as:\n\n$$\nZ_{t b a r}(\\mathbf{p},\\rho)=\\frac{\\sqrt{N}\\{t-b a r_{N T}-E(\\eta)\\}}{\\sqrt{V a r(\\eta)}}\n$$\n\nUnder Assumptions 4.1 and 4.2, the individual ADF statistics converge to η_i as T→∞. The text establishes that the standardized t-bar statistic converges in distribution to a standard normal variate sequentially, as T→∞ first and then N→∞.",
    "category": "CORRECT",
    "question_context_used": "The text considers the general case where errors may be serially correlated with different patterns across groups, with T and N sufficiently large. The data generating process is specified as finite-order AR(p_i+1) processes, which can be written equivalently as ADF(p_i) regressions. Key formulas include:\n\n$$\n\\Delta\\mathbf{y}_{i}=\\beta_{i}\\mathbf{y}_{i,-1}+\\mathbf{Q}_{i}\\gamma_{i}+\\varepsilon_{i}\n$$\n\nwhere $\\mathbf{Q}_{i}=(\\tau_{T},\\varDelta\\mathbf{y}_{i,-1},\\varDelta\\mathbf{y}_{i,-2},\\ldots,\\varDelta\\mathbf{y}_{i,-p_{i}})$. The standardized t-bar statistic is defined as:\n\n$$\nZ_{t b a r}(\\mathbf{p},\\rho)=\\frac{\\sqrt{N}\\{t-b a r_{N T}-E(\\eta)\\}}{\\sqrt{V a r(\\eta)}}\n$$\n\nUnder Assumptions 4.1 and 4.2, the individual ADF statistics converge to η_i as T→∞. The text establishes that the standardized t-bar statistic converges in distribution to a standard normal variate sequentially, as T→∞ first and then N→∞.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4089",
    "question": "Which of the following empirical findings from the U.S. CPI microdata (1988–2004) are correctly matched with their implications for first-generation sticky price models?",
    "options": [
      "A) The size and probability of price changes are unrelated to the time since the last price change, which contradicts the Taylor model's prediction of fixed intervals between price changes.",
      "B) The variance of aggregate inflation stems mostly from the size of price changes (intensive margin), which aligns perfectly with the DKW model's emphasis on the extensive margin.",
      "C) The fraction of price increases and decreases move in opposite directions with inflation, a pattern not captured by the Calvo model's random adjustment assumption.",
      "D) The presence of frequent large price changes (around 10%) is inconsistent with the GL model's prediction of predominantly small price changes."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "State-Dependent or Time-Dependent Pricing Does it Matter for Recent U.S. Inflation.md"
    ],
    "question_context": "In the 1988–2004 microdata collected by the U.S. Bureau of Labor Statistics for the Consumer Price Index, price changes are frequent (every 4–7 months, depending on the treatment of sale prices) and large in absolute value (on the order of 10%). The size and timing of price changes vary considerably for a given item, but the size and probability of a price change are unrelated to the time since the last price change. Movements in aggregate inflation reflect movements in the size of price changes rather than the fraction of items changing price, because of offsetting movements in the fraction of price increases and decreases. Neither leading time-dependent models (Taylor or Calvo) nor first-generation state-dependent models match all of these facts.\n\nThe variance of inflation stems mostly from the intensive margin (the size of price changes) rather than the extensive margin (the fraction of prices changing). But this dichotomy is due to offsetting movements in the fraction of price increases and price decreases, which correlate strongly with inflation. The GL model produces too few small price changes. The DKW model does not generate enough large price changes, and gives too much prominence to the extensive margin. The Taylor model falters in predicting a degenerate duration for a given item. The Calvo and Taylor models wrongly predict bigger price changes for older prices.",
    "category": "CORRECT",
    "question_context_used": "In the 1988–2004 microdata collected by the U.S. Bureau of Labor Statistics for the Consumer Price Index, price changes are frequent (every 4–7 months, depending on the treatment of sale prices) and large in absolute value (on the order of 10%). The size and timing of price changes vary considerably for a given item, but the size and probability of a price change are unrelated to the time since the last price change. Movements in aggregate inflation reflect movements in the size of price changes rather than the fraction of items changing price, because of offsetting movements in the fraction of price increases and decreases. Neither leading time-dependent models (Taylor or Calvo) nor first-generation state-dependent models match all of these facts.\n\nThe variance of inflation stems mostly from the intensive margin (the size of price changes) rather than the extensive margin (the fraction of prices changing). But this dichotomy is due to offsetting movements in the fraction of price increases and price decreases, which correlate strongly with inflation. The GL model produces too few small price changes. The DKW model does not generate enough large price changes, and gives too much prominence to the extensive margin. The Taylor model falters in predicting a degenerate duration for a given item. The Calvo and Taylor models wrongly predict bigger price changes for older prices.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2485",
    "question": "In the context of decision-making under complete ignorance, which of the following statements are true regarding the conditions (I), (M), and (WPD)?",
    "options": [
      "A) Condition (WPD) implies both condition (I) and condition (M).",
      "B) Condition (I) is equivalent to Sen's condition of Weak Preference Dominance (WPD).",
      "C) Condition (M) is necessary for the maximax extension rule to hold.",
      "D) Condition (I) alone is sufficient to characterize the maximax extension rule in a continuous framework."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Continuous-Extensions-of-an-Order-on-a-Set-to-the_1996_Journal-of-Economic-T.md"
    ],
    "question_context": "The central condition of this paper, condition (I), has been introduced in Puppe [21]. It has been noted there that (I) is implied by Sen's condition of ``Weak Preference Dominance'' (Sen [22], [23]): (WPD) Suppose there exists a subset $A^{\\prime}$ of $A$ with $\\#A^{\\prime}=\\#B$ and a one-to-one correspondence $f\\colon A^{\\prime}\\to B$ such that for all $x\\in A^{\\prime}$ one has $x R f(x)$ . Then $A\\succcurlyeq B$ . Obviously, (WPD) also implies condition (M). As a consequence of Lemma 1 below, (WPD) is in fact equivalent to the conjunction of (I) and (M). Thus by Corollary 1, (WPD) characterizes the maximax extension rule (resp. the indirect utility principle) in a continuous framework. This conclusion seems surprising since condition (WPD) was originally motivated by the search for a ``preference-based'' formulation of ``freedom of choice.''",
    "category": "CORRECT",
    "question_context_used": "The central condition of this paper, condition (I), has been introduced in Puppe [21]. It has been noted there that (I) is implied by Sen's condition of ``Weak Preference Dominance'' (Sen [22], [23]): (WPD) Suppose there exists a subset $A^{\\prime}$ of $A$ with $\\#A^{\\prime}=\\#B$ and a one-to-one correspondence $f\\colon A^{\\prime}\\to B$ such that for all $x\\in A^{\\prime}$ one has $x R f(x)$ . Then $A\\succcurlyeq B$ . Obviously, (WPD) also implies condition (M). As a consequence of Lemma 1 below, (WPD) is in fact equivalent to the conjunction of (I) and (M). Thus by Corollary 1, (WPD) characterizes the maximax extension rule (resp. the indirect utility principle) in a continuous framework. This conclusion seems surprising since condition (WPD) was originally motivated by the search for a ``preference-based'' formulation of ``freedom of choice.''",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4029",
    "question": "In the numerical general equilibrium model of the Philippines, which of the following distortions are explicitly incorporated to analyze immiserizing growth?",
    "options": [
      "A) Quantitative import restrictions and foreign exchange rationing",
      "B) Endogenous protection through import licensing and rent-seeking",
      "C) Fixed urban wage and Harris-Todaro migration",
      "D) Perfect competition in all sectors with no policy distortions"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Immiserizing-growth-and-endogenous-protec_1994_Journal-of-Development-Econom.md"
    ],
    "question_context": "The model incorporates a number of domestic and trade-related policy distortions; quantitative import restrictions, foreign exchange rationing, trade taxes, rent seeking, and a fixed urban wage (and associated urban-rural migration as in Harris and Todaro (1971)). The presence of some of these features reflects earlier applications of the model. Trade taxes including import tariffs and export taxes are modelled in ad valorem form and are applied to the international value of imports and exports. Revenues collected from these are transferred back to consumers. The model treatment of quantitative import restrictions implies that the domestic price of importables is endogenously determined, as is also the case with non-traded goods. Domestic prices for quota-restricted importables adjust until domestic output plus quota-restricted imports equal domestic demands. The government sells import licences to importers, with a licence required to import a unit of any good subject to quantitative restrictions. The price of the import licence represents the scarcity premium associated with the import quota (Clarete and Whalley, 1988). The scarcity premium associated with the quantitative import restriction constitutes an element of the endogenously determined protection in the model.",
    "category": "CORRECT",
    "question_context_used": "The model incorporates a number of domestic and trade-related policy distortions; quantitative import restrictions, foreign exchange rationing, trade taxes, rent seeking, and a fixed urban wage (and associated urban-rural migration as in Harris and Todaro (1971)). The presence of some of these features reflects earlier applications of the model. Trade taxes including import tariffs and export taxes are modelled in ad valorem form and are applied to the international value of imports and exports. Revenues collected from these are transferred back to consumers. The model treatment of quantitative import restrictions implies that the domestic price of importables is endogenously determined, as is also the case with non-traded goods. Domestic prices for quota-restricted importables adjust until domestic output plus quota-restricted imports equal domestic demands. The government sells import licences to importers, with a licence required to import a unit of any good subject to quantitative restrictions. The price of the import licence represents the scarcity premium associated with the import quota (Clarete and Whalley, 1988). The scarcity premium associated with the quantitative import restriction constitutes an element of the endogenously determined protection in the model.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1940",
    "question": "Under global misspecification, which of the following conditions are necessary for the ETHD estimator to maintain its asymptotic properties as described in Theorem 5.1?",
    "options": [
      "A) The objective function ΔP(θ,λ(θ)) must be maximized at a unique pseudo-true value θ*",
      "B) The matrix Rn(λ,θ) must converge uniformly to a nonsingular limit R(λ*,θ*)",
      "C) The moment conditions must be correctly specified with λ* = 0",
      "D) The parameter space Θ must be compact and θ* must lie in its interior"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Robust-estimation-with-exponentially-tilted-Hellinge_2021_Journal-of-Econome.md"
    ],
    "question_context": "The ETHD estimator's asymptotic properties under global misspecification rely on several key regularity conditions. The objective function ΔP(θ,λ(θ)) must be maximized at a unique pseudo-true value θ* ∈ int(Θ) where Θ is compact (Assumption 5(ii)). The matrix Rn(λ,θ) must converge uniformly to a nonsingular limit R(λ*,θ*) (Assumption 5(v)). The moment conditions do not need to be correctly specified (λ* ≠ 0 in general), but the parameter space requirements and uniqueness condition are crucial. The asymptotic distribution is given by:\n\n√n(θ̂ - θ*) →d N(0, R⁻¹Ω*R⁻¹)\n\nwhere R and Ω* are defined in the proof. These conditions ensure ETHD remains √n-consistent and asymptotically normal even under global misspecification, unlike HD which fails in such cases.",
    "category": "CORRECT",
    "question_context_used": "The ETHD estimator's asymptotic properties under global misspecification rely on several key regularity conditions. The objective function ΔP(θ,λ(θ)) must be maximized at a unique pseudo-true value θ* ∈ int(Θ) where Θ is compact (Assumption 5(ii)). The matrix Rn(λ,θ) must converge uniformly to a nonsingular limit R(λ*,θ*) (Assumption 5(v)). The moment conditions do not need to be correctly specified (λ* ≠ 0 in general), but the parameter space requirements and uniqueness condition are crucial. The asymptotic distribution is given by:\n\n√n(θ̂ - θ*) →d N(0, R⁻¹Ω*R⁻¹)\n\nwhere R and Ω* are defined in the proof. These conditions ensure ETHD remains √n-consistent and asymptotically normal even under global misspecification, unlike HD which fails in such cases.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-702",
    "question": "Which of the following conditions are necessary for exact aggregation in a diverse economy according to Fisher's analysis?",
    "options": [
      "A) Absence of specialization in labour or output aggregation",
      "B) Firms having identical technologies",
      "C) Efficient labour allocation with unrestricted variable ranges",
      "D) Systematic restrictions on investment proportions across firms"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Embodied Technology and the Existence of Labour and Output Aggregates1.md"
    ],
    "question_context": "The paper establishes that strong conditions must be satisfied for aggregation, particularly for capital aggregation but also for labour or output aggregation. These conditions include the absence of specialization and efficient labour allocation with the range of variables unrestricted except by such requirements. The analysis shows that approximate aggregation over a limited range still requires the approximate satisfaction of these exact conditions. Additionally, while institutional restrictions on variables (e.g., firms investing in the same proportion) could theoretically ease aggregation, such restrictions are deemed accidental rather than systematic. Key formula: $L_{i}$ (mobile factors) may include some types of capital, requiring consideration of their aggregation with elements of $\\kappa$ for complete capital aggregation conditions.",
    "category": "CORRECT",
    "question_context_used": "The paper establishes that strong conditions must be satisfied for aggregation, particularly for capital aggregation but also for labour or output aggregation. These conditions include the absence of specialization and efficient labour allocation with the range of variables unrestricted except by such requirements. The analysis shows that approximate aggregation over a limited range still requires the approximate satisfaction of these exact conditions. Additionally, while institutional restrictions on variables (e.g., firms investing in the same proportion) could theoretically ease aggregation, such restrictions are deemed accidental rather than systematic. Key formula: $L_{i}$ (mobile factors) may include some types of capital, requiring consideration of their aggregation with elements of $\\kappa$ for complete capital aggregation conditions.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4811",
    "question": "In the context of Lemma B.4, which of the following conditions ensure that $Q_{n w}(t) \\leq -\\min(c_{q}\\tilde{\\alpha}_{n}^{2}, t^{\\intercal}V t/4)$ for sufficiently large $n$?",
    "options": [
      "A) Continuity of $\\partial_{\\theta\\theta^{\\intercal}}Q$ in both $\\theta$ and $\\eta$",
      "B) Existence of neighborhoods $\\mathfrak{N}_{\\eta}, \\mathfrak{N}_{\\theta}$ where specific quadratic bounds hold",
      "C) Polynomial divergence rate of $\\{\\gamma_{n}\\}$ sequence",
      "D) Weak convergence of $\\tilde{\\pmb{S}}_{n w}^{*}(t,w^{*})$ to a Gaussian process"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Classical-Laplace-estimation-for-n3-consistent-estimators--_2015_Journal-of-.md"
    ],
    "question_context": "Lemma B.4 states that for some $0 < c_{q} < \\infty$, all sufficiently large $n$, all $t$ for which $\\theta_{0n}(w)+t/\\tilde{\\alpha}_{n} \\in \\Theta$, and all $w\\in\\mathcal{W}$, $Q_{n w}(t) \\leq -\\min(c_{q}\\tilde{\\alpha}_{n}^{2}, t^{\\intercal}V t/4)$. The proof relies on choosing neighborhoods $\\mathfrak{N}_{\\eta}, \\mathfrak{N}_{\\theta}$ of $\\eta_{0}, \\theta_{0}$ such that for all $\\eta\\in\\mathfrak{N}_{\\eta}$ and some $0 < c_{q} < \\infty$, (i) $\\theta\\in\\mathfrak{N}_{\\theta} \\Rightarrow Q(\\theta,\\eta) \\leq -\\{\\theta-\\theta_{0}(\\eta)\\}^{\\intercal}V\\{\\theta-\\theta_{0}(\\eta)\\}/4$, and (ii) $\\theta\\notin\\mathfrak{N}_{\\theta} \\Rightarrow Q(\\theta,\\eta) \\leq -c_{q}$. Such neighborhoods exist because of Assumptions B and C. The continuity of $\\partial_{\\theta\\theta^{\\intercal}}Q$ in both $\\theta$ and $\\eta$ (Assumption C) is crucial for ensuring these bounds hold uniformly.",
    "category": "CORRECT",
    "question_context_used": "Lemma B.4 states that for some $0 < c_{q} < \\infty$, all sufficiently large $n$, all $t$ for which $\\theta_{0n}(w)+t/\\tilde{\\alpha}_{n} \\in \\Theta$, and all $w\\in\\mathcal{W}$, $Q_{n w}(t) \\leq -\\min(c_{q}\\tilde{\\alpha}_{n}^{2}, t^{\\intercal}V t/4)$. The proof relies on choosing neighborhoods $\\mathfrak{N}_{\\eta}, \\mathfrak{N}_{\\theta}$ of $\\eta_{0}, \\theta_{0}$ such that for all $\\eta\\in\\mathfrak{N}_{\\eta}$ and some $0 < c_{q} < \\infty$, (i) $\\theta\\in\\mathfrak{N}_{\\theta} \\Rightarrow Q(\\theta,\\eta) \\leq -\\{\\theta-\\theta_{0}(\\eta)\\}^{\\intercal}V\\{\\theta-\\theta_{0}(\\eta)\\}/4$, and (ii) $\\theta\\notin\\mathfrak{N}_{\\theta} \\Rightarrow Q(\\theta,\\eta) \\leq -c_{q}$. Such neighborhoods exist because of Assumptions B and C. The continuity of $\\partial_{\\theta\\theta^{\\intercal}}Q$ in both $\\theta$ and $\\eta$ (Assumption C) is crucial for ensuring these bounds hold uniformly.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-938",
    "question": "In the context of Monte Carlo simulations for long-horizon regressions under the null hypothesis of independence, which of the following statements are supported by the empirical findings?",
    "options": [
      "A) The median estimated slope coefficients ($\\hat{\\beta}_{k}$) increase with the forecast horizon $k$.",
      "B) The empirical critical values of the $t$-statistics decrease as the horizon $k$ increases.",
      "C) The distribution of Diebold-Mariano statistics becomes more fat-tailed for larger $k$.",
      "D) The ratio of root-mean-squared error for regression forecasts to random-walk forecasts is consistently below 1 for all horizons."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Berkowitz-LongHorizonExchangeRate-2001.md"
    ],
    "question_context": "The study investigates the small-sample distribution of diagnostic statistics in long-horizon regressions under the null hypothesis of independence between exchange rates and monetary fundamentals. Key findings include: the median estimated slope coefficients ($\\hat{\\beta}_{k}$) rise with the horizon $k$, reaching 0.635 for $k=16$. The empirical distributions of $t$-statistics become right-skewed and kurtotic as $k$ increases, with the 95th percentile of the Newey-West $t$-statistic reaching 9.49 for $k=16$ (compared to the asymptotic critical value of 1.64). The Diebold-Mariano statistics also exhibit fat-tailed distributions for large $k$, with the 95th percentile of DM(A) increasing from 1.76 for $k=1$ to 4.12 for $k=16$. These results highlight the challenges of finite-sample inference in long-horizon regressions when cointegration does not hold.",
    "category": "CORRECT",
    "question_context_used": "The study investigates the small-sample distribution of diagnostic statistics in long-horizon regressions under the null hypothesis of independence between exchange rates and monetary fundamentals. Key findings include: the median estimated slope coefficients ($\\hat{\\beta}_{k}$) rise with the horizon $k$, reaching 0.635 for $k=16$. The empirical distributions of $t$-statistics become right-skewed and kurtotic as $k$ increases, with the 95th percentile of the Newey-West $t$-statistic reaching 9.49 for $k=16$ (compared to the asymptotic critical value of 1.64). The Diebold-Mariano statistics also exhibit fat-tailed distributions for large $k$, with the 95th percentile of DM(A) increasing from 1.76 for $k=1$ to 4.12 for $k=16$. These results highlight the challenges of finite-sample inference in long-horizon regressions when cointegration does not hold.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4403",
    "question": "In the context of Monte Carlo simulations for tail index estimation with censored data, which of the following statements are supported by the empirical findings?",
    "options": [
      "A) The maximum likelihood estimator (MLE) exhibits superior bias performance for extreme quantiles compared to the Beirlant and Guillou (2001) estimator.",
      "B) The estimator proposed by Einmahl et al. (2008) is appropriate for non-random censoring scenarios due to its asymptotic properties.",
      "C) The fixed-k confidence intervals maintain nominal coverage even for small values of k, unlike the ML intervals.",
      "D) Location shifts in the data significantly deteriorate the performance of estimators relying on log-ratios, such as those by Beirlant and Guillou (2001) and Einmahl et al. (2008)."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Estimation-and-inference-about-tail-features-with-tai_2022_Journal-of-Econom.md"
    ],
    "question_context": "Key formula: The Burr distribution CDF is given by $$1-F(y)=\\left({\\frac{c_{1}}{c_{1}+y^{c_{2}}}}\\right)^{c_{3}},{\\mathrm{~for~}}y>0{\\mathrm{~and~}}c_{1},c_{2},c_{3}>0,$$ with parameters set to $c_{1}=c_{3}=1$ and $c_{2}=2$. The simulations compare the maximum likelihood estimator (MLE) with existing methods under various censoring scenarios. The MLE demonstrates invariance to location shifts, while other estimators exhibit significant bias when data are transformed. The fixed-k confidence intervals consistently achieve nominal coverage, unlike the ML intervals which require larger k for validity.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The Burr distribution CDF is given by $$1-F(y)=\\left({\\frac{c_{1}}{c_{1}+y^{c_{2}}}}\\right)^{c_{3}},{\\mathrm{~for~}}y>0{\\mathrm{~and~}}c_{1},c_{2},c_{3}>0,$$ with parameters set to $c_{1}=c_{3}=1$ and $c_{2}=2$. The simulations compare the maximum likelihood estimator (MLE) with existing methods under various censoring scenarios. The MLE demonstrates invariance to location shifts, while other estimators exhibit significant bias when data are transformed. The fixed-k confidence intervals consistently achieve nominal coverage, unlike the ML intervals which require larger k for validity.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3004",
    "question": "In the context of aggregators over feasible and pseudo-evaluations, which of the following statements are true regarding the conditions under which an IIA and Paretian aggregator is oligarchic?",
    "options": [
      "A) The aggregator is oligarchic if and only if the set of feasible evaluations is totally blocked.",
      "B) The aggregator is oligarchic if and only if the set of feasible evaluations is not an affine subspace.",
      "C) The aggregator is oligarchic if the set of feasible evaluations is both totally blocked and not an affine subspace.",
      "D) The aggregator is oligarchic if the set of feasible evaluations is either totally blocked or not an affine subspace."
    ],
    "gold_answer": "C, D",
    "source_files": [
      "Aggregation-of-binary-evaluations-with-abstenti_2010_Journal-of-Economic-The.md"
    ],
    "question_context": "The main result states that for a non-degenerate subset $X$ of $\\{0,1\\}^{m}$, the following are equivalent: (1) $X$ is totally blocked and is not an affine subspace, (2) For every $n$, every IIA and Paretian aggregator $f:(X^{*})^{n}\\to X^{*}$ is oligarchic, (3) For some $n\\geqslant3$, every IIA and Paretian aggregator $f:(X^{*})^{n}\\to X^{*}$ is oligarchic, (4) For every $n$, every IIA and Paretian aggregator $f:X^{n}\\to X^{*}$ is oligarchic, (5) For some $n\\geqslant3$, every IIA and Paretian aggregator $f:X^{n}\\to X^{*}$ is oligarchic. The theorem establishes that the oligarchic nature of the aggregator is guaranteed under the combined condition of total blockedness and not being an affine subspace, but also suggests that either condition alone might suffice under certain interpretations.",
    "category": "INCORRECT",
    "question_context_used": "The main result states that for a non-degenerate subset $X$ of $\\{0,1\\}^{m}$, the following are equivalent: (1) $X$ is totally blocked and is not an affine subspace, (2) For every $n$, every IIA and Paretian aggregator $f:(X^{*})^{n}\\to X^{*}$ is oligarchic, (3) For some $n\\geqslant3$, every IIA and Paretian aggregator $f:(X^{*})^{n}\\to X^{*}$ is oligarchic, (4) For every $n$, every IIA and Paretian aggregator $f:X^{n}\\to X^{*}$ is oligarchic, (5) For some $n\\geqslant3$, every IIA and Paretian aggregator $f:X^{n}\\to X^{*}$ is oligarchic. The theorem establishes that the oligarchic nature of the aggregator is guaranteed under the combined condition of total blockedness and not being an affine subspace, but also suggests that either condition alone might suffice under certain interpretations.",
    "gold_answer_parsed": [
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\"]}",
      "model_answer": [
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1656",
    "question": "Which of the following fields were represented by the Program Committee members of the 2003 Joint Meeting of the Econometric Society and the American Economic Association?",
    "options": [
      "A) Time Series Econometrics and Financial Econometrics",
      "B) Quantum Economics and Behavioral Finance",
      "C) International Trade and Development",
      "D) Theoretical Industrial Organization and Applied Theory"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Announcements-2002.md"
    ],
    "question_context": "The Program Committee for the 2003 Joint Meeting of the Econometric Society and the American Economic Association included experts from various subfields of economics. For instance, Francis Diebold specialized in Time Series Econometrics, Financial Econometrics, and Macroeconometrics. James Levinsohn focused on International Trade and Development, while Michael Riordan worked on Theoretical Industrial Organization and Applied Theory. These members were selected to cover a broad spectrum of advanced economic research areas, ensuring a comprehensive academic program for the conference. The committee's composition reflects the interdisciplinary nature of modern economic research, bridging theoretical and applied perspectives.",
    "category": "CORRECT",
    "question_context_used": "The Program Committee for the 2003 Joint Meeting of the Econometric Society and the American Economic Association included experts from various subfields of economics. For instance, Francis Diebold specialized in Time Series Econometrics, Financial Econometrics, and Macroeconometrics. James Levinsohn focused on International Trade and Development, while Michael Riordan worked on Theoretical Industrial Organization and Applied Theory. These members were selected to cover a broad spectrum of advanced economic research areas, ensuring a comprehensive academic program for the conference. The committee's composition reflects the interdisciplinary nature of modern economic research, bridging theoretical and applied perspectives.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2134",
    "question": "In the stochastic self-enforcing contractual framework, which of the following mechanisms can transactors use to widen the self-enforcing range of a contract?",
    "options": [
      "A) Limiting expected gains from nonperformance via contract terms",
      "B) Shifting reputational capital between transactors to align with potential gains from nonperformance",
      "C) Relying solely on court-enforced terms without reputational considerations",
      "D) Implementing resale-price maintenance or exclusive territories to redistribute reputational capital"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Klein-VerticalIntegrationSelfEnforcing-1997.md"
    ],
    "question_context": "The role of contract terms within this framework is very different from the standard principal-agent view of contracts, where contract terms are designed to create optimal incentives for some imperfect court-enforceable proxy for performance. Instead, contract terms are chosen by transactors more generally to facilitate self-enforcement. Facilitation of self-enforcement may be accomplished in either one of two ways. First, contract terms can operate on the left-hand side of equation (1), limiting the expected gains from nonperformance and thereby reducing the amount of reputational capital necessary to make the arrangement self-enforcing. Alternatively, contract terms can operate on the right-hand side of (1), shifting reputational capital between transactors so that each party's reputational capital coincides more closely with its potential expected gain from nonperformance. This second effect is the major rationale for many contractual restrictions found in distribution arrangements, such as resale-price maintenance or exclusive territories, which have the effect of shifting some of the manufacturer's reputational capital to the distributor and creating a situation where the distributor has more to lose if it does not perform (Klein and Murphy, 1988).",
    "category": "CORRECT",
    "question_context_used": "The role of contract terms within this framework is very different from the standard principal-agent view of contracts, where contract terms are designed to create optimal incentives for some imperfect court-enforceable proxy for performance. Instead, contract terms are chosen by transactors more generally to facilitate self-enforcement. Facilitation of self-enforcement may be accomplished in either one of two ways. First, contract terms can operate on the left-hand side of equation (1), limiting the expected gains from nonperformance and thereby reducing the amount of reputational capital necessary to make the arrangement self-enforcing. Alternatively, contract terms can operate on the right-hand side of (1), shifting reputational capital between transactors so that each party's reputational capital coincides more closely with its potential expected gain from nonperformance. This second effect is the major rationale for many contractual restrictions found in distribution arrangements, such as resale-price maintenance or exclusive territories, which have the effect of shifting some of the manufacturer's reputational capital to the distributor and creating a situation where the distributor has more to lose if it does not perform (Klein and Murphy, 1988).",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2130",
    "question": "In Farebrother's test with multiple regressions, which of the following conditions lead to the closest approximation between the upper bound $d_{U}$ and the critical value $c_{U}$?",
    "options": [
      "A) Large values of $q_{1}$ and $q_{2}$ (e.g., $q_{1}=q_{2}=100$)",
      "B) Small values of $q_{1}$ and $q_{2}$ (e.g., $q_{1}=q_{2}=10$)",
      "C) Intermediate values of $m$ (e.g., $m=3$)",
      "D) High values of $h$ (e.g., $h=10$)"
    ],
    "gold_answer": "B, C",
    "source_files": [
      "DUFOUR和MAHSEREDJIAN - 1993 - TABULATION OF FAREBROTHER TEST FOR LINEAR RESTRICTION.md"
    ],
    "question_context": "The text provides critical values for Farebrother's test across different parameter settings, including $h=2,3,4,5$, $q_{i}=q$ for $i=1,\\ldots,h$, and $\\alpha=0.05$. The upper bound $d_{U}$ yielded by (5) is typically quite close to $c_{U}$; for example, for $q_{1}=q_{2}=10$, the maximal difference between $d_{U}$ and $c_{U}$ (for $m=1,\\ldots,5$) is 0.62. The tables show that the difference between $d_{U}$ and $c_{U}$ varies with the parameters $q_{1}$, $q_{2}$, $m$, and $h$. The question focuses on identifying the parameter combinations that minimize this difference.",
    "category": "INCORRECT",
    "question_context_used": "The text provides critical values for Farebrother's test across different parameter settings, including $h=2,3,4,5$, $q_{i}=q$ for $i=1,\\ldots,h$, and $\\alpha=0.05$. The upper bound $d_{U}$ yielded by (5) is typically quite close to $c_{U}$; for example, for $q_{1}=q_{2}=10$, the maximal difference between $d_{U}$ and $c_{U}$ (for $m=1,\\ldots,5$) is 0.62. The tables show that the difference between $d_{U}$ and $c_{U}$ varies with the parameters $q_{1}$, $q_{2}$, $m$, and $h$. The question focuses on identifying the parameter combinations that minimize this difference.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3753",
    "question": "In the context of coordinated versus noncooperative capital taxation, which of the following statements are supported by the analysis?",
    "options": [
      "A) Coordinated capital taxes are higher than noncooperative ones in the short run but lower in the medium run.",
      "B) The gains from coordination are equivalent to an increase by 1.21% of private consumption in every period in each country.",
      "C) Capital tax harmonization is roughly optimal even when countries differ in initial asset positions.",
      "D) The larger country gains less from coordination relative to the smaller country."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Dynamic Capital Tax Competition under the Source Principle.md"
    ],
    "question_context": "Our first result is unsurprising: the coordinated solution in a symmetric environment is identical to the solution in the closed economy, which is the second-best policy. Therefore, coordinated capital taxes are higher than the noncooperative ones in the short run but lower in the medium run. The latter is in contrast to common beliefs that have influenced policy debates in Europe and elsewhere. Capital taxes are initially at the maximum 100 percent for years 0 to 8 and immediately switch from 100 percent to 0 with only one intermediate period, as in Atkeson, Chari, and Kehoe (1999). The welfare implications are worth noting as well. The gains from coordination relative to the noncooperative outcome under capital mobility are equivalent to an increase by 1.21 percent of private consumption in every period in each country. With symmetry, there is no upside from capital-market integration: because the two countries are identical, there are no potential gains from trade in capital. Without coordination, the tax externalities that emerge when the capital market is integrated lead to welfare losses. Coordination then reverses the welfare losses by internalizing the externalities. Interestingly, if one country is more populous or more productive than the other, then the coordinated solution is still identical to the closed economy solution. While the noncooperative game features different policies between the larger and the smaller countries, the coordinated solution has identical policies for both countries. The larger country gains more from coordination (relative to noncooperation) than the smaller country: 1.3 percent and 0.79 percent, respectively.",
    "category": "CORRECT",
    "question_context_used": "Our first result is unsurprising: the coordinated solution in a symmetric environment is identical to the solution in the closed economy, which is the second-best policy. Therefore, coordinated capital taxes are higher than the noncooperative ones in the short run but lower in the medium run. The latter is in contrast to common beliefs that have influenced policy debates in Europe and elsewhere. Capital taxes are initially at the maximum 100 percent for years 0 to 8 and immediately switch from 100 percent to 0 with only one intermediate period, as in Atkeson, Chari, and Kehoe (1999). The welfare implications are worth noting as well. The gains from coordination relative to the noncooperative outcome under capital mobility are equivalent to an increase by 1.21 percent of private consumption in every period in each country. With symmetry, there is no upside from capital-market integration: because the two countries are identical, there are no potential gains from trade in capital. Without coordination, the tax externalities that emerge when the capital market is integrated lead to welfare losses. Coordination then reverses the welfare losses by internalizing the externalities. Interestingly, if one country is more populous or more productive than the other, then the coordinated solution is still identical to the closed economy solution. While the noncooperative game features different policies between the larger and the smaller countries, the coordinated solution has identical policies for both countries. The larger country gains more from coordination (relative to noncooperation) than the smaller country: 1.3 percent and 0.79 percent, respectively.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2136",
    "question": "According to the analysis of producer's liability unification in the EEC, which of the following outcomes are theoretically supported?",
    "options": [
      "A) Global welfare improvement due to reduced information asymmetry",
      "B) Increase in exports from the country adopting producer's liability under lex loci contractus",
      "C) Higher recurrent rents for hazardous factors in the long run",
      "D) Effective price reduction due to standardized liability rules"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Divergent_Liability_Rules_and_Economic_Integration.md"
    ],
    "question_context": "The unification of liability rules to producer's liability has been analysed from the point of view of its impact via the producer-consumer relationship, with additional effects on resource use, factor incomes, and market structure. The assumption that consumers incur higher costs in identifying changes in production technology than producers in identifying changes in consumption technology leads to the conclusion that a shift to producer's liability from a divergent state will improve global welfare and lower effective prices. Key formula: The shift will increase exports of goods from the country adopting producer's liability as its new rule if the lex loci contractus prevailed prior to unification; it will increase the export of goods into that country if the conflict-of-law convention adhered to prior to unification was the lex loci delicti.",
    "category": "CORRECT",
    "question_context_used": "The unification of liability rules to producer's liability has been analysed from the point of view of its impact via the producer-consumer relationship, with additional effects on resource use, factor incomes, and market structure. The assumption that consumers incur higher costs in identifying changes in production technology than producers in identifying changes in consumption technology leads to the conclusion that a shift to producer's liability from a divergent state will improve global welfare and lower effective prices. Key formula: The shift will increase exports of goods from the country adopting producer's liability as its new rule if the lex loci contractus prevailed prior to unification; it will increase the export of goods into that country if the conflict-of-law convention adhered to prior to unification was the lex loci delicti.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2653",
    "question": "In the context of the empirical strategy described, which of the following statements are correct regarding the OLS specifications and provider behavior?",
    "options": [
      "A) The returned-received ratio (R/R) can theoretically range from 0 to 3, with 3 indicating full return of the tripled amount.",
      "B) Session-level fixed effects are included in the OLS specification to control for individual provider characteristics.",
      "C) Wild-cluster bootstrapping is used due to the small number of clusters (sessions) to ensure robust standard errors.",
      "D) The vector of controls (X) in the OLS specification includes provider demographics, sector of employment, and recognition of patients."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Mbiti和Serra - 2022 - Health workers' behavior, patient reporting and reputational concerns Lab-in-the-field experimental.md"
    ],
    "question_context": "The OLS specification for estimating treatment effects on provider behavior is given by: $$ y_{i k}^{r}=\\beta_{0}+T_{i}^{'}\\beta_{1}+X_{i}^{'}\\beta_{2}+\\nu_{i}^{'}\\beta_{3}+\\varepsilon_{i k} $$ where $y_{i k}^{r}$ is the ratio of the amount provider $i$ returned to the matched patient and the amount the patient had sent, ranging from 0 to 3. The vector T represents binary treatment indicators, and X includes controls such as demographics, sector of employment, and whether the provider knows other participants. Given the small number of clusters (sessions), wild-cluster bootstrapping is employed to address potential issues with standard error estimation. The analysis also explores heterogeneity in treatment effects by interacting treatment variables with sector of employment and baseline reciprocity measures.",
    "category": "CORRECT",
    "question_context_used": "The OLS specification for estimating treatment effects on provider behavior is given by: $$ y_{i k}^{r}=\\beta_{0}+T_{i}^{'}\\beta_{1}+X_{i}^{'}\\beta_{2}+\\nu_{i}^{'}\\beta_{3}+\\varepsilon_{i k} $$ where $y_{i k}^{r}$ is the ratio of the amount provider $i$ returned to the matched patient and the amount the patient had sent, ranging from 0 to 3. The vector T represents binary treatment indicators, and X includes controls such as demographics, sector of employment, and whether the provider knows other participants. Given the small number of clusters (sessions), wild-cluster bootstrapping is employed to address potential issues with standard error estimation. The analysis also explores heterogeneity in treatment effects by interacting treatment variables with sector of employment and baseline reciprocity measures.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2851",
    "question": "Based on the regression analysis of mortgage rate forecast errors, which of the following statements are supported by the empirical evidence?",
    "options": [
      "A) Homeowners with mortgages have more accurate mortgage rate perceptions than outright homeowners",
      "B) Renters demonstrate better mortgage rate forecasting accuracy than homeowners with refinancing plans",
      "C) Recent refinancers show statistically significant improvements in 1-year ahead forecasts but not in 3-year ahead forecasts",
      "D) The refinancing motive creates asymmetric sensitivity in homeowners' inflation expectations to mortgage rate changes"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Effects-of-monetary-policy-on-household-expectations_2024_Journal-of-Monetar.md"
    ],
    "question_context": "The housing module examines mortgage rate forecast errors through the regression: \n\n$$\n\\begin{array}{r l}&{F E_{i,t}^{h}=\\alpha_{1}\\times\\mathbf{I}(o w n e d~o u t r i g h t)_{i,t}+\\alpha_{2}\\times\\mathbf{I}(o w n e d~m o r t g a g e)_{i,t}+\\alpha_{3}\\times\\mathbf{I}(r e f i n a n c e d~l a s t~y e a r)_{i,t}}\\ &{\\qquad+~\\alpha_{4}\\times\\mathbf{I}(p l a n~t o~r e f i n a n c e)_{i,t}+\\xi_{t}+\\delta X_{i,t}+\\epsilon_{i,t},}\\end{array}\n$$\n\nTable 4 shows homeowners with mortgages (α₂ = -0.8042***) outperform outright homeowners (α₁ = -0.4027***) in forecast accuracy. Recent refinancers (α₃ = -0.1040***) show significant 1-year ahead improvements but not for 3-year ahead (α₃ = -0.0566, p>0.1). The asymmetric effects specification:\n\n$$\n\\begin{array}{r l}&{E_{i,t+6}^{h-y r}-E_{i,t}^{h-y r}=\\alpha+\\beta_{1}\\mathrm{homeowner}_{i}\\times A R_{t}\\times I_{t}^{+}+\\beta_{2}\\mathrm{homeowner}_{i}\\times A R_{t}\\times I_{t}^{-}}\\ &{\\qquad+\\beta_{3}\\mathrm{renter}_{i}\\times A R_{t}\\times I_{t}^{+}+\\beta_{4}\\mathrm{renter}_{i}\\times A R_{t}\\times I_{t}^{-}+\\gamma Z_{t}+\\delta X_{i,t}+\\epsilon_{i,t},}\\end{array}\n$$\n\nreveals homeowners' inflation expectations respond more strongly to rate declines (β₂ = -1.0020***) than increases (β₁ = -0.2971, p>0.1), with F-test rejecting equality (p<0.05).",
    "category": "CORRECT",
    "question_context_used": "The housing module examines mortgage rate forecast errors through the regression: \n\n$$\n\\begin{array}{r l}&{F E_{i,t}^{h}=\\alpha_{1}\\times\\mathbf{I}(o w n e d~o u t r i g h t)_{i,t}+\\alpha_{2}\\times\\mathbf{I}(o w n e d~m o r t g a g e)_{i,t}+\\alpha_{3}\\times\\mathbf{I}(r e f i n a n c e d~l a s t~y e a r)_{i,t}}\\ &{\\qquad+~\\alpha_{4}\\times\\mathbf{I}(p l a n~t o~r e f i n a n c e)_{i,t}+\\xi_{t}+\\delta X_{i,t}+\\epsilon_{i,t},}\\end{array}\n$$\n\nTable 4 shows homeowners with mortgages (α₂ = -0.8042***) outperform outright homeowners (α₁ = -0.4027***) in forecast accuracy. Recent refinancers (α₃ = -0.1040***) show significant 1-year ahead improvements but not for 3-year ahead (α₃ = -0.0566, p>0.1). The asymmetric effects specification:\n\n$$\n\\begin{array}{r l}&{E_{i,t+6}^{h-y r}-E_{i,t}^{h-y r}=\\alpha+\\beta_{1}\\mathrm{homeowner}_{i}\\times A R_{t}\\times I_{t}^{+}+\\beta_{2}\\mathrm{homeowner}_{i}\\times A R_{t}\\times I_{t}^{-}}\\ &{\\qquad+\\beta_{3}\\mathrm{renter}_{i}\\times A R_{t}\\times I_{t}^{+}+\\beta_{4}\\mathrm{renter}_{i}\\times A R_{t}\\times I_{t}^{-}+\\gamma Z_{t}+\\delta X_{i,t}+\\epsilon_{i,t},}\\end{array}\n$$\n\nreveals homeowners' inflation expectations respond more strongly to rate declines (β₂ = -1.0020***) than increases (β₁ = -0.2971, p>0.1), with F-test rejecting equality (p<0.05).",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1536",
    "question": "In the context of attribute correlation and similarity, which of the following statements are correct regarding the ex ante uncertainty and covariance of a project's value?",
    "options": [
      "A) The ex ante uncertainty about the project’s value is given by the expected similarity of a randomly drawn pair of attributes.",
      "B) The ex ante covariance between the principal’s and the agent’s values is determined by the similarity between attributes drawn from their respective densities.",
      "C) The residual uncertainty after sampling is minimized when out-of-sample attributes are as dissimilar as possible.",
      "D) The sample weight τj(a) reduces to the expected similarity of attribute aj to a random attribute if aj is independent of other sample attributes."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "ecta18355.md"
    ],
    "question_context": "The ex ante uncertainty about the project’s value v can be rewritten as \n\n$$\\int_{A}\\int_{A}\\sigma(a,a^{\\prime})\\omega(a)\\omega(a^{\\prime})\\mathrm{d}a\\mathrm{d}a^{\\prime}=\\mathbb{E}_{a,a^{\\prime}\\sim\\omega}\\big[\\sigma(a,a^{\\prime})\\big].$$\n\nThe right-hand side corresponds to the similarity of a randomly drawn pair of attributes, where attributes are drawn independently in the pair and the probability of attribute a being drawn is ω(a). The more similar the relevant attributes are on average, the more their realizations tend to move together, hence the more uncertain the project is. Analogously, the ex ante covariance between the principal’s and the agent’s values is given by \n\n$$\\int_{A}\\int_{A}\\sigma(a,a^{\\prime})\\omega_{A}(a)\\omega_{P}(a^{\\prime})\\mathrm{d}a\\mathrm{d}a^{\\prime}=\\mathbb{E}_{a\\sim\\omega_{A},a^{\\prime}\\sim\\omega_{P}}\\big[\\sigma(a,a^{\\prime})\\big],$$\n\nwhich corresponds to the similarity between a pair of random attributes, one drawn from density ωA and the other from ωP. The residual uncertainty about the project after sampling corresponds to the average similarity between out-of-sample attributes 𝔼a,a′∼ω[σa(a,a′)]. Single-player sampling leads to the greatest reduction in the average attribute similarity—it makes out-of-sample attributes as dissimilar ex post as possible. The sample weight τj(a) quantifies the similarity of attribute aj∈a to the rest of the attribute space and reduces to simply 𝔼a∼ω[σ(a,aj)] if aj is independent of other sample attributes.",
    "category": "CORRECT",
    "question_context_used": "The ex ante uncertainty about the project’s value v can be rewritten as \n\n$$\\int_{A}\\int_{A}\\sigma(a,a^{\\prime})\\omega(a)\\omega(a^{\\prime})\\mathrm{d}a\\mathrm{d}a^{\\prime}=\\mathbb{E}_{a,a^{\\prime}\\sim\\omega}\\big[\\sigma(a,a^{\\prime})\\big].$$\n\nThe right-hand side corresponds to the similarity of a randomly drawn pair of attributes, where attributes are drawn independently in the pair and the probability of attribute a being drawn is ω(a). The more similar the relevant attributes are on average, the more their realizations tend to move together, hence the more uncertain the project is. Analogously, the ex ante covariance between the principal’s and the agent’s values is given by \n\n$$\\int_{A}\\int_{A}\\sigma(a,a^{\\prime})\\omega_{A}(a)\\omega_{P}(a^{\\prime})\\mathrm{d}a\\mathrm{d}a^{\\prime}=\\mathbb{E}_{a\\sim\\omega_{A},a^{\\prime}\\sim\\omega_{P}}\\big[\\sigma(a,a^{\\prime})\\big],$$\n\nwhich corresponds to the similarity between a pair of random attributes, one drawn from density ωA and the other from ωP. The residual uncertainty about the project after sampling corresponds to the average similarity between out-of-sample attributes 𝔼a,a′∼ω[σa(a,a′)]. Single-player sampling leads to the greatest reduction in the average attribute similarity—it makes out-of-sample attributes as dissimilar ex post as possible. The sample weight τj(a) quantifies the similarity of attribute aj∈a to the rest of the attribute space and reduces to simply 𝔼a∼ω[σ(a,aj)] if aj is independent of other sample attributes.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4428",
    "question": "Under the assumptions of Theorem 4.1, which of the following statements are true regarding the rates of convergence for $\\mathcal{D}_{n}(k)$ and $\\mathrm{RIP}_{n}(k)$?",
    "options": [
      "A) Under Assumption (MExp), $\\mathcal{D}_{n}(k)$ has a rate involving $k^{1/2}$ in the second term.",
      "B) Under Assumption (JExp), $\\mathrm{RIP}_{n}(k)$ has a rate involving $k$ in the second term.",
      "C) The simplified rates under both assumptions are $\\sqrt{\\frac{\\Upsilon_{n,k}^{\\Gamma}k\\log(e p/k)}{n}}$ for $\\mathcal{D}_{n}(k)$ and $\\sqrt{\\frac{\\Upsilon_{n,k}^{\\Sigma}k\\log(e p/k)}{n}}$ for $\\mathrm{RIP}_{n}(k)$.",
      "D) The second term in the rates is always of lower order than the first term under both assumptions."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "UNIFORM-IN-SUBMODEL_BOUNDS_FOR_LINEAR_REGRESSION_IN_A_MODEL-FREE_FRAMEWORK.md"
    ],
    "question_context": "Key formula: Under Assumption (MExp), the rates of convergence are given by:\n\n$$\n\\begin{array}{r}{\\mathcal{D}_{n}(k)=O_{p}\\left(\\sqrt{\\frac{\\Upsilon_{n,k}^{\\Gamma}k\\log(e p/k)}{n}}+\\frac{k^{1/2}(\\log n)^{2/\\alpha}(k\\log(e p/k))^{1/T_{1}(\\alpha/2)}}{n}\\right),}\\ {R I P_{n}(k)=O_{p}\\left(\\sqrt{\\frac{\\Upsilon_{n,k}^{\\Sigma}k\\log(e p/k)}{n}}+\\frac{k(\\log n)^{2/\\alpha}(k\\log(e p/k))^{1/T_{1}(\\alpha/2)}}{n}\\right).}\\end{array}\n$$\n\nUnder Assumption (JExp), the rates are:\n\n$$\n\\begin{array}{r}{\\mathcal{D}_{n}(k)=O_{p}\\left(\\sqrt{\\frac{\\Upsilon_{n,k}^{\\Gamma}k\\log(e p/k)}{n}}+\\frac{(\\log n)^{2/\\alpha}(k\\log(e p/k))^{1/T_{1}(\\alpha/2)}}{n}\\right),}\\ {R I P_{n}(k)=O_{p}\\left(\\sqrt{\\frac{\\Upsilon_{n,k}^{\\Sigma}k\\log(e p/k)}{n}}+\\frac{(\\log n)^{2/\\alpha}(k\\log(e p/k))^{1/T_{1}(\\alpha/2)}}{n}\\right).}\\end{array}\n$$\n\nRemark 4.1 notes that in most cases, the second term in the rate of convergence is of lower order than the first term, leading to simplified rates under both assumptions.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: Under Assumption (MExp), the rates of convergence are given by:\n\n$$\n\\begin{array}{r}{\\mathcal{D}_{n}(k)=O_{p}\\left(\\sqrt{\\frac{\\Upsilon_{n,k}^{\\Gamma}k\\log(e p/k)}{n}}+\\frac{k^{1/2}(\\log n)^{2/\\alpha}(k\\log(e p/k))^{1/T_{1}(\\alpha/2)}}{n}\\right),}\\ {R I P_{n}(k)=O_{p}\\left(\\sqrt{\\frac{\\Upsilon_{n,k}^{\\Sigma}k\\log(e p/k)}{n}}+\\frac{k(\\log n)^{2/\\alpha}(k\\log(e p/k))^{1/T_{1}(\\alpha/2)}}{n}\\right).}\\end{array}\n$$\n\nUnder Assumption (JExp), the rates are:\n\n$$\n\\begin{array}{r}{\\mathcal{D}_{n}(k)=O_{p}\\left(\\sqrt{\\frac{\\Upsilon_{n,k}^{\\Gamma}k\\log(e p/k)}{n}}+\\frac{(\\log n)^{2/\\alpha}(k\\log(e p/k))^{1/T_{1}(\\alpha/2)}}{n}\\right),}\\ {R I P_{n}(k)=O_{p}\\left(\\sqrt{\\frac{\\Upsilon_{n,k}^{\\Sigma}k\\log(e p/k)}{n}}+\\frac{(\\log n)^{2/\\alpha}(k\\log(e p/k))^{1/T_{1}(\\alpha/2)}}{n}\\right).}\\end{array}\n$$\n\nRemark 4.1 notes that in most cases, the second term in the rate of convergence is of lower order than the first term, leading to simplified rates under both assumptions.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3799",
    "question": "In the context of the Tobit model with a Normal-Inverse-Gamma (NIG) conjugate prior, which of the following statements are true regarding the posterior conditional distributions?",
    "options": [
      "A) The posterior conditional distribution of $(\\beta, \\sigma^2)$ given $(y, z)$ follows a NIG distribution with updated hyperparameters.",
      "B) The posterior conditional distribution of $z$ given $y$ is independent of the parameters $(\\beta, \\sigma^2)$ in the DMS approach.",
      "C) The posterior conditional distribution of $(\\beta, \\sigma^2)$ given $(y, z)$ requires Metropolis-Hastings steps for sampling.",
      "D) The posterior conditional distribution of $z$ given $(y, \\beta, \\sigma^2)$ is a truncated normal distribution."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "08-Bayesian_Inference_in_Common_Microeconometric_Models_With_Massive_Datasets_by_Double_Marginali.md"
    ],
    "question_context": "The Tobit model is defined by the equations:\n\n$$\n\\begin{array}{r}{z_{i}=x_{i}\\beta+\\sigma\\varepsilon_{i},}\\ {y_{i}=\\operatorname*{max}\\left(c,z_{i}\\right),}\\end{array}\n$$\n\nwhere $\\left\\{\\varepsilon_{i}\\right\\}$ are IID standard normal disturbances. The NIG conjugate prior is specified as $(\\beta,\\sigma^{2}) \\sim \\text{NIG} (\\mu,\\Lambda,a,b)$. The posterior conditional distribution of $(\\beta, \\sigma^2)$ given $(y, z)$ follows $\\mathrm{NIG}\\left(\\overline{{\\mu}},\\overline{{\\Lambda}},\\overline{{a}},\\overline{{b}}\\right)$, where the hyperparameters are updated as:\n\n$$\n\\overline{{\\mu}}=\\left(\\Lambda+\\sum_{i=1}^{n}x_{i}^{\\prime}x_{i}\\right)^{-1}\\left(\\Lambda\\mu+\\sum_{i=1}^{n}x_{i}^{\\prime}z_{i}\\right),\n$$\n\n$$\n\\begin{array}{c}{{\\overline{{{\\Lambda}}}=\\Lambda+\\displaystyle\\sum_{i=1}^{n}x_{i}^{\\prime}x_{i},}}\\ {{\\overline{{{a}}}=a+\\displaystyle\\frac{n}{2}},}}\\ {{\\overline{{{b}}}=b+\\displaystyle\\frac{1}{2}\\sum_{i=1}^{n}z_{i}^{2}+\\displaystyle\\frac{1}{2}{\\mu^{\\prime}}\\Lambda\\mu-\\displaystyle\\frac{1}{2}{\\overline{{\\mu}}^{\\prime}}\\overline{{\\Lambda}}\\overline{{\\mu}}.}}}\\end{array}\n$$\n\nIn the DMS approach, the posterior conditional distribution of $z$ given $y$ is evaluated as $p(z|y) \\propto p(z)p(y|z)$, which does not depend on $(\\beta, \\sigma^2)$. For observations where $y_i > c$, $z_i$ is deterministic, and for $y_i = c$, $z_i$ follows a truncated normal distribution below $c$.",
    "category": "INCORRECT",
    "question_context_used": "The Tobit model is defined by the equations:\n\n$$\n\\begin{array}{r}{z_{i}=x_{i}\\beta+\\sigma\\varepsilon_{i},}\\ {y_{i}=\\operatorname*{max}\\left(c,z_{i}\\right),}\\end{array}\n$$\n\nwhere $\\left\\{\\varepsilon_{i}\\right\\}$ are IID standard normal disturbances. The NIG conjugate prior is specified as $(\\beta,\\sigma^{2}) \\sim \\text{NIG} (\\mu,\\Lambda,a,b)$. The posterior conditional distribution of $(\\beta, \\sigma^2)$ given $(y, z)$ follows $\\mathrm{NIG}\\left(\\overline{{\\mu}},\\overline{{\\Lambda}},\\overline{{a}},\\overline{{b}}\\right)$, where the hyperparameters are updated as:\n\n$$\n\\overline{{\\mu}}=\\left(\\Lambda+\\sum_{i=1}^{n}x_{i}^{\\prime}x_{i}\\right)^{-1}\\left(\\Lambda\\mu+\\sum_{i=1}^{n}x_{i}^{\\prime}z_{i}\\right),\n$$\n\n$$\n\\begin{array}{c}{{\\overline{{{\\Lambda}}}=\\Lambda+\\displaystyle\\sum_{i=1}^{n}x_{i}^{\\prime}x_{i},}}\\ {{\\overline{{{a}}}=a+\\displaystyle\\frac{n}{2}},}}\\ {{\\overline{{{b}}}=b+\\displaystyle\\frac{1}{2}\\sum_{i=1}^{n}z_{i}^{2}+\\displaystyle\\frac{1}{2}{\\mu^{\\prime}}\\Lambda\\mu-\\displaystyle\\frac{1}{2}{\\overline{{\\mu}}^{\\prime}}\\overline{{\\Lambda}}\\overline{{\\mu}}.}}}\\end{array}\n$$\n\nIn the DMS approach, the posterior conditional distribution of $z$ given $y$ is evaluated as $p(z|y) \\propto p(z)p(y|z)$, which does not depend on $(\\beta, \\sigma^2)$. For observations where $y_i > c$, $z_i$ is deterministic, and for $y_i = c$, $z_i$ follows a truncated normal distribution below $c$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-508",
    "question": "In the context of the baseline logit estimation of citation probabilities, which of the following statements are supported by the empirical results?",
    "options": [
      "A) The elasticity of citation odds with respect to distance is constant after accounting for positive distance.",
      "B) Controlling for academic ties reduces the estimated negative effects of geographic separation by approximately 50%.",
      "C) The inclusion of career ties in the model triples the pseudo-R2, indicating a dramatic improvement in model fit.",
      "D) The effect of crossing a national border on citation odds is stronger in academic citations than in patent citations."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "rest_a_00771.md"
    ],
    "question_context": "The baseline logit coefficients have the interpretation of marginal effects on the log odds, with standard errors clustered at the cited article level. The first specification includes four geographic explanatory variables: an indicator for distance greater than 0, log distance (interacted with the positive distance indicator), and indicators for residing in different countries and from countries with different official languages. The two-part distance function is necessary because there is no direct way to measure the distance between scholars at the same institution. The elasticity of citation odds with respect to distance is constant after accounting for positive distance, a standard assumption in gravity equations. However, the inclusion of academic ties (e.g., co-authorship, past colocation) significantly reduces the estimated negative effects of geographic separation by approximately 50%, as ties are negatively correlated with distance. The pseudo-R2 nearly triples from 0.033 to 0.091 when ties are included, indicating a dramatic improvement in model fit. The effect of crossing a national border on citation odds is found to be smaller in academic citations compared to patent citations, with a reduction of 18% versus 41% in Singh and Marx (2013).",
    "category": "CORRECT",
    "question_context_used": "The baseline logit coefficients have the interpretation of marginal effects on the log odds, with standard errors clustered at the cited article level. The first specification includes four geographic explanatory variables: an indicator for distance greater than 0, log distance (interacted with the positive distance indicator), and indicators for residing in different countries and from countries with different official languages. The two-part distance function is necessary because there is no direct way to measure the distance between scholars at the same institution. The elasticity of citation odds with respect to distance is constant after accounting for positive distance, a standard assumption in gravity equations. However, the inclusion of academic ties (e.g., co-authorship, past colocation) significantly reduces the estimated negative effects of geographic separation by approximately 50%, as ties are negatively correlated with distance. The pseudo-R2 nearly triples from 0.033 to 0.091 when ties are included, indicating a dramatic improvement in model fit. The effect of crossing a national border on citation odds is found to be smaller in academic citations compared to patent citations, with a reduction of 18% versus 41% in Singh and Marx (2013).",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-351",
    "question": "Based on the experimental design and results in Table A.1, which of the following statements about motives in the dictator game are statistically significant at the 95% confidence level or higher?",
    "options": [
      "A) Baseline sharing (B_i) is significant across all player categories",
      "B) Directed altruism (D_i) is significant for players who chose recipients based on need",
      "C) Reciprocity (R_i) is significant when revelation is important to players",
      "D) Sanctions (S_i) show higher values than directed altruism after controlling for order effects"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Motives-for-sharing-in-social-networks_2012_Journal-of-Development-Economics.md"
    ],
    "question_context": "The table presents four motives measured in the dictator game: baseline sharing (B_i), directed altruism (D_i), sanctions (S_i), and reciprocity (R_i). Key findings show that baseline sharing remains significant at 99% confidence (5.75*** with SE=0.19) across all categories. For players who consider revelation important, reciprocity becomes significant at 90% confidence (0.85* with SE=0.52). The text states: \"The value of sanctions still seems to be higher than the value of directed altruism\" even after controlling for order effects. The formula $B_{i}$ represents baseline sharing, with coefficients shown as 5.75*** (0.19) for all players.",
    "category": "INCORRECT",
    "question_context_used": "The table presents four motives measured in the dictator game: baseline sharing (B_i), directed altruism (D_i), sanctions (S_i), and reciprocity (R_i). Key findings show that baseline sharing remains significant at 99% confidence (5.75*** with SE=0.19) across all categories. For players who consider revelation important, reciprocity becomes significant at 90% confidence (0.85* with SE=0.52). The text states: \"The value of sanctions still seems to be higher than the value of directed altruism\" even after controlling for order effects. The formula $B_{i}$ represents baseline sharing, with coefficients shown as 5.75*** (0.19) for all players.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1938",
    "question": "Under global misspecification, which of the following statements are true regarding the properties of the Hellinger Distance (HD) and Exponentially Tilted Hellinger Distance (ETHD) estimators?",
    "options": [
      "A) HD is robust to global misspecification and converges at the standard rate of √n.",
      "B) ETHD is robust to global misspecification and has nonnegative implied probabilities.",
      "C) HD is not robust to global misspecification and does not converge at the standard rate of √n.",
      "D) ETHD is the only Cressie–Read estimator that is well-behaved under global misspecification with nonnegative implied probabilities."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Robust-estimation-with-exponentially-tilted-Hellinge_2021_Journal-of-Econome.md"
    ],
    "question_context": "Statistical models being simplifications of reality, the data generating process may be such that (1) does not actually have a solution in the parameter set Θ. This can actually be expected in settings where the model is overidentified in the sense that more restrictions than unknown parameters are available (i.e. m>p). This type of misspecification is referred to as global misspecification. Formally, (1) is globally misspecified if \n\n$$ E\\left(g(X,\\theta)\\right)\\neq0,\\quad\\forall\\theta\\in\\Theta. $$ \n\nUnder global misspecification, the notion of consistent estimator no longer makes much sense even though a particular estimator is expected to converge to a specific value in the parameter set, so-called pseudo-true value. Of course, in correctly specified models with mild identification conditions, pseudo-true values are the same for all consistent estimators with common limit being the solution of the model. In fact, asymptotic theory for estimators can be derived either assuming that the model is correctly specified or allowing for global misspecification. If the asymptotic distribution of an estimator derived under global misspecification corresponds – under correct specification – to its asymptotic distribution derived when assuming correct specification, this estimator is said to be robust to global misspecification. Such robustness is desirable because it allows for the possibility to carry out valid and reliable inference whether the model is correctly specified or not by using such misspecification-robust asymptotic distribution. Next, we establish that HD is not robust to global misspecification by showing that this is not √n -convergent.\n\nIt is worth mentioning that the lack of robustness of HD to global misspecification is not surprising. The intuition for such a lackluster performance follows from Schennach’s (2007, p.641) conjecture that connects the poor performance of estimators from the Cressie–Read family to the negative value of their indexing parameter; see also related conjectures in Smith (2007, p. 250) and Broniatowski and Keziou (2012, p. 2566). As recalled in (5), HD is associated with index a=−1/2. Actually, it is expected that power divergence estimators associated with negative Cressie–Read index have nonnegative implied probabilities, πi.s., but are not robust to global misspecification whereas those with positive index are robust to global misspecification but have implied probabilities that can be negative. It turns out that the only Cressie–Read estimator that is well-behaved under global misspecification with nonnegative implied probabilities is the ET estimator with index a=0.",
    "category": "INCORRECT",
    "question_context_used": "Statistical models being simplifications of reality, the data generating process may be such that (1) does not actually have a solution in the parameter set Θ. This can actually be expected in settings where the model is overidentified in the sense that more restrictions than unknown parameters are available (i.e. m>p). This type of misspecification is referred to as global misspecification. Formally, (1) is globally misspecified if \n\n$$ E\\left(g(X,\\theta)\\right)\\neq0,\\quad\\forall\\theta\\in\\Theta. $$ \n\nUnder global misspecification, the notion of consistent estimator no longer makes much sense even though a particular estimator is expected to converge to a specific value in the parameter set, so-called pseudo-true value. Of course, in correctly specified models with mild identification conditions, pseudo-true values are the same for all consistent estimators with common limit being the solution of the model. In fact, asymptotic theory for estimators can be derived either assuming that the model is correctly specified or allowing for global misspecification. If the asymptotic distribution of an estimator derived under global misspecification corresponds – under correct specification – to its asymptotic distribution derived when assuming correct specification, this estimator is said to be robust to global misspecification. Such robustness is desirable because it allows for the possibility to carry out valid and reliable inference whether the model is correctly specified or not by using such misspecification-robust asymptotic distribution. Next, we establish that HD is not robust to global misspecification by showing that this is not √n -convergent.\n\nIt is worth mentioning that the lack of robustness of HD to global misspecification is not surprising. The intuition for such a lackluster performance follows from Schennach’s (2007, p.641) conjecture that connects the poor performance of estimators from the Cressie–Read family to the negative value of their indexing parameter; see also related conjectures in Smith (2007, p. 250) and Broniatowski and Keziou (2012, p. 2566). As recalled in (5), HD is associated with index a=−1/2. Actually, it is expected that power divergence estimators associated with negative Cressie–Read index have nonnegative implied probabilities, πi.s., but are not robust to global misspecification whereas those with positive index are robust to global misspecification but have implied probabilities that can be negative. It turns out that the only Cressie–Read estimator that is well-behaved under global misspecification with nonnegative implied probabilities is the ET estimator with index a=0.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1431",
    "question": "In the context of the proof of Theorem 7.4, which of the following conditions are necessary for the existence of a RSME (Rational Expectations Stochastic Monetary Equilibrium) with $0 < \\bar{s} < 1$ and $0 < \\theta^{*} \\leq 1$?",
    "options": [
      "A) The function $\\phi(\\cdot, \\bar{s}, 1)$ is continuous and bounded below on $\\pmb{\\Delta}$.",
      "B) For any sequence $\\rho^{k}$ in $\\Delta$ converging to $\\widetilde{p} \\in \\mathbb{\\bar{\\Delta}} / \\Delta$, $\\parallel \\phi(p^{k}, \\bar{s}, 1) \\parallel$ tends to $+\\infty$.",
      "C) There exists $p^{*} \\in \\pmb{\\Delta}$ such that $\\phi(p^{*}, \\bar{s}, 1) \\leq 0$ and $p^{*} \\gg 0$.",
      "D) The sequence $q^{k}$ is bounded and converges to $q^{*} = \\phi(p^{*}, \\bar{s}, \\theta^{*})$."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "On Money and Banking12.md"
    ],
    "question_context": "The proof involves finding a price vector $p^{*}$ in the simplex $\\pmb{\\Delta}$ such that $0 = \\phi(p^{*}, \\bar{s}, 1)$. The function $\\phi(\\cdot, \\bar{s}, 1)$ is continuous and bounded below on $\\pmb{\\Delta}$. For every $\\rho$ in $\\Delta$, $p \\cdot \\phi(p, \\bar{s}, 1) = 0$. If $\\bar{s} = 0$, a trader never borrows. For any sequence $\\rho^{k}$ in $\\Delta$ converging to $\\widetilde{p} \\in \\mathbb{\\bar{\\Delta}} / \\Delta$, $\\parallel \\phi(p^{k}, \\bar{s}, 1) \\parallel$ tends to $+\\infty$. By a known theorem, there exists $p^{*}$ in $\\pmb{\\Delta}$ such that $\\phi(p^{*}, \\bar{s}, 1) \\leq 0$. Then $p^{*} \\gg 0$ and $p^{*} \\cdot \\phi(p^{*}, \\bar{s}, 1) = 0$ imply $\\mathbf{0} = \\phi(p^{*}, \\bar{s}, 1)$. For the case $0 < \\bar{s} < 1$, the proof uses a fixed point argument to show the existence of $p^{*}$ and $\\theta^{*}$ such that $\\mathbf{0} = \\phi(p^{*}, \\bar{s}, \\theta^{*})$.",
    "category": "INCORRECT",
    "question_context_used": "The proof involves finding a price vector $p^{*}$ in the simplex $\\pmb{\\Delta}$ such that $0 = \\phi(p^{*}, \\bar{s}, 1)$. The function $\\phi(\\cdot, \\bar{s}, 1)$ is continuous and bounded below on $\\pmb{\\Delta}$. For every $\\rho$ in $\\Delta$, $p \\cdot \\phi(p, \\bar{s}, 1) = 0$. If $\\bar{s} = 0$, a trader never borrows. For any sequence $\\rho^{k}$ in $\\Delta$ converging to $\\widetilde{p} \\in \\mathbb{\\bar{\\Delta}} / \\Delta$, $\\parallel \\phi(p^{k}, \\bar{s}, 1) \\parallel$ tends to $+\\infty$. By a known theorem, there exists $p^{*}$ in $\\pmb{\\Delta}$ such that $\\phi(p^{*}, \\bar{s}, 1) \\leq 0$. Then $p^{*} \\gg 0$ and $p^{*} \\cdot \\phi(p^{*}, \\bar{s}, 1) = 0$ imply $\\mathbf{0} = \\phi(p^{*}, \\bar{s}, 1)$. For the case $0 < \\bar{s} < 1$, the proof uses a fixed point argument to show the existence of $p^{*}$ and $\\theta^{*}$ such that $\\mathbf{0} = \\phi(p^{*}, \\bar{s}, \\theta^{*})$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2154",
    "question": "Which of the following statements accurately describe the concept of 'death externality' as discussed in the context of federal health-care and safety legislation?",
    "options": [
      "A) The death externality is negative for nonworking, highly consuming individuals, justifying minimum-quality consumption standards.",
      "B) Sumptuary taxes on dangerous commodities like liquor and cigarettes are designed to internalize the negative death externality.",
      "C) The death externality is positive for negatively productive (criminal) consumers, leading to life-disrespecting systems of capture and confinement.",
      "D) OSHA regulation optimally reduces workplace fatalities through precise and non-arbitrary safety standards."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Thompson-RationalizingObservedHealth-1995.md"
    ],
    "question_context": "The extensive mourning and insurance loss following the death of a useful member of our society should be taken as evidence of a significant loss to the society. The losing individuals, who typically live in different parts of a country, are the collective victims of a 'death externality.' Risking one's life for a high consumptive return typically imposes substantial external costs on a collective of insurers and potential mourners, whose only possible element of gain is the present value of the possible excess of the victim's life-contingent future consumption over the person's life-contingent future employment. This excess, minus the person's life-contingent future friendship services, formally represents the entire death externality. Although this total excludes life insurance, which merely transfers some of the external loss, life insurance is relevant, both for the above-employed empirical purposes and because it aggravates the useful individual's incentive to engage in overly risky activities.\n\nFor nonworking, sufficiently highly consuming individuals, the death externality loses its negativity. This rationalizes the above-described minimum-quality consumption standards. For such standards are quite irrelevant for luxury consumer goods. The high life-risk involved in the consumption of luxury foods (prime beef, rich desserts, etc.) and luxury durable goods (yachts, race cars, private airplanes) has thus been largely ignored by observed governments. In contrast, where ordinary working people persist in consuming significant quantities of dangerous commodities, high 'sumptuary' taxes (e.g., the per-unit taxes on liquor and cigarettes) have efficiently arisen to internalize significantly the predominantly negative death externality.",
    "category": "INCORRECT",
    "question_context_used": "The extensive mourning and insurance loss following the death of a useful member of our society should be taken as evidence of a significant loss to the society. The losing individuals, who typically live in different parts of a country, are the collective victims of a 'death externality.' Risking one's life for a high consumptive return typically imposes substantial external costs on a collective of insurers and potential mourners, whose only possible element of gain is the present value of the possible excess of the victim's life-contingent future consumption over the person's life-contingent future employment. This excess, minus the person's life-contingent future friendship services, formally represents the entire death externality. Although this total excludes life insurance, which merely transfers some of the external loss, life insurance is relevant, both for the above-employed empirical purposes and because it aggravates the useful individual's incentive to engage in overly risky activities.\n\nFor nonworking, sufficiently highly consuming individuals, the death externality loses its negativity. This rationalizes the above-described minimum-quality consumption standards. For such standards are quite irrelevant for luxury consumer goods. The high life-risk involved in the consumption of luxury foods (prime beef, rich desserts, etc.) and luxury durable goods (yachts, race cars, private airplanes) has thus been largely ignored by observed governments. In contrast, where ordinary working people persist in consuming significant quantities of dangerous commodities, high 'sumptuary' taxes (e.g., the per-unit taxes on liquor and cigarettes) have efficiently arisen to internalize significantly the predominantly negative death externality.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4066",
    "question": "In the context of the optimal contract characterized by Proposition 2.2, which of the following statements are correct regarding the principal's allocation strategy and the agent's behavior?",
    "options": [
      "A) The principal strictly prefers funding the project earlier due to the cumulative nature of the project's benefit.",
      "B) The optimal contract may include a side payment in the final stage if the agent's marginal utility of diversion is sufficiently high.",
      "C) The socially optimal contract involves funding the project in multiple stages to prevent diversion.",
      "D) Agents with high marginal utilities of diversion cause the principal to overinvest in the project, leading to bloated projects."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "The-optimal-sequencing-of-carrots_2010_Journal-of-Development-Economics.md"
    ],
    "question_context": "The principal's optimal contract involves a sequence of allocations up to some finite stage $j$, after which all allocations are zero. This is due to the principal's preference for earlier funding (cumulative benefit) and the point where marginal cost outweighs marginal benefit. The contract ensures the agent is indifferent between applying the allocation to the project and diversion in all stages $t<j$, with a possible side payment $s_j$ in the final stage. The allocation in each stage $t<j$ solves $\\biggl[\\sum_{n=t}^{\\infty}\\mathrm{{\\Large~\\delta~}}^{n-t}b(P_{n})+Z\\biggr]\\frac{1}{x}=p_{t}$. The socially optimal funding level $P^{\\circ}$ is where $\\frac{b^{'}(P^{\\circ})}{1-\\delta}=1$. Contracts funding the project in more than one stage result in overinvestment, exceeding $P^{\\circ}$, especially for agents with high marginal utilities of diversion, who may also necessitate a side payment in the final stage.",
    "category": "CORRECT",
    "question_context_used": "The principal's optimal contract involves a sequence of allocations up to some finite stage $j$, after which all allocations are zero. This is due to the principal's preference for earlier funding (cumulative benefit) and the point where marginal cost outweighs marginal benefit. The contract ensures the agent is indifferent between applying the allocation to the project and diversion in all stages $t<j$, with a possible side payment $s_j$ in the final stage. The allocation in each stage $t<j$ solves $\\biggl[\\sum_{n=t}^{\\infty}\\mathrm{{\\Large~\\delta~}}^{n-t}b(P_{n})+Z\\biggr]\\frac{1}{x}=p_{t}$. The socially optimal funding level $P^{\\circ}$ is where $\\frac{b^{'}(P^{\\circ})}{1-\\delta}=1$. Contracts funding the project in more than one stage result in overinvestment, exceeding $P^{\\circ}$, especially for agents with high marginal utilities of diversion, who may also necessitate a side payment in the final stage.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-300",
    "question": "In the context of the MAY model with Brownian motion and rational expectations, which of the following statements are true regarding the flow cost function $f(x)=k|x|^n$ and the optimal adjustment threshold $h$?",
    "options": [
      "A) For $n=1$, $h$ is proportional to $g^{1/3}$",
      "B) For $n=4$, $h$ is proportional to $g^{1/6}$",
      "C) The general conjecture suggests $h \\propto g^{1/(n+2)}$ for any positive integer $n$",
      "D) The dimension of $k$ must be adjusted for each $n$ to maintain dimensional consistency"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Analytical Approximations in Models of Hysteresis.md"
    ],
    "question_context": "The text examines the MAY model with flow cost $f(x)=k|x|^n$ under Brownian motion $dx=\\sigma dz$. For $n=1$, the optimal adjustment threshold is derived as $h=(6\\sigma^2g/k)^{1/3}$, showing $h \\propto g^{1/3}$. For $n=4$, the result is $h=(15\\sigma^2g/(2k))^{1/6}$, demonstrating $h \\propto g^{1/6}$. The author conjectures a general pattern where $h \\propto g^{1/(n+2)}$ for any positive integer $n$, though notes the calculations become increasingly tedious. Key equations include the differential equation $\\frac{1}{2}\\sigma^2V''(x)-\\rho V(x)+k|x|=0$ and the value matching conditions at the adjustment boundaries.",
    "category": "INCORRECT",
    "question_context_used": "The text examines the MAY model with flow cost $f(x)=k|x|^n$ under Brownian motion $dx=\\sigma dz$. For $n=1$, the optimal adjustment threshold is derived as $h=(6\\sigma^2g/k)^{1/3}$, showing $h \\propto g^{1/3}$. For $n=4$, the result is $h=(15\\sigma^2g/(2k))^{1/6}$, demonstrating $h \\propto g^{1/6}$. The author conjectures a general pattern where $h \\propto g^{1/(n+2)}$ for any positive integer $n$, though notes the calculations become increasingly tedious. Key equations include the differential equation $\\frac{1}{2}\\sigma^2V''(x)-\\rho V(x)+k|x|=0$ and the value matching conditions at the adjustment boundaries.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1312",
    "question": "In the context of short-run price equilibria with monopolistic competition, which of the following statements are true regarding the equilibrium prices and their determinants?",
    "options": [
      "A) Equilibrium prices are independent of the distribution of firms between regions when trade costs are negligible.",
      "B) The price charged by local firms in region H (p_HH*) increases with trade costs (τ) due to reduced foreign competition.",
      "C) Freight absorption under monopolistic competition always results in exactly half of the trade cost being passed to consumers.",
      "D) The equilibrium price gap between local and foreign markets (p_HF* - p_HH*) is strictly less than the trade cost (τ)."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Agglomeration and Trade Revisited.md"
    ],
    "question_context": "Key formula: The equilibrium prices are determined by the following expressions:\n\n$$\np_{H H}^{*}=\\frac{1}{2}\\frac{2a+\\tau c(1-\\lambda)N}{2b+c N}\n$$\n\n$$\np_{H F}^{*}=p_{F F}^{*}+\\frac{\\tau}{2}\n$$\n\n$$\np_{F H}^{*}=p_{H H}^{*}+\\frac{\\tau}{2}\n$$\n\nIn the model, firms maximize profits while neglecting their impact on regional price indices. The equilibrium prices depend on the demand and firm distributions between regions. When the mass of local firms increases, prices charged by both local and foreign firms fall due to fiercer competition, but this effect weakens as trade costs (τ) decrease. In the limit where τ is negligible, prices become independent of the firm distribution between regions.\n\nFreight absorption occurs, with only a fraction of the trade cost passed to consumers. The price gap between local and foreign markets is always less than the trade cost, as shown by:\n\n$$\np_{H F}^{*}-p_{H H}^{*}=\\tau\\frac{b+c\\lambda N}{2b+c N}<\\tau\n$$\n\nThis contrasts with monopoly, where exactly half the trade cost is absorbed. Under monopolistic competition, freight absorption varies with market size, leading to less (more) absorption when the foreign market is small (large).",
    "category": "CORRECT",
    "question_context_used": "Key formula: The equilibrium prices are determined by the following expressions:\n\n$$\np_{H H}^{*}=\\frac{1}{2}\\frac{2a+\\tau c(1-\\lambda)N}{2b+c N}\n$$\n\n$$\np_{H F}^{*}=p_{F F}^{*}+\\frac{\\tau}{2}\n$$\n\n$$\np_{F H}^{*}=p_{H H}^{*}+\\frac{\\tau}{2}\n$$\n\nIn the model, firms maximize profits while neglecting their impact on regional price indices. The equilibrium prices depend on the demand and firm distributions between regions. When the mass of local firms increases, prices charged by both local and foreign firms fall due to fiercer competition, but this effect weakens as trade costs (τ) decrease. In the limit where τ is negligible, prices become independent of the firm distribution between regions.\n\nFreight absorption occurs, with only a fraction of the trade cost passed to consumers. The price gap between local and foreign markets is always less than the trade cost, as shown by:\n\n$$\np_{H F}^{*}-p_{H H}^{*}=\\tau\\frac{b+c\\lambda N}{2b+c N}<\\tau\n$$\n\nThis contrasts with monopoly, where exactly half the trade cost is absorbed. Under monopolistic competition, freight absorption varies with market size, leading to less (more) absorption when the foreign market is small (large).",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4734",
    "question": "In the context of the revenue equations estimation, which of the following statements are correct regarding the econometric methodology and variable specifications?",
    "options": [
      "A) The revenue equations are estimated directly by two-stage least-squares due to the overidentification of the reduced form relationships.",
      "B) Personal and corporate taxes are specified to have a negative relationship with income less the tax base, as per the budget constraint logic.",
      "C) Population is included as an independent variable in all revenue equations to account for its influence on consumption and excise tax revenues.",
      "D) The coefficients with subscripts one and two in the revenue equations are expected to be negative, while those with subscripts three and five are expected to be positive."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "State and Local Governments and Their Budget Constraint.md"
    ],
    "question_context": "The revenue equations in (3) are overidentified, leading to direct estimation by two-stage least-squares. The expenditure predictions from the previous section serve as instrumental variables in the second stage. Personal and corporate taxes are specified to relate more closely to specific portions of income in the legal tax base than to the overall total, with a negative relationship to income less the tax base due to budget constraint logic. The equations for personal and indirect taxes are expressed in per capita terms to account for population's influence beyond expenditures. The expected signs of the coefficients are specified as follows: those with subscripts one and two are positive, those with subscripts three and five are negative, and those with subscript four are ambiguous. The regression equations for tax revenues are given by:\n\n$$\n\\begin{array}{l}{{{\\displaystyle{\\frac{\\mathrm{Pers.}\\mathrm{Tx.}}{N}}=c_{0}+c_{1}{\\frac{T P Y}{N}}+c_{2}{\\frac{(\\hat{E}+\\hat{C}-G)}{N}}+c_{3}{\\frac{\\hat{C}}{\\hat{E}+\\hat{C}}}+c_{4}r+c_{5}{\\frac{Y-T P Y}{N}}}}}\n{{~}}\n{{{\\displaystyle{\\frac{\\mathrm{Ind.}\\mathrm{Tx.}}{N}}}=d_{0}+d_{1}{\\frac{Y}{N}}+d_{2}{\\frac{(\\hat{E}+\\hat{C}-G)}{N}}+d_{3}{\\frac{\\hat{C}}{\\hat{E}+\\hat{C}}}+d_{4}r}}\n{{~}}\n{{\\mathrm{Corp.}\\mathrm{Tx.}=e_{0}+e_{1}C P+e_{2}(\\hat{E}+\\hat{C}+G)+e_{3}{\\frac{\\hat{C}}{\\hat{E}+\\hat{C}}}+e_{4}r+e_{8}(Y-C P)}}\n{{~}}\n{{\\mathrm{Surp.}\\mathrm{G.E.}=f_{0}+f_{1}Y+f_{2}(\\hat{E}+\\hat{C}-G)+f_{3}{\\frac{\\hat{C}}{\\hat{E}+\\hat{C}}}+f_{4}r.}}\\end{array}\n$$",
    "category": "CORRECT",
    "question_context_used": "The revenue equations in (3) are overidentified, leading to direct estimation by two-stage least-squares. The expenditure predictions from the previous section serve as instrumental variables in the second stage. Personal and corporate taxes are specified to relate more closely to specific portions of income in the legal tax base than to the overall total, with a negative relationship to income less the tax base due to budget constraint logic. The equations for personal and indirect taxes are expressed in per capita terms to account for population's influence beyond expenditures. The expected signs of the coefficients are specified as follows: those with subscripts one and two are positive, those with subscripts three and five are negative, and those with subscript four are ambiguous. The regression equations for tax revenues are given by:\n\n$$\n\\begin{array}{l}{{{\\displaystyle{\\frac{\\mathrm{Pers.}\\mathrm{Tx.}}{N}}=c_{0}+c_{1}{\\frac{T P Y}{N}}+c_{2}{\\frac{(\\hat{E}+\\hat{C}-G)}{N}}+c_{3}{\\frac{\\hat{C}}{\\hat{E}+\\hat{C}}}+c_{4}r+c_{5}{\\frac{Y-T P Y}{N}}}}}\n{{~}}\n{{{\\displaystyle{\\frac{\\mathrm{Ind.}\\mathrm{Tx.}}{N}}}=d_{0}+d_{1}{\\frac{Y}{N}}+d_{2}{\\frac{(\\hat{E}+\\hat{C}-G)}{N}}+d_{3}{\\frac{\\hat{C}}{\\hat{E}+\\hat{C}}}+d_{4}r}}\n{{~}}\n{{\\mathrm{Corp.}\\mathrm{Tx.}=e_{0}+e_{1}C P+e_{2}(\\hat{E}+\\hat{C}+G)+e_{3}{\\frac{\\hat{C}}{\\hat{E}+\\hat{C}}}+e_{4}r+e_{8}(Y-C P)}}\n{{~}}\n{{\\mathrm{Surp.}\\mathrm{G.E.}=f_{0}+f_{1}Y+f_{2}(\\hat{E}+\\hat{C}-G)+f_{3}{\\frac{\\hat{C}}{\\hat{E}+\\hat{C}}}+f_{4}r.}}\\end{array}\n$$",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3547",
    "question": "Based on the small-sample properties of rank tests for cointegration, which of the following statements are supported by the simulation results?",
    "options": [
      "A) Rank tests outperform Dickey-Fuller tests when β = 1 and α is close to 1.",
      "B) The power of rank tests is invariant to the functional form of f(x_t), including nonlinear cases like f(x) = log(x).",
      "C) Dickey-Fuller tests are more powerful than rank tests for all values of β.",
      "D) Rank tests require estimation of the cointegration parameter β to maintain their power properties."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "06-Rank_Tests_for_Nonlinear_Cointegration.md"
    ],
    "question_context": "The simulation results investigate the small-sample properties of rank tests for cointegration, comparing them with Dickey-Fuller tests. The model equations are given by:\n\n$$\n\\begin{array}{r l}{y_{t}=\\beta f(x_{t})+u_{t},}&{{}\\quad u_{t}=\\alpha u_{t-1}+\\varepsilon_{t}}\\\\ {f(x_{t})=f(x_{t-1})+v_{t},}\\end{array}\n$$\n\nwhere \n\n$$\n\\left[\\varepsilon_{t}\\right]\\sim\\mathrm{~iid~N}\\left(\\mathbf{0},\\left[\\begin{array}{c}{1\\rho}\\\\ {\\rho1}\\end{array}\\right]\\right).\n$$\n\nKey findings include that rank tests are more powerful than Dickey-Fuller tests when β = 1 and α is close to 1, and their performance is robust to nonlinear transformations of the variables, such as f(x) = log(x). The rank tests do not require estimating the cointegration parameter β, maintaining their power properties without this estimation.",
    "category": "CORRECT",
    "question_context_used": "The simulation results investigate the small-sample properties of rank tests for cointegration, comparing them with Dickey-Fuller tests. The model equations are given by:\n\n$$\n\\begin{array}{r l}{y_{t}=\\beta f(x_{t})+u_{t},}&{{}\\quad u_{t}=\\alpha u_{t-1}+\\varepsilon_{t}}\\\\ {f(x_{t})=f(x_{t-1})+v_{t},}\\end{array}\n$$\n\nwhere \n\n$$\n\\left[\\varepsilon_{t}\\right]\\sim\\mathrm{~iid~N}\\left(\\mathbf{0},\\left[\\begin{array}{c}{1\\rho}\\\\ {\\rho1}\\end{array}\\right]\\right).\n$$\n\nKey findings include that rank tests are more powerful than Dickey-Fuller tests when β = 1 and α is close to 1, and their performance is robust to nonlinear transformations of the variables, such as f(x) = log(x). The rank tests do not require estimating the cointegration parameter β, maintaining their power properties without this estimation.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-61",
    "question": "In the dynamic model of adverse selection in durable goods markets by Hendel and Lizzeri, which of the following statements are correct regarding equilibrium outcomes?",
    "options": [
      "A) The used market never shuts down due to endogenous ownership patterns and quality depreciation.",
      "B) New car prices are always lower under adverse selection compared to a market with no quality uncertainty.",
      "C) The volume of trade in the used market can be substantial even under severe information asymmetries.",
      "D) Unreliable brands exhibit steeper price declines and lower volumes of trade in the secondary market compared to reliable brands."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Hendel-AdverseSelectionDurable-1999.md"
    ],
    "question_context": "The paper examines a dynamic model of adverse selection in durable goods markets, focusing on interactions between new and used goods markets. Key findings include that the used market never shuts down, trade volumes can be large despite information asymmetries, and distortions are less severe than in static models. The model incorporates quality depreciation ($w_h \\lesssim \\nu$) and endogenous ownership decisions. Formula for equilibrium prices under no asymmetric information: $P_d^n = \\theta_y(\\nu - w) + (1+\\delta)\\theta_{2y}w$. The extension to multiple brands shows unreliable brands have steeper price declines and lower trade volumes, contrasting with physical depreciation effects.",
    "category": "CORRECT",
    "question_context_used": "The paper examines a dynamic model of adverse selection in durable goods markets, focusing on interactions between new and used goods markets. Key findings include that the used market never shuts down, trade volumes can be large despite information asymmetries, and distortions are less severe than in static models. The model incorporates quality depreciation ($w_h \\lesssim \\nu$) and endogenous ownership decisions. Formula for equilibrium prices under no asymmetric information: $P_d^n = \\theta_y(\\nu - w) + (1+\\delta)\\theta_{2y}w$. The extension to multiple brands shows unreliable brands have steeper price declines and lower trade volumes, contrasting with physical depreciation effects.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-688",
    "question": "In the context of estimating the elasticity of corporate taxable income with respect to the statutory tax rate, which of the following statements are supported by the empirical findings in the UK tax records study?",
    "options": [
      "A) The elasticity of corporate taxable income is higher around the £10,000 kink than around the £300,000 kink.",
      "B) The marginal deadweight cost of the tax around the £10,000 kink is approximately 29% of the resulting tax revenue.",
      "C) The elasticity of total income with respect to the net of tax rate ranges between 0.20 and 0.31.",
      "D) The elasticity of the share of income taken as profit with respect to the difference between personal and corporate tax rates is negligible (close to zero)."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Devereux-ElasticityCorporateTaxable-2014.md"
    ],
    "question_context": "The study estimates the elasticity of corporate taxable income with respect to the statutory tax rate in the UK using confidential tax return data. Key findings include an elasticity of between 0.13 and 0.17 for companies around the £300,000 kink, implying a marginal deadweight cost of 6% of marginal tax revenue, and a much higher elasticity of between 0.53 and 0.56 for companies around the £10,000 kink. The elasticity is decomposed into two components: an elasticity of total income with respect to the net of tax rate (0.20 to 0.31) and an elasticity of the share of income taken as profit with respect to the difference between personal and corporate tax rates (0.05 to 0.08). The marginal deadweight cost around the £10,000 kink is estimated at approximately 29% of tax revenue. The study also highlights that companies rarely follow pure tax minimization strategies, possibly due to salience problems or liquidity costs associated with declaring income as corporate profit.",
    "category": "CORRECT",
    "question_context_used": "The study estimates the elasticity of corporate taxable income with respect to the statutory tax rate in the UK using confidential tax return data. Key findings include an elasticity of between 0.13 and 0.17 for companies around the £300,000 kink, implying a marginal deadweight cost of 6% of marginal tax revenue, and a much higher elasticity of between 0.53 and 0.56 for companies around the £10,000 kink. The elasticity is decomposed into two components: an elasticity of total income with respect to the net of tax rate (0.20 to 0.31) and an elasticity of the share of income taken as profit with respect to the difference between personal and corporate tax rates (0.05 to 0.08). The marginal deadweight cost around the £10,000 kink is estimated at approximately 29% of tax revenue. The study also highlights that companies rarely follow pure tax minimization strategies, possibly due to salience problems or liquidity costs associated with declaring income as corporate profit.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1049",
    "question": "In the context of the quantitative analysis of crime and financial frictions in Brazil and Peru, which of the following statements are correct based on the calibrated model results?",
    "options": [
      "A) Brazil's model under-predicts the probability of facing crime and aggregate crime relative to output.",
      "B) Peru's calibrated value of Φ is lower than Colombia's, reflecting its lower external finance to capital ratio.",
      "C) Adding crime to a model with financial frictions in Brazil leads to a greater reduction in output than in Peru.",
      "D) The external finance to capital ratio in Peru is higher than in Brazil according to the model calibration."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Financial-frictions-and-the-rule-of-law_2018_Journal-of-Development-Economic.md"
    ],
    "question_context": "The study calibrates the model to Brazil and Peru economies to evaluate sensitivity. Brazil has a similar external finance to capital ratio as Colombia but higher probability of crime; Peru has a similar probability of crime as Colombia but a lower external finance to capital ratio. Key formula: The calibrated parameters imply a lower value for $\\phi$ in Peru, relative to Colombia, and a lower value of $\\lambda$ in Brazil (though the model under-predicts the probability of facing crime and aggregate crime relative to output in Brazil). For Brazil, adding crime to a model with financial frictions generates an additional 50, 42 and 84 percent reduction in output, TFP and consumption; in Peru these values are 30, 29 and 50 percent. Table A.6 reports calibration parameters, showing Brazil's external finance to capital ratio as 0.33 (data) vs. 0.34 (model), and Peru's as 0.13 (data) vs. 0.15 (model).",
    "category": "CORRECT",
    "question_context_used": "The study calibrates the model to Brazil and Peru economies to evaluate sensitivity. Brazil has a similar external finance to capital ratio as Colombia but higher probability of crime; Peru has a similar probability of crime as Colombia but a lower external finance to capital ratio. Key formula: The calibrated parameters imply a lower value for $\\phi$ in Peru, relative to Colombia, and a lower value of $\\lambda$ in Brazil (though the model under-predicts the probability of facing crime and aggregate crime relative to output in Brazil). For Brazil, adding crime to a model with financial frictions generates an additional 50, 42 and 84 percent reduction in output, TFP and consumption; in Peru these values are 30, 29 and 50 percent. Table A.6 reports calibration parameters, showing Brazil's external finance to capital ratio as 0.33 (data) vs. 0.34 (model), and Peru's as 0.13 (data) vs. 0.15 (model).",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2596",
    "question": "In the context of bubbly equilibrium with credit constraints, which of the following statements are correct regarding the role and characteristics of bubbles?",
    "options": [
      "A) Bubbles relax credit constraints and enable higher investment in productive firms.",
      "B) The growth rate of bubbles on intrinsically useless assets equals the interest rate.",
      "C) Tobin's marginal Q equals the average Q due to constant returns to scale technology.",
      "D) Bubbles generate dividends only when the firm remains productive in the next period."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Miao-BubblesTotalFactor-2012.md"
    ],
    "question_context": "The bubbly equilibrium is characterized by the presence of bubbles ($b_{i t}>0$) in stock prices, which relax credit constraints and allow firms to increase investment when they are more productive. This leads to capital accumulation, higher profits, and more dividends, justifying the initial bubble beliefs. The equilibrium involves a system of equations for variables such as $Q_{1t}, Q_{2t}, K_{1t}, K_{2t}, B_{1t},$ and $B_{2t}$. A key feature is that Tobin's marginal $Q$ is not equal to the average $Q$ despite constant returns to scale technology. The return on bubbles includes capital gains and dividend yields, with the latter arising only when the firm remains productive. Bubbles on intrinsically useless assets, however, grow at the interest rate since they do not generate dividends.",
    "category": "CORRECT",
    "question_context_used": "The bubbly equilibrium is characterized by the presence of bubbles ($b_{i t}>0$) in stock prices, which relax credit constraints and allow firms to increase investment when they are more productive. This leads to capital accumulation, higher profits, and more dividends, justifying the initial bubble beliefs. The equilibrium involves a system of equations for variables such as $Q_{1t}, Q_{2t}, K_{1t}, K_{2t}, B_{1t},$ and $B_{2t}$. A key feature is that Tobin's marginal $Q$ is not equal to the average $Q$ despite constant returns to scale technology. The return on bubbles includes capital gains and dividend yields, with the latter arising only when the firm remains productive. Bubbles on intrinsically useless assets, however, grow at the interest rate since they do not generate dividends.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2152",
    "question": "According to Thompson's Pascalian approach to health and safety legislation, which of the following are valid critiques or unresolved issues in the U.S. health-care system?",
    "options": [
      "A) The persistence of unaddressed transaction-cost-elimination arguments for universal governmental provision",
      "B) The absence of state-subsidized immunization programs for communicable diseases",
      "C) The lack of a welfare-economic rationale for federal (as opposed to state) intervention",
      "D) The failure of local monopolistic hospitals to adopt socially optimal nonprofit forms"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Thompson-RationalizingObservedHealth-1995.md"
    ],
    "question_context": "The paper identifies several perplexities in the U.S. health-care system. First, the transaction-cost-elimination argument for universal governmental provision (the 'single-payer plan') has never been addressed in the United States, unlike in other countries with socialized health-care systems. Second, existing arguments for governmental health-care intervention do not justify federal over state intervention, creating a gap in the welfare-economic rationale for federal legislation. The author addresses this by proposing a new rationale for federal health-care and safety legislation in Section II. Key formula: The welfare-economic rationale must bridge the gap between state and federal intervention levels to justify existing legislation.",
    "category": "CORRECT",
    "question_context_used": "The paper identifies several perplexities in the U.S. health-care system. First, the transaction-cost-elimination argument for universal governmental provision (the 'single-payer plan') has never been addressed in the United States, unlike in other countries with socialized health-care systems. Second, existing arguments for governmental health-care intervention do not justify federal over state intervention, creating a gap in the welfare-economic rationale for federal legislation. The author addresses this by proposing a new rationale for federal health-care and safety legislation in Section II. Key formula: The welfare-economic rationale must bridge the gap between state and federal intervention levels to justify existing legislation.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2699",
    "question": "Under the assumptions of the asymptotic properties framework, which of the following conditions are necessary for the k-step bootstrap bias correction to achieve effective bias reduction?",
    "options": [
      "A) The parameter space Γ is compact and the log-likelihood function is continuous in γ_i ∈ Γ",
      "B) The strong mixing coefficients satisfy sup sup α_i(γ_i0, m) ≤ C_1 exp(−C_2 m) for some constants C_1, C_2 > 0",
      "C) The number of bootstrap steps k must be at least 2 (k ≥ 2)",
      "D) The moment condition E_{γ_i0}{[M(Z_it(γ_i0))]^Q} < ∞ holds for Q > d_θ + 12"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Kim和Sun - 2016 - BOOTSTRAP AND k-STEP BOOTSTRAP BIAS CORRECTIONS FOR THE FIXED EFFECTS ESTIMATOR IN NONLINEAR PANEL D.md"
    ],
    "question_context": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. Following HN (2004), we define u_{it}(γ_i) ≡ ∂/∂θ l(θ, α_i; Z_it) and υ_{it}(γ_i) ≡ ∂/∂α_i l(θ, α_i; Z_it). Let additional subscripts denote partial derivatives, e.g., υ_{itα_i}(γ_i) ≡ ∂²/∂α_i² l(θ, α_i; Z_it). Let U_{it}(γ_i) = u_{it}(γ_i) − ρ_{i0}υ_{it}(γ_i), where ρ_{i0} = E[∑_{t=1}^T u_{itα_i}(γ_i0)] / E[∑_{t=1}^T υ_{itα_i}(γ_i0)].\n\nAssumption 1 ensures that the maximization problem is well defined. The mixing and moment conditions in Assumptions 2 and 3 ensure that the ULLN and/or CLT hold for l(γ_i; Z_it) and its derivatives. Assumption 3(iii) maintains that M(Z_it(γ_i0)) has enough moments. Assumption 4 is the identification assumption for extremum estimators. Assumption 5 specifies the asymptotic sequence we consider. Theorem 4 states that for all k ≥ 2, the k-step bootstrap method achieves effective bias reduction, whereas for k = 1, the difference between the two bias estimators is large enough to affect the asymptotic distribution.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. Following HN (2004), we define u_{it}(γ_i) ≡ ∂/∂θ l(θ, α_i; Z_it) and υ_{it}(γ_i) ≡ ∂/∂α_i l(θ, α_i; Z_it). Let additional subscripts denote partial derivatives, e.g., υ_{itα_i}(γ_i) ≡ ∂²/∂α_i² l(θ, α_i; Z_it). Let U_{it}(γ_i) = u_{it}(γ_i) − ρ_{i0}υ_{it}(γ_i), where ρ_{i0} = E[∑_{t=1}^T u_{itα_i}(γ_i0)] / E[∑_{t=1}^T υ_{itα_i}(γ_i0)].\n\nAssumption 1 ensures that the maximization problem is well defined. The mixing and moment conditions in Assumptions 2 and 3 ensure that the ULLN and/or CLT hold for l(γ_i; Z_it) and its derivatives. Assumption 3(iii) maintains that M(Z_it(γ_i0)) has enough moments. Assumption 4 is the identification assumption for extremum estimators. Assumption 5 specifies the asymptotic sequence we consider. Theorem 4 states that for all k ≥ 2, the k-step bootstrap method achieves effective bias reduction, whereas for k = 1, the difference between the two bias estimators is large enough to affect the asymptotic distribution.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3818",
    "question": "In the context of the employment equation specification, which of the following statements are correct regarding the measurement of capital services and the model constraints?",
    "options": [
      "A) The flow of capital services can be measured by adjusting the capital stock series with the capacity utilization index.",
      "B) The unconstrained model explicitly includes disembodied technical change as a determinant of employment.",
      "C) The perpetual inventory method requires knowledge of the initial and terminal values of capital stock to derive the replacement rate.",
      "D) The constrained model assumes a linear relationship between capital services and time, capital stock, and capacity utilization."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Effects of Relative Prices and Capacity on the Demand for Labour in the U.S. Manufacturing Sector1.md"
    ],
    "question_context": "The capital stock at the beginning of each period is calculated by the perpetual inventory method using the formula\n\n$$\nK_{t+1}=I_{t}+(1-\\delta)K_{t},\n$$\n\nwhere $K_{t+1}$ is the capital stock at the beginning of period $t+1$, $I$ is the gross investment in constant prices in period $t$, and $\\delta$ is the rate of replacement. $\\delta$ is derived by interpolating between the initial and terminal values of capital stock at the beginning of years 1948 and 1960. The flow of capital services is measured by simply adjusting the capital stock series, $K_{t}$, by the index of capacity utilization, $m_{t}$.\n\nAn alternative way of measuring the flow of capital services is to assume that $K_{t}^{m}$ is a linear function of three variables, $K_{t},m_{t}$ and $t$. That is,\n\n$$\nK_{t}^{m}=c_{0}+c_{1}t+c_{2}K_{t}+c_{3}m_{t}.\n$$\n\nThis measure of $K_{t}^{m}$ explicitly allows for disembodied technical change as a determinant of employment. Depending on which measure of capital services we use, we can develop the constrained and unconstrained form of our basic model.",
    "category": "INCORRECT",
    "question_context_used": "The capital stock at the beginning of each period is calculated by the perpetual inventory method using the formula\n\n$$\nK_{t+1}=I_{t}+(1-\\delta)K_{t},\n$$\n\nwhere $K_{t+1}$ is the capital stock at the beginning of period $t+1$, $I$ is the gross investment in constant prices in period $t$, and $\\delta$ is the rate of replacement. $\\delta$ is derived by interpolating between the initial and terminal values of capital stock at the beginning of years 1948 and 1960. The flow of capital services is measured by simply adjusting the capital stock series, $K_{t}$, by the index of capacity utilization, $m_{t}$.\n\nAn alternative way of measuring the flow of capital services is to assume that $K_{t}^{m}$ is a linear function of three variables, $K_{t},m_{t}$ and $t$. That is,\n\n$$\nK_{t}^{m}=c_{0}+c_{1}t+c_{2}K_{t}+c_{3}m_{t}.\n$$\n\nThis measure of $K_{t}^{m}$ explicitly allows for disembodied technical change as a determinant of employment. Depending on which measure of capital services we use, we can develop the constrained and unconstrained form of our basic model.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2608",
    "question": "Under multiplicative mean independence (Assumption 2) and response error mixture model (Assumption 3), which of the following conditions are necessary for deriving sharp bounds on the expected outcome E(W)?",
    "options": [
      "A) The fraction of observations drawn from the distribution of interest z is known",
      "B) The outcome variable W has bounded support [k0, k1]",
      "C) The accurate reporting rate z satisfies the constraints in Lemma 1",
      "D) The parameter γ is known or bounded"
    ],
    "gold_answer": "C, D",
    "source_files": [
      "04-Identification of Expected Outcomes in a Data Error Mixing Model With Multiplicative Mean Inde.md"
    ],
    "question_context": "The multiplicative mean independence assumption generalizes the mean independence restriction by allowing conditional means to differ by a factor of proportionality: \n\n$$ E(W|Z=0)=\\gamma E(W|Z=1) $$\n\nfor some known or bounded value of γ∈[0,∞). Under the response error mixture model (Assumption 3), all draws from the alternative distribution are known to be erroneous: \n\n$$ P(W=X|Z=0)=0. $$\n\nThis provides a link between F and G that is informative for discrete outcome distributions. Lemma 1 establishes critical restrictions on the accurate reporting rate z based on γ and p, which are necessary for deriving identification regions when combining Assumptions 1 through 3.",
    "category": "INCORRECT",
    "question_context_used": "The multiplicative mean independence assumption generalizes the mean independence restriction by allowing conditional means to differ by a factor of proportionality: \n\n$$ E(W|Z=0)=\\gamma E(W|Z=1) $$\n\nfor some known or bounded value of γ∈[0,∞). Under the response error mixture model (Assumption 3), all draws from the alternative distribution are known to be erroneous: \n\n$$ P(W=X|Z=0)=0. $$\n\nThis provides a link between F and G that is informative for discrete outcome distributions. Lemma 1 establishes critical restrictions on the accurate reporting rate z based on γ and p, which are necessary for deriving identification regions when combining Assumptions 1 through 3.",
    "gold_answer_parsed": [
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2531",
    "question": "In the context of the linearised rational expectations model with floating exchange rates, which of the following conditions are necessary for fiscal federalism to have a neutral impact on output?",
    "options": [
      "A) The elasticity of output with respect to the real product wage is zero",
      "B) There is no consumption bias towards domestic output (γ₁ = γ₂)",
      "C) The Ricardian equivalence holds (η = 0)",
      "D) The degree of contract indexing is complete (κ = 1)"
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Choice_of_‘Conservative’_Bankers_in_Open_Economies_Monetary_Regime_Options_for_Europe.md"
    ],
    "question_context": "The model linearisation around a deterministic balanced growth rational expectations steady state yields key relationships for output and the real exchange rate. Under floating exchange rates, the rational expectations solution involves complex interactions between domestic and foreign variables, with spillover effects depending on parameters like ξ, which combines wage elasticity, contract indexing, and spillover size. Fiscal federalism's impact on output neutrality is explicitly stated: 'fiscal federalism is neutral in its impact on output γ₁=γ₂ (no consumption bias towards domestic output) or η=0 (the Ricardian case).' The parameter η captures non-Ricardian effects, while γ₁ and γ₂ represent consumption preferences for domestic versus foreign goods.",
    "category": "CORRECT",
    "question_context_used": "The model linearisation around a deterministic balanced growth rational expectations steady state yields key relationships for output and the real exchange rate. Under floating exchange rates, the rational expectations solution involves complex interactions between domestic and foreign variables, with spillover effects depending on parameters like ξ, which combines wage elasticity, contract indexing, and spillover size. Fiscal federalism's impact on output neutrality is explicitly stated: 'fiscal federalism is neutral in its impact on output γ₁=γ₂ (no consumption bias towards domestic output) or η=0 (the Ricardian case).' The parameter η captures non-Ricardian effects, while γ₁ and γ₂ represent consumption preferences for domestic versus foreign goods.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2257",
    "question": "In the context of the robust investor portfolio problem, which of the following statements are correct regarding the equilibrium prices and distortions?",
    "options": [
      "A) The drift distortion η∗ from the robust planner’s problem determines competitive equilibrium prices for model uncertainty.",
      "B) The shock exposure vectors for log A and log C must coincide, leading to f∗ = (.01)σc x.",
      "C) The equilibrium risk price ω∗(z) is given by ω∗(z) = (.01)σc + η∗(z).",
      "D) The equilibrium risk-free rate ι∗(z) includes a term accounting for the worst-case drift η∗(z)."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Macroeconomic-uncertainty-prices-when-beliefs-are-_2021_Journal-of-Econometr.md"
    ],
    "question_context": "Key formula: The equilibrium risk price is given by ω∗(z) = (.01)σc − η∗(z), where η∗ is the worst-case drift from the robust planning problem. The shock exposure vectors for log A and log C must coincide, implying f∗ = (.01)σc x. The equilibrium risk-free rate ι∗(z) is derived as ι∗(z) = δ + .01(α̂c + β̂c z) + .01σy · η∗(z) − .0001/2 σc · σc. These formulas collectively establish the link between the robust planner’s problem and the competitive equilibrium prices, ensuring consistency in the treatment of model uncertainty and risk.",
    "category": "CORRECT",
    "question_context_used": "Key formula: The equilibrium risk price is given by ω∗(z) = (.01)σc − η∗(z), where η∗ is the worst-case drift from the robust planning problem. The shock exposure vectors for log A and log C must coincide, implying f∗ = (.01)σc x. The equilibrium risk-free rate ι∗(z) is derived as ι∗(z) = δ + .01(α̂c + β̂c z) + .01σy · η∗(z) − .0001/2 σc · σc. These formulas collectively establish the link between the robust planner’s problem and the competitive equilibrium prices, ensuring consistency in the treatment of model uncertainty and risk.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-980",
    "question": "According to the theoretical framework and empirical evidence presented, which of the following outcomes are directly influenced by male-biased sex ratios through the mechanism of male-male competition?",
    "options": [
      "A) Increased female bargaining power in household decision-making",
      "B) Heightened aggression and violent behavior among males",
      "C) Greater occupational gender segregation in male-stereotypical jobs",
      "D) Reduced support for same-sex marriage due to masculinity norms"
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Baranov 等 - 2023 - Men. Male-biased sex ratios and masculinity norms evidence from Australia's colonial past.md"
    ],
    "question_context": "The sex ratio, the number of males relative to females, is a central concept in evolutionary biology. The idea that behavioral differences between the sexes originate in the conditions of reproductive competition, among which the sex ratio plays a primordial role, is the cornerstone of Darwin’s The Descent of Man (1871). When the sex ratio is more male biased, competition between males is more intense. Across a wide range of taxa, strong male-male competition induces risk taking, violence and control, oftentimes exerted through violent means, over the reproductive opportunities of dominated males as well as females (Emlen & Oring, 1977; Buss, 2016). Experimental studies of lizards, birds, and primates find that male-biased sex ratios increase male aggression towards males as well as females (Sapolsky, 1990, 1991). In human populations, skewed sex ratios have likewise been shown to correlate with rape and other violent crime. Based on the CMNI framework, we expect that areas that were historically characterized by male-biased sex ratios and, therefore, intense male-male competition, developed stricter masculinity norms that continue to manifest themselves across four broad domains: (i) violence and bullying; (ii) risk taking, help avoidance and unhealthy behavior; (iii) male-stereotypical occupational segregation; and (iv) negative attitudes towards homosexuals.",
    "category": "INCORRECT",
    "question_context_used": "The sex ratio, the number of males relative to females, is a central concept in evolutionary biology. The idea that behavioral differences between the sexes originate in the conditions of reproductive competition, among which the sex ratio plays a primordial role, is the cornerstone of Darwin’s The Descent of Man (1871). When the sex ratio is more male biased, competition between males is more intense. Across a wide range of taxa, strong male-male competition induces risk taking, violence and control, oftentimes exerted through violent means, over the reproductive opportunities of dominated males as well as females (Emlen & Oring, 1977; Buss, 2016). Experimental studies of lizards, birds, and primates find that male-biased sex ratios increase male aggression towards males as well as females (Sapolsky, 1990, 1991). In human populations, skewed sex ratios have likewise been shown to correlate with rape and other violent crime. Based on the CMNI framework, we expect that areas that were historically characterized by male-biased sex ratios and, therefore, intense male-male competition, developed stricter masculinity norms that continue to manifest themselves across four broad domains: (i) violence and bullying; (ii) risk taking, help avoidance and unhealthy behavior; (iii) male-stereotypical occupational segregation; and (iv) negative attitudes towards homosexuals.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2602",
    "question": "In the context of Dye evidence and simple type dependence, which of the following statements are true about the equilibrium outcomes and optimal mechanisms?",
    "options": [
      "A) The principal's optimal mechanism outcome is always deterministic when $v_{i}^{+} > v_{i}^{-}$.",
      "B) The equilibrium outcome $O(t)(a) = \\hat{p}(a \\mid \\hat{v}(t))$ holds only when $v_{i}^{+} \\leq v_{i}^{-}$.",
      "C) The principal's belief about $v_{i}(t_{i})$ is updated based on whether the agent presents evidence or not.",
      "D) The optimal mechanism can be characterized using auxiliary games where agents' strategies depend on their type's evidence feasibility."
    ],
    "gold_answer": "C, D",
    "source_files": [
      "BenPorath-MECHANISMSEVIDENCECOMMITMENT-2019.md"
    ],
    "question_context": "Key formula: The principal's belief about $v_{i}(t_{i})$ is given by:\n\n$$\n\\hat{v}_{i}(t_{i}) = \\begin{cases}\nv_{i}^{+} & \\text{if } t_{i} \\in T_{i}^{0} \\cap T_{i}^{+} \\text{ or } t_{i} \\in T_{i}^{+} \\setminus T_{i}^{0} \\text{ and } v_{i}(t_{i}) \\leq v_{i}^{+}, \\\\\nv_{i}^{-} & \\text{if } t_{i} \\in T_{i}^{0} \\cap T_{i}^{-} \\text{ or } t_{i} \\in T_{i}^{-} \\setminus T_{i}^{0} \\text{ and } v_{i}(t_{i}) \\geq v_{i}^{-}, \\\\\nv_{i}(t_{i}), & \\text{otherwise.}\n\\end{cases}\n$$\n\nIn the game without commitment, the principal forms beliefs based on the agents’ equilibrium strategies and chooses a best action conditional on these beliefs. The surprising result is that the equilibrium and optimal mechanism constructed have the property that $\\boldsymbol{a}^{*}(t) = \\hat{\\boldsymbol{a}}(t)$ for every profile $t$. This implies that commitment is not needed to achieve the optimal mechanism's outcome.",
    "category": "CORRECT",
    "question_context_used": "Key formula: The principal's belief about $v_{i}(t_{i})$ is given by:\n\n$$\n\\hat{v}_{i}(t_{i}) = \\begin{cases}\nv_{i}^{+} & \\text{if } t_{i} \\in T_{i}^{0} \\cap T_{i}^{+} \\text{ or } t_{i} \\in T_{i}^{+} \\setminus T_{i}^{0} \\text{ and } v_{i}(t_{i}) \\leq v_{i}^{+}, \\\\\nv_{i}^{-} & \\text{if } t_{i} \\in T_{i}^{0} \\cap T_{i}^{-} \\text{ or } t_{i} \\in T_{i}^{-} \\setminus T_{i}^{0} \\text{ and } v_{i}(t_{i}) \\geq v_{i}^{-}, \\\\\nv_{i}(t_{i}), & \\text{otherwise.}\n\\end{cases}\n$$\n\nIn the game without commitment, the principal forms beliefs based on the agents’ equilibrium strategies and chooses a best action conditional on these beliefs. The surprising result is that the equilibrium and optimal mechanism constructed have the property that $\\boldsymbol{a}^{*}(t) = \\hat{\\boldsymbol{a}}(t)$ for every profile $t$. This implies that commitment is not needed to achieve the optimal mechanism's outcome.",
    "gold_answer_parsed": [
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1830",
    "question": "In the context of hyperbolic discounting models of labour supply and welfare participation, which of the following statements are supported by the empirical findings?",
    "options": [
      "A) Most individuals exhibit time-inconsistent preferences with varying degrees of present bias.",
      "B) A dynamic sanction triggered by past employment can serve as a commitment device preferred by present-biased individuals.",
      "C) Introducing a welfare component to the tax system universally improves long-run utility by mitigating self-control problems.",
      "D) Time-consistent models accurately predict the qualitative patterns of policy effects among individuals with severe present bias."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Welfare Dependence and Self-Control An Empirical Analysis.md"
    ],
    "question_context": "The article estimates a hyperbolic discounting model of labour supply and welfare participation with heterogeneous time preference parameters. Exclusion restrictions are constructed from variations in behaviour induced by time limits in a welfare reform experiment. Key findings include that most individuals are time-inconsistent, exhibiting varying degrees of present bias and perception of the commitment problem. The model also identifies policies that restrict the budget set but alleviate the commitment problem substantially, such as dynamic sanctions triggered by past employment, which can increase the long-run utility of present-biased individuals. The formula for the deterministic part of the utility function is given by: \n\n$$\\begin{array}{r}{\\bar{u}_{i t}=y_{i t}(h_{i t},d_{a i t})+\\alpha_{y i}[y_{i t}(h_{i t},d_{a i t})]^{2}+\\alpha_{h i}d_{h i t}+\\alpha_{h2}d_{h i t}^{f t}+\\alpha_{a i}d_{a i t}+\\alpha_{h a i}d_{h i t}d_{a i t}}\\ {+\\gamma_{h}d_{h i t}d_{h i,t-1}+\\gamma_{h2}d_{h i t}^{f t}d_{h i,t-1}+\\gamma_{a}d_{a i t}d_{a i t}-1+\\gamma_{h a}d_{h i t}d_{a i t}d_{h i,t-1}d_{a i,t-1}.}\\end{array}$$",
    "category": "CORRECT",
    "question_context_used": "The article estimates a hyperbolic discounting model of labour supply and welfare participation with heterogeneous time preference parameters. Exclusion restrictions are constructed from variations in behaviour induced by time limits in a welfare reform experiment. Key findings include that most individuals are time-inconsistent, exhibiting varying degrees of present bias and perception of the commitment problem. The model also identifies policies that restrict the budget set but alleviate the commitment problem substantially, such as dynamic sanctions triggered by past employment, which can increase the long-run utility of present-biased individuals. The formula for the deterministic part of the utility function is given by: \n\n$$\\begin{array}{r}{\\bar{u}_{i t}=y_{i t}(h_{i t},d_{a i t})+\\alpha_{y i}[y_{i t}(h_{i t},d_{a i t})]^{2}+\\alpha_{h i}d_{h i t}+\\alpha_{h2}d_{h i t}^{f t}+\\alpha_{a i}d_{a i t}+\\alpha_{h a i}d_{h i t}d_{a i t}}\\ {+\\gamma_{h}d_{h i t}d_{h i,t-1}+\\gamma_{h2}d_{h i t}^{f t}d_{h i,t-1}+\\gamma_{a}d_{a i t}d_{a i t}-1+\\gamma_{h a}d_{h i t}d_{a i t}d_{h i,t-1}d_{a i,t-1}.}\\end{array}$$",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1995",
    "question": "Which of the following scenarios could lead to an increase in the long-run growth rate of income per person, according to the production function $y=A K^{\\alpha}(L_{I}^{\\beta_{1}}L_{2}^{\\beta_{2}}\\cdot\\cdot\\cdot\\cdot L_{n}^{\\beta_{n}})^{1-\\alpha}$?",
    "options": [
      "A) Replacement of labor tasks with capital over time",
      "B) A decline in the number of researchers contributing to idea production",
      "C) A decrease in the capital share in the production function",
      "D) A balanced growth path with constant capital and labor shares"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Fernald-FutureUSEconomic-2014.md"
    ],
    "question_context": "The production function for new ideas is given by ${\\dot{A}}=R f(A)=\\beta R A^{\\phi}$, where $\\phi<0$ historically implies diminishing returns to idea production. However, balanced growth can still occur due to exponential growth in the number of researchers, $R$. The shape of $f(A)$ in the future may differ from the past, potentially leading to different growth paths. For example, if capital can replace labor entirely in the production function $y=A K^{\\alpha}(L_{I}^{\\beta_{1}}L_{2}^{\\beta_{2}}\\cdot\\cdot\\cdot\\cdot L_{n}^{\\beta_{n}})^{1-\\alpha}$, the capital share will rise, and the long-run growth rate of income per person will increase, as it is proportional to $1/(1 - \\text{capital share}) \\times$ the growth rate of $A$.",
    "category": "INCORRECT",
    "question_context_used": "The production function for new ideas is given by ${\\dot{A}}=R f(A)=\\beta R A^{\\phi}$, where $\\phi<0$ historically implies diminishing returns to idea production. However, balanced growth can still occur due to exponential growth in the number of researchers, $R$. The shape of $f(A)$ in the future may differ from the past, potentially leading to different growth paths. For example, if capital can replace labor entirely in the production function $y=A K^{\\alpha}(L_{I}^{\\beta_{1}}L_{2}^{\\beta_{2}}\\cdot\\cdot\\cdot\\cdot L_{n}^{\\beta_{n}})^{1-\\alpha}$, the capital share will rise, and the long-run growth rate of income per person will increase, as it is proportional to $1/(1 - \\text{capital share}) \\times$ the growth rate of $A$.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3106",
    "question": "In the context of offshoring production with incomplete contracting, which of the following conditions are necessary for a firm in the North to offshore production of specialized inputs to the South?",
    "options": [
      "A) The bargaining weight of the input producer, β, must be less than (σ-1)/σ",
      "B) The productivity of the firm, α, must exceed a cutoff level α_O such that B(α, α_O) ≥ F_O",
      "C) The wage in the South, w_S, must be greater than the wage in the North, w_N",
      "D) The cost of producing the input domestically, w_N/α, must be greater than the cost of outsourcing, ϕw_S/α"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Temporary-shocks-and-offshoring--The-role-of-externa_2008_Journal-of-Develop.md"
    ],
    "question_context": "The production function for non-numeraire goods is given by x(α) = αy, where α represents firm productivity. The unit cost of production when inputs are produced domestically is c(α) = w_N/α. The profit maximization for a firm producing inputs domestically yields x(α) = A′(σ-1)^σ σ^(-σ) α^σ w_N^(-σ) and maximized profit π_D(α) = A′σ^(-σ)(σ-1)^(σ-1) w_N^(1-σ) α^(σ-1). For offshoring, the benefit B(α) must cover the fixed cost F_O, and the equilibrium condition is B(α_O, α_O) = F_O. The bargaining weight β must satisfy β < (σ-1)/σ for the final goods producer to maximize profits. Additionally, the condition ϕw_S < w_N ensures the South has a comparative advantage in input production.",
    "category": "CORRECT",
    "question_context_used": "The production function for non-numeraire goods is given by x(α) = αy, where α represents firm productivity. The unit cost of production when inputs are produced domestically is c(α) = w_N/α. The profit maximization for a firm producing inputs domestically yields x(α) = A′(σ-1)^σ σ^(-σ) α^σ w_N^(-σ) and maximized profit π_D(α) = A′σ^(-σ)(σ-1)^(σ-1) w_N^(1-σ) α^(σ-1). For offshoring, the benefit B(α) must cover the fixed cost F_O, and the equilibrium condition is B(α_O, α_O) = F_O. The bargaining weight β must satisfy β < (σ-1)/σ for the final goods producer to maximize profits. Additionally, the condition ϕw_S < w_N ensures the South has a comparative advantage in input production.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2220",
    "question": "In the context of heterogeneous treatment effects modeling, which of the following conditions are necessary to justify the inequalities involving covariances and variances in the threshold crossing model?",
    "options": [
      "A) Doctors choose treatment C to minimize mortality based on knowledge of γ_c and γ_nc",
      "B) The sickest patients are more likely to receive treatment C",
      "C) The covariance between X'β and X'γ_c is greater than the covariance between u and ε_c",
      "D) The variance of ε_c is equal to the variance of ε_nc"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Altonji-UsingSelectionObserved-2008.md"
    ],
    "question_context": "The threshold crossing model with heterogeneous effects is given by:\n\n$$\n\\begin{array}{r l}&{C^{*}=X^{\\prime}\\pmb{\\beta}+u,\\qquad C=1(C^{*}>0),}\\ &{\\quad}\\ &{Y_{c}^{*}=X^{\\prime}\\pmb{\\gamma}_{c}+\\varepsilon_{c},}\\ &{\\quad}\\ &{Y_{n c}^{*}=X^{\\prime}\\pmb{\\gamma}_{n c}+\\varepsilon_{n c},}\\ &{\\quad}\\ &{Y=1(C\\cdot Y_{c}^{*}+(1-C)Y_{n c}^{*}>0),}\\end{array}\n$$\n\nDoctors choose C at least in part to minimize mortality, so if they know γ_c and γ_nc, then X'β is negatively related to [X'γ_c - X'γ_nc + ε_c - ε_nc]. The inequalities derived from this setup include:\n\n$$\n\\frac{c o\\nu\\left(X^{\\prime}\\pmb{\\beta},X^{\\prime}\\pmb{\\gamma}_{c}\\right)}{\\nu a r\\left(X^{\\prime}\\pmb{\\gamma}_{c}\\right)}>\\frac{c o\\nu\\left(u,\\pmb{\\varepsilon}_{c}\\right)}{\\nu a r\\left(\\pmb{\\varepsilon}_{c}\\right)}\\equiv\\rho_{\\mathrm{u}\\pmb{\\varepsilon}_{c}}>0.\n$$\n\nAdditionally, unless there are very large interactions,\n\n$$\n\\frac{c o v\\left(X^{\\prime}\\pmb{\\beta},X^{\\prime}\\pmb{\\gamma}_{n c}\\right)}{v a r\\left(X^{\\prime}\\pmb{\\gamma}_{n c}\\right)}>\\frac{c o v\\left(u,\\pmb{\\varepsilon}_{n c}\\right)}{\\nu a r\\left(\\pmb{\\varepsilon}_{n c}\\right)}\\equiv\\rho_{\\mathfrak{u}\\pmb{\\varepsilon}_{n c}}>0.\n$$",
    "category": "INCORRECT",
    "question_context_used": "The threshold crossing model with heterogeneous effects is given by:\n\n$$\n\\begin{array}{r l}&{C^{*}=X^{\\prime}\\pmb{\\beta}+u,\\qquad C=1(C^{*}>0),}\\ &{\\quad}\\ &{Y_{c}^{*}=X^{\\prime}\\pmb{\\gamma}_{c}+\\varepsilon_{c},}\\ &{\\quad}\\ &{Y_{n c}^{*}=X^{\\prime}\\pmb{\\gamma}_{n c}+\\varepsilon_{n c},}\\ &{\\quad}\\ &{Y=1(C\\cdot Y_{c}^{*}+(1-C)Y_{n c}^{*}>0),}\\end{array}\n$$\n\nDoctors choose C at least in part to minimize mortality, so if they know γ_c and γ_nc, then X'β is negatively related to [X'γ_c - X'γ_nc + ε_c - ε_nc]. The inequalities derived from this setup include:\n\n$$\n\\frac{c o\\nu\\left(X^{\\prime}\\pmb{\\beta},X^{\\prime}\\pmb{\\gamma}_{c}\\right)}{\\nu a r\\left(X^{\\prime}\\pmb{\\gamma}_{c}\\right)}>\\frac{c o\\nu\\left(u,\\pmb{\\varepsilon}_{c}\\right)}{\\nu a r\\left(\\pmb{\\varepsilon}_{c}\\right)}\\equiv\\rho_{\\mathrm{u}\\pmb{\\varepsilon}_{c}}>0.\n$$\n\nAdditionally, unless there are very large interactions,\n\n$$\n\\frac{c o v\\left(X^{\\prime}\\pmb{\\beta},X^{\\prime}\\pmb{\\gamma}_{n c}\\right)}{v a r\\left(X^{\\prime}\\pmb{\\gamma}_{n c}\\right)}>\\frac{c o v\\left(u,\\pmb{\\varepsilon}_{n c}\\right)}{\\nu a r\\left(\\pmb{\\varepsilon}_{n c}\\right)}\\equiv\\rho_{\\mathfrak{u}\\pmb{\\varepsilon}_{n c}}>0.\n$$",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2624",
    "question": "Which of the following factors are identified as primary causes of the inflationary surges in the 1970s, according to the text?",
    "options": [
      "A) Excess demand in labor markets",
      "B) Supply shocks such as food shortages",
      "C) Wage acceleration due to labor market tightness",
      "D) Oil cartel price increases"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Krashevski-NaturalHighUnemployment-1988.md"
    ],
    "question_context": "The view that a 6 to 7 percent range is the lowest that unemployment can descend without causing an inflation speedup incorrectly characterizes labor markets as the principal cause of inflation and wrongly extrapolates the conditions of the 1970's to today's economy. Supply shocks, not excess demand, labor market tightness or wage acceleration, set off the major inflationary surges of the 1970s. In 1973, food shortages and the oil cartel's enormous oil price hike launched the decade's first wave of double-digit inflation. Another huge oil cartel price increase in 1979 initiated a second round of inflation above 10 percent. Workers' purchasing power eroded as wage increases failed to keep up with increases in the general price level.",
    "category": "CORRECT",
    "question_context_used": "The view that a 6 to 7 percent range is the lowest that unemployment can descend without causing an inflation speedup incorrectly characterizes labor markets as the principal cause of inflation and wrongly extrapolates the conditions of the 1970's to today's economy. Supply shocks, not excess demand, labor market tightness or wage acceleration, set off the major inflationary surges of the 1970s. In 1973, food shortages and the oil cartel's enormous oil price hike launched the decade's first wave of double-digit inflation. Another huge oil cartel price increase in 1979 initiated a second round of inflation above 10 percent. Workers' purchasing power eroded as wage increases failed to keep up with increases in the general price level.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3404",
    "question": "Which of the following factors contribute to inflation persistence in the context of expectations-based models?",
    "options": [
      "A) Imperfect information about the nature of economic shocks",
      "B) Use of simple recursive learning algorithms by private agents",
      "C) A credible monetary policy regime focused on price stability",
      "D) Automatic indexation of prices in structural models"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Angeloni-NewEvidenceInflation-2006.md"
    ],
    "question_context": "Most theories of inflation dynamics accord a significant role to inflation expectations in the determination of inflation. Under the assumption of rational expectations, inflation expectations by themselves will not contribute to the persistence of the inflation process. However, relatively small deviations from the assumption of perfect information can change this result dramatically. For example, imperfect information about which shocks (e.g., temporary vs. permanent) are affecting the economy may lead to more persistent and gradual responses of inflation to shocks. Similarly, when private agents use relatively simple recursive learning algorithms to form inflation expectations in the presence of model uncertainty, activist central banks that care a lot about stabilising the output gap may slow down the learning process of agents trying to forecast inflation and may thereby increase the persistence of the inflation process. Generally speaking, a credible policy regime focused on price stability will reduce the persistence of inflation.",
    "category": "INCORRECT",
    "question_context_used": "Most theories of inflation dynamics accord a significant role to inflation expectations in the determination of inflation. Under the assumption of rational expectations, inflation expectations by themselves will not contribute to the persistence of the inflation process. However, relatively small deviations from the assumption of perfect information can change this result dramatically. For example, imperfect information about which shocks (e.g., temporary vs. permanent) are affecting the economy may lead to more persistent and gradual responses of inflation to shocks. Similarly, when private agents use relatively simple recursive learning algorithms to form inflation expectations in the presence of model uncertainty, activist central banks that care a lot about stabilising the output gap may slow down the learning process of agents trying to forecast inflation and may thereby increase the persistence of the inflation process. Generally speaking, a credible policy regime focused on price stability will reduce the persistence of inflation.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1807",
    "question": "In the context of heterogeneous populations and amenity valuation, which of the following statements are correct regarding the relationship between land rents and amenity resources?",
    "options": [
      "A) Differences in land rents between communities systematically underestimate the value of differences in amenities when individuals are not identical.",
      "B) The aggregate income-equivalent benefit of a higher amenity resource level equals the valuation by residents of the higher amenity community, which is greater than dALR/dA.",
      "C) When individuals are identical, differences in aggregate land rents across communities accurately reflect the value of differences in their characteristics.",
      "D) The Henry George Theorem holds unconditionally, regardless of whether the spatial distribution of economic activity is Pareto optimal."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Aggregate Land Rents, Expenditure on Public Goods, and Optimal City Size.md"
    ],
    "question_context": "The paper examines how differences in amenity valuation among individuals affect the relationship between land rents and amenity resources. Key formula: \n\n$$\\frac{d R(t)}{d A}=\\frac{{\\bf\\hat{\\boldsymbol{\\beta}}}g^{\\prime}}{v_{2}(t)T(t)}.$$\n\nIntegrating this over all $t$ gives \n\n$$\\frac{d A L R}{d A}=\\int_{0}^{t^{*}}\\frac{\\hat{\\beta}g^{\\prime}}{v_{2}T}\\phi~d t.$$\n\n(3.12) indicates that the difference in aggregate land rents between two islands equals what would be the aggregate valuation of the difference in the amenity resource levels if all individuals had the same tastes as the marginal individual. However, the aggregate income-equivalent benefit of the higher amenity resource level, \n\n$$\\int_{0}^{t^{*}}\\frac{\\beta(t)g^{\\prime}\\phi}{v_{2}T}d t,$$\n\nsince $\\boldsymbol{\\beta}(t)\\ge\\hat{\\boldsymbol{\\beta}}$ for all $t$ and $\\boldsymbol{\\beta}(t)>\\dot{\\boldsymbol{\\beta}}$ for some $t$, is greater than $dALR/dA$. This shows that intercommunity differences in land rents do not capture inframarginal benefits, leading to systematic underestimation of amenity values when populations are heterogeneous.",
    "category": "CORRECT",
    "question_context_used": "The paper examines how differences in amenity valuation among individuals affect the relationship between land rents and amenity resources. Key formula: \n\n$$\\frac{d R(t)}{d A}=\\frac{{\\bf\\hat{\\boldsymbol{\\beta}}}g^{\\prime}}{v_{2}(t)T(t)}.$$\n\nIntegrating this over all $t$ gives \n\n$$\\frac{d A L R}{d A}=\\int_{0}^{t^{*}}\\frac{\\hat{\\beta}g^{\\prime}}{v_{2}T}\\phi~d t.$$\n\n(3.12) indicates that the difference in aggregate land rents between two islands equals what would be the aggregate valuation of the difference in the amenity resource levels if all individuals had the same tastes as the marginal individual. However, the aggregate income-equivalent benefit of the higher amenity resource level, \n\n$$\\int_{0}^{t^{*}}\\frac{\\beta(t)g^{\\prime}\\phi}{v_{2}T}d t,$$\n\nsince $\\boldsymbol{\\beta}(t)\\ge\\hat{\\boldsymbol{\\beta}}$ for all $t$ and $\\boldsymbol{\\beta}(t)>\\dot{\\boldsymbol{\\beta}}$ for some $t$, is greater than $dALR/dA$. This shows that intercommunity differences in land rents do not capture inframarginal benefits, leading to systematic underestimation of amenity values when populations are heterogeneous.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2739",
    "question": "Which of the following statements are true regarding the IGARCH and FIGARCH models based on the empirical findings discussed?",
    "options": [
      "A) IGARCH behavior in high-frequency data may arise as an artefact from approximating continuous time diffusions.",
      "B) The forecasting properties of IGARCH are similar to those of stable GARCH models.",
      "C) Data generated from a FIGARCH process can be easily mistaken for IGARCH behavior due to the concentration of the persistence parameter estimates near unity.",
      "D) The IGARCH model is compatible with the persistence observed after large shocks such as the Crash of October 1987."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Fractionally-integrated-generalized-autoregressive-cond_1996_Journal-of-Econ.md"
    ],
    "question_context": "A striking empirical regularity that emerges from numerous studies of high-frequency, say daily, asset pricing data with ARCH-type models, concerns the apparent widespread finding of integrated GARCH behavior. Nelson (1990b, 1992) and Nelson and Foster (1994) have argued that the apparent IGARCH property of high-frequency data may arise as an artefact from empirically approximating continuous time diffusions or semi-martingale processes. In particular, a GARCH(1,1) model with parameters $\\omega^{(h)}=\\omega h$, $\\boldsymbol{\\alpha}_{1}^{(h)}$ $=\\alpha(h/2)^{1/2}$, and $\\beta_{1}^{(h)}=1-\\alpha(h/2)^{1/2}-\\phi h,$ where $\\omega\\geqslant0.$ ${\\pmb{\\alpha}}>{\\pmb{0}}$ and $\\phi>0$ are fixed, provides a consistent approximation to the true continuous time variance process as the sampling interval, $h,$ goes to zero, for a wide variety of continuous time processes. Note, that in the limit, the sum of the two GARCH(1,1) parameters, $\\phi^{(h)}\\equiv\\alpha^{(h)}+\\beta^{(h)}=1-\\phi h.$ tends to one, indicative of IGARCH behavior.\n\nData generated from a process exhibiting long-memory FIGARCH volatility may be easily mistaken for IGARCH behavior. This is analogous to the results of Diebold and Rudebusch (1991), who demonstrate that the conventional unit root testing procedures for the mean have low power for distinguishing against fractional white noise alternatives. For both the mean and the variance, being confined to only considering the extreme cases of I(0) and I(1), or stable GARCH and IGARCH processes, can be very misleading when long-memory, but eventual mean-reverting processes are generating the observed data.",
    "category": "CORRECT",
    "question_context_used": "A striking empirical regularity that emerges from numerous studies of high-frequency, say daily, asset pricing data with ARCH-type models, concerns the apparent widespread finding of integrated GARCH behavior. Nelson (1990b, 1992) and Nelson and Foster (1994) have argued that the apparent IGARCH property of high-frequency data may arise as an artefact from empirically approximating continuous time diffusions or semi-martingale processes. In particular, a GARCH(1,1) model with parameters $\\omega^{(h)}=\\omega h$, $\\boldsymbol{\\alpha}_{1}^{(h)}$ $=\\alpha(h/2)^{1/2}$, and $\\beta_{1}^{(h)}=1-\\alpha(h/2)^{1/2}-\\phi h,$ where $\\omega\\geqslant0.$ ${\\pmb{\\alpha}}>{\\pmb{0}}$ and $\\phi>0$ are fixed, provides a consistent approximation to the true continuous time variance process as the sampling interval, $h,$ goes to zero, for a wide variety of continuous time processes. Note, that in the limit, the sum of the two GARCH(1,1) parameters, $\\phi^{(h)}\\equiv\\alpha^{(h)}+\\beta^{(h)}=1-\\phi h.$ tends to one, indicative of IGARCH behavior.\n\nData generated from a process exhibiting long-memory FIGARCH volatility may be easily mistaken for IGARCH behavior. This is analogous to the results of Diebold and Rudebusch (1991), who demonstrate that the conventional unit root testing procedures for the mean have low power for distinguishing against fractional white noise alternatives. For both the mean and the variance, being confined to only considering the extreme cases of I(0) and I(1), or stable GARCH and IGARCH processes, can be very misleading when long-memory, but eventual mean-reverting processes are generating the observed data.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1949",
    "question": "In the three-aggregate model analysis, which of the following conditions must hold for the share of good Y in total consumption to increase?",
    "options": [
      "A) The pure substitution own-price elasticity of demand for Y must be greater than the product of the income elasticity of demand for Y and the net second best effects.",
      "B) The pure substitution own-price elasticity of demand for Y must be less than the product of the income elasticity of demand for Y and the net second best effects.",
      "C) The income elasticity of demand for Y must be greater than unity.",
      "D) The net second best effects must be greater than the pure substitution own-price elasticity of demand for Y."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Welfare_Effects_of_Customs_Union_An_Anatomy.md"
    ],
    "question_context": "The overall change in the pattern of consumption can be determined by considering how the share of $Y$ in total consumption changes. A $\\mathrm{~\\bf~I~}\\%$ fall in the price of $\\boldsymbol{Y}$ will raise $Y$ consumption through the substitution effect by $-\\eta^{\\mathfrak{s}}_{Y}\\%$ (the pure substitution own-price elasticity of demand for $Y$) and will change income through the net second best effects by $\\alpha\\%$ where $\\pmb{\\alpha}$ is very much less than unity. This change in income will give rise to a change in the consumption of $Y$ of $\\alpha y_{\\pmb{\\chi}}$ (the income elasticity of demand for $Y$) so that there will be an increase in the share of $\\gamma$ in total consumption if and only if \n\n$$\n-\\eta_{\\textsl{Y}}^{s}<\\alpha(y_{\\textsl{Y}}-\\mathfrak{r}),\n$$\n\nwhich is unlikely. Whilst failure to meet this condition ensures a fall in the share of $X{+}Z$ in consumption, either $X$ or $z$ could increase its share individually if it is a Hicksian net complement of $Y$ and has a reasonable income elasticity, the precise condition to be satisfied for an increase in the share of $X$ being $-\\eta^{\\ s}_{X Y}>\\alpha(\\mathbf{r}-y_{X})$.",
    "category": "INCORRECT",
    "question_context_used": "The overall change in the pattern of consumption can be determined by considering how the share of $Y$ in total consumption changes. A $\\mathrm{~\\bf~I~}\\%$ fall in the price of $\\boldsymbol{Y}$ will raise $Y$ consumption through the substitution effect by $-\\eta^{\\mathfrak{s}}_{Y}\\%$ (the pure substitution own-price elasticity of demand for $Y$) and will change income through the net second best effects by $\\alpha\\%$ where $\\pmb{\\alpha}$ is very much less than unity. This change in income will give rise to a change in the consumption of $Y$ of $\\alpha y_{\\pmb{\\chi}}$ (the income elasticity of demand for $Y$) so that there will be an increase in the share of $\\gamma$ in total consumption if and only if \n\n$$\n-\\eta_{\\textsl{Y}}^{s}<\\alpha(y_{\\textsl{Y}}-\\mathfrak{r}),\n$$\n\nwhich is unlikely. Whilst failure to meet this condition ensures a fall in the share of $X{+}Z$ in consumption, either $X$ or $z$ could increase its share individually if it is a Hicksian net complement of $Y$ and has a reasonable income elasticity, the precise condition to be satisfied for an increase in the share of $X$ being $-\\eta^{\\ s}_{X Y}>\\alpha(\\mathbf{r}-y_{X})$.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-160",
    "question": "Under which conditions does the PenII estimator achieve asymptotic normality with a non-zero mean and a modified asymptotic variance, according to Theorem 3?",
    "options": [
      "A) When the penalty function's first derivative is o(T^(-1/2)) and its second derivative is o(1)",
      "B) When the penalty function's first derivative is O(T^(-1/2)) and its second derivative is o(1)",
      "C) When the penalty function's first derivative is o(T^(-1/2)) and its second derivative is O(1)",
      "D) When the penalty function's first derivative is O(T^(-1/2)) and its second derivative is O(1)"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Penalized-indirect-inference_2018_Journal-of-Econometrics.md"
    ],
    "question_context": "The asymptotic normality of the PenII estimator requires that the penalty function converges in an appropriate manner to some limit. Assumption 5 states alternative conditions that render the PenII estimator asymptotically normal. Each condition yields a different limit result in terms of the asymptotic bias and the asymptotic variance. The role of Assumption 5 can be easily understood by analyzing an expansion of the PenII estimator. Statistical inference on θ₀ can thus be affected by both the slope and curvature of the penalty function at θ₀, even when the penalty function vanishes asymptotically.\n\nTheorem 3 establishes the asymptotic normality of the PenII estimator θ̂_{T,S}, and describes its asymptotic mean and variance under the four different limit cases described by Assumption 5. If Assumption 5(ii) holds then: μ = -½A⁻¹π and W = (1 + 1/S)A⁻¹BA⁻¹. If Assumption 5(iv) holds then: μ = -½(A + ½∇²π(θ₀))⁻¹π and W = (1 + 1/S)(A + ½∇²π(θ₀))⁻¹B(A + ½∇²π(θ₀))⁻¹.",
    "category": "INCORRECT",
    "question_context_used": "The asymptotic normality of the PenII estimator requires that the penalty function converges in an appropriate manner to some limit. Assumption 5 states alternative conditions that render the PenII estimator asymptotically normal. Each condition yields a different limit result in terms of the asymptotic bias and the asymptotic variance. The role of Assumption 5 can be easily understood by analyzing an expansion of the PenII estimator. Statistical inference on θ₀ can thus be affected by both the slope and curvature of the penalty function at θ₀, even when the penalty function vanishes asymptotically.\n\nTheorem 3 establishes the asymptotic normality of the PenII estimator θ̂_{T,S}, and describes its asymptotic mean and variance under the four different limit cases described by Assumption 5. If Assumption 5(ii) holds then: μ = -½A⁻¹π and W = (1 + 1/S)A⁻¹BA⁻¹. If Assumption 5(iv) holds then: μ = -½(A + ½∇²π(θ₀))⁻¹π and W = (1 + 1/S)(A + ½∇²π(θ₀))⁻¹B(A + ½∇²π(θ₀))⁻¹.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-291",
    "question": "Which of the following factors contribute to the observed gap between privately beneficial energy efficiency investments and actual household take-up rates?",
    "options": [
      "A) High non-monetary costs associated with participation in energy efficiency programs",
      "B) Perfect information dissemination about benefits and costs of retrofits",
      "C) Significant out-of-pocket expenses for energy efficiency improvements",
      "D) Transaction costs and information barriers in household decision-making"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Fowlie-NonMonetaryCostsEnergy-2015.md"
    ],
    "question_context": "The study highlights a persistent gap between privately beneficial energy efficiency investments and actual household take-up rates, even when improvements involve zero out-of-pocket costs (e.g., roughly $\\$5,000$ in home improvements). The findings suggest that extraordinary efforts to inform households about benefits and costs only modestly increase participation. This is consistent with high non-monetary costs (e.g., time, hassle) associated with programs like WAP or energy efficiency investments. The energy efficiency gap in the residential sector may thus be narrower than it appears when these non-monetary factors are considered.",
    "category": "INCORRECT",
    "question_context_used": "The study highlights a persistent gap between privately beneficial energy efficiency investments and actual household take-up rates, even when improvements involve zero out-of-pocket costs (e.g., roughly $\\$5,000$ in home improvements). The findings suggest that extraordinary efforts to inform households about benefits and costs only modestly increase participation. This is consistent with high non-monetary costs (e.g., time, hassle) associated with programs like WAP or energy efficiency investments. The energy efficiency gap in the residential sector may thus be narrower than it appears when these non-monetary factors are considered.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4590",
    "question": "According to the volatility accounting framework presented by Stiroh (2009), which of the following factors are identified as primary contributors to the decline in U.S. output volatility after 1984?",
    "options": [
      "A) Decreased volatility in labor productivity growth and hours growth",
      "B) Increased covariance between labor input and total factor productivity growth",
      "C) Smaller covariances between industries in the U.S. economy",
      "D) Structural shifts from manufacturing to the service sector"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Stiroh-VolatilityAccountingProduction-2009.md"
    ],
    "question_context": "The paper examines the declining volatility of U.S. output growth from a production perspective, decomposing the sources of volatility associated with capital, labor, and total factor productivity (TFP). Key findings include: (1) At the aggregate level, increased output stability reflects decreased volatility in both labor productivity growth ($\\sigma_{LP}$) and hours growth ($\\sigma_H$), as well as a significant decline in their covariance ($\\text{Cov}(LP, H)$). The correlation between hours growth and labor productivity growth fell from 0.01 for 1947:Q1-1983:Q4 to $-0.42$ for 1984:Q1-2005:Q2. (2) At the industry level, about $80\\%$ of the decline in aggregate output variance reflects smaller covariances between industries, while only $20\\%$ stems from reduced within-industry variances. The paper also discusses competing explanations for these patterns, including labor market flexibility and measurement issues.",
    "category": "CORRECT",
    "question_context_used": "The paper examines the declining volatility of U.S. output growth from a production perspective, decomposing the sources of volatility associated with capital, labor, and total factor productivity (TFP). Key findings include: (1) At the aggregate level, increased output stability reflects decreased volatility in both labor productivity growth ($\\sigma_{LP}$) and hours growth ($\\sigma_H$), as well as a significant decline in their covariance ($\\text{Cov}(LP, H)$). The correlation between hours growth and labor productivity growth fell from 0.01 for 1947:Q1-1983:Q4 to $-0.42$ for 1984:Q1-2005:Q2. (2) At the industry level, about $80\\%$ of the decline in aggregate output variance reflects smaller covariances between industries, while only $20\\%$ stems from reduced within-industry variances. The paper also discusses competing explanations for these patterns, including labor market flexibility and measurement issues.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3802",
    "question": "In the stochastic borrowing model for optimal economic growth, which of the following conditions are necessary for the existence of optimal investment and consumption policies?",
    "options": [
      "A) The production function f_ω(x) must be increasing and concave for each ω∈Ω",
      "B) The utility function u(c) must be strictly convex for c>0",
      "C) The stochastic production process must be stationary across all time periods",
      "D) There must exist a bound M such that ρf_ω(x)-x≤M for all x≥0"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Optimal-growth-and-uncertainty--The-borrowing-m_1981_Journal-of-Economic-The.md"
    ],
    "question_context": "The stochastic production process is assumed to be stationary in that the same probability space is used in all time periods. We also assume that, for each ω∈Ω, f_ω(x) is an increasing concave function such that f_ω(0)=0 and ρf_ω(x)-x≤M holds for all x≥0 for some bound M. The utility function u(c) is also assumed to be an increasing, strictly concave function for c>0.\n\nThe properties of this and related deterministic borrowing models are given elsewhere (see [8]). Our present purpose is to consider the case of a stochastic production process. The single production function f is now replaced by a family f_ω, where ω∈Ω and (Ω,B,P) is a probability space. Thus the output Y(ω)=F(x)=f_ω(x) is a random variable, for each fixed input x.",
    "category": "INCORRECT",
    "question_context_used": "The stochastic production process is assumed to be stationary in that the same probability space is used in all time periods. We also assume that, for each ω∈Ω, f_ω(x) is an increasing concave function such that f_ω(0)=0 and ρf_ω(x)-x≤M holds for all x≥0 for some bound M. The utility function u(c) is also assumed to be an increasing, strictly concave function for c>0.\n\nThe properties of this and related deterministic borrowing models are given elsewhere (see [8]). Our present purpose is to consider the case of a stochastic production process. The single production function f is now replaced by a family f_ω, where ω∈Ω and (Ω,B,P) is a probability space. Thus the output Y(ω)=F(x)=f_ω(x) is a random variable, for each fixed input x.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-745",
    "question": "Which of the following factors are empirically supported by the robustness checks to significantly influence the tax-price elasticity estimates in the study?",
    "options": [
      "A) Inclusion of taxpayers close to retirement age (60-64)",
      "B) Exclusion of all taxpayers with S corporation income in either year",
      "C) Portfolio shifts from high-capital-gain to high-dividend assets post-TRA",
      "D) Symmetric application of sample exclusions based on 1989 income thresholds"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Auten-EffectIncomeTaxes-1999.md"
    ],
    "question_context": "The study conducts multiple robustness checks to assess the sensitivity of tax-price elasticity estimates. Key findings include: (1) Expanding the sample to include taxpayers aged 60-64 in 1985 (retirement-adjacent cohort) increases the estimated elasticity to 0.70 (s.e. = 0.12) from 0.57, suggesting behavioral heterogeneity. (2) Portfolio effects are significant: TRA's repeal of capital-gains exclusion induced shifts to high-dividend assets, with a positive capital-gains dummy coefficient (0.023, s.e. = 0.012). The revenue-maximizing tax rate formula is derived as $\\uptau=\\frac{1}{(1+\\upbeta_{\\uptau})}$, where $\\upbeta_{\\uptau}$ is the elasticity. For the preferred model, this yields $\\approx 65\\%$ under proportionality assumptions.",
    "category": "CORRECT",
    "question_context_used": "The study conducts multiple robustness checks to assess the sensitivity of tax-price elasticity estimates. Key findings include: (1) Expanding the sample to include taxpayers aged 60-64 in 1985 (retirement-adjacent cohort) increases the estimated elasticity to 0.70 (s.e. = 0.12) from 0.57, suggesting behavioral heterogeneity. (2) Portfolio effects are significant: TRA's repeal of capital-gains exclusion induced shifts to high-dividend assets, with a positive capital-gains dummy coefficient (0.023, s.e. = 0.012). The revenue-maximizing tax rate formula is derived as $\\uptau=\\frac{1}{(1+\\upbeta_{\\uptau})}$, where $\\upbeta_{\\uptau}$ is the elasticity. For the preferred model, this yields $\\approx 65\\%$ under proportionality assumptions.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2425",
    "question": "Under the unfriendly authority, which of the following statements are true regarding the party's payoff and equilibrium levels of effort?",
    "options": [
      "A) The party's payoff in the high-effort equilibrium strictly decreases with the approval probability q.",
      "B) The equilibrium level of effort in the low-effort equilibrium strictly increases with q.",
      "C) The party's payoff in the low-effort equilibrium is bounded above by e*.",
      "D) The equilibrium level of effort in the high-effort equilibrium strictly decreases with q."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "The-perils-of-friendly-oversight_2022_Journal-of-Economic-Theory.md"
    ],
    "question_context": "Proposition 2 shows that for any $q<q^{\\dagger}$ there exist exactly two interior equilibria, $(e^{\\ell},\\mu^{\\ell})$ and $\\left(e^{h},\\mu^{h}\\right)$. The equilibrium levels of effort $e^{\\ell}$ and $e^{h}$ are the two roots of $f_{q}(e)=0$, where the function $f_{q}$ is as defined in (A-4) and $e^{\\ell}<e^{h}$. So for any $q<q^{\\dagger}$, we let $e^{\\ell}(q)$ and $e^{h}(q)$ be such that $e^{\\ell}(q)<e^{h}(q)$ and $f_{q}(e^{k}(q))=0$ for $k\\in\\{\\ell,h\\}$. First, note that $f_{q}(e)<f_{q}(e^{\\ell}(q))=0$ for all $e<e^{\\ell}(q)$. Second, note that if $q<q^{\\prime}$, then $f_{q^{\\prime}}(e)<f_{q}(e)$ for any $e$. Then, whenever $q<q^{\\prime}\\leq q^{\\dagger}$, \n\n$$\nf_{q^{\\prime}}(e)<f_{q}(e)\\leq f_{q}(e^{\\ell}(q))=0\\qquad\\forall e\\in\\left(0,e^{\\ell}(q)\\right]\n$$\n\nThen, $e^{\\ell}(q^{\\prime})>e^{\\ell}(q)$. A similar argument shows that $e^{h}(q^{\\prime})<e^{h}(q)$. \n\nIn the interior equilibrium $(e^{k},\\mu^{k})$, the politician’s payoff is equal to \n\n$$\ne^{k}\\mu_{1}^{k}-c(e^{k})=e^{k}c^{\\prime}(e^{k})-c(e^{k}).\n$$\n\nThe derivative of the politician’s payoff with respect to $q$ is thus equal to \n\n$$\ne^{k}c^{\\prime\\prime}(e^{k}){\\frac{d e^{k}}{d q}}+{\\frac{d e^{k}}{d q}}c^{\\prime}(e^{k})-c^{\\prime}(e^{k}){\\frac{d e^{k}}{d q}}=e^{k}c^{\\prime\\prime}(e^{k}){\\frac{d e^{k}}{d q}}.\n$$\n\nHence, the politician’s payoff increases with $q$ in the low-effort equilibrium and decreases with $q$ in the high-effort equilibrium.",
    "category": "INCORRECT",
    "question_context_used": "Proposition 2 shows that for any $q<q^{\\dagger}$ there exist exactly two interior equilibria, $(e^{\\ell},\\mu^{\\ell})$ and $\\left(e^{h},\\mu^{h}\\right)$. The equilibrium levels of effort $e^{\\ell}$ and $e^{h}$ are the two roots of $f_{q}(e)=0$, where the function $f_{q}$ is as defined in (A-4) and $e^{\\ell}<e^{h}$. So for any $q<q^{\\dagger}$, we let $e^{\\ell}(q)$ and $e^{h}(q)$ be such that $e^{\\ell}(q)<e^{h}(q)$ and $f_{q}(e^{k}(q))=0$ for $k\\in\\{\\ell,h\\}$. First, note that $f_{q}(e)<f_{q}(e^{\\ell}(q))=0$ for all $e<e^{\\ell}(q)$. Second, note that if $q<q^{\\prime}$, then $f_{q^{\\prime}}(e)<f_{q}(e)$ for any $e$. Then, whenever $q<q^{\\prime}\\leq q^{\\dagger}$, \n\n$$\nf_{q^{\\prime}}(e)<f_{q}(e)\\leq f_{q}(e^{\\ell}(q))=0\\qquad\\forall e\\in\\left(0,e^{\\ell}(q)\\right]\n$$\n\nThen, $e^{\\ell}(q^{\\prime})>e^{\\ell}(q)$. A similar argument shows that $e^{h}(q^{\\prime})<e^{h}(q)$. \n\nIn the interior equilibrium $(e^{k},\\mu^{k})$, the politician’s payoff is equal to \n\n$$\ne^{k}\\mu_{1}^{k}-c(e^{k})=e^{k}c^{\\prime}(e^{k})-c(e^{k}).\n$$\n\nThe derivative of the politician’s payoff with respect to $q$ is thus equal to \n\n$$\ne^{k}c^{\\prime\\prime}(e^{k}){\\frac{d e^{k}}{d q}}+{\\frac{d e^{k}}{d q}}c^{\\prime}(e^{k})-c^{\\prime}(e^{k}){\\frac{d e^{k}}{d q}}=e^{k}c^{\\prime\\prime}(e^{k}){\\frac{d e^{k}}{d q}}.\n$$\n\nHence, the politician’s payoff increases with $q$ in the low-effort equilibrium and decreases with $q$ in the high-effort equilibrium.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-135",
    "question": "In the context of the random lottery-selection procedure with von Neumann-Morgenstern utility, which of the following statements are correct regarding the control for income effects?",
    "options": [
      "A) The procedure ensures that wealth effects are neutralized by randomly selecting only one lottery for payment.",
      "B) The separable form of the expected-utility expression implies that choices of selling prices and lotteries can be made independently.",
      "C) The Becker/DeGroot/Marschak method guarantees truthful revelation of selling prices regardless of risk attitudes.",
      "D) The addition of a non-random amount ensures that the subject's utility function remains linear."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Holt-PreferenceReversalsIndependence-1986.md"
    ],
    "question_context": "The random lottery-selection procedure is designed to control for income effects by ensuring that only one lottery, selected randomly, determines the subject's payoff. This prevents wealth effects from contaminating the data. The subject's decision problem involves maximizing an expected-utility expression that is separable, allowing independent choices of selling prices and lotteries. The Becker/DeGroot/Marschak method is used to elicit selling prices, and its revelation property ensures truthful reporting independent of risk attitudes. The analysis assumes that the subject's preferences are represented by a von Neumann-Morgenstern utility function. Key formula: The expected-utility expression is given by:\n\n$$\n\\frac{1}{3}E\\{u(w+\\tilde{x})\\}\n$$\n\n$$\n+\\frac{1}{3}E\\left\\{u\\bigl(w+\\tilde{b}\\bigl(r_{\\mathfrak{g}};X_{\\mathfrak{g}}\\bigr)\\right)\\right\\}\n$$\n\n$$\n+\\frac{1}{3}E\\left\\{u\\bigl(w+\\tilde{b}\\bigl(r_{P};X_{P}\\bigr)\\right)\\right\\},\n$$\n\nwhere each term represents the expected utility from one of the three possible lotteries.",
    "category": "CORRECT",
    "question_context_used": "The random lottery-selection procedure is designed to control for income effects by ensuring that only one lottery, selected randomly, determines the subject's payoff. This prevents wealth effects from contaminating the data. The subject's decision problem involves maximizing an expected-utility expression that is separable, allowing independent choices of selling prices and lotteries. The Becker/DeGroot/Marschak method is used to elicit selling prices, and its revelation property ensures truthful reporting independent of risk attitudes. The analysis assumes that the subject's preferences are represented by a von Neumann-Morgenstern utility function. Key formula: The expected-utility expression is given by:\n\n$$\n\\frac{1}{3}E\\{u(w+\\tilde{x})\\}\n$$\n\n$$\n+\\frac{1}{3}E\\left\\{u\\bigl(w+\\tilde{b}\\bigl(r_{\\mathfrak{g}};X_{\\mathfrak{g}}\\bigr)\\right)\\right\\}\n$$\n\n$$\n+\\frac{1}{3}E\\left\\{u\\bigl(w+\\tilde{b}\\bigl(r_{P};X_{P}\\bigr)\\right)\\right\\},\n$$\n\nwhere each term represents the expected utility from one of the three possible lotteries.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4738",
    "question": "In the context of precision-based methods for sampling missing observations in mixed-frequency VAR models, which of the following statements are supported by the empirical results?",
    "options": [
      "A) Precision-based methods with soft constraints outperform hard constraints in computational efficiency for large T and p.",
      "B) Kalman-filter based methods exhibit lower MSEs compared to precision-based methods in high-dimensional settings.",
      "C) The computational cost of precision-based methods remains stable even with increasing lag lengths.",
      "D) Precision-based methods can handle irregular missing data patterns as effectively as standard filtering methods."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "High-dimensional-conditionally-Gaussian-state-space-mo_2023_Journal-of-Econo.md"
    ],
    "question_context": "The empirical analysis compares four methods for sampling missing observations in mixed-frequency VAR models: precision-based samplers with hard (P-hard) and soft (P-soft) constraints, and two Kalman-filter based simulation smoothers (CK and DK). Key formula for MSE evaluation: $\\mathrm{MSE}_{i}(\\widehat{\\mathbf{y}}^{m(i,j)})=\\sum_{t=1}^{T}\\|\\mathbf{y}_{t}^{m(i)}-\\widehat{\\mathbf{y}}_{t}^{m(i,j)}\\|^{2}/T$. Results indicate that precision-based methods, especially P-soft, scale efficiently to high dimensions and long lag lengths (Fig. 2), while maintaining accuracy comparable to Kalman-filter methods (Table 2). The methods also successfully handle unbalanced panels with arbitrary missing patterns (Fig. 3). Computational advantages are attributed to avoiding state-space dimensionality inflation inherent in Kalman-filter approaches for large p.",
    "category": "INCORRECT",
    "question_context_used": "The empirical analysis compares four methods for sampling missing observations in mixed-frequency VAR models: precision-based samplers with hard (P-hard) and soft (P-soft) constraints, and two Kalman-filter based simulation smoothers (CK and DK). Key formula for MSE evaluation: $\\mathrm{MSE}_{i}(\\widehat{\\mathbf{y}}^{m(i,j)})=\\sum_{t=1}^{T}\\|\\mathbf{y}_{t}^{m(i)}-\\widehat{\\mathbf{y}}_{t}^{m(i,j)}\\|^{2}/T$. Results indicate that precision-based methods, especially P-soft, scale efficiently to high dimensions and long lag lengths (Fig. 2), while maintaining accuracy comparable to Kalman-filter methods (Table 2). The methods also successfully handle unbalanced panels with arbitrary missing patterns (Fig. 3). Computational advantages are attributed to avoiding state-space dimensionality inflation inherent in Kalman-filter approaches for large p.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-849",
    "question": "Based on the analysis of cyclical mortality among the elderly, which of the following mechanisms are supported by empirical evidence as contributing to pro-cyclical mortality patterns in nursing homes?",
    "options": [
      "A) Counter-cyclical staffing levels of certified nursing aides in nursing homes",
      "B) Pro-cyclical transitions of elderly individuals into nursing homes due to family caregivers' time constraints",
      "C) Cyclical changes in Medicaid funding affecting nursing home quality",
      "D) Business cycle-driven variations in the stability of nursing home staff"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Stevens-BestTimesWorst-2015.md"
    ],
    "question_context": "Key findings from the analysis include: (i) Nursing home deaths exhibit strong pro-cyclical patterns, with a 1 percentage point increase in unemployment rate associated with a 3-5% reduction in mortality (Table 4). (ii) Staffing levels in nursing homes, particularly certified aides, rise during high unemployment periods (Table 7). A 1 percentage point increase in unemployment raises total nursing home employment by ~3%, with certified aides increasing by 3.77% (p < 0.05). (iii) Transitions into nursing homes are counter-cyclical (Table 6), contradicting the hypothesis that family caregivers' time constraints drive pro-cyclical mortality. (iv) Medicaid funding shows no systematic variation over the business cycle, ruling it out as a primary mechanism. The evidence suggests that staffing fluctuations (both quantity and stability) are central to explaining cyclical mortality patterns in nursing homes.",
    "category": "CORRECT",
    "question_context_used": "Key findings from the analysis include: (i) Nursing home deaths exhibit strong pro-cyclical patterns, with a 1 percentage point increase in unemployment rate associated with a 3-5% reduction in mortality (Table 4). (ii) Staffing levels in nursing homes, particularly certified aides, rise during high unemployment periods (Table 7). A 1 percentage point increase in unemployment raises total nursing home employment by ~3%, with certified aides increasing by 3.77% (p < 0.05). (iii) Transitions into nursing homes are counter-cyclical (Table 6), contradicting the hypothesis that family caregivers' time constraints drive pro-cyclical mortality. (iv) Medicaid funding shows no systematic variation over the business cycle, ruling it out as a primary mechanism. The evidence suggests that staffing fluctuations (both quantity and stability) are central to explaining cyclical mortality patterns in nursing homes.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-202",
    "question": "In the instrumental-variables (IV) strategy to address endogeneity in faculty gender effects, which of the following conditions are essential for the validity of the instrument?",
    "options": [
      "A) The term-to-term variation in female faculty composition must be exogenous to student pre-college interest",
      "B) The steady-state proportion of female faculty must remain constant across all departments",
      "C) The instrument must correlate with the likelihood of having a female instructor",
      "D) Course fixed effects must be included to control for unobserved heterogeneity"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Bettinger-FacultyServeRole-2005.md"
    ],
    "question_context": "The analysis uses an instrumental-variables (IV) strategy to deal with potential endogeneity due to non-random distribution of students across courses taught by faculty of different genders. Due to sabbaticals, hirings, retirements, and temporary shifts in the number of sections offered, there is term-by-term variation in the proportion of courses taught by female faculty in a department. This variation serves as the IV that is related to a student's likelihood of having a female instructor but unrelated to a student's pre-college interest in a subject. The first-stage equation is given by:\n\n$$\n{\\begin{array}{r l}&{{\\mathrm{(Female~Faculty)}}_{i k}}\\ &{=\\alpha+\\eta({\\mathrm{Deviation~from~Steady-State}}}\\ &{\\qquad{\\mathrm{Female~Composition}})_{i k}+\\gamma\\mathbf{X}_{i}+\\varepsilon_{i k}}\\end{array}}\n$$\n\nThe model includes controls for school and department, identifying deviations from a department's steady state rather than differences across subjects or schools. Course fixed effects are added to compare students who take the same courses but have different types of instructors.",
    "category": "INCORRECT",
    "question_context_used": "The analysis uses an instrumental-variables (IV) strategy to deal with potential endogeneity due to non-random distribution of students across courses taught by faculty of different genders. Due to sabbaticals, hirings, retirements, and temporary shifts in the number of sections offered, there is term-by-term variation in the proportion of courses taught by female faculty in a department. This variation serves as the IV that is related to a student's likelihood of having a female instructor but unrelated to a student's pre-college interest in a subject. The first-stage equation is given by:\n\n$$\n{\\begin{array}{r l}&{{\\mathrm{(Female~Faculty)}}_{i k}}\\ &{=\\alpha+\\eta({\\mathrm{Deviation~from~Steady-State}}}\\ &{\\qquad{\\mathrm{Female~Composition}})_{i k}+\\gamma\\mathbf{X}_{i}+\\varepsilon_{i k}}\\end{array}}\n$$\n\nThe model includes controls for school and department, identifying deviations from a department's steady state rather than differences across subjects or schools. Course fixed effects are added to compare students who take the same courses but have different types of instructors.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2481",
    "question": "Based on the analysis of crowd-out effects in retirement savings, which of the following statements are supported by the empirical results?",
    "options": [
      "A) Workers with higher education exhibit greater crowd-out effects than those with lower education.",
      "B) The treatment effect of educational attainment on savings behavior is homogeneous across all education groups.",
      "C) Workers who save more regularly in voluntary retirement accounts demonstrate better understanding of substitution between saving plans.",
      "D) The crowd-out effect approaches zero for workers with larger unused contribution room in voluntary retirement accounts."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "rest_a_00711.md"
    ],
    "question_context": "The study decomposes crowd-out effects based on workers' educational attainment and saving behaviors. Key findings include: (1) Workers with high education underrespond compared to those with low education, suggesting human capital influences active versus passive choice. (2) Panel b of Figure 2 shows crowd-out approaches 0 as the sample is conditioned on workers with progressively larger unused contribution room. The regression model for high school completion is given by:\n\n$$\\mathbb{1}\\left(H S_{i p c}=1\\right)=\\mathbf{w}+\\vartheta M_{p c}+\\bar{X}_{i p c}^{\\prime}\\Lambda+\\bar{Z}_{i p c}^{\\prime}\\Omega+K_{p c}^{\\prime}\\Phi+\\mathfrak{e}_{i p c},$$\n\nwhere $M_{p c}$ is mandatory years of schooling. The results indicate education affects savings behavior through both direct and indirect channels.",
    "category": "INCORRECT",
    "question_context_used": "The study decomposes crowd-out effects based on workers' educational attainment and saving behaviors. Key findings include: (1) Workers with high education underrespond compared to those with low education, suggesting human capital influences active versus passive choice. (2) Panel b of Figure 2 shows crowd-out approaches 0 as the sample is conditioned on workers with progressively larger unused contribution room. The regression model for high school completion is given by:\n\n$$\\mathbb{1}\\left(H S_{i p c}=1\\right)=\\mathbf{w}+\\vartheta M_{p c}+\\bar{X}_{i p c}^{\\prime}\\Lambda+\\bar{Z}_{i p c}^{\\prime}\\Omega+K_{p c}^{\\prime}\\Phi+\\mathfrak{e}_{i p c},$$\n\nwhere $M_{p c}$ is mandatory years of schooling. The results indicate education affects savings behavior through both direct and indirect channels.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1828",
    "question": "In the context of Modigliani-Shiller's term structure rationality tests, which of the following statements are correct regarding the implications of using averaged data?",
    "options": [
      "A) Averaged data transforms a random walk process into an ARIMA(0,1,1) with lag-one autocorrelation of 0.25",
      "B) The Modigliani-Shiller approximation yields identical time-series processes for averaged short and long rates when γ=0.95",
      "C) The aggregation problem causes the long-rate series to display autocorrelation closer to a random walk than the appropriate 0.25",
      "D) The likelihood ratio tests in Table 5 invalidate Modigliani-Shiller's rationality proposition at 5% significance"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Mishkin-MarketForecastsRational-1981.md"
    ],
    "question_context": "The Modigliani-Shiller approximation for the expectations hypothesis of the term structure is given by:\n\n$$\nR L_{t}{=}k{+}\\left(1{-}\\gamma\\right)\\sum_{i{=0}}^{\\infty}\\gamma^{i}r_{t{+}i}^{e}\n$$\n\nwhere $R L_{t}=$ the long bond rate, $k=$ the liquidity premium, $\\gamma=1/1+r^{\\ast}$ where $r^{*}$ is a representative short rate, and $r_{t+i}^{e}=$ the one-period rate expected to hold at $t+i$ conditional on information available at time $t$. \n\nWorking has shown that a variable that has a random walk characterization will, if it is averaged, have an ARIMA $(0,1,1)$ time-series process with the correlation coefficient at lag one equal to 0.25. Hence, if the short rate is a random walk as is the long rate, then averages of both these variables should have the same ARIMA (0,1,1) characterization with the 0.25 coefficient at lag one. Using the approximation with γ=.95, the implied time-series process of the averaged long-rate series is ARIMA (0,1,1) with the autocorrelation at lag one equal to .01 rather than the appropriate .25.",
    "category": "CORRECT",
    "question_context_used": "The Modigliani-Shiller approximation for the expectations hypothesis of the term structure is given by:\n\n$$\nR L_{t}{=}k{+}\\left(1{-}\\gamma\\right)\\sum_{i{=0}}^{\\infty}\\gamma^{i}r_{t{+}i}^{e}\n$$\n\nwhere $R L_{t}=$ the long bond rate, $k=$ the liquidity premium, $\\gamma=1/1+r^{\\ast}$ where $r^{*}$ is a representative short rate, and $r_{t+i}^{e}=$ the one-period rate expected to hold at $t+i$ conditional on information available at time $t$. \n\nWorking has shown that a variable that has a random walk characterization will, if it is averaged, have an ARIMA $(0,1,1)$ time-series process with the correlation coefficient at lag one equal to 0.25. Hence, if the short rate is a random walk as is the long rate, then averages of both these variables should have the same ARIMA (0,1,1) characterization with the 0.25 coefficient at lag one. Using the approximation with γ=.95, the implied time-series process of the averaged long-rate series is ARIMA (0,1,1) with the autocorrelation at lag one equal to .01 rather than the appropriate .25.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2259",
    "question": "In the context of strategic form games and coalitional refinements of Nash equilibrium, which of the following statements are true regarding the relationships between different equilibrium concepts?",
    "options": [
      "A) Every strictly strong Nash equilibrium (SSNE) is also a weakly strong Nash equilibrium (WSNE).",
      "B) The set of strictly coalition-proof Nash equilibria (SCPNE) is always a subset of the set of weakly coalition-proof Nash equilibria (WCPNE).",
      "C) The sets of SCPNE and WCPNE can be disjoint, as demonstrated in Example 1.",
      "D) Every weakly strong Nash equilibrium (WSNE) is also a strictly coalition-proof Nash equilibrium (SCPNE)."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "On-Coalition-Proof-Nash-Equilibria-in-Common-Agen_1999_Journal-of-Economic-T.md"
    ],
    "question_context": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism...\n\nIn strategic form games, a profile of strategies $\\mathbf{x}^{*}=(x_{1},...,x_{n})$ is a Nash equilibrium if for all $i\\in N$ and every $x_{i}\\in X_{i}$ it is true that $u_{i}(\\mathbf{x}^{*})\\geqslant u_{i}(x_{i},\\mathbf{x}_{N\\setminus\\{i\\}}^{*})$. The notions of weakly strong Nash equilibrium (WSNE) and strictly strong Nash equilibrium (SSNE) are defined based on coalitional deviations. Similarly, weakly coalition-proof Nash equilibrium (WCPNE) and strictly coalition-proof Nash equilibrium (SCPNE) are recursively defined, incorporating different requirements of coalitional stability.\n\nRemark 1 states that $S S N E(T)\\subseteq W S N E(T)\\subseteq W C P N E(T)$ and $S S N E(T)\\subseteq S C P N E(T)$. Example 1 demonstrates that the sets of SCPNE and WCPNE can be disjoint, with the unique WCPNE strictly dominating the unique SCPNE in payoff terms.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism...\n\nIn strategic form games, a profile of strategies $\\mathbf{x}^{*}=(x_{1},...,x_{n})$ is a Nash equilibrium if for all $i\\in N$ and every $x_{i}\\in X_{i}$ it is true that $u_{i}(\\mathbf{x}^{*})\\geqslant u_{i}(x_{i},\\mathbf{x}_{N\\setminus\\{i\\}}^{*})$. The notions of weakly strong Nash equilibrium (WSNE) and strictly strong Nash equilibrium (SSNE) are defined based on coalitional deviations. Similarly, weakly coalition-proof Nash equilibrium (WCPNE) and strictly coalition-proof Nash equilibrium (SCPNE) are recursively defined, incorporating different requirements of coalitional stability.\n\nRemark 1 states that $S S N E(T)\\subseteq W S N E(T)\\subseteq W C P N E(T)$ and $S S N E(T)\\subseteq S C P N E(T)$. Example 1 demonstrates that the sets of SCPNE and WCPNE can be disjoint, with the unique WCPNE strictly dominating the unique SCPNE in payoff terms.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3216",
    "question": "According to the empirical model and analysis of village-level public goods allocation, which of the following statements are supported by the evidence?",
    "options": [
      "A) High-spillover public goods allocation is higher in the Pradhan's village regardless of GP reservation status",
      "B) Political reservation significantly affects overall village allocation of low-spillover public goods",
      "C) The GP activism index is approximately 0.04 points higher in the Pradhan's village for high-spillover goods",
      "D) SC/ST Pradhans allocate more low-spillover public goods to their own villages when the position is reserved"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Besley-PoliticsPublicGood-2004.md"
    ],
    "question_context": "The empirical model for village level regressions is given by:\n\n$$\ny_{v g}=\\alpha_{b}+\\gamma_{1}R_{g}+\\gamma_{2}P_{v g}+\\gamma_{3}G_{v g}+\\gamma_{4}(R_{g}\\times P_{v g})+\\phi X_{v g}+\\varepsilon_{v g}\n$$\n\nwhere $\\alpha_{b}$ are block dummy variables and $X_{v g}$ are village level controls. For high-spillover public goods, measured by the GP activism index, the analysis finds this index is on average 0.04 points higher in the Pradhan's village. This supports the assumption that $\\mu$ exceeds 1/2, indicating the Pradhan's agenda-setting power. The reservation status of the GP does not affect village-level provision of high-spillover goods, consistent with their spillover characteristics.\n\nFor low-spillover public goods targeted towards SC/ST, the evidence shows political reservation affects within-village allocation but not overall village allocation. This creates a distinction between the mechanisms governing different types of public goods provision.",
    "category": "INCORRECT",
    "question_context_used": "The empirical model for village level regressions is given by:\n\n$$\ny_{v g}=\\alpha_{b}+\\gamma_{1}R_{g}+\\gamma_{2}P_{v g}+\\gamma_{3}G_{v g}+\\gamma_{4}(R_{g}\\times P_{v g})+\\phi X_{v g}+\\varepsilon_{v g}\n$$\n\nwhere $\\alpha_{b}$ are block dummy variables and $X_{v g}$ are village level controls. For high-spillover public goods, measured by the GP activism index, the analysis finds this index is on average 0.04 points higher in the Pradhan's village. This supports the assumption that $\\mu$ exceeds 1/2, indicating the Pradhan's agenda-setting power. The reservation status of the GP does not affect village-level provision of high-spillover goods, consistent with their spillover characteristics.\n\nFor low-spillover public goods targeted towards SC/ST, the evidence shows political reservation affects within-village allocation but not overall village allocation. This creates a distinction between the mechanisms governing different types of public goods provision.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2422",
    "question": "In the context of oversight authorities and reform approval, which of the following conditions are necessary for a friendly authority to be detrimental to the approval of reforms?",
    "options": [
      "A) The approval threshold is relatively low and the marginal cost of effort is relatively flat",
      "B) The approval threshold is relatively high and the marginal cost of effort is steep",
      "C) The politician's effort is observable and the approval threshold is below 1/2",
      "D) The authority's reporting strategy is fixed and the politician's effort is unobservable"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "The-perils-of-friendly-oversight_2022_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The paper examines how the agenda of oversight authorities affects the likelihood of approval of proposals. A friendly authority, which wants the reform to be approved, can be detrimental when two conditions hold: (1) the approval threshold is relatively low, and (2) the marginal cost of effort is relatively flat. These conditions ensure that the favorable effect of the friendly authority’s reporting strategy does not prevail, making the unfriendly authority more beneficial for approval. Additionally, when the politician's effort is observable and the approval threshold is below 1/2, the party always prefers the friendly authority, unlike the case of unobservable effort. The key formula for the friendly authority's reporting strategy is: \n\n$$(\\mu_{0}(e),\\mu_{1}(e))=\\left(\\frac{1-q}{q}\\frac{e}{1-e},1\\right)$$ \n\nfor $e\\in[0,q]$.",
    "category": "INCORRECT",
    "question_context_used": "The paper examines how the agenda of oversight authorities affects the likelihood of approval of proposals. A friendly authority, which wants the reform to be approved, can be detrimental when two conditions hold: (1) the approval threshold is relatively low, and (2) the marginal cost of effort is relatively flat. These conditions ensure that the favorable effect of the friendly authority’s reporting strategy does not prevail, making the unfriendly authority more beneficial for approval. Additionally, when the politician's effort is observable and the approval threshold is below 1/2, the party always prefers the friendly authority, unlike the case of unobservable effort. The key formula for the friendly authority's reporting strategy is: \n\n$$(\\mu_{0}(e),\\mu_{1}(e))=\\left(\\frac{1-q}{q}\\frac{e}{1-e},1\\right)$$ \n\nfor $e\\in[0,q]$.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1066",
    "question": "In the context of counterfeit-product trade as analyzed by Grossman and Shapiro (1988), which of the following factors are critical in determining the equilibrium market share of counterfeit goods?",
    "options": [
      "A) The relative cost differential between producing genuine and counterfeit goods",
      "B) The effectiveness of consumer detection technologies",
      "C) The absolute price level of genuine goods in the market",
      "D) The degree of risk aversion among counterfeit producers"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Grossman-CounterfeitProductTrade-1988.md"
    ],
    "question_context": "Grossman and Shapiro (1988) analyze the market for counterfeit products, focusing on the interplay between consumer detection capabilities and production costs. The equilibrium market share of counterfeit goods is influenced by the cost advantage counterfeiters have over genuine producers, as well as the likelihood that consumers can distinguish between genuine and fake products. Key formula: The profit function for counterfeiters depends on the probability of detection, which is a function of consumer vigilance and technological aids. The model suggests that even small changes in detection probabilities can lead to significant shifts in counterfeit market penetration.",
    "category": "CORRECT",
    "question_context_used": "Grossman and Shapiro (1988) analyze the market for counterfeit products, focusing on the interplay between consumer detection capabilities and production costs. The equilibrium market share of counterfeit goods is influenced by the cost advantage counterfeiters have over genuine producers, as well as the likelihood that consumers can distinguish between genuine and fake products. Key formula: The profit function for counterfeiters depends on the probability of detection, which is a function of consumer vigilance and technological aids. The model suggests that even small changes in detection probabilities can lead to significant shifts in counterfeit market penetration.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3793",
    "question": "In the context of disentangling net earthquake effects from structural change effects, which of the following statements are correct regarding the selection of $T_{2}$?",
    "options": [
      "A) $T_{2}$ should be chosen as early as possible to maximize the number of post-$T_{2}$ observations for reliable approximation.",
      "B) $T_{2}$ should be chosen such that the earthquake effects are no longer present in the observed $y_{1t}$ from $T_{2}+1$ onwards.",
      "C) The choice of $T_{2}$ critically affects the estimated net earthquake and structural change effects.",
      "D) $T_{2}$ should be set to the year of the earthquake to isolate pure earthquake effects."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Disentangling-the-effects-of-multiple-treatments-Measuring-t_2015_Journal-of.md"
    ],
    "question_context": "The selection of $T_{2}$ is based on two considerations: (i) The observed value of $y_{1t}$ from $T_{2}+1$ onwards no longer contains the earthquake effect; (ii) There are reasonably large number of post-$T_{2}$ observations to get a reliable approximation of $E(\\hat{y}_{1t}^{2}\\mid\\tilde{y}_{t})$ for $t$ from $T_{2}+1$ onwards. Consideration of (i) favors pushing $T_{2}$ far ahead from the quake year 1995. Consideration of (ii) favors pushing $T_{2}$ as close to 1995 as possible. The separation of the quake effects and structural break effects depends critically on the choice of $T_{2}$. For instance, setting $T_{2}=2000$ yields negligible net quake effects post-2000, supporting the hypothesis that earthquake effects are transitory.",
    "category": "CORRECT",
    "question_context_used": "The selection of $T_{2}$ is based on two considerations: (i) The observed value of $y_{1t}$ from $T_{2}+1$ onwards no longer contains the earthquake effect; (ii) There are reasonably large number of post-$T_{2}$ observations to get a reliable approximation of $E(\\hat{y}_{1t}^{2}\\mid\\tilde{y}_{t})$ for $t$ from $T_{2}+1$ onwards. Consideration of (i) favors pushing $T_{2}$ far ahead from the quake year 1995. Consideration of (ii) favors pushing $T_{2}$ as close to 1995 as possible. The separation of the quake effects and structural break effects depends critically on the choice of $T_{2}$. For instance, setting $T_{2}=2000$ yields negligible net quake effects post-2000, supporting the hypothesis that earthquake effects are transitory.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2585",
    "question": "Based on the empirical analysis of intrafirm trade determinants, which of the following statements are supported by the findings in Table 1?",
    "options": [
      "A) Higher revealed product contractibility is associated with less intrafirm trade on both extensive and intensive margins.",
      "B) Improved governance quality uniformly increases the share of intrafirm trade across all product categories.",
      "C) Industry capital intensity and country capital abundance interact positively to increase intrafirm trade shares.",
      "D) Greater country skill abundance reduces the share of intrafirm trade, especially for more skill-intensive products."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Bernard-IntrafirmTradeProduct-2010.md"
    ],
    "question_context": "The empirical specification regresses measures of intrafirm trade $(I F_{p c})$ on product characteristics $(X_{p})$, country characteristics $(Z_{c})$, and interactions $(X_{p} \\times Z_{c})$:\n\n$$\nI F_{p c}=\\theta+\\alpha X_{p}+\\beta Z_{c}+\\gamma(X_{p}Z_{c})+\\epsilon_{p c}.\n$$\n\nKey findings include: Higher revealed product contractibility reduces intrafirm trade (columns 1-2). Governance quality increases the probability of affiliate presence but decreases trade intensity (columns 1 vs 2). The interaction between industry capital intensity and country capital abundance has a positive coefficient (column 4), while skill intensity and abundance show complex negative interactions (column 4).",
    "category": "INCORRECT",
    "question_context_used": "The empirical specification regresses measures of intrafirm trade $(I F_{p c})$ on product characteristics $(X_{p})$, country characteristics $(Z_{c})$, and interactions $(X_{p} \\times Z_{c})$:\n\n$$\nI F_{p c}=\\theta+\\alpha X_{p}+\\beta Z_{c}+\\gamma(X_{p}Z_{c})+\\epsilon_{p c}.\n$$\n\nKey findings include: Higher revealed product contractibility reduces intrafirm trade (columns 1-2). Governance quality increases the probability of affiliate presence but decreases trade intensity (columns 1 vs 2). The interaction between industry capital intensity and country capital abundance has a positive coefficient (column 4), while skill intensity and abundance show complex negative interactions (column 4).",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4408",
    "question": "Based on the empirical evidence from Tables 1-3, which of the following statements accurately describe the effects of Zero Tolerance (ZT) laws and Minimum Legal Drinking Age (MLDA) policies on arrest ratios?",
    "options": [
      "A) ZT laws increased DUI arrest ratios for 18-20-year-olds but had no significant effect on 22-24-year-olds",
      "B) MLDA increases to age 21 reduced vandalism and disorderly conduct arrests for 18-20-year-olds at the 5% significance level",
      "C) ZT laws reduced public drunkenness and disorderly conduct arrest ratios for 18-20-year-olds by approximately 5%",
      "D) MLDA policies had a statistically significant effect on DUI arrest ratios for 22-24-year-olds"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Carpenter-HeavyAlcoholUse-2005.md"
    ],
    "question_context": "Table 1 indicates that ZT laws had a positive, significant, and direct effect on arrests of 18-20-year-olds for DUI, with a reduction of about 5 percent relative to its pre-reform mean. The Zero Tolerance coefficient was 0.004* (standard error 0.002), statistically significant at the 5-percent level, while showing no effect for 22-24-year-olds. Table 2 shows ZT laws reduced 18-20-year-old arrest ratios for public drunkenness/disorderly conduct by 5% (coefficient -0.009*, SE 0.004) and vandalism by 4.5% (coefficient -0.014*, SE 0.006), again with no effects for older age groups. Table 3 replicates Joksch and Jones (1993), showing MLDA increases to age 21 reduced vandalism arrests by 3.2% (coefficient -0.012t, SE 0.006) and disorderly conduct by 3.4% (coefficient -0.009t, SE 0.005), significant at the 10-percent level.",
    "category": "CORRECT",
    "question_context_used": "Table 1 indicates that ZT laws had a positive, significant, and direct effect on arrests of 18-20-year-olds for DUI, with a reduction of about 5 percent relative to its pre-reform mean. The Zero Tolerance coefficient was 0.004* (standard error 0.002), statistically significant at the 5-percent level, while showing no effect for 22-24-year-olds. Table 2 shows ZT laws reduced 18-20-year-old arrest ratios for public drunkenness/disorderly conduct by 5% (coefficient -0.009*, SE 0.004) and vandalism by 4.5% (coefficient -0.014*, SE 0.006), again with no effects for older age groups. Table 3 replicates Joksch and Jones (1993), showing MLDA increases to age 21 reduced vandalism arrests by 3.2% (coefficient -0.012t, SE 0.006) and disorderly conduct by 3.4% (coefficient -0.009t, SE 0.005), significant at the 10-percent level.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-469",
    "question": "Under Assumption 3, which of the following properties must hold for an interim-optimal judicial mechanism $(F,C,S)$ in $\\mathcal{M}$?",
    "options": [
      "A) The guilty defendant's sentence is deterministic and does not exceed $\\hat{s}$.",
      "B) The innocent defendant's sentence is a step function of the signal $t$, jumping from zero to $\\overline{s}$ at some cutoff $\\bar{t}$.",
      "C) The guilty defendant's sentence has a two-point support lying in $[0,\\hat{s}]$.",
      "D) The guilty defendant is indifferent between reporting truthfully and misreporting."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Judicial Mechanism Design.md"
    ],
    "question_context": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. A judicial mechanism $(F,C,S)$ in $\\mathcal{M}$ is interim optimal if, given the prior probability $\\lambda$ that the defendant is guilty, the mechanism maximizes interim welfare (3) among all the mechanisms in $\\mathcal{M}$. Studying interim-optimal mechanisms allows us to disentangle deterrence from other welfare considerations and makes the arguments in the proof easier to follow. Under Assumption 3, the functions $W(\\cdot,g)$ and $u(\\cdot)$ are concave, and at least one is strictly concave. Function $W(\\cdot,g)$ has a unique maximizer, denoted $\\hat{s}$. Then, the guilty defendant's sentence is deterministic and does not exceed $\\hat{s}$. Any mechanism that violates this property is strictly welfare dominated for all nondegenerate priors $\\lambda$ by a single mechanism with these properties.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. A judicial mechanism $(F,C,S)$ in $\\mathcal{M}$ is interim optimal if, given the prior probability $\\lambda$ that the defendant is guilty, the mechanism maximizes interim welfare (3) among all the mechanisms in $\\mathcal{M}$. Studying interim-optimal mechanisms allows us to disentangle deterrence from other welfare considerations and makes the arguments in the proof easier to follow. Under Assumption 3, the functions $W(\\cdot,g)$ and $u(\\cdot)$ are concave, and at least one is strictly concave. Function $W(\\cdot,g)$ has a unique maximizer, denoted $\\hat{s}$. Then, the guilty defendant's sentence is deterministic and does not exceed $\\hat{s}$. Any mechanism that violates this property is strictly welfare dominated for all nondegenerate priors $\\lambda$ by a single mechanism with these properties.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2048",
    "question": "In the Brander-Lewis limited-liability model, under which conditions does a unilateral increase in debt lead to a reduction in the leveraged firm's scale and an increase in the rival's scale?",
    "options": [
      "A) When the strategic variable is quantity and marginal profit is higher in better states",
      "B) When the strategic variable is price and marginal return with respect to price is higher in better states",
      "C) When the strategic variable is quantity and marginal return to capacity is lower in good states",
      "D) When the strategic variable is price and marginal return with respect to price is lower in better states"
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Kovenock-CapitalStructureProductMarket-1995.md"
    ],
    "question_context": "The Brander and Lewis (1986) limited-liability model showed that a firm's capital structure may serve as a credible precommitment in affecting strategic interaction between firms. They consider a two-stage game in which debt levels are simultaneously set in the first stage to maximize firm value and quantity is chosen simultaneously in the second stage to maximize the return to equity. In the second stage, demand (or some other profit-relevant variable) is still uncertain, so output choice affects the probability of default. Due to the limited liability of equity, a unilateral increase in debt leads to an output strategy that raises returns in good states and lowers returns in bad states.\n\nLimited liability has a different effect if the strategies available to firms are strategic complements. Suppose that the strategic variable is price and that the marginal return with respect to price is higher for states in which the total return is higher (as would be the case with demand-intercept, but not unit-cost, shocks). Starting from a position of zero debt, a small increase in debt by one firm causes that firm to increase its price best-response for each price chosen by the rival. If the rival firm maintains a zero debt level, equilibrium in the price-setting game will involve higher profits and prices for both firms. At the resulting prices the quantity produced by the leveraged firm is lower than the pre-debt level, while the quantity of the unleveraged rival is higher. Interpreting quantity as capacity, this represents a reduction in the leveraged firm's scale and an increase in the rival's scale.",
    "category": "INCORRECT",
    "question_context_used": "The Brander and Lewis (1986) limited-liability model showed that a firm's capital structure may serve as a credible precommitment in affecting strategic interaction between firms. They consider a two-stage game in which debt levels are simultaneously set in the first stage to maximize firm value and quantity is chosen simultaneously in the second stage to maximize the return to equity. In the second stage, demand (or some other profit-relevant variable) is still uncertain, so output choice affects the probability of default. Due to the limited liability of equity, a unilateral increase in debt leads to an output strategy that raises returns in good states and lowers returns in bad states.\n\nLimited liability has a different effect if the strategies available to firms are strategic complements. Suppose that the strategic variable is price and that the marginal return with respect to price is higher for states in which the total return is higher (as would be the case with demand-intercept, but not unit-cost, shocks). Starting from a position of zero debt, a small increase in debt by one firm causes that firm to increase its price best-response for each price chosen by the rival. If the rival firm maintains a zero debt level, equilibrium in the price-setting game will involve higher profits and prices for both firms. At the resulting prices the quantity produced by the leveraged firm is lower than the pre-debt level, while the quantity of the unleveraged rival is higher. Interpreting quantity as capacity, this represents a reduction in the leveraged firm's scale and an increase in the rival's scale.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"D\"]}",
      "model_answer": [
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3686",
    "question": "In the extended principal-agent game with two-sided moral hazard, which of the following conditions are necessary for the principal's maximum expected payoff $\\hat{V}_{1}$ to be strictly less than the commitment payoff $V^{+}$?",
    "options": [
      "A) The outcome function $\\pi(a,b)$ has full support for every $a, b$ and $y^{m}$.",
      "B) The agent's best response to $(s^{+}, \\alpha^{+})$ is not single-valued.",
      "C) The principal can commit to a given action $a$, transforming the problem into a classical principal-agent problem.",
      "D) There exists a contract $s_{\\varepsilon}^{+}$ arbitrarily close to $s^{+}$ such that $b^{+}$ is the unique best response to $(s_{\\varepsilon}^{+}, a^{+})$."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Incentive-Contracts-in-Two-Sided-Moral-Hazards-with_1997_Journal-of-Economic.md"
    ],
    "question_context": "The principal and the agent have finite action sets $A$ and $B$ with generic elements denoted $a$ and $b$, respectively. A principal's mixed strategy is a probability distribution $\\alpha\\in\\mathcal{A}_{\\mathfrak{A}}$, while an agent's mixed strategy is $\\beta\\in\\varDelta\\v{\\theta}_{B}$. The outcome $y$ is a random variable whose distribution is determined by the actions of the principal and the agent, given by the outcome function $\\pi\\colon A\\times B\\to\\varDelta_{Y}$. The standard assumption A1 requires that $\\pi(a,b)$ has full support, i.e., $\\pi_{m}(a,b)>0$ for every $a,b$ and $y^{m}$. The principal's problem is formulated as maximizing the expected payoff $V(s, \\alpha, \\beta)$ subject to equilibrium constraints. The commitment payoff $V^{+}$ is achieved when the principal's incentive constraint is removed, and it is assumed that $V^{+} > \\hat{V}_{1}$ (A2). Additionally, A3 guarantees the existence of contracts $s_{\\varepsilon}^{+}$ arbitrarily close to $s^{+}$ with $b^{+}$ as the unique best response.",
    "category": "INCORRECT",
    "question_context_used": "The principal and the agent have finite action sets $A$ and $B$ with generic elements denoted $a$ and $b$, respectively. A principal's mixed strategy is a probability distribution $\\alpha\\in\\mathcal{A}_{\\mathfrak{A}}$, while an agent's mixed strategy is $\\beta\\in\\varDelta\\v{\\theta}_{B}$. The outcome $y$ is a random variable whose distribution is determined by the actions of the principal and the agent, given by the outcome function $\\pi\\colon A\\times B\\to\\varDelta_{Y}$. The standard assumption A1 requires that $\\pi(a,b)$ has full support, i.e., $\\pi_{m}(a,b)>0$ for every $a,b$ and $y^{m}$. The principal's problem is formulated as maximizing the expected payoff $V(s, \\alpha, \\beta)$ subject to equilibrium constraints. The commitment payoff $V^{+}$ is achieved when the principal's incentive constraint is removed, and it is assumed that $V^{+} > \\hat{V}_{1}$ (A2). Additionally, A3 guarantees the existence of contracts $s_{\\varepsilon}^{+}$ arbitrarily close to $s^{+}$ with $b^{+}$ as the unique best response.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-878",
    "question": "In the context of auction theory and information acquisition, which of the following statements are supported by the analysis of risk-sensitivity and statistical structure?",
    "options": [
      "A) First price auctions induce stronger incentives for information acquisition than second price auctions when statistical dependence is present",
      "B) All independent private-values auction mechanisms provide identical incentives for information acquisition",
      "C) The value of information is determined by the degree of risk-sensitivity of the payoff function",
      "D) In Tan's R&D model, first and second price auctions are equivalent due to the independence assumption in cost reduction"
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "Persico-InformationAcquisitionAuctions-2000.md"
    ],
    "question_context": "The paper analyzes the marginal value of information in decision problems where the payoff function exhibits the single-crossing property and the statistical structure is affiliated. A key finding shows that the value of information is determined by the degree of risk-sensitivity of the payoff. This leads to the conclusion that first price auctions are more risk-sensitive than second price auctions, making information more valuable in the former due to the ability to reduce money left on the table. The framework also applies to R&D modeling, where the analysis reveals that first price auctions induce more investment than second price when the independence assumption in cost reduction is relaxed. The mathematical foundation includes Lemma 1, which establishes comparative statics results for setups with monotone likelihood ratio and single-crossing property: \n\n$$\\int_{c}^{d}H(v)J(v)d\\mu(v)\\geq0$$\n\nwhere $H$ is quasi-monotone and $J$ is nondecreasing.",
    "category": "INCORRECT",
    "question_context_used": "The paper analyzes the marginal value of information in decision problems where the payoff function exhibits the single-crossing property and the statistical structure is affiliated. A key finding shows that the value of information is determined by the degree of risk-sensitivity of the payoff. This leads to the conclusion that first price auctions are more risk-sensitive than second price auctions, making information more valuable in the former due to the ability to reduce money left on the table. The framework also applies to R&D modeling, where the analysis reveals that first price auctions induce more investment than second price when the independence assumption in cost reduction is relaxed. The mathematical foundation includes Lemma 1, which establishes comparative statics results for setups with monotone likelihood ratio and single-crossing property: \n\n$$\\int_{c}^{d}H(v)J(v)d\\mu(v)\\geq0$$\n\nwhere $H$ is quasi-monotone and $J$ is nondecreasing.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3967",
    "question": "In the context of Bayesian mixture models with MCMC sampling, which of the following strategies are effective for addressing the label-switching problem?",
    "options": [
      "A) Imposing identifiability constraints on cluster parameters",
      "B) Using decision-theoretical criteria to relabel clusters post-sampling",
      "C) Employing perfect sampling schemes for all parameter families",
      "D) Applying random permutation schemes based on posterior structure"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "04-Model-Based_Clustering_of_Non-Gaussian_Panel_Data_Based_on_Skew-t_Distributions.md"
    ],
    "question_context": "The label-switching problem in Bayesian mixture models arises due to the multimodality of the posterior distribution, which is invariant under permutation of cluster labels. Key approaches to address this include: (1) Diebolt and Robert (1994)'s identifiability constraints, (2) Celeux, Hurn, and Robert (2000) and Stephens (2000b)'s decision-theoretical criteria, and (3) Frühwirth-Schnatter (2001)'s random permutation scheme. The formula for posterior invariance is given by $p(\\theta|y) = p(\\sigma(\\theta)|y)$ for any permutation $\\sigma$ of component labels. Perfect sampling (Casella et al. 2002) is not generally applicable outside exponential families, making option C incorrect. The text emphasizes that meaningful cluster differentiation should be based on dynamics ($\\pmb{\\alpha}$), long-run behavior ($\\pmb{\\beta}$), or covariate effects ($M$), not mixing weights $\\eta_j$.",
    "category": "CORRECT",
    "question_context_used": "The label-switching problem in Bayesian mixture models arises due to the multimodality of the posterior distribution, which is invariant under permutation of cluster labels. Key approaches to address this include: (1) Diebolt and Robert (1994)'s identifiability constraints, (2) Celeux, Hurn, and Robert (2000) and Stephens (2000b)'s decision-theoretical criteria, and (3) Frühwirth-Schnatter (2001)'s random permutation scheme. The formula for posterior invariance is given by $p(\\theta|y) = p(\\sigma(\\theta)|y)$ for any permutation $\\sigma$ of component labels. Perfect sampling (Casella et al. 2002) is not generally applicable outside exponential families, making option C incorrect. The text emphasizes that meaningful cluster differentiation should be based on dynamics ($\\pmb{\\alpha}$), long-run behavior ($\\pmb{\\beta}$), or covariate effects ($M$), not mixing weights $\\eta_j$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2745",
    "question": "Which of the following statements are true regarding the properties and applications of the smoothed maximum score (SMS) estimator in binary response models?",
    "options": [
      "A) The SMS estimator's dependence on a bandwidth parameter complicates its finite sample performance.",
      "B) Bootstrap methods provide improved finite sample critical values for SMS-based tests.",
      "C) First-order asymptotic theory accurately describes the finite sample distribution of SMS test statistics.",
      "D) The SMS estimator is invariant to the choice of bandwidth parameter under standard conditions."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Finite-sample-and-asymptotic-methods-in-econometr_2002_Journal-of-Econometri.md"
    ],
    "question_context": "The focus of the paper by Joel Horowitz is the smoothed maximum score (SMS) estimator for binary response models. The nominal distribution obtained from first-order asymptotic theory for $t-$ - and $\\chi^{2}$ -type test statistics based on the SMS estimator has been shown in a number of Monte Carlo studies to differ substantially from that of the statistics in finite samples. This paper demonstrates that the bootstrap offers improved finite sample critical values for tests based on the SMS estimator. The difficulty with the SMS estimator and associated statistics is their dependence on a bandwidth parameter. The method of proof establishes that the distributions of the test statistics and their bootstrap analogues admit identical and valid Edgeworth expansions under slightly stronger conditions than those required for the first-order asymptotic theory. A small set of simulation experiments confirms the efficacy of the bootstrap method.",
    "category": "CORRECT",
    "question_context_used": "The focus of the paper by Joel Horowitz is the smoothed maximum score (SMS) estimator for binary response models. The nominal distribution obtained from first-order asymptotic theory for $t-$ - and $\\chi^{2}$ -type test statistics based on the SMS estimator has been shown in a number of Monte Carlo studies to differ substantially from that of the statistics in finite samples. This paper demonstrates that the bootstrap offers improved finite sample critical values for tests based on the SMS estimator. The difficulty with the SMS estimator and associated statistics is their dependence on a bandwidth parameter. The method of proof establishes that the distributions of the test statistics and their bootstrap analogues admit identical and valid Edgeworth expansions under slightly stronger conditions than those required for the first-order asymptotic theory. A small set of simulation experiments confirms the efficacy of the bootstrap method.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3065",
    "question": "In the context of the US branching deregulations between 1994 and 2005, which of the following statements accurately describe the causal effects on mortgage credit and house prices?",
    "options": [
      "A) Deregulation led to an expansion in mortgage credit primarily due to geographic diversification and lower cost of funds for banks.",
      "B) House prices increased uniformly across all areas regardless of housing supply elasticity.",
      "C) The credit expansion induced by deregulation had no significant effect on house prices in areas with elastic housing supply.",
      "D) Deregulation affected mortgage credit supply but had no causal impact on house prices due to reverse causality."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Favara-CreditSupplyPrice-2015.md"
    ],
    "question_context": "The paper identifies exogenous shifts in the supply of credit using regulatory changes to bank branching across US states. Key findings include that deregulation enabled banks to diversify deposit collection across locations, lowering the cost of funds and expanding credit supply. This led to increased housing demand, with house prices rising more in areas with inelastic housing supply, while the housing stock increased in elastic areas. The framework uses two control groups to establish causality: lenders unaffected by deregulation due to legal status (IMCs and TCUs) and banks unaffected due to location (in-state banks). The differential responses rule out demand-based explanations, confirming a credit supply shock. The instrumental variable approach shows that a 1% change in instrumented credit increases house price growth by 0.2%, peaking after two years and reverting to zero after five to six years.",
    "category": "CORRECT",
    "question_context_used": "The paper identifies exogenous shifts in the supply of credit using regulatory changes to bank branching across US states. Key findings include that deregulation enabled banks to diversify deposit collection across locations, lowering the cost of funds and expanding credit supply. This led to increased housing demand, with house prices rising more in areas with inelastic housing supply, while the housing stock increased in elastic areas. The framework uses two control groups to establish causality: lenders unaffected by deregulation due to legal status (IMCs and TCUs) and banks unaffected due to location (in-state banks). The differential responses rule out demand-based explanations, confirming a credit supply shock. The instrumental variable approach shows that a 1% change in instrumented credit increases house price growth by 0.2%, peaking after two years and reverting to zero after five to six years.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2985",
    "question": "Under the conditions of Strong Richness and Responsiveness, which of the following statements are true regarding the optimality of linear contracts in the principal-agent model?",
    "options": [
      "A) If Responsiveness is violated between two contracts where one is grounded and not scaling-dominated, a nonlinear contract can outperform all linear contracts for some valuation.",
      "B) Scaling Monotonicity ensures that condition (ii) of Proposition 3 cannot arise, simplifying the characterization of scaling-dominated contracts.",
      "C) The existence of an optimal contract is guaranteed if the correspondence $\\widetilde{\\Phi}$ is lower hemicontinuous, and the maximum is attained by a linear contract.",
      "D) Strong Richness alone is sufficient to ensure that linear contracts are optimal for any valuation, without requiring Responsiveness."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "ecta17386.md"
    ],
    "question_context": "Key formula: The principal’s guarantee is defined as $$V_{P}(w|v)=\\operatorname*{inf}_{F\\in\\Phi(w)}\\mathbb{E}_{F}\\big[v(z)-w(z)\\big].$$ Under Strong Richness and Responsiveness, Theorem 1 implies that for any contract $w$, there is a linear contract $w'$ such that $V_{P}(w'|v)\\geq V_{P}(w|v)$. However, if Responsiveness is violated between contracts $w,w'$ where $w'$ is grounded and not scaling-dominated, Theorem 4 shows that a valuation $v$ can be chosen so that $w$ outperforms all linear contracts. Proposition 3 characterizes scaling-dominated contracts through five conditions, which simplify under Scaling Monotonicity. Proposition 2 states that if $\\widetilde{\\Phi}$ is lower hemicontinuous, an optimal contract exists and is linear.",
    "category": "CORRECT",
    "question_context_used": "Key formula: The principal’s guarantee is defined as $$V_{P}(w|v)=\\operatorname*{inf}_{F\\in\\Phi(w)}\\mathbb{E}_{F}\\big[v(z)-w(z)\\big].$$ Under Strong Richness and Responsiveness, Theorem 1 implies that for any contract $w$, there is a linear contract $w'$ such that $V_{P}(w'|v)\\geq V_{P}(w|v)$. However, if Responsiveness is violated between contracts $w,w'$ where $w'$ is grounded and not scaling-dominated, Theorem 4 shows that a valuation $v$ can be chosen so that $w$ outperforms all linear contracts. Proposition 3 characterizes scaling-dominated contracts through five conditions, which simplify under Scaling Monotonicity. Proposition 2 states that if $\\widetilde{\\Phi}$ is lower hemicontinuous, an optimal contract exists and is linear.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-195",
    "question": "In the context of ambiguity aversion and optimal learning in consumption/savings models, which of the following statements are supported by the key findings of Dew-Becker and Nathanson (2019)?",
    "options": [
      "A) Agents optimally learn about low-frequency income dynamics to mitigate the most severe effects of ambiguity aversion.",
      "B) Endogenous learning completely eliminates overextrapolation of income shocks when information costs are equal across frequencies.",
      "C) High-frequency consumption mistakes arise because agents allocate minimal attention to transitory income variations.",
      "D) The model predicts underreaction of consumption to predictable short-run income variation due to excessive focus on long-run dynamics."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Directed-attention-and-nonparametric-learning_2019_Journal-of-Economic-Theor.md"
    ],
    "question_context": "The paper examines how agents' endogenous learning about income dynamics interacts with ambiguity aversion in a consumption/savings framework. Key formula: The optimal information policy when information costs are equal across frequencies is given by $\\tau^{*} \\propto Z$, where $Z(\\omega_j)$ represents utility weights. Agents focus learning on low-frequency income dynamics because these have the largest impact on utility through ambiguity aversion. This leads to two main results: 1) In the benchmark case with equal information costs, learning perfectly cancels out overextrapolation of income shocks (contrasting with prior ambiguity models like Hansen and Sargent (2010)), and 2) Agents make quantitatively realistic high-frequency consumption mistakes by overreacting to transitory income variations, as captured by the positive correlation between consumption growth and predictable income growth at short horizons. The spectral density of consumption growth under the optimal policy shows excess sensitivity at middle frequencies, violating the permanent income hypothesis but matching empirical evidence from Jappelli and Pistaferri (2010).",
    "category": "INCORRECT",
    "question_context_used": "The paper examines how agents' endogenous learning about income dynamics interacts with ambiguity aversion in a consumption/savings framework. Key formula: The optimal information policy when information costs are equal across frequencies is given by $\\tau^{*} \\propto Z$, where $Z(\\omega_j)$ represents utility weights. Agents focus learning on low-frequency income dynamics because these have the largest impact on utility through ambiguity aversion. This leads to two main results: 1) In the benchmark case with equal information costs, learning perfectly cancels out overextrapolation of income shocks (contrasting with prior ambiguity models like Hansen and Sargent (2010)), and 2) Agents make quantitatively realistic high-frequency consumption mistakes by overreacting to transitory income variations, as captured by the positive correlation between consumption growth and predictable income growth at short horizons. The spectral density of consumption growth under the optimal policy shows excess sensitivity at middle frequencies, violating the permanent income hypothesis but matching empirical evidence from Jappelli and Pistaferri (2010).",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2410",
    "question": "In the context of consumer search and firm investment in quality, which of the following statements are correct regarding the impact of an increase in quality investment on consumer search behavior?",
    "options": [
      "A) The number of searches always increases with quality investment due to the choosiness effect.",
      "B) The number of searches may decrease if the direct effect dominates the choosiness effect.",
      "C) In the additive case, the number of searches remains constant with changes in quality investment.",
      "D) The decreasing MRL ordering ensures that the number of searches weakly decreases with quality investment."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Product Quality and Consumer Search.md"
    ],
    "question_context": "The model features a unit mass of consumers and sellers, with firms investing in quality to improve product match values. Consumers search sequentially, incurring a cost $c$ per search, and hold correct beliefs about equilibrium prices and quality investments. The utility of consumer $m$ from product $i$ is given by $u_{i}^{m}=\\varepsilon_{i}^{m}-p_{i}$, where $\\varepsilon_{i}^{m}$ is the match value distributed according to $F(\\varepsilon;\\lambda_{i})$. Quality investment $\\lambda_{i}$ shifts this distribution in the sense of FOSD. The reservation value $\\hat{\\varepsilon}$ is determined by $h(\\varepsilon;\\lambda)\\equiv\\int_{\\varepsilon}^{\\overline{\\varepsilon}(\\lambda)}\\bigl(1-F(z;\\lambda)\\bigr)d z=c$. The number of searches $S(\\hat{\\varepsilon};\\lambda)\\equiv\\frac{1}{1-F(\\hat{\\varepsilon};\\lambda)}$ depends on the balance between the direct effect and the choosiness effect of quality investment. Proposition 1 states that the number of searches increases (decreases) if $\\frac{\\partial\\big(1-F\\big(\\hat{\\varepsilon};\\lambda\\big)\\big)}{\\partial\\lambda}<(>)\\frac{\\int_{\\hat{\\varepsilon}}\\frac{\\partial\\big(1-F\\big(z;\\lambda\\big)\\big)}{\\partial\\lambda}d z}{1-F\\big(\\hat{\\varepsilon};\\lambda\\big)}$. The additive case leaves $S$ constant, while the multiplicative case increases it. Proposition 2 links the decreasing MRL ordering to a weakly decreasing number of searches.",
    "category": "CORRECT",
    "question_context_used": "The model features a unit mass of consumers and sellers, with firms investing in quality to improve product match values. Consumers search sequentially, incurring a cost $c$ per search, and hold correct beliefs about equilibrium prices and quality investments. The utility of consumer $m$ from product $i$ is given by $u_{i}^{m}=\\varepsilon_{i}^{m}-p_{i}$, where $\\varepsilon_{i}^{m}$ is the match value distributed according to $F(\\varepsilon;\\lambda_{i})$. Quality investment $\\lambda_{i}$ shifts this distribution in the sense of FOSD. The reservation value $\\hat{\\varepsilon}$ is determined by $h(\\varepsilon;\\lambda)\\equiv\\int_{\\varepsilon}^{\\overline{\\varepsilon}(\\lambda)}\\bigl(1-F(z;\\lambda)\\bigr)d z=c$. The number of searches $S(\\hat{\\varepsilon};\\lambda)\\equiv\\frac{1}{1-F(\\hat{\\varepsilon};\\lambda)}$ depends on the balance between the direct effect and the choosiness effect of quality investment. Proposition 1 states that the number of searches increases (decreases) if $\\frac{\\partial\\big(1-F\\big(\\hat{\\varepsilon};\\lambda\\big)\\big)}{\\partial\\lambda}<(>)\\frac{\\int_{\\hat{\\varepsilon}}\\frac{\\partial\\big(1-F\\big(z;\\lambda\\big)\\big)}{\\partial\\lambda}d z}{1-F\\big(\\hat{\\varepsilon};\\lambda\\big)}$. The additive case leaves $S$ constant, while the multiplicative case increases it. Proposition 2 links the decreasing MRL ordering to a weakly decreasing number of searches.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-530",
    "question": "Based on the international evidence on Downward Nominal Wage Rigidity (DNWR), which of the following statements are supported by the data in Tables 4 and 5?",
    "options": [
      "A) Mexico exhibits higher DNWR than the United States, Canada, and Australia.",
      "B) The Kahn test statistic 'n' is consistently negative across all countries except for one subgroup in Mexico.",
      "C) The percentage of rigid wage observations in Mexico is lower than in developed countries but higher than previously estimated.",
      "D) Nominal wage cuts are more prevalent in Mexico than in the United States, Canada, and Australia."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Nominal-wage-rigidities-in-Mexico--evidence-from-_2004_Journal-of-Developmen.md"
    ],
    "question_context": "The estimated results suggest there is less rigidity in Mexico than in other developed countries such as the United States, Canada, and Australia, but much more rigidity than the previously obtained estimates for Mexico suggest. Key statistics include the Kahn test parameter 'n' and the percentage of rigid wage observations. For instance, the 'n' parameter is negative in most cases (-0.47 in the United States, -0.92 in Australia) but positive (0.11) for wage earners who stay in the same job in Mexico's formal sector. The percentage of rigid wage observations in Mexico ranges from 7.60% to 9.32%, compared to 6.8% to 17.9% in developed countries. Meanwhile, nominal wage cuts are more frequent in Mexico (22.40% to 24.16%) than in developed countries (0.1% to 14.4%).",
    "category": "INCORRECT",
    "question_context_used": "The estimated results suggest there is less rigidity in Mexico than in other developed countries such as the United States, Canada, and Australia, but much more rigidity than the previously obtained estimates for Mexico suggest. Key statistics include the Kahn test parameter 'n' and the percentage of rigid wage observations. For instance, the 'n' parameter is negative in most cases (-0.47 in the United States, -0.92 in Australia) but positive (0.11) for wage earners who stay in the same job in Mexico's formal sector. The percentage of rigid wage observations in Mexico ranges from 7.60% to 9.32%, compared to 6.8% to 17.9% in developed countries. Meanwhile, nominal wage cuts are more frequent in Mexico (22.40% to 24.16%) than in developed countries (0.1% to 14.4%).",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"C\", \"D\"]\n}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4463",
    "question": "In the context of the two-stage three-player game with a monopoly supplier and two downstream Cournot duopolists, which of the following statements are true regarding price discrimination and uniform pricing?",
    "options": [
      "A) Under price discrimination, the supplier charges a higher price to the firm with the lower marginal cost.",
      "B) The optimal uniform price lies between the discriminatory prices when the supplier is constrained to charge the same price to both firms.",
      "C) Welfare is higher under price discrimination in the short run due to increased production by the low-marginal-cost producer.",
      "D) The firm with a lower marginal cost has a more inelastic demand for the input, leading to a higher price under discrimination."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "DeGraba-InputMarketPrice-1990.md"
    ],
    "question_context": "The model presents a two-stage three-player game involving a monopoly supplier and two downstream Cournot duopolists. The supplier sets input prices in stage 1, and firms compete in quantities in stage 2. Key formulas include the Nash equilibrium strategies for producers: \n\n$$ Q_{1}^{*}=\\big(a-2c_{1}-2k_{1}+c_{2}+k_{2}\\big)/3b $$\n\n$$ Q_{2}^{*}=\\big(a-2c_{2}-2k_{2}+c_{1}+k_{2}\\big)/3b $$\n\nUnder price discrimination, equilibrium input prices are:\n\n$$ k_{1}^{\\mathrm{d}*}=\\big(a-c_{1}\\big)/2 $$\n\n$$ k_{2}^{\\mathrm{d}*}=\\big(a-c_{2}\\big)/2 $$\n\nUnder uniform pricing, the equilibrium price is:\n\n$$ k^{\\mathrm{u}*}=\\big(2a-c_{1}-c_{2}\\big)/4 $$\n\nObservations indicate that the supplier charges a higher price to the firm with lower marginal cost under discrimination, and welfare is lower in the short run under discrimination.",
    "category": "INCORRECT",
    "question_context_used": "The model presents a two-stage three-player game involving a monopoly supplier and two downstream Cournot duopolists. The supplier sets input prices in stage 1, and firms compete in quantities in stage 2. Key formulas include the Nash equilibrium strategies for producers: \n\n$$ Q_{1}^{*}=\\big(a-2c_{1}-2k_{1}+c_{2}+k_{2}\\big)/3b $$\n\n$$ Q_{2}^{*}=\\big(a-2c_{2}-2k_{2}+c_{1}+k_{2}\\big)/3b $$\n\nUnder price discrimination, equilibrium input prices are:\n\n$$ k_{1}^{\\mathrm{d}*}=\\big(a-c_{1}\\big)/2 $$\n\n$$ k_{2}^{\\mathrm{d}*}=\\big(a-c_{2}\\big)/2 $$\n\nUnder uniform pricing, the equilibrium price is:\n\n$$ k^{\\mathrm{u}*}=\\big(2a-c_{1}-c_{2}\\big)/4 $$\n\nObservations indicate that the supplier charges a higher price to the firm with lower marginal cost under discrimination, and welfare is lower in the short run under discrimination.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1782",
    "question": "In the context of the New Keynesian model discussed, which of the following modifications significantly improved the model's fit to sectoral consumption data?",
    "options": [
      "A) Introduction of habit formation in sectoral consumption",
      "B) Inclusion of a time-varying inflation target",
      "C) Adoption of a more flexible specification of exogenous processes",
      "D) Incorporation of inflation indexation"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Carvalho-SectoralPriceFacts-2021.md"
    ],
    "question_context": "The baseline model, while simple, fits the data fairly well—more so with respect to the sectoral price data than the quantity data. The baseline model, however, fails to match some key second moments due to its highly stylized nature. To further analyze this issue, we estimated a version of the model with habit formation in sectoral consumption. This endows the model with an additional persistence mechanism as well as some more flexibility. Figure 11 shows that this additional feature enables the model to fit the persistence of sectoral consumption growth—and inflation—better. Furthermore, the more flexible specification also improves the model’s prediction of the standard deviations of sectoral consumption growth as shown in panel B of Figure 10. The model abstracts from features that may help explain the data, such as habit formation in consumption, inflation indexation, a time-varying inflation target, and a more flexible specification of exogenous processes.",
    "category": "INCORRECT",
    "question_context_used": "The baseline model, while simple, fits the data fairly well—more so with respect to the sectoral price data than the quantity data. The baseline model, however, fails to match some key second moments due to its highly stylized nature. To further analyze this issue, we estimated a version of the model with habit formation in sectoral consumption. This endows the model with an additional persistence mechanism as well as some more flexibility. Figure 11 shows that this additional feature enables the model to fit the persistence of sectoral consumption growth—and inflation—better. Furthermore, the more flexible specification also improves the model’s prediction of the standard deviations of sectoral consumption growth as shown in panel B of Figure 10. The model abstracts from features that may help explain the data, such as habit formation in consumption, inflation indexation, a time-varying inflation target, and a more flexible specification of exogenous processes.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2502",
    "question": "In the simplified model of monetary policy and firm dynamics, which of the following statements are true regarding the effects of stabilization policies on preset prices and firm entry?",
    "options": [
      "A) Under no stabilization policy, preset prices are increasing in the variance of productivity shocks due to Jensen’s inequality.",
      "B) A stabilization policy that sets the monetary stance proportional to productivity eliminates the covariance term in the optimal preset price equation.",
      "C) A foreign currency peg leads to a higher number of manufacturing firms in the foreign country compared to the home country.",
      "D) In the absence of international asset trade, production risk is efficiently shared between countries due to trade in a single homogeneous good."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Bergin-BeyondCompetitiveDevaluations-2020.md"
    ],
    "question_context": "The simplified model assumes production of differentiated goods involves only labor with no intermediates and entry costs are in labor units. Firms operate for one period only and preset prices over the same horizon. The non-differentiated good sector is simplified with zero trade costs and infinite elasticity of substitution, making it a homogeneous good. Productivity shocks are i.i.d. and only occur in the differentiated goods sector. Utility is log in consumption and linear in leisure. International asset trade is abstracted from, but production risk is efficiently shared due to trade in the homogeneous good. The optimal preset price in the domestic market is given by:\n\n$$p_{t+1}(h)=\\frac{\\phi}{\\phi-1}\\frac{E_{t}\\Big[\\Omega_{t+1}\\Big(\\frac{W_{t+1}}{\\alpha_{t+1}}\\Big)\\Big]}{E_{t}\\big[\\Omega_{t+1}\\big]},$$\n\nwhere $W_{t+1}/\\alpha_{t+1}=\\mu_{t+1}/\\alpha_{t+1}$ is the firm's marginal costs. Under no stabilization policy, preset prices are increasing in the variance of productivity shocks due to Jensen’s inequality. A stabilization policy that sets the monetary stance proportional to productivity ($\\mu_{t}=\\alpha_{t}$) eliminates the covariance term in the optimal preset price equation, insulating the ex post markup from productivity uncertainty.",
    "category": "INCORRECT",
    "question_context_used": "The simplified model assumes production of differentiated goods involves only labor with no intermediates and entry costs are in labor units. Firms operate for one period only and preset prices over the same horizon. The non-differentiated good sector is simplified with zero trade costs and infinite elasticity of substitution, making it a homogeneous good. Productivity shocks are i.i.d. and only occur in the differentiated goods sector. Utility is log in consumption and linear in leisure. International asset trade is abstracted from, but production risk is efficiently shared due to trade in the homogeneous good. The optimal preset price in the domestic market is given by:\n\n$$p_{t+1}(h)=\\frac{\\phi}{\\phi-1}\\frac{E_{t}\\Big[\\Omega_{t+1}\\Big(\\frac{W_{t+1}}{\\alpha_{t+1}}\\Big)\\Big]}{E_{t}\\big[\\Omega_{t+1}\\big]},$$\n\nwhere $W_{t+1}/\\alpha_{t+1}=\\mu_{t+1}/\\alpha_{t+1}$ is the firm's marginal costs. Under no stabilization policy, preset prices are increasing in the variance of productivity shocks due to Jensen’s inequality. A stabilization policy that sets the monetary stance proportional to productivity ($\\mu_{t}=\\alpha_{t}$) eliminates the covariance term in the optimal preset price equation, insulating the ex post markup from productivity uncertainty.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2464",
    "question": "In the context of adult antiretroviral therapy (ART) scale-up in Zambia, which of the following mechanisms are empirically supported to improve child health outcomes?",
    "options": [
      "A) Direct viral load suppression in HIV-positive parents reducing household transmission",
      "B) Increased parental labor productivity leading to higher household healthcare expenditure",
      "C) ART-induced behavioral changes in parental caregiving practices",
      "D) Community-level herd immunity effects from reduced viral circulation"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Lucas-AdultAntiretroviralTherapy-2013.md"
    ],
    "question_context": "The study examines how adult ART scale-up affects child health through multiple pathways. Key findings indicate that parental ART adherence reduces intra-household HIV transmission (p < 0.01) and increases working hours by 15-20% among treated adults. The viral suppression mechanism operates through the formula: Transmission Risk = β0 + β1(ART_adherence) + ε, where β1 is negative and significant. Labor productivity effects are mediated by reduced opportunistic infections, with elasticity of child health expenditure to parental income estimated at 0.7. However, no statistically significant evidence was found for community-level herd immunity or systematic changes in caregiving behaviors.",
    "category": "CORRECT",
    "question_context_used": "The study examines how adult ART scale-up affects child health through multiple pathways. Key findings indicate that parental ART adherence reduces intra-household HIV transmission (p < 0.01) and increases working hours by 15-20% among treated adults. The viral suppression mechanism operates through the formula: Transmission Risk = β0 + β1(ART_adherence) + ε, where β1 is negative and significant. Labor productivity effects are mediated by reduced opportunistic infections, with elasticity of child health expenditure to parental income estimated at 0.7. However, no statistically significant evidence was found for community-level herd immunity or systematic changes in caregiving behaviors.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4719",
    "question": "In the context of ultimatum games with responder competition, which of the following statements are supported by empirical evidence?",
    "options": [
      "A) The presence of reciprocal responders leads to higher rejection rates of low offers as competition increases.",
      "B) The responders' share of the surplus decreases significantly when the number of competing responders increases.",
      "C) Selfish responders dominate the market outcomes, leading to no significant changes in surplus distribution with increased competition.",
      "D) The rejection probability of low offers declines as the number of competing responders increases due to the presence of selfish responders."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Why_Social_Preferences_Matter_–_the_Impact_of_non‐Selfish_Motives_on_Competition,_Cooperation_and_Incentives.md"
    ],
    "question_context": "The text discusses the strategic differences between one-shot and repeated interactions in economic experiments, particularly focusing on the ultimatum game with responder competition. Key findings include that the responders' share of the surplus decreases dramatically with increased competition, from $40\\%$ in the bilateral case to below $20\\%$ with five responders. This is attributed to the declining rejection probability of low offers as the number of competing responders increases, driven by the presence of selfish responders who accept low offers, thereby reducing the expected non-pecuniary return from rejection for reciprocal responders. The formula for the responders' share can be represented as $S = \\frac{1}{1 + n}$, where $n$ is the number of competing responders, illustrating the inverse relationship between competition and the responders' share.",
    "category": "CORRECT",
    "question_context_used": "The text discusses the strategic differences between one-shot and repeated interactions in economic experiments, particularly focusing on the ultimatum game with responder competition. Key findings include that the responders' share of the surplus decreases dramatically with increased competition, from $40\\%$ in the bilateral case to below $20\\%$ with five responders. This is attributed to the declining rejection probability of low offers as the number of competing responders increases, driven by the presence of selfish responders who accept low offers, thereby reducing the expected non-pecuniary return from rejection for reciprocal responders. The formula for the responders' share can be represented as $S = \\frac{1}{1 + n}$, where $n$ is the number of competing responders, illustrating the inverse relationship between competition and the responders' share.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2052",
    "question": "In the context of kernel regression estimators and their performance under different designs, which of the following statements are supported by the simulation results?",
    "options": [
      "A) The ROT-based kernel estimator consistently performs best across all designs when $R^{2}$ is moderate to large.",
      "B) The KAE estimator ($\\hat{\\mu}(\\hat{w})$) is never the worst-performing estimator in any of the considered cases.",
      "C) The AICc-based kernel estimator performs poorly, especially when $R^{2}$ is small to moderate.",
      "D) The CV-based kernel estimator uniformly performs worse than the KAE, with more pronounced differences when $n$ is small."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "13-Kernel_Averaging_Estimators.md"
    ],
    "question_context": "Key formula: The risk is measured by $\\mathrm{Risk}=\\frac{1}{n T}\\sum_{\\tau=1}^{T}(\\bar{\\mu}^{(\\tau)}-\\mu^{(\\tau)})^{\\mathrm{T}}(\\bar{\\mu}^{(\\tau)}-\\mu^{(\\tau)})$, where $\\bar{\\mu}=\\hat{\\mu}_{\\mathrm{AIC}c},\\hat{\\mu}_{\\mathrm{CV}},\\hat{\\mu}_{\\mathrm{ROT}},\\hat{\\mu}(\\hat{w})$. The simulation results reveal that under Designs 1, 2, and 4, the ROT-based kernel estimator commonly results in the lowest risk when $R^{2}$ is small to moderate, but performs poorly when $R^{2}$ is moderate to large. The KAE estimator ($\\hat{\\mu}(\\hat{w})$) is the most preferred estimator when $\\hat{\\mu}_{\\mathrm{ROT}}$ does not perform best and is never the worst-performing estimator. The AICc-based kernel estimator often performs poorly, especially when $R^{2}$ is small to moderate, and the CV-based kernel estimator uniformly performs worse than the KAE, with more pronounced differences when $n$ is small.",
    "category": "CORRECT",
    "question_context_used": "Key formula: The risk is measured by $\\mathrm{Risk}=\\frac{1}{n T}\\sum_{\\tau=1}^{T}(\\bar{\\mu}^{(\\tau)}-\\mu^{(\\tau)})^{\\mathrm{T}}(\\bar{\\mu}^{(\\tau)}-\\mu^{(\\tau)})$, where $\\bar{\\mu}=\\hat{\\mu}_{\\mathrm{AIC}c},\\hat{\\mu}_{\\mathrm{CV}},\\hat{\\mu}_{\\mathrm{ROT}},\\hat{\\mu}(\\hat{w})$. The simulation results reveal that under Designs 1, 2, and 4, the ROT-based kernel estimator commonly results in the lowest risk when $R^{2}$ is small to moderate, but performs poorly when $R^{2}$ is moderate to large. The KAE estimator ($\\hat{\\mu}(\\hat{w})$) is the most preferred estimator when $\\hat{\\mu}_{\\mathrm{ROT}}$ does not perform best and is never the worst-performing estimator. The AICc-based kernel estimator often performs poorly, especially when $R^{2}$ is small to moderate, and the CV-based kernel estimator uniformly performs worse than the KAE, with more pronounced differences when $n$ is small.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4282",
    "question": "In the context of state-dependent utility maximization, which of the following statements accurately characterize the role and properties of information structures in economic decision-making?",
    "options": [
      "A) Information structures are defined as specific observed values of random variables rather than the entire partition of possible events.",
      "B) Economic agents must acquire information before observing its outcome, as the value lies in the potential to learn subsets of the partition.",
      "C) Different σ-fields within F represent distinct types of information, and agents may value them asymmetrically based on their utility functions.",
      "D) The equivalence of information structures is universally perceived by all agents, as they rely on the same probability measure μ defined on F."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Allen-InformationEconomicCommodity-1990.md"
    ],
    "question_context": "Formally, let Ω be an abstract set of states of the world and let the collection F of subsets of Ω represent the measurable events. For simplicity, assume that all agents know the probability μ defined on the σ-field F of Ω. Information is technically defined to be a σ-field of measurable events. A special case involves information signals correlated with states of the world. Since signals can be viewed as random variables defined on some larger probability space Ω×Ω′, they therefore generate σ-fields on Ω×Ω′. An important aspect of the definition is that information involves the entire partition rather than some particular set. Information is the ability to learn a subset in the partition, in contrast to the statement that one particular subset has occurred. Since F always contains more than one σ-field, except in the trivial F={Ω,∅} case, different types of information are potentially available. Moreover, consumers generally do not perceive distinct information structures to be equivalent; they may attach more importance to the possibility of learning some events than others.",
    "category": "CORRECT",
    "question_context_used": "Formally, let Ω be an abstract set of states of the world and let the collection F of subsets of Ω represent the measurable events. For simplicity, assume that all agents know the probability μ defined on the σ-field F of Ω. Information is technically defined to be a σ-field of measurable events. A special case involves information signals correlated with states of the world. Since signals can be viewed as random variables defined on some larger probability space Ω×Ω′, they therefore generate σ-fields on Ω×Ω′. An important aspect of the definition is that information involves the entire partition rather than some particular set. Information is the ability to learn a subset in the partition, in contrast to the statement that one particular subset has occurred. Since F always contains more than one σ-field, except in the trivial F={Ω,∅} case, different types of information are potentially available. Moreover, consumers generally do not perceive distinct information structures to be equivalent; they may attach more importance to the possibility of learning some events than others.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4609",
    "question": "In the context of Häckner and Herzing's analysis of marginal costs of public funds under imperfect competition, which of the following market characteristics are associated with lower marginal costs of public funds?",
    "options": [
      "A) Wider firms' product ranges",
      "B) Higher degree of market concentration",
      "C) Lower degree of product differentiation",
      "D) Bertrand competition compared to Cournot competition"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Welfare-effects-of-taxation-in-oligopolistic-ma_2016_Journal-of-Economic-The.md"
    ],
    "question_context": "The paper discusses how marginal costs of public funds are related to various market characteristics under imperfect competition. Under a general tax scheme, these costs are lower the wider the firms’ product ranges, the lower the degree of market concentration and the lower the degree of product differentiation. Moreover, marginal costs of taxation are lower in Bertrand markets compared to Cournot markets. Key formulas include $M C_{t}=\\rho_{t}\\theta$ for unit tax $t$ and $M C_{v}=\\frac{\\rho_{v}\\theta}{p}$ for ad valorem tax $v$, where $\\rho$ is the pass-through rate, $\\theta$ is the elasticity-adjusted Lerner index, and $p$ is the market price. The marginal cost of public funds is typically largest when competitive pressure is weak, because prices are high also in the absence of taxation. However, the sensitivity of tax revenues to changes in taxes must also be accounted for, which is affected by market characteristics.",
    "category": "CORRECT",
    "question_context_used": "The paper discusses how marginal costs of public funds are related to various market characteristics under imperfect competition. Under a general tax scheme, these costs are lower the wider the firms’ product ranges, the lower the degree of market concentration and the lower the degree of product differentiation. Moreover, marginal costs of taxation are lower in Bertrand markets compared to Cournot markets. Key formulas include $M C_{t}=\\rho_{t}\\theta$ for unit tax $t$ and $M C_{v}=\\frac{\\rho_{v}\\theta}{p}$ for ad valorem tax $v$, where $\\rho$ is the pass-through rate, $\\theta$ is the elasticity-adjusted Lerner index, and $p$ is the market price. The marginal cost of public funds is typically largest when competitive pressure is weak, because prices are high also in the absence of taxation. However, the sensitivity of tax revenues to changes in taxes must also be accounted for, which is affected by market characteristics.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-958",
    "question": "In the context of $K$-equilibrium and its properties, which of the following statements are correct based on the propositions in the text?",
    "options": [
      "A) Order implies voluntary exchange under convex and differentiable preferences with at least three goods.",
      "B) Constrained Pareto optima are always weakly orderly and voluntary.",
      "C) Voluntary Pareto optima must necessarily be weakly orderly.",
      "D) Non-dominated $K$-equilibria ($K$-Pareto optima) need not be voluntary Pareto optima."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "On-the-efficiency-of-fixed-price-equilibrium_1984_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The paper examines the efficiency properties of $K$-equilibrium, focusing on its two key properties: order (at most one side of the market is quantity-constrained) and voluntariness (no one trades more than desired). Proposition 1 establishes that order implies voluntary exchange under convex and differentiable preferences with at least three goods, making orderly and $K$-equilibrium allocations identical. Proposition 3 reveals that constrained Pareto optima, while weakly orderly, are generally non-voluntary and non-orderly. Proposition 4 shows that voluntary Pareto optima need not be weakly orderly. Finally, Proposition 5 demonstrates that even non-dominated $K$-equilibria ($K$-Pareto optima) may not be voluntary Pareto optima. Key formula: $K$-equilibrium allocations = orderly allocations under convexity, differentiability, and ≥3 goods.",
    "category": "CORRECT",
    "question_context_used": "The paper examines the efficiency properties of $K$-equilibrium, focusing on its two key properties: order (at most one side of the market is quantity-constrained) and voluntariness (no one trades more than desired). Proposition 1 establishes that order implies voluntary exchange under convex and differentiable preferences with at least three goods, making orderly and $K$-equilibrium allocations identical. Proposition 3 reveals that constrained Pareto optima, while weakly orderly, are generally non-voluntary and non-orderly. Proposition 4 shows that voluntary Pareto optima need not be weakly orderly. Finally, Proposition 5 demonstrates that even non-dominated $K$-equilibria ($K$-Pareto optima) may not be voluntary Pareto optima. Key formula: $K$-equilibrium allocations = orderly allocations under convexity, differentiability, and ≥3 goods.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-766",
    "question": "In the context of Bayesian–Nash equilibrium in trade agreements, which of the following statements are correct regarding the optimal tariff and its implications?",
    "options": [
      "A) The optimal tariff is inversely proportional to the elasticity of domestic demand.",
      "B) A higher political weight of high-cost producers (lower α) leads to a smaller optimal tariff.",
      "C) The optimal tariff formula includes the ratio of changes in the world and domestic prices of imports.",
      "D) Home's welfare increases when high-cost producers have less influence on domestic politics."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "From_Inefficient_Behind-the-Border_Policies_to_Inefficient_Trade_Agreements_A_Two-Tier_Asymmetric_Information_Model.md"
    ],
    "question_context": "The optimal tariff trades off a domestic deadweight loss against improved terms of trade. A tariff decreases the world price so that the benefits are captured on the right-hand side of (12). Because any change in the tariff is taken into account when BTB policies are designed so as to let domestic producers internalise their impact on tariff revenues, there is no deadweight loss on the supply side of the domestic market. Only Home’s consumers suffer from an increase in the domestic price. This effect is captured on the left-hand side of (12). Rearranging (12) and noticing that $p_{t}(\\alpha,t^{n c}(\\alpha),t^{*n c})=1+r_{t}(\\alpha,t^{n c}(\\alpha),t^{*n c})$ , we obtain: \n\n$$ t^{n c}(\\alpha)=-\\frac{r_{t}(\\alpha,t^{n c}(\\alpha),t^{*n c})}{1+r_{t}(\\alpha,t^{n c}(\\alpha),t^{*n c})}\\frac{p(\\alpha,t^{n c}(\\alpha),t^{*n c})\\zeta(\\alpha,t^{n c}(\\alpha),t^{*n c})}{\\varepsilon(p(\\alpha,t^{n c}(\\alpha),t^{*n c}))}, $$\n\nwhere $\\begin{array}{r}{\\varepsilon(p)=-\\frac{p D^{\\prime}(p)}{D(p)}}\\end{array}$ is good 2 demand elasticity, $-\\textstyle{\\frac{r_{t}(\\alpha,t,t^{*})}{1+r_{t}(\\alpha,t,t^{*})}}$ is the ratio of changes in the world and domestic prices of imports and $\\begin{array}{r}{\\zeta(\\alpha,t,t^{*})=1-\\frac{\\tilde{S}(\\alpha,r(\\alpha,t,t^{*}))}{D(p(\\alpha,t,t^{*}))}}\\end{array}$ is the ratio of imports over domestic demand for good 2. From (14), the percentage tariff $\\frac{t^{n c}(\\alpha)}{p(\\alpha,t^{n c}(\\alpha))}$ is proportional to the inverse of the elasticity of domestic demand, the ratio of changes in the world and domestic prices of imports, and the ratio of imports over domestic demand. The standard textbook formula38 for optimal tariff differs from ours at this point by replacing the elasticity of domestic demand by the elasticity of imports. The difference is explained by the fact that, when BTB policies are optimally designed, Home makes domestic producers internalise the impact of their supply on tariff revenues. Giving more weight to those producers (i.e., reducing $\\alpha$ ) further contracts Home’s supply and calls for a greater tariff. When $\\alpha$ is closer to 0, high-cost producers have more influence. Redistribution matters and the contraction of Home’s production is significant, leading to significant imports of good 2. The optimal tariff is larger.",
    "category": "INCORRECT",
    "question_context_used": "The optimal tariff trades off a domestic deadweight loss against improved terms of trade. A tariff decreases the world price so that the benefits are captured on the right-hand side of (12). Because any change in the tariff is taken into account when BTB policies are designed so as to let domestic producers internalise their impact on tariff revenues, there is no deadweight loss on the supply side of the domestic market. Only Home’s consumers suffer from an increase in the domestic price. This effect is captured on the left-hand side of (12). Rearranging (12) and noticing that $p_{t}(\\alpha,t^{n c}(\\alpha),t^{*n c})=1+r_{t}(\\alpha,t^{n c}(\\alpha),t^{*n c})$ , we obtain: \n\n$$ t^{n c}(\\alpha)=-\\frac{r_{t}(\\alpha,t^{n c}(\\alpha),t^{*n c})}{1+r_{t}(\\alpha,t^{n c}(\\alpha),t^{*n c})}\\frac{p(\\alpha,t^{n c}(\\alpha),t^{*n c})\\zeta(\\alpha,t^{n c}(\\alpha),t^{*n c})}{\\varepsilon(p(\\alpha,t^{n c}(\\alpha),t^{*n c}))}, $$\n\nwhere $\\begin{array}{r}{\\varepsilon(p)=-\\frac{p D^{\\prime}(p)}{D(p)}}\\end{array}$ is good 2 demand elasticity, $-\\textstyle{\\frac{r_{t}(\\alpha,t,t^{*})}{1+r_{t}(\\alpha,t,t^{*})}}$ is the ratio of changes in the world and domestic prices of imports and $\\begin{array}{r}{\\zeta(\\alpha,t,t^{*})=1-\\frac{\\tilde{S}(\\alpha,r(\\alpha,t,t^{*}))}{D(p(\\alpha,t,t^{*}))}}\\end{array}$ is the ratio of imports over domestic demand for good 2. From (14), the percentage tariff $\\frac{t^{n c}(\\alpha)}{p(\\alpha,t^{n c}(\\alpha))}$ is proportional to the inverse of the elasticity of domestic demand, the ratio of changes in the world and domestic prices of imports, and the ratio of imports over domestic demand. The standard textbook formula38 for optimal tariff differs from ours at this point by replacing the elasticity of domestic demand by the elasticity of imports. The difference is explained by the fact that, when BTB policies are optimally designed, Home makes domestic producers internalise the impact of their supply on tariff revenues. Giving more weight to those producers (i.e., reducing $\\alpha$ ) further contracts Home’s supply and calls for a greater tariff. When $\\alpha$ is closer to 0, high-cost producers have more influence. Redistribution matters and the contraction of Home’s production is significant, leading to significant imports of good 2. The optimal tariff is larger.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1578",
    "question": "In the context of quantal response equilibrium (QRE) and equilibrium-plus-noise models, which of the following statements are supported by the experimental data and likelihood ratio tests?",
    "options": [
      "A) The equilibrium-plus-noise model outperforms purely random switching in all treatments except the triples δ=0.5 case.",
      "B) QRE estimates for the pairs treatment show higher rationality parameter λ values compared to the triples treatment.",
      "C) The QRE model provides a better fit than the equilibrium-plus-noise model for the triples δ=0.5 treatment.",
      "D) Both models confirm that switching probabilities deviate from equilibrium predictions toward random behavior (0.5)."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Blume-DecentralizedOrganizationalLearning-2009.md"
    ],
    "question_context": "The equilibrium-plus-noise model specifies that subjects switch in round 2 with probability: \n\n$$ p(\\eta)=\\eta p+(1-\\eta)\\frac{1}{2}, $$ \n\nwhere $\\eta\\in[0,1]$. The QRE model uses a logit-choice rule: \n\n$$ p=\\frac{e^{\\lambda u_{s}}}{e^{\\lambda u_{s}}+e^{\\lambda u_{n}}}, $$ \n\nwhere $\\lambda$ is the rationality parameter. The likelihood ratio tests show that the equilibrium-plus-noise model outperforms random switching in all treatments except triples δ=0.5, while QRE estimates for the pairs treatment exhibit higher $\\lambda$ values than the triples treatment. Both models indicate deviations from equilibrium toward random behavior.",
    "category": "CORRECT",
    "question_context_used": "The equilibrium-plus-noise model specifies that subjects switch in round 2 with probability: \n\n$$ p(\\eta)=\\eta p+(1-\\eta)\\frac{1}{2}, $$ \n\nwhere $\\eta\\in[0,1]$. The QRE model uses a logit-choice rule: \n\n$$ p=\\frac{e^{\\lambda u_{s}}}{e^{\\lambda u_{s}}+e^{\\lambda u_{n}}}, $$ \n\nwhere $\\lambda$ is the rationality parameter. The likelihood ratio tests show that the equilibrium-plus-noise model outperforms random switching in all treatments except triples δ=0.5, while QRE estimates for the pairs treatment exhibit higher $\\lambda$ values than the triples treatment. Both models indicate deviations from equilibrium toward random behavior.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3416",
    "question": "In the context of Lemma A3 and the given economic framework, which conditions must be satisfied for $\\tilde{q}(\\theta,\\eta_{i})=q(\\theta,\\eta_{i})$, $\\tilde{u}_{A}(\\theta,\\eta_{i})=u_{A}(\\theta,\\eta_{i})$, and $P(\\cdot\\mid\\eta_{i})=I_{2}(\\eta_{i})$ to solve $\\bar{P}^{E}(\\alpha:\\eta_{i})$?",
    "options": [
      "A) There exists a non-decreasing $\\Lambda_{i}:\\Theta\\to[0,1]$ with $\\Lambda_{i}(\\underline{\\theta})=0$ and $\\Lambda_{i}(\\bar{\\theta})=1$.",
      "B) The condition $p_{i}=\\arg\\max_{\\tilde{p}_{i}\\in\\Theta}\\int_{\\underline{\\theta}}^{\\tilde{p}_{i}}[b-W_{i}(\\theta:\\beta)]d F_{i}(\\theta)$ holds.",
      "C) The inequality $\\int_{\\underline{\\theta}}^{p_{i}}\\Lambda_{i}(\\theta)d\\theta+u_{i}\\le\\int_{\\underline{\\theta}}^{p_{j}}\\Lambda_{i}(\\theta)d\\theta+u_{j}$ is satisfied for $j\\in\\{L,H,0\\}$.",
      "D) The function $W_{i}(\\theta:\\beta)\\equiv\\theta+\\beta\\frac{F_{i}(\\theta)-\\Lambda_{i}(\\theta)}{f_{i}(\\theta)}$ is strictly decreasing in $\\theta$."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Regulatory-mechanism-design-with-extortionary-co_2023_Journal-of-Economic-Th.md"
    ],
    "question_context": "Lemma A3 states that for $i\\in\\{L,H\\}$, $\\tilde{q}(\\theta,\\eta_{i})=q(\\theta,\\eta_{i})$, $\\tilde{u}_{A}(\\theta,\\eta_{i})=u_{A}(\\theta,\\eta_{i})$, and $P(\\cdot\\mid\\eta_{i})=I_{2}(\\eta_{i})$ solve $\\bar{P}^{E}(\\alpha:\\eta_{i})$ if there exists a non-decreasing $\\Lambda_{i}:\\Theta\\to[0,1]$ with $\\Lambda_{i}(\\underline{\\theta})=0$ and $\\Lambda_{i}(\\bar{\\theta})=1$ such that:\n\n$$\np_{i}=\\arg\\max_{\\tilde{p}_{i}\\in\\Theta}\\int_{\\underline{\\theta}}^{\\tilde{p}_{i}}[b-W_{i}(\\theta:\\beta)]d F_{i}(\\theta)\n$$\n\nand\n\n$$\n\\int_{\\underline{\\theta}}^{p_{i}}\\Lambda_{i}(\\theta)d\\theta+u_{i}\\le\\int_{\\underline{\\theta}}^{p_{j}}\\Lambda_{i}(\\theta)d\\theta+u_{j}\n$$\n\nfor $j\\in\\{L,H,0\\}$, where\n\n$$\nW_{i}(\\theta:\\beta)\\equiv\\theta+\\beta\\frac{F_{i}(\\theta)-\\Lambda_{i}(\\theta)}{f_{i}(\\theta)}.\n$$\n\nThe proof leverages these conditions to show optimality in the relaxed problem, ensuring the solution adheres to the constraints and maximizes the objective function.",
    "category": "INCORRECT",
    "question_context_used": "Lemma A3 states that for $i\\in\\{L,H\\}$, $\\tilde{q}(\\theta,\\eta_{i})=q(\\theta,\\eta_{i})$, $\\tilde{u}_{A}(\\theta,\\eta_{i})=u_{A}(\\theta,\\eta_{i})$, and $P(\\cdot\\mid\\eta_{i})=I_{2}(\\eta_{i})$ solve $\\bar{P}^{E}(\\alpha:\\eta_{i})$ if there exists a non-decreasing $\\Lambda_{i}:\\Theta\\to[0,1]$ with $\\Lambda_{i}(\\underline{\\theta})=0$ and $\\Lambda_{i}(\\bar{\\theta})=1$ such that:\n\n$$\np_{i}=\\arg\\max_{\\tilde{p}_{i}\\in\\Theta}\\int_{\\underline{\\theta}}^{\\tilde{p}_{i}}[b-W_{i}(\\theta:\\beta)]d F_{i}(\\theta)\n$$\n\nand\n\n$$\n\\int_{\\underline{\\theta}}^{p_{i}}\\Lambda_{i}(\\theta)d\\theta+u_{i}\\le\\int_{\\underline{\\theta}}^{p_{j}}\\Lambda_{i}(\\theta)d\\theta+u_{j}\n$$\n\nfor $j\\in\\{L,H,0\\}$, where\n\n$$\nW_{i}(\\theta:\\beta)\\equiv\\theta+\\beta\\frac{F_{i}(\\theta)-\\Lambda_{i}(\\theta)}{f_{i}(\\theta)}.\n$$\n\nThe proof leverages these conditions to show optimality in the relaxed problem, ensuring the solution adheres to the constraints and maximizes the objective function.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1573",
    "question": "In the context of rational inattention and portfolio choice under CARA utility, which of the following statements are supported by the numerical results in Table 1?",
    "options": [
      "A) Higher risk aversion (α) leads to reduced leverage in optimal portfolios.",
      "B) Lower information cost (λ) results in sharper boundaries between conditional distributions of returns.",
      "C) The optimal portfolio distribution always concentrates on a finite set of points, regardless of parameter values.",
      "D) Increased irreducible risk (σ_z) reduces the number of portfolios in the support of the optimal distribution."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Discrete Actions in Information-Constrained Decision Problems.md"
    ],
    "question_context": "The problem considered is maximization of expected constant absolute risk aversion (CARA) utility by choosing portfolio weights on two risky, and one riskless, securities. The returns on the risky securities have two components: one, Z, that is inherently uncertain; another, Y, that is uncertain only because of limited information processing capacity. The formal problem is:\n\n$$\n\\mathrm{max}E[-\\mathrm{exp}(-\\alpha\\theta^{\\prime}(Y+Z))]-\\lambda I(\\theta,Y)\n$$\n\nsubject to\n\n$$\n\\begin{array}{r}{\\theta^{\\prime}{\\bf1}=1,}\\end{array}\n$$\n\nwhere θ is a vector of portfolio weights summing to one, and Y+Z is the vector of random yields. Numerical solutions show that the support of the distribution for the portfolio weights θ is just three or four portfolios, indicating optimal discreteness. Key findings include that higher risk aversion (α) leads to reduced leverage, and lower information cost (λ) results in sharper boundaries between conditional distributions of returns.",
    "category": "CORRECT",
    "question_context_used": "The problem considered is maximization of expected constant absolute risk aversion (CARA) utility by choosing portfolio weights on two risky, and one riskless, securities. The returns on the risky securities have two components: one, Z, that is inherently uncertain; another, Y, that is uncertain only because of limited information processing capacity. The formal problem is:\n\n$$\n\\mathrm{max}E[-\\mathrm{exp}(-\\alpha\\theta^{\\prime}(Y+Z))]-\\lambda I(\\theta,Y)\n$$\n\nsubject to\n\n$$\n\\begin{array}{r}{\\theta^{\\prime}{\\bf1}=1,}\\end{array}\n$$\n\nwhere θ is a vector of portfolio weights summing to one, and Y+Z is the vector of random yields. Numerical solutions show that the support of the distribution for the portfolio weights θ is just three or four portfolios, indicating optimal discreteness. Key findings include that higher risk aversion (α) leads to reduced leverage, and lower information cost (λ) results in sharper boundaries between conditional distributions of returns.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1254",
    "question": "Which of the following articles from the American Economic Review address fundamental questions about market structures and competitive dynamics?",
    "options": [
      "A) \"Contestability” vs. Competition by William G. Shepherd",
      "B) \"Do Oligopolists Earn 'Noncompetitive' Rates of Return?\" by William W. Alberts",
      "C) \"Race and Human Capital\" by James P. Smith",
      "D) \"An Empirical Analysis of Welfare Dependence\" by Mark W. Plant"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "FrontMatter-1984.md"
    ],
    "question_context": "The American Economic Review features several articles that delve into market structures and competitive dynamics. For instance, William G. Shepherd's article \"Contestability” vs. Competition\" examines the theoretical underpinnings of contestable markets and their implications for competition policy. Similarly, William W. Alberts' article \"Do Oligopolists Earn 'Noncompetitive' Rates of Return?\" investigates whether firms in oligopolistic markets earn returns that exceed competitive levels. These articles contribute to the ongoing debate about market efficiency and the role of competition in economic theory. The other articles listed, while valuable, focus on different aspects of economics such as human capital and welfare dependence, which are not directly related to market structures.",
    "category": "CORRECT",
    "question_context_used": "The American Economic Review features several articles that delve into market structures and competitive dynamics. For instance, William G. Shepherd's article \"Contestability” vs. Competition\" examines the theoretical underpinnings of contestable markets and their implications for competition policy. Similarly, William W. Alberts' article \"Do Oligopolists Earn 'Noncompetitive' Rates of Return?\" investigates whether firms in oligopolistic markets earn returns that exceed competitive levels. These articles contribute to the ongoing debate about market efficiency and the role of competition in economic theory. The other articles listed, while valuable, focus on different aspects of economics such as human capital and welfare dependence, which are not directly related to market structures.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2849",
    "question": "According to the rational inattention model presented in the paper, which of the following factors are key mechanisms driving heterogeneous responses of homeowners and renters to monetary policy shocks?",
    "options": [
      "A) Mortgage payments creating incentives for homeowners to acquire information on monetary policy",
      "B) Renters' higher financial literacy compared to homeowners",
      "C) Forward guidance shocks having stronger pass-through to 30-year mortgage rates",
      "D) Homeowners spending more time on finance-related activities than renters"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Effects-of-monetary-policy-on-household-expectations_2024_Journal-of-Monetar.md"
    ],
    "question_context": "The paper characterizes empirical findings with a rational inattention model where mortgage payments create an incentive for homeowners to acquire information on monetary policy, unlike renters. This housing-driven endogenous attentiveness is the key mechanism behind the compelling empirical link among homeownership, attention, and the transmission of monetary policy. Furthermore, the study shows that homeowners respond similarly to forward guidance shocks, which have the strongest pass-through to 30-year mortgage rates among monetary policy tools. Additional evidence from the American Time Use Survey (ATUS) shows that homeowners spend more time on finance-related activities, enhancing their exposure to economic information. Key formula: The sensitivity of homeowners' revisions in short-term inflation expectations is given by ΔE[π_t+1] = β_0 + β_1Δr_t^M + ε_t, where β_1 is significantly larger for homeowners than renters.",
    "category": "CORRECT",
    "question_context_used": "The paper characterizes empirical findings with a rational inattention model where mortgage payments create an incentive for homeowners to acquire information on monetary policy, unlike renters. This housing-driven endogenous attentiveness is the key mechanism behind the compelling empirical link among homeownership, attention, and the transmission of monetary policy. Furthermore, the study shows that homeowners respond similarly to forward guidance shocks, which have the strongest pass-through to 30-year mortgage rates among monetary policy tools. Additional evidence from the American Time Use Survey (ATUS) shows that homeowners spend more time on finance-related activities, enhancing their exposure to economic information. Key formula: The sensitivity of homeowners' revisions in short-term inflation expectations is given by ΔE[π_t+1] = β_0 + β_1Δr_t^M + ε_t, where β_1 is significantly larger for homeowners than renters.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3010",
    "question": "Which of the following statements are true regarding the equivalence between different inequality reduction criteria among vertically differentiated agents?",
    "options": [
      "A) The Ordered Poverty Gap (OPG) dominance criterion is equivalent to the possibility of going from one distribution to another by a finite sequence of income transfers from richer and more highly ranked agents to poorer and less highly ranked ones.",
      "B) The Bourguignon (1989) OPG dominance criterion requires that poverty, measured by the income poverty gap, be smaller in the dominating distribution for any collection of poverty lines that are increasing with respect to the agent’s vertical standing.",
      "C) The equivalence between the four notions of inequality reduction among unequals holds only when the total income and the number of agents in each category are fixed across distributions.",
      "D) The CLI dominance criterion generalizes Lorenz dominance by considering cumulative lowest incomes and is equivalent to the OPG dominance criterion."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Int Economic Review - 2020 - Faure - REDUCING INEQUALITIES AMONG UNEQUALS.md"
    ],
    "question_context": "The article establishes an equivalence between four incomplete rankings of distributions of income among agents who are vertically differentiated with respect to some nonincome characteristic. The first ranking is the possibility of going from one distribution to the other by a finite sequence of income transfers from richer and more highly ranked agents to poorer and less highly ranked ones. The second ranking is the unanimity among utilitarian planners who assume that agents’ marginal utility of income is decreasing with respect to both income and the source of vertical differentiation. The third ranking is the Bourguignon (1989) Ordered Poverty Gap (OPG) dominance criterion. The fourth ranking is a new dominance criterion based on cumulative lowest incomes (CLI).\n\nKey formula: The OPG dominance criterion requires that poverty, measured by the income poverty gap, be smaller in the dominating distribution than in the dominated distribution for any collection of poverty lines that are decreasing with respect to the agent’s vertical standing. The CLI dominance criterion generalizes Lorenz dominance to the setting of vertically differentiated agents by considering partial sums of incomes of the poorest agents, where the poorest agents are identified based on their income and vertical standing.",
    "category": "INCORRECT",
    "question_context_used": "The article establishes an equivalence between four incomplete rankings of distributions of income among agents who are vertically differentiated with respect to some nonincome characteristic. The first ranking is the possibility of going from one distribution to the other by a finite sequence of income transfers from richer and more highly ranked agents to poorer and less highly ranked ones. The second ranking is the unanimity among utilitarian planners who assume that agents’ marginal utility of income is decreasing with respect to both income and the source of vertical differentiation. The third ranking is the Bourguignon (1989) Ordered Poverty Gap (OPG) dominance criterion. The fourth ranking is a new dominance criterion based on cumulative lowest incomes (CLI).\n\nKey formula: The OPG dominance criterion requires that poverty, measured by the income poverty gap, be smaller in the dominating distribution than in the dominated distribution for any collection of poverty lines that are decreasing with respect to the agent’s vertical standing. The CLI dominance criterion generalizes Lorenz dominance to the setting of vertically differentiated agents by considering partial sums of incomes of the poorest agents, where the poorest agents are identified based on their income and vertical standing.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3009",
    "question": "In the context of statistical inference for inequality and poverty measurement with dependent data, which of the following statements are correct regarding the challenges posed by data dependence?",
    "options": [
      "A) Dependence in data can lead to biased standard errors if not properly accounted for.",
      "B) The Gini coefficient remains robust to any form of data dependence without modification.",
      "C) Bootstrap methods may fail to provide valid inference under strong temporal dependence.",
      "D) Poverty measures based on independent data assumptions can underestimate true uncertainty."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Statistical Inference for Inequality and Poverty Measurement with Dependent Data.md"
    ],
    "question_context": "The paper by Schluter and Trede (2002) addresses the challenges of statistical inference for inequality and poverty measures when data exhibit dependence, such as temporal or spatial correlation. Key issues include the invalidity of standard asymptotic theory under dependence and the potential for misleading inference if dependence is ignored. For instance, the variance of the Gini coefficient estimator may be severely underestimated under positive dependence. The authors highlight that bootstrap methods, while useful, require careful adaptation to handle dependent data structures. Formula: The asymptotic variance of the Gini coefficient under dependence involves covariance terms that vanish only under independence.",
    "category": "CORRECT",
    "question_context_used": "The paper by Schluter and Trede (2002) addresses the challenges of statistical inference for inequality and poverty measures when data exhibit dependence, such as temporal or spatial correlation. Key issues include the invalidity of standard asymptotic theory under dependence and the potential for misleading inference if dependence is ignored. For instance, the variance of the Gini coefficient estimator may be severely underestimated under positive dependence. The authors highlight that bootstrap methods, while useful, require careful adaptation to handle dependent data structures. Formula: The asymptotic variance of the Gini coefficient under dependence involves covariance terms that vanish only under independence.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3550",
    "question": "Which of the following statements are correct regarding the Livingston survey's properties and its forecasting performance for real GDP growth?",
    "options": [
      "A) The Livingston survey's forecasts exhibit first-order moving average structure in their errors, consistent with optimal two-step-ahead forecasts.",
      "B) The survey's mean annualized real GDP growth rate forecast of 2.54% significantly deviates from historical growth realizations.",
      "C) The Livingston forecast remains a significant predictor of future GDP growth even when combined with other macroeconomic variables.",
      "D) The survey's forecast errors display predictable patterns and non-zero mean, indicating suboptimal forecasting."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "09-Stock Returns and Expected Business Conditions_ Half a Century of Direct Evidence.md"
    ],
    "question_context": "The two-step-ahead real growth rate forecasts constructed from the Livingston data appear well behaved. In Figure 1 we show the actual growth rates, the Livingston forecasts, and the corresponding forecast errors. The forecasts move with the actual growth rates but are smoother, which is a well-known property of optimal forecasts of stationary series (e.g., see Diebold 2007). Moreover, the forecast errors appear to have zero mean and display no obvious predictable patterns. The sharp cutoff in the sample autocorrelation function of the forecast errors beyond displacement one, as shown in Figure 2, indicates first-order moving average structure, which is consistent with optimality of the two-step-ahead forecasts. The mean Livingston (annualized) real GDP growth rate forecast in Table 1 is $2.54\\%$ , which closely accords with historical growth realizations.\n\nIn Table 2, we examine the performance of the Livingston forecasts in more detail by regressing realized real GDP growth, two steps ahead, on the Livingston forecasts as well as lagged real GDP growth, real consumption growth, and real investment growth. The first column of Table 2 indicates that the Livingston growth forecasts are informative about future real GDP growth; the forecasts predict future growth with a $t$ -statistic of 3.0. When compared with the forecasting performance of lagged real GDP, consumption, and investment growth the Livingston forecast stands out. Specifically, in each of the univariate specifications only the Livingston forecast is a significant predictor of future GDP growth. Moreover, when the Livingston forecast is combined with the other predictors the size and significance of the coefficient on the Livingston forecast is unaffected.",
    "category": "CORRECT",
    "question_context_used": "The two-step-ahead real growth rate forecasts constructed from the Livingston data appear well behaved. In Figure 1 we show the actual growth rates, the Livingston forecasts, and the corresponding forecast errors. The forecasts move with the actual growth rates but are smoother, which is a well-known property of optimal forecasts of stationary series (e.g., see Diebold 2007). Moreover, the forecast errors appear to have zero mean and display no obvious predictable patterns. The sharp cutoff in the sample autocorrelation function of the forecast errors beyond displacement one, as shown in Figure 2, indicates first-order moving average structure, which is consistent with optimality of the two-step-ahead forecasts. The mean Livingston (annualized) real GDP growth rate forecast in Table 1 is $2.54\\%$ , which closely accords with historical growth realizations.\n\nIn Table 2, we examine the performance of the Livingston forecasts in more detail by regressing realized real GDP growth, two steps ahead, on the Livingston forecasts as well as lagged real GDP growth, real consumption growth, and real investment growth. The first column of Table 2 indicates that the Livingston growth forecasts are informative about future real GDP growth; the forecasts predict future growth with a $t$ -statistic of 3.0. When compared with the forecasting performance of lagged real GDP, consumption, and investment growth the Livingston forecast stands out. Specifically, in each of the univariate specifications only the Livingston forecast is a significant predictor of future GDP growth. Moreover, when the Livingston forecast is combined with the other predictors the size and significance of the coefficient on the Livingston forecast is unaffected.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1685",
    "question": "In the empirical analysis of sex ratios and demographic outcomes in 1959 Russia, which of the following factors are explicitly controlled for in the fixed-effects regression model?",
    "options": [
      "A) Birth cohort fixed effects ($\\lambda_{c}$)",
      "B) Region fixed effects ($\\mu_{j}$)",
      "C) Log of the population size of the opposite sex ($\\ln(pop_{c j})$)",
      "D) Urban-rural differentials in marriage market thickness"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Brainerd-LASTINGEFFECTSEX-2017.md"
    ],
    "question_context": "The regression model is specified as: $$Y_{c j}=\\upbeta_{1}R_{c j}+\\upbeta_{2}\\mathrm{{ln}}(p o p_{c j})+\\uplambda_{c}+\\upmu_{j}+\\upvarepsilon_{c j}$$ where $R_{c j}$ is the sex ratio for birth cohort $c$ in region $j;\\lambda_{c}$ are birth cohort fixed effects; $\\upmu_{j}$ are region fixed effects; and $Y_{c j}$ is an outcome such as the proportion married, the birth rate, or divorce rate. The model includes the log of the size of the population of the opposite sex to control for marriage market thickness. While urban-rural differences are acknowledged as important contextually, they are not explicitly included as a control variable in this specification. The regressions are estimated separately for urban and rural areas to account for potential differences in marriage market dynamics and social norms.",
    "category": "CORRECT",
    "question_context_used": "The regression model is specified as: $$Y_{c j}=\\upbeta_{1}R_{c j}+\\upbeta_{2}\\mathrm{{ln}}(p o p_{c j})+\\uplambda_{c}+\\upmu_{j}+\\upvarepsilon_{c j}$$ where $R_{c j}$ is the sex ratio for birth cohort $c$ in region $j;\\lambda_{c}$ are birth cohort fixed effects; $\\upmu_{j}$ are region fixed effects; and $Y_{c j}$ is an outcome such as the proportion married, the birth rate, or divorce rate. The model includes the log of the size of the population of the opposite sex to control for marriage market thickness. While urban-rural differences are acknowledged as important contextually, they are not explicitly included as a control variable in this specification. The regressions are estimated separately for urban and rural areas to account for potential differences in marriage market dynamics and social norms.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-720",
    "question": "Which of the following statements are true regarding subgroup consistent poverty indices?",
    "options": [
      "A) A decomposable poverty index must be subgroup consistent.",
      "B) The Sen index is subgroup consistent because it relies on rank-order weighting.",
      "C) The income gap ratio violates subgroup consistency due to its normalization factor.",
      "D) The Clark et al. family of indices is subgroup consistent but not decomposable."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Foster-SubgroupConsistentPoverty-1991.md"
    ],
    "question_context": "The subgroup consistency property ensures that the aggregate poverty value does not respond perversely to changes in the level of poverty within subgroups of the population. A poverty index $P$ is subgroup consistent if, for every $z\\in D$ and any $x,x^{\\prime},y,y^{\\prime}\\in\\mathcal{D}$ for which $n(x)=n(x^{\\prime})$ and $n(y)=n(y^{\\prime})$, we have $P(x,y;z)>P(x^{\\prime},y^{\\prime};z)$ whenever $P(x;z)>P(x^{\\prime};z)\\qquad\\mathrm{and}\\qquad P(y;z)=P(y^{\\prime};z)$. The axiom places constraints on the number of subgroups and the precise way their poverty levels alter, but these conditions are less restrictive than may appear at first glance. The subgroup consistency property is closely allied to the stronger condition of decomposability. A poverty index $P$ is decomposable if, for every $K\\geq2$ and any $x^{k}\\in{\\mathcal{D}}$ $k=1,\\ldots,K$, $P(x^{1},\\ldots,x^{K};z)=\\sum_{k=1}^{K}\\omega_{k}P(x^{k};z)$, where $\\omega_{k}=n(x^{k})/n(x)$. For decomposable indices, overall poverty is a weighted average of subgroup poverty, where the weights are given by the subgroup population shares. If these population shares stay constant, an increase in the level of poverty in one subgroup will increase overall poverty. So a decomposable index must be subgroup consistent.",
    "category": "INCORRECT",
    "question_context_used": "The subgroup consistency property ensures that the aggregate poverty value does not respond perversely to changes in the level of poverty within subgroups of the population. A poverty index $P$ is subgroup consistent if, for every $z\\in D$ and any $x,x^{\\prime},y,y^{\\prime}\\in\\mathcal{D}$ for which $n(x)=n(x^{\\prime})$ and $n(y)=n(y^{\\prime})$, we have $P(x,y;z)>P(x^{\\prime},y^{\\prime};z)$ whenever $P(x;z)>P(x^{\\prime};z)\\qquad\\mathrm{and}\\qquad P(y;z)=P(y^{\\prime};z)$. The axiom places constraints on the number of subgroups and the precise way their poverty levels alter, but these conditions are less restrictive than may appear at first glance. The subgroup consistency property is closely allied to the stronger condition of decomposability. A poverty index $P$ is decomposable if, for every $K\\geq2$ and any $x^{k}\\in{\\mathcal{D}}$ $k=1,\\ldots,K$, $P(x^{1},\\ldots,x^{K};z)=\\sum_{k=1}^{K}\\omega_{k}P(x^{k};z)$, where $\\omega_{k}=n(x^{k})/n(x)$. For decomposable indices, overall poverty is a weighted average of subgroup poverty, where the weights are given by the subgroup population shares. If these population shares stay constant, an increase in the level of poverty in one subgroup will increase overall poverty. So a decomposable index must be subgroup consistent.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2279",
    "question": "In the context of bivariate multifractal models (MSM), which of the following statements are correct regarding the two-step estimation procedure?",
    "options": [
      "A) The first stage estimates parameters unique to the bivariate model using univariate likelihoods.",
      "B) The second stage requires analytical bivariate likelihood computation for all values of $\\bar{k}$.",
      "C) The two-step procedure is a special case of GMM, ensuring consistency and asymptotic normality.",
      "D) Particle filters are used in the second stage when $\\bar{k}$ exceeds 5 to handle computational complexity."
    ],
    "gold_answer": "C, D",
    "source_files": [
      "Volatility-comovement--a-multifrequency-approach_2006_Journal-of-Econometric.md"
    ],
    "question_context": "Two-step estimation offers additional computational benefits, permitting the econometrician to decompose inference into a sequence of lower-dimensional optimization problems. In the bivariate multifractal, each series $c\\in\\{\\alpha,\\beta\\}$ follows a univariate MSM with parameters $m_{0}^{c},\\sigma_{c},b$ and $\\gamma_{\\overline{{k}}}$ : This implies that we can estimate six of the eight parameters using the likelihood and smaller state space of the univariate model. Additionally, univariate estimation gives good precision in finite samples (Calvet and Fisher, 2004). This motivates us to develop the two-step method described below. The Appendix A shows that this procedure is a special case of GMM, implying consistency and asymptotic normality of the estimator.\n\nThe second stage gives estimates for the remaining two parameters, $(\\rho_{\\varepsilon},\\lambda)$ ; which are unique to the bivariate model. When the state space is not too large, $(\\bar{k}\\leqslant5)$ , computation of the analytical bivariate likelihood is practical. We therefore maximize the exact bivariate MSM probability density conditional on the first-stage estimates. For higher-dimensional specifications, $(\\bar{k}=6,7,8)$ , computation of the analytical bivariate likelihood is difficult. We therefore use the particle filter to optimize the simulated likelihood as described in Section 4.3.12 In this paper, the two-step procedure aids empirical implementation of bivariate specifications with state spaces as large as $2^{16}$.",
    "category": "INCORRECT",
    "question_context_used": "Two-step estimation offers additional computational benefits, permitting the econometrician to decompose inference into a sequence of lower-dimensional optimization problems. In the bivariate multifractal, each series $c\\in\\{\\alpha,\\beta\\}$ follows a univariate MSM with parameters $m_{0}^{c},\\sigma_{c},b$ and $\\gamma_{\\overline{{k}}}$ : This implies that we can estimate six of the eight parameters using the likelihood and smaller state space of the univariate model. Additionally, univariate estimation gives good precision in finite samples (Calvet and Fisher, 2004). This motivates us to develop the two-step method described below. The Appendix A shows that this procedure is a special case of GMM, implying consistency and asymptotic normality of the estimator.\n\nThe second stage gives estimates for the remaining two parameters, $(\\rho_{\\varepsilon},\\lambda)$ ; which are unique to the bivariate model. When the state space is not too large, $(\\bar{k}\\leqslant5)$ , computation of the analytical bivariate likelihood is practical. We therefore maximize the exact bivariate MSM probability density conditional on the first-stage estimates. For higher-dimensional specifications, $(\\bar{k}=6,7,8)$ , computation of the analytical bivariate likelihood is difficult. We therefore use the particle filter to optimize the simulated likelihood as described in Section 4.3.12 In this paper, the two-step procedure aids empirical implementation of bivariate specifications with state spaces as large as $2^{16}$.",
    "gold_answer_parsed": [
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\"]}",
      "model_answer": [
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-390",
    "question": "In the context of social interactions and choice theory, which of the following conditions are necessary for the unique identification of underlying interaction structures from observable behavior?",
    "options": [
      "A) The utility function must be a linear combination of intrinsic utility and the choice probability of others",
      "B) The model must satisfy the Independence of Irrelevant Alternatives (IIA) axiom",
      "C) The choice sets must exhibit sufficient variation to reveal underlying parameters",
      "D) The weights for each alternative must satisfy the condition w_i(x) > 0 for all x ∈ X"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Chambers 等 - 2023 - Behavioral Influence.md"
    ],
    "question_context": "The identification of social interactions from observable behavior is an important and highly topical agenda for economists. We believe that the use of choice theoretic tools to study social interactions introduces a new perspective to this problem that has traditionally been dealt with mostly econometric tools. The strength of our identification strategy relies on the novel source of variation we have introduced: The variation of the choice sets. Key formula for the utility function: \n\n$$ U_{i}(x|S,\\alpha_{i},p_{j}) = w_{i}(x) + \\alpha_{i}p_{j}(x,S) $$\n\nwhere $w_{i}(x)$ represents the intrinsic utility and $\\alpha_{i}$ captures the influence of others' choices. For unique identification, the model must satisfy $w_{i}(x) > 0$ for all $x \\in X$ and the variation in choice sets must be sufficient to reveal the underlying parameters $\\alpha_{i}$ and $w_{i}(x)$.",
    "category": "INCORRECT",
    "question_context_used": "The identification of social interactions from observable behavior is an important and highly topical agenda for economists. We believe that the use of choice theoretic tools to study social interactions introduces a new perspective to this problem that has traditionally been dealt with mostly econometric tools. The strength of our identification strategy relies on the novel source of variation we have introduced: The variation of the choice sets. Key formula for the utility function: \n\n$$ U_{i}(x|S,\\alpha_{i},p_{j}) = w_{i}(x) + \\alpha_{i}p_{j}(x,S) $$\n\nwhere $w_{i}(x)$ represents the intrinsic utility and $\\alpha_{i}$ captures the influence of others' choices. For unique identification, the model must satisfy $w_{i}(x) > 0$ for all $x \\in X$ and the variation in choice sets must be sufficient to reveal the underlying parameters $\\alpha_{i}$ and $w_{i}(x)$.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1944",
    "question": "Based on the analysis of intergenerational mobility in education in South Africa, which of the following statements are supported by the data?",
    "options": [
      "A) Parental education has a uniformly smaller impact on white children compared to other racial groups.",
      "B) Intergenerational mobility has remained constant for all racial groups across cohorts.",
      "C) The average education of Asians, blacks, and coloreds has risen significantly over time, independent of parental education.",
      "D) Maternal education has a larger impact on black and Asian children than on white or colored children."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Thomas-EducationAcrossGenerations-1996.md"
    ],
    "question_context": "Educational attainment is influenced by both public and private investments, with parental resources being key determinants of child education. In a rigid society with no mobility, parents' education fully determines one's own schooling. Among adults aged 20-70 in the 1991/1993 South African Social Stratification Survey, one-third of the variation in education of Asians is explained by parental education alone. Controlling for age and gender, maternal and paternal education significantly influences children's education. The effect of paternal schooling is smaller for whites but essentially the same for the other three groups: an additional year of father's education translates into an additional quarter-year of child schooling. An additional year of maternal education has a bigger impact on black and Asian children (0.3-0.4 year) than on whites or coloreds (0.2 year).\n\nThus, parental education has the biggest impact on children's education among blacks and Asians, the two groups for whom education has risen the most over the last 50 years. Put another way, intergenerational mobility is lowest among them. Nevertheless, increases in educational attainment across cohorts are large in comparison with the impact of parental education: the average education of Asians, blacks, and even coloreds has risen each three or four years by the same amount that would be associated with a parent having an additional year of schooling.",
    "category": "INCORRECT",
    "question_context_used": "Educational attainment is influenced by both public and private investments, with parental resources being key determinants of child education. In a rigid society with no mobility, parents' education fully determines one's own schooling. Among adults aged 20-70 in the 1991/1993 South African Social Stratification Survey, one-third of the variation in education of Asians is explained by parental education alone. Controlling for age and gender, maternal and paternal education significantly influences children's education. The effect of paternal schooling is smaller for whites but essentially the same for the other three groups: an additional year of father's education translates into an additional quarter-year of child schooling. An additional year of maternal education has a bigger impact on black and Asian children (0.3-0.4 year) than on whites or coloreds (0.2 year).\n\nThus, parental education has the biggest impact on children's education among blacks and Asians, the two groups for whom education has risen the most over the last 50 years. Put another way, intergenerational mobility is lowest among them. Nevertheless, increases in educational attainment across cohorts are large in comparison with the impact of parental education: the average education of Asians, blacks, and even coloreds has risen each three or four years by the same amount that would be associated with a parent having an additional year of schooling.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3137",
    "question": "In the context of the Pacific Salmon Treaty model with moral hazard and side payments, which of the following conditions are necessary for the optimal contract to improve total welfare?",
    "options": [
      "A) The contract must include both variation in continuation payoffs and side payments as incentive devices",
      "B) The sharing rule must assign a larger share to the more efficient producer (USA)",
      "C) The transaction cost of side payments (τ) must be zero to ensure efficiency",
      "D) The contract must completely eliminate all stochastic shocks (σ₁, σ₂) to utility flows"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "On_the_Pacific_Salmon_Treaty.md"
    ],
    "question_context": "The article studies the optimal design of the Pacific Salmon Treaty between the USA and Canada, featuring a continuous-time hidden-action game with moral hazard. The model incorporates two incentive devices: (i) variation of continuation payoffs in response to public signals, and (ii) side payments when participation constraints bind. The utility flows are given by:\n\n$$\n\\begin{array}{r}{(p_{1}(x_{t})+e_{1t})d t+(-(1+\\mu)e_{2t}d t+\\sigma_{1}d Z_{1t}),}\\ {(p_{2}(x_{t})+e_{2t})d t+(-(1+\\mu)e_{1t}d t+\\sigma_{2}d Z_{2t}).}\\end{array}\n$$\n\nEmpirical results show that assigning a larger share to the more efficient producer (USA) improves total welfare by 1.55%. The optimal contract utilizes both incentive mechanisms while acknowledging transaction costs (τ > 0) and environmental uncertainty (σ₁, σ₂ > 0).",
    "category": "CORRECT",
    "question_context_used": "The article studies the optimal design of the Pacific Salmon Treaty between the USA and Canada, featuring a continuous-time hidden-action game with moral hazard. The model incorporates two incentive devices: (i) variation of continuation payoffs in response to public signals, and (ii) side payments when participation constraints bind. The utility flows are given by:\n\n$$\n\\begin{array}{r}{(p_{1}(x_{t})+e_{1t})d t+(-(1+\\mu)e_{2t}d t+\\sigma_{1}d Z_{1t}),}\\ {(p_{2}(x_{t})+e_{2t})d t+(-(1+\\mu)e_{1t}d t+\\sigma_{2}d Z_{2t}).}\\end{array}\n$$\n\nEmpirical results show that assigning a larger share to the more efficient producer (USA) improves total welfare by 1.55%. The optimal contract utilizes both incentive mechanisms while acknowledging transaction costs (τ > 0) and environmental uncertainty (σ₁, σ₂ > 0).",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-818",
    "question": "In the context of measuring government economic activity, which of the following are significant challenges identified by George F. Break?",
    "options": [
      "A) The exclusion of non-market activities from GDP calculations",
      "B) The difficulty in accounting for the shadow economy",
      "C) The lack of standardized metrics for public sector efficiency",
      "D) The overestimation of government expenditures due to double-counting"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Break-IssuesMeasuringLevel-1982.md"
    ],
    "question_context": "George F. Break highlights the complexities in accurately measuring government economic activity, particularly the exclusion of non-market activities which are not captured in traditional GDP metrics. Another critical issue is the shadow economy, which includes unreported income and unrecorded transactions, further complicating the assessment of true economic activity. These challenges underscore the limitations of conventional economic indicators in providing a comprehensive picture of government involvement in the economy.",
    "category": "CORRECT",
    "question_context_used": "George F. Break highlights the complexities in accurately measuring government economic activity, particularly the exclusion of non-market activities which are not captured in traditional GDP metrics. Another critical issue is the shadow economy, which includes unreported income and unrecorded transactions, further complicating the assessment of true economic activity. These challenges underscore the limitations of conventional economic indicators in providing a comprehensive picture of government involvement in the economy.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1706",
    "question": "In the context of Bayesian hierarchical hidden Markov chain (HMC) models for forecasting time series with structural breaks, which of the following statements are correct?",
    "options": [
      "A) The transition probability matrix in the HMC model assumes a constant hazard rate for regime durations.",
      "B) The hierarchical prior structure allows parameters across regimes to be drawn from a common meta-distribution.",
      "C) The Gibbs sampler for the HMC model requires only a forward pass through the data to simulate the states.",
      "D) The meta-distribution's hyperparameters are updated using maximum likelihood estimation."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Forecasting Time Series Subject to Multiple Structural Breaks.md"
    ],
    "question_context": "The paper introduces a Bayesian hierarchical hidden Markov chain (HMC) model to forecast time series subject to structural breaks. The transition probability matrix in the HMC model is constrained to reflect a multiple-change point model, with regime durations following a geometric distribution due to the assumption of a constant hazard rate. The hierarchical prior structure assumes that parameters within each break segment are drawn from a common meta-distribution, which is updated using Bayes' rule as new regimes occur. Key formula: The transition probability matrix is given by \n\n$$ \n\\mathbf{P}=\\left(\\begin{array}{c c c c c c}{p_{11}}&{p_{12}}&{0}&{\\cdots}&{}&{0}\\\\ {0}&{p_{22}}&{p_{23}}&{\\cdots}&{}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{}&{\\vdots}\\\\ {0}&{\\cdots}&{0}&{p_{K K}}&{p_{K,K+1}}\\\\ {0}&{0}&{\\cdots}&{0}&{}&{1}\\end{array}\\right), \n$$ \n\nwhere $p_{i i}+p_{i,i+1}=1$ and $p_{K+1,K+1}=1$. The Gibbs sampler for the HMC model involves both forward and backward passes through the data to simulate the states, and the meta-distribution's hyperparameters are updated using Bayesian methods, not maximum likelihood estimation.",
    "category": "INCORRECT",
    "question_context_used": "The paper introduces a Bayesian hierarchical hidden Markov chain (HMC) model to forecast time series subject to structural breaks. The transition probability matrix in the HMC model is constrained to reflect a multiple-change point model, with regime durations following a geometric distribution due to the assumption of a constant hazard rate. The hierarchical prior structure assumes that parameters within each break segment are drawn from a common meta-distribution, which is updated using Bayes' rule as new regimes occur. Key formula: The transition probability matrix is given by \n\n$$ \n\\mathbf{P}=\\left(\\begin{array}{c c c c c c}{p_{11}}&{p_{12}}&{0}&{\\cdots}&{}&{0}\\\\ {0}&{p_{22}}&{p_{23}}&{\\cdots}&{}&{0}\\\\ {\\vdots}&{\\vdots}&{\\vdots}&{\\vdots}&{}&{\\vdots}\\\\ {0}&{\\cdots}&{0}&{p_{K K}}&{p_{K,K+1}}\\\\ {0}&{0}&{\\cdots}&{0}&{}&{1}\\end{array}\\right), \n$$ \n\nwhere $p_{i i}+p_{i,i+1}=1$ and $p_{K+1,K+1}=1$. The Gibbs sampler for the HMC model involves both forward and backward passes through the data to simulate the states, and the meta-distribution's hyperparameters are updated using Bayesian methods, not maximum likelihood estimation.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1959",
    "question": "In the triple differences design used to estimate the impact of government data access on AI software production, which of the following are critical for identification?",
    "options": [
      "A) Firm fixed effects to account for time-invariant heterogeneity",
      "B) Time period fixed effects to control for common shocks",
      "C) Clustering standard errors at the mother firm-level",
      "D) Exogenous assignment of all public security contracts"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Data-intensive Innovation and the State Evidence from AI Firms in China.md"
    ],
    "question_context": "The empirical strategy exploits variation across time and across firms in the receipt of a public security contract, and across the data-richness of the contracts that firms receive. Specifically, as in an event study design, we compare firms’ AI software releases before and after they receive their first public security contracts, controlling for firm and time period fixed effects. The empirical model is given by: $$y_{i t}=\\sum_{T}\\beta_{1T}T_{i t}D a t a_{i}+\\sum_{T}\\beta_{2T}T_{i t}+\\alpha_{t}+\\gamma_{i}+\\epsilon_{i t}.$$ The coefficients $\\beta_{1T}$ non-parametrically capture a firm’s differential production of new software approaching or following the arrival of initial data-rich contracts, relative to data-scarce ones. The identifying assumption is the exogenous access to greater amounts of government data conditional on the receipt of any public security contract and the controls we include.",
    "category": "INCORRECT",
    "question_context_used": "The empirical strategy exploits variation across time and across firms in the receipt of a public security contract, and across the data-richness of the contracts that firms receive. Specifically, as in an event study design, we compare firms’ AI software releases before and after they receive their first public security contracts, controlling for firm and time period fixed effects. The empirical model is given by: $$y_{i t}=\\sum_{T}\\beta_{1T}T_{i t}D a t a_{i}+\\sum_{T}\\beta_{2T}T_{i t}+\\alpha_{t}+\\gamma_{i}+\\epsilon_{i t}.$$ The coefficients $\\beta_{1T}$ non-parametrically capture a firm’s differential production of new software approaching or following the arrival of initial data-rich contracts, relative to data-scarce ones. The identifying assumption is the exogenous access to greater amounts of government data conditional on the receipt of any public security contract and the controls we include.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2433",
    "question": "In the context of robust bootstrap methods for conditional moment tests with nuisance parameters, which of the following statements are correct regarding the proposed method by Hill (2020)?",
    "options": [
      "A) The method requires knowledge of the covariance kernel of the simulated stochastic process.",
      "B) The method combines bootstrapped p-values across polar identification cases to form an asymptotically valid p-value approximation.",
      "C) The method is robust to any degree of identification, including weak and nonidentification.",
      "D) The method assumes strong identification universally for all parameters."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "WEAK-IDENTIFICATION_ROBUST_WILD_BOOTSTRAP_APPLIED_TO_A_CONSISTENT_MODEL_SPECIFICATION_TEST.md"
    ],
    "question_context": "The paper presents a new robust bootstrap method for conditional moment tests when there is a nuisance parameter under the alternative, and some parameters are possibly weakly or nonidentified. The method focuses on a Bierens (1990)-type conditional moment test of omitted nonlinearity. Key formula: The proposed method combines bootstrapped p-values across polar identification cases to form an asymptotically valid p-value approximation that is robust to any identification case. The wild bootstrap procedure does not require knowledge of the covariance structure of the bootstrapped processes, unlike Andrews and Cheng’s (2012a, 2013, 2014) simulation approach. The method allows for robust bootstrap critical value computation and achieves uniform size control for any degree of identification. The paper also demonstrates through simulation studies that the proposed method delivers accurate empirical size and comparatively high power under weak or nonidentification scenarios.",
    "category": "CORRECT",
    "question_context_used": "The paper presents a new robust bootstrap method for conditional moment tests when there is a nuisance parameter under the alternative, and some parameters are possibly weakly or nonidentified. The method focuses on a Bierens (1990)-type conditional moment test of omitted nonlinearity. Key formula: The proposed method combines bootstrapped p-values across polar identification cases to form an asymptotically valid p-value approximation that is robust to any identification case. The wild bootstrap procedure does not require knowledge of the covariance structure of the bootstrapped processes, unlike Andrews and Cheng’s (2012a, 2013, 2014) simulation approach. The method allows for robust bootstrap critical value computation and achieves uniform size control for any degree of identification. The paper also demonstrates through simulation studies that the proposed method delivers accurate empirical size and comparatively high power under weak or nonidentification scenarios.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1772",
    "question": "In the context of endogenous institutional choice in public goods games, which of the following statements are supported by the experimental findings?",
    "options": [
      "A) Endogenous institutional choice leads to higher cooperation levels compared to exogenously imposed institutions, regardless of the institution type.",
      "B) Groups predominantly choose the punishment option when the leverage is high, due to its effectiveness in sustaining cooperation.",
      "C) The reward option is almost exclusively chosen when the leverage is high, despite punishment being more effective in raising contributions.",
      "D) The standard VCM is preferred when the leverage is low, as subjects expect minimal effects from reward or punishment."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Choosing the Carrot or the Stick Endogenous Institutional Choice in Social Dilemma Situations.md"
    ],
    "question_context": "The paper analyzes an experimental public goods game where group members can endogenously decide whether to supplement a standard voluntary contribution mechanism (VCM) with reward or punishment options. Key findings include a significantly positive effect of endogenous institutional choice on cooperation levels compared to exogenously imposed institutions. Groups typically vote for the reward option when the leverage is high, even though punishment is more effective in sustaining high cooperation levels. The standard VCM is preferred when the leverage is low, as subjects anticipate minimal effects from reward or punishment. The formula for individual payoff in the VCM with punishment or reward is given by: $$\\pi_{i,t}=E-c_{i,t}+\\gamma C_{t}+L\\sum_{k\\neq i}p_{i k,t}-\\sum_{h\\neq i}p_{h i,t}$$ where $L$ represents the leverage of the punishment or reward technology.",
    "category": "INCORRECT",
    "question_context_used": "The paper analyzes an experimental public goods game where group members can endogenously decide whether to supplement a standard voluntary contribution mechanism (VCM) with reward or punishment options. Key findings include a significantly positive effect of endogenous institutional choice on cooperation levels compared to exogenously imposed institutions. Groups typically vote for the reward option when the leverage is high, even though punishment is more effective in sustaining high cooperation levels. The standard VCM is preferred when the leverage is low, as subjects anticipate minimal effects from reward or punishment. The formula for individual payoff in the VCM with punishment or reward is given by: $$\\pi_{i,t}=E-c_{i,t}+\\gamma C_{t}+L\\sum_{k\\neq i}p_{i k,t}-\\sum_{h\\neq i}p_{h i,t}$$ where $L$ represents the leverage of the punishment or reward technology.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1274",
    "question": "In the context of large-dimensional factor models under elliptical distributions, which of the following statements are true regarding the proposed Robust Two-Step (RTS) procedure?",
    "options": [
      "A) The RTS procedure estimates factor loadings by performing PCA on the sample covariance matrix.",
      "B) The RTS procedure does not require any moment constraints on the factors or idiosyncratic errors.",
      "C) The RTS procedure is computationally more intensive than the maximum likelihood estimation (MLE) method.",
      "D) The RTS procedure leverages the spatial Kendall’s tau matrix to estimate the factor space."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "21-Large-Dimensional_Factor_Analysis_Without_Moment_Constraints.md"
    ],
    "question_context": "The article proposes a Robust Two-Step (RTS) procedure for estimating factor loadings, scores, and common components in large-dimensional factor models under elliptical distributions. The first step involves recovering the factor space by performing PCA on the spatial Kendall’s tau matrix instead of the sample covariance matrix. The spatial Kendall’s tau matrix shares the same eigenspace with the scatter matrix of elliptically distributed data vectors, making the method robust to heavy-tailedness. The second step estimates factor scores via ordinary least squares regression. Key formula: The scatter matrix has nothing to do with the moment of the data, enabling consistent estimation without moment constraints. The RTS method outperforms PCA in heavy-tailed settings, as demonstrated by simulations with $t$-distributions ($t_3$, $t_2$, $t_1$).",
    "category": "CORRECT",
    "question_context_used": "The article proposes a Robust Two-Step (RTS) procedure for estimating factor loadings, scores, and common components in large-dimensional factor models under elliptical distributions. The first step involves recovering the factor space by performing PCA on the spatial Kendall’s tau matrix instead of the sample covariance matrix. The spatial Kendall’s tau matrix shares the same eigenspace with the scatter matrix of elliptically distributed data vectors, making the method robust to heavy-tailedness. The second step estimates factor scores via ordinary least squares regression. Key formula: The scatter matrix has nothing to do with the moment of the data, enabling consistent estimation without moment constraints. The RTS method outperforms PCA in heavy-tailed settings, as demonstrated by simulations with $t$-distributions ($t_3$, $t_2$, $t_1$).",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-827",
    "question": "Based on the simulation studies comparing CPD, CH-GP, and HHB methods, which of the following statements are correct regarding their performance in subgroup identification?",
    "options": [
      "A) The CPD method achieves higher accuracy in subgroup identification as sample size increases, with correct identification rates exceeding 90%.",
      "B) The HHB method consistently outperforms CPD and CH-GP in cases with heavy-tailed distributions (e.g., Case VI).",
      "C) The CH-GP method shows comparable performance to CPD in cases with multiple subgroups (Cases VII-VIII).",
      "D) The HHB method fails to identify subgroups correctly in Cases II, III, and IV due to its design limitations for cross-sectional data."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "07-Multi-Threshold_Structural_Equation_Model.md"
    ],
    "question_context": "The simulation studies evaluate the performance of CPD, CH-GP, and HHB methods across eight cases with varying data distributions and subgroup structures. Key findings include: For Cases VII and VIII with three subgroups, CPD and CH-GP demonstrate similar accuracy, both outperforming HHB. The HHB method, designed for time series data, shows poor performance in cross-sectional settings (Cases II-IV) with coverage rates as low as 28%. Formula: The correct identification rate $R$ exceeds 90% for CPD when $n$ increases, while HHB's coverage rate for $\\beta_{e2}$ drops to 16% in Case IV. Heavy-tailed distributions (Case VI) further degrade HHB's performance, with coverage rates of 60% even at $n=500$.",
    "category": "CORRECT",
    "question_context_used": "The simulation studies evaluate the performance of CPD, CH-GP, and HHB methods across eight cases with varying data distributions and subgroup structures. Key findings include: For Cases VII and VIII with three subgroups, CPD and CH-GP demonstrate similar accuracy, both outperforming HHB. The HHB method, designed for time series data, shows poor performance in cross-sectional settings (Cases II-IV) with coverage rates as low as 28%. Formula: The correct identification rate $R$ exceeds 90% for CPD when $n$ increases, while HHB's coverage rate for $\\beta_{e2}$ drops to 16% in Case IV. Heavy-tailed distributions (Case VI) further degrade HHB's performance, with coverage rates of 60% even at $n=500$.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2834",
    "question": "In the Poisson equilibrium of the LUPI game, which of the following properties are satisfied by the unique mixed equilibrium strategy $\\mathbf{p}=(p_{1},p_{2},\\cdots,p_{K})$?",
    "options": [
      "A) Full support: $p_{k}>0$ for all $k$",
      "B) Increasing probabilities: $p_{k+1}>p_{k}$ for all $k$",
      "C) Convergence to uniform play as $n\\to\\infty$ for fixed $K$",
      "D) Probability asymptotes to zero as $K\\to\\infty$ for fixed $n$"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "stling-TestingGameTheory-2011.md"
    ],
    "question_context": "In the (symmetric) Poisson equilibrium, all players employ the same mixed strategy $\\mathbf{p}=\\left(p_{1},p_{2},\\ldots,p_{K}\\right)$ where $\\dot{\\sum}_{i=1}^{K}p_{i}=1$. The equilibrium is characterized by the following properties:\n\n1) Full support: $p_{k}>0$ for all $k$.\n2) Decreasing probabilities: $p_{k+1}<p_{k}$ for all $k$.\n3) Convergence to uniform play with many players: for any fixed $K$, $n\\to\\infty$ implies $p_{k+1} \\to p_{k}$.\n4) Probability asymptotes to zero with more numbers to guess: for any fixed $n$, $K\\to\\infty$ implies $p_{K}\\to0$.\n\nThe expected payoffs for choosing different numbers are given by:\n\n$$\n\\pi(k)=\\left(\\sum_{i=1}^{k-1}\\left[1-n p_{i}e^{-m_{1}}\\right]\\right)\\cdot e^{-m_{0}}\n$$\n\nwhere $m_{0}$ and $m_{1}$ are parameters derived from the Poisson distribution.",
    "category": "CORRECT",
    "question_context_used": "In the (symmetric) Poisson equilibrium, all players employ the same mixed strategy $\\mathbf{p}=\\left(p_{1},p_{2},\\ldots,p_{K}\\right)$ where $\\dot{\\sum}_{i=1}^{K}p_{i}=1$. The equilibrium is characterized by the following properties:\n\n1) Full support: $p_{k}>0$ for all $k$.\n2) Decreasing probabilities: $p_{k+1}<p_{k}$ for all $k$.\n3) Convergence to uniform play with many players: for any fixed $K$, $n\\to\\infty$ implies $p_{k+1} \\to p_{k}$.\n4) Probability asymptotes to zero with more numbers to guess: for any fixed $n$, $K\\to\\infty$ implies $p_{K}\\to0$.\n\nThe expected payoffs for choosing different numbers are given by:\n\n$$\n\\pi(k)=\\left(\\sum_{i=1}^{k-1}\\left[1-n p_{i}e^{-m_{1}}\\right]\\right)\\cdot e^{-m_{0}}\n$$\n\nwhere $m_{0}$ and $m_{1}$ are parameters derived from the Poisson distribution.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3782",
    "question": "In the context of nonparametric testing for symmetry in demand systems, which of the following statements are supported by the Monte Carlo analysis?",
    "options": [
      "A) The bootstrap-based test outperforms Lewbel's sup-norm test in detecting small deviations from symmetry.",
      "B) Using a bandwidth smaller than the recommended choice significantly reduces the test's power against alternatives.",
      "C) Sample sizes below 1000 observations lead to over-rejection of the null hypothesis.",
      "D) The test's performance is highly sensitive to changes in the model specification (e.g., QAI vs. AI models)."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Testing-and-imposing-Slutsky-symmetry-in-nonparametri_2009_Journal-of-Econom.md"
    ],
    "question_context": "The Monte Carlo analysis compares the bootstrap-based symmetry test with Lewbel's (1995) sup-norm test. Key findings include: (1) The bootstrap test has greater power to detect small deviations from symmetry while maintaining correct size. (2) Bandwidth selection affects test size but not power substantially. (3) Sample sizes below 1000 lead to size distortion (9.4% rejection at n=500 vs 5% nominal size). The test statistic under H₀ approximates:\n\n$$\n\\hat{T}_{S}\\approx\\frac{1}{n}\\sum_{j=1}^{M-2}\\sum_{k=j+1}^{M-1}\\sum_{i=1}^{n}\\Bigl(\\sum_{l=1}^{n}V_{i l}^{j k}\\varepsilon_{l}^{j}+V_{i l}^{k j}\\varepsilon_{l}^{k}\\Bigr)^{2}A_{i}.\n$$\n\nRemarkably, changing from QAI to Almost Ideal or homothetic specifications did not affect test performance. The bootstrap procedure matches moments:\n\n$$\n\\mathbb{E}_{\\hat{F}_{i}}\\varepsilon_{i}^{*}=0,\\ \\mathbb{E}_{\\hat{F}_{i}}\\varepsilon_{i}^{*}(\\varepsilon_{i}^{*})^{\\prime}=\\hat{\\varepsilon}_{i}(\\hat{\\varepsilon}_{i})^{\\prime},\\ \\mathbb{E}_{\\hat{F}_{i}}(\\varepsilon_{i}^{j,*})^{3}=(\\hat{\\varepsilon}_{i}^{j})^{3}.\n$$",
    "category": "CORRECT",
    "question_context_used": "The Monte Carlo analysis compares the bootstrap-based symmetry test with Lewbel's (1995) sup-norm test. Key findings include: (1) The bootstrap test has greater power to detect small deviations from symmetry while maintaining correct size. (2) Bandwidth selection affects test size but not power substantially. (3) Sample sizes below 1000 lead to size distortion (9.4% rejection at n=500 vs 5% nominal size). The test statistic under H₀ approximates:\n\n$$\n\\hat{T}_{S}\\approx\\frac{1}{n}\\sum_{j=1}^{M-2}\\sum_{k=j+1}^{M-1}\\sum_{i=1}^{n}\\Bigl(\\sum_{l=1}^{n}V_{i l}^{j k}\\varepsilon_{l}^{j}+V_{i l}^{k j}\\varepsilon_{l}^{k}\\Bigr)^{2}A_{i}.\n$$\n\nRemarkably, changing from QAI to Almost Ideal or homothetic specifications did not affect test performance. The bootstrap procedure matches moments:\n\n$$\n\\mathbb{E}_{\\hat{F}_{i}}\\varepsilon_{i}^{*}=0,\\ \\mathbb{E}_{\\hat{F}_{i}}\\varepsilon_{i}^{*}(\\varepsilon_{i}^{*})^{\\prime}=\\hat{\\varepsilon}_{i}(\\hat{\\varepsilon}_{i})^{\\prime},\\ \\mathbb{E}_{\\hat{F}_{i}}(\\varepsilon_{i}^{j,*})^{3}=(\\hat{\\varepsilon}_{i}^{j})^{3}.\n$$",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2187",
    "question": "In the context of endogenous business cycles in overlapping-generations models, which of the following conditions are sufficient for the existence of cycles of period two when preferences are non-homothetic?",
    "options": [
      "A) The Engel curve has a point where the slope is smaller than one at equal prices",
      "B) The individual discount rate is close to zero",
      "C) The relative risk aversion is at a high level",
      "D) The endowments are positive and the discount factor is smaller than one"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "On-the-Existence-of-Endogenous-Cycles_1995_Journal-of-Economic-Theory.md"
    ],
    "question_context": "Endogenous business cycles have been known to exist in the overlapping-generations model for quite a long time. The literature seems to imply that these cycles may exist only if individuals show rates of preference for the present that are high enough, and these values must be so high that they are unrealistic. While this observation is true if preferences are homothetic, we show in this paper that, once the homotheticity requirement is dropped, a sufficient condition for the existence of cycles is that the Engel curve defined for equal prices (therefore, taken equal to one) has at least a point where the slope is smaller than one. This condition expresses a mild form of impatience. Given this condition, it is always possible to find a discounted utility function (for any arbitrary given discount rate smaller than one) and positive endowments such that the corresponding overlapping-generations model features cycles of order two.\n\nOur main result is that it suffices that the slope of the tangent to the Engel curve associated with unit prices is smaller than one at some point. This slope condition is easily satisfied for any arbitrary discount factor smaller than one; i.e., we can then find discounted utility functions and positive initial endowments for which the resulting overlapping-generations model displays cycles of period two. Incidentally, existence then does not require the high level of relative risk aversion necessary when preferences are homothetic.",
    "category": "CORRECT",
    "question_context_used": "Endogenous business cycles have been known to exist in the overlapping-generations model for quite a long time. The literature seems to imply that these cycles may exist only if individuals show rates of preference for the present that are high enough, and these values must be so high that they are unrealistic. While this observation is true if preferences are homothetic, we show in this paper that, once the homotheticity requirement is dropped, a sufficient condition for the existence of cycles is that the Engel curve defined for equal prices (therefore, taken equal to one) has at least a point where the slope is smaller than one. This condition expresses a mild form of impatience. Given this condition, it is always possible to find a discounted utility function (for any arbitrary given discount rate smaller than one) and positive endowments such that the corresponding overlapping-generations model features cycles of order two.\n\nOur main result is that it suffices that the slope of the tangent to the Engel curve associated with unit prices is smaller than one at some point. This slope condition is easily satisfied for any arbitrary discount factor smaller than one; i.e., we can then find discounted utility functions and positive initial endowments for which the resulting overlapping-generations model displays cycles of period two. Incidentally, existence then does not require the high level of relative risk aversion necessary when preferences are homothetic.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-840",
    "question": "In the context of unobservable aggregate shocks and lagged information, which of the following statements are correct regarding the behavior of individual and aggregate consumption?",
    "options": [
      "A) Individual consumption changes follow a martingale with respect to the history of individual income and consumption.",
      "B) Aggregate consumption changes follow a random walk as predicted by the representative agent model.",
      "C) The orthogonality condition holds at all further lags in the lagged information model because agents incorporate all aggregate information after one period.",
      "D) Excess smoothness in aggregate consumption can arise even if aggregate shocks are more persistent than individual shocks."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Pischke-IndividualIncomeIncomplete-1995.md"
    ],
    "question_context": "The income process the individual observes can be written as \n\n$$\\begin{array}{r}{\\varDelta y_{i t}=\\eta_{i t}-\\theta\\eta_{i t-1}.}\\end{array}$$ \n\nThe random variable $\\eta_{i t}$ will contain information on current and lagged aggregate and individual income innovations. Note that $\\{\\eta_{i t}\\}$ , though not a fundamental driving process of the model, is an innovation sequence with respect to the history of individual income changes. The MA parameter $\\pmb{\\theta}$ in (6) depends on the relative variances of the aggregate and individual income shocks.\n\nEquation (1) still holds so that changes in individual consumption follow \n\n$$\\Delta c_{i t}=\\left(1-\\frac{\\theta}{1+r}\\right)\\eta_{i t}=\\frac{1+r-\\theta}{1+r}\\eta_{i t}\\equiv A\\eta_{i t}=A\\frac{\\varDelta y_{i t}}{1-\\theta L}.$$ \n\nIndividual consumption changes are a martingale with respect to the history of individual consumption and income. A researcher doing Hall's (1978) analysis on panel data for individuals should not reject the permanent income model. This type of testing procedure has been carried out, for example, by Altonji and Siow (1987) who do not reject the model. Estimating a structural model as in Hall and Mishkin (1982) would not be correct because their model assumes that consumers know the income components in (2). The correct structural model would use the income process in (6) instead. This has been pointed out by Speight (no date) who finds support for the model with incomplete information on Austrian panel data while the Hall and Mishkin model is rejected.",
    "category": "INCORRECT",
    "question_context_used": "The income process the individual observes can be written as \n\n$$\\begin{array}{r}{\\varDelta y_{i t}=\\eta_{i t}-\\theta\\eta_{i t-1}.}\\end{array}$$ \n\nThe random variable $\\eta_{i t}$ will contain information on current and lagged aggregate and individual income innovations. Note that $\\{\\eta_{i t}\\}$ , though not a fundamental driving process of the model, is an innovation sequence with respect to the history of individual income changes. The MA parameter $\\pmb{\\theta}$ in (6) depends on the relative variances of the aggregate and individual income shocks.\n\nEquation (1) still holds so that changes in individual consumption follow \n\n$$\\Delta c_{i t}=\\left(1-\\frac{\\theta}{1+r}\\right)\\eta_{i t}=\\frac{1+r-\\theta}{1+r}\\eta_{i t}\\equiv A\\eta_{i t}=A\\frac{\\varDelta y_{i t}}{1-\\theta L}.$$ \n\nIndividual consumption changes are a martingale with respect to the history of individual consumption and income. A researcher doing Hall's (1978) analysis on panel data for individuals should not reject the permanent income model. This type of testing procedure has been carried out, for example, by Altonji and Siow (1987) who do not reject the model. Estimating a structural model as in Hall and Mishkin (1982) would not be correct because their model assumes that consumers know the income components in (2). The correct structural model would use the income process in (6) instead. This has been pointed out by Speight (no date) who finds support for the model with incomplete information on Austrian panel data while the Hall and Mishkin model is rejected.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3965",
    "question": "In the context of the skew-t distribution model for panel data clustering proposed by Juárez and Steel (2010), which of the following statements are correct regarding the interpretation and impact of skewness and fat tails?",
    "options": [
      "A) The skewness parameter γ > 1 indicates negative skewness in the error distribution.",
      "B) The degrees of freedom ν in the Student-t distribution controls the tail behavior, with smaller values indicating heavier tails.",
      "C) Ignoring skewness in the model can lead to incorrect conclusions about long-run equilibrium levels, as demonstrated in the Spanish firm employment example.",
      "D) The skew-t distribution retains a zero mode, which is crucial for interpreting the individual effects βi as long-run modal tendencies."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "04-Model-Based_Clustering_of_Non-Gaussian_Panel_Data_Based_on_Skew-t_Distributions.md"
    ],
    "question_context": "The model proposed by Juárez and Steel (2010) uses a skew-t distribution for the error term, defined as: \n\n$$ t_{\\nu}^{s}(\\varepsilon|\\gamma)=\\frac{2}{\\gamma+\\gamma^{-1}}\\frac{\\Gamma[(\\nu+1)/2]}{\\Gamma[\\nu/2]}\\sqrt{\\frac{1}{\\nu\\pi}} \\left[1+\\frac{1}{\\nu}\\varepsilon^{2}(\\gamma^{2}1_{[\\varepsilon\\leq0]}+\\gamma^{-2}1_{[\\varepsilon>0]})\\right]^{-(\\nu+1)/2}, $$\n\nwhere γ > 0 controls skewness (γ = 1 implies symmetry, γ > 1 implies positive skewness, and γ < 1 implies negative skewness), and ν controls tail behavior (smaller ν implies heavier tails). The model's flexibility allows for clustering based on dynamic behavior, equilibrium levels, and covariate effects, with Bayesian inference conducted using hierarchical priors. The Spanish firm employment example showed that ignoring skewness led to incorrect conclusions about long-run employment growth, highlighting the importance of the skew-t specification.",
    "category": "INCORRECT",
    "question_context_used": "The model proposed by Juárez and Steel (2010) uses a skew-t distribution for the error term, defined as: \n\n$$ t_{\\nu}^{s}(\\varepsilon|\\gamma)=\\frac{2}{\\gamma+\\gamma^{-1}}\\frac{\\Gamma[(\\nu+1)/2]}{\\Gamma[\\nu/2]}\\sqrt{\\frac{1}{\\nu\\pi}} \\left[1+\\frac{1}{\\nu}\\varepsilon^{2}(\\gamma^{2}1_{[\\varepsilon\\leq0]}+\\gamma^{-2}1_{[\\varepsilon>0]})\\right]^{-(\\nu+1)/2}, $$\n\nwhere γ > 0 controls skewness (γ = 1 implies symmetry, γ > 1 implies positive skewness, and γ < 1 implies negative skewness), and ν controls tail behavior (smaller ν implies heavier tails). The model's flexibility allows for clustering based on dynamic behavior, equilibrium levels, and covariate effects, with Bayesian inference conducted using hierarchical priors. The Spanish firm employment example showed that ignoring skewness led to incorrect conclusions about long-run employment growth, highlighting the importance of the skew-t specification.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2566",
    "question": "In the simulated no-commitment model of household intertemporal behavior, which of the following statements are supported by the empirical evidence regarding the effect of changes in relative decision power?",
    "options": [
      "A) An increase in the wife's income reduces the husband's decision power by approximately 13% at the mean.",
      "B) Households dominated by the husband save 10% less than those dominated by the wife when initial wealth is at the first quartile.",
      "C) A shift in decision power in favor of the husband increases private consumption by 24% at the mean.",
      "D) Public consumption is unaffected by changes in relative decision power due to offsetting effects of risk aversion and preference heterogeneity."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Household Intertemporal Behaviour A Collective Characterization and a Test of Commitment.md"
    ],
    "question_context": "The no-commitment model is simulated using a recursive formulation of household behavior, with utility functions given by:\n\n$$\nu^{i}(c^{i},Q)=\\frac{[(c^{i})^{\\sigma_{i}}(Q)^{1-\\sigma_{i}}]^{1-\\gamma_{i}}}{(1-\\gamma_{i})},\n$$\n\nwhere $\\gamma_{i}>0$ and $0<\\sigma_{i}<1$. The parameter $\\gamma_{i}$ captures intertemporal elasticity of substitution, while $\\sigma_{i}$ measures intraperiod allocation to private consumption. Empirical estimates show $\\gamma$ is 2.5 for men and 4.5 for women, with $\\sigma$ assumed to be 0.5 for women and 0.6 for men.\n\nThe simulation results demonstrate that changes in relative decision power significantly affect household behavior. An increase in the wife's income by $1000 reduces the husband's decision power by 0.066 (13% at mean). Households with husband-dominated decision power save 10% less at the first wealth quartile (14,000 dollars). Private consumption increases by 24% when decision power shifts toward the husband, while public consumption shows smaller effects due to opposing influences of risk aversion and preference heterogeneity.",
    "category": "INCORRECT",
    "question_context_used": "The no-commitment model is simulated using a recursive formulation of household behavior, with utility functions given by:\n\n$$\nu^{i}(c^{i},Q)=\\frac{[(c^{i})^{\\sigma_{i}}(Q)^{1-\\sigma_{i}}]^{1-\\gamma_{i}}}{(1-\\gamma_{i})},\n$$\n\nwhere $\\gamma_{i}>0$ and $0<\\sigma_{i}<1$. The parameter $\\gamma_{i}$ captures intertemporal elasticity of substitution, while $\\sigma_{i}$ measures intraperiod allocation to private consumption. Empirical estimates show $\\gamma$ is 2.5 for men and 4.5 for women, with $\\sigma$ assumed to be 0.5 for women and 0.6 for men.\n\nThe simulation results demonstrate that changes in relative decision power significantly affect household behavior. An increase in the wife's income by $1000 reduces the husband's decision power by 0.066 (13% at mean). Households with husband-dominated decision power save 10% less at the first wealth quartile (14,000 dollars). Private consumption increases by 24% when decision power shifts toward the husband, while public consumption shows smaller effects due to opposing influences of risk aversion and preference heterogeneity.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2216",
    "question": "In the context of assessing bias from unobservables in evaluating Swan-Ganz catheterization (C), which of the following statements are supported by the findings of Altonji, Elder, and Taber (AET)?",
    "options": [
      "A) A correlation of 0.15 between unobserved determinants of C and mortality at 90 days could produce a harmful estimated effect when the true effect is zero.",
      "B) The true mortality effect of C is likely to be less negative than the lower bound estimates provided by AET.",
      "C) Selection on unobservables that is 0.8 times as strong as selection on observables could account for the positive probit estimate of the C effect if the true effect is zero.",
      "D) The raw difference in mortality rates between those with C=1 and C=0 decreases over time from 7 to 180 days."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Altonji-UsingSelectionObserved-2008.md"
    ],
    "question_context": "The study by Altonji, Elder, and Taber (AET) revisits the issue of Swan-Ganz catheterization (C) using methods to examine the sensitivity of estimates to assumptions about the amount of selection on unobservables. Key findings include: a correlation of roughly 0.15 between the unobserved determinants of C and mortality at 90 days would be enough to produce the harmful estimated effect when the true effect is zero. Lower bound estimates for the effect of C on mortality range from -0.042 in the short run to -0.005 in the long run, based on the assumption that the degree of selection on observed characteristics is the same as the degree of selection on unobserved characteristics. Given that mortality is caused by many factors that are more or less random at the time C is chosen, equality of selection is unlikely, and so the true mortality effect of C is likely to be larger (i.e., less negative) than the lower bound. Additionally, selection on unobservables that is 0.8 times as strong as selection on observables could account for the positive probit estimate of the C effect if the true effect is 0.",
    "category": "CORRECT",
    "question_context_used": "The study by Altonji, Elder, and Taber (AET) revisits the issue of Swan-Ganz catheterization (C) using methods to examine the sensitivity of estimates to assumptions about the amount of selection on unobservables. Key findings include: a correlation of roughly 0.15 between the unobserved determinants of C and mortality at 90 days would be enough to produce the harmful estimated effect when the true effect is zero. Lower bound estimates for the effect of C on mortality range from -0.042 in the short run to -0.005 in the long run, based on the assumption that the degree of selection on observed characteristics is the same as the degree of selection on unobserved characteristics. Given that mortality is caused by many factors that are more or less random at the time C is chosen, equality of selection is unlikely, and so the true mortality effect of C is likely to be larger (i.e., less negative) than the lower bound. Additionally, selection on unobservables that is 0.8 times as strong as selection on observables could account for the positive probit estimate of the C effect if the true effect is 0.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4481",
    "question": "In the context of instrumental variable (IV) regressions using historical instruments, which of the following conditions are necessary for the exclusion restriction to be satisfied?",
    "options": [
      "A) The historical instrument must have no direct effect on the contemporary outcome variable",
      "B) The contemporary endogenous variable must be perfectly measured",
      "C) The historical instrument must affect the contemporary outcome only through the contemporary endogenous variable",
      "D) The historical drivers of the outcome must differ from contemporary drivers"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Historical-instruments-and-contemporary-endoge_2021_Journal-of-Development-E.md"
    ],
    "question_context": "The exclusion restriction in IV regressions requires that the instrument affects the outcome only through the endogenous regressor. As noted in the text: 'Our framework demonstrates that this condition is necessary, but not sufficient, for the exclusion restriction to be satisfied. Even if the drivers of high productivity differ across time, past density may affect contemporary productivity through channels other than contemporary density.' Key formula: $Y_C = \beta X_C + \\gamma A_C + \\epsilon$, where $A_C$ represents unobserved contemporary factors. The IV estimate will be biased if $Z$ (historical instrument) affects $Y_C$ through any channel other than $X_C$.",
    "category": "CORRECT",
    "question_context_used": "The exclusion restriction in IV regressions requires that the instrument affects the outcome only through the endogenous regressor. As noted in the text: 'Our framework demonstrates that this condition is necessary, but not sufficient, for the exclusion restriction to be satisfied. Even if the drivers of high productivity differ across time, past density may affect contemporary productivity through channels other than contemporary density.' Key formula: $Y_C = \beta X_C + \\gamma A_C + \\epsilon$, where $A_C$ represents unobserved contemporary factors. The IV estimate will be biased if $Z$ (historical instrument) affects $Y_C$ through any channel other than $X_C$.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2272",
    "question": "Under the conditions of Proposition 8, which of the following statements are true regarding the kurtosis of price changes and the generalized hazard functions?",
    "options": [
      "A) A constant generalized hazard function maximizes kurtosis for a fixed adjustment frequency and finite boundary.",
      "B) A strictly increasing hazard function generates higher kurtosis than a constant hazard function when the adjustment frequency is fixed.",
      "C) The kurtosis of price changes is higher when the share of adjustments independent of the price gap (C) is larger, holding curvature and adjustment frequency constant.",
      "D) For power hazard functions, kurtosis decreases monotonically with the power parameter ν when the share of adjustments strictly between barriers (s) is fixed."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Macroeconomics of Sticky Prices with Generalized Hazard Functions.md"
    ],
    "question_context": "Proposition 8 states that for two hazard functions Λ₁(x) and Λ₂(x) with Λ₁(0) > Λ₂(0) and Λ₁(x) - Λ₂(x) changing sign at most once, Λ₁(x) generates higher kurtosis of price changes. Corollary 2 shows that for a fixed adjustment frequency Nₐ and finite boundary X, the constant generalized hazard function maximizes kurtosis. Corollary 4 establishes that for two hazard functions with the same curvature and adjustment frequency, the one with higher share of adjustments independent of the price gap (C) has higher kurtosis. For power hazard functions Λ(x) = κ|x/X|^ν, Proposition 9 demonstrates that kurtosis decreases with ν when s (share of adjustments strictly between barriers) is fixed. The key formula for curvature is given by: \n\n$$k(x) = \\frac{\\Lambda''(x)x}{\\Lambda'(x)}.$$",
    "category": "CORRECT",
    "question_context_used": "Proposition 8 states that for two hazard functions Λ₁(x) and Λ₂(x) with Λ₁(0) > Λ₂(0) and Λ₁(x) - Λ₂(x) changing sign at most once, Λ₁(x) generates higher kurtosis of price changes. Corollary 2 shows that for a fixed adjustment frequency Nₐ and finite boundary X, the constant generalized hazard function maximizes kurtosis. Corollary 4 establishes that for two hazard functions with the same curvature and adjustment frequency, the one with higher share of adjustments independent of the price gap (C) has higher kurtosis. For power hazard functions Λ(x) = κ|x/X|^ν, Proposition 9 demonstrates that kurtosis decreases with ν when s (share of adjustments strictly between barriers) is fixed. The key formula for curvature is given by: \n\n$$k(x) = \\frac{\\Lambda''(x)x}{\\Lambda'(x)}.$$",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2855",
    "question": "Based on the editorial process data from Econometrica (2014-2015), which of the following statements are empirically supported by the reported statistics?",
    "options": [
      "A) The journal's first-decision time for new submissions follows a Pareto distribution, with 63% of decisions made within three months.",
      "B) The acceptance rate for second revisions (76%) is significantly higher than the overall acceptance rate (7.06%).",
      "C) The transfer policy for rejected manuscripts to TE/QE creates a right-censored dataset in the editorial duration analysis.",
      "D) The improvement in decision times represents a first-order stochastic dominance shift compared to previous years."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Sobel-ECONOMETRICSOCIETYANNUAL-2016.md"
    ],
    "question_context": "Key statistics from the report: The journal received 949 new submissions (highest ever) with 67 acceptances (7.06% acceptance rate). Decision times showed $63\\%$ of new submissions decided within three months and $93\\%$ within six months. For second revisions, $76\\%$ were accepted, showing higher success rates in later stages. The transfer policy allows rejected Econometrica manuscripts to move to TE/QE with their review materials. The report emphasizes continued improvement in decision times, with current performance strengthening trends from recent years. Formula for acceptance rate: $\\text{Acceptance Rate} = \\frac{\\text{Papers Accepted}}{\\text{New Submissions}} = \\frac{67}{949} \\approx 7.06\\%$. The time distribution data shows a right-skewed pattern with decreasing probabilities for longer decision periods.",
    "category": "INCORRECT",
    "question_context_used": "Key statistics from the report: The journal received 949 new submissions (highest ever) with 67 acceptances (7.06% acceptance rate). Decision times showed $63\\%$ of new submissions decided within three months and $93\\%$ within six months. For second revisions, $76\\%$ were accepted, showing higher success rates in later stages. The transfer policy allows rejected Econometrica manuscripts to move to TE/QE with their review materials. The report emphasizes continued improvement in decision times, with current performance strengthening trends from recent years. Formula for acceptance rate: $\\text{Acceptance Rate} = \\frac{\\text{Papers Accepted}}{\\text{New Submissions}} = \\frac{67}{949} \\approx 7.06\\%$. The time distribution data shows a right-skewed pattern with decreasing probabilities for longer decision periods.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4283",
    "question": "In the context of pure exchange economies with information as a commodity, which of the following statements are true regarding the properties and challenges of information structures?",
    "options": [
      "A) Information structures are divisible and can be acquired in fractional amounts.",
      "B) The demand for information is a derived demand and is price dependent.",
      "C) The metric d(G,H) defines a topology for information that ensures continuous dependence of individual demand functions on consumers' information.",
      "D) Information structures introduce nonconvexities into the model due to their inherent indivisibility."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Allen-InformationEconomicCommodity-1990.md"
    ],
    "question_context": "The inclusion of information as an economic commodity leads to infinite-dimensional commodity spaces and mathematical challenges. Information structures are similar if they induce virtually identical expressions for conditional expected utility. The distance between information structures is formalized by the metric: \n\n$$ d(\\mathbf{G},\\mathbf{H})=\\operatorname*{sup}_{G\\in\\mathbf{G}}\\quad\\operatorname*{inf}_{H\\in\\mathbf{H}}\\mu\\big(G\\Delta H\\big) +\\operatorname*{sup}_{H\\in\\mathbf{H}}\\quad\\operatorname*{inf}_{G\\in\\mathbf{G}}\\mu\\big(G\\Delta H\\big), $$\n\nwhere $\\mathbf{G}$ and $\\mathbf{H}$ are complete sub-σ-fields of $\\mathbf{F}$. This metric defines a topology for information, ensuring that similar agents behave similarly. However, information structures are inherently indivisible, introducing fundamental nonconvexities into the model. The demand for information is derived and price dependent, as pure information is worthless without the ability to make conditional purchases.",
    "category": "CORRECT",
    "question_context_used": "The inclusion of information as an economic commodity leads to infinite-dimensional commodity spaces and mathematical challenges. Information structures are similar if they induce virtually identical expressions for conditional expected utility. The distance between information structures is formalized by the metric: \n\n$$ d(\\mathbf{G},\\mathbf{H})=\\operatorname*{sup}_{G\\in\\mathbf{G}}\\quad\\operatorname*{inf}_{H\\in\\mathbf{H}}\\mu\\big(G\\Delta H\\big) +\\operatorname*{sup}_{H\\in\\mathbf{H}}\\quad\\operatorname*{inf}_{G\\in\\mathbf{G}}\\mu\\big(G\\Delta H\\big), $$\n\nwhere $\\mathbf{G}$ and $\\mathbf{H}$ are complete sub-σ-fields of $\\mathbf{F}$. This metric defines a topology for information, ensuring that similar agents behave similarly. However, information structures are inherently indivisible, introducing fundamental nonconvexities into the model. The demand for information is derived and price dependent, as pure information is worthless without the ability to make conditional purchases.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1194",
    "question": "Based on Fujita and Ramey's analysis of CPS gross flow data, which of the following statements accurately describe the cyclical relationships between separation rates, job finding rates, and productivity?",
    "options": [
      "A) Separation rates are negatively correlated with productivity changes and move contemporaneously with them",
      "B) Job finding rates are positively correlated with productivity changes but lag behind them",
      "C) Separation rates explain less than 30% of unemployment fluctuations when dynamic interactions are considered",
      "D) Job finding rates are more volatile than separation rates during business cycles"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "The Cyclicality of Separation and Job Finding Rates.md"
    ],
    "question_context": "This article uses CPS gross flow data to analyze the business cycle dynamics of separation and job finding rates and quantify their contributions to overall unemployment variability. Cyclical changes in the separation rate are negatively correlated with changes in productivity and move contemporaneously with them, whereas the job finding rate is positively correlated with and tends to lag productivity. Contemporaneous fluctuations in the separation rate explain between 40and $50\\%$ of fluctuations in unemployment, depending on how the data are detrended. This figure becomes larger when dynamic interactions between the separationandjobfindingrates areconsidered.",
    "category": "INCORRECT",
    "question_context_used": "This article uses CPS gross flow data to analyze the business cycle dynamics of separation and job finding rates and quantify their contributions to overall unemployment variability. Cyclical changes in the separation rate are negatively correlated with changes in productivity and move contemporaneously with them, whereas the job finding rate is positively correlated with and tends to lag productivity. Contemporaneous fluctuations in the separation rate explain between 40and $50\\%$ of fluctuations in unemployment, depending on how the data are detrended. This figure becomes larger when dynamic interactions between the separationandjobfindingrates areconsidered.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3006",
    "question": "In the context of preference aggregation over a finite set of alternatives $A$ with $|A|\\geqslant3$, which of the following statements are true regarding the properties of aggregators?",
    "options": [
      "A) Every IIA and Paretian aggregator $f:(\\mathcal{P}a r t(A))^{n}\\rightarrow\\mathcal{P}a r t(A)$ is oligarchic.",
      "B) Every IIA and Paretian aggregator $f:(\\mathcal{W}e a k(A))^{n}\\to\\mathcal{W}e a k(A)$ is weakly dictatorial.",
      "C) The class $\\mathcal{D}i m_{\\leqslant2}(A)$ is the smallest possibility range in preference aggregation.",
      "D) For $\\mathcal{W}e a k(A)\\subseteq\\mathcal{R}\\subseteq\\mathcal{P}a r t(A)$, $\\mathcal{R}$ is an impossibility range if it excludes at least one partial order of dimension 2."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "Aggregation-of-binary-evaluations-with-abstenti_2010_Journal-of-Economic-The.md"
    ],
    "question_context": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism...\n\nIn the context of preference aggregation, we consider a finite set of alternatives $A=\\{a_{1},\\ldots,a_{k}\\}$ with $k\\geqslant3$. Various classes of preference relations over $A$ are subclasses of ${\\mathcal{A}}s y m(A)$, the class of all asymmetric binary relations over $A$. The relation $a\\succ b$ is interpreted as '$a$ is strictly preferred to $b$.' We define $a\\succcurlyeq b$ for $a,b\\in A$ to hold if and only if $b\\succ a$ does not hold. The classes of preference relations include $\\mathscr{L}i n(A)$ (linear orders), $\\mathcal{P}a r t(A)$ (partial orders), and $\\mathcal{W}e a k(A)$ (weak orders). The set of vectors corresponding to $\\mathscr{L}i n(A)$ is given by:\n\n$$\nX=\\big\\{x\\in\\{0,1\\}^{m}\\big|\\forall1\\leqslant r<s<t\\leqslant k,(x_{r s},x_{s t},x_{r t})\\neq(0,0,1),(1,1,0)\\big\\}.\n$$\n\nCorollary 4.1 states that for every $n$, every IIA and Paretian aggregator $f:(\\mathcal{P}a r t(A))^{n}\\rightarrow\\mathcal{P}a r t(A)$ is oligarchic. Corollary 4.4 (Arrow's theorem) states that for every $n$, every IIA and Paretian aggregator $f:(\\mathcal{W}e a k(A))^{n}\\to\\mathcal{W}e a k(A)$ is weakly dictatorial. Corollary 4.6 characterizes impossibility ranges as those excluding at least one partial order of dimension 2, with $\\mathcal{D}i m_{\\leqslant2}(A)$ being the smallest possibility range.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism...\n\nIn the context of preference aggregation, we consider a finite set of alternatives $A=\\{a_{1},\\ldots,a_{k}\\}$ with $k\\geqslant3$. Various classes of preference relations over $A$ are subclasses of ${\\mathcal{A}}s y m(A)$, the class of all asymmetric binary relations over $A$. The relation $a\\succ b$ is interpreted as '$a$ is strictly preferred to $b$.' We define $a\\succcurlyeq b$ for $a,b\\in A$ to hold if and only if $b\\succ a$ does not hold. The classes of preference relations include $\\mathscr{L}i n(A)$ (linear orders), $\\mathcal{P}a r t(A)$ (partial orders), and $\\mathcal{W}e a k(A)$ (weak orders). The set of vectors corresponding to $\\mathscr{L}i n(A)$ is given by:\n\n$$\nX=\\big\\{x\\in\\{0,1\\}^{m}\\big|\\forall1\\leqslant r<s<t\\leqslant k,(x_{r s},x_{s t},x_{r t})\\neq(0,0,1),(1,1,0)\\big\\}.\n$$\n\nCorollary 4.1 states that for every $n$, every IIA and Paretian aggregator $f:(\\mathcal{P}a r t(A))^{n}\\rightarrow\\mathcal{P}a r t(A)$ is oligarchic. Corollary 4.4 (Arrow's theorem) states that for every $n$, every IIA and Paretian aggregator $f:(\\mathcal{W}e a k(A))^{n}\\to\\mathcal{W}e a k(A)$ is weakly dictatorial. Corollary 4.6 characterizes impossibility ranges as those excluding at least one partial order of dimension 2, with $\\mathcal{D}i m_{\\leqslant2}(A)$ being the smallest possibility range.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1802",
    "question": "In the context of the PRESCIENCE procedure for binary response models, which of the following statements are supported by the empirical results?",
    "options": [
      "A) The variable CARS was consistently selected across all cases of q and CV folds due to its large parameter estimate magnitude.",
      "B) The MIO formulation (4.5) outperformed (4.14) in computational efficiency for q=1 and q=5 but not for q=3.",
      "C) The refined parameter space Θ̂(τ) reduced the original parameter space Θ to approximately 99% of its original size.",
      "D) The out-of-sample predictive performance was best for the most complex model (q=5) across all CV folds."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Best-subset-binary-prediction_2018_Journal-of-Econometrics.md"
    ],
    "question_context": "The empirical application of PRESCIENCE in work-trip mode choice analysis utilized a dataset with 842 observations, focusing on covariate selection via MIO algorithms. Key findings include the consistent selection of the variable CARS across all cases of q (1, 3, 5) and CV folds, attributed to its large parameter estimate magnitude. The MIO formulations (4.5) and (4.14) showed varying computational efficiencies: (4.5) was more efficient for q=1 and q=5, while both formulations incurred higher costs for q=3. The refined parameter space Θ̂(τ) significantly reduced the search space to about 0.99% of Θ's original size. However, contrary to expectation, the parsimonious model (q=1) demonstrated the best out-of-sample predictive performance, not the more complex models.",
    "category": "INCORRECT",
    "question_context_used": "The empirical application of PRESCIENCE in work-trip mode choice analysis utilized a dataset with 842 observations, focusing on covariate selection via MIO algorithms. Key findings include the consistent selection of the variable CARS across all cases of q (1, 3, 5) and CV folds, attributed to its large parameter estimate magnitude. The MIO formulations (4.5) and (4.14) showed varying computational efficiencies: (4.5) was more efficient for q=1 and q=5, while both formulations incurred higher costs for q=3. The refined parameter space Θ̂(τ) significantly reduced the search space to about 0.99% of Θ's original size. However, contrary to expectation, the parsimonious model (q=1) demonstrated the best out-of-sample predictive performance, not the more complex models.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1976",
    "question": "Which of the following statements about affine subspaces in a finite-dimensional Euclidean space are correct?",
    "options": [
      "A) The affine hull of any subset X is the smallest affine subspace containing X.",
      "B) The dimension of the affine hull of X is equal to the maximal number of affinely independent elements in X.",
      "C) For two affine subspaces X and Y with nonempty intersection, the dimension of their intersection is given by dim(X) + dim(Y) - dim(aff(X ∪ Y)).",
      "D) A set Z is an affine subspace if and only if it is closed under affine combinations."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Generic-Finiteness-of-Equilibrium-Outcome-Distributions_1997_Journal-of-Econ.md"
    ],
    "question_context": "An affine combination of elements $x^{1},...,x^{r}\\in E$ is a sum $\\alpha_{1}x^{1}+\\dots+\\alpha_{r}x^{r}$ in which $\\mathfrak{x}_{1},...,\\mathfrak{x}_{r}$ are scalars with ${\\alpha}_{1}+\\dots+{\\alpha}_{r}=1$. The affine hull of a subset $X\\subset E$ is the set of affine combinations of elements of $X$. An affine subspace is a set $Z\\subset E$ that is closed under affine combination. Key formula: For two affine subspaces $X$ and $Y$ with nonempty intersection, $\\dim(X\\cap Y)=\\dim(X)+\\dim(Y)-\\dim(\\operatorname{aff}(X\\cup Y))$.",
    "category": "INCORRECT",
    "question_context_used": "An affine combination of elements $x^{1},...,x^{r}\\in E$ is a sum $\\alpha_{1}x^{1}+\\dots+\\alpha_{r}x^{r}$ in which $\\mathfrak{x}_{1},...,\\mathfrak{x}_{r}$ are scalars with ${\\alpha}_{1}+\\dots+{\\alpha}_{r}=1$. The affine hull of a subset $X\\subset E$ is the set of affine combinations of elements of $X$. An affine subspace is a set $Z\\subset E$ that is closed under affine combination. Key formula: For two affine subspaces $X$ and $Y$ with nonempty intersection, $\\dim(X\\cap Y)=\\dim(X)+\\dim(Y)-\\dim(\\operatorname{aff}(X\\cup Y))$.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4167",
    "question": "In the context of security demand functions and agent behavior under uncertainty, which of the following conditions are necessary for the truncated budget set $\\widetilde{T}^{i}(s,p)$ to be nonempty, convex, and compact?",
    "options": [
      "A) The dividend matrix $\\pmb{D}$ must have full column rank under all possible private information scenarios",
      "B) The utility function $u^{i}(\\cdot,s)$ must be strictly differentiably concave and strictly monotone on the interior of the consumption set",
      "C) The no-redundant-securities condition must hold for every common knowledge event $W\\in\\mathcal{C}$",
      "D) The endowment must satisfy $(e_{1}^{i},e_{2}^{i}(s))\\in\\mathrm{int}X^{i}$ for all $s\\in S$"
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Rational-behavior-and-rational-expectations_1990_Journal-of-Economic-Theory.md"
    ],
    "question_context": "Key formula for the truncated budget set: \n$$\\widetilde{T}^{i}(s,p)=\\big\\{y^{i}\\in T^{i}(s,p)\\colon\\forall\\widetilde{s}\\in\\mathcal{C}[s],e_{2}^{i}(\\widetilde{s})+y^{i}\\delta(\\widetilde{s})\\leqslant m\\big\\}.$$\n\nUnder Assumption 2 (no redundant securities condition), for each $W\\in\\mathcal{C}$, the only vector $y$ such that $\\delta(s)~y=0$ for every $s\\in W$ is $y=0$. This property implies that it is never common knowledge among the agents that there are redundant securities. \n\nAssumption 4 states that for each $i\\in N$ and $s\\in S$, $u^{i}(\\cdot,s)$ must be continuous on $X^{i}$, of class $C^{\\infty}$ on the interior, concave and strictly differentiably concave on the interior, and strictly monotone. The endowment condition $(e_{1}^{i},e_{2}^{i}(s))\\in\\mathrm{int}X^{i}$ is specified in Assumption 1.",
    "category": "INCORRECT",
    "question_context_used": "Key formula for the truncated budget set: \n$$\\widetilde{T}^{i}(s,p)=\\big\\{y^{i}\\in T^{i}(s,p)\\colon\\forall\\widetilde{s}\\in\\mathcal{C}[s],e_{2}^{i}(\\widetilde{s})+y^{i}\\delta(\\widetilde{s})\\leqslant m\\big\\}.$$\n\nUnder Assumption 2 (no redundant securities condition), for each $W\\in\\mathcal{C}$, the only vector $y$ such that $\\delta(s)~y=0$ for every $s\\in W$ is $y=0$. This property implies that it is never common knowledge among the agents that there are redundant securities. \n\nAssumption 4 states that for each $i\\in N$ and $s\\in S$, $u^{i}(\\cdot,s)$ must be continuous on $X^{i}$, of class $C^{\\infty}$ on the interior, concave and strictly differentiably concave on the interior, and strictly monotone. The endowment condition $(e_{1}^{i},e_{2}^{i}(s))\\in\\mathrm{int}X^{i}$ is specified in Assumption 1.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3828",
    "question": "In the context of the dynamic general equilibrium model with heterogeneous consumers, which of the following statements are true regarding the effects of a mean-preserving dispersion in time preference?",
    "options": [
      "A) If the marginal tax function is concave, a more dispersed distribution of time preference leads to a lower average marginal tax rate and higher capital accumulation.",
      "B) If the marginal tax function is convex, a more dispersed distribution of time preference leads to a higher average marginal tax rate and lower capital accumulation.",
      "C) The effect on aggregate labour supply is solely determined by the curvature of the marginal tax function.",
      "D) The income effect on individual labour supply complicates the analysis of time preference heterogeneity."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Diversity_and_Economic_Performance_in_a_Model_with_Progressive_Taxation.md"
    ],
    "question_context": "The article examines the effects of diversity in a dynamic general equilibrium model with heterogeneous consumers differing in labour productivity and time preference. Key formula: The marginal tax function's curvature determines the effect of time preference dispersion on capital accumulation. Specifically, if the marginal tax function is concave, a mean-preserving spread in time preference leads to a lower average marginal tax rate and higher capital accumulation. Conversely, if the marginal tax function is convex, the opposite occurs. The analysis is complicated by the income effect on individual labour supply, which affects aggregate labour supply differently depending on the utility function's properties. The model uses the following utility function specifications: \n\n1. Additively separable: $U(c,n) = \\frac{c^{1-\\sigma}-1}{1-\\sigma} - A\\frac{n^{1+\\theta}}{1+\\theta}$\n2. GHH preferences: $U(c,n) = \\frac{(c - A n^{1+\\theta})^{1-\\sigma} - 1}{1-\\sigma}$\n3. Homothetic: $U(c,n) = \\frac{[c^\\lambda (1-n)^{1-\\lambda}]^{1-\\sigma} - 1}{1-\\sigma}$",
    "category": "CORRECT",
    "question_context_used": "The article examines the effects of diversity in a dynamic general equilibrium model with heterogeneous consumers differing in labour productivity and time preference. Key formula: The marginal tax function's curvature determines the effect of time preference dispersion on capital accumulation. Specifically, if the marginal tax function is concave, a mean-preserving spread in time preference leads to a lower average marginal tax rate and higher capital accumulation. Conversely, if the marginal tax function is convex, the opposite occurs. The analysis is complicated by the income effect on individual labour supply, which affects aggregate labour supply differently depending on the utility function's properties. The model uses the following utility function specifications: \n\n1. Additively separable: $U(c,n) = \\frac{c^{1-\\sigma}-1}{1-\\sigma} - A\\frac{n^{1+\\theta}}{1+\\theta}$\n2. GHH preferences: $U(c,n) = \\frac{(c - A n^{1+\\theta})^{1-\\sigma} - 1}{1-\\sigma}$\n3. Homothetic: $U(c,n) = \\frac{[c^\\lambda (1-n)^{1-\\lambda}]^{1-\\sigma} - 1}{1-\\sigma}$",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1486",
    "question": "In the context of the proof of Theorem 2.1, which of the following statements are correct regarding the terms $I_1(n)$ and $I_2(n)$?",
    "options": [
      "A) $I_1(n)$ involves a term of order $h^4$ and depends on the second derivative of the regression function $m(x)$.",
      "B) $I_2(n)$ is dominated by a term of order $(n^{1-d}h)^{-1}$ and involves the variance function $\\sigma^2(x)$.",
      "C) The term $I_1(n)$ can be expressed purely in terms of the kernel function $K(t)$ without any dependence on $m(x)$.",
      "D) The term $I_2(n)$ includes an integral of the squared kernel function $\\int_{-\\infty}^{\\infty} K^2(t) dt$."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "OPTIMAL_BANDWIDTH_SELECTION_IN_NONLINEAR_COINTEGRATING_REGRESSION.md"
    ],
    "question_context": "The proof involves decomposing the mean squared error into two terms, $I_1(n)$ and $I_2(n)$. For $I_1(n)$, we have:\n\n$$\nI_{1}(n)=\\frac{1}{4}h^{4}\\left\\{[m^{\\prime\\prime}(x)]^{2}+o_{P}(1)\\right\\}l_{n}^{2},\n$$\n\nuniformly in $x\\in\\Omega$, where $l_n$ is defined in terms of kernel functions. For $I_2(n)$, the analysis yields:\n\n$$\nI_{2}(n)=\\sigma^{2}(x)\\displaystyle\\int_{-\\infty}^{\\infty}K^{2}(t)d t(h A_{n})^{-1}+o_{P}\\big[(n^{1-d}h)^{-1}\\big].\n$$\n\nThese results are derived under conditions involving the kernel $K(x)$, which has finite support, and the regression function $m(x)$, which is twice differentiable.",
    "category": "CORRECT",
    "question_context_used": "The proof involves decomposing the mean squared error into two terms, $I_1(n)$ and $I_2(n)$. For $I_1(n)$, we have:\n\n$$\nI_{1}(n)=\\frac{1}{4}h^{4}\\left\\{[m^{\\prime\\prime}(x)]^{2}+o_{P}(1)\\right\\}l_{n}^{2},\n$$\n\nuniformly in $x\\in\\Omega$, where $l_n$ is defined in terms of kernel functions. For $I_2(n)$, the analysis yields:\n\n$$\nI_{2}(n)=\\sigma^{2}(x)\\displaystyle\\int_{-\\infty}^{\\infty}K^{2}(t)d t(h A_{n})^{-1}+o_{P}\\big[(n^{1-d}h)^{-1}\\big].\n$$\n\nThese results are derived under conditions involving the kernel $K(x)$, which has finite support, and the regression function $m(x)$, which is twice differentiable.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-945",
    "question": "In the context of repeated coordination games with pure Nash equilibria, which of the following statements are true regarding stable outcome paths as per Proposition 1?",
    "options": [
      "A) Any pure outcome path containing more than one occurrence of (a,a) is unstable if it ends with a sequence of (b,b).",
      "B) A deviation in the first period to an action yielding payoff 0 can satisfy the conditions of Proposition 1 for instability.",
      "C) Stability among pure outcome paths predicts that the average payoff converges to 3 as T approaches infinity.",
      "D) Mixed outcome paths are always stable if they contain at least one occurrence of (a,a) and (b,b)."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Signaling--forward-induction--and-stability-in-finit_1990_Journal-of-Economi.md"
    ],
    "question_context": "The game $G_{2}$ satisfies the conditions of the subgame perfect folk theorem, but Proposition 1 shows that among pure outcome paths, only those with average payoffs close to 3 can be stable. Key conditions include: (a) paths with alternating (a,a) and (b,b) are unstable due to deviations satisfying Proposition 1's conditions; (b) paths starting with $(-\\varepsilon,0)$ or $(0,-\\varepsilon)$ followed by (b,b) are unstable; (c) paths with one (a,a) amidst (b,b) are unstable if starting with $(0,-\\varepsilon)$ or $(-\\varepsilon,0)$. The only stable pure outcome paths are those with at most one (a,a), the rest being (b,b), leading to average payoffs converging to 3 as $T\\to\\infty$. Proposition 1's application requires $\\varepsilon>0$ to enable signaling through payoff adjustments.",
    "category": "CORRECT",
    "question_context_used": "The game $G_{2}$ satisfies the conditions of the subgame perfect folk theorem, but Proposition 1 shows that among pure outcome paths, only those with average payoffs close to 3 can be stable. Key conditions include: (a) paths with alternating (a,a) and (b,b) are unstable due to deviations satisfying Proposition 1's conditions; (b) paths starting with $(-\\varepsilon,0)$ or $(0,-\\varepsilon)$ followed by (b,b) are unstable; (c) paths with one (a,a) amidst (b,b) are unstable if starting with $(0,-\\varepsilon)$ or $(-\\varepsilon,0)$. The only stable pure outcome paths are those with at most one (a,a), the rest being (b,b), leading to average payoffs converging to 3 as $T\\to\\infty$. Proposition 1's application requires $\\varepsilon>0$ to enable signaling through payoff adjustments.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-791",
    "question": "In the context of structural VAR models with external instruments, which of the following statements are true regarding the identification of impulse responses?",
    "options": [
      "A) The exogenous variable approach requires the instrument $z_t$ to be uncorrelated with all other regressors for consistent estimation of subsequent relative impulse responses.",
      "B) The external instrument approach is robust to measurement errors in $z_t$ as long as the errors are orthogonal to other variables.",
      "C) The contemporaneous relative impulse responses from the exogenous variable approach and the external instrument approach are always identical in small samples.",
      "D) The local projection instrumental variable approach (LP-IV) requires stronger conditions on the instrument $z_t$ than the VARX approach for consistent estimation of relative impulse responses."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "rest_a_00840.md"
    ],
    "question_context": "The econometric problem in identifying absolute and relative impulse responses is that the structural shocks $\\epsilon_{t}$ are not observed. In addition, the covariance matrix of the reduced-form innovations, $\\vec{\\bot}\\left[u_{t}u_{t}^{\\prime}\\right]=S S^{\\prime}$, does not provide enough identifying restrictions to obtain at least one of the columns in $S$ or ratios of elements within such a column. Until recently, the structural VAR literature has achieved identification from restrictions that are directly imposed on the system of simultaneous equations (2) (Christiano et al., 1999; Uhlig, 2005).\n\nHere, I follow the approach of the external instrument literature. The idea of this identification approach is to bring in information from external sources to identify the effects of structural shocks. In particular, assume that a proxy $z_{t}$ for the latent shock of interest $\\epsilon_{1,t}$ exists and that $z_{t}$ satisfies the following conditions,\n\n$$\n\\begin{array}{r}{\\mathbb{E}\\left[z_{t}\\epsilon_{1,t}\\right]=\\Phi,}\\ {\\mathbb{E}\\left[z_{t}\\epsilon_{2,t}\\right]=0,}\\end{array}\n$$\n\nwith $\\boldsymbol{\\Phi}$ unknown but different from 0 and $z_{t}$ assumed to have a 0 mean for simplicity. The key difference between the external instrument approach and the one that I propose in this paper lies in how the proxy $z_{t}$ is used. The external instrument approach proceeds in multiple steps. First, system (2) is estimated for a sample of observables. In a second step, the estimated reduced-form innovations $\\hat{u}_{t}$ are regressed on $z_{t}$. These steps give consistent estimates of the true relative impulse responses (see appendix A.1).",
    "category": "INCORRECT",
    "question_context_used": "The econometric problem in identifying absolute and relative impulse responses is that the structural shocks $\\epsilon_{t}$ are not observed. In addition, the covariance matrix of the reduced-form innovations, $\\vec{\\bot}\\left[u_{t}u_{t}^{\\prime}\\right]=S S^{\\prime}$, does not provide enough identifying restrictions to obtain at least one of the columns in $S$ or ratios of elements within such a column. Until recently, the structural VAR literature has achieved identification from restrictions that are directly imposed on the system of simultaneous equations (2) (Christiano et al., 1999; Uhlig, 2005).\n\nHere, I follow the approach of the external instrument literature. The idea of this identification approach is to bring in information from external sources to identify the effects of structural shocks. In particular, assume that a proxy $z_{t}$ for the latent shock of interest $\\epsilon_{1,t}$ exists and that $z_{t}$ satisfies the following conditions,\n\n$$\n\\begin{array}{r}{\\mathbb{E}\\left[z_{t}\\epsilon_{1,t}\\right]=\\Phi,}\\ {\\mathbb{E}\\left[z_{t}\\epsilon_{2,t}\\right]=0,}\\end{array}\n$$\n\nwith $\\boldsymbol{\\Phi}$ unknown but different from 0 and $z_{t}$ assumed to have a 0 mean for simplicity. The key difference between the external instrument approach and the one that I propose in this paper lies in how the proxy $z_{t}$ is used. The external instrument approach proceeds in multiple steps. First, system (2) is estimated for a sample of observables. In a second step, the estimated reduced-form innovations $\\hat{u}_{t}$ are regressed on $z_{t}$. These steps give consistent estimates of the true relative impulse responses (see appendix A.1).",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2139",
    "question": "Under what conditions does the asymmetry in liability rules between producers and consumers become significant in the short-run?",
    "options": [
      "A) When the costs of information are negligible for both parties",
      "B) When the costs of information are unequal between seller and buyer",
      "C) When both parties are risk-neutral and transaction costs are zero",
      "D) When the courts are omniscient and impartial, ensuring no fraudulent claims"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Divergent_Liability_Rules_and_Economic_Integration.md"
    ],
    "question_context": "The premises for a taxonomic summary of contractual situations are thus provided by the following assumptions : for a given situation, producers (sellers) behave identically as a class and, as a class, incur the same transaction-related costs. The same holds true of consumers (buyers). In addition, both buyers and sellers are presumed risk neutral, that is, the utility of a dollar gained equals the disutility of a dollar lost. We shall ignore the possibility of third party damage claims, or assume costless subrogation where they exist. The scope of possible damages is thus limited to the buyer-seller relationship. In addition, buyers and sellers refrain from making fraudulent claims, their integrity being assured by the omniscience and impartiality of the courts.\n\nIt is only where the costs of information are unequal between seller and buyer that the impact of liability rules will be asymmetrical, at least in the short-run. Suppose that consumers incur lower costs in identifying changes in production technologies than do producers in identifying changes in consumption technology. Note that this does not imply that the party experiencing the higher costs will remain forever ignorant of his counterpart's technology. What it does mean is that in a non-co-operative environment - which will obtain if the cost of co-operation exceeds its benefits -- higher costs will cause the adjustment lag to be longer for those incurring them than for the lower cost party. In this event, consumer's liability will be the more efficient rule, since it discourages consumers from taking advantage of the adjustment speed differential in order to behave carelessly at the producer's (and society's) expense.",
    "category": "INCORRECT",
    "question_context_used": "The premises for a taxonomic summary of contractual situations are thus provided by the following assumptions : for a given situation, producers (sellers) behave identically as a class and, as a class, incur the same transaction-related costs. The same holds true of consumers (buyers). In addition, both buyers and sellers are presumed risk neutral, that is, the utility of a dollar gained equals the disutility of a dollar lost. We shall ignore the possibility of third party damage claims, or assume costless subrogation where they exist. The scope of possible damages is thus limited to the buyer-seller relationship. In addition, buyers and sellers refrain from making fraudulent claims, their integrity being assured by the omniscience and impartiality of the courts.\n\nIt is only where the costs of information are unequal between seller and buyer that the impact of liability rules will be asymmetrical, at least in the short-run. Suppose that consumers incur lower costs in identifying changes in production technologies than do producers in identifying changes in consumption technology. Note that this does not imply that the party experiencing the higher costs will remain forever ignorant of his counterpart's technology. What it does mean is that in a non-co-operative environment - which will obtain if the cost of co-operation exceeds its benefits -- higher costs will cause the adjustment lag to be longer for those incurring them than for the lower cost party. In this event, consumer's liability will be the more efficient rule, since it discourages consumers from taking advantage of the adjustment speed differential in order to behave carelessly at the producer's (and society's) expense.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-131",
    "question": "In the context of procurement contracts with limited commitment and renegotiation, which of the following statements are supported by the analysis in the text?",
    "options": [
      "A) The limited-commitment contract always yields higher firm profits than the full-commitment contract.",
      "B) The single-incentive (SI) contract performs nearly as well as the menu of contracts in the uniform-quadratic case.",
      "C) The value of a menu of contracts is maximized when the cost support range is intermediate, not when it is very large or very small.",
      "D) A fixed-price contract is optimal when the cost distribution has zero support."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Procurement Policy and Contracting Efficiency.md"
    ],
    "question_context": "The text analyzes procurement contracts under limited commitment, where renegotiation is possible. Key findings include: (i) The limited-commitment solution yields higher firm profits than the full-commitment solution (Proposition 6). (ii) The single-incentive (SI) contract performs 98-99.8% as well as a menu of contracts in the uniform-quadratic case, making it nearly equivalent. (iii) The value of a menu has an interior maximum with respect to the cost support range, being zero for zero or very large support. The formula for the government's problem under limited commitment is:\n\n$$\n\\begin{array}{c}{{\\displaystyle{\\operatorname*{min}\\int_{C_{\\mathrm{min}}}^{C_{\\mathrm{max}}}\\left\\{(1+\\alpha)\\Biggl[\\lambda\\psi(\\eta_{1}(C))-\\frac{\\lambda^{2}}2\\eta_{1}(C)-\\lambda(1-\\lambda)\\eta_{1}(C)}}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle{+(1-\\lambda)\\psi(\\eta_{2}^{f})-\\frac{(1-\\lambda)^{2}}2\\eta_{2}^{f}+C\\Biggr]+\\alpha U(C)\\Biggr\\}f(C)d C}}}\\end{array}\n$$\n\nConstraints include the incentive compatibility condition $U^{\\prime}(C)=-2\\psi^{\\prime}(\\eta_{1}(C))+1-\\lambda$ and participation constraints. Numerical examples show that the SI contract captures 61% of potential savings compared to fixed-price contracts, while the limited-commitment menu captures slightly more.",
    "category": "CORRECT",
    "question_context_used": "The text analyzes procurement contracts under limited commitment, where renegotiation is possible. Key findings include: (i) The limited-commitment solution yields higher firm profits than the full-commitment solution (Proposition 6). (ii) The single-incentive (SI) contract performs 98-99.8% as well as a menu of contracts in the uniform-quadratic case, making it nearly equivalent. (iii) The value of a menu has an interior maximum with respect to the cost support range, being zero for zero or very large support. The formula for the government's problem under limited commitment is:\n\n$$\n\\begin{array}{c}{{\\displaystyle{\\operatorname*{min}\\int_{C_{\\mathrm{min}}}^{C_{\\mathrm{max}}}\\left\\{(1+\\alpha)\\Biggl[\\lambda\\psi(\\eta_{1}(C))-\\frac{\\lambda^{2}}2\\eta_{1}(C)-\\lambda(1-\\lambda)\\eta_{1}(C)}}}\\\\ {{\\mathrm{}}}\\\\ {{\\displaystyle{+(1-\\lambda)\\psi(\\eta_{2}^{f})-\\frac{(1-\\lambda)^{2}}2\\eta_{2}^{f}+C\\Biggr]+\\alpha U(C)\\Biggr\\}f(C)d C}}}\\end{array}\n$$\n\nConstraints include the incentive compatibility condition $U^{\\prime}(C)=-2\\psi^{\\prime}(\\eta_{1}(C))+1-\\lambda$ and participation constraints. Numerical examples show that the SI contract captures 61% of potential savings compared to fixed-price contracts, while the limited-commitment menu captures slightly more.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4828",
    "question": "In the context of the financial accelerator model with co-ordination failure, which of the following mechanisms amplify the macroeconomic effects of illiquidity shocks?",
    "options": [
      "A) Leverage-induced magnification of co-ordination inefficiency in debt rollover",
      "B) Perfectly symmetric information sets among creditors leading to multiple equilibria",
      "C) Endogenous countercyclical proportion of borrowers experiencing runs",
      "D) Direct lending policies that disincentivize balance sheet adjustment"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Financial_Accelerator_through_Coordination_Failure.md"
    ],
    "question_context": "The model highlights the role that maturity- and liquidity-mismatch play in the propagation of shocks through the economy. Creditors financing a project face a co-ordination problem where fear of other creditors not rolling over funding may lead to pre-emptive action, undermining the project. Leverage magnifies this co-ordination inefficiency, and illiquidity shocks in credit markets result in sharp contractions in output. The proportion of borrowers experiencing a run is endogenously countercyclical, determined by the aggregate state. Key formula: $R_{K,t}\\equiv(M P K_{t}+(1-\\delta)Q_{t})/Q_{t-1}$ shows the risk-adjusted return on capital.",
    "category": "CORRECT",
    "question_context_used": "The model highlights the role that maturity- and liquidity-mismatch play in the propagation of shocks through the economy. Creditors financing a project face a co-ordination problem where fear of other creditors not rolling over funding may lead to pre-emptive action, undermining the project. Leverage magnifies this co-ordination inefficiency, and illiquidity shocks in credit markets result in sharp contractions in output. The proportion of borrowers experiencing a run is endogenously countercyclical, determined by the aggregate state. Key formula: $R_{K,t}\\equiv(M P K_{t}+(1-\\delta)Q_{t})/Q_{t-1}$ shows the risk-adjusted return on capital.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1436",
    "question": "In the context of large dynamic panel data models with strong cross-sectional dependence, which of the following statements are correct regarding the proposed tests for breaks/homogeneity?",
    "options": [
      "A) The tests are based on the CUSUM principle and a Hausman–Durbin–Wu approach.",
      "B) The tests have trivial power when the break happens towards the end of the sample.",
      "C) The bootstrap algorithms do not require knowledge of the underlying model of cross-sectional dependence.",
      "D) The tests are designed to handle only weak cross-sectional dependence."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Inference-and-testing-breaks-in-large-dynamic-panels-wit_2017_Journal-of-Eco.md"
    ],
    "question_context": "In this paper we provide a new Central Limit Theorem for estimators of the slope papers in large dynamic panel data models (where both n and T increase without bound) in the presence of, possibly, strong cross-sectional dependence. We proceed by providing two related tests for breaks/homogeneity in the time dimension. The first test is based on the CUSUM principle; the second test is based on a Hausman–Durbin–Wu approach. Some of the key features of the tests are that they have nontrivial power when the number of individuals, for which the slope parameters may differ, is a ‘‘negligible’’ fraction or when the break happens to be towards the end of the sample, and do not suffer from the incidental parameter problem. We provide a simple bootstrap algorithm to obtain (asymptotic) valid critical values for our statistics. An important feature of the bootstrap is that there is no need to know the underlying model of the cross-sectional dependence.\n\nNowadays it is widely recognized that economic agents are interrelated due to common factors, contagion, spillovers and so on. This dependence has been systematically neglected until quite recently in econometrics, possibly, due to a lack of a clear framework to characterize such a dependence which is exacerbated by the fact that, contrary to time series data, there is an absence of a clear or natural ordering of the data. In response to this, in the last decade or so, a huge amount of work has been directed to the study of cross-sectional dependence and several approaches or models have been put forward.",
    "category": "CORRECT",
    "question_context_used": "In this paper we provide a new Central Limit Theorem for estimators of the slope papers in large dynamic panel data models (where both n and T increase without bound) in the presence of, possibly, strong cross-sectional dependence. We proceed by providing two related tests for breaks/homogeneity in the time dimension. The first test is based on the CUSUM principle; the second test is based on a Hausman–Durbin–Wu approach. Some of the key features of the tests are that they have nontrivial power when the number of individuals, for which the slope parameters may differ, is a ‘‘negligible’’ fraction or when the break happens to be towards the end of the sample, and do not suffer from the incidental parameter problem. We provide a simple bootstrap algorithm to obtain (asymptotic) valid critical values for our statistics. An important feature of the bootstrap is that there is no need to know the underlying model of the cross-sectional dependence.\n\nNowadays it is widely recognized that economic agents are interrelated due to common factors, contagion, spillovers and so on. This dependence has been systematically neglected until quite recently in econometrics, possibly, due to a lack of a clear framework to characterize such a dependence which is exacerbated by the fact that, contrary to time series data, there is an absence of a clear or natural ordering of the data. In response to this, in the last decade or so, a huge amount of work has been directed to the study of cross-sectional dependence and several approaches or models have been put forward.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3456",
    "question": "Based on the empirical analysis of pre-colonial institutions and socioeconomic development, which of the following statements are supported by the robustness checks and extensions?",
    "options": [
      "A) The coefficient on pre-colonial institutions remains statistically significant when controlling for pre-colonial population density.",
      "B) The 'reversal of fortune' hypothesis is fully supported across all measures of socioeconomic development.",
      "C) The effect of pre-colonial institutions is stronger in rural areas compared to urban areas.",
      "D) Proto-democracy and proto-property rights are highly correlated with pre-colonial institutional complexity."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Pre-colonial-institutions-and-socioeconomic-develop_2017_Journal-of-Developm.md"
    ],
    "question_context": "The study examines the persistent role of pre-colonial institutions on current socioeconomic development, controlling for various pre-colonial characteristics such as population density, economic activities, and social structures. Key findings include the robustness of pre-colonial institutions' positive effect, with coefficients remaining statistically significant across multiple specifications. For instance, the coefficient on pre-colonial institutions for secondary education is 0.0673*** [0.0177] in the baseline regression. The 'reversal of fortune' hypothesis is only partially supported, with pre-colonial population density showing a negative effect on GDP per capita but not uniformly across all measures. Notably, the effect of pre-colonial institutions is significantly stronger in rural areas, supporting the thesis of institutional survival in these regions. For example, a one-unit increase in pre-colonial institutions is associated with an 8.6% increase in secondary education in rural areas versus 3.2% in urban areas.",
    "category": "INCORRECT",
    "question_context_used": "The study examines the persistent role of pre-colonial institutions on current socioeconomic development, controlling for various pre-colonial characteristics such as population density, economic activities, and social structures. Key findings include the robustness of pre-colonial institutions' positive effect, with coefficients remaining statistically significant across multiple specifications. For instance, the coefficient on pre-colonial institutions for secondary education is 0.0673*** [0.0177] in the baseline regression. The 'reversal of fortune' hypothesis is only partially supported, with pre-colonial population density showing a negative effect on GDP per capita but not uniformly across all measures. Notably, the effect of pre-colonial institutions is significantly stronger in rural areas, supporting the thesis of institutional survival in these regions. For example, a one-unit increase in pre-colonial institutions is associated with an 8.6% increase in secondary education in rural areas versus 3.2% in urban areas.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-392",
    "question": "In the study of credit programs in rural Bangladesh, which of the following methodological approaches were employed to address endogeneity and selection bias?",
    "options": [
      "A) Instrumental variable approach based on exclusion restrictions",
      "B) Quasi-experimental survey design with village-level fixed effects",
      "C) Factor-analytic structure on residuals with covariance restrictions",
      "D) Random placement of credit programs across villages"
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Credit Programs for the Poor and the Health Status of Children in Rural Bangladesh.md"
    ],
    "question_context": "The research results presented below are based upon a 1991/92 survey of 1,798 households in 87 villages in rural Bangladesh. The earlier studies, cited above, which used these survey data confirm that program participation by households is self-selective. The estimation method used in these earlier studies corrects for the potential bias arising from unobserved individual- and village-level heterogeneity by taking advantage of a quasi-experimental survey design to provide statistical identification of the program effects. The survey design covers one group of households which has the choice to enter a credit program and which may alter their behavior in response to the program, and a 'control' group that is not given the choice of entering the program but whose behavior is still measured. Similarly, the identification of these programs' impact by the gender of the participant is accomplished based on the comparison between groups of each gender with and without the choice to participate. These programs, whose professed goal is to better the lives of the poor, may have chosen villages in a conscious manner based on their wealth, attitudes or other attributes. To deal with the possibility of endogenous program placement, these studies couple the quasi-experimental design with village-level fixed effects to sweep out village unobservables that might otherwise bias estimates of the impacts of these credit programs. To estimate the level effects of credit programs by gender of the credit program participant on individual-specific indicators of well-being, we propose a set of restrictions on the error covariances of a set of gender-specific health behaviors and the decision to participate in a credit program. Chamberlain (1976, 1977a, 1977b) and Chamberlain and Griliches (1975) first demonstrated how to identify the parameters of a structural equation model from restrictions on error covariances. Extending their approach to our estimation problem permits identification of the level effects of credit program participation by gender of participant without requiring the imposition of difficult to justify zero restrictions that are inconsistent with a general household model, or relying exclusively on arbitrary assumptions about the distribution of errors.",
    "category": "INCORRECT",
    "question_context_used": "The research results presented below are based upon a 1991/92 survey of 1,798 households in 87 villages in rural Bangladesh. The earlier studies, cited above, which used these survey data confirm that program participation by households is self-selective. The estimation method used in these earlier studies corrects for the potential bias arising from unobserved individual- and village-level heterogeneity by taking advantage of a quasi-experimental survey design to provide statistical identification of the program effects. The survey design covers one group of households which has the choice to enter a credit program and which may alter their behavior in response to the program, and a 'control' group that is not given the choice of entering the program but whose behavior is still measured. Similarly, the identification of these programs' impact by the gender of the participant is accomplished based on the comparison between groups of each gender with and without the choice to participate. These programs, whose professed goal is to better the lives of the poor, may have chosen villages in a conscious manner based on their wealth, attitudes or other attributes. To deal with the possibility of endogenous program placement, these studies couple the quasi-experimental design with village-level fixed effects to sweep out village unobservables that might otherwise bias estimates of the impacts of these credit programs. To estimate the level effects of credit programs by gender of the credit program participant on individual-specific indicators of well-being, we propose a set of restrictions on the error covariances of a set of gender-specific health behaviors and the decision to participate in a credit program. Chamberlain (1976, 1977a, 1977b) and Chamberlain and Griliches (1975) first demonstrated how to identify the parameters of a structural equation model from restrictions on error covariances. Extending their approach to our estimation problem permits identification of the level effects of credit program participation by gender of participant without requiring the imposition of difficult to justify zero restrictions that are inconsistent with a general household model, or relying exclusively on arbitrary assumptions about the distribution of errors.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\"]\n}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4334",
    "question": "According to Gennaioli, Shleifer, and Vishny's analysis of neglected risks in financial crises, which psychological mechanisms are most likely to amplify systemic risk?",
    "options": [
      "A) Overreliance on recent historical data leading to underestimation of tail risks",
      "B) Excessive diversification across uncorrelated asset classes",
      "C) Selective attention to salient events at the expense of less visible risks",
      "D) Hyperbolic discounting of future crisis probabilities"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Gennaioli-NeglectedRisksPsychology-2015.md"
    ],
    "question_context": "The paper examines how psychological biases in risk perception contribute to financial crises. Key mechanisms include the neglect of low-probability risks due to their invisibility in normal times (\\\"what you don\\'t see is what you don\\'t think about\\\") and the overweighting of recent experience. Formula: The authors model risk perception as Pr(risk) = f(salience, recency) where f is increasing in both arguments. This leads to systematic underestimation of tail risks during periods of stability, creating conditions for severe crises when neglected risks materialize.",
    "category": "CORRECT",
    "question_context_used": "The paper examines how psychological biases in risk perception contribute to financial crises. Key mechanisms include the neglect of low-probability risks due to their invisibility in normal times (\\\"what you don\\'t see is what you don\\'t think about\\\") and the overweighting of recent experience. Formula: The authors model risk perception as Pr(risk) = f(salience, recency) where f is increasing in both arguments. This leads to systematic underestimation of tail risks during periods of stability, creating conditions for severe crises when neglected risks materialize.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3359",
    "question": "In the context of modeling leasehold property prices, which of the following statements are correct regarding the function $f(t)$ and the estimation methods used?",
    "options": [
      "A) $f(t)$ must satisfy $f(0)=0$, $f^{\\prime}(t)>0$, and $\\lim_{t\\to\\infty}f(t)=1$ to align with economic intuition.",
      "B) The bucket method estimates price effects by assigning each integer lease length up to 999 years its own categorical variable.",
      "C) The semiparametric approach involves differencing observations sorted by $t$ and using OLS to estimate a version of the model without $f(t)$.",
      "D) Gordon's constant discount rate model implies $\\ln f(t) = \\ln(1 - e^{-R_{t}T})$, where $R_{t}$ must vary over the time horizon."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Time_Value_of_Housing_Historical_Evidence_on_Discount_Rates.md"
    ],
    "question_context": "The function $f(t)$ represents the discount associated with a given lease length as opposed to a property held forever. Prior expectations are that $f(t)$ should satisfy $f(0)=0$, $f^{\\prime}(t)>0$ and $\\textstyle\\operatorname*{lim}_{x\\to\\infty}f(t)=1$, indicating that a zero year lease should have no market value, that all else equal more years on a lease should make the property more valuable, and that at some point very long but finite leases should be equivalent to infinite leases. Three methods are employed to estimate $\\ln f(t)$: (i) leasehold buckets, (ii) leasehold dummies, and (iii) a semiparametric approach based on Yatchew (1997). The semiparametric estimation involves sorting observations in ascending order with respect to $t$ and differencing them, then using OLS to estimate a version of the model that does not contain $f(t)$.",
    "category": "INCORRECT",
    "question_context_used": "The function $f(t)$ represents the discount associated with a given lease length as opposed to a property held forever. Prior expectations are that $f(t)$ should satisfy $f(0)=0$, $f^{\\prime}(t)>0$ and $\\textstyle\\operatorname*{lim}_{x\\to\\infty}f(t)=1$, indicating that a zero year lease should have no market value, that all else equal more years on a lease should make the property more valuable, and that at some point very long but finite leases should be equivalent to infinite leases. Three methods are employed to estimate $\\ln f(t)$: (i) leasehold buckets, (ii) leasehold dummies, and (iii) a semiparametric approach based on Yatchew (1997). The semiparametric estimation involves sorting observations in ascending order with respect to $t$ and differencing them, then using OLS to estimate a version of the model that does not contain $f(t)$.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1458",
    "question": "In the context of recursive formulations for repeated agency with history dependence, which of the following conditions are essential for ensuring dynamic consistency and incentive compatibility?",
    "options": [
      "A) Complete state-contingent contracts with full commitment",
      "B) Markovian strategies that depend only on payoff-relevant states",
      "C) History-dependent contracts with bounded memory constraints",
      "D) Recursive enforcement mechanisms with time-consistent penalties"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Cumulative-Author-Index_2001_Journal-of-Economic-Theory.md"
    ],
    "question_context": "Key formula: The recursive formulation for repeated agency with history dependence is given by the Bellman equation: V(s) = max_{a} {u(a,s) + βE[V(s')|s,a]}, where s is the state, a is the action, and β is the discount factor. This framework ensures dynamic consistency by embedding future incentives into current decisions. The work of Phelan and Fernandes (1991) demonstrates that recursive enforcement mechanisms are crucial for maintaining incentive compatibility over time, particularly when agents have private information or limited commitment. Their model highlights the role of Markovian strategies in simplifying the equilibrium characterization while preserving the essential dynamics of the repeated interaction.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The recursive formulation for repeated agency with history dependence is given by the Bellman equation: V(s) = max_{a} {u(a,s) + βE[V(s')|s,a]}, where s is the state, a is the action, and β is the discount factor. This framework ensures dynamic consistency by embedding future incentives into current decisions. The work of Phelan and Fernandes (1991) demonstrates that recursive enforcement mechanisms are crucial for maintaining incentive compatibility over time, particularly when agents have private information or limited commitment. Their model highlights the role of Markovian strategies in simplifying the equilibrium characterization while preserving the essential dynamics of the repeated interaction.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1597",
    "question": "According to the 1991 membership and subscription details of the Econometric Society, which of the following statements are correct regarding payment structures?",
    "options": [
      "A) Membership dues for individuals in high-income countries are \\$58.00 and include a subscription to Econometrica.",
      "B) Libraries in countries outside the U.S., Canada, Europe, Japan, Australia, and New Zealand pay a higher subscription rate than those within these regions.",
      "C) Student subscriptions require proof of student status and are available at a fixed rate of \\$24.00 for up to 3 years.",
      "D) Membership dues for countries with per capita incomes below \\$500 must be paid directly to the Society's administrative office to qualify for the reduced rate."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "BackMatter-1991.md"
    ],
    "question_context": "The Econometric Society is an international society for the advancement of economic theory in its relation to statistics and mathematics. It operates as a disinterested scientific organization without political or financial bias. Membership dues and subscriptions are payable in United States dollars, with specific rates for different regions and income levels. For example, membership dues for 1991 for individuals in high-income countries (U.S.A., Canada, Europe, Japan, Australia, and New Zealand) are \\$58.00 and include a subscription to Econometrica. Countries with per capita incomes below \\$500 qualify for a reduced rate of \\$24.00, provided payment is remitted directly to the Society's office. Student subscriptions are available at \\$24.00 for a maximum of 3 years, with proof of student status required.",
    "category": "INCORRECT",
    "question_context_used": "The Econometric Society is an international society for the advancement of economic theory in its relation to statistics and mathematics. It operates as a disinterested scientific organization without political or financial bias. Membership dues and subscriptions are payable in United States dollars, with specific rates for different regions and income levels. For example, membership dues for 1991 for individuals in high-income countries (U.S.A., Canada, Europe, Japan, Australia, and New Zealand) are \\$58.00 and include a subscription to Econometrica. Countries with per capita incomes below \\$500 qualify for a reduced rate of \\$24.00, provided payment is remitted directly to the Society's office. Student subscriptions are available at \\$24.00 for a maximum of 3 years, with proof of student status required.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3907",
    "question": "In the context of the firm's investment technology model, which of the following conditions are necessary to ensure that the first best level of β (β*) is a global interior maximizer of expected returns?",
    "options": [
      "A) ϕ₁′(β)(H₁ - L₁) + ϕ₂′(β)(H₂ - L₂) = 0 at β*",
      "B) P(β) is strictly concave in β",
      "C) ϕ₁′(βᵥ)(H₁ - L₁) + ϕ₂′(βᵥ)(H₂ - L₂) > 0 and ϕ₁′(βᵤ)(H₁ - L₁) + ϕ₂′(βᵤ)(H₂ - L₂) < 0",
      "D) The manager's ability is known with certainty"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Myopic Investment Decisions and Competitive Labor Markets.md"
    ],
    "question_context": "The first best level of β is determined by maximizing the expected present value of the firm's investment returns, given the prior ability a. The expected present value is defined as:\n\nP(β) ≡ (a + ϕ₁)H₁ + (1 - a - ϕ₁)L₁ + (a + ϕ₂)H₂ + (1 - a - ϕ₂)L₂.\n\nThe first order necessary condition is:\n\nϕ₁′(β*)(H₁ - L₁) + ϕ₂′(β*)(H₂ - L₂) = 0,\n\nwhere β* is the first best level of β. Under (A.1) and (A.2), P(β) is a strictly concave function, so that if β* exists, it is a global interior maximizer of expected returns. To guarantee the existence of such an interior optimum, we assume:\n\nϕ₁′(βᵥ)(H₁ - L₁) + ϕ₂′(βᵥ)(H₂ - L₂) > 0 and ϕ₁′(βᵤ)(H₁ - L₁) + ϕ₂′(βᵤ)(H₂ - L₂) < 0.",
    "category": "CORRECT",
    "question_context_used": "The first best level of β is determined by maximizing the expected present value of the firm's investment returns, given the prior ability a. The expected present value is defined as:\n\nP(β) ≡ (a + ϕ₁)H₁ + (1 - a - ϕ₁)L₁ + (a + ϕ₂)H₂ + (1 - a - ϕ₂)L₂.\n\nThe first order necessary condition is:\n\nϕ₁′(β*)(H₁ - L₁) + ϕ₂′(β*)(H₂ - L₂) = 0,\n\nwhere β* is the first best level of β. Under (A.1) and (A.2), P(β) is a strictly concave function, so that if β* exists, it is a global interior maximizer of expected returns. To guarantee the existence of such an interior optimum, we assume:\n\nϕ₁′(βᵥ)(H₁ - L₁) + ϕ₂′(βᵥ)(H₂ - L₂) > 0 and ϕ₁′(βᵤ)(H₁ - L₁) + ϕ₂′(βᵤ)(H₂ - L₂) < 0.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1875",
    "question": "Which of the following statements are supported by the cash-in-advance models and empirical evidence presented in the text?",
    "options": [
      "A) Higher inflation rates systematically decrease capital stocks and output levels in all scenarios.",
      "B) The Tobin effect is reversed when investment is subject to inflation tax or changes in labor supply affect the marginal product of capital.",
      "C) The welfare cost of inflation is significantly underestimated if the endogenous response of the private financial sector is ignored.",
      "D) In the long run, the growth rates of capital and output converge regardless of the inflation rate."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Ireland-MoneyGrowthAlternative-1994.md"
    ],
    "question_context": "The cash-in-advance models presented by Cooley and Hansen (1989, 1991) and Stockman (1981) associate higher rates of inflation with smaller stocks of capital and lower levels of output. The Tobin effect is reversed since an increase in the rate of inflation lowers the return on investment, either because changes in labor supply affect the marginal product of capital (as in Cooley and Hansen's models) or because investment is subject to the inflation tax (as in Stockman's model). Here, changes in labor supply are not considered, and investment is a credit good, so neither of these two effects is present. Instead, higher rates of inflation are associated with larger capital stocks and higher levels of output, just as in Tobin (1965).\n\nTable 2 reveals that the welfare cost of a 10-percent inflation is 0.624 percent of output, over 50 percent larger than the estimates of approximately 0.4 percent of output obtained by Cooley and Hansen (1989, 1991). For a 20-percent inflation, the cost of 1.268 percent of output is nearly twice as large as the estimate reported by Cooley and Hansen (1991). These estimates suggest that accounting for the endogenous response of the private financial sector may be important in measuring the true welfare costs of sustained inflation. Figure 6 shows that, regardless of the rate of inflation, the growth rate of capital and output approaches 1.97 percent asymptotically.",
    "category": "CORRECT",
    "question_context_used": "The cash-in-advance models presented by Cooley and Hansen (1989, 1991) and Stockman (1981) associate higher rates of inflation with smaller stocks of capital and lower levels of output. The Tobin effect is reversed since an increase in the rate of inflation lowers the return on investment, either because changes in labor supply affect the marginal product of capital (as in Cooley and Hansen's models) or because investment is subject to the inflation tax (as in Stockman's model). Here, changes in labor supply are not considered, and investment is a credit good, so neither of these two effects is present. Instead, higher rates of inflation are associated with larger capital stocks and higher levels of output, just as in Tobin (1965).\n\nTable 2 reveals that the welfare cost of a 10-percent inflation is 0.624 percent of output, over 50 percent larger than the estimates of approximately 0.4 percent of output obtained by Cooley and Hansen (1989, 1991). For a 20-percent inflation, the cost of 1.268 percent of output is nearly twice as large as the estimate reported by Cooley and Hansen (1991). These estimates suggest that accounting for the endogenous response of the private financial sector may be important in measuring the true welfare costs of sustained inflation. Figure 6 shows that, regardless of the rate of inflation, the growth rate of capital and output approaches 1.97 percent asymptotically.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2419",
    "question": "In the context of the economic model of high school completion and adolescent fertility, which of the following statements are correct regarding the identification of the model's parameters?",
    "options": [
      "A) The coefficients and error variances in the equations for high school completion and adolescent fertility are only identified up to their proportions.",
      "B) The effect of teenage fertility on high school completion is identified without any exclusion or covariance restrictions.",
      "C) The paper applies the normalization σ_ε = σ_η = 1 to identify the coefficients and error variances.",
      "D) Exclusion restrictions are imposed on the vector X_i to identify the effect of teenage fertility on high school completion."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Ribar-TeenageFertilityHigh-1994.md"
    ],
    "question_context": "The paper develops a simple empirical model to analyze the determinants of high school completion and adolescent childbearing. The model specifies high school completion as a probit with teen fertility as an endogenous dummy determinant. Key formulas include:\n\n$$E_{i}^{*}=\\beta_{f}F_{i}+\\beta_{x}^{\\prime}X_{i}+\\epsilon_{i}$$\n\n$$F_{i}^{*}=\\delta_{z}^{\\prime}Z_{i}+\\eta_{i}$$\n\nThe error terms are assumed to be distributed as:\n\n$$\\left[\\begin{array}{c}{{\\epsilon_{i}}}\\\\ {{\\eta_{i}}}\\end{array}\\right]\\sim N\\left(\\left[\\begin{array}{c}{{0}}\\\\ {{0}}\\end{array}\\right],\\left[\\begin{array}{c c}{{\\sigma_{\\epsilon}^{2}}}&{{\\rho\\sigma_{\\epsilon}\\sigma_{\\eta}}}\\\\ {{}}&{{\\sigma_{\\eta}^{2}}}\\end{array}\\right]\\right).$$\n\nMaximum likelihood estimation of this specification is straightforward, although there are some identification issues. The coefficients and error variances in the equations are only identified up to their proportions, and the paper applies the standard normalization σ_ε = σ_η = 1. The effect of teenage fertility on high school completion is only identified subject to exclusion or covariance restrictions, and the paper imposes exclusion restrictions on the vector X_i.",
    "category": "CORRECT",
    "question_context_used": "The paper develops a simple empirical model to analyze the determinants of high school completion and adolescent childbearing. The model specifies high school completion as a probit with teen fertility as an endogenous dummy determinant. Key formulas include:\n\n$$E_{i}^{*}=\\beta_{f}F_{i}+\\beta_{x}^{\\prime}X_{i}+\\epsilon_{i}$$\n\n$$F_{i}^{*}=\\delta_{z}^{\\prime}Z_{i}+\\eta_{i}$$\n\nThe error terms are assumed to be distributed as:\n\n$$\\left[\\begin{array}{c}{{\\epsilon_{i}}}\\\\ {{\\eta_{i}}}\\end{array}\\right]\\sim N\\left(\\left[\\begin{array}{c}{{0}}\\\\ {{0}}\\end{array}\\right],\\left[\\begin{array}{c c}{{\\sigma_{\\epsilon}^{2}}}&{{\\rho\\sigma_{\\epsilon}\\sigma_{\\eta}}}\\\\ {{}}&{{\\sigma_{\\eta}^{2}}}\\end{array}\\right]\\right).$$\n\nMaximum likelihood estimation of this specification is straightforward, although there are some identification issues. The coefficients and error variances in the equations are only identified up to their proportions, and the paper applies the standard normalization σ_ε = σ_η = 1. The effect of teenage fertility on high school completion is only identified subject to exclusion or covariance restrictions, and the paper imposes exclusion restrictions on the vector X_i.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1934",
    "question": "In the context of the structural macro model of Smets and Wouters (2007), which of the following statements are true regarding the equivalence of LP and VAR impulse response estimands?",
    "options": [
      "A) The LP(∞) and VAR(∞) estimands agree at all horizons when the government spending innovation is directly observed.",
      "B) The LP(p) and VAR(p) estimands differ from each other when the impulse variable depends on the lag length p.",
      "C) The LP and VAR impulse response estimators are equally asymptotically efficient under weak regularity conditions.",
      "D) The difference between LP and VAR impulse response estimators converges to zero asymptotically in sample when large lag lengths are used."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "ecta17813.md"
    ],
    "question_context": "The left panel of Figure 1 shows LP and VAR impulse response estimands of the response of output to an innovation in government spending. We assume the model’s government spending innovation is directly observed by the econometrician, who additionally controls for lags of output and the fiscal spending innovation itself. As ensured by Proposition 1, the $\\mathrm{LP}(\\infty)$ and $\\operatorname{VAR}(\\infty)$ estimands—that is, with infinitely many lags as controls—agree at all horizons. Since by assumption the 'impulse' variable $x_{t}$ is a direct measure of the government spending innovation, we have $\\tilde{x}_{t}(p)=x_{t}$ . Thus, any $\\operatorname{LP}(p)$ estimand for finite $p$ also agrees with the $\\mathrm{LP}(\\infty)$ limit at all horizons. The right panel of Figure 1 shows LP and VAR impulse response estimands for the response of output to an innovation in the nominal interest rate. Here, the model’s innovation is not directly observed by the econometrician, only the interest rate. The LP specifications control for the contemporaneous value of output and inflation as well as lags of output, inflation, and the nominal interest rate; as discussed, this set of control variables is equivalent to ordering the interest rate last in the VAR. Again, the $\\mathrm{LP}(\\infty)$ and $\\operatorname{VAR}(\\infty)$ estimands agree at all horizons. Now, however, the 'impulse' $\\tilde{x}_{t}(p)$ upon which the $\\operatorname{LP}(p)$ and $\\operatorname{VAR}(p)$ methods project depends on the lag length $p$ , so these estimands differ from the $\\mathrm{LP(\\infty)}/\\mathrm{VAR(\\infty)}$ estimands. Furthermore, the $\\operatorname{LP}(p)$ and $\\operatorname{VAR}(p)$ estimands also differ from each other, as the remainder term $\\phi_{h}(p)$ in Proposition 2 is not exactly zero.",
    "category": "INCORRECT",
    "question_context_used": "The left panel of Figure 1 shows LP and VAR impulse response estimands of the response of output to an innovation in government spending. We assume the model’s government spending innovation is directly observed by the econometrician, who additionally controls for lags of output and the fiscal spending innovation itself. As ensured by Proposition 1, the $\\mathrm{LP}(\\infty)$ and $\\operatorname{VAR}(\\infty)$ estimands—that is, with infinitely many lags as controls—agree at all horizons. Since by assumption the 'impulse' variable $x_{t}$ is a direct measure of the government spending innovation, we have $\\tilde{x}_{t}(p)=x_{t}$ . Thus, any $\\operatorname{LP}(p)$ estimand for finite $p$ also agrees with the $\\mathrm{LP}(\\infty)$ limit at all horizons. The right panel of Figure 1 shows LP and VAR impulse response estimands for the response of output to an innovation in the nominal interest rate. Here, the model’s innovation is not directly observed by the econometrician, only the interest rate. The LP specifications control for the contemporaneous value of output and inflation as well as lags of output, inflation, and the nominal interest rate; as discussed, this set of control variables is equivalent to ordering the interest rate last in the VAR. Again, the $\\mathrm{LP}(\\infty)$ and $\\operatorname{VAR}(\\infty)$ estimands agree at all horizons. Now, however, the 'impulse' $\\tilde{x}_{t}(p)$ upon which the $\\operatorname{LP}(p)$ and $\\operatorname{VAR}(p)$ methods project depends on the lag length $p$ , so these estimands differ from the $\\mathrm{LP(\\infty)}/\\mathrm{VAR(\\infty)}$ estimands. Furthermore, the $\\operatorname{LP}(p)$ and $\\operatorname{VAR}(p)$ estimands also differ from each other, as the remainder term $\\phi_{h}(p)$ in Proposition 2 is not exactly zero.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1360",
    "question": "According to the data appendix and the methodologies described, which of the following strategies are employed to mitigate the impact of segment reorganization on the analysis?",
    "options": [
      "A) Excluding segments with incomplete variable information",
      "B) Constructing a narrow sample with continuity in segment activities and accounting practices",
      "C) Using only firms with sales exceeding $1 billion in 1977 dollars",
      "D) Eliminating segments where accounting ratios exceed one"
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Are Internal capital Markets Efficient.md"
    ],
    "question_context": "An important difficulty in using these data is that firms reorganize their segments over time. To prevent such reorganizations from affecting our conclusions, we pursue three different strategies. First, for our base sample, we eliminate segment years in which any of the following five ratios exceeds one.12 The ratios are the current value of net capital expenditure (gross capital expenditure minus depreciation) over the previous year’s segment assets, sales change over the previous year’s sales, cash flow to segment assets, and, other segments’ cash flow divided by total assets of these segments. Cash flow is defined as operating profit (loss) plus depreciation. The ratios we use are determined by data availability. After excluding outliers, we end up with 49,851 segment-year records corresponding to 3265 different firms. We construct a second sample, which we call the narrow sample. This narrow sample is constructed in such a way that we use only segments that have continuity in their activities and accounting practices. With this sample we eliminate all segments from our base sample when firms change their number of segments or their SIC code for a period of three years centered on the year the segment is in the sample. Finally, we construct a sample of large firms to isolate firms that are subjected to more scrutiny from analysts and regulators. Firms are included in the large firm sample if they have sales of more than $1 billion in 1977 dollars. A firm that meets this condition one year is kept in the sample in subsequent years, thus insuring that losers do not drop out of that sample.",
    "category": "INCORRECT",
    "question_context_used": "An important difficulty in using these data is that firms reorganize their segments over time. To prevent such reorganizations from affecting our conclusions, we pursue three different strategies. First, for our base sample, we eliminate segment years in which any of the following five ratios exceeds one.12 The ratios are the current value of net capital expenditure (gross capital expenditure minus depreciation) over the previous year’s segment assets, sales change over the previous year’s sales, cash flow to segment assets, and, other segments’ cash flow divided by total assets of these segments. Cash flow is defined as operating profit (loss) plus depreciation. The ratios we use are determined by data availability. After excluding outliers, we end up with 49,851 segment-year records corresponding to 3265 different firms. We construct a second sample, which we call the narrow sample. This narrow sample is constructed in such a way that we use only segments that have continuity in their activities and accounting practices. With this sample we eliminate all segments from our base sample when firms change their number of segments or their SIC code for a period of three years centered on the year the segment is in the sample. Finally, we construct a sample of large firms to isolate firms that are subjected to more scrutiny from analysts and regulators. Firms are included in the large firm sample if they have sales of more than $1 billion in 1977 dollars. A firm that meets this condition one year is kept in the sample in subsequent years, thus insuring that losers do not drop out of that sample.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1007",
    "question": "Under the assumptions of the grouped fixed-effects (GFE) estimator, which of the following conditions are necessary for the asymptotic equivalence between the GFE estimator and the infeasible least squares estimator with known population groups?",
    "options": [
      "A) The number of groups G is known and fixed, and group-specific effects are well-separated.",
      "B) The covariates x_{it} are strictly exogenous and independent across units.",
      "C) The errors v_{it} are strongly mixing with a faster-than-polynomial decay rate and have tails decaying at a faster-than-polynomial rate.",
      "D) The number of groups G is estimated consistently, and the group-specific effects are time-invariant."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Bonhomme-GROUPEDPATTERNSHETEROGENEITY-2015.md"
    ],
    "question_context": "The GFE estimator minimizes a piecewise-quadratic function where the partition of the parameter space is defined by the different values of group membership. The statistical properties of GFE in nonlinear models are characterized under specific assumptions. Key formula for the GFE estimator: \n\n$$ \n(\\widehat{\\theta},\\widehat{\\alpha},\\widehat{\\gamma})=\\underset{(\\theta,\\alpha,\\gamma)\\in\\Theta^{G}\\times A^{G T}\\times T_{G}}{\\arg\\operatorname*{min}}\\sum_{i=1}^{N}\\sum_{t=1}^{T}m_{i t}(\\theta,\\alpha_{t},g_{i}). \n$$ \n\nAssumption 2(c) requires that the errors v_{it} are strongly mixing with a faster-than-polynomial decay rate, and Assumption 2(d) imposes that the tails of v_{it} decay at a faster-than-polynomial rate. These conditions are crucial for bounding misclassification probabilities and ensuring the asymptotic equivalence between the GFE estimator and the infeasible least squares estimator.",
    "category": "CORRECT",
    "question_context_used": "The GFE estimator minimizes a piecewise-quadratic function where the partition of the parameter space is defined by the different values of group membership. The statistical properties of GFE in nonlinear models are characterized under specific assumptions. Key formula for the GFE estimator: \n\n$$ \n(\\widehat{\\theta},\\widehat{\\alpha},\\widehat{\\gamma})=\\underset{(\\theta,\\alpha,\\gamma)\\in\\Theta^{G}\\times A^{G T}\\times T_{G}}{\\arg\\operatorname*{min}}\\sum_{i=1}^{N}\\sum_{t=1}^{T}m_{i t}(\\theta,\\alpha_{t},g_{i}). \n$$ \n\nAssumption 2(c) requires that the errors v_{it} are strongly mixing with a faster-than-polynomial decay rate, and Assumption 2(d) imposes that the tails of v_{it} decay at a faster-than-polynomial rate. These conditions are crucial for bounding misclassification probabilities and ensuring the asymptotic equivalence between the GFE estimator and the infeasible least squares estimator.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1842",
    "question": "Under which conditions do the noncooperative equilibrium strategies in Friedman's time-dependent supergames converge to the same path as time approaches infinity?",
    "options": [
      "A) The initial conditions of the strategies are identical ($s_0 = s_0'$)",
      "B) The Lipschitz conditions on $\\phi_t$ and $g_t$ are satisfied with $\\lambda_{t-1} + \\mu_{t-1} \\leq k$",
      "C) The function $\\phi_t$ maps a compact set into itself and is a contraction",
      "D) The strategies are derived from stationary games"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Friedman-NoteTurnpikeProperties-1981.md"
    ],
    "question_context": "The turnpike result, which is stated formally below, is that if $\\pmb{\\sigma}$ and $\\bullet^{\\prime}$ are two noncooperative equilibria, then as $t$ goes to infinity, they become close to one another. The theorem states: Let Assumptions A1-A4 and $A5^{\\prime}$ hold, and let $\\sigma$ and $\\sigma^{\\prime}$ be strategies satisfying $s_{t}=\\phi_{t}(s_{t+1},s_{t-1})$ and $\\hat{s_{t}^{\\prime}}=\\phi_{t}(s_{t+1}^{\\prime},s_{t-1}^{\\prime})$ $t=1,2$ ..., respectively. (a) If $\\bar{s}_{0}=s_{0}^{\\prime}$ then $s_{t}=s_{t}^{\\prime}$ , $t=1,2,\\ldots$ ,and (b) if $s_{0}\\neq s_{0}^{\\prime}$ ,then $d(s_{t},s_{t}^{\\prime})$ goes to zero as t goes to $\\infty$. The proof relies on the Lipschitz conditions on the $g_t$, with $\\lambda_{t-1} + \\mu_{t-1} \\leq k$, and the fact that $g_t(s_{t+1}, s_0)$ is the fixed point of a continuous function mapping a compact set into itself, ensuring uniqueness as it is a contraction.",
    "category": "INCORRECT",
    "question_context_used": "The turnpike result, which is stated formally below, is that if $\\pmb{\\sigma}$ and $\\bullet^{\\prime}$ are two noncooperative equilibria, then as $t$ goes to infinity, they become close to one another. The theorem states: Let Assumptions A1-A4 and $A5^{\\prime}$ hold, and let $\\sigma$ and $\\sigma^{\\prime}$ be strategies satisfying $s_{t}=\\phi_{t}(s_{t+1},s_{t-1})$ and $\\hat{s_{t}^{\\prime}}=\\phi_{t}(s_{t+1}^{\\prime},s_{t-1}^{\\prime})$ $t=1,2$ ..., respectively. (a) If $\\bar{s}_{0}=s_{0}^{\\prime}$ then $s_{t}=s_{t}^{\\prime}$ , $t=1,2,\\ldots$ ,and (b) if $s_{0}\\neq s_{0}^{\\prime}$ ,then $d(s_{t},s_{t}^{\\prime})$ goes to zero as t goes to $\\infty$. The proof relies on the Lipschitz conditions on the $g_t$, with $\\lambda_{t-1} + \\mu_{t-1} \\leq k$, and the fact that $g_t(s_{t+1}, s_0)$ is the fixed point of a continuous function mapping a compact set into itself, ensuring uniqueness as it is a contraction.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4602",
    "question": "In the context of testing business cycle duration dependence using the probit specification for transition probabilities, which of the following conditions are necessary to establish positive duration dependence?",
    "options": [
      "A) γ₂,d₂ > 0 and γ₃,d₃ < 0",
      "B) γ₂,d₂ = 0 and γ₃,d₃ = 0",
      "C) γ₂,d₂ < 0 and γ₃,d₃ > 0",
      "D) The prior probabilities p₂ and p₃ are set to 0.5"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Kim-BusinessCycleTurning-1998.md"
    ],
    "question_context": "Duration dependence in the business cycle and time-varying transition probabilities are related but not identical concepts. Duration dependence asks whether recessions or booms 'age,' that is, are more likely to end as they last longer. It is a special form of time-varying transition probabilities in a regime-switching model of the business cycle. The transition probabilities are given by the probit specification for the evolution of the business cycle regime variable Sₜ, where Sₜ* is a latent variable defined as:\n\n$$\nS_{t}^{*}=\\gamma_{0}+\\gamma_{1}S_{t-1}+\\gamma_{2,d_{2}}(1-S_{t-1})N_{0,t-1}+\\gamma_{3,d_{3}}S_{t-1}N_{1,t-1}+u_{t}\n$$\n\nWhen γ₂,d₂ = 0 and γ₃,d₃ = 0, we have fixed transition probabilities or no business cycle duration dependence; when γ₂,d₂ > 0 and γ₃,d₃ < 0, business cycles are characterized by positive duration dependence. The prior probabilities p₂ and p₃ are independent prior probabilities of no duration dependence for recessions and booms, and their values can influence the posterior probabilities of duration dependence.",
    "category": "INCORRECT",
    "question_context_used": "Duration dependence in the business cycle and time-varying transition probabilities are related but not identical concepts. Duration dependence asks whether recessions or booms 'age,' that is, are more likely to end as they last longer. It is a special form of time-varying transition probabilities in a regime-switching model of the business cycle. The transition probabilities are given by the probit specification for the evolution of the business cycle regime variable Sₜ, where Sₜ* is a latent variable defined as:\n\n$$\nS_{t}^{*}=\\gamma_{0}+\\gamma_{1}S_{t-1}+\\gamma_{2,d_{2}}(1-S_{t-1})N_{0,t-1}+\\gamma_{3,d_{3}}S_{t-1}N_{1,t-1}+u_{t}\n$$\n\nWhen γ₂,d₂ = 0 and γ₃,d₃ = 0, we have fixed transition probabilities or no business cycle duration dependence; when γ₂,d₂ > 0 and γ₃,d₃ < 0, business cycles are characterized by positive duration dependence. The prior probabilities p₂ and p₃ are independent prior probabilities of no duration dependence for recessions and booms, and their values can influence the posterior probabilities of duration dependence.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1183",
    "question": "In the context of the controlled cash-balance dynamics with proportional transfer costs, which of the following statements are supported by the simulation results in Table 1?",
    "options": [
      "A) The linear model consistently overestimates the speed of adjustment in cash balances.",
      "B) The coefficient on the lagged dependent variable increases as the proportional transfer cost (k) increases.",
      "C) The Durbin-Watson statistic decreases monotonically with higher transfer costs, indicating increased autocorrelation.",
      "D) The R² values suggest that the linear model explains a higher proportion of variance when adjustment speeds are slower."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Transactions Demand for Money with a Stochastic, Time-Varying Interest Rate.md"
    ],
    "question_context": "The text analyzes the dynamics of net disbursements under control, where the error term $e(t+j)\\equiv x(t+j)-E\\big(\\dot{x}(t+j)|\\bar{\\mathcal{F}}_{\\tau}\\big)=\\int_{\\tau}^{t+j}d z_{x}(s)$ follows an Ito integral distribution. The study focuses on how adjustment frequencies vary with interest rates, leading to conditional heteroskedasticity. Key formula for the Fourier series representation of the money-demand analogue is given by:\n\n$$\nP(x_{t},j)=\\sum_{n=1}^{\\infty}\\sin{(n\\pi x_{t}/u)}\\exp{(-\\gamma_{n}j)}\\exp{(-\\mu x_{t}/\\sigma^{2})}\\cdot\\left(\\frac{n\\pi\\mu\\sigma^{2}}{\\gamma_{n}^{2}u^{2}}\\right)\\cdot\\left[\\exp\\left(\\mu u/\\sigma^{2}\\right)\\cdot(-1)^{n}\\cdot\\left[1-(u\\gamma_{n}/\\mu)\\right]-1\\right]\n$$\n\nwhere $\\gamma_{n}=(1/2)\\cdot(\\mu^{2}/\\sigma^{2}+n^{2}\\pi^{2}\\sigma^{2}/u^{2})$. The simulation results in Table 1 demonstrate that with proportional transfer costs, the linear model underestimates adjustment speeds, while the coefficient on the lagged dependent variable and R² values increase with higher transfer costs.",
    "category": "CORRECT",
    "question_context_used": "The text analyzes the dynamics of net disbursements under control, where the error term $e(t+j)\\equiv x(t+j)-E\\big(\\dot{x}(t+j)|\\bar{\\mathcal{F}}_{\\tau}\\big)=\\int_{\\tau}^{t+j}d z_{x}(s)$ follows an Ito integral distribution. The study focuses on how adjustment frequencies vary with interest rates, leading to conditional heteroskedasticity. Key formula for the Fourier series representation of the money-demand analogue is given by:\n\n$$\nP(x_{t},j)=\\sum_{n=1}^{\\infty}\\sin{(n\\pi x_{t}/u)}\\exp{(-\\gamma_{n}j)}\\exp{(-\\mu x_{t}/\\sigma^{2})}\\cdot\\left(\\frac{n\\pi\\mu\\sigma^{2}}{\\gamma_{n}^{2}u^{2}}\\right)\\cdot\\left[\\exp\\left(\\mu u/\\sigma^{2}\\right)\\cdot(-1)^{n}\\cdot\\left[1-(u\\gamma_{n}/\\mu)\\right]-1\\right]\n$$\n\nwhere $\\gamma_{n}=(1/2)\\cdot(\\mu^{2}/\\sigma^{2}+n^{2}\\pi^{2}\\sigma^{2}/u^{2})$. The simulation results in Table 1 demonstrate that with proportional transfer costs, the linear model underestimates adjustment speeds, while the coefficient on the lagged dependent variable and R² values increase with higher transfer costs.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1023",
    "question": "In the context of particle filters for dynamic macroeconomic models, which of the following statements are correct regarding the conditions for avoiding stochastic singularity?",
    "options": [
      "A) Assumption 1 alone is sufficient to ensure the model is not stochastically singular.",
      "B) The combination of Assumptions 1 and 3 provides a sufficient condition to avoid stochastic singularity.",
      "C) If $\\dim(V_{t}) < \\dim(\\mathbb{Y}_{t})$, the model can still be non-singular if $\\dim(W_{t})$ is sufficiently large.",
      "D) Stochastic singularity occurs when $\\dim(W_{t}) + \\dim(V_{t}) \\geq \\dim(\\mathbb{Y}_{t})$."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Estimating Macroeconomic Models A Likelihood Approach.md"
    ],
    "question_context": "Assumption 1 is a necessary condition for the model not to be stochastically singular. If Assumption 1 does not hold, that is, $\\dim(W_{t})+\\dim(V_{t})<\\dim(\\mathbb{Y}_{t})$ , the model cannot assign positive probability to the data because for an arbitrary $S_{t-1}$ it is not possible to find a combination of $W_{t}$ and $V_{t}$ such that the model can explain $\\mathcal{D}_{t}$. When we complement Assumption 1 with Assumption 3, we get a sufficient condition for the model not to be stochastically singular. The case where $\\dim(V_{t})<\\dim(\\mathbb{Y}_{t})$ is difficult to handle with standard notation, but with the appropriate partition of shocks, the likelihood can still be evaluated.",
    "category": "CORRECT",
    "question_context_used": "Assumption 1 is a necessary condition for the model not to be stochastically singular. If Assumption 1 does not hold, that is, $\\dim(W_{t})+\\dim(V_{t})<\\dim(\\mathbb{Y}_{t})$ , the model cannot assign positive probability to the data because for an arbitrary $S_{t-1}$ it is not possible to find a combination of $W_{t}$ and $V_{t}$ such that the model can explain $\\mathcal{D}_{t}$. When we complement Assumption 1 with Assumption 3, we get a sufficient condition for the model not to be stochastically singular. The case where $\\dim(V_{t})<\\dim(\\mathbb{Y}_{t})$ is difficult to handle with standard notation, but with the appropriate partition of shocks, the likelihood can still be evaluated.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3372",
    "question": "Which of the following statements are true regarding the properties and applications of $H$-sssi processes with infinite variance?",
    "options": [
      "A) The codifference and covariation measures coincide for any stationary symmetric Gaussian process.",
      "B) For $\\alpha$-stable $H$-sssi processes, the parameter $H$ must lie in the interval $(0, 1/\\alpha)$ when $\\alpha < 1$.",
      "C) Fractional Gaussian noise displays long-range dependence when $1/2 < H < 1$.",
      "D) The marginal distributions of $\\alpha$-stable $H$-sssi processes decay exponentially fast as $|x| \\to \\infty$."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "The-asymptotic-codifference-and-covariation-of-log-fra_2014_Journal-of-Econo.md"
    ],
    "question_context": "The study of $H$-sssi processes and their codependence structures often involves processes that are $\\alpha$-stable, $0 < \\alpha \\leq 2$. For these processes, the parameter $H$ must be in the interval (0, 1]. In fact, if $\\alpha < 1$ then $H$ is restricted to the interval $(0,1/\\alpha)$, while $H$ can take any value in (0, 1] if $1 \\leq \\alpha \\leq 2$. The boundary case $\\alpha = 2$ consists of the Gaussian processes, and for these one also has $0 < H \\leq 1$. The finite-dimensional distributions of any pair of $H$-sssi Gaussian processes, for fixed $0 < H < 1$, have mean zero and are defined up to a multiplicative constant. Thus there is essentially only one $H$-sssi Gaussian process. This process is fractional Brownian motion. Its increments are called fractional Gaussian noise (Samorodnitsky and Taqqu, 1994, Sec. 7.2). The special case $H = 1/2$ corresponds to Brownian motion. The increments of Brownian motion, called white noise, are independent. On the other hand, for fixed $0 < \\alpha < 2$ and $0 < H < 1$, there are different $\\alpha$-stable $H$-sssi processes and these are nonGaussian (Samorodnitsky and Taqqu, 1994, Secs. 7.3–7.9). One of them, the $\\alpha$-stable Lévy motion, is self-similar with stationary increments with $H = 1/\\alpha$. It is the stable counterpart to Brownian motion. It has independent increments, called $\\alpha$-stable Lévy noise. In fact, $\\alpha$-stable Lévy motion is the only nondegenerate $1/\\alpha$-sssi for $0 < \\alpha < 1.$ When $1 < \\alpha < 2$, however, there is another $1/\\alpha$-sssi process, called log-fractional stable motion.",
    "category": "INCORRECT",
    "question_context_used": "The study of $H$-sssi processes and their codependence structures often involves processes that are $\\alpha$-stable, $0 < \\alpha \\leq 2$. For these processes, the parameter $H$ must be in the interval (0, 1]. In fact, if $\\alpha < 1$ then $H$ is restricted to the interval $(0,1/\\alpha)$, while $H$ can take any value in (0, 1] if $1 \\leq \\alpha \\leq 2$. The boundary case $\\alpha = 2$ consists of the Gaussian processes, and for these one also has $0 < H \\leq 1$. The finite-dimensional distributions of any pair of $H$-sssi Gaussian processes, for fixed $0 < H < 1$, have mean zero and are defined up to a multiplicative constant. Thus there is essentially only one $H$-sssi Gaussian process. This process is fractional Brownian motion. Its increments are called fractional Gaussian noise (Samorodnitsky and Taqqu, 1994, Sec. 7.2). The special case $H = 1/2$ corresponds to Brownian motion. The increments of Brownian motion, called white noise, are independent. On the other hand, for fixed $0 < \\alpha < 2$ and $0 < H < 1$, there are different $\\alpha$-stable $H$-sssi processes and these are nonGaussian (Samorodnitsky and Taqqu, 1994, Secs. 7.3–7.9). One of them, the $\\alpha$-stable Lévy motion, is self-similar with stationary increments with $H = 1/\\alpha$. It is the stable counterpart to Brownian motion. It has independent increments, called $\\alpha$-stable Lévy noise. In fact, $\\alpha$-stable Lévy motion is the only nondegenerate $1/\\alpha$-sssi for $0 < \\alpha < 1.$ When $1 < \\alpha < 2$, however, there is another $1/\\alpha$-sssi process, called log-fractional stable motion.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2623",
    "question": "In the context of demographic explanations for unemployment rate changes, which of the following statements are supported by the analysis of constant-weight unemployment rates?",
    "options": [
      "A) The demographic gap reached its peak in 1975 due to the rise in teenagers' labor force share.",
      "B) The sustained increase in adult female labor force participation consistently raised the demographic gap in the 1980s.",
      "C) The percentage of unemployment's increase explained by demographic shifts declined continuously after 1973.",
      "D) The fixed-weight measure using 1950's labor force composition weights yields a higher explained portion of unemployment's deterioration than using 1960's weights."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Krashevski-NaturalHighUnemployment-1988.md"
    ],
    "question_context": "The demographic gap's evolution is influenced by changes in labor force shares and unemployment rates of young people and women. For instance, teenagers' labor force share rose from 7.8% in the 1960s to a 9.6% peak in 1974, causing the gap to increase and reach its high the following year. The labor force presence of teenagers then steadily receded until, at 6.7% in 1986, it had dropped back to the 1950 level. Similarly, the demographic gap declined to a mere 0.1% by 1986. The growth in women's labor force participation was sustained, but the continued rise in adult women's labor force share tended to reduce the demographic gap in the 1980s due to the erosion of manufacturing industries and the concentration of women in less cyclical occupations. Table 1 reveals that the portion of the unemployment rate's climb attributable to labor force changes dropped without interruption throughout the 1970s and 1980s. Using 1950's weights for the fixed weights increases each period's gap, but the percentage of unemployment's increase explained by the gap suffers the same relentless decline.",
    "category": "CORRECT",
    "question_context_used": "The demographic gap's evolution is influenced by changes in labor force shares and unemployment rates of young people and women. For instance, teenagers' labor force share rose from 7.8% in the 1960s to a 9.6% peak in 1974, causing the gap to increase and reach its high the following year. The labor force presence of teenagers then steadily receded until, at 6.7% in 1986, it had dropped back to the 1950 level. Similarly, the demographic gap declined to a mere 0.1% by 1986. The growth in women's labor force participation was sustained, but the continued rise in adult women's labor force share tended to reduce the demographic gap in the 1980s due to the erosion of manufacturing industries and the concentration of women in less cyclical occupations. Table 1 reveals that the portion of the unemployment rate's climb attributable to labor force changes dropped without interruption throughout the 1970s and 1980s. Using 1950's weights for the fixed weights increases each period's gap, but the percentage of unemployment's increase explained by the gap suffers the same relentless decline.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3046",
    "question": "According to the definitions of fractional processes and cofraction matrix polynomials, which of the following statements are correct?",
    "options": [
      "A) A process $X_{t}$ is fractional of order $d$ if $\\Delta_{+}^{d}X_{t}-\\mu_{t}\\in\\mathcal{F}(0)_{+}$ for some deterministic function $\\mu_{t}$.",
      "B) The spectrum of a fractional process of order zero must satisfy $f_{X}(1)=(1/2\\pi)C(1)\\Omega C(1)^{\\prime}=0$.",
      "C) A cofraction matrix polynomial $\\gamma_{n}(u)$ ensures that the order of fractionality of $\\gamma_{n}^{\\prime}(L_{b})X_{t}$ cannot be reduced by taking linear combinations or including additional powers of $\\Delta^{b}\\boldsymbol{X}_{t}$.",
      "D) The case $n=0$ in $\\gamma_{n}(u)$ is excluded from the definition of cofraction matrix polynomials."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Franchi - 2010 - A REPRESENTATION THEORY FOR POLYNOMIAL COFRACTIONALITY IN VECTOR AUTOREGRESSIVE MODELS.md"
    ],
    "question_context": "DEFINITION 2.1. If $\\textstyle\\sum_{i=0}^{\\infty}||C_{i}||^{2}<\\infty$ and $\\begin{array}{r}{C(z)=\\sum_{i=0}^{\\infty}C_{i}z^{i}}\\end{array}$ , $|z|<1$ can be extended to a continuous function on the boundary $|z|=1$ , we call the linear process $X_{t}=C(L)\\epsilon_{t}$ fractional of order zero, $\\mathcal{F}(0)$ , if the spectrum at $I$ is different from $O$ , i.e., if $f_{X}(1)=(1/2\\pi)C(1)\\Omega C(1)^{\\prime}\\neq0.$ . For such processes we denote by ${\\mathcal{F}}(0)_{+}$ the class of asymptotically stationary processes of the form \n\n$$\nX_{t}^{+}=\\left\\{\\begin{array}{l l}{\\sum_{i=0}^{t-1}C_{i}\\epsilon_{t-i}}&{t=1,2,...}\\\\{0}&{t=0,-1,...}\\end{array}\\right..\n$$ \n\nIf $\\Delta_{+}^{d}X_{t}-\\mu_{t}\\in\\mathcal{F}(0)_{+}$ for some deterministic function $\\mu_{t}$ that depends on initial values we say that $X_{t}$ is fractional of order d and write $X_{t}\\in\\mathcal{F}(d)$ .\n\nDEFINITION 2.2. Let $X_{t}\\in\\mathcal{F}(d)$ and $\\gamma_{n}(u):=\\Sigma_{i=0}^{n}\\gamma_{n,i}(1-u)^{i}$ be such that $\\gamma_{n}^{\\prime}(L_{b})X_{t}\\in\\mathcal{F}(d_{n})$ , where $d_{n}:=d-(n+1)b$ and $0\\leq d_{n}<d$ . If for any $\\varphi\\neq0$ and any polynomial $\\phi(u):=\\gamma_{n}(u)+(1-u)^{n+1}\\psi(u)$ one has \n\n$$\n\\varphi^{\\prime}\\phi^{\\prime}(L_{b})X_{t}\\in{\\mathcal{F}}(d_{n}),\n$$ \n\nwe say that $\\gamma_{n}(u)$ is a cofraction matrix polynomial and $\\gamma_{n}^{\\prime}(L_{b})X_{t}$ are polynomially cofractional relations.",
    "category": "CORRECT",
    "question_context_used": "DEFINITION 2.1. If $\\textstyle\\sum_{i=0}^{\\infty}||C_{i}||^{2}<\\infty$ and $\\begin{array}{r}{C(z)=\\sum_{i=0}^{\\infty}C_{i}z^{i}}\\end{array}$ , $|z|<1$ can be extended to a continuous function on the boundary $|z|=1$ , we call the linear process $X_{t}=C(L)\\epsilon_{t}$ fractional of order zero, $\\mathcal{F}(0)$ , if the spectrum at $I$ is different from $O$ , i.e., if $f_{X}(1)=(1/2\\pi)C(1)\\Omega C(1)^{\\prime}\\neq0.$ . For such processes we denote by ${\\mathcal{F}}(0)_{+}$ the class of asymptotically stationary processes of the form \n\n$$\nX_{t}^{+}=\\left\\{\\begin{array}{l l}{\\sum_{i=0}^{t-1}C_{i}\\epsilon_{t-i}}&{t=1,2,...}\\\\{0}&{t=0,-1,...}\\end{array}\\right..\n$$ \n\nIf $\\Delta_{+}^{d}X_{t}-\\mu_{t}\\in\\mathcal{F}(0)_{+}$ for some deterministic function $\\mu_{t}$ that depends on initial values we say that $X_{t}$ is fractional of order d and write $X_{t}\\in\\mathcal{F}(d)$ .\n\nDEFINITION 2.2. Let $X_{t}\\in\\mathcal{F}(d)$ and $\\gamma_{n}(u):=\\Sigma_{i=0}^{n}\\gamma_{n,i}(1-u)^{i}$ be such that $\\gamma_{n}^{\\prime}(L_{b})X_{t}\\in\\mathcal{F}(d_{n})$ , where $d_{n}:=d-(n+1)b$ and $0\\leq d_{n}<d$ . If for any $\\varphi\\neq0$ and any polynomial $\\phi(u):=\\gamma_{n}(u)+(1-u)^{n+1}\\psi(u)$ one has \n\n$$\n\\varphi^{\\prime}\\phi^{\\prime}(L_{b})X_{t}\\in{\\mathcal{F}}(d_{n}),\n$$ \n\nwe say that $\\gamma_{n}(u)$ is a cofraction matrix polynomial and $\\gamma_{n}^{\\prime}(L_{b})X_{t}$ are polynomially cofractional relations.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2366",
    "question": "In the context of simulated maximum likelihood (SML) estimation for nonlinear labor supply models, which of the following conditions are necessary for the SML estimator to be consistent and asymptotically efficient?",
    "options": [
      "A) The number of observations (N) and the number of draws per observation (H) both tend to infinity",
      "B) The ratio of the square root of the number of observations to the number of draws (√N/H) tends to zero",
      "C) The draws for different observations are independent",
      "D) The likelihood function is a continuous and differentiable function of the parameters"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Pradhan-HouseholdLaborSupply-1997.md"
    ],
    "question_context": "The SML estimator is consistent if $N\\rightarrow\\infty$ (number of observations) and $H\\rightarrow\\infty$ (number of draws per observation). Moreover, if draws for different observations are independent and $\\sqrt{N}/H\\rightarrow0$ (i.e., $H\\longrightarrow\\infty$ \"fast enough\"), SML is asymptotically efficient and equivalent to maximum likelihood (Gourieroux and Monfort (1993)). Because the errors in the labor supply part of the model are retained and not simulated, the approximate likelihood is a continuous and differentiable function of the parameters. This makes maximization feasible and should, according to previous studies on similar models (Boersch-Supan and Hajivassiliou (1993), lead to satisfactory results for small $H$ already. We use $H=60.$ 8\n\n$^7\\mathbf{In}$ principle, simulations could be avoided, since the likelihood contribution can alwaysbe rewritten as a two-dimensionalintegral(over $w_{m}^{*}$ and $w_{f}^{*})$ .The integral could be computed numerically. However, accurate two-dimensional integration can be quite time consuming and requires a larger programming effort. Note that, in spite of the independence assumptions, the double integral cannot be written as the product of two single integrals because of the factor $L_{I}.$",
    "category": "INCORRECT",
    "question_context_used": "The SML estimator is consistent if $N\\rightarrow\\infty$ (number of observations) and $H\\rightarrow\\infty$ (number of draws per observation). Moreover, if draws for different observations are independent and $\\sqrt{N}/H\\rightarrow0$ (i.e., $H\\longrightarrow\\infty$ \"fast enough\"), SML is asymptotically efficient and equivalent to maximum likelihood (Gourieroux and Monfort (1993)). Because the errors in the labor supply part of the model are retained and not simulated, the approximate likelihood is a continuous and differentiable function of the parameters. This makes maximization feasible and should, according to previous studies on similar models (Boersch-Supan and Hajivassiliou (1993), lead to satisfactory results for small $H$ already. We use $H=60.$ 8\n\n$^7\\mathbf{In}$ principle, simulations could be avoided, since the likelihood contribution can alwaysbe rewritten as a two-dimensionalintegral(over $w_{m}^{*}$ and $w_{f}^{*})$ .The integral could be computed numerically. However, accurate two-dimensional integration can be quite time consuming and requires a larger programming effort. Note that, in spite of the independence assumptions, the double integral cannot be written as the product of two single integrals because of the factor $L_{I}.$",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4630",
    "question": "In the context of estimating labor supply functions with unobserved costs of work, which of the following statements are correct regarding the estimation strategies and model specifications?",
    "options": [
      "A) When time and money costs of work are unavailable, quasi-reduced form estimates reflect both utility parameters and cost variations.",
      "B) The augmented hours equation includes the natural logarithm of the wife's hourly wage, husband's income, and demographic variables.",
      "C) Maximum likelihood estimation is required when reservation hours and wage offers are unobserved for nonworkers.",
      "D) The likelihood function for workers involves integrating over the trivariate normal density of hours, wages, and reservation hours."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "Cogan-FixedCostsLabor-1981.md"
    ],
    "question_context": "To estimate labor supply functions when there are costs of labor force participation, one of two empirical strategies may be followed. If information on the time and money costs of labor market entry incurred by participants is available, then these costs can be used to estimate the structural parameters of not only the labor supply function but also of costs of work functions. On the other hand, if information on the time and money costs of work is not available, then one can only obtain estimates of a quasi-reduced form labor supply function. The quasi-reduced form estimates will reflect not only the parameters of the individual's utility function but also the effects of variations in the cost of work.\n\nThe augmented hours equation is assumed to take the following form for the ith individual:\n\n$$\n\\begin{array}{c}{{h_{i}^{*}=\\gamma_{0}+\\gamma_{1}N(W_{i})+\\gamma_{2}Y_{H_{i}}+\\gamma_{3}C_{i}+\\gamma_{4}E_{i}+\\gamma_{5}A_{i}+\\epsilon_{0,i}}}\n{{\\equiv\\gamma_{1}N(W_{i})+\\gamma^{\\prime}Z_{1}+\\epsilon_{0},}}\\end{array}\n$$\n\nwhere $N(W_{i})$ is the natural logarithm of the wife's hourly wage, $E$ is her years of education, $C$ is the number of preschool children (age 0-5) in the home, $Y_{H_{\\ell}}$ is the husband's income, and $A_{i}$ is the wife's age. The theoretical and empirical relevance of each of these variables, in the absence of costs of work, is well known.\n\nClearly, if the endogenous variables, $h^{*},h_{R}$ ,and $W$ , could be observed for all individuals, then the parameters or equations (10)-(13) could be estimated with two- or three-stage least squares. Unfortunately, these variables are not observed for all individuals. Reservation hours cannot be observed for any individual in the data, and $h^{*}$ and wage offers cannot be observed for nonworkers. Without these data, an alternative statistical approach, such as maximum likelihood is required to estimate the parameters.",
    "category": "INCORRECT",
    "question_context_used": "To estimate labor supply functions when there are costs of labor force participation, one of two empirical strategies may be followed. If information on the time and money costs of labor market entry incurred by participants is available, then these costs can be used to estimate the structural parameters of not only the labor supply function but also of costs of work functions. On the other hand, if information on the time and money costs of work is not available, then one can only obtain estimates of a quasi-reduced form labor supply function. The quasi-reduced form estimates will reflect not only the parameters of the individual's utility function but also the effects of variations in the cost of work.\n\nThe augmented hours equation is assumed to take the following form for the ith individual:\n\n$$\n\\begin{array}{c}{{h_{i}^{*}=\\gamma_{0}+\\gamma_{1}N(W_{i})+\\gamma_{2}Y_{H_{i}}+\\gamma_{3}C_{i}+\\gamma_{4}E_{i}+\\gamma_{5}A_{i}+\\epsilon_{0,i}}}\n{{\\equiv\\gamma_{1}N(W_{i})+\\gamma^{\\prime}Z_{1}+\\epsilon_{0},}}\\end{array}\n$$\n\nwhere $N(W_{i})$ is the natural logarithm of the wife's hourly wage, $E$ is her years of education, $C$ is the number of preschool children (age 0-5) in the home, $Y_{H_{\\ell}}$ is the husband's income, and $A_{i}$ is the wife's age. The theoretical and empirical relevance of each of these variables, in the absence of costs of work, is well known.\n\nClearly, if the endogenous variables, $h^{*},h_{R}$ ,and $W$ , could be observed for all individuals, then the parameters or equations (10)-(13) could be estimated with two- or three-stage least squares. Unfortunately, these variables are not observed for all individuals. Reservation hours cannot be observed for any individual in the data, and $h^{*}$ and wage offers cannot be observed for nonworkers. Without these data, an alternative statistical approach, such as maximum likelihood is required to estimate the parameters.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1691",
    "question": "Based on the GEMS/Water data analysis, which of the following statements are correct regarding Biochemical Oxygen Demand (BOD)?",
    "options": [
      "A) BOD is chosen as a pollution measure due to its consistency in measurement across countries and its common occurrence in GEMS data.",
      "B) Elevated BOD levels are primarily caused by industrial activity heterogeneity rather than sewage.",
      "C) Approximately 34% of pollution from an average upstream station reaches an international border, with the rest attenuating naturally.",
      "D) BOD concentrations above 4 mg/l are considered unacceptable for any recreational use in the United States."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Sigman-InternationalSpilloversWater-2002.md"
    ],
    "question_context": "The UN's Global Environmental Monitoring System Water Quality Monitoring Project (GEMS/Water) provides data on various water quality measures in rivers, lakes, and groundwater. This study focuses specifically on the data for rivers. Annual average pollution levels are available for 1979-1990, and triennial averages for 1988-1996. Biochemical oxygen demand (BOD) is used as the pollution measure for several reasons. First, BOD is among the most common water quality measures in GEMS, providing many observations for analysis. Second, BOD levels are easily measured by standard procedures, which helps assure consistency in data quality across countries. Third, elevated BOD is attributable to a range of human activities, especially sewage, so heterogeneity in local industrial activity is not very important to BOD levels. Fourth, BOD may travel reasonably far downstream, allowing significant spillovers at many stations on international rivers. My rough calculation is that 34 percent of pollution at an average station upstream of a border will reach the border, with the remainder having attenuated naturally. Rivers with BOD higher than $4~\\mathrm{mg/l}$ would not be acceptable for any recreational use (including boating) in the United States (William J. Vaughan, 1986).",
    "category": "CORRECT",
    "question_context_used": "The UN's Global Environmental Monitoring System Water Quality Monitoring Project (GEMS/Water) provides data on various water quality measures in rivers, lakes, and groundwater. This study focuses specifically on the data for rivers. Annual average pollution levels are available for 1979-1990, and triennial averages for 1988-1996. Biochemical oxygen demand (BOD) is used as the pollution measure for several reasons. First, BOD is among the most common water quality measures in GEMS, providing many observations for analysis. Second, BOD levels are easily measured by standard procedures, which helps assure consistency in data quality across countries. Third, elevated BOD is attributable to a range of human activities, especially sewage, so heterogeneity in local industrial activity is not very important to BOD levels. Fourth, BOD may travel reasonably far downstream, allowing significant spillovers at many stations on international rivers. My rough calculation is that 34 percent of pollution at an average station upstream of a border will reach the border, with the remainder having attenuated naturally. Rivers with BOD higher than $4~\\mathrm{mg/l}$ would not be acceptable for any recreational use (including boating) in the United States (William J. Vaughan, 1986).",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2961",
    "question": "In the context of the Haughwout-Inman general equilibrium model of city fiscal competition, which of the following statements correctly describe the equilibrium effects of city tax rates on city tax bases?",
    "options": [
      "A) The property tax base (Bₚ) is defined as the sum of the endogenous market value of residential housing and residential land plus endogenous firm capital and commercial land divided by endogenous city population.",
      "B) The city's sales tax base (Bₛ) equals exogenous consumption per resident within the city.",
      "C) The equilibrium elasticity of the base Bₜ with respect to changes in rate τⱼ (εₜⱼ) can be positive when land taxes finance valued public goods.",
      "D) The city's revenue hill is specified as REV = Σₜ τₜ⋅bₜ(τ; Z, c, r, q, 1, V₀) + Z, where Z represents exogenous city revenues."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Haughwout-LocalRevenueHills-2004.md"
    ],
    "question_context": "The Haughwout-Inman model defines an equilibrium wage and an equilibrium land value for the city as well as equilibrium quantities of city residential housing and consumption, firm use of capital, land, and labor, and finally, total city output. Each endogenous city price and quantity is a function of a vector of all city tax rates (τ) and the level of city services (G), the market interest rate (r), alternative wages (q) for workers who commute, the world price for city exports (≡1), the level of profit available to firms in their next best alternative (Π₀≡0), and the level of utility available to city residents in their next best location (V₀). The vector of city tax rates (τ) includes a tax on residential and commercial property (τₚ), a tax on resident consumption (τₛ), taxes on resident (τ_w) and commuter (τₘ) labor incomes, and a tax on gross receipts from city production (τₓ).\n\nThe city's property tax base (Bₚ) is defined as the sum of the endogenous market value of residential housing and residential land plus endogenous firm capital and commercial land divided by endogenous city population. The city sales tax base (Bₛ) equals endogenous consumption per resident within the city. The city's resident earned income tax base (B_w) equals the endogenous resident wage. The city's nonresident earned income tax base (Bₘ) equals the exogenous commuter wage times the endogenous number of city commuter workers divided by the endogenous city population. Finally, the city's gross receipts tax base (Bₓ) equals the endogenous city output per resident. Each tax base depends on all city tax rates [τ = (τₚ, τₛ, τ_w, τₘ, τₓ)] and city public services (G), exogenous market prices (q, 1, r), and the attractiveness of alternative locations available to city firms (Π₀≡0) and households (V₀). Generally, Bₜ = Bₜ(τ, G; q, 1, r, V₀), where t = p, s, w, m, or x for each of the five possible city tax bases.",
    "category": "INCORRECT",
    "question_context_used": "The Haughwout-Inman model defines an equilibrium wage and an equilibrium land value for the city as well as equilibrium quantities of city residential housing and consumption, firm use of capital, land, and labor, and finally, total city output. Each endogenous city price and quantity is a function of a vector of all city tax rates (τ) and the level of city services (G), the market interest rate (r), alternative wages (q) for workers who commute, the world price for city exports (≡1), the level of profit available to firms in their next best alternative (Π₀≡0), and the level of utility available to city residents in their next best location (V₀). The vector of city tax rates (τ) includes a tax on residential and commercial property (τₚ), a tax on resident consumption (τₛ), taxes on resident (τ_w) and commuter (τₘ) labor incomes, and a tax on gross receipts from city production (τₓ).\n\nThe city's property tax base (Bₚ) is defined as the sum of the endogenous market value of residential housing and residential land plus endogenous firm capital and commercial land divided by endogenous city population. The city sales tax base (Bₛ) equals endogenous consumption per resident within the city. The city's resident earned income tax base (B_w) equals the endogenous resident wage. The city's nonresident earned income tax base (Bₘ) equals the exogenous commuter wage times the endogenous number of city commuter workers divided by the endogenous city population. Finally, the city's gross receipts tax base (Bₓ) equals the endogenous city output per resident. Each tax base depends on all city tax rates [τ = (τₚ, τₛ, τ_w, τₘ, τₓ)] and city public services (G), exogenous market prices (q, 1, r), and the attractiveness of alternative locations available to city firms (Π₀≡0) and households (V₀). Generally, Bₜ = Bₜ(τ, G; q, 1, r, V₀), where t = p, s, w, m, or x for each of the five possible city tax bases.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4433",
    "question": "In the analysis of missionary expansion in sub-Saharan Africa, which of the following factors are identified as significant determinants of mission locations based on the regression results?",
    "options": [
      "A) Healthier environments (lower malaria and tsetse prevalence)",
      "B) Proximity to large pre-colonial cities before 1850",
      "C) Presence of cash crops and slavery indicators",
      "D) Accessibility via transportation networks (ports, rivers, railroads)"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Jedwab 等 - 2022 - The economics of missionary expansion evidence from Africa and implications for development.md"
    ],
    "question_context": "Key findings from the regression analysis: Missionaries appear to have chosen locations with healthier environments (malaria, tsetse). Especially before 1850, missionaries seem to have avoided large pre-colonial cities, ethnic homelands colonized later, and Muslim centers, three measures of potential local resistance. Transportation possibly played an important role: ports and coastal proximity may have facilitated initial access, while rivers, explorer routes, and railroads possibly enabled internal diffusion. We find positive correlations for slavery and cash crops. The regression equation for mission location probability is given by: $P(mission) = \\beta_0 + \\beta_1(health) + \\beta_2(transport) + \\beta_3(cash crops) + \\beta_4(slavery) + \\epsilon$, where all coefficients except $\\beta_2$ (pre-colonial cities) are positive and significant at p<0.05 level.",
    "category": "CORRECT",
    "question_context_used": "Key findings from the regression analysis: Missionaries appear to have chosen locations with healthier environments (malaria, tsetse). Especially before 1850, missionaries seem to have avoided large pre-colonial cities, ethnic homelands colonized later, and Muslim centers, three measures of potential local resistance. Transportation possibly played an important role: ports and coastal proximity may have facilitated initial access, while rivers, explorer routes, and railroads possibly enabled internal diffusion. We find positive correlations for slavery and cash crops. The regression equation for mission location probability is given by: $P(mission) = \\beta_0 + \\beta_1(health) + \\beta_2(transport) + \\beta_3(cash crops) + \\beta_4(slavery) + \\epsilon$, where all coefficients except $\\beta_2$ (pre-colonial cities) are positive and significant at p<0.05 level.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-610",
    "question": "According to David Mayes' econometric model in 'The Property Boom', which of the following factors are identified as critical in explaining the house price inflation during 1971-1973?",
    "options": [
      "A) Building Societies actively causing price inflation through abnormal lending patterns",
      "B) Buoyant housing demand coupled with price-inelastic supply of new housing",
      "C) Substantial inflow of funds to Building Societies due to favorable interest rates",
      "D) Government policies directly controlling Building Society liquidity ratios"
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Property_Boom_The_Effects_of_Building_Society_Behaviour_on_House_Prices.md"
    ],
    "question_context": "Mayes concludes that although the Building Societies were a contributory factor in the initial stages of the inflation, there was no departure from their normal pattern of activities over this period and, therefore, they cannot be said to have ‘caused' the house price inflation. Certainly there was an increase in mortgage lending, but this was a natural consequence of the substantial inflow of funds resulting from favourable interest rates. And this supply of funds could only produce higher prices if housing demand was buoyant and the supply of new housing price inelastic. Indeed a similarly large increase in mortgage funds in 1975-6 failed to replicate the earlier pattern of price inflation because it took place in the context of falling real incomes and rising unemployment. In short, Mayes views the Societies as endogenous to the system, responding to events rather than instigating them.",
    "category": "CORRECT",
    "question_context_used": "Mayes concludes that although the Building Societies were a contributory factor in the initial stages of the inflation, there was no departure from their normal pattern of activities over this period and, therefore, they cannot be said to have ‘caused' the house price inflation. Certainly there was an increase in mortgage lending, but this was a natural consequence of the substantial inflow of funds resulting from favourable interest rates. And this supply of funds could only produce higher prices if housing demand was buoyant and the supply of new housing price inelastic. Indeed a similarly large increase in mortgage funds in 1975-6 failed to replicate the earlier pattern of price inflation because it took place in the context of falling real incomes and rising unemployment. In short, Mayes views the Societies as endogenous to the system, responding to events rather than instigating them.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3946",
    "question": "Based on the AUROC analysis of the Conference Board's Index of Leading Indicators, which of the following statements are correct regarding the classification ability of its components?",
    "options": [
      "A) The 10-year T-bond-FFR spread achieves its maximum AUROC at a horizon of 18 months, indicating its predictive strength for distant future recessions.",
      "B) Year-on-year growth transformations generally yield higher AUROC values than month-on-month transformations across most indicators.",
      "C) An AUROC value below 0.5 implies the classifier is useless and cannot be inverted to provide useful information.",
      "D) Initial claims for unemployment insurance show the highest AUROC for year-on-year transformation at a horizon of 2 months, indicating strong short-term predictive power."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Berge-EvaluatingClassificationEconomic-2011.md"
    ],
    "question_context": "The Conference Board's Index of Leading Indicators includes ten components, some capturing market expectations (e.g., S&P 500, Treasury yield spread) and others measuring direct precursors of economic activity (e.g., building permits, unemployment claims). Figure 8 displays AUROCs across horizons $h=0,1,\\ldots,24$ for these indicators, showing that many achieve maxima near $h=0$ but minima at 12-18 months. An $AUROC<0.5$ indicates a 'perverse' classifier, but its reciprocal can be useful ($AUROC>0.5$). Table 6 highlights that the 10-year T-bond-FFR spread's AUROC peaks at 18 months, while initial claims for unemployment show high short-term predictive power (AUROC 0.96 at $h=2$ for year-on-year transformation). The logistic model for posterior probabilities is given by: $$\\log\\frac{P[S_{t+h}=0|w_{t}]}{P[S_{t+h}=1|w_{t}]}=\\beta_{h0}+\\beta_{h}^{\\prime}w_{t};\\qquad h\\in\\{0,...,24\\}.$$",
    "category": "CORRECT",
    "question_context_used": "The Conference Board's Index of Leading Indicators includes ten components, some capturing market expectations (e.g., S&P 500, Treasury yield spread) and others measuring direct precursors of economic activity (e.g., building permits, unemployment claims). Figure 8 displays AUROCs across horizons $h=0,1,\\ldots,24$ for these indicators, showing that many achieve maxima near $h=0$ but minima at 12-18 months. An $AUROC<0.5$ indicates a 'perverse' classifier, but its reciprocal can be useful ($AUROC>0.5$). Table 6 highlights that the 10-year T-bond-FFR spread's AUROC peaks at 18 months, while initial claims for unemployment show high short-term predictive power (AUROC 0.96 at $h=2$ for year-on-year transformation). The logistic model for posterior probabilities is given by: $$\\log\\frac{P[S_{t+h}=0|w_{t}]}{P[S_{t+h}=1|w_{t}]}=\\beta_{h0}+\\beta_{h}^{\\prime}w_{t};\\qquad h\\in\\{0,...,24\\}.$$",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2179",
    "question": "In the context of the three-output FFCQ cost function analysis for public and private IHEs, which of the following statements are empirically supported by the results?",
    "options": [
      "A) Higher undergraduate enrollments (UD) increase costs at an increasing rate for public institutions",
      "B) The UDRES interaction coefficient is significantly negative for private IHEs, indicating complementarity",
      "C) The positive interaction terms (UDGR and GRRES) suggest discomplementarity among respective outputs",
      "D) The public and private cost functions exhibit different structural coefficients"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Cohn-InstitutionsHigherEducation-1989.md"
    ],
    "question_context": "The empirical analysis draws from a cross-sectional survey of IHEs for the academic year 1981-82, using a usable sample of 1887 institutions. Costs are specified as a function of three outputs: undergraduate FTE enrollment (UD), graduate FTE enrollment (GR), and research (RES). The three-output FFCQ cost function was estimated separately for public and private institutions. Key findings include: higher UD enrollments increase costs at an increasing rate for public IHEs and at a decreasing rate for private IHEs. The UDRES interaction coefficient is significantly negative for public IHEs (suggesting complementarity) but not significant for private IHEs. Positive interaction terms (UDGR and GRRES) indicate discomplementarity among outputs, though pervasive economies of scope suggest overall complementarity.",
    "category": "CORRECT",
    "question_context_used": "The empirical analysis draws from a cross-sectional survey of IHEs for the academic year 1981-82, using a usable sample of 1887 institutions. Costs are specified as a function of three outputs: undergraduate FTE enrollment (UD), graduate FTE enrollment (GR), and research (RES). The three-output FFCQ cost function was estimated separately for public and private institutions. Key findings include: higher UD enrollments increase costs at an increasing rate for public IHEs and at a decreasing rate for private IHEs. The UDRES interaction coefficient is significantly negative for public IHEs (suggesting complementarity) but not significant for private IHEs. Positive interaction terms (UDGR and GRRES) indicate discomplementarity among outputs, though pervasive economies of scope suggest overall complementarity.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1507",
    "question": "Based on the decision-theoretic framework and empirical results presented, which of the following statements accurately describe the relationship between willingness to pay (WTP) and dishonest reporting behavior?",
    "options": [
      "A) Subjects with higher WTP in the Untruthful Reporting Condition are significantly less likely to misreport",
      "B) Female subjects show no significant difference in misreporting behavior compared to male subjects",
      "C) The probability of misreporting decreases by 8-10 percentage points for each EUR 1 increase in WTP",
      "D) Subjects who self-select into the Good Lottery have a higher propensity to misreport than those in the Bad Lottery"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Konrad 等 - 2021 - Pecunia non olet On the self-selection into (dis)honest earning opportunities.md"
    ],
    "question_context": "The analysis reveals that subjects with higher WTP in the Untruthful Reporting Condition (URC) are less likely to misreport, with a marginal effect of EUR 1 increase in WTP corresponding to an 8-10 percentage-point decrease in misreporting probability ($\\chi^{2}$-test: $p<0.01$). The multivariate probit regression shows robust coefficients for WTP URC (-0.300*** to -0.329***) and female subjects (-0.704** to -0.541*). The lying cost estimation follows the equation $\\theta=z/(p_{G}-p_{B})$ if $\\xi(x_{L})=x_{H}$ and $\\theta\\geq x_{H}-x_{L}$ if $\\xi(x_{L})=x_{L}$, revealing a bimodal distribution with 30% having zero lying costs and 50% having prohibitively high costs.",
    "category": "INCORRECT",
    "question_context_used": "The analysis reveals that subjects with higher WTP in the Untruthful Reporting Condition (URC) are less likely to misreport, with a marginal effect of EUR 1 increase in WTP corresponding to an 8-10 percentage-point decrease in misreporting probability ($\\chi^{2}$-test: $p<0.01$). The multivariate probit regression shows robust coefficients for WTP URC (-0.300*** to -0.329***) and female subjects (-0.704** to -0.541*). The lying cost estimation follows the equation $\\theta=z/(p_{G}-p_{B})$ if $\\xi(x_{L})=x_{H}$ and $\\theta\\geq x_{H}-x_{L}$ if $\\xi(x_{L})=x_{L}$, revealing a bimodal distribution with 30% having zero lying costs and 50% having prohibitively high costs.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\"]}",
      "model_answer": [
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2215",
    "question": "In the context of asymptotic theory for time series, which of the following statements are correct regarding the process $Y_t$ and its limiting distribution?",
    "options": [
      "A) The subscript $T$ in $Y_{T,t}$ is dropped for notational simplicity despite its dependence on $T$.",
      "B) The weak convergence of $T^{-1/2}Y_{[T\\cdot]}$ to $\\sigma R$ is guaranteed by the given conditions.",
      "C) The limiting distribution can be expressed in terms of $R$ without requiring $T^{-1/2}Y_{[T\\cdot]}$ to converge weakly to $\\sigma R$.",
      "D) The array $Y_t$ is strictly stationary for all $t=1,\\ldots,T$."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Cavaliere和Georgiev - 2008 - Regime-switching autoregressive coefficients and the asymptotics for unit root tests.md"
    ],
    "question_context": "The text discusses the asymptotic properties of the process $Y_t$, which is an array of the form $Y_{T,t}$ for $t=1,\\ldots,T$, with the $T$ subscript typically dropped for simplicity. A key point is that the limiting distribution is expressed in terms of $R$, but the process $T^{-1/2}Y_{[T\\cdot]}$ does not necessarily converge weakly to $\\sigma R$. This distinction is crucial for understanding the behavior of the process in the limit. The proofs for these statements are collected in the Appendix, and further references can be found in Lin (1997).",
    "category": "CORRECT",
    "question_context_used": "The text discusses the asymptotic properties of the process $Y_t$, which is an array of the form $Y_{T,t}$ for $t=1,\\ldots,T$, with the $T$ subscript typically dropped for simplicity. A key point is that the limiting distribution is expressed in terms of $R$, but the process $T^{-1/2}Y_{[T\\cdot]}$ does not necessarily converge weakly to $\\sigma R$. This distinction is crucial for understanding the behavior of the process in the limit. The proofs for these statements are collected in the Appendix, and further references can be found in Lin (1997).",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3175",
    "question": "According to the Modigliani-Miller theorem and the experimental setup described, which of the following statements are correct regarding the cost of capital and firm valuation?",
    "options": [
      "A) The cost of capital remains constant regardless of the firm's leverage ratio in perfect capital markets.",
      "B) Investors systematically underestimate the risk of low-leveraged equity and overestimate the risk of high-leveraged equity.",
      "C) The market value of a firm is determined by capitalizing its expected return at a rate appropriate to its risk level, independent of its capital structure.",
      "D) Arbitrage opportunities are necessary for the Modigliani-Miller theorem to hold in equilibrium."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Levati 等 - 2012 - Testing the modigliani-miller theorem directly in the lab.md"
    ],
    "question_context": "The Modigliani-Miller theorem states that in a perfect capital market, the value of a firm is independent of how that firm is financed. The cost of capital, represented by the formula $\\rho = (1-\\tau)\\cdot i + \\tau\\cdot r$, should remain constant if the theorem holds perfectly. However, experimental results show a U-shaped cost of capital curve, indicating that subjects tend to underestimate the systematic risk of low-leveraged equity and overestimate the systematic risk of high-leveraged equity. The theorem's validity does not rely on arbitrage, as demonstrated by Stiglitz's general equilibrium model, which shows that the theorem holds under mild assumptions about individual risk attitudes and initial wealth positions. The market value of the firm is derived from capitalizing its expected return at a rate appropriate to its risk level, as shown by the equilibrium calculation where $V = 100 \\times s = 628.07$ in a zero-leverage scenario.",
    "category": "INCORRECT",
    "question_context_used": "The Modigliani-Miller theorem states that in a perfect capital market, the value of a firm is independent of how that firm is financed. The cost of capital, represented by the formula $\\rho = (1-\\tau)\\cdot i + \\tau\\cdot r$, should remain constant if the theorem holds perfectly. However, experimental results show a U-shaped cost of capital curve, indicating that subjects tend to underestimate the systematic risk of low-leveraged equity and overestimate the systematic risk of high-leveraged equity. The theorem's validity does not rely on arbitrage, as demonstrated by Stiglitz's general equilibrium model, which shows that the theorem holds under mild assumptions about individual risk attitudes and initial wealth positions. The market value of the firm is derived from capitalizing its expected return at a rate appropriate to its risk level, as shown by the equilibrium calculation where $V = 100 \\times s = 628.07$ in a zero-leverage scenario.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2078",
    "question": "In Lynn C. Burbridge's analysis of African-American women's employment reliance, which of the following factors are identified as critical determinants of their dependence on government and third-sector employment?",
    "options": [
      "A) Structural discrimination in private-sector hiring practices",
      "B) Higher educational attainment compared to other demographic groups",
      "C) Historical exclusion from labor unions and collective bargaining",
      "D) Policy-driven expansion of public-sector job opportunities"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Burbridge-RelianceAfricanAmericanWomen-1994.md"
    ],
    "question_context": "The study examines the structural and policy-driven factors that have shaped African-American women's employment patterns. A key finding is that systemic discrimination in private-sector hiring has historically limited their opportunities, pushing them toward government and third-sector jobs. Additionally, policy initiatives during the 1960s and 1970s expanded public-sector employment, which disproportionately benefited African-American women. The intersection of these factors created a reliance that persists despite fluctuations in the broader economy.",
    "category": "INCORRECT",
    "question_context_used": "The study examines the structural and policy-driven factors that have shaped African-American women's employment patterns. A key finding is that systemic discrimination in private-sector hiring has historically limited their opportunities, pushing them toward government and third-sector jobs. Additionally, policy initiatives during the 1960s and 1970s expanded public-sector employment, which disproportionately benefited African-American women. The intersection of these factors created a reliance that persists despite fluctuations in the broader economy.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-44",
    "question": "Under the regularity conditions specified in the text, which of the following statements are true regarding the asymptotic properties of the estimators?",
    "options": [
      "A) The estimator $\\hat{\\pmb{\\beta}}_{j}$ converges at a rate of $\\mathcal{N}^{-1/2}$ and is asymptotically normal.",
      "B) The estimator $\\widehat{\\lambda}_{k,l}$ has a convergence rate slower than $\\mathcal{N}^{-1/2}$ due to kernel smoothing.",
      "C) The estimator $\\hat{\\Lambda}_{0}(t)$ for the common cumulative baseline hazard function achieves the highest possible convergence rate for a monotonic function estimator.",
      "D) The asymptotic normality of $\\widehat{\\lambda}_{k,l}$ depends on the matrix $\\bar{\\Omega}_{11}^{-2}\\bar{\\nu}_{22}$."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "A_NEW_MULTILEVEL_MODELING_APPROACH_FOR_CLUSTERED_SURVIVAL_DATA.md"
    ],
    "question_context": "The text discusses the asymptotic properties of estimators in a clustered survival analysis framework. Key formulas include the baseline hazard function: \n\n$$ h_{1,j}(t)=h_{0}(t)\\exp\\left(\\sum_{k=1}^{q}\\sum_{l=1}^{c_{k}}\\lambda_{k,l}w_{j k,l}\\right) $$ \n\nand Breslow’s estimator for the cumulative baseline hazard function: \n\n$$ \\hat{\\Lambda}_{1,j}(t_{(\\ell),j})=\\sum_{m=1}^{\\ell}\\left\\{\\sum_{k\\in{\\cal R}_{m,j}}\\exp\\left(X_{k j}^{T}\\hat{\\pmb\\beta}_{j}\\right)\\right\\}^{-1}. $$ \n\nTheorems 1-3 establish that under regularity conditions, the estimators $\\hat{\\pmb{\\beta}}_{j}$, $\\widehat{\\lambda}_{k,l}$, and $\\hat{\\Lambda}_{0}(t)$ are all asymptotically normal with convergence rates of order $\\mathcal{N}^{-1/2}$. Theorem 1 specifically states: \n\n$$ \\mathcal{N}^{1/2}\\left(\\hat{\\pmb{\\beta}}_{j}-\\pmb{\\beta}_{j}\\right)\\overset{D}{\\longrightarrow}N\\left(0_{p},\\Psi_{j}\\mathcal{T}^{-1}\\Psi_{j}^{T}\\right). $$ \n\nTheorem 3 shows the estimator for the common cumulative baseline hazard function achieves the highest possible convergence rate for a monotonic function estimator.",
    "category": "INCORRECT",
    "question_context_used": "The text discusses the asymptotic properties of estimators in a clustered survival analysis framework. Key formulas include the baseline hazard function: \n\n$$ h_{1,j}(t)=h_{0}(t)\\exp\\left(\\sum_{k=1}^{q}\\sum_{l=1}^{c_{k}}\\lambda_{k,l}w_{j k,l}\\right) $$ \n\nand Breslow’s estimator for the cumulative baseline hazard function: \n\n$$ \\hat{\\Lambda}_{1,j}(t_{(\\ell),j})=\\sum_{m=1}^{\\ell}\\left\\{\\sum_{k\\in{\\cal R}_{m,j}}\\exp\\left(X_{k j}^{T}\\hat{\\pmb\\beta}_{j}\\right)\\right\\}^{-1}. $$ \n\nTheorems 1-3 establish that under regularity conditions, the estimators $\\hat{\\pmb{\\beta}}_{j}$, $\\widehat{\\lambda}_{k,l}$, and $\\hat{\\Lambda}_{0}(t)$ are all asymptotically normal with convergence rates of order $\\mathcal{N}^{-1/2}$. Theorem 1 specifically states: \n\n$$ \\mathcal{N}^{1/2}\\left(\\hat{\\pmb{\\beta}}_{j}-\\pmb{\\beta}_{j}\\right)\\overset{D}{\\longrightarrow}N\\left(0_{p},\\Psi_{j}\\mathcal{T}^{-1}\\Psi_{j}^{T}\\right). $$ \n\nTheorem 3 shows the estimator for the common cumulative baseline hazard function achieves the highest possible convergence rate for a monotonic function estimator.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3283",
    "question": "In the context of revealed preference analysis and empirical performance measures, which of the following statements are correct regarding the predictive success measure?",
    "options": [
      "A) Predictive success is defined as the difference between the pass rate and the complement of the power measure.",
      "B) A predictive success value close to -1 indicates a model with high power and high fit.",
      "C) The predictive success measure assigns equal weight to pass rate and power, following an axiomatic characterization.",
      "D) For a model to be considered meaningful, its predictive success rate must be positive."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Nash‐Bargained_Consumption_Decisions_A_Revealed_Preference_Analysis.md"
    ],
    "question_context": "The predictive success measure combines pass rate and power into a single metric, defined as: \n\n$$\\begin{array}{r}{p r e d i c t i v e s u c c e s s=p a s s r a t e-(1-f o w e r).}\\end{array}$$\n\nThis measure ranges between -1 and 1, where a value close to 1 indicates a model with high power and high fit, while a value close to -1 suggests a model void of empirical content. The equal weighting of pass rate and power in this measure is justified by an axiomatic characterization, providing a theoretical foundation for its use. A model is considered meaningful if its predictive success rate is positive, indicating that it can distinguish observed behavior from irrational behavior more effectively than random chance.",
    "category": "INCORRECT",
    "question_context_used": "The predictive success measure combines pass rate and power into a single metric, defined as: \n\n$$\\begin{array}{r}{p r e d i c t i v e s u c c e s s=p a s s r a t e-(1-f o w e r).}\\end{array}$$\n\nThis measure ranges between -1 and 1, where a value close to 1 indicates a model with high power and high fit, while a value close to -1 suggests a model void of empirical content. The equal weighting of pass rate and power in this measure is justified by an axiomatic characterization, providing a theoretical foundation for its use. A model is considered meaningful if its predictive success rate is positive, indicating that it can distinguish observed behavior from irrational behavior more effectively than random chance.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2661",
    "question": "Based on the analysis of spatial agglomeration in China, which of the following statements accurately reflect the implications of under-agglomeration?",
    "options": [
      "A) Doubling the size of TVE employment in a township would yield a 23% increase in output per worker.",
      "B) Moving a prefecture level city from 50% below its peak size to peak size would result in a 4% gain in output per worker.",
      "C) The social marginal product of labor in the rural sector is about a third of that in the urban sector.",
      "D) Free migration would lead to the disappearance of all county cities and townships."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "How-migration-restrictions-limit-agglomeration-an_2006_Journal-of-Developmen.md"
    ],
    "question_context": "The analysis highlights significant under-agglomeration in China, with TVE industry and county cities being undersized and exhibiting unexhausted scale economies. For TVE’s, doubling local scale (e.g., total TVE employment in a township) results in a 23% increase in output per worker, holding input ratios fixed. For county cities, the gain is 11%. For prefecture level cities, moving from a size that is 50% below the peak size (where output per worker peaks) to the peak size yields a 35% gain in output per worker. The social marginal product of labor in the rural sector is about a third of that in the urban sector, indicating a significant rural surplus of labor. Free migration would redefine the rural sector, with some townships becoming cities and others dying out, but not all county cities and townships would disappear.",
    "category": "INCORRECT",
    "question_context_used": "The analysis highlights significant under-agglomeration in China, with TVE industry and county cities being undersized and exhibiting unexhausted scale economies. For TVE’s, doubling local scale (e.g., total TVE employment in a township) results in a 23% increase in output per worker, holding input ratios fixed. For county cities, the gain is 11%. For prefecture level cities, moving from a size that is 50% below the peak size (where output per worker peaks) to the peak size yields a 35% gain in output per worker. The social marginal product of labor in the rural sector is about a third of that in the urban sector, indicating a significant rural surplus of labor. Free migration would redefine the rural sector, with some townships becoming cities and others dying out, but not all county cities and townships would disappear.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1700",
    "question": "According to the submission guidelines for the 2011 ESEM and EEA Congress, which of the following statements are correct?",
    "options": [
      "A) An author can submit the same paper to both ESEM and EEA Congress if they are a member of the Econometric Society.",
      "B) At least one co-author must be a member of the Econometric Society or join at submission time.",
      "C) Paper presenters must register by 1 May, 2011 to confirm their participation.",
      "D) The Program Committee Chairs are responsible for both theoretical and applied economics tracks."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Cho-ANNOUNCEMENTS-2011.md"
    ],
    "question_context": "The 2011 European Meeting of the Econometric Society (ESEM) was held in Oslo, Norway, from 25 to 29 August 2011, jointly organized by the University of Oslo and running parallel with the Congress of the European Economic Association (EEA). The Program Committee Chairs were John van Reenen (London School of Economics) for Econometrics and Empirical Economics, and Ernst-Ludwig von Thadden (University of Mannheim) for Theoretical and Applied Economics. Key submission rules included: each author may submit only one paper to ESEM and one to EEA Congress, with no overlap permitted between submissions. Membership requirement: at least one co-author must be a member of the Econometric Society or join at submission time. Important deadlines: decisions were notified by 15 April 2011, and presenters had to register by 1 May 2011.",
    "category": "INCORRECT",
    "question_context_used": "The 2011 European Meeting of the Econometric Society (ESEM) was held in Oslo, Norway, from 25 to 29 August 2011, jointly organized by the University of Oslo and running parallel with the Congress of the European Economic Association (EEA). The Program Committee Chairs were John van Reenen (London School of Economics) for Econometrics and Empirical Economics, and Ernst-Ludwig von Thadden (University of Mannheim) for Theoretical and Applied Economics. Key submission rules included: each author may submit only one paper to ESEM and one to EEA Congress, with no overlap permitted between submissions. Membership requirement: at least one co-author must be a member of the Econometric Society or join at submission time. Important deadlines: decisions were notified by 15 April 2011, and presenters had to register by 1 May 2011.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4829",
    "question": "In the context of wholesale banks' solvency and liquidity conditions, which of the following statements are correct regarding the cut-off value of idiosyncratic productivity, $\bar{\\omega}_{t}$?",
    "options": [
      "A) $\bar{\\omega}_{t}$ is defined such that the wholesale bank can exactly repay its debt when all retail banks roll over.",
      "B) $\bar{\\omega}_{t}$ is independent of the leverage ratio $\\ell_{t-1}$ of the wholesale bank.",
      "C) $\bar{\\omega}_{t}$ must satisfy $\bar{\\omega}_{t} > \\lambda_{t}$ for the wholesale bank to be vulnerable to a credit run.",
      "D) $\bar{\\omega}_{t}$ is determined endogenously by the equilibrium condition $\bar{\\omega}_{t}R_{K,t}\\ell_{t-1}=R_{L,t}\\left(\\ell_{t-1}-1\\right)$."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Financial_Accelerator_through_Coordination_Failure.md"
    ],
    "question_context": "The solvency condition for a wholesale bank is defined in terms of a cut-off value of idiosyncratic productivity, $\bar{\\omega}_{t}$, such that the wholesale bank has just enough resources to repay the debt when all retail banks roll over. The condition is given by:\n\n$$\n\\begin{array}{r}{\\bar{\\omega}_{t}R_{K,t}\\ell_{t-1}=R_{L,t}\\left(\\ell_{t-1}-1\\right).}\\end{array}\n$$\n\nHowever, if $\bar{\\omega}_{t} > \\lambda_{t}$, the wholesale bank is illiquid and vulnerable to a credit run. This liquidity mismatch arises because the intra-period value of the wholesale bank's capital is $\\lambda_{t}K_{w t-1}$, while the claims on the wholesale bank, if no retail banks roll over, is $\bar{\\omega}_{t}K_{w t-1}$. The cut-off $\bar{\\omega}_{t}$ is endogenously determined and plays a critical role in the coordination problem among retail banks.",
    "category": "INCORRECT",
    "question_context_used": "The solvency condition for a wholesale bank is defined in terms of a cut-off value of idiosyncratic productivity, $\bar{\\omega}_{t}$, such that the wholesale bank has just enough resources to repay the debt when all retail banks roll over. The condition is given by:\n\n$$\n\\begin{array}{r}{\\bar{\\omega}_{t}R_{K,t}\\ell_{t-1}=R_{L,t}\\left(\\ell_{t-1}-1\\right).}\\end{array}\n$$\n\nHowever, if $\bar{\\omega}_{t} > \\lambda_{t}$, the wholesale bank is illiquid and vulnerable to a credit run. This liquidity mismatch arises because the intra-period value of the wholesale bank's capital is $\\lambda_{t}K_{w t-1}$, while the claims on the wholesale bank, if no retail banks roll over, is $\bar{\\omega}_{t}K_{w t-1}$. The cut-off $\bar{\\omega}_{t}$ is endogenously determined and plays a critical role in the coordination problem among retail banks.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3976",
    "question": "Which of the following papers address fundamental econometric challenges in instrumental variables estimation?",
    "options": [
      "A) Bekker and Kleibergen (2003) on finite-sample IV inference",
      "B) Cogley and Sbordone (2008) on trend inflation in NKPC",
      "C) Hansen, Hausman, and Newey (2008) on many IV estimation",
      "D) Stock and Yogo (2003) on weak instrument testing"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "08-Rejoinder.md"
    ],
    "question_context": "The references cover advanced econometric topics, particularly in instrumental variables (IV) estimation. Bekker and Kleibergen (2003) develop pivotal statistics for finite-sample IV inference, addressing small-sample bias. Hansen, Hausman, and Newey (2008) tackle the many-instrument problem where the number of instruments grows with sample size. Stock and Yogo (2003) provide formal tests for weak instruments, which can lead to biased estimates. Key formula for weak instrument testing: \\(F = \\frac{(R_{unrestricted}^2 - R_{restricted}^2)/q}{(1 - R_{unrestricted}^2)/(n - k - 1)}\\), where \\(q\\) is the number of excluded instruments.",
    "category": "CORRECT",
    "question_context_used": "The references cover advanced econometric topics, particularly in instrumental variables (IV) estimation. Bekker and Kleibergen (2003) develop pivotal statistics for finite-sample IV inference, addressing small-sample bias. Hansen, Hausman, and Newey (2008) tackle the many-instrument problem where the number of instruments grows with sample size. Stock and Yogo (2003) provide formal tests for weak instruments, which can lead to biased estimates. Key formula for weak instrument testing: \\(F = \\frac{(R_{unrestricted}^2 - R_{restricted}^2)/q}{(1 - R_{unrestricted}^2)/(n - k - 1)}\\), where \\(q\\) is the number of excluded instruments.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1542",
    "question": "In the presence of microstructure noise, which of the following statements are true regarding the performance of model-based and reduced-form forecasts?",
    "options": [
      "A) The variance of the model-based forecasting error is more sensitive to microstructure effects (order h^(-3/2)) than reduced-form forecasting error (order h^(-1)).",
      "B) TSRV-reduced-form forecasts consistently outperform RV-reduced-form forecasts when microstructure noise is present.",
      "C) The reduced-form forecast based on RV is almost always the best choice under high noise-to-signal ratios.",
      "D) The performance gap between model-based and reduced-form forecasts narrows as the sampling frequency decreases."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Integrated-variance-forecasting--Model-based-vs--re_2011_Journal-of-Economet.md"
    ],
    "question_context": "The observed asset price is modeled as $p_{t}=p_{t}^{*}+n_{t}$, where $p_{t}^{*}$ follows the dynamics (1) and $n_{t}$ represents zero-mean i.i.d. microstructure noise. The effect of microstructure noise on realized variance grows indefinitely as $h_{\\alpha}\\to 0$. The variance of the model-based forecasting error is $h^{-3/2}$—sensitive to microstructure effects, while the sensitivity of the reduced-form forecasting error is of order $h^{-1}$. TSRV, defined as $\\mathrm{TSRV}_{t}^{t+1}=\\frac{\\frac{1}{K}\\sum_{j=1}^{K}\\mathrm{RV}_{t+(j-1)h}^{t+1+(j-1)h}-\\frac{1}{K}\\mathrm{RV}_{t}^{\\mathrm{all},t+1}}{1-\\frac{1}{K}}$, is robust to microstructure noise and outperforms RV in forecasting under noise. Fig. 3 shows that parameter sets for which TSRV outperforms the model are significantly larger than those of RV, confirming TSRV's superior performance in noisy environments.",
    "category": "INCORRECT",
    "question_context_used": "The observed asset price is modeled as $p_{t}=p_{t}^{*}+n_{t}$, where $p_{t}^{*}$ follows the dynamics (1) and $n_{t}$ represents zero-mean i.i.d. microstructure noise. The effect of microstructure noise on realized variance grows indefinitely as $h_{\\alpha}\\to 0$. The variance of the model-based forecasting error is $h^{-3/2}$—sensitive to microstructure effects, while the sensitivity of the reduced-form forecasting error is of order $h^{-1}$. TSRV, defined as $\\mathrm{TSRV}_{t}^{t+1}=\\frac{\\frac{1}{K}\\sum_{j=1}^{K}\\mathrm{RV}_{t+(j-1)h}^{t+1+(j-1)h}-\\frac{1}{K}\\mathrm{RV}_{t}^{\\mathrm{all},t+1}}{1-\\frac{1}{K}}$, is robust to microstructure noise and outperforms RV in forecasting under noise. Fig. 3 shows that parameter sets for which TSRV outperforms the model are significantly larger than those of RV, confirming TSRV's superior performance in noisy environments.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3314",
    "question": "In the context of equilibrium contracts and efficiency in competitive search markets with product returns, which of the following statements are correct according to Proposition 4?",
    "options": [
      "A) The social optimum is achieved when the difference between price and refund equals the difference between production cost and salvage value.",
      "B) Consumers will always inspect goods before purchase at the social optimum.",
      "C) When ψ(ω) > 0, consumers inspect goods after purchasing them at the social optimum.",
      "D) The welfare-maximizing contract must include environmental costs in the calculation of social salvage value."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Consumer Search and Product Returns in ECommerce.md"
    ],
    "question_context": "PROPOSITION 4: The social optimum is achieved by having all firms offer a contract (ˆp, ˆτ) with ˆp − ˆτ = c − η and c ≤ ˆp ≤ max{ψ(ω), 0} + r(s + ϵ). At the social optimum, (i) if ψ(ω) > 0, consumers inspect a good after purchasing it, (ii) if ψ(ω) < 0, consumers inspect a good before purchasing it, and (iii) if ψ(ω) = 0, consumers either inspect a good before or after purchasing it. The welfare condition stated in Proposition 4 only takes the various costs borne by the individual market participants into account. In reality, repeatedly purchasing and returning products might, however, create additional social costs, for example, in the form of pollution due to excessive transportation or the waste from discarding returned items (Tian and Sarkis 2022). These aspects can easily be incorporated by including a social (environmental) cost from each sale es ≥ 0 and each return er ≥ 0. If the social costs of market activity are too large, then efficiency naturally requires the market to be inactive. For less extreme externalities, when returns are efficient, the welfare maximizing contract is computed to be ˆp − ˆτ = c − ηe, where ηe is the social salvage value ηe = η − er − es.",
    "category": "INCORRECT",
    "question_context_used": "PROPOSITION 4: The social optimum is achieved by having all firms offer a contract (ˆp, ˆτ) with ˆp − ˆτ = c − η and c ≤ ˆp ≤ max{ψ(ω), 0} + r(s + ϵ). At the social optimum, (i) if ψ(ω) > 0, consumers inspect a good after purchasing it, (ii) if ψ(ω) < 0, consumers inspect a good before purchasing it, and (iii) if ψ(ω) = 0, consumers either inspect a good before or after purchasing it. The welfare condition stated in Proposition 4 only takes the various costs borne by the individual market participants into account. In reality, repeatedly purchasing and returning products might, however, create additional social costs, for example, in the form of pollution due to excessive transportation or the waste from discarding returned items (Tian and Sarkis 2022). These aspects can easily be incorporated by including a social (environmental) cost from each sale es ≥ 0 and each return er ≥ 0. If the social costs of market activity are too large, then efficiency naturally requires the market to be inactive. For less extreme externalities, when returns are efficient, the welfare maximizing contract is computed to be ˆp − ˆτ = c − ηe, where ηe is the social salvage value ηe = η − er − es.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2787",
    "question": "According to P.T. Bauer's critique in 'Reality and Rhetoric', which of the following are positions held by prominent economists that challenge conventional economic analysis?",
    "options": [
      "A) Myrdal's rejection of conventional economic analysis based on empirical studies",
      "B) Seers' objection to the economics of 'the special case'",
      "C) Galbraith's classification of neoclassical economics as 'conventional wisdom'",
      "D) Balogh's advocacy for exclusively fashionable economic theories"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Laws_of_Human_Relations_and_the_Rules_of_Human_Action_Derived_Therefrom.md"
    ],
    "question_context": "Professor Bauer believes that he sees reality whereas others indulge in rhetoric unrelated to evidence. The role of the individual taking a stand against the massed conventional ranks is clearly attractive: Myrdal has claimed that his empirical studies lead him to reject conventional economic analysis; Seers objects to the application of the economics of 'the special case'; Galbraith lumps neoclassical economics into ‘conventional wisdom'; Balogh's festschrift is entitled ‘Unfashionable Economics’ to distinguish it, one supposes, from the more fashionable variety which would include inter alia many of the views espoused by Professor Bauer. Whatever one thinks of the protagonists it would appear that reports of a consensus are exaggerated.",
    "category": "CORRECT",
    "question_context_used": "Professor Bauer believes that he sees reality whereas others indulge in rhetoric unrelated to evidence. The role of the individual taking a stand against the massed conventional ranks is clearly attractive: Myrdal has claimed that his empirical studies lead him to reject conventional economic analysis; Seers objects to the application of the economics of 'the special case'; Galbraith lumps neoclassical economics into ‘conventional wisdom'; Balogh's festschrift is entitled ‘Unfashionable Economics’ to distinguish it, one supposes, from the more fashionable variety which would include inter alia many of the views espoused by Professor Bauer. Whatever one thinks of the protagonists it would appear that reports of a consensus are exaggerated.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3191",
    "question": "Under what conditions does the 'law of diminishing elasticity of demand' imply a tendency towards stabilization in the context of price-elasticity variation?",
    "options": [
      "A) When price-elasticity decreases with quantity bought and the derivative of elasticity with respect to quantity is less than (e-1)/q",
      "B) When price-elasticity increases with quantity bought and the derivative of elasticity with respect to quantity is greater than (e-1)/q",
      "C) When elasticity at the old equilibrium price is fixed and the derivative of elasticity with respect to quantity is less than (e-1)/q",
      "D) When the variation in price-elasticity along one curve is sufficient to determine stabilization without additional constraints"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Elasticity of Demand and Stabilisation.md"
    ],
    "question_context": "Neither the variation in price-elasticity along one curve, nor the relation between price-elasticity of pre-boom and post-boom curves, at a given price, is sufficient by itself to determine whether the 'law of diminishing elasticity of demand' implies a tendency towards stabilisation. If however we fix the way in which elasticity along a curve can change, we may express the condition for stabilisation by (explicit) reference to the single parameter: elasticity at the old equilibrium price. Finally, we can find the way in which elasticity must vary with consumption in order that the 'law' should tend to produce stabilisation: dg must be less than $\\frac{e-\\mathbf{I}}{q}$ Where $e>\\texttt{I}$ this condition must be fulfilled if price-elasticity does not increase with quantity bought.",
    "category": "INCORRECT",
    "question_context_used": "Neither the variation in price-elasticity along one curve, nor the relation between price-elasticity of pre-boom and post-boom curves, at a given price, is sufficient by itself to determine whether the 'law of diminishing elasticity of demand' implies a tendency towards stabilisation. If however we fix the way in which elasticity along a curve can change, we may express the condition for stabilisation by (explicit) reference to the single parameter: elasticity at the old equilibrium price. Finally, we can find the way in which elasticity must vary with consumption in order that the 'law' should tend to produce stabilisation: dg must be less than $\\frac{e-\\mathbf{I}}{q}$ Where $e>\\texttt{I}$ this condition must be fulfilled if price-elasticity does not increase with quantity bought.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3955",
    "question": "Which of the following are key advantages of using book-based indicators over traditional patent-based measures for identifying technology shocks?",
    "options": [
      "A) They are directly linked to the commercialization date of new technologies",
      "B) They eliminate the long and variable lags between invention and market impact",
      "C) They are unaffected by changes in patent laws or administrative resources",
      "D) They provide a comprehensive measure of all R&D expenditures"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Alexopoulos-ReadItHappens-2011.md"
    ],
    "question_context": "Existing indicators of technical change are plagued by shortcomings. I present new measures based on books published in the field of technology that resolve many of these problems and use them to identify the impact of technology shocks on economic activity. They are positively linked to changes in R&D and scientific knowledge, and capture the new technologies' commercialization dates. Changes in information technology are found to be important sources of economic fluctuations in the post-WWII period, and total factor productivity, investment, and, to a lesser extent, labor are all shown to increase following a positive technology shock. Key formula: The lag between changes in technology captured by book measures and economic activity is much shorter than those associated with traditional indicators like patents or R&D data.",
    "category": "CORRECT",
    "question_context_used": "Existing indicators of technical change are plagued by shortcomings. I present new measures based on books published in the field of technology that resolve many of these problems and use them to identify the impact of technology shocks on economic activity. They are positively linked to changes in R&D and scientific knowledge, and capture the new technologies' commercialization dates. Changes in information technology are found to be important sources of economic fluctuations in the post-WWII period, and total factor productivity, investment, and, to a lesser extent, labor are all shown to increase following a positive technology shock. Key formula: The lag between changes in technology captured by book measures and economic activity is much shorter than those associated with traditional indicators like patents or R&D data.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4593",
    "question": "Based on the industry-level analysis of volatility contributions, which of the following statements are empirically supported by the data?",
    "options": [
      "A) The motor vehicle industry's decline in volatility contributed substantially to aggregate output stability.",
      "B) Services sector showed increased hours volatility despite declining output and productivity volatility.",
      "C) Durable manufacturing industries accounted for the majority of the aggregate volatility decline.",
      "D) The correlation between hours and labor productivity growth became more negative post-1983."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Stiroh-VolatilityAccountingProduction-2009.md"
    ],
    "question_context": "The analysis reveals a broad-based decline in volatility across industries, with 29 out of 35 industries showing more stable contributions to aggregate output value-added growth. Key formula for within-industry correlation: \n\n$$\\bar{\\rho}_{W,UN}=\\sum_{i}\\mathrm{Corr}(\\dot{H}_{i},A\\dot{L}P_{i})\\cdot1/I$$\n\nBetween-industry correlation formula:\n\n$$\\bar{\\rho}_{B,UN}=\\sum_{i}\\sum_{j=i+1}\\mathrm{Corr}(\\dot{H}_{i},A\\dot{L}P_{j})/(I\\cdot(I-1))$$\n\nThe motor vehicle industry, while showing significant declines in volatility, contributed only -0.06 to aggregate volatility reduction compared to -0.96 across all industries. Panel regressions demonstrate that the relationship between hours and labor productivity growth became significantly more negative after 1984, with the interaction coefficient δ in the regression equation being negative and highly significant.",
    "category": "INCORRECT",
    "question_context_used": "The analysis reveals a broad-based decline in volatility across industries, with 29 out of 35 industries showing more stable contributions to aggregate output value-added growth. Key formula for within-industry correlation: \n\n$$\\bar{\\rho}_{W,UN}=\\sum_{i}\\mathrm{Corr}(\\dot{H}_{i},A\\dot{L}P_{i})\\cdot1/I$$\n\nBetween-industry correlation formula:\n\n$$\\bar{\\rho}_{B,UN}=\\sum_{i}\\sum_{j=i+1}\\mathrm{Corr}(\\dot{H}_{i},A\\dot{L}P_{j})/(I\\cdot(I-1))$$\n\nThe motor vehicle industry, while showing significant declines in volatility, contributed only -0.06 to aggregate volatility reduction compared to -0.96 across all industries. Panel regressions demonstrate that the relationship between hours and labor productivity growth became significantly more negative after 1984, with the interaction coefficient δ in the regression equation being negative and highly significant.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2879",
    "question": "In the context of aggregate land rents (ALR) and aggregate transport costs (ATC) in a circular city with linear transport costs, which of the following statements are correct based on the general equilibrium analysis presented?",
    "options": [
      "A) ALR is always equal to ATC regardless of the city's geometry",
      "B) For a linear city with linear transport costs, ALR equals ATC",
      "C) Differential land rents (DLR) are exactly half of ATC when transport costs are linear",
      "D) The benefits from a transport improvement can be measured by the change in ALR only under partial equilibrium conditions"
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Aggregate_Land_Rents_and_Aggregate_Transport_Costs.md"
    ],
    "question_context": "The paper presents a general characterization of the relationship between aggregate land rents (ALR) and aggregate transport costs (ATC). For a circular city with linear transport costs, aggregate transport costs are precisely twice aggregate land rents, independent of the distribution of tastes or income. For a linear city with linear transport costs, aggregate transport costs are equal to aggregate land rents. The relationship is derived from the spatial equilibrium condition where land rent differentials reflect transport cost savings. Key formulas include:\n\n$$ R'(t) = -f'(t) $$\n\n$$ ALR \\equiv \\int_{0}^{t^{*}} R(t) \\: 2\\pi t dt $$\n\n$$ ATC \\equiv \\int_{0}^{t^{*}} f(t) \\: 2\\pi t dt $$\n\nUnder linear transport costs, differential land rents satisfy:\n\n$$ DLR = \\frac{1}{2}ATC $$",
    "category": "INCORRECT",
    "question_context_used": "The paper presents a general characterization of the relationship between aggregate land rents (ALR) and aggregate transport costs (ATC). For a circular city with linear transport costs, aggregate transport costs are precisely twice aggregate land rents, independent of the distribution of tastes or income. For a linear city with linear transport costs, aggregate transport costs are equal to aggregate land rents. The relationship is derived from the spatial equilibrium condition where land rent differentials reflect transport cost savings. Key formulas include:\n\n$$ R'(t) = -f'(t) $$\n\n$$ ALR \\equiv \\int_{0}^{t^{*}} R(t) \\: 2\\pi t dt $$\n\n$$ ATC \\equiv \\int_{0}^{t^{*}} f(t) \\: 2\\pi t dt $$\n\nUnder linear transport costs, differential land rents satisfy:\n\n$$ DLR = \\frac{1}{2}ATC $$",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4673",
    "question": "In the context of the model's identification strategy, which of the following statements are correct regarding the identification of delay cost distribution parameters $(\\beta_{\\alpha}, \\delta_{\\alpha})$?",
    "options": [
      "A) They are identified by the variation in total claims across all individuals, regardless of their proximity to the maximum benefit.",
      "B) They are identified by the division of claims between the two years among individuals with claims close to the maximum benefit in the first year.",
      "C) They require the assumption that plan enrollment is unrelated to delay costs for application to the entire sample.",
      "D) They can be estimated without integrating over the distribution of latent parameters $\\lambda_i$ and $\\alpha_i$."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Claim Timing and Ex Post Adverse Selection.md"
    ],
    "question_context": "The delay cost distribution parameters, $(\\beta_{\\alpha}, \\delta_{\\alpha})$, are identified by the division of claims between the 2 years among individuals with claims close to the maximum benefit in the first year (who likely had an incentive to delay claims). Intuitively, only individuals with the incentive to delay claims give us any information about the delay costs. In this environment, individuals have an incentive to delay claims if and only if they receive year 1 events that put them beyond the maximum benefit. Because events are not observed directly, the model, in combination with the claims data, must be used to infer which individuals may have received events that would have put them at the plan maximum benefit in the first year.\n\nIn the counterfactual analysis, the estimated parameters governing the delay cost distribution are applied to the entire sample. Though the delay parameters are identified by these incentivized individuals, the estimated parameters are relevant in the entire sample under two maintained assumptions: (1) conditional independence between risk types and delay costs and (2) plan enrolment is unrelated to delay costs. The likelihood function integrates over the distribution of latent parameters, $F(\\lambda, \\alpha|\\Theta)$, because $\\lambda_i$ and $\\alpha_i$ are known to the individual but not observed by the researcher.",
    "category": "CORRECT",
    "question_context_used": "The delay cost distribution parameters, $(\\beta_{\\alpha}, \\delta_{\\alpha})$, are identified by the division of claims between the 2 years among individuals with claims close to the maximum benefit in the first year (who likely had an incentive to delay claims). Intuitively, only individuals with the incentive to delay claims give us any information about the delay costs. In this environment, individuals have an incentive to delay claims if and only if they receive year 1 events that put them beyond the maximum benefit. Because events are not observed directly, the model, in combination with the claims data, must be used to infer which individuals may have received events that would have put them at the plan maximum benefit in the first year.\n\nIn the counterfactual analysis, the estimated parameters governing the delay cost distribution are applied to the entire sample. Though the delay parameters are identified by these incentivized individuals, the estimated parameters are relevant in the entire sample under two maintained assumptions: (1) conditional independence between risk types and delay costs and (2) plan enrolment is unrelated to delay costs. The likelihood function integrates over the distribution of latent parameters, $F(\\lambda, \\alpha|\\Theta)$, because $\\lambda_i$ and $\\alpha_i$ are known to the individual but not observed by the researcher.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2916",
    "question": "Which of the following statements are empirically supported by the analysis of major currency realignments in the 1970s on LDC terms of trade?",
    "options": [
      "A) Exchange rate realignments had a first-order magnitude impact on LDC terms of trade",
      "B) The Bretton Woods system collapse necessitated explicit consideration of exchange rate impacts on terms of trade",
      "C) Trade theorists developed pegging rules solely to maximize LDC export revenues",
      "D) The study compared alternative weighting procedures for effective exchange rate calculations"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Major-currency-realignments-and-the-terms-of-trad_1982_Journal-of-Developmen.md"
    ],
    "question_context": "The terms of trade are a matter of particular concern to developing countries. The belief that developing countries’ terms of trade are subject to secular decline and more volatile cyclical fluctuations than those of industrialized countries was an important motivation of the import-substitution industrialization strategy. The collapse of the Bretton Woods system has made it necessary to consider the potential impact of exchange rate realignments on the terms of trade. Trade theorists have worked to devise exchange rate pegging rules to minimize the impact of exchange rate realignments on the terms of trade, among other objective functions [e.g., Branson and Katseli-Papaefstratiou (1980)]. The evidence suggests that exchange rate realignments in the 1970s had an influence on LDC terms of trade, but one of second-order magnitude. The issue of what the appropriate weighting procedure is in calculating effective exchange rates is discussed and alternative measures are compared.",
    "category": "CORRECT",
    "question_context_used": "The terms of trade are a matter of particular concern to developing countries. The belief that developing countries’ terms of trade are subject to secular decline and more volatile cyclical fluctuations than those of industrialized countries was an important motivation of the import-substitution industrialization strategy. The collapse of the Bretton Woods system has made it necessary to consider the potential impact of exchange rate realignments on the terms of trade. Trade theorists have worked to devise exchange rate pegging rules to minimize the impact of exchange rate realignments on the terms of trade, among other objective functions [e.g., Branson and Katseli-Papaefstratiou (1980)]. The evidence suggests that exchange rate realignments in the 1970s had an influence on LDC terms of trade, but one of second-order magnitude. The issue of what the appropriate weighting procedure is in calculating effective exchange rates is discussed and alternative measures are compared.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2328",
    "question": "In the context of McKenzie's irreducibility condition and decentralized economies, which of the following statements are correct?",
    "options": [
      "A) The condition ensures that if one trader has more than minimal income, all traders have more than minimal income.",
      "B) The condition is necessary for the existence of equilibrium in all economic systems.",
      "C) The condition implies that any group of traders can improve the condition of the rest without hurting themselves if their resources are doubled.",
      "D) The condition guarantees continuity of the demand function at points where traders have more than minimal income."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "A Note on Arrow-Hahn's Resource-Relatedness (Or Mckenzie's Irreducibility).md"
    ],
    "question_context": "McKenzie's irreducibility condition states that if the traders in the economy are divided into two groups, doubling the initial resources of one group allows them to improve the condition of the other group without hurting themselves. This condition ensures that if one trader has more than minimal income, then all traders have more than minimal income, which in turn ensures the continuity of the demand function at that point, and thus the existence of equilibrium. The condition is the weakest one known in McKenzie's or Arrow-Hahn's versions, but it is not necessary in general, as demonstrated by the Edgeworth-Box example where consumers have the utility $u(x,y)=\\operatorname*{min}\\left[x,y\\right]$.\n\nThe 'informational' message of this result is that for an economy to function in a decentralized manner, there must be sufficient knowledge so that any two economic units can arrange for one to supply something the other wants. This is a minimal condition on information, highlighting the importance of interconnectedness and information sharing within the economic system.",
    "category": "INCORRECT",
    "question_context_used": "McKenzie's irreducibility condition states that if the traders in the economy are divided into two groups, doubling the initial resources of one group allows them to improve the condition of the other group without hurting themselves. This condition ensures that if one trader has more than minimal income, then all traders have more than minimal income, which in turn ensures the continuity of the demand function at that point, and thus the existence of equilibrium. The condition is the weakest one known in McKenzie's or Arrow-Hahn's versions, but it is not necessary in general, as demonstrated by the Edgeworth-Box example where consumers have the utility $u(x,y)=\\operatorname*{min}\\left[x,y\\right]$.\n\nThe 'informational' message of this result is that for an economy to function in a decentralized manner, there must be sufficient knowledge so that any two economic units can arrange for one to supply something the other wants. This is a minimal condition on information, highlighting the importance of interconnectedness and information sharing within the economic system.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4028",
    "question": "Which of the following statements accurately describe the impact of endogenous protection on the likelihood of immiserizing growth according to Bhagwati and Johnson?",
    "options": [
      "A) Endogenous protection reduces the likelihood of immiserizing growth in Bhagwati's sense by weakening foreign exchange rationing.",
      "B) Endogenous protection increases the likelihood of immiserizing growth in Johnson's sense by raising the level of protection due to factor augmentation.",
      "C) Endogenous protection has no effect on the likelihood of immiserizing growth in either Bhagwati's or Johnson's sense.",
      "D) Endogenous protection increases the likelihood of immiserizing growth in Bhagwati's sense by exacerbating terms-of-trade deterioration."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Immiserizing-growth-and-endogenous-protec_1994_Journal-of-Development-Econom.md"
    ],
    "question_context": "In the case of immiserizing growth in the sense of Bhagwati, the intuition is that as growth occurs and the terms of trade deteriorates, the demand for imports will fall, reducing excess demand for foreign exchange. This weakens the severity of foreign exchange rationing which also lowers the ad valorem equivalent protection implied by it. The likelihood of immiserization occurring in the presence of endogenous protection would, therefore, seem to be lowered. \n\nJohnson immiserization occurs where a tariff is used to protect a sector, and unbalanced growth is concentrated on the factor in which the protected sector is intensive. The effect of growth is to pull more resources into the protected sector, and immiserization occurs if the induced additional costs of protection negate the benefits from augmentation of the factor. If the level of protection is instead endogenously determined, production shifts to $P^{\\prime\\prime}$ , since due to factor augmentation concentrated on the factor-intensive in the protected sector the level of endogenously determined protection will rise. Real income is now indicated by the world price line through $P^{\\prime\\prime}$ , and thus the extent of immiserization is worse compared to the case of tariff protection. The likelihood of immiserization in the sense of Johnson occurring would seemingly have been increased by the presence of endogenous protection.",
    "category": "CORRECT",
    "question_context_used": "In the case of immiserizing growth in the sense of Bhagwati, the intuition is that as growth occurs and the terms of trade deteriorates, the demand for imports will fall, reducing excess demand for foreign exchange. This weakens the severity of foreign exchange rationing which also lowers the ad valorem equivalent protection implied by it. The likelihood of immiserization occurring in the presence of endogenous protection would, therefore, seem to be lowered. \n\nJohnson immiserization occurs where a tariff is used to protect a sector, and unbalanced growth is concentrated on the factor in which the protected sector is intensive. The effect of growth is to pull more resources into the protected sector, and immiserization occurs if the induced additional costs of protection negate the benefits from augmentation of the factor. If the level of protection is instead endogenously determined, production shifts to $P^{\\prime\\prime}$ , since due to factor augmentation concentrated on the factor-intensive in the protected sector the level of endogenously determined protection will rise. Real income is now indicated by the world price line through $P^{\\prime\\prime}$ , and thus the extent of immiserization is worse compared to the case of tariff protection. The likelihood of immiserization in the sense of Johnson occurring would seemingly have been increased by the presence of endogenous protection.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1322",
    "question": "In the context of dynamic panel data models with lagged dependent variables, which of the following statements are true regarding the Least-Squares Dummy Variable (LSDV) estimator?",
    "options": [
      "A) The LSDV estimator is inconsistent when N→∞ and T is finite, but its bias can be accurately approximated with an error of O(N⁻¹T⁻³⸍²)",
      "B) The LSDV estimator has a smaller variance compared to consistent GMM estimators, making bias correction a promising approach",
      "C) The LSDV estimator's inconsistency arises solely from the presence of exogenous explanatory variables in the model",
      "D) The approximation error for the LSDV bias can be reduced to o(N⁻¹T⁻¹) through further analytical derivations"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "On-bias--inconsistency--and-efficiency-of-various-estima_1995_Journal-of-Eco.md"
    ],
    "question_context": "When a model for panel data includes lagged dependent explanatory variables, the LSDV estimator is asymptotically valid only when T→∞. However, in practice T is often small while N is large. The paper derives a formula for the bias of the LSDV estimator with an O(N⁻¹T⁻³⸍²) approximation error, which proves remarkably accurate in simulations. Key formula: The bias approximation depends on conditioning variables and unknown true parameters, but yields corrected estimators consistent for finite T and N→∞.\n\nThe LSDV estimator, though biased, has relatively small dispersion compared to consistent estimators like Anderson-Hsiao IV or GMM. This property makes bias correction attractive since the corrected estimator (LSDVc) maintains low variance while reducing bias. The approximation error could potentially be improved to o(N⁻¹T⁻¹) through further analytical work, requiring less stringent conditions on N for consistency.",
    "category": "CORRECT",
    "question_context_used": "When a model for panel data includes lagged dependent explanatory variables, the LSDV estimator is asymptotically valid only when T→∞. However, in practice T is often small while N is large. The paper derives a formula for the bias of the LSDV estimator with an O(N⁻¹T⁻³⸍²) approximation error, which proves remarkably accurate in simulations. Key formula: The bias approximation depends on conditioning variables and unknown true parameters, but yields corrected estimators consistent for finite T and N→∞.\n\nThe LSDV estimator, though biased, has relatively small dispersion compared to consistent estimators like Anderson-Hsiao IV or GMM. This property makes bias correction attractive since the corrected estimator (LSDVc) maintains low variance while reducing bias. The approximation error could potentially be improved to o(N⁻¹T⁻¹) through further analytical work, requiring less stringent conditions on N for consistency.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-921",
    "question": "In the context of simulating lifetime earnings distributions for graduates, which of the following methodological choices are critical for ensuring the realism and accuracy of the simulations?",
    "options": [
      "A) Using copulas to model the dependence in wages between ages under a first-order Markov assumption",
      "B) Incorporating a stochastic component to employment and assigning zero earnings during unemployment periods",
      "C) Assuming a fixed 2% real earnings growth per year without variation across individuals",
      "D) Relying solely on cross-sectional earnings data without accounting for individual earnings dynamics"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Higher_Education_Funding_Reforms_in_England_The_Distributional_Effects_and_the_Shifting_Balance_of_Costs.md"
    ],
    "question_context": "The lifetime earnings distributions are constructed using functions known as copulas, the name being a reference to the fact that they couple together $k$ univariate distributions to form a k-variate distribution. In our case, the construction of k-variate distributions is simplified by assuming that wages follow a first-order Markov process, meaning that only the most recent draw of wages affects the distribution of the next draw. This allows us to model the dynamics of earnings over the life cycle from just two wage observations for the same individual. Another important feature of the earnings paths is that in simulating them, we allow for a stochastic component to employment and assign earnings accordingly. In particular, if an individual is unemployed in a particular period, (s)he is assigned zero earnings; if (s)he becomes employed, his/her earnings are allowed to depend on the length of time unemployed and his/her wage when last employed.",
    "category": "CORRECT",
    "question_context_used": "The lifetime earnings distributions are constructed using functions known as copulas, the name being a reference to the fact that they couple together $k$ univariate distributions to form a k-variate distribution. In our case, the construction of k-variate distributions is simplified by assuming that wages follow a first-order Markov process, meaning that only the most recent draw of wages affects the distribution of the next draw. This allows us to model the dynamics of earnings over the life cycle from just two wage observations for the same individual. Another important feature of the earnings paths is that in simulating them, we allow for a stochastic component to employment and assign earnings accordingly. In particular, if an individual is unemployed in a particular period, (s)he is assigned zero earnings; if (s)he becomes employed, his/her earnings are allowed to depend on the length of time unemployed and his/her wage when last employed.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-302",
    "question": "In the context of the analytical approximations for optimal decision-making under uncertainty, which of the following statements are correct regarding the derivation of the approximation formula?",
    "options": [
      "A) The subtracted equation involves hyperbolic sine functions and their derivatives.",
      "B) The ratio R(z) is derived using Taylor series expansions of the numerator and denominator functions.",
      "C) The dimensions of the lump-sum costs k and l are [\\$]/[T].",
      "D) The leading term in the equation obtained by adding the two value matching conditions simplifies to M = C."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Analytical Approximations in Models of Hysteresis.md"
    ],
    "question_context": "The appendix contains technical details of the derivation of the approximation formula for the entry-exit model. Solve the smooth pasting conditions (32) and (33) of the text for the constants $A$ and $\\pmb{B}$ and substitute the result in the value matching conditions (30) and (31) to obtain two equations in the control parameters $R_{\\prime\\prime}$ and $\\scriptstyle R_{L}$. Next, we add and subtract the equations to get two others that are better suited for the approximation. Finally, define \n\n$$\nM=(R_{H}R_{L})^{1/2},\\qquad z={\\textstyle\\frac{1}{2}}\\ln{\\textstyle(R_{H}/R_{L})},\n$$\n\nor\n\n$$\nR_{H}=M e^{-},\\qquad R_{L}=M e^{-_{-}}.\n$$\n\nThe subtracted equation becomes\n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{(\\rho-\\mu)(k+l)}{M}=(e^{z}-e^{-z})}\\ {\\displaystyle-\\frac{\\alpha(e^{B z}-e^{-\\beta z})(e^{(\\alpha+1)z}-e^{-(\\alpha+1)z})+\\beta(e^{\\alpha z}-e^{-\\alpha z})(e^{(\\beta-1)z}-e^{-(\\beta-1)z})}{\\alpha\\beta(e^{(\\alpha+\\beta)z}-e^{-(\\alpha+\\beta)z})}.}\\end{array}\n$$\n\nThe approximations require appropriate expansions of the right-hand side. First consider the case when $k$ and $t$ are small, we proceed on the assumption that $z$ is sufficiently small that the exponentials can be expanded in a Taylor series and the leading terms retained. To facilitate this, we define the numerator and the denominator functions\n\n$$\n\\begin{array}{r l}&{N(z)=\\alpha(e^{\\beta z}-e^{-\\beta z})(e^{(\\alpha+1)z}-e^{-(\\alpha+1)z})+\\beta(e^{\\alpha z}-e^{-\\alpha z})(e^{(\\beta-1)z}\\quad e^{-(\\beta-1)z})}\\ &{\\qquad=4\\alpha\\sinh{(\\beta z)}\\sinh{\\left((\\alpha+1)z\\right)}+4\\beta\\sinh{\\left(\\alpha z\\right)}\\sinh{\\left((\\beta-1)z\\right)},}\\end{array}\n$$\n\nand\n\n$$\n\\begin{array}{c}{{D(z)=\\alpha\\beta(e^{(\\alpha+\\beta)z}-e^{-(\\alpha+\\beta)z}).}}\\ {{{}}}\\\\ {{=2\\alpha\\beta\\sinh{((\\alpha+\\beta)z)}}}\\end{array}\n$$",
    "category": "INCORRECT",
    "question_context_used": "The appendix contains technical details of the derivation of the approximation formula for the entry-exit model. Solve the smooth pasting conditions (32) and (33) of the text for the constants $A$ and $\\pmb{B}$ and substitute the result in the value matching conditions (30) and (31) to obtain two equations in the control parameters $R_{\\prime\\prime}$ and $\\scriptstyle R_{L}$. Next, we add and subtract the equations to get two others that are better suited for the approximation. Finally, define \n\n$$\nM=(R_{H}R_{L})^{1/2},\\qquad z={\\textstyle\\frac{1}{2}}\\ln{\\textstyle(R_{H}/R_{L})},\n$$\n\nor\n\n$$\nR_{H}=M e^{-},\\qquad R_{L}=M e^{-_{-}}.\n$$\n\nThe subtracted equation becomes\n\n$$\n\\begin{array}{l}{\\displaystyle\\frac{(\\rho-\\mu)(k+l)}{M}=(e^{z}-e^{-z})}\\ {\\displaystyle-\\frac{\\alpha(e^{B z}-e^{-\\beta z})(e^{(\\alpha+1)z}-e^{-(\\alpha+1)z})+\\beta(e^{\\alpha z}-e^{-\\alpha z})(e^{(\\beta-1)z}-e^{-(\\beta-1)z})}{\\alpha\\beta(e^{(\\alpha+\\beta)z}-e^{-(\\alpha+\\beta)z})}.}\\end{array}\n$$\n\nThe approximations require appropriate expansions of the right-hand side. First consider the case when $k$ and $t$ are small, we proceed on the assumption that $z$ is sufficiently small that the exponentials can be expanded in a Taylor series and the leading terms retained. To facilitate this, we define the numerator and the denominator functions\n\n$$\n\\begin{array}{r l}&{N(z)=\\alpha(e^{\\beta z}-e^{-\\beta z})(e^{(\\alpha+1)z}-e^{-(\\alpha+1)z})+\\beta(e^{\\alpha z}-e^{-\\alpha z})(e^{(\\beta-1)z}\\quad e^{-(\\beta-1)z})}\\ &{\\qquad=4\\alpha\\sinh{(\\beta z)}\\sinh{\\left((\\alpha+1)z\\right)}+4\\beta\\sinh{\\left(\\alpha z\\right)}\\sinh{\\left((\\beta-1)z\\right)},}\\end{array}\n$$\n\nand\n\n$$\n\\begin{array}{c}{{D(z)=\\alpha\\beta(e^{(\\alpha+\\beta)z}-e^{-(\\alpha+\\beta)z}).}}\\ {{{}}}\\\\ {{=2\\alpha\\beta\\sinh{((\\alpha+\\beta)z)}}}\\end{array}\n$$",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4142",
    "question": "Under the assumptions (H), (C), (W), (B), and (S) for a vector of excess demand functions, which of the following statements are true regarding the properties of $F_{s}(P_{s},0)$?",
    "options": [
      "A) $F_{s}(P_{s},0)$ satisfies (H), (C), (B), and (S) as functions of $P_{s}$.",
      "B) $F_{s}(P_{s},0)$ necessarily violates (W) if $F(P)$ is unbounded from above.",
      "C) If $\\overline{P}_{s}$ is an equilibrium for $F_{s}(P_{s},0)$, then $(\\overline{P}_{s},0)$ is an equilibrium for $F(P)$.",
      "D) $F_{s}(P_{s},0)$ is independent of $P_{\\tilde{s}}$ by Lemma 1."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Competitive Stability under Weak Gross Substitutability The Euclidean Distance Approach.md"
    ],
    "question_context": "LEMMA 3 states: If assumptions (H), (C), (W), (B), and (S) hold for a vector $F(P)$ of excess demand functions, and if, for some set $s$ of indices, the inequality $F_{\\tilde{s}}(P)\\le0$ holds for all $P$, then the vector of excess demand functions $F_{s}(P_{s},0)$ also satisfies (H), (C), (W), (B), and (S) as functions of $P_{s}$; further, if $\\overline{P}_{s}$ is any equilibrium point for $F_{s}(P_{s},0)$, then $(\\overline{P}_{s},0)$ is an equilibrium point for $F(P)$. The proof involves showing that under these conditions, the properties (H), (C), (B), and (S) are preserved for $F_{s}(P_{s},0)$, and (W) holds even if $F(P)$ is unbounded from above, as demonstrated by the limit $P_{s}\\cdot F_{s}(P_{s},0)=0$.",
    "category": "INCORRECT",
    "question_context_used": "LEMMA 3 states: If assumptions (H), (C), (W), (B), and (S) hold for a vector $F(P)$ of excess demand functions, and if, for some set $s$ of indices, the inequality $F_{\\tilde{s}}(P)\\le0$ holds for all $P$, then the vector of excess demand functions $F_{s}(P_{s},0)$ also satisfies (H), (C), (W), (B), and (S) as functions of $P_{s}$; further, if $\\overline{P}_{s}$ is any equilibrium point for $F_{s}(P_{s},0)$, then $(\\overline{P}_{s},0)$ is an equilibrium point for $F(P)$. The proof involves showing that under these conditions, the properties (H), (C), (B), and (S) are preserved for $F_{s}(P_{s},0)$, and (W) holds even if $F(P)$ is unbounded from above, as demonstrated by the limit $P_{s}\\cdot F_{s}(P_{s},0)=0$.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3787",
    "question": "In the context of nonlinear inequality constraints (NIC) hypothesis testing, which of the following statements are true regarding the least favorable parameter value?",
    "options": [
      "A) The least favorable value always satisfies all inequality constraints with equality.",
      "B) The asymptotic distribution of the test statistic depends on the local geometry of the constraint set relative to the parameter value.",
      "C) For NIC tests with p > 2, the set of possible least favorable values expands beyond T to B.",
      "D) The least favorable value can be uniquely determined without considering the functional dependence of the asymptotic covariance matrix on the parameter."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Wolak-LocalNatureHypothesis-1991.md"
    ],
    "question_context": "Key formula: The likelihood ratio (LR) statistic is defined as a minimum chi-squared-like distance between the estimated parameter and the set under the null hypothesis: $$\\mathrm{LR}=\\operatorname*{inf}_{m}\\left[({\\hat{\\mu}}-m)^{\\prime}\\varOmega^{-1}({\\hat{\\mu}}-m){\\mathrm{subject~to~}}m\\geqslant0\\right].$$ The least favorable value of the parameter under the null hypothesis is crucial for determining the exact size of the test. Lemma 1 in the text specifies that the set B must contain the least favorable value, but it does not yield a unique solution. The asymptotic distribution of the test statistic is influenced by the local geometry of the constraint set and the functional dependence of the asymptotic covariance matrix on the parameter, leading to multiple potential null asymptotic distributions.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The likelihood ratio (LR) statistic is defined as a minimum chi-squared-like distance between the estimated parameter and the set under the null hypothesis: $$\\mathrm{LR}=\\operatorname*{inf}_{m}\\left[({\\hat{\\mu}}-m)^{\\prime}\\varOmega^{-1}({\\hat{\\mu}}-m){\\mathrm{subject~to~}}m\\geqslant0\\right].$$ The least favorable value of the parameter under the null hypothesis is crucial for determining the exact size of the test. Lemma 1 in the text specifies that the set B must contain the least favorable value, but it does not yield a unique solution. The asymptotic distribution of the test statistic is influenced by the local geometry of the constraint set and the functional dependence of the asymptotic covariance matrix on the parameter, leading to multiple potential null asymptotic distributions.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3878",
    "question": "According to the critique of Ram's model, which of the following statements are valid regarding the measurement of government output and productivity?",
    "options": [
      "A) Government output is inherently immeasurable due to the absence of market valuations.",
      "B) National income accounting uses cost-based data, which fails to capture true government efficiency.",
      "C) The government production function exhibits constant returns to scale by definition, not by equilibrium conditions.",
      "D) Ram's original results on relative factor productivity are invalid due to the a priori definition of factor productivities."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Carr-GovernmentSizeEconomic-1989.md"
    ],
    "question_context": "Ram models production both in a private and government sector and allows for an externality effect of government production on private production. Ram defines 8 as the measure of the relative marginal factor productivities in the government and private sectors. The estimate of 8 depends crucially on the valuation of government output. Most government goods do not pass through an organized market which explicitly values these goods. With government goods there is a serious imputation problem. National income accountants have followed the quick and dirty 'solution' of evaluating government goods at their cost of production. The national income accounting cost-based data cannot possibly be used to measure government efficiency in producing goods and services. Equation (2) in the Ram model (which is the production function of the government sector) should be replaced by $$\\frac{P_{L}\\cdot L_{g}+P_{K}\\cdot K_{g}}{P_{G}}=G\\big(L_{g},K_{g}\\big),$$ where $P_{L}$ and $P_{K}$ are the prices of the factor inputs and $P_{G}$ is a price index for government output. By definition, the government production function exhibits constant returns to scale, and the marginal productivities are constant everywhere and equal to real factor prices by definition, not determined by the data.",
    "category": "INCORRECT",
    "question_context_used": "Ram models production both in a private and government sector and allows for an externality effect of government production on private production. Ram defines 8 as the measure of the relative marginal factor productivities in the government and private sectors. The estimate of 8 depends crucially on the valuation of government output. Most government goods do not pass through an organized market which explicitly values these goods. With government goods there is a serious imputation problem. National income accountants have followed the quick and dirty 'solution' of evaluating government goods at their cost of production. The national income accounting cost-based data cannot possibly be used to measure government efficiency in producing goods and services. Equation (2) in the Ram model (which is the production function of the government sector) should be replaced by $$\\frac{P_{L}\\cdot L_{g}+P_{K}\\cdot K_{g}}{P_{G}}=G\\big(L_{g},K_{g}\\big),$$ where $P_{L}$ and $P_{K}$ are the prices of the factor inputs and $P_{G}$ is a price index for government output. By definition, the government production function exhibits constant returns to scale, and the marginal productivities are constant everywhere and equal to real factor prices by definition, not determined by the data.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2385",
    "question": "According to the self-selection model of immigration discussed by Jasso and Rosenzweig, which of the following factors would lead to an increase in the expected quality of immigrants (E(Q)) when immigrants are positively selected (γ > 0)?",
    "options": [
      "A) An increase in migration costs (c)",
      "B) A decrease in mean origin-country income (μ)",
      "C) A rise in the emigration rate (P)",
      "D) A reduction in the standard normal density (φ(z))"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Jasso-SelfSelectionEarningsImmigrants-1990.md"
    ],
    "question_context": "The expected quality of immigrants in the United States is given by the formula: \n\n$$E(Q)=\\mu+\\gamma\\lambda,$$\n\nwhere $\\mu=$ earnings of the U.S. native born and $\\lambda=\\phi(z)/P$, where $\\pmb{P}$ is the emigration rate of the origin country and $\\phi(z)$ is the density of the standard normal. Immigrants are positively selected if $\\gamma>0$ and negatively selected if $\\gamma<0$. The effect of migration costs on $E(Q)$ is thus:\n\n$$\\frac{\\partial E(Q)}{\\partial c}=\\gamma\\frac{\\partial\\lambda}{\\partial z}\\frac{\\partial z}{\\partial c}.$$\n\nSince $\\partial\\lambda/\\partial z>0$ and $\\partial z/\\partial c>0$ (as migration costs increase fewer persons migrate), increases in migration costs raise (lower) immigrant quality when immigrants are positively (negatively) selected. Similarly, when mean incomes (earnings opportunities) rise in the origin country, emigration declines and the quality of immigrants rises or falls depending on whether selectivity is positive or negative. Whatever the type of selectivity, mean origin-country income and migration costs should affect the quality of immigrants in the same way.",
    "category": "CORRECT",
    "question_context_used": "The expected quality of immigrants in the United States is given by the formula: \n\n$$E(Q)=\\mu+\\gamma\\lambda,$$\n\nwhere $\\mu=$ earnings of the U.S. native born and $\\lambda=\\phi(z)/P$, where $\\pmb{P}$ is the emigration rate of the origin country and $\\phi(z)$ is the density of the standard normal. Immigrants are positively selected if $\\gamma>0$ and negatively selected if $\\gamma<0$. The effect of migration costs on $E(Q)$ is thus:\n\n$$\\frac{\\partial E(Q)}{\\partial c}=\\gamma\\frac{\\partial\\lambda}{\\partial z}\\frac{\\partial z}{\\partial c}.$$\n\nSince $\\partial\\lambda/\\partial z>0$ and $\\partial z/\\partial c>0$ (as migration costs increase fewer persons migrate), increases in migration costs raise (lower) immigrant quality when immigrants are positively (negatively) selected. Similarly, when mean incomes (earnings opportunities) rise in the origin country, emigration declines and the quality of immigrants rises or falls depending on whether selectivity is positive or negative. Whatever the type of selectivity, mean origin-country income and migration costs should affect the quality of immigrants in the same way.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2148",
    "question": "Under what conditions does robustness to incomplete information become equivalent to robustness to canonical elaborations in game theory?",
    "options": [
      "A) When the equilibrium concept in nearby incomplete information games is Nash equilibrium",
      "B) When the solution concept used in the elaborations is agent normal form correlated equilibrium",
      "C) When the game has a unique potential maximizer in a potential game",
      "D) When players' higher-order beliefs are common knowledge"
    ],
    "gold_answer": "B, C",
    "source_files": [
      "On-the-equivalence-of-robustness-to-canonical-and-ge_2019_Journal-of-Economi.md"
    ],
    "question_context": "Robustness to incomplete information and robustness to canonical elaborations are central concepts in game theory. A target equilibrium in a game of complete information is called robust to incomplete information when all nearby games of incomplete information have equilibria that generate similar ex-ante distributions over actions to the distribution generated by the target equilibrium. Robustness to canonical elaborations considers only nearby games with a special structure where each player-type either has payoffs given by the payoffs in the original game, or has a strictly dominant action. The key result shows that robustness to incomplete information and robustness to canonical elaborations are equivalent when the equilibrium concept in the nearby incomplete information games is agent normal form correlated equilibrium. This equivalence is particularly useful in potential games, where a unique potential maximizer is robust to canonical elaborations, implying robustness more generally under correlated equilibrium.",
    "category": "CORRECT",
    "question_context_used": "Robustness to incomplete information and robustness to canonical elaborations are central concepts in game theory. A target equilibrium in a game of complete information is called robust to incomplete information when all nearby games of incomplete information have equilibria that generate similar ex-ante distributions over actions to the distribution generated by the target equilibrium. Robustness to canonical elaborations considers only nearby games with a special structure where each player-type either has payoffs given by the payoffs in the original game, or has a strictly dominant action. The key result shows that robustness to incomplete information and robustness to canonical elaborations are equivalent when the equilibrium concept in the nearby incomplete information games is agent normal form correlated equilibrium. This equivalence is particularly useful in potential games, where a unique potential maximizer is robust to canonical elaborations, implying robustness more generally under correlated equilibrium.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-565",
    "question": "In the context of the constrained-efficient allocation problem for an EM government, which of the following statements are correct regarding the policy implementation?",
    "options": [
      "A) Macroprudential liquidity regulation alone can achieve the constrained-efficient allocation without capital controls.",
      "B) The government's balance sheet operations involving debt issuance and reserves accumulation can substitute for capital controls entirely.",
      "C) The optimal policy mix requires both reserves management and capital controls to internalize the social cost of reserves.",
      "D) The private sector's external borrowing does not require any reserves accumulation in equilibrium under the government's balance sheet operations."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Jeanne-MacroprudentialRoleInternational-2016.md"
    ],
    "question_context": "The constrained-efficient allocation maximizes (1) subject to (2), (3), and (4), with the solution satisfying the first-order condition: \n\n$$ u^{\\prime}(c_{1})=1+\\frac{r}{1-s}, $$ \n\nand the liquidity constraint $m\\geq s b$ is binding. The social marginal cost of external borrowing exceeds $r$ due to the need for non-interest-bearing reserves. The government can implement this allocation through macroprudential regulation or balance sheet operations. However, macroprudential policy's reach is limited, and balance sheet operations involve issuing debt $b^{g}$ to accumulate reserves $m^{g}=b^{g}$. The optimal policy mix combines reserves management with capital controls to internalize the social cost of reserves, as private sector borrowing $b$ does not require reserves accumulation in equilibrium: \n\n$$ m^{g}={\\frac{s}{1-s}}b. $$",
    "category": "INCORRECT",
    "question_context_used": "The constrained-efficient allocation maximizes (1) subject to (2), (3), and (4), with the solution satisfying the first-order condition: \n\n$$ u^{\\prime}(c_{1})=1+\\frac{r}{1-s}, $$ \n\nand the liquidity constraint $m\\geq s b$ is binding. The social marginal cost of external borrowing exceeds $r$ due to the need for non-interest-bearing reserves. The government can implement this allocation through macroprudential regulation or balance sheet operations. However, macroprudential policy's reach is limited, and balance sheet operations involve issuing debt $b^{g}$ to accumulate reserves $m^{g}=b^{g}$. The optimal policy mix combines reserves management with capital controls to internalize the social cost of reserves, as private sector borrowing $b$ does not require reserves accumulation in equilibrium: \n\n$$ m^{g}={\\frac{s}{1-s}}b. $$",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\"]}",
      "model_answer": [
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3545",
    "question": "Which of the following statements are true regarding the rank tests for nonlinear cointegration as described in the text?",
    "options": [
      "A) The rank transformation preserves the properties of the original series under monotonic transformations.",
      "B) The statistic ξT converges to 0 under the alternative hypothesis of cointegration.",
      "C) The power of the rank test is unaffected by the parameter β in the cointegration relationship.",
      "D) The rank tests are robust against additive outliers compared to residual Dickey-Fuller tests."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "06-Rank_Tests_for_Nonlinear_Cointegration.md"
    ],
    "question_context": "The rank transformation is applied to overcome difficulties in detecting nonlinear cointegration. Under the alternative hypothesis, a nonlinear cointegration relationship exists, given by u_t = g(y_t) - f(x_t), where f(x_t) and g(y_t) are I(1) and u_t is I(0). The ranked series R_T(x_t) and R_T(y_t) are constructed, and the test statistics κT and ξT are defined to measure the distance between these ranked series. The null hypothesis of no cointegration is rejected if these statistics are sufficiently small. The text also discusses the robustness of rank tests against outliers and their power properties, noting that the power depends on the signal-to-noise ratio β²σ̄x²/σu².",
    "category": "INCORRECT",
    "question_context_used": "The rank transformation is applied to overcome difficulties in detecting nonlinear cointegration. Under the alternative hypothesis, a nonlinear cointegration relationship exists, given by u_t = g(y_t) - f(x_t), where f(x_t) and g(y_t) are I(1) and u_t is I(0). The ranked series R_T(x_t) and R_T(y_t) are constructed, and the test statistics κT and ξT are defined to measure the distance between these ranked series. The null hypothesis of no cointegration is rejected if these statistics are sufficiently small. The text also discusses the robustness of rank tests against outliers and their power properties, noting that the power depends on the signal-to-noise ratio β²σ̄x²/σu².",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3319",
    "question": "In the context of cross-sectional multipliers and the American Recovery and Reinvestment Act (ARRA) analysis, which of the following statements are correct regarding the relationship between employment and output multipliers?",
    "options": [
      "A) The employment multiplier can be converted to an output multiplier by dividing output per worker by the cost per job, ensuring temporal alignment.",
      "B) The production function used assumes full capital adjustment in the short run to derive the relationship between employment and output multipliers.",
      "C) The combined multiplicative factor (1−ξ)(1+χ) is approximately 1 for the United States, where ξ is the capital share and χ is the elasticity of hours per worker to total employment.",
      "D) The output multiplier estimated directly from gross state product (GSP) data is statistically indistinguishable from the implied output multiplier derived from employment data."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "ChodorowReich-GeographicCrossSectionalFiscal-2019.md"
    ],
    "question_context": "The text discusses the relationship between employment and output multipliers, particularly in the context of the ARRA. Key formulas include the production function $Y_{t}=A(N_{t}E_{t})^{1−ξ}$ and the derived relationship $\\beta_{h}^{Y}≈(1−ξ)(1+χ)\\frac{Y_{t}}{E_{t}}\\beta_{h}^{E}$, where $ξ≈1/3$ and $χ≈0.5$ for the U.S., making $(1−ξ)(1+χ)≈1$. The ARRA analysis shows that the implied output multiplier from employment data (≈2) is close to the directly estimated output multiplier (1.53), with no statistical difference. The conversion from employment to output multipliers requires dividing output per worker by the cost per job, ensuring the same time frame for $Y/E$ and $\\beta^{E}$.",
    "category": "CORRECT",
    "question_context_used": "The text discusses the relationship between employment and output multipliers, particularly in the context of the ARRA. Key formulas include the production function $Y_{t}=A(N_{t}E_{t})^{1−ξ}$ and the derived relationship $\\beta_{h}^{Y}≈(1−ξ)(1+χ)\\frac{Y_{t}}{E_{t}}\\beta_{h}^{E}$, where $ξ≈1/3$ and $χ≈0.5$ for the U.S., making $(1−ξ)(1+χ)≈1$. The ARRA analysis shows that the implied output multiplier from employment data (≈2) is close to the directly estimated output multiplier (1.53), with no statistical difference. The conversion from employment to output multipliers requires dividing output per worker by the cost per job, ensuring the same time frame for $Y/E$ and $\\beta^{E}$.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2149",
    "question": "Which of the following statements are true regarding agent normal form correlated equilibrium (ANFCE) in games of incomplete information?",
    "options": [
      "A) ANFCE requires that no player can benefit from unilateral deviation given the equilibrium distribution over strategy profiles.",
      "B) ANFCE is equivalent to Bayes Nash equilibrium in the Selten game representation.",
      "C) The set of ANFCE is convex due to the linearity of the obedience constraints.",
      "D) ANFCE can be characterized by a distribution over strategy profiles that satisfies obedience constraints for all types of all players."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "On-the-equivalence-of-robustness-to-canonical-and-ge_2019_Journal-of-Economi.md"
    ],
    "question_context": "Definition 4. An agent normal form correlated equilibrium (ANFCE) is a Borel probability measure, $\\lambda$, over the set of strategy profiles, $\\tau$, such that for all $i\\in I$, $s\\in S^{*}$ and all pairs $(a,a^{\\prime})\\in A\\times A$:\n\n$$\n\\begin{array}{c}{{\\displaystyle\\sum_{{\\tilde{s}}:{\\tilde{s}}_{i}=s_{i}}\\displaystyle\\sum_{{\\tilde{a}}:{\\tilde{a}}_{i}=a_{i}}\\displaystyle\\sum_{{\\tilde{\\tau}}:{\\tilde{\\tau}}({\\tilde{s}})={\\tilde{a}}}P[{\\tilde{s}}]\\lambda({\\tilde{\\tau}})u_{i}({\\tilde{s}},{\\tilde{a}})\\geq}}\\ {{\\displaystyle\\sum_{{\\tilde{s}}:{\\tilde{s}}_{i}=s_{i}}\\displaystyle\\sum_{{\\tilde{a}}:{\\tilde{a}}_{i}=a_{i}}\\displaystyle\\sum_{{\\tilde{\\tau}}:{\\tilde{\\tau}}({\\tilde{s}})={\\tilde{a}}}P[{\\tilde{s}}]\\lambda({\\tilde{\\tau}})u_{i}({\\tilde{s}},a_{i}^{\\prime},{\\tilde{a}}_{-i}).}}\\end{array}\n$$\n\nThis definition identifies an ANFCE with a distribution over strategy profiles in the Selten game, and requires that no player is better off playing $a_{i}^{\\prime}$ when the equilibrium calls for $a_{i}$. We will call a distribution, $\\lambda$, obedient for type $s_{i}$ of player $i$ if the inequality in Definition 4 holds for all pairs $a_{i},a_{i}^{\\prime}\\in A_{i}\\times A_{i}$, and simply obedient if it is obedient for all types of all players.",
    "category": "INCORRECT",
    "question_context_used": "Definition 4. An agent normal form correlated equilibrium (ANFCE) is a Borel probability measure, $\\lambda$, over the set of strategy profiles, $\\tau$, such that for all $i\\in I$, $s\\in S^{*}$ and all pairs $(a,a^{\\prime})\\in A\\times A$:\n\n$$\n\\begin{array}{c}{{\\displaystyle\\sum_{{\\tilde{s}}:{\\tilde{s}}_{i}=s_{i}}\\displaystyle\\sum_{{\\tilde{a}}:{\\tilde{a}}_{i}=a_{i}}\\displaystyle\\sum_{{\\tilde{\\tau}}:{\\tilde{\\tau}}({\\tilde{s}})={\\tilde{a}}}P[{\\tilde{s}}]\\lambda({\\tilde{\\tau}})u_{i}({\\tilde{s}},{\\tilde{a}})\\geq}}\\ {{\\displaystyle\\sum_{{\\tilde{s}}:{\\tilde{s}}_{i}=s_{i}}\\displaystyle\\sum_{{\\tilde{a}}:{\\tilde{a}}_{i}=a_{i}}\\displaystyle\\sum_{{\\tilde{\\tau}}:{\\tilde{\\tau}}({\\tilde{s}})={\\tilde{a}}}P[{\\tilde{s}}]\\lambda({\\tilde{\\tau}})u_{i}({\\tilde{s}},a_{i}^{\\prime},{\\tilde{a}}_{-i}).}}\\end{array}\n$$\n\nThis definition identifies an ANFCE with a distribution over strategy profiles in the Selten game, and requires that no player is better off playing $a_{i}^{\\prime}$ when the equilibrium calls for $a_{i}$. We will call a distribution, $\\lambda$, obedient for type $s_{i}$ of player $i$ if the inequality in Definition 4 holds for all pairs $a_{i},a_{i}^{\\prime}\\in A_{i}\\times A_{i}$, and simply obedient if it is obedient for all types of all players.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3125",
    "question": "In the context of North-South capital flows, which of the following factors are empirically supported as key ingredients for explaining the observed patterns of capital flows to developing countries?",
    "options": [
      "A) Diminishing returns at the country level and country-specific production risk",
      "B) Superior technology and human capital in rich countries alone",
      "C) Sovereign risk and inefficient renegotiation of claims",
      "D) Equalization of capital stocks per person across countries"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Kraay-CountryPortfolios-2005.md"
    ],
    "question_context": "Capital flows to developing countries are positive, small, and mostly take the form of lending instead of foreign investment. The bold line in Figure 1 shows the combined net foreign asset position of 47 non-OECD countries as a share of their combined wealth from 1970 to 1997. Throughout this period these countries have been net recipients of capital. Their net capital imports average about $10\\%$ of their wealth, fluctuating moderately between $8\\%$ and $12\\%$. It seems safe to argue that a theory of North-South capital flows requires at least two ingredients to be able to explain these empirical observations. The first one would be one or more incentives for capital to flow from rich to poor countries. Natural candidates for this role are diminishing returns at the country level and/or country-specific production risk. A popular view is that the theory just needs to recognize that rich countries have better technologies and human capital, and this is why investors keep most of their capital in rich countries even in the presence of diminishing returns and production risk. In this paper we explore the alternative hypothesis that sovereign risk might be the second ingredient that the theory needs.",
    "category": "CORRECT",
    "question_context_used": "Capital flows to developing countries are positive, small, and mostly take the form of lending instead of foreign investment. The bold line in Figure 1 shows the combined net foreign asset position of 47 non-OECD countries as a share of their combined wealth from 1970 to 1997. Throughout this period these countries have been net recipients of capital. Their net capital imports average about $10\\%$ of their wealth, fluctuating moderately between $8\\%$ and $12\\%$. It seems safe to argue that a theory of North-South capital flows requires at least two ingredients to be able to explain these empirical observations. The first one would be one or more incentives for capital to flow from rich to poor countries. Natural candidates for this role are diminishing returns at the country level and/or country-specific production risk. A popular view is that the theory just needs to recognize that rich countries have better technologies and human capital, and this is why investors keep most of their capital in rich countries even in the presence of diminishing returns and production risk. In this paper we explore the alternative hypothesis that sovereign risk might be the second ingredient that the theory needs.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2302",
    "question": "In the context of OLS estimation where a microvariable is explained by aggregates and contemporaneous disturbances are equicorrelated, which of the following statements are correct?",
    "options": [
      "A) The OLS estimator remains unbiased under equicorrelated disturbances.",
      "B) Aggregation of microvariables leads to heteroskedasticity in the error terms.",
      "C) The presence of equicorrelated disturbances violates the Gauss-Markov assumptions.",
      "D) Generalized Least Squares (GLS) is more efficient than OLS in this scenario."
    ],
    "gold_answer": "C, D",
    "source_files": [
      "Kloek-OLSEstimationModel-1981.md"
    ],
    "question_context": "The paper examines OLS estimation in a model where a microvariable is explained by aggregates and contemporaneous disturbances are equicorrelated. Key issues arise due to the violation of classical assumptions, particularly when disturbances exhibit equicorrelation. The Gauss-Markov theorem no longer holds, necessitating alternative estimation techniques like GLS. The aggregation of microvariables introduces additional complications in error structure, impacting estimator efficiency.",
    "category": "INCORRECT",
    "question_context_used": "The paper examines OLS estimation in a model where a microvariable is explained by aggregates and contemporaneous disturbances are equicorrelated. Key issues arise due to the violation of classical assumptions, particularly when disturbances exhibit equicorrelation. The Gauss-Markov theorem no longer holds, necessitating alternative estimation techniques like GLS. The aggregation of microvariables introduces additional complications in error structure, impacting estimator efficiency.",
    "gold_answer_parsed": [
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1387",
    "question": "Based on the econometric analysis of primacy and productivity growth, which of the following statements are supported by the GMM results in Table 2, column (4)?",
    "options": [
      "A) Best primacy declines linearly with output per worker.",
      "B) A one-standard deviation increase in primacy leads to a 0.12 decrease in productivity growth over five years.",
      "C) The optimal level of primacy is invariant to the level of development.",
      "D) OLS and GMM estimations yield identical best primacy values at all output per worker levels."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Henderson-UrbanizationProcessEconomic-2003.md"
    ],
    "question_context": "The econometric analysis examines the relationship between urban primacy and productivity growth, employing a quadratic form for primacy and its interaction with output per worker. Key findings from the GMM results (Table 2, column (4)) indicate that best primacy declines linearly with output per worker. Specifically, at low output per worker ($1,100), best primacy is 0.48, decreasing to 0.32 at middle output ($8,100) and 0.21 at high output ($36,000). Deviations from optimal primacy are costly: a one-standard deviation (0.15) increase in primacy results in a 0.12 decrease in productivity growth over five years. The formula for the quadratic specification is $[\\alpha_{0}+\\alpha_{2}\\ln(Y/N)]\\mathrm{primacy} + \\beta_{0} \\mathrm{primacy}^2$, with expected signs $\\alpha_{0}>0$, $\\beta_{0}<0$, and $\\alpha_{2}<0$ but $\\alpha_{0}+\\alpha_{2}\\ln(Y/N)>0$ for relevant $Y/N$.",
    "category": "CORRECT",
    "question_context_used": "The econometric analysis examines the relationship between urban primacy and productivity growth, employing a quadratic form for primacy and its interaction with output per worker. Key findings from the GMM results (Table 2, column (4)) indicate that best primacy declines linearly with output per worker. Specifically, at low output per worker ($1,100), best primacy is 0.48, decreasing to 0.32 at middle output ($8,100) and 0.21 at high output ($36,000). Deviations from optimal primacy are costly: a one-standard deviation (0.15) increase in primacy results in a 0.12 decrease in productivity growth over five years. The formula for the quadratic specification is $[\\alpha_{0}+\\alpha_{2}\\ln(Y/N)]\\mathrm{primacy} + \\beta_{0} \\mathrm{primacy}^2$, with expected signs $\\alpha_{0}>0$, $\\beta_{0}<0$, and $\\alpha_{2}<0$ but $\\alpha_{0}+\\alpha_{2}\\ln(Y/N)>0$ for relevant $Y/N$.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2936",
    "question": "In the context of dynamic task allocation with endogenous training, which of the following statements are supported by the model's findings?",
    "options": [
      "A) Welfare gains from centralization are greater when tasks arrive more rapidly and training technologies improve.",
      "B) Monitoring seniors' backlog of clients always increases welfare but may decrease training.",
      "C) Perfect monitoring leads to higher welfare but can result in less training when the training technology is inefficient.",
      "D) In discretionary settings, clients always prefer senior service regardless of queue length."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Task-allocation-and-on-the-job-training_2023_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The study examines dynamic task allocation where providers' expertise evolves through training, comparing optimal assignment protocols to discretionary procedures. Key findings include that welfare gains from centralization are greater with rapid task arrival and improved training technologies. Monitoring seniors' backlog increases welfare but may reduce training. The model introduces a trade-off between immediate service quality and future expertise, utilizing queueing theory adaptations for environments with endogenous types. Key formula: $\\mathbf{E}[W_{P}]<\\mathbf{E}[W_{L}]$ under perfect monitoring, indicating lower expected wait times. The analysis also shows that perfect monitoring can lead to higher welfare but potentially less training, depending on the training technology's efficacy.",
    "category": "CORRECT",
    "question_context_used": "The study examines dynamic task allocation where providers' expertise evolves through training, comparing optimal assignment protocols to discretionary procedures. Key findings include that welfare gains from centralization are greater with rapid task arrival and improved training technologies. Monitoring seniors' backlog increases welfare but may reduce training. The model introduces a trade-off between immediate service quality and future expertise, utilizing queueing theory adaptations for environments with endogenous types. Key formula: $\\mathbf{E}[W_{P}]<\\mathbf{E}[W_{L}]$ under perfect monitoring, indicating lower expected wait times. The analysis also shows that perfect monitoring can lead to higher welfare but potentially less training, depending on the training technology's efficacy.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2592",
    "question": "Under Assumptions 1–7, which of the following statements are true regarding the asymptotic behavior of the terms in Lemma 2?",
    "options": [
      "A) The term (i) in Lemma 2 is of order O_p(√φ_N).",
      "B) The term (iv) in Lemma 2 is of order O_p(N/T).",
      "C) The term (vi) in Lemma 2 is of order O_p(√(N/T)).",
      "D) The term (ii) in Lemma 2 is of order O_p(√(φ_N/T))."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Estimation-of-heterogeneous-panels-with-structural_2016_Journal-of-Econometr.md"
    ],
    "question_context": "Lemma 2 under Assumptions 1–7 states several asymptotic results uniformly on K(C_k). Key results include: (i) ∑_{i=1}^N δ_i' Z_{2i}' ε_i = O_p(√φ_N); (ii) (1/√T) ∑_{i=1}^N δ_i' Z_{2i}' X_i (Z_i' M_i / T)^{-1} (X_i' ε_i / √T) = O_p(√(φ_N / T)); (iv) (1/T) ∑_{i=1}^N ε_i' M_i Z_{2i} (Z_{2i}' M_{2i} Z_{2i} / T)^{-1} Z_{2i}' M_i ε_i = O_p(N/T); (vi) ∑_{i=1}^N (ε_i' M_i Z_{2i} / √T) [(Z_{2i}' M_i Z_{2i} / T)^{-1} - (Z_{2i}' M_{2i} Z_{2i} / T)^{-1}] (Z_{2i}' M_i ε_i / √T) = O_p(N/T). The proof leverages Assumptions 3–7 to derive these orders, with detailed steps provided in the Supplementary Appendix.",
    "category": "CORRECT",
    "question_context_used": "Lemma 2 under Assumptions 1–7 states several asymptotic results uniformly on K(C_k). Key results include: (i) ∑_{i=1}^N δ_i' Z_{2i}' ε_i = O_p(√φ_N); (ii) (1/√T) ∑_{i=1}^N δ_i' Z_{2i}' X_i (Z_i' M_i / T)^{-1} (X_i' ε_i / √T) = O_p(√(φ_N / T)); (iv) (1/T) ∑_{i=1}^N ε_i' M_i Z_{2i} (Z_{2i}' M_{2i} Z_{2i} / T)^{-1} Z_{2i}' M_i ε_i = O_p(N/T); (vi) ∑_{i=1}^N (ε_i' M_i Z_{2i} / √T) [(Z_{2i}' M_i Z_{2i} / T)^{-1} - (Z_{2i}' M_{2i} Z_{2i} / T)^{-1}] (Z_{2i}' M_i ε_i / √T) = O_p(N/T). The proof leverages Assumptions 3–7 to derive these orders, with detailed steps provided in the Supplementary Appendix.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2652",
    "question": "In the context of the Reporting Game, which of the following conditions would theoretically lead to no complaints and no differences in providers' behaviors across treatments?",
    "options": [
      "A) If individuals are purely money-maximizers and filing a complaint is costly",
      "B) If the non-monetary benefit of complaining outweighs the cost C(F)",
      "C) If the possibility of receiving punishment significantly affects individual decision-making",
      "D) If the cost of filing a complaint is zero (i.e., C(F) = 0)"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Mbiti和Serra - 2022 - Health workers' behavior, patient reporting and reputational concerns Lab-in-the-field experimental.md"
    ],
    "question_context": "Theoretically, since filing a complaint is costly (i.e., $\\mathrm{{C}}(\\mathrm{{F}}){=}\\mathrm{{c}}{>}0$ ), if individuals are purely money-maximizers we should expect to see no complaints and no differences in providers’ behaviors across treatments in our reporting game. In other words, in all treatments we should observe $\\mathrm{F}{=}0$ , ${\\mathrm{X}}{=}0$ , and ${\\mathrm{Y}=0}$ , and all reporting systems should prove ineffective. However, a large number of experimental studies (e.g., Calabuig et al., 2016; Fehr & Fischbacher, 2004; Fehr & Gächter, 2002; Masclet et al., 2003; Xiao & Houser, 2005) have shown that individuals are willing to incur monetary costs to impose monetary or non-monetary penalties to others and that the possibility of receiving punishment, either formal or informal, significantly affects individual decision-making. It is easy to augment health providers’ payoff by including a non-monetary cost, I(F), generated by the receipt of negative feedback by patients, in the form of disapproval cards. In the Monetary Penalty treatment, ${\\bf K}({\\bf P})$ would then become equal to $\\mathrm{kF}{+}\\mathrm{I}(\\mathrm{F})$ , with $\\mathrm{I}(\\mathrm{F})\\geq0$ and increasing in F. In the other treatments, $\\mathbf{K}(\\mathrm{F})$ would be simply equal to I(F).",
    "category": "INCORRECT",
    "question_context_used": "Theoretically, since filing a complaint is costly (i.e., $\\mathrm{{C}}(\\mathrm{{F}}){=}\\mathrm{{c}}{>}0$ ), if individuals are purely money-maximizers we should expect to see no complaints and no differences in providers’ behaviors across treatments in our reporting game. In other words, in all treatments we should observe $\\mathrm{F}{=}0$ , ${\\mathrm{X}}{=}0$ , and ${\\mathrm{Y}=0}$ , and all reporting systems should prove ineffective. However, a large number of experimental studies (e.g., Calabuig et al., 2016; Fehr & Fischbacher, 2004; Fehr & Gächter, 2002; Masclet et al., 2003; Xiao & Houser, 2005) have shown that individuals are willing to incur monetary costs to impose monetary or non-monetary penalties to others and that the possibility of receiving punishment, either formal or informal, significantly affects individual decision-making. It is easy to augment health providers’ payoff by including a non-monetary cost, I(F), generated by the receipt of negative feedback by patients, in the form of disapproval cards. In the Monetary Penalty treatment, ${\\bf K}({\\bf P})$ would then become equal to $\\mathrm{kF}{+}\\mathrm{I}(\\mathrm{F})$ , with $\\mathrm{I}(\\mathrm{F})\\geq0$ and increasing in F. In the other treatments, $\\mathbf{K}(\\mathrm{F})$ would be simply equal to I(F).",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1748",
    "question": "Based on the empirical results of the hospital cost function analysis, which of the following statements are correct regarding the inclusion of physician costs in the average cost per case?",
    "options": [
      "A) The quadratic specification for BEDS provides a better fit when physician costs are excluded.",
      "B) The linear specification for BEDS is superior when physician costs are included.",
      "C) The F-test confirms that both linear and squared BEDS terms jointly contribute when physician costs are included.",
      "D) The inclusion of physician costs implies that average cost decreases with size."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Bays-SpecificationErrorEstimation-1980.md"
    ],
    "question_context": "The empirical analysis examines the average cost per case in hospitals, considering both the exclusion and inclusion of physician costs. The cost function is specified as $$A C=f(B E D S,C F,W)$$ where $A C$ is the average cost per case, $B E D S$ is the number of beds, $C F$ is the case flow, and $W$ is a vector of case-mix proportions. The study finds that the quadratic specification for BEDS gives a slightly better fit when physician costs are excluded (equations 1 and 2), but the linear specification is superior when physician costs are included (equations 3 and 4). An F-test confirms that both linear and squared BEDS terms jointly contribute when physician costs are included. The results imply that average cost increases with size when physician costs are included, contrary to the case when they are excluded.",
    "category": "INCORRECT",
    "question_context_used": "The empirical analysis examines the average cost per case in hospitals, considering both the exclusion and inclusion of physician costs. The cost function is specified as $$A C=f(B E D S,C F,W)$$ where $A C$ is the average cost per case, $B E D S$ is the number of beds, $C F$ is the case flow, and $W$ is a vector of case-mix proportions. The study finds that the quadratic specification for BEDS gives a slightly better fit when physician costs are excluded (equations 1 and 2), but the linear specification is superior when physician costs are included (equations 3 and 4). An F-test confirms that both linear and squared BEDS terms jointly contribute when physician costs are included. The results imply that average cost increases with size when physician costs are included, contrary to the case when they are excluded.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4642",
    "question": "Which of the following individuals were chairs or key organizers of the Econometric Society meetings in 1998 and 1999?",
    "options": [
      "A) Anjan Mukherji, Jawaharlal Nehru University",
      "B) Randall Wright, University of Pennsylvania",
      "C) David Levine, University of California, Los Angeles",
      "D) Latifah Wan Mohamad, Universiti Malaysia Sarawak"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Announcements-1998.md"
    ],
    "question_context": "The 1998 India and South East Asia Meeting of the Econometric Society was held at Universiti Malaysia Sarawak, with Anjan Mukherji as Chairperson of the Program Committee and Dr. Latifah Wan Mohamad chairing the Local Arrangements Committee. The 1999 Joint Meeting with the American Economic Association in New York was chaired by Randall Wright of the University of Pennsylvania, who also coordinated inquiries for the event. These meetings featured prominent econometricians and economists across various subfields, with detailed organizational structures documented in the announcements.",
    "category": "CORRECT",
    "question_context_used": "The 1998 India and South East Asia Meeting of the Econometric Society was held at Universiti Malaysia Sarawak, with Anjan Mukherji as Chairperson of the Program Committee and Dr. Latifah Wan Mohamad chairing the Local Arrangements Committee. The 1999 Joint Meeting with the American Economic Association in New York was chaired by Randall Wright of the University of Pennsylvania, who also coordinated inquiries for the event. These meetings featured prominent econometricians and economists across various subfields, with detailed organizational structures documented in the announcements.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1824",
    "question": "According to Mishkin's analysis of market rationality, which of the following conditions are sufficient for a market to display rational expectations?",
    "options": [
      "A) All market participants must individually exhibit rational expectations.",
      "B) Survey forecasts must accurately reflect the behavior of market participants.",
      "C) Unexploited profit opportunities are eliminated by some participants in the market.",
      "D) The behavior of the market must differ from the behavior of the average individual."
    ],
    "gold_answer": "C, D",
    "source_files": [
      "Mishkin-MarketForecastsRational-1981.md"
    ],
    "question_context": "The behavior of a market is not necessarily the same as the behavior of the average individual. As long as unexploited profit opportunities are eliminated by some participants in a market (this is analogous to an arbitrage condition), then the market will behave as though expectations are rational despite irrational participants in that market. Therefore, survey forecasts do not necessarily describe the forecasts inherent in market behavior, and irrationality of survey forecasts does not in itself imply that market forecasts are also irrational. Not all market participants have to be rational in order for a market to display rational expectations. The key insight is that market-level rationality emerges from the elimination of arbitrage opportunities by some participants, even if others behave irrationally.",
    "category": "INCORRECT",
    "question_context_used": "The behavior of a market is not necessarily the same as the behavior of the average individual. As long as unexploited profit opportunities are eliminated by some participants in a market (this is analogous to an arbitrage condition), then the market will behave as though expectations are rational despite irrational participants in that market. Therefore, survey forecasts do not necessarily describe the forecasts inherent in market behavior, and irrationality of survey forecasts does not in itself imply that market forecasts are also irrational. Not all market participants have to be rational in order for a market to display rational expectations. The key insight is that market-level rationality emerges from the elimination of arbitrage opportunities by some participants, even if others behave irrationally.",
    "gold_answer_parsed": [
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-576",
    "question": "Based on the Monte Carlo studies presented in Tables 1 and 2, which of the following statements accurately describe the relationship between the Type 1 error probabilities and the estimators (OLS vs. 2SLS)?",
    "options": [
      "A) OLS-based tests consistently exhibit Type 1 error probabilities above the nominal 5% level when there is no misspecification.",
      "B) 2SLS-based tests show Type 1 error probabilities below the nominal 5% level under misspecification, except when L=0.",
      "C) The Type 1 error probabilities for OLS-based tests decrease towards the nominal 5% level under misspecification.",
      "D) For L>2, 2SLS-based tests are more satisfactory in terms of Type 1 error probabilities compared to OLS-based tests."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "Testing-for-serial-correlation-in-simultaneous-equation_1981_Journal-of-Econ.md"
    ],
    "question_context": "The results for Tests 2 and 3 are based upon the modified von Neumann ratio, (3.2), which is tabulated by Koerts and Abrahamse (1969). The first column of the results shows the situation when there is no misspecification and all structural disturbances are free of serial correlation. In this case the comparison between OLS and 2SLS based tests is similar for all three tests: the Type I error probabilities are above the nominal $5\\%$ level for OLS and below that level for 2SLS. There is a tendency for the probabilities to increase with $L$ , and for $L=6$ the increase over the $5\\%$ level for OLS is substantial being close to $11\\%$ for Test 1 : $(\\beta$ approximation). For 2SLS the increase with $L$ istowards the $5\\%$ level. Increasing  the contemporaneous correlation between the structural disturbances, α, from 0.4 to 0.8 had little effect and so results are shown for $\\mathfrak{x}=0.4$ only. The remaining results in table 1 show what happens when there is serial correlation elsewhere in the system. The effect of this misspecification is, surprisingly, to bring the Type 1 error probability closer to the nominal 5% level for OLS based tests while reducing the probabilities even further below $5\\%$ for  2SLS.  The  case  of $L=0$ is  an  exception  in  that  the  Type 1 probabilities become extremely high. The overall situation, then, is that when $L$ is low the OLS based tests tend to have a Type 1 error probability close to the $5\\%$ level  in  all  cases (including cases of misspecification). For $L>2$ the 2SLS tests are more satisfactory.",
    "category": "INCORRECT",
    "question_context_used": "The results for Tests 2 and 3 are based upon the modified von Neumann ratio, (3.2), which is tabulated by Koerts and Abrahamse (1969). The first column of the results shows the situation when there is no misspecification and all structural disturbances are free of serial correlation. In this case the comparison between OLS and 2SLS based tests is similar for all three tests: the Type I error probabilities are above the nominal $5\\%$ level for OLS and below that level for 2SLS. There is a tendency for the probabilities to increase with $L$ , and for $L=6$ the increase over the $5\\%$ level for OLS is substantial being close to $11\\%$ for Test 1 : $(\\beta$ approximation). For 2SLS the increase with $L$ istowards the $5\\%$ level. Increasing  the contemporaneous correlation between the structural disturbances, α, from 0.4 to 0.8 had little effect and so results are shown for $\\mathfrak{x}=0.4$ only. The remaining results in table 1 show what happens when there is serial correlation elsewhere in the system. The effect of this misspecification is, surprisingly, to bring the Type 1 error probability closer to the nominal 5% level for OLS based tests while reducing the probabilities even further below $5\\%$ for  2SLS.  The  case  of $L=0$ is  an  exception  in  that  the  Type 1 probabilities become extremely high. The overall situation, then, is that when $L$ is low the OLS based tests tend to have a Type 1 error probability close to the $5\\%$ level  in  all  cases (including cases of misspecification). For $L>2$ the 2SLS tests are more satisfactory.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3821",
    "question": "In Theil's inequality statistics framework, which of the following components directly contribute to the decomposition of forecast errors?",
    "options": [
      "A) Error due to bias ($U M_{1}$)",
      "B) Error due to standard deviation ($U S_{1}$)",
      "C) Error due to correlation ($U C_{1}$)",
      "D) Error due to omitted variables in the regression model"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Effects of Relative Prices and Capacity on the Demand for Labour in the U.S. Manufacturing Sector1.md"
    ],
    "question_context": "Theil's inequality statistics decompose forecast errors into three distinct components: bias ($U M_{1}$), variance ($U S_{1}$), and covariance ($U C_{1}$). The formulas are given by: (a) $U M_{1}=(\\bar{P}{-}\\bar{A})/D$ ; (b) $U S_{1}=(S_{P}-S_{A})/D$ ; (c) $U C_{1}=2(1-r)S_{P}\\cdot S_{A}/D$. Here, $\\bar{A}$ and $\\bar{P}$ represent the means of actual and predicted values, $S_{A}$ and $S_{P}$ their standard deviations, and $r$ the correlation coefficient. The denominator $D$ is defined as $D=\\left[\\sum_{i}^{n}A_{i}^{2}\\right]+\\left[\\sum_{i}^{n}P_{i}^{2}\\right]$. These components collectively measure the accuracy and reliability of economic forecasts, particularly in employment prediction models.",
    "category": "CORRECT",
    "question_context_used": "Theil's inequality statistics decompose forecast errors into three distinct components: bias ($U M_{1}$), variance ($U S_{1}$), and covariance ($U C_{1}$). The formulas are given by: (a) $U M_{1}=(\\bar{P}{-}\\bar{A})/D$ ; (b) $U S_{1}=(S_{P}-S_{A})/D$ ; (c) $U C_{1}=2(1-r)S_{P}\\cdot S_{A}/D$. Here, $\\bar{A}$ and $\\bar{P}$ represent the means of actual and predicted values, $S_{A}$ and $S_{P}$ their standard deviations, and $r$ the correlation coefficient. The denominator $D$ is defined as $D=\\left[\\sum_{i}^{n}A_{i}^{2}\\right]+\\left[\\sum_{i}^{n}P_{i}^{2}\\right]$. These components collectively measure the accuracy and reliability of economic forecasts, particularly in employment prediction models.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2471",
    "question": "Under the described trading model with Bayesian incentive compatibility and individual rationality constraints, which of the following conditions are necessary for ensuring ex post efficiency?",
    "options": [
      "A) The arbitrator's price scheme must align with traders' true reservation values",
      "B) The buyer's reported value must always exceed the seller's reported value for trade to occur",
      "C) The traders' beliefs about each other's reservation values must be consistent with the arbitrator's beliefs",
      "D) The probability distributions of reservation values must satisfy ε + δ = 1"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "On-incentive-compatible--individually-rational--and-ex-p_1989_Journal-of-Eco.md"
    ],
    "question_context": "The model involves a seller, buyer, and benevolent arbitrator trading an indivisible object. The buyer's utility gain is their true reservation value minus the money paid, while the seller's utility gain is the money received minus their true reservation value. The trading rule ensures ex post efficiency if true values are reported. The buyer's reservation value is either $B_{H}$ or $B_{L}$ ($\\infty>B_{H}>B_{L}\\geqslant0$), and the seller's is either $S_{H}$ or $S_{L}$ ($\\infty>S_{H}>S_{L}\\geqslant0$), with distinct values. Buyer's beliefs: $(1-\\varepsilon)=P(S_{L})$, $\\varepsilon=P(S_{H})$. Seller's beliefs: $\\delta=P(B_{L})$, $(1-\\delta)=P(B_{H})$. Arbitrator's beliefs must be consistent with traders' beliefs for Bayesian incentive compatibility.",
    "category": "INCORRECT",
    "question_context_used": "The model involves a seller, buyer, and benevolent arbitrator trading an indivisible object. The buyer's utility gain is their true reservation value minus the money paid, while the seller's utility gain is the money received minus their true reservation value. The trading rule ensures ex post efficiency if true values are reported. The buyer's reservation value is either $B_{H}$ or $B_{L}$ ($\\infty>B_{H}>B_{L}\\geqslant0$), and the seller's is either $S_{H}$ or $S_{L}$ ($\\infty>S_{H}>S_{L}\\geqslant0$), with distinct values. Buyer's beliefs: $(1-\\varepsilon)=P(S_{L})$, $\\varepsilon=P(S_{H})$. Seller's beliefs: $\\delta=P(B_{L})$, $(1-\\delta)=P(B_{H})$. Arbitrator's beliefs must be consistent with traders' beliefs for Bayesian incentive compatibility.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3570",
    "question": "In the context of dynamic market processes with asymmetrically informed traders, which of the following statements are supported by the authors' analysis?",
    "options": [
      "A) Market processes are informative if traders' beliefs converge to the true parameter value using only endogenous data",
      "B) Static Walrasian models are well-suited for studying statistical properties of equilibrium prices",
      "C) The Beja paradox arises when equilibrium prices are sufficient statistics for private information",
      "D) Rational expectations equilibria can be learned by Bayesians from endogenous market data in the long run"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Rational-expectations-equilibrium--An-alternative_1984_Journal-of-Economic-T.md"
    ],
    "question_context": "The authors study a dynamic market process where traders condition beliefs on past endogenous and current exogenous data. A market process is termed informative if beliefs converge almost surely to the true parameter value using only endogenous data. Key findings include: (1) Under standard regularity conditions, generic market processes are informative; (2) Static Walrasian models are ill-suited for studying statistical properties of equilibrium due to simultaneous inference and market clearing; (3) The Beja paradox occurs when equilibrium prices are sufficient statistics, preventing implementation via excess demand data alone. The model uses statistical decision theory to analyze limit behavior, showing that uninformed traders can learn structural parameters θ from market data under certain conditions. The proof of Theorem 1 relies on martingale convergence: μₜ(B) = E{1_{B×M}|M̄ₜ} converges weakly to a limit measure μ∞(B) almost surely.",
    "category": "INCORRECT",
    "question_context_used": "The authors study a dynamic market process where traders condition beliefs on past endogenous and current exogenous data. A market process is termed informative if beliefs converge almost surely to the true parameter value using only endogenous data. Key findings include: (1) Under standard regularity conditions, generic market processes are informative; (2) Static Walrasian models are ill-suited for studying statistical properties of equilibrium due to simultaneous inference and market clearing; (3) The Beja paradox occurs when equilibrium prices are sufficient statistics, preventing implementation via excess demand data alone. The model uses statistical decision theory to analyze limit behavior, showing that uninformed traders can learn structural parameters θ from market data under certain conditions. The proof of Theorem 1 relies on martingale convergence: μₜ(B) = E{1_{B×M}|M̄ₜ} converges weakly to a limit measure μ∞(B) almost surely.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3850",
    "question": "Under the constant elasticity quality/cost relation $x = c^{\\gamma}$, which of the following statements are consistent with Proposition 5?",
    "options": [
      "A) For $\\gamma > 0$, profit distributions are positive-power related to unit costs",
      "B) For $\\gamma < \\left(\\frac{1-\\rho}{\\rho}\\right)$, output distributions are negative-power related to unit costs",
      "C) Price and cost distributions are additively related",
      "D) For $\\gamma > \\left(\\frac{1-\\rho}{\\rho}\\right)$, output and cost distributions are in inverse power families"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Decoupling_the_CES_Distribution_Circle_with_Quality_and_Beyond_Equilibrium_Distributions_and_the_CES-Logit_Nexus.md"
    ],
    "question_context": "Key formula: $x = c^{\\gamma}$ defines the quality/cost relation. Proposition 5 states: (a) price and cost distributions are multiplicatively related; (b) profits, sales revenue and quality/cost distributions are positive-power related to unit costs for $\\gamma>0$ and negative-power related for $\\gamma<0$; (c) the output distribution is negative-power related to unit costs for $\\gamma<\\left(\\frac{1-\\rho}{\\rho}\\right)$, and positive-power related for $\\gamma>\\left(\\frac{1-\\rho}{\\rho}\\right)$. The actionable tests for the model are that estimated profit and revenue distributions should be the same up to a multiplicative factor. Likewise for prices and cost. If these hold, the test for the constant elasticity bridge function is that these distributions lie in the same CPF.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: $x = c^{\\gamma}$ defines the quality/cost relation. Proposition 5 states: (a) price and cost distributions are multiplicatively related; (b) profits, sales revenue and quality/cost distributions are positive-power related to unit costs for $\\gamma>0$ and negative-power related for $\\gamma<0$; (c) the output distribution is negative-power related to unit costs for $\\gamma<\\left(\\frac{1-\\rho}{\\rho}\\right)$, and positive-power related for $\\gamma>\\left(\\frac{1-\\rho}{\\rho}\\right)$. The actionable tests for the model are that estimated profit and revenue distributions should be the same up to a multiplicative factor. Likewise for prices and cost. If these hold, the test for the constant elasticity bridge function is that these distributions lie in the same CPF.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4260",
    "question": "In the two-country exchange economy model with international asset trade, which of the following statements are true regarding the equilibrium conditions and implications?",
    "options": [
      "A) When the tax on foreign dividends (τ) is zero, the equilibrium portfolio share (λ) is 0.5, leading to perfect risk sharing.",
      "B) The volatility of net exports (σₙₓ) increases as the correlation of shocks (ρ) decreases, holding other parameters constant.",
      "C) The equilibrium portfolio share (λ) depends only on the tax rate (τ), the correlation of shocks (ρ), and the ratio μ/(Aσ²).",
      "D) A reduction in the shock correlation (ρ) always leads to a decrease in the consumption correlation (corr(c, c*)), regardless of the tax rate (τ)."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Financial-globalization-and-real-regionalizati_2004_Journal-of-Economic-Theo.md"
    ],
    "question_context": "The model describes a two-country exchange economy where agents trade shares in domestic and foreign trees. The domestic household's consumption is given by: \n\n$$ c(s) = \\lambda X(s) + \\frac{P(1-\\lambda)}{P^{*}}(1-\\tau)X^{*}(s). $$\n\nThe equilibrium portfolio share (λ) is derived as:\n\n$$ \\lambda = \\operatorname*{min}\\left(\\frac{{(1-\\tau)}^{2}-{(1-\\tau)}\\rho+\\frac{\\tau\\mu}{A\\sigma^{2}}}{1-2{(1-\\tau)}\\rho+{(1-\\tau)}^{2}},1\\right). $$\n\nKey implications include: when τ = 0, λ = 0.5, achieving perfect risk sharing. The volatility of net exports is given by:\n\n$$ \\sigma_{n x}^{2}=2(1-\\lambda)^{2}\\sigma^{2}(1-\\rho). $$\n\nA reduction in ρ increases σₙₓ and affects λ, but the impact on consumption correlation depends on τ.",
    "category": "CORRECT",
    "question_context_used": "The model describes a two-country exchange economy where agents trade shares in domestic and foreign trees. The domestic household's consumption is given by: \n\n$$ c(s) = \\lambda X(s) + \\frac{P(1-\\lambda)}{P^{*}}(1-\\tau)X^{*}(s). $$\n\nThe equilibrium portfolio share (λ) is derived as:\n\n$$ \\lambda = \\operatorname*{min}\\left(\\frac{{(1-\\tau)}^{2}-{(1-\\tau)}\\rho+\\frac{\\tau\\mu}{A\\sigma^{2}}}{1-2{(1-\\tau)}\\rho+{(1-\\tau)}^{2}},1\\right). $$\n\nKey implications include: when τ = 0, λ = 0.5, achieving perfect risk sharing. The volatility of net exports is given by:\n\n$$ \\sigma_{n x}^{2}=2(1-\\lambda)^{2}\\sigma^{2}(1-\\rho). $$\n\nA reduction in ρ increases σₙₓ and affects λ, but the impact on consumption correlation depends on τ.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2950",
    "question": "In the context of calculating market access for Chinese cities, which of the following assumptions are critical for the allocation of provincial market capacity to cities?",
    "options": [
      "A) Homotheticity of city expenditures relative to provincial GDP",
      "B) Constant supply index within provinces for all cities",
      "C) Perfect substitutability of goods across cities within a province",
      "D) Uniform transport costs within and between provinces"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Hering-MARKETACCESSINDIVIDUAL-2010.md"
    ],
    "question_context": "The allocation of provincial market capacity to cities relies on two key hypotheses. The first is the assumption of homotheticity, so that the expenditure of city $c$ is given by $E_{c}=(y_{c}/y_{j})E_{j},$ where $y_{c}/y_{j}$ is city c's share of provincial GDP. The second is that $G_{j},$ the supply index, is approximately constant within provinces- ${\\cal G}_{c}={\\cal G}_{j},$ for all cities inside $j$. The combination of these assumptions yields the market capacity of each city $m_{c}=(y_{c}/y_{j})m_{j}$. The province-level market capacity $(m_{j}=G_{j}^{\\sigma-1}E_{j}=$ $\\mathsf{e x p}(F M_{j}))$ is then allocated to cities inside province $j$ according to the GDP share of each constituent city.",
    "category": "CORRECT",
    "question_context_used": "The allocation of provincial market capacity to cities relies on two key hypotheses. The first is the assumption of homotheticity, so that the expenditure of city $c$ is given by $E_{c}=(y_{c}/y_{j})E_{j},$ where $y_{c}/y_{j}$ is city c's share of provincial GDP. The second is that $G_{j},$ the supply index, is approximately constant within provinces- ${\\cal G}_{c}={\\cal G}_{j},$ for all cities inside $j$. The combination of these assumptions yields the market capacity of each city $m_{c}=(y_{c}/y_{j})m_{j}$. The province-level market capacity $(m_{j}=G_{j}^{\\sigma-1}E_{j}=$ $\\mathsf{e x p}(F M_{j}))$ is then allocated to cities inside province $j$ according to the GDP share of each constituent city.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3812",
    "question": "Under the given assumptions (A8)-(A10) and the properties of the value function V, which of the following statements about the policy function ϕ(x) are correct?",
    "options": [
      "A) There exists some x* ∈ (0,B) such that ϕ(x*) = x*",
      "B) For all x ∈ (0,a), ϕ(x) > x for some a > 0",
      "C) ϕ(B) > B under assumption (A10)",
      "D) The single-crossing property implies a unique stationary optimal stock in (0,B)"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Intertemporal Complementarity and Optimality A Study of a Two-Dimensional Dynamical System.md"
    ],
    "question_context": "The value function V satisfies key properties under assumptions (A8)-(A10). Lemma 1 establishes that V(x,x/δ) ≥ [(N-1)θ/ˆx]x for a given integer N ≥ 2. Propositions 1 and 2 show that [V(x,x/δ)/x] → ∞ and [V(x,x)/x] → ∞ as x → 0, respectively. The policy function ϕ(x) = h(x,x) is analyzed under these conditions. Proposition 3 proves there exists a > 0 such that for all x ∈ (0,a), ϕ(x) > x. Proposition 4 shows ϕ(B) < B under assumption (A10). Proposition 5 establishes the existence of a stationary optimal stock x* ∈ (0,B) where ϕ(x*) = x*. The single-crossing property is introduced, which holds if there is a unique stationary optimal stock in (0,B), as verified in the example with δf'(x) = 1.",
    "category": "CORRECT",
    "question_context_used": "The value function V satisfies key properties under assumptions (A8)-(A10). Lemma 1 establishes that V(x,x/δ) ≥ [(N-1)θ/ˆx]x for a given integer N ≥ 2. Propositions 1 and 2 show that [V(x,x/δ)/x] → ∞ and [V(x,x)/x] → ∞ as x → 0, respectively. The policy function ϕ(x) = h(x,x) is analyzed under these conditions. Proposition 3 proves there exists a > 0 such that for all x ∈ (0,a), ϕ(x) > x. Proposition 4 shows ϕ(B) < B under assumption (A10). Proposition 5 establishes the existence of a stationary optimal stock x* ∈ (0,B) where ϕ(x*) = x*. The single-crossing property is introduced, which holds if there is a unique stationary optimal stock in (0,B), as verified in the example with δf'(x) = 1.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2682",
    "question": "In the context of multi-state urban areas with tax differentials, which of the following statements are correct regarding the impact of residence-based versus employment-based taxation on commuting patterns?",
    "options": [
      "A) Under residence-based taxation (reciprocity), high-tax states experience shorter commutes due to reduced interstate commuting.",
      "B) Employment-based taxation without reciprocity leads to increased commuting times if employment is more tax-elastic than population.",
      "C) Commuting times serve as a sufficient statistic for measuring spatial welfare costs of tax differentials in both tax regimes.",
      "D) Tax differentials have negligible effects on commuting patterns for low-income households regardless of tax regime."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Commuting_and_Taxes_Theory,_Empirics_and_Welfare_Implications.md"
    ],
    "question_context": "The theoretical model examines how interstate tax differentials distort commuting patterns, with effects contingent on whether taxes are residence-based (reciprocity) or employment-based. Key formula: Tax liability for a resident of state $i$ working in state $j$ is $(1-I_{k})T^{i}+I_{k}T^{j}$, where $I_{k}=1$ if $T^{j}>T^{i}$ without reciprocity. Under reciprocity, high-tax states see reduced interstate commuting and shorter commutes. Without reciprocity, employment-based taxation increases commuting times if employment responds more strongly to taxes than population. The model demonstrates that commuting times are a sufficient statistic for spatial welfare costs, with significant effects concentrated among high-income households.",
    "category": "INCORRECT",
    "question_context_used": "The theoretical model examines how interstate tax differentials distort commuting patterns, with effects contingent on whether taxes are residence-based (reciprocity) or employment-based. Key formula: Tax liability for a resident of state $i$ working in state $j$ is $(1-I_{k})T^{i}+I_{k}T^{j}$, where $I_{k}=1$ if $T^{j}>T^{i}$ without reciprocity. Under reciprocity, high-tax states see reduced interstate commuting and shorter commutes. Without reciprocity, employment-based taxation increases commuting times if employment responds more strongly to taxes than population. The model demonstrates that commuting times are a sufficient statistic for spatial welfare costs, with significant effects concentrated among high-income households.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2601",
    "question": "In the context of mechanism design with evidence, which of the following statements are true regarding robustly incentive compatible mechanisms?",
    "options": [
      "A) Robust incentive compatibility implies both ex post and dominant strategy incentive compatibility.",
      "B) A robustly incentive compatible mechanism requires the principal to know the agents' beliefs about each other's types.",
      "C) The outcome of a robustly incentive compatible mechanism can change if agents report publicly and sequentially.",
      "D) Under the given assumptions, there exists an optimal incentive compatible mechanism that is robustly incentive compatible."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "BenPorath-MECHANISMSEVIDENCECOMMITMENT-2019.md"
    ],
    "question_context": "The principal's utility function has two natural interpretations. First, we can interpret $v$ as a social welfare function where $\\bar{v}_{i}(t_{i})$ reflects how much the principal cares about agent i's utility. Second, we can think of $\\bar{v}_{i}(t_{i})$ as measuring the extent to which the principal's interests are aligned with agent $i$'s. That is, a high value of $\\bar{v}_{i}(t_{i})$ does not mean that the principal likes agent $i$ but that the principal likes what agent $i$ likes. A mechanism is robustly incentive compatible if for every agent $i$, $\\mathcal{U}_{i}\\left(t_{i},M_{i}(t_{i}),t_{-i},e_{-i}\\mid t_{i},P\\right)\\ge\\mathcal{U}_{i}(s_{i},e_{i},t_{-i},e_{-i}\\mid t_{i},P)$, for all $s_{i},t_{i}\\in T_{i}$, all $t_{-i}\\in T_{-i}$, all $e_{-i}\\in\\mathcal{E}_{-i}$, and all $e_{i}\\in\\mathcal{E}_{i}(t_{i})$. Robust incentive compatibility implies ex post incentive compatibility and dominant strategy incentive compatibility, but is not implied by either. Under our assumptions, there is no difference—there is an optimal incentive compatible mechanism for the principal which is robustly incentive compatible.",
    "category": "CORRECT",
    "question_context_used": "The principal's utility function has two natural interpretations. First, we can interpret $v$ as a social welfare function where $\\bar{v}_{i}(t_{i})$ reflects how much the principal cares about agent i's utility. Second, we can think of $\\bar{v}_{i}(t_{i})$ as measuring the extent to which the principal's interests are aligned with agent $i$'s. That is, a high value of $\\bar{v}_{i}(t_{i})$ does not mean that the principal likes agent $i$ but that the principal likes what agent $i$ likes. A mechanism is robustly incentive compatible if for every agent $i$, $\\mathcal{U}_{i}\\left(t_{i},M_{i}(t_{i}),t_{-i},e_{-i}\\mid t_{i},P\\right)\\ge\\mathcal{U}_{i}(s_{i},e_{i},t_{-i},e_{-i}\\mid t_{i},P)$, for all $s_{i},t_{i}\\in T_{i}$, all $t_{-i}\\in T_{-i}$, all $e_{-i}\\in\\mathcal{E}_{-i}$, and all $e_{i}\\in\\mathcal{E}_{i}(t_{i})$. Robust incentive compatibility implies ex post incentive compatibility and dominant strategy incentive compatibility, but is not implied by either. Under our assumptions, there is no difference—there is an optimal incentive compatible mechanism for the principal which is robustly incentive compatible.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2629",
    "question": "Which of the following features are characteristic of econometric models in centrally planned economies as described in the text?",
    "options": [
      "A) Heavy reliance on exogenous government policy variables for resource allocation",
      "B) Detailed treatment of sectoral production functions with capital and labor inputs",
      "C) Primary focus on short-term fluctuations in aggregate demand",
      "D) Explicit modeling of price adjustments to balance supply and demand"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Macro-Econometric Model Building in Socialist and Non-Socialist Countries A Comparative Study.md"
    ],
    "question_context": "The text highlights that econometric models in centrally planned economies are shaped by the institutional framework of central planning. Key features include: (1) Medium to Long Term Outlook, where models focus on annual measurements and the evolution of the system, with some recent attention to short-run fluctuations in aggregate demand. (2) Material Product System (MPS), which accounts only for 'productive sectors' and often omits services, leading to models expressed in real terms with few nominal values. (3) Industrial and Agricultural Development, where detailed sectoral production functions match capital and labor inputs to output, often with exogenous factor inputs reflecting central planning. (4) Aggregate Demand, which is decomposed into personal consumption, public consumption, net capital accumulation, and foreign trade balance, with increasing attention to demand equations. (5) Stage of Economic Development, influencing the structure and focus of relationships, with less concern for capacity utilization than in market economies. The models typically consist of three blocks: output determination, aggregate demand components, and balancing identities, with government policy variables playing a decisive role.",
    "category": "CORRECT",
    "question_context_used": "The text highlights that econometric models in centrally planned economies are shaped by the institutional framework of central planning. Key features include: (1) Medium to Long Term Outlook, where models focus on annual measurements and the evolution of the system, with some recent attention to short-run fluctuations in aggregate demand. (2) Material Product System (MPS), which accounts only for 'productive sectors' and often omits services, leading to models expressed in real terms with few nominal values. (3) Industrial and Agricultural Development, where detailed sectoral production functions match capital and labor inputs to output, often with exogenous factor inputs reflecting central planning. (4) Aggregate Demand, which is decomposed into personal consumption, public consumption, net capital accumulation, and foreign trade balance, with increasing attention to demand equations. (5) Stage of Economic Development, influencing the structure and focus of relationships, with less concern for capacity utilization than in market economies. The models typically consist of three blocks: output determination, aggregate demand components, and balancing identities, with government policy variables playing a decisive role.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1789",
    "question": "Which of the following statements accurately describe properties or implications of the GARCH(1,1) model as applied to commodity price volatility?",
    "options": [
      "A) The conditional variance in GARCH(1,1) depends on both past squared residuals and past conditional variances.",
      "B) GARCH(1,1) captures ex ante perceptions of volatility by treating conditional variance as a measure of anticipated volatility.",
      "C) The model assumes that unconditional and conditional variances are identical in the data-generating process.",
      "D) GARCH(1,1) specifications cannot account for systematic changes like seasonality in agricultural prices."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Jacks-COMMODITYPRICEVOLATILITY-2011.md"
    ],
    "question_context": "The generalized autoregressive conditional heteroskedastic (GARCH) framework, introduced by Engle (1982) and Bollerslev (1986), is robust for modeling time-series volatility. It distinguishes between unconditional and conditional variances and incorporates long memory and flexible lag structures. The GARCH(p,q) specification defines conditional variance as: \n\n$$\\upsigma_{t}^{2}=E\\mathopen{}\\mathclose{}\\bgroup\\left(e_{t}^{2}|\\Omega_{t}\\aftergroup\\egroup\\right)=\\alpha+\\sum_{i=1}^{p}\\gamma_{i}e_{t-i}^{2}+\\sum_{j=1}^{q}\\delta_{j}\\upsigma_{t-j}^{2}.$$\n\nGARCH(1,1) effectively captures commodity price volatility, treating conditional variance as an ex ante measure that incorporates anticipated systematic changes (e.g., seasonality). Empirical results show no increasing trend in commodity price volatility over 1720–1896, with spikes mainly during conflicts. Commodities consistently exhibit higher volatility than manufactures, with globalization reducing volatility by integrating markets.",
    "category": "CORRECT",
    "question_context_used": "The generalized autoregressive conditional heteroskedastic (GARCH) framework, introduced by Engle (1982) and Bollerslev (1986), is robust for modeling time-series volatility. It distinguishes between unconditional and conditional variances and incorporates long memory and flexible lag structures. The GARCH(p,q) specification defines conditional variance as: \n\n$$\\upsigma_{t}^{2}=E\\mathopen{}\\mathclose{}\\bgroup\\left(e_{t}^{2}|\\Omega_{t}\\aftergroup\\egroup\\right)=\\alpha+\\sum_{i=1}^{p}\\gamma_{i}e_{t-i}^{2}+\\sum_{j=1}^{q}\\delta_{j}\\upsigma_{t-j}^{2}.$$\n\nGARCH(1,1) effectively captures commodity price volatility, treating conditional variance as an ex ante measure that incorporates anticipated systematic changes (e.g., seasonality). Empirical results show no increasing trend in commodity price volatility over 1720–1896, with spikes mainly during conflicts. Commodities consistently exhibit higher volatility than manufactures, with globalization reducing volatility by integrating markets.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2334",
    "question": "In the context of comparative statics analysis of migration between regions, which of the following exogenous changes would lead to migration toward region 1 (the improved area)?",
    "options": [
      "A) An increase in the climatic differential ($c_{1}/c_{2}$)",
      "B) A differentially high and uncompensated tax on labor in area 2",
      "C) A technological change in production in region 1 ($\\theta_{1}/\\theta_{2}$ increases)",
      "D) A nationwide technological improvement ($E(Q_{1}/Q_{2})=0$)"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Regional Distribution of Population, Migration, and Climate.md"
    ],
    "question_context": "The analysis examines how exogenous changes affect migration patterns between regions with different climates and economic conditions. From equation (27), if $c_{1}/c_{2}$ increases, $\\mathbf{{L_{1}}}/{L_{2}}$ will also increase, indicating migration toward the area with improved climate. This could result from direct climate changes or relative price changes in climate-related goods. Similarly, a differentially high tax on labor in area 2 leads to $E(L_{1}/L_{2})/E t>0$, causing migration toward the fiscally efficient area. Technological improvements in region 1, modeled as $\\theta_{1}/\\theta_{2}$ increases, also induce migration toward region 1 as shown in equation (28): $E\\left(\\frac{L_{1}}{L_{2}}\right)=\\frac{\\delta(1-\\alpha_{3})}{K_{2}}E\\left(\\frac{Q_{1}}{Q_{2}}\right)>0$. However, a nationwide technological improvement with $E(Q_{1}/Q_{2})=0$ results in no migration between regions.",
    "category": "CORRECT",
    "question_context_used": "The analysis examines how exogenous changes affect migration patterns between regions with different climates and economic conditions. From equation (27), if $c_{1}/c_{2}$ increases, $\\mathbf{{L_{1}}}/{L_{2}}$ will also increase, indicating migration toward the area with improved climate. This could result from direct climate changes or relative price changes in climate-related goods. Similarly, a differentially high tax on labor in area 2 leads to $E(L_{1}/L_{2})/E t>0$, causing migration toward the fiscally efficient area. Technological improvements in region 1, modeled as $\\theta_{1}/\\theta_{2}$ increases, also induce migration toward region 1 as shown in equation (28): $E\\left(\\frac{L_{1}}{L_{2}}\right)=\\frac{\\delta(1-\\alpha_{3})}{K_{2}}E\\left(\\frac{Q_{1}}{Q_{2}}\right)>0$. However, a nationwide technological improvement with $E(Q_{1}/Q_{2})=0$ results in no migration between regions.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2972",
    "question": "In the context of heterogeneous dynamic panels, which of the following statements are true regarding the standardized t-bar test statistic?",
    "options": [
      "A) The t-bar statistic converges to a standard normal variate sequentially as T→∞ followed by N→∞.",
      "B) The modified version of the t-bar statistic, Z_tbar, is standard normal as N→∞ for fixed T when errors are serially uncorrelated.",
      "C) The t-bar test requires N/T→0 as both N and T tend to infinity for consistency.",
      "D) The t-bar test performs better than the Levin and Lin test when a sufficiently large lag order is selected for ADF regressions."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Reprint-of--Testing-for-unit-roots-in-heterogeneous_2023_Journal-of-Economet.md"
    ],
    "question_context": "The paper proposes unit root tests for dynamic heterogeneous panels based on the mean of individual unit root statistics. A standardized t-bar test statistic is proposed, based on the (augmented) Dickey–Fuller statistics averaged across groups. Under a general setting, this statistic converges in probability to a standard normal variate sequentially with T→∞ followed by N→∞. A diagonal convergence result with T and N→∞ while N/T→k, k being a finite non-negative constant, is also conjectured. In the special case where errors in individual Dickey–Fuller (DF) regressions are serially uncorrelated, a modified version of the standardized t-bar statistic is shown to be distributed as standard normal as N→∞ for a fixed T, so long as T>5 in the case of DF regressions with intercepts and T>6 in the case of DF regressions with intercepts and linear time trends. Monte Carlo results show that if a large enough lag order is selected for the underlying ADF regressions, the small sample performance of the t-bar test is reasonably satisfactory and generally better than the test proposed by Levin and Lin (1993).",
    "category": "CORRECT",
    "question_context_used": "The paper proposes unit root tests for dynamic heterogeneous panels based on the mean of individual unit root statistics. A standardized t-bar test statistic is proposed, based on the (augmented) Dickey–Fuller statistics averaged across groups. Under a general setting, this statistic converges in probability to a standard normal variate sequentially with T→∞ followed by N→∞. A diagonal convergence result with T and N→∞ while N/T→k, k being a finite non-negative constant, is also conjectured. In the special case where errors in individual Dickey–Fuller (DF) regressions are serially uncorrelated, a modified version of the standardized t-bar statistic is shown to be distributed as standard normal as N→∞ for a fixed T, so long as T>5 in the case of DF regressions with intercepts and T>6 in the case of DF regressions with intercepts and linear time trends. Monte Carlo results show that if a large enough lag order is selected for the underlying ADF regressions, the small sample performance of the t-bar test is reasonably satisfactory and generally better than the test proposed by Levin and Lin (1993).",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1651",
    "question": "According to the arguments presented in 'Wealth, Poverty and Politics', which of the following are identified as challenges to prevailing economic narratives on wealth concentration and income distribution?",
    "options": [
      "A) The reliance on ambiguous terminology and misleading statistics",
      "B) The uncritical acceptance of prevailing orthodoxies by leading economists",
      "C) The focus on deterministic geographic factors as the sole explanation for disparities",
      "D) The exclusion of historical, demographic, and cultural analyses in policy discussions"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "BackMatter-2017.md"
    ],
    "question_context": "The text critiques contemporary discussions on wealth concentration and income distribution, highlighting the use of ambiguous words and misleading statistics, even by leading economists. It challenges underlying assumptions embedded in these narratives, particularly regarding the much-discussed 'top one percent.' The book advocates for a multidisciplinary approach, incorporating history, economics, geography, demography, and culture to understand economic disparities. Key formula: Disparities = f(history, economics, geography, demography, culture). This perspective is positioned as a counter to prevailing orthodoxies and aims to provide a more nuanced understanding of wealth and poverty dynamics.",
    "category": "INCORRECT",
    "question_context_used": "The text critiques contemporary discussions on wealth concentration and income distribution, highlighting the use of ambiguous words and misleading statistics, even by leading economists. It challenges underlying assumptions embedded in these narratives, particularly regarding the much-discussed 'top one percent.' The book advocates for a multidisciplinary approach, incorporating history, economics, geography, demography, and culture to understand economic disparities. Key formula: Disparities = f(history, economics, geography, demography, culture). This perspective is positioned as a counter to prevailing orthodoxies and aims to provide a more nuanced understanding of wealth and poverty dynamics.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-78",
    "question": "In the context of robustness checks for GATT/WTO membership effects on trade, which of the following estimation techniques or sample restrictions yield economically significant positive coefficients for joint membership?",
    "options": [
      "A) Restricting the sample to industrial countries only",
      "B) Using the treatment methodology developed by James J. Heckman",
      "C) Incorporating quadratic gravity terms as nuisance variables",
      "D) Employing the Prais-Winsten estimator with autoregressive errors"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Rose-ReallyKnowWTO-2004.md"
    ],
    "question_context": "Key findings from the robustness checks include: When the sample is restricted to industrial countries only, GATT/WTO membership has a somewhat important effect on trade, with an estimate indicating that a pair of industrial GATT/WTO members trades about 60 percent more than an otherwise-identical pair of nonmembers. The treatment methodology attempts to correct for selection bias, yet it delivers even more negative results. The formula for the industrial countries effect is $\\approx\\exp(0.47)-1$. Other robustness checks, such as adding quadratic gravity terms or using the Prais-Winsten estimator, do not yield economically significant positive effects.",
    "category": "INCORRECT",
    "question_context_used": "Key findings from the robustness checks include: When the sample is restricted to industrial countries only, GATT/WTO membership has a somewhat important effect on trade, with an estimate indicating that a pair of industrial GATT/WTO members trades about 60 percent more than an otherwise-identical pair of nonmembers. The treatment methodology attempts to correct for selection bias, yet it delivers even more negative results. The formula for the industrial countries effect is $\\approx\\exp(0.47)-1$. Other robustness checks, such as adding quadratic gravity terms or using the Prais-Winsten estimator, do not yield economically significant positive effects.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3003",
    "question": "In the extended framework of binary evaluations with abstentions, which of the following conditions on the set $X$ of feasible evaluations lead to oligarchic aggregation rules?",
    "options": [
      "A) $X$ is totally blocked and not an affine subspace",
      "B) $X$ is partially blocked and forms an affine subspace",
      "C) $X$ is totally blocked and forms an affine subspace",
      "D) $X$ is partially blocked and not an affine subspace"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Aggregation-of-binary-evaluations-with-abstenti_2010_Journal-of-Economic-The.md"
    ],
    "question_context": "The paper extends the framework of binary evaluations by allowing abstentions, represented by $^{0-1-*}$ vectors. A $^{0-1-*}$ vector is feasible if it can be completed to a 0-1 vector in $X$ without uniquely determining the abstentions. The main result characterizes sets $X$ for which every IIA and Paretian aggregator into $X^{*}$ must be oligarchic. The conditions are that $X$ be totally blocked and not be an affine subspace. The formula for the set $X^{*}$ is derived as follows: a $^{0-1-*}$ vector is in $X^{*}$ if there exists a completion to a vector in $X$ where none of the $^*$ entries are uniquely determined by the constraints of $X$. This ensures abstentions represent genuine non-determination. The robustness of these structural conditions is highlighted, showing they lead to oligarchy when abstentions are allowed, just as they lead to dictatorship in the original framework without abstentions.",
    "category": "INCORRECT",
    "question_context_used": "The paper extends the framework of binary evaluations by allowing abstentions, represented by $^{0-1-*}$ vectors. A $^{0-1-*}$ vector is feasible if it can be completed to a 0-1 vector in $X$ without uniquely determining the abstentions. The main result characterizes sets $X$ for which every IIA and Paretian aggregator into $X^{*}$ must be oligarchic. The conditions are that $X$ be totally blocked and not be an affine subspace. The formula for the set $X^{*}$ is derived as follows: a $^{0-1-*}$ vector is in $X^{*}$ if there exists a completion to a vector in $X$ where none of the $^*$ entries are uniquely determined by the constraints of $X$. This ensures abstentions represent genuine non-determination. The robustness of these structural conditions is highlighted, showing they lead to oligarchy when abstentions are allowed, just as they lead to dictatorship in the original framework without abstentions.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-525",
    "question": "In the context of deviations from the Law of One Price (LOP) and arbitrage opportunities, which of the following factors contribute to the persistence of deviations within a band of inaction?",
    "options": [
      "A) Transaction costs associated with currency exchange",
      "B) Costs of optimizing and comparing prices across currencies",
      "C) Transport costs associated with arbitrage of goods by consumers",
      "D) Perfectly elastic demand for goods across different currencies"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Asplund-LawOnePrice-2001.md"
    ],
    "question_context": "The text discusses how deviations from the Law of One Price (LOP) can persist due to various costs that create a 'band of inaction' where arbitrage is not profitable. Key factors include transaction costs of currency exchange, which differ across consumers (e.g., exchanging at home, on board, or via credit cards). The formula for arbitrage costs is given as $1.4746-(1.4301+$ $1.4706)/2=0.02225$ SEK in each direction, or 1.53 percent. Additionally, costs of optimizing (e.g., comparing prices across currencies) and transport costs for goods arbitrage further widen this band. The text also highlights that commercial transactions have narrower bands (e.g., 0.4 percent) due to lower relative importance of exchange costs. Figure 2 illustrates how deviations greater than the implied transaction cost bands are short-lived, suggesting that exchange costs are a significant barrier to arbitrage.",
    "category": "CORRECT",
    "question_context_used": "The text discusses how deviations from the Law of One Price (LOP) can persist due to various costs that create a 'band of inaction' where arbitrage is not profitable. Key factors include transaction costs of currency exchange, which differ across consumers (e.g., exchanging at home, on board, or via credit cards). The formula for arbitrage costs is given as $1.4746-(1.4301+$ $1.4706)/2=0.02225$ SEK in each direction, or 1.53 percent. Additionally, costs of optimizing (e.g., comparing prices across currencies) and transport costs for goods arbitrage further widen this band. The text also highlights that commercial transactions have narrower bands (e.g., 0.4 percent) due to lower relative importance of exchange costs. Figure 2 illustrates how deviations greater than the implied transaction cost bands are short-lived, suggesting that exchange costs are a significant barrier to arbitrage.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2180",
    "question": "Based on the analysis of scale and scope economies in public and private IHEs, which of the following statements are supported by the empirical results?",
    "options": [
      "A) Public sector exhibits product-specific economies of scale for GR even at high output levels",
      "B) Private sector shows persistent ray economies of scale up to 600% of mean output levels",
      "C) Economies of scope exist in both sectors for output levels below 150% of the means",
      "D) Specialty IHEs experience lower production costs compared to complex IHEs"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Cohn-InstitutionsHigherEducation-1989.md"
    ],
    "question_context": "Key findings from the study include: (1) ray economies of scale persist for the private sector even at 600% of the output means, but are exhausted in the public sector around the output means. (2) Product-specific scale economies for GR exist for the public sector even at high levels of output, but do not appear to exist at all in the private sector. (3) Except for output levels below 150% of the output means for the public sector, economies of scope are observed for all output levels in both sectors. The results suggest that complex IHEs, serving both undergraduate and graduate students and also being involved in research, appear to be less costly than specialty IHEs. Formula: The cost savings for private IHEs at 300% of mean output levels is calculated as 33.2% compared to specialty IHEs.",
    "category": "INCORRECT",
    "question_context_used": "Key findings from the study include: (1) ray economies of scale persist for the private sector even at 600% of the output means, but are exhausted in the public sector around the output means. (2) Product-specific scale economies for GR exist for the public sector even at high levels of output, but do not appear to exist at all in the private sector. (3) Except for output levels below 150% of the output means for the public sector, economies of scope are observed for all output levels in both sectors. The results suggest that complex IHEs, serving both undergraduate and graduate students and also being involved in research, appear to be less costly than specialty IHEs. Formula: The cost savings for private IHEs at 300% of mean output levels is calculated as 33.2% compared to specialty IHEs.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4836",
    "question": "In the context of quasi-differenced (QD) estimation and maximum likelihood estimation (MLE) for capital intensity $\\psi$ in a DSGE model, which of the following statements are supported by the simulation results?",
    "options": [
      "A) QD estimates of $\\psi$ are unbiased and have correct test sizes for all values of $\\alpha$.",
      "B) MLE estimates of $\\psi$ are severely biased when $\\alpha$ is close to one.",
      "C) The $t$-test for $\\psi$ in MLE is well-approximated by the normal distribution for all $\\alpha$.",
      "D) QD estimation uses an OLS estimate of the standard deviation of innovations for consumption."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Gorodnichenko 等 - 2012 - ESTIMATORS FOR PERSISTENT AND POSSIBLY NONSTATIONARY DATA WITH CLASSICAL PROPERTIES.md"
    ],
    "question_context": "The simulations evaluate the finite-sample properties of QD and MLE for capital intensity $\\psi$ in a DSGE model. The true value of $\\psi$ is fixed at 0.25, and five values of $\\alpha$ are considered: 1, 0.98, 0.95, 0.9, 0.8. Key findings include: QD estimates of $\\psi$ are close to the true value (0.25) with accurate test sizes for all $\\alpha$, while MLE estimates are biased when $\\alpha$ is near unity, with severely distorted $t$-tests. The QD method uses an OLS estimate of the standard deviation of innovations for consumption. The state-space representation for quasi-differenced data is given by $w_{t}=D_{0}w_{t-1}+D_{1}\\varepsilon_{t}$, where $\\Omega_{w}(j)=D_{0}^{j}\\Omega_{w}(0)$ defines the autocovariance matrices.",
    "category": "INCORRECT",
    "question_context_used": "The simulations evaluate the finite-sample properties of QD and MLE for capital intensity $\\psi$ in a DSGE model. The true value of $\\psi$ is fixed at 0.25, and five values of $\\alpha$ are considered: 1, 0.98, 0.95, 0.9, 0.8. Key findings include: QD estimates of $\\psi$ are close to the true value (0.25) with accurate test sizes for all $\\alpha$, while MLE estimates are biased when $\\alpha$ is near unity, with severely distorted $t$-tests. The QD method uses an OLS estimate of the standard deviation of innovations for consumption. The state-space representation for quasi-differenced data is given by $w_{t}=D_{0}w_{t-1}+D_{1}\\varepsilon_{t}$, where $\\Omega_{w}(j)=D_{0}^{j}\\Omega_{w}(0)$ defines the autocovariance matrices.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3290",
    "question": "Under what conditions does the Weinstein and Yildiz (WY) discontinuity in rationalizable actions persist or vanish when weakening common belief in rationality to $\\mathrm{ICR}^{\\lambda}$?",
    "options": [
      "A) The discontinuity persists if $\\lambda_{n}$ is above a threshold close to 1 for all $n$, and vanishes if $\\lambda_{n} \\to 0$ as $n \\to \\infty$.",
      "B) The discontinuity vanishes if $\\lambda_{n}$ is above a threshold close to 1 for all $n$, and persists if $\\lambda_{n} \\to 0$ as $n \\to \\infty$.",
      "C) The discontinuity persists only under full common belief in rationality ($\\lambda = (1,1,\\dots)$) and vanishes otherwise.",
      "D) The discontinuity is unaffected by the sequence $\\lambda$ and always persists regardless of the weakening of common belief in rationality."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Uncertain rationality_ depth of reasoning and robustness in games with incomplete information.md"
    ],
    "question_context": "The paper examines the impact of weakening common belief in rationality on the WY-discontinuity by introducing the concept of $\\mathrm{ICR}^{\\lambda}$, where $\\lambda = (\\lambda_{n})_{n\\in\\mathbb{N}}$ is a sequence representing the reliability players attach to nth-order belief in rationality. Key findings include: the discontinuity persists when each $\\lambda_{n}$ is above a threshold close to 1 (Proposition 5), but vanishes when $\\lambda_{n} \\to 0$ as $n \\to \\infty$ (Proposition 6). This implies that continuity of behavior is restored when confidence in mutual rationality vanishes at high orders. The $\\mathrm{ICR}^{\\lambda}$ framework is flexible, covering concepts close to ICR as well as those further away, such as rationality without mutual belief in rationality.",
    "category": "INCORRECT",
    "question_context_used": "The paper examines the impact of weakening common belief in rationality on the WY-discontinuity by introducing the concept of $\\mathrm{ICR}^{\\lambda}$, where $\\lambda = (\\lambda_{n})_{n\\in\\mathbb{N}}$ is a sequence representing the reliability players attach to nth-order belief in rationality. Key findings include: the discontinuity persists when each $\\lambda_{n}$ is above a threshold close to 1 (Proposition 5), but vanishes when $\\lambda_{n} \\to 0$ as $n \\to \\infty$ (Proposition 6). This implies that continuity of behavior is restored when confidence in mutual rationality vanishes at high orders. The $\\mathrm{ICR}^{\\lambda}$ framework is flexible, covering concepts close to ICR as well as those further away, such as rationality without mutual belief in rationality.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3277",
    "question": "Which of the following critiques are raised against Lakatosian and Kuhnian philosophies of science in the context of economics?",
    "options": [
      "A) They are too permissive in allowing theories that should be methodologically eliminated.",
      "B) They incorrectly classify economics as an 'immature' discipline analogous to early-stage physics.",
      "C) They provide decisive grounds for eliminating theories based on empirical falsifiability.",
      "D) They blur the distinction between empirical and purely conceptual aspects of economic theories."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Gambling_and_Society._Interdisciplinary_Studies_on_the_Subject_of_Gambling..md"
    ],
    "question_context": "Hutchison moves on to consider some of the more recent developments in the philosophy of science, in particular the work of Kuhn, Ravetz, and Lakatos. He is suspicious of the way in which each of these writers discusses economics under the category of 'immature' disciplines, his suspicion (rightly, in my view) arising from the suggestion that economics is just like physics but at an earlier stage of development. On the whole he sees the Lakatosian philosophy of science as a retreat from, rather than a refinement of, Popperism in that it does not provide such decisive grounds for the elimination, on methodological grounds, of those theories that he feels should be eliminated. That is, the Lakatosian philosophy of science is, for Hutchison, unsatisfactory for precisely the same reason that the Kuhnian philosophy of science is unsatisfactory: it is too permissive.",
    "category": "INCORRECT",
    "question_context_used": "Hutchison moves on to consider some of the more recent developments in the philosophy of science, in particular the work of Kuhn, Ravetz, and Lakatos. He is suspicious of the way in which each of these writers discusses economics under the category of 'immature' disciplines, his suspicion (rightly, in my view) arising from the suggestion that economics is just like physics but at an earlier stage of development. On the whole he sees the Lakatosian philosophy of science as a retreat from, rather than a refinement of, Popperism in that it does not provide such decisive grounds for the elimination, on methodological grounds, of those theories that he feels should be eliminated. That is, the Lakatosian philosophy of science is, for Hutchison, unsatisfactory for precisely the same reason that the Kuhnian philosophy of science is unsatisfactory: it is too permissive.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2175",
    "question": "In the context of dynamic models with diminishing marginal utility, which of the following statements are true regarding the optimal policy for the home country?",
    "options": [
      "A) The home country never cuts its own consumption to zero, ensuring it retains some gains from trade.",
      "B) The steady state is reached in finite time due to the infinite period of maximal consumption.",
      "C) The foreign country may gain from trade for a finite time if the initial stock of foreign-owned capital is less than its steady-state level.",
      "D) The optimal steady-state stock of foreign-owned capital is always less than its autarkic level."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Perfectly Discriminatory Policy towards International Capital Movements in a Dynamic World.md"
    ],
    "question_context": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism... It has been shown that in dynamic models it is not always optimal for a country to fully exploit its trading partner even when this is possible. This is unlike the outcome in static models which ignore the effect of current policy on future trading opportunities. By permitting the foreign country to gain from trade the home country secures a long-term advantage. Propositions 5 and 8 established when this would occur. In fact an even more striking result holds. Although Proposition 8 is similar to Proposition 5, some differences should be noted. First, with diminishing marginal utility the home country never cuts its own consumption to zero, and consequently it never allows its trading partner all of the gains from trade. This is in contrast to what happens with linear utility. Secondly, the steady state is approached asymptotically with diminishing marginal utility. It is reached in finite time with linear utility because that gives an infinite period of maximal consumption. However, the maximum flow of consumption must be approached eventually even if the marginal utility of current consumption is high. The intuition supporting the results about optimal policy is the same for both forms of the utility function.",
    "category": "CORRECT",
    "question_context_used": "Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism... It has been shown that in dynamic models it is not always optimal for a country to fully exploit its trading partner even when this is possible. This is unlike the outcome in static models which ignore the effect of current policy on future trading opportunities. By permitting the foreign country to gain from trade the home country secures a long-term advantage. Propositions 5 and 8 established when this would occur. In fact an even more striking result holds. Although Proposition 8 is similar to Proposition 5, some differences should be noted. First, with diminishing marginal utility the home country never cuts its own consumption to zero, and consequently it never allows its trading partner all of the gains from trade. This is in contrast to what happens with linear utility. Secondly, the steady state is approached asymptotically with diminishing marginal utility. It is reached in finite time with linear utility because that gives an infinite period of maximal consumption. However, the maximum flow of consumption must be approached eventually even if the marginal utility of current consumption is high. The intuition supporting the results about optimal policy is the same for both forms of the utility function.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2666",
    "question": "Under what conditions does the model predict local indeterminacy of equilibrium in the absence of externalities?",
    "options": [
      "A) When the Frisch NSC is steeper than the labor demand curve and $n_{\\lambda} < 0$",
      "B) When the labor demand curve is upward sloping and steeper than the Frisch NSC",
      "C) When higher marginal utility of wealth increases employment and output",
      "D) When there is perfect income insurance ($\\mu = 1$)"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Unemployment-and-indeterminacy_2006_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The model's equilibrium conditions are log-linearized around the steady state, with key variables defined as deviations from their steady-state levels. The Frisch NSC slope is given by $-\\frac{(\\mu-1)\\overline{{N}}}{\\mu-(\\mu-1)\\overline{{N}}}$, and the labor demand curve slope is $b-1$. The sign of $n_{\\lambda}$ is crucial for determinacy, where $n_{\\lambda} < 0$ if and only if the Frisch NSC is steeper than the labor demand curve. This occurs when income insurance is smaller (higher $\\mu$). The determinant of the ODE system is positive if and only if $n_{\\lambda} < 0$, implying local indeterminacy when the Frisch NSC is steeper. The trace condition further supports this, with calibration showing the trace's sign aligns with $n_{\\lambda}$.",
    "category": "INCORRECT",
    "question_context_used": "The model's equilibrium conditions are log-linearized around the steady state, with key variables defined as deviations from their steady-state levels. The Frisch NSC slope is given by $-\\frac{(\\mu-1)\\overline{{N}}}{\\mu-(\\mu-1)\\overline{{N}}}$, and the labor demand curve slope is $b-1$. The sign of $n_{\\lambda}$ is crucial for determinacy, where $n_{\\lambda} < 0$ if and only if the Frisch NSC is steeper than the labor demand curve. This occurs when income insurance is smaller (higher $\\mu$). The determinant of the ODE system is positive if and only if $n_{\\lambda} < 0$, implying local indeterminacy when the Frisch NSC is steeper. The trace condition further supports this, with calibration showing the trace's sign aligns with $n_{\\lambda}$.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1631",
    "question": "In the context of GMM sample moments and their derivatives, which of the following statements are correct regarding the Dif and Lev moment conditions?",
    "options": [
      "A) The Dif moment conditions involve a quadratic function of θ, while the Lev moment conditions are linear in θ.",
      "B) The Dif moment conditions are specified as $f_{i}^{Dif}(\\theta)=Z_{i}^{Dif}\\varphi_{i}^{Dif}(\\theta)$, where $Z_{i}^{Dif}$ is a matrix of instruments.",
      "C) The Lev moment conditions are characterized by $f_{i}^{Lev}(\\theta)=Z_{i}^{Lev}\\varphi_{i}^{Lev}(\\theta)$, where $\\varphi_{i}^{Lev}(\\theta)$ includes terms like $(y_{i3}-\\theta y_{i2})$.",
      "D) Both Dif and Lev moment conditions require the same number of moment conditions, $k_{Dif} = k_{Lev}$, for any given T."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "IDENTIFICATION_ROBUST_INFERENCE_FOR_MOMENTS-BASED_ANALYSIS_OF_LINEAR_DYNAMIC_PANEL_DATA_MODELS.md"
    ],
    "question_context": "The Dif moment conditions are specified as $f_{i}^{Dif}(\\theta)=Z_{i}^{Dif}\\varphi_{i}^{Dif}(\\theta)$, where $Z_{i}^{Dif}$ is a matrix of instruments and $\\varphi_{i}^{Dif}(\\theta)$ captures the moment conditions. The number of moment conditions for Dif, $k_{Dif}$, equals $\\frac{1}{2}(T-2)(T-1)$. For the Lev moment conditions, the specification is $f_{i}^{Lev}(\\theta)=Z_{i}^{Lev}\\varphi_{i}^{Lev}(\\theta)$, where $\\varphi_{i}^{Lev}(\\theta)=(y_{i3}-\\theta y_{i2}...y_{iT}-\\theta y_{iT-1})'$. The number of moment conditions for Lev, $k_{Lev}$, equals $T-2$. The matrices $Z_{i}^{Dif}$ and $Z_{i}^{Lev}$ are constructed from the data and have specific dimensions depending on T. The moment conditions are used to estimate the parameter θ in dynamic panel data models, and their derivatives play a crucial role in the asymptotic analysis of the GMM estimator.",
    "category": "CORRECT",
    "question_context_used": "The Dif moment conditions are specified as $f_{i}^{Dif}(\\theta)=Z_{i}^{Dif}\\varphi_{i}^{Dif}(\\theta)$, where $Z_{i}^{Dif}$ is a matrix of instruments and $\\varphi_{i}^{Dif}(\\theta)$ captures the moment conditions. The number of moment conditions for Dif, $k_{Dif}$, equals $\\frac{1}{2}(T-2)(T-1)$. For the Lev moment conditions, the specification is $f_{i}^{Lev}(\\theta)=Z_{i}^{Lev}\\varphi_{i}^{Lev}(\\theta)$, where $\\varphi_{i}^{Lev}(\\theta)=(y_{i3}-\\theta y_{i2}...y_{iT}-\\theta y_{iT-1})'$. The number of moment conditions for Lev, $k_{Lev}$, equals $T-2$. The matrices $Z_{i}^{Dif}$ and $Z_{i}^{Lev}$ are constructed from the data and have specific dimensions depending on T. The moment conditions are used to estimate the parameter θ in dynamic panel data models, and their derivatives play a crucial role in the asymptotic analysis of the GMM estimator.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3849",
    "question": "In the context of the log-A family and its application to the CES model, which of the following statements are correct regarding the elasticity properties of the cost, output, and revenue densities?",
    "options": [
      "A) The cost density elasticity is given by η_fc = α_c - 1, where α_c is the tail parameter of the cost distribution.",
      "B) For the Pareto distribution, the output and revenue density elasticities are η_fY = -(α_y + 1) and η_fR = -(α_r + 1), respectively.",
      "C) The elasticity of the revenue density with respect to the output density is given by η_fR = (1/ρ)η_fY + (1 - ρ)/ρ.",
      "D) An increasing cost density implies that both output and revenue densities are increasing."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Decoupling_the_CES_Distribution_Circle_with_Quality_and_Beyond_Equilibrium_Distributions_and_the_CES-Logit_Nexus.md"
    ],
    "question_context": "The log-A family is defined by densities of the form f_U(u) ∝ (1/u) exp(-|(ln u - ξ)/σ|^A), which nests log-normal (A=2) and log-Laplace (A=1) distributions. For the Pareto distribution, densities are always decreasing, but the price or cost ones can be increasing. The elasticity of the cost density is given by η_fc = α_c - 1, where α_c is the tail parameter. The relations between cost, output, and revenue densities are characterized by their elasticities, with key formulas including η_fY = -(1 - ρ)(η_fc + 1 + 1/(1 - ρ)) and η_fR = -((1 - ρ)/ρ)(η_fc + 1/(1 - ρ)). These elasticities are linked by η_fR = (1/ρ)η_fY + (1 - ρ)/ρ, showing that revenue density elasticity is more leveraged and responsive to output density elasticity when ρ is smaller.",
    "category": "CORRECT",
    "question_context_used": "The log-A family is defined by densities of the form f_U(u) ∝ (1/u) exp(-|(ln u - ξ)/σ|^A), which nests log-normal (A=2) and log-Laplace (A=1) distributions. For the Pareto distribution, densities are always decreasing, but the price or cost ones can be increasing. The elasticity of the cost density is given by η_fc = α_c - 1, where α_c is the tail parameter. The relations between cost, output, and revenue densities are characterized by their elasticities, with key formulas including η_fY = -(1 - ρ)(η_fc + 1 + 1/(1 - ρ)) and η_fR = -((1 - ρ)/ρ)(η_fc + 1/(1 - ρ)). These elasticities are linked by η_fR = (1/ρ)η_fY + (1 - ρ)/ρ, showing that revenue density elasticity is more leveraged and responsive to output density elasticity when ρ is smaller.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4549",
    "question": "In the context of the New Keynesian model with a zero lower bound (ZLB) constraint, which of the following statements accurately describe the effects of increased uncertainty on economic variables when the policy rate is at the ZLB?",
    "options": [
      "A) Increased uncertainty raises the conditional expectation of future real interest rates, leading to reduced current consumption.",
      "B) Increased uncertainty decreases the conditional expectation of future marginal costs, causing firms to set lower prices today.",
      "C) Increased uncertainty has no significant effect on output and inflation when the policy rate is at the ZLB.",
      "D) Increased uncertainty exacerbates the decline in output by approximately 10% in a deep recession scenario at the ZLB."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Nakata-UncertaintyZeroLower-2017.md"
    ],
    "question_context": "The paper examines how uncertainty affects economic variables when the policy rate is constrained at the zero lower bound (ZLB). Key findings include that the ZLB creates kinks in policy functions for real interest rates and real marginal costs, making them highly convex and concave respectively. As a result, a mean-preserving spread in the shock distribution increases the conditional expectation of future real interest rates and decreases the conditional expectation of future marginal costs when the nominal interest rate is zero. Households and firms, being forward-looking, respond by reducing consumption and setting lower prices today. The model demonstrates that uncertainty can exacerbate the decline in output by about 10% in a deep recession scenario at the ZLB. The robustness of these findings is confirmed under various alternative specifications, including different shocks and monetary policy rules.",
    "category": "CORRECT",
    "question_context_used": "The paper examines how uncertainty affects economic variables when the policy rate is constrained at the zero lower bound (ZLB). Key findings include that the ZLB creates kinks in policy functions for real interest rates and real marginal costs, making them highly convex and concave respectively. As a result, a mean-preserving spread in the shock distribution increases the conditional expectation of future real interest rates and decreases the conditional expectation of future marginal costs when the nominal interest rate is zero. Households and firms, being forward-looking, respond by reducing consumption and setting lower prices today. The model demonstrates that uncertainty can exacerbate the decline in output by about 10% in a deep recession scenario at the ZLB. The robustness of these findings is confirmed under various alternative specifications, including different shocks and monetary policy rules.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1302",
    "question": "Which of the following strategies are effective in addressing the convergence problems in genetic algorithms as discussed in the text?",
    "options": [
      "A) Scaling down the largest values of $f(\\theta)$ relative to the average value during early generations",
      "B) Dynamically adjusting the crossover rate to compensate for shifts in the diversity of the reproduction pool",
      "C) Scaling up the largest values of $f(\\theta)$ during later generations to make selection probabilities more heterogeneous",
      "D) Introducing into the population a solution created by mutation alone periodically"
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "DORSEY和MAYER - 1995 - Genetic algorithms for estimation problems with multiple optima, nondifferentiability, and other irr.md"
    ],
    "question_context": "The text discusses two primary convergence problems in genetic algorithms: premature convergence during early generations and insufficient exploitation of the best solutions during later generations. To mitigate premature convergence, Goldberg (1989) suggested scaling down the largest values of $f(\\theta)$ relative to the average value during early generations. Booker (1987) proposed dynamically adjusting the crossover rate using the formula: \n\n$$\\eta_{s}=\\operatorname*{max}\\big\\{.2,\\operatorname*{min}[\\eta_{s-1}-(\\Psi_{s}-\\Psi_{s-1}),1]\\big\\},$$\n\nwhere $\\Psi_{s}$ is the percent involvement in generation $G^{s}$. For later generations, strategies include scaling up the largest values of $f(\\theta)$, replacing candidate solutions with the best-so-far solution, restricting mutation neighborhoods, and switching to gradient-type algorithms. Additionally, Hartl (1990) emphasized periodically retrying the best-so-far solution and introducing mutation-only solutions to ensure global convergence.",
    "category": "CORRECT",
    "question_context_used": "The text discusses two primary convergence problems in genetic algorithms: premature convergence during early generations and insufficient exploitation of the best solutions during later generations. To mitigate premature convergence, Goldberg (1989) suggested scaling down the largest values of $f(\\theta)$ relative to the average value during early generations. Booker (1987) proposed dynamically adjusting the crossover rate using the formula: \n\n$$\\eta_{s}=\\operatorname*{max}\\big\\{.2,\\operatorname*{min}[\\eta_{s-1}-(\\Psi_{s}-\\Psi_{s-1}),1]\\big\\},$$\n\nwhere $\\Psi_{s}$ is the percent involvement in generation $G^{s}$. For later generations, strategies include scaling up the largest values of $f(\\theta)$, replacing candidate solutions with the best-so-far solution, restricting mutation neighborhoods, and switching to gradient-type algorithms. Additionally, Hartl (1990) emphasized periodically retrying the best-so-far solution and introducing mutation-only solutions to ensure global convergence.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2311",
    "question": "Which of the following factors contribute to the FTSE-100 index's resilience against biases in volatility estimation and stale pricing issues?",
    "options": [
      "A) Use of midpoint quotes from best bid-and-ask prices rather than transaction prices",
      "B) Absence of a call auction mechanism at market opening",
      "C) High liquidity and large-quote depth maintained by the LSE",
      "D) Frequent dividend reinvestment adjustments to index composition"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "MASULIS和NG - 1995 - Overnight and daytime stock-return dynamics on the london stock-exchange - the impacts of big-bang a.md"
    ],
    "question_context": "The FTSE-100 stock index is a value-weighted arithmetic mean based on equity capitalization without dividend reinvestment. Prices are based on the midpoint of the best bid-and-ask quotes (the touch) displayed on the trading screen for constituent stocks, minimizing stale quote inclusion. The LSE does not face problems with delayed openings or artificial serial correlations because it reports live quotes rather than transaction prices, maintains large-quote depth without diverting large orders, and does not open trading with a call auction. Key formula: Cash-dividend adjustments are calculated as (cash dividend per share)/(beginning-of-period share price) × (stock's weight in the index), though these adjustments do not significantly alter estimates.",
    "category": "INCORRECT",
    "question_context_used": "The FTSE-100 stock index is a value-weighted arithmetic mean based on equity capitalization without dividend reinvestment. Prices are based on the midpoint of the best bid-and-ask quotes (the touch) displayed on the trading screen for constituent stocks, minimizing stale quote inclusion. The LSE does not face problems with delayed openings or artificial serial correlations because it reports live quotes rather than transaction prices, maintains large-quote depth without diverting large orders, and does not open trading with a call auction. Key formula: Cash-dividend adjustments are calculated as (cash dividend per share)/(beginning-of-period share price) × (stock's weight in the index), though these adjustments do not significantly alter estimates.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3201",
    "question": "Which of the following statements are true regarding the axioms and properties of stochastic choice functions with replicas?",
    "options": [
      "A) The Neutral Replication (NR) axiom implies that adding a replica of an alternative does not change the choice probabilities of non-replica alternatives.",
      "B) The Independence of Irrelevant Non-Replicas (IINR) axiom is a weaker form of IIA that holds only when adding alternatives that are not replicas of the considered options.",
      "C) The stochastic choice function ρ⋄ from Example 4 satisfies both NR and IINR.",
      "D) The Luce model with replicas can be represented as a multinomial logit model where the utility function is unique up to an additive constant."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "The-Luce-model-with-replicas_2023_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The Neutral Replication (NR) axiom asserts that for any menu A∈M, alternative x∈A, and corresponding non-replica z∈A∖[x], every replica y of z is neutral for x. Formally, NR is given by: \n\nρ(x,A)=ρ(x,A∪{y}).\n\nThe Independence of Irrelevant Non-Replicas (IINR) axiom is a weaker form of IIA that holds when adding alternatives that are not replicas of the considered options. It is defined as:\n\n(ρ(x,{x,y})/ρ(y,{x,y}))=(ρ(x,{x,y}∪A)/ρ(y,{x,y}∪A))\n\nfor all x,y∈X and A∈M such that A∩([x]∪[y])=∅.\n\nTheorem 7 states that a full support stochastic choice function ρ satisfies NR and IINR if and only if ρ is a Luce model with replicas. Moreover, the Luce-type values vAi can be replaced by exp(uAi), where uAi is the utility level generated by any replica of x, and the utility function u:X→R is unique up to an additive constant.",
    "category": "INCORRECT",
    "question_context_used": "The Neutral Replication (NR) axiom asserts that for any menu A∈M, alternative x∈A, and corresponding non-replica z∈A∖[x], every replica y of z is neutral for x. Formally, NR is given by: \n\nρ(x,A)=ρ(x,A∪{y}).\n\nThe Independence of Irrelevant Non-Replicas (IINR) axiom is a weaker form of IIA that holds when adding alternatives that are not replicas of the considered options. It is defined as:\n\n(ρ(x,{x,y})/ρ(y,{x,y}))=(ρ(x,{x,y}∪A)/ρ(y,{x,y}∪A))\n\nfor all x,y∈X and A∈M such that A∩([x]∪[y])=∅.\n\nTheorem 7 states that a full support stochastic choice function ρ satisfies NR and IINR if and only if ρ is a Luce model with replicas. Moreover, the Luce-type values vAi can be replaced by exp(uAi), where uAi is the utility level generated by any replica of x, and the utility function u:X→R is unique up to an additive constant.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4492",
    "question": "In the parametric analysis of SPF probabilistic forecasts, which of the following assumptions are imposed when fitting subjective distributions to forecasters' beliefs?",
    "options": [
      "A) The subjective distribution is always symmetric around its mean.",
      "B) The generalized Beta distribution is used when forecasts assign positive probability to at least three intervals.",
      "C) An isosceles triangle distribution is assumed when forecasts assign positive probability to one or two intervals.",
      "D) The support of the distribution is always restricted to the range of historical U.S. data."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "02-Comparing the Point Predictions and Subjective Probability Distributions of Professional Forec.md"
    ],
    "question_context": "The parametric analysis assumes that probabilistic beliefs are unimodal. When an SPF probabilistic forecast assigns positive probability to three or more intervals, the subjective distribution is assumed to be a member of the generalized Beta family. The generalized Beta distribution, which uses two parameters to describe the shape of beliefs and two more to give their support, is a flexible form that permits a distribution to have different values for its mean, median, and mode. For forecasts that assign positive probability to one or two intervals, an isosceles triangle distribution is assumed. The support parameters $l$ and $r$ are restricted to lie within the most extreme values that have actually occurred in the United States since 1930. For GDP growth, the range is $-0.13<l<r<0.19$, and for inflation, it is $-0.12<l<r<0.12$.",
    "category": "CORRECT",
    "question_context_used": "The parametric analysis assumes that probabilistic beliefs are unimodal. When an SPF probabilistic forecast assigns positive probability to three or more intervals, the subjective distribution is assumed to be a member of the generalized Beta family. The generalized Beta distribution, which uses two parameters to describe the shape of beliefs and two more to give their support, is a flexible form that permits a distribution to have different values for its mean, median, and mode. For forecasts that assign positive probability to one or two intervals, an isosceles triangle distribution is assumed. The support parameters $l$ and $r$ are restricted to lie within the most extreme values that have actually occurred in the United States since 1930. For GDP growth, the range is $-0.13<l<r<0.19$, and for inflation, it is $-0.12<l<r<0.12$.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-630",
    "question": "In the context of the index theorem for regular economies, which of the following conditions are necessary for the uniqueness of equilibrium?",
    "options": [
      "A) The index of every equilibrium is +1",
      "B) The economy is regular and all equilibria are isolated",
      "C) The excess demand function is upper-semi-continuous",
      "D) The activity analysis matrix A has no outputs without any inputs"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Multiplicity of Equilibria and Comparative Statics.md"
    ],
    "question_context": "The index theorem developed by Dierker [1972] for pure exchange economies can be extended to the model with production. We define the index of an equilibrium $\\hat{\\pmb{\\pi}}$ of a regular economy to be $\\mathbf{sgn}(\\mathbf{det}[I-D g(\\hat{\\pmb{\\pi}})])$. Kehoe [1980] proves the following theorem: \n\nTHEOREM 2. If $(\\xi,{\\cal A})$ is a regular economy, then \n\n$$\\begin{array}{r}{\\sum_{\\pi=g(\\pi)}\\mathrm{index}\\left(\\pi\\right)=+1.}\\end{array}$$\n\nThe most significant consequence of the index theorem is that it permits us to establish conditions sufficient for uniqueness of equilibria. If the parameters of an economy $(\\pmb{\\xi},\\pmb{A})$ are such that at every equilibrium the index is equal to $^\\texttt{+1}$ , then there is a unique equilibrium. Conversely, if an economy $(\\pmb{\\xi},\\pmb{A})$ has a unique equilibrium $\\hat{\\pmb{\\pi}}_{i}$ , it cannot be the case that $\\mathbf{index}(\\hat{\\pmb{\\pi}})=-\\textbf{1}$ . Thus, an economy with a unique equilibrium either is a critical economy or else is such that $\\mathrm{index}({\\hat{\\pi}})=+\\quad1.$ The condition that $\\mathrm{index}(\\hat{\\pi})=~+~1$ at every equilibrium is, therefore, necessary as well as sufficient for uniqueness in almost all cases.",
    "category": "CORRECT",
    "question_context_used": "The index theorem developed by Dierker [1972] for pure exchange economies can be extended to the model with production. We define the index of an equilibrium $\\hat{\\pmb{\\pi}}$ of a regular economy to be $\\mathbf{sgn}(\\mathbf{det}[I-D g(\\hat{\\pmb{\\pi}})])$. Kehoe [1980] proves the following theorem: \n\nTHEOREM 2. If $(\\xi,{\\cal A})$ is a regular economy, then \n\n$$\\begin{array}{r}{\\sum_{\\pi=g(\\pi)}\\mathrm{index}\\left(\\pi\\right)=+1.}\\end{array}$$\n\nThe most significant consequence of the index theorem is that it permits us to establish conditions sufficient for uniqueness of equilibria. If the parameters of an economy $(\\pmb{\\xi},\\pmb{A})$ are such that at every equilibrium the index is equal to $^\\texttt{+1}$ , then there is a unique equilibrium. Conversely, if an economy $(\\pmb{\\xi},\\pmb{A})$ has a unique equilibrium $\\hat{\\pmb{\\pi}}_{i}$ , it cannot be the case that $\\mathbf{index}(\\hat{\\pmb{\\pi}})=-\\textbf{1}$ . Thus, an economy with a unique equilibrium either is a critical economy or else is such that $\\mathrm{index}({\\hat{\\pi}})=+\\quad1.$ The condition that $\\mathrm{index}(\\hat{\\pi})=~+~1$ at every equilibrium is, therefore, necessary as well as sufficient for uniqueness in almost all cases.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4025",
    "question": "In the context of the three-step MB and EB methods for testing moment inequalities, which of the following conditions are necessary for the validity of the test as per Theorem 4.6?",
    "options": [
      "A) Existence of constants 0 < c1 < 1/2 and C1 > 0 such that (M_{n,3}^{3} ∨ M_{n,4}^{2} ∨ B_{n})^{2} log^{7/2}(pn) ≤ C1n^{1/2−c1}",
      "B) The condition log(1/(βn − φn)) ≤ C2 log n and φn log n ≥ c2 for some constants c2, C2 > 0",
      "C) The test statistic T^B is defined as T^B = max_{j∈Ĵ_B′} (√n μ̂_j / σ̂_j)",
      "D) The critical value c^{B,3S}(α) is calculated using the conditional (1 − α + 4βn)-quantile of W_{Ĵ_B ∩ Ĵ_B′′} given X_1^n"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Inference on Causal and Structural Parameters using Many Moment Inequalities.md"
    ],
    "question_context": "Theorem 4.6 establishes the validity of the three-step MB and EB methods under specific conditions. The theorem requires the existence of constants 0 < c1 < 1/2 and C1 > 0 such that (M_{n,3}^{3} ∨ M_{n,4}^{2} ∨ B_{n})^{2} log^{7/2}(pn) ≤ C1n^{1/2−c1} and ((M_{n,3}^{V})^{3} ∨ (M_{n,4}^{V})^{2} ∨ B_{n}^{V})^{2} log^{7/2}(prn) ≤ C1n^{1/2−c1}. Additionally, it requires log(1/(βn − φn)) ≤ C2 log n and φn log n ≥ c2 for some constants c2, C2 > 0. The test statistic T^B is defined as T^B = max_{j∈Ĵ_B′} (√n μ̂_j / σ̂_j), and the critical value c^{B,3S}(α) is calculated using the conditional (1 − α + 4βn)-quantile of W_{Ĵ_B ∩ Ĵ_B′′} given X_1^n. The theorem ensures that under H0, P(T^B > c^{B,3S}(α)) ≤ α + Cn^{-c}, with the bound holding uniformly over all distributions satisfying the specified conditions.",
    "category": "INCORRECT",
    "question_context_used": "Theorem 4.6 establishes the validity of the three-step MB and EB methods under specific conditions. The theorem requires the existence of constants 0 < c1 < 1/2 and C1 > 0 such that (M_{n,3}^{3} ∨ M_{n,4}^{2} ∨ B_{n})^{2} log^{7/2}(pn) ≤ C1n^{1/2−c1} and ((M_{n,3}^{V})^{3} ∨ (M_{n,4}^{V})^{2} ∨ B_{n}^{V})^{2} log^{7/2}(prn) ≤ C1n^{1/2−c1}. Additionally, it requires log(1/(βn − φn)) ≤ C2 log n and φn log n ≥ c2 for some constants c2, C2 > 0. The test statistic T^B is defined as T^B = max_{j∈Ĵ_B′} (√n μ̂_j / σ̂_j), and the critical value c^{B,3S}(α) is calculated using the conditional (1 − α + 4βn)-quantile of W_{Ĵ_B ∩ Ĵ_B′′} given X_1^n. The theorem ensures that under H0, P(T^B > c^{B,3S}(α)) ≤ α + Cn^{-c}, with the bound holding uniformly over all distributions satisfying the specified conditions.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-934",
    "question": "Which of the following statements are true regarding the continuous ranked probability score (CRPS) and its weighted versions?",
    "options": [
      "A) The threshold-weighted CRPS reduces to the unweighted score when the weight function u(z) ≡ 1.",
      "B) The quantile score QSα(F⁻¹(α), y) equals the loss function L(x, y) = 2(1 - α)|y - x| for overpredictions and L(x, y) = 2α|y - x| for underpredictions.",
      "C) The CRPS is strictly proper only for forecast densities with finite second moments.",
      "D) The threshold-weighted CRPS with an integrable weight function does not require moment conditions for asymptotic normality of the test statistic tn."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "06-Comparing_Density_Forecasts_Using_Threshold-_and_Quantile-Weighted_Scoring_Rules.md"
    ],
    "question_context": "The continuous ranked probability score (CRPS) can be defined in three equivalent ways, including the threshold decomposition and quantile score representation. Key formula for the threshold-weighted CRPS is: \n\n$$\\mathrm{S}(f,y)=\\int_{-\\infty}^{\\infty}\\mathrm{PS}(F(z),\\mathbb{I}\\{y\\leq z\\})u(z)\\mathrm{d}z,$$\n\nwhere $u$ is a nonnegative weight function; if $u\\equiv1$, this reduces to the unweighted score. The quantile-weighted version is:\n\n$$\\mathrm{S}(f,y)=\\int_{0}^{1}\\mathrm{QS}_{\\alpha}(F^{-1}(\\alpha),y)\\nu(\\alpha)\\mathrm{d}\\alpha,$$\n\nwhere $\\nu$ is a nonnegative weight function on the unit interval. The CRPS is strictly proper within the class of forecast densities with finite first moment. For threshold weighting with integrable weight functions, the moment condition can be dropped for asymptotic normality of the test statistic $t_n$.",
    "category": "INCORRECT",
    "question_context_used": "The continuous ranked probability score (CRPS) can be defined in three equivalent ways, including the threshold decomposition and quantile score representation. Key formula for the threshold-weighted CRPS is: \n\n$$\\mathrm{S}(f,y)=\\int_{-\\infty}^{\\infty}\\mathrm{PS}(F(z),\\mathbb{I}\\{y\\leq z\\})u(z)\\mathrm{d}z,$$\n\nwhere $u$ is a nonnegative weight function; if $u\\equiv1$, this reduces to the unweighted score. The quantile-weighted version is:\n\n$$\\mathrm{S}(f,y)=\\int_{0}^{1}\\mathrm{QS}_{\\alpha}(F^{-1}(\\alpha),y)\\nu(\\alpha)\\mathrm{d}\\alpha,$$\n\nwhere $\\nu$ is a nonnegative weight function on the unit interval. The CRPS is strictly proper within the class of forecast densities with finite first moment. For threshold weighting with integrable weight functions, the moment condition can be dropped for asymptotic normality of the test statistic $t_n$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1785",
    "question": "Which of the following mechanisms in the model generate differential responses of sectoral prices to aggregate and sector-specific shocks?",
    "options": [
      "A) Pricing interactions from intermediate inputs (δ > 0)",
      "B) Labor market segmentation with finite Frisch elasticity (φ > 0)",
      "C) Exogenous nominal consumption (m_t)",
      "D) Monetary policy responses via a Taylor rule"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Carvalho-SectoralPriceFacts-2021.md"
    ],
    "question_context": "The model identifies three mechanisms that produce differential price responses: (i) intermediate inputs (δ > 0), (ii) labor market segmentation (φ > 0), and (iii) monetary policy responses. Under strategic neutrality where δ = 0, φ = 0, and m_t is exogenous, the frictionless optimal price is given by: \n\n$$ p_{k,t}^{**}(i)=m_{t}-a_{t}-a_{k,t}. $$\n\nIntroducing intermediate inputs (δ > 0) modifies this to:\n\n$$ p_{k,t}^{**}(i)=(1-\\delta)m_{t}-a_{t}-a_{k,t}+\\delta p_{t}. $$\n\nLabor market segmentation (φ > 0) further complicates the pricing dynamics by introducing sector-specific wage effects:\n\n$$ p_{k,t}^{**}(i)=(1+\\varphi)m_{t}+\\varphi d_{k,t}-(1+\\varphi)a_{t}-(1+\\varphi)a_{k,t}-\\varphi p_{t} + \\varphi\\eta p_{t}-\\varphi\\eta p_{k,t}. $$",
    "category": "CORRECT",
    "question_context_used": "The model identifies three mechanisms that produce differential price responses: (i) intermediate inputs (δ > 0), (ii) labor market segmentation (φ > 0), and (iii) monetary policy responses. Under strategic neutrality where δ = 0, φ = 0, and m_t is exogenous, the frictionless optimal price is given by: \n\n$$ p_{k,t}^{**}(i)=m_{t}-a_{t}-a_{k,t}. $$\n\nIntroducing intermediate inputs (δ > 0) modifies this to:\n\n$$ p_{k,t}^{**}(i)=(1-\\delta)m_{t}-a_{t}-a_{k,t}+\\delta p_{t}. $$\n\nLabor market segmentation (φ > 0) further complicates the pricing dynamics by introducing sector-specific wage effects:\n\n$$ p_{k,t}^{**}(i)=(1+\\varphi)m_{t}+\\varphi d_{k,t}-(1+\\varphi)a_{t}-(1+\\varphi)a_{k,t}-\\varphi p_{t} + \\varphi\\eta p_{t}-\\varphi\\eta p_{k,t}. $$",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4564",
    "question": "Based on the National Longitudinal Survey data analysis, which of the following factors contribute significantly to the attrition of sibling pair observations in the dataset?",
    "options": [
      "A) Missing IQ scores and late wage data due to sample attrition",
      "B) Oversampling of non-white respondents in the South",
      "C) Non-random assignment of individuals to brother-brother or sister-sister pairs",
      "D) Requirement that both siblings must have completed schooling and have both early and late wage data"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Wages, Schooling and IQ of Brothers and Sisters Do the Family Factors Differ.md"
    ],
    "question_context": "The dataset originates from the National Longitudinal Survey of Young Men (1966-1980) and Young Women (1968-1980), with initial samples of about 5000 respondents each. Attrition reduced these numbers to approximately 4000 by the end of the survey period due to reasons like death, inability to locate, and refusal to answer. The text states: 'By the time we ask that both siblings should have completed school, had observations on both an early and later wage and data on IQ scores, we are down to less than one third of the original number, about 520 pairs or 1040 individuals.' Key formula for data completeness: $\\text{Complete pairs} = \\text{Original sample} \\times \\text{Completeness conditions}$. The major attrition occurs due to missing IQ scores and missing late wage (due to attrition from the sample, late school leaving, or non labor force participation). Overall attrition is slightly higher for males than females.",
    "category": "CORRECT",
    "question_context_used": "The dataset originates from the National Longitudinal Survey of Young Men (1966-1980) and Young Women (1968-1980), with initial samples of about 5000 respondents each. Attrition reduced these numbers to approximately 4000 by the end of the survey period due to reasons like death, inability to locate, and refusal to answer. The text states: 'By the time we ask that both siblings should have completed school, had observations on both an early and later wage and data on IQ scores, we are down to less than one third of the original number, about 520 pairs or 1040 individuals.' Key formula for data completeness: $\\text{Complete pairs} = \\text{Original sample} \\times \\text{Completeness conditions}$. The major attrition occurs due to missing IQ scores and missing late wage (due to attrition from the sample, late school leaving, or non labor force participation). Overall attrition is slightly higher for males than females.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3620",
    "question": "In the described model of equilibrium pricing under perfect information, which of the following statements are true regarding the firms' pricing strategies and consumer search behavior?",
    "options": [
      "A) When firms have identical production costs, the equilibrium price is the common monopoly price corresponding to their cost level.",
      "B) The high-cost firm's price is always equal to its monopoly price, regardless of the search cost.",
      "C) The reservation price p*(1) is decreasing in the search cost s.",
      "D) When s < s*(1), the idiosyncratically high-cost firm's price is constrained by the threat of consumer search."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Search with Learning and Price Adjustment Dynamics.md"
    ],
    "question_context": "The model describes an industry with two infinitely lived risk-neutral firms supplying a homogeneous product at constant marginal cost, competing in prices. Consumers are born at even-numbered periods, participate for two periods, and have downward-sloping demand curves. The firms' production costs randomly switch between low (c) and high (c̄) states, with the industry state following a Markov process. Key formulas include the reservation price p*(1) solving [V(p) - V(p*(1))] = s, and the threshold search cost s*(1) solving V(p) - V(p̄) = s*(1). When firms have identical costs, the equilibrium price is the common monopoly price (p if cost is low, p̄ if high). When costs differ, the high-cost firm's price is constrained by consumer search threat if s < s*(1), charging min{p*(1), p̄}.",
    "category": "INCORRECT",
    "question_context_used": "The model describes an industry with two infinitely lived risk-neutral firms supplying a homogeneous product at constant marginal cost, competing in prices. Consumers are born at even-numbered periods, participate for two periods, and have downward-sloping demand curves. The firms' production costs randomly switch between low (c) and high (c̄) states, with the industry state following a Markov process. Key formulas include the reservation price p*(1) solving [V(p) - V(p*(1))] = s, and the threshold search cost s*(1) solving V(p) - V(p̄) = s*(1). When firms have identical costs, the equilibrium price is the common monopoly price (p if cost is low, p̄ if high). When costs differ, the high-cost firm's price is constrained by consumer search threat if s < s*(1), charging min{p*(1), p̄}.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2715",
    "question": "Based on the symposium papers, which of the following statements accurately describe the effects of temporary employment policies in different countries?",
    "options": [
      "A) In Britain, temporary jobs lead to lower wages and reduced training opportunities compared to permanent jobs.",
      "B) In France, partial reforms increasing fixed-term contracts improved welfare for young workers by reducing unemployment.",
      "C) In Sweden, the rise in temporary work during the 1990s was primarily driven by legislative changes and sectoral shifts.",
      "D) In Spain, labor market reforms reduced temporary employment in the private sector but increased it in the public sector."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Symposium_on_Temporary_Work_Introduction.md"
    ],
    "question_context": "The symposium papers analyze the effects of temporary employment policies across different European countries. In Britain, temporary jobs are associated with lower wages, reduced training, and income penalties for men, while women eventually catch up. France's partial reforms led to higher turnover and worsened welfare for young workers. Sweden's rise in temporary work was triggered by macroeconomic shocks rather than legislative changes. Spain's segmented labor market saw a reduction in private sector temporary jobs offset by an increase in public sector temporary employment. Key formula: The scarring effect for men in temporary jobs is given by \\( \\Delta Y = \\beta_0 + \\beta_1 T + \\epsilon \\), where \\( T \\) represents temporary work experience.",
    "category": "CORRECT",
    "question_context_used": "The symposium papers analyze the effects of temporary employment policies across different European countries. In Britain, temporary jobs are associated with lower wages, reduced training, and income penalties for men, while women eventually catch up. France's partial reforms led to higher turnover and worsened welfare for young workers. Sweden's rise in temporary work was triggered by macroeconomic shocks rather than legislative changes. Spain's segmented labor market saw a reduction in private sector temporary jobs offset by an increase in public sector temporary employment. Key formula: The scarring effect for men in temporary jobs is given by \\( \\Delta Y = \\beta_0 + \\beta_1 T + \\epsilon \\), where \\( T \\) represents temporary work experience.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4792",
    "question": "In the context of games with atomless moves by Nature, which of the following conditions are necessary for two strategy profiles to be payoff-equivalent at a given history?",
    "options": [
      "A) The expected discounted payoffs from both profiles must be equal for every active player at every action profile.",
      "B) The strategy profiles must coincide in all subgames following the given history.",
      "C) The continuation payoffs must be independent of public signals in all preceding periods.",
      "D) Nature's moves must be atomless at the given history."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Subgame_perfect equilibrium in games with almost perfect information_ dispensing with public randomization.md"
    ],
    "question_context": "Given strategy profiles $\\hat{f}$ and $\\hat{f^{\\prime}}$ in the extended game $\\hat{G}$, we say that $\\hat{f}^{\\prime}$ is payoff-equivalent to $\\hat{f}$ at $\\hat{x}_{t-1}$ if for all $i\\in N$ and all $y\\in\\times_{i\\in N}A_{t,i}(x_{t-1})$, we have \n\n$$\n\\begin{array}{r l}&{\\displaystyle\\int_{z}\\biggl[u_{t,i}(x_{t-1},y,z)+\\delta_{i}\\int_{\\omega}\\hat{U}_{t+1,i}\\bigl((x_{t-1},y,z,\\omega),\\hat{f}^{\\prime}\\bigr)\\lambda(d\\omega)\\biggr]\\varphi_{t}(x_{t-1})(d z)}\\ &{\\quad=\\displaystyle\\int_{z}\\biggl[u_{t,i}(x_{t-1},y,z)+\\delta_{i}\\int_{\\omega}\\hat{U}_{t+1,i}\\bigl((x_{t-1},y,z,\\omega),\\hat{f}\\bigr)\\lambda(d\\omega)\\biggr]\\varphi_{t}(x_{t-1})(d z),}\\end{array}\n$$ \n\nso that the expected discounted payoffs from $\\hat{f}^{\\prime}$ and $\\hat{f}$, calculated at $\\hat{x}_{t-1}$, from every action profile are the same for every active player. In addition, as in HRR, let $E_{t+1}(x_{t})$ be the set of SPE payoffs in any subgame $\\hat{G}(\\hat{x}_{t})$ of $\\hat{G}$ such that the history of actions in $\\hat{\\boldsymbol{x}}_{t}$ is $x_{t}$. Because the signals $\\omega_{1},\\ldots,\\omega_{t}$ are payoff irrelevant, this set is well-defined, and by Theorem 5 of HRR, the correspondence $E_{t+1}\\colon X_{t}\\stackrel{}{\\Rightarrow}\\mathbb{R}^{n}$ so-defined has a closed graph, and thus is lower measurable (Theorem 18.20, Aliprantis and Border (2006)).",
    "category": "INCORRECT",
    "question_context_used": "Given strategy profiles $\\hat{f}$ and $\\hat{f^{\\prime}}$ in the extended game $\\hat{G}$, we say that $\\hat{f}^{\\prime}$ is payoff-equivalent to $\\hat{f}$ at $\\hat{x}_{t-1}$ if for all $i\\in N$ and all $y\\in\\times_{i\\in N}A_{t,i}(x_{t-1})$, we have \n\n$$\n\\begin{array}{r l}&{\\displaystyle\\int_{z}\\biggl[u_{t,i}(x_{t-1},y,z)+\\delta_{i}\\int_{\\omega}\\hat{U}_{t+1,i}\\bigl((x_{t-1},y,z,\\omega),\\hat{f}^{\\prime}\\bigr)\\lambda(d\\omega)\\biggr]\\varphi_{t}(x_{t-1})(d z)}\\ &{\\quad=\\displaystyle\\int_{z}\\biggl[u_{t,i}(x_{t-1},y,z)+\\delta_{i}\\int_{\\omega}\\hat{U}_{t+1,i}\\bigl((x_{t-1},y,z,\\omega),\\hat{f}\\bigr)\\lambda(d\\omega)\\biggr]\\varphi_{t}(x_{t-1})(d z),}\\end{array}\n$$ \n\nso that the expected discounted payoffs from $\\hat{f}^{\\prime}$ and $\\hat{f}$, calculated at $\\hat{x}_{t-1}$, from every action profile are the same for every active player. In addition, as in HRR, let $E_{t+1}(x_{t})$ be the set of SPE payoffs in any subgame $\\hat{G}(\\hat{x}_{t})$ of $\\hat{G}$ such that the history of actions in $\\hat{\\boldsymbol{x}}_{t}$ is $x_{t}$. Because the signals $\\omega_{1},\\ldots,\\omega_{t}$ are payoff irrelevant, this set is well-defined, and by Theorem 5 of HRR, the correspondence $E_{t+1}\\colon X_{t}\\stackrel{}{\\Rightarrow}\\mathbb{R}^{n}$ so-defined has a closed graph, and thus is lower measurable (Theorem 18.20, Aliprantis and Border (2006)).",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2866",
    "question": "According to the methodological pluralism argument presented in Gordon Foxall's 'Marketing Psychology', which of the following are claimed benefits of adopting multiple research paradigms in consumer psychology?",
    "options": [
      "A) Enhanced understanding of marketing behavior through paradigm interaction",
      "B) Democratic intellectual inquiry where all research lenses are considered equal",
      "C) Guaranteed dominance of radical behaviorism over social-cognitive approaches",
      "D) Dynamic engine for new knowledge acquisition in marketing research"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Book_Notes.md"
    ],
    "question_context": "Gordon Foxall argues clearly and eloquently for methodological pluralism in research, portraying it as offering the dynamic engine for the acquisition of new knowledge and the enhancement of understanding. Chapter 1, 'Only diverge', outlines the general argument succinctly but fails to support the claim that 'Intellectual inquiry is democratic: all lenses are equal' (pp. 1-2). The rest of the book seeks to illuminate the general argument by reference to two competing research traditions in consumer psychology: the current dominant paradigm - the social-cognitive approach, reprised in chapters 2 and 3 - and radical behaviourism and its embodiment in the behavioural perspective model (BPM) - the paradigm in the wings, reprised in chapters 4 and 5. Furnished with insights gathered from the interaction of the two paradigms Foxall, in chapters 6, 7, and 8, sketches out the implications for, and the enhanced understanding produced with respect to the marketing firm, marketing behaviour, and marketing relationships.",
    "category": "INCORRECT",
    "question_context_used": "Gordon Foxall argues clearly and eloquently for methodological pluralism in research, portraying it as offering the dynamic engine for the acquisition of new knowledge and the enhancement of understanding. Chapter 1, 'Only diverge', outlines the general argument succinctly but fails to support the claim that 'Intellectual inquiry is democratic: all lenses are equal' (pp. 1-2). The rest of the book seeks to illuminate the general argument by reference to two competing research traditions in consumer psychology: the current dominant paradigm - the social-cognitive approach, reprised in chapters 2 and 3 - and radical behaviourism and its embodiment in the behavioural perspective model (BPM) - the paradigm in the wings, reprised in chapters 4 and 5. Furnished with insights gathered from the interaction of the two paradigms Foxall, in chapters 6, 7, and 8, sketches out the implications for, and the enhanced understanding produced with respect to the marketing firm, marketing behaviour, and marketing relationships.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1024",
    "question": "In the context of the business cycle model with stochastic volatility, which of the following statements are correct regarding the measurement equation and state variables?",
    "options": [
      "A) The measurement equation includes a term for the change in the relative price of investment, which is directly related to the negative log difference of the investment-specific technological change.",
      "B) The state variables of the model include the differences in volatilities of the shocks with respect to their mean, which households track for optimal decision-making.",
      "C) The measurement errors in the model are introduced solely to avoid stochastic singularity and have no other substantive role in the estimation process.",
      "D) The policy functions for rescaled investment and labor are derived from a first-order perturbation around the deterministic steady state."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Estimating Macroeconomic Models A Likelihood Approach.md"
    ],
    "question_context": "The measurement equation for the business cycle model is given by:\n\n$$\n\\left(\\begin{array}{c}{\\Delta\\log p_{t}}\\\\ {\\Delta\\log y_{t}}\\\\ {\\Delta\\log x_{t}}\\\\ {\\log l_{t}}\\end{array}\\right)=\\left(\\begin{array}{c}{-\\theta}\\\\ {\\frac{\\gamma+\\alpha\\theta}{1-\\alpha}}\\\\ {\\frac{\\gamma+\\alpha\\theta}{1-\\alpha}}\\\\ {\\log L_{\\mathrm{ss}}+\\Psi_{l3}}\\end{array}\\right)\n$$\n\nThis equation accounts for the change in the relative price of investment, real output per capita growth, real gross investment per capita growth, and hours worked per capita. The state variables include the differences in volatilities of the shocks with respect to their mean, which are crucial for households' optimal decisions. The policy functions for rescaled investment and labor are derived from a second-order perturbation around the deterministic steady state, not a first-order one.",
    "category": "CORRECT",
    "question_context_used": "The measurement equation for the business cycle model is given by:\n\n$$\n\\left(\\begin{array}{c}{\\Delta\\log p_{t}}\\\\ {\\Delta\\log y_{t}}\\\\ {\\Delta\\log x_{t}}\\\\ {\\log l_{t}}\\end{array}\\right)=\\left(\\begin{array}{c}{-\\theta}\\\\ {\\frac{\\gamma+\\alpha\\theta}{1-\\alpha}}\\\\ {\\frac{\\gamma+\\alpha\\theta}{1-\\alpha}}\\\\ {\\log L_{\\mathrm{ss}}+\\Psi_{l3}}\\end{array}\\right)\n$$\n\nThis equation accounts for the change in the relative price of investment, real output per capita growth, real gross investment per capita growth, and hours worked per capita. The state variables include the differences in volatilities of the shocks with respect to their mean, which are crucial for households' optimal decisions. The policy functions for rescaled investment and labor are derived from a second-order perturbation around the deterministic steady state, not a first-order one.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2446",
    "question": "In the context of non-linear dynamic models with exponential and LSTR DGPs, which of the following statements are true regarding the power and performance of the Index-test compared to other non-linearity tests?",
    "options": [
      "A) The Index-test has higher power than the RESET test when detecting non-linearities in exponential DGPs under collinearity.",
      "B) The Volterra-expansion based tests (V23 and PCV23) have no power to detect exponential DGPs regardless of the regressors' collinearity.",
      "C) The Index-test outperforms the RESET test in LSTR DGPs for small n due to its ability to capture non-linearities via polynomial approximations.",
      "D) Under orthogonality, the Index-test's power for detecting exponential DGPs is higher than that of the Volterra-expansion based tests."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "A-low-dimension-portmanteau-test-for-non-lineari_2010_Journal-of-Econometric.md"
    ],
    "question_context": "The text discusses various non-linear models, including exponential and Logistic Smooth Transition Regression (LSTR) DGPs, and evaluates the performance of different non-linearity tests like the Index-test, RESET test, and Volterra-expansion based tests. Key findings include that the Index-test has higher power than the RESET test under collinearity for exponential DGPs, and polynomial approximations in the Index-test effectively capture non-linearities in LSTR models. The formula for the exponential DGP approximation is given by:\n\n$$\\begin{array}{l}{{y_{t}=\\beta_{1}\\left\\{x_{1,t}\\mathrm{e}^{-\\left|x_{2,t}\\right|}\\right\\}+\\epsilon_{t}}}\\\\ {{\\mathrm{}\\approx\\beta_{1}\\left(x_{1,t}-x_{1,t}\\left|x_{2,t}\\right|+\\displaystyle\\frac{1}{2}x_{1,t}x_{2,t}^{2}-\\displaystyle\\frac{1}{6}x_{1,t}\\left|x_{2,t}^{3}\\right|\\right)+v_{t}.}}\\end{array}$$\n\nAdditionally, the Taylor approximation for the LSTR model results in a highly parameterized form, but the Index-test shows degrees-of-freedom gains and better performance for small n.",
    "category": "INCORRECT",
    "question_context_used": "The text discusses various non-linear models, including exponential and Logistic Smooth Transition Regression (LSTR) DGPs, and evaluates the performance of different non-linearity tests like the Index-test, RESET test, and Volterra-expansion based tests. Key findings include that the Index-test has higher power than the RESET test under collinearity for exponential DGPs, and polynomial approximations in the Index-test effectively capture non-linearities in LSTR models. The formula for the exponential DGP approximation is given by:\n\n$$\\begin{array}{l}{{y_{t}=\\beta_{1}\\left\\{x_{1,t}\\mathrm{e}^{-\\left|x_{2,t}\\right|}\\right\\}+\\epsilon_{t}}}\\\\ {{\\mathrm{}\\approx\\beta_{1}\\left(x_{1,t}-x_{1,t}\\left|x_{2,t}\\right|+\\displaystyle\\frac{1}{2}x_{1,t}x_{2,t}^{2}-\\displaystyle\\frac{1}{6}x_{1,t}\\left|x_{2,t}^{3}\\right|\\right)+v_{t}.}}\\end{array}$$\n\nAdditionally, the Taylor approximation for the LSTR model results in a highly parameterized form, but the Index-test shows degrees-of-freedom gains and better performance for small n.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4087",
    "question": "Under Assumptions 1-8 and $N h^{5} \\rightarrow 0$, which of the following statements are true regarding the asymptotic properties of the estimated counterfactual distribution function $\\widehat{F}_{t,h}(y)$?",
    "options": [
      "A) $\\sqrt{N h}\\left\\{\\widehat{F}_{t,h}(\\cdot)-F_{t}(\\cdot)\\right\\}$ converges weakly to a mean-zero Gaussian process.",
      "B) The covariance function $\\Omega_{t}(y_{1},y_{2})$ depends on the kernel function $K(\\cdot)$ through $\\kappa_{02}$.",
      "C) $\\widehat{F}_{t,h}(y)$ is less efficient than the infeasible estimator $\\overline{F}_{t,h}(y)$.",
      "D) The asymptotic variance of $\\widehat{F}_{t,h}(y)$ is minimized when $h$ shrinks at rate $N^{-1/5}$."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Estimation-and-inference-for-the-counterfactual-distributio_2022_Journal-of-.md"
    ],
    "question_context": "Theorem 2 states that under Assumptions 1-8 and $N h^{5} \\rightarrow 0$, for any fixed $t \\in \\mathcal{T}$, \n\n$$\\sqrt{N h}\\left\\{\\widehat{F}_{t,h}(\\cdot)-F_{t}(\\cdot)\\right\\} \\Rightarrow \\psi_{t}(\\cdot),$$\n\nwhere $\\psi_{t}(\\cdot)$ is a mean-zero Gaussian process with covariance function \n\n$$\\Omega_{t}(y_{1},y_{2}) = \\frac{\\kappa_{02}}{f_{T}(t)} \\times \\mathbb{E}\\left[\\pi_{0}(T_{i},X_{i})^{2}\\left\\{I(Y_{i} \\leq y_{1})-F_{Y|T,X}(y_{1}|T_{i},X_{i})\\right\\}\\left\\{I(Y_{i} \\leq y_{2})-F_{Y|T,X}(y_{2}|T_{i},X_{i})\\right\\}\\bigg|T_{i}=t\\right].$$\n\nHere, $\\kappa_{02} = \\int K^{2}(u) du$ is a property of the kernel function. Theorem 3 further shows that $\\widehat{F}_{t,h}(y)$ is more efficient than the infeasible estimator $\\overline{F}_{t,h}(y)$, with $\\Omega_{t}(y,y) < \\overline{\\Omega}_{t}(y,y)$. The bandwidth condition $N h^{5} \\rightarrow 0$ ensures that the bias term is asymptotically negligible, but does not specify an optimal rate for $h$.",
    "category": "CORRECT",
    "question_context_used": "Theorem 2 states that under Assumptions 1-8 and $N h^{5} \\rightarrow 0$, for any fixed $t \\in \\mathcal{T}$, \n\n$$\\sqrt{N h}\\left\\{\\widehat{F}_{t,h}(\\cdot)-F_{t}(\\cdot)\\right\\} \\Rightarrow \\psi_{t}(\\cdot),$$\n\nwhere $\\psi_{t}(\\cdot)$ is a mean-zero Gaussian process with covariance function \n\n$$\\Omega_{t}(y_{1},y_{2}) = \\frac{\\kappa_{02}}{f_{T}(t)} \\times \\mathbb{E}\\left[\\pi_{0}(T_{i},X_{i})^{2}\\left\\{I(Y_{i} \\leq y_{1})-F_{Y|T,X}(y_{1}|T_{i},X_{i})\\right\\}\\left\\{I(Y_{i} \\leq y_{2})-F_{Y|T,X}(y_{2}|T_{i},X_{i})\\right\\}\\bigg|T_{i}=t\\right].$$\n\nHere, $\\kappa_{02} = \\int K^{2}(u) du$ is a property of the kernel function. Theorem 3 further shows that $\\widehat{F}_{t,h}(y)$ is more efficient than the infeasible estimator $\\overline{F}_{t,h}(y)$, with $\\Omega_{t}(y,y) < \\overline{\\Omega}_{t}(y,y)$. The bandwidth condition $N h^{5} \\rightarrow 0$ ensures that the bias term is asymptotically negligible, but does not specify an optimal rate for $h$.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4145",
    "question": "In the context of generational differences in female occupational attainment during the 1970s, which of the following factors were identified as significant contributors to changes in women's opportunities?",
    "options": [
      "A) Legislative changes such as the Equal Pay Act and Title IX",
      "B) Shifts in societal attitudes towards gender roles",
      "C) Increased access to higher education for women",
      "D) Technological advancements in traditionally male-dominated industries"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Zalokar-GenerationalDifferencesFemale-1986.md"
    ],
    "question_context": "The study examines whether the 1970s marked a significant turning point in women's occupational opportunities. Key factors considered include legislative changes like the Equal Pay Act and Title IX, which aimed to reduce gender discrimination in pay and education. Additionally, shifts in societal attitudes towards gender roles and increased access to higher education for women played crucial roles in altering the occupational landscape. The analysis focuses on generational cohorts to identify persistent versus transitory changes in occupational attainment.",
    "category": "CORRECT",
    "question_context_used": "The study examines whether the 1970s marked a significant turning point in women's occupational opportunities. Key factors considered include legislative changes like the Equal Pay Act and Title IX, which aimed to reduce gender discrimination in pay and education. Additionally, shifts in societal attitudes towards gender roles and increased access to higher education for women played crucial roles in altering the occupational landscape. The analysis focuses on generational cohorts to identify persistent versus transitory changes in occupational attainment.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3718",
    "question": "Based on the empirical findings regarding marginal cost and markups in manufacturing, which of the following statements are supported by the evidence?",
    "options": [
      "A) Marginal cost is procyclical due to adjustment costs and overtime pay",
      "B) Prices move countercyclically despite procyclical marginal costs",
      "C) Wage stickiness is the primary driver of business cycle fluctuations",
      "D) Goods market imperfections play a significant role in cyclical markup variation"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Bils-CyclicalBehaviorMarginal-1987.md"
    ],
    "question_context": "Marginal cost is very procyclical. Using two-digit manufacturing data for after 1956, 1 find that a 10 percent short-run increase in production-worker employment was associated with a 2.4 percent increase in nominal marginal cost. The major cause is that employment is not perfectly fexible. In booms firms must incur a high 'adjustment' cost if they expand employment, or considerable overtime pay if they expand hours per worker. Prices did not respond to the cyclical movement in marginal cost; in fact, prices were countercyclical. Markups over marginal cost decline by 3.3 percent with a 10 percent expansion.\n\nThis evidence is clearly inconsistent with a perfectly competitive view of manufacturing. It is also inconsistent with the view that wage stickiness is an important cause of the business cycle. Even if wage schedules are not cyclically sensitive, there is much cyclical variation in the marginal cost of labor due to variation in average hours. This implies that imperfections in goods markets play a primary role in the cycle.",
    "category": "CORRECT",
    "question_context_used": "Marginal cost is very procyclical. Using two-digit manufacturing data for after 1956, 1 find that a 10 percent short-run increase in production-worker employment was associated with a 2.4 percent increase in nominal marginal cost. The major cause is that employment is not perfectly fexible. In booms firms must incur a high 'adjustment' cost if they expand employment, or considerable overtime pay if they expand hours per worker. Prices did not respond to the cyclical movement in marginal cost; in fact, prices were countercyclical. Markups over marginal cost decline by 3.3 percent with a 10 percent expansion.\n\nThis evidence is clearly inconsistent with a perfectly competitive view of manufacturing. It is also inconsistent with the view that wage stickiness is an important cause of the business cycle. Even if wage schedules are not cyclically sensitive, there is much cyclical variation in the marginal cost of labor due to variation in average hours. This implies that imperfections in goods markets play a primary role in the cycle.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-85",
    "question": "Under what conditions does the positive-part Stein-rule (PSR) estimator dominate the Stein-rule (SR) estimator in terms of predictive risk when relevant regressors are omitted?",
    "options": [
      "A) When the model is correctly specified and the noncentrality parameters λ₁ and λ₂ are minimized",
      "B) When the specification error results from omitted variables and the sufficient condition derived in Section 4 is satisfied",
      "C) When the pretest estimator reduces to the OLS estimator (c=∞ and p=0)",
      "D) When the doubly noncentral F-distribution of F has degrees of freedom k and ν with λ₁ = λ₂ = 0"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "OHTANI - 1993 - A COMPARISON OF THE STEIN-RULE AND POSITIVE-PART STEIN-RULE ESTIMATORS IN A MISSPECIFIED LINEAR-REGR.md"
    ],
    "question_context": "The paper examines the performance of the predictive risk of the Steinrule (SR) and positive-part Stein-rule (PSR) estimators when relevant regressors are omitted. The exact formula of the predictive risk of the PSR estimator is derived, and the sufficient condition for the PSR estimator to dominate the SR estimator under a specification error is given. The true linear regression model is specified as: \n\n$$ y=X_{1}\\beta_{1}+X_{2}\\beta_{2}+\\epsilon,~\\epsilon\\sim N(0,\\sigma^{2}I_{n}), $$\n\nwhere $X_{2}$ is mistakenly omitted. The PSR estimator is written as:\n\n$$ b_{1(\\mathrm{PSR})}=I_{(a,\\infty)}(F)(1-a/F)b_{1}, $$\n\nwhere $F$ follows a doubly noncentral $F$ distribution with noncentrality parameters $\\lambda_{1}$ and $\\lambda_{2}$. The sufficient condition for PSR dominance over SR is derived in Section 4, and numerical computations suggest PSR performs best even with omitted variables.",
    "category": "INCORRECT",
    "question_context_used": "The paper examines the performance of the predictive risk of the Steinrule (SR) and positive-part Stein-rule (PSR) estimators when relevant regressors are omitted. The exact formula of the predictive risk of the PSR estimator is derived, and the sufficient condition for the PSR estimator to dominate the SR estimator under a specification error is given. The true linear regression model is specified as: \n\n$$ y=X_{1}\\beta_{1}+X_{2}\\beta_{2}+\\epsilon,~\\epsilon\\sim N(0,\\sigma^{2}I_{n}), $$\n\nwhere $X_{2}$ is mistakenly omitted. The PSR estimator is written as:\n\n$$ b_{1(\\mathrm{PSR})}=I_{(a,\\infty)}(F)(1-a/F)b_{1}, $$\n\nwhere $F$ follows a doubly noncentral $F$ distribution with noncentrality parameters $\\lambda_{1}$ and $\\lambda_{2}$. The sufficient condition for PSR dominance over SR is derived in Section 4, and numerical computations suggest PSR performs best even with omitted variables.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-822",
    "question": "In the context of the simulation study on finite-sample properties of estimators for the quadratic model, which of the following statements are true regarding the performance of the estimators?",
    "options": [
      "A) The least squares estimator $\\hat{\\beta_{i}}$ has a small variance but a very large bias, making it practically useless even when the error variance is small.",
      "B) The ordinary instrumental variable estimator $\\hat{\\beta}$ has a bias that decreases with sample size $b$ and decreases with the curvature measure $|\\phi_{n}|=\\beta\\sigma_{u u}$.",
      "C) The two-stage instrumental variable estimators $\tilde{\\beta_{1}}$ and $\tilde{\\beta_{2}}$ have biases that are considerably smaller than the bias of $\\hat{\\beta}$, but these biases do not decrease with sample size.",
      "D) The mean squared errors (MSEs) of $\tilde{\\beta_{1}}$ and $\tilde{\\beta_{2}}$ are substantially smaller than the MSE of $\\hat{\\beta}$ for most cases, except when the sample size is small and the error variance is small."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Two-stage-instrumental-variables-estimators-for-the-nonl_1990_Journal-of-Eco.md"
    ],
    "question_context": "The study examines finite-sample properties of estimators for the quadratic model $Y_{t}=\\beta x_{t}^{2}+e_{t},\\quad X_{t}=x_{t}+u_{t},\\quad\\quad t=1,2,...,b,$ with observations on an additional variable $W_{t}^{*}$. The asymptotic theory was developed for fixed $x_{t}$ and $W_{i}^{*}$, and Monte Carlo samples were generated under specific conditions. The least squares estimator $\\hat{\\beta_{i}}$ ignores measurement error and exhibits a small variance but a very large bias. The ordinary instrumental variable estimator $\\hat{\\beta}$ has a sizable bias that does not decrease with sample size and increases with the curvature measure $|\\phi_{n}|=\\beta\\sigma_{u u}$. The two-stage instrumental variable estimators $\tilde{\\beta_{1}}$ and $\tilde{\\beta_{2}}$ have smaller biases than $\\hat{\\beta}$, but these biases do not decrease with sample size and generally increase with the curvature measure. The MSEs of $\tilde{\\beta_{1}}$ and $\tilde{\\beta_{2}}$ are substantially smaller than that of $\\hat{\\beta}$ for most cases, except when the sample size is small and the error variance is small.",
    "category": "INCORRECT",
    "question_context_used": "The study examines finite-sample properties of estimators for the quadratic model $Y_{t}=\\beta x_{t}^{2}+e_{t},\\quad X_{t}=x_{t}+u_{t},\\quad\\quad t=1,2,...,b,$ with observations on an additional variable $W_{t}^{*}$. The asymptotic theory was developed for fixed $x_{t}$ and $W_{i}^{*}$, and Monte Carlo samples were generated under specific conditions. The least squares estimator $\\hat{\\beta_{i}}$ ignores measurement error and exhibits a small variance but a very large bias. The ordinary instrumental variable estimator $\\hat{\\beta}$ has a sizable bias that does not decrease with sample size and increases with the curvature measure $|\\phi_{n}|=\\beta\\sigma_{u u}$. The two-stage instrumental variable estimators $\tilde{\\beta_{1}}$ and $\tilde{\\beta_{2}}$ have smaller biases than $\\hat{\\beta}$, but these biases do not decrease with sample size and generally increase with the curvature measure. The MSEs of $\tilde{\\beta_{1}}$ and $\tilde{\\beta_{2}}$ are substantially smaller than that of $\\hat{\\beta}$ for most cases, except when the sample size is small and the error variance is small.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-197",
    "question": "In the context of rational inattention and model plausibility, which of the following statements are correct regarding the optimal information policy and its implications?",
    "options": [
      "A) Under the optimal information policy, the worst-case model exhibits no systematic bias towards over- or under-extrapolation.",
      "B) The optimal information policy implies that agents gather information in proportion to the utility weights, focusing on frequencies most important for utility.",
      "C) The statistical benchmark case, where signal precision is constant across frequencies, leads to a worst-case model with excessively high power at low frequencies.",
      "D) The parameter λ, which determines the strength of the prior, has no effect on the worst-case model when the quality of information grows."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Directed-attention-and-nonparametric-learning_2019_Journal-of-Economic-Theor.md"
    ],
    "question_context": "The log posterior probability of a model is given by:\n\n$$\n^{D}\\left(\\hat{f}\\mid x,\\tau\\right)=-\\frac{1}{2}\\sum_{j=1}^{n}\\left(x\\left(\\omega_{j}\\right)-\\hat{f}\\left(\\omega_{j}\\right)\\right)^{2}\\tau\\left(\\omega_{j}\\right)d\\omega-\\frac{\\lambda}{2}\\sum_{j=2}^{n}\\left(\\frac{\\hat{f}\\left(\\omega_{j}\\right)-\\hat{f}\\left(\\omega_{j-1}\\right)}{d\\omega}\\right)^{2}d\\omega\n$$\n\nUnder the optimal information policy, the worst-case model is derived as:\n\n$$\nE\\left[f^{w}\\left(\\omega;x,\\tau^{*}\\right)-f\\mid f\\right]\\approx\\psi^{1/2}\\theta^{1/2}+\\lambda\\tau^{*}\\left(\\omega\\right)^{-1}f^{\\prime\\prime}\\left(\\omega\\right).\n$$\n\nThis shows that when agents optimally choose what to learn about, focusing on frequencies most important for utility, the bias towards excessive extrapolation is eliminated. In contrast, under the statistical benchmark with constant signal precision, the worst-case model exhibits excessive persistence, as it is biased towards high power at low frequencies.",
    "category": "CORRECT",
    "question_context_used": "The log posterior probability of a model is given by:\n\n$$\n^{D}\\left(\\hat{f}\\mid x,\\tau\\right)=-\\frac{1}{2}\\sum_{j=1}^{n}\\left(x\\left(\\omega_{j}\\right)-\\hat{f}\\left(\\omega_{j}\\right)\\right)^{2}\\tau\\left(\\omega_{j}\\right)d\\omega-\\frac{\\lambda}{2}\\sum_{j=2}^{n}\\left(\\frac{\\hat{f}\\left(\\omega_{j}\\right)-\\hat{f}\\left(\\omega_{j-1}\\right)}{d\\omega}\\right)^{2}d\\omega\n$$\n\nUnder the optimal information policy, the worst-case model is derived as:\n\n$$\nE\\left[f^{w}\\left(\\omega;x,\\tau^{*}\\right)-f\\mid f\\right]\\approx\\psi^{1/2}\\theta^{1/2}+\\lambda\\tau^{*}\\left(\\omega\\right)^{-1}f^{\\prime\\prime}\\left(\\omega\\right).\n$$\n\nThis shows that when agents optimally choose what to learn about, focusing on frequencies most important for utility, the bias towards excessive extrapolation is eliminated. In contrast, under the statistical benchmark with constant signal precision, the worst-case model exhibits excessive persistence, as it is biased towards high power at low frequencies.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2669",
    "question": "In the context of contracts and allocations, which of the following conditions must hold for a strategy configuration to be an equilibrium?",
    "options": [
      "A) Each entrepreneur maximizes expected utility given the strategies of all other players.",
      "B) Financial intermediaries must make zero profits in equilibrium.",
      "C) The contract offer $R_{i}$ must be non-decreasing in its second argument.",
      "D) The set of types accepting a contract must maximize the intermediary's expected profit."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Credit-Market-Imperfections-and-the-Separation-of-Ow_1998_Journal-of-Economi.md"
    ],
    "question_context": "The equilibrium concept is Subgame Perfect (or Perfect Bayesian): a strategy configuration will be an equilibrium if each financial intermediary is maximizing expected profits and each entrepreneur is maximizing expected utility given the strategies of all other players. Although the requirement that financial intermediaries make zero-profit is not formally part of the equilibrium definition, given the unlimited supplies of funds, it is implied by condition 2 of equilibrium: namely, if an intermediary were making positive profits, one of the inactive intermediaries could undercut and also make positive profits. \n\nDefinition 1. A strategy configuration $\\left\\{(r(x),p(x),q(x),\\hat{y}(y\\vert x))\\right\\}$ for every $x\\in E\\}$ and ${\\mathcal{R}}=\\{R$ such that $R_{i}=R$ for some $i\\in I\\}$ constitutes an equilibrium if and only if: \n1. For every set of contracts, $\\Re^{\\prime}$ , offered, each $x\\in E$ chooses $r(x)\\in\\Re^{\\prime}$ , $p(x)\\in\\{0,1\\}$ , $q(x)\\in\\{1,2\\}$ and ${\\hat{y}}(y\\vert x)$ to maximize $u(p,q,r,\\hat{y}(y|x))$ . \n2. $R_{i}\\in\\Re$ only if $R_{i}\\in\\arg\\operatorname*{max}_{R}\\int_{x\\in\\chi(R,\\Re)}\\pi_{i}(x,p(x),q(x),\\hat{y}(y|x)|R)d G(x)$ where $p(x),q(x)$ and ${\\hat{y}}(y\\vert x)$ are given by condition 1.",
    "category": "INCORRECT",
    "question_context_used": "The equilibrium concept is Subgame Perfect (or Perfect Bayesian): a strategy configuration will be an equilibrium if each financial intermediary is maximizing expected profits and each entrepreneur is maximizing expected utility given the strategies of all other players. Although the requirement that financial intermediaries make zero-profit is not formally part of the equilibrium definition, given the unlimited supplies of funds, it is implied by condition 2 of equilibrium: namely, if an intermediary were making positive profits, one of the inactive intermediaries could undercut and also make positive profits. \n\nDefinition 1. A strategy configuration $\\left\\{(r(x),p(x),q(x),\\hat{y}(y\\vert x))\\right\\}$ for every $x\\in E\\}$ and ${\\mathcal{R}}=\\{R$ such that $R_{i}=R$ for some $i\\in I\\}$ constitutes an equilibrium if and only if: \n1. For every set of contracts, $\\Re^{\\prime}$ , offered, each $x\\in E$ chooses $r(x)\\in\\Re^{\\prime}$ , $p(x)\\in\\{0,1\\}$ , $q(x)\\in\\{1,2\\}$ and ${\\hat{y}}(y\\vert x)$ to maximize $u(p,q,r,\\hat{y}(y|x))$ . \n2. $R_{i}\\in\\Re$ only if $R_{i}\\in\\arg\\operatorname*{max}_{R}\\int_{x\\in\\chi(R,\\Re)}\\pi_{i}(x,p(x),q(x),\\hat{y}(y|x)|R)d G(x)$ where $p(x),q(x)$ and ${\\hat{y}}(y\\vert x)$ are given by condition 1.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-258",
    "question": "Based on the analysis of the LTV limit's effect on mortgage repayment performance, which of the following statements are supported by the empirical findings?",
    "options": [
      "A) The LTV limit reduces mortgage arrears more significantly for high-income borrowers than for low-income borrowers.",
      "B) The reduction in mortgage arrears is statistically significant at conventional levels only among low-income borrowers.",
      "C) The average effect of the LTV limit on mortgage arrears is a 0.8 percentage point reduction, which is sizable given the baseline default rate.",
      "D) The LTV limit worsens liquidity positions, leading to increased mortgage arrears in the short run."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "The-real-effects-of-borrower-based-macroprudential-polic_2024_Journal-of-Mon.md"
    ],
    "question_context": "The study examines the impact of the LTV regulation on households’ financial health, measured by mortgage payment arrears. Key findings include: The average effect among the sample of mortgages shows a point estimate of $-0.008$ (statistically significant at the $12\\%$ level). Given the average default rate among mortgages issued before the policy shock is $3.3\\%$, a 0.8 percentage point reduction is a sizable effect. Columns [2] and [3] consider subgroups of mortgages based on family income at origination. The reduction in mortgage arrears is larger in magnitude (1.4 percentage points) and statistically significant at conventional levels only among low-income borrowers. The formula for the regression is: Payment Arrears = β₀ + β₁(Affected) + ε, where Affected is an indicator for the LTV limit period.",
    "category": "CORRECT",
    "question_context_used": "The study examines the impact of the LTV regulation on households’ financial health, measured by mortgage payment arrears. Key findings include: The average effect among the sample of mortgages shows a point estimate of $-0.008$ (statistically significant at the $12\\%$ level). Given the average default rate among mortgages issued before the policy shock is $3.3\\%$, a 0.8 percentage point reduction is a sizable effect. Columns [2] and [3] consider subgroups of mortgages based on family income at origination. The reduction in mortgage arrears is larger in magnitude (1.4 percentage points) and statistically significant at conventional levels only among low-income borrowers. The formula for the regression is: Payment Arrears = β₀ + β₁(Affected) + ε, where Affected is an indicator for the LTV limit period.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2461",
    "question": "Based on the empirical findings regarding school starting age (SSA) and its heterogeneous effects, which of the following statements are supported by the data?",
    "options": [
      "A) Females born just after the school entry cut-off experience a larger increase in educational attainment than males.",
      "B) Individuals with high-educated parents show a greater discontinuity in educational attainment at the cut-off than those with low-educated parents.",
      "C) Earnings at age 35 are positively affected by being born after the cut-off for women but negatively affected for men.",
      "D) The comprehensive school reform in Norway amplified the effect of SSA on educational attainment."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Life‐cycle_Effects_of_Age_at_School_Start.md"
    ],
    "question_context": "The study examines the impact of school starting age (SSA) on educational attainment and earnings across different stages of life. Key findings include a discontinuity at the school entry cut-off, with individuals born just after the cut-off gaining 0.14 additional years of schooling on average. Heterogeneous effects are observed by gender and parental education: females experience a larger increase in educational attainment (0.162 years) compared to males (0.112 years), and individuals with low-educated parents show a greater discontinuity (0.181 years) than those with high-educated parents (0.121 years). Earnings at age 35 are higher for women born after the cut-off (+1.1% of prime-age earnings) but lower for men (−0.6%). The comprehensive school reform in Norway, modeled after Sweden's, is noted to have reduced the importance of SSA, contrary to amplifying its effects.",
    "category": "CORRECT",
    "question_context_used": "The study examines the impact of school starting age (SSA) on educational attainment and earnings across different stages of life. Key findings include a discontinuity at the school entry cut-off, with individuals born just after the cut-off gaining 0.14 additional years of schooling on average. Heterogeneous effects are observed by gender and parental education: females experience a larger increase in educational attainment (0.162 years) compared to males (0.112 years), and individuals with low-educated parents show a greater discontinuity (0.181 years) than those with high-educated parents (0.121 years). Earnings at age 35 are higher for women born after the cut-off (+1.1% of prime-age earnings) but lower for men (−0.6%). The comprehensive school reform in Norway, modeled after Sweden's, is noted to have reduced the importance of SSA, contrary to amplifying its effects.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4291",
    "question": "In the context of China's 'War on Air Pollution' monitoring program, which of the following outcomes were directly observed as a result of strategic upwind-focused environmental regulations?",
    "options": [
      "A) A $6.2\\%$ reduction in industrial output for upwind firms compared to non-upwind firms",
      "B) A $3.0~{\\upmu\\mathrm{g}}/{\\mathrm{m}}^{3}$ reduction in PM2.5 concentrations in upwind grids",
      "C) A $5.6\\%$ increase in new commercial and residential businesses in upwind counties",
      "D) A uniform enforcement of regulations across all industrial emitters regardless of wind direction"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Go-with-the-wind--Spatial-impacts-of-environmental-re_2023_Journal-of-Develo.md"
    ],
    "question_context": "The paper examines China's national pollution control program, which installed monitoring networks in urban centers. Key findings include: upwind polluting firms experienced a $6.2\\%$ reduction in output compared to non-upwind firms, PM2.5 concentrations decreased by $3.0~{\\upmu\\mathrm{g}}/{\\mathrm{m}}^{3}$ in upwind grids, and new commercial/residential businesses increased by $5.6\\%$ in upwind counties. Formula: The upwind–nonupwind gap in industrial output is calculated as $\\Delta Y = Y_{\\text{upwind}} - Y_{\\text{non-upwind}}$, where $\\Delta Y = -6.2\\%$ after the program. The strategic enforcement was driven by political incentives to reduce pollution detectable by the monitoring network, leading to spatial redistribution of economic activities.",
    "category": "CORRECT",
    "question_context_used": "The paper examines China's national pollution control program, which installed monitoring networks in urban centers. Key findings include: upwind polluting firms experienced a $6.2\\%$ reduction in output compared to non-upwind firms, PM2.5 concentrations decreased by $3.0~{\\upmu\\mathrm{g}}/{\\mathrm{m}}^{3}$ in upwind grids, and new commercial/residential businesses increased by $5.6\\%$ in upwind counties. Formula: The upwind–nonupwind gap in industrial output is calculated as $\\Delta Y = Y_{\\text{upwind}} - Y_{\\text{non-upwind}}$, where $\\Delta Y = -6.2\\%$ after the program. The strategic enforcement was driven by political incentives to reduce pollution detectable by the monitoring network, leading to spatial redistribution of economic activities.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1635",
    "question": "In the Monte Carlo experiments assessing the size and power of ISD tests, which of the following statements are supported by the results?",
    "options": [
      "A) The IF estimator for ISD3 maintains correct size and power in small samples when the number of abscissae is set to 5.",
      "B) The bootstrap estimator (ISD3-BS) consistently outperforms the IF estimator in terms of both size and power across all sample sizes.",
      "C) The size of the ISD3-IF test increases with the number of abscissae when the sample size is held fixed.",
      "D) The power of the ISD3-IF test exceeds 0.7 when the sample size is 1500 and the number of abscissae is 20."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "13-Robust Inference for Inverse Stochastic Dominance.md"
    ],
    "question_context": "The Monte Carlo experiments evaluate the size and power of ISD tests using different estimators. Key findings include: The IF estimator for ISD3 shows correct size and power in baseline cases, with size generally below 0.10 for 5 abscissae. The size of the dominance test increases slightly with the number of abscissae, as the test becomes more demanding. For power, the ISD3-IF test achieves levels above 0.7 with a sample size of 1500 and 20 abscissae. The bootstrap estimator (ISD3-BS) shows larger size and smaller power compared to the IF estimator in small samples. The formula for generating lognormal draws is $Y_{i}=\\exp(\\sigma Z_{i}+\\mu)$, where $Z_{i}\\sim\\mathcal{N}(0,1)$.",
    "category": "CORRECT",
    "question_context_used": "The Monte Carlo experiments evaluate the size and power of ISD tests using different estimators. Key findings include: The IF estimator for ISD3 shows correct size and power in baseline cases, with size generally below 0.10 for 5 abscissae. The size of the dominance test increases slightly with the number of abscissae, as the test becomes more demanding. For power, the ISD3-IF test achieves levels above 0.7 with a sample size of 1500 and 20 abscissae. The bootstrap estimator (ISD3-BS) shows larger size and smaller power compared to the IF estimator in small samples. The formula for generating lognormal draws is $Y_{i}=\\exp(\\sigma Z_{i}+\\mu)$, where $Z_{i}\\sim\\mathcal{N}(0,1)$.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4370",
    "question": "Based on the empirical analysis of medieval maritime insurance contracts, which of the following factors had a statistically significant impact on insurance premia?",
    "options": [
      "A) Seasonal variations in weather patterns",
      "B) Attacks by corsairs and warfare",
      "C) Distance traveled by the shipment",
      "D) Specific trade routes in the Tyrrhenian and western Mediterranean"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Botticini 等 - 2023 - The Beauty of Uncertainty The Rise of Insurance Contracts and Markets in Medieval Europe.md"
    ],
    "question_context": "The empirical analysis delivers two main findings that are consistent with the predictions of our conceptual framework. First, risks related to human activities (e.g., attacks by corsairs, warfare) seem to have had a relatively greater impact on insurance premia compared to natural risks (proxied by seasonal risks). Second, distance mattered but the route seems to have had a greater impact on insurance premia. Longer routes potentially increased the probability of losses from natural risks; however, these risks were mostly avoidable by choosing longer but safer routes. In contrast, regardless of distance, specific routes (e.g., in the Tyrrhenian and the western Mediterranean) were more plagued by human risks (e.g., attacks by corsairs) which were harder to avoid for the majority of sedentary merchants; these merchants did not have a broad information network compared to the few wealthy merchants, who became the key players in pooling risks and selling insurance in the early stages of the development of insurance markets. The insurance premium percentages ranged from $0.75\\%$ to $22\\%$.",
    "category": "INCORRECT",
    "question_context_used": "The empirical analysis delivers two main findings that are consistent with the predictions of our conceptual framework. First, risks related to human activities (e.g., attacks by corsairs, warfare) seem to have had a relatively greater impact on insurance premia compared to natural risks (proxied by seasonal risks). Second, distance mattered but the route seems to have had a greater impact on insurance premia. Longer routes potentially increased the probability of losses from natural risks; however, these risks were mostly avoidable by choosing longer but safer routes. In contrast, regardless of distance, specific routes (e.g., in the Tyrrhenian and the western Mediterranean) were more plagued by human risks (e.g., attacks by corsairs) which were harder to avoid for the majority of sedentary merchants; these merchants did not have a broad information network compared to the few wealthy merchants, who became the key players in pooling risks and selling insurance in the early stages of the development of insurance markets. The insurance premium percentages ranged from $0.75\\%$ to $22\\%$.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3351",
    "question": "In the context of combining macro and micro forecasts, which of the following conditions are necessary for the proposed linear combination to improve forecast accuracy?",
    "options": [
      "A) The forecasts must be uncorrelated to avoid overfitting.",
      "B) The relative precisions of the macro and micro forecasts must be known or estimable.",
      "C) The linear constraint η_t = a'y_t must hold exactly for all time periods.",
      "D) The macro and micro models must be estimated jointly using all available data."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Magnus和Vasnev - 2008 - Using macro data to obtain better micro forecasts.md"
    ],
    "question_context": "The paper develops a theoretical framework for combining forecasts from macro and micro levels, where only forecasts and their precisions are available, not the full dataset. The proposed procedure is robust and improves forecast accuracy at both levels. Key formula for the macro forecast is given by: \n\n$$\n\\hat{\\eta}_{T+1} = \\alpha\\hat{\\eta}_{T+1}^{(1)} + (1-\\alpha)\\hat{\\eta}_{T+1}^{(2)},\n$$\n\nwhere \\(\\hat{\\eta}_{T+1}^{(1)}\\) and \\(\\hat{\\eta}_{T+1}^{(2)}\\) are macro and micro forecasts, respectively. The micro forecast is updated subject to the constraint \\(a'\\hat{y}_{T+1} = \\hat{\\eta}_{T+1}\\). The procedure assumes knowledge of relative precisions but does not require exact aggregation or joint estimation of models.",
    "category": "INCORRECT",
    "question_context_used": "The paper develops a theoretical framework for combining forecasts from macro and micro levels, where only forecasts and their precisions are available, not the full dataset. The proposed procedure is robust and improves forecast accuracy at both levels. Key formula for the macro forecast is given by: \n\n$$\n\\hat{\\eta}_{T+1} = \\alpha\\hat{\\eta}_{T+1}^{(1)} + (1-\\alpha)\\hat{\\eta}_{T+1}^{(2)},\n$$\n\nwhere \\(\\hat{\\eta}_{T+1}^{(1)}\\) and \\(\\hat{\\eta}_{T+1}^{(2)}\\) are macro and micro forecasts, respectively. The micro forecast is updated subject to the constraint \\(a'\\hat{y}_{T+1} = \\hat{\\eta}_{T+1}\\). The procedure assumes knowledge of relative precisions but does not require exact aggregation or joint estimation of models.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-461",
    "question": "Based on the Chilean study of emergency contraception (EC) pill effects, which of the following statements are empirically supported by the findings?",
    "options": [
      "A) The EC pill reduced teenage births by approximately 2.7% and early-gestation foetal deaths by 40%",
      "B) The EC pill had no statistically significant effect on fertility rates for women aged 20-34",
      "C) The EC pill's availability prevented nearly 500 births to teen mothers over the study period",
      "D) The EC pill's demand peaked immediately post-reform but declined steadily in subsequent years"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Assessing_Plan_B_The_Effect_of_the_Morning_After_Pill_on_Children_and_Women.md"
    ],
    "question_context": "The study exploits a legal variation in Chile's 346 municipalities to estimate EC pill effects. Key findings include: For teenagers, the EC pill caused a $2.7\\%$ reduction in births and $40\\%$ reduction in early-gestation foetal deaths (proxying illegal abortions). For 20–34-year-old women, births decreased by a statistically significant $1.8\\%$. The general fertility rate dropped by 0.88 births per 1,000 women ($1.6\\%$ of average GFR). Post-reform demand remained steady at $8{,}000~\\mathrm{EC}$ pills annually. Counterfactual analysis suggests the EC pill prevented ~500 teen births ($1.5{-}3\\%$ higher teenage childbearing without it). Formula: $\\Delta \\text{Fertility}_{\\text{teen}} = -2.7\\% \\cdot B_{\\text{base}} + \\epsilon$, where $B_{\\text{base}}$ is baseline births.",
    "category": "CORRECT",
    "question_context_used": "The study exploits a legal variation in Chile's 346 municipalities to estimate EC pill effects. Key findings include: For teenagers, the EC pill caused a $2.7\\%$ reduction in births and $40\\%$ reduction in early-gestation foetal deaths (proxying illegal abortions). For 20–34-year-old women, births decreased by a statistically significant $1.8\\%$. The general fertility rate dropped by 0.88 births per 1,000 women ($1.6\\%$ of average GFR). Post-reform demand remained steady at $8{,}000~\\mathrm{EC}$ pills annually. Counterfactual analysis suggests the EC pill prevented ~500 teen births ($1.5{-}3\\%$ higher teenage childbearing without it). Formula: $\\Delta \\text{Fertility}_{\\text{teen}} = -2.7\\% \\cdot B_{\\text{base}} + \\epsilon$, where $B_{\\text{base}}$ is baseline births.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-198",
    "question": "In the context of optimal information policy and model uncertainty, which of the following statements are consistent with the spectral analysis of income and consumption behavior?",
    "options": [
      "A) The optimal information policy leads to accurate modeling of low-frequency income components but poor fit at high frequencies.",
      "B) Agents under the statistical benchmark policy exhibit excessive extrapolation of income shocks, leading to long-run mean reversion in consumption growth.",
      "C) The spectrum of consumption growth inherits peaks from the income spectrum that are ignored in the worst-case model.",
      "D) Rational inattention models predict rapid response to permanent income shocks due to optimal low-frequency attention."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Directed-attention-and-nonparametric-learning_2019_Journal-of-Economic-Theor.md"
    ],
    "question_context": "The primary mistakes in the agent’s worst-case model come from the term in (27) involving $f^{\\prime\\prime}\\left(\\omega\\right)$. That part of the formula is driven by the agent’s smoothness prior. In the face of noisy data, agents estimate the spectrum of income by smoothing information across frequencies. Since $f^{w}\\left({\\boldsymbol{\\omega}};{\\boldsymbol{x}},{\\boldsymbol{\\tau}}\\right)$ is a convex combination of the data $x$ local to $\\omega$, it is biased upward when $f^{\\prime\\prime}>0$ and downward when $f^{\\prime\\prime}<0$. Intuitively, if there is a narrow peak in $f$, a simple model will tend to smooth the peak out, and thus be biased downward. The log spectrum of consumption growth is $f_{\\Delta C}^{w}\\left(\\omega;x,\\tau\\right)=\\log{\\left(\\left(1-R^{-1}\\right)^{2}b^{w}\\left(R^{-1};x,\\tau\\right)^{2}\\right)}+f\\left(\\omega\\right)-f^{w}\\left(\\omega;x,\\tau\\right)$. When the agent knows the true model, $f_{\\Delta C}^{w}$ is perfectly flat, and we have the usual result that consumption growth is uncorrelated over time and the level of consumption is a random walk.",
    "category": "INCORRECT",
    "question_context_used": "The primary mistakes in the agent’s worst-case model come from the term in (27) involving $f^{\\prime\\prime}\\left(\\omega\\right)$. That part of the formula is driven by the agent’s smoothness prior. In the face of noisy data, agents estimate the spectrum of income by smoothing information across frequencies. Since $f^{w}\\left({\\boldsymbol{\\omega}};{\\boldsymbol{x}},{\\boldsymbol{\\tau}}\\right)$ is a convex combination of the data $x$ local to $\\omega$, it is biased upward when $f^{\\prime\\prime}>0$ and downward when $f^{\\prime\\prime}<0$. Intuitively, if there is a narrow peak in $f$, a simple model will tend to smooth the peak out, and thus be biased downward. The log spectrum of consumption growth is $f_{\\Delta C}^{w}\\left(\\omega;x,\\tau\\right)=\\log{\\left(\\left(1-R^{-1}\\right)^{2}b^{w}\\left(R^{-1};x,\\tau\\right)^{2}\\right)}+f\\left(\\omega\\right)-f^{w}\\left(\\omega;x,\\tau\\right)$. When the agent knows the true model, $f_{\\Delta C}^{w}$ is perfectly flat, and we have the usual result that consumption growth is uncorrelated over time and the level of consumption is a random walk.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-260",
    "question": "Under Assumption 3 regarding the bandwidth parameter expansion rate \\( K = O_e(T^k) \\), which of the following conditions ensure that \\( \\frac{K^2}{T} \\to 0 \\) as \\( T \\to \\infty \\)?",
    "options": [
      "A) BW(a): \\( k \\in \\left\\{\\frac{1}{4}, \\frac{1}{2}\\right\\} \\)",
      "B) BW(b): \\( k \\in \\left(\\frac{1}{4}, 1\\right) \\)",
      "C) BW(c): \\( k \\in \\left(0, \\frac{1}{2}\\right) \\)",
      "D) BW(d): \\( k \\in (0, 1) \\)"
    ],
    "gold_answer": "C, D",
    "source_files": [
      "Chang和Phillips - 1995 - Time series regression with mixtures of integrated processes.md"
    ],
    "question_context": "Assumption 3 specifies the expansion rate of the bandwidth parameter \\( K \\) in kernel estimates as \\( K = O_e(T^k) \\) for some \\( k \\in (0,1) \\). The text elaborates on the implications of different ranges for \\( k \\):\n\nFor any \\( k \\in (0,1) \\), \\( \\frac{K}{T} = c_T T^{k-1} \\to 0 \\) as \\( T \\to \\infty \\). However, the condition \\( \\frac{K^2}{T} \\to 0 \\) requires \\( k < \\frac{1}{2} \\), as shown by the derivation:\n\n$$\\frac{K^2}{T} = \\frac{(c_T T^{1/2-\\delta})^2}{T} = c_T^2 T^{-2\\delta} \\to 0.$$\n\nThis condition is explicitly satisfied under BW(c) (\\( k \\in (0, \\frac{1}{2}) \\)) and BW(d) (\\( k \\in (0,1) \\)), but not necessarily under BW(a) or BW(b) for all \\( k \\) in their specified ranges.",
    "category": "INCORRECT",
    "question_context_used": "Assumption 3 specifies the expansion rate of the bandwidth parameter \\( K \\) in kernel estimates as \\( K = O_e(T^k) \\) for some \\( k \\in (0,1) \\). The text elaborates on the implications of different ranges for \\( k \\):\n\nFor any \\( k \\in (0,1) \\), \\( \\frac{K}{T} = c_T T^{k-1} \\to 0 \\) as \\( T \\to \\infty \\). However, the condition \\( \\frac{K^2}{T} \\to 0 \\) requires \\( k < \\frac{1}{2} \\), as shown by the derivation:\n\n$$\\frac{K^2}{T} = \\frac{(c_T T^{1/2-\\delta})^2}{T} = c_T^2 T^{-2\\delta} \\to 0.$$\n\nThis condition is explicitly satisfied under BW(c) (\\( k \\in (0, \\frac{1}{2}) \\)) and BW(d) (\\( k \\in (0,1) \\)), but not necessarily under BW(a) or BW(b) for all \\( k \\) in their specified ranges.",
    "gold_answer_parsed": [
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-247",
    "question": "Under the conditions of conditional independence of link decisions and Assumption 3, which of the following statements about the projection $\\bar{s}_{i j}(\\beta_{0})$ and its properties are correct?",
    "options": [
      "A) $\\mathbb{E}[\\bar{s}_{i j}(\\beta_{0})|X_{k},X_{l},A_{k},A_{l},U_{k l}]=0$ for all $\\{i,j\\}\\neq\\{k,l\\}$",
      "B) The Hajek Projection is given by $U_{N}^{*}=\\frac{6}{n}\\sum_{i<j}\\bar{s}_{i j}(\\beta_{0})$",
      "C) $\\mathbb{E}[U_{N}|X_{i},X_{j},A_{i},A_{j},U_{i j}]=\\frac{6}{n}\\bar{s}_{i j}(\\beta_{0})$",
      "D) $\\mathbb{V}(U_{N}^{*})=\\frac{36}{n}\\Delta_{2,N}$"
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "Graham-ECONOMETRICMODELNETWORK-2017.md"
    ],
    "question_context": "Consider the link structure of two tetrads, $\\{i,j,k,l\\}$ and $\\{i,j,m,p\\}$ , sharing the dyad $\\{i,j\\}$ in common. By virtue of conditional independence of link decisions, and Assumption 3, the configurations of $\\{i,j,k,l\\}$ and $\\{i,j,m,p\\}$ are independent conditional on $X_{i},X_{j},A_{i},A_{j}$ and $U_{i j}$ giving \n\n$$ \n\\mathbb{E}\\big[s_{i j k l}(\\beta_{0})s_{i j m p}(\\beta_{0})^{\\prime}|X_{i},X_{j},A_{i},A_{j},U_{i j}\\big]=\\bar{s}_{i j}(\\beta_{0})\\bar{s}_{i j}(\\beta_{0})^{\\prime}\n$$ \n\nfor ${\\bar{s}}_{i j}(\\beta_{0}){\\stackrel{\\mathrm{def}}{=}}\\mathbb{E}[s_{i j k l}(\\beta_{0})|X_{i},X_{j},A_{i},A_{j},U_{i j}]$. Iterated expectations then implies that, for $\\Delta_{q,N}$ as defined in (30) above, $\\Delta_{2,N}=\\mathbb{E}[\\bar{s}_{i j}(\\beta_{0})\\bar{s}_{i j}(\\beta_{0})^{\\prime}]$. Projecting $U_{N}$ onto an arbitrary function of $(X_{i},X_{j},A_{i},A_{j},U_{i j})$ then yields \n\n$$ \n\\begin{array}{r l}&{\\mathbb{E}[U_{N}|X_{i},X_{j},A_{i},A_{j},U_{i j}]}\\ &{\\quad=\\binom{N}{4}^{-1}\\displaystyle\\sum_{k_{1}<k_{2}<k_{3}<k_{3}}\\mathbb{E}\\big[s_{k_{1}k_{2}k_{3}k_{4}}(\\beta_{0})|X_{i},X_{j},A_{i},A_{j},U_{i j}\\big].}\\end{array}\n$$ \n\nThe expectation in the summand of (33) is, by iterated expectations and conditionally independent link formation, zero if $\\{\\dot{i},j\\}\\not\\subset\\{k_{1},k_{2},k_{3},k_{4}\\}$ and equals $\\overline{{s}}_{i j}(\\beta_{0})$ if $\\{i,j\\}\\subseteq$ $\\{k_{1},k_{2},k_{3},k_{4}\\}$. Since dyad $\\{i,j\\}$ appears in a total of $\\binom{N-2}{2}$ of the $\\binom{N}{4}$ tetrads, we get that \n\n$$ \n\\mathbb{E}[U_{N}|X_{i},X_{j},A_{i},A_{j},U_{i j}]=\\binom{N}{4}^{-1}\\binom{N-2}{2}=\\frac{12}{N(N-1)}\\bar{s}_{i j}(\\beta_{0})=\\frac{6}{n}\\bar{s}_{i j}(\\beta_{0}).\n$$",
    "category": "INCORRECT",
    "question_context_used": "Consider the link structure of two tetrads, $\\{i,j,k,l\\}$ and $\\{i,j,m,p\\}$ , sharing the dyad $\\{i,j\\}$ in common. By virtue of conditional independence of link decisions, and Assumption 3, the configurations of $\\{i,j,k,l\\}$ and $\\{i,j,m,p\\}$ are independent conditional on $X_{i},X_{j},A_{i},A_{j}$ and $U_{i j}$ giving \n\n$$ \n\\mathbb{E}\\big[s_{i j k l}(\\beta_{0})s_{i j m p}(\\beta_{0})^{\\prime}|X_{i},X_{j},A_{i},A_{j},U_{i j}\\big]=\\bar{s}_{i j}(\\beta_{0})\\bar{s}_{i j}(\\beta_{0})^{\\prime}\n$$ \n\nfor ${\\bar{s}}_{i j}(\\beta_{0}){\\stackrel{\\mathrm{def}}{=}}\\mathbb{E}[s_{i j k l}(\\beta_{0})|X_{i},X_{j},A_{i},A_{j},U_{i j}]$. Iterated expectations then implies that, for $\\Delta_{q,N}$ as defined in (30) above, $\\Delta_{2,N}=\\mathbb{E}[\\bar{s}_{i j}(\\beta_{0})\\bar{s}_{i j}(\\beta_{0})^{\\prime}]$. Projecting $U_{N}$ onto an arbitrary function of $(X_{i},X_{j},A_{i},A_{j},U_{i j})$ then yields \n\n$$ \n\\begin{array}{r l}&{\\mathbb{E}[U_{N}|X_{i},X_{j},A_{i},A_{j},U_{i j}]}\\ &{\\quad=\\binom{N}{4}^{-1}\\displaystyle\\sum_{k_{1}<k_{2}<k_{3}<k_{3}}\\mathbb{E}\\big[s_{k_{1}k_{2}k_{3}k_{4}}(\\beta_{0})|X_{i},X_{j},A_{i},A_{j},U_{i j}\\big].}\\end{array}\n$$ \n\nThe expectation in the summand of (33) is, by iterated expectations and conditionally independent link formation, zero if $\\{\\dot{i},j\\}\\not\\subset\\{k_{1},k_{2},k_{3},k_{4}\\}$ and equals $\\overline{{s}}_{i j}(\\beta_{0})$ if $\\{i,j\\}\\subseteq$ $\\{k_{1},k_{2},k_{3},k_{4}\\}$. Since dyad $\\{i,j\\}$ appears in a total of $\\binom{N-2}{2}$ of the $\\binom{N}{4}$ tetrads, we get that \n\n$$ \n\\mathbb{E}[U_{N}|X_{i},X_{j},A_{i},A_{j},U_{i j}]=\\binom{N}{4}^{-1}\\binom{N-2}{2}=\\frac{12}{N(N-1)}\\bar{s}_{i j}(\\beta_{0})=\\frac{6}{n}\\bar{s}_{i j}(\\beta_{0}).\n$$",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-155",
    "question": "Under the conditions of Theorem 1 in an n-agent economy with production, which of the following statements are true regarding the stability and behavior of the steady state?",
    "options": [
      "A) The steady state is locally saddlepoint stable if all agents' utilities satisfy normality and increasing marginal impatience conditions.",
      "B) The roots of the Jacobian matrix J are all real and positive under normality assumptions on preferences.",
      "C) If future utility is inferior for one or more agents, the system may exhibit oscillatory behavior around the steady state.",
      "D) The emergence of multiple steady states is guaranteed when assumption (M) holds."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "The-dynamics-of-efficient-intertemporal-allocations-with-_1988_Journal-of-Ec.md"
    ],
    "question_context": "In this section, we provide a complete characterization of the local behavior of optimal paths in an n-agent economy with production. One of the main conclusions of this section is that a steady state is locally saddlepoint stable if the agents’ utilities satisfy the normality and increasing marginal impatience conditions. These assumptions imply the uniqueness of the steady state as well. The more detailed results are given in Theorem 1 below. Theorem 1 states that under normality assumptions, the Jacobian matrix J has 2n characteristic roots which are all real and positive. If, in addition, assumption (M) (increasing marginal impatience) holds, then for every root inside (outside) the unit circle, its conjugate is outside (inside) the unit circle. Remark 2 notes that if future utility is inferior for one or more agents, the accumulation of capital will lead to higher current consumption and a reduction in the capital stock, giving rise to oscillations.",
    "category": "INCORRECT",
    "question_context_used": "In this section, we provide a complete characterization of the local behavior of optimal paths in an n-agent economy with production. One of the main conclusions of this section is that a steady state is locally saddlepoint stable if the agents’ utilities satisfy the normality and increasing marginal impatience conditions. These assumptions imply the uniqueness of the steady state as well. The more detailed results are given in Theorem 1 below. Theorem 1 states that under normality assumptions, the Jacobian matrix J has 2n characteristic roots which are all real and positive. If, in addition, assumption (M) (increasing marginal impatience) holds, then for every root inside (outside) the unit circle, its conjugate is outside (inside) the unit circle. Remark 2 notes that if future utility is inferior for one or more agents, the accumulation of capital will lead to higher current consumption and a reduction in the capital stock, giving rise to oscillations.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2062",
    "question": "In the context of the quasi-likelihood ratio test for threshold nonlinearity in a diffusion process, which of the following statements are correct?",
    "options": [
      "A) The quasi-log-likelihood function is exactly quadratic in the parameter vector θ.",
      "B) The constrained maximum quasi-likelihood estimator is obtained by setting the threshold parameter γ to zero.",
      "C) The test statistic Λ(r) reduces to the classical likelihood ratio test if the working diffusion term is the true diffusion term.",
      "D) The quasi-likelihood ratio test statistic for known threshold r can be expressed as a random quadratic form under the null hypothesis."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "04-Testing for Threshold Diffusion.md"
    ],
    "question_context": "The quasi-log-likelihood function is obtained by setting the diffusion term to unity in the log-likelihood function, as introduced by Su and Chan (2015). The function is given by:\n\n$$\n\\ell_{\\theta,r}(X)=\\int_{0}^{T}\\mu\\{X(t)\\}d X(t)-\\frac{1}{2}\\int_{0}^{T}\\mu^{2}\\{X(t)\\}d t.\n$$\n\nUnder the null hypothesis γ*=0, the constrained maximum quasi-likelihood estimator β̃ is obtained by solving the score equation. The quasi-log-likelihood function is exactly quadratic in the parameter θ=(β⊤,γ⊤)⊤, which leads to the test statistic Λ(r) admitting a quadratic form representation:\n\n$$\n\\Lambda(r)=2\\{\\ell_{\\hat{\\theta}_{r}}(X)-\\ell_{\\tilde{\\theta}}(X)\\}=-\\hat{\\gamma}_{r}^{\\top}M_{r}^{-1}\\hat{\\gamma}_{r}.\n$$\n\nLemma 1 shows that under H0 and for known r, the likelihood ratio test statistic equals a random quadratic form. If the working diffusion term is the true diffusion term, the test reduces to the classical likelihood ratio test.",
    "category": "INCORRECT",
    "question_context_used": "The quasi-log-likelihood function is obtained by setting the diffusion term to unity in the log-likelihood function, as introduced by Su and Chan (2015). The function is given by:\n\n$$\n\\ell_{\\theta,r}(X)=\\int_{0}^{T}\\mu\\{X(t)\\}d X(t)-\\frac{1}{2}\\int_{0}^{T}\\mu^{2}\\{X(t)\\}d t.\n$$\n\nUnder the null hypothesis γ*=0, the constrained maximum quasi-likelihood estimator β̃ is obtained by solving the score equation. The quasi-log-likelihood function is exactly quadratic in the parameter θ=(β⊤,γ⊤)⊤, which leads to the test statistic Λ(r) admitting a quadratic form representation:\n\n$$\n\\Lambda(r)=2\\{\\ell_{\\hat{\\theta}_{r}}(X)-\\ell_{\\tilde{\\theta}}(X)\\}=-\\hat{\\gamma}_{r}^{\\top}M_{r}^{-1}\\hat{\\gamma}_{r}.\n$$\n\nLemma 1 shows that under H0 and for known r, the likelihood ratio test statistic equals a random quadratic form. If the working diffusion term is the true diffusion term, the test reduces to the classical likelihood ratio test.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4184",
    "question": "In the context of testing $H_{0}:\\pmb{\\alpha}=\\pmb{0}$ in high-dimensional multi-factor pricing models, which of the following statements are true regarding the test statistics $S_{Wald}$ and $L_{max}$?",
    "options": [
      "A) $S_{Wald}$ is asymptotically pivotal and enjoys asymptotic normality under the null hypothesis.",
      "B) $L_{max}$ is designed to be powerful against dense alternatives where most components of $\\pmb{\\alpha}$ are non-zero.",
      "C) The asymptotic null distribution of $L_{max}$ is a Gumbel distribution, $G(y)=\\exp(-\\frac{1}{\\sqrt{\\pi}}\\exp(-\\frac{y}{2}))$.",
      "D) The Fisher combination test $F_{joint}$ combines $p$-values from $S_{Wald}$ and $L_{max}$ to achieve scale invariance and asymptotic optimality."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Power-enhancement-for-testing-multi-factor-asset-pricin_2024_Journal-of-Econ.md"
    ],
    "question_context": "The Wald statistic $S_{Wald} = \\frac{T a_{f,T}\\widehat{\\pmb{\\alpha}}^{\\prime}\\widehat{\\pmb{\\Sigma}}_{pd}^{-1}\\widehat{\\pmb{\\alpha}}-p}{\\sqrt{2p}}$ is asymptotically pivotal and converges in distribution to $N(0,1)$ under $H_{0}$. However, it performs poorly against sparse alternatives. To address this, the maximum test statistic $L_{max} = \\operatorname*{max}_{1\\leq j\\leq p}\\frac{\\widehat{\\alpha}_{j}^{2}}{\\widehat{\\nu_{j}}}-2\\log p+\\log\\log p$ is introduced, which asymptotically follows a Gumbel distribution $G(y)=\\exp(-\\frac{1}{\\sqrt{\\pi}}\\exp(-\\frac{y}{2}))$ under $H_{0}$. The Fisher combination test $F_{joint} = -2\\log(p_{Wald}) - 2\\log(p_{max})$ combines $p$-values from both statistics, leveraging their asymptotic independence to achieve a $\\chi_{4}^{2}$ null distribution and optimal power properties.",
    "category": "CORRECT",
    "question_context_used": "The Wald statistic $S_{Wald} = \\frac{T a_{f,T}\\widehat{\\pmb{\\alpha}}^{\\prime}\\widehat{\\pmb{\\Sigma}}_{pd}^{-1}\\widehat{\\pmb{\\alpha}}-p}{\\sqrt{2p}}$ is asymptotically pivotal and converges in distribution to $N(0,1)$ under $H_{0}$. However, it performs poorly against sparse alternatives. To address this, the maximum test statistic $L_{max} = \\operatorname*{max}_{1\\leq j\\leq p}\\frac{\\widehat{\\alpha}_{j}^{2}}{\\widehat{\\nu_{j}}}-2\\log p+\\log\\log p$ is introduced, which asymptotically follows a Gumbel distribution $G(y)=\\exp(-\\frac{1}{\\sqrt{\\pi}}\\exp(-\\frac{y}{2}))$ under $H_{0}$. The Fisher combination test $F_{joint} = -2\\log(p_{Wald}) - 2\\log(p_{max})$ combines $p$-values from both statistics, leveraging their asymptotic independence to achieve a $\\chi_{4}^{2}$ null distribution and optimal power properties.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-723",
    "question": "Which of the following conditions are necessary for a subgroup consistent poverty index \\( P \\) to satisfy the transfer axiom?",
    "options": [
      "A) The individual deprivation function \\( \\phi(t) \\) must be strictly convex for \\( t \\le z \\)",
      "B) The individual deprivation function \\( \\phi(t) \\) must be decreasing for \\( t < z \\)",
      "C) The poverty index must be decomposable into a canonical index and the headcount ratio",
      "D) The poverty index must satisfy the normalization condition \\( F(0) = 0 \\)"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Foster-SubgroupConsistentPoverty-1991.md"
    ],
    "question_context": "The paper examines subgroup consistency in poverty indices, ensuring that overall poverty does not respond perversely to changes in poverty levels within population subgroups. The additive structure of the canonical indices \\( P^{\\phi} \\) and \\( P^{d} \\) allows for easy incorporation of additional restrictions. For instance, the normalization axiom implies that \\( F(0) = 0 \\) in Proposition 1 and that \\( c = 0 \\) in Corollary 1. Given the form of the subgroup consistent poverty index \\( P \\) described in (33), we may deduce that \\( P \\) is strictly monotonic if and only if the individual deprivation function \\( \\phi(t) \\) is decreasing for \\( t < z \\); and that \\( P \\) satisfies the transfer axiom if and only if \\( \\phi(t) \\) is strictly convex for \\( t \\le z \\). Notice that either strict monotonicity or the transfer axiom is sufficient to rule out the headcount ratio.",
    "category": "CORRECT",
    "question_context_used": "The paper examines subgroup consistency in poverty indices, ensuring that overall poverty does not respond perversely to changes in poverty levels within population subgroups. The additive structure of the canonical indices \\( P^{\\phi} \\) and \\( P^{d} \\) allows for easy incorporation of additional restrictions. For instance, the normalization axiom implies that \\( F(0) = 0 \\) in Proposition 1 and that \\( c = 0 \\) in Corollary 1. Given the form of the subgroup consistent poverty index \\( P \\) described in (33), we may deduce that \\( P \\) is strictly monotonic if and only if the individual deprivation function \\( \\phi(t) \\) is decreasing for \\( t < z \\); and that \\( P \\) satisfies the transfer axiom if and only if \\( \\phi(t) \\) is strictly convex for \\( t \\le z \\). Notice that either strict monotonicity or the transfer axiom is sufficient to rule out the headcount ratio.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3048",
    "question": "Under what conditions does the matrix polynomial $\\Pi(u)^{-1}$ have a pole of order two at $u=1$?",
    "options": [
      "A) $M_{\\alpha_{0}}\\Pi_{1}M_{\\beta_{0}}=-\\alpha_{1}\\beta_{1}^{\\prime}$ has rank $r_{1}<p-r_{0}$ and $M_{(\\alpha_{0};\\alpha_{1})}\\theta_{2}M_{(\\beta_{0};\\beta_{1})}=-\\alpha_{2}\\beta_{2}^{\\prime}$ has rank $r_{2}=p-r_{0}-r_{1}$",
      "B) $\\Pi(1)=-\\alpha_{0}\\beta_{0}^{\\prime}$ has rank $r_{0}<p$ and $M_{\\alpha_{0}}\\Pi_{1}M_{\\beta_{0}}=-\\alpha_{1}\\beta_{1}^{\\prime}$ has rank $r_{1}=p-r_{0}$",
      "C) $M_{(\\alpha_{0}:\\alpha_{1}:\\alpha_{2})}\\theta_{3}M_{(\\beta_{0}:\\beta_{1}:\\beta_{2})}=-\\alpha_{3}\\beta_{3}^{\\prime}$ has rank $r_{3}=p-r_{0}-r_{1}-r_{2}$",
      "D) $\\Pi(u)^{-1}$ has a pole of order one at $u=1$ and $\\theta_{2}:=\\Pi_{2}+\\Pi_{1}\\bar{\\beta}_{0}\\bar{\\alpha}_{0}^{\\prime}\\bar{\\Pi_{1}}$ is nonsingular"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Franchi - 2010 - A REPRESENTATION THEORY FOR POLYNOMIAL COFRACTIONALITY IN VECTOR AUTOREGRESSIVE MODELS.md"
    ],
    "question_context": "The order of the pole of $\\Pi(u)^{-1}$ is one if and only if $\\Pi(1)= -\\alpha_{0}\\beta_{0}^{\\prime}$ has rank $r_{0}<p$ and $M_{\\alpha_{0}}\\Pi_{1}M_{\\beta_{0}}=-\\alpha_{1}\\beta_{1}^{\\prime}$ has rank $r_{1}=p-r_{0}$ , where $M_{\\gamma}:=I-P_{\\gamma}=P_{\\gamma_{\\perp}}$ and the square matrices $(a_{0}:a_{1})$ , $(\\beta_{0}:\\beta_{1})$ are nonsingular with orthogonal blocks. This is called the $I(1)$ condition. The pole has order two if and only if $M_{\\alpha_{0}}\\Pi_{1}M_{\\beta_{0}}=-\\alpha_{1}\\beta_{1}^{\\prime}$ has rank $r_{1}<p-r_{0}$ and $M_{(\\alpha_{0};\\alpha_{1})}\\theta_{2}M_{(\\beta_{0};\\beta_{1})}= -\\alpha_{2}\\beta_{2}^{\\prime}$ has rank $r_{2}=p-r_{0}-r_{1}$ , where $\\theta_{2}:=\\Pi_{2}+\\Pi_{1}\\bar{\\beta}_{0}\\bar{\\alpha}_{0}^{\\prime}\\bar{\\Pi_{1}}$ and the square matrices $(\\alpha_{0}:\\alpha_{1}:\\alpha_{2})$ , $(\\beta_{0}:\\beta_{1}:\\beta_{2})$ are nonsingular with orthogonal blocks. This is called the $I(2)$ condition. The characterization of the reduced rank matrices is important because it allows the cofraction matrices and the corresponding adjustment coefficients to be defined. When the order of the pole is greater than two, the additional reduced rank restriction $M_{(\\alpha_{0};\\alpha_{1})}\\theta_{2}M_{(\\beta_{0};\\beta_{1})}=-\\alpha_{2}\\beta_{2}^{\\prime}$ of rank $r_{2}<p-r_{0}-r_{1}$ holds, but the next matrix that would either define an additional orthogonal subspace if $c>3$ or stop the iteration if $c=3$ is not known.",
    "category": "INCORRECT",
    "question_context_used": "The order of the pole of $\\Pi(u)^{-1}$ is one if and only if $\\Pi(1)= -\\alpha_{0}\\beta_{0}^{\\prime}$ has rank $r_{0}<p$ and $M_{\\alpha_{0}}\\Pi_{1}M_{\\beta_{0}}=-\\alpha_{1}\\beta_{1}^{\\prime}$ has rank $r_{1}=p-r_{0}$ , where $M_{\\gamma}:=I-P_{\\gamma}=P_{\\gamma_{\\perp}}$ and the square matrices $(a_{0}:a_{1})$ , $(\\beta_{0}:\\beta_{1})$ are nonsingular with orthogonal blocks. This is called the $I(1)$ condition. The pole has order two if and only if $M_{\\alpha_{0}}\\Pi_{1}M_{\\beta_{0}}=-\\alpha_{1}\\beta_{1}^{\\prime}$ has rank $r_{1}<p-r_{0}$ and $M_{(\\alpha_{0};\\alpha_{1})}\\theta_{2}M_{(\\beta_{0};\\beta_{1})}= -\\alpha_{2}\\beta_{2}^{\\prime}$ has rank $r_{2}=p-r_{0}-r_{1}$ , where $\\theta_{2}:=\\Pi_{2}+\\Pi_{1}\\bar{\\beta}_{0}\\bar{\\alpha}_{0}^{\\prime}\\bar{\\Pi_{1}}$ and the square matrices $(\\alpha_{0}:\\alpha_{1}:\\alpha_{2})$ , $(\\beta_{0}:\\beta_{1}:\\beta_{2})$ are nonsingular with orthogonal blocks. This is called the $I(2)$ condition. The characterization of the reduced rank matrices is important because it allows the cofraction matrices and the corresponding adjustment coefficients to be defined. When the order of the pole is greater than two, the additional reduced rank restriction $M_{(\\alpha_{0};\\alpha_{1})}\\theta_{2}M_{(\\beta_{0};\\beta_{1})}=-\\alpha_{2}\\beta_{2}^{\\prime}$ of rank $r_{2}<p-r_{0}-r_{1}$ holds, but the next matrix that would either define an additional orthogonal subspace if $c>3$ or stop the iteration if $c=3$ is not known.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1126",
    "question": "In the context of multi-security markets with insider trading, which of the following conditions can lead to destructive interference among securities?",
    "options": [
      "A) High correlation between the future values of the securities",
      "B) Independence of future values between the securities",
      "C) Introduction of a new security with payoffs independent of existing securities",
      "D) Sufficiently low insider risk aversion parameter"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Destructive-Interference-in-an-Imperfectly-Competitiv_1995_Journal-of-Econom.md"
    ],
    "question_context": "The paper explores how the presence of an insider with private information can lead to market breakdowns in multi-security settings. Key formula: The condition for destructive interference is given by the eigenvalues of the matrix $U_k = J_k^{\\top}\\Sigma_{\\varepsilon}J_k[J_k^{\\top}(\\theta^2\\Sigma_{\\eta}\\Sigma_{W}\\Sigma_{\\eta} + \\Sigma_{\\varepsilon})J_k]^{-1}$. If any eigenvalue exceeds 1/2, the securities cannot simultaneously trade in equilibrium. This occurs when the insider's trading motive is largely informational, particularly when the securities' future values are highly correlated or when the insider's risk aversion is sufficiently low. The model demonstrates that introducing new securities can disrupt existing markets, especially if their payoffs are correlated with those of existing securities, leading to destructive interference.",
    "category": "CORRECT",
    "question_context_used": "The paper explores how the presence of an insider with private information can lead to market breakdowns in multi-security settings. Key formula: The condition for destructive interference is given by the eigenvalues of the matrix $U_k = J_k^{\\top}\\Sigma_{\\varepsilon}J_k[J_k^{\\top}(\\theta^2\\Sigma_{\\eta}\\Sigma_{W}\\Sigma_{\\eta} + \\Sigma_{\\varepsilon})J_k]^{-1}$. If any eigenvalue exceeds 1/2, the securities cannot simultaneously trade in equilibrium. This occurs when the insider's trading motive is largely informational, particularly when the securities' future values are highly correlated or when the insider's risk aversion is sufficiently low. The model demonstrates that introducing new securities can disrupt existing markets, especially if their payoffs are correlated with those of existing securities, leading to destructive interference.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3374",
    "question": "Which of the following statements about the codifference and covariation of SαS log-FSN are correct based on the provided theorems?",
    "options": [
      "A) The codifference τ_Y(t) ∼ P t^(1−α) as t → ∞, where P is a positive constant.",
      "B) The covariation [Y(t), Y(0)]_α ∼ Q t^(1−α) as t → ∞, where Q is a negative constant.",
      "C) The codifference is symmetric in its arguments when α = 2.",
      "D) The covariation is symmetric in its arguments for all 1 < α < 2."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "The-asymptotic-codifference-and-covariation-of-log-fra_2014_Journal-of-Econo.md"
    ],
    "question_context": "The codifference of SαS log-FSN satisfies τ_Y(t) ∼ P t^(1−α) as t → ∞, where P is a positive constant (Theorem 3.1). The covariation [Y(t), Y(0)]_α is symmetric in its arguments for all 1 < α < 2 and satisfies [Y(t), Y(0)]_α ∼ Q t^(1−α) as t → ∞, where Q is also a positive constant (Theorem 3.2). Key formulas: τ_Y(t) = 2∫_E |g(0,x)|^α m(dx) - ∫_E |g(t,x) - g(0,x)|^α m(dx) and [Y(t), Y(0)]_α = ∫_E g(t,x)g(0,x)^⟨α−1⟩ m(dx). When α = 2, the process becomes Gaussian, and the codifference coincides with the covariance, but symmetry is not guaranteed for codifference in the general SαS case.",
    "category": "INCORRECT",
    "question_context_used": "The codifference of SαS log-FSN satisfies τ_Y(t) ∼ P t^(1−α) as t → ∞, where P is a positive constant (Theorem 3.1). The covariation [Y(t), Y(0)]_α is symmetric in its arguments for all 1 < α < 2 and satisfies [Y(t), Y(0)]_α ∼ Q t^(1−α) as t → ∞, where Q is also a positive constant (Theorem 3.2). Key formulas: τ_Y(t) = 2∫_E |g(0,x)|^α m(dx) - ∫_E |g(t,x) - g(0,x)|^α m(dx) and [Y(t), Y(0)]_α = ∫_E g(t,x)g(0,x)^⟨α−1⟩ m(dx). When α = 2, the process becomes Gaussian, and the codifference coincides with the covariance, but symmetry is not guaranteed for codifference in the general SαS case.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1159",
    "question": "In the context of capital stock estimation from accumulation data, which of the following statements are correct regarding the treatment of fixed assets and circulating funds?",
    "options": [
      "A) Accumulation is treated as a net increase in capital stock, encompassing both fixed assets and circulating funds.",
      "B) Newly increased fixed assets are equivalent to investments in fixed assets by Chinese official usage.",
      "C) Circulating funds of state enterprises are distributed among sectors using ratios derived from comparison of net value of fixed assets and quota circulating funds.",
      "D) Initial values of circulating funds for agriculture in 1952 are explicitly recorded in the official data."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Capital Formation and Economic Growth in China.md"
    ],
    "question_context": "The analysis is based on official data on accumulation and capital stock estimates. Accumulation is defined as that part of the national income used for expanded reproduction, nonproductive construction, and increase of production and nonproductive stock of the society. Its material form includes newly added fixed assets of material and non-material sectors (less depreciation) and circulating funds. Productive accumulation includes newly added fixed assets of productive use in material production sectors and the increase in circulating assets held by enterprises. Nonproductive accumulation covers newly added fixed assets of nonproductive use and residential buildings, as well as the increase in stock of consumer goods. Accumulation is treated as a net increase in capital stock. Circulating funds of state enterprises are distributed among sectors using ratios derived from comparison of Table 2-7 (quota circulating funds) and Table 2-5 (net value of fixed assets). Initial values for agriculture in 1952 are not recorded in the official data.",
    "category": "INCORRECT",
    "question_context_used": "The analysis is based on official data on accumulation and capital stock estimates. Accumulation is defined as that part of the national income used for expanded reproduction, nonproductive construction, and increase of production and nonproductive stock of the society. Its material form includes newly added fixed assets of material and non-material sectors (less depreciation) and circulating funds. Productive accumulation includes newly added fixed assets of productive use in material production sectors and the increase in circulating assets held by enterprises. Nonproductive accumulation covers newly added fixed assets of nonproductive use and residential buildings, as well as the increase in stock of consumer goods. Accumulation is treated as a net increase in capital stock. Circulating funds of state enterprises are distributed among sectors using ratios derived from comparison of Table 2-7 (quota circulating funds) and Table 2-5 (net value of fixed assets). Initial values for agriculture in 1952 are not recorded in the official data.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-438",
    "question": "In the context of optimal search mechanisms for auctions with costly bidder participation, which of the following statements are true regarding the inefficiencies introduced by the seller's profit-maximizing strategy?",
    "options": [
      "A) The optimal mechanism leads to fewer participants compared to the socially efficient mechanism.",
      "B) The sequence of bidder entry is always efficient in the optimal mechanism.",
      "C) The optimal mechanism results in longer search durations conditional on the same set of participants.",
      "D) The seller's mechanism avoids all distortions present in static mechanism design problems."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Optimal-search-auctions_2007_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The paper studies the design of profit-maximizing single-unit auctions where the seller incurs costs to contact prospective bidders. The seller's problem is reduced to a search problem where the surplus is measured in terms of virtual utilities minus search costs. Key findings include that the optimal mechanism features fewer participants, longer search conditional on the same set of participants, and an inefficient sequence of entry compared to the socially efficient mechanism. The model allows for discounting and asymmetry among bidders, with interdependent valuations. The optimal search mechanisms introduce new types of distortions absent in static mechanism design, such as asymmetry of information leading to fewer participants and longer search durations. The formula for bidder i's value of the good is given by: $$u_{i}(x):=x_{i}+\\sum_{j\\in I\\backslash i}e_{i j}(x_{j}),$$ where $e_{i j}$ reflects bidder $j$'s influence on bidder $i$'s value.",
    "category": "CORRECT",
    "question_context_used": "The paper studies the design of profit-maximizing single-unit auctions where the seller incurs costs to contact prospective bidders. The seller's problem is reduced to a search problem where the surplus is measured in terms of virtual utilities minus search costs. Key findings include that the optimal mechanism features fewer participants, longer search conditional on the same set of participants, and an inefficient sequence of entry compared to the socially efficient mechanism. The model allows for discounting and asymmetry among bidders, with interdependent valuations. The optimal search mechanisms introduce new types of distortions absent in static mechanism design, such as asymmetry of information leading to fewer participants and longer search durations. The formula for bidder i's value of the good is given by: $$u_{i}(x):=x_{i}+\\sum_{j\\in I\\backslash i}e_{i j}(x_{j}),$$ where $e_{i j}$ reflects bidder $j$'s influence on bidder $i$'s value.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1043",
    "question": "Based on the findings discussed, which of the following statements are substantiated regarding financial literacy and household wealth?",
    "options": [
      "A) OLS estimates accurately capture the impact of financial literacy on pension contributions.",
      "B) IV estimates reveal financial literacy has a greater effect on wealth accumulation than schooling.",
      "C) Certain components of financial literacy, like the HRs 'core' questions, are disproportionately influential.",
      "D) Financial literacy has negligible effects on household net wealth via pension contributions."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Behrman-FinancialLiteracyAffects-2012.md"
    ],
    "question_context": "The study highlights that OLS estimates are likely misleading due to measurement error and unobserved factors, whereas IV estimates demonstrate financial literacy's greater importance over schooling in explaining household wealth and pension contributions. Key findings include the substantial impact of financial literacy, with specific components like the HRs 'core' questions being particularly influential. Additionally, financial literacy enhances the likelihood of pension contributions, thereby improving household net wealth and retirement well-being. As noted by Federal Reserve Board Chairman Ben Bernanke (2010), promoting financial literacy is crucial for enhancing family and community well-being.",
    "category": "CORRECT",
    "question_context_used": "The study highlights that OLS estimates are likely misleading due to measurement error and unobserved factors, whereas IV estimates demonstrate financial literacy's greater importance over schooling in explaining household wealth and pension contributions. Key findings include the substantial impact of financial literacy, with specific components like the HRs 'core' questions being particularly influential. Additionally, financial literacy enhances the likelihood of pension contributions, thereby improving household net wealth and retirement well-being. As noted by Federal Reserve Board Chairman Ben Bernanke (2010), promoting financial literacy is crucial for enhancing family and community well-being.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-869",
    "question": "According to modern financial economic theory and empirical evidence, which of the following statements are true regarding speculative price changes in an informationally efficient capital market?",
    "options": [
      "A) Speculative price changes follow martingale or submartingale processes under general assumptions.",
      "B) Changes in speculative prices occur only in response to new information or reassessments of existing information.",
      "C) The time series behavior of speculative price changes imposes no restrictions on macroeconomic models.",
      "D) Samuelson's theory applies only to futures prices and not to speculative prices in general."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "The-behavior-of-speculative-prices-and-the-consistency_1985_Journal-of-Econo.md"
    ],
    "question_context": "Modern financial economic theory suggests that changes in speculative prices should follow simple 'fair game' processes in an informationally efficient capital market. The observation that changes in speculative prices follow simple time series processes both supports this theoretical proposition and suggests restrictions on the transfer functions of structural econometric models in which speculative prices appear as output variables. The simplicity of the time series processes for observed changes in speculative prices are shown to impose strong restrictions on potential equilibrium models of asset pricing, informational disequilibrium models of financial markets, and many monetary and macroeconomic models as well.\n\nSamuelson demonstrates that changes in futures prices will follow martingale or submartingale processes under very general assumptions concerning the stochastic behavior of spot commodity prices. Although Samuelson's theory is illustrated with respect to futures prices it applies to fluctuations in speculative prices in general. The critical assumption is that speculative prices are determined in an informationally efficient capital market. Essentially, Samuelson develops a 'general stochastic model of price changes′ given the assumption of an informationally efficient capital market and without specifying an equilibrium model of asset pricing.",
    "category": "CORRECT",
    "question_context_used": "Modern financial economic theory suggests that changes in speculative prices should follow simple 'fair game' processes in an informationally efficient capital market. The observation that changes in speculative prices follow simple time series processes both supports this theoretical proposition and suggests restrictions on the transfer functions of structural econometric models in which speculative prices appear as output variables. The simplicity of the time series processes for observed changes in speculative prices are shown to impose strong restrictions on potential equilibrium models of asset pricing, informational disequilibrium models of financial markets, and many monetary and macroeconomic models as well.\n\nSamuelson demonstrates that changes in futures prices will follow martingale or submartingale processes under very general assumptions concerning the stochastic behavior of spot commodity prices. Although Samuelson's theory is illustrated with respect to futures prices it applies to fluctuations in speculative prices in general. The critical assumption is that speculative prices are determined in an informationally efficient capital market. Essentially, Samuelson develops a 'general stochastic model of price changes′ given the assumption of an informationally efficient capital market and without specifying an equilibrium model of asset pricing.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2560",
    "question": "Based on the empirical power analysis of Granger causality tests in distribution, which of the following statements are supported by the results?",
    "options": [
      "A) The Daniell kernel consistently yields higher rejection frequencies than the truncated uniform kernel across all DGPs.",
      "B) The Granger causality test in mean outperforms the test in distribution when causality stems from nonlinear mean effects.",
      "C) Rejection frequencies decrease with the truncation parameter M, as observed by Hong, Liu, and Wang (2009).",
      "D) For DGP3 (causality in variance), the test in distribution shows lower power due to tail dynamics and data scarcity."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "06-A_Nonparametric_Test_for_Granger_Causality_in_Distribution_With_Application_to_Financial_Conta.md"
    ],
    "question_context": "The empirical power analysis evaluates Granger causality tests in distribution under different DGPs. Key formula for DGP1: \n$$\n\\left\\{\\begin{array}{l l}{X_{t}=0.5X_{t-1}+0.4Y_{t-1}+u_{t,X},}\\ {u_{t,X}=\\sigma_{t,X}v_{t,X},}\\ {\\sigma_{t,X}^{2}=0.1+0.9\\sigma_{t-1,X}^{2}+0.08u_{t-1,X}^{2},}\\ {\\underset{{v_{t,X}}}{\\mathrm{iid}}N(0,1).}\\end{array}\\right.\n$$\nResults show that the Daniell kernel (DAN) achieves 84.40% rejection for (T,M)=(500,6) at η=5%, while the truncated uniform kernel (TR) yields only 55.40%. For DGP3 (causality in variance), rejection frequencies are lower due to tail dynamics and data scarcity, with DAN achieving 35.2% vs. TR's 36.60% for the same (T,M,η). The study also confirms that rejection frequencies decrease with M, aligning with prior findings.",
    "category": "CORRECT",
    "question_context_used": "The empirical power analysis evaluates Granger causality tests in distribution under different DGPs. Key formula for DGP1: \n$$\n\\left\\{\\begin{array}{l l}{X_{t}=0.5X_{t-1}+0.4Y_{t-1}+u_{t,X},}\\ {u_{t,X}=\\sigma_{t,X}v_{t,X},}\\ {\\sigma_{t,X}^{2}=0.1+0.9\\sigma_{t-1,X}^{2}+0.08u_{t-1,X}^{2},}\\ {\\underset{{v_{t,X}}}{\\mathrm{iid}}N(0,1).}\\end{array}\\right.\n$$\nResults show that the Daniell kernel (DAN) achieves 84.40% rejection for (T,M)=(500,6) at η=5%, while the truncated uniform kernel (TR) yields only 55.40%. For DGP3 (causality in variance), rejection frequencies are lower due to tail dynamics and data scarcity, with DAN achieving 35.2% vs. TR's 36.60% for the same (T,M,η). The study also confirms that rejection frequencies decrease with M, aligning with prior findings.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4502",
    "question": "Which of the following factors contributed to the success of Russia's 1995 economic stabilization program despite initial unfavorable conditions?",
    "options": [
      "A) Implementation of a currency board to stabilize the ruble exchange rate",
      "B) Direct credits to the budget from the central bank's loans",
      "C) Tight monetary policy leading to a sharp decrease in inflation rates",
      "D) Utilization of a nominal anchor in the form of fixed hard-currency exchange rate"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Gaidar-IMFRussia-1997.md"
    ],
    "question_context": "The 1995 economic stabilization program in Russia was implemented under extremely unfavorable conditions, including the Chechenya war and a lack of trust in the government's efforts. Despite these challenges, the program achieved significant success. Key formula: The inflation rate dropped from 200-250% annualized in early 1995 to 3% monthly by the end of the year. The program's orthodox character rejected direct credits to the budget and instead focused on tightening monetary policy. The introduction of a currency board later stabilized the ruble exchange rate, preventing overshooting and speculative pressures. The inflation rate further decreased to 22% in 1996, demonstrating the program's long-term effectiveness.",
    "category": "INCORRECT",
    "question_context_used": "The 1995 economic stabilization program in Russia was implemented under extremely unfavorable conditions, including the Chechenya war and a lack of trust in the government's efforts. Despite these challenges, the program achieved significant success. Key formula: The inflation rate dropped from 200-250% annualized in early 1995 to 3% monthly by the end of the year. The program's orthodox character rejected direct credits to the budget and instead focused on tightening monetary policy. The introduction of a currency board later stabilized the ruble exchange rate, preventing overshooting and speculative pressures. The inflation rate further decreased to 22% in 1996, demonstrating the program's long-term effectiveness.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"C\", \"D\"]\n}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-696",
    "question": "Based on the analysis of the merit scholarship program in Busia and Teso districts, which of the following statements are supported by the data?",
    "options": [
      "A) The program significantly increased teacher attendance in Busia but not in Teso.",
      "B) The program had a statistically significant positive impact on test scores for girls across all baseline quartiles.",
      "C) The program led to a significant increase in the number of textbooks at home for girls in the pooled Busia and Teso sample.",
      "D) The program's impact on teacher attendance was concentrated among teachers who taught at least one grade 6 class."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Kremer-INCENTIVESLEARN-2009.md"
    ],
    "question_context": "The estimated program impact on overall teacher school attendance in the pooled Busia and Teso sample is large and statistically significant at 4.8 percentage points (standard error 2.0 percentage points, table 7, panel A, regression 1). However, the strongest effects are in Busia district: the impact on teacher attendance there was 7.0 percentage points (standard error 2.4, significant at 99% confidence, table 7, panel A, regression 2). If all attendance gains were concentrated among teachers who teach at least one grade 6 class (62.9% of all teachers), the implied program effect for this subset would be an even larger 7.6 percentage point increase in attendance. Additionally, the program had positive estimated treatment effects for girls throughout the baseline test score distribution (table 6), indicating broad-based impacts beyond just the top performers.",
    "category": "CORRECT",
    "question_context_used": "The estimated program impact on overall teacher school attendance in the pooled Busia and Teso sample is large and statistically significant at 4.8 percentage points (standard error 2.0 percentage points, table 7, panel A, regression 1). However, the strongest effects are in Busia district: the impact on teacher attendance there was 7.0 percentage points (standard error 2.4, significant at 99% confidence, table 7, panel A, regression 2). If all attendance gains were concentrated among teachers who teach at least one grade 6 class (62.9% of all teachers), the implied program effect for this subset would be an even larger 7.6 percentage point increase in attendance. Additionally, the program had positive estimated treatment effects for girls throughout the baseline test score distribution (table 6), indicating broad-based impacts beyond just the top performers.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-800",
    "question": "Based on the experimental evidence regarding wishful thinking and accuracy incentives, which of the following statements are supported by the data?",
    "options": [
      "A) Higher accuracy bonuses consistently reduce wishful thinking across all participant groups",
      "B) Participants who count dots exhibit higher responsiveness to accuracy incentives in terms of both accuracy and response time",
      "C) Self-reported concentration increases significantly with higher accuracy bonuses",
      "D) The null effect of accuracy incentives on wishful thinking suggests participants do not trade off psychological benefits against material costs at the margin"
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Anticipatory Anxiety and Wishful Thinking.md"
    ],
    "question_context": "Key findings from the experiments show that wishful thinking persists despite meaningful monetary costs, with an expected cost of about £0.87 in Experiment 2. The data reveals that participants do care about accuracy incentives, as evidenced by increased response times (log response time coefficients: 0.0433 in Experiment 2 and 0.159 in Experiment 4) and self-reported concentration (coefficients: 0.145 in Experiment 2 and 0.204 in Experiment 4). However, this increased cognitive effort only translates to improved accuracy in Experiment 4 where the task was elastic to effort. \n\nThe results specifically show differential effects among dot-counters, with 'Always' counters achieving 88% accuracy versus 73% for 'Never' counters. The augmented model proposed in the study suggests self-deception operates more like an automatic process (System 1), constrained primarily by signal precision rather than through explicit cost-benefit calculations. The equation for the regression of cognitive effort shows significant coefficients for high accuracy bonus (HAB) on both log response time (0.159, p<0.0179) and concentration (0.204, p<0.0229) in Experiment 4.",
    "category": "INCORRECT",
    "question_context_used": "Key findings from the experiments show that wishful thinking persists despite meaningful monetary costs, with an expected cost of about £0.87 in Experiment 2. The data reveals that participants do care about accuracy incentives, as evidenced by increased response times (log response time coefficients: 0.0433 in Experiment 2 and 0.159 in Experiment 4) and self-reported concentration (coefficients: 0.145 in Experiment 2 and 0.204 in Experiment 4). However, this increased cognitive effort only translates to improved accuracy in Experiment 4 where the task was elastic to effort. \n\nThe results specifically show differential effects among dot-counters, with 'Always' counters achieving 88% accuracy versus 73% for 'Never' counters. The augmented model proposed in the study suggests self-deception operates more like an automatic process (System 1), constrained primarily by signal precision rather than through explicit cost-benefit calculations. The equation for the regression of cognitive effort shows significant coefficients for high accuracy bonus (HAB) on both log response time (0.159, p<0.0179) and concentration (0.204, p<0.0229) in Experiment 4.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"C\"]\n}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3469",
    "question": "Which of the following conditions are necessary for the emergence of a Central Business District (CBD) according to the model?",
    "options": [
      "A) The percentage increase in business revenue from proximity to the city center must exceed the percentage increase in total costs from moving closer to the center.",
      "B) The land share parameter α must be exactly 0.5 to balance wage and rent effects.",
      "C) The externality parameter γ must be sufficiently high to outweigh the commuting cost parameter κ.",
      "D) Workers' expenditure share on land (1-β) must be sufficiently low to allow firms to outbid workers for central land."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "A TRACTABLE CITY MODEL FOR AGGREGATIVE ANALYSIS.md"
    ],
    "question_context": "The condition for the emergence of a CBD is given by: \n\n$$\\alpha\\kappa+(1-\\alpha)\\frac{\\kappa}{1-\\beta}<\\gamma\\delta.$$\n\nThe right-hand side represents the percentage increase in business revenue from proximity to the city center, while the left-hand side captures the percentage increase in total costs (wages and rents) from moving closer to the center. Key insights include:\n\n1) A CBD emerges when revenue gains (γδ) outweigh cost increases (weighted by α and 1-α).\n2) The condition simplifies to κ < γδ when α → 1 (land becomes unimportant in production).\n3) For α < 1, workers' land consumption (1-β) becomes crucial - if β is too close to 1, firms cannot outbid workers for central land.\n\nThe model shows that CBD formation depends critically on the interplay between production externalities (γ), commuting costs (κ), and land use parameters (α, β).",
    "category": "INCORRECT",
    "question_context_used": "The condition for the emergence of a CBD is given by: \n\n$$\\alpha\\kappa+(1-\\alpha)\\frac{\\kappa}{1-\\beta}<\\gamma\\delta.$$\n\nThe right-hand side represents the percentage increase in business revenue from proximity to the city center, while the left-hand side captures the percentage increase in total costs (wages and rents) from moving closer to the center. Key insights include:\n\n1) A CBD emerges when revenue gains (γδ) outweigh cost increases (weighted by α and 1-α).\n2) The condition simplifies to κ < γδ when α → 1 (land becomes unimportant in production).\n3) For α < 1, workers' land consumption (1-β) becomes crucial - if β is too close to 1, firms cannot outbid workers for central land.\n\nThe model shows that CBD formation depends critically on the interplay between production externalities (γ), commuting costs (κ), and land use parameters (α, β).",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3525",
    "question": "In the context of customs unions and trade diversion, which of the following statements are supported by the analysis of points A and C in Figure 1?",
    "options": [
      "A) Trade diversion always reduces welfare due to switching to a more expensive supplier.",
      "B) A trade-diverting customs union can raise welfare if it supports a higher-valued trade indifference curve.",
      "C) The trade diversion and trade creation terminology is exhaustive for all possible effects of customs unions.",
      "D) Welfare effects of customs unions depend on the magnitudes of opposing effects like terms-of-trade and volume-of-trade."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Welfare and Integration.md"
    ],
    "question_context": "The analysis of points A and C in Figure 1 reveals that trade diversion need not reduce welfare. Country a was assumed to specialize in production both before and after joining the customs union; thus no trade creation has taken place. However, since country a has switched to a more expensive supplier of its imports, it has experienced trade diversion, and yet point C supports a higher-valued trade indifference curve than does point A. Hence a trade-diverting customs union can raise welfare. Additionally, the formation of a customs union has the potential for causing changes that have opposing effects on welfare, making the welfare consequences dependent on the magnitudes of these effects. The shift from A to C can be decomposed into a move from A to B (welfare-worsening due to higher import price) and a move from B to C (welfare-improving due to increased imports of a good that commands a higher domestic price).",
    "category": "CORRECT",
    "question_context_used": "The analysis of points A and C in Figure 1 reveals that trade diversion need not reduce welfare. Country a was assumed to specialize in production both before and after joining the customs union; thus no trade creation has taken place. However, since country a has switched to a more expensive supplier of its imports, it has experienced trade diversion, and yet point C supports a higher-valued trade indifference curve than does point A. Hence a trade-diverting customs union can raise welfare. Additionally, the formation of a customs union has the potential for causing changes that have opposing effects on welfare, making the welfare consequences dependent on the magnitudes of these effects. The shift from A to C can be decomposed into a move from A to B (welfare-worsening due to higher import price) and a move from B to C (welfare-improving due to increased imports of a good that commands a higher domestic price).",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2072",
    "question": "In the context of international labor mobility with non-homogeneous time preferences, which of the following statements are consistent with Theorem 4's welfare implications?",
    "options": [
      "A) Unilateral migration worsens the welfare of non-migrants in the immigration country but improves it in the emigration country.",
      "B) Bilateral migration uniformly improves the welfare of all non-migrants, regardless of their preference type.",
      "C) Bilateral migration improves the welfare of non-migrants with identical preferences to migrants but worsens it for those with different preferences.",
      "D) Unilateral migration has no effect on the welfare of non-migrants in the emigration country."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Time-preference-and-international-labor-migrat_1986_Journal-of-Economic-Theo.md"
    ],
    "question_context": "The steady-state welfare implications of international labor mobility are analyzed through changes in indirect utility. Key formulas include:\n\n$$\n(1/\\lambda^{i})d\\tilde{u}^{i}=[1,(1/r^{i})s^{i}]d p^{i}\n$$\n\nwhere $\\lambda^{i}\\equiv\\partial\\tilde{u}^{i}/\\partial w^{i}$ is the marginal utility of income. Theorem 4 states that unilateral migration worsens welfare for non-migrants in the immigration country but improves it in the emigration country, while bilateral migration has differential effects based on preference alignment with migrants. The analysis assumes non-homogeneous time preferences within and across countries, with migration patterns driven by relative savings rates and capital-labor ratios.",
    "category": "CORRECT",
    "question_context_used": "The steady-state welfare implications of international labor mobility are analyzed through changes in indirect utility. Key formulas include:\n\n$$\n(1/\\lambda^{i})d\\tilde{u}^{i}=[1,(1/r^{i})s^{i}]d p^{i}\n$$\n\nwhere $\\lambda^{i}\\equiv\\partial\\tilde{u}^{i}/\\partial w^{i}$ is the marginal utility of income. Theorem 4 states that unilateral migration worsens welfare for non-migrants in the immigration country but improves it in the emigration country, while bilateral migration has differential effects based on preference alignment with migrants. The analysis assumes non-homogeneous time preferences within and across countries, with migration patterns driven by relative savings rates and capital-labor ratios.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4229",
    "question": "In the context of the EDR model with cognitive bounds and behavioral levels, which of the following conditions ensure that the behavioral level $k_i$ weakly increases when moving from the low payoff to the asymmetric payoff game?",
    "options": [
      "A) $c_j^i \\in C^+(c_i)$ and $c_i^{ij} \\in C^+(c_i)$",
      "B) $c_j^i \\in C^-(c_i)$ and $\\hat{k}_i = k_i$ in the low payoff game",
      "C) $v_i'(k) \\geq v_i(k)$ for all $k$ and $\\hat{k}_i' \\geq \\hat{k}_i$",
      "D) $c_j^i \\in C^+(c_i)$ and $c_i^{ij} \\in C^-(c_i)$ with $\\mathcal{K}(c_i^{ij}, v_{i(j)}) < \\hat{k}_i - 1$"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Reasoning-about-others--reasoning_2020_Journal-of-Economic-Theory.md"
    ],
    "question_context": "Key formula: The behavioral level $k_i$ is determined by $k_i = k_j^i + 1$, where $k_j^i$ is derived from $\\hat{k}_j^i = \\hat{k}_i - 1$ when $c_j^i \\in C^+(c_i)$. The cognitive bound $\\hat{k}_i$ is defined as $\\hat{k}_i := \\mathcal{K}(c_i, v_i)$, and it weakly increases to $\\hat{k}_i' := \\mathcal{K}(c_i, v_i')$ in the asymmetric payoff game if $v_i'(k) \\geq v_i(k)$ for all $k$. The conditions under which $k_i$ weakly increases depend on the cost configurations $c_j^i$ and $c_i^{ij}$ relative to $c_i$, as well as the value functions $v_i$ and $v_i'$.",
    "category": "CORRECT",
    "question_context_used": "Key formula: The behavioral level $k_i$ is determined by $k_i = k_j^i + 1$, where $k_j^i$ is derived from $\\hat{k}_j^i = \\hat{k}_i - 1$ when $c_j^i \\in C^+(c_i)$. The cognitive bound $\\hat{k}_i$ is defined as $\\hat{k}_i := \\mathcal{K}(c_i, v_i)$, and it weakly increases to $\\hat{k}_i' := \\mathcal{K}(c_i, v_i')$ in the asymmetric payoff game if $v_i'(k) \\geq v_i(k)$ for all $k$. The conditions under which $k_i$ weakly increases depend on the cost configurations $c_j^i$ and $c_i^{ij}$ relative to $c_i$, as well as the value functions $v_i$ and $v_i'$.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"D\"]}",
      "model_answer": [
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-9",
    "question": "Under what conditions does a steady-growth path exist in the given economic model?",
    "options": [
      "A) When the condition $s A^{*} \\geq \\mu + n$ is satisfied for some interval of $s$",
      "B) When $a(t)$ is constant and $b(t) = b(0)e^{-u}$",
      "C) When $\\bar{x}$ is infinite for all values of $s$",
      "D) When $\\lambda = s A^{*}$ and $\\bar{x}$ becomes infinite"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Fixed Factor Coefficients and Harrod-Neutral Technical Progress1.md"
    ],
    "question_context": "The condition $s A^{*} \\geq \\mu + n$ is crucial for the existence of a steady-growth path. If this condition is not satisfied for a value of $s$, then there does not exist $\\bar{x}$ which satisfies (15), and thus a steady-growth path does not exist for this value of $s$. Conversely, if this condition is satisfied for any value of $s$ in a certain interval, and $a(t) =$ constant and $b(t) = b(0)e^{-u}$, then there exists a steady-growth path for any value of $s$ in the interval. This is shown by Solow, Tobin, Weizsacker and Yaari. The analysis also considers the case where $\\bar{x}$ is infinite for some value of $s$, concluding that $\\bar{x}$ may be infinite for at most only one value of $s$ in a non-degenerate interval, and the arguments remain legitimate as $\\bar{x}$ is always finite for all (except at most one) values of $s$.",
    "category": "CORRECT",
    "question_context_used": "The condition $s A^{*} \\geq \\mu + n$ is crucial for the existence of a steady-growth path. If this condition is not satisfied for a value of $s$, then there does not exist $\\bar{x}$ which satisfies (15), and thus a steady-growth path does not exist for this value of $s$. Conversely, if this condition is satisfied for any value of $s$ in a certain interval, and $a(t) =$ constant and $b(t) = b(0)e^{-u}$, then there exists a steady-growth path for any value of $s$ in the interval. This is shown by Solow, Tobin, Weizsacker and Yaari. The analysis also considers the case where $\\bar{x}$ is infinite for some value of $s$, concluding that $\\bar{x}$ may be infinite for at most only one value of $s$ in a non-degenerate interval, and the arguments remain legitimate as $\\bar{x}$ is always finite for all (except at most one) values of $s$.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-376",
    "question": "In the context of dynamic tax incidence analysis within a neoclassical growth model, which of the following statements are correct regarding the adjustment path and welfare effects?",
    "options": [
      "A) The adjustment path for any variable depends solely on its own short- and long-run changes, independent of other variables.",
      "B) The welfare effect of a tax change is a weighted average of short- and long-run effects, with weights sensitive to behavioral assumptions and parameter values.",
      "C) The half-life of the system, denoted as T_0.5, guarantees uniform adjustment rates across all variables in the system.",
      "D) The social welfare function's gradient evaluated at the steady state is crucial for determining the welfare weight λ."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Note on Dynamic Tax Incidence.md"
    ],
    "question_context": "The paper presents a general framework for analyzing dynamic tax incidence in a neoclassical growth model. Key results include the derivation of the adjustment path between steady states, which is given by the equation: \n\n$$\\left\\langle{\\frac{d x}{d y}}\\right\\rangle_{t}=\\{A^{-1}(0)B(0)\\}^{t}\\left\\langle{\\frac{d x}{d y}}\\right\\rangle_{0}+\\{I-\\{A^{-1}(0)B(0)\\}^{t}\\}\\left\\langle{\\frac{d x}{d y}}\\right\\rangle_{\\infty},$$\n\nwhere $(d x/d y)_{\\infty}$ refers to the change in steady states. The welfare effect is shown to be a weighted average of short- and long-run effects, with the weight λ given by:\n\n$$\\lambda=\\rho/(1+\\rho-\\beta)$$\n\nwhere ρ is the social discount rate. The gradient of the social welfare function, evaluated at the steady state, plays a critical role in determining the overall welfare impact.",
    "category": "CORRECT",
    "question_context_used": "The paper presents a general framework for analyzing dynamic tax incidence in a neoclassical growth model. Key results include the derivation of the adjustment path between steady states, which is given by the equation: \n\n$$\\left\\langle{\\frac{d x}{d y}}\\right\\rangle_{t}=\\{A^{-1}(0)B(0)\\}^{t}\\left\\langle{\\frac{d x}{d y}}\\right\\rangle_{0}+\\{I-\\{A^{-1}(0)B(0)\\}^{t}\\}\\left\\langle{\\frac{d x}{d y}}\\right\\rangle_{\\infty},$$\n\nwhere $(d x/d y)_{\\infty}$ refers to the change in steady states. The welfare effect is shown to be a weighted average of short- and long-run effects, with the weight λ given by:\n\n$$\\lambda=\\rho/(1+\\rho-\\beta)$$\n\nwhere ρ is the social discount rate. The gradient of the social welfare function, evaluated at the steady state, plays a critical role in determining the overall welfare impact.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-963",
    "question": "In the context of adjusting output and productivity indexes for changes in the terms of trade, which of the following statements are consistent with the theoretical framework proposed by Diewert and Morrison?",
    "options": [
      "A) An increase in the relative price of exports has an effect analogous to an increase in total factor productivity.",
      "B) The traditional paradigm requires a general equilibrium framework to analyze traded goods.",
      "C) Changes in the terms of trade can be analyzed without distinguishing between export prices and domestic commodity prices.",
      "D) The deficit effect measures the impact of changes in the balance of trade deficit on output produced for domestic purchasers."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Adjusting_Output_and_Productivity_Indexes_for_Changes_in_the_Terms_of_Trade.md"
    ],
    "question_context": "The theoretical development and empirical implementation of our indexes requires recognition of three distinct classes of (net) outputs, and their corresponding prices, produced by the private business sector: (i) sales to domestic purchasers; (ii) sales to foreign purchasers; and (iii) purchases of foreign inputs. We view foreign commodity trade as passing through the private production sector: all commodity exports are produced by the production sector and all commodity imports enter the private production sector and are combined with transportation, wholesaling, retailing and other inputs before being sold to domestic purchasers. This treatment of trade allows us to answer hypothetical questions about internationally traded goods using only production theory, whereas in the traditional paradigm which treats traded goods as perfectly substitutable with a class of domestic goods, a general equilibrium framework is required. Consider a hypothetical increase in the relative price of an exported good between periods. If other factors remain constant, the economy can divert resources from export production toward domestic goods production and still maintain the previous balance of trade deficit or surplus. Thus the effect of an increase in export price is similar to an increase in total factor productivity.",
    "category": "INCORRECT",
    "question_context_used": "The theoretical development and empirical implementation of our indexes requires recognition of three distinct classes of (net) outputs, and their corresponding prices, produced by the private business sector: (i) sales to domestic purchasers; (ii) sales to foreign purchasers; and (iii) purchases of foreign inputs. We view foreign commodity trade as passing through the private production sector: all commodity exports are produced by the production sector and all commodity imports enter the private production sector and are combined with transportation, wholesaling, retailing and other inputs before being sold to domestic purchasers. This treatment of trade allows us to answer hypothetical questions about internationally traded goods using only production theory, whereas in the traditional paradigm which treats traded goods as perfectly substitutable with a class of domestic goods, a general equilibrium framework is required. Consider a hypothetical increase in the relative price of an exported good between periods. If other factors remain constant, the economy can divert resources from export production toward domestic goods production and still maintain the previous balance of trade deficit or surplus. Thus the effect of an increase in export price is similar to an increase in total factor productivity.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3885",
    "question": "Which of the following factors can lead to spurious evidence of long memory in stock-market data according to Lobato and Savin (1998)?",
    "options": [
      "A) Nonstationarity in the mean of squared returns",
      "B) Aggregation of weakly dependent series",
      "C) Short-term correlation in daily returns",
      "D) Use of robust semiparametric procedures"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "00-Real_and_Spurious_Long-Memory_Properties_of_Stock-Market_Data.md"
    ],
    "question_context": "The article discusses testing for long memory in daily stock returns and their squares using a robust semiparametric procedure. Spurious results can be produced by nonstationarity and aggregation. Nonstationarity is a plausible explanation for findings, especially in S&P 500 data from 1928 to 1992, where changes in the mean of squared returns occurred. Aggregation of independent weakly dependent series can also produce a strong dependent series, as seen in the squares of daily returns of the S&P 500. The authors address these problems by analyzing subperiods of returns and using individual stocks. Key formula: The test statistic has an asymptotic chi-squared distribution under the null hypothesis of weak dependence.",
    "category": "CORRECT",
    "question_context_used": "The article discusses testing for long memory in daily stock returns and their squares using a robust semiparametric procedure. Spurious results can be produced by nonstationarity and aggregation. Nonstationarity is a plausible explanation for findings, especially in S&P 500 data from 1928 to 1992, where changes in the mean of squared returns occurred. Aggregation of independent weakly dependent series can also produce a strong dependent series, as seen in the squares of daily returns of the S&P 500. The authors address these problems by analyzing subperiods of returns and using individual stocks. Key formula: The test statistic has an asymptotic chi-squared distribution under the null hypothesis of weak dependence.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-453",
    "question": "Under what conditions does the incidence of vertical integration exhibit a U-shaped relationship with the degree of product market competition χ?",
    "options": [
      "A) When the marginal efficiency of the manager's effort (ε₂) is large enough and the probability of imitation (η) is small relative to ε₂",
      "B) When the marginal efficiency of the manager's effort (ε₂) is large enough and the probability of imitation (η) is large enough relative to ε₂",
      "C) When the marginal efficiency of the manager's effort (ε₂) is small and the probability of imitation (η) is large enough relative to ε₂",
      "D) When the marginal efficiency of the manager's effort (ε₂) is small and the probability of imitation (η) is small relative to ε₂"
    ],
    "gold_answer": "B",
    "source_files": [
      "Aghion-VerticalIntegrationCompetition-2006.md"
    ],
    "question_context": "The entrepreneur's choice to integrate or not affects the manager's innovation effort, which is determined by maximizing expected profit. The effort under integration (eᵛ) and non-integration (eᴺ) are given by:\n\neᵛ = (π/2)^(ε₂)\n\neᴺ = (ηχ + (1-η)/2)^(ε₂) π^(ε₂)\n\nwhere ε₂ = (u₂ - 1)^(-1) measures the marginal efficiency of the manager's effort. The difference in payoffs (Uᵛ - Uᴺ) as a function of χ can exhibit a U-shape under specific conditions. Proposition 1 states that the incidence of vertical integration will be U-shaped when the marginal efficiency of the manager's effort (ε₂) is large enough and the probability of imitation (η) is large enough relative to ε₂.",
    "category": "CORRECT",
    "question_context_used": "The entrepreneur's choice to integrate or not affects the manager's innovation effort, which is determined by maximizing expected profit. The effort under integration (eᵛ) and non-integration (eᴺ) are given by:\n\neᵛ = (π/2)^(ε₂)\n\neᴺ = (ηχ + (1-η)/2)^(ε₂) π^(ε₂)\n\nwhere ε₂ = (u₂ - 1)^(-1) measures the marginal efficiency of the manager's effort. The difference in payoffs (Uᵛ - Uᴺ) as a function of χ can exhibit a U-shape under specific conditions. Proposition 1 states that the incidence of vertical integration will be U-shaped when the marginal efficiency of the manager's effort (ε₂) is large enough and the probability of imitation (η) is large enough relative to ε₂.",
    "gold_answer_parsed": [
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-501",
    "question": "In the context of producible and nonproducible public commodities, which of the following statements are correct regarding market failures and nonconvexities?",
    "options": [
      "A) Nonconvexities are fundamental to the Arrow/Starrett framework because it does not place bounds on externality observation by victim firms.",
      "B) Markets fail for producible public commodities whose disposal is costly, regardless of whether they are beneficial or detrimental.",
      "C) Nonproducible public commodities lead to fundamental technological nonconvexities if their aggregate endowment is bounded.",
      "D) Foley's public competitive equilibrium concept is efficient even in the presence of nonconvexities caused by external effects."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Externalities-and-fundamental-nonconvexities--A-reconciliatio_2010_Journal-o.md"
    ],
    "question_context": "The paper unifies the Arrow/Starrett (AS) and Boyd and Conley (BC) approaches to externality modeling by distinguishing between producible and nonproducible public commodities. Nonconvexities are associated with detrimental producible public commodities but not with nonproducible public commodities in BC. Markets fail for both beneficial and detrimental by-products. Nonconvexities imply that price-based equilibria, e.g., Pigovian tax equilibrium, may not be Pareto-efficient. Foley’s (1967, 1970) 'public competitive equilibrium' combines price and quantity signals with a unanimity criterion and restores the equivalence between equilibrium and efficiency.\n\nKey formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. The paper shows that Starrett type technological nonconvexities will remain fundamental when detrimental producible public commodities are present. However, it will also be seen that nonconvexities in the technologies of the victim firms will not, on their own, imply market failure. Recalling that these are producible goods, market failure results when these nonconvexities are combined with some peculiarities in the production technologies of the generating/externality producing firms.",
    "category": "INCORRECT",
    "question_context_used": "The paper unifies the Arrow/Starrett (AS) and Boyd and Conley (BC) approaches to externality modeling by distinguishing between producible and nonproducible public commodities. Nonconvexities are associated with detrimental producible public commodities but not with nonproducible public commodities in BC. Markets fail for both beneficial and detrimental by-products. Nonconvexities imply that price-based equilibria, e.g., Pigovian tax equilibrium, may not be Pareto-efficient. Foley’s (1967, 1970) 'public competitive equilibrium' combines price and quantity signals with a unanimity criterion and restores the equivalence between equilibrium and efficiency.\n\nKey formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. The paper shows that Starrett type technological nonconvexities will remain fundamental when detrimental producible public commodities are present. However, it will also be seen that nonconvexities in the technologies of the victim firms will not, on their own, imply market failure. Recalling that these are producible goods, market failure results when these nonconvexities are combined with some peculiarities in the production technologies of the generating/externality producing firms.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1957",
    "question": "In the context of the firm's production functions for commercial and government software, which of the following statements accurately describe the effects of increased government data access ($\\Delta d_{g}$) on commercial software production ($\\Delta q_{c}$)?",
    "options": [
      "A) The direct effect of $\\Delta d_{g}$ is always positive due to the shareability of government data.",
      "B) The indirect effect can be negative if non-shareable inputs $n_{c}$ are crowded out by government software production.",
      "C) The total effect $\\frac{\\Delta q_{c}}{\\Delta d_{g}}$ is guaranteed to be positive if private data $d_{p}$ is a perfect substitute for government data.",
      "D) The indirect effect can amplify the direct effect if shareable inputs $s$ increase due to learning-by-doing effects."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Data-intensive Innovation and the State Evidence from AI Firms in China.md"
    ],
    "question_context": "The production functions for commercial ($q_{c}$) and government ($q_{g}$) software are given by:\n\n$$\n\\begin{array}{l}{q_{c}=F_{c}\\left(d_{g},d_{p},s,n_{c};X,C\\right)}\\ {q_{g}=F_{g}\\left(d_{g},d_{p},s,n_{g};X,C\\right).}\\end{array}\n$$\n\nThe causal effect of government data on commercial software development is composed of: (i) a direct positive effect from data shareability, and (ii) an indirect effect that can amplify or dampen the direct effect. The indirect effect augments the direct effect when shareable inputs $s$ increase (e.g., through learning-by-doing), but can offset it when non-shareable inputs $n_{c}$ are crowded out or when private data $d_{p}$ is substituted by $d_{g}$. The total effect $\\frac{\\Delta q_{c}}{\\Delta d_{g}}$ can be positive, nil, or negative depending on these mechanisms.",
    "category": "INCORRECT",
    "question_context_used": "The production functions for commercial ($q_{c}$) and government ($q_{g}$) software are given by:\n\n$$\n\\begin{array}{l}{q_{c}=F_{c}\\left(d_{g},d_{p},s,n_{c};X,C\\right)}\\ {q_{g}=F_{g}\\left(d_{g},d_{p},s,n_{g};X,C\\right).}\\end{array}\n$$\n\nThe causal effect of government data on commercial software development is composed of: (i) a direct positive effect from data shareability, and (ii) an indirect effect that can amplify or dampen the direct effect. The indirect effect augments the direct effect when shareable inputs $s$ increase (e.g., through learning-by-doing), but can offset it when non-shareable inputs $n_{c}$ are crowded out or when private data $d_{p}$ is substituted by $d_{g}$. The total effect $\\frac{\\Delta q_{c}}{\\Delta d_{g}}$ can be positive, nil, or negative depending on these mechanisms.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-170",
    "question": "In the context of the model's dynamics, which of the following conditions are necessary for the system to exhibit locally convergent trajectories around the low inflation equilibrium?",
    "options": [
      "A) The determinant of the Jacobian matrix is positive and its trace is negative",
      "B) The speed of adjustment of inflation β is sufficiently small (β < k e^k / α^2)",
      "C) The deficit is financed by printing money without any price flexibility",
      "D) The inflation tax πm becomes larger than the deficit to be financed U₁b"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Unemployment_and_Inflationary_Finance_Dynamics_at_the_Early_Stages_of_Transition.md"
    ],
    "question_context": "The model's dynamics around the low inflation equilibrium are analyzed using the Jacobian matrix evaluated at a steady state such as π = k/α, m = e^(-k), for a generic k ∈ (0,1). The Jacobian is given by:\n\n$${\\bf J}=\\begin{array}{l}{{\\displaystyle\\left(\\begin{array}{l l}{{\\frac{\\alpha\\beta}{e^{k}}}}&{{\\beta}}\\\\ {{-e^{-k}}}&{{-\\displaystyle\\frac{k}{\\alpha}}}\\end{array}\\right).}}\\end{array}$$\n\nFor locally convergent trajectories, Det (J) > 0 and, if β < k e^k / α^2, Trace (J) < 0. Thus both roots are negative, and the trajectories around the low inflation equilibrium are locally convergent. This condition ensures stability in the system's adjustment path following a shock.",
    "category": "CORRECT",
    "question_context_used": "The model's dynamics around the low inflation equilibrium are analyzed using the Jacobian matrix evaluated at a steady state such as π = k/α, m = e^(-k), for a generic k ∈ (0,1). The Jacobian is given by:\n\n$${\\bf J}=\\begin{array}{l}{{\\displaystyle\\left(\\begin{array}{l l}{{\\frac{\\alpha\\beta}{e^{k}}}}&{{\\beta}}\\\\ {{-e^{-k}}}&{{-\\displaystyle\\frac{k}{\\alpha}}}\\end{array}\\right).}}\\end{array}$$\n\nFor locally convergent trajectories, Det (J) > 0 and, if β < k e^k / α^2, Trace (J) < 0. Thus both roots are negative, and the trajectories around the low inflation equilibrium are locally convergent. This condition ensures stability in the system's adjustment path following a shock.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3836",
    "question": "Which of the following factors contributed to the success of Chinese entrepreneurs in Southeast Asia as discussed by Yuan-Li Wu?",
    "options": [
      "A) Strong reliance on government subsidies and protectionist policies",
      "B) Extensive use of informal credit networks and kinship ties",
      "C) Exclusive focus on large-scale industrial enterprises",
      "D) Adaptation to local market conditions and cultural integration"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Wu-ChineseEntrepreneursSoutheast-1983.md"
    ],
    "question_context": "Chinese entrepreneurs in Southeast Asia have historically succeeded due to their ability to leverage informal credit networks and kinship ties, which provided them with flexible financing options. Additionally, their success was bolstered by their adaptability to local market conditions and their efforts to integrate culturally, allowing them to navigate diverse economic landscapes effectively. These factors are highlighted in Yuan-Li Wu's analysis, which underscores the importance of social capital and local market adaptation in entrepreneurial success.",
    "category": "CORRECT",
    "question_context_used": "Chinese entrepreneurs in Southeast Asia have historically succeeded due to their ability to leverage informal credit networks and kinship ties, which provided them with flexible financing options. Additionally, their success was bolstered by their adaptability to local market conditions and their efforts to integrate culturally, allowing them to navigate diverse economic landscapes effectively. These factors are highlighted in Yuan-Li Wu's analysis, which underscores the importance of social capital and local market adaptation in entrepreneurial success.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1309",
    "question": "In the context of the disequilibrium consumption function and public sector preference functions, which of the following statements are correct based on the analysis provided?",
    "options": [
      "A) If leisure and consumption are Hicks substitutes and leisure is over-supplied, then ∂C/∂L > 0.",
      "B) The Hicksian notion of substitutability is sufficient for the Barro-Grossman hypothesis to hold under all conditions.",
      "C) Separability of the direct utility function implies horizontal and vertical ridge lines in the induced preference function.",
      "D) In cases of substitutes and under-supply or complements and over-supply, the sign of the Hicks substitute-complement relation is not sufficient to determine the direction of response to a change in the quantity constraint."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Mackay-ComparativeStaticsQuantity-1980.md"
    ],
    "question_context": "The utility function is given by U=U(C,m/P,l), where C is current consumption, m/P is additions to real money balances, and l is leisure time. The budget constraint is w(T0−l)+M0/P−C−m/P=0, with w as the real wage rate and M0/P as initial real money balances. A constraint on leisure time implies involuntary unemployment, leading to a disequilibrium consumption function C=C(w,M0/P,bl). The impact of a change in the constraint on leisure consumed shows that consumption demand and employment are positively related if consumption and leisure are Howard substitutes. Furthermore, if leisure and consumption are Hicks substitutes and leisure is over-supplied, then ∂C/∂l<0⇒∂C/∂L>0. The analysis also extends to public sector preference functions, where the slope of a ridge line is given by ∂xi/∂xj|xi|xj=−(∂2V/∂xi∂xj/∂2V/∂xi2)|xi|xj. Separability of the direct utility function does not imply separability of the induced preference function, which requires zero income elasticity of demand for the goods in question.",
    "category": "CORRECT",
    "question_context_used": "The utility function is given by U=U(C,m/P,l), where C is current consumption, m/P is additions to real money balances, and l is leisure time. The budget constraint is w(T0−l)+M0/P−C−m/P=0, with w as the real wage rate and M0/P as initial real money balances. A constraint on leisure time implies involuntary unemployment, leading to a disequilibrium consumption function C=C(w,M0/P,bl). The impact of a change in the constraint on leisure consumed shows that consumption demand and employment are positively related if consumption and leisure are Howard substitutes. Furthermore, if leisure and consumption are Hicks substitutes and leisure is over-supplied, then ∂C/∂l<0⇒∂C/∂L>0. The analysis also extends to public sector preference functions, where the slope of a ridge line is given by ∂xi/∂xj|xi|xj=−(∂2V/∂xi∂xj/∂2V/∂xi2)|xi|xj. Separability of the direct utility function does not imply separability of the induced preference function, which requires zero income elasticity of demand for the goods in question.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1454",
    "question": "In Deborah M. Figart's analysis of gender and unions in public sector labor markets, which of the following factors are identified as key determinants of wage differentials and internal labor market structures?",
    "options": [
      "A) The degree of unionization among female-dominated occupations",
      "B) The presence of collective bargaining agreements with gender-neutral pay scales",
      "C) The extent of occupational segregation by gender",
      "D) The implementation of merit-based promotion systems"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Figart-GenderUnionsInternal-1987.md"
    ],
    "question_context": "The study examines how gender and unionization interact within public sector labor markets in two states, focusing on wage differentials and internal labor market structures. A key finding is that occupational segregation by gender significantly influences wage disparities, with female-dominated occupations often exhibiting lower unionization rates. The analysis also highlights the role of collective bargaining in shaping pay scales, though its effectiveness in reducing gender wage gaps depends on the specific provisions of the agreements. The research underscores the importance of institutional factors, such as union density and occupational segregation, in determining labor market outcomes for women in the public sector.",
    "category": "INCORRECT",
    "question_context_used": "The study examines how gender and unionization interact within public sector labor markets in two states, focusing on wage differentials and internal labor market structures. A key finding is that occupational segregation by gender significantly influences wage disparities, with female-dominated occupations often exhibiting lower unionization rates. The analysis also highlights the role of collective bargaining in shaping pay scales, though its effectiveness in reducing gender wage gaps depends on the specific provisions of the agreements. The research underscores the importance of institutional factors, such as union density and occupational segregation, in determining labor market outcomes for women in the public sector.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2576",
    "question": "In the context of competing risks models for unemployment spell durations, which of the following statements are supported by the theoretical and empirical findings discussed?",
    "options": [
      "A) The new job finding rate exhibits positive duration dependence due to decreasing reservation wages as benefits exhaustion approaches.",
      "B) The recall hazard remains constant over time, unaffected by the proximity to UI benefits exhaustion.",
      "C) The overall escape rate from unemployment is the sum of the recall and new job finding rates, but their individual patterns differ significantly.",
      "D) The spike in the hazard function at the week of benefits exhaustion is primarily driven by seasonal fluctuations in the construction industry."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Unemployment Insurance, Recall Expectations, and Unemployment Outcomes.md"
    ],
    "question_context": "The duration of unemployment is typically analyzed using a standard job search model in which unemployed workers generate job offers by costly search. This approach leads to a single risk model of unemployment spell durations in which unemployment spells can only end through the finding of an acceptable new job. This formulation is less appropriate when analyzing the unemployment durations of workers on layoff with some possibility of recall. The prospect of recall affects the probability of leaving unemployment directly through the rate of actual recalls and indirectly by affecting worker search behavior. Katz [1986] finds in a standard job search model extended to include an exogenous recall probability that better recall prospects are likely to reduce the new job finding rate by raising the reservation wage and reducing the likelihood of search. This suggests that workers who expect to be recalled may have extremely long unemployment spells if their expectations are not fulfilled.\n\nThe statistical model of unemployment spell durations generated by job search models extended to allow for recalls is a competing risks model in which unemployment spells can end either through recall or through the finding of an acceptable new job. The predictions of standard job search models for how variables affect the escape rate from unemployment really refer to the new job finding rate and these predictions need not hold for the overall escape rate from unemployment (the sum of the recall and new job finding rates). Information on whether spells ended through recall or the finding of a new job allows an econometrician to estimate a competing risks model. The competing risks specification has the advantage of permitting one to identify the distinct impact of variables on the recall rate and the new job finding rates.",
    "category": "CORRECT",
    "question_context_used": "The duration of unemployment is typically analyzed using a standard job search model in which unemployed workers generate job offers by costly search. This approach leads to a single risk model of unemployment spell durations in which unemployment spells can only end through the finding of an acceptable new job. This formulation is less appropriate when analyzing the unemployment durations of workers on layoff with some possibility of recall. The prospect of recall affects the probability of leaving unemployment directly through the rate of actual recalls and indirectly by affecting worker search behavior. Katz [1986] finds in a standard job search model extended to include an exogenous recall probability that better recall prospects are likely to reduce the new job finding rate by raising the reservation wage and reducing the likelihood of search. This suggests that workers who expect to be recalled may have extremely long unemployment spells if their expectations are not fulfilled.\n\nThe statistical model of unemployment spell durations generated by job search models extended to allow for recalls is a competing risks model in which unemployment spells can end either through recall or through the finding of an acceptable new job. The predictions of standard job search models for how variables affect the escape rate from unemployment really refer to the new job finding rate and these predictions need not hold for the overall escape rate from unemployment (the sum of the recall and new job finding rates). Information on whether spells ended through recall or the finding of a new job allows an econometrician to estimate a competing risks model. The competing risks specification has the advantage of permitting one to identify the distinct impact of variables on the recall rate and the new job finding rates.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4770",
    "question": "In the structural estimation model of high school attendance and employment decisions, which of the following factors are explicitly incorporated into the utility function?",
    "options": [
      "A) The psychic value of school attendance, which depends on work activity and age",
      "B) The value of nonwork time, which has both systematic and stochastic components",
      "C) The potential wage returns to completing additional years of high school without graduating",
      "D) The utility from family transfers contingent on the youth's choices and school performance"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Eckstein-YouthsDropHigh-1999.md"
    ],
    "question_context": "The utility specification in the model is linear and additive in consumption, school attendance, and nonwork time. Consumption is the sum of the youth's earnings and family transfers, with the assumption that family transfers are not contingent on the youth's choices or school performance. The psychic value of school attendance, $b_{t}^{s j}$, depends on the level of work activity and varies with age both systematically and stochastically: $b_{t}^{s j}=\\overline{{b}}^{s j}(t)+\\epsilon_{t}^{s j}$ for $j=\\{n,p,f\\}$. Nonwork time is valued at $b_{t}^{n}$ dollars per hour and also has a random and systematic age-varying component: $b_{t}^{n}={\\overline{{b}}}^{n}(t)+\\epsilon_{t}^{n}$. The model explicitly rules out wage returns to additional years of high school without graduating and does not model family transfers as contingent on behavior or performance.",
    "category": "CORRECT",
    "question_context_used": "The utility specification in the model is linear and additive in consumption, school attendance, and nonwork time. Consumption is the sum of the youth's earnings and family transfers, with the assumption that family transfers are not contingent on the youth's choices or school performance. The psychic value of school attendance, $b_{t}^{s j}$, depends on the level of work activity and varies with age both systematically and stochastically: $b_{t}^{s j}=\\overline{{b}}^{s j}(t)+\\epsilon_{t}^{s j}$ for $j=\\{n,p,f\\}$. Nonwork time is valued at $b_{t}^{n}$ dollars per hour and also has a random and systematic age-varying component: $b_{t}^{n}={\\overline{{b}}}^{n}(t)+\\epsilon_{t}^{n}$. The model explicitly rules out wage returns to additional years of high school without graduating and does not model family transfers as contingent on behavior or performance.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1271",
    "question": "In the critique of Discounted Cash Flow (DCF) techniques presented by H. P. J. Heukensfeldt-Jansen, which of the following arguments are explicitly raised against the Net Present Value (NPV) criterion?",
    "options": [
      "A) NPV may sanction an end-loaded project ('late cash flow'), which the author considers problematic despite correct discounting.",
      "B) NPV requires unwarranted assumptions about cashing in working capital at historic cost at the project's end.",
      "C) NPV fails to account for the dual structure of the economy in its calculations.",
      "D) NPV does not incorporate the Profitability over the Initial Period (PIP) as a criterion."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Book_Notes.md"
    ],
    "question_context": "The book 'Project Evaluation and Discounted Cash Flow' by H. P. J. Heukensfeldt-Jansen critiques the use of DCF techniques in project evaluation. The author argues that since a company's profitability is judged by its net income, this should be the basis for analyzing investment projects rather than DCF. The DCF approach is criticized for summarizing multi-year data into a single figure, which may obscure unattractive individual years. The author specifically targets the NPV criterion for sanctioning 'late cash flow' projects, asserting that even with correct discounting, such projects should be subject to a cut-off. Additionally, the author claims DCF techniques rely on the dubious assumption that working capital can be liquidated at historic cost at the project's end. Alternative criteria proposed are Profitability over the Initial Period (PIP) and Profitability After the Initial Period (PAP), with industry-specific cut-off rates.",
    "category": "INCORRECT",
    "question_context_used": "The book 'Project Evaluation and Discounted Cash Flow' by H. P. J. Heukensfeldt-Jansen critiques the use of DCF techniques in project evaluation. The author argues that since a company's profitability is judged by its net income, this should be the basis for analyzing investment projects rather than DCF. The DCF approach is criticized for summarizing multi-year data into a single figure, which may obscure unattractive individual years. The author specifically targets the NPV criterion for sanctioning 'late cash flow' projects, asserting that even with correct discounting, such projects should be subject to a cut-off. Additionally, the author claims DCF techniques rely on the dubious assumption that working capital can be liquidated at historic cost at the project's end. Alternative criteria proposed are Profitability over the Initial Period (PIP) and Profitability After the Initial Period (PAP), with industry-specific cut-off rates.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-295",
    "question": "Based on the experimental results and balance assessment, which of the following statements are correct regarding the treatment groups and control variables?",
    "options": [
      "A) The double LASSO machine learning approach (DML) was used to select an optimal set of control variables to account for baseline differences in covariates.",
      "B) The randomisation was completely successful across all demographic characteristics and agricultural indices with no imbalances detected.",
      "C) An imbalance in the index for wheat management was observed between the AOE group and the control group.",
      "D) The area of cotton planted in 2010 showed an imbalance between the AO group and the control group, but this imbalance did not persist in 2011."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "‘Mobile’izing_Agricultural_Advice_Technology_Adoption_Diffusion_and_Sustainability.md"
    ],
    "question_context": "The study assesses balance between treatment groups (AOE, AO, Treat) and the control group using baseline survey data. Key findings include: The randomisation was largely successful across demographic characteristics (Panel A) and indices capturing information sources (Panel B). However, an imbalance exists in the index for wheat management between the AOE group and control, and another imbalance exists in the area of cotton planted between the AO group and the control group in 2010 but not in 2011. The double LASSO machine learning approach (DML) was adopted to systematically account for baseline differences in covariates across treatment groups, as proposed by Belloni et al. (2014). Formula: The DML approach minimizes the objective function $\\min_{\\beta} \\left\\| Y - X\\beta \\right\\|_2^2 + \\lambda \\left\\| \\beta \\right\\|_1$, where $Y$ is the outcome, $X$ is the matrix of covariates, and $\\lambda$ is the regularization parameter.",
    "category": "CORRECT",
    "question_context_used": "The study assesses balance between treatment groups (AOE, AO, Treat) and the control group using baseline survey data. Key findings include: The randomisation was largely successful across demographic characteristics (Panel A) and indices capturing information sources (Panel B). However, an imbalance exists in the index for wheat management between the AOE group and control, and another imbalance exists in the area of cotton planted between the AO group and the control group in 2010 but not in 2011. The double LASSO machine learning approach (DML) was adopted to systematically account for baseline differences in covariates across treatment groups, as proposed by Belloni et al. (2014). Formula: The DML approach minimizes the objective function $\\min_{\\beta} \\left\\| Y - X\\beta \\right\\|_2^2 + \\lambda \\left\\| \\beta \\right\\|_1$, where $Y$ is the outcome, $X$ is the matrix of covariates, and $\\lambda$ is the regularization parameter.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2944",
    "question": "Based on the randomized experiment in rural Bangladesh and India, which of the following statements are statistically significant findings regarding the effectiveness of health information campaigns?",
    "options": [
      "A) Phone calls alone improved awareness by 28–53 percentage points compared to text messages alone",
      "B) The combination of text messages and phone calls improved compliance by 2.2–2.7 standard deviations in India",
      "C) Automated interactive voice responses (IVR) were more effective than personal phone calls in raising compliance",
      "D) The impact on compliance persisted after three months, particularly among female participants"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "rest_a_01182.md"
    ],
    "question_context": "Key formula: The conditional average treatment effects (CATE) is computed using the function: $$ s_{0}(Z)=E[Y(1)|Z]-E[Y(0)|Z], $$ which is the difference in expected potential outcomes between the treated and nontreated individuals, conditional on covariates. The study finds that disseminating information through both text messages and phone calls (T3) is the most effective means of communication in improving rural people’s knowledge about COVID-19 precautions, followed by communications only via phone calls only (T2). Specifically, relative to people who only received text messages (T1), knowledge about COVID-19 precautions improved by 45–85 percentage points when they received both text messages and phone calls, whereas awareness among those who only received phone calls improved by 28–53 percentage points in both countries, both statistically significant at the $1\\%$ significance level. Furthermore, we also identify a significant improvement in people’s compliance with health guidelines. Both treatments 2 and 3 had positive impacts on an index of people’s compliance: between 1–1.5 standard deviations higher in Bangladesh and between 2.2–2.7 standard deviations higher in India (both statistically significant at the $1\\%$ level). The difference in compliance between treatments 2 and 3 is also statistically significant at the $1\\%$ level. In mid-August 2020, we conducted a second survey on roughly 1,600 randomly selected women participants from Bangladesh. We find that the impact of our campaign on compliance persists after three months.",
    "category": "CORRECT",
    "question_context_used": "Key formula: The conditional average treatment effects (CATE) is computed using the function: $$ s_{0}(Z)=E[Y(1)|Z]-E[Y(0)|Z], $$ which is the difference in expected potential outcomes between the treated and nontreated individuals, conditional on covariates. The study finds that disseminating information through both text messages and phone calls (T3) is the most effective means of communication in improving rural people’s knowledge about COVID-19 precautions, followed by communications only via phone calls only (T2). Specifically, relative to people who only received text messages (T1), knowledge about COVID-19 precautions improved by 45–85 percentage points when they received both text messages and phone calls, whereas awareness among those who only received phone calls improved by 28–53 percentage points in both countries, both statistically significant at the $1\\%$ significance level. Furthermore, we also identify a significant improvement in people’s compliance with health guidelines. Both treatments 2 and 3 had positive impacts on an index of people’s compliance: between 1–1.5 standard deviations higher in Bangladesh and between 2.2–2.7 standard deviations higher in India (both statistically significant at the $1\\%$ level). The difference in compliance between treatments 2 and 3 is also statistically significant at the $1\\%$ level. In mid-August 2020, we conducted a second survey on roughly 1,600 randomly selected women participants from Bangladesh. We find that the impact of our campaign on compliance persists after three months.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-897",
    "question": "In the context of signaling games with quadratic payoffs, which of the following conditions are necessary and sufficient for the existence and uniqueness of a linear Riley equilibrium?",
    "options": [
      "A) The marginal impact r is positive and the preference-aligned state θ₀ lies within the state space Θ",
      "B) The sender's ideal points a₁^S and a₂^S are nonlinear in the state θ",
      "C) The receiver's payoff u^R(θ,·,a₁) is maximized at a₂ = a^R(θ), which depends on the sender's action",
      "D) The state space Θ is unbounded and the sender's payoff is insensitive to the receiver's action (δ = 0)"
    ],
    "gold_answer": "A",
    "source_files": [
      "Linear-Riley-equilibria-in-quadratic-signaling-_2023_Journal-of-Economic-The.md"
    ],
    "question_context": "The paper studies signaling games with quadratic payoffs, focusing on the conditions for Riley equilibria to be linear. A sender, privately informed about the state θ ∈ Θ, takes an action a₁ ∈ ℝ. The receiver, after observing a₁, chooses an action a₂ ∈ ℝ. The sender's payoff is given by U^S(θ, a₂, a₁) = -(a₁ - a₁^S(θ))² - δ(a₂ - a₂^S(θ))², where a₁^S and a₂^S are the sender's ideal points, linear in θ. The receiver's payoff u^R(θ, a₂, a₁) is uniquely maximized at a₂ = a^R(θ) = θ, independent of a₁. The key result is Theorem 1: There exists a linear Riley equilibrium if and only if the marginal impact r ≡ δ(k₂ - 1) > 0 and the preference-aligned state θ₀ ≡ b₂/(1 - k₂) ∈ Θ. Moreover, whenever it exists, the linear Riley equilibrium is unique.",
    "category": "CORRECT",
    "question_context_used": "The paper studies signaling games with quadratic payoffs, focusing on the conditions for Riley equilibria to be linear. A sender, privately informed about the state θ ∈ Θ, takes an action a₁ ∈ ℝ. The receiver, after observing a₁, chooses an action a₂ ∈ ℝ. The sender's payoff is given by U^S(θ, a₂, a₁) = -(a₁ - a₁^S(θ))² - δ(a₂ - a₂^S(θ))², where a₁^S and a₂^S are the sender's ideal points, linear in θ. The receiver's payoff u^R(θ, a₂, a₁) is uniquely maximized at a₂ = a^R(θ) = θ, independent of a₁. The key result is Theorem 1: There exists a linear Riley equilibrium if and only if the marginal impact r ≡ δ(k₂ - 1) > 0 and the preference-aligned state θ₀ ≡ b₂/(1 - k₂) ∈ Θ. Moreover, whenever it exists, the linear Riley equilibrium is unique.",
    "gold_answer_parsed": [
      "A"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4326",
    "question": "In the context of English premium auctions (EPA) with symmetric interdependent values, which of the following statements are true regarding the net-premium effect and its implications?",
    "options": [
      "A) The net-premium effect implies that the expected utility for the net premium value equals the utility level if a bidder quits at the current price.",
      "B) The expected revenue in an EPA is invariant to changes in the bidders' risk preferences.",
      "C) For risk-neutral bidders, the net-premium effect reduces to a revenue equivalence result where the ex ante expected net premium value is zero.",
      "D) The inclusion of a premium in an EPA increases expected revenue when bidders are risk averse."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Premium-auctions-and-risk-preferences_2011_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The net-premium effect is a key result in the analysis of English premium auctions (EPA). It shows that, conditional on all updated information, every bidder’s expected utility for the net premium value equals his utility level if he quits at the current price. This effect holds for arbitrary utility functions and premium functions, providing a generalization of the revenue equivalence theorem for risk-neutral bidders. Specifically, for risk-neutral bidders, the ex ante expected net premium value is zero. The net-premium effect also has implications for expected revenue: Theorem 4 shows that the expected revenue in an EPA decreases (increases) when bidders are more risk averse (preferring). This leads to the conclusion that adding a premium to an English auction increases expected revenue when bidders are risk preferring and decreases it when they are risk averse.",
    "category": "CORRECT",
    "question_context_used": "The net-premium effect is a key result in the analysis of English premium auctions (EPA). It shows that, conditional on all updated information, every bidder’s expected utility for the net premium value equals his utility level if he quits at the current price. This effect holds for arbitrary utility functions and premium functions, providing a generalization of the revenue equivalence theorem for risk-neutral bidders. Specifically, for risk-neutral bidders, the ex ante expected net premium value is zero. The net-premium effect also has implications for expected revenue: Theorem 4 shows that the expected revenue in an EPA decreases (increases) when bidders are more risk averse (preferring). This leads to the conclusion that adding a premium to an English auction increases expected revenue when bidders are risk preferring and decreases it when they are risk averse.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-422",
    "question": "Which of the following statements are correct regarding the transformation of test scores under Hypothesis H1 in the study of schooling output quality?",
    "options": [
      "A) The transformed test score formula assumes constant mean and standard deviation across all subtests within the OECD sample.",
      "B) The coefficient of variation from the TIMSS subtests is used to adjust the original test scores.",
      "C) The formula does not account for differences in test designs between 1970 and 1994.",
      "D) The transformed test scores allow for direct quantitative comparisons of performance changes across countries."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Decline_of_Schooling_Productivity_in_OECD_Countries.md"
    ],
    "question_context": "The study addresses the challenge of measuring changes in the quality of schooling output by comparing cognitive achievement of pupils across OECD countries from 1970 to 1994. A key methodological tool is the transformation of test scores under Hypothesis H1, which assumes constant mean and standard deviation across all subtests within the OECD sample. The transformation formula is given by:\n\n$$ T_{t}^{i}(H1)=\\left(\\frac{S_{t}^{i}}{\\overline{{S}}_{t}}-1\\right)\\frac{(\\sigma/\\overline{{S}})_{T I M S S}}{(\\sigma/\\bar{S})_{t}}+1, $$\n\nwhere $T_{t}^{i}(H1)$ is the transformed test score for country $i$ in subtest $t$ under H1, $S_{t}^{i}$ is the original test score, $\\overline{{S}}_{t}$ is the mean of test scores of the OECD sample in subtest $t$, $(\\sigma/\\overline{{S}})_{T I M S S}$ is the average coefficient of variation from the TIMSS subtests, and $(\\sigma/\\overline{{S}})_{t}$ is the actual coefficient of variation in subtest $t$. This adjustment is necessary because the standard deviation of test results varied significantly between 1970 and 1994, which would otherwise lead to misleading comparisons.",
    "category": "INCORRECT",
    "question_context_used": "The study addresses the challenge of measuring changes in the quality of schooling output by comparing cognitive achievement of pupils across OECD countries from 1970 to 1994. A key methodological tool is the transformation of test scores under Hypothesis H1, which assumes constant mean and standard deviation across all subtests within the OECD sample. The transformation formula is given by:\n\n$$ T_{t}^{i}(H1)=\\left(\\frac{S_{t}^{i}}{\\overline{{S}}_{t}}-1\\right)\\frac{(\\sigma/\\overline{{S}})_{T I M S S}}{(\\sigma/\\bar{S})_{t}}+1, $$\n\nwhere $T_{t}^{i}(H1)$ is the transformed test score for country $i$ in subtest $t$ under H1, $S_{t}^{i}$ is the original test score, $\\overline{{S}}_{t}$ is the mean of test scores of the OECD sample in subtest $t$, $(\\sigma/\\overline{{S}})_{T I M S S}$ is the average coefficient of variation from the TIMSS subtests, and $(\\sigma/\\overline{{S}})_{t}$ is the actual coefficient of variation in subtest $t$. This adjustment is necessary because the standard deviation of test results varied significantly between 1970 and 1994, which would otherwise lead to misleading comparisons.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2037",
    "question": "In the context of the proof of Theorem 2, which of the following conditions are necessary for the identified set to converge to the true parameter as the sample size increases?",
    "options": [
      "A) The quantile functions must be uniformly consistent across all parameter spaces.",
      "B) The error distributions must have bounded positive densities in the neighborhood of their quantiles.",
      "C) The sample analogue criterion function must converge uniformly to the population criterion function.",
      "D) The lead times must be deterministic and known a priori."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Set-identification-of-the-censored-quantile-regression-mo_2015_Journal-of-Ec.md"
    ],
    "question_context": "Key formula: $\\operatorname*{lim}_{c\\to\\infty}Q_{\\varDelta U_{i}^{(2)}}(\\tau(1-\\tau)|\\varDelta x_{i,k}>c,\\mathbf{x}_{i,-k}\\in\\mathcal{X}_{i,-k})=0$. The proof of Theorem 2 involves establishing uniform consistency of the sample analogue criterion function, which requires several technical conditions. First, the quantile functions must be uniformly consistent across the parameter space, as shown by the convergence of $\\hat{G}(b:\\hat{Q},\\hat{h})$ to $G(b:Q,h)$. Second, the error distributions must have bounded positive densities near their quantiles, ensuring that the quantile estimators are well-behaved. Third, the uniform convergence of the sample criterion function to its population counterpart is essential for the Hausdorff consistency of the set estimator. These conditions collectively ensure that the identified set converges to the true parameter as the sample size grows.",
    "category": "CORRECT",
    "question_context_used": "Key formula: $\\operatorname*{lim}_{c\\to\\infty}Q_{\\varDelta U_{i}^{(2)}}(\\tau(1-\\tau)|\\varDelta x_{i,k}>c,\\mathbf{x}_{i,-k}\\in\\mathcal{X}_{i,-k})=0$. The proof of Theorem 2 involves establishing uniform consistency of the sample analogue criterion function, which requires several technical conditions. First, the quantile functions must be uniformly consistent across the parameter space, as shown by the convergence of $\\hat{G}(b:\\hat{Q},\\hat{h})$ to $G(b:Q,h)$. Second, the error distributions must have bounded positive densities near their quantiles, ensuring that the quantile estimators are well-behaved. Third, the uniform convergence of the sample criterion function to its population counterpart is essential for the Hausdorff consistency of the set estimator. These conditions collectively ensure that the identified set converges to the true parameter as the sample size grows.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3265",
    "question": "In the context of wage arrears reaction functions, which of the following statements are supported by the model's comparative static results and empirical findings?",
    "options": [
      "A) Wage arrears are positively related to the firm's cost of capital and managerial private benefits.",
      "B) Wage arrears are negatively related to worker monitoring difficulty and legal system effectiveness.",
      "C) The reaction function exhibits negative neighborhood effects, implying strategic substitution.",
      "D) The coefficient on Ωjt (local arrears) is statistically insignificant in IV specifications."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Earle-COMPLEMENTARITYCUSTOMWAGE-2009.md"
    ],
    "question_context": "The model derives the reaction function $\\pmb{\\varpi}=f(\\pmb{\\Omega},X,Z)$, where wage arrears $\\omega$ respond to local labor market conditions $\\Omega$, firm-specific factors $X$, and cost factors $Z$. Key comparative static results show:\n\n$$\\frac{d\\omega}{d X}=\\frac{r_{X}}{C_{\\infty}}>0\\mathrm{~and~}\\frac{d\\omega}{d Z}=\\frac{-C_{\\infty Z}}{C_{\\infty}}<0.$$\n\nThis implies wage arrears increase with the firm's cost of capital ($X$) and decrease with monitoring difficulty ($Z$). The reaction function's positive slope $(\\partial\\omega/\\partial\\Omega>0)$ is empirically confirmed through multiple specifications, including OLS, fixed effects, and IV approaches. Table 4 shows IV estimates of the coefficient on $\\Omega_{jt}$ ranging from 0.61 to 0.96, all statistically significant at least at the 10% level, rejecting strategic substitution (negative neighborhood effects).",
    "category": "INCORRECT",
    "question_context_used": "The model derives the reaction function $\\pmb{\\varpi}=f(\\pmb{\\Omega},X,Z)$, where wage arrears $\\omega$ respond to local labor market conditions $\\Omega$, firm-specific factors $X$, and cost factors $Z$. Key comparative static results show:\n\n$$\\frac{d\\omega}{d X}=\\frac{r_{X}}{C_{\\infty}}>0\\mathrm{~and~}\\frac{d\\omega}{d Z}=\\frac{-C_{\\infty Z}}{C_{\\infty}}<0.$$\n\nThis implies wage arrears increase with the firm's cost of capital ($X$) and decrease with monitoring difficulty ($Z$). The reaction function's positive slope $(\\partial\\omega/\\partial\\Omega>0)$ is empirically confirmed through multiple specifications, including OLS, fixed effects, and IV approaches. Table 4 shows IV estimates of the coefficient on $\\Omega_{jt}$ ranging from 0.61 to 0.96, all statistically significant at least at the 10% level, rejecting strategic substitution (negative neighborhood effects).",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4683",
    "question": "In the context of nonemployment spell duration analysis, which of the following statements accurately describe the properties or implications of the duration spline function $g(t,X_{2},\\alpha)$ and the benefit spline function $b(t,X_{3},\\theta)$?",
    "options": [
      "A) The duration spline $g(t,X_{2},\\alpha)$ is designed to capture monotonic duration dependence exclusively, without allowing for interactions with covariates.",
      "B) The benefit spline $b(t,X_{3},\\theta)$ divides the nonemployment spell into distinct periods, allowing the effect of UI benefits to vary by spell duration.",
      "C) The function $g(t,X_{2},\\alpha)$ incorporates cubic polynomials to model nonmonotonic duration dependence, switching between different polynomials over specified ranges of $t$.",
      "D) The specification of $b(t,X_{3},\\theta)$ assumes that the effect of UI benefits on hazard rates is constant throughout the entire nonemployment spell."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "02-Measuring the Influence of Unemployment Insurance on Unemployment Experiences.md"
    ],
    "question_context": "The duration spline $g(t,X_{2},\\alpha)$ and the benefit spline $b(t,X_{3},\\theta)$ are critical components in modeling the probability $P(X,t)$ of nonemployment spell durations. The duration spline is specified to capture nonmonotonic duration dependence, allowing the form of dependence to vary with covariates $X_{2}$. It uses cubic polynomials over different ranges of $t$, with smooth transitions controlled by normal cumulative distribution functions. For instance, $g(t,X_{2},\\alpha)$ switches from a cubic polynomial to a linear polynomial around $t=7$ weeks, and again to another linear polynomial around $t=39$ weeks.\n\nThe benefit spline $b(t,X_{3},\\theta)$ is designed to model interactions between UI benefits and duration dependence, particularly to capture 'exhaustion effects.' It divides the nonemployment spell into four 13-week periods, with distinct coefficients $\\theta_{k}$ for the covariates $X_{3}$ in each period. This allows the effect of UI benefits on hazard rates to vary depending on the spell duration, reflecting the changing incentives as benefit exhaustion approaches.",
    "category": "CORRECT",
    "question_context_used": "The duration spline $g(t,X_{2},\\alpha)$ and the benefit spline $b(t,X_{3},\\theta)$ are critical components in modeling the probability $P(X,t)$ of nonemployment spell durations. The duration spline is specified to capture nonmonotonic duration dependence, allowing the form of dependence to vary with covariates $X_{2}$. It uses cubic polynomials over different ranges of $t$, with smooth transitions controlled by normal cumulative distribution functions. For instance, $g(t,X_{2},\\alpha)$ switches from a cubic polynomial to a linear polynomial around $t=7$ weeks, and again to another linear polynomial around $t=39$ weeks.\n\nThe benefit spline $b(t,X_{3},\\theta)$ is designed to model interactions between UI benefits and duration dependence, particularly to capture 'exhaustion effects.' It divides the nonemployment spell into four 13-week periods, with distinct coefficients $\\theta_{k}$ for the covariates $X_{3}$ in each period. This allows the effect of UI benefits on hazard rates to vary depending on the spell duration, reflecting the changing incentives as benefit exhaustion approaches.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2582",
    "question": "Based on the empirical findings regarding intrafirm trade and foreign affiliates, which of the following statements are correct?",
    "options": [
      "A) Foreign affiliates are more likely to be established in larger countries with better governance.",
      "B) The share of intrafirm trade increases with both country size and governance quality after affiliate establishment.",
      "C) Improvements in country governance disproportionately affect products with low contractibility.",
      "D) The decision to establish a foreign affiliate is independent of sourcing decisions post-establishment."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Bernard-IntrafirmTradeProduct-2010.md"
    ],
    "question_context": "The literature on firms and international trade has focused attention on issues of contracting and the boundaries of the firm. Our results provide evidence on the role of country governance and product contractibility in determining intrafirm trade. We find evidence of selection: the decision to establish a foreign affiliate in a country differs from the choice of how much to source from the affiliate once it is established. While affiliates are more likely to be situated in countries that are larger and have better governance, once affiliates exist, the share of intrafirm trade is negatively related to both country size and country governance quality. Our results also highlight interactions between country and product characteristics—e.g., improvements in country governance matter most for products for which contracting is relatively difficult.",
    "category": "CORRECT",
    "question_context_used": "The literature on firms and international trade has focused attention on issues of contracting and the boundaries of the firm. Our results provide evidence on the role of country governance and product contractibility in determining intrafirm trade. We find evidence of selection: the decision to establish a foreign affiliate in a country differs from the choice of how much to source from the affiliate once it is established. While affiliates are more likely to be situated in countries that are larger and have better governance, once affiliates exist, the share of intrafirm trade is negatively related to both country size and country governance quality. Our results also highlight interactions between country and product characteristics—e.g., improvements in country governance matter most for products for which contracting is relatively difficult.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2896",
    "question": "In the context of the Mercantilist Trade Restrictiveness Index (MTRI), which of the following statements are correct regarding its theoretical properties and empirical implications?",
    "options": [
      "A) The MTRI always exceeds the trade-weighted average tariff when import demand is sufficiently elastic.",
      "B) The MTRI is defined as the uniform tariff that yields the same welfare as a given tariff structure.",
      "C) The MTRI cannot exceed the Trade Restrictiveness Index (TRI) uniform tariff.",
      "D) The MTRI correctly accounts for tariff revenue redistribution in general equilibrium."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "The Mercantilist Index of Trade Policy.md"
    ],
    "question_context": "The MTRI is introduced as an index of trade policy restrictiveness defined as the uniform tariff that maintains the same trade volume as a given tariff/quota structure. It overcomes the problems of the trade-weighted average tariff by avoiding substitution bias, correctly accounting for general equilibrium transfers, and taking import volume instead of welfare as benchmark. Key theoretical results include: the MTRI uniform tariff exceeds the trade-weighted average tariff if import demand is sufficiently elastic, and it cannot exceed the TRI uniform tariff. The MTRI is a general-equilibrium index that correctly accounts for tariff revenue, distinguishing it from other indices like the trade-weighted average tariff or the TRI, which focuses on welfare effects. Empirical applications confirm that trade-weighted average tariffs generally underestimate the true height of tariffs as measured by the MTRI, which in turn always underestimates the welfare-equivalent index.",
    "category": "INCORRECT",
    "question_context_used": "The MTRI is introduced as an index of trade policy restrictiveness defined as the uniform tariff that maintains the same trade volume as a given tariff/quota structure. It overcomes the problems of the trade-weighted average tariff by avoiding substitution bias, correctly accounting for general equilibrium transfers, and taking import volume instead of welfare as benchmark. Key theoretical results include: the MTRI uniform tariff exceeds the trade-weighted average tariff if import demand is sufficiently elastic, and it cannot exceed the TRI uniform tariff. The MTRI is a general-equilibrium index that correctly accounts for tariff revenue, distinguishing it from other indices like the trade-weighted average tariff or the TRI, which focuses on welfare effects. Empirical applications confirm that trade-weighted average tariffs generally underestimate the true height of tariffs as measured by the MTRI, which in turn always underestimates the welfare-equivalent index.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3740",
    "question": "Based on the analysis of price elasticities and markups in the automobile market, which of the following statements are consistent with the findings?",
    "options": [
      "A) First-time buyers exhibit higher price elasticity of demand compared to repeat buyers.",
      "B) Luxury and sports cars have the highest price elasticities among all vehicle classes.",
      "C) The estimated markups are consistent with dealer markups reported in Consumer Reports when accounting for manufacturer margins.",
      "D) Cross-price elasticities between domestic and foreign cars of the same class are higher than those between foreign cars of different classes."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Goldberg-ProductDifferentiationOligopoly-1995.md"
    ],
    "question_context": "The estimated own price elasticities of demand for specific products range from $-1.00$ to $-10.13$. The average unweighted elasticity is $-3.28$. All are greater than one in absolute value. A comparison of elasticities of demand across classes also shows an intuitive pattern. Table II reports price elasticities of demand by car size and country of origin. The most price elastic classes are those corresponding to intermediate and standard automobiles, with average elasticities of -4.6 and $-4.71$ respectively, while luxury and sports cars are the least price elastic--the average elasticities for these classes are $-2.02$ and $-1.35$ respectively. First time buyers are indeed more price elastic than repeat buyers. Their average price elasticity of demand is $-4.1$ as opposed to $-2.1$ for repeat buyers. According to Consumer Reports, dealer markups are between 15 and $25\\%$, and the reported ratio implies that manufacturer margins are between 22 and $50\\%$. These numbers are consistent with my estimates of manufacturer markups.",
    "category": "INCORRECT",
    "question_context_used": "The estimated own price elasticities of demand for specific products range from $-1.00$ to $-10.13$. The average unweighted elasticity is $-3.28$. All are greater than one in absolute value. A comparison of elasticities of demand across classes also shows an intuitive pattern. Table II reports price elasticities of demand by car size and country of origin. The most price elastic classes are those corresponding to intermediate and standard automobiles, with average elasticities of -4.6 and $-4.71$ respectively, while luxury and sports cars are the least price elastic--the average elasticities for these classes are $-2.02$ and $-1.35$ respectively. First time buyers are indeed more price elastic than repeat buyers. Their average price elasticity of demand is $-4.1$ as opposed to $-2.1$ for repeat buyers. According to Consumer Reports, dealer markups are between 15 and $25\\%$, and the reported ratio implies that manufacturer margins are between 22 and $50\\%$. These numbers are consistent with my estimates of manufacturer markups.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1612",
    "question": "Which of the following statements are true regarding the corrected maximum likelihood estimate in the context of the sample selection model?",
    "options": [
      "A) It remains consistent for the slope vector in the outcome equation even under model misspecification.",
      "B) It requires strict parametric assumptions about the joint error distribution.",
      "C) It demonstrates that Olsen's corrected ordinary least squares estimate is consistent under linear outcome equations without distributional assumptions.",
      "D) It is sensitive to departures from the underlying parametric assumptions like traditional methods."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Distribution-free-and-link-free-estimation-for-the-sam_1987_Journal-of-Econo.md"
    ],
    "question_context": "The prevalent estimation methods for the sample selection model rely heavily on parametric assumptions and are sensitive to departures from the underlying parametric assumptions [see, e.g. Goldberger (1983)]. We propose an alternative estimation method, the corrected maximum likelihood estimate, which is consistent for the slope vector in the outcome equation up to a multiplicative scalar, even through the parametric model on which the estimate is based might be misspecified. As an important corollary, it follows from our result that Olsen's (1980) corrected ordinary least squares estimate is consistent if the outcome equation is linear, without requiring Olsen's assumptions on the joint error distribution.",
    "category": "INCORRECT",
    "question_context_used": "The prevalent estimation methods for the sample selection model rely heavily on parametric assumptions and are sensitive to departures from the underlying parametric assumptions [see, e.g. Goldberger (1983)]. We propose an alternative estimation method, the corrected maximum likelihood estimate, which is consistent for the slope vector in the outcome equation up to a multiplicative scalar, even through the parametric model on which the estimate is based might be misspecified. As an important corollary, it follows from our result that Olsen's (1980) corrected ordinary least squares estimate is consistent if the outcome equation is linear, without requiring Olsen's assumptions on the joint error distribution.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2632",
    "question": "Based on the analysis of age discrimination in labor markets, which of the following statements are empirically supported by the findings?",
    "options": [
      "A) The Heckman critique correction eliminates evidence of age discrimination in retail sales",
      "B) Security jobs show weakly significant evidence of age discrimination after correction",
      "C) Janitorial jobs demonstrate strongly significant discrimination post-correction",
      "D) Experimental designs must account for variance differences in unobservables"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Neumark-ExperimentalAgeDiscrimination-2016.md"
    ],
    "question_context": "Our evidence points to more ambiguous evidence of age discrimination against older men than past research. For the three occupations we study-—-retail, security, and janitors——-the point estimates always indicate age discrimination; but the standard evidence is only strongly significant for retail. Moreover, the analysis indicates that conclusions are sensitive to accounting for the Heckman critique, adding to the ambiguity. The strongest evidence of age discrimination-—-in retail sales disappears completely once the estimate is corrected for the bias identified by this critique. For security and janitor jobs, in contrast, the evidence of discrimination strengthens, although it is significant—-and only weakly-just for security jobs. The more demanding analysis of the data results, quite naturally, in less precise estimates. Nonetheless, the sensitivity of the conclusions demonstrates the need to design experimental studies of labor market discrimination to have the capacity to correct for biases from differences in the variances of unobservables across the groups studied.",
    "category": "INCORRECT",
    "question_context_used": "Our evidence points to more ambiguous evidence of age discrimination against older men than past research. For the three occupations we study-—-retail, security, and janitors——-the point estimates always indicate age discrimination; but the standard evidence is only strongly significant for retail. Moreover, the analysis indicates that conclusions are sensitive to accounting for the Heckman critique, adding to the ambiguity. The strongest evidence of age discrimination-—-in retail sales disappears completely once the estimate is corrected for the bias identified by this critique. For security and janitor jobs, in contrast, the evidence of discrimination strengthens, although it is significant—-and only weakly-just for security jobs. The more demanding analysis of the data results, quite naturally, in less precise estimates. Nonetheless, the sensitivity of the conclusions demonstrates the need to design experimental studies of labor market discrimination to have the capacity to correct for biases from differences in the variances of unobservables across the groups studied.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2042",
    "question": "In the context of the extended model with pure warm glow preferences and focal points, which of the following statements are correct regarding the model's performance in explaining donation behavior?",
    "options": [
      "A) The model accurately predicts the response rate and mean donation amounts across all treatments T1-T6.",
      "B) The model underperforms in explaining the mass of donations at €20 in treatment T6.",
      "C) The model successfully explains the entire distribution of positive donations in treatment T5.",
      "D) The model's inability to explain behavior in T6 is due to the absence of income effects in the specification."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Huck-ComparingCharitableFundraising-2015.md"
    ],
    "question_context": "The extended model incorporates pure warm glow preferences and focal points to better explain donation behavior. Pure warm glow preferences are estimated for 33% of individuals, and focal attachment is significant at €50 and €100. The model performs well in explaining the distribution of donations in treatments T1-T5 but struggles with treatment T6. Key formula for utility with focal points: \n\n$$\\small{u(c,d_{r};\\theta)=\\xi_{k}+c+\\theta d_{r}-\\frac{\\alpha}{2}d_{r}^{2}.}$$ \n\nThe model's underperformance in T6 is attributed to the lack of income effects and specific factors influencing fixed gift treatments, which are not captured within the neoclassical framework.",
    "category": "CORRECT",
    "question_context_used": "The extended model incorporates pure warm glow preferences and focal points to better explain donation behavior. Pure warm glow preferences are estimated for 33% of individuals, and focal attachment is significant at €50 and €100. The model performs well in explaining the distribution of donations in treatments T1-T5 but struggles with treatment T6. Key formula for utility with focal points: \n\n$$\\small{u(c,d_{r};\\theta)=\\xi_{k}+c+\\theta d_{r}-\\frac{\\alpha}{2}d_{r}^{2}.}$$ \n\nThe model's underperformance in T6 is attributed to the lack of income effects and specific factors influencing fixed gift treatments, which are not captured within the neoclassical framework.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3073",
    "question": "In the context of the obsolescence model for computing equipment, which of the following statements are correct regarding the relationship between productive stocks and NIPA real wealth stocks?",
    "options": [
      "A) The productive stocks are always larger than NIPA real wealth stocks due to quality-adjusted depreciation rates.",
      "B) The obsolescence model's productive stocks for mainframes and terminals imply higher physical decay rates than NIPA stocks.",
      "C) The gap between productive stocks and NIPA stocks varies across equipment types due to differences in average age of the stocks.",
      "D) Printers are excluded from the obsolescence model's productive stock estimates because their depreciation follows a geometric pattern."
    ],
    "gold_answer": "C, D",
    "source_files": [
      "Whelan-ComputersObsolescenceProductivity-2002.md"
    ],
    "question_context": "The obsolescence model suggests that quality-adjusted depreciation rates for computing equipment are higher than physical decay rates, resulting in smaller NIPA real wealth stocks compared to productive stocks. The model provides an alternative estimation strategy using parameters like $s$, $\\delta$, $r$, and $\\gamma$ to simulate retirement age and depreciation schedules. Table 2 shows that the obsolescence model's depreciation schedules fit mainframes, storage devices, and terminals better than geometric alternatives, except for printers which exhibit geometric depreciation due to slower technological change ($\\gamma-g$ being small).\n\nFigure 4 compares productive stocks implied by the obsolescence model with NIPA real wealth stocks, showing that productive stocks in 1997 were 24% to 72% higher than NIPA stocks, depending on the equipment type. This variation is partly due to differences in the average age of the stocks, with NIPA stocks placing lower weights on older machines. Printers are excluded from this comparison as the obsolescence model does not apply to them. The productive stock for PCs, treated symmetrically to mainframes, was 44% larger than the NIPA-style stock.",
    "category": "INCORRECT",
    "question_context_used": "The obsolescence model suggests that quality-adjusted depreciation rates for computing equipment are higher than physical decay rates, resulting in smaller NIPA real wealth stocks compared to productive stocks. The model provides an alternative estimation strategy using parameters like $s$, $\\delta$, $r$, and $\\gamma$ to simulate retirement age and depreciation schedules. Table 2 shows that the obsolescence model's depreciation schedules fit mainframes, storage devices, and terminals better than geometric alternatives, except for printers which exhibit geometric depreciation due to slower technological change ($\\gamma-g$ being small).\n\nFigure 4 compares productive stocks implied by the obsolescence model with NIPA real wealth stocks, showing that productive stocks in 1997 were 24% to 72% higher than NIPA stocks, depending on the equipment type. This variation is partly due to differences in the average age of the stocks, with NIPA stocks placing lower weights on older machines. Printers are excluded from this comparison as the obsolescence model does not apply to them. The productive stock for PCs, treated symmetrically to mainframes, was 44% larger than the NIPA-style stock.",
    "gold_answer_parsed": [
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2463",
    "question": "Based on the instrumental variable estimates of School Starting Age (SSA) effects, which of the following statements are empirically supported by the data?",
    "options": [
      "A) SSA has uniformly positive effects on discounted lifetime earnings across all demographic groups",
      "B) Women and individuals with low-educated parents exhibit positive employment responses to higher SSA during prime-age",
      "C) The initial loss of experience from delayed labor market entry leads to persistent negative wage effects for men until age 50",
      "D) With a 2% discount rate, SSA reduces the present value of lifetime earnings by 0.9% of prime-age earnings for the average worker"
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Life‐cycle_Effects_of_Age_at_School_Start.md"
    ],
    "question_context": "Key findings from the IV estimates: The top-left graph in Figure 9 shows that SSA initially causes earnings losses due to delayed labor market entry, but generates positive effects after age 55 through delayed retirement. Table 5 reveals differential effects by demographic group: women show a 0.7 percentage point increase in employment (column 2) while men exhibit a -0.8% wage effect (column 3). For individuals with low-educated parents, SSA raises employment by 0.7 percentage points (column 4). The persistent wage penalty for men is shown to last until age 50. Table 6 presents the present value calculations with the formula: PV = Σ(Earnings_age/(1+r)^(age-18)), where r is the discount rate. At r=0.02, the average worker experiences a -0.9% effect on discounted lifetime earnings relative to prime-age earnings.",
    "category": "CORRECT",
    "question_context_used": "Key findings from the IV estimates: The top-left graph in Figure 9 shows that SSA initially causes earnings losses due to delayed labor market entry, but generates positive effects after age 55 through delayed retirement. Table 5 reveals differential effects by demographic group: women show a 0.7 percentage point increase in employment (column 2) while men exhibit a -0.8% wage effect (column 3). For individuals with low-educated parents, SSA raises employment by 0.7 percentage points (column 4). The persistent wage penalty for men is shown to last until age 50. Table 6 presents the present value calculations with the formula: PV = Σ(Earnings_age/(1+r)^(age-18)), where r is the discount rate. At r=0.02, the average worker experiences a -0.9% effect on discounted lifetime earnings relative to prime-age earnings.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1767",
    "question": "In the context of optimal tax penalties for health insurance mandates, which of the following statements are supported by the empirical analysis?",
    "options": [
      "A) The optimal tax penalty for universal coverage is calculated to be 24.9% ($1,461), exceeding Massachusetts' actual penalty but aligning with proposed national reforms.",
      "B) A smaller tax penalty linearly increases welfare due to more elastic market demand, regardless of coverage levels.",
      "C) The net welfare effect peaks at a penalty of approximately 20%, with negligible impact from penalty calibration within the 16.2%-21.3% range.",
      "D) Non-pecuniary perceptions of the penalty (e.g., civic duty or resentment) have no measurable effect on coverage decisions or welfare estimates."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Hackmann-AdverseSelectionIndividual-2015.md"
    ],
    "question_context": "The study computes the optimal individual mandate penalty using empirical estimates of demand and cost curves, assuming linearity. The minimal tax penalty for universal coverage is derived as 24.9% ($1,461), which exceeds Massachusetts' penalty but resembles proposed national reforms. Equation (6) supports this calculation, accounting for average costs and post-reform markups. The welfare effect of the penalty is linear in coverage changes, but net welfare effects are hump-shaped, peaking at ~20%. Robustness checks confirm that penalty calibration within 16.2%-21.3% has minimal impact on net welfare estimates (4%-4.1% per individual/year). Non-pecuniary factors (e.g., civic duty) may influence behavior, but their effects are implicitly captured by penalty robustness. Key formula: The minimal penalty for universal coverage is derived from $\\text{Penalty} = \\text{Average Cost} + \\text{Markup} - \\text{Lowest WTP}$.",
    "category": "CORRECT",
    "question_context_used": "The study computes the optimal individual mandate penalty using empirical estimates of demand and cost curves, assuming linearity. The minimal tax penalty for universal coverage is derived as 24.9% ($1,461), which exceeds Massachusetts' penalty but resembles proposed national reforms. Equation (6) supports this calculation, accounting for average costs and post-reform markups. The welfare effect of the penalty is linear in coverage changes, but net welfare effects are hump-shaped, peaking at ~20%. Robustness checks confirm that penalty calibration within 16.2%-21.3% has minimal impact on net welfare estimates (4%-4.1% per individual/year). Non-pecuniary factors (e.g., civic duty) may influence behavior, but their effects are implicitly captured by penalty robustness. Key formula: The minimal penalty for universal coverage is derived from $\\text{Penalty} = \\text{Average Cost} + \\text{Markup} - \\text{Lowest WTP}$.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3733",
    "question": "Under the rational expectations framework described, which of the following conditions must hold for the variance of the observed OLS residual $E[u_{t+1}^2]$?",
    "options": [
      "A) $E[u_{t+1}^2] = \\sigma_{\\alpha}^2 + \\sigma_a^2$ where $\\sigma_{\\alpha}^2$ is the variance of the omitted information term",
      "B) $E[u_{t+1}^2] = \\sigma_a^2$ where $\\sigma_a^2$ is the variance of people's true forecast error",
      "C) $E[u_{t+1}^2] = \\sigma_{\\alpha}^2$ where $\\sigma_{\\alpha}^2$ is the variance of the omitted information term",
      "D) $E[u_{t+1}^2] = \\sigma_{\\alpha}^2 - \\sigma_a^2$ where $\\sigma_{\\alpha}^2$ is the variance of the omitted information term"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Hamilton-DeflationGreatDepression-1992.md"
    ],
    "question_context": "The paper models the relation between commodity prices and the aggregate price level using a rational-expectations framework. Key equations include the forecasting regression: $p_{t+1} = \\mathbf{x}_{t}^{\\prime}\\hat{\\mathbf{\\alpha}} + u_{t+1}$, where $u_{t+1}$ is the observed OLS residual. Under rational expectations, the variance of $u_{t+1}$ decomposes into two components: $E[u_{t+1}^2] = \\sigma_{\\alpha}^2 + \\sigma_a^2$, where $\\sigma_{\\alpha}^2$ is the variance of the omitted information term $\\alpha_t$ and $\\sigma_a^2$ is the variance of people's true forecast error $a_{t+1}$. This decomposition is critical for understanding how market expectations and forecasting errors interact in commodity price dynamics.",
    "category": "INCORRECT",
    "question_context_used": "The paper models the relation between commodity prices and the aggregate price level using a rational-expectations framework. Key equations include the forecasting regression: $p_{t+1} = \\mathbf{x}_{t}^{\\prime}\\hat{\\mathbf{\\alpha}} + u_{t+1}$, where $u_{t+1}$ is the observed OLS residual. Under rational expectations, the variance of $u_{t+1}$ decomposes into two components: $E[u_{t+1}^2] = \\sigma_{\\alpha}^2 + \\sigma_a^2$, where $\\sigma_{\\alpha}^2$ is the variance of the omitted information term $\\alpha_t$ and $\\sigma_a^2$ is the variance of people's true forecast error $a_{t+1}$. This decomposition is critical for understanding how market expectations and forecasting errors interact in commodity price dynamics.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2906",
    "question": "In the context of dynamic external economies and diseconomies, which of the following statements are correct regarding the optimal path and production set?",
    "options": [
      "A) If the activity set is convex, the production set will also be convex in the case of diseconomies.",
      "B) The converse of the convexity condition holds, meaning if the production set is convex, the activity set must also be convex.",
      "C) The optimal path under a positive discount rate coincides with the free trade path if the growth potential ratios are identical for both sectors.",
      "D) For the case of diseconomies, the production set remains convex even if the activity set is not convex."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Learning-by-Doing and Infant Industry Protection1.md"
    ],
    "question_context": "The model deals with dynamic external economies and has been reinterpreted in the static context. Using an entirely symmetric analysis one can tackle problems of exhaustible resources or bottlenecks with ease. In the case of diseconomies, the novel feature is that if the activity set is convex, so will be the production set. The converse does not hold though. Under a positive discount rate, if the zero-discount optimal path (free trade path) is optimal, (i) It coincides with the free trade path (zero-discount optimal path); (ii) The 'growth potential ratios' must always be identical for the two sectors. Key formula: $\\Gamma_{i}(t)=\\biggl[a_{i}(T)+\\delta\\int_{t}^{T}a_{i}(\\tau)e^{\\delta(T-\\tau)}d\\tau\\biggr]$.",
    "category": "CORRECT",
    "question_context_used": "The model deals with dynamic external economies and has been reinterpreted in the static context. Using an entirely symmetric analysis one can tackle problems of exhaustible resources or bottlenecks with ease. In the case of diseconomies, the novel feature is that if the activity set is convex, so will be the production set. The converse does not hold though. Under a positive discount rate, if the zero-discount optimal path (free trade path) is optimal, (i) It coincides with the free trade path (zero-discount optimal path); (ii) The 'growth potential ratios' must always be identical for the two sectors. Key formula: $\\Gamma_{i}(t)=\\biggl[a_{i}(T)+\\delta\\int_{t}^{T}a_{i}(\\tau)e^{\\delta(T-\\tau)}d\\tau\\biggr]$.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\"]}",
      "model_answer": [
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-320",
    "question": "Under what conditions does expected compensating variation provide a valid ranking of stochastic prices against stabilized prices, according to the analysis by L. Jay Helms?",
    "options": [
      "A) When consumer preferences satisfy stringent restrictions on attitudes toward risk",
      "B) When the utility function is linear in prices",
      "C) When the consumer's surplus is approximated by Marshallian consumer's surplus",
      "D) When the price stabilization policy is costless to implement"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Expected Consumer's Surplus and the Welfare Effects of Price Stabilization.md"
    ],
    "question_context": "In Section 2, we demonstrate that expected compensating variation is not in general a valid utility indicator. In essence, while utility comparisons under uncertainty require the use of the cardinal properties of (von Neumann-- Morgenstern) utility functions, the expected surplus measures depend only on ordinal preference rankings. Expected compensating variation, then, is completely insensitive to, and cannot reflect, the precise factor of interest in suchstudies: the consumer's attitudes toward risk. We then derive a set of restrictions on the utility function which are necessary in order for expected compensating variation to be a valid measure. We see that price stabilization policies can only be evaluated with compensating variation if consumer preferences are assumed to satisfy quite stringent requirements. This is in constrast to the familiar certainty case in which compensating variation always provides correct rankings.",
    "category": "INCORRECT",
    "question_context_used": "In Section 2, we demonstrate that expected compensating variation is not in general a valid utility indicator. In essence, while utility comparisons under uncertainty require the use of the cardinal properties of (von Neumann-- Morgenstern) utility functions, the expected surplus measures depend only on ordinal preference rankings. Expected compensating variation, then, is completely insensitive to, and cannot reflect, the precise factor of interest in suchstudies: the consumer's attitudes toward risk. We then derive a set of restrictions on the utility function which are necessary in order for expected compensating variation to be a valid measure. We see that price stabilization policies can only be evaluated with compensating variation if consumer preferences are assumed to satisfy quite stringent requirements. This is in constrast to the familiar certainty case in which compensating variation always provides correct rankings.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1866",
    "question": "In the context of functional principal component analysis of inflation expectations, which of the following statements are correct regarding the interpretation and properties of the principal components?",
    "options": [
      "A) The first principal component captures changes in the variance of the distributions of expectations.",
      "B) The second principal component captures distributional skew, as inferred from the harmonics.",
      "C) The principal component scores are unconditionally correlated due to their construction.",
      "D) The first three principal components exhibit high persistence, with first autocorrelations of 0.95, 0.73, and 0.69 respectively."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Heterogeneous-beliefs-and-the-Phillips-curv_2023_Journal-of-Monetary-Economi.md"
    ],
    "question_context": "Functional principal component analysis reveals that a handful of features are sufficient to account for the bulk of the variation in the distributions of expectations. Fig. 2, Panel b, displays the eight largest normalized eigenvalues associated with each principal component $\\mathsf{e}_{k}$ , and their cumulative sum. It can be seen that to explain 90, 95 or 99 percent of variation in the survey requires 2, 3, or 6 components respectively. The first component—which is associated with the largest eigenvalue—captures changes in the variance of the distributions. The second component captures distributional skew. The scores are mean zero by construction, and their variances are given by the eigenvalues. The components are highly persistent: on monthly data the first autocorrelations are 0.95, 0.73, and 0.69 for components 1, 2, and 3 respectively. The scores are unconditionally uncorrelated, again by construction, but do exhibit periods of comovement that are particularly evident in the early and late parts of the sample.",
    "category": "CORRECT",
    "question_context_used": "Functional principal component analysis reveals that a handful of features are sufficient to account for the bulk of the variation in the distributions of expectations. Fig. 2, Panel b, displays the eight largest normalized eigenvalues associated with each principal component $\\mathsf{e}_{k}$ , and their cumulative sum. It can be seen that to explain 90, 95 or 99 percent of variation in the survey requires 2, 3, or 6 components respectively. The first component—which is associated with the largest eigenvalue—captures changes in the variance of the distributions. The second component captures distributional skew. The scores are mean zero by construction, and their variances are given by the eigenvalues. The components are highly persistent: on monthly data the first autocorrelations are 0.95, 0.73, and 0.69 for components 1, 2, and 3 respectively. The scores are unconditionally uncorrelated, again by construction, but do exhibit periods of comovement that are particularly evident in the early and late parts of the sample.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-782",
    "question": "In the context of international trade and financial economics, which of the following PhD theses directly address the impact of trade liberalization on economic variables?",
    "options": [
      "A) 'Trade liberalisation in Mexico and its impact on exports, imports and the balance of payments' by Pacheco Lopez",
      "B) 'Currency crises and contagion' by Fazio",
      "C) 'Mauritian trade liberalisation policies and the reproductive economy: from theory to policy' by Blin",
      "D) 'Essays on the monetary policy transmission mechanism in industrial countries' by Giuliodori"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Thesis_Titles_for_Degrees_in_the_United_Kingdom_200304_and_200405.md"
    ],
    "question_context": "The text lists numerous PhD theses focusing on international trade, monetary policy, and financial economics. Two notable studies explicitly examine trade liberalization effects: Pacheco Lopez's work analyzes Mexico's trade liberalization impacts on exports, imports, and balance of payments, while Blin's thesis investigates Mauritian trade liberalization policies and their economic consequences. Key formula for trade impact analysis often involves: $\\Delta Trade = \\alpha + \\beta_1 Liberalization + \\beta_2 Controls + \\epsilon$. The other theses address related but distinct topics such as currency crises (Fazio) and monetary policy transmission mechanisms (Giuliodori), which are not directly focused on trade liberalization effects.",
    "category": "CORRECT",
    "question_context_used": "The text lists numerous PhD theses focusing on international trade, monetary policy, and financial economics. Two notable studies explicitly examine trade liberalization effects: Pacheco Lopez's work analyzes Mexico's trade liberalization impacts on exports, imports, and balance of payments, while Blin's thesis investigates Mauritian trade liberalization policies and their economic consequences. Key formula for trade impact analysis often involves: $\\Delta Trade = \\alpha + \\beta_1 Liberalization + \\beta_2 Controls + \\epsilon$. The other theses address related but distinct topics such as currency crises (Fazio) and monetary policy transmission mechanisms (Giuliodori), which are not directly focused on trade liberalization effects.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1804",
    "question": "Under which conditions does the Henry George Theorem (HGT) hold in the context of optimal city size and public good provision?",
    "options": [
      "A) When the city is of optimal size and expenditures on public goods equal differential land rents (DLR)",
      "B) When all individuals are identical and there is no land scarcity",
      "C) When the marginal social utility of private goods differs across individuals",
      "D) When the boundary of the city is fixed and the shadow rent on land in nonresidential use is greater than zero"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Aggregate Land Rents, Expenditure on Public Goods, and Optimal City Size.md"
    ],
    "question_context": "The Henry George Theorem (HGT) states that in a city of optimal size, expenditures on public goods just equal (imputed) land rents. This result holds whether or not the level of the pure public good is optimal. The analysis involves maximizing social welfare per capita subject to resource constraints, with individuals having a utility function U(C,T,P), where C is consumption of the private good, T is consumption of land, and P is the supply of the pure public good. The optimal city size condition requires that the marginal social benefit of increasing population equals the marginal social costs. Key formula: \n\n$$\nP = \\int_{0}^{t^{*}} \\frac{U_{T}}{U_{C}} \\phi \\: d t - \\left(N \\eta \\frac{U_{T}}{U_{C}} \\right)_{t^{*}}.\n$$\n\nThis equation shows that expenditure on public goods must equal differential land rents (DLR), which includes the shadow rent on land at the boundary of the city times the area of the city. The result also extends to cases where the city boundary is fixed or where the shadow rent on land in nonresidential use is greater than zero.",
    "category": "CORRECT",
    "question_context_used": "The Henry George Theorem (HGT) states that in a city of optimal size, expenditures on public goods just equal (imputed) land rents. This result holds whether or not the level of the pure public good is optimal. The analysis involves maximizing social welfare per capita subject to resource constraints, with individuals having a utility function U(C,T,P), where C is consumption of the private good, T is consumption of land, and P is the supply of the pure public good. The optimal city size condition requires that the marginal social benefit of increasing population equals the marginal social costs. Key formula: \n\n$$\nP = \\int_{0}^{t^{*}} \\frac{U_{T}}{U_{C}} \\phi \\: d t - \\left(N \\eta \\frac{U_{T}}{U_{C}} \\right)_{t^{*}}.\n$$\n\nThis equation shows that expenditure on public goods must equal differential land rents (DLR), which includes the shadow rent on land at the boundary of the city times the area of the city. The result also extends to cases where the city boundary is fixed or where the shadow rent on land in nonresidential use is greater than zero.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3243",
    "question": "In the context of asymmetric loss function estimation for FOMC inflation targeting, which of the following statements are supported by the empirical results?",
    "options": [
      "A) The slope coefficient for inflation gap below zero is larger in absolute value than for inflation gap above zero.",
      "B) The estimated inflation target remains stable around 1.5% across different forecast horizons.",
      "C) Financial variables have a stronger effect on positivity than on negativity in sentiment analysis.",
      "D) The loss function becomes symmetric when including farther-ahead forecasts of macroeconomic variables."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Taking the Fed at its Word A New Approach to Estimating Central Bank Objectives using Text Analysis.md"
    ],
    "question_context": "The results in column (4) are shown visually in Figure 3(c). The slope by which loss increases away from target is about the same (in absolute value) above and below the target. The coefficients on the inflation gap above and below zero indicate that negativity rises faster as inflation moves further below target than it does when inflation moves further above target. That is, the absolute value of the slope coefficient for the inflation gap below zero is larger than the slope coefficient for the inflation gap above zero. The 'Symmetry p-val' row at the bottom of Table 2 shows the p-value on the null hypothesis that $\\hat{\\delta}_{1}=-\\hat{\\delta}_{2}$ based on a t-test using the two-way clustered variance-covariance matrix. In the full specification, the difference in slopes is found to be significant, with a p-value of 0.001.\n\nWe see that the estimated loss function is quite similar across these alternative specifications. In particular, the estimated inflation target is little affected by including farther-ahead forecasts in the measures of the macroeconomic variables. For example, both the baseline specification, using averages of current-quarter and one-quarter-ahead forecasts, and the specification using averages of current-quarter through four-quarter-ahead forecasts, yield $\\hat{\\pi}^{*}\\approx1.5$.",
    "category": "CORRECT",
    "question_context_used": "The results in column (4) are shown visually in Figure 3(c). The slope by which loss increases away from target is about the same (in absolute value) above and below the target. The coefficients on the inflation gap above and below zero indicate that negativity rises faster as inflation moves further below target than it does when inflation moves further above target. That is, the absolute value of the slope coefficient for the inflation gap below zero is larger than the slope coefficient for the inflation gap above zero. The 'Symmetry p-val' row at the bottom of Table 2 shows the p-value on the null hypothesis that $\\hat{\\delta}_{1}=-\\hat{\\delta}_{2}$ based on a t-test using the two-way clustered variance-covariance matrix. In the full specification, the difference in slopes is found to be significant, with a p-value of 0.001.\n\nWe see that the estimated loss function is quite similar across these alternative specifications. In particular, the estimated inflation target is little affected by including farther-ahead forecasts in the measures of the macroeconomic variables. For example, both the baseline specification, using averages of current-quarter and one-quarter-ahead forecasts, and the specification using averages of current-quarter through four-quarter-ahead forecasts, yield $\\hat{\\pi}^{*}\\approx1.5$.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2076",
    "question": "In the context of optimal investment dynamics with time delays, which of the following conditions are necessary for the stability of detrended optimal investment trajectories?",
    "options": [
      "A) All roots of the characteristic equation $K(z)=0$ must have strictly negative real parts.",
      "B) The transversality condition must eliminate roots with real parts larger than or equal to $z_0 = -(g + g_{\\lambda})$.",
      "C) The initial investment profile must ensure $0 \\leq \\hat{c} \\leq A k(t)$ for all $t \\geq 0$.",
      "D) The detrended co-state variable $\\hat{\\lambda}(t)$ must be constant over time."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Vintage-capital-and-the-dynamics-of-the-AK-mod_2005_Journal-of-Economic-Theo.md"
    ],
    "question_context": "The stability of detrended optimal investment trajectories is governed by the characteristic equation $K(z) = z - (A - g) + A e^{-g T} e^{-z T}$, derived from the dynamics of detrended investment. From Lemma 13, $K(z)$ does not admit any root with real part in $[0, z_0[$, where $z_0 = -(g + g_{\\lambda}) > 0$. The transversality condition (17) eliminates roots with real parts larger than or equal to $z_0$, ensuring stability. Additionally, Proposition 11 establishes that the optimal detrended co-state variable $\\hat{\\lambda}(t)$ must be constant over time, i.e., $\\hat{\\lambda}(t) = \\hat{\\lambda} > 0$ for all $t \\geq 0$. The initial investment profile $\\hat{\\iota}_0(t)$ for $t \\in [-T, 0[$ must be specified to determine the optimal paths, but it does not directly ensure stability. The condition $0 \\leq \\hat{c} \\leq A k(t)$ is related to the admissibility of the interior solution but is not a stability condition per se.",
    "category": "INCORRECT",
    "question_context_used": "The stability of detrended optimal investment trajectories is governed by the characteristic equation $K(z) = z - (A - g) + A e^{-g T} e^{-z T}$, derived from the dynamics of detrended investment. From Lemma 13, $K(z)$ does not admit any root with real part in $[0, z_0[$, where $z_0 = -(g + g_{\\lambda}) > 0$. The transversality condition (17) eliminates roots with real parts larger than or equal to $z_0$, ensuring stability. Additionally, Proposition 11 establishes that the optimal detrended co-state variable $\\hat{\\lambda}(t)$ must be constant over time, i.e., $\\hat{\\lambda}(t) = \\hat{\\lambda} > 0$ for all $t \\geq 0$. The initial investment profile $\\hat{\\iota}_0(t)$ for $t \\in [-T, 0[$ must be specified to determine the optimal paths, but it does not directly ensure stability. The condition $0 \\leq \\hat{c} \\leq A k(t)$ is related to the admissibility of the interior solution but is not a stability condition per se.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-714",
    "question": "In the context of evaluating India's development strategy post-independence, which of the following methodological approaches are critiqued or highlighted in the text?",
    "options": [
      "A) Relying on intuitive assertions without formal counterfactual modeling",
      "B) Using aggregate production function models despite their perceived obsolescence",
      "C) Prioritizing presentable production on recycled paper over substantive content",
      "D) Detailed historical analysis backed by precise statistical evidence"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Why_Poor_People_Stay_Poor_Urban_Bias_in_World_Development.md"
    ],
    "question_context": "The text critiques the habit of substituting words for formal models in economic analysis, particularly in evaluating India's post-independence development. It highlights the insufficiency of intuitive claims (e.g., export orientation) without rigorous counterfactual modeling of implications like aggregate saving. Conversely, Tang and Liang's work on Taiwan is praised for its precision and statistical backing, while Tolley's aggregate production function model on Korea is noted—despite criticisms of its relevance—as an explicit formal approach. The Reynolds volume is commended for high-quality papers, whereas the Tolley volume is critiqued as 'lightweight' despite its trade focus.",
    "category": "INCORRECT",
    "question_context_used": "The text critiques the habit of substituting words for formal models in economic analysis, particularly in evaluating India's post-independence development. It highlights the insufficiency of intuitive claims (e.g., export orientation) without rigorous counterfactual modeling of implications like aggregate saving. Conversely, Tang and Liang's work on Taiwan is praised for its precision and statistical backing, while Tolley's aggregate production function model on Korea is noted—despite criticisms of its relevance—as an explicit formal approach. The Reynolds volume is commended for high-quality papers, whereas the Tolley volume is critiqued as 'lightweight' despite its trade focus.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2524",
    "question": "In the context of robust covariance matrix estimation for the joint vector $(y_{t}, f_{t})$, which of the following statements are correct regarding the proposed methodology?",
    "options": [
      "A) The covariance matrix $\\Sigma_{z}$ is estimated element-wise using Huber's M-estimator to achieve robustness against heavy-tailed distributions.",
      "B) The final estimator $\\widehat{\\Sigma}^{\\tau}$ is constructed by adding the thresholded matrix $\\widehat{\\Sigma}_{u}^{\\mathcal{T}}$ to the estimator of $B\\Sigma_{f}B^{T}$ derived from $\\widehat{\\Sigma}_{z}$.",
      "C) The thresholding step applied to $\\widehat{\\Sigma}_{u}$ ensures sparsity and improves the estimation error in terms of the max-norm and operator norm.",
      "D) The sample covariance matrix is always sufficient as the initial estimator in Step 1, regardless of the data distribution."
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Robust-covariance-estimation-for-approximate-factor_2019_Journal-of-Economet.md"
    ],
    "question_context": "The proposed robust estimation procedure involves three key steps. First, the covariance matrix $\\Sigma_{z}$ of the joint vector $(y_{t}, f_{t})$ is estimated element-wise using Huber's M-estimator, which is robust against heavy-tailed distributions. The estimator $\\widehat{\\Sigma}_{z}$ is partitioned into blocks $\\widehat{\\Sigma}_{11}, \\widehat{\\Sigma}_{12}, \\widehat{\\Sigma}_{21}, \\widehat{\\Sigma}_{22}$, from which an estimator of $\\Sigma_{u}$ is derived via the identity $\\widehat{\\Sigma}_{u} = \\widehat{\\Sigma}_{11} - \\widehat{\\Sigma}_{12}\\widehat{\\Sigma}_{22}^{-1}\\widehat{\\Sigma}_{21}$. A thresholding step is then applied to $\\widehat{\\Sigma}_{u}$ to enforce sparsity, yielding $\\widehat{\\Sigma}_{u}^{\\mathcal{T}}$ with guaranteed error bounds. The final estimator $\\widehat{\\Sigma}^{\\tau}$ is constructed by adding $\\widehat{\\Sigma}_{u}^{\\mathcal{T}}$ to the estimator of $B\\Sigma_{f}B^{T}$.\n\nKey formula: The final estimator is given by $$\\widehat{\\Sigma}^{\\tau} = \\widehat{\\Sigma}_{12}\\widehat{\\Sigma}_{22}^{-1}\\widehat{\\Sigma}_{21} + \\widehat{\\Sigma}_{u}^{\\tau}.$$ The theoretical analysis shows that under bounded fourth moments, the proposed estimator achieves optimal convergence rates in various norms, including the max-norm and operator norm. The sample covariance matrix is only sufficient as the initial estimator if the data is known to be generated from more restricted families, such as sub-Gaussian distributions.",
    "category": "CORRECT",
    "question_context_used": "The proposed robust estimation procedure involves three key steps. First, the covariance matrix $\\Sigma_{z}$ of the joint vector $(y_{t}, f_{t})$ is estimated element-wise using Huber's M-estimator, which is robust against heavy-tailed distributions. The estimator $\\widehat{\\Sigma}_{z}$ is partitioned into blocks $\\widehat{\\Sigma}_{11}, \\widehat{\\Sigma}_{12}, \\widehat{\\Sigma}_{21}, \\widehat{\\Sigma}_{22}$, from which an estimator of $\\Sigma_{u}$ is derived via the identity $\\widehat{\\Sigma}_{u} = \\widehat{\\Sigma}_{11} - \\widehat{\\Sigma}_{12}\\widehat{\\Sigma}_{22}^{-1}\\widehat{\\Sigma}_{21}$. A thresholding step is then applied to $\\widehat{\\Sigma}_{u}$ to enforce sparsity, yielding $\\widehat{\\Sigma}_{u}^{\\mathcal{T}}$ with guaranteed error bounds. The final estimator $\\widehat{\\Sigma}^{\\tau}$ is constructed by adding $\\widehat{\\Sigma}_{u}^{\\mathcal{T}}$ to the estimator of $B\\Sigma_{f}B^{T}$.\n\nKey formula: The final estimator is given by $$\\widehat{\\Sigma}^{\\tau} = \\widehat{\\Sigma}_{12}\\widehat{\\Sigma}_{22}^{-1}\\widehat{\\Sigma}_{21} + \\widehat{\\Sigma}_{u}^{\\tau}.$$ The theoretical analysis shows that under bounded fourth moments, the proposed estimator achieves optimal convergence rates in various norms, including the max-norm and operator norm. The sample covariance matrix is only sufficient as the initial estimator if the data is known to be generated from more restricted families, such as sub-Gaussian distributions.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2840",
    "question": "In the context of the steady-state equilibrium characterization with no entry and exit, which of the following statements are true regarding the aggregate productivity dynamics?",
    "options": [
      "A) The persistence of aggregate output, ρ, is independent of the tail index of the stationary productivity distribution.",
      "B) The unconditional variance of aggregate output scales at a rate slower than 1/N when the firm size distribution is fat-tailed.",
      "C) The conditional variance of aggregate output is directly proportional to the dispersion of firm size at time t.",
      "D) The persistence of aggregate output decreases with the probability that firms do not change their productivity from one period to the next."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Carvalho-LargeFirmDynamics-2019.md"
    ],
    "question_context": "Key formula: The persistence of aggregate output, ρ, is given by ρ = aφ^(-1/(1-α)) + b + cφ^(1/(1-α)), where a, b, and c are parameters of the Markovian process for firm-level productivity. The unconditional variance of aggregate output scales as (ϱG₁)/N^(2-2/(δ(1-α))) when 1 < δ(1-α) < 2, where ϱ is the variance of firm-level output growth and G₁ is a function of model parameters. The conditional variance of aggregate output is given by varₜ[Ŷₜ₊₁] = ψϱ(Dₜ/A²), where ψ is a constant, Dₜ is proportional to the second moment of the firm size distribution at time t, and A is the steady-state value of aggregate productivity.\n\nProposition 4 states that the unconditional expectation of the variance of aggregate output scales at a slower rate than 1/N when the firm size distribution is fat-tailed (1 < δ(1-α) < 2). Furthermore, the dynamics of the conditional variance of aggregate output depend positively on the dispersion of firm size, as ∂varₜ[Ŷₜ₊₁]/∂Dₜ = ψϱ/A² ≥ 0. These results highlight the endogenous link between micro-level dispersion and macro-level volatility in the model.",
    "category": "CORRECT",
    "question_context_used": "Key formula: The persistence of aggregate output, ρ, is given by ρ = aφ^(-1/(1-α)) + b + cφ^(1/(1-α)), where a, b, and c are parameters of the Markovian process for firm-level productivity. The unconditional variance of aggregate output scales as (ϱG₁)/N^(2-2/(δ(1-α))) when 1 < δ(1-α) < 2, where ϱ is the variance of firm-level output growth and G₁ is a function of model parameters. The conditional variance of aggregate output is given by varₜ[Ŷₜ₊₁] = ψϱ(Dₜ/A²), where ψ is a constant, Dₜ is proportional to the second moment of the firm size distribution at time t, and A is the steady-state value of aggregate productivity.\n\nProposition 4 states that the unconditional expectation of the variance of aggregate output scales at a slower rate than 1/N when the firm size distribution is fat-tailed (1 < δ(1-α) < 2). Furthermore, the dynamics of the conditional variance of aggregate output depend positively on the dispersion of firm size, as ∂varₜ[Ŷₜ₊₁]/∂Dₜ = ψϱ/A² ≥ 0. These results highlight the endogenous link between micro-level dispersion and macro-level volatility in the model.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3028",
    "question": "Based on the statistical properties of natural disasters in Latin America, which of the following statements are supported by the data?",
    "options": [
      "A) The distribution of disaster intensity exhibits positive skewness and heavy tails due to extreme events",
      "B) Floods have the highest predictability among all disaster types in the region",
      "C) Tropical cyclones consistently cause the highest number of fatalities per event",
      "D) Earthquakes demonstrate the greatest variability in economic damage costs"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "The-legacy-of-natural-disasters--The-intergenerational_2017_Journal-of-Devel.md"
    ],
    "question_context": "The text highlights that Latin America experiences diverse natural disasters with varying statistical properties. Key findings include: The distribution of disaster intensity shows positive skewness (mean > 75th percentile) and heavy tails (high kurtosis), indicating extreme events significantly affect the distribution. For floods, while more than four occur annually, their consequences range from minimal damage to extreme cases like the 1982 El Salvador floods causing $280 million in damages. Earthquakes show particularly high variability, with some causing minimal damage while others like the 1939 Chilean earthquake killed nearly 30,000 people. The economic damage from earthquakes varies greatly, with one in four causing over $180 million in damage. Formula for damage variability: $Damage_{earthquake} \\sim \\mu + \\sigma Z$ where $\\sigma$ is particularly large for earthquakes compared to other disaster types.",
    "category": "CORRECT",
    "question_context_used": "The text highlights that Latin America experiences diverse natural disasters with varying statistical properties. Key findings include: The distribution of disaster intensity shows positive skewness (mean > 75th percentile) and heavy tails (high kurtosis), indicating extreme events significantly affect the distribution. For floods, while more than four occur annually, their consequences range from minimal damage to extreme cases like the 1982 El Salvador floods causing $280 million in damages. Earthquakes show particularly high variability, with some causing minimal damage while others like the 1939 Chilean earthquake killed nearly 30,000 people. The economic damage from earthquakes varies greatly, with one in four causing over $180 million in damage. Formula for damage variability: $Damage_{earthquake} \\sim \\mu + \\sigma Z$ where $\\sigma$ is particularly large for earthquakes compared to other disaster types.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1291",
    "question": "Which of the following conditions must be satisfied for an abstract to be considered for the 1981 Summer Meeting of the Econometric Society?",
    "options": [
      "A) The abstract must not exceed 300 words in length",
      "B) The abstract must include the author's office telephone number",
      "C) The abstract must represent an original manuscript not previously presented to the Econometric Society",
      "D) The abstract must be submitted to other associations for presentation at the same meetings"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "1981SummerMeeting-1980.md"
    ],
    "question_context": "The 1981 Summer Meeting of the Econometric Society will take place in San Diego, California, June 24-27, 1981. The program will consist primarily of invited papers. Contributed papers will be considered as well. Abstracts of papers should be sent before February 1, 1981, to the program committee chairman, Professor Robert E. Hall, Hoover Institution, Stanford University, Stanford, California 94305, U.S.A. Each submitted abstract must be labeled at the top with the author's name, affiliation, complete address, and office telephone number (if possible). Abstracts should not exceed 300 words in length and should represent original manuscripts not previously presented to the Econometric Society or other professional organizations and not submitted to other associations for presentation at these same meetings.",
    "category": "INCORRECT",
    "question_context_used": "The 1981 Summer Meeting of the Econometric Society will take place in San Diego, California, June 24-27, 1981. The program will consist primarily of invited papers. Contributed papers will be considered as well. Abstracts of papers should be sent before February 1, 1981, to the program committee chairman, Professor Robert E. Hall, Hoover Institution, Stanford University, Stanford, California 94305, U.S.A. Each submitted abstract must be labeled at the top with the author's name, affiliation, complete address, and office telephone number (if possible). Abstracts should not exceed 300 words in length and should represent original manuscripts not previously presented to the Econometric Society or other professional organizations and not submitted to other associations for presentation at these same meetings.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-404",
    "question": "In the context of optimal predictors for county and state means, which of the following statements are correct regarding the properties and computation of the GLS predictor $\\hat{\\lambda}$?",
    "options": [
      "A) The GLS predictor $\\hat{\\lambda}$ is the BLUP of $\\pmb{\\lambda}$ and can be computed using weighted regression software.",
      "B) The prediction variance of $\\hat{\\lambda}$ is given by $(X^{0\\prime}V^{-1}X^{0})^{-1}$.",
      "C) The GLS predictor $\\hat{\\lambda}$ requires the matrix $X^{0}$ to be singular for a unique solution.",
      "D) The robust predictor $\\pmb{\\lambda}^{*}$ satisfies Condition (13) and is unbiased under the model."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "PFEFFERMANN和BARNARD_-_1991_-_Some_new_estimators_for_small-area_means_with_application_to_the_assessment_of_farmland_values.md"
    ],
    "question_context": "The optimal predictors of the county and state means are obtained straightforwardly from the optimal predictor of the vector $\\pmb{\\lambda}^{\\prime}=(\\pmb{\\beta}^{\\prime},\\pmb{\\alpha}^{\\prime},\\pmb{\\gamma}^{\\prime})$. A simple way to derive the optimal predictor $\\hat{\\lambda}$ and the variance-covariance (V-C) matrix of the corresponding prediction errors $(\\hat{\\lambda}-\\lambda)$ is to compute $\\hat{\\lambda}$ as the generalized least squares (GLS) solution of the regression model. The GLS predictor of $\\pmb{\\lambda}$ is $\\hat{\\bf\\cal A}=(X^{0\\prime}V^{-1}X^{0})^{-1}X^{0\\prime}V^{-1}{\\bf Y}^{0}$. Notice that $X^{0}$ is nonsingular (unless $X$ is singular), which guarantees a unique solution. Pfeffermann (1984) showed that $\\hat{\\lambda}$ defined by (7) is the BLUP of $\\pmb{\\lambda}$.\n\nThe robust predictor $\\pmb{\\lambda}^{*}$ solves the minimization problem subject to a linear restriction, ensuring that the predictors of the county and state means perform well on average if the model fails to hold. The predictor $\\lambda^{*}$ satisfies Condition (13) and is unbiased under the model. The prediction variance of $\\lambda^{*}$ follows from the property that $\\hat{\\lambda}$ is uncorrelated with $(\\mathbf{f}^{\\prime}\\hat{\\mathbf{A}}-\\sum_{s=1}^{S}a_{s}^{+}\\sum_{c=1}^{C(s)}\\tilde{a}_{s c}\\overline{{Y}}_{s c})$.",
    "category": "INCORRECT",
    "question_context_used": "The optimal predictors of the county and state means are obtained straightforwardly from the optimal predictor of the vector $\\pmb{\\lambda}^{\\prime}=(\\pmb{\\beta}^{\\prime},\\pmb{\\alpha}^{\\prime},\\pmb{\\gamma}^{\\prime})$. A simple way to derive the optimal predictor $\\hat{\\lambda}$ and the variance-covariance (V-C) matrix of the corresponding prediction errors $(\\hat{\\lambda}-\\lambda)$ is to compute $\\hat{\\lambda}$ as the generalized least squares (GLS) solution of the regression model. The GLS predictor of $\\pmb{\\lambda}$ is $\\hat{\\bf\\cal A}=(X^{0\\prime}V^{-1}X^{0})^{-1}X^{0\\prime}V^{-1}{\\bf Y}^{0}$. Notice that $X^{0}$ is nonsingular (unless $X$ is singular), which guarantees a unique solution. Pfeffermann (1984) showed that $\\hat{\\lambda}$ defined by (7) is the BLUP of $\\pmb{\\lambda}$.\n\nThe robust predictor $\\pmb{\\lambda}^{*}$ solves the minimization problem subject to a linear restriction, ensuring that the predictors of the county and state means perform well on average if the model fails to hold. The predictor $\\lambda^{*}$ satisfies Condition (13) and is unbiased under the model. The prediction variance of $\\lambda^{*}$ follows from the property that $\\hat{\\lambda}$ is uncorrelated with $(\\mathbf{f}^{\\prime}\\hat{\\mathbf{A}}-\\sum_{s=1}^{S}a_{s}^{+}\\sum_{c=1}^{C(s)}\\tilde{a}_{s c}\\overline{{Y}}_{s c})$.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1484",
    "question": "In the context of nonparametric cointegrating regression with stochastic trend processes, which of the following statements are true regarding the optimal bandwidth selection and asymptotic properties of kernel estimators?",
    "options": [
      "A) The optimal bandwidth is a deterministic function rule, similar to cross-section and stationary kernel regression.",
      "B) The optimal bandwidth is a random sequence dependent on the sojourn time of the process.",
      "C) Local level and local linear kernel estimates are asymptotically normal for bandwidth sequences within a certain band of rates.",
      "D) Nonparametric t-statistics exhibit standard Gaussian limit behavior only in stationary exogenous regressor settings."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "OPTIMAL_BANDWIDTH_SELECTION_IN_NONLINEAR_COINTEGRATING_REGRESSION.md"
    ],
    "question_context": "We study optimal bandwidth selection in nonparametric cointegrating regression where the regressor is a stochastic trend process driven by short or long memory innovations. Unlike stationary regression, the optimal bandwidth is found to be a random sequence which depends on the sojourn time of the process. All random sequences $h_{n}$ that lie within a wide band of rates as the sample size $n\\to\\infty$ have the property that local level and local linear kernel estimates are asymptotically normal, which enables inference and conveniently corresponds to limit theory in the stationary regression case. This finding reinforces the distinctive flexibility of data-based nonparametric regression procedures for nonstationary nonparametric regression. The present results are obtained under exogenous regressor conditions, which are restrictive but which enable flexible data-based methods of practical implementation in nonparametric predictive regressions within that environment.\n\nRemarkably, nonparametric $t\\cdot$ -statistics enjoy standard Gaussian limit behavior in these environments precisely as they do in the conventional stationary exogenous regressor setting. It is further known that nonparametric kernel estimators are uniformly consistent over very wide regions with nonstationary data, a property that is particularly useful given the typical random wandering nature of such data. These findings have brought estimation and inference in bivariate nonparametric cointegrating regression to a level of generality comparable to linear cointegrating regression but with the unexpected advantages of (i) not requiring endogeneity or serial correlation bias corrections, (ii) none of the difficulties of the ill-posedness that arise in the stationary nonparametric context with endogenous regressors, and (iii) simple Gaussian inferential methods that facilitate application.",
    "category": "INCORRECT",
    "question_context_used": "We study optimal bandwidth selection in nonparametric cointegrating regression where the regressor is a stochastic trend process driven by short or long memory innovations. Unlike stationary regression, the optimal bandwidth is found to be a random sequence which depends on the sojourn time of the process. All random sequences $h_{n}$ that lie within a wide band of rates as the sample size $n\\to\\infty$ have the property that local level and local linear kernel estimates are asymptotically normal, which enables inference and conveniently corresponds to limit theory in the stationary regression case. This finding reinforces the distinctive flexibility of data-based nonparametric regression procedures for nonstationary nonparametric regression. The present results are obtained under exogenous regressor conditions, which are restrictive but which enable flexible data-based methods of practical implementation in nonparametric predictive regressions within that environment.\n\nRemarkably, nonparametric $t\\cdot$ -statistics enjoy standard Gaussian limit behavior in these environments precisely as they do in the conventional stationary exogenous regressor setting. It is further known that nonparametric kernel estimators are uniformly consistent over very wide regions with nonstationary data, a property that is particularly useful given the typical random wandering nature of such data. These findings have brought estimation and inference in bivariate nonparametric cointegrating regression to a level of generality comparable to linear cointegrating regression but with the unexpected advantages of (i) not requiring endogeneity or serial correlation bias corrections, (ii) none of the difficulties of the ill-posedness that arise in the stationary nonparametric context with endogenous regressors, and (iii) simple Gaussian inferential methods that facilitate application.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1051",
    "question": "According to the economic theory of job search as discussed by Steven H. Sandell, which of the following statements are empirically supported by the National Longitudinal Surveys (NLS) data on unemployed women aged 35 to 49 in 1972?",
    "options": [
      "A) The reservation wage is set where the expected marginal cost of search equals its expected marginal benefit.",
      "B) Higher asking wages lead to shorter periods of unemployment but lower post-unemployment wages.",
      "C) The average duration of unemployment for women is shorter than that for men, despite higher unemployment rates for women.",
      "D) Empirical estimates of the effect of changes in the asking wage can be used to calculate the ex post costs and benefits of job search."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Sandell-UnemploymentRateWomen-1980.md"
    ],
    "question_context": "The reservation wage reflects an unemployed woman's job search strategy. If she acts rationally, then the reservation wage is set so that the (expected) marginal cost of search equals its (expected) marginal benefit. Since the higher her asking wage, the longer the period of unemployment and the higher her post-unemployment wage, empirical estimates of the effect of changes in the asking wage on these outcomes can be used to calculate the ex post costs and benefits of job search. Models of the duration of unemployment and the post-unemployment rate of pay that make explicit use of the reported asking wage, are estimated with the National Longitudinal Surveys (NLS) of women who were 35 to 49 years of age in 1972. While the unemployment rate for women is considerably higher than that for men, the average duration of women's unemployment is shorter. For example, in October 1977 the mean duration of men's unemployment was 15.2 weeks compared to 11.4 weeks for women.",
    "category": "INCORRECT",
    "question_context_used": "The reservation wage reflects an unemployed woman's job search strategy. If she acts rationally, then the reservation wage is set so that the (expected) marginal cost of search equals its (expected) marginal benefit. Since the higher her asking wage, the longer the period of unemployment and the higher her post-unemployment wage, empirical estimates of the effect of changes in the asking wage on these outcomes can be used to calculate the ex post costs and benefits of job search. Models of the duration of unemployment and the post-unemployment rate of pay that make explicit use of the reported asking wage, are estimated with the National Longitudinal Surveys (NLS) of women who were 35 to 49 years of age in 1972. While the unemployment rate for women is considerably higher than that for men, the average duration of women's unemployment is shorter. For example, in October 1977 the mean duration of men's unemployment was 15.2 weeks compared to 11.4 weeks for women.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3790",
    "question": "In the context of adjusting Japanese prefecture GDP series for discontinuities, which of the following methods are correctly applied to ensure consistency across different SNA methods and base years?",
    "options": [
      "A) Multiplying series 3 data (1990-1995) by the ratio of 1996 data in series 3 to series 4 to adjust the 1996 discontinuity",
      "B) Using the growth rate of national CPI to estimate missing deflator data for Fukushima, Saitama, Okayama, and Okinawa",
      "C) Multiplying series 1 data (1955-1974) by the ratio of 1990 data in series 2 to adjusted series 3 to adjust the 1975 and 1990 discontinuities",
      "D) Multiplying series 1 deflator data (1955-1974) by the ratio of 1980 data in adjusted series 2 to 100 to adjust the 1975 discontinuity"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Disentangling-the-effects-of-multiple-treatments-Measuring-t_2015_Journal-of.md"
    ],
    "question_context": "The prefecture aggregates are revised every year, and the consistent constant price data for the period 1955–2009 do not exist because of the changes in the base years. Moreover, the estimates are compiled based on two different methods of the system of National Account; the 1968 System of National Account (SNA) or the 1993 SNA. Currently, the 1955–2009 prefecture data are compiled using different accounting methods and price indexes. For the period from 1955 to 1974, the data were compiled using the 1968 SNA method (base year 1980, hereafter series 1); from 1975 to 1989, the 1968 SNA method (base year 1990, hereafter series 2); from 1990 to 2003, the 1993 SNA method (base year 1995, hereafter series 3); and from 1996 to 2009, the 1993 SNA method (base year 2000, hereafter series 4). The Japanese Cabinet Office selects the following series as the official statistics: series 1 from 1955 to 1974; series 2 from 1975 to 1989; series 3 from 1990 to 1995; and series 4 from 1996 to 2009. Therefore, the official statistics have jumps in the year 1975, 1990 and 1996 either due to changes in the base year or choice of the SNA methods. Regarding the nominal gross prefecture expenditure, we adjust three jumps in the series as follows: First, we obtain the adjusted series 3 by multiplying series 3 data from 1990 to 1995 by the ratio of the 1996 data in series 3 to the 1996 data in series 4 to adjust the discontinuity in the year 1996. Second, to adjust the discontinuity in the year 1990 and 1975, we multiply series 2 data from 1975 to 1989 and series 1 data from 1955 to 1974 by the ratio of the 1990 data in series 2 to the 1990 data in the adjusted series 3.",
    "category": "INCORRECT",
    "question_context_used": "The prefecture aggregates are revised every year, and the consistent constant price data for the period 1955–2009 do not exist because of the changes in the base years. Moreover, the estimates are compiled based on two different methods of the system of National Account; the 1968 System of National Account (SNA) or the 1993 SNA. Currently, the 1955–2009 prefecture data are compiled using different accounting methods and price indexes. For the period from 1955 to 1974, the data were compiled using the 1968 SNA method (base year 1980, hereafter series 1); from 1975 to 1989, the 1968 SNA method (base year 1990, hereafter series 2); from 1990 to 2003, the 1993 SNA method (base year 1995, hereafter series 3); and from 1996 to 2009, the 1993 SNA method (base year 2000, hereafter series 4). The Japanese Cabinet Office selects the following series as the official statistics: series 1 from 1955 to 1974; series 2 from 1975 to 1989; series 3 from 1990 to 1995; and series 4 from 1996 to 2009. Therefore, the official statistics have jumps in the year 1975, 1990 and 1996 either due to changes in the base year or choice of the SNA methods. Regarding the nominal gross prefecture expenditure, we adjust three jumps in the series as follows: First, we obtain the adjusted series 3 by multiplying series 3 data from 1990 to 1995 by the ratio of the 1996 data in series 3 to the 1996 data in series 4 to adjust the discontinuity in the year 1996. Second, to adjust the discontinuity in the year 1990 and 1975, we multiply series 2 data from 1975 to 1989 and series 1 data from 1955 to 1974 by the ratio of the 1990 data in series 2 to the 1990 data in the adjusted series 3.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3882",
    "question": "In the context of Theorem 4.1 and its proof, which of the following statements are necessary for the implementation of the social choice correspondence (SCC) $f$ via game form $G$?",
    "options": [
      "A) The outcome $x$ must be $R^{2}$-maximal for both agents in their respective sets $C_{1}(R,a)$ and $C_{2}(R^{1},b)$.",
      "B) The game form $G$ must allow agents to raise flags and announce non-negative integers to influence the outcome.",
      "C) For each $R \\in \\mathcal{R}$ and $a \\in f(R)$, there exists a strategy combination $\\bar{s}(R,a) \\in NE(G,R)$ such that $\\pi(\\bar{s}(R,a)) = a$.",
      "D) The sets $C_{i}(R,a)$ must satisfy $C_{i}(R,a) \\subseteq L_{i}(R,a)$ for all $i \\in I$."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Necessary and Sufficient Condition for Two-Person Nash Implementation.md"
    ],
    "question_context": "Theorem 4.1 states that the social choice correspondence (SCC) $f$ is implementable if and only if it satisfies Condition $\\beta$. The proof involves constructing a game form $G$ where agents can announce preference profiles, outcomes, and non-negative integers, and optionally raise flags. Key components include the sets $C_{i}(R,a)$, which represent outcomes obtainable by unilateral deviations, and the requirement that $C_{i}(R,a) \\subseteq L_{i}(R,a)$. The proof also relies on the existence of strategy combinations $\\bar{s}(R,a) \\in NE(G,R)$ such that $\\pi(\\bar{s}(R,a)) = a$. The outcome $x = \\pi(\\bar{s}_{1}(R,a), \\bar{s}_{2}(R^{1},b))$ must be $R^{2}$-maximal for both agents in $C_{1}(R,a)$ and $C_{2}(R^{1},b)$ to ensure $x \\in f(R^{2})$.",
    "category": "INCORRECT",
    "question_context_used": "Theorem 4.1 states that the social choice correspondence (SCC) $f$ is implementable if and only if it satisfies Condition $\\beta$. The proof involves constructing a game form $G$ where agents can announce preference profiles, outcomes, and non-negative integers, and optionally raise flags. Key components include the sets $C_{i}(R,a)$, which represent outcomes obtainable by unilateral deviations, and the requirement that $C_{i}(R,a) \\subseteq L_{i}(R,a)$. The proof also relies on the existence of strategy combinations $\\bar{s}(R,a) \\in NE(G,R)$ such that $\\pi(\\bar{s}(R,a)) = a$. The outcome $x = \\pi(\\bar{s}_{1}(R,a), \\bar{s}_{2}(R^{1},b))$ must be $R^{2}$-maximal for both agents in $C_{1}(R,a)$ and $C_{2}(R^{1},b)$ to ensure $x \\in f(R^{2})$.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4723",
    "question": "Based on the empirical analysis of instruction time and classroom quality, which of the following statements are supported by the findings?",
    "options": [
      "A) The return to additional instruction time is monotonically increasing with classroom quality in school-by-grade fixed effect specifications.",
      "B) The interaction-term coefficients for classroom quality are significant at the 1% level in all fixed-effect models.",
      "C) Schools with low-quality classroom environments realize a much smaller benefit from additional instruction time.",
      "D) The benefit of additional instruction time is invariant to the quality of the classroom environment."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Instruction_time,_Classroom_Quality,_and_Academic_Achievement.md"
    ],
    "question_context": "The results in Table 9 show a monotonically increasing return to additional instruction time as the quality of the classroom environment increases in the school-by-grade fixed effect specification. More specifically, the estimated benefit of additional class time for students in schools in the top half of the classroom quality distribution is significant and more than twice as large as the benefit for students attending schools in the bottom half of the distribution: the coefficients for the 3rd and 4th quartile interactions exceed 0.02. Perhaps the most important finding, despite the lack of precision in some specifications, is that the benefit of additional instruction time appears to vary with the quality of the classroom environment. Specifically, the results show that schools with low-quality classroom environments likely realise a much smaller benefit from additional instruction time.",
    "category": "CORRECT",
    "question_context_used": "The results in Table 9 show a monotonically increasing return to additional instruction time as the quality of the classroom environment increases in the school-by-grade fixed effect specification. More specifically, the estimated benefit of additional class time for students in schools in the top half of the classroom quality distribution is significant and more than twice as large as the benefit for students attending schools in the bottom half of the distribution: the coefficients for the 3rd and 4th quartile interactions exceed 0.02. Perhaps the most important finding, despite the lack of precision in some specifications, is that the benefit of additional instruction time appears to vary with the quality of the classroom environment. Specifically, the results show that schools with low-quality classroom environments likely realise a much smaller benefit from additional instruction time.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4010",
    "question": "In the context of binary response correlated random coefficient (CRC) panel data models, which of the following conditions are essential for the identification and estimation of the average slope using the special regressor method?",
    "options": [
      "A) The special regressor must be continuous and independent of random coefficients and error terms conditional on other regressors.",
      "B) The support of the special regressor must be sufficiently large to cover the range of the transformed linear moment condition.",
      "C) The random coefficients must be uncorrelated with all regressors to ensure unbiased estimation.",
      "D) The error terms must follow a known parametric distribution for maximum likelihood estimation."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Binary-response-correlated-random-coefficient-panel-_2015_Journal-of-Econome.md"
    ],
    "question_context": "The binary response CRC panel data model is given by: $$y_{i t}=\\mathbf{1}(v_{i t}+x_{i t}^{\\top}\\beta_{i}+u_{i t}>0),$$ where $v_{it}$ is a special regressor. Assumption A1 states that the conditional distribution of $v_{it}$ given $x_{it}$ and $z_i$ must have a continuous density with positive support. Assumption A2 requires that $\\alpha_i$ and $u_{it}$ are independent of $v_{it}$ conditional on $x_{it}$ and $z_i$. Assumption A3 imposes that the support of $v_{it}$ must be sufficiently large to cover the transformed linear moment condition. These assumptions are crucial for transforming the binary response model into a linear moment condition, enabling identification and $\\sqrt{n}$-consistent estimation of the average slope.\n\nThe special regressor method, proposed by Lewbel (1998, 2000), leverages these conditions to address heterogeneity and endogeneity. Without these assumptions, the identification of the binary response model becomes problematic, as the model cannot be transformed into a linear form suitable for standard estimation techniques. The method's effectiveness is demonstrated through simulations, where the estimator achieves nominal size and high power in testing correlations between random coefficients and regressors.",
    "category": "CORRECT",
    "question_context_used": "The binary response CRC panel data model is given by: $$y_{i t}=\\mathbf{1}(v_{i t}+x_{i t}^{\\top}\\beta_{i}+u_{i t}>0),$$ where $v_{it}$ is a special regressor. Assumption A1 states that the conditional distribution of $v_{it}$ given $x_{it}$ and $z_i$ must have a continuous density with positive support. Assumption A2 requires that $\\alpha_i$ and $u_{it}$ are independent of $v_{it}$ conditional on $x_{it}$ and $z_i$. Assumption A3 imposes that the support of $v_{it}$ must be sufficiently large to cover the transformed linear moment condition. These assumptions are crucial for transforming the binary response model into a linear moment condition, enabling identification and $\\sqrt{n}$-consistent estimation of the average slope.\n\nThe special regressor method, proposed by Lewbel (1998, 2000), leverages these conditions to address heterogeneity and endogeneity. Without these assumptions, the identification of the binary response model becomes problematic, as the model cannot be transformed into a linear form suitable for standard estimation techniques. The method's effectiveness is demonstrated through simulations, where the estimator achieves nominal size and high power in testing correlations between random coefficients and regressors.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-515",
    "question": "According to the marginal productivity theory of wages and its empirical testing framework, which of the following statements are correct?",
    "options": [
      "A) The theory is only applicable in long-run equilibrium due to fixed input proportions in the short run.",
      "B) The theory can be directly tested using individual firm-level data on labor and co-operating factors.",
      "C) The empirical test in 1947 confirmed that the theory and the assumption of identical production functions are compatible with the data.",
      "D) The study's primary concern was the validity, not the applicability, of the marginal productivity theory."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Predictive Accuracy of the Marginal Productivity Theory of Wages.md"
    ],
    "question_context": "The marginal productivity theory of wages maintains that, in the long run when the forms of the co-operating factors can be adjusted, a firm will combine its inputs in such a manner as to equate the marginal (revenue) product of labor with the wage rate (assuming no monopsony power). There is no necessity, in any meaningful sense, for this equality to be achieved in the short run. Once a given plant is built, the forms of the co-operating factors will be restricted and the productive process may require essentially fixed proportions of the inputs. As a result, the short-run iso-product curve will often be kinked, and no well-defined marginal product of labor need exist in short-run equilibrium. For this reason, we follow Hicksi in maintaining that the marginal productivity theory of wages refers basically to the long-run equilibrium.\n\nThe predictive accuracy of the theory is tested in the following manner. Two composite hypotheses, subject to test, are developed. Each hypothesis states the following : (A) If the marginal productivity theory correctly describes the situation existing in 1947 and $(B)$ if each firm in a given industry has the same production function, subject to constant returns to scale, then (C) certain relations will be found between wages and salaries and valueadded. Our study reveals that each hypothesis subject to test is compatible with the data. Hence, the marginal productivity theory and assumption $(B)$ are not incompatible with the data.",
    "category": "INCORRECT",
    "question_context_used": "The marginal productivity theory of wages maintains that, in the long run when the forms of the co-operating factors can be adjusted, a firm will combine its inputs in such a manner as to equate the marginal (revenue) product of labor with the wage rate (assuming no monopsony power). There is no necessity, in any meaningful sense, for this equality to be achieved in the short run. Once a given plant is built, the forms of the co-operating factors will be restricted and the productive process may require essentially fixed proportions of the inputs. As a result, the short-run iso-product curve will often be kinked, and no well-defined marginal product of labor need exist in short-run equilibrium. For this reason, we follow Hicksi in maintaining that the marginal productivity theory of wages refers basically to the long-run equilibrium.\n\nThe predictive accuracy of the theory is tested in the following manner. Two composite hypotheses, subject to test, are developed. Each hypothesis states the following : (A) If the marginal productivity theory correctly describes the situation existing in 1947 and $(B)$ if each firm in a given industry has the same production function, subject to constant returns to scale, then (C) certain relations will be found between wages and salaries and valueadded. Our study reveals that each hypothesis subject to test is compatible with the data. Hence, the marginal productivity theory and assumption $(B)$ are not incompatible with the data.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4078",
    "question": "In the study of long-run health effects of the Danish home visiting program, which of the following methodological challenges are explicitly addressed in the analysis?",
    "options": [
      "A) Differential mortality among treated and untreated individuals prior to age 45",
      "B) Inability to link families in the Danish register data for the given cohorts",
      "C) Potential parental responses to program exposure differing across treated and untreated siblings",
      "D) Impact of the home visiting program on spacing or family size"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Universal Investment in Infants and Long-Run Health.md"
    ],
    "question_context": "The study combines unique data from three sources, including municipal-level and district-level data on pretreatment characteristics and individual-level adult outcome data (1980-2012) for the population of individuals born in Denmark between 1935 and 1949. Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. However, the analysis faces several methodological challenges. First, the inability to link families in the Danish register data for the given cohorts prevents comparing between siblings to test for the robustness of results. Second, in the likely presence of parental responses to program exposure that may systematically differ across treated and untreated siblings, it is not straightforward to interpret results from a within-family estimate. Third, while one mechanism for a short-run and long-run effect of the home visiting program may be its impact on the timing of subsequent births, the study cannot examine the impact of the home visiting program on spacing or family size. The study also addresses concerns about differential mortality among treated and untreated individuals prior to age 45, showing that omitting 1 percent of individuals in the sample does not impact main results and that there are no survival gains of the program at earlier ages (40-45).",
    "category": "INCORRECT",
    "question_context_used": "The study combines unique data from three sources, including municipal-level and district-level data on pretreatment characteristics and individual-level adult outcome data (1980-2012) for the population of individuals born in Denmark between 1935 and 1949. Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. However, the analysis faces several methodological challenges. First, the inability to link families in the Danish register data for the given cohorts prevents comparing between siblings to test for the robustness of results. Second, in the likely presence of parental responses to program exposure that may systematically differ across treated and untreated siblings, it is not straightforward to interpret results from a within-family estimate. Third, while one mechanism for a short-run and long-run effect of the home visiting program may be its impact on the timing of subsequent births, the study cannot examine the impact of the home visiting program on spacing or family size. The study also addresses concerns about differential mortality among treated and untreated individuals prior to age 45, showing that omitting 1 percent of individuals in the sample does not impact main results and that there are no survival gains of the program at earlier ages (40-45).",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3682",
    "question": "In the context of Constrained Demange and Laroque- (CDL-) optimality, which of the following statements are correct regarding the transfer policy and futures innovation?",
    "options": [
      "A) An IVA transfer policy can only serve as much as date 0 riskless transfers under mean-variance utility representations.",
      "B) DL-optimality and H-optimality coincide in a static economy but may diverge in a dynamic economy without restrictions on transfers.",
      "C) The authority can force investors to accept transfers at any date to create new instruments effectively.",
      "D) CDL-optimality of futures innovation is equivalent to H-optimality when transfer policies are intertemporally voluntarily acceptable."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Optimal-Futures-Innovation-in-a-Dynamic-Economy--The_1997_Journal-of-Economi.md"
    ],
    "question_context": "In a dynamic economy, the concept of DL-optimality allows the authority to redistribute wealth at each date through transfers of futures and credits. However, this approach may not be suitable for evaluating futures innovation dynamically, as illustrated in Example 2, where intertemporal transfers of credits effectively create new instruments for risk-sharing up to the last date but one. The DL-optimal contract in this example is $f_{\\mathrm{DL}}=\\pm(0,1)$, differing from the H-optimal contract $f_{\\mathrm{H}}=\\pm(1,0)$. Lemma 3 shows that under mean-variance utility representations, an IVA transfer policy serves only as much as date 0 riskless transfers. Proposition 3 establishes that futures innovation is CDL-optimal if and only if it is H-optimal, highlighting the equivalence under IVA conditions.",
    "category": "INCORRECT",
    "question_context_used": "In a dynamic economy, the concept of DL-optimality allows the authority to redistribute wealth at each date through transfers of futures and credits. However, this approach may not be suitable for evaluating futures innovation dynamically, as illustrated in Example 2, where intertemporal transfers of credits effectively create new instruments for risk-sharing up to the last date but one. The DL-optimal contract in this example is $f_{\\mathrm{DL}}=\\pm(0,1)$, differing from the H-optimal contract $f_{\\mathrm{H}}=\\pm(1,0)$. Lemma 3 shows that under mean-variance utility representations, an IVA transfer policy serves only as much as date 0 riskless transfers. Proposition 3 establishes that futures innovation is CDL-optimal if and only if it is H-optimal, highlighting the equivalence under IVA conditions.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4190",
    "question": "In the context of the structural equation model and LIML estimation, which of the following conditions are necessary for the identification and consistent estimation of the parameters?",
    "options": [
      "A) The matrix Z of exogenous variables must be of full column rank (Assumption 2)",
      "B) The disturbances u must be independently normally distributed with mean 0 (Assumption 1)",
      "C) The number of excluded exogenous instruments K2 must exactly equal G1 + 1 (Assumption 5)",
      "D) The reduced-form coefficient matrix must satisfy rank(π21 Π22) = G1 (Assumption 3)"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Hosoya-AncillarityLimitedInformation-1989.md"
    ],
    "question_context": "The structural equation is given by:\n\ny₁ = Y₂β + Z₁γ + u,\n\nwhere y₁ and Y₂ are matrices of endogenous variables, Z₁ is a matrix of exogenous variables, and u is a vector of disturbances. The reduced form includes:\n\nY = ZΠ + V = (Z₁ Z₂)(π₁ Π₂) + (v₁ V₂).\n\nKey assumptions include: disturbances being independently normally distributed (Assumption 1), full rank of Z (Assumption 2), rank condition on reduced-form coefficients (Assumption 3), and exact identification when K₂ ≡ G₁ + 1 (Assumption 5). The LIML estimator solves [G - λC](1, -β̂')' = 0, where G and C are Wishart matrices.",
    "category": "INCORRECT",
    "question_context_used": "The structural equation is given by:\n\ny₁ = Y₂β + Z₁γ + u,\n\nwhere y₁ and Y₂ are matrices of endogenous variables, Z₁ is a matrix of exogenous variables, and u is a vector of disturbances. The reduced form includes:\n\nY = ZΠ + V = (Z₁ Z₂)(π₁ Π₂) + (v₁ V₂).\n\nKey assumptions include: disturbances being independently normally distributed (Assumption 1), full rank of Z (Assumption 2), rank condition on reduced-form coefficients (Assumption 3), and exact identification when K₂ ≡ G₁ + 1 (Assumption 5). The LIML estimator solves [G - λC](1, -β̂')' = 0, where G and C are Wishart matrices.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4434",
    "question": "Based on the regression analysis of missionary activity in Africa, which of the following factors are significantly correlated with the presence of missions in 1900 and 1924?",
    "options": [
      "A) Historical Malaria Index",
      "B) Log Distance to Coast",
      "C) Dummy if Largest or 2nd Largest City 1901",
      "D) Dummy if Polygamy (Murdock)"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Jedwab 等 - 2022 - The economics of missionary expansion evidence from Africa and implications for development.md"
    ],
    "question_context": "The regression analysis examines the presence of missions in 203,574 cells across 43 sub-Saharan African countries. Key findings include: the Historical Malaria Index shows a negative correlation (-0.0253*** in 1900, -0.0478*** in 1924), indicating missions were less likely in high-malaria areas. Log Distance to Coast is consistently negative and significant (-0.0030*** in both 1900 and 1924), suggesting coastal proximity favored mission establishment. The presence of the largest or second-largest city in 1901 is positively correlated (0.0579*** in 1900, 0.1734*** in 1924), highlighting urban centers as focal points for missionary activity. The analysis controls for numerous other factors, including country fixed effects, to isolate these relationships.",
    "category": "INCORRECT",
    "question_context_used": "The regression analysis examines the presence of missions in 203,574 cells across 43 sub-Saharan African countries. Key findings include: the Historical Malaria Index shows a negative correlation (-0.0253*** in 1900, -0.0478*** in 1924), indicating missions were less likely in high-malaria areas. Log Distance to Coast is consistently negative and significant (-0.0030*** in both 1900 and 1924), suggesting coastal proximity favored mission establishment. The presence of the largest or second-largest city in 1901 is positively correlated (0.0579*** in 1900, 0.1734*** in 1924), highlighting urban centers as focal points for missionary activity. The analysis controls for numerous other factors, including country fixed effects, to isolate these relationships.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2534",
    "question": "In the context of dynamic duopolistic competition with sticky prices, which of the following factors are critical for equilibrium analysis?",
    "options": [
      "A) The presence of perfect information symmetry between competitors",
      "B) The degree of price rigidity and adjustment costs",
      "C) The assumption of instantaneous market clearing",
      "D) The intertemporal optimization behavior of firms"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "VolumeInformation-1987.md"
    ],
    "question_context": "The article by Fershtman and Kamien (1987) examines dynamic duopolistic competition with sticky prices, where firms face adjustment costs when changing prices. Key formula: The optimal pricing strategy involves solving an intertemporal optimization problem, where the Hamiltonian is given by H = π(p₁, p₂) - C(Δp) + λΔp, where π represents profit, C(Δp) is the adjustment cost, and λ is the co-state variable. The equilibrium analysis hinges on the interplay between price rigidity and firms' forward-looking behavior.",
    "category": "CORRECT",
    "question_context_used": "The article by Fershtman and Kamien (1987) examines dynamic duopolistic competition with sticky prices, where firms face adjustment costs when changing prices. Key formula: The optimal pricing strategy involves solving an intertemporal optimization problem, where the Hamiltonian is given by H = π(p₁, p₂) - C(Δp) + λΔp, where π represents profit, C(Δp) is the adjustment cost, and λ is the co-state variable. The equilibrium analysis hinges on the interplay between price rigidity and firms' forward-looking behavior.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-705",
    "question": "In the context of the modified rate of technical progress (μₜ), which of the following conditions must hold for the traditional measure of technical progress to equal μₜ?",
    "options": [
      "A) Prices of final output, energy, and imports escalate at the same rate through time",
      "B) The capital-labor ratio remains constant over time",
      "C) The depreciation rate of capital is zero",
      "D) The discount rate equals the rate of technical progress"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Contribution of Changing Energy and Import Prices to Changing Average Labor Productivity A Profit Formulation for Canada.md"
    ],
    "question_context": "The modified rate of technical progress (μₜ) is derived from the profit function and accounts for changes in energy and import prices. The traditional measure of technical progress corresponds to μₜ only under specific conditions. The key formula is:\n\n$$\n\\dot{\\overline{{{u}}}}^{\\ast}(t)-\\overline{{{\\overline{{{M}}}\\bar{L}}}}\\cdot\\dot{\\overline{{{L}}}}(t)-\\overline{{{\\overline{{{M}}}\\bar{K}}}}\\cdot\\dot{\\overline{{{K}}}}(t).\n$$\n\nThis traditional measure equals μₜ only when prices of the final output, energy, and imports escalate at the same rate through time, which is the condition for Hicks's aggregation. The discount rate (i) also plays a critical role in the dynamic adjustment of quasi-fixed inputs, as seen in the integral:\n\n$$\n\\int_{0}^{\\infty}e_{\\cdot\\imath T}(Q(T)-P E D(T)E D(T)-P E M(T)E M(T))\\,d T.\n$$",
    "category": "INCORRECT",
    "question_context_used": "The modified rate of technical progress (μₜ) is derived from the profit function and accounts for changes in energy and import prices. The traditional measure of technical progress corresponds to μₜ only under specific conditions. The key formula is:\n\n$$\n\\dot{\\overline{{{u}}}}^{\\ast}(t)-\\overline{{{\\overline{{{M}}}\\bar{L}}}}\\cdot\\dot{\\overline{{{L}}}}(t)-\\overline{{{\\overline{{{M}}}\\bar{K}}}}\\cdot\\dot{\\overline{{{K}}}}(t).\n$$\n\nThis traditional measure equals μₜ only when prices of the final output, energy, and imports escalate at the same rate through time, which is the condition for Hicks's aggregation. The discount rate (i) also plays a critical role in the dynamic adjustment of quasi-fixed inputs, as seen in the integral:\n\n$$\n\\int_{0}^{\\infty}e_{\\cdot\\imath T}(Q(T)-P E D(T)E D(T)-P E M(T)E M(T))\\,d T.\n$$",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1781",
    "question": "Which of the following factors were critical in the postponement and rescheduling of the sixth U.S.-Soviet Economic Symposium?",
    "options": [
      "A) Incompatibility of October dates with American university teaching schedules",
      "B) Political tensions between the U.S. and U.S.S.R. in 1980",
      "C) Logistical challenges in organizing parallel paper presentations",
      "D) The need to align with the Soviet academic calendar"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "ReportCommitteeUSSoviet-1981.md"
    ],
    "question_context": "Following the fifth U.S.-Soviet Economic Symposium held in the United States in June 1979, the Soviet Economic Association invited ten American economists to a sixth symposium in the U.S.S.R. in 1980. The October dates suggested, however, were not feasible in terms of American university teaching schedules, so the symposium was postponed until the following year. It is now scheduled to be held in Alma-Ata, U.S.S.R., June 1981. The conference theme is \\",
    "category": "INCORRECT",
    "question_context_used": "Following the fifth U.S.-Soviet Economic Symposium held in the United States in June 1979, the Soviet Economic Association invited ten American economists to a sixth symposium in the U.S.S.R. in 1980. The October dates suggested, however, were not feasible in terms of American university teaching schedules, so the symposium was postponed until the following year. It is now scheduled to be held in Alma-Ata, U.S.S.R., June 1981. The conference theme is \\",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-374",
    "question": "Which of the following factors are correctly classified as affecting the residual wealth in the nursing home according to the text?",
    "options": [
      "A) State Medicaid eligibility standards and personal needs allowance",
      "B) The user cost of housing for renters and owners",
      "C) The present value of nonbequeathable income and nonhousing assets",
      "D) The hourly wage paid to aides in nursing homes"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Hoerger-PublicSubsidiesPrivate-1996.md"
    ],
    "question_context": "Residual wealth in the nursing home measures the present value of wealth which the family will have available for consumption after paying for the parent's nursing home care. Such wealth can be consumed by the parent while in the nursing home or be bequeathed to the kids. Residual wealth in the nursing home was calculated as the difference between total nonhousing wealth (the value of nonhousing assets plus the present value of nonbequeathable income accruing to the parent) and the present value of payments from the household to the nursing home. Nonhousing assets came directly from the survey, while nonbequeathable income was from the 1989 survey month. The present values reflect the probabilities of living each of 30 years from 1989, which we obtained from a hazard model, and were calculated using a real discount rate of $3\\%$.\n\nTo compute each year's net price of nursing home care to the parent, we subtracted the subsidy that the person could anticipate from Medicaid if she entered a nursing home on the survey date, but added back the cash transfer from public sources that the person might lose if she became institutionalized. Because of variation in (1) income and wealth eligibility thresholds for Medicaid subsidies in nursing homes; (2) amounts Medicaid-subsidized residents are required to pay for such care; and (3) elderly persons’ income, wealth, and marital status, there is appreciable variation in our data in the subsidy that an elderly person could expect conditional on nursing home entry. Our calculations of the subsidy reflect these variations in state Medicaid policy: income eligibility standard; personal needs allowance (and subsidies made for individuals with incomes below the allowance); the home maintenance allowance for unmarried persons who owned a home; set-asides for maintenance of the outside spouse when the institutionalized spouse's income was more than enough to cover the personal needs allowance; the outside spouse's required contribution to the inside spouse's nursing home care; and the length of time an unmarried institutionalized person's home could be excluded from countable (by Medicaid) assets.",
    "category": "CORRECT",
    "question_context_used": "Residual wealth in the nursing home measures the present value of wealth which the family will have available for consumption after paying for the parent's nursing home care. Such wealth can be consumed by the parent while in the nursing home or be bequeathed to the kids. Residual wealth in the nursing home was calculated as the difference between total nonhousing wealth (the value of nonhousing assets plus the present value of nonbequeathable income accruing to the parent) and the present value of payments from the household to the nursing home. Nonhousing assets came directly from the survey, while nonbequeathable income was from the 1989 survey month. The present values reflect the probabilities of living each of 30 years from 1989, which we obtained from a hazard model, and were calculated using a real discount rate of $3\\%$.\n\nTo compute each year's net price of nursing home care to the parent, we subtracted the subsidy that the person could anticipate from Medicaid if she entered a nursing home on the survey date, but added back the cash transfer from public sources that the person might lose if she became institutionalized. Because of variation in (1) income and wealth eligibility thresholds for Medicaid subsidies in nursing homes; (2) amounts Medicaid-subsidized residents are required to pay for such care; and (3) elderly persons’ income, wealth, and marital status, there is appreciable variation in our data in the subsidy that an elderly person could expect conditional on nursing home entry. Our calculations of the subsidy reflect these variations in state Medicaid policy: income eligibility standard; personal needs allowance (and subsidies made for individuals with incomes below the allowance); the home maintenance allowance for unmarried persons who owned a home; set-asides for maintenance of the outside spouse when the institutionalized spouse's income was more than enough to cover the personal needs allowance; the outside spouse's required contribution to the inside spouse's nursing home care; and the length of time an unmarried institutionalized person's home could be excluded from countable (by Medicaid) assets.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2977",
    "question": "In the context of heterogeneous panel unit root testing with serially correlated disturbances, which of the following statements are correct regarding the $t$-bar test?",
    "options": [
      "A) The $t$-bar test performs well with small $T$ when no serial correlation is present",
      "B) Under serial correlation, the $t$-bar test requires both $N$ and $T$ to be sufficiently large",
      "C) The $t$-bar test's finite sample performance is worse than the $LL$ test when the lag order of ADF regressions is underestimated",
      "D) Rejection of the null hypothesis implies the unit root is rejected for all panel members"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Reprint-of--Testing-for-unit-roots-in-heterogeneous_2023_Journal-of-Economet.md"
    ],
    "question_context": "The small sample properties of the proposed tests are investigated via Monte Carlo methods. It is found that when there are no serial correlations, the $t$ -bar test performs very well even when $T=10$. In this case it is possible to substantially augment the power of the unit root tests applied to single time series. The situation is more complicated when the disturbances in the dynamic panel are serially correlated. The $t$ -bar test procedures now require that both $N$ and $T$ should be sufficiently large. Furthermore, in the case of serially correlated errors, it is critically important not to under-estimate the order of the underlying ADF regressions. When we allow for serial correlation and heterogeneity in the underlying data generating process, the simulation results clearly show that if a large enough lag order is selected for the underlying ADF regressions, then the finite sample performances of the $t$ -bar test is reasonably satisfactory and generally better than that of the $L L$ test.",
    "category": "INCORRECT",
    "question_context_used": "The small sample properties of the proposed tests are investigated via Monte Carlo methods. It is found that when there are no serial correlations, the $t$ -bar test performs very well even when $T=10$. In this case it is possible to substantially augment the power of the unit root tests applied to single time series. The situation is more complicated when the disturbances in the dynamic panel are serially correlated. The $t$ -bar test procedures now require that both $N$ and $T$ should be sufficiently large. Furthermore, in the case of serially correlated errors, it is critically important not to under-estimate the order of the underlying ADF regressions. When we allow for serial correlation and heterogeneity in the underlying data generating process, the simulation results clearly show that if a large enough lag order is selected for the underlying ADF regressions, then the finite sample performances of the $t$ -bar test is reasonably satisfactory and generally better than that of the $L L$ test.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\"]}",
      "model_answer": [
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3640",
    "question": "In the context of labor demand analysis, which of the following factors are empirically shown by Fabbri et al. to be significantly influenced by the nationality of firm ownership?",
    "options": [
      "A) Elasticity of substitution between domestic and foreign labor",
      "B) Wage differentials between skilled and unskilled workers",
      "C) Sensitivity of employment levels to exchange rate fluctuations",
      "D) Propensity to offshore production activities"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Fabbri-NationalityOwnershipMatter-2003.md"
    ],
    "question_context": "The study examines whether the nationality of firm ownership affects labor demand elasticities using UK establishment-level data. Key findings indicate that foreign-owned firms exhibit greater employment sensitivity to exchange rate movements compared to domestic firms. The paper derives this through estimating translog cost functions where the elasticity of substitution between labor types varies by ownership. Formula: σ_{ij} = ∂ln(x_i/x_j)/∂ln(w_j/w_i), where σ_{ij} is the elasticity of substitution between factors i and j, x denotes quantities, and w denotes prices.",
    "category": "INCORRECT",
    "question_context_used": "The study examines whether the nationality of firm ownership affects labor demand elasticities using UK establishment-level data. Key findings indicate that foreign-owned firms exhibit greater employment sensitivity to exchange rate movements compared to domestic firms. The paper derives this through estimating translog cost functions where the elasticity of substitution between labor types varies by ownership. Formula: σ_{ij} = ∂ln(x_i/x_j)/∂ln(w_j/w_i), where σ_{ij} is the elasticity of substitution between factors i and j, x denotes quantities, and w denotes prices.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1998",
    "question": "Which of the following factors contribute to the bias in the Consumer Price Index (CPI) due to the delayed inclusion of cellular telephone services?",
    "options": [
      "A) The BLS neglected the 51% decrease in combined service and handset prices prior to 1998.",
      "B) The average expenditure per month for cellular service decreased by approximately 50% from 1987 to 1997.",
      "C) The BLS did not account for the welfare gains from new products or services in the COLI.",
      "D) The growth rate of cellular subscribership was 25%-35% per year for the past decade."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "06-Cellular_Telephone__New_Products__and_the_CPI.md"
    ],
    "question_context": "The average cellular subscriber spends $49 per month on cellular service, or about $58 per year. Thus, about $32 billion per year is spent on cellular service, with additional amounts spent by consumers on purchasing cellular telephones. Cellular revenue now is somewhat over 1/3 as large as long-distance revenue, so cellular telephone represents a significant expenditure category in telecommunications. To construct a price index for cellular service, I have surveyed cellular companies in the top 30 MSA's since 1985. The top 30 MSA's make up about 41% of the total U.S. population. To account for changes in cellular usage, I calculate the lowest price subscriber plan for 160 minutes per month of cellular usage with 80% peak and 20% off-peak usage. I then use this plan to compute a price index. In Figure 1 I graph the price index for cellular service, with a base of 100 in 1985. By the beginning of the year 1998 the index had decreased to .73 so that cellular prices had decreased by about 27% over the period. Combined with the significant decrease in retail cellular-telephone prices, which I graph in Figure 2, the failure of the BLS to include cellular telephone in the CPI creates a significant bias. An overall index of the cost of cellular service and the cost of cellular telephones is graphed in Figure 3, in which the overall decrease from the base of 100 in 1985 is 51%. To create the combined index, I assume a 'churn' (customer drop) rate of .33, which has been the approximate rate for the past decade. When the BLS finally included cellular telephone in the CPI in 1998 with the actual price index at .49, the BLS began its index at the 1998 service and handset prices, neglecting the previous 51% decrease that had occurred. This neglect causes a bias in the CPI, as well as measures of standards of living and U.S. economic performance. I strongly recommend that the BLS speed up its inclusion of new goods and services in the CPI. A 15-year delay is much too long.",
    "category": "INCORRECT",
    "question_context_used": "The average cellular subscriber spends $49 per month on cellular service, or about $58 per year. Thus, about $32 billion per year is spent on cellular service, with additional amounts spent by consumers on purchasing cellular telephones. Cellular revenue now is somewhat over 1/3 as large as long-distance revenue, so cellular telephone represents a significant expenditure category in telecommunications. To construct a price index for cellular service, I have surveyed cellular companies in the top 30 MSA's since 1985. The top 30 MSA's make up about 41% of the total U.S. population. To account for changes in cellular usage, I calculate the lowest price subscriber plan for 160 minutes per month of cellular usage with 80% peak and 20% off-peak usage. I then use this plan to compute a price index. In Figure 1 I graph the price index for cellular service, with a base of 100 in 1985. By the beginning of the year 1998 the index had decreased to .73 so that cellular prices had decreased by about 27% over the period. Combined with the significant decrease in retail cellular-telephone prices, which I graph in Figure 2, the failure of the BLS to include cellular telephone in the CPI creates a significant bias. An overall index of the cost of cellular service and the cost of cellular telephones is graphed in Figure 3, in which the overall decrease from the base of 100 in 1985 is 51%. To create the combined index, I assume a 'churn' (customer drop) rate of .33, which has been the approximate rate for the past decade. When the BLS finally included cellular telephone in the CPI in 1998 with the actual price index at .49, the BLS began its index at the 1998 service and handset prices, neglecting the previous 51% decrease that had occurred. This neglect causes a bias in the CPI, as well as measures of standards of living and U.S. economic performance. I strongly recommend that the BLS speed up its inclusion of new goods and services in the CPI. A 15-year delay is much too long.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2515",
    "question": "In the context of electoral competition as a one-shot game without precommitments, which of the following statements are true regarding equilibrium policies?",
    "options": [
      "A) The equilibrium policies exhibit full convergence regardless of the value of k.",
      "B) The equilibrium policies are given by (x̄, ȳ) = (c, 0).",
      "C) The distance between equilibrium policies is inversely related to k.",
      "D) The equilibrium policies are time-inconsistent and rely on binding precommitments."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Alesina-CredibilityPolicyConvergence-1988.md"
    ],
    "question_context": "Consider one electoral competition, in isolation. Two cases should be distinguished: 1) the two parties can make binding precommitments to their preelectoral platforms; 2) no precommitments are possible. The first case has been analyzed by Wittman (1983) and Calvert (1985). With precommitments we can impose $x_{t}^{a}=x_{t}^{e}=x_{t}$ and $y_{t}^{a}=y_{t}^{e}=y_{t}^{,}$ : the announced platforms are identical to the policies expected by the voters and implemented by the elected party. The Nash equilibrium of this game can then be found by solving the following problem (the time subscripts are dropped to simplify notation): \n\n$$\n\\begin{array}{r l}&{\\underset{x}{\\mathrm{max}}w^{1}=P\\big(x,y\\big)\\big[u\\big(x\\big)+k\\big]}\\ &{\\qquad\\quad+\\big[1-P\\big(x,y\\big)\\big]u\\big(y\\big),}\\ &{\\qquad\\quad}\\ &{\\underset{y}{\\mathrm{max}}w^{2}=P\\big(x,y\\big)v\\big(x\\big)}\\ &{\\qquad\\quad}\\ &{\\qquad\\quad+\\big[1-P\\big(x,y\\big)\\big]\\big[v\\big(y\\big)+k\\big].}\\end{array}\n$$\n\nProposition 1 shows that in a one-shot game without precommitments, there cannot be any policy convergence in equilibrium, regardless of the value of $k$ .Thisresult implies that the convergent equilibria described in the literature are “time inconsistent,” or, to put it differently, that they rely upon the assumption of binding precommitments to policy announcements.",
    "category": "INCORRECT",
    "question_context_used": "Consider one electoral competition, in isolation. Two cases should be distinguished: 1) the two parties can make binding precommitments to their preelectoral platforms; 2) no precommitments are possible. The first case has been analyzed by Wittman (1983) and Calvert (1985). With precommitments we can impose $x_{t}^{a}=x_{t}^{e}=x_{t}$ and $y_{t}^{a}=y_{t}^{e}=y_{t}^{,}$ : the announced platforms are identical to the policies expected by the voters and implemented by the elected party. The Nash equilibrium of this game can then be found by solving the following problem (the time subscripts are dropped to simplify notation): \n\n$$\n\\begin{array}{r l}&{\\underset{x}{\\mathrm{max}}w^{1}=P\\big(x,y\\big)\\big[u\\big(x\\big)+k\\big]}\\ &{\\qquad\\quad+\\big[1-P\\big(x,y\\big)\\big]u\\big(y\\big),}\\ &{\\qquad\\quad}\\ &{\\underset{y}{\\mathrm{max}}w^{2}=P\\big(x,y\\big)v\\big(x\\big)}\\ &{\\qquad\\quad}\\ &{\\qquad\\quad+\\big[1-P\\big(x,y\\big)\\big]\\big[v\\big(y\\big)+k\\big].}\\end{array}\n$$\n\nProposition 1 shows that in a one-shot game without precommitments, there cannot be any policy convergence in equilibrium, regardless of the value of $k$ .Thisresult implies that the convergent equilibria described in the literature are “time inconsistent,” or, to put it differently, that they rely upon the assumption of binding precommitments to policy announcements.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\"]}",
      "model_answer": [
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4624",
    "question": "In the instrumental variables (IV) analysis of 'The Birth of a Nation' screenings' impact on Klavern formation, which of the following statements are supported by the empirical results?",
    "options": [
      "A) The 2SLS estimates are consistently larger than the OLS estimates due to nonclassical measurement error.",
      "B) The film's effect on Klavern formation was homogeneous across all demographic and geographic conditions.",
      "C) The IV estimates suggest a persuasion rate of approximately 18%, translating to over 1.5 million additional Klan members.",
      "D) The difference between OLS and IV estimates converges when the sample is restricted to counties with greater newspaper coverage."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "The Birth of a Nation Media and Racial Hate.md"
    ],
    "question_context": "The analysis employs a two-stage least squares (2SLS) approach, instrumenting for 'The Birth of a Nation' screenings using theater stock in 1914. Key formula: $\\beta_{IV} = 0.95, p = 0.036$ for Klan activity prevalence. The 2SLS estimate for the main sample is 66.2 percentage points ($p<0.001$), roughly twice the sample mean (0.31). The state-level IV regression shows $\\beta_{IV} = 0.13, p = 0.107$, implying a persuasion rate of 18%. The divergence between OLS and IV estimates is attributed to measurement error, particularly false negatives in treatment classification (Pischke 2007). When restricting samples to reduce false negatives (e.g., counties with better newspaper coverage), the estimates converge.",
    "category": "CORRECT",
    "question_context_used": "The analysis employs a two-stage least squares (2SLS) approach, instrumenting for 'The Birth of a Nation' screenings using theater stock in 1914. Key formula: $\\beta_{IV} = 0.95, p = 0.036$ for Klan activity prevalence. The 2SLS estimate for the main sample is 66.2 percentage points ($p<0.001$), roughly twice the sample mean (0.31). The state-level IV regression shows $\\beta_{IV} = 0.13, p = 0.107$, implying a persuasion rate of 18%. The divergence between OLS and IV estimates is attributed to measurement error, particularly false negatives in treatment classification (Pischke 2007). When restricting samples to reduce false negatives (e.g., counties with better newspaper coverage), the estimates converge.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1601",
    "question": "In the context of estimating household-specific expected consumption uncertainty, which of the following statements are correct regarding the split triangular distribution assumption?",
    "options": [
      "A) The split triangular distribution allows for asymmetry by incorporating the probability parameter \\(\\uppi \\) that consumption exceeds the midpoint between \\(y_m\\) and \\(y_M\\)",
      "B) The simple triangular distribution is a special case of the split triangular distribution when \\(\\uppi = 0.5\\)",
      "C) The split triangular distribution requires respondents to report exact probability density functions for future consumption",
      "D) The moments of the subjective distribution are computed using only \\(y_m\\) and \\(y_M\\), ignoring \\(\\uppi\\)"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "rest_a_00819.md"
    ],
    "question_context": "To estimate the moments of the subjective distribution of future consumption, we assume that the subjective distribution is either simple triangular (i.e., symmetric around \\(\\left(y_{m}+y_{M}\\right)/2\\), assuming \\(\\uppi=0.5\\)) or split triangular (\\(\\uppi\\neq\\) 0.5; see figure A.1. in the online appendix). Based on the elicited values for \\(y_{m}\\), \\(y_{M}\\) (and for \\(\\uppi\\) if we assume a split triangular distribution), we compute the household-specific mean and standard deviation of the distribution of expected consumption one year ahead. The formulas of these statistics are reported in online appendix A.2. Results are essentially unchanged between the split and simple triangular distributions, and thus we report in the paper results only for the split triangular, while in the online appendix we report those for the simple triangular.",
    "category": "CORRECT",
    "question_context_used": "To estimate the moments of the subjective distribution of future consumption, we assume that the subjective distribution is either simple triangular (i.e., symmetric around \\(\\left(y_{m}+y_{M}\\right)/2\\), assuming \\(\\uppi=0.5\\)) or split triangular (\\(\\uppi\\neq\\) 0.5; see figure A.1. in the online appendix). Based on the elicited values for \\(y_{m}\\), \\(y_{M}\\) (and for \\(\\uppi\\) if we assume a split triangular distribution), we compute the household-specific mean and standard deviation of the distribution of expected consumption one year ahead. The formulas of these statistics are reported in online appendix A.2. Results are essentially unchanged between the split and simple triangular distributions, and thus we report in the paper results only for the split triangular, while in the online appendix we report those for the simple triangular.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2282",
    "question": "According to the submission guidelines for the 2011 North American Winter Meeting of the Econometric Society, which of the following statements are correct?",
    "options": [
      "A) A single author may submit multiple papers if they are co-authored with different researchers.",
      "B) At least one coauthor of each submitted paper must be a member of the Econometric Society.",
      "C) Papers previously presented at other Econometric Society regional meetings are eligible for submission.",
      "D) Each presenter is limited to presenting only one paper at the conference."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Brunnermeier-ANNOUNCEMENTS-2010.md"
    ],
    "question_context": "The program committee invites contributions in the form of individual papers and entire sessions (of three or four papers). Each person may submit only one paper and only present one paper. However, each person is allowed to be the co-author of several papers submitted to the conference. At least one coauthor must be a member of the Society or must join prior to submission. The submissions should represent original manuscripts not previously presented to any Econometric Society regional meeting or submitted to other professional organizations for presentation at these same meetings.",
    "category": "INCORRECT",
    "question_context_used": "The program committee invites contributions in the form of individual papers and entire sessions (of three or four papers). Each person may submit only one paper and only present one paper. However, each person is allowed to be the co-author of several papers submitted to the conference. At least one coauthor must be a member of the Society or must join prior to submission. The submissions should represent original manuscripts not previously presented to any Econometric Society regional meeting or submitted to other professional organizations for presentation at these same meetings.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2153",
    "question": "Based on the analysis of the U.S. health-care system, which of the following factors are identified as contributing to the electorate's opposition to socialized medicine?",
    "options": [
      "A) The role of private insurance companies in resisting tort liability suits",
      "B) The increasing share of GDP spent on health-care expenditures",
      "C) The political influence of trial lawyers on health-care legislation",
      "D) The efficiency of government in managing health-care costs"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Thompson-RationalizingObservedHealth-1995.md"
    ],
    "question_context": "The Clinton Administration's health care proposal has been especially revealing in that it amounted to a currently economical substitution of publicly for privately purchased health-care insurance (see Victor Fuchs, 1993). An informed electorate's negative response then represents a common understanding that private insurance companies have an extra-economic role in the United States, one so important that a currently economical replacement of private with public insurance is nevertheless more of a curse than a blessing. The actual cost experience, which must nevertheless have been qualitatively clear to the bill-paying public, and will be detailed on request to the interested reader, is summarized as follows. The real costs of supplyingthebasicinputsof thehealth-careindustryhaveallbeenquiteundercontrolsince theearly $1960^{\\ '}\\mathbf{s},$ .Nevertheless, recognizing that actual input costs can rise above the basic cost of supplying the inputs, there has beena phenomenal increaseinthe actual costs of two inputs: prescription drugs and hospital beds.In each case, the accelerated increase in cost was initiated by a landmark lawsuit opening the floodgates to tort liability suits against private suppliers.",
    "category": "CORRECT",
    "question_context_used": "The Clinton Administration's health care proposal has been especially revealing in that it amounted to a currently economical substitution of publicly for privately purchased health-care insurance (see Victor Fuchs, 1993). An informed electorate's negative response then represents a common understanding that private insurance companies have an extra-economic role in the United States, one so important that a currently economical replacement of private with public insurance is nevertheless more of a curse than a blessing. The actual cost experience, which must nevertheless have been qualitatively clear to the bill-paying public, and will be detailed on request to the interested reader, is summarized as follows. The real costs of supplyingthebasicinputsof thehealth-careindustryhaveallbeenquiteundercontrolsince theearly $1960^{\\ '}\\mathbf{s},$ .Nevertheless, recognizing that actual input costs can rise above the basic cost of supplying the inputs, there has beena phenomenal increaseinthe actual costs of two inputs: prescription drugs and hospital beds.In each case, the accelerated increase in cost was initiated by a landmark lawsuit opening the floodgates to tort liability suits against private suppliers.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1761",
    "question": "In the context of wage offer distributions and unemployment duration, which of the following statements are true when the function ψ(w) is monotonically increasing?",
    "options": [
      "A) The expected duration of unemployment for a worker hired by a firm is a decreasing function of the wage offered by that firm.",
      "B) High wage firms tend to hire workers who have spent more time searching for jobs compared to low wage firms.",
      "C) The relationship between wage and employment probability is inversely related, similar to the standard directed search model.",
      "D) The function ψ(w) represents the expectation of the ratio of any wage to the harmonic mean of higher wages in the distribution G."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Peters-NONCONTRACTIBLEHETEROGENEITYDIRECTED-2010.md"
    ],
    "question_context": "Key formula: The expected duration of unemployment for a worker hired by a firm is given by the reciprocal of ∫Q(y)dF̃(y|w). When ψ(w) is monotonically increasing, the expected duration of unemployment for a worker hired by a firm is a decreasing function of the wage offered by that firm. This is unlike standard directed search where high wages and long duration must go together. The function ψ(w) represents the expectation of the ratio of any wage to the harmonic mean of higher wages in the distribution G. It is not particularly simple conceptually, but it is relatively easy to check. For example, it is straightforward to check that the equilibrium distribution given in closed form in Example 6.3 has the property that this function is increasing.",
    "category": "INCORRECT",
    "question_context_used": "Key formula: The expected duration of unemployment for a worker hired by a firm is given by the reciprocal of ∫Q(y)dF̃(y|w). When ψ(w) is monotonically increasing, the expected duration of unemployment for a worker hired by a firm is a decreasing function of the wage offered by that firm. This is unlike standard directed search where high wages and long duration must go together. The function ψ(w) represents the expectation of the ratio of any wage to the harmonic mean of higher wages in the distribution G. It is not particularly simple conceptually, but it is relatively easy to check. For example, it is straightforward to check that the equilibrium distribution given in closed form in Example 6.3 has the property that this function is increasing.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2226",
    "question": "Which of the following statements are consistent with the implications of industrial ownership determinants as discussed in the text?",
    "options": [
      "A) State-owned enterprises act as residual investors in sectors where private local firms lack capital and multinational firms face high entry costs.",
      "B) Restrictions on multinational firms have no impact on the sectoral composition of a country's industrial mix.",
      "C) The absence of a strong business class in a country leads to an industrial structure favoring sectors with small, incremental investments.",
      "D) Trade policies can influence the aggregate representation of multinational, state-owned, and private local firms in a country's industrial sector."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "The-determinants-of-manufacturing-ownership-in-less-d_1988_Journal-of-Develo.md"
    ],
    "question_context": "The empirical results suggest that underlying differences among the members of the institutional triad in their responses to factor market imperfections influence the resulting pattern of manufacturing ownership in all five countries, although the relative influence of market and other political considerations varies from country to country. The analysis points to a specific role for state-owned enterprises as residual investors in the five countries. Both theory and evidence suggest that their market shares are highest in sectors where production may be profitable, yet private local firms lack access to the requisite investment capital and multinational firms lack sufficient competitive advantages to meet the costs of setting up a subsidiary in unfamiliar territory and the costs of offsetting any special support governments might give to locally-owned firms. The fourth and final implication has to do with the interaction between trade policies and ownership policies. Insofar as the market shares of multinational, state-owned and private local firms vary systematically across sectors, policies that alter the sectoral mix of industry, notably trade policies, will alter the aggregate representation in the country's industrial sector of the various members of the institutional triad.",
    "category": "CORRECT",
    "question_context_used": "The empirical results suggest that underlying differences among the members of the institutional triad in their responses to factor market imperfections influence the resulting pattern of manufacturing ownership in all five countries, although the relative influence of market and other political considerations varies from country to country. The analysis points to a specific role for state-owned enterprises as residual investors in the five countries. Both theory and evidence suggest that their market shares are highest in sectors where production may be profitable, yet private local firms lack access to the requisite investment capital and multinational firms lack sufficient competitive advantages to meet the costs of setting up a subsidiary in unfamiliar territory and the costs of offsetting any special support governments might give to locally-owned firms. The fourth and final implication has to do with the interaction between trade policies and ownership policies. Insofar as the market shares of multinational, state-owned and private local firms vary systematically across sectors, policies that alter the sectoral mix of industry, notably trade policies, will alter the aggregate representation in the country's industrial sector of the various members of the institutional triad.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2593",
    "question": "In the model proposed by Miao and Wang (2012), which of the following statements are true regarding the relationship between bubbles, credit constraints, and total factor productivity (TFP)?",
    "options": [
      "A) Bubbles relax credit constraints for more productive firms, improving investment efficiency.",
      "B) The collapse of bubbles leads to a recession and a fall in TFP due to tightened credit constraints.",
      "C) Bubbles always lead to a decrease in TFP by misallocating capital to less productive firms.",
      "D) In the bubbly equilibrium, less productive firms have higher stock price bubbles."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Miao-BubblesTotalFactor-2012.md"
    ],
    "question_context": "In the bubbly equilibrium, more productive firms have higher stock price bubbles. These bubbles allow firms to relax credit constraints and improve investment efficiency and capital allocation. Consequently, more capital is allocated to more productive firms, leading to a rise in total factor productivity (TFP). On the other hand, the collapse of bubbles tightens the credit constraints and worsens investment efficiency. Thus, the collapse of bubbles leads to a recession and a fall of TFP. Our theory may help explain the empirical evidence documented by Meza and Quintin (2005); Pratap and Urrutia (forthcoming); and Queralto (2011), who find that TFP fell markedly during East Asian, Mexican, and Argentine financial crises in the 1990s.\n\nConsider an infinite-horizon economy consisting of households and firms. There is no aggregate uncertainty, but firms face idiosyncratic productivity shocks. Time is discrete and denoted by $t=0,1,2,\\ldots$. There is a continuum of identical households of unit mass. Each household is risk neutral and derives utility from a consumption stream $\\left\\{C_{t}\\right\\}$ according to the utility function, $\\textstyle\\sum_{t=0}^{\\infty}\\beta^{t}C_{t}$, where $\\beta=1/(1+r)$ is the subjective discount factor.",
    "category": "CORRECT",
    "question_context_used": "In the bubbly equilibrium, more productive firms have higher stock price bubbles. These bubbles allow firms to relax credit constraints and improve investment efficiency and capital allocation. Consequently, more capital is allocated to more productive firms, leading to a rise in total factor productivity (TFP). On the other hand, the collapse of bubbles tightens the credit constraints and worsens investment efficiency. Thus, the collapse of bubbles leads to a recession and a fall of TFP. Our theory may help explain the empirical evidence documented by Meza and Quintin (2005); Pratap and Urrutia (forthcoming); and Queralto (2011), who find that TFP fell markedly during East Asian, Mexican, and Argentine financial crises in the 1990s.\n\nConsider an infinite-horizon economy consisting of households and firms. There is no aggregate uncertainty, but firms face idiosyncratic productivity shocks. Time is discrete and denoted by $t=0,1,2,\\ldots$. There is a continuum of identical households of unit mass. Each household is risk neutral and derives utility from a consumption stream $\\left\\{C_{t}\\right\\}$ according to the utility function, $\\textstyle\\sum_{t=0}^{\\infty}\\beta^{t}C_{t}$, where $\\beta=1/(1+r)$ is the subjective discount factor.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3958",
    "question": "Based on the bivariate VAR analysis of technology indicators and economic variables, which of the following statements are supported by the empirical results?",
    "options": [
      "A) Computer and telecommunications indicators explain more variance in GDP than general technology indicators in the short run.",
      "B) Patents and R&D expenditures have a stronger Granger-causal relationship with GDP than the new technology indicators.",
      "C) The TECH series (Bowker's new technology books) has a stronger relationship to productivity measures than the TECH2 series (Library of Congress new technology books).",
      "D) A one-standard-deviation shock to technology indicators results in a peak GDP increase of 0.008 to 0.014 percent after 2 to 4 years."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Alexopoulos-ReadItHappens-2011.md"
    ],
    "question_context": "The bivariate VAR model used is $Y_{t}=\\alpha+\\gamma t+\\rho Y_{t-1}+\\varepsilon_{t}$ for most indicators, and $Y_{t}=\\alpha+\\gamma t+\\rho_{1}Y_{t-1}+\\rho_{2}Y_{t-2}+\\varepsilon_{t}$ for R&D, where $Y_{t}=[\\ln(G D P_{t}),\\ln(X_{t})]^{\\prime}$, $Y_{t}=[\\ln(T F P_{t}),\\ln(X_{t})]^{\\prime}$, or $Y_{t}=[\\ln(Y/L_{t}),\\ln(X_{t})]^{\\prime}$. The results show that new technology indicators significantly Granger-cause $\\ln(GDP)$ but there is no significant reverse causation. Computer and telecommunications indicators explain more variance in GDP than general technology indicators in the short run, with computer technology indicators accounting for up to 49% of variation at a nine-year horizon versus only about 4% for patents and R&D. The TECH series shows a stronger relationship to productivity measures than TECH2, and impulse responses indicate GDP increases by 0.008 to 0.014% after 2-4 years following a technology shock.",
    "category": "CORRECT",
    "question_context_used": "The bivariate VAR model used is $Y_{t}=\\alpha+\\gamma t+\\rho Y_{t-1}+\\varepsilon_{t}$ for most indicators, and $Y_{t}=\\alpha+\\gamma t+\\rho_{1}Y_{t-1}+\\rho_{2}Y_{t-2}+\\varepsilon_{t}$ for R&D, where $Y_{t}=[\\ln(G D P_{t}),\\ln(X_{t})]^{\\prime}$, $Y_{t}=[\\ln(T F P_{t}),\\ln(X_{t})]^{\\prime}$, or $Y_{t}=[\\ln(Y/L_{t}),\\ln(X_{t})]^{\\prime}$. The results show that new technology indicators significantly Granger-cause $\\ln(GDP)$ but there is no significant reverse causation. Computer and telecommunications indicators explain more variance in GDP than general technology indicators in the short run, with computer technology indicators accounting for up to 49% of variation at a nine-year horizon versus only about 4% for patents and R&D. The TECH series shows a stronger relationship to productivity measures than TECH2, and impulse responses indicate GDP increases by 0.008 to 0.014% after 2-4 years following a technology shock.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1420",
    "question": "In the context of the Bertrand model with strategic buyers and sellers, which of the following conditions are necessary for the Nash equilibrium allocations to be competitive?",
    "options": [
      "A) At least two buyers and two sellers actively trading in every market",
      "B) Perfect elasticity to the right of the residual demand curve",
      "C) Buyers acting as price-takers with no strategic behavior",
      "D) Sellers facing no capacity constraints on their endowments"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Bertrand, the Cournot Paradigm and the Theory of Perfect Competition.md"
    ],
    "question_context": "The paper extends Bertrand's critique of Cournot to a general equilibrium context, presenting a game-theoretic model where buyers and sellers announce both quantities and prices. A key result is that under weak conditions, a pure strategy Nash equilibrium exists, and all Nash equilibria yield competitive allocations provided there are at least two buyers and two sellers actively trading in every market. The model demonstrates that 'two is enough for competition.' The residual demand curve's perfect elasticity to the right ensures no price above the competitive equilibrium can be sustained, while strategic buyer behavior ensures perfect elasticity to the left, resolving the Edgeworth nonexistence problem. The formula for the unit vectors in the $l_1$ norm is given by $\\|\\pmb{x}\\|_1 = \\sum_{\\lambda \\in L} |\\pmb{x}_{\\lambda}|$, which is central to defining buyers' strategy sets.",
    "category": "CORRECT",
    "question_context_used": "The paper extends Bertrand's critique of Cournot to a general equilibrium context, presenting a game-theoretic model where buyers and sellers announce both quantities and prices. A key result is that under weak conditions, a pure strategy Nash equilibrium exists, and all Nash equilibria yield competitive allocations provided there are at least two buyers and two sellers actively trading in every market. The model demonstrates that 'two is enough for competition.' The residual demand curve's perfect elasticity to the right ensures no price above the competitive equilibrium can be sustained, while strategic buyer behavior ensures perfect elasticity to the left, resolving the Edgeworth nonexistence problem. The formula for the unit vectors in the $l_1$ norm is given by $\\|\\pmb{x}\\|_1 = \\sum_{\\lambda \\in L} |\\pmb{x}_{\\lambda}|$, which is central to defining buyers' strategy sets.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2588",
    "question": "Based on the regression results in Table 3, which of the following statements are supported by the data?",
    "options": [
      "A) The interaction term USDependence*FD remains statistically significant when both USDependence and USGrowth are included in the regression.",
      "B) The coefficient on USGrowth*FD is robust to the inclusion of controls for a country's level of economic development.",
      "C) Instrumenting for FD*USGrowth using settler mortality and legal origin dummies qualitatively changes the OLS results.",
      "D) The magnitude of coefficients in the instrumental variables regressions is similar to those estimated by OLS."
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Fisman-FinancialDependenceGrowth-2007.md"
    ],
    "question_context": "Table 3 presents regression results where the dependent variable is real growth in value added for each industry and country. Key findings include: (1) USDependence*FD is significant when included alone (coefficient 0.069, p < 0.01) but loses significance when USGrowth*FD is added (coefficient 0.019, p > 0.10). (2) USGrowth*FD remains significant at the 5% level (coefficient 0.843) in the joint specification and robust to including log(GDP) interactions. (3) Instrumental variable estimates using settler mortality and legal origin dummies yield qualitatively similar coefficients to OLS (e.g., USGrowth*FD coefficient of 0.984 vs. 0.843). The R² values range from 0.27 to 0.30 across specifications. Standard errors are heteroskedasticity-robust with ***, **, * denoting significance at 1%, 5%, and 10% levels respectively.",
    "category": "CORRECT",
    "question_context_used": "Table 3 presents regression results where the dependent variable is real growth in value added for each industry and country. Key findings include: (1) USDependence*FD is significant when included alone (coefficient 0.069, p < 0.01) but loses significance when USGrowth*FD is added (coefficient 0.019, p > 0.10). (2) USGrowth*FD remains significant at the 5% level (coefficient 0.843) in the joint specification and robust to including log(GDP) interactions. (3) Instrumental variable estimates using settler mortality and legal origin dummies yield qualitatively similar coefficients to OLS (e.g., USGrowth*FD coefficient of 0.984 vs. 0.843). The R² values range from 0.27 to 0.30 across specifications. Standard errors are heteroskedasticity-robust with ***, **, * denoting significance at 1%, 5%, and 10% levels respectively.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-441",
    "question": "In the context of profit-maximizing search mechanisms with private values and no discounting, which of the following statements are true regarding the optimal mechanism?",
    "options": [
      "A) The optimal mechanism may exclude bidders who would participate in a socially efficient mechanism.",
      "B) The optimal search procedure is generated by Weitzman's solution with cutoffs defined by actual utilities.",
      "C) The optimal mechanism may result in a longer search duration compared to the socially efficient mechanism.",
      "D) The order of bidder entry in the optimal mechanism is always the same as in the socially efficient mechanism."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Optimal-search-auctions_2007_Journal-of-Economic-Theory.md"
    ],
    "question_context": "In the private values case where $e_{i j}=0$ for all $i, j\\in I$ and there is no discounting, the seller’s optimal search mechanism is characterized by Weitzman’s solution with the cutoffs being implicitly defined by Eq. (18). By contrast, the cutoffs for a (socially) efficient procedure are defined by the equation $\\mathbb E_{x_{i}}\\left[x_{i}-x_{i}^{*}\\right]^{+}=c_{i}$. This difference arises because the payoff in the associated search problem is measured in virtual utilities in the former mechanism and is measured in actual utilities in the latter. \n\nProposition 1 states that every bidder $i$’s probability of participation in a socially efficient mechanism is positive if his probability of participation in an optimal mechanism is positive, but the converse is not necessarily true. Proposition 2 shows that an optimal search lasts at least as long as, and with a positive probability strictly longer than, a socially efficient search when types $x_{i}$ are identically distributed across bidders and Assumption 1 holds.",
    "category": "INCORRECT",
    "question_context_used": "In the private values case where $e_{i j}=0$ for all $i, j\\in I$ and there is no discounting, the seller’s optimal search mechanism is characterized by Weitzman’s solution with the cutoffs being implicitly defined by Eq. (18). By contrast, the cutoffs for a (socially) efficient procedure are defined by the equation $\\mathbb E_{x_{i}}\\left[x_{i}-x_{i}^{*}\\right]^{+}=c_{i}$. This difference arises because the payoff in the associated search problem is measured in virtual utilities in the former mechanism and is measured in actual utilities in the latter. \n\nProposition 1 states that every bidder $i$’s probability of participation in a socially efficient mechanism is positive if his probability of participation in an optimal mechanism is positive, but the converse is not necessarily true. Proposition 2 shows that an optimal search lasts at least as long as, and with a positive probability strictly longer than, a socially efficient search when types $x_{i}$ are identically distributed across bidders and Assumption 1 holds.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1909",
    "question": "Based on the empirical evidence from the study, which of the following statements are correct regarding the productivity growth estimates and returns to scale for the representative utility?",
    "options": [
      "A) The residual measure overestimates shifts in the production structure due to increasing returns to scale and positive output growth.",
      "B) The variable-cost estimate of returns to scale (RTSvc) is larger than the long-run approach (RTSTC) when using the average level of capital.",
      "C) The productivity growth differential between scale-adjusted long-run and short-run estimates is approximately 0.66% per year over the 35-year period.",
      "D) The envelope condition cannot be rejected during the early periods, suggesting firms are operating with optimal capital stock."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "CALLAN_-_1991_-_The_sensitivity_of_productivity_growth_measures_to_alternative_structural_and_behavioral_assumptions.md"
    ],
    "question_context": "The study employs Zellner's (1962) iterative seemingly unrelated estimation technique to estimate total and variable cost functions for 35 privately owned steam-generated electric utilities from 1951-1984. Key findings include the rejection of constant returns to scale, with the representative utility operating in the region of increasing returns to scale throughout the period. The price of capital is defined as $P_{K} = \\frac{\\text{capital expenditures}}{\\text{steam-generating capacity}}$, and technology is indexed by a time trend $T$.\n\nDenny et al. (1981) showed that increasing returns to scale and positive output growth cause the residual measure to overestimate production structure shifts. For the 1951-1984 period, the residual estimate overstates productivity growth by approximately 0.74% per year. The study also finds significant disequilibrium effects, with firms operating with excess capital in early periods, though moving toward optimal capital stock over time. The productivity growth differential between long-run and short-run estimates is approximately 0.66% per year, highlighting the importance of accounting for disequilibrium effects in performance evaluation.",
    "category": "INCORRECT",
    "question_context_used": "The study employs Zellner's (1962) iterative seemingly unrelated estimation technique to estimate total and variable cost functions for 35 privately owned steam-generated electric utilities from 1951-1984. Key findings include the rejection of constant returns to scale, with the representative utility operating in the region of increasing returns to scale throughout the period. The price of capital is defined as $P_{K} = \\frac{\\text{capital expenditures}}{\\text{steam-generating capacity}}$, and technology is indexed by a time trend $T$.\n\nDenny et al. (1981) showed that increasing returns to scale and positive output growth cause the residual measure to overestimate production structure shifts. For the 1951-1984 period, the residual estimate overstates productivity growth by approximately 0.74% per year. The study also finds significant disequilibrium effects, with firms operating with excess capital in early periods, though moving toward optimal capital stock over time. The productivity growth differential between long-run and short-run estimates is approximately 0.66% per year, highlighting the importance of accounting for disequilibrium effects in performance evaluation.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2857",
    "question": "In the context of linked decision problems and incentive compatibility, which of the following statements are correct regarding the mechanism's properties?",
    "options": [
      "A) Linking decisions allows agents to truthfully reveal intensity of preferences by trading off across multiple problems.",
      "B) The mechanism converges to truthful revelation as the number of linked problems K approaches infinity.",
      "C) Transfers are always necessary to achieve incentive compatibility in linked decision problems.",
      "D) The mechanism's efficiency improves by converting ex ante inefficiencies into ex post inefficiencies through linking."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Jackson-OvercomingIncentiveConstraints-2007.md"
    ],
    "question_context": "Key formula: The utility that agent $i$ gets from decisions $(d^{1},\\dots,d^{K}) \\in D^{K}$ given preferences $(u_{i}^{1},\\dots,u_{i}^{K}) \\in U_{i}^{K}$ is given by $\\sum_{k}u_{i}^{k}(d^{k})$. The mechanism works by linking $K$ independent decisions, allowing agents to announce their types across problems subject to budget constraints. For large $K$, the fraction of problems where the correct decision is made converges to 1 in probability. This converts ex ante inefficiencies (desire to trade across future selves) into ex post inefficiencies (actual trades across linked problems). The mechanism achieves approximate truthfulness without transfers, as agents have no incentive to lie when their type distributions are balanced across problems.",
    "category": "CORRECT",
    "question_context_used": "Key formula: The utility that agent $i$ gets from decisions $(d^{1},\\dots,d^{K}) \\in D^{K}$ given preferences $(u_{i}^{1},\\dots,u_{i}^{K}) \\in U_{i}^{K}$ is given by $\\sum_{k}u_{i}^{k}(d^{k})$. The mechanism works by linking $K$ independent decisions, allowing agents to announce their types across problems subject to budget constraints. For large $K$, the fraction of problems where the correct decision is made converges to 1 in probability. This converts ex ante inefficiencies (desire to trade across future selves) into ex post inefficiencies (actual trades across linked problems). The mechanism achieves approximate truthfulness without transfers, as agents have no incentive to lie when their type distributions are balanced across problems.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-733",
    "question": "Under Assumptions 4.1–4.4 in the simultaneous equations model, which of the following conditions are necessary for identifying the derivatives of the function $m^{1}$?",
    "options": [
      "A) Mutual independence of $\\varepsilon_{1}, \\varepsilon_{2}, \\varepsilon_{3}$ with positive, thrice differentiable densities",
      "B) Linearity of the second and third structural equations",
      "C) Non-vanishing third-order derivatives of log marginal densities of $\\varepsilon_{2}$ and $\\varepsilon_{3}$",
      "D) Strict monotonicity of $m^{1}$ in $\\varepsilon_{1}$ with $\\left(1-\\beta\\gamma\\frac{\\partial m^{1}(y_{2},\\varepsilon_{1})}{\\partial y_{2}}\\right)>0$"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "On-independence-conditions-in-nonseparable-models--Obser_2016_Journal-of-Eco.md"
    ],
    "question_context": "The model considers simultaneous equations:\n\n$$\n\\begin{array}{l}{{Y_{1}=m^{1}\\left(Y_{2},\\varepsilon_{1}\\right)}}\\ {{Y_{2}=\\beta Y_{3}+\\varepsilon_{2}}}\\ {{Y_{3}=\\gamma Y_{1}+\\varepsilon_{3}.}}\\end{array}\n$$\n\nAssumption 4.2 requires $\\varepsilon_{1},\\varepsilon_{2},\\varepsilon_{3}$ to be mutually independent with thrice differentiable densities. Assumption 4.4 imposes non-vanishing third-order derivatives of log marginal densities for identification. The strict monotonicity condition in Assumption 4.1 ($m^{1}$ strictly increasing in $\\varepsilon_{1}$) and the curvature restriction in Assumption 4.3 ($\\left(1-\\beta\\gamma\\frac{\\partial m^{1}}{\\partial y_{2}}\\right)>0$) are essential for invertibility and reduced form existence. Theorem 4.1's proof leverages these to derive:\n\n$$\n\\frac{\\partial m^{1}\\left(y_{2},\\varepsilon_{1}\\right)}{\\partial y_{2}}=\\frac{\\frac{\\partial F_{Y_{1}|\\eta}\\left(y_{1}\\right)}{\\partial\\eta}}{f_{Y_{1}|\\eta}\\left(y_{1}\\right)}\\frac{f_{Y_{2}|\\eta}\\left(y_{2}\\right)}{\\frac{\\partial F_{Y_{2}|\\eta}\\left(y_{2}\\right)}{\\partial\\eta}}.\n$$",
    "category": "CORRECT",
    "question_context_used": "The model considers simultaneous equations:\n\n$$\n\\begin{array}{l}{{Y_{1}=m^{1}\\left(Y_{2},\\varepsilon_{1}\\right)}}\\ {{Y_{2}=\\beta Y_{3}+\\varepsilon_{2}}}\\ {{Y_{3}=\\gamma Y_{1}+\\varepsilon_{3}.}}\\end{array}\n$$\n\nAssumption 4.2 requires $\\varepsilon_{1},\\varepsilon_{2},\\varepsilon_{3}$ to be mutually independent with thrice differentiable densities. Assumption 4.4 imposes non-vanishing third-order derivatives of log marginal densities for identification. The strict monotonicity condition in Assumption 4.1 ($m^{1}$ strictly increasing in $\\varepsilon_{1}$) and the curvature restriction in Assumption 4.3 ($\\left(1-\\beta\\gamma\\frac{\\partial m^{1}}{\\partial y_{2}}\\right)>0$) are essential for invertibility and reduced form existence. Theorem 4.1's proof leverages these to derive:\n\n$$\n\\frac{\\partial m^{1}\\left(y_{2},\\varepsilon_{1}\\right)}{\\partial y_{2}}=\\frac{\\frac{\\partial F_{Y_{1}|\\eta}\\left(y_{1}\\right)}{\\partial\\eta}}{f_{Y_{1}|\\eta}\\left(y_{1}\\right)}\\frac{f_{Y_{2}|\\eta}\\left(y_{2}\\right)}{\\frac{\\partial F_{Y_{2}|\\eta}\\left(y_{2}\\right)}{\\partial\\eta}}.\n$$",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1926",
    "question": "Based on the analysis of the Review's internationalization trends, which of the following statements are empirically supported?",
    "options": [
      "A) The Review has consistently mentioned foreign countries more frequently than the U.S. since at least 1975",
      "B) References to subnational regions (e.g., states or cities) are included in the country-mention ratio calculations",
      "C) The current growth rate of foreign country mentions will equalize the U.S.-world population ratio by 2100",
      "D) The foreign country mention ratio has been growing since 1975 but won't match population ratios by 2100 at current trends"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "rest_e_00766.md"
    ],
    "question_context": "The Review has always mentioned foreign countries more often than the United States, and the share of mentions of foreign countries has been growing since 1975. Still, if the current trend continues, the ratio of foreign to domestic mentions will not catch up with the relative population ratio of the U.S. and the world even by 2100. Mentions of the U.S. or foreign countries are measured by references to the country name, and not states or localities within these countries (e.g., mentions of ‘‘Arizona’’ or ‘‘Rome’’ do not count). Projections for the U.S. to world population ratio were obtained from: United Nations, Department of Economic and Social Affairs, Population Division (2017), World Population Prospects: The 2017 Revision, DVD Edition.",
    "category": "INCORRECT",
    "question_context_used": "The Review has always mentioned foreign countries more often than the United States, and the share of mentions of foreign countries has been growing since 1975. Still, if the current trend continues, the ratio of foreign to domestic mentions will not catch up with the relative population ratio of the U.S. and the world even by 2100. Mentions of the U.S. or foreign countries are measured by references to the country name, and not states or localities within these countries (e.g., mentions of ‘‘Arizona’’ or ‘‘Rome’’ do not count). Projections for the U.S. to world population ratio were obtained from: United Nations, Department of Economic and Social Affairs, Population Division (2017), World Population Prospects: The 2017 Revision, DVD Edition.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"D\"]\n}",
      "model_answer": [
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3655",
    "question": "In the context of the Peruvian Living Standards Survey, which of the following methodological choices are explicitly justified to address potential biases in agricultural productivity estimation?",
    "options": [
      "A) Using village-level median prices to compute the value of crop output",
      "B) Excluding oxen, horses, and mules from the calculation of livestock value",
      "C) Setting the appreciation rate of livestock stock at 0.2 for all households",
      "D) Ignoring seasonal variations in labor input due to high correlation between actual and usual hours"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Productivity-of-men-and-women-and-the-sexual-division-o_1991_Journal-of-Deve.md"
    ],
    "question_context": "The survey uses village-level median prices to compute the value of crop output to deflect Bardhan's (1973) criticism of using the same set of prices for all farms. This is essential given the huge variation in prices for a given crop within a village, likely due to measurement errors. Furthermore, oxen, horses, and mules are excluded from the calculation of livestock value because they are usually inputs into the production of crops, and their value appears as a right-hand side variable in the production function. The appreciation rate of livestock stock is set at 0.2, though this is acknowledged as arbitrary and subject to robustness checks. The high correlation (about 0.95) between actual and usual hours worked suggests that seasonality of hours is not significant in the Sierra, or that respondents base their estimate of usual hours on their most recent work experience.",
    "category": "CORRECT",
    "question_context_used": "The survey uses village-level median prices to compute the value of crop output to deflect Bardhan's (1973) criticism of using the same set of prices for all farms. This is essential given the huge variation in prices for a given crop within a village, likely due to measurement errors. Furthermore, oxen, horses, and mules are excluded from the calculation of livestock value because they are usually inputs into the production of crops, and their value appears as a right-hand side variable in the production function. The appreciation rate of livestock stock is set at 0.2, though this is acknowledged as arbitrary and subject to robustness checks. The high correlation (about 0.95) between actual and usual hours worked suggests that seasonality of hours is not significant in the Sierra, or that respondents base their estimate of usual hours on their most recent work experience.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3018",
    "question": "In the context of monetary equilibrium with endogenous matching, which of the following statements are correct regarding the comparison between endogenous and random matching models?",
    "options": [
      "A) The incentive condition for monetary equilibrium is stricter in the random-matching model than in the endogenous-matching model.",
      "B) Endogenous matching allows agents to spend money faster, making money more desirable compared to random matching.",
      "C) With random matching, monetary exchange can achieve the efficient outcome conditional on bilateral trade when M = 1/2.",
      "D) In the endogenous-matching model, money serves as a perfect substitute for record-keeping, achieving the same efficiency as complete record-keeping when M = 1/2."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Corbae-MatchingMoney-2002.md"
    ],
    "question_context": "Key formulas: The probabilities of trade for agents with and without money under endogenous matching are given by \\(a_{1}^{\\mathrm{e}}=\\min\\left\\{1,\\frac{1-M}{M}\\right\\}\\) and \\(a_{0}^{\\mathrm{e}}=\\min\\left\\{1,\\frac{M}{1-M}\\right\\}\\), respectively. The Bellman equations are \\(V_{0}=\\beta\\{a_{0}^{\\sf e}(-C+V_{1})+(1-a_{0}^{\\sf e})V_{0}\\}\\) and \\(V_{1}=\\beta\\{a_{1}^{\\sf e}(U+V_{0})+(1-a_{1}^{\\sf e})V_{1}\\}\\). The binding incentive condition is \\(a_{1}^{\\mathrm{e}}(U-C)\\geq\\rho C\\).\n\nIn the endogenous-matching model, when \\(M=1/2\\), monetary exchange achieves the efficient outcome conditional on bilateral trade, where each agent consumes every second period. This is as good as the outcome achievable with complete record-keeping. By contrast, with random matching, money can never achieve the same efficiency as complete record-keeping, as there can be meetings where one wants to trade but has no cash, which does not happen with endogenous matching.",
    "category": "INCORRECT",
    "question_context_used": "Key formulas: The probabilities of trade for agents with and without money under endogenous matching are given by \\(a_{1}^{\\mathrm{e}}=\\min\\left\\{1,\\frac{1-M}{M}\\right\\}\\) and \\(a_{0}^{\\mathrm{e}}=\\min\\left\\{1,\\frac{M}{1-M}\\right\\}\\), respectively. The Bellman equations are \\(V_{0}=\\beta\\{a_{0}^{\\sf e}(-C+V_{1})+(1-a_{0}^{\\sf e})V_{0}\\}\\) and \\(V_{1}=\\beta\\{a_{1}^{\\sf e}(U+V_{0})+(1-a_{1}^{\\sf e})V_{1}\\}\\). The binding incentive condition is \\(a_{1}^{\\mathrm{e}}(U-C)\\geq\\rho C\\).\n\nIn the endogenous-matching model, when \\(M=1/2\\), monetary exchange achieves the efficient outcome conditional on bilateral trade, where each agent consumes every second period. This is as good as the outcome achievable with complete record-keeping. By contrast, with random matching, money can never achieve the same efficiency as complete record-keeping, as there can be meetings where one wants to trade but has no cash, which does not happen with endogenous matching.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"C\", \"D\"]}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2667",
    "question": "Under what conditions does the model exhibit local indeterminacy in equilibrium?",
    "options": [
      "A) When the Frisch NSC is steeper than the labor demand curve (nλ < 0)",
      "B) When the degree of unemployment insurance is high (small μ)",
      "C) When externalities are sufficiently large (high η)",
      "D) When the consumption of unemployed is less than 75% of employed (μ > 1.33)"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Unemployment-and-indeterminacy_2006_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The model's parameters are calibrated as follows: \n\n$$\\big\\{\\rho,\\delta,\\propto,\\eta,S_{\\mathrm{C}},S_{\\mathrm{I}},\\mu,\\overline{{N}}\\big\\}.$$\n\nLocal indeterminacy occurs when the trace of the matrix associated with (35)–(36) is negative and the determinant is positive, which is satisfied when $n_{\\lambda}<0$. This condition is equivalent to the Frisch NSC being steeper than the labor demand curve. It is more likely to occur with less income insurance (large $\\mu$) or larger externalities (high $\\eta$). The critical value $\\mu^{*}(\\eta)$ is defined as:\n\n$$\\mu^{*}(\\eta)\\equiv\\operatorname*{inf}\\big\\{\\mu:n_{\\lambda}<0\\big\\}.$$\n\nAssuming $\\overline{{N}}>(1-(1-\\alpha)(1+\\eta))/(2-(1-\\alpha)(1+\\eta)))$, $\\mu^{*}(\\eta)$ can be written as:\n\n$$\\mu^{*}(\\eta)=\\frac{(2-(1-\\alpha)(1+\\eta))\\overline{{N}}}{(2-(1-\\alpha)(1+\\eta))\\overline{{N}}-1+(1-\\alpha)(1+\\eta)}.$$\n\nThis function is increasing in $\\eta$. Additionally, $\\mu^{*}(0)\\approx1.33$, meaning indeterminacy obtains if $\\mu>1.33$ (i.e., the consumption of unemployed is less than 75% of employed).",
    "category": "CORRECT",
    "question_context_used": "The model's parameters are calibrated as follows: \n\n$$\\big\\{\\rho,\\delta,\\propto,\\eta,S_{\\mathrm{C}},S_{\\mathrm{I}},\\mu,\\overline{{N}}\\big\\}.$$\n\nLocal indeterminacy occurs when the trace of the matrix associated with (35)–(36) is negative and the determinant is positive, which is satisfied when $n_{\\lambda}<0$. This condition is equivalent to the Frisch NSC being steeper than the labor demand curve. It is more likely to occur with less income insurance (large $\\mu$) or larger externalities (high $\\eta$). The critical value $\\mu^{*}(\\eta)$ is defined as:\n\n$$\\mu^{*}(\\eta)\\equiv\\operatorname*{inf}\\big\\{\\mu:n_{\\lambda}<0\\big\\}.$$\n\nAssuming $\\overline{{N}}>(1-(1-\\alpha)(1+\\eta))/(2-(1-\\alpha)(1+\\eta)))$, $\\mu^{*}(\\eta)$ can be written as:\n\n$$\\mu^{*}(\\eta)=\\frac{(2-(1-\\alpha)(1+\\eta))\\overline{{N}}}{(2-(1-\\alpha)(1+\\eta))\\overline{{N}}-1+(1-\\alpha)(1+\\eta)}.$$\n\nThis function is increasing in $\\eta$. Additionally, $\\mu^{*}(0)\\approx1.33$, meaning indeterminacy obtains if $\\mu>1.33$ (i.e., the consumption of unemployed is less than 75% of employed).",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-486",
    "question": "Which of the following findings are supported by the empirical results comparing uniform price and discriminatory price auction formats in the Chinese treasury securities experiment?",
    "options": [
      "A) The normalized highest and lowest winning bids of discriminatory price auctions show no statistical difference compared to uniform price auctions.",
      "B) The bid-to-cover ratios are statistically different across standard and reissued securities.",
      "C) The revenue generated from the two auction mechanisms has no statistical differences regardless of the institution.",
      "D) The liquidity effect of reissued bonds is stronger than the supply effect, leading to an increase in the bid-to-cover ratio."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Auction Mechanisms and Treasury Revenue Evidence from the Chinese Experiment.md"
    ],
    "question_context": "The text discusses empirical results from a large auction experiment conducted by two Chinese government treasury security issuers to compare uniform price and discriminatory price auction mechanisms. Key findings include that outcome yield rates for both formats are not statistically different, and there is no relevant economic difference in terms of revenue between the two mechanisms. The results also indicate that the normalized highest and lowest winning bids of discriminatory price auctions show no statistical difference compared to uniform price auctions. Additionally, the revenue generated from the two auction mechanisms has no statistical differences regardless of the institution. The bid-to-cover ratios are not statistically different across standard and reissued securities, suggesting that the liquidity and supply effects of reissued bonds are balanced.",
    "category": "CORRECT",
    "question_context_used": "The text discusses empirical results from a large auction experiment conducted by two Chinese government treasury security issuers to compare uniform price and discriminatory price auction mechanisms. Key findings include that outcome yield rates for both formats are not statistically different, and there is no relevant economic difference in terms of revenue between the two mechanisms. The results also indicate that the normalized highest and lowest winning bids of discriminatory price auctions show no statistical difference compared to uniform price auctions. Additionally, the revenue generated from the two auction mechanisms has no statistical differences regardless of the institution. The bid-to-cover ratios are not statistically different across standard and reissued securities, suggesting that the liquidity and supply effects of reissued bonds are balanced.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3808",
    "question": "Based on the analysis of shipbuilding data from 1851-1910, which of the following statements are correct regarding the composition and coverage of the dataset?",
    "options": [
      "A) The dataset includes warships and merchant ships from all global regions.",
      "B) The registers used in the study were published annually and included information on ship type, construction material, and tonnage.",
      "C) The main analysis focuses on 18,700 ships built in the Great Lakes or Atlantic Coast of the U.S. or Canada after eliminating duplicates.",
      "D) Metal ship production was concentrated in 43 locations, with the majority of North American shipbuilding locations focusing on wood ships."
    ],
    "gold_answer": "B, C, D",
    "source_files": [
      "Hanlon - 2020 - THE PERSISTENT EFFECT OF TEMPORARY INPUT COST ADVANTAGES IN SHIPBUILDING, 1850 TO 1911.md"
    ],
    "question_context": "The main analysis relies on a unique new data set derived from individual ship listings on two registers, one produced by Lloyd’s and the other by the American Bureau of Shipping (ABS, sometimes called “American Lloyd’s”). The registers were published annually and included a variety of information about each ship. From each register, the ship name, type (sail vs. steam), construction material (wood vs. metal), tonnage, the location and year of construction, and in some cases the shipyard and current home port were digitized. The full data set includes just over 69,000 ships. Most of the analysis focuses on the subset of these built in the United States or Canada from 1851 to 1910. After eliminating duplicates, the main analysis relies on observations for 18,700 ships built in the Great Lakes or Atlantic Coast of U.S. or Canada between 1851 and 1910. In 1901–1910, metal ships were being produced in 43 locations, some of which also produced wood ships, with the majority of North American shipbuilding locations continuing to focus on wood ship production.",
    "category": "CORRECT",
    "question_context_used": "The main analysis relies on a unique new data set derived from individual ship listings on two registers, one produced by Lloyd’s and the other by the American Bureau of Shipping (ABS, sometimes called “American Lloyd’s”). The registers were published annually and included a variety of information about each ship. From each register, the ship name, type (sail vs. steam), construction material (wood vs. metal), tonnage, the location and year of construction, and in some cases the shipyard and current home port were digitized. The full data set includes just over 69,000 ships. Most of the analysis focuses on the subset of these built in the United States or Canada from 1851 to 1910. After eliminating duplicates, the main analysis relies on observations for 18,700 ships built in the Great Lakes or Atlantic Coast of U.S. or Canada between 1851 and 1910. In 1901–1910, metal ships were being produced in 43 locations, some of which also produced wood ships, with the majority of North American shipbuilding locations continuing to focus on wood ship production.",
    "gold_answer_parsed": [
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3433",
    "question": "Which of the following conditions are explicitly stated as eligibility criteria for the American Council of Learned Societies (ACLS) travel grants for economists?",
    "options": [
      "A) Applicants must hold a Ph.D. degree or its equivalent",
      "B) Proposed meetings must be broadly international in sponsorship or participation",
      "C) Applicants must be presenting econometric or highly mathematical papers",
      "D) Applicants must be citizens or permanent residents of the United States"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Notes-1983.md"
    ],
    "question_context": "Economists who are strongly oriented toward the humanities, who use humanistic methods in their research, and who will be participating in meetings held outside the United States, Mexico, and Canada that are concerned with the humanistic aspects of their discipline are eligible to apply for small travel grants of the American Council of Learned Societies. Financial assistance is limited to air fare between major commercial airports and will not exceed one-half of projected economy-class fare. Social scientists and legal scholars who specialize in the history or philosophy of their disciplines are eligible if the meeting they wish to attend is so oriented. Applicants must hold a Ph.D. degree or its equivalent, and must be citizens or permanent residents of the United States. To be eligible, proposed meetings must be broadly international in sponsorship or participation, or both.",
    "category": "CORRECT",
    "question_context_used": "Economists who are strongly oriented toward the humanities, who use humanistic methods in their research, and who will be participating in meetings held outside the United States, Mexico, and Canada that are concerned with the humanistic aspects of their discipline are eligible to apply for small travel grants of the American Council of Learned Societies. Financial assistance is limited to air fare between major commercial airports and will not exceed one-half of projected economy-class fare. Social scientists and legal scholars who specialize in the history or philosophy of their disciplines are eligible if the meeting they wish to attend is so oriented. Applicants must hold a Ph.D. degree or its equivalent, and must be citizens or permanent residents of the United States. To be eligible, proposed meetings must be broadly international in sponsorship or participation, or both.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2039",
    "question": "Based on the structural model of charitable giving in the experiment, which of the following statements are supported by the evidence?",
    "options": [
      "A) The presence of a lead donor nearly doubles donations without changing response rates.",
      "B) Linear matching schemes lead to partial crowding out of donations given by individuals.",
      "C) Nonlinear matching schemes introduce focal points that influence donation amounts.",
      "D) Pure warm glow preferences are exhibited by approximately one-third of individuals."
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "Huck-ComparingCharitableFundraising-2015.md"
    ],
    "question_context": "The study presents evidence from a natural field experiment involving 25,000 opera attendees, randomly assigned to different fundraising schemes. Key findings include: (i) the announcement of a lead donor nearly doubles donations (from €1 to €1.93 on average) without changing response rates; (ii) linear matching leads to partial crowding out—individuals donate less themselves as the match rate increases; (iii) nonlinear matching creates focal points at threshold amounts (e.g., €50); and (iv) the structural model estimates that 33% of individuals exhibit pure warm glow preferences, deriving utility from the act of giving itself. The utility function is specified as: \\(U(c_i, d_i) = c_i + \\alpha_i (d_i + \\beta_i L)^{\\gamma_i}\\), where \\(c_i\\) is consumption, \\(d_i\\) is donation, and \\(L\\) is the lead gift. The model fits observed behavior across all treatments, including counterfactual scenarios where nonconvex matching schemes outperform linear ones.",
    "category": "CORRECT",
    "question_context_used": "The study presents evidence from a natural field experiment involving 25,000 opera attendees, randomly assigned to different fundraising schemes. Key findings include: (i) the announcement of a lead donor nearly doubles donations (from €1 to €1.93 on average) without changing response rates; (ii) linear matching leads to partial crowding out—individuals donate less themselves as the match rate increases; (iii) nonlinear matching creates focal points at threshold amounts (e.g., €50); and (iv) the structural model estimates that 33% of individuals exhibit pure warm glow preferences, deriving utility from the act of giving itself. The utility function is specified as: \\(U(c_i, d_i) = c_i + \\alpha_i (d_i + \\beta_i L)^{\\gamma_i}\\), where \\(c_i\\) is consumption, \\(d_i\\) is donation, and \\(L\\) is the lead gift. The model fits observed behavior across all treatments, including counterfactual scenarios where nonconvex matching schemes outperform linear ones.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2718",
    "question": "In the context of optimal unemployment policy with biased beliefs, which of the following statements are correct regarding the adjusted Baily formula?",
    "options": [
      "A) The standard Baily formula overestimates the cost from reduced incentives when agents are control-pessimistic.",
      "B) The standard Baily formula underestimates the gain from providing insurance when agents are baseline-pessimistic.",
      "C) The search internality correction in the adjusted Baily formula depends on the control bias.",
      "D) The crowd-out of savings correction in the adjusted Baily formula depends on the baseline bias."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Spinnewijn-UNEMPLOYEDOPTIMISTICOPTIMAL-2015.md"
    ],
    "question_context": "The adjusted Baily formula accounts for biased beliefs in optimal unemployment policy. Key formula: \n\n$$\\frac{u^{\\prime}(c_{u})-\\bar{u}^{\\prime}(c_{e})}{\\bar{u}^{\\prime}(c_{e})}\\left[1-\\frac{\\hat{\\pi}\\left(\\tilde{e}\\right)-\\pi\\left(\\tilde{e}\\right)}{1-\\pi\\left(\\tilde{e}\\right)}I\\left(b\\right)\\right]=\\varepsilon_{1-\\pi(\\tilde{e}),b}\\left[1-\\frac{\\hat{\\pi}^{\\prime}\\left(\\tilde{e}\\right)-\\pi^{\\prime}\\left(\\tilde{e}\\right)}{\\pi^{\\prime}\\left(\\tilde{e}\\right)}J\\left(b\\right)\\right]$$\n\nWhen beliefs are unbiased, this simplifies to the standard Baily formula. The adjusted formula corrects for two types of biases: (1) control bias (difference in marginal returns to effort) which affects the search internality, and (2) baseline bias (difference in level of employment probability) which affects the crowd-out of savings. Control-pessimistic agents underestimate marginal returns to effort, leading to suboptimally low effort. Baseline-optimistic agents save too little, causing first-order welfare losses when benefits increase.",
    "category": "INCORRECT",
    "question_context_used": "The adjusted Baily formula accounts for biased beliefs in optimal unemployment policy. Key formula: \n\n$$\\frac{u^{\\prime}(c_{u})-\\bar{u}^{\\prime}(c_{e})}{\\bar{u}^{\\prime}(c_{e})}\\left[1-\\frac{\\hat{\\pi}\\left(\\tilde{e}\\right)-\\pi\\left(\\tilde{e}\\right)}{1-\\pi\\left(\\tilde{e}\\right)}I\\left(b\\right)\\right]=\\varepsilon_{1-\\pi(\\tilde{e}),b}\\left[1-\\frac{\\hat{\\pi}^{\\prime}\\left(\\tilde{e}\\right)-\\pi^{\\prime}\\left(\\tilde{e}\\right)}{\\pi^{\\prime}\\left(\\tilde{e}\\right)}J\\left(b\\right)\\right]$$\n\nWhen beliefs are unbiased, this simplifies to the standard Baily formula. The adjusted formula corrects for two types of biases: (1) control bias (difference in marginal returns to effort) which affects the search internality, and (2) baseline bias (difference in level of employment probability) which affects the crowd-out of savings. Control-pessimistic agents underestimate marginal returns to effort, leading to suboptimally low effort. Baseline-optimistic agents save too little, causing first-order welfare losses when benefits increase.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4343",
    "question": "Which of the following factors contributed to the delayed Malthusian convergence in former-Morisco districts after the 1609 expulsion?",
    "options": [
      "A) Persistence of extractive institutions controlled by the nobility",
      "B) Cultural differences leading to smaller families and higher incomes",
      "C) Rapid adaptation of Christian in-migrants to local agricultural practices",
      "D) Immediate recovery of pre-expulsion output levels due to surplus labor"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Economic_Dynamics_in_the_Malthusian_Era_Evidence_from_the_1609_Spanish_Expulsion_of_the_Moriscos.md"
    ],
    "question_context": "The long-run response in former-Morisco areas deviates from textbook Malthusian predictions, with output per capita and population remaining differentially higher and lower, respectively, for 177 years. Archival sources suggest two main interpretations for this delay. First, the persistence of extractive institutions in former-Morisco areas, where noble control maintained high tax rates and restricted labor mobility. Second, cultural differences between Christians and Moriscos, where Moriscos were noted to have higher fertility rates and lower subsistence requirements. These factors created a sustained divergence from the expected Malthusian equilibrium. The regression estimates show that population and per capita output converged abnormally slowly in former-Morisco areas, with institutional and cultural factors playing significant roles in this delay.",
    "category": "CORRECT",
    "question_context_used": "The long-run response in former-Morisco areas deviates from textbook Malthusian predictions, with output per capita and population remaining differentially higher and lower, respectively, for 177 years. Archival sources suggest two main interpretations for this delay. First, the persistence of extractive institutions in former-Morisco areas, where noble control maintained high tax rates and restricted labor mobility. Second, cultural differences between Christians and Moriscos, where Moriscos were noted to have higher fertility rates and lower subsistence requirements. These factors created a sustained divergence from the expected Malthusian equilibrium. The regression estimates show that population and per capita output converged abnormally slowly in former-Morisco areas, with institutional and cultural factors playing significant roles in this delay.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4381",
    "question": "Based on the analysis of limited network plans, which of the following statements accurately describe the fiscal and behavioral impacts observed in the study?",
    "options": [
      "A) The premium holiday led to a 1.2% reduction in employer premium contributions in fiscal year 2012, combining a 2.8% reduction from limited network enrollment and a 1.6% increase due to the premium holiday.",
      "B) The spending reduction in limited network plans is driven primarily by patients who are unable to keep their primary care physician when switching plans.",
      "C) The study finds strong evidence of adverse health effects due to reduced access to primary care in limited network plans.",
      "D) Inertia in insurance plan enrollment suggests that fiscal benefits of the premium holiday were likely larger in subsequent years when the holiday was no longer in place."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Gruber-ControllingHealthCare-2016.md"
    ],
    "question_context": "The study finds that the large premium differential between broad and limited network plans is driven by real reductions in spending among those induced to switch plans, arising from less use of specialists and hospital care. Primary care use rises, while emergency room and hospital spending falls, suggesting no adverse health impacts. The spending reduction is driven primarily by those who keep their primary care physician when moving to a limited network plan. The premium holiday led to a 1.2% reduction in employer premium contributions in fiscal year 2012, combining a 2.8% reduction from limited network enrollment and a 1.6% increase due to the premium holiday. Inertia in plan enrollments suggests that fiscal benefits were likely larger in subsequent years when the premium holiday was no longer in place.",
    "category": "CORRECT",
    "question_context_used": "The study finds that the large premium differential between broad and limited network plans is driven by real reductions in spending among those induced to switch plans, arising from less use of specialists and hospital care. Primary care use rises, while emergency room and hospital spending falls, suggesting no adverse health impacts. The spending reduction is driven primarily by those who keep their primary care physician when moving to a limited network plan. The premium holiday led to a 1.2% reduction in employer premium contributions in fiscal year 2012, combining a 2.8% reduction from limited network enrollment and a 1.6% increase due to the premium holiday. Inertia in plan enrollments suggests that fiscal benefits were likely larger in subsequent years when the premium holiday was no longer in place.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3429",
    "question": "In the context of Jamaica's bauxite taxation policy, which of the following statements are supported by the theoretical and empirical analysis presented?",
    "options": [
      "A) A pure corporate income tax would have been sufficient to capture all Ricardian rent if Jamaica had perfect information about MNE costs",
      "B) The optimal incentive-compatible tax mechanism combines a profit tax with a nonlinear production levy",
      "C) Jamaica's actual production levy rate of $8.51/ton was close to the theoretically optimal level derived in the simulation",
      "D) The formation of the International Bauxite Association successfully prevented MNEs from shifting production to non-member countries"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Incentive-compatible-regulation-of-foreign-owned-subsid_1994_Journal-of-Deve.md"
    ],
    "question_context": "The paper analyzes Jamaica's 1974 replacement of corporate income tax with a production levy on bauxite MNEs. Key theoretical results show that with complete information, a corporate income tax could capture all Ricardian rent without distortion (Gersovitz 1987). However, under incomplete information about MNE costs, the optimal mechanism combines a profit tax with a nonlinear production levy. Formula for incentive compatibility: \n\n$$\\begin{array}{r l}&{I I(\\theta;\\theta)-I I(\\hat{\\theta};\\theta)=\\Big[I I(\\theta;\\theta)-I I(\\hat{\\theta};\\hat{\\theta})\\Big]}\\ &{\\qquad-\\Big[I I(\\hat{\\theta};\\theta)-I I(\\hat{\\theta};\\hat{\\theta})\\Big].}\\end{array}$$\n\nEmpirically, Jamaica's actual levy of $8.51/ton far exceeded the simulated optimal rate of $1.63/ton, causing significant production shifts. The IBA failed to maintain cartel discipline as non-member countries like Australia benefited from not implementing the levy.",
    "category": "CORRECT",
    "question_context_used": "The paper analyzes Jamaica's 1974 replacement of corporate income tax with a production levy on bauxite MNEs. Key theoretical results show that with complete information, a corporate income tax could capture all Ricardian rent without distortion (Gersovitz 1987). However, under incomplete information about MNE costs, the optimal mechanism combines a profit tax with a nonlinear production levy. Formula for incentive compatibility: \n\n$$\\begin{array}{r l}&{I I(\\theta;\\theta)-I I(\\hat{\\theta};\\theta)=\\Big[I I(\\theta;\\theta)-I I(\\hat{\\theta};\\hat{\\theta})\\Big]}\\ &{\\qquad-\\Big[I I(\\hat{\\theta};\\theta)-I I(\\hat{\\theta};\\hat{\\theta})\\Big].}\\end{array}$$\n\nEmpirically, Jamaica's actual levy of $8.51/ton far exceeded the simulated optimal rate of $1.63/ton, causing significant production shifts. The IBA failed to maintain cartel discipline as non-member countries like Australia benefited from not implementing the levy.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1443",
    "question": "Based on the empirical study of economics PhD completion rates, which of the following statements are supported by the data?",
    "options": [
      "A) Tier 1 programs exhibit the lowest eight-year attrition rates and the highest completion rates.",
      "B) The majority of attrition occurs after the second year of study in economics PhD programs.",
      "C) The five-year completion rates are significantly higher in tier 1 and 2 programs compared to tier 3 and 4 programs.",
      "D) The eight-year completion rate for the fall 2002 cohort is higher than historical cohorts from 1972-1976 and 1992-1995."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Stock-CompletionRatesTimetoDegree-2011.md"
    ],
    "question_context": "The study tracks 586 individuals who entered 27 economics PhD programs in fall 2002. By October 2010, 59 percent had earned a PhD, 37 percent had dropped out, and 4 percent were still writing their dissertations. Attrition is lowest at the highest ranked programs and climbs to over half of the entering class at the lowest ranked programs. The eight-year completion rate for the fall 2002 cohort is 62 percent, which is 12 percentage points higher than the 50 percent completion rate for economics PhDs reported by Bowen and Rudenstine (1992) for cohorts entering nine leading economics programs from 1972 to 1976, and 4 percentage points higher than the 58 percent completion rate reported by the Council of Graduate Schools (King 2008) for cohorts entering six economics PhD programs from 1992-1993 through 1994-1995. The formula for calculating the completion rate is: $\\text{Completion Rate} = \\frac{\\text{Number of Completers}}{\\text{Total Entering Students}} \\times 100$.",
    "category": "INCORRECT",
    "question_context_used": "The study tracks 586 individuals who entered 27 economics PhD programs in fall 2002. By October 2010, 59 percent had earned a PhD, 37 percent had dropped out, and 4 percent were still writing their dissertations. Attrition is lowest at the highest ranked programs and climbs to over half of the entering class at the lowest ranked programs. The eight-year completion rate for the fall 2002 cohort is 62 percent, which is 12 percentage points higher than the 50 percent completion rate for economics PhDs reported by Bowen and Rudenstine (1992) for cohorts entering nine leading economics programs from 1972 to 1976, and 4 percentage points higher than the 58 percent completion rate reported by the Council of Graduate Schools (King 2008) for cohorts entering six economics PhD programs from 1992-1993 through 1994-1995. The formula for calculating the completion rate is: $\\text{Completion Rate} = \\frac{\\text{Number of Completers}}{\\text{Total Entering Students}} \\times 100$.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4168",
    "question": "In the context of rational expectations equilibrium with common knowledge of rationality and market clearing, which of the following conditions must be satisfied by an admissible vector of priors μ?",
    "options": [
      "A) μ^i must assign zero probability to events where any agent's demand does not solve their utility maximization problem.",
      "B) The conditional marginal measure c_{s,p}^i μ^i must equal the prior probability over exogenous states π̅^i conditioned on the set of consistent states.",
      "C) The prior μ^i must be consistent with the prior probability over states π̅^i, such that μ^i(W × P × Y^{-i}) = π̅^i(W) for any W ⊂ S.",
      "D) The price p must be a deterministic function of the exogenous state s, eliminating all uncertainty in the market."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Rational-behavior-and-rational-expectations_1990_Journal-of-Economic-Theory.md"
    ],
    "question_context": "The paper discusses rational expectations equilibrium in a market where agents have common knowledge of rationality and market clearing. The agent's prior probability measure μ^i on (Ω^i, A^i) must satisfy certain restrictions. Key conditions include consistency with the prior probability over exogenous states π̅^i, such that μ^i(W × P × Y^{-i}) = π̅^i(W) for any W ⊂ S. Additionally, μ^i must assign zero probability to events where any agent's demand does not solve their utility maximization problem. The conditional marginal measure c_{s,p}^i μ^i is derived from μ^i and used to compute probabilities conditioned on observed information. The paper emphasizes that the prior measures μ^i are subjective probability assessments, not objective probabilities, and there is no deterministic function mapping exogenous states to equilibrium prices.",
    "category": "INCORRECT",
    "question_context_used": "The paper discusses rational expectations equilibrium in a market where agents have common knowledge of rationality and market clearing. The agent's prior probability measure μ^i on (Ω^i, A^i) must satisfy certain restrictions. Key conditions include consistency with the prior probability over exogenous states π̅^i, such that μ^i(W × P × Y^{-i}) = π̅^i(W) for any W ⊂ S. Additionally, μ^i must assign zero probability to events where any agent's demand does not solve their utility maximization problem. The conditional marginal measure c_{s,p}^i μ^i is derived from μ^i and used to compute probabilities conditioned on observed information. The paper emphasizes that the prior measures μ^i are subjective probability assessments, not objective probabilities, and there is no deterministic function mapping exogenous states to equilibrium prices.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-556",
    "question": "In the context of nested model comparison for mean squared prediction errors (MSPEs), which of the following statements are supported by the asymptotic theory discussed?",
    "options": [
      "A) Under the null hypothesis, the MSPE of the alternative model is expected to be smaller than that of the null model due to overfitting.",
      "B) The difference in MSPEs between the null and alternative models converges to a negative value as the number of predictions increases.",
      "C) The adjustment term for the alternative model's MSPE is derived from the expected squared prediction of the regressors.",
      "D) The asymptotic distribution of the MSPE difference is normal with zero mean under the null hypothesis."
    ],
    "gold_answer": "B, C",
    "source_files": [
      "Using-out-of-sample-mean-squared-prediction-errors-to-tes_2006_Journal-of-Ec.md"
    ],
    "question_context": "The text discusses the comparison of MSPEs between a null model (model 1) and an alternative model (model 2) where the null model posits that a scalar variable $y_{t}$ is a zero mean martingale difference. The alternative model assumes a linear relationship between $y_{t}$ and a vector of variables $X_{t}$. Key formula: $\\hat{\\sigma}_{1}^{2}-\\hat{\\sigma}_{2}^{2}\\rightarrow_{p}-\\operatorname{E}(X_{t+1}^{\\prime}\\hat{\\beta}_{t})^{2}<0$. This indicates that under the null, the MSPE of the null model is expected to be smaller than that of the alternative model due to the noise term introduced by the regressors in the alternative model. The adjustment term for the alternative model's MSPE is crucial for accurate inference, as it accounts for the spurious small-sample fit.",
    "category": "INCORRECT",
    "question_context_used": "The text discusses the comparison of MSPEs between a null model (model 1) and an alternative model (model 2) where the null model posits that a scalar variable $y_{t}$ is a zero mean martingale difference. The alternative model assumes a linear relationship between $y_{t}$ and a vector of variables $X_{t}$. Key formula: $\\hat{\\sigma}_{1}^{2}-\\hat{\\sigma}_{2}^{2}\\rightarrow_{p}-\\operatorname{E}(X_{t+1}^{\\prime}\\hat{\\beta}_{t})^{2}<0$. This indicates that under the null, the MSPE of the null model is expected to be smaller than that of the alternative model due to the noise term introduced by the regressors in the alternative model. The adjustment term for the alternative model's MSPE is crucial for accurate inference, as it accounts for the spurious small-sample fit.",
    "gold_answer_parsed": [
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\", \"D\"]}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4646",
    "question": "In the context of strategic experimentation and epistemic conditions for Nash equilibrium, which of the following statements are correct?",
    "options": [
      "A) Strategic experimentation involves learning through repeated interactions under uncertainty.",
      "B) Common knowledge of rationality is a sufficient condition for Nash equilibrium in all games.",
      "C) Epistemic conditions for Nash equilibrium require players to have mutual knowledge of each other's strategies.",
      "D) Strategic experimentation can lead to suboptimal outcomes if agents are myopic."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Announcements-1998.md"
    ],
    "question_context": "The manuscript by Bolton and Harris, titled 'Strategic Experimentation,' explores how agents learn through repeated interactions under uncertainty. Key formula: The expected payoff from experimentation can be modeled as a dynamic programming problem. The manuscript by Polak, titled 'Epistemic Conditions for Nash Equilibrium, and Common Knowledge of Rationality,' examines the conditions under which Nash equilibrium emerges. Key formula: The epistemic conditions require mutual knowledge of rationality and strategies, but common knowledge is not always sufficient.",
    "category": "INCORRECT",
    "question_context_used": "The manuscript by Bolton and Harris, titled 'Strategic Experimentation,' explores how agents learn through repeated interactions under uncertainty. Key formula: The expected payoff from experimentation can be modeled as a dynamic programming problem. The manuscript by Polak, titled 'Epistemic Conditions for Nash Equilibrium, and Common Knowledge of Rationality,' examines the conditions under which Nash equilibrium emerges. Key formula: The epistemic conditions require mutual knowledge of rationality and strategies, but common knowledge is not always sufficient.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"D\"]\n}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-711",
    "question": "In the structural model of education and migration decisions in China, which of the following factors are explicitly accounted for in the migration cost function?",
    "options": [
      "A) The individual's cognitive and non-cognitive skills (θᵢ)",
      "B) The calendar year (t) and its interaction with post-1982 policy changes",
      "C) The temporary prohibition of migration during 1988-89 (γ₇ᵐ and γ₈ᵐ)",
      "D) The individual's accumulated urban work experience (expᵢ,ₐᵘ)"
    ],
    "gold_answer": "A, B, C",
    "source_files": [
      "Human-capital-and-migration--A-cautionary-tale_2024_Journal-of-Econometrics.md"
    ],
    "question_context": "The migration cost function is given by: \n\n$$ m c_{i,a,t}=\\gamma_{0}^{m}+t\\gamma_{1}^{m}+a\\gamma_{2}^{m}+a^{2}\\gamma_{3}^{m}+\\theta_{i}^{\\prime}\\gamma_{4}^{m}+\\mathbb{1}_{t>1982}\\gamma_{5}^{m}+\\mathbb{1}_{t>1982}t\\gamma_{6}^{m}+\\mathbb{1}_{t>1989}\\gamma_{7}^{m}+\\mathbb{1}_{t>1989}t\\gamma_{8}^{m} $$\n\nThis specification accounts for: (1) individual endowments θᵢ = (θᵢᶜ, θᵢᴺ)′ (cognitive and non-cognitive skills), (2) calendar year t and its interaction with post-1982 policy changes (γ₅ᵐ and γ₆ᵐ), and (3) the temporary prohibition of migration during 1988-89 (γ₇ᵐ and γ₈ᵐ). The model does not include accumulated urban work experience in the migration cost function, though it appears in the urban earnings equation.",
    "category": "CORRECT",
    "question_context_used": "The migration cost function is given by: \n\n$$ m c_{i,a,t}=\\gamma_{0}^{m}+t\\gamma_{1}^{m}+a\\gamma_{2}^{m}+a^{2}\\gamma_{3}^{m}+\\theta_{i}^{\\prime}\\gamma_{4}^{m}+\\mathbb{1}_{t>1982}\\gamma_{5}^{m}+\\mathbb{1}_{t>1982}t\\gamma_{6}^{m}+\\mathbb{1}_{t>1989}\\gamma_{7}^{m}+\\mathbb{1}_{t>1989}t\\gamma_{8}^{m} $$\n\nThis specification accounts for: (1) individual endowments θᵢ = (θᵢᶜ, θᵢᴺ)′ (cognitive and non-cognitive skills), (2) calendar year t and its interaction with post-1982 policy changes (γ₅ᵐ and γ₆ᵐ), and (3) the temporary prohibition of migration during 1988-89 (γ₇ᵐ and γ₈ᵐ). The model does not include accumulated urban work experience in the migration cost function, though it appears in the urban earnings equation.",
    "gold_answer_parsed": [
      "A",
      "B",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3124",
    "question": "In the context of the gravity model of trade, which of the following statements are supported by the empirical results regarding ethnic Chinese networks and bilateral trade?",
    "options": [
      "A) The coefficients on CHINSHARE are largest for the differentiated commodity group and smallest for the organized exchange commodity group.",
      "B) The impact of ethnic Chinese networks is consistently greater for homogeneous goods than for differentiated goods.",
      "C) The coefficients on COLOTIE are always largest for the differentiated commodity group and smallest for the organized exchange commodity group.",
      "D) The marginal effect of CHINSHARE on trade between countries with large ethnic Chinese population shares is less than that for other country pairs."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Rauch-EthnicChineseNetworks-2002.md"
    ],
    "question_context": "The gravity model predicts proportionality between bilateral trade and the product of GNPs. Empirical results show that coefficients on CHINSHARE are positive and significant for all years and commodity classifications, with the largest coefficients for the differentiated commodity group and smallest for the organized exchange commodity group. The marginal effect of CHINSHARE on trade between countries with large ethnic Chinese population shares (≥1%) is less than 2%–3% of that for other country pairs. Key formula: The percentage increase in bilateral trade attributable to ethnic Chinese networks is computed as 100[exp(x̄b₁) − exp(x̄b₀)]/[exp(x̄b₀) − â], where â is the estimated threshold. The coefficients on COLOTIE are always largest for the differentiated commodity group and smallest for the organized exchange commodity group, except for the liberal aggregation in 1990.",
    "category": "INCORRECT",
    "question_context_used": "The gravity model predicts proportionality between bilateral trade and the product of GNPs. Empirical results show that coefficients on CHINSHARE are positive and significant for all years and commodity classifications, with the largest coefficients for the differentiated commodity group and smallest for the organized exchange commodity group. The marginal effect of CHINSHARE on trade between countries with large ethnic Chinese population shares (≥1%) is less than 2%–3% of that for other country pairs. Key formula: The percentage increase in bilateral trade attributable to ethnic Chinese networks is computed as 100[exp(x̄b₁) − exp(x̄b₀)]/[exp(x̄b₀) − â], where â is the estimated threshold. The coefficients on COLOTIE are always largest for the differentiated commodity group and smallest for the organized exchange commodity group, except for the liberal aggregation in 1990.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-265",
    "question": "Based on the experimental results and econometric analysis, which of the following statements are statistically supported regarding resource exploitation behavior?",
    "options": [
      "A) Regulatory institutions reduce exploitation more effectively in large resource pools than in small ones",
      "B) Resource wealth increases exploitation only when regulatory institutions are absent",
      "C) Period effects dominate the interaction between voting institutions and resource wealth",
      "D) The voting institution's effect is independent of initial resource endowment size"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Leibbrandt和Lynham - 2018 - Does the paradox of plenty exist Experimental evidence on the curse of resource abundance.md"
    ],
    "question_context": "The Tobit regression model $y=\\beta_{0}+\\beta_{1}VOTE+\\beta_{2}WEALTH+\\beta_{3}(VOTE\\times WEALTH)+\\beta_{4}t+\\epsilon$ reveals significant interaction effects. Columns 1-2 show the Voting Institution × Resource Wealth interaction is negative and significant (p<0.001), with coefficient -165.188*** in period 1. Resource wealth significantly increases exploitation without regulation (β₂>0; p<0.01), while the institution reduces exploitation even for small resources (β₁<0; p<0.05). The histograms demonstrate that in L-treatment (large pool), individuals exploited 68% on average versus 37.4% in S-treatment, while VL-treatment (regulated large) showed only 9.2% exploitation versus 18.1% in VS-treatment.",
    "category": "INCORRECT",
    "question_context_used": "The Tobit regression model $y=\\beta_{0}+\\beta_{1}VOTE+\\beta_{2}WEALTH+\\beta_{3}(VOTE\\times WEALTH)+\\beta_{4}t+\\epsilon$ reveals significant interaction effects. Columns 1-2 show the Voting Institution × Resource Wealth interaction is negative and significant (p<0.001), with coefficient -165.188*** in period 1. Resource wealth significantly increases exploitation without regulation (β₂>0; p<0.01), while the institution reduces exploitation even for small resources (β₁<0; p<0.05). The histograms demonstrate that in L-treatment (large pool), individuals exploited 68% on average versus 37.4% in S-treatment, while VL-treatment (regulated large) showed only 9.2% exploitation versus 18.1% in VS-treatment.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3316",
    "question": "Under what conditions does the model guarantee the existence of a unique asymmetric equilibrium in the context of refund contracts and Wolinsky pricing?",
    "options": [
      "A) When β < 1 and c ∈ (c̲(β), c̄(β))",
      "B) When β = 1 and c = η",
      "C) When β < 1 and c ∈ [η, c̲(β)]",
      "D) When β < 1 and c > c̄(β)"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Consumer Search and Product Returns in ECommerce.md"
    ],
    "question_context": "The proof characterizes equilibria by varying β and c, using Lemma B.9 to analyze the restricted game. A key formula is the profit difference between strategies: \n\n$$\n\\Pi_{R}(a)-\\Pi_{W}(a)=\\frac{p_{R}-c-(\\tau_{R}-\\eta)G(a+\\tau_{R})}{1-q}-\\frac{(p_{W}-c)\\big[1-G(a+p_{W})\\big]}{1-q}.\n$$\n\nWhen β < 1, Lemma B.7 ensures p_W > τ_R where Π_R(a) - Π_W(a) is nonpositive and strictly decreasing, leading to single-crossing. This property is used to prove uniqueness of equilibria under certain cost conditions. For instance, when β < 1 and c ∈ (c̲(β), c̄(β)), only a unique asymmetric equilibrium exists, while for c ∈ [η, c̲(β)], only a refund equilibrium exists.",
    "category": "INCORRECT",
    "question_context_used": "The proof characterizes equilibria by varying β and c, using Lemma B.9 to analyze the restricted game. A key formula is the profit difference between strategies: \n\n$$\n\\Pi_{R}(a)-\\Pi_{W}(a)=\\frac{p_{R}-c-(\\tau_{R}-\\eta)G(a+\\tau_{R})}{1-q}-\\frac{(p_{W}-c)\\big[1-G(a+p_{W})\\big]}{1-q}.\n$$\n\nWhen β < 1, Lemma B.7 ensures p_W > τ_R where Π_R(a) - Π_W(a) is nonpositive and strictly decreasing, leading to single-crossing. This property is used to prove uniqueness of equilibria under certain cost conditions. For instance, when β < 1 and c ∈ (c̲(β), c̄(β)), only a unique asymmetric equilibrium exists, while for c ∈ [η, c̲(β)], only a refund equilibrium exists.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2083",
    "question": "In the decomposition of wage dispersion, which of the following factors are explicitly accounted for in the variance expressions derived from the model?",
    "options": [
      "A) The correlation between offer equation error and employment equation error",
      "B) The mean variance due to search considerations across individuals",
      "C) The fixed constant treatment of the reservation wage elasticity",
      "D) The unconditional distribution of wages without any approximations"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Kiefer-IndividualEffectsNonlinear-1981.md"
    ],
    "question_context": "The dispersion in wage offers has been separated into that due to individual differences and that arising from wage distributions facing each individual. The variance in accepted wages is approximately given by: $$\\operatorname{var}(w_{i}|w_{i}>w_{i}^{r})=\\sigma^{2}(1+\\rho^{2}r_{i}\\lambda_{i}-\\rho^{2}\\lambda_{i}^{2}).$$ Here, $\\rho$ is the correlation between the error $f+\\epsilon$ in the offer equation and the error $(1-g_{1i}-\\eta)f$ in the employment equation. The sign of $\\rho$ is determined by the sign of $(1-g_{1i}-\\eta)$. The total wage variance is decomposed into variance due to search considerations (approximated by the mean over $i$ of (2.1)) and variance due to individual unobservables (approximated by the mean over $i$ of (5.1) minus (2.1)).",
    "category": "CORRECT",
    "question_context_used": "The dispersion in wage offers has been separated into that due to individual differences and that arising from wage distributions facing each individual. The variance in accepted wages is approximately given by: $$\\operatorname{var}(w_{i}|w_{i}>w_{i}^{r})=\\sigma^{2}(1+\\rho^{2}r_{i}\\lambda_{i}-\\rho^{2}\\lambda_{i}^{2}).$$ Here, $\\rho$ is the correlation between the error $f+\\epsilon$ in the offer equation and the error $(1-g_{1i}-\\eta)f$ in the employment equation. The sign of $\\rho$ is determined by the sign of $(1-g_{1i}-\\eta)$. The total wage variance is decomposed into variance due to search considerations (approximated by the mean over $i$ of (2.1)) and variance due to individual unobservables (approximated by the mean over $i$ of (5.1) minus (2.1)).",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3164",
    "question": "In the context of the Calvo (1983) price-setting model with price dispersion, which of the following statements are correct regarding the impact of price dispersion on the economy?",
    "options": [
      "A) Price dispersion acts as a downward shift in productivity, affecting utility and solvency constraints similarly.",
      "B) Price dispersion increases aggregate production costs for a given level of output.",
      "C) Optimal monetary policy should always aim for perfect price stability to eliminate price dispersion.",
      "D) Price dispersion reduces the disutility from labor by decreasing the required labor input for a given output level."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Relative_Price_Distortions_and_Inflation_Persistence.md"
    ],
    "question_context": "The Calvo (1983) approach to modelling price-stickiness is adopted. This is a convenient and familiar approach to modelling sticky prices but the same basic issues that we are interested in would seem to arise in any model where price dispersion is present. Each period a measure, $1-\\alpha$, of firms is allowed to adjust prices. Those firms choose the nominal price which maximises their expected profit given that they may have to charge the same price in $k$-periods time with probability $\\alpha^{k}$. Importantly, we are assuming that firms are cost-takers and that they do not anticipate the change in equilibrium wages in reaction to their price setting decision. The price setting problem can then be characterised by the maximization of expected profits over an infinite horizon, considering the probability of not being able to adjust prices in future periods. The impact of price dispersion is analogous to a productivity shock, as shown by the change of variables $A_{t}^{R} := A_{t}\\Delta_{t}^{-\\frac{1}{\\phi}}$, which demonstrates that price dispersion greater than unity affects the utility function and solvency constraint similarly to a downward shift in productivity. Additionally, Proposition 3 establishes that rising price dispersion, ceteris paribus, makes the economy behave like a high-cost economy, increasing labor input, aggregate production costs, and the disutility from labor for a given output level.",
    "category": "CORRECT",
    "question_context_used": "The Calvo (1983) approach to modelling price-stickiness is adopted. This is a convenient and familiar approach to modelling sticky prices but the same basic issues that we are interested in would seem to arise in any model where price dispersion is present. Each period a measure, $1-\\alpha$, of firms is allowed to adjust prices. Those firms choose the nominal price which maximises their expected profit given that they may have to charge the same price in $k$-periods time with probability $\\alpha^{k}$. Importantly, we are assuming that firms are cost-takers and that they do not anticipate the change in equilibrium wages in reaction to their price setting decision. The price setting problem can then be characterised by the maximization of expected profits over an infinite horizon, considering the probability of not being able to adjust prices in future periods. The impact of price dispersion is analogous to a productivity shock, as shown by the change of variables $A_{t}^{R} := A_{t}\\Delta_{t}^{-\\frac{1}{\\phi}}$, which demonstrates that price dispersion greater than unity affects the utility function and solvency constraint similarly to a downward shift in productivity. Additionally, Proposition 3 establishes that rising price dispersion, ceteris paribus, makes the economy behave like a high-cost economy, increasing labor input, aggregate production costs, and the disutility from labor for a given output level.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2119",
    "question": "According to Gould, Weinberg, and Mustard (2002), which of the following statements accurately describe the causal relationship between labor market conditions and crime rates among young, unskilled men in the United States from 1979 to 1997?",
    "options": [
      "A) Wages of unskilled men have a larger long-term effect on crime rates than unemployment rates due to the secular decline in wages over the period.",
      "B) Unemployment rates have a larger long-term effect on crime rates than wages due to cyclical fluctuations in unemployment.",
      "C) Both wages and unemployment rates are significantly related to crime, but wages played a larger role in the crime trends over the last few decades.",
      "D) The relationship between labor market conditions and crime is spurious and disappears when controlling for individual characteristics."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Gould-CrimeRatesLocal-2002.md"
    ],
    "question_context": "The paper examines the impact of both wages and unemployment on crime, and uses instrumental variables to establish causality. The authors conclude that both wages and unemployment are significantly related to crime, but that wages played a larger role in the crime trends over the last few decades. These results are robust to the inclusion of deterrence variables, controls for simultaneity, and controlling for individual and family characteristics. Key formula: $Crime_{i} = \\upbeta_{HS1}HS_{i}W_{si}^{HS} + \\upbeta_{HS2}HS_{i}U_{si}^{HS} + \\upbeta_{HS3}HS_{i}HouseInc_{si} + \\upbeta_{COL1}COL_{i}W_{si}^{COL} + \\upbeta_{COL2}COL_{i}U_{si}^{COL} + \\upbeta_{COL3}COL_{i}HouseInc_{si} + \\Gamma Z_{i} + \\uptheta X_{si} + \\upepsilon_{i}$. The analysis shows that low-skilled workers are most affected by changes in labor market opportunities, and these results are robust to controlling for personal and family characteristics.",
    "category": "INCORRECT",
    "question_context_used": "The paper examines the impact of both wages and unemployment on crime, and uses instrumental variables to establish causality. The authors conclude that both wages and unemployment are significantly related to crime, but that wages played a larger role in the crime trends over the last few decades. These results are robust to the inclusion of deterrence variables, controls for simultaneity, and controlling for individual and family characteristics. Key formula: $Crime_{i} = \\upbeta_{HS1}HS_{i}W_{si}^{HS} + \\upbeta_{HS2}HS_{i}U_{si}^{HS} + \\upbeta_{HS3}HS_{i}HouseInc_{si} + \\upbeta_{COL1}COL_{i}W_{si}^{COL} + \\upbeta_{COL2}COL_{i}U_{si}^{COL} + \\upbeta_{COL3}COL_{i}HouseInc_{si} + \\Gamma Z_{i} + \\uptheta X_{si} + \\upepsilon_{i}$. The analysis shows that low-skilled workers are most affected by changes in labor market opportunities, and these results are robust to controlling for personal and family characteristics.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"C\"]}",
      "model_answer": [
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1783",
    "question": "In the context of the multisector sticky-price DSGE model developed by Carvalho, Lee, and Park (2021), which of the following features are essential for generating differential responses of sectoral prices to aggregate versus sector-specific shocks?",
    "options": [
      "A) Input-output production linkages and labor market segmentation",
      "B) Exogenous nominal aggregate demand and firm-specific factor markets",
      "C) A standard monetary policy rule and economy-wide labor markets",
      "D) Labor market segmentation and a standard monetary policy rule"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Carvalho-SectoralPriceFacts-2021.md"
    ],
    "question_context": "The model deviates from the 'first-pass specification' in three dimensions that allow it to deliver endogenous differential responses of prices to aggregate and sector-specific shocks. The first feature is a roundabout input-output production structure, which imparts a direct dependence of firms’ marginal costs on the aggregate price level. The second distinguishing feature is the assumption that markets for a second factor of production are sector specific, creating a direct (negative) dependence of a firm's marginal cost on its sectoral relative price. The third departure is the (standard) assumption that monetary policy responds to endogenous variables. Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. The model's ability to match sectoral price facts relies on all three features, with labor market segmentation being particularly crucial for speeding up responses to sector-specific shocks.",
    "category": "CORRECT",
    "question_context_used": "The model deviates from the 'first-pass specification' in three dimensions that allow it to deliver endogenous differential responses of prices to aggregate and sector-specific shocks. The first feature is a roundabout input-output production structure, which imparts a direct dependence of firms’ marginal costs on the aggregate price level. The second distinguishing feature is the assumption that markets for a second factor of production are sector specific, creating a direct (negative) dependence of a firm's marginal cost on its sectoral relative price. The third departure is the (standard) assumption that monetary policy responds to endogenous variables. Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. The model's ability to match sectoral price facts relies on all three features, with labor market segmentation being particularly crucial for speeding up responses to sector-specific shocks.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2260",
    "question": "In the context of common agency games, which of the following statements are correct regarding strictly coalition-proof Nash equilibria (SCPNE) and truthful Nash equilibria?",
    "options": [
      "A) In all SCPNE, the agent's action is efficient and belongs to the set $A_{T}^{*}(N)$.",
      "B) Every vector $u\\in\\bar{U}_{T}$ can be supported by a truthful Nash equilibrium but not necessarily by a SCPNE.",
      "C) The agent receives a rent ($u_{0}>0$) in every SCPNE of a common agency game with the No Rent Property.",
      "D) There is a one-to-one relationship between truthful Nash equilibria and SCPNE in common agency games."
    ],
    "gold_answer": "A, D",
    "source_files": [
      "On-Coalition-Proof-Nash-Equilibria-in-Common-Agen_1999_Journal-of-Economic-T.md"
    ],
    "question_context": "The common agency game framework involves principals offering contingent payments to an agent, who then chooses an action maximizing her total payoff. Key formulas include the joint payoff maximization: \n\n$$W_{T}(S)=\\operatorname*{max}_{a\\in A}\\left[\\sum_{i\\in S}V_{i}(a)+V_{0}(a)\\right]$$\n\nand the set of efficient actions:\n\n$$A_{T}^{*}(S)=\\underset{a\\in A}{\\arg\\operatorname*{max}}\\left[\\sum_{i\\in S}V_{i}(a)+V_{0}(a)\\right].$$\n\nTheorem 1 states that in all truthful Nash equilibria, the agent chooses an efficient action from $A_{T}^{*}(N)$, and the principals' payoffs belong to the Pareto efficient frontier $\\bar{U}_{T}$. Theorem 2 establishes a one-to-one relationship between truthful Nash equilibria and SCPNE, with both ensuring efficient actions and Pareto-efficient payoffs. The No Rent Property implies $u_{0}=0$ in every SCPNE.",
    "category": "INCORRECT",
    "question_context_used": "The common agency game framework involves principals offering contingent payments to an agent, who then chooses an action maximizing her total payoff. Key formulas include the joint payoff maximization: \n\n$$W_{T}(S)=\\operatorname*{max}_{a\\in A}\\left[\\sum_{i\\in S}V_{i}(a)+V_{0}(a)\\right]$$\n\nand the set of efficient actions:\n\n$$A_{T}^{*}(S)=\\underset{a\\in A}{\\arg\\operatorname*{max}}\\left[\\sum_{i\\in S}V_{i}(a)+V_{0}(a)\\right].$$\n\nTheorem 1 states that in all truthful Nash equilibria, the agent chooses an efficient action from $A_{T}^{*}(N)$, and the principals' payoffs belong to the Pareto efficient frontier $\\bar{U}_{T}$. Theorem 2 establishes a one-to-one relationship between truthful Nash equilibria and SCPNE, with both ensuring efficient actions and Pareto-efficient payoffs. The No Rent Property implies $u_{0}=0$ in every SCPNE.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3986",
    "question": "According to Kaplan and Stromberg's analysis of venture capitalists (VCs) as principals, which of the following mechanisms are empirically validated to mitigate principal-agent conflicts in financial contracting?",
    "options": [
      "A) Structuring financial contracts to allocate cash flow and control rights optimally",
      "B) Engaging in pre-investment screening to exclude unprofitable projects ex ante",
      "C) Relying solely on post-investment monitoring without contractual incentives",
      "D) Generating private benefits of control to align interests with entrepreneurs"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Kaplan-VentureCapitalistsPrincipals-2001.md"
    ],
    "question_context": "The principal-agent problem in financial contracting involves conflicts between an entrepreneur (agent) and an investor (principal). Theory suggests three mechanisms to mitigate these conflicts: (1) structuring financial contracts to allocate cash flow and control rights optimally, (2) pre-investment screening to exclude unprofitable projects, and (3) post-investment monitoring. Venture capitalists (VCs) empirically approximate theoretical investors, as they invest in entrepreneurs, maximize value, and receive few private benefits of control. Their success is evident in high-profile corporate successes (e.g., Cisco, eBay) and substantial returns. Key formula: Optimal contract design minimizes agency costs by aligning incentives via \\( \\text{Contract} = \\arg\\min_{\\theta} \\left[ \\text{Agency Costs}(\\theta) \\right] \\), where \\( \\theta \\) represents contractual terms.",
    "category": "CORRECT",
    "question_context_used": "The principal-agent problem in financial contracting involves conflicts between an entrepreneur (agent) and an investor (principal). Theory suggests three mechanisms to mitigate these conflicts: (1) structuring financial contracts to allocate cash flow and control rights optimally, (2) pre-investment screening to exclude unprofitable projects, and (3) post-investment monitoring. Venture capitalists (VCs) empirically approximate theoretical investors, as they invest in entrepreneurs, maximize value, and receive few private benefits of control. Their success is evident in high-profile corporate successes (e.g., Cisco, eBay) and substantial returns. Key formula: Optimal contract design minimizes agency costs by aligning incentives via \\( \\text{Contract} = \\arg\\min_{\\theta} \\left[ \\text{Agency Costs}(\\theta) \\right] \\), where \\( \\theta \\) represents contractual terms.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1461",
    "question": "In the context of the Metropolis-Hastings (M-H) algorithm for sampling the factor loadings matrix $\\beta$, which of the following statements are correct?",
    "options": [
      "A) The proposal density for $\\beta$ is chosen as a multivariate-$t$ distribution centered at the approximate mode of the log-likelihood function.",
      "B) The acceptance probability in the M-H step depends on the ratio of the prior densities and the likelihoods evaluated at the current and proposed values of $\\beta$.",
      "C) The degrees of freedom parameter $\\nu$ in the proposal density is set to maximize the acceptance rate.",
      "D) The covariance matrix $\\Sigma$ in the proposal density is derived from the negative inverse of the Hessian matrix of the log-likelihood function."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Analysis-of-high-dimensional-multivariate-stochastic-_2006_Journal-of-Econom.md"
    ],
    "question_context": "The sampling of $\\beta$ is a key step in the algorithm, where the proposal density is chosen as a multivariate-$t$ distribution $T(\\beta|m,\\Sigma,\\nu)$. Here, $m$ is the approximate mode of the log-likelihood function $l = \\log\\{\\prod_{t=1}^{n}\\mathrm{N}_{p}(y_{t}|K_{t}q_{t},\\Omega_{t})\\}$, and $\\Sigma$ is minus the inverse of the second derivative matrix of $l$. The degrees of freedom $\\nu$ is set arbitrarily at 15. The M-H step involves drawing a proposal value $\\beta^{*}$ from $T(m,\\Sigma,\\nu)$ and accepting it with probability:\n\n$$\\alpha(\\beta,\\beta^{*}|\\widetilde{y},\\{h_{j}\\},\\{\\lambda_{j,\\cdot}\\})=\\operatorname*{min}\\biggr\\{1,\\frac{p(\\beta^{*})\\prod_{t=1}^{n}\\mathrm{N}_{p}(\\widetilde{y_{t}}|0,V_{t}^{*}+B^{*}D_{t}B^{*\\prime})}{p(\\beta)\\prod_{t=1}^{n}\\mathrm{N}_{p}(\\widetilde{y_{t}}|0,V_{t}^{*}+B D_{t}B^{\\prime})}\\frac{T(\\beta|m,\\Sigma,\\nu)}{T(\\beta^{*}|m,\\Sigma,\\nu)}\\biggr\\}.$$",
    "category": "CORRECT",
    "question_context_used": "The sampling of $\\beta$ is a key step in the algorithm, where the proposal density is chosen as a multivariate-$t$ distribution $T(\\beta|m,\\Sigma,\\nu)$. Here, $m$ is the approximate mode of the log-likelihood function $l = \\log\\{\\prod_{t=1}^{n}\\mathrm{N}_{p}(y_{t}|K_{t}q_{t},\\Omega_{t})\\}$, and $\\Sigma$ is minus the inverse of the second derivative matrix of $l$. The degrees of freedom $\\nu$ is set arbitrarily at 15. The M-H step involves drawing a proposal value $\\beta^{*}$ from $T(m,\\Sigma,\\nu)$ and accepting it with probability:\n\n$$\\alpha(\\beta,\\beta^{*}|\\widetilde{y},\\{h_{j}\\},\\{\\lambda_{j,\\cdot}\\})=\\operatorname*{min}\\biggr\\{1,\\frac{p(\\beta^{*})\\prod_{t=1}^{n}\\mathrm{N}_{p}(\\widetilde{y_{t}}|0,V_{t}^{*}+B^{*}D_{t}B^{*\\prime})}{p(\\beta)\\prod_{t=1}^{n}\\mathrm{N}_{p}(\\widetilde{y_{t}}|0,V_{t}^{*}+B D_{t}B^{\\prime})}\\frac{T(\\beta|m,\\Sigma,\\nu)}{T(\\beta^{*}|m,\\Sigma,\\nu)}\\biggr\\}.$$",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2319",
    "question": "In the context of MCMC algorithms for exploring posterior distributions in dynamic economic models, which of the following statements are correct regarding computational efficiency and model restrictions?",
    "options": [
      "A) The Gibbs sampler is more effective than the Metropolis-Hastings algorithm when the parameter vector has high dimensionality.",
      "B) The computational burden of the MCMC algorithm increases exponentially with the size of the parameter vector due to the linear program constraints.",
      "C) Additional restrictions on the distribution of unobserved states (e.g., via quantile bounds) can significantly reduce the size of the identified set for payoff parameters.",
      "D) The performance of MCMC algorithms is invariant to the complexity and dimensionality of the model, making them universally applicable without modification."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Semiparametric Inference in Dynamic Binary Choice Models.md"
    ],
    "question_context": "The posterior distribution of $(p,\\widetilde{p},\\pi,z,\\widetilde{z},\\alpha,m,s)$ is proportional to the product of the prior in (4.22) and the likelihood in (3.19) with $G_{k l}^{j}$ replaced by the corresponding elements of $\\pi$. Its density can be computed up to a normalizing constant from (3.19) and (4.22). Therefore, a Metropolis-Hastings MCMC algorithm can in principle be used for exploring the posterior distribution. To achieve good performance in practice, the proposal transition density in a Metropolis-Hastings algorithm should mimic the posterior distribution. In actual applications, the dimension of $p$ can be high and constructing good proposal distributions for a Metropolis-Hastings algorithm can be challenging. In this case, an MCMC algorithm that uses the Gibbs sampler, which updates only one or a few coordinates of the parameter vector at a time, can be much more effective. Also, different variations of the Metropolis-Hastings algorithm can be used together with the Gibbs sampler to construct robust hybrid MCMC algorithms. The main computational burden of our algorithm is the solution of the linear program for verification of $\\tilde{p}\\in\\mathcal{P}(p,\\pi)$ on every iteration of the MCMC algorithm. The number of constraints and variables in the linear program increases only linearly in the size of $p$. Nevertheless, our semi-parametric estimation algorithm is more computationally intensive than Rust’s algorithm for parametric models because the MCMC algorithm typically requires more iterations for convergence than the likelihood maximization in Rust’s algorithm. In addition to exploring the posterior distribution of $(p,\\pi,\\tilde{p})$ for Bayesian inference, we use the MCMC algorithm for constructing confidence sets for $\\theta$ and recovering the identified set of counterfactual CCPs. Additional restrictions on the distribution of unobserved states, such as quantile bounds, can significantly reduce the size of the identified set for payoff parameters, as demonstrated in Figure 1.",
    "category": "INCORRECT",
    "question_context_used": "The posterior distribution of $(p,\\widetilde{p},\\pi,z,\\widetilde{z},\\alpha,m,s)$ is proportional to the product of the prior in (4.22) and the likelihood in (3.19) with $G_{k l}^{j}$ replaced by the corresponding elements of $\\pi$. Its density can be computed up to a normalizing constant from (3.19) and (4.22). Therefore, a Metropolis-Hastings MCMC algorithm can in principle be used for exploring the posterior distribution. To achieve good performance in practice, the proposal transition density in a Metropolis-Hastings algorithm should mimic the posterior distribution. In actual applications, the dimension of $p$ can be high and constructing good proposal distributions for a Metropolis-Hastings algorithm can be challenging. In this case, an MCMC algorithm that uses the Gibbs sampler, which updates only one or a few coordinates of the parameter vector at a time, can be much more effective. Also, different variations of the Metropolis-Hastings algorithm can be used together with the Gibbs sampler to construct robust hybrid MCMC algorithms. The main computational burden of our algorithm is the solution of the linear program for verification of $\\tilde{p}\\in\\mathcal{P}(p,\\pi)$ on every iteration of the MCMC algorithm. The number of constraints and variables in the linear program increases only linearly in the size of $p$. Nevertheless, our semi-parametric estimation algorithm is more computationally intensive than Rust’s algorithm for parametric models because the MCMC algorithm typically requires more iterations for convergence than the likelihood maximization in Rust’s algorithm. In addition to exploring the posterior distribution of $(p,\\pi,\\tilde{p})$ for Bayesian inference, we use the MCMC algorithm for constructing confidence sets for $\\theta$ and recovering the identified set of counterfactual CCPs. Additional restrictions on the distribution of unobserved states, such as quantile bounds, can significantly reduce the size of the identified set for payoff parameters, as demonstrated in Figure 1.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4681",
    "question": "In the context of isolating variation in UI benefits attributable to shifts in UI policies, which of the following conditions are necessary to ensure that the variation in $B$ in $f(U|B,Z)$ reflects purely programmatic changes in UI benefit schemes?",
    "options": [
      "A) Incorporating controls in $Z$ to neutralize variations in $B$ attributable to differences in workers' $H$ variables and state labor-market $S$ variables",
      "B) Assigning UI benefits based solely on individual-specific earnings patterns without considering state laws",
      "C) Including only macroeconomic variables in $Z$ to account for the influence of $S$",
      "D) Formulating $Z$ to absorb variation in $B$ due to differences in the generosity of UI systems"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "02-Measuring the Influence of Unemployment Insurance on Unemployment Experiences.md"
    ],
    "question_context": "To ensure that variations in the UI-benefit variables included in unemployment distributions identify and accurately predict responses to shifts in UI policy regimes, variation in $B$ in $f(U|B,Z)$ must reflect purely programmatic changes in UI benefit schemes. Thus, one must carefully incorporate controls in $Z$ to neutralize other sources of variations in $B$. Three factors determine $B$ -state rules reflecting the generosity of their systems, various attributes of individual's previous work history $(H)$, and sometimes information on local economic conditions $(S)$, such as state unemployment rates. The first factor represents differences in UI policy regimes, whereas the second and third do not. Consequently, to associate variation of $B$ in $f(U|B,Z)$ with differences in the generosities of UI systems, we need to formulate $Z$ in a way to absorb all variation in $B$ attributable to differences in the relevant components of workers' $H$ variables and the state labor-market $S$ variables. If we fail to do so, then the coefficients associated with $B$ in estimation will in part measure the predictive power of persons’ previous work history on their reemployment rates or possibly the effects of local economic conditions on these rates.",
    "category": "CORRECT",
    "question_context_used": "To ensure that variations in the UI-benefit variables included in unemployment distributions identify and accurately predict responses to shifts in UI policy regimes, variation in $B$ in $f(U|B,Z)$ must reflect purely programmatic changes in UI benefit schemes. Thus, one must carefully incorporate controls in $Z$ to neutralize other sources of variations in $B$. Three factors determine $B$ -state rules reflecting the generosity of their systems, various attributes of individual's previous work history $(H)$, and sometimes information on local economic conditions $(S)$, such as state unemployment rates. The first factor represents differences in UI policy regimes, whereas the second and third do not. Consequently, to associate variation of $B$ in $f(U|B,Z)$ with differences in the generosities of UI systems, we need to formulate $Z$ in a way to absorb all variation in $B$ attributable to differences in the relevant components of workers' $H$ variables and the state labor-market $S$ variables. If we fail to do so, then the coefficients associated with $B$ in estimation will in part measure the predictive power of persons’ previous work history on their reemployment rates or possibly the effects of local economic conditions on these rates.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-304",
    "question": "In the context of dynamic second-price auctions with costly entry, which of the following statements are true regarding delayed revelation equilibria?",
    "options": [
      "A) Delayed revelation equilibria involve coordinated low early bids to deter entry of high-value bidders.",
      "B) Social welfare is maximized in the delayed revelation equilibrium compared to the immediate revelation equilibrium.",
      "C) The expected surplus for buyers is zero in the immediate revelation equilibrium but positive in delayed revelation equilibria.",
      "D) The seller's expected revenue is always higher in delayed revelation equilibria than in the immediate revelation equilibrium."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Che-EntryDeterrenceDynamic-2016.md"
    ],
    "question_context": "The paper examines dynamic second-price auctions with independent private values and sequential costly entry. Key findings include the existence of delayed revelation equilibria where buyers place coordinated low early bids to deter entry, revising their bids to true valuations just prior to the auction's end. The immediate revelation equilibrium involves bidders entering if the current price is below a cutoff $p_{1}^{*}$ and bidding their valuation immediately. In contrast, the delayed revelation equilibrium has a lower cutoff $p_{2}^{*}<p_{1}^{*}$, with bidders initially bidding $p_{2}^{*}$ if their valuation exceeds it, deterring further entry. Social welfare is higher in the immediate revelation equilibrium, while buyers achieve positive surplus only in delayed revelation equilibria. The seller's revenue is higher in the immediate revelation equilibrium under uniform valuations.",
    "category": "CORRECT",
    "question_context_used": "The paper examines dynamic second-price auctions with independent private values and sequential costly entry. Key findings include the existence of delayed revelation equilibria where buyers place coordinated low early bids to deter entry, revising their bids to true valuations just prior to the auction's end. The immediate revelation equilibrium involves bidders entering if the current price is below a cutoff $p_{1}^{*}$ and bidding their valuation immediately. In contrast, the delayed revelation equilibrium has a lower cutoff $p_{2}^{*}<p_{1}^{*}$, with bidders initially bidding $p_{2}^{*}$ if their valuation exceeds it, deterring further entry. Social welfare is higher in the immediate revelation equilibrium, while buyers achieve positive surplus only in delayed revelation equilibria. The seller's revenue is higher in the immediate revelation equilibrium under uniform valuations.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-708",
    "question": "In the context of oil price shocks and macroeconomic instability, which of the following factors are empirically supported by BjØrnland et al. (2018) as significant contributors to economic volatility?",
    "options": [
      "A) Asymmetric responses of output to positive vs. negative oil price shocks",
      "B) Endogenous monetary policy reactions to oil price fluctuations",
      "C) Time-varying elasticity of substitution between oil and other inputs",
      "D) Non-linear propagation mechanisms through financial markets"
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Bjrnland-OilMacroeconomicInStability-2018.md"
    ],
    "question_context": "The study by BjØrnland et al. (2018) examines how oil price shocks propagate through the economy, identifying several channels of macroeconomic instability. Key findings include asymmetric effects where negative oil price shocks have larger impacts than positive ones (\\frac{\\partial Y}{\\partial O^-} > \\frac{\\partial Y}{\\partial O^+}), and the role of monetary policy in amplifying or dampening these effects. The authors also highlight the importance of financial market linkages, where oil price volatility transmits to broader economic instability through credit constraints and risk premia adjustments. Their structural VAR approach controls for endogenous policy responses and identifies non-linearities in the transmission mechanism.",
    "category": "CORRECT",
    "question_context_used": "The study by BjØrnland et al. (2018) examines how oil price shocks propagate through the economy, identifying several channels of macroeconomic instability. Key findings include asymmetric effects where negative oil price shocks have larger impacts than positive ones (\\frac{\\partial Y}{\\partial O^-} > \\frac{\\partial Y}{\\partial O^+}), and the role of monetary policy in amplifying or dampening these effects. The authors also highlight the importance of financial market linkages, where oil price volatility transmits to broader economic instability through credit constraints and risk premia adjustments. Their structural VAR approach controls for endogenous policy responses and identifies non-linearities in the transmission mechanism.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3267",
    "question": "In the context of the New Keynesian model with demographic shocks, which of the following statements accurately describe the impact of demographic changes on the U.S. economy?",
    "options": [
      "A) Demographic shocks account for approximately 40% of the decline in output relative to its pre-crisis trend by 2019.",
      "B) Aging populations lead to higher real interest rates due to increased savings behavior among older cohorts.",
      "C) Demographic changes caused the zero lower bound (ZLB) to bind more frequently after the Great Recession.",
      "D) Forward guidance policy mitigated the impact of the ZLB, preventing an additional 2–5 percentage point drop in output."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "rest_a_01177.md"
    ],
    "question_context": "Demographic changes generate significant trends in real and nominal variables. I find that demographic shocks alone are responsible for about $40\\%$ of the decline in output relative to trend between 2008 and the end of 2019. Demographics also account for about a 1 percentage point decline in the real interest rate and a 2 percentage point decline in the nominal interest rate between 1990 and 2019. I find that the interactions between demographic changes and business cycle shocks are quantitatively important such that, had demographics not changed between 1986 and 2019, the ZLB would have been a much less significant constraint on monetary policy. In this counterfactual, the Federal Reserve would have been able to lower the real interest rate by an additional 1–2 percentage points in 2009 and 2010. Furthermore, the decline in interest rates caused by demographics brings the policy rate closer to the ZLB. In simulations of the estimated model, I find that the frequency of ZLB episodes increases over time. If demographics are fixed at their 1990 level, the ZLB binds for $1\\%$ of the time. If instead demographics are fixed at their 2020 level, the ZLB binds for $12\\%$ of the time. Forward guidance, or lower-for-longer policy, would extend these lower bound episodes.",
    "category": "INCORRECT",
    "question_context_used": "Demographic changes generate significant trends in real and nominal variables. I find that demographic shocks alone are responsible for about $40\\%$ of the decline in output relative to trend between 2008 and the end of 2019. Demographics also account for about a 1 percentage point decline in the real interest rate and a 2 percentage point decline in the nominal interest rate between 1990 and 2019. I find that the interactions between demographic changes and business cycle shocks are quantitatively important such that, had demographics not changed between 1986 and 2019, the ZLB would have been a much less significant constraint on monetary policy. In this counterfactual, the Federal Reserve would have been able to lower the real interest rate by an additional 1–2 percentage points in 2009 and 2010. Furthermore, the decline in interest rates caused by demographics brings the policy rate closer to the ZLB. In simulations of the estimated model, I find that the frequency of ZLB episodes increases over time. If demographics are fixed at their 1990 level, the ZLB binds for $1\\%$ of the time. If instead demographics are fixed at their 2020 level, the ZLB binds for $12\\%$ of the time. Forward guidance, or lower-for-longer policy, would extend these lower bound episodes.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\"]\n}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1614",
    "question": "Under the assumptions of Theorem 1 and Theorem 2, which of the following statements are correct regarding the OLS slope vector and its correction for selection bias?",
    "options": [
      "A) The OLS slope vector $\\pmb{b}$ converges almost surely to $\\beta + \\lambda\\zeta$ under the given conditions.",
      "B) The asymptotic bias in the OLS slope vector is proportional to $\\pmb{\\zeta}$ only if all components of $\\pmb{\\zeta}$ are non-zero.",
      "C) The corrected OLS estimate $b^{(c)} = b - (b_1/z_1) \\cdot z$ is strongly consistent for $\\pmb{\\beta}$ if $z$ is a consistent estimate for $\\pmb{\\zeta}$ up to a non-zero scalar.",
      "D) The term $\\pmb{b}^{(3)}$ in the decomposition of the OLS slope has a non-zero mean conditioned on $y$ being observed."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Distribution-free-and-link-free-estimation-for-the-sam_1987_Journal-of-Econo.md"
    ],
    "question_context": "The observed outcome can be decomposed into three terms as follows: \n\n$$\n\\begin{array}{r}{y=\\big(\\alpha+\\beta x\\big)+q\\big(\\eta+\\zeta x\\big)+\\big(\\varepsilon-q\\big(\\eta+\\zeta x\\big)\\big),}\\end{array}\n$$\n\nwhere $q(\\boldsymbol{\\eta}+\\boldsymbol{\\xi}\\boldsymbol{x})$ is the bias due to selection: \n\n$$\nq\\big(\\eta+\\xi x\\big)=\\mathrm{E}\\big(\\varepsilon|x,h\\big(\\eta+\\xi x,\\delta\\big)>0\\big).\n$$\n\nThe OLS slope $\\pmb{b}$ can be decomposed into $b=\\beta+b^{(2)}+b^{(3)}$, where $b^{(2)}$ is the OLS regression of $\\pmb q$ on $x$, and $\\pmb{b}^{(3)}$ is the OLS regression of $\\pmb{\\varepsilon}-\\pmb{q}$ on ${\\pmb x}$. The term $\\pmb{b}^{(3)}$ has mean zero conditioned on $y$ being observed, while $b^{(2)}$ generally has a non-zero mean. \n\nTheorem 1 states that the OLS slope vector $\\pmb{b}$ converges almost surely to $\\beta^{*}=\\beta+\\lambda\\zeta$ under certain conditions. Theorem 2 provides the corrected OLS estimate $b^{(c)}=b-\\left(b_{1}/z_{1}\\right)\\cdot z$, which is strongly consistent for $\\pmb\\beta$ if $z$ is a strongly consistent estimate for $\\pmb{\\zeta}$ up to a non-zero multiplicative scalar.",
    "category": "CORRECT",
    "question_context_used": "The observed outcome can be decomposed into three terms as follows: \n\n$$\n\\begin{array}{r}{y=\\big(\\alpha+\\beta x\\big)+q\\big(\\eta+\\zeta x\\big)+\\big(\\varepsilon-q\\big(\\eta+\\zeta x\\big)\\big),}\\end{array}\n$$\n\nwhere $q(\\boldsymbol{\\eta}+\\boldsymbol{\\xi}\\boldsymbol{x})$ is the bias due to selection: \n\n$$\nq\\big(\\eta+\\xi x\\big)=\\mathrm{E}\\big(\\varepsilon|x,h\\big(\\eta+\\xi x,\\delta\\big)>0\\big).\n$$\n\nThe OLS slope $\\pmb{b}$ can be decomposed into $b=\\beta+b^{(2)}+b^{(3)}$, where $b^{(2)}$ is the OLS regression of $\\pmb q$ on $x$, and $\\pmb{b}^{(3)}$ is the OLS regression of $\\pmb{\\varepsilon}-\\pmb{q}$ on ${\\pmb x}$. The term $\\pmb{b}^{(3)}$ has mean zero conditioned on $y$ being observed, while $b^{(2)}$ generally has a non-zero mean. \n\nTheorem 1 states that the OLS slope vector $\\pmb{b}$ converges almost surely to $\\beta^{*}=\\beta+\\lambda\\zeta$ under certain conditions. Theorem 2 provides the corrected OLS estimate $b^{(c)}=b-\\left(b_{1}/z_{1}\\right)\\cdot z$, which is strongly consistent for $\\pmb\\beta$ if $z$ is a strongly consistent estimate for $\\pmb{\\zeta}$ up to a non-zero multiplicative scalar.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\"]}",
      "model_answer": [
        "A",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3052",
    "question": "In the context of international trade between capital-rich and capital-poor economies, which of the following statements are consistent with Lemma 1 regarding the gender wage gap $\\omega_{t}$ and sectoral outputs?",
    "options": [
      "A) For $\\pi_{t} \\leq \\underline{\\pi}$, the gender wage gap $\\omega_{t}$ remains constant as all factors are employed in the first sector.",
      "B) For $\\pi_{t} \\in (\\underline{\\pi}, \\overline{\\pi})$, the gender wage gap $\\omega_{t}$ increases with $\\pi_{t}$ due to labor reallocation effects.",
      "C) For $\\pi_{t} \\geq \\overline{\\pi}$, the gender wage gap $\\omega_{t}$ is increasing in $\\pi_{t}$ as all male workers are employed in the second sector.",
      "D) Output of the $X_{1}$ sector is weakly decreasing in $\\pi_{t}$, while output of the $X_{2}$ sector is weakly increasing in $\\pi_{t}$."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "International-trade--the-gender-wage-gap-and-femal_2014_Journal-of-Developme.md"
    ],
    "question_context": "Lemma 1 establishes the relationship between the world price $\\pi_{t}$ and the gender wage gap $\\omega_{t}$ in a small open economy. For $\\pi_{t} \\leq \\underline{\\pi}$, $\\omega_{t}$ is constant because all factors are employed in the first sector, and small price changes do not alter labor allocation. For $\\pi_{t} \\in (\\underline{\\pi}, \\overline{\\pi})$, $\\omega_{t}$ decreases due to labor reallocation effects. For $\\pi_{t} \\geq \\overline{\\pi}$, $\\omega_{t}$ increases as all male workers shift to the second sector. The outputs of the $X_{1}$ and $X_{2}$ sectors are weakly decreasing and increasing in $\\pi_{t}$, respectively, reflecting the basic scheme of international trade.\n\nKey formula: \n$$\n\\omega_{t}=\\frac{\\pi_{t}}{\\pi_{t}-1}\n$$\nThis shows the inverse relationship between $\\omega_{t}$ and $\\pi_{t}$ when $l_{1,t}, l_{2,t} > 0$. The lemma also highlights the thresholds $\\underline{\\pi}$ and $\\overline{\\pi}$ that partition the behavior of $\\omega_{t}$ and sectoral outputs.",
    "category": "INCORRECT",
    "question_context_used": "Lemma 1 establishes the relationship between the world price $\\pi_{t}$ and the gender wage gap $\\omega_{t}$ in a small open economy. For $\\pi_{t} \\leq \\underline{\\pi}$, $\\omega_{t}$ is constant because all factors are employed in the first sector, and small price changes do not alter labor allocation. For $\\pi_{t} \\in (\\underline{\\pi}, \\overline{\\pi})$, $\\omega_{t}$ decreases due to labor reallocation effects. For $\\pi_{t} \\geq \\overline{\\pi}$, $\\omega_{t}$ increases as all male workers shift to the second sector. The outputs of the $X_{1}$ and $X_{2}$ sectors are weakly decreasing and increasing in $\\pi_{t}$, respectively, reflecting the basic scheme of international trade.\n\nKey formula: \n$$\n\\omega_{t}=\\frac{\\pi_{t}}{\\pi_{t}-1}\n$$\nThis shows the inverse relationship between $\\omega_{t}$ and $\\pi_{t}$ when $l_{1,t}, l_{2,t} > 0$. The lemma also highlights the thresholds $\\underline{\\pi}$ and $\\overline{\\pi}$ that partition the behavior of $\\omega_{t}$ and sectoral outputs.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"D\"]}",
      "model_answer": [
        "A",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2308",
    "question": "Which of the following experimental findings are consistent with the self-serving bias in fairness judgments as discussed in the literature?",
    "options": [
      "A) Subjects informed of their roles after reading case materials showed higher settlement rates and agreement on fair settlements.",
      "B) Dictators in experiments consistently exhibited generosity bias regardless of experimental procedures.",
      "C) Labor union and school board negotiators selected comparison groups in a self-serving manner, correlating with strike activity.",
      "D) Subjects in the control condition, informed of their roles before reading materials, exhibited greater differences in fair settlement estimates."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Konow-FairSharesAccountability-2000.md"
    ],
    "question_context": "The literature discusses extensive evidence of self-serving biases in fairness judgments. Babcock et al. (1995) manipulated the timing of role assignment in experiments, finding that subjects informed of their roles after reading materials were significantly more likely to settle and agree on fair settlements. In contrast, control condition subjects, informed of roles beforehand, exhibited greater disparities in fair settlement estimates. Additionally, Babcock et al. (1996) observed self-serving bias in real-world teacher contract negotiations, where negotiators' selection of comparison groups correlated with strike activity. These findings contrast with generosity biases observed in some dictator experiments, which are often attributed to procedural artifacts rather than intrinsic fairness norms. Key formula: Settlement rates ∝ (1 / |Δfair estimates|), where Δfair estimates represent the divergence in parties' perceptions of fairness.",
    "category": "CORRECT",
    "question_context_used": "The literature discusses extensive evidence of self-serving biases in fairness judgments. Babcock et al. (1995) manipulated the timing of role assignment in experiments, finding that subjects informed of their roles after reading materials were significantly more likely to settle and agree on fair settlements. In contrast, control condition subjects, informed of roles beforehand, exhibited greater disparities in fair settlement estimates. Additionally, Babcock et al. (1996) observed self-serving bias in real-world teacher contract negotiations, where negotiators' selection of comparison groups correlated with strike activity. These findings contrast with generosity biases observed in some dictator experiments, which are often attributed to procedural artifacts rather than intrinsic fairness norms. Key formula: Settlement rates ∝ (1 / |Δfair estimates|), where Δfair estimates represent the divergence in parties' perceptions of fairness.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3637",
    "question": "Based on the revenue regression analysis of hygiene grade cards, which of the following statements are empirically supported by the data?",
    "options": [
      "A) Mandatory disclosure of A-grade cards leads to a statistically significant 5.7% increase in revenue",
      "B) Voluntary disclosure has no measurable effect on restaurant revenue regardless of hygiene grade",
      "C) The revenue increase for B-grade restaurants under mandatory disclosure is approximately 4.97% lower than for A-grade restaurants",
      "D) D-grade restaurants experience a revenue increase under voluntary disclosure regimes"
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Effect of Information on Product Quality Evidence from Restaurant Hygiene Grade Cards.md"
    ],
    "question_context": "The regression model for log revenue is specified as:\n\n$$\n\\ln{(r_{i t})}=\\alpha_{i}+\\tau_{t}+\\sum_{j}\\beta_{j}h_{i j t}+\\sum_{k}\\gamma_{k}g_{i k t}+\\sum_{j}\\sum_{k}\\S_{j k}h_{i j t}g_{i k t}+\\epsilon_{i t},\n$$\n\nwhere $h_{i t}=\\{h_{i1t},\\ldots,h_{i4t}\\}\\equiv\\{A_{i t},B_{i t},C_{i t},D_{i t}\\}$ represents hygiene grades and $g_{i t}=\\{g_{i1t},g_{i2t}\\}\\equiv\\{m_{i t},v_{i t}\\}$ represents disclosure regimes. The results show that mandatory disclosure of A-grade cards leads to a 5.7% revenue increase (coefficient 0.0569, p<0.01), while the interaction term for mandatory × B-grade shows a -0.0497 coefficient (p<0.01), indicating a 4.97% smaller effect compared to A-grade. \n\nFor voluntary disclosure, A-grade restaurants show a 3.3% revenue increase (coefficient 0.0326, p<0.05), but the effects for other grades are statistically insignificant. The table explicitly shows that D-grade restaurants have negative coefficients under both disclosure regimes (-0.0565 for mandatory, -0.0758 for voluntary), though only the latter is statistically insignificant.",
    "category": "CORRECT",
    "question_context_used": "The regression model for log revenue is specified as:\n\n$$\n\\ln{(r_{i t})}=\\alpha_{i}+\\tau_{t}+\\sum_{j}\\beta_{j}h_{i j t}+\\sum_{k}\\gamma_{k}g_{i k t}+\\sum_{j}\\sum_{k}\\S_{j k}h_{i j t}g_{i k t}+\\epsilon_{i t},\n$$\n\nwhere $h_{i t}=\\{h_{i1t},\\ldots,h_{i4t}\\}\\equiv\\{A_{i t},B_{i t},C_{i t},D_{i t}\\}$ represents hygiene grades and $g_{i t}=\\{g_{i1t},g_{i2t}\\}\\equiv\\{m_{i t},v_{i t}\\}$ represents disclosure regimes. The results show that mandatory disclosure of A-grade cards leads to a 5.7% revenue increase (coefficient 0.0569, p<0.01), while the interaction term for mandatory × B-grade shows a -0.0497 coefficient (p<0.01), indicating a 4.97% smaller effect compared to A-grade. \n\nFor voluntary disclosure, A-grade restaurants show a 3.3% revenue increase (coefficient 0.0326, p<0.05), but the effects for other grades are statistically insignificant. The table explicitly shows that D-grade restaurants have negative coefficients under both disclosure regimes (-0.0565 for mandatory, -0.0758 for voluntary), though only the latter is statistically insignificant.",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3187",
    "question": "In the context of the nonlinear multivariate regression model for the APT with observable macroeconomic variables, which of the following conditions are necessary for the NLSUR estimators to exist and be strongly consistent?",
    "options": [
      "A) The Jacobian of the response function must be of full column rank.",
      "B) The error terms must be multivariate normally distributed.",
      "C) The factor matrix F and the sensitivities matrix B must be of full column rank.",
      "D) The number of time periods T must be greater than the number of assets N, which in turn must be greater than the number of factors K."
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "MCELROY和BURMEISTER - 1988 - Arbitrage pricing theory as a restricted nonlinear multivariate regression-model - iterated nonlinea.md"
    ],
    "question_context": "The system (4) is a multivariate nonlinear regression model with across-equation restrictions for which McElroy et al. (1985) showed that, conditional on the factor realizations, Gallant's (1975) NLSUR technique can be used to obtain joint estimators for the $b_{i j}$ 's and the $\\lambda_{j}^{*}\\mathbf{s}.$ Specifically, rewrite the system (4) as \n\n$$\n\\rho_{i}\\equiv r_{i}-\\lambda_{0}=\\sum_{j=1}^{K}(\\lambda_{j}\\iota_{T}+f_{j})b_{i j}+\\varepsilon_{i},\n$$\n\nwhere $\\iota_{T}$ is a vector of $T$ ones and the following $T\\times$ 1 vectors are defined: \n\n$$\n\\begin{array}{r l}{r_{i}=(r_{i}(1),\\dots,r_{i}(T))^{\\prime},\\quad}&{i=1,\\dots,N,}\\ {\\lambda_{0}=(\\lambda_{0}(1),\\dots,\\lambda_{0}(T))^{\\prime},}\\ {f_{j}=(f_{j}(1),\\dots,f_{j}(T))^{\\prime},\\quad}&{j=1,\\dots,K,}\\ {\\varepsilon_{i}=(\\varepsilon_{i}(1),\\dots,\\varepsilon_{i}(T))^{\\prime},\\quad}&{i=1,\\dots,N.}\\end{array}\n$$\n\nWe assume that the $T\\times K$ factor matrix $F$ ,as well as the $N\\times K$ sensitivities matrix $B=[b_{i j}]$ , are of full column rank. We also assume that $T>N>K$ and that $N T>K(N+1)$ , the last ensuring that (7) has more equations than unknowns. Consequently the necessary condition for the NLSUR estimators to exist is satisfied; the Jacobian of the response function in (7) is of full column rank.",
    "category": "CORRECT",
    "question_context_used": "The system (4) is a multivariate nonlinear regression model with across-equation restrictions for which McElroy et al. (1985) showed that, conditional on the factor realizations, Gallant's (1975) NLSUR technique can be used to obtain joint estimators for the $b_{i j}$ 's and the $\\lambda_{j}^{*}\\mathbf{s}.$ Specifically, rewrite the system (4) as \n\n$$\n\\rho_{i}\\equiv r_{i}-\\lambda_{0}=\\sum_{j=1}^{K}(\\lambda_{j}\\iota_{T}+f_{j})b_{i j}+\\varepsilon_{i},\n$$\n\nwhere $\\iota_{T}$ is a vector of $T$ ones and the following $T\\times$ 1 vectors are defined: \n\n$$\n\\begin{array}{r l}{r_{i}=(r_{i}(1),\\dots,r_{i}(T))^{\\prime},\\quad}&{i=1,\\dots,N,}\\ {\\lambda_{0}=(\\lambda_{0}(1),\\dots,\\lambda_{0}(T))^{\\prime},}\\ {f_{j}=(f_{j}(1),\\dots,f_{j}(T))^{\\prime},\\quad}&{j=1,\\dots,K,}\\ {\\varepsilon_{i}=(\\varepsilon_{i}(1),\\dots,\\varepsilon_{i}(T))^{\\prime},\\quad}&{i=1,\\dots,N.}\\end{array}\n$$\n\nWe assume that the $T\\times K$ factor matrix $F$ ,as well as the $N\\times K$ sensitivities matrix $B=[b_{i j}]$ , are of full column rank. We also assume that $T>N>K$ and that $N T>K(N+1)$ , the last ensuring that (7) has more equations than unknowns. Consequently the necessary condition for the NLSUR estimators to exist is satisfied; the Jacobian of the response function in (7) is of full column rank.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2331",
    "question": "In the context of regional migration models with climate as a site-specific factor, which of the following statements are correct regarding compensation mechanisms and efficiency?",
    "options": [
      "A) Compensation for inferior climate occurs solely through wage differentials if climate affects production decisions",
      "B) The market distribution of households is always optimal when utility is derived from climate consumption",
      "C) Property taxes can be decomposed into uniform national and local differential components according to Mieszkowski's hypothesis",
      "D) Migration flows adjust population stocks in response to exogenous changes in urban equilibrium conditions"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Regional Distribution of Population, Migration, and Climate.md"
    ],
    "question_context": "The model analyzes regional migration through a system of urban areas with endogenous housing prices. Climate is treated as a nontraded, nonproducible public good that enters both production and consumption decisions. Compensation occurs through wage or housing price differentials, with the form dependent on how climate affects production/consumption. The market distribution is suboptimal when utility derives directly from climate consumption. Key formula: Migration flows = Δ(stock of population) / Δ(exogenous variables). The Mieszkowski hypothesis tests whether property taxes can be separated into uniform national and local differential components. The model extends prior work by incorporating spatially variant endogenous housing prices.",
    "category": "CORRECT",
    "question_context_used": "The model analyzes regional migration through a system of urban areas with endogenous housing prices. Climate is treated as a nontraded, nonproducible public good that enters both production and consumption decisions. Compensation occurs through wage or housing price differentials, with the form dependent on how climate affects production/consumption. The market distribution is suboptimal when utility derives directly from climate consumption. Key formula: Migration flows = Δ(stock of population) / Δ(exogenous variables). The Mieszkowski hypothesis tests whether property taxes can be separated into uniform national and local differential components. The model extends prior work by incorporating spatially variant endogenous housing prices.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"C\", \"D\"]\n}",
      "model_answer": [
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-829",
    "question": "In Hopenhayn's model of firm dynamics, which of the following conditions are necessary for a stationary equilibrium with positive entry and exit rates?",
    "options": [
      "A) A positive fixed cost for incumbent firms ($c_f > 0$)",
      "B) A Markov process for productivity shocks that is strictly decreasing in current shock (Assumption A.3.b)",
      "C) The existence of a non-recoverable entry cost ($c_e \\geq 0$)",
      "D) A recurrence condition ensuring finite firm lifespan (Assumption A.4)"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "Hopenhayn-EntryExitfirm-1992.md"
    ],
    "question_context": "The paper develops a dynamic stochastic model for a competitive industry with endogenous entry/exit decisions. Key assumptions include: (i) Firms face individual productivity shocks following a Markov process $F(\\varphi'|\\varphi)$ (Assumption A.3), (ii) A positive fixed cost $c_f$ must be paid by incumbents (equivalent to outside opportunity cost), and (iii) Entry requires non-recoverable cost $c_e$. The stationary equilibrium requires equal entry/exit rates, with distributions remaining constant through offsetting firm dynamics. The model's key equation for aggregate supply is:\n\n$$Q^{s}(\\mu,p,w)=\\int q(\\varphi,p,w)\\mu(d\\varphi)$$\n\nCritical conditions for positive turnover include: recurrence (A.4) ensuring finite firm lifespans, positive fixed costs enabling exit, and entry costs shaping the entry decision. The productivity shock process must satisfy $F^{n}(\\varepsilon|\\varphi)>0$ for some $n$, preventing mass accumulation in no-exit regions.",
    "category": "CORRECT",
    "question_context_used": "The paper develops a dynamic stochastic model for a competitive industry with endogenous entry/exit decisions. Key assumptions include: (i) Firms face individual productivity shocks following a Markov process $F(\\varphi'|\\varphi)$ (Assumption A.3), (ii) A positive fixed cost $c_f$ must be paid by incumbents (equivalent to outside opportunity cost), and (iii) Entry requires non-recoverable cost $c_e$. The stationary equilibrium requires equal entry/exit rates, with distributions remaining constant through offsetting firm dynamics. The model's key equation for aggregate supply is:\n\n$$Q^{s}(\\mu,p,w)=\\int q(\\varphi,p,w)\\mu(d\\varphi)$$\n\nCritical conditions for positive turnover include: recurrence (A.4) ensuring finite firm lifespans, positive fixed costs enabling exit, and entry costs shaping the entry decision. The productivity shock process must satisfy $F^{n}(\\varepsilon|\\varphi)>0$ for some $n$, preventing mass accumulation in no-exit regions.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"C\", \"D\"]}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-1104",
    "question": "According to the network structure analysis in economic frameworks, which of the following statements are correct regarding the effects of increasing network density and span on unraveling?",
    "options": [
      "A) Increasing a network's span always leads to greater unraveling regardless of initial conditions.",
      "B) Increasing a network's density leads to greater unraveling if the initial network is sparse, but lesser unraveling if the initial network is dense.",
      "C) In the context of Proposition 2, a complete market always exhibits the highest level of unraveling due to perfect information diffusion.",
      "D) The non-monotonicity in Proposition 2 arises from the opposing effects of network span and density during the pruning and coordination steps."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Social-networks-and-unraveling-in-labor-market_2013_Journal-of-Economic-Theo.md"
    ],
    "question_context": "The text distinguishes between increasing a network's density and increasing its span, highlighting their differing effects on unraveling. Increasing the span by connecting previously unconnected workers or firms leads to greater unraveling, as stated in Proposition 1. The effects of increasing density are non-monotonic: greater density increases unraveling in sparse networks but decreases it in dense networks, as formalized in Proposition 2. Key formula: For degree distributions $\\theta_{W},\\theta_{F}$, let $\\theta_{W}^{\\rho}, \\theta_{F}^{\\rho}$ be degree distributions such that for every $r$, $\\theta_{W}^{\\rho}(\\rho\\cdot r)=\\theta_{W}(r)$, and $\\theta_{F}^{\\rho}(\\rho\\cdot r)=\\theta_{F}(r)$. The non-monotonicity in Proposition 2 is driven by the trade-off between the direct effect of pruning (increasing span) and the indirect effect of coordination (reducing effective span).",
    "category": "INCORRECT",
    "question_context_used": "The text distinguishes between increasing a network's density and increasing its span, highlighting their differing effects on unraveling. Increasing the span by connecting previously unconnected workers or firms leads to greater unraveling, as stated in Proposition 1. The effects of increasing density are non-monotonic: greater density increases unraveling in sparse networks but decreases it in dense networks, as formalized in Proposition 2. Key formula: For degree distributions $\\theta_{W},\\theta_{F}$, let $\\theta_{W}^{\\rho}, \\theta_{F}^{\\rho}$ be degree distributions such that for every $r$, $\\theta_{W}^{\\rho}(\\rho\\cdot r)=\\theta_{W}(r)$, and $\\theta_{F}^{\\rho}(\\rho\\cdot r)=\\theta_{F}(r)$. The non-monotonicity in Proposition 2 is driven by the trade-off between the direct effect of pruning (increasing span) and the indirect effect of coordination (reducing effective span).",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"B\", \"D\"]\n}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3064",
    "question": "Which of the following statements are supported by the empirical evidence on the effects of interstate branching deregulation in the United States between 1994 and 2005?",
    "options": [
      "A) Deregulation led to lower mortgage rates due to cost savings passed through to borrowers.",
      "B) The proportion of nonperforming loans increased significantly as a result of deregulation.",
      "C) House prices increased more in counties with inelastic housing supply than in those with elastic supply.",
      "D) Commercial banks headquartered in deregulating states expanded credit more than out-of-state banks."
    ],
    "gold_answer": "A, C",
    "source_files": [
      "Favara-CreditSupplyPrice-2015.md"
    ],
    "question_context": "The evidence indicates that exogenous deregulation triggered an expansion in mortgage credit supply, shifting housing demand. In counties with inelastic construction, housing prices rose, whereas housing stock increased where supply was elastic. Balance sheet data show deposit growth increased and deposit costs fell for deregulated banks, consistent with diversification gains. Mortgage rates fell, with cost savings passed through to borrowers, sustaining riskier borrowers without increasing nonperforming loans. Key formula: The causal link from deregulation to credit expansion and house prices is captured by $\\beta_{1}$ in regressions, showing pronounced effects in inelastic supply areas. Table 6 confirms deposit growth (0.0436**) and lower deposit costs (-0.00016***), with mortgage rates falling (-0.00483***) and unchanged nonperforming loans (0.00013).",
    "category": "INCORRECT",
    "question_context_used": "The evidence indicates that exogenous deregulation triggered an expansion in mortgage credit supply, shifting housing demand. In counties with inelastic construction, housing prices rose, whereas housing stock increased where supply was elastic. Balance sheet data show deposit growth increased and deposit costs fell for deregulated banks, consistent with diversification gains. Mortgage rates fell, with cost savings passed through to borrowers, sustaining riskier borrowers without increasing nonperforming loans. Key formula: The causal link from deregulation to credit expansion and house prices is captured by $\\beta_{1}$ in regressions, showing pronounced effects in inelastic supply areas. Table 6 confirms deposit growth (0.0436**) and lower deposit costs (-0.00016***), with mortgage rates falling (-0.00483***) and unchanged nonperforming loans (0.00013).",
    "gold_answer_parsed": [
      "A",
      "C"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-235",
    "question": "In the context of self-organized criticality in economic fluctuations, which of the following conditions are necessary for aggregate fluctuations to persist in the large-economy limit?",
    "options": [
      "A) Global and linear interactions between economic units",
      "B) Local and significantly nonlinear interactions between economic units",
      "C) Exogenous shocks synchronized across the entire economy",
      "D) High-dimensional attractors in economic time series"
    ],
    "gold_answer": "B",
    "source_files": [
      "Scheinkman-SelfOrganizedCriticalityEconomic-1994.md"
    ],
    "question_context": "The type of macroscopic instability that can result has been studied by condensed-matter physicists, under the name of \\\"self-organized criticality\\\" (Per Bak and Kan Chen, 1991). Physicists have noted, in several contexts, the possibility of a \\\"critical state,\\\" in which independent microscopic fluctuations can propagate so as to give rise to instability on a macroscopic scale. This is a state in which chain reactions initiated by local disturbances neither damp out over a short distance (the \\\"subcritical\\\" case) nor propagate explosively so that the system cannot remain in that state (the \\\"supercritical\\\" case). In our work with Bak and Chen (Bak et al., 1993), we show the occurrence of a self-organized critical state as a result of the factor-demand linkages between sectors in a large economy. Our model is one in which autonomous shocks to final-goods demand have effects on production that propagate, both through time and between sectors, due to inventory dynamics. Before describing this model, we first discuss why, in order for aggregate fluctuations to persist in the large-economy limit, it is important that interactions between economic units be both local and significantly nonlinear.",
    "category": "CORRECT",
    "question_context_used": "The type of macroscopic instability that can result has been studied by condensed-matter physicists, under the name of \\\"self-organized criticality\\\" (Per Bak and Kan Chen, 1991). Physicists have noted, in several contexts, the possibility of a \\\"critical state,\\\" in which independent microscopic fluctuations can propagate so as to give rise to instability on a macroscopic scale. This is a state in which chain reactions initiated by local disturbances neither damp out over a short distance (the \\\"subcritical\\\" case) nor propagate explosively so that the system cannot remain in that state (the \\\"supercritical\\\" case). In our work with Bak and Chen (Bak et al., 1993), we show the occurrence of a self-organized critical state as a result of the factor-demand linkages between sectors in a large economy. Our model is one in which autonomous shocks to final-goods demand have effects on production that propagate, both through time and between sectors, due to inventory dynamics. Before describing this model, we first discuss why, in order for aggregate fluctuations to persist in the large-economy limit, it is important that interactions between economic units be both local and significantly nonlinear.",
    "gold_answer_parsed": [
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\"]}",
      "model_answer": [
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2519",
    "question": "In the theoretical model analyzing segregation's effects on black communities, which of the following conditions must hold for Proposition 1 (increased racial segregation reduces welfare for unskilled blacks) to be valid?",
    "options": [
      "A) The housing price effect dominates the human capital spillover effect in the skilled black neighborhood",
      "B) The human capital spillover effect dominates the housing price effect in the skilled black neighborhood",
      "C) The complementarity condition H₁₂(.,.) > 0 is violated",
      "D) The discrimination cost δ_b decreases while maintaining equilibrium stability"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Are Ghettos Good or Bad.md"
    ],
    "question_context": "The model analyzes segregation through a utility function: \n\n$$U_{k}=H(H_{Parent},H_{Community})-C_{j}(P_{j})-\\updelta_{k}I(Community)$$\n\nwhere $H_{12}(.,.)>0$ ensures complementarity between parental and community human capital. Proposition 1 states that increased racial segregation (higher δ_b) reduces welfare for unskilled blacks when it leads to increased skill segregation within the black community. This occurs when the housing price effect ($C'_s(P_s)P_s > H_2(\\underline{H},H_s)(\\overline{H}-H_s)$) dominates spillover effects, forcing unskilled blacks into neighborhoods with lower human capital. The equilibrium remains stable when $C'_W(P_W) + C'_S(P_S) > H_2(\\overline{H},H_S)\\frac{dH_S}{dN_{SW}}$.",
    "category": "INCORRECT",
    "question_context_used": "The model analyzes segregation through a utility function: \n\n$$U_{k}=H(H_{Parent},H_{Community})-C_{j}(P_{j})-\\updelta_{k}I(Community)$$\n\nwhere $H_{12}(.,.)>0$ ensures complementarity between parental and community human capital. Proposition 1 states that increased racial segregation (higher δ_b) reduces welfare for unskilled blacks when it leads to increased skill segregation within the black community. This occurs when the housing price effect ($C'_s(P_s)P_s > H_2(\\underline{H},H_s)(\\overline{H}-H_s)$) dominates spillover effects, forcing unskilled blacks into neighborhoods with lower human capital. The equilibrium remains stable when $C'_W(P_W) + C'_S(P_S) > H_2(\\overline{H},H_S)\\frac{dH_S}{dN_{SW}}$.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\"]}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3762",
    "question": "In the context of the Fujita-Krugman model of city formation, which of the following statements accurately describe the causes and implications of equilibrium indeterminacy?",
    "options": [
      "A) The equilibrium set's high dimensionality stems from insufficient market conditions to determine relative distances between city locations.",
      "B) Indeterminacy arises due to parallel spatial shifts of variables, which can be resolved by normalizing one city's location.",
      "C) The model's predictive power is enhanced by the high-dimensional equilibrium sets, allowing for precise comparative statics.",
      "D) Adding land markets for firms as an input would reduce the dimension of the equilibrium set by introducing additional equations."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "The-indeterminacy-of-equilibrium-city-formation-under-mo_2006_Journal-of-Eco.md"
    ],
    "question_context": "The Fujita-Krugman model examines the indeterminacy of equilibrium in city formation under monopolistic competition and increasing returns. Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. The paper demonstrates that for any positive integer $K$, the set of equilibria with $K$ distinct cities has a smooth manifold of dimension $K-1$ as its interior for almost all parameter values. This high-dimensional indeterminacy arises because the market lacks sufficient conditions to pin down the relative distances between city locations, beyond the trivial indeterminacy from parallel spatial shifts (resolvable by normalization). The model's predictive power is thus limited, and differential comparative statics are invalid. Attempts to reduce indeterminacy by adding markets (e.g., land markets for firms) fail because they introduce as many new variables as equations.",
    "category": "INCORRECT",
    "question_context_used": "The Fujita-Krugman model examines the indeterminacy of equilibrium in city formation under monopolistic competition and increasing returns. Key formula: The total cost is minimized if the lead time is deterministic and there's a data-sharing mechanism. The paper demonstrates that for any positive integer $K$, the set of equilibria with $K$ distinct cities has a smooth manifold of dimension $K-1$ as its interior for almost all parameter values. This high-dimensional indeterminacy arises because the market lacks sufficient conditions to pin down the relative distances between city locations, beyond the trivial indeterminacy from parallel spatial shifts (resolvable by normalization). The model's predictive power is thus limited, and differential comparative statics are invalid. Attempts to reduce indeterminacy by adding markets (e.g., land markets for firms) fail because they introduce as many new variables as equations.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4159",
    "question": "Which of the following statements are true regarding the effects of measurement error and school selectivity on earnings estimates in the National Longitudinal Study of the High School Class of 1972 (NLS-72)?",
    "options": [
      "A) The attenuation bias in the school-average SAT coefficient is estimated to be 5% in the OLS model and 31% in the matched-applicant model.",
      "B) The self-revelation model suggests that the effect of school-average SAT score on earnings is close to zero, with a standard error of 0.023.",
      "C) The interaction between school-average SAT and parental income is positive, indicating a higher payoff for children from higher income households.",
      "D) The Barron's dummies are jointly significant at the 10% level in the basic model but insignificant in the self-revelation model."
    ],
    "gold_answer": "A, B, D",
    "source_files": [
      "Estimating the Payoff to Attending a More Selective College An Application of Selection on Observables and Unobservables.md"
    ],
    "question_context": "As is well-known, attenuation bias due to classical measurement error in an explanatory variable is exacerbated in fixed effects models (e.g., Griliches [1986]). Average SAT scores for some colleges as recorded in the HERI data are measured with error, since the data are self-reported by colleges, and colleges have an incentive to misrepresent their data. The correlation (weighted by number of students) between the school-average SAT score as measured by HERI data and the school average calculated from the students in the C&B database for 30 schools is .95. This correlation provides a rough estimate of the reliability of the SAT data, which we denote 入. The proportional attenuation bias in the school-average SAT coefficient for a large sample is given by $\\lambda^{\\prime}=(\\lambda-R^{2})/(1-R^{2})$ ,where $R^{2}$ isthe coefficient of determination from a regression of the school SAT score on the other variables in the regression equation. In the OLS model in column 1, the attenuation bias is estimated to equal 5 percent. Relative to the OLS model, the estimated attenuation bias is 31 percent in the matched-applicant model and 8 percent in the self-revelation model.16 Although the attenuation bias is nontrivial, even with this amount of measurement error it is likely that sizable effects would be detected. Moreover, one would not expect attenuation bias to cause the estimate to become negative, as was found in Table Ifl. The self-revelation model reported in column 3 suggests that the effect of school-average SAT score is close to zero, although the standard error of .023 makes it difficult to draw a precise inference. The school SAT score estimates based on the comparable C&B sample are similar: the coefficient (standard error) on school-average SAT score was .074 (.014) in the basic model and -.006 (.015) in the self-revelation model using the C&B sample and imposing similar sample restrictions (in 1995 dollars). These results suggest that our findings in Table III are not unique to the schools covered by the C&B survey. The Barron's dummies are jointly significant at the 10 percent level in the basic model $(p=.06)$ , they are insignificant in the self-revelation model $(p=.59)$ . Thus, our findings for the school-average SAT scores appear to be robust when other measures of school selectivity are used.",
    "category": "CORRECT",
    "question_context_used": "As is well-known, attenuation bias due to classical measurement error in an explanatory variable is exacerbated in fixed effects models (e.g., Griliches [1986]). Average SAT scores for some colleges as recorded in the HERI data are measured with error, since the data are self-reported by colleges, and colleges have an incentive to misrepresent their data. The correlation (weighted by number of students) between the school-average SAT score as measured by HERI data and the school average calculated from the students in the C&B database for 30 schools is .95. This correlation provides a rough estimate of the reliability of the SAT data, which we denote 入. The proportional attenuation bias in the school-average SAT coefficient for a large sample is given by $\\lambda^{\\prime}=(\\lambda-R^{2})/(1-R^{2})$ ,where $R^{2}$ isthe coefficient of determination from a regression of the school SAT score on the other variables in the regression equation. In the OLS model in column 1, the attenuation bias is estimated to equal 5 percent. Relative to the OLS model, the estimated attenuation bias is 31 percent in the matched-applicant model and 8 percent in the self-revelation model.16 Although the attenuation bias is nontrivial, even with this amount of measurement error it is likely that sizable effects would be detected. Moreover, one would not expect attenuation bias to cause the estimate to become negative, as was found in Table Ifl. The self-revelation model reported in column 3 suggests that the effect of school-average SAT score is close to zero, although the standard error of .023 makes it difficult to draw a precise inference. The school SAT score estimates based on the comparable C&B sample are similar: the coefficient (standard error) on school-average SAT score was .074 (.014) in the basic model and -.006 (.015) in the self-revelation model using the C&B sample and imposing similar sample restrictions (in 1995 dollars). These results suggest that our findings in Table III are not unique to the schools covered by the C&B survey. The Barron's dummies are jointly significant at the 10 percent level in the basic model $(p=.06)$ , they are insignificant in the self-revelation model $(p=.59)$ . Thus, our findings for the school-average SAT scores appear to be robust when other measures of school selectivity are used.",
    "gold_answer_parsed": [
      "A",
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"D\"]}",
      "model_answer": [
        "A",
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-653",
    "question": "Which of the following statements are correct regarding the computation of S-Gini and E-Gini indices from empirical data?",
    "options": [
      "A) The S-Gini index computation requires integration over the empirical quantile function weighted by a power of (1-p).",
      "B) The E-Gini index computation involves integrating a linear function of p over intervals defined by empirical distribution steps.",
      "C) Both S-Gini and E-Gini indices can be computed without knowledge of the empirical Lorenz curve.",
      "D) The computation of E-Gini indices requires solving a nonlinear optimization problem for each interval."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "00-Statistical_Inference_with_Generalized_Gini_Indices_of_Inequality__Poverty__and_Welfare.md"
    ],
    "question_context": "The S-Gini indices are computed using the representation: \n$$\\hat{W}^{\\delta}=\\sum_{j=1}^{\\hat{N}}y_{j}\\Big\\{\\big(1-\\hat{p}_{j-1}\\big)\\big)^{\\delta}-\\big(1-\\hat{p}_{j}\\big)^{\\delta}\\Big\\},$$\nwhere $\\hat{p}_{j}$ are the steps of the empirical distribution function. For the E-Gini indices, the computation involves:\n$$\\hat{T}_{E}=\\frac{1}{\\alpha+1}\\sum_{j=1}^{\\dot{N}}\\left[1(\\hat{\\mu}\\neq y_{j})\\frac{1}{b_{j}}\\left\\{(b_{j}\\hat{p}_{j}-a_{j})^{\\alpha+1}-(b_{j}\\hat{p}_{j-1}-a_{j})^{\\alpha+1}\\right\\}\\right.$$\nwhere $b_{j}=(\\hat{\\mu}-y_{j})$ and $a_{j}=\\sum_{l=1}^{j-1}\\hat{\\pi}_{l}(y_{l}-y_{j})$. These computations rely on the piecewise linear nature of the empirical generalized Lorenz curve and quantile function.",
    "category": "CORRECT",
    "question_context_used": "The S-Gini indices are computed using the representation: \n$$\\hat{W}^{\\delta}=\\sum_{j=1}^{\\hat{N}}y_{j}\\Big\\{\\big(1-\\hat{p}_{j-1}\\big)\\big)^{\\delta}-\\big(1-\\hat{p}_{j}\\big)^{\\delta}\\Big\\},$$\nwhere $\\hat{p}_{j}$ are the steps of the empirical distribution function. For the E-Gini indices, the computation involves:\n$$\\hat{T}_{E}=\\frac{1}{\\alpha+1}\\sum_{j=1}^{\\dot{N}}\\left[1(\\hat{\\mu}\\neq y_{j})\\frac{1}{b_{j}}\\left\\{(b_{j}\\hat{p}_{j}-a_{j})^{\\alpha+1}-(b_{j}\\hat{p}_{j-1}-a_{j})^{\\alpha+1}\\right\\}\\right.$$\nwhere $b_{j}=(\\hat{\\mu}-y_{j})$ and $a_{j}=\\sum_{l=1}^{j-1}\\hat{\\pi}_{l}(y_{l}-y_{j})$. These computations rely on the piecewise linear nature of the empirical generalized Lorenz curve and quantile function.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-2929",
    "question": "Based on the experimental evidence from Benin, which of the following statements are correct regarding the impact of different contract farming attributes on rural transformation?",
    "options": [
      "A) Fixed-price contracts alone produce outcomes statistically indistinguishable from more complex contracts with extension training and input loans",
      "B) Extension training consistently provided significant added value above fixed-price contracts in improving yields",
      "C) Input-supply contracts were found to be strictly necessary for achieving productivity gains in staple crop cultivation",
      "D) The primary constraint facing farmers was output price risk, which when resolved allowed them to address other constraints independently"
    ],
    "gold_answer": "A, D",
    "source_files": [
      "Contract-farming-and-rural-transformation--Evidence_2021_Journal-of-Developm.md"
    ],
    "question_context": "The study presents experimental evidence from Benin showing that contract farming significantly impacts rural transformation measures. Key findings include: (1) All contracts improved welfare and productivity, with fixed-price contracts performing nearly as well as more complex ones. The formula for input use distortion shows: Input distortion = f(price risk, technical inefficiency, capital constraints). (2) Surprisingly, extension training added no significant value beyond fixed-price contracts, possibly due to farmers' resource limitations or training misalignment. (3) The fixed-price contract's strong performance suggests price risk resolution is primary, after which farmers independently address other constraints. This aligns with theoretical models where ΔProductivity = α(price certainty) + β(self-resolution). The results challenge assumptions about necessary contract complexity for staple crops.",
    "category": "CORRECT",
    "question_context_used": "The study presents experimental evidence from Benin showing that contract farming significantly impacts rural transformation measures. Key findings include: (1) All contracts improved welfare and productivity, with fixed-price contracts performing nearly as well as more complex ones. The formula for input use distortion shows: Input distortion = f(price risk, technical inefficiency, capital constraints). (2) Surprisingly, extension training added no significant value beyond fixed-price contracts, possibly due to farmers' resource limitations or training misalignment. (3) The fixed-price contract's strong performance suggests price risk resolution is primary, after which farmers independently address other constraints. This aligns with theoretical models where ΔProductivity = α(price certainty) + β(self-resolution). The results challenge assumptions about necessary contract complexity for staple crops.",
    "gold_answer_parsed": [
      "A",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\"]\n}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3597",
    "question": "In the context of Theorem 2, which of the following conditions ensure that the probability of ultimate extinction is less than 1?",
    "options": [
      "A) The maximal Lyapunov exponent $E(\\lambda)$ is strictly positive for sufficiently small $\\lambda$",
      "B) The characteristic root $\\rho^1$ is strictly positive",
      "C) The mutation vector $\\lambda$ is sufficiently small such that $\\widetilde{E}(\\lambda) < 0$",
      "D) The norm $\\|z_t\\|$ converges to infinity with probability bounded away from zero as $\\lambda \\to 0$"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "A-Biological-Basis-for-Expected-and-Non-expected_1996_Journal-of-Economic-Th.md"
    ],
    "question_context": "Key results from Theorem 2: (i) If $E(\\lambda) < 0$ for sufficiently small $\\lambda$, then the probability of ultimate extinction is 1. (ii) If $\\rho^1 > 0$, then for sufficiently small $\\lambda$, $E(\\lambda) \\geq \\ln(A_1) + \\rho^1 > 0$, implying the probability of ultimate extinction is less than 1. The proof involves analyzing the Lyapunov exponent $E(\\lambda) = \\lim_{T \\to \\infty} (1/T) \\ln \\|M(\\xi_0, \\lambda) \\cdots M(\\xi_{T-1}, \\lambda)\\|$ and the characteristic root $\\rho^1$ of the matrix $M(\\xi_t, 0)$. The condition $\\widetilde{E}(\\lambda) < 0$ (from Claim 1) ensures convergence properties but does not directly relate to extinction probability.",
    "category": "INCORRECT",
    "question_context_used": "Key results from Theorem 2: (i) If $E(\\lambda) < 0$ for sufficiently small $\\lambda$, then the probability of ultimate extinction is 1. (ii) If $\\rho^1 > 0$, then for sufficiently small $\\lambda$, $E(\\lambda) \\geq \\ln(A_1) + \\rho^1 > 0$, implying the probability of ultimate extinction is less than 1. The proof involves analyzing the Lyapunov exponent $E(\\lambda) = \\lim_{T \\to \\infty} (1/T) \\ln \\|M(\\xi_0, \\lambda) \\cdots M(\\xi_{T-1}, \\lambda)\\|$ and the characteristic root $\\rho^1$ of the matrix $M(\\xi_t, 0)$. The condition $\\widetilde{E}(\\lambda) < 0$ (from Claim 1) ensures convergence properties but does not directly relate to extinction probability.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-845",
    "question": "Which of the following statements are supported by the empirical findings regarding pro-cyclical mortality and its relationship with unemployment rates?",
    "options": [
      "A) Mortality rates among the elderly are more responsive to unemployment rates than those among younger age groups.",
      "B) The quality of health care in nursing homes deteriorates during economic expansions due to staffing shortages.",
      "C) Motor vehicle accidents account for the majority of cyclically induced deaths across all age groups.",
      "D) Age-adjusting mortality rates eliminates the negative correlation between unemployment and mortality."
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Stevens-BestTimesWorst-2015.md"
    ],
    "question_context": "The study finds that pro-cyclical mortality is not primarily driven by individual-level behavioral changes but rather by systemic factors affecting the elderly, particularly in nursing homes. Key findings include that deaths in nursing homes are highly responsive to state unemployment rates and that employment levels in skilled nursing facilities decline when unemployment falls. The regression model used is:\n\n$$\nH_{j t}=\\alpha_{t}+{\\bf X}_{j t}\\beta+E_{j t}\\gamma+S_{j}+S_{j}T+\\varepsilon_{j t},\n$$\n\nwhere $H$ is the log mortality rate, $E$ is the state's economic health measure, and $\\mathbf{X}$ includes demographic controls. The analysis shows a consistent negative relationship between unemployment rates and mortality, with age-adjustment playing a crucial role in isolating this effect from demographic shifts.",
    "category": "INCORRECT",
    "question_context_used": "The study finds that pro-cyclical mortality is not primarily driven by individual-level behavioral changes but rather by systemic factors affecting the elderly, particularly in nursing homes. Key findings include that deaths in nursing homes are highly responsive to state unemployment rates and that employment levels in skilled nursing facilities decline when unemployment falls. The regression model used is:\n\n$$\nH_{j t}=\\alpha_{t}+{\\bf X}_{j t}\\beta+E_{j t}\\gamma+S_{j}+S_{j}T+\\varepsilon_{j t},\n$$\n\nwhere $H$ is the log mortality rate, $E$ is the state's economic health measure, and $\\mathbf{X}$ includes demographic controls. The analysis shows a consistent negative relationship between unemployment rates and mortality, with age-adjustment playing a crucial role in isolating this effect from demographic shifts.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\"]\n}",
      "model_answer": [
        "A"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3894",
    "question": "In the context of structural transformation and human capital accumulation, which of the following factors are identified by Herrendorf and Schoellman (2018) as significant barriers?",
    "options": [
      "A) Institutional rigidities in labor market regulations",
      "B) Mismatch between sectoral skill requirements and workforce capabilities",
      "C) Inefficient capital allocation across sectors due to financial frictions",
      "D) Persistent wage differentials between traditional and modern sectors"
    ],
    "gold_answer": "B, D",
    "source_files": [
      "Herrendorf-WagesHumanCapital-2018.md"
    ],
    "question_context": "The study by Herrendorf and Schoellman (2018) examines the role of human capital in structural transformation, focusing on wage differentials and sectoral shifts. Key findings highlight that wage gaps between traditional and modern sectors persist due to skill mismatches, slowing down the reallocation of labor. The authors emphasize that structural transformation is hindered when the workforce lacks the necessary skills for high-productivity sectors, creating a barrier to economic modernization. Their model incorporates sector-specific human capital accumulation, showing how persistent wage differentials can arise even in competitive labor markets.",
    "category": "CORRECT",
    "question_context_used": "The study by Herrendorf and Schoellman (2018) examines the role of human capital in structural transformation, focusing on wage differentials and sectoral shifts. Key findings highlight that wage gaps between traditional and modern sectors persist due to skill mismatches, slowing down the reallocation of labor. The authors emphasize that structural transformation is hindered when the workforce lacks the necessary skills for high-productivity sectors, creating a barrier to economic modernization. Their model incorporates sector-specific human capital accumulation, showing how persistent wage differentials can arise even in competitive labor markets.",
    "gold_answer_parsed": [
      "B",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"B\", \"D\"]}",
      "model_answer": [
        "B",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-4588",
    "question": "According to recent research on probability weighting in prospect theory, which of the following phenomena can be explained by the overweighting of tail events?",
    "options": [
      "A) The overpricing of positively skewed stocks in financial markets",
      "B) The preference for low-deductible insurance policies",
      "C) The high historical US equity premium",
      "D) The popularity of casino gambling and the favorite-longshot bias"
    ],
    "gold_answer": "A, B, C, D",
    "source_files": [
      "Barberis-PsychologyTailEvents-2013.md"
    ],
    "question_context": "Probability weighting, a key component of prospect theory, has been empirically linked to various economic phenomena. Barberis and Huang (2008) demonstrate that in financial markets, probability weighting leads to the pricing of skewness in asset returns, causing positively skewed stocks to be overpriced. This is because investors overweight the small chance of a highly favorable outcome. Similarly, the aggregate stock market's negative skewness, characterized by occasional large crashes, leads investors to demand a high equity premium due to the overweighting of these tail events (De Giorgi and Legg 2012). In insurance markets, probability weighting explains the preference for low-deductible policies, as individuals overweight the unlikely event of making a claim (Barseghyan et al. forthcoming). Additionally, probability weighting sheds light on the favorite-longshot bias in betting and the popularity of casino gambling, where individuals overweight the small probability of winning large payouts (Snowberg and Wolfers 2010; Barberis 2012).",
    "category": "INCORRECT",
    "question_context_used": "Probability weighting, a key component of prospect theory, has been empirically linked to various economic phenomena. Barberis and Huang (2008) demonstrate that in financial markets, probability weighting leads to the pricing of skewness in asset returns, causing positively skewed stocks to be overpriced. This is because investors overweight the small chance of a highly favorable outcome. Similarly, the aggregate stock market's negative skewness, characterized by occasional large crashes, leads investors to demand a high equity premium due to the overweighting of these tail events (De Giorgi and Legg 2012). In insurance markets, probability weighting explains the preference for low-deductible policies, as individuals overweight the unlikely event of making a claim (Barseghyan et al. forthcoming). Additionally, probability weighting sheds light on the favorite-longshot bias in betting and the popularity of casino gambling, where individuals overweight the small probability of winning large payouts (Snowberg and Wolfers 2010; Barberis 2012).",
    "gold_answer_parsed": [
      "A",
      "B",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\n    \"answer\": [\"A\", \"B\", \"C\", \"D\"]\n}",
      "model_answer": [
        "A",
        "B",
        "C",
        "D"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-51",
    "question": "In the context of beliefs and portfolios as discussed by Giglio et al., which of the following factors are empirically shown to significantly influence portfolio choices?",
    "options": [
      "A) Heterogeneous beliefs about long-term economic growth",
      "B) Subjective mortality expectations",
      "C) Time-varying risk aversion",
      "D) Social media sentiment trends"
    ],
    "gold_answer": "A, B",
    "source_files": [
      "Giglio-FiveFactsBeliefs-2021.md"
    ],
    "question_context": "The study by Giglio et al. examines how subjective beliefs influence household portfolio choices. Key findings include that heterogeneous beliefs about long-term economic growth and subjective mortality expectations are significant determinants of portfolio allocations. The authors utilize large-scale survey data combined with actual portfolio holdings to establish these relationships. The empirical framework controls for traditional factors like wealth, income, and demographic characteristics, isolating the role of beliefs. Formula: Portfolio allocation = f(Beliefs, Wealth, Controls) + ε, where Beliefs are decomposed into economic growth expectations and mortality risk perceptions.",
    "category": "CORRECT",
    "question_context_used": "The study by Giglio et al. examines how subjective beliefs influence household portfolio choices. Key findings include that heterogeneous beliefs about long-term economic growth and subjective mortality expectations are significant determinants of portfolio allocations. The authors utilize large-scale survey data combined with actual portfolio holdings to establish these relationships. The empirical framework controls for traditional factors like wealth, income, and demographic characteristics, isolating the role of beliefs. Formula: Portfolio allocation = f(Beliefs, Wealth, Controls) + ε, where Beliefs are decomposed into economic growth expectations and mortality risk perceptions.",
    "gold_answer_parsed": [
      "A",
      "B"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\"]}",
      "model_answer": [
        "A",
        "B"
      ],
      "error": null
    }
  },
  {
    "qid": "economic-multiple-choice-3673",
    "question": "Based on the regression model and empirical results, which of the following statements are statistically significant findings regarding the effect of social ties on publication outcomes?",
    "options": [
      "A) Faculty colleagues experience an 8% increase in the number of articles when their connection is editor",
      "B) Coauthors show a statistically significant increase in lead articles when their connection is editor",
      "C) Ph.D. advisors experience a 6-page increase in published pages per term when their former student is editor",
      "D) Editors in the same field show no statistically significant effect on publication outcomes of field peers"
    ],
    "gold_answer": "A, C, D",
    "source_files": [
      "rest_a_00666.md"
    ],
    "question_context": "The study estimates publication outcomes using the linear regression model: $$y_{i j t}=\\upbeta_{0}+\\upbeta_{1}I n C h a r g e_{i j t}+\\uplambda_{i}+\\upzeta_{j t}+u_{i j t},$$ where $y_{i j t}$ represents outcomes for scholars connected to editor $i$ in journal $j$ at time $t$. Panel A shows faculty colleagues experience an 8% increase in articles (p<0.05) and Ph.D. advisors a 14% increase (p<0.10) when their connection is editor. Panel B indicates Ph.D. advisors gain 6 pages/term (p<0.05). Panel C finds only faculty colleagues show statistically significant increases in lead articles (p<0.05). Column 6 demonstrates no significant effects for same-field connections, suggesting field effects don't drive the results.",
    "category": "INCORRECT",
    "question_context_used": "The study estimates publication outcomes using the linear regression model: $$y_{i j t}=\\upbeta_{0}+\\upbeta_{1}I n C h a r g e_{i j t}+\\uplambda_{i}+\\upzeta_{j t}+u_{i j t},$$ where $y_{i j t}$ represents outcomes for scholars connected to editor $i$ in journal $j$ at time $t$. Panel A shows faculty colleagues experience an 8% increase in articles (p<0.05) and Ph.D. advisors a 14% increase (p<0.10) when their connection is editor. Panel B indicates Ph.D. advisors gain 6 pages/term (p<0.05). Panel C finds only faculty colleagues show statistically significant increases in lead articles (p<0.05). Column 6 demonstrates no significant effects for same-field connections, suggesting field effects don't drive the results.",
    "gold_answer_parsed": [
      "A",
      "C",
      "D"
    ],
    "model_evaluation_result": {
      "model_raw_response": "{\"answer\": [\"A\", \"B\", \"C\"]}",
      "model_answer": [
        "A",
        "B",
        "C"
      ],
      "error": null
    }
  }
]